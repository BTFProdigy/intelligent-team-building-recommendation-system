Memory-Inductive Categorial Grammar:
An Approach to Gap Resolution in Analytic-Language Translation
Prachya Boonkwan Thepchai Supnithi
Human Language Technology Laboratory
National Electronics and Computer Technology Center (NECTEC)
112 Thailand Science Park, Phaholyothin Road,
Khlong 1, Pathumthani 12120, Thailand
{prachya.boonkwan, thepchai.supnithi}@nectec.or.th
Abstract
This paper presents a generalized frame-
work of syntax-based gap resolution in ana-
lytic language translation using an extended
version of categorial grammar. Translat-
ing analytic languages into Indo-European
languages suffers the issues of gapping,
because ?deletion under coordination? and
?verb serialization? are necessary to be re-
solved beforehand. Rudimentary operations,
i.e. antecedent memorization, gap induction,
and gap resolution, were introduced to the
categorial grammar to resolve gapping is-
sues syntactically. Hereby, pronominal ref-
erences can be generated for deletion under
coordination, while sentence structures can
be properly selected for verb serialization.
1 Background
Analytic language, such as Chinese, Thai, and Viet-
namese, is any language whose syntax and meaning
relies on particles and word orders rather than inflec-
tion. Pronouns and other grammatical information,
such as tense, aspect, and number, expressed by use
of adverbs and adjectives, are often omitted. In addi-
tion to deletion under coordination and verb serial-
ization, called gapping (Hendriks, 1995), translation
from analytic languages into Indo-European ones
becomes a hard task because (1) an ordinary parser
cannot parse some problematic gapping patterns and
(2) these omissions are necessary to be resolved be-
forehand. We classify resolution of the issue into
two levels: syntactic/semantic and pragmatic. Gap-
ping, which we considered as a set of bound vari-
ables, can be resolved in syntactic/semantic level
(Partee, 1975). Omission of other grammatical in-
formation is, on the contrary, to be resolved in prag-
matic level because some extra-linguistic knowledge
is required. Consequently, we concentrate in this pa-
per the resolution of gapping by means of syntax and
semantics.
Many proposals to gap resolution were intro-
duced, but we classify them into two groups: non-
ellipsis-based and ellipsis-based. Non-ellipsis-based
approach is characterized by: (a) strong proof sys-
tem (Lambek, 1958), and (b) functional composition
and type raising that allow coordination of incom-
plete constituents, such as CG (Ajdukiewicz, 1935;
Bar-Hillel, 1953; Moortgat, 2002), CCG (Steed-
man, 2000), and multimodal CCG (Baldridge and
Kruijff, 2003). Proposals in this approach, such
as (Hendriks, 1995; Ja?ger, 1998a; Ja?ger, 1998b),
introduced specialized operators to resolve overt
anaphora, while covert anaphora is left unsolved.
Ellipsis-based approach is characterized by treat-
ing incomplete constituents as if they are of the
same simple type but contain ellipsis inside (Yatabe,
2002; Cryssmann, 2003; Beavers and Sag, 2004).
However, Beavers and Sag (2004) evidenced that
ellipsis-based analysis possibly reduces the accept-
ability of language, because the resolution is per se
completely uncontrolled.
In this paper, we introduce an integration of the
two approaches that incorporates strong proof sys-
tem and ellipsis-based analysis. Antecedent memo-
rization and gap induction are introduced to imitate
ellipsis-based analysis. The directions of ellipsis are
80
also used to improve the acceptability of language.
The rest of the paper is structured as follows. Sec-
tion 2 describes the formalization of our method.
Section 3 evidences the coverage of the framework
on coping with the gapping issues in analytic lan-
guages. Section 4 further discusses coverage and
limitations of the framework comparing with CG
and its descendants. Section 5 explains relevance
of the proposed formalism to MT. Finally, Section 6
concludes the paper and lists up future work.
2 Memory-Inductive Categorial Grammar
Memory-Inductive Categorial Grammar, abbrevi-
ated MICG, is a version of pure categorial grammar
extended by ellipsis-based analysis. On the con-
trary, it relies on antecedent memorization, gap in-
duction, and gap resolution that outperform CCG?s
functional composition and type raising.
All grammatical expressions of MICG are, like
CG, distinguished by a syntactic category identify-
ing them as either a function from arguments of one
type to result another (a.k.a. function), or an argu-
ment (a.k.a. primitive category). Let us exemplify
the MICG by defining an example grammar G be-
low.
John,Mary,sandwich,noodle ? np
eats ? (np\s)/np
and ? &
The lexicons John, Mary, sandwich, and noodle are as-
signed with a primitive category np. The lexicon
eats is assigned with a function that forms a sentence
s after taking np from the right side (/np) and then
taking np from the left side (np\). The lexicon and is
assigned with a conjunction category (&). By means
of syntactic categories assigned to each lexicon, the
derivation for a simple sentence ?John eats noodle? is
shown in (1).
John eats noodle
John ? np eats ? (np\s)/np noodle ? np
eats?noodle ? np\s
John ? (eats?noodle) ? s
(1)
CG suffers some patterns of coordination e.g.
SVO&SO as exemplified in (2).
John eats noodle, and Mary, sandwich.(2)
One should find that the second conjunct cannot be
reduced into s by means of CG, because it lacks of
the main verb ?eats.? The main verb in the first con-
junct should be remembered and then filled up to
the ellipsis of the second conjunct to accomplish the
derivation. This matter of fact motivated us to de-
velop MICG by introducing to CG the process of
remembering an antecedent from a conjunct, called
memorization, and filling up an ellipsis in the other
conjunct, called induction. There are three manda-
tory operations in MICG: antecedent memorization,
gap induction, and gap resolution.
One of two immediate formulae combined in
the derivation can be memorized as an antecedent.
The resulted syntactic category is modalized by the
modality 2DF , where D is a direction of memoriza-
tion (< for the left side and > for the right side),
and F is the memorized formula. The syntactic
structure of the memorized formula is also modal-
ized with the notation 2 to denote the memoriza-
tion. It is restricted in MICG that the memorized for-
mula must be unmodalized to maintain mild context-
sensitivity. For example, let us consider the deriva-
tion of the first conjunct of (2), ?John eats noodle,?
with antecedent memorization at the verb ?eats? in
(3). As seen, a modalized formula can combine with
another unmodalized formula while all modalities
are preserved.
John eats noodle
John ? np 2eats ? (np\s)/np noodle ? np
2eats?noodle ? 2<eats?(np\s)/np(np\s)
John ? (2eats?noodle) ? 2<eats?(np\s)/nps
(3)
Any given formula can be induced for a missing
formula, or a gap, at any direction, and the induced
gap contains a syntactic category that can be com-
bined to that of the formula. The resulted syntactic
category of combining the formula and the gap is
modalized by the modality 3DF , where D is a direc-
tion of induction, and F is the induced formula at the
gap. The syntactic structure of F is an uninstantiated
variable and also modalized with the notation 3 to
denote the induction. The induced formula is neces-
sary to be unmodalized for mild context-sensitivity.
For example, let us consider the derivation of the
second conjunct of (2), ?Mary, sandwich,? with gap in-
duction before the word ?sandwich? in (4). The vari-
81
able of syntactic structure will be resolved with an
appropriate antecedent containing the same syntac-
tic category in the gap resolution process.
Mary sandwich
Mary ? np sandwich ? np
3X ? sandwich ? 3<X?(np\s)/np(np\s)
Mary ? (3X ? sandwich) ? 3<X?(np\s)/nps
(4)
Gap resolution matches between memorized an-
tecedents and induced gaps to associate ellipses to
their antecedents during derivation of coordination
and serialization. That is, two syntactic categories
2D1F1 C and 3
D2
F2 C are matched up and canceled from
the resulted syntactic category, if they have the same
syntactic categories C, their directions D1 and D2
are equal, and their memorized/induced formulae F1
and F2 are unified. For example, let us consider the
derivation of ?John eats noodle, and Mary, sandwich?
in Figure 1. The modalities 2<eats?(np\s)/nps and
3<X?(np\s)/nps are matched up together. Their mem-
orized/induced formulae are also unified by instan-
tiating the variable X with ?eats?. Eventually, af-
ter combining them and the conjunction ?and,? the
derivation yields out the formula (John ? (2eats ?
noodle))? (and? (Mary? (3eats? sandwich))) ? s.
Gap resolution could also indicate argument shar-
ing in coordination and serialization. 3D1F1 C and
3D2F2 C can be also matched up, if they have the same
syntactic categories C, their directions D1 and D2
are equal, and their memorized/induced formulae F1
and F2 are unified. However, they must be preserved
in the resulted syntactic category. For example, let
us consider the derivation in Figure 2. By means of
unification of induced formulae, the variables X and
Y are unified into the variable Z.
A formal definition of MICG is given in Ap-
pendix A. MICG is applied to resolve deletion under
coordination and serialization in analytic languages
in the next section.
3 Gap Resolution in Analytic Languages
There are two causes of gapping in analytic lan-
guages: coordination and serial verb construction.
Each of which complicates the analysis module of
MT to resolve such issue before transferring. In this
section, problematic gapping patterns are analyzed
in forms of generalized patterns by MICG. For sim-
plification reason, syntactic structure is suppressed
during derivation.
3.1 To resolve gapping under coordination
Coordination in analytic languages is more com-
plex than that of Indo-European ones. Multi-
conjunct coordination is suppressed here because
biconjunct coordination can be applied. Besides
SVO&VO and SV&SVO patterns already resolved
by CCG (Steedman, 2000), there are also SVO&SV,
SVO&V, SVO&SO (already illustrated in Figure 1),
and SVO&SA patterns.
The pattern SVO&SV exhibits ellipsis at the ob-
ject position of the second conjunct. The analysis of
SVO&SV is illustrated in (5). It shows that the ob-
ject of the first conjunct is memorized while the verb
of the second conjunct is induced for the object.
S V O & S V
np (np\s)/np np & np (np\s)/np
2>np(np\s) 3>np(np\s)
2>nps 3>nps
s
(5)
Analysis of the sentence pattern SVO&V, illus-
trated in (6), exhibits ellipses at the subject and the
object positions of the second conjunct. The subject
and the object of the first conjunct are memorized,
while the verb of the second conjunct is induced
twice for the object and for the subject, respectively.
S V O & V
np (np\s)/np np & (np\s)/np
2>np(np\s) 3>np(np\s)
2<np2>nps 3<np3>nps
s
(6)
The pattern SVO&SA exhibits ellipsis at the pred-
icate position of the second conjunct, because only
the adverb (A) is left. Suppose the adverb, typed
(np\s)/(np\s), precedes the predicate. Illustrated in
(7), the predicate of the first conjunct is memorized,
while the adverb of the second conjunct is inducted
for the predicate.
S V O & S A
np (np\s)/np np & np (np\s)/(np\s)
np\s 3>
np\s(np\s)
2>
np\ss 3
>
np\ss
s
(7)
82
John eats noodle and Mary, sandwich
John? (2eats ?noodle) ? 2<eats?(np\s)/nps and ? & Mary? (3X ? sandwich) ? 3
<
X?(np\s)/nps
(John ? (2eats?noodle))? (and ? (Mary ? (3eats ? sandwich))) ? s
Figure 1: Derivation of ?John eats noodle, and Mary, sandwich.?
eats noodle and drinks coke
3X ? (eats?noodle) ? 3<X?nps and ? & 3Y ? (drinks?coke) ? 3<Y?nps
(3Z ? (eats ?noodle))? (and ? (3Z ? (drinks?coke))) ? 3<Z?nps
Figure 2: Preservation of modalities in derivation
3.2 To resolve gapping under serial verb
construction
Serial verb construction (SVC) (Baker, 1989) is con-
struction in which a sequence of verbs appears in
what seems to be a single clause. Usually, the
verbs have a single structural object and share log-
ical arguments (Baker, 1989). Following (Li and
Thompson, 1981; Wang, 2007; Thepkanjana, 2006),
we classify SVC into three main types: consecu-
tive/concurrent events, purpose, and circumstance.
No operation specialized for tracing antecedent
projection in consecutive/concurrent event construc-
tion has been proposed in CG or its descendants. In
MICG, the serialization operation is specialized for
this construction. For example, a Chinese sentence
from (Wang, 2007) in (8) is analyzed as in (9).
ta? ma?i pia`o j??n qu`
he buy ticket enter go
?He buys a ticket and then goes inside.?
(8)
ta? ma?i pia`o j??n qu`
np (np\s)/np np np\s np\s
np\s 3<nps 3<nps
2<nps 3<nps
s
(9)
Illustrated in (9), the subject argument ta? ?he? is pro-
jected through the verb sequence by means of mem-
orization and induction modalities.
Purpose construction can also be handled by
MICG. For example, a Thai sentence in (10) is ana-
lyzed as in (11).
kha?V tO`: thO?: paj Cha?j naj ba?:n
he attach pipe go use in house
?He attaches pipes to use in the house.?
(10)
kha?V tO`: thO?: paj Cha?j naj ba?:n
np (np\s)/np np s\s (np\s)/np (s\s)/np np
2>np(np\s) 3>np(np\s) s\s
2<np2>nps 3<np3>nps
2<np2>nps 3<np3>nps
s
(11)
Illustrated in (11), the two logical arguments, i.e. the
subject kha?V ?he? and the object thO?: ?pipe,? are pro-
jected through the construction.
SVC expressing circumstance of action is syntac-
tically considered much as consecutive event con-
struction. For example, a Chinese sentence from
(Wang, 2007) in (12) is analyzed as in (13).
wo? yo`ng kua`izi ch?? fa`n
I use chopstick eat meal
?I eat meal with chopsticks.?
(12)
wo? yo`ng kua`izi ch?? fa`n
np (np\s)/np np (np\s)/np np
np\s np\s
2<nps 3<nps
s
(13)
4 Coverage and Limitations
Proven in Theorem 1 in Appendix A, memorized
constituents and induced constituents are cross-
serially associated. Controlled by order and di-
rection, each memorized constituent is guaranteed
to be cross-serially associated to its corresponding
induced gap, while each gap pair is also cross-
serially associated revealing argument sharing. This
causes cross-serial association, illustrated in Fig-
ure 3, among memorized constituents and induced
gaps. Since paired modalities are either eliminated
or preserved and no modalities are left on the start
83
symbol, it guarantees that there is eventually no
modality in derivation. In conclusion, no excessive
gap is over-generated in the language.
p1 q1 p2 q2 . . . pn qn pn+1 qn+1 pn+2 qn+2 . . . p2n q2n p2n+1
Figure 3: Cross-serial association
MICG?s antecedent memorization and gap induc-
tion perform well in handling node raising. Node
raising is analyzed in terms of MICG by memorizing
the raised constituent at the conjunct it occurs and
inducing a gap at the other conjunct. For example,
the right node ?ice cream? is raised in the sentence ?I
like but you don?t like ice cream.? The sentence can
be analyzed in terms of MICG in (14).
I like but you don?t like ice cream
np (np\s)/np & np (np\s)/np np
3>np(np\s) 2>np(np\s)
3>nps 2>nps
s
(14)
Topicalization and contraposition are still the is-
sues to be concerned for coverage over CCG. For
example, in an example sentence ?Bagels, Yo said
that Jan likes? from (Beavers and Sag, 2004), the
NP ?Bagels? is topicalized from the object position
of the relative clause?s complement. (15) shows un-
parsability of the sentence.
Bagels, Yo said that Jan likes
np np (np\s)/cl cl/s np (np\s)/np
3>np(np\s)
3>nps
3>nps
3>np(np\s)
3>nps
?????
(15)
Furthermore, constituent shifting, such as dative
shift and adjunct shift, is not supported by MICG.
We found that it is also constituent extraction as
consecutive constituents other than the shifted one
are extracted from the sentence. For example, the
adjunct ?skillfully? is shifted next to the main verb
in the sentence ?Kahn blocked skillfully a powerful
shot by Ronaldo? from (Baldridge, 2002) in (16).
a powerful shot
Kahn blocked skillfully by Ronaldo
np (np\s)/np (np\s)\(np\s) np
3>np(np\s)
3>np(np\s)
3>nps
?????
(16)
Since MICG was inspired by reasons other than
those of CCG, the coverage of MICG is therefore
different from CCG. Let us compare CG, CCG, and
MICG in Table 1. CCG initially attempted to han-
dle linguistic phenomena in English and other Indo-
European languages, in which topicalization and da-
tive shift play an important role. Applied to many
other languages such as German, Dutch, Japanese,
and Turkish, CCG is still unsuitable for analytic lan-
guages. MICG instead was inspired by deletion un-
der coordination and serial verb construction in ana-
lytic languages. We are in progress to develop an ex-
tension of MICG that allows topicalization and da-
tive shift avoiding combinatoric explosion.
5 Relevance to RBMT
Major issues of MT from analytic languages into
Indo-European ones include three issues: anaphora
generation, semantic duplication, and sentence
structuring. Both syntax and semantics are used to
solve such problems by MICG?s capability of gap
resolution. Case studies from our RBMT are exem-
plified for better understanding.
Our Thai-English MT system is rule-based and
consists of three modules: analysis, transfer, and
generation. MICG is used to tackle sentences with
deletion under coordination and SVC which cannot
be parsed by ordinary parsers. For good speed effi-
ciency, an MICG parser was implemented in GLR-
based approach and used to analyze the syntactic
structure of a given sentence before transferring.
The parser detects zero anaphora and resolves their
antecedents in coordinate structure, and reveals ar-
gument sharing in SVC. Therefore, coordinate struc-
ture and SVC can be properly translated.
No experiment has been done on our system yet,
but we hope to see an improvement of translation
quality. We planned to evaluate the translation accu-
racy by using both statistical and human methods.
84
Table 1: Coverage comparison among CG, CCG, and MICG (Y = supported, N = not supported)
Linguistic phenomena CG CCG MICG
Basic application Y Y Y
Node raising N Y Y
Topicalization/contraposition N Y N
Constituent shifting N Y N
Deletion under coordination N N Y
Serial verb construction N N Y
5.1 Translation of deletion under coordination
Coordinate structures in Thai drastically differ from
those of English. This is because Thai allows zero
anaphora at subject and object positions while En-
glish does not. Pronouns and VP ellipses must there-
fore be generated in place of deletion under coordi-
nation for grammaticality of English. Moreover, se-
mantic duplication is often made use to emphasize
the meaning of sentence, but its direct translation be-
comes redundant.
MICG helps us detect zero anaphora and resolve
their antecedents, so that appropriate pronouns and
ellipses can be generated at the right positions. By
tracing resolved antecedents and ellipses, argument
projections are disclosed and they can be used to
control verb fusion. We exemplify three cases of
translation of coordinate structure.
Case 1: Pronouns are generated to maintain
grammaticality of English translation if the two
verbs are not postulated in the verb-fusion table. For
example, a Thai sentence in (17) is translated, while
pronouns ?he? and ?it? are generated from Thai NPs
na?k;rian ?student? and kha`;no?m ?candy,? respectively.
na?k;rianS sW?:V kha`;no?mO lE?:V& kinV
student buy candy then eat
?A student buys candy, then he eats it.?
(17)
Case 2: Two verbs V1 and V2 are fused togeth-
erif they are postulated in the verb-fusion table to
eliminate semantic duplication in English transla-
tion. The object form of S2 is necessary to be gener-
ated in some cases. For example, in (18), the trans-
lation becomes ?He reports her this matter? instead
of ?He tells her to know this matter.? Two verbs bO`:k
?tell? and sa?:b ?know? are fused into a single verb ?re-
port.? The object form of ?she,? ?her,? is also gener-
ated.
kha?VS bO`:kV ha?j& th@:S sa?:pV rW?:@N n??:O
he tell TO she know this matter
?He reports her this matter.?
(18)
Case 3: A VP ellipsis is generated to main-
tain English grammaticality. For example, in (19),
a VP ellipsis ?do? is generated from a Thai VP
ma?i ChO?:b don;tri: rO?k ?not like rock music.?
CO:nS ChO?:pV don;tri: rO?kO tE`:& Cha?nS ma?iA
John like rock music but I not
?John likes rock music, but I do not.?
(19)
5.2 Translation of SVC
Sentence structuring is also nontrivial for translation
of Thai SVC. Thai uses SVC to describe consecu-
tive/concurrent events, purposes, and circumstances.
On the other hand, English describes each of those
with different sentence structure. A series of verbs
with duplicated semantics can be also clustered to
emphasize the meaning of sentence in Thai, while
English does not allow this phenomenon.
Because MICG reveals argument sharing in SVC,
appropriate sentence structures can be selected by
tracing argument sharing between two consecutive
verbs. We exemplify two cases of translation of
SVC.
Case 1: The second verb is participialized if the
first verb is intransitive and its semantic concept is
an action. For example, the present participial form
of the verb ?see,? ?seeing,? is generated in (20) .
so?m;Cha:jS d@:nV ChomV pha?:p;khia?nO
Somchai walk see paintings
?Somchai walks seeing paintings.?
(20)
Case 2: If the two cases above do not apply to
the two verbs, they are translated directly by de-
fault. The conjunction ?and? is automatically added
85
to conjoin two verb phrases. In case of multiple-
conjunct coordination, the conjunction will be added
only before the last conjunct. For example, in (21),
a pronoun ?it? is generated from the NP kho?:k ?coke,?
while the conjunction ?and? is automatically added.
ph?i:;sa?:VS sW?:V kho?:kO dW`:mV
my elder sister buy coke drink
?My elder sister buys coke and drinks it.?
(21)
6 Conclusion and Future Work
This paper presents Memory-Inductive Categorial
Grammar (MICG), an extended version of catego-
rial grammar, for gap resolution in analytic language
translation. Antecedent memorization, gap induc-
tion, and gap resolution, are proposed to cope with
deletion under coordination and serial verb construc-
tion. By means of MICG, anaphora can be gen-
erated for deletion under coordination, while sen-
tence structure can be properly selected for serial
verb construction. No experiment has been done to
show improvement of translation quality by MICG.
The following future work remains. First, we will
experiment on our Thai-English RBMT to measure
improvement of translation quality. Second, crite-
ria for pronominal reference generation in place of
deletion under coordination will be studied. Third,
once serial verb construction is analyzed, criteria of
sentence structuring will further be studied based on
an analysis of antecedent projection. Fourth and fi-
nally, constituent extraction and the use of extraction
direction in the extraction resolution will be studied
to avoid combinatoric explosion.
References
K. Ajdukiewicz. 1935. Die Syntaktische Konnexita?t.
Polish Logic, pages 207?231.
M. C. Baker. 1989. Object Sharing and Projection in Se-
rial Verb Constructions. Linguistic Inquiry, 20:513?
553.
J. Baldridge and G. J. M. Kruijff. 2003. Multimodal
combinatory categorial grammar. In Proceedings of
the 10th Conference of the European Chapter of the
ACL 2003, Budapest, Hungary.
J. Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Y. Bar-Hillel. 1953. A Quasi-Arithmetical Notation for
Syntactic Description. Language, 29:47?58.
J. Beavers and I. A. Sag. 2004. Coordinate ellipsis and
apparent non-constituent coordination. In Proceed-
ings of the HPSG04 Conference. Center for Compu-
tational Linguistics, Katholieke Universiteit Leuven,
CSLI Publications.
B. Cryssmann. 2003. An asymmetric theory of periph-
eral sharing in HPSG: Conjunction reduction and coor-
dination of unlikes. In Proceedings of Formal Gram-
mar Conference.
P. Hendriks. 1995. Ellipsis and multimodal categorial
type logic. In Proceedings of Formal Grammar Con-
ference. Barcelona, Spain.
G. Ja?ger. 1998a. Anaphora and ellipsis in type-logical
grammar. In Proceedings of the 1th Amsterdam Col-
loquium, Amsterdam, the Netherland. ILLC, Univer-
siteit van Amsterdam.
G. Ja?ger. 1998b. Anaphora and quantification in cate-
gorial grammar. In Lecture Notes in Computer Sci-
ence; Selected papers from the 3rd International Con-
ference, on logical aspects of Computational Linguis-
tics, volume 2014, pages 70?89.
J. Lambek. 1958. The Mathematics of Sentence Struc-
ture. American Mathematical Monthly, 65:154?170.
C. N. Li and S. A. Thompson. 1981. Mandarin Chinese:
A Functional Reference Grammar. Berkeley: Univer-
sity of California Press.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science, vol-
ume 1, pages 435?447. Nature Publishing Group.
B. H. Partee. 1975. Bound variables and other anaphors.
In Theoretical Issues in Natural Language Processing-
2 (TINLAP-2), pages 79?85, University of Illinois at
Urbana Champaign, July.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
K. Thepkanjana. 2006. Properties of events expressed
by serial verb constructions in Thai. In Proceedings
of the 11th Biennial Symposium: Intertheoretical Ap-
proaches to Complex Verb Constructions, Rice Univer-
sity.
X. Wang. 2007. Notes about Serial Verb Constructions
in Chinese. California Linguistic Notes, 32(1).
S. Yatabe. 2002. A linearization-based theory of sum-
mative agreement in peripheral-node raising construc-
tions. In Proceedings of the HPSG02 Conference,
Standford, California. CSLI Publications.
86
A Formal Definition of MICG
Definition 1 (Closure of MICG) Let VA of category symbols,
a finite set VT of terminal symbols, and a set of directions D =
{<,>}.
The set C of all category symbols is given by: (1) For all
x ? VA, x ? C. (2) If x,y ? C, then so are x\y and x/y. (3) If
x ?C, then so are 2<f x, 2>f x, 3<f x, and 3>f x, where f ? F is a
formula (described below). (4) Nothing else is in C.
The set T of all grammatical structures is given by: (1) For
all x ? VT , x ? T . (2) If x,y ? T , then so are x? y. (3) If x ? T ,
then so are 2x and 3x. (4) Nothing else is in T .
The set F of all formulae is a set of terms t ? x, where t ? T
and x ?C. The set Q of all modalities is a set of all terms 2<f ,
2>f , 3
<
f , and 3
>
f , where f ? F.
Definition 2 (Modality resolution) For any directions d ? D,
any formulae f ? F, and any modality sequences M,M1,M2 ?
Q?, the function ? : Q??Q? 7? Q? is defined as follows:
2df M1 ?3df M2 ? M1 ?M2
3df M1 ?2df M2 ? M1 ?M2
2df M1 ?2df M2 ? 2df (M1 ?M2)
3df M1 ?3df M2 ? 3df (M1 ?M2)
??M ? M? ? ? M
Definition 3 (MICG) A memory-inductive categorial gram-
mar (MICG) is defined as a quadruple G = ?VT ,VA,s,R?,
where: (1) VT and VA are as above. (2) s ?VA is the designated
symbol called ?start symbol.? (3) R :VT 7?P(F) is a function as-
signing to each terminal symbol a set of formulae from F. The
set of all strings generated from G is denoted as L(G).
Definition 4 (Acceptance of strings) For any formulae x,y ?
F, any grammatical structures t1,t2,t3 ? T , any variables
v of grammatical structures, and any modality sequences
M,M1,M2 ? Q?, the binary relation |=? F??F controls com-
bination of formulae as follows:
t1 ? y t2 ? y\x |= t1 ? t2 ? x
t1 ? x/y t2 ? y |= t1 ? t2 ? x
t1 ? y t2 ? My\x |= 2t1 ? t2 ? 2<t1?yMx
t1 ? My t2 ? y\x |= t1 ?2t2 ? 2>t2?y\xMx
t1 ? x/y t2 ? My |= 2t1 ? t2 ? 2<t1?x/yMx
t1 ? Mx/y t2 ? y |= t1 ?2t2 ? 2>t2?yMx
t2 ? My\x |= 3v? t2 ? 3<v?yMx
t1 ? My |= t1 ?3v ? 3>v?y\xMx
t2 ? My |= 3v? t2 ? 3<v?x/yMx
t1 ? Mx/y |= t1 ?3v ? 3>v?yMx
t1 ? M1x t3 ? & t2 ? M2x |= t1 ? (t3 ? t2) ? (M1 ?M2)x
t1 ? M1x t2 ? M2x |= t1 ? t2 ? (M1 ?M2)x
The binary relation ?? F??F? holds between two strings
of formulae ?X? and ?Y ?, denoted ?X? ? ?Y ?, if and only if
X |=Y , where X ,Y,?,? ? F? and |X | ? |Y |. The relation ?? is
the reflexive transitive closure of ?.
A string w ? V ?T is generated by G, denoted by w ? L(G), if
and only if w = w1 . . .wn and there is some sequence of formulae
f1 . . . fn such that fi ?R(wi) for all 1 ? i ? n, and f1 . . . fn ?? s.
That is, w1 . . .wn is generated if and only if there is some choice
of formula assignments by R to the symbols in w1 . . .wn that
reduces to s.
Definition 5 Correspondence between a grammatical struc-
ture and its syntactic category can be viewed as a tree with spe-
cialized node types. Each node is represented (m,S), where m
is a node type { /0,2,3}, and S is a modality sequence attached
to the node?s syntactic category.
Definition 6 A node that has the type m is said to be marked m
where m ? {2, 3}, while a node that has the type /0 is said to
be unmarked.
Definition 7 The function ? : Q 7? {2,3} maps a modality to
a node modality, where ?(2df ) = 2 and ?(3df ) = 3 for all d ?D
and f ? F.
Definition 8 A substring generated from a node marked ?(M)
beneath the node n is said to be unpaired under n, if and only if
n has the modality sequence S and M ? S.
Definition 9 Every string w generated from MICG can be
rewritten in the form w = p1q1 . . . plql pl+1ql+1 . . . p2lq2l p2l+1,
where qi is a substring unpaired under n, p j is a substring gen-
erated from unmarked nodes beneath n, 1 ? i ? l, 1 ? j ? l +1,
and l ? 0.
Theorem 1 (Cross-serial association) For every string gener-
ated from MICG w = p1q1 . . . plql p j(l)q j(1) . . . p j(l)q j(l) p j(l)+1,
every couple qi and q j(i) are associated by ? for all 1 ? i ? l,
where j(i) = l + i and l ? 0.
Proof Let us prove this property by mathematical induction.
Basic step: Let l = 0. We obtain that w0 = p1. Since there is
no unpaired substring, this case is trivially proven.
Hypothesis: Let l = k. Suppose that wk =
p1q1 . . . p j(k)q j(k) p j(k)+1. We rewrite wk = w1kw2k , where w1k =
p1q1 . . . pkqk p?j(1) and w
2
k = p??j(1)q j(1) . . . p j(k)q j(k) p j(k)+1.
Every couple qi and q j(k) are associated by ? for all 1 ? i ? k.
Induction: Let l = k + 1; wk+1 = p1q1 . . . p j(k)+2q j(k)+2
p j(k)+3, consequently. Let the formulae of the substrings
wk+1 = w1k+1w2k+1 be t1k+1 ? m1M1 and t2k+1 ? m2M2, respec-
tively. We can rewrite the substrings wk+1 = w1k+1w2k+1 in terms
of wk = w1kw2k in three cases.
Case I: Suppose w1k+1 = pqw1k . It follows that the direction
of q is <. Since w1k+1 combines w2k+1, we can conclude that
w2k+1 = p?q?w2k . Therefore, q and q? are also associated by ?.
Case II: Suppose w1k+1 = w1kqp. It follows that the direction
of q is >. Since w1k+1 combines w2k+1, we can conclude that
w2k+1 = w2kq?p?. Therefore, q and q? are also associated by ?.
Case III: w1k+1 = p1q1 . . . pmqm pqpm+1qm+1 . . . pnqn pk+1 and
w2k+1 = p j(1)q j(1) . . . p j(m?)q j(m?) p?q?p j(m?)+1q j(m?)+1 . . . p j(k)
q j(k) p j(k)+1, where 1 < m,m? < k. Since w1k+1 and w2k+1
combine and every qi and q j(i) are associated, we can conclude
that m = m?. Therefore, q and q? are also associated by ?.
From Case I, Case II, and Case III, we can rewrite w1k+1 =
p?1q
?
1 p
?
2q
?
2 . . . p?k+1 and w2k+1 = p?j(1)q?j(1) p?j(2)q?j(2) . . . p?j(k+1).
Since each qi in w1k and q j(i) in w
2
k are already associated by
?, it follows that all qi and q j(i)+1 are also associated. 
87
Plaesarn: Machine-Aided Translation Tool for English-to-Thai
Prachya Boonkwan and Asanee Kawtrakul
Specialty Research Unit of Natural Language Processing
and Intelligent Information System Technology
Department of Computer Engineering
Kasetsart University
Email: ak@vivaldi.cpe.ku.ac.th
Abstract
English-Thai MT systems are nowadays re-
stricted by incomplete vocabularies and trans-
lation knowledge. Users must consequently ac-
cept only one translation result that is some-
times semantically divergent or ungrammatical.
With the according reason, we propose novel
Internet-based translation assistant software in
order to facilitate document translation from
English to Thai. In this project, we utilize
the structural transfer model as the mechanism.
This project differs from current English-Thai
MT systems in the aspects that it empowers the
users to manually select the most appropriate
translation from every possibility and to manu-
ally train new translation rules to the system if
it is necessary. With the applied model, we over-
come four translation problems?lexicon rear-
rangement, structural ambiguity, phrase trans-
lation, and classifier generation. Finally, we
started the system evaluation with 322 ran-
domly selected sentences on the Future Mag-
azine bilingual corpus and the system yielded
59.87% and 83.08% translation accuracy for the
best case and the worse case based on 90.1%
average precision of the parser.
Introduction
Information comprehension of Thai people
should not be only limited in Thai; in con-
trast, it should also include a considerably large
amount of information sources from foreign
countries. Insufficient basic language knowl-
edge, a result of inadequate distribution in the
past, conversely, is the major obstruction for in-
formation comprehension. There are presently
several English-Thai MT systems?for instance,
Parsit (Sornlertlamvanich, 2000), Plae Thai,
and AgentDict. The first one applies seman-
tic transfer model via the methodology similar
to the lexical functional grammar (Kaplan et
al., 1989) and it is develop with the intention of
public use. The latter two implicitly apply the
direct transfer model with the purpose of com-
mercial use. Nonetheless, by limited vocabular-
ies and translation rules, the users must accept
the only one translation result that is occasion-
ally semantically divergent or ungrammatical.
Due to the according reason, we initiated
this project in order to relieve language prob-
lem of Thai people. In this project, we de-
velop a semi-automatic translation system to
assist them to translate English documents into
Thai. For this paper, the term semi-automatic
translation means the sentence translation with
user interaction to manually resolve structural
and semantic ambiguities during translation pe-
riod. Despite manual disambiguation, we pro-
vided a simple statistical disambiguation in or-
der to pre-select the most possible translation
for each source language sentence, though. The
automatic semantic disambiguation can be thus
excluded with this approach.
1 Translation Approaches
We can classify current translation approaches
into three major models as follows?structural
transfer, semantic transfer, and lexical transfer
(Trujillo, 1999).
? Structural transfer: this methodology
heavily depends on syntactic analysis (say,
grammar). Translation transfers the source
language structures into the target lan-
guage. This method is established by
the assumption that every language in the
world uses syntactic structure in order to
represent the meaning of sentences.
? Semantic transfer: this methodology
heavily depends on semantic analysis (say,
meaning). This model applies syntactic
analysis as well. On the contrary to the
structural transfer, a source language sen-
tence is not immediately translated into
the target language, but it is first trans-
lated into semantic representation (Inter-
lingua is mostly referred), and afterwards
into the target language. This method is
established by the assumption that every
language in the world describes the same
world; hence, there exists the semantic rep-
resentation for every language.
? Lexical transfer: this methodology heav-
ily depends on lexicon ordering patterns.
The translation occurs at the level of mor-
pheme. The translation process transfers
a set of morpheme in the source language
into that of the target language.
In this project, we decided to utilize the struc-
tural transfer approach, since it is more ap-
propriate for rapid development. In addition,
semantic representation that covers every lan-
guage is now still under research.
2 Relevant Problems and Their
Solutions
2.1 Structural Ambiguity
By the reason of the ambiguities of natural lan-
guages, a sentence may be translated or inter-
preted into many senses. An example of struc-
tural ambiguity is ?I saw a girl in the park with
a telescope.? This sentence can be grammati-
cally interpreted into four senses as follows.
? I saw a girl, to whom a telescope belonged,
who was in the park.
? I used a telescope to see a girl, who was in
the park.
? I was in the park and seeing a girl, to whom
a telescope belonged.
? I was in the park and using a telescope to
see a girl.
Furthermore, an example of word-sense am-
biguity is ?I live near the bank.? The noun bank
can be semantically interpreted into at least two
senses as follows.
? n. a financial institution that accepts de-
posits and channels the money into lending
activities
? n. sloping land (especially the slope beside
a body of water)
In order to resolve structural ambiguity, we
apply the concept of the statistical machine
translation approach (Brown et al, 1990). We
apply the Maximum-Entropy-Inspired Parser
(Charniak, 1999) (so-called Charniak Parser) to
analyze and determine the appropriate gram-
matical structure of an English sentence. From
(Charniak, 1999), Charniak presented that the
parser uses the Penn Tree Bank tag set (Marcus
et al, 1994) (or PTB in abbreviation) as a gram-
matical structure representation, and it yielded
90.1% average precision for sentences of length
40 or less, and 89.5% for sentences of length
100 and less. Moreover, with the intention to
resolve word-sense ambiguity, we embedded a
numerical statistic value with each translation
rule (including lexical transfer rule) with the
major aim of assisting to select the best transla-
tion parse tree from every possibility (Charniak,
1997). Section 3.4 will describe the method and
the tool to do so.
2.2 Phrase Translation
Phrase is a word-ordering pattern that cannot
be separately translated. An example is the
translation of the verb to be. The translation
of that depends on the context?for instance,
to be succeeding with noun phrase is trans-
lated to ???? /penm/, succeeding with preposi-
tional phrase to ???? /yuul/, in progressive tenses
to ????? /kammlangm/, in passive voice to ???
/thuukl/, and succeeding with adjectival phrase
to translation omission. Another example is the
verbal phrase to look for something. It must
be translated to ????? /m??ngmhaar/ not to
????????? /m??ngm samrrabl/. The word look
is translated to ??? /m??ngm/, and for to
?????? /samrrabl/.
From empirical observation, we found that
the PTB tag set is rather problematical to
translate into Thai. We hence implement the
parse tree modification process in order to re-
lieve the complexity of transformation process
(Trujillo, 1999). In this process, the heads of
the tree are recursively modified so as to facili-
tate phrase translation. A portion of parse tree
modification rules shown on Table 1 is described
in parenthesis format.
Obviously, from Table 1, we can more easily
compose the rules in Table 2 to translate the
verb to be and the phrasal verb look for some-
thing.
2.3 Lexicon Rearrangement
In English, we can normally modify a cer-
tain core noun with modifiers in two ways?
Table 1: Rules for the parse tree modification
process
Original PTB Modified
(VP (AUX (be)) (NP)) (VP (be) (NP))
(VP (AUX (be)) (PP)) (VP (be) (PP))
(VP (AUX (be)) (VP
(VBG) *))
(VP (be) (VBG) *)
(VP (AUX (be)) (VP (VBN)
*))
(VP (be) (VBN) *)
(VP (AUX (be)) (ADJP)) (VP (be) (ADJP))
(VP (VBP (look)) (PP (IN
(for)) (NP)))
(VP (look) (for) (NP))
Table 2: Rules to translate the verb to be and
the verbal phrase look for something
English Rules Thai Rules
VP ? be NP VP ? ???? NP
VP ? be PP VP ? ???? PP
VP ? be VBG VP ? ????? VBG
VP ? be VBN VP ? ??? VBN
VP ? be ADJP VP ? ADJP
VP ? look up NP VP ? ????? NP
putting them in front of or behind it. We
will focus the first case in this paper. The
problem occurs as soon as we would like to
translate a sequence of nouns and a sequence
of adjectives. The first case is translated
backwards, while the second forwards. An
example for this problem is that ?she is a
beautiful diligent slim laboratory member? is
translated to ??????????????????????????????/th??m
penm salmaamchikh thiif suayr khalyanr
ph??mr/. The word she is translated to ???,
is to ????, member to ??????, laboratory to ????,
beautiful to ???, diligent to ????, and slim to ???.
With the purpose to solve this problem, we
first group nouns and adjectives into groups?
NNS and ADJS?and we apply a number of
structural transfer rules. Table 3 shows a por-
tion of transfer rules.
Table 3: A portion of structural transfer rules
to solve the lexicon reordering
English Rules Thai Rules
NP ? ADJS NNS NP ? NNS ADJS
ADJS ? adj ADJS ? adj
ADJS ? adj ADJS ADJS ? adj ADJS
NNS ? nn NNS ? nn
NNS ? nn NNS NNS ? NNS nn
2.4 Classifier Generation
The vital linguistic divergence between English
and Thai is head-noun-corresponding classifiers
(Lamduan, 1983). In English, classifiers are
never used in order to identify the numeric num-
ber of a noun or definiteness. On the contrary,
classifiers are generally used in Thai?for ex-
ample, in English, a number precedes a noun
phrase; but in contrast, a classifier together with
the number succeeds in Thai.
In order to generate a classifier, we develop
the classifier matching algorithm. By empirical
observation, it is noticeable that the head noun
in the noun phrase always indicates the classi-
fier. For example, supposing the rules in Table 4
are amassed in the linguistic knowledge base.
Table 4: An example of rules for classifier gen-
eration
Head Noun Classifier
?? /rothh/ ??? /khanm/
???? /rothhfaim/ ???? /khalbuanm/
Thus, we can revise ????????????????
/rothhfaim h?l tiimlangmkaam/ 3 <cl>?
and ??????? /rothhyonm/ 4 <cl>? can be
respectively revised to ???????????????? 3 ?????
(three roller coasters) and ??????? 4 ???? (four
automobiles). If there is no rule that can match
the noun phrase, its head noun is used as the
classifier (Lamduan, 1983)?for example, ??????
Figure 1: System Overview
/praltheesf/ is a Thai word that there is, in
fact, no corresponding classifier. As soon as
we would like to specify, as the latter example,
the numeric number, we say ???????????????? 3
??????? /praltheesf phathhthahnaam l??wh/
(three developed countries).
3 System Overview
As illustrated in the Figure 1, the system com-
prises of four principle components?syntactic
analysis, structural transformation, sentence
generation, and linguistic knowledge acquisi-
tion.
3.1 Syntactic Analysis and Parse-Tree
Modification
In this process, we analyze each sentence of the
source documents with the Charniak Parser and
afterwards transform each of which into a parse
tree.
The first process that we have to accomplish
first is the sentence boundary identification. In
this step, we require users to manually pre-
pare sentence boundaries by inserting a new-line
character among sentences.
The next step is the sentence-parsing process.
We analyze the surface structure of a sentence
with the Charniak Parser. In this case, the orig-
inal Charniak Parser nevertheless spends long
time for self-initiation to load its considerably
huge database. Consequently, we patched it to
be a client-server program so as to eliminate
such time.
As stated earlier, in the view of the fact that
parse trees generated by the Charniak Parser
are quite complicated to translate into Thai, we
therefore implement the parse tree modification
process (see Section 2.2).
3.2 Structural Transformation
This process performs recursive transformation
from the source language parse trees into a set of
corresponding Thai translation parse trees with
their probabilities. As stated earlier, there are
some complexity in order to transfer a PTB-
formatted parse tree into Thai, we thus imple-
mented the parse tree modification process (see
Section 2.2) before performing transformation.
The transformation relies on the transformation
rules from the linguistic knowledge base.
A single step of transformation process
matches the root node and single-depth child
nodes with the transformation rules and after-
wards returns a set of transformation produc-
tions. As stated earlier, we embedded the prob-
ability of each rule. The probability of a parse
tree pi is given by the equation
P (pi) = ?(cpi, cpi1 , cpi2 , cpi3 , . . . , cpin)
n?
k=1
P (pik)
where pik is the k-th subtree of the parse tree pi
whose number of member subtrees is n, cpi rep-
resents the constituent of the tree pi, and ? is a
probability relation that maps the constituents
of the root and its single-depth children to the
probability value.
3.3 Sentence Generation
This process generates a target language sen-
tence from the parse tree. This stage also re-
lies on the linguistic knowledge base. The addi-
tional process is the noun classifier. We apply
the methodology defined in classifier matching
algorithm (see Section 2.4). Finally, the system
will show the translations of the most possibil-
ity and let the users change each solution if they
would like to do so.
3.4 Linguistic Knowledge Acquisition
We provided an advantageous tool so as to man-
ually train new translation knowledge. Cur-
rently, it comprises of the translation rule
learner and the English-Thai unknown word
aligner (Kampanya et al, 2002).
In this module, the translation rule learner
obtains document and analyzes that into a set
of parse trees. Afterwards, the users manually
teach it the rules to grammatically translate a
certain tree from the source language into the
target language with the rules following to the
Backus-Naur Form (Lewis and Paradimitriou,
1998) (or BNF in abbreviation). This module
will determine whether the rule is re-trained.
If so, the module will raise the probability of
that rule up. If not, it will add that rule to the
knowledge base.
Moreover, the aligner is utilized to automat-
ically update the bilingual dictionary. For our
future work, we intend to develop a system to
automatically learn new translation rules from
our corpora.
4 Evaluation
We established the system evaluation on the Fu-
ture Magazine bilingual corpus. We categorized
the evaluation into two environments?under
restricted knowledge base and under increasing
knowledge base. Each of which is also catego-
rized into two environments?with parsing er-
rors and without parsing errors.
In the evaluation, we randomly selected 322
sentences from the corpus. In order to have
a manageable task and facilitate performance
measurement, we classify translation result into
the following three categories?exact (the same
as in the corpus), moderate (understandable
result), and incomprehensible (obviously non-
understandable result). Table 5 shows the eval-
uation results.
In this evaluation, we consider the results in
the exact and moderate categories as reason-
Table 5: Evaluation Results (in percentages)
The column A represents evaluation with restricted knowl-
edge base and with parsing errors, B as with restricted knowl-
edge base but without parsing errors, C as with increasing
knowledge base but with parsing errors, and D as with in-
creasing knowledge base and without parsing errors.
Categories A B C D
Exact 3.97 4.41 4.97 5.52
Moderate 55.90 62.04 69.88 77.56
Incomprehensible 40.13 33.55 25.15 16.92
Accuracy 59.87 66.45 74.85 83.08
able translations. Moreover, we also consider
that the evaluation with restricted knowledge
base and with parsing errors is the worst case
performance, and the evaluation with increas-
ing knowledge base and without parsing errors
is the best case performance.
From the constraints we established, we found
that the system yielded the translation accuracy
for 59.87% for the worst case and 83.08% for the
best case.
5 Conclusions
In this paper, we propose novel Internet-based
translation assistant software in order to fa-
cilitate document translation from English to
Thai. We utilize the structural transfer model
as the translation mechanism. This project dif-
fers from the current MT systems in the point
that the users have a capability to manually se-
lect the most appropriate translation, and they
can, in addition, teach new translation knowl-
edge if it is necessary.
The four translation problems?Lexicon Re-
arrangement, Structural Ambiguity, Phrase
Translation, and Classifier Generation?are ac-
complished with various methodologies. To re-
solve the lexicon rearrangement problem, we
compose a number of structural transfer rules.
For the structural ambiguity, we apply the sta-
tistical method by embedding probability val-
ues to each transfer rules. In order to relieve the
complexity of the phrase translation, we develop
the parse tree modification process to modify
some tree structure so as to more easily compose
translation rules. Finally, with the purpose of
resolving the classifier generation problem, we
define the classifier matching algorithm which
matches the longest head noun to the appropri-
ate classifier.
In the evaluation, we established the sys-
tem experiment on the Future Magazine bilin-
gual corpus and we categorized the evaluation
into two environments?under restricted knowl-
edge base and under increasing knowledge base.
From the evaluation, the system yielded the
translation accuracy for 59.87% for the worst
case and 83.08% for the best case.
References
Peter F. Brown, John Cocke, Stephen Della
Pietra, Vincent J. Della Pietra, Frederick Je-
linek, John D. Lafferty, Robert L. Mercer,
and Paul S. Roossin. 1990. A statistical
approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Eugene Charniak. 1997. Statistical parsing
with a context-free grammar and word statis-
tics. In AAAI/IAAI, pages 598?603.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS-99-12,
Brown Laboratory for Natural Language Pro-
cessing.
Nithiwat Kampanya, Prachya Boonkwan, and
Asanee Kawtrakul. 2002. Bilingual unknown
word alignment tool for english-thai. In Pro-
ceedings of the SNLP-Oriental COCOSDA,
Specialty Research Unit of Natural Lan-
guage Processing and Intelligent Informa-
tion System Technology, Kasetsart Univer-
sity, Bangkok, Thailand.
Ronald M. Kaplan, Klaus Netter, Ju?rgen
Wedekind, and Annie Zaenen. 1989. Trans-
lation by structural correspondences. In Pro-
ceedings of the 4th. Annual Meeting of the
European Chapter of the Association for
Computational Linguistics, pages 272?281,
UMIST, Manchester, England.
Somchai Lamduan, 1983. Thai Grammar (in
Thai), chapter 4: Parts of Speech, pages 128?
131. Odeon Store Publisher.
Harry R. Lewis and Christos H. Paradimitriou,
1998. Elements of the Theory of Compu-
tation, chapter 3: Context-Free Grammar.
Prentice-Hall International Inc.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building
a large annotated corpus of english: The
penn treebank. Computational Linguistics,
19(2):313?330.
Virach Sornlertlamvanich. 2000. The state of
the art in thai language processing.
Arturo Trujillo, 1999. Translation Engines:
Techniques for Machine Translation, chapter
6: Transfer MT, pages 121?166. Springer-
Verlag (London Limited).
Proceedings of the EACL 2009 Student Research Workshop, pages 10?18,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
A Memory-Based Approach to the Treatment of Serial Verb Construction
in Combinatory Categorial Grammar
Prachya Boonkwan??
? School of Informatics ? National Electronics
University of Edinburgh and Computer Technology Center
10 Crichton Street 112 Phahon Yothin Rd.
Edinburgh EH8 9AB, UK Pathumthani 12120, Thailand
Email: p.boonkwan@sms.ed.ac.uk
Abstract
CCG, one of the most prominent grammar
frameworks, efficiently deals with deletion
under coordination in natural languages.
However, when we expand our attention
to more analytic languages whose degree
of pro-dropping is more free, CCG?s de-
composition rule for dealing with gapping
becomes incapable of parsing some pat-
terns of intra-sentential ellipses in serial
verb construction. Moreover, the decom-
position rule might also lead us to over-
generation problem. In this paper the
composition rule is replaced by the use
of memory mechanism, calledCCG-MM.
Fillers can be memorized and gaps can be
induced from an input sentence in func-
tional application rules, while fillers and
gaps are associated in coordination and se-
rialization. Multimodal slashes, which al-
low or ban memory operations, are utilized
for ease of resource management. As a
result, CCG-MM is more powerful than
canonical CCG, but its generative power
can be bounded by partially linear indexed
grammar.
1 Introduction
Combinatory Categorial Grammar (CCG, Steed-
man (2000)) is a prominent categorial grammar
framework. Having a strong degree of lexical-
ism (Baldridge and Kruijff, 2003), its grammars
are encoded in terms of lexicons; that is, each lex-
icon is assigned with syntactic categories which
dictate the syntactic derivation. One of its strik-
ing features is the combinatory operations that al-
low coordination of incomplete constituents. CCG
is nearly context-free yet powerful enough for
natural languages as it, as well as TAG, LIG,
and HG, exhibits the lowest generative power in
the mildly context-sensitive grammar class (Vijay-
Shanker and Weir, 1994).
CCG accounts for gapping in natural languages
as a major issue. Its combinatory operations re-
solve deletion under coordination, such as right-
node raising (SV&SVO) and gapping (SVO&SO).
In case of gapping, a specialized rule called de-
composition is used to handle with forward gap-
ping (Steedman, 1990) by extracting the filler re-
quired by a gap from a complete constituent.
However, serial verb construction is a challeng-
ing topic in CCG when we expand our attention
to more analytic languages, such as Chinese and
Thai, whose degree of pro-dropping is more free.
In this paper, I explain how we can deal with
serial verb construction with CCG by incorpo-
rating memory mechanism and how we can re-
strict the generative power of the resulted hy-
brid. The integrated memory mechanism is mo-
tivated by anaphoric resolution mechanism in Cat-
egorial Type Logic (Hendriks, 1995; Moortgat,
1997), Type Logical Grammar (Morrill, 1994;
Ja?ger, 1997; Ja?ger, 2001; Oehrle, 2007), and CCG
(Jacobson, 1999), and gap resolution in Memory-
Inductive Categorial Grammar (Boonkwan and
Supnithi, 2008), as it is designed for associating
fillers and gaps found in an input sentence. Theo-
retically, I discuss how this hybrid efficiently helps
us deal with serial verb construction and how far
the generative power grows after incorporating the
memory mechanism.
Outline: I introduce CCG in ?2, and then mo-
tivate the need of memory mechanism in dealing
with serial verb construction in CCG in ?3. I de-
scribe the hybrid model of CCG and the filler-gap
memory in ?4. I then discuss the margin of gener-
ative power introduced by the memory mechanism
in ?5. Finally, I conclude this paper in ?6.
10
2 Combinatory Categorial Grammar
CCG is a lexicalized grammar; i.e. a grammar is
encoded in terms of lexicons assigned with one
or more syntactic categories. The syntactic cat-
egories may be atomic elements or curried func-
tions specifying linear directions in which they
seek their arguments. A word is assigned with a
syntactic category by the turnstile operator `. For
example, a simplified English CCG is given below.
(1) John ` np sandwiches ` np
eats ` s\np/np
The categories X\Y (and X/Y) denotes that X seeks
the argument Y from the left (right) side.
Combinatory rules are used to combine words
forming a derivation of a sentence. For basic
combination, forward (>) and backward (<) func-
tional applications, defined in (2), are used.
(2) X/Y Y ? X [>]
Y X\Y ? X [<]
We can derive the sentence John eats sandwiches
by the rules and the grammar in (1) as illustrated
in (3). CCG is semantic-transparent; i.e. a logical
form can be built compositionally in parallel with
syntactic derivation. However, semantic interpre-
tation is suppressed in this paper.
(3) John eats sandwiches
np s\np/np np
s\np
s
For coordination of two constituents, the coor-
dination rules are used. There are two types of
coordination rules regarding their directions: for-
ward coordination (> &) and backward coordina-
tion (< &), defined in (4).
(4) & X ? [X]& [> &]
X [X]& ? X [< &]
By the coordination rules, we can derive the sen-
tence John eats sandwiches and drinks coke in (5).
(5) John eats sandwiches and drinks coke
np s\np/np np & s\np/np np
> >
s\np s\np
>&
[s\np]&
<&
s\np
<
s
Beyond functional application and coordina-
tion, CCG also makes use of rules motivated by
combinators in combinatory logics: functional
composition (B), type raising (T), and substitution
(S), namely. Classified by directions, the func-
tional composition and type raising rules are de-
scribed in (6) and (7), respectively.
(6) X/Y Y/Z ? X/Z [> B]
Y\Z X\Y ? X\Z [< B]
(7) X ? Y/(Y\X) [> T]
X ? Y\(Y/X) [< T]
These rules permit associativity in derivation re-
sulting in that coordination of incomplete con-
stituents with similar types is possible. For ex-
ample, we can derive the sentence John likes but
Mary dislikes sandwiches in (8).
(8) John likes but Mary dislikes sandwiches
np s\np/np & np s\np/np np
>T >T
s/(s\np) s/(s\np)
>B >B
s/np s/np
>&
[s/np]&
<&
s/np
>
s
CCG also allows functional composition with
permutation called disharmonic functional com-
position to handle constituent movement such as
heavy NP shift and dative shift in English. These
rules are defined in (9).
(9) X/Y Y\Z ? X\Z [> B?]
Y/Z X\Y ? X/Z [< B?]
By disharmonic functional composition rules,
we can derive the sentence I wrote briefly a long
story of Sinbad as (10).
(10) I wrote briefly a long story of Sinbad
np s\np/np s\np\(s\np) np
>B?
s\np/np
>
np
<
s
To handle the gapping coordination SVO&SO,
the decomposition rule was proposed as a separate
mechanism from CCG (Steedman, 1990). It de-
composes a complete constituent into two parts for
being coordinated with the other incomplete con-
stituent. The decomposition rule is defined as fol-
lows.
(11) X ? Y X\Y [D]
where Y and X\Ymust be seen earlier in the deriva-
tion. The decomposition rule allows us to de-
rive the sentence John eats sandwiches, and Mary,
noodles as (12). Steedman (1990) stated that En-
glish is forward gapping because gapping always
11
takes place at the right conjunct.
(12) John eats sandwiches and Mary noodles
np s\np/np np & np np
> >T <T
s\np s/VP VP\(VP/np)
< >B?
s s\(VP/np)
D >&
VP/np s\(VP/np) [s\(VP/np)]&
<&
s\(VP/np)
<
s
where VP = s\np.
A multimodal version of CCG (Baldridge,
2002; Baldridge and Kruijff, 2003) restricts gener-
ative power for a particular language by annotating
modalities to the slashes to allow or ban specific
combinatory operations. Due to the page limita-
tion, the multimodal CCG is not discussed here.
3 Dealing with Serial Verb Construction
CCG deals with deletion under coordination by
several combinatory rules: functional composi-
tion, type raising, disharmonic functional compo-
sition, and decomposition rule. This enables CCG
to handle a number of coordination patterns such
as SVO&VO, SV&SVO, and SVO&SO. However,
the decomposition rule cannot solve some patterns
of SVC in analytic languages such as Chinese and
Thai in which pro-dropping is prevalent.
The notion serial verb construction (SVC) in
this paper means a sequence of verbs or verb
phrases concatenated without connectives in a sin-
gle clause which expresses simultaneous or con-
secutive events. Each of the verbs is marked or un-
derstood to have the same grammatical categories
(such as tense, aspect, and modality), and shares
at least one argument, i.e. a grammatical subject.
As each verb is tensed, SVC is considered as coor-
dination with implicit connective rather than sub-
ordination in which either infinitivization or sub-
clause marker is made use. Motivated by Li and
Thompson (1981)?s generalized form of Chinese
SVC, the form of Chinese and Thai SVC is gener-
alized in (13).
(13) (Subj)V1(Obj1)V2(Obj2) . . .Vn(Objn)
The subject Subj and any objects Obji of the verb
Vi can be dropped. If the subject or one of the ob-
jects is not dropped, it will be understood as lin-
early shared through the sequence. Duplication of
objects in SVC is however questionable as it dete-
riorates the compactness of utterance.
In order to deal with SVC in CCG, I considered
it syntactically similar to coordination where the
connective is implicit. The serialization rule (?)
was initially defined by imitating the forward co-
ordination rule in (14).
(14) X ? [X]& [?]
This rule allows us to derive by CCG some types
of SVC in Chinese and Thai as exemplified in (15)
and (16), respectively.
(15) wo?
I
zhe?
fold
zh??
paper
zuo`
make
y??
one
ge
CL
he?zi
box
?I fold paper to make a box.?
(16) khao
he
r:p
hurry
VN
run
k
h
a:m
cross
t
h
 anon
road
?He hurriedly runs across the road.?
One can derive the sentence (15) by considering
zhe? ?fold? and zuo` ?make? as s\np/np and ap-
plying the serialization rule in (14). In (16), the
derivation can be done by assigning r:p ?hurry?
and VN ?run? as s\np, and kha:m ?cross? as
s\np/np.
Since Chinese and Thai are pro-drop languages,
they allow some arguments of the verbs to be pro-
dropped, particularly in SVC. For example, let us
consider the following Thai sentence.
(17) kla:
Kla
p	aj
goDIR
t	a:m
followV1
ha:
seekV2
n	aj
in
raj;POi
cane-field
tc	@:
findV3
l	a:j
Laay
tc a
FUT
d	@:n
walkV4
tc a:k
leaveV5
p	aj
goDIR
Lit: ?Kla goes out, he follows Laay (his cow), he
seeks it in the cane field, and he finds that it will
walk away.?
Sem: ?Kla goes out to seek Laay in the cane field
and he finds that it is about to walk away.?
The sentence in (17) are split into two SVCs: the
series of V1 to V3 and the series of V4 to V5, be-
cause they do not share their tenses. The direc-
tional verb p	aj ?go? performs as an adverb identi-
fying the outward direction of the action.
Syntactically speaking, there are two possible
analyses of this sentence. First, we can consider
the SVC V4 to V5 as a complement of the SVC
V1 to V3. Pro-drops occur at the object positions
of the verbs V1, V2, and V3. On the other hand,
we can also consider the SVC V1 to V3 and the
SVC V4 to V5 as adjoining construction (Muan-
suwan, 2002) which indicates resultative events in
Thai (Thepkanjana, 1986) as exemplified in (18).
(18) p t 
Piti
t	:
hit
N	u:
snake
t ok
fall
na:m
water
?Piti hits a snake and it falls into the water.?
12
In this case, the pro-drop occurs at the subject po-
sition of the SVC V4 to V5, and can therefore
be treated as object control (Muansuwan, 2002).
However, the sentence in (17) does not show resul-
tative events. I then assume that the first analysis
is correct and will follow it throughout this paper.
We have consequently reached the question that
the verb tc	@: ?find? should exhibit object control
by taking two arguments for the object and the
VP complementary, or it should take the entire
sentence as an argument. To explicate the prolif-
eration of arguments in SVC, we prefer the first
choice to the second one; i.e. the verb tc	@: ?find? is
preferably assigned as s\np/(s\np)/np. In (17),
the object l	a:j ?Laay? is dropped from the verbs V1
and V2 but appears as one of V3?s arguments.
Let us take a closer look on the CCG analysis
of (17). It is useful to focus on the SVCs of the
verbs V1-V2 and V3. It is shown below that the
decomposition rule fails to parse the tested sen-
tence through its application illustrated in (19).
(19) Kla go follow seek find Laay FUT walk
in cane-field leave go
np s\np/np s\np/(s\np)/np np s\np
>
s\np/(s\np)
>
s\np
D
? ? ? ? ?
The verbs V1 and V2 are transitive and assigned
as s\np/np, while V4 and V5 are intransitive and
assigned as s\np. From the case (19), it follows
that the decomposition rule cannot capture some
patterns of intra-sentential ellipses in languages
whose degree of pro-dropping is more free. Both
types of intra-sentential ellipses which are preva-
lent in SVC of analytic languages should be cap-
tured for the sake of applicability.
The use of decomposition rule in analytic lan-
guages is not appealing for two main reasons.
First, the decomposition rule does not support cer-
tain patterns of intra-sentential ellipses which are
prevalent in analytic languages. As exemplified
in (19), the decomposition rule fails to parse the
Thai SVC whose object of the left conjunct is pro-
dropped, since the right conjunct cannot be de-
composed by (11). To tackle a broader coverage of
intra-sentential ellipses, the grammar should rely
on not only decomposition but also a supplement
memory mechanism. Second, the decomposition
rule allows arbitrary decomposition which leads to
over-generation. From their definitions the vari-
able Y can be arbitrarily substituted by any syn-
tactic categories resulting in ungrammatical sen-
tences generated. For example we can derive the
ungrammatical sentence *Mary eats noodles and
quickly by means of the decomposition rule in
(20).
(20) * Mary eats noodles and quickly
np s\np/np np & s\np\(s\np)
> >&
s\np [s\np\(s\np)]&
D
s\np s\np\(s\np)
<&
s\np\(s\np)
<
s\np
<
s
The issues of handling ellipses in SVC and
overgeneration of the decomposition rule can be
resolved by replacing the decomposition rule with
a memory mechanism that associates fillers to
their gaps. The memory mechanism also makes
grammar rules more manageable because it is
more straightforward to identify particular syn-
tactic categories allowed or banned from pro-
dropping. I will show how the memory mecha-
nism improves the CCG?s coverage of serial verb
construction in the next section.
4 CCG with Memory Mechanism
(CCG-MM)
As I have elaborated in the last section, CCG
needs a memory mechanism (1) to resolve intra-
sentential ellipses in serial verb construction of an-
alytic languages, and (2) to improve resource man-
agement for over-generation avoidance. To do so,
such memory mechanism has to extend the gener-
ative power of the decomposition rule and improve
the ease of resource management in parallel.
The memory mechanism used in this paper is
motivated by a wide range of previous work from
computer science to symbolic logics. The notion
of memory mechanism in natural language pars-
ing can be traced back to HOLD registers in ATN
(Woods, 1970) in which fillers (antecedents) are
held in registers for being filled to gaps found
in the rest of the input sentence. These regis-
ters are too powerful since they enable ATN to
recognize the full class of context-sensitive gram-
mars. In Type Logical Grammar (TLG) (Morrill,
1994; Ja?ger, 1997; Ja?ger, 2001; Oehrle, 2007),
Gentzen?s sequent calculus was incorporated with
variable quantification to resolve pro-forms and
VP ellipses to their antecedents. The variable
quantification in TLG is comparable to the use
of memory in storing antecedents and anaphora.
13
In Categorial Type Logic (CTL) (Hendriks, 1995;
Moortgat, 1997), gap induction was incorporated.
Syntactic categories were modified with modal-
ities which permit or prohibit gap induction in
derivation. However, logical reasoning obtained
from TLG and CTL are an NP-complete prob-
lem. In CCG, Jacobson (1999) attempted to ex-
plicitly denote non-local anaphoric requirement
whereby she introduced the anaphoric slash (|) and
the anaphoric connective (Z) to connect anaphors
to their antecedents. However, this framework
does not support anaphora whose argument is
not its antecedent, such as possessive adjectives.
Recently, a filler-gap memory mechanism was
again introduced to Categorial Grammar, called
Memory-Inductive Categorial Grammar (MICG)
(Boonkwan and Supnithi, 2008). Fillers and gaps,
encoded as memory modalities, are modified to
syntactic categories, and they are associated by the
gap-resolution connective when coordination and
serialization take place. Though their framework
is successful in resolving a wide variety of gap-
ping, its generative power falls between LIG and
Indexed Grammar, theoretically too powerful for
natural languages.
The memory mechanism introduced in this pa-
per deals with fillers and gaps in SVC. It is similar
to anaphoric resolution in ATN, Jacobson?s model,
TLG, and CTL. However, it also has prominent
distinction from them: The anaphoric mechanisms
mentioned earlier are dealing with unbounded de-
pendency or even inter-sentential ellipses, while
the memory mechanism in this paper is dealing
only with intra-sentential bounded dependency in
SVC as generalized in (13). Moreover, choices of
filler-gap association can be pruned out by the use
of combinatory directionality because the word or-
der of analytic languages is fixed. It is notice-
able that we can simply determine the grammat-
ical function (subject or object) of arbitrary np?s
in (13) from the directionality (the subject on the
left and the object on the right). With these rea-
sons, I therefore adapted the notions of MICG?s
memory modalities and gap-resolution connective
(Boonkwan and Supnithi, 2008) for the backbone
of the memory mechanism.
In CCG with Memory Mechanism (CCG-MM),
syntactic categories are modalized with memory
modalities. For each functional application, a
syntactic category can be stored, or memorized,
into the filler storage and the resulted category is
modalized with the filler 2. A syntactic category
can also be induced as a gap in a unary deriva-
tion called induction and the resulted category is
modalized with the gap 3.
There are two constraint parameters in each
modality: the combinatory directionality d ? {<
,>} and the syntactic category c, resulting in the
filler and the gap denoted in the forms 2dc and 3
d
c ,
respectively. For example, the syntactic category
2<np3
>
nps has a filler of type np on the left side and
a gap of type np on the right side.
The filler 2dc and the gap 3
d
c of the same di-
rectionality and syntactic categories are said to be
symmetric under the gap-resolution connective ?;
that is, they are matched and canceled in the gap
resolution process. Apart from MICG, I restrict
the associative power of ? to match only a filler
and a gap, not between two gaps, so that the gener-
ative power can be preserved linear. This topic will
be discussed in ?5. Given two strings of modali-
ties m1 and m2, the gap-resolution connective ?
is defined in (21).
(21) 2dcm1 ?3
d
cm2 ? m1 ?m2
3dcm1 ? 2
d
cm2 ? m1 ?m2
?  ? 
The notation  denotes an empty string. It means
that a syntactic category modalized with an empty
modality string is simply unmodalized; that is, any
modalized syntactic categories X are equivalent to
the unmodalized ones X.
Since the syntactic categories are modalized by
a modality string, all combinatory operations in
canonical CCG must preserve the modalities af-
ter each derivation step. However, there are two
conditions to be satisfied:
Condition A: At least one operands of functional
application must be unmodalized.
Condition B: Both operands of functional com-
position, disharmonic functional composi-
tion, and type raising must be unmodalized.
Both conditions are introduced to preserve the
generative power of CCG. This topic will be dis-
cussed in ?5.
As adopted from MICG, there are two memory
operations: memorization and induction.
Memorization: a filler modality is pushed to
the top of the memory when an functional appli-
cation rule is applied, where the filler?s syntactic
category must be unmodalized. Let m be a modal-
14
ity string, the memorization operation is defined in
(22).
(22) X/Y mY ? 2<X/YmX [> MF ]
mX/Y Y ? 2>Y mX [> MA]
Y mX\Y ? 2<Y mX [< MA]
mY X\Y ? 2>X\YmX [< MF ]
Induction: a gap modality is pushed to the top
of the memory when a gap of such type is induced
at either side of the syntactic category. Let m be a
modality string, the induction operation is defined
in (23).
(23) mX/Y ? 3>Y mX [> IA]
mY ? 3<X/YmX [> IF ]
mX\Y ? 3<Y mX [< IA]
mY ? 3>X\YmX [< IF ]
Because the use of memory mechanism eluci-
dates fillers and gaps hidden in the derivation, we
can then replace the decomposition rule of the
canonical CCG with the gap resolution process of
MICG. Fillers and gaps are associated in the co-
ordination and serialization by the gap-resolution
connective ?. For any given m1,m2, if m1 ? m2
exists then always m1 ? m2 ? . Given two
modality strings m1 and m2 such that m1 ? m2
exists, the coordination rule (?) and serialization
rule (?) are redefined on ? in (24).
(24) m1X & m2X ? X [?]
m1X m2X ? X [?]
At present, the memory mechanism was devel-
oped in Prolog for the sake of unification mecha-
nism. Each induction rule is nondeterministically
applied and variables are sometimes left uninstan-
tiated. For example, the sentence in (12) can be
parsed as illustrated in (25).
(25) John eats sandwiches and Mary noodles
np s\np/np np & np np
>MF >IF
2
<
s\np/nps\np 3
<
X1/np
X1
< <
2
<
s\np/nps 3
<
X2\np/np
X2
?
s
Let us consider the derivation in the right conjunct.
The gap induction is first applied on np resulting
in 3<X1/npX1, where X1 is an uninstantiated vari-
able. Then the backward application is applied, so
that X1 is unified with X2\np. Finally, the left
and the right conjuncts are coordinated yielding
that X2 is unified with s and X1 with s\np. For
convenience of type-setting, let us suppose that we
can always choose the right type in each induction
step and suppress the unification process.
Table 1: Slash modalities for memory operations.
- Left + Left
- Right ? /
+ Right . ?
Once we instantiate X1 and X2, the derivation
obtained in (25) is quite more straightforward than
the derivation in (12). The filler eats is intro-
duced on the left conjunct, while the gap of type
s\np/np is induced on the right conjunct. The co-
ordination operation associates the filler and the
gap resulting in a complete derivation.
A significant feature of the memory mechanism
is that it handles all kinds of intra-sentential el-
lipses in SVC. This is because the coordination
and serialization rules allow pro-dropping in ei-
ther the left or the right conjunct. For example, the
intra-sentential ellipses pattern in Thai SVC illus-
trated in (19) can be derived as illustrated in (26).
(26) Kla go follow seek find Laay FUT walk
in cane-field leave go
np s\np/np s\np/(s\np)/np np s\np
>IA >MA
3
>
nps\np 2
>
nps\np/(s\np)
>
2
>
nps\np
?
s\np
<
s
By replacing the decomposition rule with the
memory mechanism, CCG accepts all patterns of
pro-dropping in SVC. It should also be noted that
the derivation in (20) is per se prohibited by the
coordination rule.
Similar to canonical CCG, CCG-MM is also
resource-sensitive; that is, each combinatory op-
eration is allowed or prohibited with respect to the
resource we have (Baldridge and Kruijff, 2003).
Baldridge (2002) showed that we can obtain a
cleaner resource management in canonical CCG
by the use of modalized slashes to control combi-
natory behavior. His multimodal schema of slash
permissions can also be applied to the memory
mechanism in much the same way. I assume that
there are four modes of memory operations ac-
cording to direction and allowance of memory op-
erations as in Table 1.
The modes can be organized into the type hier-
archy shown in Figure 1. The slash modality ?,
the most limited mode, does not allow any mem-
ory operations on both sides. The slash modalities
/ and . allow memorization and induction on the
15
?






?
?
?
?
?
?
?
/
?
?
?
?
?
?
?
?
.








?
Figure 1: Hierarchy of slash modalities for mem-
ory operations.
left and right sides, respectively. Finally, the slash
modality ? allows memorization and induction on
both sides. In order to distinguish the memory op-
eration?s slash modalities from Baldridge?s slash
modalities, I annotate the first as a superscript
and the second as a subscript of the slashes. For
example, the syntactic category s\/?np denotes
that s\np allows permutation in crossed functional
composition (?) and memory operations on the
left side (/). As with Baldridge?s multimodal
framework, the slash modality ? can be omitted
from writing. By defining the slash modalities, it
follows that the memory operations can be defined
in (27).
(27) mX/.Y Y ? 2>Y mX [> MF ]
X//Y mY ? 2<X//YmX [> MA]
Y mX\/Y ? 2<Y mX [< MA]
mY X\.Y ? 2>X\.YmX [< MF ]
mX/.Y ? 3>Y mX [> IA]
mY ? 3<X//YmX [> IF ]
mX\/Y ? 3<Y mX [< IA]
mY ? 3>X\.YmX [< IF ]
When incorporating with the memory mech-
anism and the slash modalities, CCG becomes
flexible enough to handle all patterns of intra-
sentential ellipses in SVC which are prevalent in
analytic languages, and to manage its lexical re-
source. I will now show that CCG-MM extends
the generative power of the canonical CCG.
5 Generative Power
In this section, we will informally discuss the mar-
gin of generative power introduced by the memory
mechanism. Since Vijay-Shanker (1994) showed
that CCG and Linear Indexed Grammar (LIG)
(Gazdar, 1988) are weakly equivalent; i.e. they
generate the same sets of strings, we will first
compare the CCG-MM with the LIG. As will be
shown, its generative power is beyond LIG; we
will find the closest upper bound in order to locate
it in the Chomsky?s hierarchy.
We will follow the equivalent proof of Vijay-
Shanker and Weir (1994) to investigate the gen-
erative power of CCG-MM. Let us first assume
that we are going to construct an LIG G =
(VN , VT , VS , S, P ) that subsumes CCG-MM. To
construct G, let us define each of its component as
follows.
VN is a finite set of syntactic categories,
VT is a finite set of terminals,
VS is a finite set of stack symbols having the form
2dc , 3
d
c , /c, or \c,
S ? VN is the start symbol, and
P is a finite set of productions, having the form
A[] ? a
A[? ? l] ? A1[] . . . Ai[? ? l
?] . . . An[]
where each Ak ? VN , d ? {<,>}, c ? VN ,
l, l? ? VS , and a ? VT ? {}.
The notation for stacks uses [? ? l] to denote an ar-
bitrary stack whose top symbol is l. The linearity
of LIG comes from the fact that in each produc-
tion there is only one daughter that share the stack
features with its mother. Let us also define ?(?)
as the homomorphic function that converts each
modality in a modality string ? into its symmetric
counterpart, i.e. a filler 2dc into a gap3
d
c , and vice
versa. The stack in this LIG is used for storing
(1) tailing slashes of a syntactic category for har-
monic/disharmonic functional composition rules,
and (2) modalities of a syntactic category for gap
resolution.
We start out by transforming the lexical item.
For every lexical item of the formw ` Xwhere X is
a syntactic category, add the following production
to P :
(28) X[] ? w
We add two unary rules for converting between
tailing slashes and stack values. For every syntac-
tic category X and Y1, . . . , Yn, the following rules
are added.
(29) X|1Y1 . . . |nYn[??] ? X[? ? |1Y1 . . . |nYn]
X[? ? |1Y1 . . . |nYn] ? X|1Y1 . . . |nYn[??]
where the top of ?? must be a filler or a gap, or
?? must be empty. This constraint preserves the
ordering of combinatory operations.
We then transform the functional application
rules into LIG productions. From Condition A,
we can generalize the functional application rules
in (2) as follows.
16
(30) mX/Y Y ? mX
X/Y mY ? mX
mY X\Y ? mX
Y mX\Y ? mX
where m is a modality string. Condition A pre-
serves the linearity of the generative power in that
it prevents the functional application rules from in-
volving the two stacks of the daughters at once.
We can convert the rules in (30) into the following
productions.
(31) X[??] ? X[? ? /Y] Y[]
X[??] ? X[/Y] Y[??]
X[??] ? Y[??] X[\Y]
X[??] ? Y[] X[? ? \Y]
We can generalize the harmonic and dishar-
monic, forward and backward composition rules
in (6) and (9) as follows.
(32) X/Y Y|1Z1 . . . |nZn ? X|1Z1 . . . |nZn
Y|1Z1 . . . |nZn X\Y ? X|1Z1 . . . |nZn
where each |i ? {\, /}. By Condition B, we ob-
tain that all operands are unmodalized so that we
can treat only tailing slashes. That is, Condition
B prevents us from processing both tailing slashes
and memory modalities at once where the linear-
ity of the rules is deteriorated. We can therefore
convert these rules into the following productions.
(33) X[??] ? X[/Y] Y[??]
X[??] ? Y[??] X[\Y]
The memorization and induction rules de-
scribed in (27) are transformed into the following
productions.
(34) X[? ? 2<X/Y] ? X[/Y] Y[??]
X[? ? 2>Y ] ? X[? ? /Y] Y[]
X[? ? 2<Y ] ? Y[] X[? ? \Y]
X[? ? 2>X\Y] ? Y[??] X[\Y]
X[? ?3>Y ] ? X[? ? /Y]
X[? ?3<X/Y] ? Y[??]
X[? ?3<Y ] ? X[? ? \Y]
X[? ?3>X\Y] ? Y[??]
However, it is important to take into account the
coordination and serialization rules, because they
involve two stacks which have similar stack val-
ues if we convert one of them into the symmetric
form with ?. Those rules can be transformed as
follows.
(35) X[] ? X[??] &[] X[?(??)]
X[] ? X[??] X[?(??)]
It is obvious that the rules in (35) are not LIG pro-
duction; that is, CCG-MM cannot be generated by
any LIGs; or more precisely, CCG-MM is prop-
erly more powerful than CCG. We therefore have
to find an upper bound of its generative power.
Though CCG-MM is more powerful than CCG
and LIG, the rules in (35) reveal a significant prop-
erty of Partially Linear Indexed Grammar (PLIG)
(Keller and Weir, 1995), an extension of LIG
whose productions are allowed to have two or
more daughters sharing stack features with each
other but these stacks are not shared with their
mother as shown in (36).
(36) A[] ? A1[] . . . Ai[??] . . . Aj [??] . . . An[]
Whereby restricting the power of the gap-
resolution connective, the two stacks of the daugh-
ters are shared but not with their mother. An in-
teresting trait of PLIG is that it can generate the
language {wk|w is in a regular language and k ?
N}. This is similar to the pattern of SVC in which
a series of verb phrase can be reduplicated.
To conclude this section, CCG-MM is more
powerful than LIG but less powerful than PLIG.
From (Keller and Weir, 1995), we can position the
CCG-MM in the Chomsky?s hierarchy as follows:
CFG < CCG = TAG = HG = LIG < CCG-MM ? PLIG
? LCFRS < CSG.
6 Conclusion and Future Work
I have presented an approach to treating serial
verb construction in analytic languages by incor-
porating CCG with a memory mechanism. In the
memory mechanism, fillers and gaps are stored
as modalities that modalize a syntactic category.
The fillers and the gaps are then associated in the
coordination and the serialization rules. This re-
sults in a more flexible way of dealing with intra-
sentential ellipses in SVC than the decomposition
rule in canonical CCG. Theoretically speaking, the
proposed memory mechanism increases the gen-
erative power of CCG into the class of partially
linear indexed grammars.
Future research remains as follows. First, I will
investigate constraints that reduce the search space
of parsing caused by gap induction. Second, I will
apply the memory mechanism in solving discon-
tinuous gaps. Third, I will then extend this frame-
work to free word-ordered languages. Fourth and
finally, the future direction of this research is to
develop a wide-coverage parser in which statistics
is also made use to predict memory operations oc-
curing in derivation.
17
References
Jason Baldridge and Geert-Jan M. Kruijff. 2003. Mul-
timodal combinatory categorial grammar. In Pro-
ceedings of the 10th Conference of the European
Chapter of the ACL 2003, pages 211?218, Budapest,
Hungary.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Prachya Boonkwan and Thepchai Supnithi. 2008.
Memory-inductive categorial grammar: An ap-
proach to gap resolution in analytic-language trans-
lation. In Proceedings of The Third International
Joint Conference on Natural Language Processing,
volume 1, pages 80?87, Hyderabad, India, January.
Gerald Gazdar. 1988. Applicability of indexed
grammars to natural languages. In U. Reyle and
C. Rohrer, editors, Natural Language Parsing and
Linguistic Theories, pages 69?94. Reidel, Dor-
drecht.
Petra Hendriks. 1995. Ellipsis and multimodal catego-
rial type logic. In Proceedings of Formal Grammar
Conference, pages 107?122. Barcelona, Spain.
Pauline Jacobson. 1999. Towards a variable-free se-
mantics. Linguistics and Philosophy, 22:117?184,
October.
Gerhard Ja?ger. 1997. Anaphora and ellipsis in type-
logical grammar. In Proceedings of the 11th Amster-
dam Colloquium, pages 175?180, Amsterdam, the
Netherland. ILLC, Universiteit van Amsterdam.
Gerhard Ja?ger. 2001. Anaphora and quantification
in categorial grammar. In Lecture Notes in Com-
puter Science; Selected papers from the 3rd Interna-
tional Conference, on logical aspects of Computa-
tional Linguistics, volume 2014/2001, pages 70?89.
Bill Keller and David Weir. 1995. A tractable exten-
sion of linear indexed grammars. In In Proceedings
of the 7th European Chapter of ACL Conference.
Charles N. Li and Sandra A. Thompson. 1981. Man-
darin Chinese: A Functional Reference Grammar.
Berkeley: University of California Press.
Michael Moortgat. 1997. Categorial type logics. In
van Benthem and ter Meulen, editors, Handbook of
Logic and Language, chapter 2, pages 163?170. El-
sevier/MIT Press.
Glyn Morrill. 1994. Type logical grammar. In Catego-
rial Logic of Signs. Kluwer, Dordrecht.
Nuttanart Muansuwan. 2002. Verb Complexes in Thai.
Ph.D. thesis, University at Buffalo, The State Uni-
versity of New York.
Richard T. Oehrle, 2007. Non-Transformational Syn-
tax: A Guide to Current Models, chapter Multi-
modal Type Logical Grammar. Oxford: Blackwell.
Mark Steedman. 1990. Gapping as constituent coordi-
nation. Linguistics and Philosophy, 13:207?263.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Massachusetts.
Kingkarn Thepkanjana. 1986. Serial Verb Construc-
tions in Thai. Ph.D. thesis, University of Michigan.
K. Vijay-Shanker and David J. Weir. 1994. The equiv-
alence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511?546.
William A. Woods. 1970. Transition network gram-
mars for natural language analysis. Communica-
tions of the ACM, 13(10):591?606, October.
18
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 94?101,
Dublin, Ireland, August 23-29 2014.
Character-Cluster-Based Segmentation using Monolingual and Bilingual Information for Statistical Machine Translation 
Vipas Sutantayawalee  Peerachet Porkeaw  Thepchai Supnithi Prachya Boonkwan  Sitthaa Phaholphinyo  National Electronics and Computer Technology Center, Thailand  {vipas.sutantayawalee, peerachet.porkeaw,prachya.boonkwan, sitthaa.phaholphinyo,thepchai}@nectec.or.th   Abstract We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing character co-occurrence statistics and orthographic insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster alignment, co-occurrence frequency and alignment confidence into that result. We evaluate the effectiveness of our method on PB-SMT task using English-Thai language pair and report the best improvement of 8.1% increase in BLEU score. There are two main advantages of our approach. First, our method requires less effort on developing the corpus and can be applied to unsegmented corpus or poor-quality manually segmented corpus. Second, this technique does not only limited to specific language pair but also capable of automatically adjust the character cluster boundaries to be suitable for other language pairs.  1 Introduction Nowadays, it is admitted that word segmentation is a crucial part of Statistical Machine Translation (SMT) especially in the languages where there are no explicit word boundaries such as Chinese, Japanese or Thai. The writing system of these languages allow each word can be written continuously with no space appearing between words. Consequently, word ambiguities will arise if word boundary has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator is required to disambiguate each word separator before processing another task in SMT. Several word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal.  In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches make use of either manually segmented [4]  or unsegment1ed bilingual corpus [5] as a guideline information to perform a word segmentation task and improve the performance of SMT system. 
                                                            This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
94
In this paper, we propose a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information on English-Thai language pair and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result to manually segmented corpus in PB-SMT task when the good heuristics character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing heuristic algorithm and language insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster (CC) alignment, CC co-occurrence frequency and alignment confidence into that result. These two tasks can be performed repeatedly. The remainder of this paper is organized as follows. Section 2 provides some information related to our work. Section 3 describes the methodology of our approach. Section 4 present the experiments setting. Section 5 present the experimental results and empirical analysis. Section 6 and 7 gives a conclusion and future work respectively.  2 Related Work 2.1 Thai Character Clustering  In Thai writing system, there are no explicit word boundaries as in English, and a single Thai character does not have specific meanings like Chinese, Japanese and Korean. Thai characters could be consonants, vowels and tone marks and a word can be formed by combining these characters. From our observation, we found that the average length of Thai words on BEST2010 corpus (National Electronics and Computer Technology Center, Thailand 2010) is 3.855. This makes the search space of Thai word segmentation very large. To alleviate this issue, the notion of Thai character cluster (TCC), is introduced [1] to reduce the search space with predetermined unambiguious constraints for cluster formation. A cluster may not be meaningful and has to combine with other consecutive clusters to form a word. Characters in the cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result.  2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word segmentation. Several methods have used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between English and Chinese language pair. On the other hands, [4] is based on the manually segmented bilingual corpus and then try to ?repack? the word from existing alignment by using alignment confidence. Both works evaluated the performance in BLEU metric and reported the promising result of PB-SMT task.    3 Methodology This paper aim to compare translation quality based on SMT task between the systems trained on bilingual corpus that contains both segmented source and target, and on the same bilingual corpus with segmented source but unsegmented target. First, we make use of monolingual information by employing several character cluster algorithms on unsegmented data. Second, we use bilingual-guided alignment information retrieved from alignment extraction process for improving character cluster segmentation. Then, we evaluate our performance based on translation accuracy by using BLEU metric. We want to prove that (1) the result of PB-SMT task using unsegmented corpus (unsupervised) 
95
is nearly identical result to manually segmented (supervised) data and (2) when bilingual information are also applied, the performance of PB-SMT is also improved. 3.1 Notation Given a target ????  ? sentence ??? consisting of ? clusters ? ??,? , ?? , where ?? ? 1. If ?? = 1, we call ??as a single character ??. Otherwise, we call is as a character cluster ? . In addition, given a English sentence ???  consisting of ? words ? ?,? , ?? , ????? denotes a set of English-to-Thai language word alignments between ???  and ????. In addition, since we concentrate on one-to-many alignments, ????  , can be rewritten as a set of pairs ?? and ?? = ?< ?? , ?? > noting a link between one single English word and several Thai characters that are formed to one cluster ? 	 3.2 Monolingual Information Due to the issue mentioned in section 2.1, we apply character clustering (CC) technique on target text in order to reduce the search space. After performing CC, it will yield several character clusters ?which can be grouped together to obtain a larger unit which approaches the notion of word. However, for Thai and Lao, we do not only receive ? but also ? which usually has no meaning by itself. Moreover, Thai, Burmese and Lao writing rule does not allow ? to stand alone in most case. Thus, we are required to develop various adapted versions of CC by using orthographic insight and heuristic algorithm to automatically pack the characters that reside in a pre-defined grammatical word list handcrafted by linguists. Then, all of single consonants in Thai Burmese, and Lao are forced to group with either left or right cluster due to the Thai writing system. The decision has been made by consulting on character co-occurrence statistics (unigram and bigram frequency). Eventually, we obtain different character cluster alignments from the system trained on various CC approaches which effect to translation quality as shown in section 5.1 3.3 Bilingually-Guided Alignment Information We begin with the sequence of small clusters resulting from previous character clustering process.  These small clusters can be grouped together in order to form ?word? using bilingually-guided alignment information. Generally, small consecutive clusters in target side which are aligned to the same word in source data should be grouped together. Therefore, this section describes our one-to-many alignment extraction process.   For one-to-many alignment, we applied processes similar to those in phrase extraction algorithm [7] which is described as follows.  With English sentence ???  and a Thai character cluster ???, we apply IBM model 1-5 to extract word-to-cluster translation probability of source-to-target ?(?|?) and target-to-source ??(?|?). Next, the alignment points which have the highest probability are greedily selected from both ?(?|?) and ?(?|?). Figure 1.a and 1.b show examples of alignment points of source-to-target and target-to-source respectively. After that we selected the intersection of alignment pairs from both side. Then, additional alignment points are added according to the growing heuristic algorithm (grow additional alignment points, [8])  
 (a)   (b)  
96
 (c)   (d)   Figure 1. The process of one-to-many alignment extraction (a) Source-to-Target word alignment (b) Target-to-Source word alignment (c) Intersection between (a) and (b).  (d) Result of (c) after applying the growing heuristic algorithm.   Finally, we select consecutive clusters which are aligned to the same English word as candidates. From the Figure 1.d, we obtain these candidates (red, ?????) and (bicycle, ??? ? ?? ?). 3.4 Character Cluster Repacking  Although the alignment information obtained from the previous step is very helpful for the PB-SMT task, there is still plenty of room to enhance the PB-SMT performance. One way of doing that is by using word repacking [4]. However, in this paper, we perform a character cluster repacking (CCR) instead of word. The main purpose of repacking technique is to group all small consecutive clusters (or word) in target side that frequently align with one word in source data. Repacking approaches uses two simple calculations which are a co-occurrence frequency (???? ?(?? , ??)) and alignment confidence (??( ???)). (???? ?(?? , ??)) is the number of times ?? and ??? co-occurr in the bilingual corpus [4] [9] and ??( ???) is a measure of how often the aligner aligns ?? and ???  when they co-occur. AC is defined as  ??(??)  ?=  ? ?(??)???? ?(?? , ??) ?  where ?(??) denotes the number of alignments suggested by the previous-step word aligner.  Unfortunately, due to the limited memory in our experiment machine, we cannot find ????? ?(?? , ??)) for all possible < ?? , ?? > ?pairs. We, therefore, slightly modified the above equation by finding ?(??) first. Secondly, we begin searching ????? ?(?? , ??)) from all possible alignments in ??? instead of finding all occurrences in corpus. By applying this modification, we eliminate < ?? , ?? > ?pairs that co-occur together but never align to each other by previous-step aligner (?? ??  ?equals to zero) so as to reduce the search space and complexity in our algorithm. Thirdly, we choose ?? with the highest ??(??) and repack all character clusters in target side that similar to ?? to be a new single cluster unit. This process can be done repeatedly. However, we have run this task less than twice since there are few new cluster unit appear after two iterations have passed. The running example of this algorithm is described as follows  Suppose previous step aligner (GIZA++) produce two alignments ??? = ?< ??, ??,? > and ?? = ?<??, ??,?,? > ?CCR will find the frequency of each aligment and number of times ?? and ??? co-occurr in the bilingual corpus ( ???? ? ??, ??,?  and  ???? ? ??, ??,?,?  ?). Then, we will have ??(??) ? score for each alignment and the aligment with the highest ?? will  be selected. The CCR will group these cluster ( e.g. ??,? ) to be a new single cluster unit.       
97
4 Experimental Setting 4.1 Data The bilingual corpus1 we used in our experiment is constructed from several sources and consists of multiple domains (e.g news, travel, article, entertainment, computer, etc.). We divided this corpus into three sets plus one additional test set as shown below            Table 1. Information of bilingual corpus  4.2 Tools and Evaluation We evaluate our system in term of translation quality based on phrase-based SMT.  Source sentences are sequence of English words while target sentences are sequences of Thai character clusters and each cluster size depends on which approach used in the experiment.   Translation model and language model are train based on the standard phrase-based SMT. Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++ [8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data.  Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score [11]. Therefore, we re-segment the reference data (manually segmented) and the translation result data based on TCC. Some may concern about using TCC will lead to over estimation (higher than actual) due to the BLEU score is design based on word and not based on character. However, we used this BLEU score only for comparing translation quality among our experiments. Comparing to other SMT system still require running BLEU score based on the same segmentation guideline. 5 Results and Discussion We conducted all experiments on PB-SMT task and reported the performance of PB-SMT system based on the BLEU measure. First, we use a method proposed in section 3.2 followed by the approach in section 3.3 in order to the receive first translation result set (without CCR). Then, we perform a method describe in 3.4 and also follow by approach in section 3.3 in order to receive another translation result set (with CCR). Table 1 shows the number of character clusters that are decreasing over time when several different character clustering approaches are applied.     
                                                            1 Currently, the corpus we used is a proprietary of NECTEC and does not available to public yet due to the licensing issue. However, for the educational purpose, this corpus is available upon by request. 
 Data Set No. of sentence pairs Train  633,589 Dev 12,568 Test #1 3,426 Test #2 500 
98
  Table 2. Number of character clusters when different character clustering approaches are applied on the bilingual corpus	  Next, we present all translation results of PB-SMT task that using different character clustering approaches. Each training set is trained with only one character clustering method which are (1) TCC (baseline), (2) TCC with CCR, (3) TCC with only orthographic insight (TCC-FN), (4) TCC-Fn with CCR, (5) TCC with language insight and heuristic algorithm (TCC-FN-B) and (6) TCC-FN-B with CCR. The results are shown in Table 3.   
 No. of Character Clusters (or word in original data) Approaches Without CCR With CCR TCC (baseline) 9,862,271 7,187,862 TCC with language insight (TCC-FN) 8,953,437 6,636,305 TCC with language insight and heuristic algorithm (TCC-FN-B) 6,545,617 5,448,437 Manually segmented corpus (Upper bound) 5,311,648 N/A 
 Table 3. BLEU score of each character clustering method  and the percentage of the improvement when we applied CCR to the data   
 Test #1 BLEU % of BLEU Improvement Test #2 BLEU % of BLEU Improvement Approaches Without CCR With CCR Without CCR With CCR Baseline 37.12 40.13 8.11 36.78 38.87 5.68 TCC-FN 40.23 41.90 4.15 38.36 39.09 1.90 TCC-FN-B 44.69 44.43 -0.58 40.45 40.81 0.89 Upper bound 47.04 N/A N/A 40.73 N/A N/A 
 (a)  
35	 ?
36	 ?
37	 ?
38	 ?
39	 ?
40	 ?
41	 ?
42	 ?
43	 ?
44	 ?
45	 ?
46	 ?
47	 ?
48	 ?
Upperbound	 ?
No	 ?CCR	 ?
With	 ?CCR	 ?
Upperbound	 ?
99
 (b)   Figure 2. The BLEU score of (a) test set no.1 and (b) test set no.2   As seen from Table 3, when we apply the enhanced version of TCCs into the data with no CCR, BLEU score have gradually increased and almost reached the same level as original in test set #2. Furthermore, when CCR have been also deployed on each training dataset, the results of BLEU are also rise in the same manner with Without CCR method. There are certain significant points that should be noticed. First, CCR method is able to yield maximum of 8.1 % BLEU score increase. Second, when we apply the CCR methods and reach at some point, few improvement or minor degradation is received as shown in TCC-FN-B without and with CCR result. This is because the number of clusters produced by this character clustering algorithm is almost equal to number of words in original data as shown in Table 2 and this approach might suffer from the word boundary misplacement problem. Third, character clustering that use TCC with orthographic insight and heuristic algorithm combined with CCR approach is able to overcome the translation result from original data for the first time.   6 Conclusion In this paper, we introduce a new approach for performing word segmentation task for SMT. Instead of starting with word level, we focus on character cluster level because this approach can perform on unsegmented corpus or multiple-guideline manually segmented corpus. First, we apply several adapted versions of TCC on unsegmented data. Next, we use a bilingual corpus to find alignment information for all < ?? , ?? > ?  pairs and then employ character cluster repacking method in order to form the large cluster of Thai characters.   We evaluate our approach on translation task on several sources and different domain corpus and report the result in BLEU metric. Our technique demonstrates that (1) we can achieve a dramatically improvement of BLUE as of 8.1% when we apply adapted TCC with CCR and (2) it is possible to overcome the manually segmented corpus by using TCC with orthographic insight and heuristic algorithm character clustering method combined with CCR. The advantage of our approach is a reduction in time and effot for construct a billinugal corpus because we are no longer required to manually segment all sentences in target side. In addition, our approach is able to cope with larger data information (e.g. 1 million sentences pairs) and adaptable to other language pairs (e.g. English-Chinese, English-Japanese or English-Lao) 	 
35	 ?
36	 ?
37	 ?
38	 ?
39	 ?
40	 ?
41	 ?
42	 ?
Baseline	 ? TCCAFn	 ? TCCAFnB	 ?
No	 ?CCR	 ?
With	 ?CCR	 ?
Upperbound	 ?
100
7 Future Work There are some tasks that can be added into this approaches. Firstly, we can make use of trigram (and n-gram) statistics, maximum entropy or conditional random field on heuristic algorithm in adapted version of TCC. Secondly, we might report the result from another language pair in order to confirm our approach.Thirdly, we can modify CCR process to be able to rerank the alignment confidence by using discriminative approach. Lastly, name entity recognition system can be integrated with our approach in order to improve the SMT performance.  Reference 	 ?[1]  T. Teeramunkong, V. Sornlertlamvanich, T. Tanhermhong and W. Chinnan, ?Character cluster based Thai information retrieval,? in IRAL '00 Proceedings of the fifth international workshop on on Information retrieval with Asian languages, 2000.  [2]  C. Kruengkrai, K. Uchimoto, J. Kazama, K. Torisawa, H. Isahara and C. Jaruskulchai, ?A Word and Character-Cluster Hybrid Model for Thai Word Segmentation,? in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.  [3]  Y. Liu, W. Che and T. Liu, ?Enhancing Chinese Word Segmentation with Character Clustering,? in Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, China, 2013.  [4]  Y. Ma and A. Way, ?Bilingually motivated domain-adapted word segmentation for statistical machine translation,? in Proceeding EACL '09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pp. 549-557, Stroudsburg, PA, USA, 2009.  [5]  J. Xu, R. Zens and H. Ney, ?Do We Need Chinese Word Segmentation for Statistical Machine Translation?,? ACL SIGHAN Workshop 2004, pp. 122-129, 2004.  [6]  P. Limcharoen, C. Nattee and T. Theeramunkong, ?Thai Word Segmentation based-on GLR Parsing Technique and Word N-gram Model,? in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.  [7]  P. Koehn, F. J. Och and D. Marcu, ?Statistical phrase-based translation,? in NAACL '03 Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Stroudsburg, PA, USA, 2003.  [8]  F. J. Och and H. Ney, ?A systematic comparison of various statistical alignment models,? Computational Linguistics, vol. 29, no. 1, pp. 19-51, 2003.  [9]  I. D. Melamed, ?Models of translational equivalence among words,? Computational Linguistics, vol. 26, no. 2, pp. 221-249, 2000.  [10]  ?SRILM -- An extensible language modeling toolkit,? in Proceeding of the International Conference on Spoken Language Processing, 2002.  [11]  P.-C. Chang, M. Galley and C. D. Manning, ?Optimizing Chinese word segmentation for machine translation performance,? in Proceedings of the Third Workshop on Statistical Machine Translation, Columbus, Ohio, 2008.     
101
