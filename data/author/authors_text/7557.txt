Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1112?1118,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Dependency Parsing and Domain Adaptation using DeSR 
Giuseppe Attardi 
Felice Dell?Orletta 
Maria Simi 
Dipartimento di Informatica 
largo B. Pontecorvo 3 
I-56127 Pisa, Italy 
attardi@di.unipi.it 
felice.dellorletta@
ilc.cnr.it 
simi@di.unipi.it 
Atanas Chanev 
Universit? di Trento 
via Matteo del Ben 5 
I-38068 Rovereto, Italy 
Fondazione Bruno Kessler-irst 
via Sommarive 18 
I-38050 Povo, Italy 
chanev@form.unitn.it 
 
Massimiliano Ciaramita 
Yahoo! Research Barcelona 
Ocata 1 
S-08003 Barcelona, Spain 
massi@yahoo-inc.com 
 
Abstract 
We describe our experiments using the 
DeSR parser in the multilingual and do-
main adaptation tracks of the CoNLL 2007 
shared task. DeSR implements an incre-
mental deterministic Shift/Reduce parsing 
algorithm, using specific rules to handle 
non-projective dependencies. For the multi-
lingual track we adopted a second order 
averaged perceptron and performed feature 
selection to tune a feature model for each 
language. For the domain adaptation track 
we applied a tree revision method which 
learns how to correct the mistakes made by 
the base parser on the adaptation domain. 
1 Introduction 
Classifier-based dependency parsers (Yamada and 
Matsumoto, 2003; Nivre and Scholz, 2004) learn 
from an annotated corpus how to select an 
appropriate sequence of Shift/Reduce actions to 
construct the dependency tree for a sentence. 
Learning is based on techniques such as SVM 
(Vapnik 1998) or Memory Based Learning 
(Daelemans 2003), which provide high accuracy 
but are often computationally expensive. For the 
multilingual track in the CoNLL 2007 Shared 
Task, we employed a Shift/Reduce parser which 
uses a perceptron algorithm with second-order 
feature maps, in order to verify whether a simpler 
and faster algorithm can still achieve comparable 
accuracy. 
For the domain adaptation track we wished to 
explore the use of tree revisions in order to 
incorporate language knowledge from a new 
domain. 
2 Multilingual Track 
The overall parsing algorithm is a deterministic 
classifier-based statistical parser, which extends 
the approach by Yamada and Matsumoto (2003), 
by using different reduction rules that ensure 
deterministic incremental processing of the input 
sentence and by adding specific rules for handling 
non-projective dependencies. The parser also 
performs dependency labeling within a single 
processing step. 
The parser is modular and can use several 
learning algorithms. The submitted runs used a 
second order Average Perceptron, derived from the 
multiclass perceptron of Crammer and Singer 
(2003). 
No additional resources were used. No pre-
processing or post-processing was used, except 
stemming for English, by means of the Snowball 
stemmer (Porter 2001). 
3 Deterministic Classifier-based Parsing 
DeSR (Attardi, 2006) is an incremental determinis-
tic classifier-based parser. The parser constructs 
dependency trees employing a deterministic bot-
tom-up algorithm which performs Shift/Reduce 
actions while analyzing input sentences in left-to-
right order. 
Using a notation similar to (Nivre and Scholz, 
2003), the state of the parser is represented by a 
1112
quadruple ?S, I, T, A?, where S is the stack of past 
tokens, I is the list of (remaining) input tokens, T is 
a stack of temporary tokens and A is the arc rela-
tion for the dependency graph. 
Given an input string W, the parser is initialized 
to ?(), W, (), ()?, and terminates when it reaches a 
configuration ?S, (), (), A?. 
The three basic parsing rule schemas are as fol-
lows: ?S, n|I, T, A? Shift ?n|S, I, T, A? ?s|S, n|I, T, A? Rightd ?S, n|I, T, A?{(s, d, n)}? ?s|S, n|I, T, A? Leftd ?S, s|I, T, A?{(n, d, s)}? 
The schemas for the Left and Right rules are in-
stantiated for each dependency type d ? D, for a 
total of 2|D| + 1 rules. These rules perform both 
attachment and labeling. 
At each step the parser uses classifiers trained 
on a treebank corpus in order to predict which ac-
tion to perform and which dependency label to as-
sign given the current configuration. 
4 Non-Projective Relations 
For handling non-projective relations, Nivre and 
Nilsson (2005) suggested applying a pre-
processing step to a dependency parser, which con-
sists in lifting non-projective arcs to their head re-
peatedly, until the tree becomes pseudo-projective. 
A post-processing step is then required to restore 
the arcs to the proper heads. 
In DeSR non-projective dependencies are han-
dled in a single step by means of the following ad-
ditional parsing rules, slightly different from those 
in (Attardi, 2006): 
 ?s1|s2|S, n|I, T, A? Right2d ? S, s1|n|I, T, A?{(s2, d, n)}? ?s1|s2|S, n|I, T, A? Left2d ?s2|S, s1|I, T, A?{(n, d, s2)}? ?s1|s2|s3|S, n|I, T, A? Right3d ? S, s1|s2|n|I, T, A?{(s3, d, n)}? ?s1|s2|s3|S, n|I, T, A? Left3d ?s2|s3|S, s1|I, T, A?{(n, d, s3)}? 
 ?s1|s2|S, n|I, T, A? Extract ?n|s1|S, I, s2|T, A? ?S, I, s1|T, A? Insert ?s1|S, I, T, A? 
Left2, Right2 are similar to Left and Right, except 
that they create links crossing one intermediate 
node, while Left3 and Right3 cross two intermedi-
ate nodes. Notice that the RightX actions put back 
on the input the intervening tokens, allowing the 
parser to complete the linking of tokens whose 
processing had been delayed. Extract/Insert gener-
alize the previous rules by moving one token to the 
stack T and reinserting the top of T into S. 
5 Perceptron Learning and 2nd-Order 
Feature Maps 
The software architecture of the DeSR parser is 
modular. Several learning algorithms are available, 
including SVM, Maximum Entropy, Memory-
Based Learning, Logistic Regression and a few 
variants of the perceptron algorithm. 
We obtained the best accuracy with a multiclass 
averaged perceptron classifier based on the 
ultraconservative formulation of Crammer and 
Singer (2003) with uniform negative updates. The 
classifier function is: { }xxF k
k
?= ?maxarg)(  
where each parsing action k is associated with a 
weight vector ?k. To regularize the model the final 
weight vectors are computed as the average of all 
weight vectors posited during training. The number 
of learning iterations over the training data, which 
is the only adjustable parameter of the algorithm, 
was determined by cross-validation.  
In order to overcome the limitations of a linear 
perceptron, we introduce a feature map ?: IRd ? 
IRd(d+1)/2 that maps a feature vector x into a higher 
dimensional feature space consisting of all un-
ordered feature pairs: ?(x) = ?xixj | i = 1, ?, d, j = i, ?, d? 
In other words we expand the original 
representation in the input space with a feature 
map that generates all second-order feature 
combinations from each observation. We call this 
the 2nd-order model, where the inner products are 
computed as ?k ? ?(x), with ?k a vector of dimen-
sion d(d+1)/2. Applying a linear perceptron to this 
feature space corresponds to simulating a polyno-
mial kernel of degree two.  
A polynomial kernel of degree two for SVM 
was also used by Yamada and Matsumoto (2003). 
However, training SVMs on large data sets like 
those arising from a big training corpus was too 
1113
computationally expensive, forcing them to resort 
to partitioning the training data (by POS) and to 
learn several models. 
Our implementation of the perceptron algorithm 
uses sparse data structures (hash maps) so that it 
can handle efficiently even large feature spaces in 
a single model. For example the feature space for 
the 2nd-order model for English contains over 21 
million. Parsing unseen data can be performed at 
tens of sentences per second. More details on such 
aspects of the DeSR parser can be found in (Ci-
aramita and Attardi 2007). 
6 Tuning 
The base parser was tuned on several parameters to 
optimize its accuracy as follows. 
6.1 Feature Selection 
Given the different characteristics of languages and 
corpus annotations, it is worth while to select a 
different set of features for each language. For ex-
ample, certain corpora do not contain lemmas or 
morphological information so lexical information 
will be useful. Vice versa, when lemmas are pre-
sent, lexical information might be avoided, reduc-
ing the size of the feature set. 
We performed a series of feature selection ex-
periments on each language, starting from a fairly 
comprehensive set of 43 features and trying all 
variants obtained by dropping a single feature. The 
best of these alternatives feature models was cho-
sen and the process iterated until no further gains 
were achieved. The score for the alternatives was 
computed on a development set of approximately 
5000 tokens, extracted from a split of the original 
training corpus. 
Despite the process is not guaranteed to produce 
a global optimum, we noticed LAS improvements 
of up to 4 percentage points on some languages. 
The set of features to be used by DeSR is con-
trolled by a number of parameters supplied through 
a parameter file. Each parameter describes a fea-
ture and from which tokens to extract it. Tokens 
are referred through positive numbers for input 
tokens and negative numbers for tokens on the 
stack. For example 
PosFeatures -2 -1 0 1 2 3 
means to use the POS tag of the first two tokens on 
the stack and of the first four tokens on the input. 
The parameter PosPrev refers to the POS of the 
preceding token in the original sentence, PosLeftChild refers to the POS of the left chil-
dren of a token, PastActions tells how many 
previous actions to include as features. 
The selection process was started from the fol-
lowing base feature model: 
LexFeatures -1 0 1 LemmaFeatures -2 -1 0 1 2 3 LemmaPrev  -1 0 LemmaSucc  -1 0 LemmaLeftChild -1 0 LemmaRightChild -1 MorphoFeatures -1 0 1 2 PosFeatures -2 -1 0 1 2 3 PosNext  -1 0 PosPrev  -1 0 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 DepFeatures -1 0 DepLeftChild -1 0 DepRightChild -1 PastActions 1 
The selection process produced different variants 
for each language, sometimes suggesting dropping 
certain intermediate features, like the lemma of the 
third next input token in the case of Catalan: 
LemmaFeatures -2 -1 0 1 3 LemmaPrev  0 LemmaSucc  -1 LemmaLeftChild 0 LemmaRightChild -1 PosFeatures -2 -1 0 1 2 3 PosPrev  0 PosSucc  -1 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 MorphoFeatures 0 1 DepLeftChild -1 0 DepRightChild -1 
For Italian, instead, we ran a series of tests in par-
allel using a set of manually prepared feature mod-
els. The best of these models achieved a LAS of 
80.95%. The final run used this model with the 
addition of the morphological agreement feature 
discussed below. 
 
English was the only language for which no feature 
selection was done and for which lexical features 
1114
were used. English is also the language where the 
official score is significantly lower than what we 
had been getting on our development set (90.01% 
UAS). 
6.2 Prepositional Attachment 
Certain languages, such as Catalan, use detailed 
dependency labeling, that for instance distinguish 
between adverbials of location and time. We ex-
ploited this information by introducing a feature 
that captures the entity type of a child of the top 
word on the stack or in the input. During training a 
list of nouns occurring in the corpus as dependent 
on prepositions with label CCL (meaning ?com-
plement of location? for Catalan) was created and 
similarly for CCT (complement of time). The en-
tity type TIME is extracted as a feature depending 
on whether the noun occurs in the time list more 
than ? times than in the location list, and similarly 
for the feature LOCATION. ? was set to 1.5 in our 
experiments. 
6.3 Morphological Agreement 
Certain languages require gender and number 
agreement between head and dependent. The fea-
ture MorphoAgreement is computed for such lan-
guages and provided noticeable accuracy 
improvements. 
For example, for Italian, the improvement was 
from: 
  LAS: 80.95%,  UAS: 85.03% 
to: 
  LAS: 81.34%,  UAS: 85.54% 
For Catalan, adding this feature we obtained an 
unofficial score of: 
  LAS: 87.64%,  UAS: 92.20% 
with respect to the official run: 
  LAS: 86.86%,  UAS: 91.41% 
7 Accuracy 
Table 1 reports the accuracy scores in the multilin-
gual track. They are all considerably above the 
average and within 2% from the best for Catalan, 
3% for Chinese, Greek, Italian and Turkish. 
8 Performance 
The experiments were performed on a 2.4 Ghz 
AMD Opteron machine with 32 GB RAM. Train-
ing the parser using the 2nd-order perceptron on the 
English corpus required less than 3 GB of memory 
and about one hour for each iteration over the 
whole dataset. Parsing the English test set required 
39.97 sec. For comparison, we tested the MST 
parser version 0.4.3 (Mstparser, 2007), configured 
for second-order, on the same data: training took 
73.9 minutes to perform 10 iterations and parsing 
took 97.5 sec. MST parser achieved: 
LAS: 89.01%, UAS: 90.17% 
9 Error Analysis on Catalan 
The parser achieved its best score on Catalan, so 
we performed an analysis on its output for this lan-
guage. 
Among the 42 dependency relations that the 
parser had to assign to a sentence, the largest num-
ber of errors occurred assigning CC (124), SP (33), CD (27), SUJ (26), CONJUNCT (22), SN (23). 
The submitted run for Catalan did not use the 
entity feature discussed earlier and indeed 67 er-
rors were due to assigning CCT or CCL instead of 
CC (generic complement of circumstance). How-
ever over half of these appear as underspecified 
annotation errors in the corpus rather than parser 
errors. 
By adding the ChildEntityType feature, 
which distinguishes better between CCT and CCL, 
the UAS improved, while the LAS dropped 
slightly, due to the effect of underspecified annota-
tions in the corpus: 
   LAS: 87.22%,    UAS: 91.71% 
Table 1. Multilingual track official scores. 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
Arabic  76.52  72.66 68.34  86.09  82.53 78.84  
Basque  76.92  69.48 68.06  82.80  76.86 75.15  
Catalan  88.70  86.86 79.85  93.40  91.41 87.98  
Chinese  84.69  81.50 76.59  88.94  86.73 81.98  
Czech  80.19  77.37 70.12  86.28  83.40 77.56  
English  89.61  85.85 80.95  90.63  86.99 82.67  
Greek  76.31  73.92 70.22  84.08  80.75 77.78  
Hungarian  80.27  76.81 71.49  83.55  81.81 76.34  
Italian  84.40  81.34 78.06  87.91  85.54 82.45  
Turkish  79.81  76.87 73.19  86.22  83.56 80.33  
1115
A peculiar aspect of the original Catalan corpus 
was the use of a large number (195) of dependency 
labels. These labels were reduced to 42 in the ver-
sion used for CoNNL 2007, in order to make it 
comparable to other corpora. However, performing 
some preliminary experiments using the original 
Catalan collection with all 195 dependency labels, 
the DeSR parser achieved a significantly better 
score: 
LAS: 88.80%, UAS: 91.43% 
while with the modified one, the score dropped to: 
LAS: 84.55%, UAS: 89.38% 
This suggests that accuracy might improve for 
other languages as well if the training corpus was 
labeled with more precise dependencies. 
10 Adaptation Track 
The adaptation track originally covered two do-
mains, the CHILDES and the Chemistry domain.  
The CHILDES (Brown, 1973; MacWhinney, 
2000) consists of transcriptions of dialogues with 
children, typically short sentences of the kind: 
Would you like more grape juice ? 
That 's a nice box of books . 
Phrases are short, half of them are questions. The 
only difficulty that appeared from looking at the 
unlabeled collection supplied for training in the 
domain was the presence of truncated terms like goin (for going), d (for did), etc. However none 
of these unusually spelled words appeared in the 
test set, so a normal English parser performed rea-
sonably well on this task. Because of certain in-
consistencies in the annotation guidelines, the 
organizers decided to make this task optional and 
hence we submitted just the parse produced by the 
parser trained for English. 
For the second adaptation task we were given a 
large collection of unlabeled data in the chemistry 
domain (Kulick et al 2004) as well as a test set of 
5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll). 
There were three sets of unlabeled documents: 
we chose the smallest (unlab1) consisting of over 
300,000 tokens (11663 sentences). unlab1 was 
tokenized, POS and lemmas were added using our 
version of TreeTagger (Schmid, 1994), and lem-
mas replaced with stems, which had turned out to 
be more effective than lemmas. We call this set pchemtb_unlab1.conll. 
We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB col-
lection provided for CoNLL 2007. This consists of 
WSJ sections 02-11, half of the usual set 02-23, for 
a total of 460,000 tokens with dependencies gener-
ated with the converter by Johansson and Nugues 
(2007). 
We added stems and produced a parser called DeSRwsj. By parsing eng-lish_pchem_test.conll with DeSRwsj we 
obtained pchemtb_test_base.desr, our base-
line for the task. 
By visual inspection using DgAnnotator 
(DgAnnotator, 2006), the parses looked generally 
correct. Most of the errors seemed due to improper 
handling of conjunctions and disjunctions. The 
collection in fact contains several phrases like: 
Specific antibodies raised against 
P450IIB1 , P450 IA1 or IA2 , 
P450IIE1 , and P450IIIA2 inhibited 
the activation in liver microsomes 
from rats pretreated with PB , BNF , 
INH and DEX respectively 
The parser did not seem to have much of a problem 
with terminology, possibly because the supplied 
gold POS were adequate. 
For the adaptation we proceeded as follows. We 
parsed pchemtb_unlab1.conll using DeSRwsj 
obtaining pchemtb_unlab1.desr. 
We then extracted a set of 12,500 sentences 
from ptb_train.conll and 7,500 sentences 
from pchemtb_unlab1.desr, creating a corpus 
of 20,000 sentences called combined.conll. In 
both cases the selection criteria was to choose sen-
tences shorter than 30 tokens. 
We then trained a low accuracy parser (called DesrCombined) on combined.conll, by using 
a 1st-order averaged perceptron. DesrCombined 
was used to parse english_ptb_train.conll, 
the original training corpus for English. By com-
paring this parse with the original, one can detect 
where such parser makes mistakes. The rationale 
for using an inaccurate parser is to obtain parses 
with many errors so that they form a suitably large 
training set for the next step: parser revision. 
We then used a parsing revision technique (At-
tardi and Ciaramita, 2007) to learn how to correct 
these errors, producing a parse reviser called DesrReviser. The revision technique consists of 
comparing the parse trees produced by the parser 
with the gold standard parse trees, from the 
annotated corpus. Where a difference is noted, a 
1116
revision rule is determined to correct the mistake. 
Such rules consist in movements of a single link to 
a different head. Learning how to revise a parse 
tree consists in training a classifier on a set of 
training examples consisting of pairs ?(wi, d, wj), 
ti?, i.e. the link to be modified and the 
transformation rule to apply. Attardi and Ciaramita 
(2007) showed that 80% of the corrections can be 
typically dealt with just 20 tree revision rules. For 
the adaptation track we limited the training to 
errors recurring at least 20 times and to 30 rules. DesrReviser was then applied to pchemtb_test_base.desr producing pchemtb_test_rev.desr, our final submission. 
Many conjunction errors were corrected, in par-
ticular by moving the head of the sentence from a 
coordinate verb to the conjunction ?and? linking 
two coordinate phrases. 
The revision step produced an improvement of 
0.42% LAS over the score achieved by using just 
the base DeSRwsj parser. 
Table 2 reports the official accuracy scores on 
the closed adaptation track. DeSR achieved a close 
second best UAS on the ptchemtb test set and 
third best on CHILDES. The results are quite en-
couraging, particularly considering that the revi-
sion step does not yet correct the dependency 
labels and that our base English parser had a lower 
rank in the multilingual track. 
 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
CHILDES     61.37 58.67 57.89 
Pchemtb  81.06 80.40 73.03 83.42  83.08 76.42 
Table 2. Closed adaptation track scores. 
Notice that the adaptation process could be iter-
ated. Since the combination DeSRwsj+DesrReviser is a more accurate parser 
than DeSRwsj, we could use it again to parse pchemtb_unlab1.conll and so on. 
11 Conclusions 
For performing multilingual parsing in the CoNLL 
2007 shared task we employed DeSR, a classifier-
based Shift/Reduce parser. We used a second order 
averaged perceptron as classifier and achieved ac-
curacy scores quite above the average in all lan-
guages. For proper comparison with other 
approaches, one should take into account that the 
parser is incremental and deterministic; hence it is 
typically faster than other non linear algorithms. 
For the adaptation track we used a novel ap-
proach, based on the technique of tree revision, 
applied to a parser trained on a corpus combining 
sentences from both the training and the adaptation 
domain. The technique achieved quite promising 
results and it also offers the interesting possibility 
of being iterated, allowing the parser to incorporate 
language knowledge from additional domains. 
Since the technique is applicable to any parser, 
we plan to test it also with more accurate English 
parsers. 
Acknowledgments.  The following treebanks 
were used for training the parser: (Aduriz et al, 
2003; B?hmov? et al, 2003; Chen et al, 2003; Ha-
ji? et al, 2004; Marcus et al, 1993; Mart? et al, 
2002; Montemagni et al 2003; Oflazer et al, 2003; 
Prokopidis et al, 2005; Csendes et al, 2005). 
Ryan McDonald and Jason Baldridge made avail-
able mstparser and helped us using it. We grate-
fully acknowledge Hugo Zaragoza and Ricardo 
Baeza-Yates for supporting the first author during 
a sabbatical at Yahoo! Research Barcelona. 
References 
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer. 
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), 201?204. 
G. Attardi. 2006. Experiments with a Multilanguage 
non-projective dependency parser. In Proc. of the 
Tenth CoNLL, 2006. 
G. Attardi, M. Ciaramita. 2007. Tree Revision Learning 
for Dependency Parsing. In Proc. of NAACL/HLTC 
2007. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, 231?248. 
1117
M. Ciaramita, G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information. Proc. of the 12th International 
Workshop on Parsing Technologies (IWPT), 2007. 
K. Crammer, Y. Singer. 2003. Ultraconservative Online 
Algorithms for Multiclass Problems. Journ. of Ma-
chine Learning Research. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
DgAnnotator. 2006. 
http://medialab.di.unipi.it/Project/Parser/DgAnnotato
r/. 
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
R. McDonald, et al 2005. Non-projective Dependency 
Parsing using Spanning Tree Algorithms. In Proc. of 
HLT-EMNLP. 
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
189?210.  
Mstparser 0.4.3. 2007.  
http://sourceforge.net/projects/mstparser/ 
J. Nivre, et al 2004. Memory-based Dependency Pars-
ing. In Proc.s of the Eighth CoNLL, ed. H. T. Ng and 
E. Riloff, Boston, Massachusetts, 49?56. 
J. Nivre and J. Nilsson. 2005. Pseudo-Projective De-
pendency Parsing. In Proc. of the 43rd Annual Meet-
ing of the ACL, 99?106. 
J. Nivre and M. Scholz. 2004. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING 
2004, Geneva, Switzerland, 64?70. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the CoNLL 
2007 Shared Task. Joint Conf. on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, 261?277.  
M.F. Porter. 2001. Snowball Stemmer.  
http://www.snowball.tartarus.org/ 
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In Proc. of International Con-
ference on New Methods in Language Processing. 
V. N. Vapnik. 1998. The Statistical Learning Theory. 
Springer. 
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In 
Proc. of the 8th International Workshop on Parsing 
Technologies (IWPT), 195?206. 
 
1118
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 108?111,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TANL-1: Coreference Resolution by Parse Analysis and 
Similarity Clustering 
 
Giuseppe Attardi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
attardi@di.unipi.it 
Stefano Dei Rossi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
deirossi@di.unipi.it 
Maria Simi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
simi@di.unipi.it 
 
  
 
Abstract 
Our submission to the Semeval 2010 task 
on coreference resolution in multiple lan-
guages is based on parse analysis and si-
milarity clustering. The system uses a bi-
nary classifier, based on Maximum En-
tropy, to decide whether or not there is a 
relationship between each pair of men-
tions extracted from a textual document. 
Mention detection is based on the analy-
sis of the dependency parse tree. 
1 Overview 
Coreference resolution can be described as the 
problem of clustering noun phrases (NP), also 
called mentions, into sets referring to the same 
discourse entity.  
The ?Coreference Resolution in Multiple Lan-
guages task? at SemEval-2010 is meant to assess 
different machine learning techniques in a multi-
lingual context, and by means of different 
evaluation metrics. Two different scenarios are 
considered: a gold standard scenario (only avail-
able for Catalan and Spanish), where correct 
mention boundaries are provided to the partici-
pants, and a regular scenario, where mention 
boundaries are to be inferred from other linguis-
tic annotations provided in the input data. In par-
ticular the linguistic annotations provided for 
each token in a sentence are: position in sentence 
(ID), word (TOKEN), lemma and predicted 
lemma (LEMMA and PLEMMA), morpho-
syntactic information, both gold and/or predicted 
(POS and PPOS, FEAT and PFEAT), depend-
ency parsing annotations (HEAD and PHEAD, 
DEPREL and PDEPREL), named entities (NE 
and PNE), and semantic roles (PRED, PPRED, 
and corresponding roles in the following col-
umns). In the gold scenario, mention boundaries 
annotations (in column COREF) can also be used 
as input. 
Our approach to the task was to split corefer-
ence resolution into two sub-problems: mention 
identification and creation of entities. Mention 
recognition was based on the analysis of parse 
trees produced from input data, which were pro-
duced by manual annotation or state-of-the-art 
dependency parsers. Once the mentions are iden-
tified, coreference resolution involves partition-
ing them into subsets corresponding to the same 
entity. This problem is cast into the binary classi-
fication problem of deciding whether two given 
mentions are coreferent. A Maximum Entropy 
classifier is trained to predict how likely two 
mentions refer to the same entity. This is fol-
lowed by a greedy procedure whose purpose is to 
cluster mentions into entities.  
According to Ng (2005), most learning based 
coreference systems can be defined by four ele-
ments: the learning algorithm used to train the 
coreference classifier, the method of creating 
training instances for the learner, the feature set 
used to represent a training or test instance, and 
the clustering algorithm used to coordinate the 
coreference classification decisions. In the fol-
lowing we will detail our approach by making 
explicit the strategies used in each of above men-
tioned components. 
The data model used by our system is based 
on the concepts of entity and mention. The col-
lection of mentions referring to the same object 
in a document forms an entity. A mention is an 
instance referring to an object: it is represented 
by the start and end positions in a sentence, a 
type and a sequence number. For convenience it 
also contains a frequency count and a reference 
to the containing sentence. 
108
2 Mention detection 
The first stage of the coreference resolution 
process tries to identify the occurrence of men-
tions in documents. 
In the training phase mentions are obtained 
from the NE (or PNE) column of the corpus and 
are partitioned into entities using the information 
provided in the COREF column. 
In the regular setting, we used an algorithm for 
predicting boundaries that relies on the parse tree 
of the sentence produced from the gold annota-
tions in columns HEAD and DEP, if available, or 
else from columns PHEAD and PDEP, the out-
put of a dependency parser provided as input da-
ta.  
This analysis relied on minimal language 
knowledge, in order to determine possible heads 
of sub-trees counting as mentions, i.e. noun 
phrases or adverbial phrases referring to quanti-
ties, times and locations. POS tags and morpho-
logical features, when available, were mostly 
taken into account in determining mention heads. 
The leaves of the sub-trees of each detected head 
were collected as possible mentions. 
The mentions identified by the NE column 
were then added to this set, discarding duplicates 
or partial overlaps. Partial overlaps in principle 
should not occur, but were present occasionally 
in the data. When this occurred, we applied a 
strategy to split them into a pair of mentions.  
The same mention detection strategy was used 
also in the gold task, where we could have just 
returned the boundaries present in the data, scor-
ing 100% in accuracy. This explains the small 
loss in accuracy we achieved in mention identifi-
cation in the gold setting. 
Relying on parse trees turned out to be quite 
effective, especially for languages where gold 
parses where available. For some other languag-
es, the strategy was less effective. This was due 
to different annotation policies across different 
languages, and, in part, to inconsistencies in the 
data. For example in the Italian data set, named 
entities may include prepositions, which are typ-
ically the head of the noun phrase, while our 
strategy of looking for noun heads leaves the 
preposition out of the mention boundaries. 
Moreover this strategy obviously fails when 
mentions span across sentences as was the case, 
again, for Italian. 
3 Determining coreference 
For determining which mentions belong to the 
same entity, we applied a machine learning tech-
nique. We trained a Maximum Entropy classifier 
written in Python (Le, 2004) to determine 
whether two mentions refer to the same entity.  
We did do not make any effort to optimize the 
number of training instances for the pair-wise 
learner: a positive instance is created for each 
anaphoric NP, paired with each of its antecedents 
with the same number, and a negative instance is 
created by pairing each NP with each of its pre-
ceding non-coreferent noun phrases.  
The classifier is trained using the following 
features, extracted for each pair of mentions. 
Lexical features 
 Same: whether two mentions are equal; 
 Prefix: whether one mention is a prefix of 
the other;  
 Suffix: whether one mention is a suffix of 
the other; 
 Acronym: whether one mention is the 
acronym of the other. 
 Edit distance: quantized editing distance 
between two mentions. 
 Distance features 
 Sentence distance: quantized distance be-
tween the sentences containing the two 
mentions; 
 Token distance: quantized distance be-
tween the start tokens of the two mentions; 
 Mention distance: quantized number of 
other mentions between two mentions. 
Syntax features 
 Head: whether the heads of two mentions 
have the same POS; 
 Head POS: pairs of POS of the two men-
tions heads; 
Count features 
 Count: pairs of quantized numbers, each 
counting how many times a mention oc-
curs. 
Type features 
 Type: whether two mentions have the 
same associated NE (Named Entity) type. 
Pronoun features 
109
When the most recent mention is a pronominal 
anaphora, the following features are extracted: 
 Gender: pair of attributes {female, male or 
undetermined}; 
 Number: pair of attributes {singular, plur-
al, undetermined}; 
 Pronoun type: this feature is language de-
pendent and represents the type of prono-
minal mention, i.e. whether the pronoun is 
reflexive, possessive, relative, ? 
In the submitted run we used the GIS (Genera-
lized Iterative Scaling) algorithm for parameter 
estimation, with 600 iterations, which appeared 
to provide better results than using L-BFGS (a 
limited-memory algorithm for unconstrained op-
timization). Training times ranged from one 
minute for German to 8 minutes for Italian, 
hence the slower speed of GIS was not an issue. 
3.1 Entity creation 
The mentions detected in the first phase were 
clustered, according to the output of the classifi-
er, using a greedy clustering algorithm.  
Each mention is compared to all previous 
mentions, which are collected in a global men-
tions table. If the pair-wise classifier assigns a 
probability greater than a given threshold to the 
fact that a new mention belongs to a previously 
identified entity, it is assigned to that entity. In 
case more than one entity has a probability great-
er than the threshold, the mention is assigned to 
the one with highest probability. This strategy 
has been described as best-first clustering by Ng 
(2005). 
In principle the process is not optimal since, 
once a mention is assigned to an entity, it cannot 
be later assigned to another entity to which it 
more likely refers. Luo et al (2004) propose an 
approach based on the Bell tree to address this 
problem. Despite this potential limitation, our 
system performed quite well. 
4 Data preparation 
We used the data as supplied by the task organ-
izers for all languages except Italian. A modified 
version of the Hunpos tagger (Hal?csy, Kornai & 
Oravecz, 2007; Attardi et al, 2009) was used to 
add to the Italian training and development cor-
pora more accurate POS tags than those supplied, 
as well as missing information about morphol-
ogy. The POS tagger we used, in fact is capable 
of tagging sentences with detailed POS tags, 
which include morphological information; this 
was added to column PFEATS in the data. Just 
for this reason our submission for Italian is to be 
considered an open task submission. 
The Italian training corpus appears to contain 
several errors related to mention boundaries. In 
particular there are cases of entities starting in a 
sentence and ending in the following one. This 
appears to be due to sentence splitting (for in-
stance at semicolons) performed after named ent-
ities had been tagged. As explained in section 2, 
our system was not prepared to deal with these 
situations. 
Other errors in the annotations of entities oc-
curred in the Italian test data, in particular incor-
rect balancing of openings and closings named 
entities, which caused problems to our submis-
sion. We could only complete the run after the 
deadline, so we could only report unofficial re-
sults for Italian. 
5 Results 
We submitted results to the gold and regular 
challenges for the following languages: Catalan, 
English, German and Spanish. 
Table 1 summarizes the performance of our 
system, according to the different accuracy 
scores for the gold task, Table 2 for the regular 
task. We have outlined in bold the cases where 
we achieved the best scores among the partici-
pating systems. 
 
 Mention CEAF MUC B
3 BLANC 
Catalan 98.4 64.9 26.5 76.2 54.4 
German 100 77.7 25.9 85.9 57.4 
English 89.8 67.6 24.0 73.4 52.1 
Spanish 98.4 65.8 25.7 76.8 54.1 
Table 1. Gold task, Accuracy scores. 
 
 Mention CEAF MUC B
3 BLANC 
Catalan 82.7 57.1 22.9 64.6 51.0 
German 59.2 49.5 15.4 50.7 44.7 
English 73.9 57.3 24.6 61.3 49.3 
Spanish 83.1 59.3 21.7 66.0 51.4 
Table 2. Regular task. Accuracy scores. 
6 Error analysis 
We performed some preliminary error analysis. 
The goal was to identify systematic errors and 
possible corrections for improving the perfor-
mance of our system. 
We limited our analysis to the mention boun-
daries detection for the regular tasks. A similar 
110
analysis for coreference detection, would require 
the availability of gold test data.  
7 Mention detection errors 
As described above, the strategy used for the ex-
traction of mentions boundaries is based on de-
pendency parse trees and named entities. This 
proved to be a good strategy in some languages 
such as Catalan (F1 score: 82.7) and Spanish (F1 
score: 83.1) in which the dependency data avail-
able in the corpora were very accurate and con-
sistent with the annotation of named entities. In-
stead, there have been unexpected problems in 
other languages like English or German, where 
the dependencies information were annotated 
using a different approach. 
For German, while we achieved the best B3 
accuracy on coreference analysis in the gold set-
tings, we had a quite low accuracy in mention 
detection (F1: 59.2), which was responsible of a 
significant drop in coreference accuracy for the 
regular task. This degradation in performance 
was mainly due to punctuations, which in Ger-
man are linked to the sub-tree containing the 
noun phrase rather than to the root of the sen-
tence or tokens outside the noun phrase, as it 
happens in Catalan and Spanish. This misled our 
mention detection algorithm to create many men-
tions with wrong boundaries, just because punc-
tuation marks were included. 
In the English corpus different conventions 
were apparently used for dependency parsing and 
named entity annotations (Table 3), which pro-
duced discrepancies between the boundaries of 
the named entities present in the data and those 
predicted by our algorithm. This in turn affected 
negatively the coreference detection algorithm 
that uses both types of information. 
 
ID TOKEN HEAD DEPREL NE COREF 
1 Defense 2 NAME (org) (25 
2 Secretary 4 NMOD _ _ 
3 William 4 NAME (person _ 
4 Cohen 5 SBJ person) 25) 
Table 3. Example of different conventions for NE and 
COREF in the English corpus. 
 
Error analysis also has shown that further im-
provements could be obtained, for all languages, 
by using more accurate language specific extrac-
tion rules. For example, we missed to consider a 
number of specific POS tags as possible identifi-
ers for the head of noun phrases. By some simple 
tuning of the algorithm we obtained some im-
provements. 
8 Conclusions 
We reported our experiments on coreference res-
olution in multiple languages. We applied an ap-
proach based on analyzing the parse trees in or-
der to detect mention boundaries and a Maxi-
mum Entropy classifier to cluster mentions into 
entities. 
Despite a very simplistic approach, the results 
were satisfactory and further improvements are 
possible by tuning the parameters of the algo-
rithms. 
References 
G. Attardi et al, 2009. Tanl (Text Analytics and Natu-
ral Language Processing). SemaWiki project: 
http://medialab.di.unipi.it/wiki/SemaWiki. 
P. Hal?csy, A. Kornai, and C. Oravecz, 2007. Hun-
Pos: an open source trigram tagger. Proceedings of 
the ACL 2007, Prague. 
Z. Le, Maximum Entropy Modeling Toolkit for Pytho 
and C++, Reference Manual. 
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla & S. 
Roukos. 2004. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell Tree. 
Proceedings of the ACL 2004, Barcelona. 
V. Ng, Machine Learning for Coreference Resolution: 
From Local Classification to Global Ranking, Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Ann 
Arbor, MI, June 2005, pp. 157-164. 
M. Recasens, L. M?rquez,  E. Sapena, M. A. Mart?,  
M. Taul?, V. Hoste, M. Poesio and Y. Versley, 
SemEval-2010 Task 1: Coreference resolution in 
multiple languages, in Proceedings of the 5th In-
ternational Workshop on Semantic Evaluations 
(SemEval-2010), Uppsala, Sweden, 2010. 
 
 
111
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 61?69,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Converting Italian Treebanks:
Towards an Italian Stanford Dependency Treebank
Cristina Bosco
Dipartimento di Informatica
Universita` di Torino
cristina.bosco@unito.it
Simonetta Montemagni
Istituto di Linguistica
Computazionale
?Antonio Zampolli?
(ILC?CNR)
simonetta.montemagni@ilc.cnr.it
Maria Simi
Dipartimento di Informatica
Universita` di Pisa
simi@unipi.it
Abstract
The paper addresses the challenge of con-
verting MIDT, an existing dependency?
based Italian treebank resulting from the
harmonization and merging of smaller re-
sources, into the Stanford Dependencies
annotation formalism, with the final aim
of constructing a standard?compliant re-
source for the Italian language. Achieved
results include a methodology for con-
verting treebank annotations belonging
to the same dependency?based family,
the Italian Stanford Dependency Treebank
(ISDT), and an Italian localization of the
Stanford Dependency scheme.
1 Introduction
The limited availability of training resources is
a widely acknowledged bottleneck for machine
learning approaches for Natural Language Pro-
cessing (NLP). This is also the case of dependency
treebanks within statistical dependency parsing.
Moreover, the availability of a treebank in a stan-
dard format strongly improves its usefulness, in-
creasing the number of tasks for which it can be
exploited and allowing the application of a larger
variety of tools. It also has an impact on the relia-
bility of achieved results, and, last but not least, it
permits comparability with other resources.
This motivated a variety of initiatives devoted
to the definition of standards for the linguistic an-
notation of corpora. Since the early 1990s, dif-
ferent initiatives have been devoted to the defi-
nition of standards for the linguistic annotation
of corpora with a specific view to re?using and
merging existing treebanks. The starting point
is represented by the EAGLES (Expert Advisory
Groups on Language Engineering Standards) ini-
tiative, which ended up with providing provisional
standard guidelines (Leech et al, 1996), operat-
ing at the level of both content (i.e. the linguistic
categories) and encoding format. More recent ini-
tiatives, e.g. LAF/GrAF (Ide and Romary, 2006;
Ide and Suderman, 2007) and SynAF (Declerck,
2008) representing on?going ISO TC37/SC4 stan-
dardization activities1, rather focused on the def-
inition of a pivot format capable of representing
diverse annotation types of varying complexity
without providing specifications for the annotation
of content categories (i.e., the labels describing the
associated linguistic phenomena), for which stan-
dardization appeared since the beginning to be a
much trickier matter. Recently, other standard-
ization efforts such as ISOCat (Kemps-Snijders et
al., 2009) tackled this latter issue by providing a
set of data categories at various levels of granu-
larity, each accompanied by a precise definition of
its linguistic meaning. Unfortunately, the set of
dependency categories within ISOCat is still basic
and restricted. We can thus conclude that as far as
content categories are concerned de jure standards
are not suitable at the moment for being used in
the harmonization and merging of real dependency
treebanks.
The alternative to de jure standards is repre-
sented by de facto standards. For what concerns
dependency?based annotation, which in the recent
past has been increasingly exploited for a wide
range of NLP?based information extraction tasks,
the Stanford Dependency (SD) scheme (de Marn-
effe et al, 2006) is gaining popularity as a de
facto standard. Among the contexts where SD has
been applied, we can observe e.g. parsers and
corpora exploited in biomedical information ex-
traction, where it has been suggested to be a suit-
able unifying syntax formalism for several incom-
patible syntactic annotation schemes (Pyysalo et
al., 2007). SD has already been applied to differ-
ent languages, e.g. Finnish in the Turku treebank
(Haverinen et al, 2010), Swedish in the Talbanken
1
http://www.tc37sc4.org/
61
treebank2, Chinese in the Classical Chinese Liter-
ature treebank (Seraji et al, 2012) or Persian in the
Uppsala Persian Dependency Treebank (Lee and
Kong, 2012).
In this paper, we describe the conversion of
an existing Italian resource into the SD annota-
tion scheme, with the final aim of developing a
standard?compliant treebank, the Italian Stanford
Dependency Treebank (ISDT). The reference re-
source, called Merged Italian Dependency Tree-
bank (MIDT)3 (Bosco et al, 2012), is the re-
sult of a previous effort in the direction of im-
proving interoperability of data sets available for
Italian by harmonizing and merging two exist-
ing dependency?based resources, i.e. TUT and
ISST?TANL, adopting incompatible annotation
schemes. The two conversion steps are visual-
ized in Figure 1: note that in both of them the
focus is on the conversion and merging of the con-
tent of linguistic annotation; for what concerns the
representation format, all involved treebanks fol-
low the CoNLL tab?separated format (Buchholz
and Marsi, 2006) which nowadays represents a de
facto standard within the international dependency
parsing community. In this paper, we deal with the
second step, focusing on the MIDT to ISDT con-
version.
Starting from a comparative analysis of the
MIDT and SD annotation schemes, we developed
a methodology for converting treebank annota-
tions belonging to the same dependency?based
family based on:
? a comparative analysis of the source and tar-
get annotation schemes, carried out with re-
spect to different dimensions of variation,
ranging from head selection criteria, depen-
dency tagset granularity to defined annotation
criteria;
? the analysis of the performance of a state?of?
the?art dependency parser by using as train-
ing the source and the target treebanks;
? the mapping of the MIDT annotation scheme
onto the SD data categories.
2
http://stp.lingfil.uu.se/?nivre/swedish treebank/
talbanken-stanford-1.2.tar.gz
3MIDT was developed within the project PARLI
(http://parli.di.unito.it/project en.html) partially
funded in 2008-2012 by the Italian Ministry for Univer-
sity and Research, for fostering the development of new
resources and tools that can operate together, and the
harmonization of existing ones. MIDT is documented at
http://medialab.di.unipi.it/wiki/MIDT/.
Figure 1: Merging and conversion process from
TUT and ISST?TANL to MIDT and ISDT.
In this conversion process, we had to deal
with the peculiarities of the Italian language: the
tackled issues range from morphological richness,
presence of clitic pronouns to relatively free word
order and pro?drop, all properties requiring spe-
cific annotation strategies to be dealt with. There-
fore, a by product of this conversion process is rep-
resented by the specialization of the SD annotation
scheme with respect to Italian.
In the following sections, after briefly describ-
ing the methodology applied for the development
of the MIDT resource (Section 2), we focus on a
comparative analysis of the MIDT and SD anno-
tation schemes (Section 3) followed by a descrip-
tion of the implemented conversion process (Sec-
tion 4). Finally, we present the results obtained by
training a parsing system on the newly developed
resource (Section 5).
2 The starting point: MIDT
ISDT originates from the conversion towards the
SD standard of the MIDT resource, whose origins
and development are summarised below (for more
details on this harmonization and merging step the
interested reader is referred to Bosco et al (2012)).
2.1 The ancestors: TUT and ISST?TANL
The TUT and ISST?TANL resources differ under
different respects, at the level of both corpus com-
position and adopted annotation schemes.
For what concerns size and composition, TUT
(Bosco et al, 2000)4 currently includes 3,452 Ital-
ian sentences (i.e. 102,150 tokens in TUT native,
4
http://www.di.unito.it/?tutreeb/
62
and 93,987 in CoNLL) and represents five dif-
ferent text genres (newspapers, Italian Civil Law
Code, JRC-Acquis Corpus5, Wikipedia and the
Costituzione Italiana), while ISST?TANL includes
3,109 sentences (71,285 tokens in CoNLL for-
mat), which were extracted from the ?balanced?
ISST partition (Montemagni et al, 2003) exem-
plifying general language usage as testified in arti-
cles from newspapers and periodicals, selected to
cover a high variety of topics (politics, economy,
culture, science, health, sport, leisure, etc.).
As far as the annotation scheme is concerned,
TUT applies the major principles of the Word
Grammar theoretical framework (Hudson, 1984)
using a rich set of dependency relations, but it in-
cludes null elements to deal with non?projective
structures, long distance dependencies, equi phe-
nomena, pro?drop and elliptical structures6. The
ISST?TANL annotation scheme originates from
FAME (Lenci et al, 2008), an annotation scheme
which was developed starting from de facto stan-
dards and which was specifically conceived for
complying with the basic requirements of parsing
evaluation, and ? later ? for the annotation of un-
restricted Italian texts.
2.2 Creating the merged MIDT resource
The challenge we tackled in the development of
MIDT was to translate between different annota-
tion schemes and merging them. We focused on
the harmonization and merging of content cate-
gories. To this specific end, we defined a set of
linguistic categories to be used as a ?bridge? be-
tween the specific TUT and ISST?TANL schemes.
First of all, we analyzed similarities and dif-
ferences of the underlying schemes, which led to
identify a core of syntactic constructions for which
the annotations agreed, but also to highlight vari-
ations in head selection criteria, inventory of de-
pendency types and their linguistic interpretation,
projectivity constraint and analysis of specific syn-
tactic constructions. For instance, TUT always
assigns heads on the basis of syntactic criteria,
i.e. the head role is played by the function word
in all constructions where one function word and
one content word are involved (e.g. determiner?
noun, verb?auxiliary), while in ISST?TANL head
selection follows from a combination of syntactic
5
http://langtech.jrc.it/JRC-Acquis.html
6The CoNLL format does not include null elements, but
the projectivity constraint is maintained at the cost of a loss
of information with respect to native TUT in some cases.
and semantic criteria (e.g. in determiner?noun and
auxiliary?verb relations the head role is played by
the content word). Both schemes assume differ-
ent inventories of dependency types and degrees
of granularity in the representation of specific re-
lations. Moreover, whereas ISST?TANL allows
for non?projective representations, TUT assumes
the projectivity constraint. Further differences are
concerned with the treatment of coordination and
punctuation, which are particularly problematic to
deal with in the dependency framework.
As a second step, we defined a bridge anno-
tation, i.e. the MIDT dependency tagset, fol-
lowing practical considerations: bridge categories
should be automatically reconstructed by exploit-
ing morpho?syntactic and dependency informa-
tion contained in the original resources; for some
constructions, the MIDT representation is parame-
terizable, i.e. the tagset provides two different op-
tions, corresponding to the TUT and ISST?TANL
annotation styles (e.g. for determiner?noun or
preposition?noun relations).
The final MIDT tagset contains 21 dependency
tags (as opposed to the 72 tags of TUT and the
29 of ISST?TANL), including the different op-
tions provided for the same type of construction.
CoNLL is used as encoding format.
3 Comparing the MIDT and SD schemes
The MIDT and SD annotation schemes are both
dependency?based and therefore fall within the
same broader family. This fact, however, does
not guarantee per se an easy and linear conver-
sion process from one to the other: as pointed out
in Bosco et al (2012), harmonizing and convert-
ing annotation schemes can be quite a challenging
task, even when this process is carried out within
a same paradigm and with respect to the same lan-
guage. In the case at hand, this task is made easier
thanks to the fact that the MIDT and SD schemes
share similar design principles: for instance, in
both cases preference is given a) to relations which
are semantically contentful and useful to appli-
cations, or b) to relations linking content words
rather than being indirectly mediated via function
words (see design principles 2 and 5 respectively
in de Marneffe and Manning (2008a)). Another
peculiarity shared by MIDT and SD consists in the
fact that they both neutralize the argument/adjunct
distinction for what concerns prepositional com-
plements, which is taken to be ?largely useless
63
in practice? as de Marneffe and Manning (2008a)
claim. In spite of their sharing similar design prin-
ciples, there are also important differences con-
cerning the inventory of dependency types and
their linguistic interpretation, the head selection
criteria as well as the treatment of specific syn-
tactic constructions. In what follows, we summa-
rize the main dimensions of variation between the
MIDT and SD annotation schemes, with a specific
view to the conversion issues they arise.
3.1 Granularity and inventory of dependency
types
MIDT and SD annotation schemes assume differ-
ent inventories of dependency types characterized
by different degrees of granularity in the repre-
sentation of specific relations: the adopted depen-
dency tagset includes 21 dependency types in the
case of MIDT and 48 in the case of SD. Interest-
ingly however, it is not always the case that the
finer grained annotation scheme ? i.e. SD ? is the
one providing more granular distinctions: whereas
this is typically the case, there are also cases in
which more granular distinction are adopted in the
MIDT annotation scheme.
Consider first SD relational distinctions which
are neutralized at the level of the MIDT annota-
tion. As reported in de Marneffe and Manning
(2008a), so?called NP?internal relations are crit-
ical in real world applications: the SD scheme
therefore includes many relations of this kind,
e.g. appos (appositive modifier), nn (noun com-
pound), num (numeric modifier), number (ele-
ment of compound number) and abbrev (abbre-
viation). In MIDT all these relation types are
lumped together under the general heading of mod
(modifier). To deal with these cases, the MIDT to
SD conversion has to simultaneously combine de-
pendency and morpho?syntactic information (e.g.
the morpho?syntactic category of the nodes in-
volved in the relation), which however is not al-
ways sufficient as in the case of appositive modi-
fiers for which further evidence is needed.
Let us consider now the reverse case, i.e. in
which MIDT adopts finer?grained distinctions
with respect to SD. For instance, MIDT envis-
ages different relation types for auxiliary?verb and
preposition?verb (within infinitive clauses, be they
modifiers or subcategorized arguments) construc-
tions, which are aux and prep respectively. By
contrast, SD represents both cases in terms of the
same relation type, i.e. aux. Significant differ-
ences between English and Italian justify the dif-
ferent strategies adopted in SD and MIDT respec-
tively: in English, open clausal complements are
always introduced by the particle ?to?, whereas in
Italian different prepositions can introduce them
(i.e. ?a?, ?di?, ?da?), which are selected by the gov-
erning head. The SD representation of the element
introducing infinitival complements and modifiers
in terms of aux might not be appropriate as far as
Italian is concerned and it would be preferable to
have a specific relation for dealing with introduc-
ers of infinitival complements (like complm in the
case of finite clausal complements): as reported
in Section 4, we are currently evaluating different
representational options with a specific view to the
syntactic peculiarities of the Italian language.
Another interesting and more complex exam-
ple can be found for what concerns the parti-
tioning of the space of sentential complements.
MIDT distinguishes between mod(ifiers) on the
one hand and subcategorised arg(uments) on the
other hand: note that whereas arg is restricted
to clausal complements subcategorized for by the
governing head, the mod relation covers different
types of modifiers (nominal, adjectival, clausal,
adverbial, etc.). By contrast, SD resorts to spe-
cific relations for dealing with sentential comple-
ments: in particular, distinct relation types are en-
visaged depending on e.g. whether the clause is
a subcategorized complement or a modifier (see
e.g. ccomp vs advcl), or whether the gov-
ernor is a verb or a noun (see e.g. xcomp vs
infmod), or whether the clausal complement is
headed by a finite or non?finite verb (see e.g.
ccomp vs xcomp). Starting from MIDT, the
finer?grained distinctions adopted by SD for deal-
ing with clausal complements can be recovered by
combining dependency information with morpho-
syntactic one (e.g. the mood of the verbal head of
the clausal complements or the morpho?syntactic
category of the governing head).
3.2 Head selection
Criteria for distinguishing the head and the de-
pendent within relations have been widely dis-
cussed in the linguistic literature in all frameworks
where the notion of syntactic head plays an im-
portant role. Unfortunately, different criteria have
been proposed, some syntactic and some seman-
tic, which do not lead to a single coherent notion
64
of dependency (Ku?bler et al, 2009). Head se-
lection thus represents an important and unavoid-
able dimension of variation among dependency
annotation schemes, especially for what con-
cerns constructions involving grammatical func-
tion words. MIDT and SD agree on the treat-
ment of tricky cases such as the determiner?noun
relation within nominal groups, the preposition?
noun relation within prepositional phrases as well
as the auxiliary?main verb relation in complex
verbal groups. In both schemes, head selection
follows from a combination of syntactic and se-
mantic criteria: i.e. whereas in the determiner?
noun and auxiliary?verb constructions the head
role is assigned to the semantic head (noun/verb),
in preposition?noun constructions the head role is
played by the element which is subcategorized for
by the governing head, i.e. the preposition which
is the syntactic head but can also be seen as as a
kind of role marker. In this area, the only but not
negligible difference is concerned with subordi-
nate clauses whose head in SD is assumed to be the
verb, rather than the introducing element (whether
a preposition or a subordinating conjunction) as in
MIDT: in this case, the MIDT to SD conversion
requires restructuring of the dependency tree.
3.3 Coordination and punctuation
In both MIDT and SD schemes, coordinate con-
structions are considered as asymmetric structures
with a main difference: while in MIDT both
the conjunction and conjuncts starting from the
second one are linked to the immediately pre-
ceding conjunct, in SD the conjunction(s) and
the subsequent conjunct(s) are all linked to the
first one. Also the treatment of punctuation is
quite problematic in the framework of a depen-
dency annotation scheme, although this has not
been specifically dealt with in the linguistic liter-
ature. Whereas MIDT has its own linguistically?
motivated strategy to deal with punctuation, SD
does not appear to provide explicit and detailed
annotation guidelines in this respect.
3.4 MIDT? or SD?only relations
It is not always the case that a dependency type
belonging to the MIDT or SD annotation scheme
has a counterpart in the other. Let us start from SD
relation types which are not explicitly encoded in
the MIDT source annotation, due to constraints of
the CoNLL representation format. This is the case
of the ref dependency linking the relative word
introducing the relative clause and its antecedent,
or of the xsubj relation which in spite of its being
part of the original TUT and ISST resources have
been omitted from the most recent and CoNLL?
compliant versions, which represent the starting
point of in MIDT: in both cases, the ?one head
per dependent? constraint of the CoNLL repre-
sentation format is violated. From this, it fol-
lows that ISDT won?t include these dependency
types. Other SD relations which were part of the
MIDT?s ancestors but were neutralized in MIDT
are concerned with semantically?oriented distinc-
tions which turned out to be problematic to be
reliably identified in parsing in spite of their be-
ing explicitly encoded in both source annotation
schemes (Bosco et al, 2012). This is the case of
the indirect object relation (iobj) or of temporal
modifiers (tmod).
The MIDT relation types which instead do not
have a corresponding relation in SD are those
that typically represent Italian?specific peculiari-
ties. This is the case of the clit(ic) dependency,
linking clitic pronouns to the verbal head they re-
fer to. In MIDT, whenever appropriate clitic pro-
nouns are assigned a label that reflects their gram-
matical function (e.g. ?dobj? or ?iobj?): this is the
case of reflexive constructions (Maria si lava lit.
?Maria her washes? meaning that ?Maria washes
herself?) or of complements overtly realized as
clitic pronouns (Giovanni mi ha dato un libro lit.
?Giovanni to?me has given a book? meaning that
?Giovanni gave me a book?). With pronominal
verbs, in which the clitic can be seen as part of
the verbal inflection, a specific dependency rela-
tion (clit) is resorted to link the clitic pronoun
to the verbal head: for instance, in a sentence like
la sedia si e` rotta lit. ?the chair it is broken? mean-
ing that ?the chair broke?, the dependency linking
the clitic si to the verbal head is clit.
4 The MIDT to SD conversion
The conversion process followed to generate the
Italian Stanford Dependency Treebank (ISDT)
starting from MIDT is based on the results of the
comparative analysis reported in the previous sec-
tion. It is organized in two different steps: the
first one aimed at generating an enriched version
of the MIDT resource, henceforth referred to as
MIDT++, including SD?relevant distinctions neu-
tralized in MIDT, and the second one in charge
of converting the MIDT++ annotation in terms
65
of the Stanford Dependencies as described in de
Marneffe and Manning (2008b) specialized with
respect to the Italian language syntactic peculiar-
ities. Note that also the resulting ISDT resource
adheres to the CoNLL tabular format.
The first step relied on previous harmonization
work leading to the construction of the MIDT re-
source starting from the CoNLL?compliant TUT
and ISST?TANL treebanks (described in Bosco
et al (2012)). During this step, we recovered
from the native resources relevant distinctions
that have been neutralized in MIDT, because of
choices made in the design of the MIDT anno-
tation scheme (e.g. indirect objects or temporal
modifiers which are assigned an underspecified
representation in MIDT, see Section 3) or simply
because the harmonization of the source annota-
tion schemes was not possible without manual re-
vision (this is the case of appositions, explicitly
annotated only in TUT).
Other issues tackled during this first pre?
processing step include the treatment of coordi-
nation and multi?word expressions. Since in SD
conjunctions and conjuncts, after the first one, are
all linked to the first conjunct, exactly as it was
in ISST?TANL, the intermediate MIDT++ is gen-
erated according to this scheme, with no conver-
sion for ISST?TANL and by restructuring the dif-
ferent cascading coordination style of TUT. For
what concerns multi?word expressions, we unified
the multi?word repertoires of the two resources.
Another area that required some pre?processing
with manual revision is concerned with the anno-
tation of the parataxis relation. The augmented re-
source resulting from this pre?processing step, i.e.
MIDT++, is used as a ?bridge? towards the SD
representation format.
Starting from the results of the comparative
analysis detailed in Section 3, we defined conver-
sion patterns which can be grouped into two main
classes according to whether they refer to individ-
ual dependencies (case A) or they involve depen-
dency subtrees due to head reassignment (case B).
A) Structure?preserving mapping rules involv-
ing dependency retyping without restructur-
ing of the tree:
A.1) 1:1 mapping requiring dependency retyp-
ing only (e.g. MIDT prep > SD pobj, or
MIDT subj > SD nsubj);
A.2) 1:n mapping requiring finer?grained de-
pendency retyping (e.g. MIDT mod > SD
abbrev | amod | appos | nn | nnp |
npadvmod | num | number | partmod |
poss | preconj | predet | purplcl |
quantmod | tmod);
B) Tree restructuring mapping rules involving
head reassignment and dependency retyping.
Focusing on dependency retyping we distin-
guish the following cases:
B.1) head reassignment with 1:1 dependency
mapping (e.g. MIDT subj > SD csubj
in the case of clausal subjects);
B.2) head reassignment with 1:n dependency
mapping based on finer?grained distinctions
(e.g. MIDT arg> SD xcomp? ccomp, or
MIDT mod (with verbal head) > SD advcl
| infmod | prepc | purpcl).
In what follows, we will exemplify how the ab-
stract patterns described above have been trans-
lated into MIDT to SD conversion rules. The
conversion of the MIDT arg relation, referring
to clausal complements subcategorized for by the
governing head, represents an interesting example
of 1:n dependency mapping with tree restructuring
(case B.2 above). In MIDT, clausal complements,
either finite or non?finite clauses, are linked to the
governing head (which can be a verb, a noun or an
adjective) as arg(uments), with a main difference
with respect to SD, i.e. that the head of the clausal
complement is the word introducing it (be it a
preposition or a subordinating conjunction) rather
than the verb of the clausal complement. The main
conversion rules to SD can be summarised as fol-
lows, where the? separates the left from the right
hand side of the rule, the notation x ?dep label y
denotes that token y is governed by token x with
the dependency label specifying the relation hold-
ing between the two (a MIDT tag is found on the
left side of the rule, whereas an SD one occurs on
the right side):
1. $1[S|V |A] ?arg $2[E] ?prep $3[Vinfinitive] ?
$1 ?xcomp $3; $3 ?aux $2
2. $1[S|V |A] ?arg $2[CS] ?sub $3[Vfinite] ?
$1 ?ccomp $3; $3 ?complm $2
In the rules, the $ followed by a number is a vari-
able indentifying a given dependency node. Con-
straints on tokens in the left?hand side of the rule
66
(a) MIDT representation (b) SD representation
Figure 2: MIDT vs SD annotation of the same sentence
are reported within square brackets: they are typi-
cally concerned with the grammatical category of
the token (CS stands for subordinative conjunc-
tion, E for preposition, S for noun, V for verb).
Rule 1 above handles the transformation of the in-
finitival clause from the MIDT representation to
SD. Consider as an example the MIDT depen-
dency tree in Figure 2(a) for the sentence Gio-
vanni ha dichiarato ai giudici di avere pagato i
terroristi, lit. ?Giovanni told to?the judges to have
paid the terrorists? ?Giovanni told the judges that
he has paid the terrorists? whose SD conversion is
reported in Figure 2(b). By comparing the trees,
we see that head restructuring and dependency re-
typing have both been performed in the conversion
of the infinitival clause representation: in MIDT
the head of the infinitival clause is the preposition
whereas in SD it is the verb; the relation linking
the governing head and the head of the infinitival
clause is arg in MIDT and xcomp in SD.
Currently, the conversion script implements
over 100 rules which are still being tested with the
final aim of finding the most appropriate represen-
tation with respect to the Italian syntactic pecu-
liarities. The problematic area of sentential com-
plements is still being explored to find out ade-
quate representational solutions. Consider as an
example the case of the word introducing infiniti-
val complements: Figure 2(b) above, reporting the
result of the SD conversion, shows that the same
aux relation is used to link the preposition to the
verb heading the infinitival complement as well as
the auxiliary avere ?to have? to the main verb. This
solution might not be so appropriate given the pe-
culiarities of the Italian language, where different
prepositions (lexically selected by the governing
head) can introduce infinitival complements.
During the conversion step, the SD scheme
has been specialized with respect to the Italian
language. There are SD dependency relations
which were excluded from the Italian localization
of the standard scheme, either because not ap-
propriate given the syntactic peculiarities of this
language (this is the case e.g. of the prt re-
lation) or because they could not be recovered
from the CoNLL?compliant versions of the re-
sources we started from (see e.g. the relations
ref or xsubj). The SD tagset was also extended
with new dependency types: this is the case of
the clit relation used for dealing with clitics in
pronominal verbs, or of the nnp relation specifi-
cally defined for compound proper nouns. Other
specializations are concerned with the use of un-
derspecified categories: rather than resorting to the
most generic relation, i.e. dep used when it is im-
possible to determine a more precise dependency
relation, we exploited the hierarchical organiza-
tion of SD typed dependencies, i.e. we used the
comp and mod relations when we could not find
an appropriate relation within the set of their de-
pendency subtypes.
5 Using ISDT as training corpus
In this section, we report the results achieved
by using ISDT for training a dependency parser,
namely DeSR (Dependency Shift Reduce), a
transition?based statistical parser (Attardi, 2006),
where it is possible to specify, through a config-
uration file, the set of features to use (e.g. POS
tag, lemma, morphological features) and the clas-
sification algorithm (e.g. Multi-Layer Perceptron
(Attardi and Dell?Orletta, 2009), Support Vector
Machine, Maximum Entropy). DeSR has been
trained on TUT and ISST?TANL in the frame-
work of the evaluation campaigns Evalita, for
the last time in 2011 (Bosco and Mazzei, 2012;
Dell?Orletta et al, 2012). More recently DeSR has
been trained and tested on MIDT: the results ob-
67
Table 1: Parsing results with ISDT resources
TRAINING TEST PARSER LAS LAS no punct
TUT?SDT train TUT?SDT test DeSR MLP 84.14% 85.57%
ISST?TANL?SDT train ISST?TANL?SDT test DeSR MLP 80.55% 82.11%
TUT+ISST?TANL?SDT train TUT+ISST?TANL?SDT test DeSR MLP 83.34% 84.16%
TUT+ISST?TANL?SDT train TUT?SDT test DeSR MLP 84.14% 85.79%
TUT+ISST?TANL?SDT train ISST?TANL?SDT test DeSR MLP 79.94% 81.86%
tained on both the MIDT version of the individual
TUT and ISST?TANL resources and the merged
resource are reported in (Bosco et al, 2012): the
best scores, achieved applying a parser combina-
tion strategy and training on TUT inMIDT format,
are LAS 90.11% and LAS 91.58% without punc-
tuation.
For the experiments on the ISDT resource we
used a basic and fast variant of the DeSR parser
based on Multi-Layer Perceptron (MLP). In fact,
the purpose of the experiment was not to optimize
the parser for the new resource but to compare
relative performances of the same parser on dif-
ferent versions of the same resources. As a re-
sult, the substantial drop in performance observed
with respect to the MIDT resource is in part due to
this factor, and cannot be totally attributed to the
greater complexity of the SD scheme or quality of
the conversion output.
Table 1 reports, in the first two rows, the val-
ues of Labeled Attachment Score (LAS, with and
without punctuation) obtained against the TUT?
ISDT and ISST?TANL?ISDT datasets. The differ-
ent performance of the parser on the two converted
datasets (TUT?ISDT and ISST?TANL?ISDT) is in
line with what was observed in previous exper-
iments with native resources and MIDT (Bosco
et al, 2010; Bosco et al, 2012); therefore, the
composition of the training and test corpora can
still be identified as possible causes for such a dif-
ference. The results reported in rows 3?5 have
been obtained by training DeSR with the larger
resource including both TUT?ISDT and ISST?
TANL?ISDT. As test set, we used a combination
of the two test sets (row 3) and test sets from the
two data sets separately (rows 4 and 5). The pre-
liminary results achieved by using ISDT are en-
couraging, in line with what was obtained on the
WSJ for English and reported in (Cer et al, 2010),
where the best results in labeled attachment preci-
sion, achieved by a fast dependency parser (Nivre
Eager feature Extract), is 81.7. For the time being,
training with the larger combined resource does
not seem to provide a substantial advantage, con-
firming results obtained with MIDT, despite the
fact that in the conversion from MIDT to ISDT
a substantial effort was spent to further harmonize
the two resources.
6 Conclusion
In this paper, we addressed the challenge of con-
verting MIDT, an existing dependency?based Ital-
ian treebank resulting from the harmonization and
merging of smaller resources adopting incompati-
ble annotation schemes, into the Stanford Depen-
dencies annotation formalism, with the final aim
of constructing a standard?compliant resource for
the Italian language. SD, increasingly acknowl-
edged within the international NLP community as
a de facto standard, was selected for its being de-
fined with a specific view to supporting informa-
tion extraction tasks.
The outcome of this still ongoing effort is three?
fold. Starting from a comparative analysis of
the MIDT and SD annotation schemes, we devel-
oped a methodology for converting treebank anno-
tations belonging to the same dependency?based
family. Second, Italian has now a new standard?
compliant treebank, i.e. the Italian Stanford De-
pendency Treebank (ISDT, 200,516 tokens)7: we
believe that this conversion will significantly im-
prove the usability of the resource. Third, but not
least important, we specialized the Stanford De-
pendency annotation scheme to deal with the pe-
culiarities of the Italian language.
7 Acknowledgements
This research was supported by a Google ?gift?.
Giuseppe Attardi helped with the experiments
with the DeSR parser, Roberta Montefusco pro-
duced the converter to the collapsed/propagated
version of ISDT and in so doing helped us to re-
duce inconsistencies and errors in the resource.
7Both the MIDT and ISDT resources are released by
the authors under the Creative Commons Attribution?
NonCommercial-ShareAlike 3.0 Unported licence
(http://creativecommons.org/licenses/by-nc-sa/3.0/
legalcode.txt).
68
References
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proceedings of of NAACL HLT (2009).
G. Attardi. 2006. Experiments with a multilanguage
non?projective dependency parser. In Proceedings
of the CoNLL-X ?06, New York City, New York.
C. Bosco and A. Mazzei. 2012. The evalita 2011 pars-
ing task: the dependency track. In Working Notes of
Evalita?11, Roma, Italy.
C. Bosco, V. Lombardo, L. Lesmo, and D. Vassallo.
2000. Building a treebank for italian: a data-driven
annotation schema. In Proceedings of the LREC?00,
Athens, Greece.
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo,
F. Dell?Orletta, A. Lenci, L. Lesmo, G. Attardi,
M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre.
2010. Comparing the influence of different treebank
annotations on dependency parsing. In Proceedings
of the LREC?10, Valletta, Malta.
C. Bosco, M. Simi, and S. Montemagni. 2012. Harmo-
nization and merging of two italian dependency tree-
banks. In Proceedings of the LREC 2012 Workshop
on Language Resource Merging, Istanbul, Turkey.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
In Proc. of CoNLL, pages 149?164.
D. Cer, M.C. de Marneffe, D. Jurafsky, and C.D. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In Proceed-
ings of the LREC?10), Valletta, Malta.
M.C. de Marneffe and C. Manning. 2008a. The stan-
ford typed dependencies representation. In Col-
ing 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
M.C. de Marneffe and C.D. Manning. 2008b. Stan-
ford typed dependencies manual. Technical report,
Stanford University.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of 5th In-
ternational Conference on Language Resources and
Evaluation (LREC 2006).
T. Declerck. 2008. A framework for standardized syn-
tactic annotation. In Proceedings of the LREC?08,
Marrakech, Morocco.
F. Dell?Orletta, S. Marchi, S. Montemagni, G. Venturi,
T. Agnoloni, and E. Francesconi. 2012. Domain
adaptation for dependency parsing at evalita 2011.
In Working Notes of Evalita?11, Roma, Italy.
K. Haverinen, T. Viljanen, V. Laippala, S. Kohonen,
F. Ginter, and T. Salakoski. 2010. Treebanking
Finnish. In Proceedings of the 9th Workshop on
Treebanks and Linguistic Theories (TLT-9), pages
79?90, Tartu, Estonia.
R. Hudson. 1984. Word Grammar. Basil Blackwell,
Oxford and New York.
N. Ide and L. Romary. 2006. Representing linguistic
corpora and their annotations. In Proceedings of the
LREC?06, Genova, Italy.
N. Ide and K. Suderman. 2007. GrAF: A graph-based
format for linguistic annotations. In Proceedings of
the Linguistic Annotation Workshop, Prague, Czech
Republic.
M. Kemps-Snijders, M. Windhouwer, P. Wittenburg,
and S.E. Wright. 2009. Isocat: remodelling meta-
data for language resources. IJMSO, 4(4):261?276.
S. Ku?bler, R.T. McDonald, and J. Nivre. 2009. De-
pendency Parsing. Morgan & Claypool Publishers,
Oxford and New York.
John Lee and Yin Hei Kong. 2012. A dependency
treebank of classical chinese poems. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
191?199, Montre?al, Canada, June. Association for
Computational Linguistics.
G. Leech, R. Barnett, and P. Kahrel. 1996. Eagles rec-
ommendations for the syntactic annotation of cor-
pora. Technical report, EAG-TCWG-SASG1.8.
A. Lenci, S. Montemagni, V. Pirrelli, and C. Soria.
2008. A syntactic meta?scheme for corpus anno-
tation and parsing evaluation. In Proceedings of the
LREC?00, Athens, Greece.
S. Montemagni, F. Barsotti, M. Battista, N. Calzo-
lari, O. Corazzari, A. Lenci, A. Zampolli, F. Fan-
ciulli, M. Massetani, R. Raffaelli, R. Basili, M. T.
Pazienza, D. Saracino, F. Zanzotto, N. Mana, F. Pi-
anesi, and R. Delmonte. 2003. Building the Italian
Syntactic-Semantic Treebank. In A. Abeille?, editor,
Building and Using syntactically annotated corpora.
Kluwer, Dordrecht.
S. Pyysalo, F. Ginter, K. Haverinen, J. Heimonen,
T. Salakoski, and V. Laippala. 2007. On the uni-
fication of syntactic annotations under the Stanford
dependency scheme: A case study on Bioinfer and
GENIA. In BioNLP 2007: Biological, transla-
tional, and clinical language processing, pages 25?
32, Prague.
M. Seraji, B. Megyesi, and J. Nivre. 2012. Bootstrap-
ping a persian dependency treebank. Special Issue
of Linguistic Issues in Language Technology (LiLT)
on Treebanks and Linguistic Theories, 7.
69
