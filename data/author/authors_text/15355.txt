Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590?600,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-Markov Phrase-based Monolingual Alignment
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Allen Institute for Artificial Intelligence
Seattle, WA, USA
Abstract
We introduce a novel discriminative model for
phrase-based monolingual alignment using a
semi-Markov CRF. Our model achieves state-
of-the-art alignment accuracy on two phrase-
based alignment datasets (RTE and para-
phrase), while doing significantly better than
other strong baselines in both non-identical
alignment and phrase-only alignment. Addi-
tional experiments highlight the potential ben-
efit of our alignment model to RTE, para-
phrase identification and question answering,
where even a naive application of our model?s
alignment score approaches the state of the art.
1 Introduction
Various NLP tasks can be treated as an alignment
problem: machine translation (aligning words in one
language with words in another language), ques-
tion answering (aligning question words with the an-
swer phrase), textual entailment recognition (align-
ing premise with hypothesis), paraphrase detection
(aligning semantically equivalent words), etc. Even
though most of these tasks involve only a single lan-
guage, alignment research has primarily focused on
the bilingual setting (i.e., machine translation) rather
than monolingual. Moreover, most work has con-
sidered token-based approaches over phrase-based.1
Here we seek to address this imbalance by proposing
better phrase-based models for monolingual word
alignment.
?Performed while faculty at Johns Hopkins University.
1In this paper we use the term token-based alignment for
one-to-one alignment and phrase-based for non one-to-one
alignment, and word alignment in general for both.
Most token-based alignment models can extrin-
sically handle phrase-based alignment to some ex-
tent. For instance, in the case of NYC align-
ing to New York City, the single source word
NYC may align three times separately to the tar-
get words: NYC?New, NYC?York, NYC?City.
Or in the case of identical alignment, New York
City aligning to New York City is simply
New?New, York?York, City?City. How-
ever, it is not as clear how to token-align New York
(as a city) with New York City. The problem is
more prominent when aligning phrasal paraphrases
or multiword expressions, such as pass away and
kick the bucket. This suggests an intrinsi-
cally phrase-based alignment model.
The token aligner jacana-align (Yao et al, 2013a)
has achieved state-of-the-art result on the task of
monolingual alignment, based on previous work of
Blunsom and Cohn (2006). It employs a Conditional
Random Field (Lafferty et al, 2001) to align tokens
from the source sentence to tokens in the target sen-
tence, by treating source tokens as ?observation? and
target tokens as ?hidden states?. However, it is not
designed to handle phrase-based alignment, largely
due to the Markov nature of the underlying model:
a state can only span one token each time, making
it unable to align multiple consecutive tokens (i.e. a
phrase). We extend this model by introducing semi-
Markov states for phrase-based alignment: a state
can instead span multiple consecutive time steps,
thus aligning phrases on the source side. Also, we
merge phrases on the target side to phrasal states,
allowing the model to align phrases on the target
side as well. We evaluate the resulting semi-Markov
590
CRF model on the task of phrase-based alignment,
and then show a basic application in the NLP tasks
of recognizing textual entailment, paraphrase iden-
tification, and question answering sentence ranking.
The final phrase-based aligner is open-source.2
2 Related Work
Most work in monolingual alignment employs de-
pendency tree/graph matching algorithms, includ-
ing tree edit distance (Punyakanok et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Yao et al, 2013b), Particle Swarm Optimiza-
tion (Mehdad, 2009), linear regression/classification
models (Chambers et al, 2007; Wang and Manning,
2010), and min-cut (Roth and Frank, 2012). These
works inherently only support token-based align-
ment, with phrase-like alignment achieved by first
merging tokens to phrases as a preprocessing step.
The MANLI aligner (MacCartney et al, 2008)
and its derivations (Thadani and McKeown, 2011;
Thadani et al, 2012) are the first known phrase-
based aligners specifically designed for aligning En-
glish sentence pairs. It applies discriminative per-
ceptron learning with various features and handles
phrase-based alignment of arbitrary phrase lengths.
MANLI suffers from slow decoding time due to its
large search space. This was optimized by Thadani
and McKeown (2011) through Integer Linear Pro-
gramming (ILP), where benefiting from modern ILP
solvers they showed an order-of-magnitude speedup
in decoding. Also, various syntactic constraints can
be easily added, significantly improving exact align-
ment match rate for whole sentence pairs. Besides
the common application of textual entailment and
question answering, monolingual alignment has also
been applied in the field of text generation (Barzilay
and Lee, 2003; Pang et al, 2003).
Word alignment has been more explored in ma-
chine translation. The IBM models (Brown et al,
1993) allow many-to-one alignment and are essen-
tially asymmetric. Phrase-based MT historically
relied on heuristics (Koehn, 2010) to merge two
sets of word alignment in opposite directions to
yield phrasal alignment. Later, researchers explored
non-heuristic phrase-based methods. Among them,
Marcu and Wong (2002) described a joint proba-
2http://code.google.com/p/jacana/
bility model that generates both the source and tar-
get sentences simultaneously. All possible pairs of
phrases in both sentences are enumerated and then
pruned with statistical evidence. Deng and Byrne
(2008) explored token-to-phrase alignment based
on HMM models (Vogel et al, 1996) by explic-
itly modeling the token-to-phrase probability and
phrase lengths. However, the token-to-phrase align-
ment is only in one direction: each target state still
only spans one source word, and thus alignment on
the source side is limited to tokens. Andre?s-Ferrer
and Juan (2009) extended the HMM-based method
to Hidden Semi-Markov Models (HSMM) (Osten-
dorf et al, 1996), allowing phrasal alignments on
the source side. Finally, Bansal et al (2011) unified
the HSMM models with the alignment by agreement
framework (Liang et al, 2006), achieving phrasal
alignment that agreed in both directions.
Despite successful usage of generative semi-
Markov models in bilingual alignment, this has not
been followed with models in discriminative mono-
lingual alignment. Essentially monolingual align-
ment would benefit more from discriminative mod-
els with various feature extractions (just like those
defined in MANLI) than generative models without
any predefined feature (just like how they were used
in bilingual alignment). To combine the strengths of
both semi-Markov models and discriminative train-
ing, we propose to use the semi-Markov Conditional
Random Field (Sarawagi and Cohen, 2004), which
was first used in information extraction to tag con-
tinuous segments of input sequences and outper-
formed conventional CRFs in the task of named en-
tity recognition. We describe this model in the fol-
lowing section.
3 The Alignment Model
Our objective is to define a model that supports
phrase-based alignment of arbitrary phrase length.
In this section we first describe a regular CRF
model that supports one-to-one token-based align-
ment (Blunsom and Cohn, 2006; Yao et al, 2013a),
then extend it to phrase-based alignment with the
semi-Markov model.
591
3.1 Token-based Model
Given a source sentence s of length M , and a target
sentence t of lengthN , the alignment from s to t is a
sequence of target word indices a, where ai?[1,M ] ?
[0, N ]. We specify that when ai = 0, source word si
is aligned to a NULL state, i.e., deleted. This models
a many-to-one alignment from source to target: mul-
tiple source words can be aligned to the same target
word, but not vice versa. One-to-many alignment
can be obtained by running the aligner in the other
direction. The probability of alignment sequence a
conditioned on both s and t is then:
p(a | s, t) =
exp(
?
i,k ?kfk(ai?1, ai, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). Since the word align-
ment task is evaluated over F1, instead of directly
optimizing it, we choose a much easier objective
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denomi-
nator:
Z(s, t) =
?
a?
exp(
?
i,k
?kfk(a?i?1, a?i, s, t)
+cost(ay, a?))
where ay is the true alignments. cost(ay, a?) can
be viewed as special ?features? that encourage de-
coding to be consistent with true labels. It is only
computed during training in the denominator be-
cause in the numerator cost(ay,ay) = 0. Ham-
ming cost is used in practice without learning the
weights (i.e., uniform weights). The more inconsis-
tence there is between ay and a?, the more penalized
is the decoding sequence a? through the cost func-
tion.
3.2 Phrase-based Model
The token-based model supports 1 : 1 alignment.
We first extend it in the direction of ls : 1, where
a target state spans ls words on the source side (ls
source words align to 1 target word). Then we ex-
tend it in the direction of 1 : lt, where lt is the tar-
get phrase length a source word aligns to (1 source
word aligns to lt target words). The final combined
shops areShops closed up for now until March
NULL
closed
temp.
are
Shops
down
shops-are
...-... 7..14
0
1
2
3
4
5
6
closed-down 15
Figure 1: A semi-Markov phrase-based model
example and the desired Viterbi decoding path.
Shaded horizontal circles represent the source
sentence (Shops are closed up for now
until March) and hollow vertical circles repre-
sent the hidden states with state IDs for the target
sentence (Shops are temporarily closed
down). State 0, a NULL state, is designated for dele-
tion. One state (e.g. state 3 and 15) can span multi-
ple consecutive source words (a semi-Markov prop-
erty) for aligning phrases on the source side. States
with an ID larger than the target sentence length
indicate ?phrasal states? (states 6-15 in this exam-
ple), where consecutive target tokens are merged for
aligning phrases on the target side. Combining the
semi-Markov property and phrasal states yields for
instance, a 2?2 alignment between closed up in
the source and closed down in the target.
model supports ls : lt alignment. Throughout this
section we use Figure 1 as an illustrative example,
which shows phrasal alignment between the source
sentence: (Shops are closed up for now
until March) and the target sentence: (Shops
are temporarily closed down).
1 : 1 alignment is a special case of ls : 1 align-
ment where the target side state spans ls = 1 source
word, i.e., at each time step i, the source side word
592
si aligns to one state ai and the next aligned state
ai+1 only depends on the current state ai. This is
the Markovian property of the CRF. When ls > 1,
during the time frame [i, i + ls), all source words
[ai, ai+ls) share the same state ai. Or in other words,
the state ai ?spans? the following ls time steps. The
Markovian property still holds ?outside? the time
frame ls, i.e., ai+ls still only depends on ai, the pre-
vious state ls time steps ago. But ?within? the time
frame ls, the Markovian property does not hold any
more: [ai, ..., ai+ls?1] are essentially the same state
ai. This is the semi-Markov property . States can be
distinguished by this property into two types: semi-
Markovian states and Markovian states.
We have generalized the regular CRF to a semi-
Markov CRF. Now we define it by generalizing the
feature function:
p(a | s, t) =
exp(
?
i,k,ls ?kfk(ai?ls , ai, s, t))
Z(s, t)
At time i, the k-th feature function fk mainly
extracts features from the pair of source words
(si?ls , ..., si] and target word tai (still with a spe-
cial case that ai = 0 marks for deletion). Inference
is still Viterbi-like: except for the fact during maxi-
mization, the Viterbi algorithm not only checks the
previous one time step, but all ls time steps. Sup-
pose the allowed maximal source phrase length is
Ls, define Vi(a | s, t) as the highest score along the
decoding path until time i ending with state a:
Vi(a | s, t) = max
a1,a2,...ai?1
p(a1, a2, . . . , ai = a | s, t)
then the recursive maximization is:
Vi(a | s, t) = max
a?
max
ls=1...Ls
[Vi?ls(a
?
| s, t)
+?i(a
?
, a, ls, s, t)]
with factor:
?i(a
?
, a, ls, s, t) =
?
k
?kfk(a
?
i?ls , ai, s, t)
and the best alignment a can be obtained by back-
tracking the last state aM from VM (aM | s, t).
Training a semi-Markov CRF is very similar to
the inference, except for replacing maximization
with summation. The forward-backward algorithm
should also be used to dynamically compute the nor-
malization function Z(s, t). Compared to regular
CRFs, a semi-Markov CRF has a decoding time
complexity of O(LsMN2), a constant factor Ls
(usually 3 or 4) slower.
To extend from 1 : 1 alignment to 1 : lt alignment
with one source word aligning to lt target words,
we simply explode the state space by Lt times with
Lt the maximal allowed target phrase length. Thus
the states can be represented as an N ? Lt ma-
trix. The state at (j, lt) represents the target phrase
[tj , ..., tj+lt). In this paper we distinguish states by
three types: NULL state (j = 0, lt = 0), token state
(lt = 1) and phrasal state (lt > 1).
To efficiently store and compute these states, we
linearize the two dimensional matrix with a linear
function mapping uniquely between the state ID and
the target phrase offset/span. Suppose the target
phrase tj of length ltj ? [1, Lt] holds a position
ptj ? [1, N ], and the source word si is aligned to
this state (ptj , ltj ), a tuple for (position, span). Then
state ID asi is computed as:
asi(ptj , ltj ) =
{
ptj ltj = 1
N + (ptj ? 1)? Lt + ltj 1 < ltj ? Lt
Assume in Figure 1, Lt = 2, then the state ID for
the phrasal state (5, 2) closed-down with ptj = 5
for the position of word down and ltj = 2 for the
span of 2 words (looking ?backward? from the word
down) is: 5 + (5? 1)? 2 + 2 = 15.
Similarly, given a state id asi , the original target
phrase position and length can be recovered through
integer division and modulation. Thus during decod-
ing, if one output state is 15, we would know that it
uniquely comes from the phrasal state (5,2), repre-
senting the target phrase closed down.
This two dimensional definition of state space ex-
pands the number of states from 1 + N to 1 +
LtN . Thus the decoding complexity becomes
O(M(LtN)2) = O(L2tMN
2) with a usual value
of 3 or 4 for Lt.
Now we have defined separately the ls : 1 model
and the 1 : lt model. We can simply merge them to
593
have an ls : lt alignment model. The semi-Markov
property makes it possible for any target states to
align phrases on the source side, while the two di-
mensional state mapping makes it possible for any
source words to align phrases on the target side. For
instance, in Figure 1, the phrasal state a15 repre-
sents the two-word phrase closed down on the
target side, while still spanning for two words on the
source side, allowing a 2? 2 alignment. State a15 is
phrasal, and at source word position 3 and 4 (span-
ning closed up) it is semi-Markovian. The final
decoding complexity is O(LsL2tMN
2), a factor of
30 ? 60 times slower than the token-based model
(with a typical value of 3 or 4 for Ls and Lt).
In the following we describe features.
3.3 Feature Design
We reused features in the original token-based
model based on string similarity, POS tags, position,
WordNet, distortion and context. Then we used an
additional chunker to mark phrase boundaries only
for feature extraction:
Chunking Features are binary indicators of
whether the phrase types of two phrases match.
Also, we added indicators for mappings between
source phrase types and target phrase types, such as
?vp2np?, meaning that a verb phrase in the source is
mapped to a noun phrase in the target.
Moreover, we introduced the following lexical
features:
PPDB Features (Ganitkevitch et al, 2013) in-
clude various similarity scores derived from a para-
phrase database with 73 million phrasal and 8 mil-
lion lexical paraphrases. Various paraphrase condi-
tional probability was employed. For instance, for
the ADJP/VP phrase pair capable of and able
to, there are the following minus-log probabilities:
p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0
p(e1|e2) = 1.3, p(e2|lhs) = 6.7, p(e2|e1) = 2.8
p(e1|e2, lhs) = 0.6, p(e2|e1, lhs) = 2.3
where e1/e2 are the phrase pair, and lhs is the
left hand side syntactic non-terminal symbol. We
did not use the syntactic part (e.g., the NP of
NNS ? the NNS of NP) of PPDB as we did not
make the assumption that the input sentence pairs
were well-formed (and newswire-like) English, or
even of a language with a parser available. Also, for
phrasal alignments, we ruled out those paraphrases
spanning multiple syntactic structures, or of differ-
ent syntactic structures (indicated as [X] in PPDB),
for instance, and crazy? , mad.
Semantic Relatedness Feature is a single scaled
number in [0, 1] from the best performing system
(Han et al, 2013) of the *Sem 2013 Semantic Tex-
tual Similarity (STS) task. We included this fea-
ture mainly to deal with cases where ?related? words
cannot be well measured by either paraphrases or
distributional similarities. For instance, in one align-
ment dataset annotators aligned married with
wife. Adding a few other words as comparison, the
Han et al (2013) system gives the following similar-
ity scores:
married/wife: 0.85
married/husband: 0.84
married/child: 0.10
married/stone: 0.01
Name Phylogeny Feature (Andrews et al, 2012)
is a similarity feature with a string transducer to
model how one name evolves to another. Examples
below show how similar is the name Bill associ-
ated with other names in log probability:
Bill/Bill: -0.8
Bill/Billy: -5.2
Bill/William: -13.6
Bill/Mary: -18.6
Finally, one decision we made during feature
design was not to use any parsing-based features,
with a permissive assumption that the input might
not be well-formed English, or even not complete
sentences (such as fragmented snippets from web
search). The ?deepest? linguistic processing stays at
the level of tagging and chunking, making the model
more easily extendable to other languages.
3.4 Feature Value
In this phrase-based model, the width of a state span
over the source words depends on the competition
between features fired on the phrases as a whole vs.
the consecutive but individual tokens. We found it
critical to assign feature values ?fairly? among to-
kens and phrases to make sure that semi-Markov
states and phrasal states fire up often enough for
phrasal alignments.
594
train test length %align.
MSR06 800 800 29/11 36%
Edinburgh++ 715 305 22/22 78%
Table 1: Statistics of the two manually aligned cor-
pora, divided into training and test in sentence pairs.
The length column shows average lengths of source
and target sentences in a pair. %align. is the per-
centage of aligned tokens.
To illustrate this in a simplified way, take
closed up?closed down in Figure 1, and as-
sume the only feature is the normalized number of
matching tokens in the pair. Then this feature firing
on the following pairs would have values (the nor-
malization factor is the maximal phrase length):
closed?closed 1.0
closed up?closed 0.5
closed up?up 0.5
closed up?closed down 0.5
...?... ...
The desired alignment closed up?closed
down would not have survived the state com-
petition due to its weak feature value. In this
case the model would simply prefer a token align-
ment closed?closed and up?... (probably
NULL).
Thus we upweighted feature values by the max-
imum source or target phrase length to encour-
age phrasal alignments, in this case closed up
?closed down:1.0. Then this alignment would
have a better chance to be picked out with additional
features, such as with the PPDB and Semantic Relat-
edness Features, which are also upweighted by max-
imum phrase lengths.
4 Experiment
4.1 Data Preparation
There are two annotated datasets for training and
testing. MSR063 (Brockett, 2007) has annotated
alignments on the 2006 PASCAL RTE2 develop-
ment and test corpora, with 1600 pairs in total.
3http://www.cs.biu.ac.il/?nlp/files/RTE_
2006_Aligned.zip
1x1 1x2 1x3 2x2 2x3 3x3 more
MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8
EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1
Table 2: Percentage of various alignment sizes
(undirectional, e.g., 1x2 and 2x1 are merged) af-
ter synthesizing phrasal alignment from token align-
ment in the training portion of two corpora.
Semantically equivalent words and phrases in the
premise and hypothesis sentences are aligned in a
manner analogous to alignments in statistical ma-
chine translation. This dataset is asymmetric: on
average the premises contain 29 words and the hy-
potheses 11 words. Edinburgh++4 (Thadani et al,
2012) is a revised version of the Edinburgh para-
phrase corpus(Cohn et al, 2008) with sentences
from the following resources: 1. the Multiple-
Translation Chinese corpus; 2. Jules Verne?s novel
Twenty Thousand Leagues Under the Sea. 3. the
Microsoft Research paraphrase corpus (Dolan et al,
2004). The corpus is more balanced and symmetric:
the source and target sentences are both 22 words
long on average. Table 1 shows some statistics.
Both corpora contain mostly token-based align-
ment. For MSR06, MacCartney et al (2008) showed
that setting the allowable phrase size to be greater
than one only increased F1 by 0.2%. For Ed-
inburgh++, the annotation guideline5 explicitly in-
structs to ?prefer smaller alignments whenever pos-
sible?. Statistics shows that single token alignment
counts 96% and 95% of total alignments in these two
corpora separately. With such a heavy imbalance to-
wards only token-based alignment, a phrase-based
aligner would learn feature weights that award token
alignments more than phrasal alignments.
Thus we synthesized phrasal alignments from
continuous monotonic token alignments in these two
corpora. We first ran the OpenNLP chunker through
the corpora. Then for each phrase pair, if each token
in the source phrase is aligned to a token in the tar-
get phrase in a monotonic way, and vice versa, we
4http://www.ling.ohio-state.edu/?scott/
#edinburgh-plusplus
5http://staffwww.dcs.shef.ac.uk/people/
T.Cohn/paraphrase_guidelines.pdf
595
merge these alignments to form one single phrasal
alignment.6 Table 2 lists the percentage of vari-
ous alignment sizes after the merge. Two obser-
vations can be made: first, the portion of phrasal
alignments increases to 10% ? 20% after merging;
second, allowing a maximal phrase length of 3 cov-
ers 98% ? 99% of total alignments, thus a phrase
length larger than 3 would be a bad trade-off for cov-
erage vs speed.
4.2 Baselines and Evaluation Metrics
MacCartney et al (2008) and Yao et al (2013a)
showed that the traditional MT bilingual aligner
GIZA++ (Och and Ney, 2003) presented weak re-
sults on the task of monolingual alignment. Thus
we instead used four other strong baselines:
Meteor (Denkowski and Lavie, 2011): a sys-
tem for evaluating machine translation by aligning
MT output with reference sentences. It is designed
for the task of monolingual alignment and supports
phrasal alignment. We used version 1.4 and default
weights to optimize by maximum accuracy.
MANLI-constraint (Thadani and McKeown,
2011): a re-implemented MANLI system with ILP-
powered decoding for speed and hard syntactic con-
straints to boost exact match rate, with reported
numbers on MSR06.
MANLI-joint (Thadani et al, 2012): an im-
proved version of MANLI-constraint that not only
models phrasal alignments, but also alignments be-
tween dependency arcs, with reported numbers on
the original Edinburgh paraphrase corpus.
jacana-token (Yao et al, 2013a): a token-
based aligner with state-of-the-art performance on
MSR06.
Note that the jacana-token aligner is open-source,
so we were able to re-train it with exactly the
same feature set used by our phrase-based model.
This allows a fair comparison of model performance
(token-based vs. phrase-based). The MANLI* sys-
tems are not available, thus we only reported their
numbers from published papers.
The standard evaluation metrics for alignments
are precision (P), recall (R), F1, and exact matching
6a few examples: two Atlanta-based
companies?two Atlanta companies, the
UK?the UK, the 17-year-old?the teenager,
was held?was held.
rate (E) based on either tokens (two tokens are con-
sidered aligned iff they are aligned) or phrases (two
tokens are considered aligned iff they are contained
within phrases that are aligned). Following Thadani
et al (2012), we only report the results based on
token alignments (which allows a partial credit if
their containing phrases are not aligned), even for
the phrase-based alignment task. The reasoning is
that if a phrase-based aligner is already doing bet-
ter than a token aligner in terms of token alignment
scores, then the difference in terms of phrase align-
ment scores will be even larger. Thus showing the
superiority of token alignment scores is sufficient.
4.3 Implementation and Training
The elements in the phrase-based model: dynamic
state indices, semi-Markov and phrasal states, are
not typically found in standard CRF implementa-
tions. Thus we implemented the phrase-based model
in the Scala programming language, which is fully
interoperable with Java, using one semi-Markov
CRF package7 as a reference. We used the L2 reg-
ularizer and LBFGS for optimization. OpenNLP8
provided the POS tagger and chunker and JWNL9
interfaced with WordNet (Fellbaum, 1998).
4.4 Results
Table 3 gives scores (in bigger fonts) of different
aligners on MSR06 and Edinburgh++ and their cor-
responding phrasal versions. Overall, the token-
based aligner did the best on the original corpora, in
which single token alignment counts more than 95%
of total alignment. The phrase-based aligner did
slightly worse. We think the main reason was that it
output more phrasal alignment, which in turn harms
scores in token-based evaluation (for instance, if the
gold alignment is New?New, York?York, then
the phrasal alignment of New York?New York
would only have half the precision because it inher-
ently also aligns New in the source with York in
the target.). Further investigation showed that on the
Edinburgh++ corpus, over-generated phrase-based
alignment, when evaluated under just token align-
ment, contributed hurting about 1.1% of overall F1,
7http://crf.sf.net
8http://opennlp.apache.org/
9http://jwordnet.sf.net/
596
a gap that would make the phrase aligner (85.9%)
outperform the token aligner (86.4%).
On the phrasal alignment corpora (represented by
MSR06P and EDB++P in Table 3), the phrase-based
aligner did significantly better. Note that the over-
all F1 and exact match rate are still much lower
than those scores obtained from the original corpora,
suggesting that the phrasal corpora present a much
harder task. Furthermore, as a more ?fair? com-
parison between the two aligners, we synthesized
phrasal alignments from the output of the token-
based aligner, just as how the phrased-based corpora
were prepared, then evaluated its performance again.
Still, on the EDB++P corpus, the token aligner was
about 1.6% (current difference is 69.1% vs. 72.8%)
worse than the phrase-based aligner.
Also, we want to emphasize that since the token-
based aligner and the phrase-based aligner shared
exactly the same features and lexical resources, the
performance boost of the phrase-based aligner on
the phrasal corpora results from a better model de-
sign: it is the semi-Markov property and phrasal
states making the phrase-based aligner better.
To further investigate the performance of aligners
with respect to different types of alignment, we di-
vided the scores into those for identical alignments
(such as New?New) and non-identical alignments
(such as wife?spouse), indicated by the sub-
scripts i and n in Table 3. In terms of identical
alignment, most aligners were able to score more
than 90%, but for non-identical alignment there was
noticeable decrease. Still, on the phrasal alignment
corpora, the phrase-based model has a much larger
recall score for non-identical alignment than others.
We also divided scores with respect to token-only
alignment and phrase-only alignment. Due to space
limit, we only show results on synthesized Edin-
burgh++, in Table 4. Meteor and the token aligner
inherently have either very limited or no support for
phrasal alignment, thus they had very low scores
on phrase-only alignment. We then ran the align-
ers in two directions and merged the results with the
?union? MT heuristic to get better phrase support.
But that still did not bring F1p?s up to over 5%.
The phrase-based aligner baseline Meteor did
worse than our aligners. We think there are two rea-
sons: First, Meteor was not trained on these corpora.
Second, Meteor only does strict word, stem, syn-
System
P% R% F1%
E%
Pi/Pn Ri/Rn F1i/F1n
M
S
R
06
(7
8.
6%
) Meteor
82.5 81.2 81.9
15.0
89.9/39.9 97.3/24.6 93.5/30.5
MANLI-cons. 89.5 86.2 87.8 33.0
token
93.6 83.5 88.3
32.1
96.6/77.7 96.9/35.6 96.8/48.8
phrase
92.1 82.8 86.8
29.1
95.7/65.0 95.9/34.7 95.8/45.2
M
S
R
06
P
(5
9.
0%
) Meteor
82.5 68.3 74.7
7.3
89.9/40.1 97.3/8.8 93.5/14.5
token
92.9 66.1 77.2
13.5
95.5/77.5 94.3/11.1 94.9/19.5
phrase
83.5 77.0 80.1
14.3
94.9/55.5 94.2/48.1 94.5/51.5
E
D
B
+
+
(7
5.
2%
) Meteor
88.3 80.5 84.2
12.7
94.0/61.4 97.8/24.1 95.9/34.7
MANLI-jnt* 76.6 83.8 79.2 12.2
token
91.3 82.0 86.4
15.0
96.4/63.9 97.4/36.4 96.9/46.4
phrase
90.4 81.9 85.9
13.7
96.0/57.4 97.8/38.3 96.9/46.0
E
D
B
+
+
P
(5
1.
7%
) Meteor
88.4 60.6 71.9
2.9
94.0/61.9 97.0/6.5 95.5/11.7
token
90.7 55.8 69.1
2.3
96.2/58.6 91.3/7.1 93.7/12.7
phrase
82.3 65.3 72.8
1.6
95.6/60.4 93.1/34.3 94.4/43.8
Table 3: Results on original (mostly token) and phrasal
(P) alignment corpora, where (x%) indicates how much
alignment is identical alignment, such as New?New. E%
stands for exact (perfect) match rate. Subscript i stands
for corresponding scores for ?identical? alignment and n
for ?non-identical?. *: scores of MANLI-joint were for
the original Edinburgh corpus instead of Edinburgh++
(with hand corrections) so it is not a direct comparison.
onym and paraphrase matching but does not use any
string similarity measures; this can be supported by
the large difference between, for instance, F1i and
F1n. In general Meteor did well on identical align-
ment, but not so well on non-identical alignment.
5 Applications
Natural language alignment can be applied to vari-
ous NLP tasks. While how to most effectively apply
597
System
P% R% F1%
E%
Pt/Pp Rt/Rp F1t/F1p
E
D
B
+
+
P
Meteor
88.4 60.6 71.9
2.9
59.5/14.9 90.6/1.1 71.8/2.0
token
90.7 55.8 69.1
2.3
59.4/21.4 85.5/0.9 70.1/1.7
phrase
82.3 65.3 72.8
1.6
73.3/48.0 73.5/44.2 73.4/46.0
Table 4: Same results on the phrasal Edinburgh++ cor-
pus but with scores divided by token-only alignment
(subscript t) and phrase-only alignment (subscript p).
it is another topic, we simply show in this section us-
ing just alignment scores in binary prediction prob-
lems. Specifically, we pick the tasks of recognizing
textual entailment (RTE), paraphrase identification
(PP), and question answering sentence ranking (QA)
described in Heilman and Smith (2010):
RTE: predicting whether a hypothesis can be in-
ferred from the premise, with training data from
RTE-1/2 and RTE-3 dev, and test from RTE-3 test.
PP: predicting whether two sentences are para-
phrases, with training and test data from the MSR
Paraphrase Corpus (Dolan et al, 2004).
QA: predicting whether a sentence contains the
answer to the question, with training data from
TREC-8 to TREC-12 and test data from TREC-13.
For each aligned pair, we can compute a normal-
ized decoding score. Following MacCartney et al
(2008), we select a threshold score and predict true
if the decoding score is above this threshold. For the
tasks of RTE and PP, we tuned this threshold w.r.t
the maximal accuracy on the training set, then re-
ported performance on the test set. For the task of
QA, since the evaluation methods in Mean Average
Precision and Mean Reciprocal Rank only need a
ranked list of answer sentences, and the scores on
the test set are sufficient to provide the ranking, we
did not tune anything on training but instead directly
ran the aligner on the test set. All three tasks shared
the same aligner model trained on the superset of
MSR06 and Edinburgh++. Results are reported in
Table 5. We could not report on Meteor as Meteor
does not explicitly output alignment scores.
We did not expect the aligners to beat any of the
system A% P% R%
de Marneffe et al (2006) 60.5 61.8 60.2
MacCartney and Manning (2008) 64.3 65.5 63.9
Heilman and Smith (2010) 62.8 61.9 71.2
the token aligner 59.1 61.2 55.4
our phrasal aligner 57.6 57.2 68.8
(a) Recognizing Textual Entailment
system A% P% R%
Wan et al (2006) 75.6 77 90
Das and Smith (2009) 73.9 74.9 91.3
Heilman and Smith (2010) 73.2 75.7 87.8
the token aligner 70.0 72.6 88.1
our phrasal aligner 68.1 68.6 95.8
(b) Paraphrase Identification
system MAP MRR
Cui et al (2005) 0.4271 0.5259
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Yao et al (2013b) 0.6307 0.7477
the token aligner 0.5982 0.6582
our phrasal aligner 0.6165 0.7333
(c) Question Answering Sentence Ranking
Table 5: Results (Accuracy, Precision, Recall, Mean
Average Precision, Mean Reciprocal Rank) on the
tasks of RTE, PP and QA.
state-of-the-art result since no sophisticated models
were additionally used but only the alignment score.
Still, the aligners showed competitive performance.
It still follows the pattern from the alignment exper-
iment that the phrasal aligner had higher recall and
lower precision than the token aligner in the task of
RTE and PP. In the QA task, the phrasal aligner per-
formed better than all systems except for the top one.
6 Conclusion
We have introduced a phrase-to-phrase alignment
model based on semi-Markov Conditional Random
Fields. The combination of semi-Markov states and
phrasal states makes phrasal alignment on both the
source and target sides possible. The final phrase-
598
based aligner performed the best on two phrasal
alignment corpora and showed its potential usage
in three NLP tasks. Future work includes aligning
discontinuous (gappy) phrases and integrating align-
ment more closely in NLP applications.
Acknowledgement
We thank Vulcan Inc. for funding this work. We also
thank Jason Smith, Travis Wolfe, Frank Ferraro and
the three anonymous reviewers for their comments
and suggestion.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Procedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: a generative model of string
variation. In Proceedings of EMNLP 2012.
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL, pages
16?23.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263?311.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614, December.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ?05, pages 400?407, New York, NY,
USA. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468?476, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D Manning. 2006. Learning to distinguish valid
textual entailments. In Second Pascal RTE Challenge
Workshop.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. Audio, Speech, and Language Processing, IEEE
Transactions on, 16(3):494?507.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, Stroudsburg, PA, USA.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages 758?
764.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin CRFs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. UMBC-EBIQUITY-
CORE: Semantic Textual Similarity Systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
599
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of ACL 2008,
pages 521?528.
Bill MacCartney, Michel Galley, and Christopher D Man-
ning. 2008. A phrase-based alignment model for nat-
ural language inference. In Proceedings of EMNLP,
pages 802?811.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP-2002, pages 133?139.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Mari Ostendorf, Vassilios V Digalakis, and Owen A Kim-
ball. 1996. From HMM?s to segment models: a uni-
fied view of stochastic modeling for speech recogni-
tion. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360?378.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL, pages 102?109.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning
predicates across monolingual comparable texts using
graph-based clustering. In Proceedings of EMNLP-
CoNLL, pages 171?182, Jeju Island, Korea, July.
Sarawagi Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems, 17:1185?1192.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings of ACL short.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A joint phrasal and dependency model for paraphrase
alignment. In Proceedings of COLING 2012: Posters,
pages 1229?1238, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics - Volume 2, COLING ?96, pages
836?841.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL, pages 22?32, Prague, Czech Repub-
lic, June.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013a. A Lightweight and
High Performance Monolingual Word Aligner. In
Proceedings of ACL 2013 short, Sofia, Bulgaria.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013b. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
600
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621?625,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Expectations of Word Sense in Parallel Corpora
Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Given a parallel corpus, if two distinct words
in language A, a1 and a2, are aligned to the
same word b1 in language B, then this might
signal that b1 is polysemous, or it might sig-
nal a1 and a2 are synonyms. Both assump-
tions with successful work have been put for-
ward in the literature. We investigate these
assumptions, along with other questions of
word sense, by looking at sampled parallel
sentences containing tokens of the same type
in English, asking how often they mean the
same thing when they are: 1. aligned to the
same foreign type; and 2. aligned to different
foreign types. Results for French-English and
Chinese-English parallel corpora show simi-
lar behavior: Synonymy is only very weakly
the more prevalent scenario, where both cases
regularly occur.
1 Introduction
Parallel corpora have been used for both paraphrase
induction and word sense disambiguation (WSD).
Usually one of the following two assumptions is
made for these tasks:
1. Polysemy If two different words in language
A are aligned to the same word in language B,
then the word in language B is polysemous.
2. Synonymy If two different words in language
A are aligned to the same word in language B,
then the two words in A are synonyms, and thus
is not evidence of polysemy in B.
Despite the alternate nature of these assumptions,
both have associated articles in which a researcher
claimed success. Under the polysemy assumption,
Gale et al (1992) used French translations as En-
glish sense indicators in the task of WSD. For in-
stance, for the English word duty, the French transla-
tion droit was taken to signal its tax sense and devoir
to signal its obligation sense. These French words
were used as labels for different English senses.
Similarly, in a cross-lingual WSD setting,1 Lefever
et al (2011) treated each English-foreign alignment
as a so-called ParaSense, using it as a proxy for hu-
man labeled training data.
Under the synonymy assumption, Diab and
Resnik (2002) did word sense tagging by grouping
together all English words that are translated into
the same French word and by further enforcing that
the majority sense for these English words was pro-
jected as the sense for the French word. Bannard and
Callison-Burch (2005) applied the idea that French
phrases aligned to the same English phrase are para-
phrases in a system that induces paraphrases by piv-
oting through aligned foreign phrases.
Based on this, and other successful prior work,
it seems neither of the assumptions must hold
universally. Therefore we investigate how often
we might expect one or the other to dominate:
we sample polysemous words from wide-domain
{French,Chinese}-English corpora, and use Ama-
zon?s Mechanical Turk (MTurk) to annotate word
sense on the English side. We calculate empirical
probabilities based on counting over the competing
polysemous and synonymous scenario labels.
A key factor deciding the validity of our conclu-
sion is the reliability of the annotations derived via
MTurk. Thus our first step is to evaluate the abil-
ity of Turkers to perform WSD. After verifying this
1E.g., given a sentence ?... more power, more duty ...?, the
task asks to give a French translation of duty, which should be
devior, after first recognizing the underlying obligation sense.
621
as a reasonable process for acquiring large amounts
of WSD labeled data, we go on to frame the experi-
mental design, giving final results in Sec. 4.
2 Turker Reliability
While Amazon?s Mechanical Turk (MTurk) has
been been considered in the past for constructing
lexical semantic resources (e.g., (Snow et al, 2008;
Akkaya et al, 2010; Parent and Eskenazi, 2010;
Rumshisky, 2011)), word sense annotation is sensi-
tive to subjectivity and usually achieves low agree-
ment rate even among experts. Thus we first asked
Turkers to re-annotate a sample of existing gold-
standard data. With an eye towards costs saving, we
also considered how many Turkers would be needed
per item to produce results of sufficient quality.
Turkers were presented sentences from the test
portion of the word sense induction task of
SemEval-2007 (Agirre and Soroa, 2007), covering
2,559 instances of 35 nouns, expert-annotated with
OntoNotes (Hovy et al, 2006) senses. Two versions
of the task were designed:
1. compare: given the same word in different
sentences, tell whether their meaning is THE
SAME, ALMOST THE SAME, UNLIKELY THE
SAME or DIFFERENT, where the results were
collapsed post-hoc into a binary same/different
categorization;
2. sense map: map the meaning of a given word
in a sentential context to its proper OntoNotes
definition.
For both tasks, 2, 599 examples were presented.
We measure inter-coder agreement using Krip-
pendorff?s Alpha (Krippendorff, 2004; Artstein and
Poesio, 2008), where ? ? 0.8 is considered to be
reliable and 0.667 ? ? < 0.8 allows for tenta-
tive conclusions. Two points emerge from Table 1:
there were greater agreement rates for sense map
than compare, and 3 Turkers were sufficient.
3 Experiment Design
Data Selection We used two parallel corpora: the
French-English 109 corpus (Callison-Burch et al,
2009) and the GALE Chinese-English corpus.
?-Turker ?-maj. maj.-agr.
compare5 0.47 0.66 0.87
compare3 0.44 0.52 0.83
sense map5 0.79 0.93 0.95
sense map3 0.75 0.87 0.91
Table 1: MTurk result on testing Turker reliability. Krip-
pendorff?s Alpha is used to measure agreement. ?-
Turker: how Turkers agree among themselves, ?-maj.:
how the majority agrees with true value, maj.-agr.: agree-
ment between the majority vote and true value. ?-maj.
indicates the confidence level about the maj.-agr. value.
Subscripts denote either 5 Turkers, or 3 randomly se-
lected of the 5.
For each corpus we selected 50 words, w, at ran-
dom from OntoNotes,2 constrained such that w: had
more than one sense; had a frequency ? 1, 000; and
was not a top 10% most frequent words.
Next we sampled 100 instances (aligned English-
foreign sentence pairs) for each word based on the
following constraints: the aligned foreign word, f ,
had a frequency ? 20 in the foreign corpus; f had a
non-trivial alignment probability.3 We sampled pro-
portionally to the distribution of the aligned foreign
words, ensuring that at least 5 instances from each
foreign translation are sampled.4
For each corpus, this results in 100 instances for
each of 50 words, totaling 5,000 instances. We used
3 Turkers per instance for sense annotation, under
the sense map task. We note that the set of 50
randomly selected English words from the Chinese-
English corpus were entirely distinct from the 50 se-
lected words from the French-English corpus.
Probability Estimation Suppose e1 and e2 are
two tokens of the same English word type e. s(e1)
is a function that returns the sense of e1, a(e1) is
a function that returns the aligned word of e1. Let
c() be our count function, where: c(e, f) returns the
2OntoNotes was used as the sense inventory over alterna-
tives, owing to its coarse-grained sense definitions.
3Defined as f having index i < k when foreign words are
ranked by most probable given e, where k is the minimum value
such that
?k
i p(fi | e) > 0.8. E.g., if we have decreasing
probabilities p(droit | duty) = 0.6, p(devoir | duty) =
0.25, p(le | duty) = 0.03, ... then only consider droit and
devoir. This ruled out many noisy alignments.
4Thus, the instances of droit compared to that of devoir
would be 0.6/0.25.
622
number of times English word e is aligned to foreign
word f ; c(es, f) returns the number of times En-
glish word e has sense s (tagged by Turkers), when
aligned to foreign word f ; c(e) is the total number
of tokens of English word e; and c(es) is the number
of tokens of e with sense s.
We estimate from labeled data the probability of
three scenarios, with scenario 1 as our primary con-
cern: when two English words of the same poly-
semous type are aligned to different foreign word
types, what is the chance that they have the same
sense? Given the tokens e1 and e2, we calculate P1
as follows:
P1e = P (s(e1) = s(e2) | a(e1) 6= a(e2))
?
?
s c
2(es)?
?
s,f c
2(es, f)
c2(e)?
?
f c2(e, f)
P1 says that given two words of the same type
(e1 and e2) that are not aligned to the same foreign
word type (a(e1) 6= a(e2)), what is the probabil-
ity that they have the same sense (s(e1) = s(e2)).
We approach this estimation combinatorially. For
instance, the number of ways to choose two words
of the same type is
( 2
c(e)
)
? 12c
2(e) when c(e) is
large.
A large value of P1 would be in support of Syn-
onymy, as the two foreign aligned words of distinct
type would have the same meaning.
Scenario 2 asks: given two English words of
the same polysemous type and aligned to the same
words (a(e1) = a(e2)), what is the probability that
they have the same sense (s(e1) = s(e2))?
P2e = P (s(e1) = s(e2) | a(e1) = a(e2))
?
?
s,f c
2(es, f)
?
f c2(e, f)
Finally, what is the probability of two tokens of
the same polysemous type agreeing when alignment
information is not known (e.g., without a parallel
corpus)?
P3e = P (s(e1) = s(e2)) ?
?
s c
2(es)
c2(e)
All the above equations are given per English word
type e. In later sections we report the average values
over multiple word types and their counts.
4 Results
Turker Experiments To minimize errors from
Turkers, for every HIT we inserted one control
sentence taken from the example sentences of
OntoNotes. Turker results with either extremely low
finishing time (<10s), or average accuracy on con-
trol sentences lower than accuracy by chance, were
rejected. On average Turkers took 185 seconds to
map 10 sentences in a HIT to their OntoNotes def-
inition, receiving $0.10 per HIT. The total time for
annotating 5000 sentences was 22 hours.
Turkers had no knowledge about alignments: we
hid the aligned French/Chinese sentences from them
and these sentences were later processed to compute
P1/2/3 values. Two foreign tokens aligned with the
same source type correspond to two senses of the
same type. To give an estimate of alignment errors,
we manually examined 1/10 of all 5000 sampled
Chinese-English alignments at random and found
only 3 of them were wrong: all due to that English
content words were aligned to common Chinese
function words. This error rate is much lower than
that typically reported by alignment tools. The main
reason is explained in footnote 3: foreign words with
trivial alignment probability were removed before
calculating P1/2/3 values. Thus we believe the align-
ment was reliable.
Probability Estimation Table 2 gives the dis-
tribution of senses and word types in the sam-
pled words. Take the second numeric column of
French-English as an example: out of 50 words ran-
domly sampled, 9 have 2 distinct sense definitions
in OntoNotes. However, 17 of 50 unique word types
had exactly 2 distinct senses annotated, out of the
100 examples of a given word type: 17 words had
2 distinct senses observed. Of the 9 words with 2
official senses, on average 1.9 of those senses were
observed.
Table 3 and Figures 1 and 2 shows the result for
P1, P2 and P3 using the {French,Chinese}-English
corpora, calculated based on the majority vote of
three Turkers. High P2 values suggests that for two
tokens of the same type, aligning to the same for-
eign type is a reasonable indicator of having the
same meaning. When working with open domain
corpora, without foreign alignments, the probabil-
ity of two English words of the same type having
623
French-English Chinese-English
#senses in OntoNotes 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 18
#types in OntoNotes 0 9 7 6 8 6 2 8 4 0 10 6 11 3 8 6 4 1 1
#types observed 2 17 9 4 7 7 4 0 0 3 19 9 12 5 2 0 0 0 0
avg #senses observed 0 1.9 2.1 3.2 3.8 4.7 6.5 4.9 5.8 0 1.9 2.2 2.9 2.7 4.4 3.8 3.8 3.0 5.0
Table 2: Statistics for words sampled from parallel corpora. Average #senses observed over all words: 2.6 (French-
English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.
P1 P2 P3 Alpha
French-English 51.2% 66.7% 59.2% 0.70
Chinese-English 59.6% 78.7% 66.7% 0.68
Table 3: Expectations of word sense in parallel corpora.
Alpha measures how Turkers agreed with themselves.
identical meaning is estimated here to be roughly
59-67% (59.2% (French), 66.7% (Chinese)). This
accords with results from WSD evaluations, where
the first-sense heuristic is roughly 75-80% accu-
rate (e.g., 80.9% in SemEval?07 (Brody and Lap-
ata, 2009)). Minor algebra translates this into an ex-
pected P3 value in a range from 56%?62.5%, up to
64%? 68%, which captures our estimates.5
Finally for our motivating scenario: values for P1
are barely higher than 50%, suggesting that Syn-
onymy more regularly holds, but not conclusively.
We expect in narrower domains, where words have
less number of senses, this is more noticeable. As
suggested by Fig.s 1 and 2, less polysemous words
tend to have higher P values.
5 Conclusion
Curious as to the distinct threads of prior work based
on alternate assumptions of word sense and parallel
corpora, we derived empirical expectations on the
shared meaning of tokens of the same type appear-
ing in the same corpus. Our results suggest neither
the assumption of Polysemy nor Synonymy holds
significantly more often than the other, at least for
individual words (as opposed to phrases) and for the
open domain corpora used here. Further, we provide
an independent data point that supports earlier find-
ings as to the expected accuracy of the first sense
heuristic in word sense disambiguation.
5Assuming worst case: no two tokens that are not the first
sense ever match, and best case: any two tokens not the first
sense always match, then assuming first-sense accuracy of 0.8
gives a range on P3 of: (0.82, 0.82 + 0.22) = (0.64, 0.68).
Num.Senses.Observed
Prob
abilit
y
0.0
0.2
0.4
0.6
0.8
1.0
l
l
l
l
l
1 2 3 4 5 6 7
TypeP1 P2 P3
Figure 1: French-English values, by number of senses.
Num.Senses.Observed
Prob
abilit
y
0.2
0.4
0.6
0.8
1.0
l
l
1 2 3 4 5 6 7
TypeP1 P2 P3
Figure 2: Chinese-English values, by number of senses.
624
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction And Dis-
crimination Systems. In Proc. SemEval ?07.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Proc.
NAACL Workshop on CSLDAMT.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4).
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proc. EACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings Of The 2009
Workshop On Statistical Machine Translation. In
Proc. StatMT.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. Using
bilingual materials to develop word sense disambigua-
tion methods. In Proc. TMI.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proc. NAACL-Short.
Klaus H. Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
word sense disambiguation. In Proc. ACL.
Gabriel Parent and Maxine Eskenazi. 2010. Clustering
dictionary definitions using amazon mechanical turk.
In Proc. NAACL Workshop on CSLDAMT.
Anna Rumshisky. 2011. Crowdsourcing word sense def-
inition. In Proc. LAW V.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP.
625
Proceedings of NAACL-HLT 2013, pages 858?867,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Answer Extraction as Sequence Tagging with Tree Edit Distance
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Our goal is to extract answers from pre-
retrieved sentences for Question Answering
(QA). We construct a linear-chain Conditional
Random Field based on pairs of questions
and their possible answer sentences, learning
the association between questions and answer
types. This casts answer extraction as an an-
swer sequence tagging problem for the first
time, where knowledge of shared structure be-
tween question and source sentence is incor-
porated through features based on Tree Edit
Distance (TED). Our model is free of man-
ually created question and answer templates,
fast to run (processing 200 QA pairs per sec-
ond excluding parsing time), and yields an F1
of 63.3% on a new public dataset based on
prior TREC QA evaluations. The developed
system is open-source, and includes an imple-
mentation of the TED model that is state of the
art in the task of ranking QA pairs.
1 Introduction
The success of IBM?s Watson system for Question
Answering (QA) (Ferrucci et al, 2010) has illus-
trated a continued public interest in this topic. Wat-
son is a sophisticated piece of software engineering
consisting of many components tied together in a
large parallel architecture. It took many researchers
working full time for years to construct. Such re-
sources are not available to individual academic re-
searchers. If they are interested in evaluating new
ideas on some aspect of QA, they must either con-
struct a full system, or create a focused subtask
?Performed while faculty at Johns Hopkins University.
paired with a representative dataset. We follow the
latter approach and focus on the task of answer ex-
traction, i.e., producing the exact answer strings for
a question.
We propose the use of a linear-chain Conditional
Random Field (CRF) (Lafferty et al, 2001) in or-
der to cast the problem as one of sequence tagging
by labeling each token in a candidate sentence as ei-
ther Beginning, Inside or Outside (BIO) of an an-
swer. This is to our knowledge the first time a
CRF has been used to extract answers.1 We uti-
lize not only traditional contextual features based on
POS tagging, dependency parsing and Named Entity
Recognition (NER), but most importantly, features
extracted from a Tree Edit Distance (TED) model
for aligning an answer sentence tree with the ques-
tion tree. The linear-chain CRF, when trained to
learn the associations between question and answer
types, is a robust approach against error propaga-
tion introduced in the NLP pipeline. For instance,
given an NER tool that always (i.e., in both train-
ing and test data) recognizes the pesticide DDT as
an ORG, our model realizes, when a question is
asked about the type of chemicals, the correct an-
swer might be incorrectly but consistently recog-
nized as ORG by NER. This helps reduce errors in-
troduced by wrong answer types, which were esti-
mated as the most significant contributer (36.4%)
of errors in the then state-of-the-art QA system of
Moldovan et al (2003).
The features based on TED allow us to draw the
1CRFs have been used in judging answer-bearing sentences
(Shima et al, 2008; Ding et al, 2008; Wang and Manning,
2010), but not extracting exact answers from these sentences.
858
connection between the question and answer sen-
tences before answer extraction, whereas tradition-
ally the exercise of answer validation (Magnini et
al., 2002; Penas et al, 2008; Rodrigo et al, 2009)
has been performed after as a remedy to ensure the
answer is really ?about? the question.
Motivated by a desire for a fast runtime,2 we
base our TED implementation on the dynamic-
programming approach of Zhang and Shasha
(1989), which helps our final system process 200
QA pairs per second on standard desktop hardware,
when input is syntactically pre-parsed.
In the following we first provide background on
the TED model, going on to evaluate our implemen-
tation against prior work in the context of question
answer sentence ranking (QASR), achieving state of
the art in that task. We then describe how we cou-
ple TED features to a linear-chain CRF for answer
extraction, providing the set of features used, and fi-
nally experimental results on an extraction dataset
we make public (together with the software) to the
community.3 Related prior work is interspersed
throughout the paper.
2 Tree Edit Distance Model
Tree Edit Distance (?2.1) models have been shown
effective in a variety of applications, including tex-
tual entailment, paraphrase identification, answer
ranking and information retrieval (Reis et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Augsten et al, 2010). We chose the variant
proposed by Heilman and Smith (2010), inspired by
its simplicity, generality, and effectiveness. Our ap-
proach differs from those authors in their reliance
on a greedy search routine to make use of a complex
tree kernel. With speed a consideration, we opted
for the dynamic-programming solution of Zhang
and Shasha (1989) (?2.1). We added new lexical-
semantic features ?(2.2) to the model and then eval-
uated our implementation on the QASR task, show-
ing strong results ?(2.3).
Feature Description
distance tree edit distance from answer
sentence to question
renNoun
renVerb
renOther
# edits changing POS from or to
noun, verb, or other types
insN, insV,
insPunc,
insDet,
insOtherPos
# edits inserting a noun, verb,
punctuation mark, determiner
or other POS types
delN, delV, ... deletion mirror of above
ins{N,V,P}Mod
insSub, insObj
insOtherRel
# edits inserting a modifier for
{noun, verb, preposition}, sub-
ject, object or other relations
delNMod, ... deletion mirror of above
renNMod, ... rename mirror of above
XEdits # basic edits plus sum of in-
s/del/ren edits
alignNodes,
alignNum,
alignN, alignV,
alignProper
# aligned nodes, and those that
are numbers, nouns, verbs, or
proper nouns
Table 1: Features for ranking QA pairs.
2.1 Cost Design and Edit Search
Following Bille (2005), we define an edit script be-
tween trees T1, T2 as the edit sequence transforming
T1 to T2 according to a cost function, with the total
summed cost known as the tree edit distance. Basic
edit operations include: insert, delete and rename.
With T a dependency tree, we represent each node
by three fields: lemma, POS and the type of depen-
dency relation to the node?s parent (DEP). For in-
stance, Mary/nnp/sub is the proper noun Mary in
subject position.
Basic edits are refined into 9 types, where the
first six (INS LEAF, INS SUBTREE, INS, DEL LEAF,
DEL SUBTREE, DEL) insert or delete a leaf node, a
whole subtree, or a node that is neither a leaf nor
part of a whole inserted subtree. The last three
(REN POS, REN DEP, REN POS DEP) serve to re-
name a POS tag, dependency relation, or both.
2For instance, Watson was designed under the constraint of
a 3 second response time, arising from its intended live use in
the television gameshow, Jeopardy!.
3http://code.google.com/p/jacana/
859
prd
playernn jennifernn
capriatinnp 23cd
bevbzsubj
nmod nmod
tennisnn
nmod
Tennis player Jennifer Capriati is 23
TreeEdit Distance capriatinnp
jennifernnp
whatwp
sportnn
playvbz
dovbzvmod vmod
nmod
What sport does Jennifer Capriati play
insSubTree:
ins(play/vbz/vmod)ins(do/vbz/root)
WordNet
Figure 1: Edits transforming a source sentence (left) to a question (right). Each node consists of: lemma, POS tag
and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (? and strikethrough
on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED
model does not capture the alignment between tennis and sport (see Section 2.2).
We begin by uniformly assigning basic edits a
cost of 1.0,4 which brings the cost of a full node in-
sertion or deletion to 3 (all the three fields inserted or
deleted). We allow renaming of POS and/or relation
type iff the lemmas of source and target nodes are
identical.5 When two nodes are identical and thus
do not appear in the edit script, or when two nodes
are renamed due to the same lemma, we say they are
aligned by the tree edit model (see Figure 1).
We used Zhang and Shasha (1989)?s dynamic
programming algorithm to produce an optimal edit
script with the lowest tree edit distance. The ap-
proach explores both trees in a bottom-up, post-
order manner, running in time:
O(|T1| |T2|min(D1, L1)min(D2, L2))
where |Ti| is the number of nodes, Di is the depth,
and Li is the number of leaves, with respect to tree
Ti.
Additionally, we fix the cost of stopword renam-
ing to 2.5, even in the case of identity, regardless
of whether two stopwords have the same POS tags
or relations. Stopwords tend to have fixed POS tags
and dependency relations, which often leads to less
expensive alignments as compared to renaming con-
4This applies separately to each element of the tripartite
structure; e.g., deleting a POS entry, inserting a lemma, etc.
5This is aimed at minimizing node variations introduced by
morphology differences, tagging or parsing errors.
tent terms. In practice this gave stopwords ?too
much say? in guiding the overall edit sequence.
The resultant system is fast in practice, processing
10,000 pre-parsed tree pairs per second on a contem-
porary machine.6
2.2 TED for Sentence Ranking
The task of Question Answer Sentence Ranking
(QASR) takes a question and a set of source sen-
tences, returning a list sorted by the probability
likelihood that each sentence contains an appropri-
ate answer. Prior work in this includes that of:
Punyakanok et al (2004), based on mapping syn-
tactic dependency trees; Wang et al (2007) utiliz-
ing Quasi-Synchronous Grammar (Smith and Eis-
ner, 2006); Heilman and Smith (2010) using TED;
and Shima et al (2008), Ding et al (2008) and Wang
and Manning (2010), who each employed a CRF in
various ways. Wang et al (2007) made their dataset
public, which we use here for system validation. To
date, models based on TED have shown the best per-
formance for this task.
Our implementation follows Heilman and Smith
(2010), with the addition of 15 new features beyond
their original 33 (see Table 1). Based on results
6In later tasks, feature extraction and decoding will slow
down the system, but the final system was still able to process
200 pairs per second.
860
set source #ques. #pairs %pos. len.
TRAIN-ALL TREC8-12 1229 53417 12.0 any
TRAIN TREC8-12 94 4718 7.4 ? 40
DEV TREC13 82 1148 19.3 ? 40
TEST TREC13 89 1517 18.7 ? 40
Table 2: Distribution of data, with imbalance towards
negative examples (sentences without an answer).
in DEV, we extract edits in the direction from the
source sentence to the question.
In addition to syntactic features, we incorporated
the following lexical-semantic relations from Word-
Net: hypernym and synonym (nouns and verbs); en-
tailment and causing (verbs); and membersOf, sub-
stancesOf, partsOf, haveMember, haveSubstance,
havePart (nouns). Such relations have been used
in prior approaches to this task (Wang et al, 2007;
Wang and Manning, 2010), but not in conjunction
with the model of Heilman and Smith (2010).
These were made into features in two ways:
WNsearch loosens renaming and alignment within
the TED model from requiring strict lemma equal-
ity to allowing lemmas that shared any of the
above relations, leading to renaming operations such
as REN ...(country, china) and REN ...(sport,
tennis); WNfeature counts how many words be-
tween the sentence and answer sentence have each
of the above relations, separately as 10 independent
features, plus an aggregate count for a total of 11
new features beyond the earlier 48.
These features were then used to train a logistic
regression model using Weka (Hall et al, 2009).
2.3 QA Sentence Ranking Experiment
We trained and tested on the dataset from Wang et
al. (2007), which spans QA pairs from TREC QA
8-13 (see Table 2). Per question, sentences with
non-stopword overlap were first retrieved from the
task collection, which were then compared against
the TREC answer pattern (in the form of Perl regu-
lar expressions). If a sentence matched, then it was
deemed a (noisy) positive example. Finally, TRAIN,
DEV and TEST were manually corrected for errors.
Those authors decided to limit candidate source sen-
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
this paper (48 features) 0.6319 0.7270
+WNsearch 0.6371 0.7301
+WNfeature (11 more feat.) 0.6307 0.7477
Table 3: Results on the QA Sentence Ranking task.
tences to be no longer than 40 words.7 Keeping
with prior work, those questions with only positive
or negative examples were removed, leaving 94 of
the original 100 questions for evaluation.
The data was processed by Wang et al (2007)
with the following tool chain: POS tags via MX-
POST (Ratnaparkhi, 1996); parse trees via MST-
Parser (McDonald et al, 2005) with 12 coarse-
grained dependency relation labels; and named enti-
ties via Identifinder (Bikel et al, 1999). Mean Av-
erage Precision (MAP) and Mean Reciprocal Rank
(MRR) are reported in Table 3. Our implementa-
tion gives state of the art performance, and is fur-
thered improved by our inclusion of semantic fea-
tures drawn from WordNet.8
3 CRF with TED for Answer Extraction
In this section we move from ranking source sen-
tences, to the next QA stage: answer extraction.
Given our competitive TED-based alignment model,
the most obvious solution to extraction would be to
report those spans aligned from a source sentence
to a question?s wh- terms. However, we show that
this approach is better formulated as a (strongly in-
dicative) feature of a larger set of answer extraction
signals.
3.1 Sequence Model
Figure 2 illustrates the task of tagging each token in
a candidate sentence with one of the following la-
7TRAIN-ALL is not used in QASR, but later for answer ex-
traction; TRAIN comes from the first 100 questions of TRAIN-
ALL.
8As the test set is of limited size (94 questions), then while
our MAP/MRR scores are 2.8% ? 5.6% higher than prior
work, this is not statistically significant according to the Paired
Randomization Test (Smucker et al, 2007), and thus should be
considered on par with the current state of the art.
861
prddla yenjri frddlcri tnyiln2l la 3b
vzsum o o o o o
Figure 2: An example of linear-chain CRF for an-
swer sequence tagging.
bels: B-ANSWER (beginning of answer), I-ANSWER
(inside of answer), O (outside of answer).
Besides local POS/NER/DEP features, at each to-
ken we need to inspect the entire input to connect the
answer sentence with the question sentence through
tree edits, drawing features from the question and
the edit script, motivating the use of a linear-chain
CRF model (Lafferty et al, 2001) over HMMs. To
the best of our knowledge this is the first time a
CRF has been used to label answer fragments, de-
spite success in other sequence tagging tasks.
3.2 Feature Design
In this subsection we describe the local and global
features used by the CRF.
Chunking We use the POS/NER/DEP tags directly
just as one would in a chunking task. Specifically,
suppose t represents the current token position and
pos[t] its POS tag, we extract unigram, bigram and
trigram features over the local context, e.g., pos[t?
2], pos[t ? 2] : pos[t ? 1], and pos[t ? 2] : pos[t ?
1] : pos[t]. Similar features are extracted for named
entity types (ner[t]), and dependency relation labels
(dep[t]).
Our intuition is these chunking features should al-
low for learning which types of words tend to be
answers. For instance, we expect adverbs to be as-
signed lower feature weights as they are rarely a
part of answer, while prepositions may have differ-
ent feature weights depending on their context. For
instance, of in kind of silly has an adjective on the
right, and is unlikely to be the Beginning of an an-
swer to a TREC-style question, as compared to in
when paired with a question on time, such as seen in
an answer in 90 days, where the preposition is fol-
lowed by a number then a noun.
Feature Description
edit=X type of edit feature. X: DEL,
DEL SUBTREE, DEL LEAF,
REN POS, REN DEP, REN POS DEP
or ALIGN.
X pos=?
X ner=?
X dep=?
Delete features. X is either DEL,
DEL SUBTREE or DEL LEAF. ?
represents the corresponding
POS/NER/DEP of the current token.
Xpos from=?f
Xpos to=?t
Xpos f t=?f ?t
Xner from=?f
Xner to=?t
Xner f t=?f ?t
Xdep from=?f
Xdep to=?t
Xdep f t=?f ?t
Rename features. X is either
REN POS, REN DEP or
REN POS DEP. Suppose word f in
answer is renamed to word t in
question, then ?f and ?t represent
corresponding POS/NER/DEP of f
and t.
align pos=?
align ner=?
align dep=?
Align features. ? represents the
corresponding POS/NER/DEP of the
current token.
Table 4: Features based on edit script for answer se-
quence tagging.
Question-type Chunking features do not capture
the connection between question word and an-
swer types. Thus they have to be combined
with question types. For instance, how many
questions are usually associated with numeric an-
swer types. We encode each major question-
type: who, whom, when, where, how many, how
much, how long, and then for each token, we
combine the question term with its chunking fea-
tures described in (most tokens have different fea-
tures because they have different POS/NER/DEP
types). One feature example of the QA pair
how much/100 dollars for the word 100 would be:
qword=how much|pos[t]=CD|pos[t+1]=NNS. We ex-
pect high weight for this feature since it is a good
pattern for matching question type and answer type.
Similar features also apply to what, which, why and
how questions, even though they do not indicate an
answer type as clearly as how much does.
Some extra features are designed for what/which
questions per required answer types. The question
862
dependency tree is analyzed and the Lexical Answer
Type (LAT) is extracted. The following are some
examples of LAT for what questions:
? color: what is Crips? gang color?
? animal: what kind of animal is an agouti?
The extra LAT=? feature is also used with chunking
features for what/which questions.
There is significant prior work in building spe-
cialized templates or classifiers for labeling question
types (Hermjakob, 2001; Li and Roth, 2002; Zhang
and Lee, 2003; Hacioglu and Ward, 2003; Metzler
and Croft, 2005; Blunsom et al, 2006; Moschitti
et al, 2007). We designed our shallow question
type features based on the intuitions of these prior
work, with the goal of having a relatively compact
approach that still extracts useful predictive signal.
One possible drawback, however, is that if an LAT is
not observed during training but shows up in testing,
the sequence tagger would not know which answer
type to associate with the question. In this case it
falls back to the more general qword=? feature and
will most likely pick the type of answers that are
mostly associated with what questions in training.
Edit script Our TED module produces an edit
trace for each word in a candidate sentence: the
word is either deleted, renamed (if there is a word
of the same lemma in the question tree) or strictly
aligned (if there is an identical node in the question
tree). A word in the deleted edit sequence is a cue
that it could be the answer. A word being aligned
suggests it is less likely to be an answer. Thus for
each word we extract features based on its edit type,
shown in Table 4.
These features are also appended with the token?s
POS/NER/DEP information. For instance, a deleted
noun usually carries higher edit feature weights than
an aligned adjective.
Alignment distance We observed that a candidate
answer often appears close to an aligned word (i.e.,
answer tokens tend to be located ?nearby? portions
of text that align across the pair), especially in com-
pound noun constructions, restrictive clauses, prepo-
sition phrases, etc. For instance, in the following
pair, the answer Limp Bizkit comes from the leading
compound noun:
? What is the name of Durst ?s group?
? Limp Bizkit lead singer Fred Durst did a lot ...
Past work has designed large numbers of specific
templates aimed at these constructions (Soubbotin,
2001; Ravichandran et al, 2003; Clark et al, 2003;
Sneiders, 2002). Here we use a single general fea-
ture that we expect to pick up much of this signal,
without the significant feature engineering.
Thus we incorporated a simple feature to roughly
model this phenomenon. It is defined as the distance
to the nearest aligned nonstop word in the original
word order. In the above example, the only aligned
nonstop word is Durst. Then this nearest alignment
distance feature for the word Limp is:
nearest dist to align(Limp):5
This is the only integer-valued feature. All other
features are binary-valued. Note this feature does
not specify answer types: an adverb close to an
aligned word can also be wrongly taken as a strong
candidate. Thus we also include a version of the
POS/NER/DEP based feature for each token:
? nearest dist pos(Limp)=NNP
? nearest dist dep(Limp)=NMOD
? nearest dist ner(Limp)=B-PERSON
3.3 Overproduce-and-vote
We make an assumption that each sentence produces
a candidate answer and then vote among all answer
candidates to select the most-voted as the answer to
the original question. Specifically, this overproduce-
and-vote strategy applies voting in two places:
1. If there are overlaps between two answer candi-
dates, a partial vote is performed. For instance,
for a when question, if one answer candidate is
April , 1994 and the other is 1994, then besides
the base vote of 1, both candidates have an ex-
tra partial vote of #overlap/#total words = 1/4. We
call this adjusted vote.
2. If the CRF fails to find an answer, we still try to
?force? an answer out of the tagged sequence,
O?s). thus forced vote. Due to its lower credi-
bility (the sequence tagger does not think it is
an answer), we manually downweight the pre-
diction score by a factor of 0.1 (divide by 10).
863
During what war d id Nimi tz serve ?
O O:0.921060 Conant
O O:0.991168 had
O O:0.997307 been
O O:0.998570 a
O O:0.998608 photographer
O O:0.999005 f o r
O O:0.877619 Adm
O O:0.988293 .
O O:0.874101 Chester
O O:0.924568 Nimi tz
O O:0.970045 dur ing
B?ANS O:0.464799 World
I?ANS O:0.493715 War
I?ANS O:0.449017 I I
O O:0.915448 .
Figure 3: A sample sequence tagging output that
fails to predict an answer. From line 2 on, the first
column is the reference output and the second col-
umn is the model output with the marginal probabil-
ity for predicated labels. Note that World War II has
much lower probabilities as an O than others.
The modified score for an answer candidate is thus:
total vote = adjusted vote + 0.1 ? forced vote. To
compute forced vote, we make the following obser-
vation. Sometimes the sequence tagger does not tag
an answer in a candidate sentence at all, if there
is not enough probability mass accumulated for B-
ANS. However, a possible answer can still be caught
if it has an ?outlier? marginal probability. Figure 3
shows an example. The answer candidate World War
II has a much lower marginal probability as an ?O?
but still not low enough to be part of B-ANS/I-ANS.
To catch such an outlier, we use Median Absolute
Deviation (MAD), which is the median of the abso-
lute deviation from the median of a data sequence.
Given a data sequence x, MAD is defined as:
MAD(x) = median(| x?median(x) |)
Compared to mean value and standard deviation,
MAD is more robust against the influence of out-
liers since it does not directly depend on them. We
select those words whose marginal probability is 50
times of MAD away from the median of the whole
sequence as answer candidates. They contribute to
the forced vote. Downweight ratio (0.1) and MAD
System Train Prec.% Rec.% F1%
CRF
TRAIN 55.7 43.8 49.1
TRAIN-ALL 67.2 50.6 57.7
CRF
+WNsearch
TRAIN 58.6 46.1 51.6
TRAIN-ALL 66.7 49.4 56.8
CRF forced
TRAIN 54.5 53.9 54.2
TRAIN-ALL 60.9 59.6 60.2
CRF forced
+WNsearch
TRAIN 55.2 53.9 54.5
TRAIN-ALL 63.6 62.9 63.3
Table 5: Performance on TEST. ?CRF? only takes
votes from candidates tagged by the sequence tag-
ger. ?CRF forced? (described in ?3.3) further col-
lects answer candidates from sentences that CRF
does not tag an answer by detecting outliers.
ratio (50) were hand-tuned on DEV.9
4 Experiments
4.1 QA Results
The dataset listed in Table 2 was not designed to
include an answer for each positive answer sen-
tence, but only a binary indicator on whether a sen-
tence contains an answer. We used the answer pat-
tern files (in Perl regular expressions) released along
with TREC8-13 to pinpoint the exact answer frag-
ments. Then we manually checked TRAIN, DEV, and
TEST for errors. TRAIN-ALL already came as a noisy
dataset so we did not manually clean it, also due to
its large size.
We trained on only the positive examples of
TRAIN and TRAIN-ALL separately with CRFsuite
(Okazaki, 2007). The reason for training solely with
positive examples is that they only constitute 10% of
all training data and if trained on all, the CRF tagger
was very biased on negative examples and reluctant
to give an answer for most of the questions. The
CRF tagger attempted an answer for about 2/3 of all
questions when training on just positive examples.
DEV was used to help design features. A practi-
cal benefit of our compact approach is that an entire
round of feature extraction, training on TRAIN and
testing on DEV took less than one minute. Table 5
9One might further improve this by leveraging the probabil-
ity of a sentence containing an answer from the QA pair ranker
described in Section 2 or via the conditional probability of the
sequence labels, p(y | x), under the CRF.
864
reports F1 scores on both the positive and negative
examples of TEST.
Our baseline model, which aligns the question
word with some content word in the answer sen-
tence,10 achieves 31.4% in F1. This model does not
require any training. ?CRF? only takes votes from
those sentences with an identified answer. It has the
best precision among all models. ?CRF forced? also
detects outliers from sentences not tagged with an
answer. Large amount of training data, even noisy,
is helpful. In general TRAIN-ALL is able to boost the
F1 value by 7 ? 8%. Also, the overgenerate-and-
vote strategy, used by the ?forced? approach, greatly
increased recall and achieved the best F1 value.
We also experimented with the two methods uti-
lizing WordNet in Section 2.2 , i.e., WNsearch and
WNfeature. In general, WNsearch helps F1 and
yields the best score (63.3%) for this task. For
WNfeature11 we observed that the CRF model con-
verged to a larger objective likelihood with these
features. However, it did not make a difference in
F1 after overgenerate-and-vote.
Finally, we found it difficult to do a head-to-head
comparison with other QA systems on this task.12
Thus we contribute this dataset to the community,
hoping to solicit direct comparisons in the future.
Also, we believe our chunking and question-type
features capture many intuitions most current QA
systems rely on, while our novel features are based
on TED. We further conduct an ablation test to com-
pare traditional and new QA features.
4.2 Ablation Test
We did an ablation test for each of the four types of
features. Note that the question type features are
used in combination with chunking features (e.g.,
qword=how much|pos[t]=CD|pos[t+1]=NN), while
the chunking feature is defined over POS/NER/DEP
10This only requires minimal modification to the original
TED algorithm: the question word is aligned with a certain
word in the answer tree instead of being inserted. Then the
whole subtree headed by the aligned word counts as the answer.
11These are binary features indicating whether an answer
candidate has a WordNet relation ( c.f. ?2.2) with the LAT.
For instance, tennis is a hyponym of the LAT word sport in the
what sport question in Figure 1.
12Reasons include: most available QA systems either retrieve
sentences from the web, have different preprocessing steps, or
even include templates learned from our test set.
CRF Forced CRF Forced
All 49.1 54.2 -above 3 19.4 25.3
-POS 44.7 48.9 -EDIT 44.3 47.5
-NER 44.0 50.8 -ALIGN 47.4 51.1
-DEP 49.4 54.5 -above 2 40.5 42.0
Table 6: F1 based on feature ablation tests.
NONE CHUNKING CHUNKING+TEDFeatures Used
0
10
20
30
40
50
60
F1(%
) 31.4
40.5
49.1
42.0
54.2
F1 with Different Features
BaselineCRFCRF forced
Figure 4: Impact of adding features based on chunk-
ing and question-type (CHUNKING) and tree edits
(TED), e.g., EDIT and ALIGN.
separately. We tested the CRF model with deletion
of one of the following features each time:
? POS, NER or DEP. These features are all com-
bined with question types.
? The three of the above. Deletion of these fea-
tures also deletes question type feature implic-
itly.
? EDIT. Features extracted from edit script.
? ALIGN. Alignment distance features.
? The two of the above, based on the TED model.
Table 6 shows the F1 scores of ablation test when
trained on TRAIN. NER and EDIT are the two single
most significant features. NER is important because
it closely relates question types with answer entity
types (e.g., qword=who|ner[t]=PERSON). EDIT is
also important because it captures the syntactic asso-
ciation between question tree and answer tree. Tak-
ing out all three POS/NER/DEP features means the
chunking and question type features do not fire any-
more. This has the biggest impact on F1. Note the
feature redundancy here: the question type features
are combined with all three POS/NER/DEP features
865
thus taking out a single one does not decrease per-
formance much. However, since TED related fea-
tures do not combine question type features, taking
out all three POS/NER/DEP features decreases F1 by
30%. Without TED related features (both EDIT and
ALIGN) F1 also drops more than 10%.
Figure 4 is a bar chart showing how much im-
provement each feature brings. While having a
baseline model with 31.4% in F1, traditional fea-
tures based on POS/DEP/NER and question types
brings a 10% increase with a simple sequence tag-
ging model (second bar labeled ?CHUNKING? in
the figure). Furthermore, adding TED based features
to the model boosted F1 by another 10%.
5 Conclusion
Answer extraction is an essential task for any text-
based question-answering system to perform. In this
paper, we have cast answer extraction as a sequence
tagging problem by deploying a fast and compact
CRF model with simple features that capture many
of the intuitions in prior ?deep pipeline? approaches.
We introduced novel features based on TED that
boosted F1 score by 10% compared with the use of
more standard features. Besides answer extraction,
our modified design of the TED model is the state
of the art in the task of ranking QA pairs. Finally,
to improve the community?s ability to evaluate QA
components without requiring increasingly imprac-
tical end-to-end implementations, we have proposed
answer extraction as a subtask worth evaluating in
its own right, and contributed a dataset that could
become a potential standard for this purpose. We
believe all these developments will contribute to the
continuing improvement of QA systems in the fu-
ture.
Acknowledgement We thank Vulcan Inc. for
funding this work. We also thank Michael Heil-
man and Mengqiu Wang for helpful discussion and
dataset, and the three anonymous reviewers for in-
sightful comments.
References
Nikolaus Augsten, Denilson Barbosa, Michael Bo?hlen,
and Themis Palpanas. 2010. TASM: Top-k Approx-
imate Subtree Matching. In Proceedings of the Inter-
national Conference on Data Engineering (ICDE-10),
pages 353?364, Long Beach, California, USA, March.
IEEE Computer Society.
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
learning, 34(1):211?231.
P. Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1):217?239.
P. Blunsom, K. Kocik, and J.R. Curran. 2006. Question
classification with log-linear models. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 615?616. ACM.
Peter Clark, Vinay Chaudhri, Sunil Mishra, Je?ro?me
Thome?re?, Ken Barker, and Bruce Porter. 2003. En-
abling domain experts to convey questions to a ma-
chine: a modified, template-based approach. In
Proceedings of the 2nd international conference on
Knowledge Capture, pages 13?19, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
In Proceedings of ACL-08: HLT.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek,
A.A. Kalyanpur, A. Lally, J.W. Murdock, E. Nyberg,
J. Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI Magazine, 31(3):59?79.
K. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In Proceedings of NAACL 2003, short papers,
pages 28?30.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia.
U. Hermjakob. 2001. Parsing and question classification
for question answering. In Proceedings of the work-
shop on Open-domain question answering-Volume 12,
pages 1?6.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
866
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL 2002, pages 1?7.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer?: exploiting web redundancy for
answer validation. In Proceedings of ACL 2002, pages
425?432.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL 2005, pages 91?98.
D. Metzler and W.B. Croft. 2005. Analysis of statistical
question classification for fact-based questions. Infor-
mation Retrieval, 8(3):481?504.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Surdeanu.
2003. Performance issues and error analysis in an
open-domain question answering system. ACM Trans-
actions on Information Systems (TOIS), 21(2):133?
154.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.
2007. Exploiting syntactic and shallow semantic ker-
nels for question answer classification. In Proceedings
of ACL 2007, volume 45, page 776.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
A. Penas, A. Rodrigo, V. Sama, and F. Verdejo. 2008.
Testing the reasoning for question answering valida-
tion. Journal of Logic and Computation, 18(3):459?
474.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP
1996, volume 1, pages 133?142.
Deepak Ravichandran, Abraham Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of NAACL 2003, short
papers, pages 85?87, Stroudsburg, PA, USA.
D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laen-
der. 2004. Automatic web news extraction using tree
edit distance. In Proceedings of the 13th international
conference on World Wide Web, pages 502?511, New
York, NY, USA. ACM.
A?. Rodrigo, A. Pen?as, and F. Verdejo. 2009. Overview of
the answer validation exercise 2008. Evaluating Sys-
tems for Multilingual and Multimodal Information Ac-
cess, pages 296?313.
H. Shima, N. Lao, E. Nyberg, and T. Mitamura. 2008.
Complex cross-lingual question answering as sequen-
tial classification and multi-document summarization
task. In Proceedings of NTICIR-7 Workshop, Japan.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30, New York, June.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In CIKM ?07:
Proceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 623?632, New York, NY, USA. ACM.
E. Sneiders. 2002. Automated question answering:
template-based approach. Ph.D. thesis, KTH.
Martin M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proceed-
ings of the Tenth Text REtrieval Conference (TREC
2001).
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of ACL 2010, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June.
D. Zhang and W.S. Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related prob-
lems. SIAM J. Comput., 18(6):1245?1262, December.
867
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Coupling of Answer Extraction and Information Retrieval
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Information Retrieval (IR) and Answer
Extraction are often designed as isolated
or loosely connected components in Ques-
tion Answering (QA), with repeated over-
engineering on IR, and not necessarily per-
formance gain for QA. We propose to
tightly integrate them by coupling auto-
matically learned features for answer ex-
traction to a shallow-structured IR model.
Our method is very quick to implement,
and significantly improves IR for QA
(measured in Mean Average Precision and
Mean Reciprocal Rank) by 10%-20%
against an uncoupled retrieval baseline
in both document and passage retrieval,
which further leads to a downstream 20%
improvement in QA F1.
1 Introduction
The overall performance of a Question Answer-
ing system is bounded by its Information Re-
trieval (IR) front end, resulting in research specif-
ically on Information Retrieval for Question An-
swering (IR4QA) (Greenwood, 2008; Sakai et al,
2010). Common approaches such as query expan-
sion, structured retrieval, and translation models
show patterns of complicated engineering on the
IR side, or isolate the upstream passage retrieval
from downstream answer extraction. We argue
that: 1. an IR front end should deliver exactly
what a QA1 back end needs; 2. many intuitions
employed by QA should be and can be re-used in
IR, rather than re-invented. We propose a coupled
retrieval method with prior knowledge of its down-
stream QA component, that feeds QA with exactly
the information needed.
1After this point in the paper we use the term QA in a
narrow sense: QA without the IR component, i.e., answer
extraction.
As a motivating example, using the ques-
tion When was Alaska purchased from
the TREC 2002 QA track as the query to the In-
dri search engine, the top sentence retrieved from
the accompanying AQUAINT corpus is:
Eventually Alaska Airlines will
allow all travelers who have
purchased electronic tickets
through any means.
While this relates Alaska and purchased, it
is not a useful passage for the given question.2 It
is apparent that the question asks for a date. Prior
work proposed predictive annotation (Prager et al,
2000; Prager et al, 2006): text is first annotated in
a predictive manner (of what types of questions it
might answer) with 20 answer types and then in-
dexed. A question analysis component (consisting
of 400 question templates) maps the desired an-
swer type to one of the 20 existing answer types.
Retrieval is then performed with both the question
and predicated answer types in the query.
However, predictive annotation has the limita-
tion of being labor intensive and assuming the un-
derlying NLP pipeline to be accurate. We avoid
these limitations by directly asking the down-
stream QA system for the information about which
entities answer which questions, via two steps:
1. reusing the question analysis components from
QA; 2. forming a query based on the most relevant
answer features given a question from the learned
QA model. There is no query-time overhead and
no manual template creation. Moreover, this ap-
proach is more robust against, e.g., entity recog-
nition errors, because answer typing knowledge is
learned from how the data was actually labeled,
not from how the data was assumed to be labeled
(e.g., manual templates usually assume perfect la-
beling of named entities, but often it is not the case
2Based on a non-optimized IR configuration, none of the
top 1000 returned passages contained the correct answer:
1867.
159
in practice).
We use our statistically-trained QA system (Yao
et al, 2013) that recognizes the association be-
tween question type and expected answer types
through various features. The QA system employs
a linear chain Conditional Random Field (CRF)
(Lafferty et al, 2001) and tags each token as either
an answer (ANS) or not (O). This will be our off-
the-shelf QA system, which recognizes the associ-
ation between question type and expected answer
types through various features based on e.g., part-
of-speech tagging (POS) and named entity recog-
nition (NER).
With weights optimized by CRF training (Ta-
ble 1), we can learn how answer features are cor-
related with question features. These features,
whose weights are optimized by the CRF train-
ing, directly reflect what the most important an-
swer types associated with each question type are.
For instance, line 2 in Table 1 says that if there is a
when question, and the current token?s NER label
is DATE, then it is likely that this token is tagged
as ANS. IR can easily make use of this knowledge:
for a when question, IR retrieves sentences with
tokens labeled as DATE by NER, or POS tagged as
CD. The only extra processing is to pre-tag and
index the text with POS and NER labels. The ana-
lyzing power of discriminative answer features for
IR comes for free from a trained QA system. Un-
like predictive annotation, statistical evidence de-
termines the best answer features given the ques-
tion, with no manual pattern or templates needed.
To compare again predictive annotation with
our approach: predictive annotation works in a
forward mode, downstream QA is tailored for up-
stream IR, i.e., QA works on whatever IR re-
trieves. Our method works in reverse (backward):
downstream QA dictates upstream IR, i.e., IR re-
trieves what QA wants. Moreover, our approach
extends easily beyond fixed answer types such as
named entities: we are already using POS tags as a
demonstration. We can potentially use any helpful
answer features in retrieval. For instance, if the
QA system learns that in order to is highly
correlated with why question through lexicalized
features, or some certain dependency relations are
helpful in answering questions with specific struc-
tures, then it is natural and easy for the IR compo-
nent to incorporate them.
There is also a distinction between our method
and the technique of learning to rank applied in
feature label weight
qword=when|POS0=CD ANS 0.86
qword=when|NER0=DATE ANS 0.79
qword=when|POS0=CD O -0.74
Table 1: Learned weights for sampled features with respect
to the label of current token (indexed by [0]) in a CRF. The
larger the weight, the more ?important? is this feature to help
tag the current token with the corresponding label. For in-
stance, line 1 says when answering a when question, and
the POS of current token is CD (cardinal number), it is likely
(large weight) that the token is tagged as ANS.
QA (Bilotti et al, 2010; Agarwal et al, 2012). Our
method is a QA-driven approach that provides su-
pervision for IR from a learned QA model, while
learning to rank is essentially an IR-driven ap-
proach: the supervision for IR comes from a la-
beled ranking list of retrieval results.
Overall, we make the following contributions:
? Our proposed method tightly integrates QA
with IR and the reuse of analysis from QA does
not put extra overhead on the IR queries. This
QA-driven approach provides a holistic solution
to the task of IR4QA.
? We learn statistical evidence about what the
form of answers to different questions look like,
rather than using manually authored templates.
This provides great flexibility in using answer
features in IR queries.
We give a full spectrum evaluation of all three
stages of IR+QA: document retrieval, passage re-
trieval and answer extraction, to examine thor-
oughly the effectiveness of the method.3 All of
our code and datasets are publicly available.4
2 Background
Besides Predictive Annotation, our work is closest
to structured retrieval, which covers techniques of
dependency path mapping (Lin and Pantel, 2001;
Cui et al, 2005; Kaisser, 2012), graph matching
with Semantic Role Labeling (Shen and Lapata,
2007) and answer type checking (Pinchak et al,
2009), etc. Specifically, Bilotti et al (2007) pro-
posed indexing text with their semantic roles and
named entities. Queries then include constraints
of semantic roles and named entities for the pred-
icate and its arguments in the question. Improve-
ments in recall of answer-bearing sentences were
shown over the bag-of-words baseline. Zhao and
3Rarely are all three aspects presented in concert (see ?2).
4http://code.google.com/p/jacana/
160
Callan (2008) extended this work with approx-
imate matching and smoothing. Most research
uses parsing to assign deep structures. Com-
pared to shallow (POS, NER) structured retrieval,
deep structures need more processing power and
smoothing, but might also be more precise. 5
Most of the above (except Kaisser (2012)) only
reported on IR or QA, but not both, assuming that
improvement in one naturally improves the other.
Bilotti and Nyberg (2008) challenged this assump-
tion and called for tighter coupling between IR and
QA. This paper is aimed at that challenge.
3 Method
Table 1 already shows some examples of features
associating question types with answer types. We
store the features and their learned weights from
the trained model for IR usage.
We let the trained QA system guide the query
formulation when performing coupled retrieval
with Indri (Strohman et al, 2005), given a corpus
already annotated with POS tags and NER labels.
Then retrieval runs in four steps (Figure 1):
1. Question Analysis. The question analysis com-
ponent from QA is reused here. In this imple-
mentation, the only information we have cho-
sen to use from the question is the question
word (e.g., how, who) and the lexical answer
types (LAT) in case of what/which questions.
2. Answer Feature Selection. Given the question
word, we select the 5 highest weighted features
(e.g., POS[0]=CD for a when question).
3. Query Formulation. The original question is
combined with the top features as the query.
4. Coupled Retrieval. Indri retrieves a ranked list
of documents or passages.
As motivated in the introduction, this framework
is aimed at providing the following benefits:
Reuse of QA components on the IR side. IR
reuses both code for question analysis and top
weighted features from QA.
Statistical selection of answer features. For in-
stance, the NER tagger we used divides location
into two categories: GPE (geo locations) and LOC
5Ogilvie (2010) showed in chapter 4.3 that keyword and
named entities based retrieval actually outperformed SRL-
based structured retrieval in MAP for the answer-bearing sen-
tence retrieval task in their setting. In this paper we do not
intend to re-invent another parse-based structure matching al-
gorithm, but only use shallow structures to show the idea of
coupling QA with IR; in the future this might be extended to
incorporate ?deeper? structure.
(non-GPE ). Both of them are learned to be impor-
tant to where questions.
Error tolerance along the NLP pipeline. IR
and QA share the same processing pipeline. Sys-
tematic errors made by the processing tools are
tolerated, in the sense that if the same pre-
processing error is made on both the question
and sentence, an answer may still be found.
Take the previous where question, besides
NER[0]=GPE and NER[0]=LOC, we also found
oddly NER[0]=PERSON an important feature, due
to that the NER tool sometimes mistakes PERSON
for LOC. For instance, the volcano name Mauna
Loa is labeled as a PERSON instead of a LOC. But
since the importance of this feature is recognized
by downstream QA, the upstream IR is still moti-
vated to retrieve it.
Queries were lightly optimized using the fol-
lowing strategies:
Query Weighting In practice query words are
weighted:
#weight(1.0 When 1.0 was 1.0 Alaska 1.0 purchased
? #max(#any:CD #any:DATE))
with a weight ? for the answer types tuned via
cross-validation.
Since NER and POS tags are not lexicalized
they accumulate many more counts (i.e. term fre-
quency) than individual words, thus we in gen-
eral downweight by setting ? < 1.0, giving the
expected answer types ?enough say? but not ?too
much say?:
NER Types First We found NER labels better in-
dicators of expected answer types than POS tags.
The reasons are two-fold: 1. In general POS tags
are too coarse-grained in answer types than NER
labels. E.g., NNP can answer who and where
questions, but is not as precise as PERSON and
GPE. 2. POS tags accumulate even more counts
than NER labels, thus they need separate down-
weighting. Learning the interplay of these weights
in a joint IR/QA model, is an interesting path for
future work. If the top-weighted features are based
on NER, then we do not include POS tags for that
question. Otherwise POS tags are useful, for in-
stance, in answering how questions.
Unigram QA Model The QA system uses up to
trigram features (Table 1 shows examples of uni-
gram and bigram features). Thus it is able to learn,
for instance, that a POS sequence of IN CD NNS
is likely an answer to a when question (such as:
in 5 years). This requires that the IR queries
161
When was Alaska purchased?
qword=when
qword=when|POS[0]=CD ? ANS: 0.86qword=when|NER[0]=DATE ? ANS: 0.79...
#combine(Alaska purchased #max(#any:CD  #any:DATE))
1. Simple question analysis(reuse from QA)2. Get top weighted features w.r.t qword(from trained QA model)
3. Query formulation
4. Coupled retrieval
On <DATE>March 30, <CD> 1867 </CD> </DATE>, U.S. ... reached agreement ... to purchase ... Alaska ...The islands were sold to the United States in <CD>1867</CD> with the purchase of Alaska.?...Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets ...
12...50
Figure 1: Coupled retrieval with queries directly con-
structed from highest weighted features of downstream QA.
The retrieved and ranked list of sentences is POS and NER
tagged, but only query-relevant tags are shown due to space
limit. A bag-of-words retrieval approach would have the sen-
tence shown above at rank 50 at its top position instead.
look for a consecutive IN CD NNS sequence. We
drop this strict constraint (which may need further
smoothing) and only use unigram features, not by
simply extracting ?good? unigram features from
the trained model, but by re-training the model
with only unigram features. In answer extraction,
we still use up to trigram features. 6
4 Experiments
We want to measure and compare the performance
of the following retrieval techniques:
1. uncoupled retrieval with an off-the-shelf IR en-
gine by using the question as query (baseline),
2. QA-driven coupled retrieval (proposed), and
3. answer-bearing retrieval by using both the
question and known answer as query, only eval-
uated for answer extraction (upper bound),
at the three stages of question answering:
1. Document retrieval (for relevant docs from cor-
pus), measured by Mean Average Precision
(MAP) and Mean Reciprocal Rank (MRR).
2. Passage retrieval (finding relevant sentences
from the document), also by MAP and MRR.
3. Answer extraction, measured by F1.
6This is because the weights of unigram to trigram fea-
tures in a loglinear CRF model is a balanced consequence for
maximization. A unigram feature might end up with lower
weight because another trigram containing this unigram gets
a higher weight. Then we would have missed this feature
if we only used top unigram features. Thus we re-train the
model with only unigram features to make sure weights are
?assigned properly? among only unigram features.
set questions sentences#all #pos. #all #pos.
TRAIN 2205 1756 (80%) 22043 7637 (35%)
TESTgold 99 88 (89%) 990 368 (37%)
Table 2: Statistics for AMT-collected data (total cost was
around $800 for paying three Turkers per sentence). Positive
questions are those with an answer found. Positive sentences
are those bearing an answer.
All coupled and uncoupled queries are performed
with Indri v5.3 (Strohman et al, 2005).
4.1 Data
Test Set for IR and QA The MIT109 test col-
lection by Lin and Katz (2006) contains 109
questions from TREC 2002 and provides a near-
exhaustive judgment of relevant documents for
each question. We removed 10 questions that do
not have an answer by matching the TREC answer
patterns. Then we call this test set MIT99.
Training Set for QA We used Amazon Mechani-
cal Turk to collect training data for the QA system
by issuing answer-bearing queries for TREC1999-
2003 questions. For the top 10 retrieved sen-
tences for each question, three Turkers judged
whether each sentence contained the answer. The
inter-coder agreement rate was 0.81 (Krippen-
dorff, 2004; Artstein and Poesio, 2008).
The 99 questions of MIT99 were extracted from
the Turk collection as our TESTgold with the re-
maining as TRAIN, with statistics shown in Table
2. Note that only 88 questions out of MIT99 have
an answer from the top 10 query results.
Finally both the training and test data were
sentence-segmented and word-tokenized by
NLTK (Bird and Loper, 2004), dependency-
parsed by the Stanford Parser (Klein and
Manning, 2003), and NER-tagged by the Illinois
Named Entity Tagger (Ratinov and Roth, 2009)
with an 18-label type set.
Corpus Preprocessing for IR The AQUAINT
(LDC2002T31) corpus, on which the MIT99
questions are based, was processed in exactly the
same manner as was the QA training set. But
only sentence boundaries, POS tags and NER la-
bels were kept as the annotation of the corpus.
4.2 Document and Passage Retrieval
We issued uncoupled queries consisting of ques-
tion words, and QA-driven coupled queries con-
sisting of both the question and expected answer
types, then retrieved the top 1000 documents, and
162
type coupled uncoupledMAP MRR MAP MRR
document 0.2524 0.4835 0.2110 0.4298
sentence 0.1375 0.2987 0.1200 0.2544
Table 3: Coupled vs. uncoupled document/sentence re-
trieval in MAP and MRR on MIT99. Significance level
(Smucker et al, 2007) for both MAP: p < 0.001 and for
both MRR: p < 0.05.
finally computed MAP and MRR against the gold-
standard MIT99 per-document judgment.
To find the best weighting ? for coupled re-
trieval, we used 5-fold cross-validation and final-
ized at ? = 0.1. Table 3 shows the results.
Coupled retrieval outperforms (20% by MAP with
p < 0.001 and 12% by MRR with p < 0.01) un-
coupled retrieval significantly according to paired
randomization test (Smucker et al, 2007).
For passage retrieval, we extracted relevant sin-
gle sentences. Recall that MIT99 only contains
document-level judgment. To generate a test set
for sentence retrieval, we matched each sentence
from relevant documents provided by MIT99 for
each question against the TREC answer patterns.
We found no significant difference between re-
trieving sentences from the documents returned
by document retrieval or directly from the corpus.
Numbers of the latter are shown in Table 3. Still,
coupled retrieval is significantly better by about
10% in MAP and 17% in MRR.
4.3 Answer Extraction
Lastly we sent the sentences to the downstream
QA engine (trained on TRAIN) and computed F1
per K for the top K retrieved sentences, 7 shown
in Figure 2. The best F1 with coupled sentence re-
trieval is 0.231, 20% better than F1 of 0.192 with
uncoupled retrieval, both at K = 1.
The two descending lines at the bottom reflect
the fact that the majority-voting mechanism from
the QA system was too simple: F1 drops as K in-
creases. Thus we also computed F1?s assuming
perfect voting: a voting oracle that always selects
the correct answer as long as the QA system pro-
duces one, thus the two ascending lines in the cen-
ter of Figure 2. Still, F1 with coupled retrieval is
always better: reiterating the fact that coupled re-
trieval covers more answer-bearing sentences.
7Lin (2007), Zhang et al (2007), and Kaisser (2012) also
evaluated on MIT109. However their QA engines used web-
based search engines, thus leading to results that are neither
reproducible nor directly comparable with ours.
Finally, to find the upper bound for QA, we
drew the two upper lines, testing on TESTgold de-
scribed in Table 2. The test sentences were ob-
tained with answer-bearing queries. This is as-
suming almost perfect IR. The gap between the
top two and other lines signals more room for im-
provements for IR in terms of better coverage and
better rank for answer-bearing sentences.
1 2 3 5 10 15 20 50 100 200 500 1000Top K Sentences Retrieved
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
F1
Coupled (0.231)Uncoupled (0.192)
Gold Oracle (0.755)Gold (0.596)Coupled Oracle (0.609)Uncoupled Oracle (0.569)
Figure 2: F1 values for answer extraction on MIT99. Best
F1?s for each method are parenthesized in the legend. ?Or-
acle? methods assumed perfect voting of answer candidates
(a question is answered correctly if the system ever produced
one correct answer for it). ?Gold? was tested on TESTgold.
5 Conclusion
We described a method to perform coupled in-
formation retrieval with a prior knowledge of the
downstream QA system. Specifically, we coupled
IR queries with automatically learned answer fea-
tures from QA and observed significant improve-
ments in document/passage retrieval and boosted
F1 in answer extraction. This method has the mer-
its of not requiring hand-built question and answer
templates and being flexible in incorporating vari-
ous answer features automatically learned and op-
timized from the downstream QA system.
Acknowledgement
We thank Vulcan Inc. for funding this work. We
also thank Paul Ogilvie, James Mayfield, Paul Mc-
Namee, Jason Eisner and the three anonymous re-
viewers for insightful comments.
163
References
Arvind Agarwal, Hema Raghavan, Karthik Subbian,
Prem Melville, Richard D. Lawrence, David C.
Gondek, and James Fan. 2012. Learning to rank
for robust question answering. In Proceedings of
the 21st ACM international conference on Informa-
tion and knowledge management, CIKM ?12, pages
833?842, New York, NY, USA. ACM.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
M.W. Bilotti and E. Nyberg. 2008. Improving text
retrieval precision and answer accuracy in question
answering systems. In Coling 2008: Proceedings
of the 2nd workshop on Information Retrieval for
Question Answering, pages 1?8.
M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007. Structured retrieval for question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 351?358. ACM.
M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg.
2010. Rank learning for factoid question answer-
ing with linguistic and semantic constraints. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
459?468. ACM.
Steven Bird and Edward Loper. 2004. Nltk: The nat-
ural language toolkit. In The Companion Volume to
the Proceedings of 42st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 214?
217, Barcelona, Spain, July.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of the 28th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?05, pages 400?407, New
York, NY, USA. ACM.
Mark A. Greenwood, editor. 2008. Coling 2008: Pro-
ceedings of the 2nd workshop on Information Re-
trieval for Question Answering. Coling 2008 Orga-
nizing Committee, Manchester, UK, August.
Michael Kaisser. 2012. Answer Sentence Retrieval by
Matching Dependency Paths acquired from Ques-
tion/Answer Sentence Pairs. In EACL, pages 88?98.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In In Proc. the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Klaus H. Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage Publications,
Inc, 2nd edition.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
J. Lin and B. Katz. 2006. Building a reusable test
collection for question answering. Journal of the
American Society for Information Science and Tech-
nology, 57(7):851?861.
D. Lin and P. Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Jimmy Lin. 2007. An exploration of the principles un-
derlying redundancy-based factoid question answer-
ing. ACM Trans. Inf. Syst., 25(2), April.
P. Ogilvie. 2010. Retrieval using Document Struc-
ture and Annotations. Ph.D. thesis, Carnegie Mellon
University.
Christopher Pinchak, Davood Rafiei, and Dekang Lin.
2009. Answer typing for information retrieval. In
Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ?09,
pages 1955?1958, New York, NY, USA. ACM.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive an-
notation. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?00,
pages 184?191, New York, NY, USA. ACM.
J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba.
2006. Question answering by predictive annota-
tion. Advances in Open Domain Question Answer-
ing, pages 307?347.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, 6.
Tetsuya Sakai, Hideki Shima, Noriko Kando, Rui-
hua Song, Chuan-Jie Lin, Teruko Mitamura, Miho
Sugimito, and Cheng-Wei Lee. 2010. Overview
of the ntcir-7 aclia ir4qa task. In Proceedings of
NTCIR-8 Workshop Meeting, Tokyo, Japan.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12?21.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623?
632. ACM.
164
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language model-based search engine
for complex queries. In Proceedings of the Interna-
tional Conference on Intelligent Analysis, volume 2,
pages 2?6. Citeseer.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and
David R. Cheriton. 2007. Information distance
from a question to an answer. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?07,
pages 874?883, New York, NY, USA. ACM.
L. Zhao and J. Callan. 2008. A generative retrieval
model for structured documents. In Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 1163?1172. ACM.
165
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702?707,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lightweight and High Performance Monolingual Word Aligner
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Fast alignment is essential for many nat-
ural language tasks. But in the setting of
monolingual alignment, previous work has
not been able to align more than one sen-
tence pair per second. We describe a dis-
criminatively trained monolingual word
aligner that uses a Conditional Random
Field to globally decode the best align-
ment with features drawn from source and
target sentences. Using just part-of-speech
tags and WordNet as external resources,
our aligner gives state-of-the-art result,
while being an order-of-magnitude faster
than the previous best performing system.
1 Introduction
In statistical machine translation, alignment is typ-
ically done as a one-off task during training. How-
ever for monolingual tasks, like recognizing tex-
tual entailment or question answering, alignment
happens repeatedly: once or multiple times per
test item. Therefore, the efficiency of the aligner is
of utmost importance for monolingual alignment
tasks. Monolingual word alignment also has a va-
riety of distinctions than the bilingual case, for ex-
ample: there is often less training data but more
lexical resources available; semantic relatedness
may be cued by distributional word similarities;
and, both the source and target sentences share the
same grammar.
These distinctions suggest a model design that
utilizes arbitrary features (to make use of word
similarity measure and lexical resources) and ex-
ploits deeper sentence structures (especially in the
case of major languages where robust parsers are
available). In this setting the balance between
precision and speed becomes an issue: while we
might leverage an extensive NLP pipeline for a
?Performed while faculty at Johns Hopkins University.
language like English, such pipelines can be com-
putationally expensive. One earlier attempt, the
MANLI system (MacCartney et al, 2008), used
roughly 5GB of lexical resources and took 2 sec-
onds per alignment, making it hard to be deployed
and run in large scale. On the other extreme, a sim-
ple non-probabilistic Tree Edit Distance (TED)
model (c.f. ?4.2) is able to align 10, 000 pairs
per second when the sentences are pre-parsed, but
with significantly reduced performance. Trying to
embrace the merits of both worlds, we introduce a
discriminative aligner that is able to align tens to
hundreds of sentence pairs per second, and needs
access only to a POS tagger and WordNet.
This aligner gives state-of-the-art performance
on the MSR RTE2 alignment dataset (Brockett,
2007), is faster than previous work, and we re-
lease it publicly as the first open-source monolin-
gual word aligner: Jacana.Align.1
2 Related Work
The MANLI aligner (MacCartney et al, 2008)
was first proposed to align premise and hypothe-
sis sentences for the task of natural language in-
ference. It applies perceptron learning and han-
dles phrase-based alignment of arbitrary phrase
lengths. Thadani and McKeown (2011) opti-
mized this model by decoding via Integer Linear
Programming (ILP). Benefiting from modern ILP
solvers, this led to an order-of-magnitude speedup.
With extra syntactic constraints added, the exact
alignment match rate for whole sentence pairs was
also significantly improved.
Besides the above supervised methods, indirect
supervision has also been explored. Among them,
Wang and Manning (2010) extended the work of
McCallum et al (2005) and modeled alignment
as latent variables. Heilman and Smith (2010)
used tree kernels to search for the alignment that
1http://code.google.com/p/jacana/
702
yields the lowest tree edit distance. Other tree
or graph matching work for alignment includes
that of (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Chambers et al, 2007; Mehdad,
2009; Roth and Frank, 2012).
Finally, feature and model design in monolin-
gual alignment is often inspired by bilingual work,
including distortion modeling, phrasal alignment,
syntactic constraints, etc (Och and Ney, 2003;
DeNero and Klein, 2007; Bansal et al, 2011).
3 The Alignment Model
3.1 Model Design
Our work is heavily influenced by the bilingual
alignment literature, especially the discriminative
model proposed by Blunsom and Cohn (2006).
Given a source sentence s of length M , and a tar-
get sentence t of length N , the alignment from s
to t is a sequence of target word indices a, where
am?[1,M ] ? [0, N ]. We specify that when am = 0,
source word st is aligned to a NULL state, i.e.,
deleted. This models a many-to-one alignment
from source to target. Multiple source words can
be aligned to the same target word, but not vice
versa. One-to-many alignment can be obtained
by running the aligner in the other direction. The
probability of alignment sequence a conditioned
on both s and t is then:
p(a | s, t) =
exp(
?
m,k ?kfk(am?1, am, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). The word alignment
task is evaluated over F1. Instead of directly op-
timizing F1, we employ softmax-margin training
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denom-
inator, which becomes:
?
a?
exp(
?
m,k
?kfk(a?m?1, a?m, s, t) + cost(at, a?))
where at is the true alignments. cost(at, a?)
can be viewed as special ?features? with uniform
weights that encourage consistent with true align-
ments. It is only computed during training in the
denominator because cost(at,at) = 0 in the nu-
merator. Hamming cost is used in practice.
One distinction of this alignment model com-
pared to other commonly defined CRFs is that
the input is two dimensional: at each position m,
the model inspects both the entire sequence of
source words (as the observation) and target words
(whose offset indices are states). The other dis-
tinction is that the size of its state space is not
fixed (e.g., unlike POS tagging, where states are
for instance 45 Penn Treebank tags), but depends
on N , the length of target sentence. Thus we can
not ?memorize? what features are mostly associ-
ated with what states. For instance, in the task of
tagging mail addresses, a feature of ?5 consecu-
tive digits? is highly indicative of a POSTCODE.
However, in the alignment model, it does not make
sense to design features based on a hard-coded
state, say, a feature of ?source word lemma match-
ing target word lemma? fires for state index 6.
To avoid this data sparsity problem, all features
are defined implicitly with respect to the state. For
instance:
fk(am?1, am, s, t) =
{
1 lemmas match: sm, tam
0 otherwise
Thus this feature fires for, e.g.:
(s3 = sport, t5 = sports, a3 = 5), and:
(s2 = like, t10 = liked, a2 = 10).
3.2 Feature Design
String Similarity Features include the following
similarity measures: Jaro Winkler, Dice Sorensen,
Hamming, Jaccard, Levenshtein, NGram overlap-
ping and common prefix matching.2 Also, two
binary features are added for identical match and
identical match ignoring case.
POS Tags Features are binary indicators of
whether the POS tags of two words match. Also,
a ?possrc2postgt? feature fires for each word pair,
with respect to their POS tags. This would capture,
e.g., ?vbz2nn?, when a verb such as arrests aligns
with a noun such as custody.
Positional Feature is a real-valued feature for the
positional difference of the source and target word
(abs(mM ? amN )).WordNet Features indicate whether two words
are of the following relations of each other: hyper-
nym, hyponym, synonym, derived form, entailing,
causing, members of, have member, substances of,
have substances, parts of, have part; or whether
2Of these features the trained aligner preferred Dice
Sorensen and NGram overlapping.
703
their lemmas match.3
Distortion Features measure how far apart the
aligned target words of two consecutive source
words are: abs(am + 1 ? am?1). This learns a
general pattern of whether these two target words
aligned with two consecutive source words are
usually far away from each other, or very close.
We also added special features for corner cases
where the current word starts or ends the source
sentence, or both the previous and current words
are deleted (a transition from NULL to NULL).
Contextual Features indicate whether the left or
the right neighbor of the source word and aligned
target word are identical or similar. This helps
especially when aligning functional words, which
usually have multiple candidate target functional
words to align to and string similarity features can-
not help. We also added features for neighboring
POS tags matching.
3.3 Symmetrization
To expand from many-to-one alignment to many-
to-many, we ran the model in both directions and
applied the following symmetrization heuristics
(Koehn, 2010): INTERSECTION, UNION, GROW-
DIAG-FINAL.
4 Experiments
4.1 Setup
Since no generic off-the-shelf CRF software is de-
signed to handle the special case of dynamic state
indices and feature functions (Blunsom and Cohn,
2006), we implemented this aligner model in the
Scala programming language, which is fully in-
teroperable with Java. We used the L2 regular-
izer and LBFGS for optimization. OpenNLP4 pro-
vided the POS tagger and JWNL5 interfaced with
WordNet (Fellbaum, 1998).
To make results directly comparable, we closely
followed the setup of MacCartney et al (2008) and
Thadani and McKeown (2011). Training and test
data (Brockett, 2007) each contains 800 manually
aligned premise and hypothesis pairs from RTE2.
Note that the premises contain 29 words on av-
erage, and the hypotheses only 11 words. We take
the premise as the source and hypothesis as the tar-
get, and use S2T to indicate the model aligns from
3We found that each word has to be POS tagged to get an
accurate relation, otherwise this feature will not help.
4http://opennlp.apache.org/
5http://jwordnet.sf.net/
source to target and T2S from target to source.
4.2 Simple Baselines
We additionally used two baseline systems for
comparison. One was GIZA++, with the IN-
TERSECTION tricks post-applied, which worked
the best among all other symmetrization heuris-
tics. The other was a Tree Edit Distance (TED)
model, popularly used in a series of NLP appli-
cations (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Heilman and Smith, 2010). We
used uniform cost for deletion, insertion and sub-
stitutions, and applied a dynamic program algo-
rithm (Zhang and Shasha, 1989) to decode the
tree edit sequence with the minimal cost, based
on the Stanford dependency tree (De Marneffe
and Manning, 2008). This non-probabilistic ap-
proach turned out to be extremely fast, processing
about 10,000 sentence pairs per second with pre-
parsed trees, performing quantitatively better than
the Stanford RTE aligner (Chambers et al, 2007).
4.3 MANLI Baselines
MANLI was first developed by MacCartney et al
(2008), and then improved by Thadani and McKe-
own (2011) with faster and exact decoding via ILP.
There are four versions to be compared here:
MANLI the original version.
MANLI-approx. re-implemented version by
Thadani and McKeown (2011).
MANLI-exact decoding via ILP solvers.
MANLI-constraint MANLI-exact with hard
syntactic constraints, mainly on common ?light?
words (determiners, prepositions, etc.) attachment
to boost exact match rate.
4.4 Results
Following Thadani and McKeown (2011), perfor-
mance is evaluated by macro-averaged precision,
recall, F1 of aligned token pairs, and exact (per-
fect) match rate for a whole pair, shown in Ta-
ble 1. As our baselines, GIZA++ (with align-
ment intersection of two directions) and TED are
on par with previously reported results using the
Stanford RTE aligner. The MANLI-family of sys-
tems provide stronger baselines, notably MANLI-
constraint, which has the best F1 and exact match
rate among themselves.
We ran our aligner in two directions: S2T and
T2S, then merged the results with INTERSECTION,
UNION and GROW-DIAG-FINAL. Our system beats
704
System P % R % F1 % E %
GIZA++, ? 82.5 74.4 78.3 14.0
TED 80.6 79.0 79.8 13.5
Stanford RTE? 82.7 75.8 79.1 -
MANLI? 85.4 85.3 85.3 21.3
MANLI-approx./ 87.2 86.3 86.7 24.5
MANLI-exact/ 87.2 86.1 86.8 24.8
MANLI-constraint/ 89.5 86.2 87.8 33.0
this work, S2T 91.8 83.4 87.4 25.9
this work, T2S 93.7 84.0 88.6 35.3
S2T ? T2S 95.4 80.8 87.5 31.3
S2T ? T2S 90.3 86.6 88.4 29.6
GROW-DIAG-FINAL 94.4 81.8 87.6 30.8
Table 1: Results on the 800 pairs of test data. E% stands
for exact (perfect) match rate. Systems marked with ? are
reported by MacCartney et al (2008), with / by Thadani and
McKeown (2011).
the weak and strong baselines6 in all measures ex-
cept recall. Some patterns are very clearly shown:
Higher precision, lower recall is due to the
higher-quality and lower-coverage of WordNet,
where the MANLI-family systems used addi-
tional, automatically derived lexical resources.
Imbalance of exact match rate between S2T and
T2S with a difference of 9.4% is due to the many-
to-one nature of the aligner. When aligning from
source (longer) to target (shorter), multiple source
words can align to the same target word. This
is not desirable since multiple duplicate ?light?
words are aligned to the same ?light? word in the
target, which breaks perfect match. When align-
ing T2S, this problem goes away: the shorter tar-
get sentence contains less duplicate words, and in
most cases there is an one-to-one mapping.
MT heuristics help, with INTERSECTION and
UNION respectively improving precision and re-
call.
4.5 Runtime Test
Table 2 shows the runtime comparison. Since the
RTE2 corpus is imbalanced, with premise length
(words) of 29 and hypothesis length of 11, we
also compare on the corpus of FUSION (McKeown
et al, 2010), with both sentences in a pair aver-
aging 27. MANLI-approx. is the slowest, with
quadratic growth in the number of edits with sen-
tence length. MANLI-exact is in second place, re-
lying on the ILP solver. This work has a precise
O(MN2) decoding time, with M the source sen-
tence length and N the target sentence length.
6Unfortunately both MacCartney and Thadani no longer
have their original output files (personal communication), so
we cannot run a significance test against their result.
corpus sent. pair
length
MANLI-
approx.
MANLI-
exact
this
work
RTE2 29/11 1.67 0.08 0.025
FUSION 27/27 61.96 2.45 0.096
Table 2: Alignment runtime in seconds per sentence pair on
two corpora: RTE2 (Cohn et al, 2008) and FUSION (McKe-
own et al, 2010). The MANLI-* results are from Thadani
and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache.
The runtime for this work takes the longest timing from S2T
and T2S, on a Xeon 2.2GHz with 4MB cache (the closest
we can find to match their hardware). Horizontally in a real-
world application where sentences have similar length, this
work is roughly 20x faster (0.096 vs. 2.45). Vertically, the
decoding time for our work increases less dramatically when
sentence length increases (0.025?0.096 vs. 0.08?2.45).
features P % R % F1 % E %
full (T2S) 93.7 84.0 88.6 35.3
- POS 93.2 83.5 88.1 31.4
- WordNet 93.2 83.7 88.2 33.5
- both 93.1 83.2 87.8 30.1
Table 3: Performance without POS and/or Word-
Net features.
While MANLI-exact is about twenty-fold faster
than MANLI-approx., our aligner is at least an-
other twenty-fold faster than MANLI-exact when
the sentences are longer and balanced. We also
benefit from shallower pre-processing (no parsing)
and can store all resources in main memory.7
4.6 Ablation Test
Since WordNet and the POS tagger is the only used
external resource, we removed them8 from the fea-
ture sets and reported performance in Table 3. This
somehow reflects how the model would perform
for a language without a suitable POS tagger, or
more commonly, WordNet in that language. At
this time, the model falls back to relying on string
similarities, distortion, positional and contextual
features, which are almost language-independent.
A loss of less than 1% in F1 suggests that the
aligner can still run reasonably well without a POS
tagger and WordNet.
7WordNet (?30MB) is a smaller footprint than the 5GB of
external resources used by MANLI.
8per request of reviewers. Note that WordNet is less pre-
cise without a POS tagger. When we removed the POS tag-
ger, we enumerated all POS tags for a word to find its hyper-
nym/synonym/... synsets.
705
4.7 Error Analysis
There were three primary categories of error:9
1. Token-based paraphrases that are not covered
by WordNet, such as program and software,
business and venture. This calls for broader-
coverage paraphrase resources.
2. Words that are semantically related but not
exactly paraphrases, such as married and
wife, beat and victory. This calls for re-
sources of close distributional similarity.
3. Phrases of the above kinds, such as elected
and won a seat, politician and presidential
candidate. This calls for further work on
phrase-based alignment.10
There is a trade-off using WordNet vs. larger,
noisier resources in exchange of higher preci-
sion vs. recall and memory/disk allocation. We
think this is an application-specific decision; other
resources could be easily incorporated into our
model, which we may explore in the future to ex-
plore the trade-off in addressing items 1 and 2.
5 Conclusion
We presented a model for monolingual sentence
alignment that gives state-of-the-art performance,
and is significantly faster than prior work. We re-
lease our implementation as the first open-source
monolingual aligner, which we hope to be of ben-
efit to other researchers in the rapidly expanding
area of natural language inference.
Acknowledgement
We thank Vulcan Inc. for funding this work. We
also thank Jason Smith, Travis Wolfe, Frank Fer-
raro for various discussion, suggestion, comments
and the three anonymous reviewers.
References
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
9We submitted a browser in JavaScript
(AlignmentBrowser.html) in the supporting material
that compares the gold alignment and test output; readers are
encouraged to try it out.
10Note that MacCartney et al (2008) showed that in the
MANLI system setting phrase size to larger than one there
was only a 0.2% gain in F1, while the complexity became
much larger.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Pro-
ceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL2007.
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17?20.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA.
B. MacCartney, M. Galley, and C.D. Manning. 2008.
A phrase-based alignment model for natural lan-
guage inference. In Proceedings of EMNLP2008,
pages 802?811.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A Conditional Random Field
for Discriminatively-trained Finite-state String Edit
Distance. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence (UAI 2005),
July.
706
Kathleen McKeown, Sara Rosenthal, Kapil Thadani,
and Coleman Moore. 2010. Time-efficient creation
of an accurate sentence fusion corpus. In ACL2010
short, pages 317?320.
Y. Mehdad. 2009. Automatic cost estimation for tree
edit distance using particle swarm optimization. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answerin. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning pred-
icates across monolingual comparable texts using
graph-based clustering. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 171?182, Jeju Island,
Korea, July.
Kapil Thadani and Kathleen McKeown. 2011. Opti-
mal and syntactically-informed decoding for mono-
lingual phrase-based alignment. In Proceedings of
ACL short.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 1164?1172, Stroudsburg, PA, USA.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
707
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956?966,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Information Extraction over Structured Data:
Question Answering with Freebase
Xuchen Yao
1
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Abstract
Answering natural language questions us-
ing the Freebase knowledge base has re-
cently been explored as a platform for ad-
vancing the state of the art in open do-
main semantic parsing. Those efforts map
questions to sophisticated meaning repre-
sentations that are then attempted to be
matched against viable answer candidates
in the knowledge base. Here we show
that relatively modest information extrac-
tion techniques, when paired with a web-
scale corpus, can outperform these sophis-
ticated approaches by roughly 34% rela-
tive gain.
1 Introduction
Question answering (QA) from a knowledge base
(KB) has a long history within natural language
processing, going back to the 1960s and 1970s,
with systems such as Baseball (Green Jr et al,
1961) and Lunar (Woods, 1977). These systems
were limited to closed-domains due to a lack of
knowledge resources, computing power, and abil-
ity to robustly understand natural language. With
the recent growth in KBs such as DBPedia (Auer
et al, 2007), Freebase (Bollacker et al, 2008)
and Yago2 (Hoffart et al, 2011), it has be-
come more practical to consider answering ques-
tions across wider domains, with commercial sys-
tems including Google Now, based on Google?s
Knowledge Graph, and Facebook Graph
Search, based on social network connections.
The AI community has tended to approach this
problem with a focus on first understanding the in-
tent of the question, via shallow or deep forms of
semantic parsing (c.f. ?3 for a discussion). Typ-
ically questions are converted into some mean-
ing representation (e.g., the lambda calculus), then
mapped to database queries. Performance is thus
bounded by the accuracy of the original seman-
tic parsing, and the well-formedness of resultant
database queries.
1
The Information Extraction (IE) community ap-
proaches QA differently: first performing rela-
tively coarse information retrieval as a way to
triage the set of possible answer candidates, and
only then attempting to perform deeper analysis.
Researchers in semantic parsing have recently
explored QA over Freebase as a way of moving
beyond closed domains such as GeoQuery (Tang
and Mooney, 2001). While making semantic pars-
ing more robust is a laudable goal, here we provide
a more rigorous IE baseline against which those
efforts should be compared: we show that ?tradi-
tional? IE methodology can significantly outper-
form prior state-of-the-art as reported in the se-
mantic parsing literature, with a relative gain of
34% F
1
as compared to Berant et al (2013).
2 Approach
We will view a KB as an interlinked collection of
?topics?. When given a question about one or sev-
eral topics, we can select a ?view? of the KB con-
cerning only involved topics, then inspect every
related node within a few hops of relations to the
topic node in order to extract the answer. We call
such a view a topic graph and assume answers can
be found within the graph. We aim to maximally
automate the answer extraction process, by mas-
sively combining discriminative features for both
the question and the topic graph. With a high per-
formance learner we have found that a system with
millions of features can be trained within hours,
leading to intuitive, human interpretable features.
For example, we learn that given a question con-
cerning money, such as: what money is used in
1
As an example, 50% of errors of the CCG-backed
(Kwiatkowski et al, 2013) system were contributed by pars-
ing or structural matching failure.
956
ukraine, the expected answer type is likely cur-
rency. We formalize this approach in ?4.
One challenge for natural language querying
against a KB is the relative informality of queries
as compared to the grammar of a KB. For exam-
ple, for the question: who cheated on celebrity
A, answers can be retrieved via the Freebase rela-
tion celebrity.infidelity.participant, but the con-
nection between the phrase cheated on and the
formal KB relation is not explicit. To allevi-
ate this problem, the best attempt so far is to
map from ReVerb (Fader et al, 2011) predicate-
argument triples to Freebase relation triples (Cai
and Yates, 2013; Berant et al, 2013). Note that
to boost precision, ReVerb has already pruned
down less frequent or credible triples, yielding not
as much coverage as its text source, ClueWeb.
Here we instead directly mine relation mappings
from ClueWeb and show that both direct relation
mapping precision and indirect QA F
1
improve by
a large margin. Details in ?5.
Finally, we tested our system, jacana-
freebase,
2
on a realistic dataset generously
contributed by Berant et al (2013), who collected
thousands of commonly asked questions by
crawling the Google Suggest service. Our
method achieves state-of-the-art performance
with F
1
at 42.0%, a 34% relative increase from
the previous F
1
of 31.4%.
3 Background
QA from a KB faces two prominent challenges:
model and data. The model challenge involves
finding the best meaning representation for the
question, converting it into a query and exe-
cuting the query on the KB. Most work ap-
proaches this via the bridge of various interme-
diate representations, including combinatory cat-
egorial grammar (Zettlemoyer and Collins, 2005,
2007, 2009; Kwiatkowski et al, 2010, 2011,
2013), synchronous context-free grammars (Wong
and Mooney, 2007), dependency trees (Liang et
al., 2011; Berant et al, 2013), string kernels (Kate
and Mooney, 2006; Chen and Mooney, 2011),
and tree transducers (Jones et al, 2012). These
works successfully showed their effectiveness in
QA, despite the fact that most of them require
hand-labeled logic annotations. More recent re-
search started to minimize this direct supervision
by using latent meaning representations (Berant et
2
https://code.google.com/p/jacana
al., 2013; Kwiatkowski et al, 2013) or distant su-
pervision (Krishnamurthy and Mitchell, 2012).
We instead attack the problem of QA from a KB
from an IE perspective: we learn directly the pat-
tern of QA pairs, represented by the dependency
parse of questions and the Freebase structure of
answer candidates, without the use of intermedi-
ate, general purpose meaning representations.
The data challenge is more formally framed as
ontology or (textual) schema matching (Hobbs,
1985; Rahm and Bernstein, 2001; Euzenat and
Shvaiko, 2007): matching structure of two on-
tologies/databases or (in extension) mapping be-
tween KB relations and NL text. In terms of
the latter, Cai and Yates (2013) and Berant et al
(2013) applied pattern matching and relation inter-
section between Freebase relations and predicate-
argument triples from the ReVerb OpenIE sys-
tem (Fader et al, 2011). Kwiatkowski et al
(2013) expanded their CCG lexicon with Wik-
tionary word tags towards more domain indepen-
dence. Fader et al (2013) learned question para-
phrases from aligning multiple questions with the
same answers generated by WikiAnswers. The
key factor to their success is to have a huge text
source. Our work pushes the data challenge to the
limit by mining directly from ClueWeb, a 5TB
collection of web data.
Finally, the KB community has developed other
means for QA without semantic parsing (Lopez et
al., 2005; Frank et al, 2007; Unger et al, 2012;
Yahya et al, 2012; Shekarpour et al, 2013). Most
of these work executed SPARQL queries on in-
terlinked data represented by RDF (Resource De-
scription Framework) triples, or simply performed
triple matching. Heuristics and manual templates
were also commonly used (Chu-Carroll et al,
2012). We propose instead to learn discriminative
features from the data with shallow question anal-
ysis. The final system captures intuitive patterns
of QA pairs automatically.
4 Graph Features
Our model is inspired by an intuition on how ev-
eryday people search for answers. If you asked
someone: what is the name of justin bieber
brother,
3
and gave them access to Freebase, that
person might first determine that the question
3
All examples used in this paper come from the train-
ing data crawled from Google Suggest. They are low-
ercased and some contain typos.
957
is about Justin Bieber (or his brother), go to
Justin Bieber?s Freebase page, and search for his
brother?s name. Unfortunately Freebase does not
contain an exact relation called brother, but in-
stead sibling. Thus further inference (i.e., brother
? male sibling) has to be made. In the following
we describe how we represent this process.
4.1 Question Graph
In answering our example query a person might
take into consideration multiple constraints. With
regards to the question, we know we are looking
for the name of a person based on the following:
? the dependency relation nsubj(what, name)
and prep of(name, brother) indicates that the
question seeks the information of a name;
4
? the dependency relation prep of(name,
brother) indicates that the name is about a
brother (but we do not know whether it is a
person name yet);
? the dependency relation nn(brother, bieber)
and the facts that, (i) Bieber is a person and (ii)
a person?s brother should also be a person, indi-
cate that the name is about a person.
This motivates the design of dependency-based
features. We show one example in Figure 1(a),
left side. The following linguistic information is
of interest:
? question word (qword), such as what/who/how
many. We use a list of 9 common qwords.
5
? question focus (qfocus), a cue of expected an-
swer types, such as name/money/time. We
keep our analysis simple and do not use a ques-
tion classifier, but simply extract the noun de-
pendent of qword as qfocus.
? question verb (qverb), such as is/play/take, ex-
tracted from the main verb of the question.
Question verbs are also good hints of answer
types. For instance, play is likely to be followed
by an instrument, a movie or a sports team.
? question topic (qtopic). The topic of the ques-
tion helps us find relevant Freebase pages. We
simply apply a named entity recognizer to find
the question topic. Note that there can be more
than one topic in the question.
Then we convert the dependency parse into a more
generic question graph, in the following steps:
4
We use the Stanford collapsed dependency form.
5
who, when, what, where, how, which, why, whom,
whose.
1. if a node was tagged with a question feature,
then replace this node with its question feature,
e.g., what? qword=what;
2. (special case) if a qtopic node was tagged as
a named entity, then replace this node with
its its named entity form, e.g., bieber ?
qtopic=person;
3. drop any leaf node that is a determiner, prepo-
sition or punctuation.
The converted graph is shown in Figure 1(a),
right side. We call this a question feature graph,
with every node and relation a potential feature
for this question. Then features are extracted
in the following form: with s the source and
t the target node, for every edge e(s, t) in the
graph, extract s, t, s | t and s | e | t as
features. For the edge, prep of(qfocus=name,
brother), this would mean the following features:
qfocus=name, brother, qfocus=name|brother,
and qfocus=name|prep of|brother.
We show with examples why these features
make sense later in ?6 Table 6. Furthermore, the
reason that we have kept some lexical features,
such as brother, is that we hope to learn from
training a high correlation between brother and
some Freebase relations and properties (such as
sibling and male) if we do not possess an exter-
nal resource to help us identify such a correlation.
4.2 Freebase Topic Graph
Given a topic, we selectively roll out the Free-
base graph by choosing those nodes within a few
hops of relationship to the topic node, and form
a topic graph. Besides incoming and/or outgo-
ing relationships, nodes also have properties: a
string that describes the attribute of a node, for
instance, node type, gender or height (for a per-
son). One major difference between relations and
properties is that both arguments of a relation are
nodes, while only one argument of a property is a
node, the other a string. Arguments of relations are
usually interconnected, e.g., London can be the
place of birth for Justin Bieber, or capital of
the UK. Arguments of properties are attributes that
are only ?attached? to certain nodes and have no
outgoing edges. Figure 1(b) shows an example.
Both relationship and property of a node are
important to identifying the answer. They con-
nect the nodes with the question and describe
some unique characteristics. For instance, with-
out the properties type:person and gender:male,
958
what
is name
the brother
justin bieber
nsubj     cop    
nn  
prep_of    det  
nn    
qword
qtopic
qfocus
qtopic
qword=what
qfocus=name
brother
qtopic=person qtopic=person
nsubj    
nn  
prep_of    
nn    
qverb=be
 cop    
qverb
(a) Dependence parse with annotated question features in dashed boxes (left) and converted feature graph (right) with
only relevant and general information about the original question kept. Note that the left is a real but incorrect parse.
Justin Bieber
dummy node
Jazmyn Bieber
person.sibling_s    
Jaxon Bieber
sibling sibling
person
type
person
type
female
gender
male
gender
London
awards_won  
place_of_birth
?... type person
male
gender
(b) A view of Freebase graph on the Justin Bieber topic with nodes in solid boxes and properties in
dashed boxes. The hatching node, Jaxon Bieber, is the answer. Freebase uses a dummy parent node
for a list of nodes with the same relation.
Figure 1: Dependency parse and excerpted Freebase topic graph on the question what is the name of
justin bieber brother.
959
we would not have known the node Jaxon Bieber
represents a male person. These properties, along
with the sibling relationship to the topic node, are
important cues for answering the question. Thus
for the Freebase graph, we use relations (with di-
rections) and properties as features for each node.
Additionally, we have analyzed how Freebase
relations map back to the question. Some of the
mapping can be simply detected as paraphras-
ing or lexical overlap. For example, the per-
son.parents relationship helps answering ques-
tions about parenthood. However, most Freebase
relations are framed in a way that is not com-
monly addressed in natural language questions.
For instance, for common celebrity gossip ques-
tions like who cheated on celebrity A, it is
hard for a system to find the Freebase relation
celebrity.infidelity.participant as the target rela-
tion if it had not observed this pattern in training.
Thus assuming there is an alignment model that
is able to tell how likely one relation maps to the
original question, we add extra alignment-based
features for the incoming and outgoing relation of
each node. Specifically, for each relation rel in
a topic graph, we compute P (rel | question) to
rank the relations. Finally the ranking (e.g., top
1/2/5/10/100 and beyond) of each relation is used
as features instead of a pure probability. We de-
scribe such an alignment model in ? 5.
4.3 Feature Production
We combine question features and Freebase fea-
tures (per node) by doing a pairwise concatena-
tion. In this way we hope to capture the associa-
tion between question patterns and answer nodes.
For instance, in a loglinear model setting, we ex-
pect to learn a high feature weight for features like:
qfocus=money|node type=currency
and a very low weight for:
qfocus=money|node type=person.
This combination greatly enlarges the total
number of features, but owing to progress in large-
scale machine learning such feature spaces are less
of a concern than they once were (concrete num-
bers in ? 6 Model Tuning).
5 Relation Mapping
In this section we describe a ?translation? table be-
tween Freebase relations and NL words was built.
5.1 Formula
The objective is to find the most likely rela-
tion a question prompts. For instance, for the
question who is the father of King George
VI, the most likely relation we look for is peo-
ple.person.parents. To put it more formally,
given a question Q of a word vector w, we want
to find out the relation R that maximizes the prob-
ability P (R | Q).
More interestingly, for the question who is
the father of the Periodic Table, the ac-
tual relation that encodes its original mean-
ing is law.invention.inventor, rather than peo-
ple.person.parents. This simple example points
out that every part of the question could change
what the question inquires eventually. Thus we
need to count for each word w in Q. Due to the
bias and incompleteness of any data source, we
approximate the true probability of P with
?
P un-
der our specific model. For the simplicity of com-
putation, we assume conditional independence be-
tween words and apply Naive Bayes:
?
P (R | Q) ?
?
P (Q | R)
?
P (R)
?
?
P (w | R)
?
P (R)
?
?
w
?
P (w | R)
?
P (R)
where
?
P (R) is the prior probability of a relation
R and
?
P (w | R) is the conditional probability of
word w given R.
It is possible that we do not observe a certain
relation R when computing the above equation.
In this case we back off to the ?sub-relations?: a
relation R is a concatenation of a series of sub-
relations R = r = r
1
.r
2
.r
3
. . . .. For instance, the
sub-relations of people.person.parents are peo-
ple, person, and parents. Again, we assume con-
ditional independence between sub-relations and
apply Naive Bayes:
?
P
backoff
(R | Q) ?
?
P (r | Q)
?
?
r
?
P (r | Q)
?
?
r
?
P (Q | r)
?
P (r)
?
?
r
?
w
?
P (w | r)
?
P (r)
One other reason that we estimated
?
P (w | r) and
?
P (r) for sub-relations is
that Freebase relations share some com-
mon structures in between them. For in-
stance, both people.person.parents and
fictional universe.fictional character.parents
960
indicate the parent relationship but the latter is
much less commonly annotated. We hope that the
shared sub-relation, parents, can help better esti-
mate for the less annotated. Note that the backoff
model would have a much smaller value than the
original, due to double multiplication
?
r
?
w
. In
practice we normalize it by the sub-relations size
to keep it at the same scale with
?
P (R | Q).
Finally, to estimate the prior and conditional
probability, we need a massive data collection.
5.2 Steps
The ClueWeb09
6
dataset is a collection of 1 billion
webpages (5TB compressed in raw HTML) in 10
languages by Carnegie Mellon University in 2009.
FACC1, the Freebase Annotation of the ClueWeb
Corpus version 1 (Gabrilovich et al, 2013), con-
tains index and offset of Freebase entities within
the English portion of ClueWeb. Out of all 500
million English documents, 340 million were au-
tomatically annotated with at least one entity, with
an average of 15 entity mentions per document.
The precision and recall of annotation were esti-
mated at 80?85% and 70?85% (Orr et al, 2013).
Given these two resources, for each binary Free-
base relation, we can find a collection of sentences
each of which contains both of its arguments, then
simply learn how words in these sentences are as-
sociated with this relation, i.e.,
?
P (w | R) and
?
P (w | r). By counting how many times each rela-
tion R was annotated, we can estimate
?
P (R) and
?
P (r). The learning task can be framed in the fol-
lowing short steps:
1. We split each HTML document by sentences
(Kiss and Strunk, 2006) using NLTK (Bird and
Loper, 2004) and extracted those with at least
two Freebase entities which has at least one di-
rect established relation according to Freebase.
2. The extraction formed two parallel corpora,
one with ?relation - sentence? pairs (for esti-
mating
?
P (w | R) and
?
P (R)) and the other with
?subrelations - sentence? pairs (for
?
P (w | r)
and
?
P (r)). Each corpus has 1.2 billion pairs.
3. The tricky part was to align these 1.2 billion
pairs. Since the relations on one side of these
pairs are not natural sentences, we ran the
most simple IBM alignment Model 1 (Brown
et al, 1993) to estimate the translation proba-
bility with GIZA++ (Och and Ney, 2003). To
speed up, the 1.2 billion pairs were split into
6
http://lemurproject.org/clueweb09/
0 ? 10 ? 10
2
? 10
3
? 10
4
> 10
4
7.0% 0.7% 1.2% 0.4% 1.3% 89.5%
Table 1: Percentage of answer relations (the in-
coming relation connected to the answer node)
with respect to how many sentences we learned
this relation from in CluewebMapping. For in-
stance, the first column says there are 7% of an-
swer relations for which we cannot find a mapping
(so we had to use the backoff probability estima-
tion); the last column says there are 89.5% of an-
swer relations that we were able to learn the map-
ping between this relation and text based on more
than 10 thousand relation-sentence pairs. The total
number of answer relations is 7886.
100 even chunks. We ran 5 iterations of EM on
each one and finally aligned the 1.2 billion pairs
from both directions. To symmetrize the align-
ment, common MT heuristics INTERSECTION,
UNION, GROW-DIAG-FINAL, and GROW-DIAG-
FINAL-AND (Koehn, 2010) were separately ap-
plied and evaluated later.
4. Treating the aligned pairs as observation, the
co-occurrence matrix between aligning rela-
tions and words was computed. There were
10,484 relations and sub-relations in all, and we
kept the top 20,000 words.
5. From the co-occurrence matrix we computed
?
P (w | R),
?
P (R),
?
P (w | r) and
?
P (r).
Hand-checking the learned probabilities shows
both success, failure and some bias. For in-
stance, for the film.actor.film relation (mapping
from film names to actor names), the top words
given by
?
P (w | R) are won, star, among, show.
For the film.film.directed by relation, some im-
portant stop words that could indicate this re-
lation, such as by and with, rank directly after
director and direct. However, due to signifi-
cant popular interest in certain news categories,
and the resultant catering by websites to those
information desires, then for example we also
learned a heavily correlated connection between
Jennifer Aniston and celebrity.infidelity.victim,
and between some other you-know-who names
and celebrity.infidelity.participant.
We next formally evaluate how the learned map-
ping help predict relations from words.
961
5.3 Evaluation
Both ClueWeb and its Freebase annotation has a
bias. Thus we were firstly interested in the cov-
erage of mined relation mappings. As a com-
parison, we used a dataset of relation mapping
contributed by Berant et al (2013) and Lin et al
(2012). The idea is very similar: they intersected
Freebase relations with predicates in (arg1, predi-
cate, arg2) triples extracted from ReVerb to learn
the mapping between Freebase relations and triple
predicates. Note the scale difference: although
ReVerb was also extracted from ClueWeb09,
there were only 15 million triples to intersect with
the relations, while we had 1.2 billion alignment
pairs. We call this dataset ReverbMapping and
ours CluewebMapping.
The evaluation dataset, WEBQUESTIONS, was
also contributed by Berant et al (2013). It con-
tains 3778 training and 2032 test questions col-
lected from the Google Suggest service. All ques-
tions were annotated with answers from Freebase.
Some questions have more than one answer, such
as what to see near sedona arizona?.
We evaluated on the training set in two aspects:
coverage and prediction performance. We define
answer node as the node that is the answer and
answer relation as the relation from the answer
node to its direct parent. Then we computed how
much and how well the answer relation was trig-
gered by ReverbMapping and CluewebMapping.
Thus for the question, who is the father of King
George VI, we ask two questions: does the map-
ping, 1. (coverage) contain the answer relation
people.person.parents? 2. (precision) predict
the answer relation from the question?
Table 1 shows the coverage of CluewebMap-
ping, which covers 93.0% of all answer rela-
tions. Among them, we were able to learn the rule
mapping using more than 10 thousand relation-
sentence pairs for each of the 89.5% of all an-
swer relations. In contrast, ReverbMapping covers
89.7% of the answer relations.
Next we evaluated the prediction performance,
using the evaluation metrics of information re-
trieval. For each question, we extracted all rela-
tions in its corresponding topic graph, and ranked
each relation with whether it is the answer re-
lation. For instance, for the previous exam-
ple question, we want to rank the relation peo-
ple.person.parents as number 1. We com-
puted standard MAP (Mean Average Precision)
and MRR (Mean Reciprocal Rank), shown in Ta-
ble 2(a). As a simple baseline, ?word overlap?
counts the overlap between relations and the ques-
tion. CluewebMapping ranks each relation by
?
P (R | Q). ReverbMapping does the same, ex-
cept that we took a uniform distribution on
?
P (w |
R) and
?
P (R) since the contributed dataset did
not include co-occurrence counts to estimate these
probabilities.
7
Note that the median rank from
CluewebMapping is only 12, indicating that half
of all answer relations are ranked in the top 12.
Table 2(b) further shows the percentage of
answer relations with respect to their rank-
ing. CluewebMapping successfully ranked 19%
of answer relations as top 1. A sample
of these includes person.place of birth, loca-
tion.containedby, country.currency used, reg-
ular tv appearance.actor, etc. These percentage
numbers are good clue for feature design: for in-
stance, we may be confident in a relation if it is
ranked top 5 or 10 by CluewebMapping.
To conclude, we found that CluewebMapping
provides satisfying coverage on the 3778 training
questions: only 7% were missing, despite the bi-
ased nature of web data. Also, CluewebMapping
gives reasonably good precision on its prediction,
despite the noisy nature of web data. We move on
to fully evaluate the final QA F
1
.
6 Experiments
We evaluate the final F
1
in this section. The sys-
tem of comparison is that of Berant et al (2013).
Data We re-used WEBQUESTIONS, a dataset
collected by Berant et al (2013). It contains 5810
questions crawled from the Google Suggest ser-
vice, with answers annotated on Amazon Mechan-
ical Turk. All questions contain at least one an-
swer from Freebase. This dataset has been split by
65%/35% into TRAIN-ALL and TEST. We further
randomly divided TRAIN-ALL by 80%/20% to a
smaller TRAIN and development set DEV. Note
that our DEV set is different from that of Berant
et al (2013), but the final result on TEST is di-
rectly comparable. Results are reported in terms
of macro F
1
with partial credit (following Berant
et al (2013)) if a predicted answer list does not
have a perfect match with all gold answers, as a
7
The way we used ReverbMapping was not how Berant et
al. (2013) originally used it: they employed a discriminative
log-linear model to judge relations and that might yield better
performance. As a fair comparison, ranking of CluewebMap-
ping under uniform distribution is also included in Table 2(a).
962
Median Rank MAP MRR
word overlap 471 0.0380 0.0590
ReverbMapping 60 0.0691 0.0829
CluewebMapping 12 0.2074 0.2900
with uniform dist. 61 0.0544 0.0561
(a) Ranking on answer relations. Best result on
CluewebMapping was under the GROW-DIAG-FINAL-AND
heuristics (row 3) when symmetrizing alignment from both
directions. The last row shows ranking of CluewebMapping
under uniform distribution (assuming counting on words and
relations is not known).
1 ? 5 ? 10 ? 50 ? 100 > 100
w. o. 3.5 4.7 2.5 3.9 4.1 81.3
R.M. 2.6 9.1 8.6 26.0 13.0 40.7
C.M. 19.0 19.9 8.9 22.3 7.5 22.4
(b) Percentage of answer relations w.r.t. ranking number
(header). w.o.: word overlap; R.M.: ReverbMapping; C.M.:
CluewebMapping.
Table 2: Evaluation on answer relation ranking
prediction on 3778 training questions.
lot of questions in WEBQUESTIONS contain more
than one answer.
Search With an Information Retrieval (IR)
front-end, we need to locate the exact Freebase
topic node a question is about. For this pur-
pose we used the Freebase Search API (Freebase,
2013a).All named entities
8
in a question were sent
to this API, which returned a ranked list of rele-
vant topics. We also evaluated how well the search
API served the IR purpose. WEBQUESTIONS not
only has answers annotated, but also which Free-
base topic nodes the answers come from. Thus
we evaluated the ranking of retrieval with the gold
standard annotation on TRAIN-ALL, shown in Ta-
ble 3. The top 2 results of the Search API con-
tain gold standard topics for more than 90% of the
questions and the top 10 results contain more than
95%. We took this as a ?good enough? IR front-
end and used it on TEST.
Once a topic is obtained we query the Freebase
Topic API (Freebase, 2013b) to retrieve all rele-
vant information, resulting in a topic graph. The
API returns almost identical information as dis-
played via a web browser to a user viewing this
topic. Given that turkers annotated answers based
on the topic page via a browser, this supports the
assumption that the same answer would be located
in the topic graph, which is then passed to the QA
engine for feature extraction and classification.
8
When no named entities are detected, we fall back to
noun phrases.
top 1 2 3 5 10
# 3263 3456 3532 3574 3604
% 86.4 91.5 93.5 94.6 95.4
Table 3: Evaluation on the Freebase Search API:
how many questions? top n retrieved results con-
tain the gold standard topic. Total number of ques-
tions is 3778 (size of TRAIN-ALL). There were
only 5 questions with no retrieved results.
P R F
1
basic 57.3 30.1 39.5
+ word overlap 56.0 31.4 40.2
+ CluewebMapping 59.9 35.4 44.5
+both 59.0 35.4 44.3
Table 4: F
1
on DEV with different feature settings.
Model Tuning We treat QA on Freebase as a
binary classification task: for each node in the
topic graph, we extract features and judge whether
it is the answer node. Every question was pro-
cessed by the Stanford CoreNLP suite with the
caseless model. Then the question features (?4.1)
and node features (?4.2) were combined (?4.3)
for each node. The learning problem is chal-
lenging: for about 3000 questions in TRAIN,
there are 3 million nodes (1000 nodes per topic
graph), and 7 million feature types. We em-
ployed a high-performance machine learning tool,
Classias (Okazaki, 2009). Training usually
took around 4 hours. We experimented with vari-
ous discriminative learners on DEV, including lo-
gistic regression, perceptron and SVM, and found
L1 regularized logistic regression to give the best
result. The L1 regularization encourages sparse
features by driving feature weights towards zero,
which was ideal for the over-generated feature
space. After training, we had around 30 thousand
features with non-zero weights, a 200 fold reduc-
tion from the original features.
Also, we did an ablation test on DEV about
how additional features on the mapping between
Freebase relations and the original questions help,
with three feature settings: 1) ?basic? features in-
clude feature productions read off from the fea-
ture graph (Figure 1); 2) ?+ word overlap? adds
additional features on whether sub-relations have
overlap with the question; and 3) ?+ CluewebMap-
ping? adds the ranking of relation prediction given
the question according to CluewebMapping. Ta-
ble 4 shows that the additional CluewebMapping
963
P R F
1
Gold Retrieval 45.4 52.2 48.6
Freebase Search API 38.8 45.8 42.0
Berant et al (2013) - - 31.4
Table 5: F
1
on TEST with Gold Retrieval and
Freebase Search API as the IR front end. Berant
et al (2013) actually reported accuracy on this
dataset. However, since their system predicted an-
swers for almost every question (p.c.), it is roughly
that precision=recall=F
1
=accuracy for them.
features improved overall F
1
by 5%, a 13% rel-
ative improvement: a remarkable gain given that
the model already learned a strong correlation be-
tween question types and answer types (explained
more in discussion and Table 6 later).
Finally, the ratio of positive vs. negative exam-
ples affect final F
1
: the more positive examples,
the lower the precision and the higher the recall.
Under the original setting, this ratio was about
1 : 275. This produced precision around 60%
and recall around 35% (c.f. Table 4). To optimize
for F
1
, we down-sampled the negative examples to
20%, i.e., a new ratio of 1 : 55. This boosted the
final F
1
on DEV to 48%. We report the final TEST
result under this down-sampled training. In prac-
tice the precision/recall balance can be adjusted by
the positive/negative ratio.
Test Results Table 5 gives the final F
1
on TEST.
?Gold Retrieval? always ranked the correct topic
node top 1, a perfect IR front-end assumption. In
a more realistic scenario, we had already evaluated
that the Freebase Search API returned the correct
topic node 95% of the time in its top 10 results (c.f.
Table 3), thus we also tested on the top 10 results
returned by the Search API. To keep things sim-
ple, we did not perform answer voting, but sim-
ply extracted answers from the first (ranked by the
Search API) topic node with predicted answer(s)
found. The final F
1
of 42.0% gives a relative im-
provement over previous best result (Berant et al,
2013) of 31.4% by one third.
One question of interest is whether our system,
aided by the massive web data, can be fairly com-
pared to the semantic parsing approaches (note
that Berant et al (2013) also used ClueWeb in-
directly through ReVerb). Thus we took out
the word overlapping and CluewebMapping based
features, and the new F
1
on TEST was 36.9%.
The other question of interest is that whether
our system has acquired some level of ?machine
wgt. feature
5.56 qfocus=money|type=Currency
5.35 qverb=die|type=Cause Of Death
5.11 qword=when|type=datetime
4.56 qverb=border|rel=location.adjoins
3.90 qword=why|incoming relation rank=top 3
2.94 qverb=go|qtopic=location|type=Tourist attraction
-3.94 qtopic=location|rel=location.imports exports.date
-2.93 qtopic=person|rel=education.end date
Table 6: A sample of the top 50 most positive/neg-
ative features. Features are production between
question and node features (c.f. Figure 1).
intelligence?: how much does it know what the
question inquires? We discuss it below through
feature and error analysis.
Discussion The combination between questions
and Freebase nodes captures some real gist of QA
pattern typing, shown in Table 6 with sampled fea-
tures and weights. Our system learned, for in-
stance, when the question asks for geographic ad-
jacency information (qverb=border), the correct
answer relation to look for is location.adjoins.
Detailed comparison with the output from Berant
et al (2013) is a work in progress and will be pre-
sented in a follow-up report.
7 Conclusion
We proposed an automatic method for Question
Answering from structured data source (Free-
base). Our approach associates question features
with answer patterns described by Freebase and
has achieved state-of-the-art results on a balanced
and realistic QA corpus. To compensate for the
problem of domain mismatch or overfitting, we
exploited ClueWeb, mined mappings between KB
relations and natural language text, and showed
that it helped both relation prediction and an-
swer extraction. Our method employs relatively
lightweight machinery but has good performance.
We hope that this result establishes a new baseline
against which semantic parsing researchers can
measure their progress towards deeper language
understanding and answering of human questions.
Acknowledgments We thank the Allen Institute
for Artificial Intelligence for funding this work.
We are also grateful to Jonathan Berant, Tom
Kwiatkowski, Qingqing Cai, Adam Lopez, Chris
Callison-Burch and Peter Clark for helpful discus-
sion and to the reviewers for insightful comments.
964
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to Interpret Natural Language Navigation In-
structions from Observations. In AAAI, volume 2,
pages 1?2.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: Search and candidate generation.
IBM Journal of Research and Development.
J?er?ome Euzenat and Pavel Shvaiko. 2007. Ontology
matching. Springer.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte J?org, and
Ulrich Sch?afer. 2007. Question answering from
structured knowledge sources. Journal of Applied
Logic, 5(1):20?48.
Freebase. 2013a. Freebase Search API.
https://developers.google.com/freebase/v1/search-
overview.
Freebase. 2013b. Freebase Topic API.
https://developers.google.com/freebase/v1/topic-
overview.
Evgeniy Gabrilovich, Michael Ringgaard, , and Amar-
nag Subramanya. 2013. FACC1: Freebase anno-
tation of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).
http://lemurproject.org/clueweb09/FACC1/, June.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219?224. ACM.
Jerry R Hobbs. 1985. Ontological promiscuity. In
Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229?232. ACM.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proceedings of ACL.
Rohit J Kate and Raymond J Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of ACL.
Tibor Kiss and Jan Strunk. 2006. Unsupervised mul-
tilingual sentence boundary detection. Computa-
tional Linguistics, 32(4):485?525.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP-CoNLL.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223?1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
Thomas Lin, Oren Etzioni, et al 2012. Entity Linking
at Web Scale. In Proceedings of Knowledge Extrac-
tion Workshop (AKBC-WEKEX), pages 84?88.
965
Vanessa Lopez, Michele Pasin, and Enrico Motta.
2005. Aqualog: An ontology-portable question an-
swering system for the semantic web. In The Seman-
tic Web: Research and Applications, pages 546?562.
Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2009. Classias: a collection of
machine-learning algorithms for classification.
Dave Orr, Amar Subramanya, Evgeniy Gabrilovich,
and Michael Ringgaard. 2013. 11 billion
clues in 800 million documents: A web re-
search corpus annotated with freebase concepts.
http://googleresearch.blogspot.com/2013/07/11-
billion-clues-in-800-million.html, July.
Erhard Rahm and Philip A Bernstein. 2001. A survey
of approaches to automatic schema matching. the
VLDB Journal, 10(4):334?350.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S?oren Auer. 2013. Question answering on in-
terlinked data. In Proceedings of WWW.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001, pages 466?477. Springer.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web.
Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of ACL.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. Linguistic structures processing, 5:521?
569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In Proceedings of EMNLP.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
966
Proceedings of the TextGraphs-6 Workshop, pages 10?14,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Nonparametric Bayesian Word Sense Induction
Xuchen Yao1 and Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We propose the use of a nonparametric Bayesian
model, the Hierarchical Dirichlet Process (HDP),
for the task of Word Sense Induction. Results are
shown through comparison against Latent Dirich-
let Allocation (LDA), a parametric Bayesian model
employed by Brody and Lapata (2009) for this task.
We find that the two models achieve similar levels
of induction quality, while the HDP confers the ad-
vantage of automatically inducing a variable num-
ber of senses per word, as compared to manually
fixing the number of senses a priori, as in LDA.
This flexibility allows for the model to adapt to
terms with greater or lesser polysemy, when ev-
idenced by corpus distributional statistics. When
trained on out-of-domain data, experimental results
confirm the model?s ability to make use of a re-
stricted set of topically coherent induced senses,
when then applied in a restricted domain.
1 Introduction
Word Sense Induction (WSI) is the task of automat-
ically discovering latent senses for each word type,
across a collection of that word?s tokens situated in
context. WSI differs from Word Sense Disambigua-
tion (WSD) in that the task does not assume access
to some prespecified sense inventory. This amounts
to a clustering task: instances of a word are parti-
tioned into the same bin based on whether a sys-
tem deems them to have the same underlying mean-
ing. A large body of related work can be found
in (Schu?tze, 1998; Pantel and Lin, 2002; Dorow
and Widdows, 2003; Purandare and Pedersen, 2004;
Bordag, 2006; Niu et al, 2007; Pedersen, 2007;
Brody and Lapata, 2009; Li et al, 2010; Klapaftis
and Manandhar, 2010).
Brody and Lapata (2009) (B&L herein) showed
that the parametric Bayesian model, Latent Dirich-
let Allocation (LDA), could be successfully em-
ployed for this task, as compared to previous re-
sults published for the WSI component of SemEval-
20071 (Agirre and Soroa, 2007). A deficiency of the
LDA model for WSI is that the number of senses
needs to be manually specified a priori, either sepa-
rately for each word type, or (as done by B&L) some
fixed value that is shared globally across all types.
Nonparametric methods instead have the flexibil-
ity of automatically deciding the number of sense
cluters (Vlachos et al, 2009; Reisinger and Mooney,
2010). In this work we first independently verify
the results of B&L, and then tackle the limitation
on fixing the number of senses through the use of
the Hierarchical Dirichlet Process (HDP) (Teh et al,
2006), a nonparametric Bayesian model. We show
this approach leads to results of similar quality as
LDA, when using a bag-of-words context model, in
addition to allowing for variability in the number of
senses across different words and domains. When
trained on a restricted domain corpus for which
manually labeled sense data was present, we verify
that the model may be tuned to posit a similar num-
ber of senses as determined by human judges. When
trained on a broader domain collection, we show that
the number of induced senses increase, in line with
the intuition that a wider set of genres should lead
to a greater diversity in underlying meanings. Auto-
matically inducing the proper number of senses has
great practical implications, especially in areas that
require word sense disambiguation. For instance, in-
ducing more senses for bank helps to tell differ-
1Klapaftis and Manandhar (2010) and Brody and Lapata
(2009) reported the best scores so far on this dataset.
10
ent word senses apart for naturally more ambigu-
ous words, and inducing less senses for job helps
to prevent assigning too fined-grained senses in case
the same words in two similar contexts are mistak-
enly regarded as carrying different senses.
2 Bayesian Word Sense Induction
wm,n
sm ,n
??m??
??k?? n?[1,Nm]m?[1,M ]k?[1,K ]
Figure 1: Latent Dirichlet Allocation (LDA) for WSI.
As in prior work including B&L, we rely on
the intuition that the senses of words are hinted at
by their contextual information (Yarowsky, 1992).
From the perspective of a generative process, neigh-
boring words of a target are generated by the target?s
underlying sense.2
Both LDA and HDP define graphical models that
generate collections of discrete data. The sense of
a target word is first drawn from a distribution and
then the context of this word is generated according
to that distribution. But while LDA assumes a fixed,
finite set of distributions, the HDP draws from an
infinite set of distributions generated by a Dirichlet
Process. This section details the distinction.
Figure 1 shows the LDA model for word sense
induction. The conventional notion of document is
replaced by a pseudo-document, consisting of every
word in an Nm-word window centered on the target
item. wm,n is the n-th token of the m-th pseudo-
document for target word w. sm,n is the correspond-
ing sense for wm,n. Suppose there are K senses for
the target word w, then the distribution over a con-
text word wm,n is:
2For instance, given the word bank with a sense river
bank, it is more likely that the neighboring words are river,
lake and water than finance, money and loan.
wm,n
sm ,n
k m? 1
n??,? Nm?m? ?,? M?
k 1
K
?
Figure 2: Hierarchical Dirichlet Process (HDP) for WSI.
p(wm,n) =
K?
k=1
p(wm,n | sm,n = k)p(sm,n = k).
Let the word distribution given a sense be
p(wm,n | sm,n = k) = ~?k, which is a vector of
length V (vocabulary size) that is generated from
a Dirichlet distribution: ~?k ? Dir(~?). Let the
sense distribution given a document be p(sm,n | d =
m) = ~?m, which is a vector of length K that is gen-
erated from a Dirichlet distribution: ~?m ? Dir(~?).
The generative story for the data under LDA is then:
For k ? (1, ...,K) senses:
Sample mixture component: ~?k ? Dir(~?).
For m ? (1, ...,M) pseudo-documents:
Sample topic components ~?m ? Dir(~?).
For n ? (1, ..., Nm) words in pseudo-document m:
Sample sense index sm,n ?Mult(~?m).
Sample word wm,n ?Mult(~?sm,n).
The sense distribution over a word is captured
as K mixture components. In the HDP however,
we assume the number of active components is un-
known, and should be inferred from the data. For
each pseudo-document, the sense component sm,n
for word wm,n has a nonparametric prior Gm. Gm
is nonparametric in the sense that for every new
pseudo-document m, a new Gm is sampled from a
base distribution G0. As the corpus grows, there are
11
more and more Gm?s. However, the mixture com-
ponent sm,n, drawn from Gm, can be shared among
pseudo-documents. Thus the number of senses do
not simply multiply out as m grows. Both G0 and
Gm?s are distributed according to a Dirichlet Process
(DP) (Ferguson, 1973). The generative story is:
Select base distribution G0 ? DP (?,H) which
provides an unlimited inventory of senses.
For m ? (1, ...,M) pseudo-documents:
Draw Gm ? DP (?0, G0).
For n ? (1, ..., Nm) words in pseudo-document m:
Sample sm,n ? Gm.
Sample wm,n ?Mult(~?sm,n).
Hyperparameters ? and ?0 are the concentration
parameters of the DP, controlling the variability of
the distributionsG0 andGm. In a Chinese restaurant
franchise metaphor of the HDP, multiple restaurants
(documents) share a set of dishes (senses). Then
? controls the variability of the global sense distri-
bution and ?0 controls the variability of each cus-
tomer?s (word) choice of dishes (senses).3
3 Experiment Setting
Model B&L experimented with variations to the
LDA model that allowed for generating multiple lay-
ers of features, such as smaller (5w) and larger (10w)
bag-of-word contexts, and syntactic features. The
additional complexity beyond the standard model
led to only tenuous performance gains. Normal
LDA, when trained on pseudo-documents built from
10 words of surrounding context, performed only
slightly below their best reported results.4 Espe-
cially as our goal here was to investigate the sense-
specification problem, rather than eking out further
improvements in the base WSI evaluation measure,
we chose to compare a standard LDA model to HDP,
both strictly using a 10 word context.5
Test Data Following B&L, we perform WSI on
nouns. The evaluation data comes from the WSI
task of SemEval-2007 (Agirre and Soroa, 2007). It
is derived from the Wall Street Journal portion of
3Gibbs sampling (Geman and Geman, 1990) can be applied
for inference. Specifically, Teh et al (2006) describes the pos-
terior sampling in the Chinese restaurant franchise.
4F-score of 86.9% (10w), as compared to 87.3% (10w+5w).
5We relied on implementations of LDA and HDP respec-
tively from MALLET (McCallum, 2002), and Wang (2010).
the Penn TreeBank (Marcus et al, 1994) and con-
tains 15,852 instances of excerpts on 35 nouns. All
the nouns are hand-annotated with their OntoNotes
senses (Hovy et al, 2006), with an average of 3.9
senses per word.
Evaluation Method WSI is an unsupervised task
that results in sense clusters with no explicit map-
ping to manually annotated sense data. To derive
such a mapping, we follow the supervised evalua-
tion strategy of Agirre and Soroa (2007). Anno-
tated senses from SemEval-2007 are partitioned into
a standard mapping set (72%), a dev set (14%) and a
test set (14%). After an WSI system has tagged the
elements in the mapping set with their ?cluster IDs?,
then a cluster to sense derivation is constructed by
simply assigning to each cluster the manual sense
label that has the highest in-cluster frequency. Once
such a mapping has been established, then results
on the dev or test set are reported based on treating
cluster assignment as a WSD operation.
Training Data As out-of-domain source, we ex-
tracted 930K instances of the 35 nouns from the
British National Corpus (BNC) (Clear, 1993). As
in-domain source we extracted another 930K in-
stances from WSJ in years 87/88/90/94. All pseudo-
documents use the ?10 contextual window.
4 Evaluation
We trained the LDA and HDP models on the WSJ
and BNC datasets separately. In their experiments
with LDA, B&L iteratively tried 3 up to 9 senses,
and then reported the number that led to best re-
sults in evaluation (4 senses for WSJ, 8 for BNC).
We repeated this approach for LDA, with hyper-
parameters ? = 0.02 and ? = 0.1. For the HDP
model, we tuned hyper-parameters on the SemEval-
2007 dev set.6 See Table 1 for results, averaged over
5 runs of LDA and 3 runs of HDP.
We report several findings based on this experi-
ment. First, for the LDA models trained on WSJ
and BNC, our F1 measures are 0.8% lower than
reported by B&L.7 Second, based on our own ex-
periment, the HDP model performance is slightly
better than that of LDA when training with BNC.
6Final parameters: H = 0.1, ?0 ? Gamma(0.1, 0.028),
? ? Gamma(1, 0.1).
7We consider this acceptable experimental deviation, given
the minor variation in respective training data.
12
WSJ BNC
LDA-4s* 86.9 LDA-8s* 84.6
LDA-4s 86.1 LDA-8s 83.8
HDP 86.7 HDP 85.74
Table 1: F-measure when training with WSJ (in-domain) and
BNC (out-of-domain). Results with * are taken from B&L. 4
or 8 senses were used per word. 4: statistically significant
against LDA-8s by paired permutation test with p < 0.001.
The standard baseline, always picking the most frequent sense
observed in training, scores 80.9.
WSJ BNC
Train Test Train Test
LDA 4.0 3.9 8.0 7.4
HDP 5.8 3.9 9.4 4.6
Table 2: The average number of senses the LDA and HDP
models output when training with WSJ/BNC and testing on
SemEval-2007, which has 3.9 senses per word on average.
Third, the HDP model appears to better adapt to data
in other domains. When switching the training set
from WSJ (in-domain) to BNC (out-of-domain), we,
along with B&L, found a 2.3% drop with LDA mod-
els. However, with the HDP model, there is only a
1% drop in F1. Moreover, even trained on out-of-
domain data, HDP can still better infer the number
of senses from the test data, which is illustrated next.
Table 2 shows the number of senses induced from
each dataset. When training on WSJ and test on
SemEval-2007, HDP induced the correct number of
senses (3.9 on average) from test, while LDA did
this by assuming 4 senses from the training data.
When there is a domain mismatch between train-
ing (BNC) and test (SemEval-2007, which comes
from the 1989 WSJ), the LDA model preferred far
more than the annotated number of senses (7.4 vs.
3.9), largely due to the fact that it assumed 8 senses
during training. However, even though the HDP
model induced more senses (9.4) when training on
the broader coverage BNC set, it still inferred a
much reduced average of 4.6 senses on test.
The BNC, being a balanced corpus, covers more
diverse genres than the WSJ: we would expect it to
lead to a more inclusive model of word sense. Fig-
ure 3 illustrates this comparison through the differ-
ence between sense numbers. For the 35 human-
annotated nouns, HDP induced the number of senses
mostly within an error of ?2, whereas LDA tended
to prefer 3 ? 6 more senses than recognized by an-
-4 -3 -2 -1 0 1 2 3 4 5 60
2
4
6
8
10
12 HDPLDA
# induced senses - # annotated senses
frequ
enc y
Figure 3: The difference between induced number of senses
and annotated senses. The training set is BNC. The test set
is SemEval-2007, containing 35 nouns with 3.9 senses. LDA
induced 7.4 senses and HDP induced 4.6 senses on average.
WSJ BNC
LDA-5.8s 86.0 LDA-9.4s 82.7
LDA-3.9s 85.3 LDA-3.9s 81.4
HDP-5.8s 86.7 HDP-9.4s 85.74
Table 3: F1 measure when training LDA with three other set-
tings: 5.8s, 9.4s and 3.9s. 4: statistically significant against
both LDA-9.4s and LDA-3.9s (for BNC) by paired permutation
test with p < 0.001.
notators (on average the HDP model was off by 1.6
senses, as compared to 3.6 by LDA). Finally, the
F1 performance of HDP is 1.9% better than LDA
(85.7% vs. 83.8%).
We further evaluated the LDA model by training
separately for each of the 35 nouns, first setting as
the number of topics the amount induced by HDP
(on average, 5.8/9.4 senses for WSJ/BNC), then us-
ing the number of senses as used by the human anno-
tators in SemEval-2007 (an average of 3.8). As seen
in Table 3, in each of these cases HDP remained the
superior model.
5 Conclusion
We proposed the use of a nonparametric Bayesian
model (HDP) for word sense induction and com-
pared it with the parametric model by Brody and
Lapata (2009), based on LDA. The HDP model con-
fers the advantage of automatically identifying the
number of senses, besides having equivalent (or bet-
ter) performance than the LDA model, verified us-
ing the SemEval-2007 dataset. Future work includes
large scale sense induction over a larger vocabulary,
in tasks such as Paraphrase Acquisition.
13
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 Task 02:
Evaluating Word Sense Induction And Discrimination Sys-
tems. In Proceedings of the 4th International Workshop on
Semantic Evaluations, SemEval ?07, pages 7?12.
Stefan Bordag. 2006. Word Sense Induction: Triplet-Based
Clustering And Automatic Evaluation. In Proceedings of the
11th EACL, pages 137?144.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word Sense
Induction. In EACL ?09: Proceedings of the 12th Confer-
ence of the European Chapter of the Association for Compu-
tational Linguistics, pages 103?111.
Jeremy H. Clear, 1993. The British national corpus, pages 163?
187. MIT Press, Cambridge, MA, USA.
Beate Dorow and Dominic Widdows. 2003. Discovering
Corpus-Specific Word Senses. In Proceedings of the tenth
conference on European chapter of the Association for Com-
putational Linguistics, EACL ?03, pages 79?82.
T. S. Ferguson. 1973. A Bayesian Analysis of Some Nonpara-
metric Problems. The Annals of Statistics, 1(2):209?230.
S. Geman and D. Geman, 1990. Stochastic Relaxation, Gibbs
Distributions, And The Bayesian Restoration Of Images,
pages 452?472.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the
90% solution. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion Volume:
Short Papers, NAACL-Short ?06, pages 57?60.
Ioannis Klapaftis and Suresh Manandhar. 2010. Word Sense
Induction & Disambiguation Using Hierarchical Random
Graphs. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages 745?
755, October.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic
Models For Word Sense Disambiguation And Token-Based
Idiom Detection. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL ?10,
pages 1138?1147.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Andrew Kachites McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2R:
Three Systems For Word Sense Discrimination, Chinese
Word Sense Disambiguation, And English Word Sense Dis-
ambiguation. In Proceedings of the 4th International Work-
shop on Semantic Evaluations, SemEval ?07, pages 177?
182.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses From Text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery
and data mining, KDD ?02, pages 613?619.
Ted Pedersen. 2007. UMND2: SenseClusters applied to the
sense induction task of Senseval-4. In Proceedings of the 4th
International Workshop on Semantic Evaluations, SemEval
?07, pages 394?397.
Amruta Purandare and Ted Pedersen. 2004. Word Sense Dis-
crimination by Clustering Contexts in Vector and Similarity
Spaces. In Proceedings of CoNLL-2004, pages 41?48.
Joseph Reisinger and Raymond J. Mooney. 2010. A Mixture
Model with Sharing for Lexical Semantics. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2010), pages 1173?1182, MIT,
Massachusetts, USA.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrimination.
Comput. Linguist., 24:97?123, March.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet Processes. Journal of the American
Statistical Association, 101(476):1566?1581.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani.
2009. Unsupervised and constrained Dirichlet process mix-
ture models for verb clustering. In Proceedings of the Work-
shop on Geometrical Models of Natural Language Seman-
tics, GEMS ?09, pages 74?82.
Chong Wang. 2010. An implementation of hierarchical dirich-
let process (HDP) with split-merge operations.
David Yarowsky. 1992. Word-Sense Disambiguation Using
Statistical Models Of Roget?s Categories Trained On Large
Corpora. In Proceedings of the 14th conference on Compu-
tational linguistics, pages 454?460.
14
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 138?143,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Finding More Bilingual Webpages with High Credibility via
Link Analysis
Chengzhi Zhang?
Nanjing University of Science and Technology
Nanjing, China
Xuchen Yao?
Johns Hopkins University
Baltimore, MD, USA
Chunyu Kit
City University of Hong Kong, Hong Kong SAR, China
Abstract
This paper presents an efficient approach
to finding more bilingual webpage pairs
with high credibility via link analysis, us-
ing little prior knowledge or heuristics.
It extends from a previous algorithm that
takes the number of bilingual URL pairs
that a key (i.e., a URL pairing pattern) can
match as the objective function to search
for the best set of keys yielding the greatest
number of webpage pairs within targeted
bilingual websites. Enhanced algorithms
are proposed to match more bilingual web-
pages following the credibility based on
statistical analysis of the link relationship
of the seed websites available. With about
12,800 seed websites as test set, the en-
hanced algorithms improve precision over
baseline by more than 5%, from 94.06%
to 99.40%, and hence find above 20%
more true bilingual URL pairs, illustrating
that significantly more bilingual webpages
with high credibility can be mined with the
help of the link analysis.
1 Introduction
Parallel corpora of bilingual text (bitext) are indis-
pensable language resources for many data-driven
tasks of natural language processing, such as sta-
tistical machine translation (Brown et al, 1990),
cross-language information retrieval (Davis and
Dunning, 1995; Oard, 1997), and bilingual lexi-
cal acquisition (Gale and Church, 1991; Melamed,
1997; Jiang et al, 2009), to name but a few. A
general way to develop such corpora from web
texts starts from exploring the structure of known
bilingual websites, which are usually organized
?Performed while a research associate at City University
of Hong Kong.
?Performed while a visiting student at City University of
Hong Kong.
by their web masters in a way to facilitate both
navigation and maintenance (Nie, 2010). The
most common strategy is to create a parallel struc-
ture in terms of URL hierarchies, exploiting some
known naming conventions for webpages of corre-
sponding languages (Huang and Tilley, 2001; Nie,
2010). Following available structures and nam-
ing conventions, researchers have been exploring
various means to mine parallel corpora from the
web and a good number of such systems have
demonstrated the feasibility and practicality in au-
tomatic acquisition of parallel corpora from bilin-
gual and/or multilingual web sites, e.g., STRAND
(Resnik, 1998; Resnik, 1999; Resnik and Smith,
2003), BITS (Ma and Liberman, 1999), PTMiner
(Chen and Nie, 2000), PTI (Chen et al, 2004),
WPDE (Zhang et al, 2006), the DOM tree align-
ment model (Shi et al, 2006), PagePairGetter (YE
et al, 2008) and Bitextor (Espla`-Gomis and For-
cada, 2010).
Most of these systems are run in three steps:
first, bilingual websites are identified and crawled;
second, pairs of parallel webpages are extracted;
and finally, the extracted pairs are validated (Kit
and Ng, 2007). Among them, prior knowledge
about parallel webpages, mostly in the form of ad
hoc heuristics for identifying webpage languages
or pre-defined patterns for matching or comput-
ing similarity between webpages, is commonly
used for webpage pair extraction (Chen and Nie,
2000; Resnik and Smith, 2003; Zhang et al, 2006;
Shi et al, 2006; Yulia and Shuly, 2010; Toma?s
et al, 2008). Specifically, these systems exploit
search engines and heuristics across webpage an-
chors to locate candidate bilingual websites and
then identify webpage pairs based on pre-defined
URL matching patterns. However, ad hoc heuris-
tics cannot exhaust all possible patterns. Many
webpages do not even have any language label
in their anchors, not to mention many untrust-
worthy labels. Also, using a limited set of pre-
138
defined URL patterns inevitably means to give up
all reachable bilingual webpages that fall outside
their coverage.
Addressing such weaknesses of the previous ap-
proaches, we instead present an efficient bilingual
web mining system based on analyzing link rela-
tionship of websites without resorting to prior ad
hoc knowledge. This approach extends, on top of
re-engineering, the previous work of Kit and Ng
(2007). It aims at (1) further advancing the idea
of finding bilingual webpages via automatic dis-
covery of non-ad-hoc bilingual URL pairing pat-
terns, (2) applying the found pairing patterns to
dig out more bilingual webpage pairs, especially
those involving a deep webpage unaccessible by
web crawling, (3) discovering more bilingual web-
sites (and then more bilingual webpages) with
high credibility via statistical analysis of bilingual
URL patterns and link relationship of available
seed websites. The results from our experiments
on 12, 800 seed websites show that the proposed
algorithms can find considerably more bilingual
webpage pairs on top of the baseline, achieving
a significant improvement of pairing precision by
more than 5%.
2 Algorithm
This section first introduces the idea of unsuper-
vised detection of bilingual URL pairing patterns
(?2.1) and then continues to formulate the use of
the detected patterns to explore more websites, in-
cluding deep webpages (?2.2), and those not in-
cluded in our initial website list (?2.3).
2.1 Bilingual URL Pattern Detection
Our current research is conducted on top of the
re-implementation of the intelligent web agent to
automatically identify bilingual URL pairing pat-
terns as described in Kit and Ng (2007). The un-
derlying assumption for this approach is that rather
than random matching, parallel webpages have
static pairing patterns assigned by web masters for
engineering purpose and these patterns are put in
use to match as many pairs of URLs as possible
within the same domain. Given a URL u from the
set U of URLs of the same domain, the web agent
goes through the set U?{u} of all other URLs and
finds among them all those that differ from u by a
single token1 ? a token is naturally separated by
1If language identification has been done on webpages, it
only needs to go through all URLs of the other language.
a special set of characters including slash /, dot .,
hyphen -, and underscore in a URL. Then, the
single-token difference of a candidate URL pairs
is taken as a candidate of URL paring pattern,
and all candidate patterns are put in competition
against each other in a way to allow a stronger one
(that matches more candidate URL pairs) to win
over a weaker one (that matches fewer). For in-
stance, the candidate pattern <en,zh> can be de-
tected from the following candidate URL pair:
www.legco.gov.hk/yr99-00/en/fc/esc/e0.htm
www.legco.gov.hk/yr99-00/zh/fc/esc/e0.htm
The re-implementation has achieved a num-
ber of improvements on the original algorithm
through re-engineering, including the following
major ones.
1. It is enhanced from token-based to character-
based URL matching. Thus, more gen-
eral patterns, such as <e,c>, can be aggre-
gated from a number of weaker ones like
<1e,1c>, <2e,2c>, ..., etc., many of which
may otherwise fail to survive the competition.
2. The original algorithm is speeded up from
O(|U |2) to O(|U |) time, by building in-
verted indices for URLs and establishing
constant lookup time for shortest matching
URL strings.2
3. The language detection component has been
expanded from bilingual to multi-lingual and
hence had the capacity to practically handle
multilingual websites such as those from EU
and UN.
When detected URL patterns are used to match
URLs in a web domain for identifying bilingual
webpages, noisy patterns (most of which are pre-
sumably weak keys) would better be filtered out.
A straightforward strategy to do this is by thresh-
olding the credibility of a pattern, which can be
defined as
C(p, w) = N(p, w)
|w|
.
where N(p, w) is the number of webpages
matched into pairs by pattern p within website w,
and |w| the size ofw in number of webpages. Note
that this is the local credibility of a key with re-
spect to a certain website w. Empirically, Kit and
2Achieved by utilizing SecondString http://second
string.sf.net/
139
Ng (2007) set a threshold of 0.1 to rule out weak
noisy keys.
Some patterns happen to generalize across do-
mains. The global credibility of such a pattern p is
thus computed by summing over all websites in-
volved, in a way that each webpage matched by p
is counted in respect to the local credibility of p in
the respective website:
C(p) =
?
w
C(p, w)N(p, w).
Interestingly, it is observed that many weak keys
ruled out by the threshold 0.1 are in fact good pat-
terns with a nice global credibility value. In prac-
tice, it is important to ?rescue? a local weak key
with strong global credibility. A common practice
is to do it straightforwardly with a global credibil-
ity threshold, e.g., C(p)> 500 as for the current
work.
Finally, the bilingual credibility of a website is
defined as
C(w) = max
p
C(p, w).
It will be used to measure the bilingual degree of a
website in a later phase of our work, for which an
assumption is that bilingual websites tend to link
with other bilingual websites.
2.2 Deep Webpage Recovery
Some websites contain webpages that cannot be
crawled by search engines. These webpages do
not ?exist? until they are created dynamically as
the result of a specific search, mostly triggered by
JavaScript or Flash actions. This kind of webpages
as a whole is called deep web. Specifically, we
are interested in the case where webpages in one
language are visible but their counterparts in the
other language are hidden. A very chance that we
may have to unearth these deep hidden webpages
is that their URLs follow some common naming
conventions for convenience of pairing with their
visible counterparts.
Thus for each of those URLs still missing a
paired URL after the URL matching using our
bilingual URL pattern collection, a candidate URL
will be automatically generated with each applica-
ble pattern in the collection for a trial to access its
possibly hidden counterpart. If found, then mark
them as a candidate pair. For example, the pattern
<english,tc chi> is found applicable to the
first URL in Table 1 and accordingly generates the
second as a candidate link to its English counter-
part, which turns out to be a valid page.
2.3 Incremental Bilingual Website
Exploration
Starting with a seed bilingual website list of size
N , bilingual URL pairing patterns are first mined,
and then used to reach out for other bilingual web-
sites. The assumption for this phase of work is
that bilingual websites are more likely to be ref-
erenced by other bilingual websites. Accordingly,
a weighted version of PageRank is formulated for
prediction.
Firstly, outgoing links and PageRank are used
as baselines. Linkout(w) is the total number of
outgoing links from website w, and the PageRank
of w is defined as (Brin and Page, 1998):
PageRank(w) = r
N
+(1?r)
?
w?M(w)
PageRank(w)
Linkout(w)
,
whereM(w) is the set of websites that link tow in
the seed set of N bilingual websites, and r? [0, 1]
a damping factor empirically set to 0.15. Initially,
the PageRank value of w is 1. In order to re-
duce time and space cost, both Linkout(w) and
PageRank(w) are computed only in terms of the
relationship of bilingual websites in the seed set.
The WeightedPageRank(w) is defined as the
PageRank(w) weighted by w?s credibility C(w).
To reach out for a related website s outside the
initial seed set of websites, our approach first
finds the set R(s) of seed websites that have
outgoing links to s, and then computes the sum
of these three values over each outgoing link,
namely,
?
wLinkout(w),
?
wPageRank(w), and
?
wWeightedPageRank(w) for each w?R(s), for
the purpose of measuring how ?likely? s is bilin-
gual. An illustration of link relationship of this
kind is presented in Figure 1.
In practice, the exploration of related websites
can be combined with bilingual URL pattern de-
tection to literately harvest both bilingual websites
and URL patterns, e.g., through the following pro-
cedure:
1. Starting from a seed set of websites as the
current set, detect bilingual URL patterns and
then use them to identify their bilingual web-
pages.
2. Select the top K linked websites from
the seed set according to either
?
Linkout,
?
PageRank, or
?
WeightedPageRank.
140
(1) http://www.fehd.gov.hk/tc chi/LLB web/cagenda 20070904.htm
(2) http://www.fehd.gov.hk/english/LLB web/cagenda 20070904.htm
Table 1: Illustration of URL generation for a deep webpage
 
 
Rel
ate
d w
ebs
ites
s 1
See
d w
ebs
ites
 
[1, 
0.1
2, 0
.08
] 
 [1
, 0.
21,
 0.1
3] 
[2, 
0.5
6, 0
.29
] 
[1, 
0.0
2, 0
.01
] 
[1, 
0.0
3, 0
.02
] 
 [0,
 0, 
0] 
 [1
, 0.
03,
 0.0
1] 
[1, 
0.1
2, 0
.08
] 
 [1
, 0.
21,
 0.1
3] 
 [3
, 0.
77,
 0.4
2] 
 [3
, 0.
59,
 0.1
3] 
[0, 
0, 0
] 
s 2 s 3 s 4 s 5
w 2
w 6
w 3
w 4
w 5 
w 7
w 1
Figure 1: Illustration of link relationship of seed websites and related websites, with associated
?
Linkout,
?
PageRank and
?
WeightedPageRank in square brackets and with arrows to indicate outgo-
ing links from a seed website to others.
3. Add the top K selected websites to the cur-
rent set, and repeat the above steps for desired
iterations.
3 Evaluation
The implementation of our method results in Pup-
Sniffer,3 a Java-based tool that has been released
for free. A series of experiments were conducted
with it to investigate the performance of the pro-
posed method on about 12, 800 seed websites. A
web interface was also implemented for evaluat-
ing the candidate bilingual webpage pairs identi-
fied by our system.
3.1 Seed Websites
The initial seed websites were collected from two
resources, namely
? Hong Kong Website Directory4 and
? Hong Kong World Wide Web Database.5
After the removal of invalid ones, 12, 800websites
were finally acquired as our seed set.6
3http://code.google.com/p/pupsniffer
4http://www.852.com
5http://www.cuhk.edu.hk/hkwww.htm
6http://mega.ctl.cityu.edu.hk/
?czhang22/pupsniffer-eval/Data/All_Seed_
Websites_List.txt
3.2 URL Pattern Detection and Deep
Webpage Recovery
The enhanced algorithm described in Section 2.1
above was ran to extract credible URL patterns. In
general, the extracted patterns are valid as long as
the threshold is not too low ? it is set to C(p, w)>
0.1 in our experiments. A number of strongest pat-
terns found are presented in Table 2 for demon-
stration. Most of them, especially <en,tc> and
<eng,chi>, are very intuitive patterns. A full
list of URL pairing patterns detected in our exper-
iments is also available.7 Particularly interesting is
that all these patterns were identified in an unsu-
pervised fashion without any manual heuristics.
Using these patterns, the original algorithm re-
trieved about 290K candidate bilingual webpage
pairs. By the simple trick of rescuing weak lo-
cal patterns with the global credibility threshold
C(p) > 500, 10K more webpage pairs were fur-
ther found. Additionally, other 16K webpage
pairs were dug out from deep webpages by auto-
matically generating paired webpages with the aid
of identified URL patterns.
7http://mega.ctl.cityu.edu.hk/
?czhang22/pupsniffer-eval/Data/Pattern_
Credibility_LargeThan100.txt
141
Pattern C(p)
<en,tc> 13997.36
<eng,tc> 12869.56
<english,tc chi> 11436.12
<english,chinese> 11032.46
<eng,chi> 7824.86
Table 2: Top 5 patterns with their global credibility
values.
Method Pairs Precision
Kit and Ng (2007) 290,247 94.06%
Weak key rescue 10,015 89.27%
Deep page recovery 15,825 95.02%
Incremental exploration 37,491 99.40%
Total 348, 058 94.72%
True pair increment 55, 674 20.76%
Table 3: Number of bilingual webpage pairs found
and their precision from sampled evaluation.
3.3 Website Exploration
To go beyond the original 12, 800 websites, the in-
cremental algorithm described in Section 2.3 was
run for one iteration to find outside bilingual web-
sites directly linked from the seeds. The top 500
of them, ranked by
?
Linkout,
?
PageRank and
?
WeightedPageRank, respectively, were manu-
ally checked by five students, giving the curves
of the total number of true bilingual websites and
overall precision per top N websites as plotted
in Figure 2. These results show that almost 50%
of the top 500 related outside websites ranked by
?
WeightedPageRank are true bilingual websites.
A higher precision indicates more bilingual web-
page pairs correctly matched by the URL patterns
in use.
After one iteration of the incremental algorithm,
37K more candidate bilingual webpage pairs were
found in the related outside websites, besides the
290K by the original algorithm. Table 3 presents
the number of webpage pairs identified by each
algorithm with a respective precision drawn from
random sampling. These results suggest that our
proposed enhancement is able to harvest above
20% more bilingual webpage pairs without de-
grading the overall precision. Error analysis shows
that around 80% of errors were due to mistakes
in language identification for webpages. For in-
stance, some Japanese webpages were mistakenly
recognized as Chinese ones.
?
?
?
?
?
0
50
100
150
200
250
300
50 100 150 200 250 300 350 400 450 500
# true 
bili
ngual w
ebs
ites
 
N
?Linkout ?PageRank ?WeightedPagerRank
0.40 
0.45 
0.50 
0.55 
0.60 
0.65 
0.70 
50 100 150 200 250 300 350 400 450 500
Precisi
on
N
?Linkout ?PageRank ?WeightedPagerRank
Figure 2: Number and precision of true bilingual
websites found per top N outside websites ranked
by various criteria.
4 Conclusion
In this paper we have presented an efficient ap-
proach to mining bilingual webpages via com-
puting highly credible bilingual URL pairing pat-
terns. With the aid of these patterns learned in
an unsupervised way, our research moves on to
exploring the possibility of rescuing weak local
keys by virtue of global credibility, uncovering
deep bilingual webpages by generating candidate
URLs using available keys, and also developing
an incremental algorithm for mining more bilin-
gual websites that are linked from the known bilin-
gual websites in our seed set. Experimental results
show that these several enhanced algorithms im-
prove the precision over the baseline from 94.06%
to 99.40% and, more importantly, help discover
above 20% more webpage pairs while maintain-
ing a high overall precision.
Acknowledgements
The research described in this paper was supported
in part by the Research Grants Council (RGC)
of Hong Kong SAR, China, through the GRF
142
grant 9041597 (CityU 144410), National Natural
Science Foundation of China through the grant
No. 70903032, and Project of the Education Min-
istry of China?s Humanities and Social Sciences
through the grant No. 13YJA870020.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web
text mining for cross-language ir. In Proc. of RIAO,
pages 62?77.
Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.
2004. Discovering parallel text from the world wide
web. In Proceedings of the second workshop on
Australasian information security, Data Mining and
Web Intelligence, and Software Internationalisation-
Volume 32, pages 157?161.
Mark W Davis and Ted E Dunning. 1995. A trec eval-
uation of query translation methods for multi-lingual
text retrieval. In Fourth Text Retrieval Conference,
pages 483?498.
Miquel Espla`-Gomis and Mikel L Forcada. 2010.
Combining content-based and URL-based heuris-
tics to harvest aligned bitexts from multilingual sites
with bitextor. The Prague Bulletin of Mathematical
Linguistics, 93(1):77?86.
William A Gale and Kenneth W Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Proceedings of the workshop on Speech and Natural
Language, pages 152?157.
Shihong Huang and Scott Tilley. 2001. Issues of con-
tent and structure for a multilingual web site. In Pro-
ceedings of the 19th annual international conference
on Computer documentation, pages 103?110.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 2, pages 870?878.
Chunyu Kit and Jessica Yee Ha Ng. 2007. An in-
telligent web agent to mine bilingual parallel pages
via automatic discovery of URL pairing patterns.
In Proceedings of the 2007 IEEE/WIC/ACM Inter-
national Conferences on Web Intelligence and In-
telligent Agent Technology - Workshops: Workshop
on Agents and Data Mining Interaction (ADMI-07),
pages 526?529.
Xiaoyi Ma and Mark Liberman. 1999. BITS: A
method for bilingual text search over the web. In
Machine Translation Summit VII, pages 538?542.
I. Dan Melamed. 1997. A word-to-word model of
translational equivalence. In Proceedings of the
Eighth Conference on European Chapter of the As-
sociation for Computational Linguistics, pages 490?
497.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan and Claypool Publishers.
Douglas W Oard. 1997. Cross-language text re-
trieval research in the USA. In Proceedings of the
Third DELOS Workshop: Cross-Language Informa-
tion Retrieval, pages 7?16.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik. 1998. Parallel strands: A preliminary
investigation into mining the web for bilingual text.
In D. Farwell, L. Gerber, and E. Hovy, editors, Ma-
chine Translation and the Information Soup: Third
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-98), pages 72?82.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 527?534.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A DOM tree alignment model for min-
ing parallel data from the web. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 489?
496.
Jesu?s Toma?s, Jordi Bataller, Francisco Casacuberta,
and Jaime Lloret. 2008. Mining Wikipedia as a par-
allel and comparable corpus. In Language Forum,
volume 34.
Sha-ni YE, Ya-juan LV, Yun Huang, and Qun Liu.
2008. Automatic parallel sentences extraction from
web. Journal of Chinese Information Processing,
22:67?73.
T Yulia and W Shuly. 2010. Automatic acquisi-
tion of parallel corpora from website with dynamic
content. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC-2010), pages 3389?3392.
Ying Zhang, Ke Wu, Jianfeng Gao, and Phil Vines.
2006. Automatic acquisition of chinese?english par-
allel corpus from the web. In Advances in Informa-
tion Retrieval, pages 420?431. Springer.
143
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82?86,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Freebase QA: Information Extraction or Semantic Parsing?
Xuchen Yao
1
Jonathan Berant
3
Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
3
Computer Science Department
Stanford University
Abstract
We contrast two seemingly distinct ap-
proaches to the task of question answering
(QA) using Freebase: one based on infor-
mation extraction techniques, the other on
semantic parsing. Results over the same
test-set were collected from two state-of-
the-art, open-source systems, then ana-
lyzed in consultation with those systems?
creators. We conclude that the differ-
ences between these technologies, both
in task performance, and in how they
get there, is not significant. This sug-
gests that the semantic parsing commu-
nity should target answering more com-
positional open-domain questions that are
beyond the reach of more direct informa-
tion extraction methods.
1 Introduction
Question Answering (QA) from structured data,
such as DBPedia (Auer et al., 2007), Freebase
(Bollacker et al., 2008) and Yago2 (Hoffart et
al., 2011), has drawn significant interest from
both knowledge base (KB) and semantic pars-
ing (SP) researchers. The majority of such work
treats the KB as a database, to which standard
database queries (SPARQL, MySQL, etc.) are is-
sued to retrieve answers. Language understand-
ing is modeled as the task of converting natu-
ral language questions into queries through inter-
mediate logical forms, with the popular two ap-
proaches including: CCG parsing (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009; Kwiatkowski et
al., 2010; Kwiatkowski et al., 2011; Krishna-
murthy and Mitchell, 2012; Kwiatkowski et al.,
2013; Cai and Yates, 2013a), and dependency-
based compositional semantics (Liang et al., 2011;
Berant et al., 2013; Berant and Liang, 2014).
We characterize semantic parsing as the task
of deriving a representation of meaning from lan-
guage, sufficient for a given task. Traditional
information extraction (IE) from text may be
coarsely characterized as representing a certain
level of semantic parsing, where the goal is to
derive enough meaning in order to populate a
database with factoids of a form matching a given
schema.
1
Given the ease with which reasonably
accurate, deep syntactic structure can be automat-
ically derived over (English) text, it is not surpris-
ing that IE researchers would start including such
?features? in their models.
Our question is then: what is the difference be-
tween an IE system with access to syntax, as com-
pared to a semantic parser, when both are targeting
a factoid-extraction style task? While our conclu-
sions should hold generally for similar KBs, we
will focus on Freebase, such as explored by Kr-
ishnamurthy and Mitchell (2012), and then others
such as Cai and Yates (2013a) and Berant et al.
(2013). We compare two open-source, state-of-
the-art systems on the task of Freebase QA: the
semantic parsing system SEMPRE (Berant et al.,
2013), and the IE system jacana-freebase (Yao
and Van Durme, 2014).
We find that these two systems are on par with
each other, with no significant differences in terms
of accuracy between them. A major distinction be-
tween the work of Berant et al. (2013) and Yao
and Van Durme (2014) is the ability of the for-
mer to represent, and compose, aggregation oper-
ators (such as argmax, or count), as well as in-
tegrate disparate pieces of information. This rep-
resentational capability was important in previous,
closed-domain tasks such as GeoQuery. The move
to Freebase by the SP community was meant to
1
So-called Open Information Extraction (OIE) is simply
a further blurring of the distinction between IE and SP, where
the schema is allowed to grow with the number of verbs, and
other predicative elements of the language.
82
provide richer, open-domain challenges. While
the vocabulary increased, our analysis suggests
that compositionality and complexity decreased.
We therefore conclude that the semantic parsing
community should target more challenging open-
domain datasets, ones that ?standard IE? methods
are less capable of attacking.
2 IE and SP Systems
jacana-freebase
2
(Yao and Van Durme, 2014)
treats QA from a KB as a binary classification
problem. Freebase is a gigantic graph with mil-
lions of nodes (topics) and billions of edges (re-
lations). For each question, jacana-freebase
first selects a ?view? of Freebase concerning only
involved topics and their close neighbors (this
?view? is called a topic graph). For instance,
for the question ?who is the brother of justin
bieber??, the topic graph of Justin Bieber, con-
taining all related nodes to the topic (think of the
?Justin Bieber? page displayed by the browser), is
selected and retrieved by the Freebase Topic API.
Usually such a topic graph contains hundreds to
thousands of nodes in close relation to the central
topic. Then each of the node is judged as answer
or not by a logistic regression learner.
Features for the logistic regression learner are
first extracted from both the question and the
topic graph. An analysis of the dependency
parse of the question characterizes the question
word, topic, verb, and named entities of the
main subject as the question features, such as
qword=who. Features on each node include the
types of relations and properties the node pos-
sesses, such as type=person. Finally features
from both the question and each node are com-
bined as the final features used by the learner, such
as qword=who|type=person. In this way the as-
sociation between the question and answer type
is enforced. Thus during decoding, for instance,
if there is a who question, the nodes with a per-
son property would be ranked higher as the an-
swer candidate.
SEMPRE
3
is an open-source system for training
semantic parsers, that has been utilized to train a
semantic parser against Freebase by Berant et al.
(2013). SEMPRE maps NL utterances to logical
forms by performing bottom-up parsing. First, a
2
https://code.google.com/p/jacana/
3
http://www-nlp.stanford.edu/software/
sempre/
lexicon is used to map NL phrases to KB predi-
cates, and then predicates are combined to form a
full logical form by a context-free grammar. Since
logical forms can be derived in multiple ways from
the grammar, a log-linear model is used to rank
possible derivations. The parameters of the model
are trained from question-answer pairs.
3 Analysis
3.1 Evaluation Metrics
Both Berant et al. (2013) and Yao and
Van Durme (2014) tested their systems on
the WEBQUESTIONS dataset, which contains
3778 training questions and 2032 test questions
collected from the Google Suggest API. Each
question came with a standard answer from
Freebase annotated by Amazon Mechanical Turk.
Berant et al. (2013) reported a score of 31.4%
in terms of accuracy (with partial credit if inexact
match) on the test set and later in Berant and Liang
(2014) revised it to 35.7%. Berant et al. focused
on accuracy ? how many questions were correctly
answered by the system. Since their system an-
swered almost all questions, accuracy is roughly
identical to F
1
. Yao and Van Durme (2014)?s sys-
tem on the other hand only answered 80% of all
test questions. Thus they report a score of 42%
in terms of F
1
on this dataset. For the purpose of
comparing among all test questions, we lowered
the logistic regression prediction threshold (usu-
ally 0.5) on jacana-freebase for the other 20%
of questions where jacana-freebase had not pro-
posed an answer to, and selected the best-possible
prediction with the highest prediction score as the
answer. In this way jacana-freebase was able
to answer all questions with a lower accuracy of
35.4%. In the following we present analysis re-
sults based on the test questions where the two
systems had very similar performance (35.7% vs.
35.4%).
4
The difference is not significant accord-
ing to the paired permutation test (Smucker et al.,
2007).
3.2 Accuracy vs. Coverage
First, we were interested to see the proportions of
questions SEMPRE and jacana-freebase jointly
and separately answered correctly. The answer to
4
In this setting accuracy equals averaged macro F
1
: first
the F
1
value on each question were computed, then averaged
among all questions, or put it in other words: ?accuracy with
partial credit?. In this section our usage of the terms ?accu-
racy? and ?F
1
? can be exchanged.
83
jacana (F
1
= 1) jacana (F
1
? 0.5)
S
E
M
P
R
E
?
?
?
?
?
153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16)
? 136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)
Table 1: The absolute and proportion of ques-
tions SEMPRE and jacana-freebase answered
correctly (
?
) and incorrectly (?) jointly and sep-
arately, running a threshold F
1
of 1 and 0.5.
many questions in the dataset is a set of answers,
for example what to see near sedona arizona?.
Since turkers did not exhaustively pick out all pos-
sible answers, evaluation is performed by comput-
ing the F
1
between the set of answers given by
the system and the answers provided by turkers.
With a strict threshold of F
1
= 1 and a permis-
sive threshold of F
1
? 0.5 to judge the correct-
ness, we list the pair-wise correctness matrix in
Table 1. Not surprisingly, both systems had most
questions wrong given that the averaged F
1
?s were
only around 35%. With the threshold F
1
= 1,
SEMPRE answered more questions exactly cor-
rectly compared to jacana-freebase, while when
F
1
? 0.5, it was the other way around. This
shows that SEMPRE is more accurate in certain
questions. The reason behind this is that SEMPRE
always fires queries that return exactly one set of
answers from Freebase, while jacana-freebase
could potentially tag multiple nodes as the answer,
which may lower the accuracy.
We have shown that both systems can be more
accurate in certain questions, but when? Is there
a correlation between the system confidence and
accuracy? Thus we took the logistic decoding
score (between 0 and 1) from jacana-freebase
and the probability from the log-linear model used
by SEMPRE as confidence, and plotted an ?accu-
racy vs. coverage? curve, which shows the accu-
racy of a QA engine with respect to its coverage
of all questions. The curve basically answers one
question: at a fixed accuracy, what is the propor-
tion of questions that can be answered? A better
system should be able to answer more questions
correctly with the same accuracy.
The curve was drawn in the following way. For
each question, we select the best answer candidate
with the highest confidence score. Then for the
whole test set, we have a list of (question, highest
ranked answer, confidence score) tuples. Running
0 10 20 30 40 50 60 70 80 90 100Percent Answered
20
30
40
50
60
70
Acc
ura
cy
Accuracy vs. Coverage
jacana-freebase
SEMPRE
Figure 1: Precision with respect to proportion of
questions answered
a threshold from 1 to 0, we select those questions
with an answer confidence score above the thresh-
old and compute accuracy at this point. The X-
axis indicates the percentage of questions above
the threshold and the Y-axis the accuracy, shown
in Figure 1.
The two curves generally follow a similar trend,
but while jacana-freebase has higher accuracy
when coverage is low, SEMPRE obtains slightly
better accuracy when more questions are an-
swered.
3.3 Accuracy by Question Length and Type
Do accuracies of the two systems differ with re-
spect to the complexity of questions? Since there
is no clear way to measure question complexity,
we use question length as a surrogate and report
accuracies by question length in Figure 2. Most of
the questions were 5 to 8 words long and there was
no substantial difference in terms of accuracies.
The major difference lies in questions of length 3,
12 and 13. However, the number of such ques-
tions was not high enough to show any statistical
significance.
Figure 3 further shows the accuracies with re-
spect to the question types (as reflected by the
WH-word). Again, there is no significant differ-
ence between the two systems.
3.4 Learned Features
What did the systems learn during training? We
compare them by presenting the top features by
weight, as listed in Table 2. Clearly, the type of
knowledge learned by the systems in these fea-
tures is similar: both systems learn to associate
certain phrases with predicates from the KB.
84
0	 ?0.05	 ?
0.1	 ?0.15	 ?
0.2	 ?0.25	 ?
0.3	 ?0.35	 ?
0.4	 ?0.45	 ?
0.5	 ?
3	 ?(9)	 ? 4	 ?(78)
	 ?
5	 ?(299
)	 ?
6	 ?(432
)	 ?
7	 ?(395
)	 ?
8	 ?(273
)	 ?
9	 ?(122
)	 ?
10	 ?(48
)	 ?
11	 ?(19
)	 ?
12	 ?(10
)	 ? 13	 ?(4)
	 ?
15	 ?(1)
	 ?
<=	 ?5(	 ?
386)	 ?
<=	 ?10
	 ?(1270
)	 ?
<=15	 ?
(34)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 2: Accuracy (Y-axis) by question length.
The X-axis specifies the question length in words
and the total number of questions in parenthesis.
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0.45	 ?
what	 ?
(929)	 ?
where	 ?
(357)	 ?
who	 ?
(261)	 ?
which	 ?
(35)	 ?
when	 ?
(100)	 ?
how	 ?	 ?
(8)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 3: Accuracy by question type (and the
number of questions).
We note, however, that SEMPRE also obtains in-
formation from the fully constructed logical form.
For instance, SEMPRE learns that logical forms
that return an empty set when executed against the
KB are usually incorrect (the weight for this fea-
ture is -8.88). In this respect the SP approach ?un-
derstands? more than the IE approach.
We did not further compare on other datasets
such as GeoQuery (Tang and Mooney, 2001) and
FREE917 (Cai and Yates, 2013b). The first one
involves geographic inference and multiple con-
traints in queries, directly fitting the compositional
nature of semantic parsing. The second one was
manually generated by looking at Freebase top-
ics. Both datasets were less realistic than the
WEBQUESTIONS dataset. Both datasets were also
less challenging (accuracy/F
1
were between 80%
and 90%) compared to WEBQUESTIONS (around
40%).
4 Discussion and Conclusion
Our analysis of two QA approaches, semantic
parsing and information extraction, has shown no
significant difference between them. Note the
feature weight
qfocus=religion|type=Religion 8.60
qfocus=money|type=Currency 5.56
qverb=die|type=CauseOfDeath 5.35
qword=when|type=datetime 5.11
qverb=border|rel=location.adjoins 4.56
(a) jacana-freebase
feature weight
die from=CauseOfDeath 10.23
die of=CauseOfDeath 7.55
accept=Currency 7.30
bear=PlaceOfBirth 7.11
in switzerland=Switzerland 6.86
(b) SEMPRE
Table 2: Learned top features and their weights for
jacana-freebase and SEMPRE.
similarity between features used in both systems
shown in Table 2: the systems learned the same
?knowledge? from data, with the distinction that
the IE approach acquired this through a direct as-
sociation between dependency parses and answer
properties, while the SP approach acquired this
through optimizing on intermediate logic forms.
With a direct information extraction technol-
ogy easily getting on par with the more sophis-
ticated semantic parsing method, it suggests that
SP-based approaches for QA with Freebase has
not yet shown its power from a ?deeper? under-
standing of the questions, among questions of var-
ious lengths. We suggest that more compositional
open-domain datasets should be created, and that
SP researchers should focus on utterances in exist-
ing datasets that are beyond the reach of direct IE
methods.
5 Acknowledgement
We thank the Allen Institute for Artificial Intelli-
gence for assistance in funding this work. This
material is partially based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreements number FA8750-13-2-0017 and
FA8750-13-2-0040 (the DEFT program).
85
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Qingqing Cai and Alexander Yates. 2013b. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229?232. ACM.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223?1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623?
632. ACM.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001. Springer.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
86
