85
86
87
88
89
90
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 43?48,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
RACAI GEC ? A hybrid approach to Grammatical Error Correction  
 
 
Tiberiu Boro?  
Calea 13 Septembrie, 13 
Bucharest 
RACAI 
tibi@racai.ro 
Stefan Daniel Dumitrescu  
Calea 13 Septembrie,13 
Bucharest 
RACAI 
sdumitrescu@racai.ro 
Adrian Zafiu  
Str. Targul din Vale, 1 
Pitesti 
UPIT - FECC 
adrian.zafiu@comin.ro 
 
Dan Tufi? 
Calea 13 Septembrie, 13 
Bucharest 
RACAI 
tufis@racai.ro 
Verginica Mititelu Barbu  
Calea 13 Septembrie, 13 
Bucharest 
RACAI 
vergi@racai.ro 
Paul Ionu? V?duva  
Calea 13 Septembrie, 13 
Bucharest 
RACAI 
ionut@racai.ro 
  
Abstract 
This paper describes RACAI?s (Research 
Institute for Artificial Intelligence) hy-
brid grammatical error correction system. 
This system was validated during the par-
ticipation into the CONLL?14 Shared 
Task on Grammatical Error Correction. 
We offer an analysis of the types of er-
rors detected and corrected by our sys-
tem, we present the necessary steps to re-
produce our experiment and also the re-
sults we obtained. 
1 Introduction 
Grammatical error correction (GEC) is a com-
plex task mainly because of the natural depend-
encies between the words of a sentence both at 
the lexical and the semantic levels, leave it aside 
the morphologic and syntactic levels, an intrinsic 
and complex attribute specific to the human lan-
guage. Grammatical error detection and correc-
tion received a significant level of interest from 
various research groups both from the academic 
and commercial environments. A testament to 
the importance of this task is the long history of 
challenges (e.g. Microsoft Speller Challenge and 
CONLL Shared Task) (Hwee et al., 2014) that 
had the primary objective of proving a common 
testing ground (i.e. resources, tools and gold 
standards) in order to assess the performance of 
various methods and tools for GEC, when ap-
plied to identical input data. 
In the task of GEC, one can easily distinguish 
two separate tasks: grammatical error detection 
and grammatical error correction. Typically, 
there are three types of approaches: statistical, 
rule-based and hybrid. The difficulty of detecting 
and correcting an error depends on its class.  
(a) Statistical approaches rely on building 
statistical models (using surface forms or 
syntactic labels) that are used for detecting 
and correcting local errors. The typical 
statistical approach is to model how likely 
the occurrence of an event is, given a his-
tory of preceding events. Thus, statistical 
approaches easily adaptable to any lan-
guage (requiring only training data in the 
form of raw or syntactically labeled text) 
are very good guessers when it comes to 
detecting and correcting collocations, idi-
oms, typos and small grammatical inad-
vertences such as the local gender and 
case agreements. The main impediments 
of such systems are two-fold: (1) they are 
resource consuming techniques 
(memory/storage) and they are highly de-
pendent on data ? large and domain 
adapted datasets are required in order to 
avoid the data-scarceness specific issue 
and currently they rely only on a limited 
horizon of events; (2) they usually lack 
semantic information and favoring high-
occurring events is not always the best 
way of detecting and correcting grammat-
ical errors. 
(b) Rule-based approaches embed linguistic 
knowledge in the form of machine parsa-
ble rules that are used to detect errors and 
43
describe via transformations how various 
error types should be corrected. The 
drawbacks to rule-based system are (1) the 
extensive effort required to build the rule-
set, (2) regardless of the size of the rule-
set, given the variability of the human lan-
guage it is virtually impossible to capture 
all possible errors and (3) the large num-
ber of exceptions to rules. 
(c) Hybrid systems that combine both rule-
based and statistical approaches are plau-
sible to overcome the weaknesses of the 
two methodologies if the mixture of the 
two components is done properly. Detec-
tion of errors can be achieved statistically 
and rule-based, the task of the hybrid ap-
proach being to resolve any conflicts that 
arise between the outputs of the two ap-
proaches.  
However, even the most advanced systems are 
only able to distinguish between a limited num-
ber of error types and the task of correcting an 
error is even more difficult. Along the typical set 
of errors that are handled by typical correction 
systems (punctuation, capitalization, spelling, 
typos, verb tense, missing verb, etc.), CONLL?s 
GEC task introduces some hard cases which re-
quire a level of semantic analysis: local redun-
dancy, unclear meaning, parallelism, etc.  
2 External tools and resources  
One step in the preparation phase was the 
analysis of the types of errors. The training set 
was automatically processed with our Bermuda 
tool (Boro? et al., 2013): it underwent sentence 
splitting, tokenization, part of speech tagging, 
lemmatization and also chunking. Comparing the 
original and the corrected sentences, we could 
rank the types of mistakes.  
The most frequent ones, i.e. occurring more 
than 1000 times, are presented in the following 
table:  
 
Type of error Occurrences Percent 
use of articles and de-
terminers 
6647 14.98 
wrong collocations or 
idioms 
5300 11.94 
local redundancies 4668 10.52 
noun number 3770 8.49 
tenses 3200 7.21 
punctuation and orthog-
raphy 
3054 6.88 
use of prepositions 2412 5.43 
word form 2160 4.87 
subject-verb agreement 1527 3.44 
Verb form 1444 3.25 
Link word/phrases 1349 3.04 
Table 1. The most frequent types of mistakes 
in the training data 
There are also some less frequent errors: pro-
noun form, noun possessive form, word order of 
adjectives and adverbs, etc. Some of these can be 
solved by means of rules, others by accessing 
lexical resources and others are extremely diffi-
cult to deal with. 
As far as the test data are concerned, the error 
distribution according to their types is the fol-
lowing: 
Type of error Occurrences 
in official-
2014.0.m2 
Occurrences 
in official-
2014.1.m2 
use of articles and 
determiners 332 437 
wrong collocations 
or idioms 339 462 
local redundancies 94 194 
noun number 214 222 
tenses 133 146 
punctuation and 
orthography 227 474 
use of prepositions 95 152 
word form 76 104 
subject-verb 
agreement 107 148 
Verb form 132 88 
Link word/phrases 93 78 
Table 2. The most frequent types of mistakes 
in the test data 
Roughly, the same types of mistakes are more 
frequent in the test set, just like as in the training 
set. 
For collocations and idioms, as well as for cor-
rect prepositions use, we consider that only lexi-
cal resources can be of help. They can take the 
form of a corpus or lists of words that subcatego-
rize for prepositional phrases obligatorily headed 
by a certain preposition (see section 3.2). We 
adopted the former solution: we used Google 1T 
n-grams corpus (see section 2.2) from which the 
selectional restrictions can be learned quite suc-
cessfully. However, dealing with collocations is 
difficult, as correction does not involve only syn-
tax, but also semantics. Changing a word in a 
sentence usually implies changing the meaning 
of the sentence as a whole. Nevertheless, a solu-
tion can be found: as mistakes in collocations 
involve the use of a related word (synonyms), a 
44
resource such as the WordNet can be of help. 
When the word used in the sentence and the one 
occurring in the corpus can be found in the same 
synset (or even in synsets in direct relation), the 
correction could be made. Otherwise, it is risky 
to try. In any scenario, this remains as future 
work for us. 
2.1 RACAI NLP Tools 
We have used our in-house Bermuda software 
suite (Boro? et al., 2013), (Boro? and 
Dumitrescu, 2013) to perform text pre-
processing. As the tool is well documented in the 
cited papers above, we summarize its main func-
tionalities concerning the task at hand and the 
algorithms behind them. 
Tokenization. A basic necessary prepro-
cessing step that needs to be applied from the 
beginning as most tools work on a certain to-
kenization format. Bermuda uses a custom-built, 
language dependent tokenizer. Based on regular 
expressions, it detects and splits words such as 
[haven?t] into [have] and [n?t]; [boy?s] into [boy] 
and [?s], while leaving abbreviations like [dr.] or 
[N.Y.] as a single token.  
Part-of-speech (POS) tagger. Tagging is es-
sential to determine each word?s part of speech 
and thus its role in the sentence. Each word is 
tagged with a morpho-syntactic descriptor, called 
MSD. The English language has around 100 
MSDs defined, while more inflected languages, 
like Romanian ? a Latin-derived language, uses 
over 600. An MSD completely characterizes the 
word morphologically and syntactically 1 . For 
example, ?Np? refers to a proper noun while 
?Ncns? refers to a common (c) noun (N) that has 
a neuter (n) gender and is in singular form (ex: 
zoo, zone). Our tagger is based on a neural net-
work, introduced in (Boro? et al., 2013). Overall, 
the Bermuda POS Tagger obtains very high ac-
curacy rates (>98%) even on the more difficult, 
highly inflected languages. 
Lemmatization. The Bermuda Lemmatizer is 
based on the MIRA algorithm (Margin Infused 
Relaxed Algorithm) (Crammer and Singer, 
2003). We treat lemmatization as a tagging task, 
in which each individual letter of the surface 
word is tagged as either remaining unchanged, 
being removed or transformed to another letter. 
The lemmatizer was trained and tested on an 
English lexicon containing a number of around 
120K surface-lemma-MSD entries.  
                                                 
1 Full description of MSDs can be found at : 
http://nl.ijs.si/ME/V4/msd/html/msd-en.html 
2.2 Google 1T corpus 
A good performing language model is a very im-
portant resource for the current task, as it allows 
discriminating between similar phrases by com-
paring their perplexities.  
Although we had several corpora available to 
extract surface-based language models from, we 
preferred to use a significantly larger model than 
we could create: Google 1T n-gram corpus 
(Brants and Franz, 2006). This 4 billion n-gram 
corpus should provide high-quality perplexity 
estimations. However, loading 4*10^9 n-grams 
without any compression scheme would require, 
even by today?s standards, a large amount of 
memory. For example, using SRILM (Stolcke, 
2002) which uses 33 bytes per n-gram, would 
require a total of ~116GB of RAM. The article 
by Adam Pauls and Dan Klein (2011) describes 
an ingenious way to create a data structure that 
reduces the amount of RAM needed to load the 
1T corpus. However, the system they propose is 
written in Java, a language that is object-
oriented, and which, for any object, introduces an 
additional overhead. Furthermore, they do not 
implement any smoothing method for the 1T 
corpus, defaulting to the +1 ?stupid smoothing? 
as they themselves named it, relying on the fact 
that smoothing is less relevant with a very large 
corpus.  For these reasons, coupled with the dif-
ficulty to understand and modify other persons? 
code, we wrote our language model software. 
We based our implementation around Pauls and 
Klein?s sorted array idea, with a few modifica-
tions. Firstly, we encoded the unigrams in a sim-
ple HashMap instead of a value-rank array. Sec-
ondly, we wrote a multi-step n-gram reader and 
loader. Thirdly, we implemented the Jelinek-
Mercer smoothing method instead of the simple 
+1 smoothing. Using deleted interpolation we 
computed the lambda parameters for the JM 
smoothing; we further built a stand-alone server 
that would load the smoothed n-gram probabili-
ties and could be queried over TCP-IP either for 
an n-gram (max 5-gram ? direct probability) or 
for an entire sentence (compute its perplexity). 
The entire software was written in C++ to avoid 
Java?s overhead problems. Overall, the simpli-
fied ranked array encoding allowed us to obtain 
very fast response times (under a millisecond per 
query) with a moderate memory usage: the entire 
1T corpus was loaded in around 60GB of RAM, 
well below our development server memory lim-
it.   
45
We are aware of the limitations of this corpus: 
as data was collected from the web, mistakes will 
occur in it. 
We also need a language model that can esti-
mate the probability of parts of speech. By learn-
ing a model from the parts of speech we can 
learn to discriminate between words different 
forms. Grantedly, a part of speech language 
model can promote a grammatically ?more? cor-
rect but semantically inferior sentence over a 
semantically sound one, due to assigning a high-
er probability  to a more common part of speech 
sequence in the sentence.  Our experiments show 
that, generally, a part of speech language model 
helps text quality overall.  
Our initial idea for this POS language model 
was to use the same 1T corpus that we could an-
notate using our tagging tools. However, given 
the limited context, performance would have 
been acceptable at the 5-gram level, decreasing 
to the point of simply picking the most common 
part of speech for the unigrams, as no context 
exists for them. As such, we used the following 
available monolingual resources for English: the 
News CRAWL corpus (2007-2012 editions), 
Europarl, UN French-English Corpus, the News 
Commentary, our own cleaned English Wikipe-
dia dump. The total size of the raw text was 
around 20GB. We joined and annotated the files 
and extracted all the 1-5 grams, using the same 
format as the 1T corpus. We then used another 
instance of the language model software to load 
this POS LM and await the main system perplex-
ity estimation requests. Overall, the part of 
speech language model turned out to be rather 
small (a hard-disk footprint of only 315MB of 
binary part of speech LM compared to the 57GB 
of surface model compressed data). This is nor-
mal, as the entire part of speech MSD vocabulary 
for English is around 100 tags, compared to the 
more than 13 million surface forms (unigrams) in 
the 1T corpus.  
3 RACAI?s Hybrid Grammatical Error 
Correction System  
3.1 An overview of the system 
In many cases, statistical methods are preferable 
over rule-based systems since they only rely on 
large available raw corpora instead of hand-
crafted rules that are difficult to design and are 
limited by the effort invested by human experts 
in their endeavor.  
However, a purely statistical method is not 
always able to validate rarely used expressions 
and always favors frequency over fine grained 
compositions.  
As a rule of thumb, hybrid systems are always 
a good choice in tasks where the complexity ex-
ceeds the capacity of converting knowledge into 
formal rules and large scale training data is 
available for developing statistical models.  
Our GEC system has three cascaded phases 
divided between two modules: (a) in the first 
phase, a statistical surface based and a POS LM 
are used to solve orthographic errors inside the 
input sentences, thus enhancing the quality of the 
NLP processing for the second stage; (b) a rule-
based system is used to detect typical grammati-
cal errors, which are labeled and then (c) correct-
ed using a statistical method to validate between 
automatically generated candidates. 
3.2 The statistical component 
Typos are a distinctive class of errors found in 
texts written by both native and non-native Eng-
lish speakers which do not violate any explicit 
(local agreement related) grammatical con-
straints. Most POS tagging systems handle pre-
viously unseen words through suffix analysis and 
are able (using the local context) to assign a tag 
which is conformant with the tags of the sur-
rounding words. Such errors cannot be detected 
by applying rules, since it is impossible to have 
lexicons that cover the entire possible vocabulary 
of a language.   
The typical approach is to generate spelling al-
ternatives for words that are outside the vocabu-
lary and to use a LM to determine the most likely 
correct word form. However, when relying on 
simple distance functions such as the unmodified 
Levenstein it is extremely difficult to differenti-
ate between spelling alternatives even with the 
help of contextual information. There are multi-
ple causes for this type of errors, starting from 
the lack of language knowledge (typically non-
native speakers rely on phonetic similarity when 
spelling words) to speed (usually results in miss-
ing letters) or keyboard related (multiple keys 
touched at once). The distance function we used 
for scoring alternatives uses a weighted Leven-
stein algorithm, which was tuned on the TREC 
dataset. 
3.3 The rule based error detection and cor-
rection 
As previously mentioned, not all grammatical 
errors are automatically detectable by pure statis-
tical methods. In our experiments we noticed 
frequent cases where the LM does not provide 
46
sufficient support to distinguish between true 
grammatical errors and simply unusual but 
grammatically correct expressions.    
For the present shared task we concentrated on 
a subset of potential errors. Our rules aimed the 
correction of the verb tense especially in time 
clauses, the use of the short infinitive after 
modals, the position of frequency adverbs in a 
sentence, subject-verb agreement, word order in 
interrogative sentences, punctuation accompany-
ing certain lexical elements, the use of articles, of 
correlatives, etc.              
For the sake of an easier understanding of our 
rule-based component of the GEC system, we 
will start by introducing some technical details 
about how the rule interpreter works, emphasiz-
ing on the structure of the configuration file, the 
input modality and the general pointers on writ-
ing rules. In our approach we treat error detec-
tion and error correction separately, in a two-
stage system. The configuration file contains a 
set of language dependent rules, each rule being 
uniquely identified by the label and its body.  
The role of using labels is two-fold: (1) they pro-
vide guidance and assistance to the user in navi-
gating through the structure of the configuration 
file (when editing or creating new rules); (2) they 
play a crucial role in the error correction process 
and serve as common denominators for different 
classes of errors. 
Our rule description system is inspired after 
the time-independent logic function (combina-
tional logic) paradigm, which stipulates that a 
fixed input size logical function, described 
through a stochastic list of input/output depend-
ence sequence, through a process of logical min-
imization, this function can be implemented as 
an array of ?AND? gates, followed by an array of 
?OR? gates. Thus, in our configuration file, each 
rule is described by a set of string pairs (i0 r0, i1 
r1? in rn) which act as ?AND? gates ? we refer 
to this as a sub-instance of a rule. At this point, a 
sub-instance is activated only if all constraints 
are met. The ?OR? gate array is simulated by 
adding rules with the same label. This way, if 
any sub-instance is active then the rule is consid-
ered active and we proceed to the errror correc-
tion step. 
Every pair (ik rk) is a single Boolean input of a 
sub-instance. A rule is checked against every 
token inside an utterance, from left to right. rk is 
a regular expression which, depending on the 
value of ik, is applied to the word?s surface form 
(s), the word?s lemma (l) or the word?s MSD 
(m). ik can also select if the regular expression 
should be applied to a neighboring token. To ex-
emplify, we have extracted two sections from our 
configuration file: (a) the modal infinitive com-
mon error for non-native English speakers (also 
found in the development set of CONLL) (lines 1 
to 7) and (b) the possible missing comma case 
(line 8): 
 
1) modal_infinitive: s must   s+1 to s-1 ^!a 
2) modal_infinitive: s could  s+1 to  
3) modal_infinitive: s can    s+1 to 
4) modal_infinitive: s might  s+1 to 
5) modal_infinitive: s may    s+1 to 
6) modal_infinitive: s would  s+1 to 
7) modal_infinitive: s should s+1 to 
8) pmc: s which m-1 ^((?!COMMA).)*$ 
Table 3: a sample of error detection rules 
 
The ?modal_infinitive? rule is complex and it 
is described using 7 sub-instances, which share 
an identical label. Line 1 of the configuration 
excerpt contains three pairs as opposed to the 
other sub-instances. This does not contradict the 
combinational logic paradigm, since we can con-
sider this rule as having a fixed input size of 
three and, as a result of logic minimization, the 
third parameter for 6 of the seven instances falls 
into the ?DON?T CARE? special input class. The 
first ikrk pair (?s must?) is used to check if the 
surface form (?s?) of the current word is ?must?.  
The second pair (?s+1 to?) checks if the word 
form of the next token is ?to?.  The third pair  
(?s-1 ^!a?) verifies that the collocation ?a must 
to? does not accidentally trigger this rule. This 
rule will detect the error in ?I must to go...?, but 
will licence a sequence like ?This book is a must 
to read...?.  
The error detection rules that we designed for 
the CONLL shared task are created, as an exter-
nal resource for the program, on the basis of the 
mistakes observed in the training set and can be 
updated/extended any time .  
In the error correction phase, for every error 
type we encompass, we provide the necessary 
transformations (at token level) through which 
the initial word sequence that generated this error 
should be corrected. The configuration file of 
this module is straightforward: rule-labels are 
marked as strings at the beginning of a new line; 
for each label, we provide a set of transformation 
rules, that are contained in the following tab-
indented lines; once a new line does not start 
with a TAB character, it should either be empty 
or contain the label for a different error type. the 
correction phase, multiple sentence candidates 
are automatically generated (based on the trans-
formation rules) and they are checked against the 
47
language model to see which one yields the low-
est perplexity. That is, once an error is found, its 
correction way tends to be applied provided that 
the language model offers another solution. 
As an example, suppose that the rule for de-
tecting a possible missing comma (pmc in Table 
3, line 8) was fired.  The corresponding correc-
tion rule is described as below: 
 
pmc:  
$w-1 , $w  
$w-1 $w  
 
The "pmc" rule is activated if the word 
"which" is not preceded by a comma. Since it is 
not always the case that the wordform "which" 
should be preceded by this punctuation mark, in 
our error correction system step we generate two 
candidates: (a) one in which we insert a comma 
before "which" and (b) one in which we keep the 
word sequence untouched. 
4 Results and Conclusions 
The RACAI hybrid GEC system obtained a pre-
cision of 31.31%, a recall of 14.23% and an F0.5 
score of 25.25% on the test set provided by the 
CONLL shared task on Grammatical Error Cor-
rection.  
We presented our system and the resources we 
used in the development process. All the data 
and tools required to run a similar experiment are 
available online and we are currently working on 
developing a self-contained GEC system that 
will be made publicly available.   
Future development plans include the en-
hancement of the lexicons we use for English 
and the extension of this system for Romanian. 
Furthermore, we plan to include an extended 
method for solving collocations errors based on 
the synsets of Princeton WordNet (PWN) (Fell-
baum, 1989). 
References 
Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proceedings of Inter-
speech 
Boro?, T., Radu, I., & Tufi?, D. (2013). Large tagset 
labeling with Feed Forward Neural Networks. Case 
study on Romanian Language. In Proceedings of 
ACL 
Boro?, T., & Dumitrescu, S. D. (2013). Improving the 
RACAI Neural Network MSD Tagger. In Engi-
neering Applications of Neural Networks (pp. 42-
51). Springer Berlin Heidelberg 
Crammer, K., & Singer, Y. (2003). Ultraconservative 
online algorithms for multiclass problems. The 
Journal of Machine Learning Research, 3, 951-991. 
Fellbaum, Ch. (1998, ed.) WordNet: An Electronic 
Lexical Database. Cambridge, MA: MIT Press 
 Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian 
Hadiwinoto, Raymond Hendy Susanto, and Chris-
topher Bryant (2014). The CoNLL-2014 Shared 
Task on Grammatical Error Correction. Proceed-
ings of the Eighteenth Conference on Computa-
tional Natural Language Learning: Shared Task 
(CoNLL-2014 Shared Task). Baltimore, Maryland, 
USA. 
Thorsten Brants and Alex Franz. 2006. Google 
Web1T 5-gram corpus, version 1. In Linguistic Da-
ta Consortium, Philadelphia, Catalog Number 
LDC2006T13 
Pauls, Adam, and Dan Klein. "Faster and smaller n-
gram language models." Proceedings of the 49th 
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1. Association for Computational Linguis-
tics, 2011. 
 
 
 
48
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 68?74,
Dublin, Ireland, August 23, 2014.
WordFinder
Catalin Mititelu
Stefanini / 6A Dimitrie Pompei Bd,
Bucharest, Romania
catalinmititelu@yahoo.com
Verginica Barbu Mititelu
RACAI / 13 Calea 13 Septembrie,
Bucharest, Romania
vergi@racai.ro
Abstract
This paper presents our relations-oriented approach to the shared task on lexical access in lan-
guage production, as well as the results we obtained. We relied mainly on the semantic and
lexical relations between words as they are recorded in the Princeton WordNet, although also
considering co-occurrence in the Google n-gram corpus. After the end of the shared task we con-
tinued working on the system and the further adjustments (involving part of speech information
and position of the candidate in the synset) and those results are presented as well.
1 Introduction
In this paper we present our experience in the shared task on lexical access in language production,
organized as part of the CogALex workshop. Given a list of five words (let us call them seeds), the
system should return a word (we will call it target) which is assumed to be the most closely associated
to all the seeds. Two remarks are worth being made here: on the one hand, what we call word is in fact
a word form, as inflected forms are both among the seeds and among the expected targets in the training
and the test sets. On the other hand, the closeness of association remains understated by the organizers.
It can be understood at several levels, given our analysis of the training data: the meaning and/or the
form, the syntagmatic associations, i.e. associations of words in texts. However, our system dealt mainly
with the semantic level. The form level is involved only to the extent to which lexical relations (usually
derivational relations and antonymy) in Princeton WordNet (PWN) are used. The syntagmatic relations
we use are the co-occurrences in the Google n-gram corpus
2 Our understanding of the lexical access task
Having already established what meaning we, as speakers, want to render, the lexical choice is influenced
by several factors: the person we talk to, the circumstances (place, other participants) of our discussion,
the social (or even other types of) relations between the participants to the discussion. The shared task
focuses on the tip of the tongue (TOT) phenomenon, as rightly described in the shared task presentation:
we do not remember the word ?mocha?, but we want to express the idea (i.e., the meaning) ?superior
dark coffee made of beans from Arabia?. In a real life conversation, dealing with TOT is much simpler:
the speaker (the one affected by TOT) has the ability of defining the word s/he is looking for or of
enumerating some words AND specifying the relation(s) they establish with the looked for word. Thus,
we consider that the task here, consisting of being able to find the target when receiving five seeds,
does not mimic the real life situation. In fact, we deprive the system of vital information that, we, as
speakers, possess, to our great advantage reflected in our success in dealing with the TOT problem, after
all. Moreover, given the information provided by the organizers once the results were send, the seeds that
we received are derived from the Edinburgh Associative Thesaurus, so they are, in fact, the associations
introduced by the users to a seed. So, the organizers implicitly considered the association of two words
is the same, irrespective of which of them is the seed and which is the target, which is definitely not the
same, especially if the association is a syntagmatic one.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
68
3 Related work
In a recent experiment (Zock and Shcwab, 2013), a set of seeds (called stimuli therein) is presented to a
system and, relying on information available in the eXtended WordNet (Mihalcea, 2001) and in DBpedia,
a list of words is returned. The authors explain the bad results by the small dimensions of the eXtended
WordNet and by the small number of syntagmatic relations it contains. Although they emphasize the
necessity of using big corpora, with heterogenous data, to help solve the TOT problem, the conclusions
speculate about various elements that can lead to, but do not guarantee the success:
? the big size of the corpus, the heterogeneity of the texts it contains;
? high density of relations in a network;
? the quality of the search;
? all these together.
4 Our approach
4.1 The data
The training set contains a list of 2000 pairs of five seeds and the target. They look quite heterogeneous:
there are content and functional words alike, lemmas and inflected forms (see ?occurs ? happens happen
often sometimes now?), capitalized (sometimes unnecessarily, for example ?Nevertheless? in the pair
?however ? but never Nevertheless when although?) and uncapitalized words.
Interestingly, two different inflected forms are targets of (partially) different sets of seeds: compare:
occur ? happen event often perfume today
with
occurs ? happens happen often sometimes now.
This means that not only semantic relations are established between the seed and the target, but also
grammatical ones.
4.2 Assumptions
In order to construct our system we made the assumption, supported by the manual analysis of the
training set, that the seeds and the target are related to each other by different kinds of relations:
? semantic relations;
? co-occurrence, in either order;
? syntactic relations;
? gloss-like relations, i.e. the target may be defined using one or more seeds;
? domain relations, i.e. the target and at least some seeds may belong to the same domain;
? form relation, i.e. the target and one or more seeds may display a partial identity of form (and
sometimes even of the acoustic form of words);
? inflection as a relation among forms of the same word;
? etc.
Given these, we were aware of the impossibility of dealing with cases involving inflected forms, some
of them occurring as seeds, while one occurs as target, such as:
am ? I not is me are.
In this case, an inflectional relation can be found between ?is? and ?am? and between ?are? and
?am?, whereas the relations between ?am? and ?I? and between ?am? and ?not? are syntagmatic (co-
occurrences). No relation can we identify between ?am? and ?me?.
69
4.3 Resources
As a consequence of the assumptions made, the language resources we used for the competition were
the Princeton WordNet (PWN) (Fellbaum, 1998) and Google n-grams corpus (Brants and Franz, 2006).
The implied limitations of our approach are:
? the impossibility of dealing with pairs involving only inflected words (as in the previous example)
or only functional words (as in the case: ?at ? home by here in on?);
? no contribution made by some of the seeds in the process of finding the target;
? the partial dealing with inflected forms such as plurals, third person singular of verbs, gerunds, as
they cannot be found in PWN; the only source of information about them is the n-grams corpus;
? some combinations (although quite frequent, according to our intuitions obout the language) cannot
be found in the Google n-gram corpus.
For all (2000x5) pairs seed-target in the training set we extracted from PWN the shortest relations chains,
as a kind of lexical chains (Moldovan and Novischi, 2002), existing between them, disregarding the part
of speech of the words. These chains are made up of both semantic and lexical relations (as they are de-
fined in the wordnet literature, i.e. lexical relations are established between word forms, while semantic
relations are established between word meanings). The most frequent relations chains are presented in
Table 1. Straightforwardly, the most frequent association between the seeds and the targets (occurring
Lexical chain Number of occurrences
synonym 548
hypernym hyponym 332
hyponym 328
hypernym 182
antonym 143
similar to 128
derivat 119
hypernym hyponym hyponym 115
hypernym hypernym hyponym 100
hyponym hyponym 81
hypernym hypernym hyponym hyponym 75
similar to similar to 59
derivat derivat 59
part meronym 49
hyponym derivat 46
hypernym derivat 42
derivat hyponym 40
hypernym hyponym derivat 37
domain TOPIC domain member TOPIC 36
derivat hypernym hyponym 35
also see 35
Table 1: The most frequent relations chains between a seed and the target.
548 times) is of the kind synonymy. However, various combinations of hyponymy and hypernymy ac-
count for a significant number of pairs: 1213. Almost half of these cases (510) are solved by only one of
the two relations (328 by hyponymy alone and 182 by hypernymy alone). Moreover, these relations con-
tribute also in chains involving the derivat relation. So, we can consider them the most useful ones. (Our
finding is similar to the weight associated to these relations by Moldovan and Novischi (Moldovan and
70
Training set
Candidate criteria
Candidates
Features extractor
Model
Maxent create model
Features
seeds
[seeds, candidate]
Figure 1: The training flowchart.
Novischi, 2002), who top rank them in finding paths between related concepts for a Question Answering
system.) However, they introduce a lot of noise, too, especially when the last relation in the chain is
hyponymy and the node from which it starts is one with very many hyponyms.
4.4 The system in the shared task competition
We reformulated this as a classification problem. Assuming that having a list of seeds and the list of
their possible candidates, the problem will be solved by considering the most probable candidate as the
closest to all seeds. We chose valid and invalid as classification categories.
The system uses the machine learning technique called Maximum Entropy Modeling (MaxEnt for
short) and the features needed by MaxEnt are extracted from the kinds of relations presented above, in
subsection 4.2. In other words, we mapped each kind of relation to a feature. The entire process has two
distinct phases: training and prediction.
The training mechanism is presented in Figure 1. For each training set entry (i.e. the list of 5 seeds and
the expected target) a list of possible candidates is generated using the PWN relations chains presented
above. We called this process Candidate Criteria. Combining each set of seeds with their candidates we
extracted the list of features needed to enter into the MaxEnt process to create the model. For instance,
giving the sequence of seeds away fonder illness leave presence and two possible candi-
dates absence and being we obtained the following lists of features ending with the corresponding
classification category:
domain=s factotum domain=t factotum src=1 wn=an wn=he he ho ho
wnshort=he ho valid
domain=s factotum domain=t factotum src=1 wn=he ho d d invalid
The following list of features were used:
71
? wn=chain: chain represents the relations chain found between any seed and the current candidate.
We used short forms to label relations: for example, an stands for antonymy, he for hypernymy,
ho for hyponymy, d for derivational relation;
? form=first upper when at least one seed and the candidate begin with a capital letter; we did
not allow for candidates with initial capital letter unless at least one seed had an initial capital letter;
? src=n marks the number n of seeds that reached the candidate using the PWN chains. In the case
of the seed presence and candidate absence there are two chains linking the two words: an
and he he ho ho and only presence contributes to them;
? gloss=n marks the number n of seeds that occur in the target gloss;
? n2gram=high used when any seed occurs in any Google 2-grams with the candidate;
? domain=s domain used to mark the seed domain(s);
? domain=t domain used to mark the candidate domain(s);
? wnshort=short chain here the short chain represents a reduced version of the PWN chain.
For example, the chain he he ho ho can be reduced to he ho (or to a co-hyponym relation, in
an extended meaning). The reason is to create an invariant chain that can hold irrespectively of the
number of similar consecutive relations. This is useful in hierarchies involving many scientific or
artificial nodes which are not known or simply disregarded by common speakers. For example, the
chain between hippopotamus and animal is 7 hyponyms long in PWN, whereas for a speaker
they are in a direct relation.
The selection of candidates is done using exclusively the PWN relations chains with a maximun length
of 5 relations in a chain and only the first literal from the target synset is taken into account (on the
assumption that literals PWN synsets are in reverse order of their frequency of occurrence in corpora,
with the first as the most frequent). To reduce the number of possible candidates some filtering criteria
are applied before pairing them with their corresponding seeds to extract the features described above.
These criteria are:
? the candidates that appear among seeds are eliminated;
? the compound terms (recognized by the use of underscore among elements) are excluded;
? the candidates should appear together with any seed among Google 5-grams with a minimum fre-
quency of 5000 (occurrences).
The prediction phase takes the test set and, using the model created in the training phase, produces for
each candidate a percent for each category (valid / invalid). The candidate selection and features
extraction are done similarly to the training phase. The prediction phase is presented in Figure 2. The
result of this phase is a list of candidates (sorted in reverse order) for each set of 5 seeds in the test set.
The list of results presented to the shared task organizers contains, for each set of seeds, the best ranked
candidate.
4.5 Modifications after the competition
After the end of the competition we tried several mechanisms that could improve our results. They were:
? adding two new features that dealt with the part of speech of the words:
? pos= s pos: the part-of-speech of the seed(s) corresponding to PWN chain that relates to
the candidate;
? pos= t pos: similar for candidate/target;
? considering more literals from synsets when creating the list of candidates.
72
Test set Candidate criteria
Candidates
Features extractor
Model
Features
Maxent classifier
Valid / Invalid
seeds
[seeds, candidate]
Figure 2: The prediction flowchart.
73
5 Results
5.1 Results within the competition
Out of the total number of items (2000) only 30 of our targets matched the ones expected by the organiz-
ers, so we obtained 1.50% accuracy.
5.2 Improved results after the competition
After considering the part of speech of the words, we were able to match 51 targets, thus increasing the
accuracy to 2.55%.
After considering two literals from a synset in the candidates list, the number of matches was 59, so
an accuracy of 2.95%.
Furthermore, if we consider the top five candidates in our list, we noticed that 140 targets could be
found.
Considering three or even four literals in the synsets did not improve the results (either for the best
ranked candidate or for the top 5 ones).
6 Conclusions
We presented here the way we dealt with the challenging task proposed by the organizers. Although
initially we intended to consider using a large corpus (ukWAC) as well for finding candidates, we found
ourselves in the technical impossibility of doing so, because of the costly (timewise especially) resources
required by its processing. What is left to be checked is to what extent the lexical and syntactic patterns
that can be extracted from a corpus help us improve the results.
We cannot boast good results of our approach mainly because we used only a dictionary (in the form
of the PWN). Although it was created on psychological principles about the way words are structured in
the speakers? mind, it cannot ensure satisfying results. At least within our approach, the contribution of
the relations encoded in PWN is very low. An evaluation of the type n top-ranked candidates could have
a higher accuracy for our type of approach. We could dare say that our approach was a further proof of
the statement tested by (Zock and Shcwab, 2013): ?Words storage does not guarantee their access?.
References
Thorsten Brants, and Alex Franz. 2006. Web 1T 5-gram Version 1 LDC2006T13. Philadelphia: Linguistic Data
Consortium.
Gemma Bel Enguix, Reinhard Rapp, and Michael Zock. 2014. How Well Can a Corpus-Derived Co-Occurrence
Network Simulate Human Associative Behavior? Proceedings of the 5th workshop on Cognitive Aspects of
Computational Language Learning (CogACLL 2014), pp. 43-48.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.
Rada Mihalcea, and Dan Moldovan. 2001. eXtended WordNet: Progress Report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources.
Dan Moldovan, and Adrian Novischi. 2002. Lexical Chains for Question Answering. Proceedings of COLING
2002.
Reinhard Rapp. 2008. The Computation of Associative Responses to Multiword Stimuli. Proceedings of the
workshop on Cognitive Aspects of the Lexicon (CogALex 2008), pp. 102-109.
Michael Zock, and Didier Schwab. 2013. L?index, une ressource vitale pour guider les auteurs `a trouver le mot
bloqu?e sur le bout de la langue. In Ressources Lexicales : contenu, construction, utilisation,
?evaluation, N.
Gala et M. Zock (eds.). John Benjamins.
74
