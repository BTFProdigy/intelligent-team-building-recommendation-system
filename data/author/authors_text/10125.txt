Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 361?368
Manchester, August 2008
Japanese Dependency Parsing Using a Tournament Model
Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
{masakazu-i, masayu-a, matsu}@is.naist.jp
Abstract
In Japanese dependency parsing, Kudo?s
relative preference-based method (Kudo
and Matsumoto, 2005) outperforms both
deterministic and probabilistic CKY-based
parsing methods. In Kudo?s method, for
each dependent word (or chunk) a log-
linear model estimates relative preference
of all other candidate words (or chunks) for
being as its head. This cannot be consid-
ered in the deterministic parsing methods.
We propose an algorithm based on a tour-
nament model, in which the relative pref-
erences are directly modeled by one-on-
one games in a step-ladder tournament. In
an evaluation experiment with Kyoto Text
Corpus Version 4.0, the proposed method
outperforms previous approaches, includ-
ing the relative preference-based method.
1 Introduction
The shared tasks of multi-lingual dependency pars-
ing took place at CoNLL-2006 (Buchholz and
Marsi, 2006) and CoNLL-2007 (Nivre et al,
2007). Many language-independent parsing al-
gorithms were proposed there. The algorithms
need to adapt to various dependency structure
constraints according to target languages: projec-
tive vs. non-projective, head-initial vs. head-final,
and single-rooted vs. multi-rooted. Eisner (1996)
proposed a CKY-like O(n3) algorithm. Yamada
and Matsumoto (2003) proposed a shift-reduce-
like O(n2) deterministic algorithm. Nivre et al
(2003; 2004) also proposed a shift-reduce-like
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
???hito-da.(man .)????yomanai(doesn?t read)??hon-wo(books)(a)  ?He is a man who doesn?t read books.?
?????yomanai.(doesn?t read .)??hon-wo(books)??kare-wa(He)(b)  ?He doesn?t read books.?
??kare-wa(He)
Figure 1: Examples of Japanese sentences.
O(n) deterministic algorithm for projective lan-
guages. The model is enhanced for non-projective
languages by Nivre and Nilsson (2005). McDon-
ald et al (2005) proposed a method based on
search of maximum spanning trees employing the
Chu-Liu-Edmonds algorithm (hereafter ?CLE al-
gorithm?) (Chu and Liu, 1965; Edmonds, 1967).
Most Japanese dependency parsers are based on
bunsetsu units, which are similar concept to En-
glish base phrases. The constraints in Japanese
dependency structure are stronger than those in
other languages. Japanese dependency structures
have the following constraints: head-final, single-
head, single-rooted, connected, acyclic and projec-
tive. Figure 1 shows examples of Japanese sen-
tences and their dependency structures. Each box
represents a bunsetsu. A dependency relation is
represented by an edge from a dependent to its
head. Though sentence (a) is similar to sentence
(b), the syntactic structures of these two are differ-
ent, especially because ?kare-wa? directly depends
on ?yomanai? in (b) but not in (a).
In dependency parsing of Japanese, determin-
istic algorithms outperform probabilistic CKY
methods. Kudo and Matsumoto (2002) applied the
361
cascaded chunking algorithm (hereafter ?CC al-
gorithm?) to Japanese dependency parsing. Ya-
mada?s method (Yamada and Matsumoto, 2003)
employed a similar algorithm. Sassano (2004)
proposed a linear-order shift-reduce-like algorithm
(hereafter ?SR algorithm?), which is similar to
Nivre?s algorithm (Nivre, 2003). These determin-
istic algorithms are biased to select nearer candi-
date heads since they examine the candidates se-
quentially, and once they find a plausible one they
never consider further candidates.
We experimented the CLE algorithm with
Japanese dependency parsing, and found that the
CLE algorithm is comparable to or in some cases
poorer than the deterministic algorithms in our ex-
periments. Actually, the CLE algorithm is not suit-
able for some of the constraints in Japanese depen-
dency structures: head-final and projective. First,
head-final means that dependency relation always
goes from left to right. Second, since the CLE al-
gorithm may produce non-projective dependency
trees, we need to conduct projectivity check in the
algorithm.
Kudo and Matsumoto (2005) proposed a rela-
tive preference-based method (hereafter ?relative
preference method?). They defined the parsing
algorithm as series of selection steps of the most
likely head for each bunsetsu out of all candidates.
The method has so far achieved the highest ac-
curacy in the experiments with Kyoto Text Cor-
pus Version 3.0 data 1, since other deterministic
methods do not consider relative preference among
candidate heads but solely consider whether the
focused-on pair of bunsetsu?s is in a dependency
relation or not.
We propose a model that takes a bunsetsu and
two candidate heads into consideration and se-
lects the better candidate head out of those two.
This step is repeated in a step ladder tournament
to get the best candidate head (hereafter we call
this model as a ?tournament model?). The tour-
nament model was first introduced by Iida et al
(2003) for coreference resolution. We applied this
model to selecting the most plausible candidate
head for each bunsetsu except for the sentence final
one.
Section 2 describes the tournament model com-
paring with previous research. Section 3 describes
1Note: Sassano?s SR algorithm is the highest by exper-
iment with the smaller data Kyoto Text Corpus Version 2.0
Relative preference method and SR algorithm are not com-
pared directly with the same data.
???hito-da.(man .)??hon-wo(books)??kare-wa(He)Focused-ondependent Its candidate heads
The most likely candidate head
????yomanai(doesn?t read)
Figure 2: Example of a tournament.
how the tournament model is applied to Japanese
dependency parsing. Section 4 shows the results
of evaluation experiments. Section 5 shows our
current and future work, and Section 6 gives con-
clusions of this research.
2 Tournament Model
The tournament model was first introduced by
Iida et al (2003) for coreference resolution. The
model chooses the most likely candidate in a step-
ladder tournament, that is a sequence of one-on-
one games between candidate referents for a given
anaphoric expression. In each game, the winner is
chosen by a binary classifier such as SVMs.
We applied the tournament model to Japanese
dependency parsing taking into consideration
Japanese constraints. The projective constraint is
easily met. When selecting candidate heads for the
focused-on dependent, we only consider those can-
didates that introduce no crossing dependency.
Figure 2 illustrates a tournament. The focused-
on dependent bunsetsu is ?kare-wa?, and the can-
didate heads are the three bunsetsu?s on the right-
hand side: ?hon-wo?, ?yomanai? and ?hito-da?.
The first game is ?hon-wo? vs. ?yomanai?. Then
the next game is the winner of the first game vs.
?hito-da?. The winner of the second game (i.e.,
?hito-da?) is chosen as the most likely candidate
of the dependent, ?kare-wa?.
In the tournament model, the most likely head of
a given bunsetsu is determined by a series of one-
on-one games in a tournament. Below, we present
the advantages of the tournament model by com-
parison with the previous methods.
2.1 Scope of Feature Views
The CC algorithm and SR algorithm consider only
a pair of bunsetsu?s ? a dependent and its candidate
362
head ? in the parsing action determination (here-
after ?2-tuple model?). The same 2-tuple may or
may not have a dependency relation when they ap-
pear in different context. For example, both (a)
and (b) in Figure 1 include the two bunsetsu?s,
?kare-wa? and ?yomanai?; in (b) they have a de-
pendency relation, but not in (a). The 2-tuple mod-
els and relative preference method cannot discrim-
inate between these two patterns without consider-
ing contextual features 2. The tournament model
can be regarded as a ?3-tuple model,? which con-
siders three bunsetsu?s ? a dependent and two can-
didate heads. The discriminative performance of
the 3-tuple model is greater than the 2-tuple mod-
els, since it directly compares two candidate heads
and selects the one that is more plausible than the
other candidate. Consider Figure 1 again. In (a),
?kare-wa? does not depend on ?yomanai? because
there is another bunsetsu ?hito-da? which is a more
plausible head. 2-tuple models may use this infor-
mation as a contextual feature, but the effect is in-
direct. On the other hand, the tournament model
directly compares these candidates and always se-
lects the better one. The situation becomes crucial
when the true head appears outside of the context
window of the current candidate. 2-tuple models
have to select the head without consulting such in-
formation. The advantage of the tournament model
is its capability of deferring the decision by al-
ways keeping the current best candidate head. On
the other hand, a disadvantage of the tournament
model is its space and time complexity. The size of
features is larger since they come from three bun-
setsu?s. The size of training instances is also larger.
2.2 Relative Position in a Sentence
We name the two candidate heads in the 3-tuple
model as ?the nearer candidate head? and ?the far-
ther candidate head.? The dependent, the nearer
candidate head and the farther candidate head ap-
pear in this order in Japanese sentences. The order
defines the relative position of the contextual fea-
tures. The distance between the dependent and a
candidate head is another feature to represent the
relative position. In previous research, the distance
has been represented by feature buckets, such as 1,
2-5, or 6+. While for some dependents and their
heads whether the distance is 1 or not is impor-
tant, absolute distance is not so important since
2Contextual features are features neither in the dependent
nor in the candidate head(s).
Japanese is a free-order language. Relative posi-
tions are more informative since some dependents
tend to appear closer to other dependents, such
as objects that tend to appear closer to predicates
compared with other complements. The tourna-
ment model represents both the distance and rela-
tive position as features.
The deterministic algorithms are biased to select
nearer candidate heads. As most dependent and
head pairs appear within a close window, this ten-
dency does not cause many errors; deterministic
algorithms are weak at finding correct heads that
appear in a long distance as pointed out in Kudo
and Matsumoto (2005).
2.3 Relative Preferences
What the dependency parsers try to learn is rela-
tive preference of bunsetsu dependency, i.e., how
a dependent selects its head among others. The
relative preference method (Kudo and Matsumoto,
2005) learns the relative preferences among the
candidate heads by a discriminative framework.
The relative preferences are learned with the log-
linear model so as to give larger probability to
the correct dependent-head pair over any other
candidates. McDonald?s method (2005) with the
CLE algorithm learns the relative preferences by
a perceptron algorithm ? MIRA (Crammer and
Singer, 2003), so that the correct dependent-head
link receives a higher score. The tournament
model learns which candidate is more likely to be
the head between two candidates in a one-on-one
game in a tournament. Therefore, all of those pars-
ing algorithms try to learn the way to give the high-
est preference to the correct dependent-head pair
among all possibilities though in different settings.
While the relative preference method and Mc-
Donald?s method consider all candidate heads in-
dependently in a discriminative model, the tour-
nament model evaluates which candidate is more
likely to be the head between the latest winner and
the new candidate. The latest winner has already
defeated all of the preceding candidates. If the
new candidate beats the latest winner, it becomes
the new winner, meaning that it is the most pre-
ferred candidate among others so far considered.
Through this way of comparison with the runner-
up candidates, the tournament model uses richer
information in learning relative preferences than
the models in which all candidates are indepen-
dently considered.
363
// N: # of bunsetsu?s in input sentence
// true_head[j]: bunsetsu j?s head at
// training data
// gen(j,i1,i2,LEFT): generate
// an example where bunsetsu j is
// dependent of i1
// gen(j,i1,i2,RIGHT): generate
// an example where bunsetsu j is
// dependent of i2
for j = 1 to N-1 do
h = true_head[j];
for i = j+1 to h-1 do
gen(j,i,h,RIGHT);
for i = h+1 to N do
gen(j,h,i,LEFT);
end-for;
Figure 3: Pseudo code of training example gener-
ation procedure.
3 Proposed Algorithm
3.1 Training Example Generation Algorithm
As shown in Figure 3, for each dependent, we gen-
erate pairs of the correct head and all other candi-
date heads. On the example generation, the proce-
dure does not take into account the projective con-
straint; all bunsetsu?s on the right-hand side of the
focused-on dependent are candidate heads.
Table 1 shows all examples generated from two
sentences shown in Figure 1. 2-tuple models gen-
erate training examples formed as (dependent, can-
didate). So, from the sentences of Figure 1, it gen-
erates opposite classes to the pair (kare-wa, hito-
da). On the other hand, the examples generated by
the tournament model do not contain such incon-
sistency.
3.2 Parsing Algorithm
The tournament model has quite wide freeness in
the parsing steps. We introduce one of the tour-
nament algorithms, in which the dependents are
picked from right to left; and the games of the tour-
nament are performed from left to right. This pars-
ing algorithm takes into account the projective and
head-final constraints.
This algorithm is shown in Figure 4. The over-
all parsing process moves from right to left. On
selecting the head for a dependent all of the bun-
setsu?s to the right of the dependent have already
been decided. In Figure 4, the array ?head?
stores the parsed results and ensures that only non-
crossing candidate heads are taken into considera-
tion.
// N: # of bunsetsu?s in
// input sentence
// head[]: (analyzed-) head of bunsetsu
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
head[] = {2,3,...,N-1,N,EOS};
for j = N-1 downto 1 do
h = j+1;
i = head[h];
while i != EOS do
if classify(j,h,i)==RIGHT
then h = i;
i = head[i];
end-while;
head[j] = h;
end-for;
Figure 4: Pseudo code of parsing algorithm.
Note that the structure of the tournament has lit-
tle effect on the results (< 0.1) in our preliminary
experiments. We tried 2 ? 2 options: the depen-
dents are picked from right to left or from left to
right; and the games of the tournament are per-
formed from right to left or from left to right. We
choose the most natural combination for Japanese
dependency parsing, which is easy to implement.
4 Experiment
4.1 Settings
We implemented the tournament model, the CC al-
gorithm (Kudo and Matsumoto, 2002), SR algo-
rithm (Sassano, 2004) and CLE algorithm (Mc-
Donald et al, 2005) with SVM classifiers. We
evaluated dependency accuracy and sentence accu-
racy using Kyoto Text Corpus Version 4.0, which is
composed by newspaper articles. Dependency ac-
curacy is the percentage of correct dependencies
out of all dependency relations. Sentence accuracy
is the percentage of sentences in which all depen-
dencies are determined correctly. Dependency ac-
curacy is calculated excluding the rightmost bun-
setsu of each sentence. 3 Sentences that consist of
one bunsetsu are not used in our experiments.
We use January 1st to 8th (7,587 sentences) for
the training data. We use January 9th (1,213 sen-
tences), 10th (1,479 sentences) and 15th (1,179
sentences) for the test data. We use TinySVM 4
as a binary classifier. Cubic polynomial kernel is
3Most research such as Kudo?s (2005) uses this criteria.
4http://chasen.org/?taku/software/
TinySVM/
364
Sentence Focused-on dependent Left(Nearer) candidate Right(Farther) candidate Class label
(a) kare-wa hon-wo hito-da. RIGHT
(a) kare-wa yomanai hito-da. RIGHT
(a) hon-wo yomanai hito-da. LEFT
(b) kare-wa hon-wo yomanai. RIGHT
Table 1: Generated examples from sentences in Figure 1.
used for the kernel function. Cost of constraint vi-
olation is 1.0. These SVM settings are the same
as previous research (Kudo and Matsumoto, 2002;
Sassano, 2004). All experiments were performed
on Dual Core Xeon 3GHz x 2 Linux machines.
4.2 Features
Here we describe features used in our experiments.
Note that for the tournament model, features cor-
responding to candidates are created for each of
the nearer and farther candidates. We define the
information of a word as the following features:
lexical forms, coarse-grained POS tags, full POS
tags and inflected forms. We also define the infor-
mation of a bunsetsu as word information for each
of syuji and gokei. Syuji is the head content word
of the bunsetsu, defined as the rightmost content
word. Gokei is the representative function word of
the bunsetsu, defined as the rightmost functional
word.
Existence of punctuations or brackets, whether
the bunsetsu is the first bunsetsu in the sentence,
and whether it is the final bunsetsu in the sentence
are also members of information of a bunsetsu.
Standard features are the following: Informa-
tion of the dependent and the candidate heads, dis-
tance between the dependent and the candidate
heads (1, 2-5 or 6+ bunsetsu?s), all punctuations,
brackets and all particles between the dependent
and the candidate heads.
Additional features are the following: All case
particles in the dependent and the candidate heads,
information of the leftmost word in the candidate
heads, and the lexical form of the neighboring bun-
setsu to the right of the candidate heads.
Case particle features are the following: All
case particles appearing in the candidates? depen-
dent. These features are intended to take into con-
sideration the correlation between the case parti-
cles in the dependent of a head. When the head is
a verb, it has a similar effect of learning case frame
information.
Standard and additional features are introduced
by Sassano (2004). The case particle feature is
newly introduced in this paper. Features corre-
sponding to the already-determined dependency
relation are called dynamic features, and the other
contextual features are called static features. Stan-
dard and additional features are static features,
and case particle features are dynamic features.
Whether a dynamic feature is available for a pars-
ing algorithm depends on the parsing order of the
algorithm.
4.3 Parsing Accuracy
The parsing accuracies of our model and previ-
ous models are summarized in Table 2. Note that,
since the CLE algorithm is non-deterministic and
dynamic features are not available for this algo-
rithm, we use only a standard and additional fea-
ture set instead of an all feature set. By McNemar
test (p < 0.01) on the dependency accuracy, the
tournament model significantly outperforms most
of other methods except for the SR algorithm on
January 10th data with all features (p = 0.083)
and the CC algorithm on January 10th data with
all features (p = 0.099). The difference between
the tournament models with all features and with
the standard feature only is significant except for
on January 9th data (p = 0.25).
The highest dependency accuracy reported for
January 9th of Kyoto Text Corpus Version 2.0 is
89.56% by Sassano(2004)?s SR algorithm. 5
Since we don?t have the outputs of Sassano?s ex-
periments, we cannot do a McNemar test between
the tournament model and Sassano?s results. Our
model outperforms Sassano?s results by the depen-
dency accuracy, but the difference between these
two is not significant by prop test (p = 0.097).
When we add the additional and case particle
features, the improvement of our model is less than
that of other algorithms. This is interpreted that
our model can consider richer contextual informa-
5This accuracy in Sassano (2004) is not for Kyoto Text
Corpus Version 4.0 but Version 2.0 The feature set of Sas-
sano?s experiment is also different from our experiment.
365
Method Features Jan. 9th Jan.10th Jan. 15th
Tournament Standard feature only 89.89/49.63 89.63/48.34 89.40/49.70
All features 90.09/49.71 90.11/49.02 90.35/52.59
SR algorithm Standard feature only 88.18/45.92 88.80/44.76 88.03/47.24
(Sassano, 2004) All features 89.22/47.90 89.79/47.87 89.55/49.79
CC algorithm Standard feature only 88.17/45.92 88.80/44.76 88.00/47.24
(Kudo and Matsumoto, 2002) All features 89.22/47.90 89.80/47.94 89.53/49.79
CLE algorithm Standard feature only 88.64/45.34 88.16/43.14 88.07/45.21
(McDonald et al, 2005) Standard and Additional 89.21/46.83 89.05/45.03 88.90/48.43
Table 2: Dependency and sentence accuracy [%] using 7,587 sentences as training data.
tion within the algorithm itself than other models.
This result also shows that the accuracies of the
SR algorithm and CC algorithm are comparable
when using the same features. However, this does
not mean that their substantial power is compara-
ble because the parsing order limits the available
dynamic features.
4.4 Parsing Speed
Parsing time and the size of the training exam-
ples are shown in Table 3. All features were
used. The column ?# Step? represents the number
of SVM classification steps in parsing all the test
data. Time complexity of the tournament model
and CC algorithm are O(n2) and that of the SR al-
gorithm is O(n). The tournament model needs 1.7
times more SVM classification steps and is 4 times
slower than the SR algorithm. The reason for this
difference in steps (x1.7) and time (x4) is the num-
ber of training examples and features in the SVM
classification.
4.5 Comparison to Relative Preference
Method
We performed another experiment under the same
settings as Kudo?s (2005) to compare the tourna-
ment model and relative preference method. The
corpus is Kyoto Text Corpus Version 3.0 since
Kudo and Matsumoto (2005) used this corpus.
Training data is articles from January 1st to 11th
and editorials from January to August (24,263 sen-
tences). Test data is articles from January 14th
to 17th and editorials from October to December
(9,287 sentences). We did not perform parameter
engineering by development data, although Kudo
and Matsumoto (2005) performed it. The criteria
for dependency accuracy are the same as the exper-
iments above. However, the criteria for sentence
accuracy in this section include all sentences, even
if the length is one as Kudo and Matsumoto (2005)
did.
The results are shown in Table 4. Note that
Kudo and Matsumoto (2005) and our feature sets
are different. Only the CC Algorithm is tested with
both feature sets. Our feature set looks better than
Kudo?s. By McNemar test (p < 0.01) on the de-
pendency accuracy, the tournament model outper-
forms both the SR and CC algorithms significantly.
Since we don?t have the outputs of relative prefer-
ence methods, we cannot do a McNemar test be-
tween the tournament model and the relative pref-
erence methods. By prop test (p < 0.01) on the
dependency accuracy, our model significantly out-
performs the relative preference method of Kudo
and Matsumoto (2005). Though our model outper-
forms the ?combination? model of Kudo and Mat-
sumoto (2005) by the dependency accuracy, the
difference between these two is not significant by
prop test (p = 0.014). 6
Note that, a log-linear model is used in Kudo?s
experiment. The log-linear model has shorter
training time than SVM. The log-linear model re-
quires feature combination engineering by hand,
while SVMs automatically consider the feature
combination by the use of polynomial kernels.
5 Discussion and Future Work
In our error analysis, many errors are observed in
coordination structures. Sassano (2004) reported
that introduction of features of coordinated bun-
6The ?combination? model is the combination of the CC
algorithm and relative preference method. In Kudo?s exper-
iment, whereas the relative preference method outperforms
the CC algorithm for long-distance relations, it is reversed for
short-distance relations. They determined the optimal combi-
nation (the threshold set at bunsetsu length 3) using the devel-
opment set. In our experiment, the tournament model outper-
forms the CC and SR algorithms for relations of all lengths.
Therefore, the tournament model doesn?t need such ad hoc
combination.
366
Method # Step Time[s] # Example # Feature
Tournament 26396 371 374579 56165
SR algorithm (Sassano, 2004) 15641 80 94669 37183
CC algorithm (Kudo and Matsumoto, 2002) 18922 99 112759 37183
Table 3: Parsing time and the size of the training examples.
Method Features Dep. Acc. Sentence Acc.
Tournament All 91.96 57.44
SR algorithm (Sassano, 2004) All 91.48 55.67
CC algorithm (Kudo and Matsumoto, 2002) All 91.47 55.65
Combination ? CC and Relative preference Kudo?s (2005) 91.66 56.30
Relative preference (Kudo and Matsumoto, 2005) Kudo?s (2005) 91.37 56.00
CC algorithm (Kudo and Matsumoto, 2002) Kudo?s (2005) 91.23 55.59
Table 4: Dependency and sentence accuracy [%] using 24,263 sentences as training data with all features:
comparison with Kudo(2005)?s experiments.
setsu improves accuracy. In Kyoto Text Corpus
Version 4.0, coordination and apposition are anno-
tated with different types of dependency relation.
We did not use this information in parsing. A sim-
ple extension is to include those dependency types.
Another extension is to employ a coordination ana-
lyzer as a separate process as proposed by Shimbo
and Hara (2007).
Incorporating co-occurrence information will
also improve the parsing accuracy. One usage of
such information is verb-noun co-occurrence in-
formation that would represent selectional prefer-
ence for case-frame information. Abekawa and
Okumura (2006) proposed a reranking method
of k-best dependency analyzer outputs using co-
occurrence information. We have already devel-
oped a method to output k-best dependency trees.
One of our future works is to test the reranking
method using co-occurrence information on the k-
best dependency trees.
Multilingual parsing is another goal. Japanese
is a strict head-final language. However, most lan-
guages do not have such constraint. A different
parsing algorithm should be employed for other
less constrained languages so as to relax this con-
straint. A simple solution is to introduce a discrim-
ination model according to whether the head is on
the left-hand-side or on the right-hand-side of a de-
pendent. Existence of projective constraint does
not matter for the tournament model. The tourna-
ment model can be extended to relax the projec-
tive constraint. The preliminary results for English
are shown in our CoNLL Shared Task 2008 report
(Watanabe et al, 2008). The unlabeled syntac-
tic dependency accuracy of 90.73% for WSJ data
shows that the model is also effective in other (not
strictly head final, non-projective) languages. In
parsing word sequences, O(n2) time complexity
becomes a serious problem compared to parsing
bunsetsu sequences. Since a bunsetsu is a base
phrase in Japanese, the number of bunsetsu?s is
much less than the number of words. One solution
is to perform base phrase chunking in advance and
to apply dependency parsing on the base phrase se-
quences.
A reviewer pointed out similarities between our
model and RankSVM. RankSVM compares pairs
of elements to find out relative ordering between
elements. Our tournament model is a special case
where two elements are compared, but with a spe-
cific viewpoint of a focused dependent.
6 Conclusions
We proposed a Japanese dependency parsing al-
gorithm using the tournament model. The tour-
nament model is a 3-tuple bunsetsu model and
improves discriminative performance of selecting
correct head compared with the conventional 2-
tuple models. The most likely candidate head is
selected by one-on-one games in the step-ladder
tournament. The proposed model considers the
relative position between the nearer and farther
candidates. The model also considers all candi-
date heads, which are not considered in determin-
istic parsing algorithms. The tournament model
is robust for the free-order language. The accu-
367
racy of our model significantly outperforms that
of the previous methods in most experiment set-
tings. Even though the problem of parsing speed
remains, our research showed that considering
two or more candidate heads simultaneously can
achieve more accurate parsing.
References
Abekawa, Takeshi and Manabu Okumura. 2006.
Japanese Dependency Parsing Using Co-occurrence
Information and a Combination of Case Elements.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 833?840.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149?164.
Chu, Yoeng-Jin and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14:1396?1400.
Crammer, Koby and Yoram Singer. 2003. Ultraconser-
vative Online Algorithms for Multiclass Problems.
Journal of Machine Learning Research, 3:951?991.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the Natural Bureau of Standards,
71B:233?240.
Eisner, Jason M. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
COLING-96: Proceedings of the 16th Conference on
Computational Linguistics - Volume 1, pages 340?
345.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ?The Computational Treatment of
Anaphora?.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis Using Cascaded Chunking. In
CoNLL-2002: Proceedings of the Sixth Conference
on Computational Language Learning, pages 1?7.
Kudo, Taku and Yuji Matsumoto. 2005. Japanese De-
pendency Parsing Using Relative Preference of De-
pendency (in Japanese). Information Processing So-
ciety of Japan, Journal, 46(4):1082?1092.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers. In ACL-2005: Proceedings of
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 523?530.
Nivre, Joakim and Jens Nilsson. 2005. Psuedo-
Projective Dependency Parsing. In ACL-2005: Pro-
ceedings of 43rd Annual Meeting of the Association
for Computational Linguistics, pages 99?106.
Nivre, Joakim and Mario Scholz. 2004. Deterministic
Dependency Parsing of English Text. In COLING-
2004: Proceedings of the 20th International Confer-
ence on Computational Linguistics, pages 64?70.
Nivre, Joakim, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915?932.
Nivre, Joakim. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In IWPT-2003: 8th In-
ternational Workshop on Parsing Technology, pages
149?160.
Sassano, Manabu. 2004. Linear-Time Dependency
Analysis for Japanese. In COLING-2004: Proceed-
ings of the 20th International Conference on Com-
putational Linguistics, pages 8?14.
Shimbo, Masashi and Kazuo Hara. 2007. A Discrimi-
native Learning Model for Coordinate Conjunctions.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 610?619.
Watanabe, Yotaro, Masakazu Iwatate, Masayuki Asa-
hara, and Yuji Matsumoto. 2008. A Pipeline Ap-
proach for Syntactic and Semantic Dependency Pars-
ing. In Proceedings of the Twelfth Conference on
Computational Natural Language Learning (To Ap-
pear).
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In IWPT-2003: 8th International Workshop
on Parsing Technology, pages 195?206.
368
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228?232
Manchester, August 2008
A Pipeline Approach for Syntactic and Semantic Dependency Parsing
Yotaro Watanabe and Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masakazu-i, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes our system for syn-
tactic and semantic dependency parsing
to participate the shared task of CoNLL-
2008. We use a pipeline approach, in
which syntactic dependency parsing, word
sense disambiguation, and semantic role
labeling are performed separately: Syn-
tactic dependency parsing is performed
by a tournament model with a support
vector machine; word sense disambigua-
tion is performed by a nearest neighbour
method in a compressed feature space by
probabilistic latent semantic indexing; and
semantic role labeling is performed by
a an online passive-aggressive algorithm.
The submitted result was 79.10 macro-
average F1 for the joint task, 87.18% syn-
tactic dependencies LAS, and 70.84 se-
mantic dependencies F1. After the dead-
line, we constructed the other configura-
tion, which achieved 80.89 F1 for the joint
task, and 74.53 semantic dependencies F1.
The result shows that the configuration of
pipeline is a crucial issue in the task.
1 Introduction
This paper presents the description of our system
in CoNLL-2008 shared task. We split the shared
task into five sub-problems ? syntactic dependency
parsing, syntactic dependency label classification,
predicate identification, word sense disambigua-
tion, and semantic role labeling. The overview
of our system is illustrated in Figure 1. Our de-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Figure 1: Overview of the System
pendency parsing module is based on a tourna-
ment model (Iida et al, 2003), in which a depen-
dency attachment is estimated in step-ladder tour-
nament matches. The relative preference of the at-
tachment is modeled by one-on-one match in the
tournament. Iwatate et al (Iwatate et al, 2008)
initially proposed the method for Japanese depen-
dency parsing, and we applied it to other languages
by relaxing some constraints (Section 2.1). Depen-
dency label classification is performed by a linear-
chain sequential labeling on the dependency sib-
lings like McDonald?s schemata (McDonald et al,
2006). We use an online passive-aggressive al-
gorithm (Crammer et al, 2006) for linear-chain
sequential labeling (Section 2.2). We also use
the other linear-chain sequential labeling method
to annotate whether each word is a predicate or
not (Section 2.3). If an identified predicate has
more than one sense, a nearest neighbour classifier
disambiguates the word sense candidates (Section
2.4). We use an online passive-aggressive algo-
rithm again for the semantic role labeling (Section
2.5). The machine learning algorithms used in sep-
arated modules are diverse due to role sharing.
1
1
Unlabeled dependency parsing was done by Iwatate, de-
pendency label classification and semantic role labeling was
done by Watanabe, predicate identification and word sense
228
We attempt to construct a framework in which
each module passes k-best solutions and the last
semantic role labeling module performs rerank-
ing of the k-best solutions using the overall infor-
mation. Unfortunately, we couldn?t complete the
framework before the deadline of the test run. Our
method is not a ?joint learning? approach but a
pipeline approach.
2 Methods
2.1 Unlabeled Dependency Parsing
The detailed description of the tournament model-
based Japanese dependency parsing is found in
(Iwatate et al, 2008). The original Iwatate?s pars-
ing algorithm was for Japanese, which is for a
strictly head-final language. We adapt the algo-
rithm to English in this shared task. The tour-
nament model chooses the most likely candidate
head of each of the focused words in a step-
ladder tournament. For a given word, the al-
gorithm repeats to compare two candidate heads
and finds the most plausible head in the series
of a tournament. On each comparison, the win-
ner is chosen by an SVM binary classifier with
a quadratic polynomial kernel
2
. The model uses
different algorithms for training example gener-
ation and parsing. Figures 2 and 3 show train-
ing example generation and parsing algorithm, re-
spectively. Time complexity of both algorithms is
O(n
2
) for the number of words in an input sen-
tence. Below, we present the features for SVM
// N: # of tokens in input sentence
// true_head[j]: token j?s head at
// training data
// gen(j,i1,i2,LEFT): generate an example
// where token j is dependent of i1
// gen(j,i1,i2,RIGHT): generate an example
// where token j is dependent of i2
// Token 0 is the virtual ROOT.
for j = 1 to N-1 do
h = true_head[j];
for i = 0 to h-1 do
if i!=j then gen(j,i,h,RIGHT);
for i = h+1 to N do
if i!=j then gen(j,h,i,LEFT);
end-for;
Figure 2: Pseudo Code of Training Example Gen-
eration
disambiguation was done by Asahara, and all tasks were su-
pervised by Matsumoto.
2
We use TinySVM as an SVM classifier. chasen.org/
?
taku/software/TinySVM/
// N: # of tokens in input sentence
// head[]: (analyzed-) head of tokens
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
// cands.push_back(k): add token index k
// to the end of cands.
// cands.erase(i): remove i-th element
// from cands.
for j = 1 to N do
cands = [];
for i = 0 to N do
if i!=j then cands.push_back(i);
end-for;
while cands.size() > 1 do
if classify(j,cands[0],
cands[1]) = LEFT then
cands.erase(1);
else
cands.erase(0);
end-if;
end-while;
head[j] = cands[0];
end-for;
Figure 3: Pseudo Code of Parsing Algorithm
in our tournament model. The FORM, LEMMA,
GPOS(for training), PPOS(for testing, instead of
GPOS), SPLIT FORM, SPLIT LEMMA, PPOSS
in the following tokens were used as the features:
? Dependent, candidate1, candidate2
? Immediately-adjacent tokens of dependent, candidate1,
candidate2, respectively
? All tokens between dependent-candidate1, dependent-
candidate2, candidate1-candidate2, respectively
We also used the distance feature: distance (1 or
2-5 or 6+ tokens) between dependent-candidate1,
dependent-candidate2, and candidate1-candidate2.
Features corresponding to the candidates, includ-
ing the distance feature, have a prefix that indicates
its side: ?L-?(the candidate appears on left-hand-
side of the dependent) or ?R-?(appears on right-
hand-side of the dependent). Training an SVM
model with all examples is time-consuming, and
split the examples by the dependent GPOS for
training (PPOS for testing, instead of GPOS
3
) to
run SVM training in parallel. Since the number of
examples with the dependent PPOS:IN, NN, NNP
3
We cannot use GPOS for testing due to the shared task
regulation.
229
is still large, we used only first 1.5 million exam-
ples for the dependent GPOS. Note that, the algo-
rithm does not check the well-formedness of de-
pendency trees
4
.
2.2 Dependency Label Classification
This phase labels a dependency relation label to
each word in a parse tree produced in the preced-
ing phase. (McDonald et al, 2006) suggests that
edges of head x
i
and its dependents x
j1
, ..., x
jM
are highly correlated, and capturing these corre-
lation improves classification accuracy. In their
approach, edges of a head and its dependents
e
i,j1
, ..., e
i,jM
are classified sequentially, and then
Viterbi algorithm is performed to find the highest
scoring label sequence. We take a similar approach
with some simplification. In our system, each edge
is classified deterministically, and the previous de-
cision is used as a feature for the subsequent clas-
sification.
We use an online passive aggressive algorithm
(Crammer et al, 2006)
5
for dependency label clas-
sification since it converges fast, gives good per-
formance and can be implemented easily. The fea-
tures used in this phase are primarily similar to that
of (McDonald et al, 2006).
Word features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the head and the dependent.
Position: Position relation between the head and the depen-
dent (Is the head anterior to dependent?). Is the word
top of the sentence? Is the word last of the sentence?
Context features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the nearest left/right word. SPLIT LEMMA
and PPOS bigram (ww, wp, pw, pp) of the head and the
dependent (window size 5).
Sibling features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the dependent?s nearest left and right siblings
in the dependency tree.
Other features: The number of dependent?s children.
Whether the dependent and the dependent?s grand
parent SPLIT LEMMA/PPOS are the same. The
previous classification result (previous label).
2.3 Predicate Identification
This phase solves which word can be a predi-
cate. In the predicate spotting, the linear-chain
4
We tried to make a k-best cascaded model among the
modules. The latter module can check the well-formedness
of the tree. The current implementation skips this well-
formedness checking.
5
We use PA algorithm among PA, PA-I and PA-II in
(Crammer et al, 2006).
CRF (Lafferty et al, 2001) annotates whether the
word is a predicate or not. The FORM, LEMMA
(itself, and whether the LEMMA is registered in
the PropBank/NomBank frames), SPLIT FORM,
SPLIT LEMMA, PPOSS within 5 token window
size are used as the features. We also use bigram
features within 3 token window size and trigram
features within 5 token window size for FORM,
LEMMA, SPLIT FORM, SPLIT LEMMA, and
PPOSS. The main reason why we use a sequence
labeling method for predicate identification was
to relax the effect of the tagging error of PPOS
and PPOSS. However, we will show later that this
module aggravates the total performance.
2.4 Word Sense Disambiguation
For the word sense disambiguation, we use 1-
nearest neighbour method in a compressed fea-
ture space by probabilistic latent semantic index-
ing (PLSI). We trained the word sense disambigua-
tion model from the example sentences in the train-
ing/development data and PropBank/NomBank
frames. The metric in the nearest neighbour
method is based on the occurrence of LEMMA
in the example sentences. However, the exam-
ples in the PropBank/NomBank do not contain the
lemma information. To lemmatize the words in
the PropBank/NomBank, we compose a lemma-
tizer from the FORM-LEMMA table in the train-
ing and development.
6
Since the metric space
is very sparse, PLSI (Hofmann, 1999) is used to
reduce the metric space dimensions. We used
KL-divergence between two examples of P (d
i
|z
k
)
of P (d
i
, w
j
) =
?
k
P (d
i
|z
k
)P (w
j
|z
k
)P (z
k
) as
hemi-metric for the nearest neighbour method
7
,
in which d
i
? D is an example sentence in the
training/devel/test data and PropBank/NomBank
frames; w
j
? W is LEMMA; and z
k
? Z is a
latent class. We use |Z| = 100, which gave the
best performance in the development data. Note,
we transductively used the test data for the PLSI
modeling within the test run period.
2.5 Semantic Role Labeling
While semantic role labeling task is generally per-
formed by two phases: argument identification and
argument classification, we did not divide the task
6
We are not violating the closed track regulation to build
the lemmatizer. If a word in the PropBank/NomBank is not in
the training/development data, we give up lemmatization.
7
We useD
KL
=
?
k
P (d
input data
|z
k
)log
P (d
input data
|z
k
)
P (d
1-nearest data
|z
k
)
as hemi-metric. It is a non-commutative measure.
230
into the two phases. That is, argument candidates
are directly assigned a particular semantic role la-
bel. We did not employ any candidate filtering pro-
cedure, so argument candidates consist of words in
any predicate-word pair. The argument candidates
that have no roles are assigned ?NONE? label. For
the reason that described in Section 2.2 (fast con-
vergence and good performance), we use an on-
line passive aggressive algorithm for learning the
semantic role classifiers.
Useful features for argument classification of
verb and noun predicates are different. For exam-
ple, voice (active or passive) is essential for verb
predicate?s argument classification. On the other
hand, presence of a genitive word is useful for
noun predicate?s argument classification. For this
reason, we created twomodels: argument classifier
for verb predicates and that for noun predicates.
Semantic frames are useful information for se-
mantic role classification. Generally, obligatory
arguments not included in semantic frames do not
appear in actual texts. For this reason, we use
PropBank/NomBank semantic frames for seman-
tic role pruning. Suppose semantic roles in the se-
mantic frame are F
i
= {A0, A1, A2, A3}. Since
obligatory arguments are {A0...AA}, the remain-
ing arguments {A4, A5, AA} are removed from
label candidates.
For verb predicates, the features used in our sys-
tem are based on (Hacioglu, 2004). We also em-
ployed some other features proposed in (Gildea
and Jurafsky, 2002; Pradhan et al, 2004b). For
noun predicates, the features are primarily based
on (Pradhan et al, 2004a). The features that we
defined for semantic role labeling are as follows:
Word features: SPLIT LEMMA and PPOS of the predicate,
dependent and dependent?s head, and its conjunctions.
Dependency label: The dependency label between the argu-
ment candidate and the its head.
Family: The position of the argument candidate with respect
to the predicate position over the dependency tree (e.g.,
child, sibling).
Position: The position of the head of the dependency relation
with respect to the predicate position in the sentence.
Pattern: The left-to-right chain of the PPOS/dependency la-
bels of the predicate?s children.
Context features: PPOS of the nearest left/right word.
Path features: SPLIT LEMMA, PPOS and dependency la-
bel paths between the predicate and the argument can-
didate, and its path bi-gram.
Distance: The number of paths between the predicate and
the argument candidate.
Voice: Voice of the predicate (active or passive) and voice-
position conjunction (for verb predicates).
Is predicate plural: Whether the predicate is singular or
plural (for noun predicates).
Genitives between the predicate and the argument: Is
there a genitive word between the predicate and the
argument? (for noun predicates)
3 Results
Table 1 shows the result of our system. The pro-
posed method was effective in dependency pars-
ing (rank 3rd), but was not good in semantic role
labeling (rank 9th). One reason of the result of
semantic role labeling could be usages of Prop-
Bank/NomBank frames. We did not achieve the
maximum use of the resources, hence the design of
features and the choice of learning algorithm may
not be optimal.
Figure 4: Overview of the Modified System
The other reason is the design of the pipeline.
We changed the design of the pipeline after the
test run. The overview of the modified system
is illustrated in Figure 4. After the syntactic de-
pendency parsing, we limited the predicate can-
didates as verbs and nouns by PPOSS, and fil-
tered the argument candidates by Xue?s method
(Xue and Palmer, 2004). Next, the candidate pair
of predicate-argument was classified by an online
passive-aggressive algorithm as shown in Section
2.5. Finally, the word sense of the predicate is de-
termined by the module in Section 2.4. The new
result is scores with ? in Table 1. The result means
that the first design was not the best for the task.
Acknowledgements
We would like to thank the CoNLL-2008 shared
task organizers and the data providers (Surdeanu
et al, 2008).
231
Problem All WSJ Brown Rank
Complete Problem 79.10 (80.89
?
) 80.30 (82.06
?
) 69.29 (71.32
?
) 9th
Semantic Dependency 70.84 (74.53
?
) 72.37 (76.01
?
) 58.21 (62.41
?
) 9th
Semantic Role Labeling 67.92 (72.31
?
) 69.31 (73.62
?
) 56.42 (61.64
?
) -
Predicate Identification & Word Sense Disambiguation 77.20 (79.17
?
) 79.02 (80.99
?
) 62.10 (64.03
?
) -
Syntactic Dependency (Labeled) 87.18 88.06 80.17 3rd
Syntactic Label Accuracy 91.63 92.31 86.26 -
Unlabeled Syntactic Dependency Unlabeled 90.20 90.73 85.94 -
The scores with ? mark are our post-evaluation results.
Table 1: The Results ? Closed Challenge
References
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149?164.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwarz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of Machine
Learning Research, 7:551?585.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Hacioglu, Kadri. 2004. Semantic role labeling using
dependency trees. In COLING-2004: Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 1273?1276.
Hofmann, Thomas. 1999. Probabilistic Latent Seman-
tic Indexing. In SIGIR-1999: Proceedings of the
22nd Annual International ACM SIGIR Conference
on Research and Development in Informatino Re-
trieval, pages 50?57.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ?The Computational Treatment of
Anaphora?, pages 23?30.
Iwatate, Masakazu, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese Dependency Parsing Using
a Tournament Model. In COLING-2008: Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (To Appear).
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML-1001: Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282?289.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In CoNLL-
2006: Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, pages 216?
220.
Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915?932.
Pradhan, Sameer, Honglin Sun, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2004a. Parsing Argu-
ments of Nominalizations in English and Chinese. In
HLT-NAACL-2004: Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 141?144.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004b. Shal-
low Semantic Parsing Using Support Vector Ma-
chines. In HLT-NAACL-2004: Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 233?240.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL-2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In EMNLP-
2004: Proceedings of 2004 Conference on Empirical
Methods in Natural Language Processing, pages 88?
94.
232
