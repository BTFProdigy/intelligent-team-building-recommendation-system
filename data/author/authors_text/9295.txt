Developments in Affect Detection in E-drama
Li Zhang, John A. Barnden, Robert J. Hendley, and Alan M. Wallington
School of Computer Science
University of Birmingham, UK
l.zhang@cs.bham.ac.uk
Abstract
We report work1 in progress on adding
affect-detection to an existing program
for virtual dramatic improvisation, moni-
tored by a human director. To partially
automate the directors? functions, we
have partially implemented the detection
of emotions, etc. in users? text input, by
means of pattern-matching, robust pars-
ing and some semantic analysis. The
work also involves basic research into
how affect is conveyed by metaphor.
1 Introduction
Improvised drama and role-play are widely used
in education, counselling and conflict resolution.
Researchers have explored frameworks for e-
drama, in which virtual characters (avatars) in-
teract under the control of human actors. The
springboard for our research is an existing sys-
tem (edrama) created by Hi8us Midlands Ltd,
used in schools for creative writing and teaching
in various subjects. The experience suggests that
e-drama helps students lose their usual inhibi-
tions, because of anonymity etc. In edrama,
characters are completely human-controlled,
their speeches textual in speech bubbles, and
their visual forms cartoon figures. The actors
(users) are given a loose scenario within which to
improvise, but are at liberty to be creative. There
is also a human director, who constantly moni-
tors the unfolding drama and can intervene by,
1 This work is supported by grant RES-328-25-0009 from
the ESRC under the ESRC/EPSRC/DTI ?PACCIT? pro-
gramme. We are grateful to Hi8us Midlands Ltd, Maverick
Television Ltd, BT, and our colleagues W.H. Edmondson,
S.R. Glasbey, M.G. Lee and Z. Wen. The work is also par-
tially supported by EPSRC grant EP/C538943/1.
for example, sending messages to actors, or by
introducing and controlling a minor ?bit-part?
character to interact with the main characters.
But this places a heavy burden on directors, es-
pecially if they are, for example, teachers and
unpracticed in the directorial role. One research
aim is thus partially to automate the directorial
functions, which importantly involve affect de-
tection. For instance, a director may intervene
when emotions expressed or discussed by char-
acters are not as expected. Hence we have devel-
oped an affect-detection module. It has not yet
actually been used for direction, but instead to
control a simple automated bit-part actor,
EmEliza. The module identifies affect in charac-
ters? speeches, and makes appropriate responses
to help stimulate the improvisation. Within affect
we include: basic and complex emotions such as
anger and embarrassment; meta-emotions such
as desiring to overcome anxiety; moods such as
hostility; and value judgments (of goodness,
etc.). Although merely detecting affect is limited
compared to extracting full meaning, this is often
enough for stimulating improvisation.
Much research has been done on creating affec-
tive virtual characters in interactive systems. Emo-
tion theories, particularly that of Ortony et al
(1988; OCC in the following), have been used
widely. Prendinger & Ishizuka (2001) used OCC
to reason about emotions. Mehdi et al (2004) used
OCC to generate emotional behaviour. Gratch and
Marsella?s (2004) model reasons about emotions.
However, few systems are aimed at detecting af-
fect as broadly as we do and in open-ended utter-
ances. Although Fa?ade (Mateas, 2002) included
processing of open-ended utterances, the broad
detection of emotions, rudeness and value judge-
ments is not covered. Zhe & Boucouvalas (2002)
demonstrated emotion extraction using a tagger
and a chunker to help detect the speaker?s own
emotions. But it focuses only on emotional adjec-
tives, considers only first-person emotions and
203
neglects deep issues such as figurative expression.
Our work is distinctive in several respects. Our
interest is not just in (a) the positive first-person
case: the affective states that a virtual character X
implies that it has (or had or will have, etc.), but
also in (b) affect that X implies it lacks, (c) affect
that X implies that other characters have or lack,
and (d) questions, commands, injunctions, etc.
concerning affect. We aim also for the software to
cope partially with the important case of meta-
phorical conveyance of affect (Fussell & Moss,
1998; K?vecses, 1998).
Our project does not involve using or develop-
ing deep, scientific models of how emotional
states, etc., function in cognition. Instead, the
deep questions investigated are on linguistic mat-
ters such as the metaphorical expression of af-
fect. Also, in studying how people understand
and talk about affect, what is of prime impor-
tance is their common-sense views of how affect
works, irrespective of scientific reality. Metaphor
is strongly involved in such views.
2 A Preliminary Approach
Various characterizations of emotion are used in
emotion theories. The OCC model uses emotion
labels and intensity, while Watson and Tellegen
(1985) use positive and negative affects as the
major dimensions. Currently, we use an evalua-
tion dimension (positive and negative), affect
labels and intensity. Affect labels with intensity
are used when strong text clues signalling affect
are detected, while the evaluation dimension
with intensity is used when only weak text clues
are detected.
2.1 Pre-processing Modules
The language in the speeches created in e-drama
sessions, especially by excited children, severely
challenges existing language-analysis tools if
accurate semantic information is sought. The
language includes misspellings, ungrammatical-
ity, abbreviations (such as in texting), slang, use
of upper case and special punctuation (such as
repeated exclamation marks) for affective em-
phasis, repetition of letters or words for empha-
sis, and open-ended onomatopoeic elements such
as ?grrrr?. The genre is similar to Internet chat.
To deal with the misspellings, abbreviations
and onomatopoeia, several pre-processing mod-
ules are used before the detection of affect starts
using pattern matching, syntactic processing by
means of the Rasp parser (Briscoe & Carroll,
2002), and subsequent semantic processing.
A lookup table has been used to deal with ab-
breviations e.g. ?im (I am)?, ?c u (see you)? and
?l8r (later)?. It includes abbreviations used in
Internet chat rooms and others found in an anly-
sis of previous edrama sessions. We handle am-
biguity (e.g.,?2? (to, too, two) in ?I?m 2 hungry 2
walk?) by considering the POS tags of immedi-
ately surrounding words. Such simple processing
inevitably leads to errors, but in evaluations us-
ing examples in a corpus of 21695 words derived
from previous transcripts we have obtained
85.7% accuracy, which is currently adequate.
The iconic use of word length (corresponding
roughly to imagined sound length) as found both
in ordinary words with repeated letters (e.g.
?seeeee?) and in onomatopoeia and interjections,
(e.g. ?wheee?, ?grr?, ?grrrrrr?, ?agh?, ?aaaggghhh?)
normally implies strong affective states. We have
a small dictionary containing base forms of some
special words (e.g. ?grr?) and some ordinary
words that often have letters repeated in e-drama.
Then the Metaphone spelling-correction algo-
rithm, which is based on pronunciation, works
with the dictionary to locate the base forms of
words with letter repetitions.
Finally, the Levenshtein distance algorithm
with a contemporary English dictionary deals
with misspelling.
2.2 Affect Detection
In the first stage after the pre-processing, our
affect detection is based on textual pattern-
matching rules that look for simple grammatical
patterns or phrasal templates. Thus keywords,
phrases and partial sentence structures are ex-
tracted. The Jess rule-based Java framework is
used to implement the pattern/template-matching
rules. This method has the robustness to deal
with ungrammatical and fragmented sentences
and varied positioning of sought-after phraseol-
ogy, but lacks other types of generality and can
be fooled by suitable syntactic embedding. For
example, if the input is ?I doubt she?s really an-
gry?, rules looking for anger in a simple way will
output incorrect results.
The transcripts analysed to inspire our initial
knowledge base and pattern-matching rules had
independently been produced earlier from edrama
improvisations based on a school bullying sce-
nario. We have also worked on another, distinctly
different scenario concerning a serious disease,
based on a TV programme produced by Maverick
Television Ltd. The rule sets created for one sce-
nario have a useful degree of applicability to an-
other, although some changes in the specific
204
knowledge database will be needed.
As a simple example of our pattern-matching,
when the bully character says ?Lisa, you Pizza
Face! You smell?, the module detects that he is
insulting Lisa. Patterns such as ?you smell? have
been used for rule implementation. The rules work
out the character?s emotions, evaluation dimension
(negative or positive), politeness (rude or polite)
and what response EmEliza might make. Although
the patterns detected are based on English, we
would expect that some of the rules would require
little modification to apply to other languages.
Multiple exclamation marks and capitalisation
of whole words are often used for emphasis in e-
drama. If exclamation marks or capitalisation are
detected, then emotion intensity is deemed to be
comparatively high (and emotion is suggested
even without other clues).
A reasonably good indicator that an inner state
is being described is the use of ?I? (see also Craggs
and Wood (2004)), especially in combination with
the present or future tense. In the school-bullying
scenario, when ?I? is followed by a future-tense
verb, a threat is normally being expressed; and the
utterance is often the shortened version of an im-
plied conditional, e.g., ?I?ll scream [if you stay
here].? When ?I? is followed by a present-tense
verb, other emotional states tend to be expressed,
as in ?I want my mum? and ?I hate you?.
Another useful signal is the imperative mood,
especially when used without softeners such as
?please?: strong emotions and/or rude attitudes are
often being expressed. There are common impera-
tive phrases we deal with explicitly, such as ?shut
up? and ?mind your own business?. But, to go
beyond the limitations of the pattern matching
we have done, we have also used the Rasp parser
and semantic information in the form of the se-
mantic profiles for the 1,000 most frequently
used English words (Heise, 1965).
Although Rasp recognizes many simple im-
peratives directly, it can parse some imperatives
as declaratives or questions. Therefore, further
analysis is applied to Rasp?s syntactic output.
For example, if the subject of an input sen-
tence is ?you? followed by certain special verbs
or verb phrases (e.g. ?shut?, ?calm?, ?get lost?, ?go
away?, etc), and Rasp parses a declarative, then it
will be changed to imperative. If the softener
?please? is followed by a base forms of the verb,
the inputs are also deemed to be imperatives. If a
singular proper noun or ?you? is followed by a
base form of the verb, the sentence is deemed to
be imperative (e.g. ?Dave bring me the menu?).
When ?you? or a singular proper noun is fol-
lowed by a verb whose base form equals its past
tense form, ambiguity arises (e.g. ?Lisa hit me?).
For one special case of this, if the direct object is
?me?, we exploit the evaluation value of the verb
from Heise?s (1965) semantic profiles. Heise
lists values of evaluation (goodness), activation,
potency, distance from neutrality, etc. for each
word covered. If the evaluation value for the
verb is negative, then the sentence is probably
not imperative but a declarative expressing a
complaint (e.g ?Mayid hurt me?). If it has a posi-
tive value, then other factors suggesting impera-
tive are checked in this sentence, such as excla-
mation marks and capitalizations. Previous con-
versation is checked to see if there is any recent
question sentence toward the speaker. If so, then
the sentence is taken to be declarative.
There is another type of sentence: ?don?t you +
(base form of verb)?, which is often a negative
version of an imperative with a ?you? subject (e.g.
?Don?t you call me a dog?). Normally Rasp re-
gards such strings as questions. Further analysis
has also been implemented for such sentence
structure, which implies negative affective state,
to change the sentence type to imperative.
Aside from imperatives, we have also imple-
mented simple types of semantic extraction of
affect using affect dictionaries and WordNet.
3 Metaphorical Expression of Affect
The explicit metaphorical description of emo-
tional states is common and has been extensively
studied (Fussell & Moss, 1998). Examples are
?He nearly exploded?, and ?Joy ran through me.?
Also, affect is often conveyed implicitly via
metaphor, as in ?His room is a cess-pit?, where
affect associated with a source item (cess-pit) is
carried over to the corresponding target item.
Physical size is often metaphorically used to
emphasize evaluations, as in ?you are a big
bully?, ?you?re a big idiot?, and ?you?re just a
little bully?, although the bigness may be literal
as well. ?Big bully? expresses strong disapproval
(Sharoff, 2005) and ?little bully? can express
contempt, although ?little? can also convey sym-
pathy. Such examples are not only practically
important but also theoretically challenging.
We have also encountered quite creative use
of metaphor in e-drama. For example, in a
school-bullying improvisation that occurred,
Mayid had already insulted Lisa by calling her a
?pizza?, developing a previous ?pizza-face? in-
sult. Mayid then said ?I?ll knock your topping
off, Lisa? ? a theoretically intriguing spontane-
205
ous creative elaboration of the ?pizza? metaphor.
Our developing approach to metaphor handling
in the affect detection module is partly to look
for stock metaphorical phraseology and straight-
forward variants of it, and partly to use a simple
version of the more open-ended, reasoning-based
techniques taken from the ATT-Meta project
(Barnden et al, 2002; 2003; 2004). ATT-Meta
includes a general-purpose reasoning engine, and
can potentially be used to reason about emotion
in relation to other factors in a situation. In turn,
the realities of metaphor usage in e-drama ses-
sions are contributing to our basic research on
metaphor processing.
4 Conclusion
We have implemented a limited degree of affect-
detection in an automated actor by means of pat-
tern-matching, robust parsing and some semantic
analysis. Although there is a considerable dis-
tance to go in terms of the practical affect-
detection that we plan to implement, the already
implemented detection is able to cause reasona-
bly appropriate contributions by the automated
character. We have conducted a two-day pilot
user test with 39 secondary school students. We
concealed the involvement of an earlier version
of EmEliza in some sessions, in order to test by
questionnaire whether its involvement affects
user satisfaction, etc. None of the measures re-
vealed a significant effect. Also, judging by the
group debriefing sessions after the e-drama ses-
sions, nobody found out that one bit-part charac-
ter was sometimes computer-controlled. Further
user testing with students at several Birmingham
schools will take place in March 2006.
References
Barnden, J.A., Glasbey, S.R., Lee, M.G. & Walling-
ton, A.M., 2002. Reasoning in metaphor under-
standing: The ATT-Meta approach and system. In
Proceedings of the 19th International Confer-
ence on Computational Linguistics.
Barnden, J.A., Glasbey, S.R., Lee, M.G. & Walling-
ton, A.M., 2003. Domain-transcending mappings
in a system for metaphorical reasoning. In Pro-
ceedings of the Research Note Sessions of the
10th Conference of EACL.
Barnden, J.A., Glasbey, S.R., Lee, M.G. & Walling-
ton, A.M. 2004. Varieties and Directions of Inter-
domain Influence in Metaphor. Metaphor and
Symbol, 19(1), pp.1-30.
Briscoe, E. & J. Carroll. 2002. Robust Accurate Sta-
tistical Annotation of General Text. In Proceed-
ings of the 3rd International Conference on
Language Resources and Evaluation, Las Pal-
mas, Gran Canaria. pp.1499-1504.
Craggs, R. & Wood. M. 2004. A Two Dimensional
Annotation Scheme for Emotion in Dialogue. In
Proceedings of AAAI Spring Symposium: Ex-
ploring Attitude and Affect in Text.
Fussell, S. & Moss, M. 1998. Figurative Language in
Descriptions of Emotional States. In S. R. Fussell
and R. J. Kreuz (Eds.), Social and cognitive ap-
proaches to interpersonal communication.
Lawrence Erlbaum.
Gratch, J. & Marsella, S. 2004. A Domain-
Independent Framework for Modeling Emotion.
Journal of Cognitive Systems Research. Vol 5,
Issue 4, pp.269-306.
Heise, D. R. 1965. Semantic Differential Profiles for
1,000 Most Frequent English Words. Psychologi-
cal Monographs 79, pp.1-31.
K?vecses, Z. 1998. Are There Any Emotion-Specific
Metaphors? In Speaking of Emotions: Concep-
tualization and Expression. Athanasiadou, A.
and Tabakowska, E. (eds.), Berlin and New York:
Mouton de Gruyter, pp.127-151.
Mateas, M. 2002. Ph.D. Thesis. Interactive Drama,
Art and Artificial Intelligence. School of Computer
Science, Carnegie Mellon University.
Mehdi, E.J., Nico P., Julie D. & Bernard P. 2004.
Modeling Character Emotion in an Interactive Vir-
tual Environment. In Proceedings of AISB 2004
Symposium: Motion, Emotion and Cognition.
Leeds, UK.
Ortony, A., Clore, G.L. & Collins, A. 1988. The
Cognitive Structure of Emotions. CUP
Prendinger, H. & Ishizuka, M. 2001. Simulating Af-
fective Communication with Animated Agents. In
Proceedings of Eighth IFIP TC.13 Conference
on Human-Computer Interaction, Tokyo, Japan,
pp.182-189.
Sharoff, S. 2005. How to Handle Lexical Semantics in
SFL: a Corpus Study of Purposes for Using Size
Adjectives. Systemic Linguistics and Corpus.
London: Continuum.
Watson, D. & Tellegen, A. 1985. Toward a Consen-
sual Structure of Mood. Psychological Bulletin,
98, pp.219-235.
Zhe, X. & Boucouvalas, A. C. 2002. Text-to-Emotion
Engine for Real Time Internet Communication. In
Proceedings of International Symposium on
Communication Systems, Networks and DSPs,
Staffordshire University, UK, pp.164-168.
206
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 37?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
Don?t worry about metaphor: affect extraction for conversational agents
Catherine Smith, Tim Rumbell, John Barnden, Bob Hendley, Mark Lee & Alan Wallington
School of Computer Science, University of Birmingham
Birmingham B15 2TT, UK
J.A.Barnden@cs.bham.ac.uk
Abstract
We demonstrate one aspect of an affect-
extraction system for use in intelligent con-
versational agents. This aspect performs a
degree of affective interpretation of some
types of metaphorical utterance.
1 Introduction
Our demonstration is of one aspect of a system
for extracting affective information from individ-
ual utterances, for use in text-based intelligent con-
versational agents (ICAs). Affect includes emo-
tions/moods (such as embarrassment, hostility) and
evaluations (of goodness, importance, etc.). Our
own particular ICA [Zhang et al 2006] is for use
in an e-drama system, where human users behave as
actors engaged in unscripted role-play. Actors type
in utterances for the on-screen characters they con-
trol to utter (via speech bubbles). Our ICA is an-
other actor, controlling a bit-part character. Through
extracting affect from other characters? utterances it
makes responses that can help keep the conversation
flowing. The same algorithms are also used for in-
fluencing the characters? gesturing (when a 3D ani-
mation mode is used).
The system aspect demonstrated handles one im-
portant way in which affect is expressed in most dis-
course genres: namely metaphor. Only a relatively
small amount of work has been done on computa-
tional processing of metaphorical meaning, for any
purpose, let alne in ICA research. Major work
apart from ours on metaphorical-meaning compu-
tation includes (Fass, 1997; Hobbs, 1990; Mar-
tin, 1990; Mason, 2004; Narayanan, 1999; Veale,
1998). The e-drama genre exhibits a variety of types
of metaphor, with a significant degree of linguistic
open-endedness. Also, note that our overarching re-
search aim is to study metaphor as such, not just how
it arises in e-drama. This increases our need for sys-
tematic, open-ended methods.
2 Metaphor and Affect
Conveying affect is one important role for metaphor,
and metaphor is one important way of conveying
affect. Emotional states and behavior often them-
selves described metaphorically (Ko?vecses, 2000;
Fussell & Moss, 1998), as in ?He was boiling
inside? [feelings of anger]. But another impor-
tant phenomenon is describing something X using
metaphorical source terms that are subject to that
affect, as in ?My son?s room [= X] is a bomb site?
or ?smelly attitude? (an e-drama transcript exam-
ple). Such carry-over of affect in metaphor is well-
recognized, e.g. in the political domain (Musolff,
2004). Our transcript analyses indicate that this type
of affect-laden metaphor is a significant issue in e-
drama: at a conservative estimate, in recent user
studies in secondary schools at least one in every
16 speech-turns has contained such metaphor (each
turn is   100 characters, and rarely more than one
sentence; 33K words across all transcripts).
There are other specific, theoretically interesting
metaphorical phenomena arising in e-drama that are
important also for discourse in general, and plausi-
bly could be handled reasonably successfully in an
ICA using current techniques. Some are:
1) Casting someone as an animal. This often con-
veys affect, from insultingly negative to affection-
ately positive. Terms for young animals (?piglet?,
?wolf cub?, etc.) are often used affectionately, even
37
when the adult form is negative. Animal words can
have a conventional metaphorical sense, often with
specific affect, but in non-conventional cases a sys-
tem may still be able to discern a particular affective
connotation; and even if it cannot, it can still plausi-
bly infer that some affect is expressed, of unknown
polarity (positivity/negativity).
2) Rather similarly, casting someone as a monster
or as a mythical or supernatural being, using words
such as ?monster?, ?dragon,? ?angel,? ?devil.?
3) Casting someone as a special type of human, us-
ing words such as ?baby? (to an adult), ?freak,? ?girl?
(to a boy), ?lunatic.?
4) Metaphorical use of size adjectives (cf. Sharoff,
2006). Particularly, using ?a little X? to convey af-
fective qualities of X such as unimportance and con-
temptibility, but sometimes affection towards X, and
?big X? to convey importance of X (?big event?) or
intensity of X-ness (?big bully?)?and X can itself
be metaphorical (?baby?, ?ape?).
Currently, our system partially addresses (1), (2) and
(4).
3 Metaphor Recognition & Analysis
3.1 The Recognition Component
The basis here is a subset of a list of
metaphoricity signals we have compiled
[http://www.cs.bham.ac.uk/?jab/ATT-
Meta/metaphoricity-signals.html], by modify-
ing and expanding a list from Goatly (1997). The
signals include specific syntactic structures, phrase-
ological items and morphological elements. We
currently focus on two special syntactic structures,
X is/are Y and You/you Y, and some lexical strings
such as ?[looks] like?, ?a bit of a? and ?such a?. The
signals are merely uncertain, heuristic indicators.
For instance, in the transcripts mentioned in section
2, we judged X is/are Y as actually indicating the
presence of metaphor in 38% of cases (18 out of
47). Other success rates are: you Y ? 61% (22 out of
36); like (including looks like)? 81% (35 out of 43).
In order to detect signals we use the Grammatical
Relations (GR) output from the RASP robust parser
[Briscoe et al, 2006] This output shows typed word-
pair dependencies between the words in the utter-
ance. E.g., the GR output for ?You are a pig? is:
|ncsubj| |be+_vbr| |you_ppy| |_|
|xcomp| _ |be+_vbr| |pig_nn1|
|det| |pig_nn1| |a_at1|
For an utterance of the type X is/are Y the GRs will
always give a subject relation (ncsubj) between X
and the verb ?to be?, as well as a complement re-
lation (xcomp) between the verb and the noun Y.
The structure is detected by finding these relations.
As for you Y, Rasp also typically delivers an easily
analysable structure, but unfortunately the POS tag-
ger in Rasp seems to favour tagging Y as a verb?
e.g., ?cow? in ?You cow?. In such a case, our system
looks the word up in a list of tagged words that forms
part of the RASP tagger. If the verb can be tagged
as a noun, the tag is changed, and the metaphoricity
signal is deemed detected. Once a signal is detected,
the word(s) in relevant positions (e.g. the Y posi-
tion) position are pulled out to be analysed. This
approach has the advantage that whether or not the
noun in, say, the Y position has adjectival modifiers
the GR between the verb and Y is the same, so the
detection tolerates a large amount of variation. Any
such modifiers are found in modifying relations and
are available for use in the Analysis Component.
3.2 The Analysis Component
We confine attention here to X?is/are?Y and You?Y
cases. The analysis element of the processing takes
the X noun (if any) and Y noun and uses Word-
Net 2.0 to analyse them. First, we try to determine
whether X refers to a person (the only case the sys-
tem currently deals with), partly by using a specified
list of proper names of characters in the drama and
partly by WordNet processing. If so, then the Y and
remaining elements are analysed using WordNet?s
taxonomy. This allows us to see if the Y noun in one
of its senses is a hyponym of animals or supernatural
beings. If this is established, the system sees if an-
other of the senses of the word is a hyponym of the
person synset, as many metaphors are already given
as senses in WordNet. If different senses of the given
word are hyponyms of both animal and person, other
categories in the tree between the noun and the per-
son synset may provide information about the eval-
uative content of the metaphor. For example the
word ?cow? in its metaphorical usage has the ?un-
pleasant person? synset as a lower hypernym, which
heuristically suggests that, when the word is used in
a metaphor, it will be negative about the target.
There is a further complication. Baby animal
names can often be used to give a statement a more
affectionate quality. Some baby animal names such
as ?piglet? do not have a metaphorical sense in Word-
38
Net. In these cases, we check the word?s gloss to see
if it is a young animal and what kind of animal it
is. We then process the adult animal name to seek a
metaphorical meaning but add the quality of affec-
tion to the result. A higher degree of confidence is
attached to the quality of affection than is attached
to the positive/negative result, if any, obtained from
the adult name. Other baby animal names such as
?lamb? do have a metaphorical sense in WordNet in-
dependently of the adult animal, and are therefore
evaluated as in the previous paragraph. They are
also flagged as potentially expressing affection but
with a lesser degree of confidence than that gained
from the metaphorical processing of the word. How-
ever, the youth of an animal is not always encoded
in a single word: e.g., ?cub? may be accompanied
by specification of an animal type, as in ?wolf cub?.
An extension to our processing would be required to
handle this and also cases like ?young wolf? or ?baby
wolf?.
If any adjectival modifiers of the Y noun were rec-
ognized the analyser then goes on to evaluate their
contribution to the metaphor?s affect. If the analyser
finds that ?big? is one of the modifying adjectives
of the noun it has analysed the metaphor is marked
as being more emphatic. If ?little? is found the fol-
lowing is done. If the metaphor has been tagged as
negative and no degree of affection has been added
(from a baby animal name, currently) then ?little? is
taken to be expressing contempt. If the metaphor
has been tagged as positive OR a degree of affection
has been added then ?little? is taken to be expressing
affection.
4 Examples of Course of Processing
?You piglet?:
(1) Detector recognises the you Y signal with Y =
?piglet?.
(2) Analyser finds that ?piglet? is a hyponym of ?an-
imal?.
(3) ?Piglet? does not have ?person? as a WordNet hy-
pernym so analyser retrieves the WordNet gloss.
(4) It finds ?young? in the gloss (?a young pig?) and
retrieves all of the following words (just ?pig? ? the
analysis process is would otherwise be repeated for
each of the words captured from the gloss), and finds
that ?pig? by itself has negative metaphorical affect.
(5) The input is labelled as an animal metaphor
which is negative but affectionate, with the affection
having higher confidence than the negativity.
?Lisa is an angel?:
(1) Detector recognises the X is Y signal with Y =
?angel?, after checking that Lisa is a person.
(2) Analyser finds that ?angel? is a hyponym of ?su-
pernatural being?.
(3) It finds that in another sense ?angel? is a hyponym
of ?person?.
(4) It finds that the tree including the ?person? synset
also passes through ?good person,? expressing posi-
tive affect.
(5) Conclusion: positive supernatural-
being metaphor.
Results from Some Other Examples:
?You cow?, ?they?re such sheep?: negative
metaphor.
?You little rat?: contemptuous metaphor.
?You little piggy?: affectionate metaphor with a neg-
ative base.
?You?re a lamb?: affectionate metaphor.
?You are a monster?: negative metaphor.
?She is such a big fat cow?: negative metaphor, in-
tensified by ?big? (currently ?fat? is not dealt with).
5 Concluding Remarks
The demonstrated processing capabilities make par-
ticular but nevertheless valuable contributions to
metaphor processing and affect-detection for ICAs,
in e-drama at least. Further work is ongoing on the
four specific metaphorical phenomena in section 3
as well as on other phenomena, such as the vari-
ation of conventional metaphorical phraseology by
synonym substitution and addition of modifiers, and
metaphorical descriptions of emotions themselves.
As many extensions are ongoing or envisaged,
it is premature to engage in large-scale evaluation.
Also, there are basic problems facing evaluation.
The language in the e-drama genre is full of mis-
spellings, ?texting? abbreviations, acronyms, gram-
matical errors, etc., so that fully automated evalua-
tion of the metaphorical processing by itself is dif-
ficult; and application of the system to manually
cleaned-up utterances is still dependent on Rasp ex-
tracting structure appropriately. Also, our own ul-
timate concerns are theoretical, to do with the na-
ture of metaphor understanding. We are interested
in covering the qualitative range of possibilities and
complications, with no strong constraint from their
39
frequency in real discourse. Thus, statistical evalua-
tion on corpora is not particularly relevant except for
practical purposes.
However, some proto-evaluative comments that
can be made about animal metaphors are as fol-
lows. The transcripts mentioned in section 2 (33K
words total) contain metaphors with the following
animal words: rhino, bitch, dog, ape, cow, mole,
from 14 metaphorical utterances in all. Seven of
the utterances are recognized by our system, and
these involve rhino, dog, ape, mole. No WordNet-
based metaphorical connotation is found for the
rhino case. Negative affect is concluded for bitch,
dog and cow cases, and affect of undetermined po-
larity is concluded for ape and mole.
The system is currently designed only to do rela-
tively simple, specialized metaphorical processing.
The system currently only deals with a small mi-
nority of our own list of metaphoricity signals (see
section 3.1), and these signals are only present in a
minority of cases of metaphor overall. It does not
do either complex reasoning or analogical structure-
matching as in our own ATT-Meta metaphor sys-
tem (Barnden, 2006) or the cited approaches of Fass,
Hobbs, Martin, Narayanan and Veale. However, we
plan to eventually add simplified versions of ATT-
Meta-style reasoning, and in particular to add the
ATT-Meta view-neutral mapping adjunct feature to
implement the default carry-over of affect (see sec-
tion 2) and certain other information, as well as han-
dling more signals.
Other work on metaphor has exploited WordNet
(see, e.g., Veale, 2003, and panel on Figurative Lan-
guage in WordNets and other Lexical Resources
at GWC?04 (http://www.fi.muni.cz/gwc2004/.
Such work uses WordNet in distinctly different ways
from us and largely for different purposes. Our sys-
tem is also distinctive in, for instance, interpreting
the contribution of size adjectives.
Acknowledgments
The research has been aided by Sheila Glasbey and
Li Zhang, and supported by ESRC/EPSRC/DTI Pac-
cit LINK grant (ESRC RES-328-25-0009) and EP-
SRC grant EP/C538943/1.
References
John Barnden. 2006. Artificial Intelligence, Figurative
Language and Cognitive Linguistics. In G. Kristiansen
et al (Eds), Cognitive Linguistics: Current Applica-
tions and Future Perspectives, 431?459. Berlin: Mou-
ton de Gruyter.
Ted Briscoe, John Carroll and Rebecca Watson. 2006.
The Second Release of the RASP System. In Procs.
COLING/ACL 2006 Interactive Presentation Sessions.
Sydney, Australia.
Dan Fass. 1997. Processing Metaphor and Metonymy.
Greenwich, Connecticut: Ablex.
Susan Fussell & Mallie Moss. 1998. Figurative Lan-
guage in Emotional Communication. Social and Cog-
nitive Approaches to Interpersonal Communication.
Lawrence Erlbaum.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Jerry Hobbs. 1990. Literature and Cognition. CSLI Lec-
ture Notes, 21, Stanford University, 1990.
Zolta?n Ko?vecses. 2000. Metaphor and Emotion: Lan-
guage, Culture and Body in Human Feeling. Cam-
bridge University Press, Cambridge.
James Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press.
Zachary Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1), 23?44.
Andreas Musolff. 2004. Metaphor and political dis-
course: Analogical reasoning in debates about Eu-
rope. Palgrave Macmillan.
Srini Narayanan. 1999. Moving right along: A compu-
tational model of metaphoric reasoning about events.
Procs. National Conference on Art. Int., 121?128.
Serge Sharoff. 2006. How to Handle Lexical Semantics
in SFL: A Corpus Study of Purposes for Using Size
Adjectives. System and Corpus: Exploring Connec-
tions. Equinox, London.
Tony Veale. 1998. ?Just in Time? analogical mapping, an
iterative-deepening approach to structure-mapping. In
Procs. 13th European Conference on Art. Intell.
Tony Veale. 2003. Dynamic Type Creation in Metaphor
Interpretation and Analogical Reasoning: A Case-
Study with WordNet. In Procs. International Confer-
ence on Conceptual Structures (Dresden).
Li Zhang, John Barnden, Bob Hendley & Alan Walling-
ton. 2006. Exploitation in Affect Detection in Impro-
visational E-Drama. In Procs. 6th Int. Conference on
Intelligent Virtual Agents: Lecture Notes in Computer
Science, 4133, 68?79. Springer.
40
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 47?54,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploitation in Affect Detection in Open-Ended Improvisational Text   
 
 
Li Zhang, John A. Barnden, Robert J. Hendley and Alan M. Wallington 
School of Computer Science 
University of Birmingham 
  Birmingham B15 2TT, UK 
l.zhang@cs.bham.ac.uk 
 
  
 
Abstract 
We report progress on adding affect-
detection to a program for virtual dra-
matic improvisation, monitored by a hu-
man director. We have developed an af-
fect-detection module to control an 
automated virtual actor and to contribute 
to the automation of directorial functions. 
The work also involves basic research 
into how affect is conveyed through 
metaphor. The project contributes to the 
application of sentiment and subjectivity 
analysis to the creation of emotionally 
believable synthetic agents for interactive 
narrative environments. 
1 Introduction 
Improvised drama and role-play are widely used 
in education, counselling and conflict resolution. 
Researchers have explored frameworks for e-
drama, in which virtual characters (avatars) 
interact under the control of human actors. The 
springboard for our research is an existing 
system (edrama) created by one of our industrial 
partners, Hi8us Midlands, used in schools for 
creative writing and teaching in various subjects. 
The experience suggests that e-drama helps 
students lose their usual inhibitions, because of 
anonymity etc. In edrama, characters are 
completely human-controlled, their speeches 
textual in speech bubbles, and their visual forms 
cartoon figures. The actors (users) are given a 
loose scenario within which to improvise, but are 
at liberty to be creative. There is also a human 
director, who constantly monitors the unfolding 
drama and can intervene by, for example, 
sending messages to actors, or by introducing 
and controlling a minor ?bit-part? character to 
interact with the main characters. But this places 
a heavy burden on directors, especially if they 
are, for example, teachers and unpracticed in the 
directorial role. One research aim is thus partially 
to automate the directorial functions, which 
importantly involve affect detection. For 
instance, a director may intervene when 
emotions expressed or discussed by characters 
are not as expected. Hence we have developed an 
affect-detection module. It has not yet actually 
been used for direction, but instead to control an 
automated bit-part actor, EMMA (emotion, 
metaphor and affect). The module identifies 
affect in characters? speeches, and makes 
appropriate responses to help stimulate the 
improvisation. Within affect we include: basic 
and complex emotions such as anger and 
embarrassment; meta-emotions such as desiring 
to overcome anxiety; moods such as hostility; 
and value judgments (of goodness, etc.). 
Although merely detecting affect is limited 
compared to extracting full meaning, this is often 
enough for stimulating improvisation. 
Much research has been done on creating af-
fective virtual characters in interactive systems. 
Indeed, Picard?s work (2000) makes great con-
tributions to building affective virtual characters. 
Also, emotion theories, particularly that of Or-
tony, et al (1988) (OCC), have been used widely 
therein. Egges et al (2003) have provided virtual 
characters with conversational emotional respon-
siveness. However, few systems are aimed at 
detecting affect as broadly as we do and in open-
ended utterances. Although Fa?ade (Mateas, 
2002) included processing of open-ended utter-
ances, the broad detection of emotions, rudeness 
and value judgements is not covered. Zhe & 
Boucouvalas (2002) demonstrated emotion ex-
traction using a tagger and a chunker to help de-
tect the speaker?s own emotions. But it focuses 
only on emotional adjectives, considers only 
47
first-person emotions and neglects deep issues 
such as figurative expression. Our work is dis-
tinctive in several respects. Our interest is not 
just in (a) the positive first-person case: the af-
fective states that a virtual character X implies 
that it has (or had or will have, etc.), but also in 
(b) affect that X implies it lacks, (c) affect that X 
implies that other characters have or lack, and (d) 
questions, commands, injunctions, etc. concern-
ing affect. We aim also for the software to cope 
partially with the important case of metaphorical 
conveyance of affect (Fussell & Moss, 1998; 
K?vecses, 1998). 
Our project does not involve using or develop-
ing deep, scientific models of how emotional 
states, etc., function in cognition. Instead, the 
deep questions investigated are on linguistic mat-
ters such as the metaphorical expression of af-
fect. Also, in studying how people understand 
and talk about affect, what is of prime impor-
tance is their common-sense views of how affect 
works, irrespective of scientific reality. Metaphor 
is strongly involved in such views. 
2 Our Current Affect Detection 
Various characterizations of emotion are used in 
emotion theories. The OCC model uses emotion 
labels (anger, etc.) and intensity, while Watson 
and Tellegen (1985) use positivity and negativity 
of affect as the major dimensions. Currently, we 
use an evaluation dimension (negative-positive), 
affect labels, and intensity. Affect labels plus 
intensity are used when strong text clues signal-
ling affect are detected, while the evaluation di-
mension plus intensity is used for weak text 
clues. Moreover, our analysis reported here is 
based on the transcripts of previous e-drama ses-
sions. Since even a person?s interpretations of 
affect can be very unreliable, our approach com-
bines various weak relevant affect indicators into 
a stronger and more reliable source of informa-
tion for affect detection. Now we summarize our 
affect detection based on multiple streams of in-
formation. 
2.1 Pre-processing Modules 
The language in the speeches created in e-drama 
sessions severely challenges existing language-
analysis tools if accurate semantic information is 
sought even for the purposes of restricted affect-
detection. The language includes misspellings, 
ungrammaticality, abbreviations (often as in text 
messaging), slang, use of upper case and special 
punctuation (such as repeated exclamation 
marks) for affective emphasis, repetition of 
letters or words also for affective emphasis, and 
open-ended interjective and onomatopoeic 
elements such as ?hm? and ?grrrr?. In the 
examples we have studied, which so far involve 
teenage children improvising around topics such 
as school bullying, the genre is similar to Internet 
chat.  
To deal with the misspellings, abbreviations, 
letter repetitions, interjections and onomatopoeia, 
several types of pre-processing occur before ac-
tual detection of affect. 
A lookup table has been used to deal with ab-
breviations e.g. ?im (I am)?, ?c u (see you)? and 
?l8r (later)?. It includes abbreviations used in 
Internet chat rooms and others found in an analy-
sis of previous edrama sessions. We handle am-
biguity (e.g.,?2? (to, too, two) in ?I?m 2 hungry 2 
walk?) by considering the POS tags of immedi-
ately surrounding words. Such simple processing 
inevitably leads to errors, but in evaluations us-
ing examples in a corpus of 21695 words derived 
from previous transcripts we have obtained 
85.7% accuracy, which is currently adequate. We 
are also considering dealing with abbreviations, 
etc. in a more general way by including them as 
special lexical items in the lexicon of the robust 
parser we are using (see below). 
The iconic use of word length (corresponding 
roughly to imagined sound length) as found both 
in ordinary words with repeated letters (e.g. 
?seeeee?) and in onomatopoeia and interjections, 
(e.g. ?wheee?, ?grr?, ?grrrrrr?, ?agh?, ?aaaggghhh?) 
normally implies strong affective states. We have 
a small dictionary containing base forms of some 
special words (e.g. ?grr?) and some ordinary 
words that often have letters repeated in e-drama. 
Then the Metaphone spelling-correction algo-
rithm (http://aspell.net/metaphone/), which is 
based on pronunciation, works with the diction-
ary to locate the base forms of words with letter 
repetitions.  
Finally, the Levenshtein distance algorithm 
(http://www.merriampark.com/ld.htm) with a 
contemporary English dictionary deals with 
spelling mistakes in users? input.  
2.2 Processing of Imperative Moods 
One useful pointer to affect is the use of impera-
tive mood, especially when used without soften-
ers such as ?please? or ?would you?. Strong emo-
tions and/or rude attitudes are often expressed in 
this case. There are special, common imperative 
phrases we deal with explicitly, such as ?shut 
up? and ?mind your own business?. They usually 
48
indicate strong negative emotions. But the phe-
nomenon is more general. 
Detecting imperatives accurately in general is 
by itself an example of the non-trivial problems 
we face. We have used the syntactic output from 
the Rasp parser (Briscoe & Carroll, 2002) and 
semantic information in the form of the semantic 
profiles for the 1,000 most frequently used Eng-
lish words (Heise, 1965) to deal with certain 
types of imperatives. 
Rasp recognises some types of imperatives di-
rectly. Unfortunately, the grammar of the 2002 
version of the Rasp parser that we have used 
does not deal properly with certain imperatives 
(John Carroll, p.c), which means that examples 
like ?you shut up?, ?Dave bring me the menu?, 
?Matt don?t be so blunt? and ?please leave me 
alone?, are not recognized as imperatives, but as 
normal declarative sentences. Therefore, further 
analysis is needed to detect imperatives, by addi-
tional processing applied to the possibly-
incorrect syntactic trees produced by Rasp.  
If Rasp outputs a subject, ?you?, followed by 
certain verbs (e.g. ?shut?, ?calm?, etc) or certain 
verb phrases (e.g. ?get lost?, ?go away? etc), the 
sentence type will be changed to imperative. 
(Note: in ?you get out? the ?you? could be a 
vocative rather than the subject of ?get?, espe-
cially as punctuation such as commas is often 
omitted in our genre; however these cases are not 
worth distinguishing and we assume that the 
?you? is a subject.) If a softener ?please? is fol-
lowed by the base forms of a verb, then the input 
is taken to be imperative. If a singular proper 
noun is followed by a base form of the verb, then 
this sentence is taken to be an imperative as well 
(e.g. ?Dave get lost?). However, when a subject 
is followed by a verb for which there is no dif-
ference at all between the base form and the past 
tense form, then ambiguity arises between im-
perative and declarative (e.g. ?Lisa hit me?).  
There is an important special case of this am-
biguity. If the object of the verb is ?me?, then in 
order to solve the ambiguity, we have adopted 
the evaluation value of the verb from Heise?s 
(1965) compilation of semantic differential pro-
files. In these profiles, Heise listed values of 
evaluation, activation, potency, distance from 
neutrality, etc. for the 1,000 most frequently used 
English words. In the evaluation dimension, 
positive values imply goodness. Because nor-
mally people tend to use ?a negative verb + me? 
to complain about an unfair fact to the others, if 
the evaluation value is negative for such a verb, 
then this sentence is probably not imperative but 
declarative (e.g. ?Mayid hurt me?). Otherwise, 
other factors implying imperative are checked in 
this sentence, such as exclamation marks and 
capitalizations. If these factors occur, then the 
input is probably an imperative. Otherwise, the 
conversation logs are checked to see if there is 
any question sentence directed toward this 
speaker recently. If there is, then the input is con-
jectured to be declarative.  
There is another type of sentence: ?don?t you + 
base form of verb? that we have started to address. 
Though such a sentence is often interrogative, it is 
also often a negative version of an imperative with 
a ?you? subject (e.g. ?Don?t you dare call me a 
dog,? ?Don?t you call me a dog?). Normally Rasp 
regards it as a question sentence. Thus, further 
analysis has also been implemented for such a sen-
tence structure to change its sentence type to im-
perative. Although currently this has limited effect, 
as we only infer a (negative) affective quality 
when the verb is ?dare?, we plan to add semantic 
processing in an attempt to glean affect more gen-
erally from ?Don?t you ?? imperatives. 
2.3 Affect Detection by Pattern Matching 
In an initial stage of our work, affect detection 
was based purely on textual pattern-matching 
rules that looked for simple grammatical patterns 
or templates partially involving lists of specific 
alternative words. This continues to be a core 
aspect of our system but we have now added ro-
bust parsing and some semantic analysis. Jess, a 
rule-based Java framework, is used to implement 
the pattern/template-matching rules in EMMA. 
In the textual pattern-matching, particular 
keywords, phrases and fragmented sentences are 
found, but also certain partial sentence structures 
are extracted. This procedure possesses the ro-
bustness and flexibility to accept many ungram-
matical fragmented sentences and to deal with 
the varied positions of sought-after phraseology 
in speeches. However, it lacks other types of 
generality and can be fooled when the phrases 
are suitably embedded as subcomponents of 
other grammatical structures. For example, if the 
input is ?I doubt she?s really angry?, rules look-
ing for anger in a simple way will fail to provide 
the expected results.  
The transcripts analysed to inspire our initial 
knowledge base and pattern-matching rules were 
derived independently from previous edrama 
improvisations based on a school bullying sce-
nario. We have also worked on another, dis-
tinctly different scenario, Crohn?s disease, based 
on a TV programme by another of our industrial 
49
partners (Maverick TV). The rule sets created for 
one scenario have a useful degree of applicability 
to other scenarios, though there will be a few 
changes in the related knowledge database ac-
cording to the nature of specific scenarios.  
The rules, as we mentioned at the beginning of 
this section, conjecture the character?s emotions, 
evaluation dimension (negative or positive), po-
liteness (rude or polite) and what response 
EMMA should make.  
Multiple exclamation marks and capitalisation 
are frequently employed to express emphasis in 
e-drama sessions. If exclamation marks or capi-
talisation are detected in a character?s utterance, 
then the emotion intensity is deemed to be com-
paratively high (and emotion is suggested even 
in the absence of other indicators).  
A reasonably good indicator that an inner state 
is being described is the use of ?I? (see also 
Craggs & Wood (2004)), especially in combina-
tion with the present or future tense. In the 
school-bullying scenario, when ?I? is followed by 
a future-tense verb the affective state ?threaten-
ing? is normally being expressed; and the utter-
ance is usually the shortened version of an im-
plied conditional, e.g., ?I?ll scream [if you stay 
here].? Note that when ?I? is followed by a pre-
sent-tense verb, a variety of other emotional 
states tend to be expressed, e.g. ?I want my 
mum? (fear) and ?I hate you? (dislike), I like you 
(liking). Further analysis of first-person, present-
tense cases is provided in the following section.  
2.4 Going Beyond Pattern Matching 
In order to go beyond the limitations of simple 
pattern matching, sentence type information ob-
tained from the Rasp parser has also been 
adopted in the pattern-matching rules. The gen-
eral sentence structure information not only helps 
EMMA to detect affective states in the user?s 
input (see the above discussion of imperatives), 
and to decide if the detected affective states 
should be counted, but also helps EMMA to 
make appropriate responses. Rasp will inform 
the pattern-matching rule with sentence type in-
formation. If the current input is a conditional or 
question sentence with affective keywords or 
structures in, then the affective states won?t be 
valued. For example, if the input is ?I like the 
place when it is quiet?, Rasp works out its sen-
tence type: a conditional sentence and the rule 
for structures containing ?like? with a normal 
declarative sentence label won?t be activated. 
Instead, the rule for the keyword ?when? with a 
conditional sentence type label will be fired. Thus 
an appropriate response will be obtained. 
Additionally, as we discussed in section 2.2, we 
use Rasp to indicate imperative sentences, such as 
when Mayid (the bully) said ?Lisa, don?t tell Miss 
about it?. The pseudo-code example rule for such 
input is as follows:  
(defrule example_rule 
?fact <- (any string containing negation and the 
sentence type is ?imperative?)  => 
(obtain affect and response from knowledge da-
tabase) 
Thus the declarative input such as ?I won?t tell 
Miss about it? won?t be able to activate the exam-
ple rule due to different sentence type information. 
Especially, we have assigned a special sentence 
type label (?imp+please?) for imperatives with sof-
tener ?please?. Only using this special sentence 
type label itself in the pattern-matching rule helps 
us effortlessly to obtain the user?s linguistic style 
(?polite?) and probably a polite response from 
EMMA as well according to different roles in spe-
cific scenarios.  
Aside from using the Rasp parser, we have also 
worked on implementing simple types of semantic 
extraction of affect using affect dictionaries and 
electronic thesauri, such as WordNet. The way we 
are currently using WordNet is briefly as follows. 
2.5 Using WordNet for a First Person Case 
As we mentioned earlier, use of the first-person 
with a present-tense verb tends to express an affec-
tive state in the speaker, especially in discourse in 
which affect is salient, as is the case in scenarios 
such as School Bullying and Crohn?s Disease. We 
have used the Rasp parser to detect such a sen-
tence. First of all, such user?s input is sent to the 
pattern-matching rules in order to obtain the 
speaker?s current affective state and EMMA?s re-
sponse to the user. If there is no rule fired (i.e. we 
don?t obtain any information of the speaker?s af-
fective state and EMMA?s response from the pat-
tern-matching rules), further processing is applied. 
We use WordNet to track down the rough syno-
nyms of the verb (possibly from different Word-
Net ?synsets?) in the verb phrase of the input sen-
tence, in order to allow a higher degree of general-
ity than would be achieved just with the use of our 
pattern-matching rules. In order to find the closest 
synonyms to the verb in different synsets, the se-
mantic profiles of the 1,000 most frequently used 
English words (Heise, 1965) have been employed, 
especially to find the evaluation values of every 
synonym of the original verb. We transform posi-
tive and negative evaluation values in Heise?s dic-
50
tionary into binary ?positive? and ?negative? only. 
Thus if any synonym has the same evaluation 
value (?positive? or ?negative?) as that of the origi-
nal verb, then it will be selected as a member of 
the set of closest synonyms. Then, we use one 
closest synonym to replace the original verb in the 
user?s input. This newly built sentence will be sent 
to the pattern-matching rules in order to obtain the 
user?s affective state and EMMA?s response. Such 
processing (using a closest synonym to replace the 
original verb and sending the newly built sentence 
to the pattern-matching rules) continues until we 
obtain the speaker?s affective state and appropriate 
response.  
2.6 Responding Regimes 
EMMA normally responds to, on average, every 
Nth speech by another character in the e-drama 
session, where N is a changeable parameter (cur-
rently set to 3). However, it also responds when 
EMMA?s character?s name is mentioned, and 
makes no response if it cannot detect anything 
useful in the utterance it is responding to. The 
one-in-N average is achieved by sampling a ran-
dom variable every time another character says 
something. As a future development, we plan to 
have N dynamically adjustable according to how 
confident EMMA is about what it has discerned 
in the utterance at hand.  
EMMA sometimes makes a random response 
from several stored response candidates that are 
suitable for the affective quality it has discerned 
in an utterance it is responding to. In addition, 
EMMA sometimes reflects back, in modified 
form, part of the user?s input string as a sub-
component of its response. Notice here that, be-
cause the pre-processing module reported in sec-
tion 2.1 expands abbreviations and corrects mis-
spellings, it helps to obscure the fact that part of 
EMMA?s response is only a reflection. E.g.: 
<Character> I?m ur (your) best m8 (mate). 
<EMMA> Why are you my best mate? 
On the other hand, we also, on a random basis, 
purposely put phenomena such as abbreviations 
and Internet-chat features in EMMA?s responses 
in order to make EMMA express itself more like 
a typical school-student user. E.g.: 
<Character> You get lost!  
<EMMA> Hey, coool it folks. (notice the 
lengthening of ?cool?) 
<Character> He didn?t do it.  
<EMMA> No1 believes you, m8. (where 
?no1? stands for ?no one?) 
It should be borne in mind that EMMA?s re-
sponses are not aimed at engaging with the de-
tailed meaning of the utterance, but simply to 
stimulate the improvisation in a way that is 
somewhat sensitive to affect being expressed. 
Furthermore, in order to make the EMMA char-
acter?s responses push the improvisation for-
ward, the character will not only ask scenario 
related questions to the main characters, but also 
introduce new topics closely related to the sce-
nario in the improvisation. In a recent user-
testing debrief session, secondary school students 
mentioned that the human bit-part character did 
not stay in character and said pointless things, 
while in another session one student, who played 
a main character, believed that the EMMA char-
acter was the only one that stuck to scenario re-
lated topics. The directors reported that, even 
when a main character was silent and the director 
did not intervene very much, the EMMA charac-
ter led the improvisation on the right track by 
raising new topics other characters were con-
cerned about.      
3 Affect via Metaphor 
In the introduction we commented on two func-
tions of metaphor. Metaphor is often used to 
convey affect and it also partly underlies folk 
theories of how affect and emotion work. As an 
example of the latter, folk theories of anger often 
talk about, and appear to conceive of, anger as if 
it were a heated fluid possibly exerting a strong 
pressure on its containing body. This motivates a 
wide range of metaphorical expressions both 
conventional such as ?he was boiling with anger 
and about to blow his top? and more creative 
variants such as ?the temperature in the office 
was getting higher and this had nothing to do 
with where the thermostat was set? (modified, 
slightly from a Google? search). Passion, or 
lack of, is also often described in terms of heat 
and the latter example could in certain contexts 
be used in this manner. So far, examples of ac-
tors reflecting or commenting on the nature of 
their or others emotions, which would require an 
appropriate vocabulary, have been infrequent in 
the e-drama transcripts, although we might ex-
pect to find more examples as more students par-
ticipate in the Crohn?s disease scenario. 
However, such metaphorically motivated folk 
models often directly motivate the terminology 
used to convey affect, as in utterances such as 
?you leave me cold?, which conveys lack of in-
terest or disdain. This use of metaphor to moti-
vate folk models of emotions and, as a conse-
quence, certain forms of direct expression of 
51
emotion has been extensively studied, albeit usu-
ally from a theoretical, linguistic, perspective 
(Fussell & Moss, 1998; K?vecses, 1998).  
Less recognised (although see Barnden et al, 
2004; Wallington et al, 2006) is the fact that 
metaphor is also frequently used to convey emo-
tion more indirectly. Here the metaphor does not 
describe some aspect of an emotional state, but 
something else. Crucially, however, it also con-
veys a negative or positive value judgement 
which is carried over to what is being described 
and this attitude hints at the emotion. For exam-
ple to say of someone?s room that ?it is a cess-
pit? allows the negative evaluation of ?cess-pit? 
to be transferred to ?the room? and we might as-
sume an emotion of disgust. In our transcripts we 
find examples such as ?smelly attitude? and ?you 
buy your clothes at the rag market? (which we 
take to be not literally true). Animal insults such 
as ?you pig? frequently take this form, although 
many are now highly conventionalised. Our 
analysis of e-drama transcripts shows that this 
type of metaphor that conveys affect indirectly is 
much more common than the direct use.  
It should be apparent that even though conven-
tional metaphorical phraseology may well be 
listed in specialised lexicons, approaches to 
metaphor and affect which rely upon a form of 
lexical look-up to determine the meaning of ut-
terances are likely to miss both the creative vari-
ants and extensions of standard metaphors and 
also the quite general carrying over of affectual 
evaluations from the literal meaning of an utter-
ance to the intended metaphorical meaning.  
At the time of writing (early June 2006) little 
in the way of metaphor handling has been incor-
porated into the EMMA affect-detection module. 
However, certain aspects of metaphor handling 
will be incorporated shortly, since they involve 
extensions of existing capabilities. Our intended 
approach is partly to look for stock metaphorical 
phraseology and straightforward variants of it, 
which is the most common form of metaphor in 
most forms of discourse, including e-drama. 
However, we also plan to employ a simple ver-
sion of the more open-ended, reasoning-based 
techniques described in the ATT-Meta project on 
metaphor processing (Barnden et al, 2004; Wal-
lington et al, 2006). 
As a first step, it should be noted that insults 
and swear words are often metaphorical. We are 
currently investigating specialised insult diction-
aries and the machine-readable version of the 
OALD, which indicates slang. 
Calling someone an animal of any sort usually 
conveys affect, but it can be either insulting or 
affectionate. We have noted that calling someone 
the young of an animal is often affectionate, and 
the same is true of diminutive (e.g., ?piglet?) and 
nursery forms (e.g., ?moo cow?), even when the 
adult form of the animal is usually used as an 
insult. Thus calling someone ?a cat? or ?catty? is 
different from describing them as kittenish. 
Likewise, ?you young pup? is different from 
?you dog?. We are constructing a dictionary of 
specific animals used in slang and as insults, but, 
more generally, for animals not listed we can use 
WordNet and electronic dictionaries to determine 
whether or not it is the young or mature form of 
the animal that is being used.  
We have already noted that in metaphor the 
affect associated with a source term will carry 
across to the target by default. EMMA already 
consults Heise?s compilation of semantic differ-
ential profiles for the evaluation value of the 
verb. We will extend the determination of the 
evaluation value to all parts of speech.  
Having the means to determine the emotion 
conveyed by a metaphor is most useful when 
metaphor can be reliably spotted. There are a 
number of means of doing this for some meta-
phors. For example, idioms are often metaphori-
cal (Moon 1988). Thus we can use an existing 
idiom dictionary, adding to it as necessary. This 
will work with fixed idioms, but, as is often 
noted, idioms frequently show some degree of 
variation, either by using synonyms of standard 
lexis, e.g., ?constructing castles in the air? in-
stead of ?building castles in the air?, or by adding 
modifiers, e.g., ?shut your big fat mouth?. This 
variability will pose a challenge if one is looking 
for fixed expressions from an idiom dictionary. 
However, if the idiom dictionary is treated as 
providing base forms, with for example the 
nouns being treated as the head nouns of a noun-
phrase, then the Rasp parser can be used to de-
termine the noun phrase and the modifiers of the 
head noun, and likewise with verbs, verb-
phrases, etc. Indeed, this approach can be ex-
tended beyond highly fixed expressions to other 
cases of metaphor, since as Deignan (2005) has 
noted metaphors tend to display a much greater 
degree of fixedness compared to non-metaphors, 
whilst not being as fixed as what are convention-
ally called idioms. 
There are other ways of detecting metaphors 
which we could utilise. Thus, metaphoricity sig-
nals (as in Goatly, 1997; Wallington et al, 2003) 
signal the use of a metaphor in some cases. Such 
52
signals include phrases such as: so to speak, sort 
of, almost, picture as. Furthermore, semantic 
restriction violations (Wilks, 1978; Fass, 1997; 
Mason, 2004), as in ?my car drinks petrol,? of-
ten indicate metaphor, although not all meta-
phors violate semantic restrictions. To determine 
whether semantic restrictions are being violated, 
domain information from ontologies/thesauri 
such as WordNet could be used and/or statistical 
techniques as used by Mason (2004). 
4 User Testing 
We conducted a two-day pilot user test with 39 
secondary school students in May 2005, in order 
to try out and a refine a testing methodology. The 
aim of the testing was primarily to measure the 
extent to which having EMMA as opposed to a 
person play a character affects users? level of 
enjoyment, sense of engagement, etc. We con-
cealed the fact that EMMA was involved in some 
sessions in order to have a fair test of the differ-
ence that is made. We obtained surprisingly good 
results. Having a minor bit-part character called 
?Dave? played by EMMA as opposed to a person 
made no statistically significant difference to 
measures of user engagement and enjoyment, or 
indeed to user perceptions of the worth of the 
contributions made by the character ?Dave?. Us-
ers did comment in debriefing sessions on some 
utterances of Dave?s, so it was not that there was 
a lack of effect simply because users did not no-
tice Dave at all. Also, the frequencies of human 
?Dave? and EMMA ?Dave? being responded to 
during the improvisation (sentences of Dave?s 
causing a response divided by all sentences said 
by ?Dave?) are both roughly around 30%, again 
suggesting that users notice Dave. Additionally, 
the frequencies of other side-characters being 
responded to are roughly the same as the ?Dave? 
character ? ?Matthew?: around 30% and ?Elise?: 
around 35%.  
Furthermore, it surprised us that no user ap-
peared to realize that sometimes Dave was com-
puter-controlled. We stress, however, that it is 
not an aim of our work to ensure that human ac-
tors do not realize this. More extensive, user test-
ing at several Birmingham secondary schools is 
being conducted at the time of writing this paper, 
now that we have tried out and somewhat modi-
fied the methodology.  
The experimental methodology used in the 
testing is as follows, in outline. Subjects are 14-
16 year old students at local Birmingham 
schools. Forty students are chosen by each 
school for the testing. Four two-hour sessions 
take place at the school, each session involving a 
different set of ten students. In a session, the 
main phases are as follows: an introduction to the 
software; a First Improvisation Phase, where five 
students are involved in a School Bullying im-
provisation and the remaining five in a Crohn?s 
Disease improvisation; a Second Improvisation 
Phase in which this assignment is reversed; fill-
ing out of a questionnaire by the students; and 
finally a group discussion acting as a debrief 
phase. For each improvisation, characters are 
pre-assigned to specific students. Each Improvi-
sation Phase involves some preliminaries fol-
lowed by ten minutes of improvisation proper.  
In half of the SB improvisations and half of 
the CD improvisations, the minor character Dave 
is played by one of the students, and by EMMA 
in the remaining. When EMMA plays Dave, the 
student who would otherwise have played him is 
instructed to sit at another student?s terminal and 
thereby to be an audience member. Students are 
told that we are interested in the experiences of 
audience members as well as of actors. Almost 
without exception students have appeared not to 
have suspected that having an audience member 
results from not having Dave played by another 
student. At the end of one exceptional session 
some students asked whether one of the directors 
from Hi8us was playing Dave.  
Of the two improvisations a given student is 
involved in, exactly one involves EMMA play-
ing Dave. This will be the first session or the sec-
ond. This EMMA-involvement order and the 
order in which the student encounters SB and CD 
are independently counterbalanced across stu-
dents.  
The questionnaire is largely composed of 
questions that are explicitly about students? feel-
ings about the experience (notably enjoyment, 
nervousness, and opinions about the worth of the 
dramatic contributions of the various characters), 
with essentially the same set of questions being 
asked separately about the SB and the CD im-
provisations. The other data collected are: for 
each debrief phase, written minutes and an audio 
and video record; notes taken by two observers 
present during each Improvisation Phase; and 
automatically stored transcripts of the sessions 
themselves, allowing analysis of linguistic forms 
used and types of interactivity. To date only the 
non-narrative questionnaire answers have been 
subjected to statistical analysis, with the sole in-
dependent variable being the involvement or oth-
erwise of EMMA in improvisations. 
53
5 Conclusion and Ongoing Work 
We have implemented a limited degree of affect-
detection in an automated bit-part character in an 
e-drama application, and fielded the actor suc-
cessfully in pilot user-testing. Although there is a 
considerable distance to go in terms of the prac-
tical affect-detection that we plan to implement, 
the already implemented detection is able to 
cause reasonably appropriate contributions by 
the automated character. We also intend to use 
the affect-detection in a module for automatically 
generating director messages to human actors. 
In general, our work contributes to the issue of 
how affect/sentiment detection from language 
can contribute to the development of believable 
responsive AI characters, and thus to a user?s 
feeling of involvement in game playing. More-
over, the development of affect detection and 
sentiment & subjectivity analysis provides a 
good test-bed for the accompanying deeper re-
search into how affect is conveyed linguistically.  
Acknowledgement 
The project is supported by grant RES-328-25-
0009 under the ESRC/EPSRC/DTI ?PACCIT? 
programme, and its metaphor aspects also by 
EPSRC grant EP/C538943/1. We thank our part-
ners?Hi8us, Maverick TV and BT?and col-
leagues W.H. Edmondson, S.R. Glasbey, M.G. 
Lee and Z. Wen.  
References 
Barnden, J.A., Glasbey, S.R., Lee, M.G. & Walling-
ton, A.M. 2004. Varieties and Directions of Inter-
domain Influence in Metaphor. Metaphor and 
Symbol, 19(1), pp.1-30. 
Briscoe, E. & J. Carroll. 2002. Robust Accurate Sta-
tistical Annotation of General Text. In Proceedings 
of the 3rd International Conference on Language 
Resources and Evaluation, Las Palmas, Gran Ca-
naria. pp.1499-1504.  
Craggs, R. & Wood. M. 2004. A Two Dimensional 
Annotation Scheme for Emotion in Dialogue. In 
Proceedings of AAAI Spring Symposium: Explor-
ing Attitude and Affect in Text. 
Deignan , A. 2005. Metaphor and corpus Linguistics. 
John Benjamins. 
Egges, A., Kshirsagar, S. & Magnenat-Thalmann, N. 
2003. A Model for Personality and Emotion Simu-
lation, In Proceedings of Knowledge-Based Intelli-
gent Information & Engineering Systems 
(KES2003), Lecture Notes in AI. Springer-Verlag. 
Fussell, S. & Moss, M. 1998. Figurative Language in 
Descriptions of Emotional States. In S. R. Fussell 
and R. J. Kreuz (Eds.), Social and cognitive ap-
proaches to interpersonal communication. Law-
rence Erlbaum.  
Fass, D. 1997. Processing metaphor and metonymy. 
Greenwich, Connecticut: Ablex 
Goatly, A. 1997. The language of metaphors. 
Routledge London and New York:  
Heise, D. R. 1965. Semantic Differential Profiles for 
1,000 Most Frequent English Words. Psychologi-
cal Monographs 79, pp.1-31. 
K?vecses, Z. 1998. Are There Any Emotion-Specific 
Metaphors? In Speaking of Emotions: Conceptuali-
zation and Expression. Athanasiadou, A. and Ta-
bakowska, E. (eds.), Berlin and New York: Mou-
ton de Gruyter, pp.127-151. 
Mason, Z.J. 2004. CorMet: a computational, corpus-
based conventional metaphor extraction system. 
Computational Linguistics 30:1. pp. 23-44. 
Mateas, M. 2002. Ph.D. Thesis. Interactive Drama, 
Art and Artificial Intelligence. School of Computer 
Science, Carnegie Mellon University. 
Moon, R. 1998. Fixed idioms and expressions in Eng-
lish. Clarendon Press: Oxford, U.K 
Ortony, A., Clore, G.L. & Collins, A. 1988. The Cog-
nitive Structure of Emotions. CUP 
Picard, R.W. 2000. Affective Computing. The MIT 
Press. Cambridge MA. 
Sharoff, S. 2005. How to Handle Lexical Semantics in 
SFL: a Corpus Study of Purposes for Using Size 
Adjectives. Systemic Linguistics and Corpus. Lon-
don: Continuum. 
Watson, D. & Tellegen, A. 1985. Toward a Consen-
sual Structure of Mood. Psychological Bulletin, 98, 
pp.219-235.  
Zhe, X. & Boucouvalas, A. C. 2002. Text-to-Emotion 
Engine for Real Time Internet Communication. In 
Proceedings of International Symposium on Com-
munication Systems, Networks and DSPs, Stafford-
shire University, UK, pp.164-168. 
Wallington, A.M., Barnden, J.A., Barnden, M.A., 
Ferguson, F.J. & Glasbey, S.R. 2003. Metaphoric-
ity Signals: A Corpus-Based Investigation. Techni-
cal Report CSRP-03-5, School of Computer Sci-
ence, The University of Birmingham, U.K. 
Wallington, A.M., Barnden, J.A. Glasbey S.R. and 
Lee M. G. 2006. Metaphorical reasoning with an 
economical set of mappings. Delta, 22:1 
Wilks, Y. (1978). Making preferences more active. 
Artificial Intelligence, 10, pp. 75- 97 
54
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 509?516,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Empirical Study on the Performance Stability of Named Entity
Recognition Model across Domains
Hong Lei Guo Li Zhang and Zhong Su
IBM China Research Laboratory
Building 19, Zhongguancun Software Park
8 Dongbeiwang WestRoad, Haidian District, Beijing, 100094, P.R.C.
{guohl, lizhang , suzhong }@cn.ibm.com
Abstract
When a machine learning-based named
entity recognition system is employed in
a new domain, its performance usually de-
grades. In this paper, we provide an em-
pirical study on the impact of training data
size and domain information on the per-
formance stability of named entity recog-
nition models. We present an informative
sample selection method for building high
quality and stable named entity recogni-
tion models across domains. Experimen-
tal results show that the performance of
the named entity recognition model is en-
hanced significantly after being trained
with these informative samples.
1 Introduction
Named entities (NE) are phrases that contain
names of persons, organizations, locations, etc.
Named entity recognition (NER) is an important
task in many natural language processing appli-
cations, such as information extraction and ma-
chine translation. There have been a number of
conferences aimed at evaluating NER systems,
for example, MUC6, MUC7, CoNLL2002 and
CoNLL2003, and ACE (automatic content extrac-
tion) evaluations.
Machine learning approaches are becoming
more attractive for NER in recent years since they
are trainable and adaptable. Recent research on
English NER has focused on the machine learning
approach (Sang and Meulder, 2003). The relevant
algorithms include Maximum Entropy (Borth-
wick, 1999; Klein et al, 2003), Hidden Markov
Model (HMM) (Bikel et al, 1999; Klein et al,
2003), AdaBoost (Carreras et al, 2003), Memory-
based learning (Meulder and Daelemans, 2003),
Support Vector Machine (Isozaki and Kazawa,
2002), Robust Risk Minimization (RRM) Classi-
fication method (Florian et al, 2003), etc.
For Chinese NER, most of the existing ap-
proaches use hand-crafted rules with word (or
character) frequency statistics. Some machine
learning algorithms also have been investigated in
Chinese NER, including HMM (Yu et al, 1998;
Jing et al, 2003), class-based language model
(Gao et al, 2005; Wu et al, 2005), RRM (Guo
et al, 2005; Jing et al, 2003), etc.
However, when a machine learning-based NER
system is directly employed in a new domain, its
performance usually degrades. In order to avoid
the performance degrading, the NER model is of-
ten retrained with domain-specific annotated cor-
pus. This retraining process usually needs more
efforts and costs. In order to enhance the perfor-
mance stability of NER models with less efforts,
some issues have to be considered in practice. For
example, how much training data is enough for
building a stable and applicable NER model? How
does the domain information and training data size
impact the NER performance?
This paper provides an empirical study on the
impact of training data size and domain informa-
tion on NER performance. Some useful observa-
tions are obtained from the experimental results
on a large-scale annotated corpus. Experimental
results show that it is difficult to significantly en-
hance the performance when the training data size
is above a certain threshold. The threshold of the
training data size varies with domains. The perfor-
mance stability of each NE type recognition also
varies with domains. Corpus statistical data show
that NE types have different distribution across do-
mains. Based on the empirical investigations, we
present an informative sample selection method
509
for building high quality and stable NER models.
Experimental results show that the performance of
the NER model is enhanced significantly across
domains after being trained with these informative
samples. In spite of our focus on Chinese, we be-
lieve that some of our observations can be poten-
tially useful to other languages including English.
This paper is organized as follows. Section 2
describes a Chinese NER system using multi-level
linguistic features. Section 3 discusses the impact
of domain information and training data size on
the NER performance. Section 4 presents an in-
formative sample selection method to enhance the
performance of the NER model across domains.
Finally the conclusion is given in Section 5.
2 Chinese NER Based on Multilevel
Linguistic Features
In this paper, we focus on recognizing four types
of NEs: Persons (PER), Locations (LOC), Orga-
nizations (ORG) and miscellaneous named enti-
ties (MISC) which do not belong to the previous
three groups (e.g. products, conferences, events,
brands, etc.). All the NER models in the follow-
ing experiments are trained with a Chinese NER
system. In this section, we simply describe this
Chinese NER system. The Robust Risk Minimiza-
tion (RRM) Classification method and multi-level
linguistic features are used in this system (Guo et
al., 2005).
2.1 Robust Risk Minimization Classifier
We can view the NER task as a sequential classi-
fication problem. If toki (i = 0, 1, ..., n) denotes
the sequence of tokenized text which is the input
to the system, then every token toki should be as-
signed a class-label ti.
The class label value ti associated with each to-
ken toki is predicted by estimating the conditional
probability P (ti = c|xi) for every possible class-
label value c, where xi is a feature vector associ-
ated with token toki.
We assume that P (ti = c|xi) = P (ti =
c|toki, {tj}j?i). The feature vector xi can depend
on previously predicted class labels {tj}j?i, but
the dependency is typically assumed to be local.
In the RRM method, the above conditional proba-
bility model has the following parametric form:
P (ti = c|xi, ti?l, ..., ti?1) = T (wTc xi + bc),
where T (y) = min(1,max(0, y)) is the truncation
of y into the interval [0, 1]. wc is a linear weight
vector and bc is a constant. Parameters wc and bc
can be estimated from the training data. Given
training data (xi, ti) for i = 1, ..., n, the model
is estimated by solving the following optimization
problem for each c (Zhang et al, 2002):
inf
w,b
1
n
n
?
i=1
f(wTc xi + bc, yic),
where yic = 1 when ti = c, and yic = ?1 other-
wise. The function f is defined as:
f(p, y) =
?
?
?
?
?
?2py py < 1
1
2(py ? 1)2 py ? [?1, 1]
0 py > 1
Given the above conditional probability model,
the best possible sequence of ti?s can be estimated
by dynamic programming in the decoding stage
(Zhang et al, 2002).
2.2 Multilevel Linguistic Features
This Chinese NER system uses Chinese charac-
ters (not Chinese words) as the basic token units,
and then maps word-based features that are as-
sociated with each word into corresponding fea-
tures of those characters that are contained in the
word. This approach can effectively incorporate
both character-based features and word-based fea-
tures. In general, we may regard this approach
as information integration from linguistic views at
different abstraction levels.
We integrate a diverse set of local linguistic fea-
tures, including word segmentation information,
Chinese word patterns, complex lexical linguis-
tic features (e.g. part of speech and semantic fea-
tures), aligned at the character level. In additional,
we also use external NE hints and gazetteers, in-
cluding surnames, location suffixes, organization
suffixes, titles, high-frequency Chinese characters
in Chinese names and translation names, and lists
of locations and organizations. In this system, lo-
cal linguistic features of a token unit are derived
from the sentence containing this token unit. All
special linguistic patterns (i.e. date, time, numeral
expression) are encoded into pattern-specific class
labels aligned with the tokens.
3 Impact of Training Data Size And
Domain Information on the NER
Performance
It is very important to keep the performance sta-
bility of NER models across domains in practice.
510
However, the performance usually becomes unsta-
ble when NER models are applied in different do-
mains. We focus on the impact of the training data
size and domain information on the NER perfor-
mance in this section.
3.1 Data
We built a large-scale high-quality Chinese NE an-
notated corpus. The corpus size is 114.25M Chi-
nese characters. All the data are news articles se-
lected from several Chinese newspapers in 2001
and 2002. All the NEs in the corpus are manually
tagged. Documents in the corpus are also man-
ually classified into eight domain categories, in-
cluding politics, sports, science, economics, enter-
tainment, life, society and others. Cross-validation
is employed to ensure the tagging quality.
All the training data and test data in the exper-
iments are selected from this Chinese annotated
corpus. The general training data are randomly se-
lected from the corpus without distinguishing their
domain categories. All the domain-specific train-
ing data are selected from the corpus according to
their domain categories. One general test data set
and seven domain-specific test data sets are used
in our experiments (see Table 1). The size of the
general test data set is 1.34M Chinese characters.
Seven domain-specific test sets are extracted from
the general test data set according to the document
domain categories.
Domain NE distribution in the domain-oriented test data set Test set
PER ORG LOC MISC Total Size
General 11,991 9,820 12,353 1,820 35,984 1.34M
Politics 2,470 1,528 2,540 480 7,018 0.2M
Economics 1,098 2,971 2,362 493 6,924 0.26M
Sports 1,802 1,323 1,246 478 4,849 0.10M
Entertainment 2,458 526 738 542 4,264 0.10M
Society 916 418 823 349 2,506 0.08M
Life 2,331 1,690 3,634 763 8,418 0.39M
Science 1,802 1,323 1,246 478 4,849 0.10M
Table 1: NE distribution in the general and
domain-specific test data sets
In our evaluation, only NEs with correct bound-
aries and correct class labels are considered as the
correct recognition. We use the standard P (i.e.
Precision), R (i.e. Recall), and F-measure (de-
fined as 2PR/(P+R)) to measure the performance
of NER models.
3.2 Impact of Training Data Size on the NER
Performance across Domains
The amount of annotated data is always a bottle-
neck for supervised learning methods in practice.
Figure 1: Performance curves of the general and
specific domain NER models
Thus, we evaluate the impact of training data size
on the NER performance across domains.
In this baseline experiment, an initial general
NER model is trained with 0.1M general data at
first. Then the NER model is incrementally re-
trained by adding 0.1M new general training data
each time till the performance isn?t enhanced sig-
nificantly. The NER performance curve (labelled
with the tag ?General? ) in the whole retraining
process is shown in Figure 1. Experimental results
show that the performance of the general NER
model is significantly enhanced in the first several
retraining cycles since more training data are used.
However, when the general training data set size is
more than 2.4M, the performance enhancement is
very slight.
In order to analyze how the training data size
impacting the performance of NER models in spe-
cific domains, seven domain-specific NER mod-
els are built using the similar retraining process.
Each domain-specific NER model is also trained
with 0.1M domain-specific data at first. Then,
each initial domain-specific NER model is incre-
mentally retrained by adding 0.1M new domain-
specific data each time.
NER F(%) Size NE distribution in the training set
Model thre-
shold
(M) PER ORG LOC MISC Total
General 80.38 2.4 24,960 27,231 21,098 7,439 80,728
Politics 83.09 0.9 11,388 6,618 14,350 1,974 34,330
Econ-
omics 85.46 1.7 7,197 21,113 15,582 3,466 47,358
Sports 90.78 0.6 11,647 8,105 7,468 3,070 30,290
Entert-
ainment 83.31 0.6 12,954 2,823 4,665 3,518 32,860
Society 76.55 0.6 7,099 3,279 6,946 1,909 19,233
Life 81.06 1.7 10,502 5,675 18,980 2,420 37,577
Science 70.02 0.4 1,625 3,010 2,083 902 7,620
Table 2: Performance of NER models, size thresh-
old and NE distribution in the corresponding train-
ing data sets
511
The performance curves of these domain-
specific NER models are also shown in Figure 1
(see the curves labelled with the domain tags). Al-
though the initial performance of each domain-
specific NER model varies with domains, the per-
formance is also significantly enhanced in the first
several retraining cycles. When the size of the
domain-specific training data set is above a certain
threshold, the performance enhancement is very
slight as well.
The final performance of the trained NER mod-
els, and the corresponding training data sets are
shown in Table 2.
From these NER performance curves, we obtain
the following observations.
1. More training data are used, higher NER per-
formance can be achieved. However, it is
difficult to significantly enhance the perfor-
mance when the training data size is above a
certain threshold.
2. The threshold of the training data size and
the final achieved performance vary with do-
mains (see Table 2). For example, in enter-
tainment domain, the threshold is 0.6M and
the final F-measure achieves 83.31%. In eco-
nomic domain, the threshold is 1.7M, and the
corresponding F-measure is 85.46%.
3.3 The Performance Stability of Each NE
Type Recognition across Domains
Statistic data on our large-scale annotated corpus
(shown in Table 3) show that the distribution of NE
types varies with domains. We define ? NE density
? to quantitatively measure the NE distribution in
an annotated data set. NE density is defined as ?the
count of NE instances in one thousand Chinese
characters?. Higher NE density usually indicates
that more NEs are contained in the data set. We
may easily measure the distribution of each NE
type across domains using NE density. In this an-
notated corpus, PER, LOC, and ORG have similar
NE density while MISC has the smallest NE den-
sity. All the NE types also have different NE den-
sity in each domain. For example, the NE density
of ORG and LOC is much higher than that of PER
in economic domain. PER and LOC have higher
NE density than ORG in politics domain. PER
has the highest NE density among these NE types
in both sports and entertainment domains. The
unbalanced NE distribution across domains shows
that news articles on different domains usually fo-
cus on different specific NE types. These NE dis-
tribution features imply that each NE type has dif-
ferent domain dependency feature. The perfor-
mance stability of domain-focused NE type recog-
nition becomes more important in domain-specific
applications. For example, since economic news
articles usually focus on ORG and LOC NEs, the
high-quality LOC and ORG recognition models
will be more valuable in economic domain. In ad-
dition, these distribution features also can be used
to guide training and test data selection.
Domain NE distribution in the specific domain
PER LOC ORG MISC ALL Ratio
(%)
Politics 167,989 180,193 105,936 30,830 484,948 16.43
Econ-
omics 117,459 200,261 352,323 76,320 746,363 25.29
Sports 129,137 73,435 98,618 33,304 334,494 11.33
Entert- 154,193 50,408 40,444 52,460 297,505 10.08
ainment
Life 200,222 234,150 145,138 65,733 645,243 21.86
Society 63,793 53,724 43,657 21,162 182,336 6.18
Science 27,878 30,737 72,413 16,824 147,852 5.00
Others 31,723 40,730 26,666 13,926 113,045 3.83
All 892,394 863,638 885,195 310,559 2,951,786 ?
Domain NE density in the Chinese annotated corpus Size
PER LOC ORG MISC ALL (M)
Politics 10.70 11.48 6.75 1.96 31.21 15.70
Econ-
omics 4.18 7.13 12.55 2.72 26.58 28.08
Sports 16.43 9.34 12.55 4.24 42.57 7.86
Entert-
ainment 16.81 5.05 4.14 5.72 32.44 9.17
Life 5.64 6.59 4.09 1.85 18.17 35.52
Society 8.57 7.22 5.87 2.84 24.51 7.44
Science 4.30 4.74 11.17 2.60 22.82 6.48
Others 7.9 10.18 6.67 3.48 28.26 4.00
All 7.81 7.56 7.75 2.72 25.89 114.25
Table 3: NE distribution in the Chinese annotated
corpus
In this experiment, the performance stability
of NER models across domains is evaluated, es-
pecially the performance stability of each NE
type recognition. The general NER model is
trained with 2.4M general data. Seven domain-
specific models are trained with the corresponding
domain-specific training sets (see Table 2 in Sec-
tion 3.2).
The performance stability of the general NER
model is firstly evaluated on the general and
domain-specific test data sets (see Table 1 in Sec-
tion 3.1 ). The experimental results are shown in
Table 4. The performance curves of the general
model are shown in Figure 2, including the total
F-measure curve of the NER model (labelled with
the tag ?All?) and F-measure curves of each NE
type recognition in the specific domains (labelled
with the NE tags respectively).
The performance stability of the seven domain-
specific NER models are also evaluated. Each
domain-specific NER model is tested on the gen-
512
Domain F(%) of general NER model
PER LOC ORG MISC ALL
General 86.69 85.55 73.59 56.00 80.38
Economic 85.11 88.22 75.91 49.53 80.50
Politics 86.26 87.00 71.31 61.50 81.90
Sports 91.87 89.03 81.67 67.41 86.10
Entertainment 84.24 85.85 68.65 60.96 79.31
Life 86.62 83.54 70.30 58.49 79.73
Society 84.53 76.16 68.89 41.14 74.50
Science 87.74 86.42 65.85 24.10 69.55
Table 4: Performance of the general NER model
in specific domains
Figure 2: Performance curves of the general NER
model in specific domains
eral test data and the other six different domain-
specific test data sets. The experimental results are
shown in Table 5. The performance curves of three
domain-specific NER models are shown in Figure
3, Figure 4 and Figure 5 respectively.
From these experimental results, we have the
following conclusions.
1. The performance stability of all the NER
models is limited across domains. When a
NER model is employed in a new domain, its
performance usually decreases. Moreover, its
performance is usually much lower than the
performance of the corresponding domain-
specific model.
2. The general NER model has better per-
Figure 3: Performance curves of economic do-
main NER model in the other specific domains
NER F(%) in specific domain
Model Gen- Eco- Poli- Spo- Enter- Life Soc- Sci-
eral nomic tics rts tainment iety ence
General 80.38 80.50 81.90 86.10 79.31 79.73 74.50 69.55
Econ-
omic 75.30 85.46 74.32 72.89 68.46 76.23 65.75 68.97
Politics 73.37 66.39 83.09 76.37 71.51 74.83 67.31 53.76
Sports 71.23 62.56 68.99 90.78 73.48 71.18 64.82 53.85
Entert-
ainment 70.82 61.52 72.04 75.34 83.31 71.80 69.10 52.50
Life 73.53 66.92 75.07 73.86 72.68 81.06 69.61 57.36
Society 70.29 62.55 72.70 70.69 72.24 74.10 76.55 53.42
Science 67.26 67.57 69.00 64.32 63.84 69.05 64.85 70.02
Table 5: Performance of NER models in specific
domains
Figure 4: Performance curves of sports domain
NER model in the other specific domains
formance stability than the domain-specific
NER model when they are applied in new do-
mains (see Table 5). Domain-specific mod-
els usually could achieve a higher perfor-
mance in its corresponding domain after be-
ing trained with a smaller amount of domain-
specific annotated data (see Table 2 in Sec-
tion 3.2). However, the performance stability
of domain-specific NER model is poor across
different domains. Thus, it is very popular to
build a general NER model for the general
applications in practice.
3. The performance of PER, LOC and ORG
recognition is better than that of MISC recog-
Figure 5: Performance curves of politics domain
NER model in the other specific domains
513
nition in NER (see Figure 2 ? Figure 5).
The main reason for the poor performance of
MISC recognition is that there are less com-
mon indicative features among various MISC
NEs which we do not distinguish. In addi-
tion, NE density of MISC is much less than
that of PER, LOC, and ORG. There are a
relatively small number of positive training
samples for MISC recognition.
4. NE types have different domain dependency
attribute. The performance stability of each
NE type recognition varies with domains (see
Figure 2 ? Figure 5). The performance of
PER and LOC recognition are more stable
across domains. Thus, few efforts are needed
to adapt the existing high-quality general
PER and LOC recognition models in domain-
specific applications. Since ORG and MISC
NEs usually contain more domain-specific
semantic information, ORG and MISC are
more domain-dependent than PER and LOC.
Thus, more domain-specific features should
be mined for ORG and MISC recognition.
4 Use Informative Training Samples to
Enhance the Performance of NER
Models across Domains
A higher performance system usually requires
more features and a larger number of training data.
This requires larger system memory and more effi-
cient training method, which may not be available.
Within the limitation of available training data and
computational resources, it is necessary for us to
either limit the number of features or select more
informative data which can be efficiently handled
by the training algorithm. Active learning method
is usually employed in text classification (McCal-
lum and Nigam et al, 1998). It is only recently
employed in NER (Shen et al, 2004).
In order to enhance the performance and over-
come the limitation of available training data and
computational resources, we present an informa-
tive sample selection method using a variant of
uncertainty-sampling (Lewis and Catlett, 1994).
The main steps are described as follows.
1. Build an initial NER model (F-
measure=76.24%) using an initial data
set. The initial data set (about 1M Chinese
characters) is randomly selected from the
large-scale candidate data set (about 9M ).
Figure 6: Performance curves of general NER
models after being trained with informative sam-
ples and random samples respectively
2. Refine the training set by adding more infor-
mative samples and removing those redun-
dant samples. In this refinement phase, all of
the data are annotated by the current recogni-
tion model (e.g. the initial model built in Step
1). Each annotation has a confidence score
associated with the prediction. In general, an
annotation with lower confidence score usu-
ally indicates a wrong prediction. The con-
fidence score of the whole sample sentence
is defined as the average of the confidence
scores of all the annotations contained in the
sentence. Thus, we add those sample sen-
tences with lower confidence scores into the
training set. Meanwhile, in order to keep a
reasonable size of the training set, those old
training sample sentences with higher confi-
dence scores are removed from the current
training set. In each retraining phase, all of
the sample sentences are sorted by the con-
fidence score. The top 1000 new sample
sentences with lowest confidence scores are
added into the current training set. The top
500 old training sample sentences with high-
est confidence scores are removed from the
current training set.
3. Retrain a new Chinese NER model with the
newly refined training set
4. Repeat Step 2 and Step 3, until the perfor-
mance doesn?t improve any more.
We apply this informative sample selection
method to incrementally build the general domain
NER model. The size of the final informative
training sample set is 1.05M Chinese characters.
This informative training sample set has higher
NE density than the random training data set (see
Table 6).
514
We denote this general NER model trained with
the informative sample set as ?general informa-
tive model?, and denote the general-domain model
which is trained with 2.4M random general train-
ing data as ?general random model?. The perfor-
mance curves of the general NER models after be-
ing trained with informative samples and random
data respectively are shown in Figure 6. Experi-
ment results (see Table 6) show that there is a sig-
nificant enhancement in F-measure if using infor-
mative training samples. Compared with the ran-
dom model, the informative model can increase F-
measure by 4.21 percent points.
Type Using informative sample set Using random training set
(1.05M) (2.4M)
F(%) NEs NE density F(%) NEs NE density
PER 89.87 18,898 18.00 86.69 24,960 10.38
LOC 89.68 24,862 23.68 85.55 21,089 11.33
ORG 79.22 22,173 21.12 73.59 27,231 8.78
MISC 64.27 8,067 7.68 56.00 7,439 3.10
Total 84.59 74,000 70.48 80.38 80,728 33.58
Table 6: Performance of informative model and
random model in the general domain
Domain F(%) of general informative model
PER LOC ORG MISC ALL
Economic 89.26 90.66 81.24 61.14 84.63
Politics 89.36 89.37 74.76 65.95 84.70
Sports 93.65 90.66 86.00 72.05 88.71
Entertainment 88.38 87.54 73.88 58.32 82.74
Life 89.15 88.35 75.68 72.01 84.66
Society 86.61 82.15 72.99 58.55 79.49
Science 90.91 88.35 71.69 25.16 72.71
Table 7: Performance of the general informative
model in specific domains
This informative model is also evaluated on the
domain-specific test sets. Experimental results are
shown in Table 7. We view the performance of the
domain-specific NER model as the baseline per-
formance in its corresponding domain (see Table
8), denoted as Fbaseline. The performance of in-
formative model in specific domains is very close
to the corresponding Fbaseline (see Figure 7). We
define the domain-specific average F-measure as
the average of all the F-measure of the NER model
in seven specific domains, denote as F . The av-
erage of all the Fbaseline in specific domains is
denoted as F baseline. The average F-measure of
the informative model and the random model in
specific domains is denoted as F informative and
F random respectively. Compared with F baseline
(F =81.47%), the informative model increases F
by 1.05 percent points. However, F decreases by
2.67 percent points if using the random model. Es-
pecially, the performance of the informative model
is better than the corresponding baseline perfor-
Figure 7: Performance comparison of informa-
tive model, random model, and the corresponding
domain-specific models
mance in politics, life, society and science do-
mains. Moreover, the size of the informative sam-
ple set is much less than the life domain training
set (1.7M).
NER F(%) in specific domains
model Eco- Poli- Spo- Entert- Life So- Sci- F
nomic tics rts ainment ciety ence
domain-
specific 85.46 83.09 90.78 83.31 81.06 76.55 70.02 81.47
(baseline)
Infor-
mative 84.63 84.70 88.71 82.74 84.66 79.49 72.71 82.52
Random 80.50 81.90 86.10 79.31 79.73 74.50 69.55 78.80
NER ?(F ) in specific domain
model ?(F ) = (F ? F ) ?
Eco- Poli- Spo- Entert- Life So- Sci-
nomic tics rts ainment ciety ence
Infor-
mative 2.11 2.18 6.19 0.22 2.14 -3.03 -9.81 4.74
Random 1.7 3.1 7.3 0.51 0.93 -4.3 -9.25 4.94
Table 8: Performance comparison of informa-
tive model, random model and the corresponding
domain-specific model in each specific domain
The informative model has much better perfor-
mance than the random model in specific domains
(see Table 8 and Figure 7). F informative is 82.52%
while F random is 78.80%. The informative model
can increase F by 3.72 percent points. The infor-
mative model is also more stable than the random
model in specific domains (see Table 8). Standard
deviation of F-measure for the informative model
is 4.74 while that for the random model is 4.94.
Our experience with the incremental sample se-
lection provides the following hints.
1. The performance of the NER model across
domains can be significantly enhanced after
being trained with informative samples. In
515
order to obtain a high-quality and stable NER
model, it is only necessary to keep the infor-
mative samples. Informative sample selec-
tion can alleviate the problem of obtaining a
large amount of annotated data. It is also an
effective method for overcoming the poten-
tial limitation of computational resources.
2. In learning NER models, annotated results
with lower confidence scores are more use-
ful than those samples with higher confidence
scores. This is consistent with other studies
on active learning.
5 Conclusion
Efficient and robust NER model is very impor-
tant in practice. This paper provides an empirical
study on the impact of training data size and do-
main information on the performance stability of
NER. Experimental results show that it is difficult
to significantly enhance the performance when the
training data size is above a certain threshold. The
threshold of the training data size varies with do-
mains. The performance stability of each NE type
recognition also varies with domains. The large-
scale corpus statistic data also show that NE types
have different distribution across domains. These
empirical investigations provide useful hints for
enhancing the performance stability of NER mod-
els across domains with less efforts. In order to en-
hance the NER performance across domains, we
present an informative training sample selection
method. Experimental results show that the per-
formance is significantly enhanced by using infor-
mative training samples.
In the future, we?d like to focus on further
exploring more effective methods to adapt NER
model to a new domain with much less efforts,
time and performance degrading.
References
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2003. A simple named entity extractor using ad-
aboost. In Proceedings of CoNLL-2003, pages 152?
155.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recogintion
through classifier combination. In Proceedings
CoNLL-2003, pages 168?171.
Jian F. Gao, Mu Li, Anndy Wu, and Chang N., Huang.
2005. Chinese Word Segmentation and Named En-
tity Recognition: A Pragmatic Approach. Computa-
tional Linguisitc,31(4):531-574.
Hong L. Guo, Jian M. Jiang, Gang Hu, and Tong
Zhang. 2005. Chinese Named Entity Recognition
Based on Multilevel Linguistic Features. Lecture
Notes in Artificial Intelligence,3248:90-99,Springer.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proceedings of Coling-2002, pages 1-7.
Hongyan Jing, Radu Florian, Xiaoqiang Luo, Tong
Zhang, and Abraham Ittycheriah. 2003. Howtoge-
tachinesename (entity) : Segmentation and combi-
nation issues. In EMNLP 2003, pages 200-207.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recogni-
tion with character-level models. In Proceedings of
CoNLL-2003, pages 180?183.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International Con-
ference on Machine Learning, pages 148?156.
Andrew Kamal McCallum and K. Nigam. 1998. Em-
ploying EM in pool-based active learning for text
classification. Proceedings of 15th International
Conference on Machine Learning, pages 350-358.
Fien De Meulder and Walter Daelemans. 2003.
Memory-based named entity recognition using
unannotated data. In Proceedings of CoNLL-2003,
pages 208?211.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Lan-
guage independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147.
Dan Shen, Jie Zhang, Jian Su, Gou D. Zhou, and Chew
L.Tan, 2004. Multi-Criteria-based Active Learn-
ing for Named Entity Recognition. Proceedings of
ACL04, pages 589-596.
Yu Z. Wu, Jun Zhao, Bo Xu, and Hao Yu. 2005. Chi-
nese Named Entity Recognition Based on Multiple
Features. Proceedings of EMNLP05, pages 427-434
Shi H. Yu, Shuan H. Bai, and Paul Wu. 1998. De-
scription of the kent ridge digital labs system used
for muc-7. In Proceedings of the Seventh Message
Understanding Conference (MUC-7).
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of
Winnow. Journal of Machine Learning Research,
2:615?637.
516
Coling 2010: Poster Volume, pages 1480?1488,
Beijing, August 2010
Metaphor Interpretation and Context-based Affect Detection  
 
Li Zhang 
School of Computing 
Teesside University 
l.zhang@tees.ac.uk 
 
Abstract 
Metaphorical and contextual affect de-
tection from open-ended text-based di-
alogue is challenging but essential for 
the building of effective intelligent user 
interfaces. In this paper, we report up-
dated developments of an affect detec-
tion model from text, including affect 
detection from one particular type of 
metaphorical affective expression and 
affect detection based on context. The 
overall affect detection model has been 
embedded in an intelligent conversa-
tional AI agent interacting with human 
users under loose scenarios. Evaluation 
for the updated affect detection compo-
nent is also provided. Our work contri-
butes to the conference themes on sen-
timent analysis and opinion mining and 
the development of dialogue and con-
versational agents. 
1 Introduction 
Affect sensing from open-ended text-based 
natural language input is a rising research area. 
Zhang et al (2008a) reported an affect detection 
component on detecting simple and complex 
emotions, meta-emotions, value judgments etc 
from literal expressions. Recently, metaphorical 
language has drawn researchers? attention since 
it has been widely used to provide effective 
vivid description. Fainsilber and Ortony (1987) 
commented that ?an important function of 
metaphorical language is to permit the 
expression of that which is difficult to express 
using literal language alone?. In Wallington et 
al?s work (2008), several metaphorical affective 
expressions (such as animal metaphor (?X is a 
rat?) and affects as external entities metaphor 
(?joy ran through me?)) have been intensively 
studied and affect has been derived from some 
simple animal metaphorical expressions.    
The work presented here reports develop-
ments on affect detection from one particular 
comparatively complex metaphorical phenome-
non with affect implication, i.e. the cooking me-
taphor (?the lawyer grilled the witness on the 
stand?, ?I knew I was cooked when the teacher 
showed up at the door?) 
(http://knowgramming.com/cooking_metaphors.
htm). Since context plays an important role in 
the interpretation of the affect conveyed by the 
user during the interaction, we have used lin-
guistic contextual analysis and cognitive emo-
tional modeling based on Markov chain model-
ing and a dynamic algorithm to interpret affect 
from context in our application. 
Our developments have been incorporated in-
to an affect detection component, which can 
detect affect and emotions from literal text input 
and has been embedded in an intelligent conver-
sational agent, engaged in a drama improvisa-
tion with human users under loose scenarios 
(school bullying and Crohn?s disease). The con-
versational AI agent also provides appropriate 
responses based on the detected affect from us-
ers? input in order to stimulate the improvisa-
tion. In both scenarios, the AI agent plays a mi-
nor role in drama improvisation. E.g. it plays a 
close friend of the bullied victim (the leading 
role) in school bullying scenario, who tries to 
stop the bullying. 
We have also analyzed affect detection per-
formance based on previously collected (other) 
transcripts from user testing by calculating 
agreements via Cohen?s Kappa between two 
human judges and between human judges and 
the AI agent with and without the new devel-
1480
opment respectively in order to verify the effi-
ciency of the metaphorical and contextual affect 
sensing.   
The content is arranged as follows. We report 
relevant work in section 2 and the new devel-
opments on affect detection from the cooking 
metaphor in section 3. Contextual affect sensing 
is discussed in section 4. System evaluation and 
conclusion are presented in section 5. 
2 Related Work 
There is well-known research work in the re-
lated fields. ConceptNet (Liu and Singh, 2004) 
is a toolkit to provide practical textual reasoning 
for affect sensing for six basic emotions, text 
summarization and topic extraction. Shaikh et 
al. (2007) provided sentence-level textual affect 
sensing to recognize evaluations (positive and 
negative). They adopted a rule-based domain-
independent approach, but they haven?t made 
attempts to recognize different affective states 
from open-ended text input.   
Although Fa?ade (Mateas, 2002) included 
shallow natural language processing for charac-
ters? open-ended utterances, the detection of 
major emotions, rudeness and value judgements 
is not mentioned. Zhe and Boucouvalas (2002) 
demonstrated an emotion extraction module 
embedded in an Internet chatting environment. 
It used a part-of-speech tagger and a syntactic 
chunker to detect the emotional words and to 
analyze emotion intensity for the first person 
(e.g. ?I?). The detection focused only on emo-
tional adjectives and first-person emotions, and 
did not address deep issues such as figurative 
expression of emotion. There is also work on 
general linguistic cues useful for affect detec-
tion (e.g. Craggs and Wood, 2004). 
In addition, there is well-known research 
work on the development of emotional conver-
sational agents. Egges et al (2003) provided 
virtual characters with conversational emotional 
responsiveness. Aylett et al (2006) also focused 
on the development of affective behavior plan-
ning for their synthetic characters. Cavazza et 
al. (2008) reported on a conversational agent 
embodied in a wireless robot to provide sugges-
tions for users on a healthy living life-style. 
Hierarchical Task Networks (HTN) planner and 
semantic interpretation have been used in this 
work. The cognitive planner plays an important 
role in assisting with dialogue management. The 
user?s response has also been considered for the 
generation of a new plan. However, the system 
will hesitate when open-ended user input going 
beyond the planner?s knowledge has been used 
intensively during interaction. The system we 
present here intends to deal with such challenge. 
Our work focuses on the following aspects: 
(1) affect detection from metaphorical expres-
sions; (2) real-time affect sensing for basic and 
complex emotions in improvisational role-play 
situations; (3) affect detection for second and 
third person cases (e.g. ?you?, ?she?); and (4) 
affect interpretation based on context profiles. 
3 Further Development on Metaphori-
cal Affect Detection 
Without pre-defined constrained scripts, our 
original system has been developed for 14-16 
year old school students to conduct creative im-
provisation within highly emotionally charged 
scenarios. Various metaphorical expressions 
were used to convey emotions (K?vecses, 
1998), which are theoretically and practically 
challenging and draw our attention. 
Metaphorical language can be used to convey 
emotions implicitly and explicitly, which also 
inspires cognitive semanticists (K?vecses, 
1998). In our previous study (Zhang et al 
2008b; 2009), we detected affect from several 
comparatively simple metaphorical affective 
phenomena. Another type of comparatively 
complex metaphor has also drawn our attention, 
i.e. the cooking metaphor. Very often, the agent 
himself/herself would become the victim of 
slow or intensive cooking (e.g. grilled, cooked). 
Or one agent can perform cooking like actions 
towards another agent to realize punishment or 
torture. Examples are as follows, ?he basted her 
with flattery to get the job?, ?she knew she was 
fried when the teacher handed back her paper?.  
In these examples, the suffering agents have 
been figuratively conceptualized as food. They 
bear the results of intensive or slow cooking. 
Thus, these agents who suffer from such cook-
ing actions carried out by other agents tend to 
feel pain and sadness, while the ?cooking per-
forming? agents may take advantage of such 
actions to achieve their intentions, such as per-
suasion, punishment or even enjoyment. The 
syntactic structures of some of the above exam-
1481
ples also indicate the submissive stance of the 
suffering agents. E.g. in the instances, passive 
sentences (?he knew he was cooked when he 
saw his boss standing at the door?) have been 
used to imply unwillingness and victimization 
of the subject agents who are in fact the objects 
of the cooking actions described by the verb 
phrases (?X + copular form + passive cooking 
action?). In other examples, the cooking actions 
have been explicitly performed by the subject 
agents towards the object agents to imply the 
former?s potential willingness and enjoyment 
and the latter?s potential suffering and pain (?A 
+ [cooking action] + B?).  
Thus in our application, we focus on the 
above two particular types of expressions. We 
use Rasp (Briscoe & Carroll, 2002) to recognize 
user input with such syntactic structures (?A + 
copular form + VVN?, ?A + VV0/VVD/VVZ 
(verb) + B?). Many sentences could possess 
such syntactic structures (e.g. ?Lisa was bul-
lied?, ?he grills Lisa?, ?I was hit by a car?, ?Li-
sa was given the task to play the victim role?, ?I 
steamed it? etc), but few of them are cooking 
metaphors. Therefore we need to resort to se-
mantic profiles to recognize the metaphorical 
expressions. Rasp has also provided a syntactic 
label for each word in the user input. Thus the 
main verbs were identified by their correspond-
ing syntactic labels (e.g. ?given? labeled as ?past 
participle form of lexical verbs (VVN)?, ?likes? 
and ?grills? labeled as ?-s form of lexical verbs 
(VVZ)?) and the semantic interpretation for 
their base forms is discovered from WordNet 
(Fellbaum, 1998). Since WordNet has provided 
hypernyms (Y is a hypernym of X if every X is 
a (kind of) Y) for the general noun and verb 
lexicon, ?COOK? has been derived as the 
hypernym of the verbs? described cooking ac-
tions. E.g. ?boil?, ?grill?, ?steam?, and ?simmer? 
are respectively interpreted as one way to 
?COOK?. ?Toast? is interpreted as one way to 
?HEAT UP? while ?cook? is interpreted as one 
way to ?CREAT?, or ?CHEAT? etc. One verb 
may recover several hypernyms and in our ap-
plication, we collect all of them. Another evalu-
ation resource (Esuli and Sebastiani, 2006) is 
resorted to in order to recover the evaluation 
values of all the hypernyms for a particular 
verb. If some hypernyms are negative (such as 
?CHEAT?) and the main object of the overall 
input refers to first/third person cases or singu-
lar proper nouns (?him?, ?her?, or ?Lisa?), then 
the user input (e.g. ?he basted her with flattery 
to get the job?) conveys potential negative af-
fect (e.g. pain and sadness) for the human ob-
jects and potential positive affect (e.g. persua-
sion or enjoyment) for the subjects. If the evalu-
ation dictionary fails to provide any evaluation 
value for any hypernyms (such as ?COOK? and 
?HEAT UP?) of the main verbs, then we still 
assume that ?verbs implying COOK/HEAT UP 
+ human objects? or ?human subjects + copular 
form + VVN verbs implying COOK/HEAT UP? 
may indicate negative emotions both for the 
human objects in the former and the human sub-
jects in the latter. E.g. for the input ?I was fried 
by the head teacher?, the processing is as fol-
lows: 
1. Rasp identifies the input has the following 
structure: ?PPIS1 (I) + copular form (was) + 
VVN (fried)?; 
2. ?Fry? (base form of the main verb) is sent 
to WordNet to obtain its hypernyms, which in-
clude ?COOK?, ?HEAT? and ?KILL?;  
3. The input has the following syntactic se-
mantic structure: ?PPIS1 (I) + copular form 
(was) + VVN (Hypernym: COOK)?, thus it is 
recognized as a cooking metaphor; 
4. The three hypernyms are sent to the evalu-
ation resource to obtain their evaluation values. 
?KILL? is labeled as negative while others can?t 
obtain any evaluation values from the profile; 
5. The input is transformed into: ??PPIS1 (I) 
+ copular form (was) + VVN (KILL: negative)? 
6. The subject is a first person case, then the 
input indicates the user who is speaking suf-
fered from a negative action and may have a 
?negative? emotional state. 
Although our processing is limited to the 
verb metaphor examples and hasn?t considered 
other instances like ?tasty tidbits of informa-
tion?, it points out promising directions for fi-
gurative language processing. After our inten-
tion to improve the performance of affect sens-
ing from individual turn-taking input, we focus 
on improvement of the performance using con-
text profiles. In future work, we intend to use a 
metaphor ontology to recognize metaphors.  
4 Affect Sensing from Context Profiles 
Our previous affect detection (Zhang et al 
2008a) has been performed solely based on in-
1482
dividual turn-taking input. Thus the context in-
formation has been ignored. However, the con-
textual and character profiles may influence the 
affect implied in the current input. In this sec-
tion, we will discuss relationships between cha-
racters, linguistic contextual indicators, cogni-
tive emotion simulation from a communication 
context and our approach developed based on 
these features to interpret affect from context.  
4.1 Relationship Interpretation 
Relationships between characters in drama im-
provisation are very crucial for the contextual 
affect interpretation for the emotionally ambi-
guous users? input. During the improvisation of 
each scenario, like any other drama progression, 
normally the recorded transcripts for creative 
roleplays are composed of three main improvi-
sational sections, including the starting of the 
drama, the climax and the final ending. Rela-
tionships in these three drama progression stag-
es between characters are different from one 
another. E.g. in the climax of the improvisation 
of the school bullying scenario, we normally 
expect very negative relationships between the 
bully and the bullied victim (Lisa) & her friends 
since the big bully is very aggressive at Lisa and 
her friends who try to stop the bullying. Moreo-
ver, in nearly the end of the improvisational ses-
sion, sometimes the big bully feels sorry for his 
behavior and is cared by Lisa and her friends 
since he is abused by his uncle. The intense 
negative relationships between the big bully and 
Lisa & her friends are changed to those with at 
least less negativity or even normal relation-
ships. Because of the creative nature of the im-
provisation, sometimes the bully and the victim 
may even have a positive relationship towards 
the ending of the drama improvisation.  
However in our current study, we only as-
sume consistent negative relationships between 
the bully and the bullied victim & her friends 
throughout the improvisation to simplify the 
processing. We will report our work on relation-
ship interpretation using fuzzy logic to dynami-
cally capture the changing relationships be-
tween characters as the drama progresses in the 
near future. 
4.2 Linguistic Contextual Indicators   
In our study, we noticed some linguistic indica-
tors for contextual communication in the rec-
orded transcripts. One useful indicator is (i) im-
peratives, which are often used to imply nega-
tive or positive responses to the previous speak-
ing characters, such as ?shut up?, ?go on then?, 
?let?s do it? and ?bring it on?. Other useful con-
textual indicators are (ii) prepositional phrases 
(e.g. ?by who??), semi-coordinating conjunc-
tions (e.g. ?so we are good then?), subordinating 
conjunctions (?because Lisa is a dog?) and 
coordinating conjunctions (?and?, ?or? and 
?but?). These indicators are normally used by 
the current ?speaker? to express further opinions 
or gain further confirmation. 
In addition, (iii) short phrases for questions 
are also used frequently in the transcripts to gain 
further communication based on context, e.g. 
?where??, ?who is Dave? or ?what?. (iv) Cha-
racter names are also normally used in the user 
input to indicate that the current input is in-
tended for particular characters, e.g. ?Dave go 
away?, ?Mrs Parton, say something?, ?Dave 
what has got into you?? etc. Very often, such 
expressions have been used to imply potential 
emotional contextual communication between 
the current speaking character and the named 
character. Therefore the current speaking cha-
racters may imply at least ?approval? or ?disap-
proval? towards the opinions/comments pro-
vided by the previous named speaking charac-
ters. Finally there are also (v) some other well 
known contextual indicators in Internet relay 
chat such as ?yeah/yes followed by a sentence 
(?yeah, we will see?)?, ?I think so?, ?no/nah fol-
lowed by a sentence?, ?me too?, ?exactly?, 
?thanks?, ?sorry?, ?grrrr?, ?hahahaha?, etc. 
Such expressions are normally used to indicate 
affective responses to the previous input.  
Since natural language is ambiguous and 
there are cases in which contextual information 
is required in order to appropriately interpret the 
affect conveyed in the input (e.g. ?go on then?), 
our approach reported in the following inte-
grates the above contextual linguistic indicators 
with cognitive contextual emotion prediction to 
uncover affect conveyed in emotionally ambi-
guous input.  
4.3 Emotion Modeling in Communication 
Context 
There are also other aspects which may influ-
ence the affect conveyed in the communication 
context. E.g. in our application, the affect con-
1483
veyed by the speaking character himself/herself 
in the recent several turn-taking, the ?improvisa-
tional mood? that the speaking character is 
created, and emotions expressed by other cha-
racters, especially by the contradictory ones 
(e.g. the big bully), have great potential to influ-
ence the affect conveyed by the current speak-
ing character (e.g. the bullied victim). Some-
times, the story themes or topics also have po-
tential impact to emotions or feelings expressed 
by characters. For example, people tend to feel 
?happy? when involved in discussions on posi-
tive topics such as harvest or raising salary, 
while people tend to feel ?sad? when engaged in 
the discussions on negative themes such as 
economy breakdown, tough examination etc. 
In our application, although the hidden story 
sub-themes used in the scenarios are not that 
dramatic, they are still highly emotionally 
charged and used as the signals for potential 
changes of emotional context for each character. 
E.g. In the school bullying scenario (which is 
mainly about the bully, Mayid, is picking on the 
new comer to the school, Lisa. Lisa?s friends, 
Elise and Dave, are trying to stop the bullying. 
The school teacher, Mrs Parton, also tries to 
find out what is going on), the director mainly 
provided interventions based on several main 
sub-themes of the story to push the improvisa-
tion forward, i.e. ?Mayid starts bullying Lisa?, 
?why Lisa is crying?, ?why Mayid is so nasty/a 
bully?, ?how Mayid feels when his uncle finds 
out about his behavior? etc. From the inspection 
of the recorded transcripts, when discussing the 
topic of ?why Lisa is crying?, we noticed that 
Mayid (the bully) tends to be really aggressive 
and rude, while Lisa (the bullied victim) tends 
to be upset and other characters (Lisa?s friends 
and the school teacher) are inclined to show 
anger at Mayid. For the improvisation of the 
hidden story sub-theme ?why Mayid is so nas-
ty/a bully?, the big bully changes from rude and 
aggressive to sad and embarrassed (e.g. because 
he is abused by his uncle), while Lisa and other 
characters become sympathetic (and sometimes 
caring) about Mayid. Usually all characters are 
trying to create the ?improvisational mood? ac-
cording to the guidance of the hidden story sub-
themes (provided via director?s intervention). 
Therefore, the story sub-themes could be used 
as the indicators for potential emotional context 
change. The emotion patterns expressed by each 
character within the improvisation of each story 
sub-theme could be very useful for the predic-
tion of the affect shown in a similar topic con-
text, although the improvisation of the charac-
ters is creative within the loose scenario. It will 
improve the performance of the emotional con-
text prediction if we allow more emotional pro-
files for each story sub-theme to be added to the 
training data to reflect the creative improvisa-
tion (e.g. some improvisations went deeper for a 
particular topic). 
Therefore, a Markov chain is used to learn 
from the emotional context shown in the rec-
orded transcripts for each sub-theme and for 
each character, and generate other possible rea-
sonable unseen emotional context similar to the 
training data for each character. Markov chains 
are usually used for word generation. In our ap-
plication, they are used to record the frequencies 
of several emotions showing up after one par-
ticular emotion. A matrix has been constructed 
dynamically for neutral and the 12 most com-
monly used emotions in our application (caring, 
arguing, disapproving, approving, grateful, hap-
py, sad, threatening, embarrassed, angry/rude, 
scared and sympathetic) with each row 
representing the previous emotion followed by 
the subsequent emotions in columns. The Mar-
kov chains employ roulette wheel selection to 
ensure to produce a greater probability to select 
emotions with higher frequencies than emotions 
with lower occurrences. This will allow the 
generation of emotional context to probabilisti-
cally follow the training data, which may reflect 
the creative nature of the improvisation.  
Then a dynamic algorithm is used to find the 
most resembling emotional context for any giv-
en new situation from the Markov chain?s train-
ing and generated emotional contexts. I.e. by 
using the algorithm, a particular series of emo-
tions for a particular story sub-theme has been 
regarded as the most resembling context to the 
test emotional situation and an emotional state 
is recommended as the most probable emotion 
for the current user input. Since the most recent 
affect histories of other characters and relation-
ships between characters may also have an im-
pact on the affect conveyed by the speaking 
character, the recommended affect will be fur-
ther evaluated (e.g. a most recent ?insulting? 
input from Mayid could make Lisa ?angry?).   
1484
At the training stage, first of all, the school 
bullying transcripts collected from previous user 
testing have been divided into several topic sec-
tions with each of them belonging to one of the 
story sub-themes. The classification of the sub-
themes is mainly based on the human director?s 
intervention which was recorded in the tran-
scripts. Then we used two human annotators to 
mark up the affect of every turn-taking input in 
the transcripts using context inference. Thus, for 
each character, we have summarized a series of 
emotions expressed throughout the improvisa-
tion of a particular story sub-theme. Since the 
improvisation is creative under the loose scena-
rio, some of the sub-themes (e.g. ?why Mayid is 
so nasty?) have been suggested for improvisa-
tion for one than once in some transcripts and 
some of the topics (e.g. ?why Lisa is crying?) 
are only shown in a few of the collected tran-
scripts. We made attempts to gather as many 
emotional contexts as possible for each charac-
ter for the improvisation of each sub-theme in 
order to enrich the training data. 
The following is a small portion of one rec-
orded transcript used for the training of the 
Markov chain. The human annotators have 
marked up the affect expressed in each turn-
taking input.  
DIRECTOR: why is Lisa crying? 
Elise Brown [caring]: lisa stop cryin 
Lisa Murdoch [disagree]: lisa aint crying!!!!  
Dave Simons [caring]: i dunno! y u cryin li-
sa? 
Mayid Rahim [rude]: cuz she dnt realise she 
is lucky to b alive  
Elise Brown [angry]: beat him up! itss onlii 
fat..he'll go down straight away 
Mayid Rahim [insulting]: lisa, y u crying? u 
big baby! 
Mrs Parton [caring]: lisa, r u ok? 
For example, the emotional context for May-
id from the above example is: ?rude? and ?insult-
ing? (we use one letter to represent each emo-
tional label, thus in this example, i.e. ?R I?), and 
in the similar way, the emotional contexts for 
other characters have been derived from the 
above example, which are used as the training 
data for the Markov chain for the topic ?why 
Lisa is crying?. We have summarized the emo-
tional context for each story sub-theme for each 
character from 4 school bullying transcripts and 
used them for the training of the Markov chain. 
The topics considered at the training stage in-
clude: ?Mayid starts bullying?, ?why is Lisa 
crying?, ?why is Mayid nasty/a bully? and ?how 
does Mayid feel if his uncle knew about his be-
havior?? 
At the test stage, our affect detection compo-
nent, EMMA, is integrated with an AI agent and 
detects affect for each user input solely based on 
the analysis of individual turn-taking input it-
self. The above algorithms for context-based 
affect sensing will be activated when the affect 
detection component recognizes ?neutral? from 
the current input during the emotionally charged 
proper improvisation after all the characters 
have known each other and went on the virtual 
drama stage. First of all, the linguistic indicators 
are used to identify if the input with ?neutral? 
implication is a contextual-based input. E.g. we 
mainly focus on the checking of the five contex-
tual implications we mentioned previously, in-
cluding imperatives, prepositional phrases, con-
junctions, simplified question sentences, charac-
ter names, and other commonly used contextual 
indicators (e.g. ?yeah?, ?I think so?). If any of 
the above contextual indicators exists, then we 
further analyze the affect embedded in the input 
with contextual emotion modeling reported 
here. 
For example, we have collected the following 
transcript for testing. Normally the director in-
tervened to suggest a topic change (e.g. ?find 
out why Mayid is a bully?). Thus for a testing 
situation for a particular character, we use the 
emotion context attached with his/her user input 
starting right after the most recent director?s 
intervention and ending at his/her last second 
input, since such a context may belong to one 
particular topic. 
DIRECTOR: U R IN THE PLAYGROUND 
(indicating bullying starts) 
1. Lisa Murdoch: leave me alone! [angry] 
2. Mayid Rahim: WAT U GONNA DU? 
[neu] -> [angry] 
3. Mayid Rahim: SHUT UR FAT MOUTH 
[angry] 
4. Elise Brown: grrrrr [angry] 
5. Elise Brown: im telin da dinna lady! 
[threatening] 
6. Mayid Rahim: go on den [neutral] -> [an-
gry] 
7. Elise Brown: misssssssssssssssss [neu] 
8. Elise Brown: lol [happy] 
1485
9. Lisa Murdoch: mayid u gna gt banned 
[threatening] 
10. Mayid Rahim: BY HU [neu] -> [angry] 
The affect detection component detected that 
Lisa was ?angry? by saying ?leave me alone!?. It 
also sensed that Mayid was ?neutral? by saying 
?WAT U GONNA DU (what are you going to 
do)?? without consideration of context. From 
Rasp, we obtained that the input is a simplified 
question sentence (a linguistic contextual indi-
cator). Thus, it implies that it could be an emo-
tional situation caused by the previous context 
(e.g. previous input from Lisa) and the further 
processing for emotion prediction is activated. 
Since we don?t have an emotional context yet at 
this stage for Mayid (the very first input from 
Mayid after the director?s intervention), we 
cannot resort to the Markov chain and the dy-
namic algorithm currently to predict the affect. 
However, we could use the emotional context of 
other characters to predict the affect for Mayid?s 
current input since we believe that an emotional 
input from a character, especially from an op-
ponent character, has great potential to affect 
the emotions expressed by the current speaking 
character.  
In the most recent chat history, there is only 
one input from Lisa after the director?s interven-
tion, which implied ?anger?. Since Lisa and 
Mayid have a negative relationship (pre-defined 
by character profiles), then we predict Mayid 
currently experiences negative emotion. Since 
capitalizations have been used in Mayid?s input, 
we conclude that the affect implied in the input 
could be ?angry?. However, EMMA could be 
fooled if the affect histories of other characters 
fail to provide any useful indication for predic-
tion (e.g. if Lisa implied ?neutral? in the most 
recent input, the interpretation of the affect con-
veyed by Mayid would be still ?neutral?).  
EMMA also detected affect for the 3rd, 4th, 
and 5th user input in the above example (based 
on individual turn-taking) until it detected ?neu-
tral? again from the 6th input ?go on den (go on 
then)? from Mayid. Since it is an imperative 
mood sentence (a linguistic contextual indica-
tor), the input may imply a potential (emotional) 
response to the previous speaking character. 
Since we couldn?t obtain the affect embedded in 
the imperative purely based on the analysis of 
the input itself, the contextual processing is re-
quired. Thus the emotional context profile for 
Mayid is retrieved, i.e. [angry (the 2nd input) 
and angry (the 3rd input)]. The Markov chain is 
used to produce the possible emotional context 
based on the training data for each sub-theme 
for Mayid.  
The following are generated example emo-
tional profiles for the sub-theme ?Mayid starts 
bullying? for the Mayid character: 
1. T A A N A A [?threatening, angry, angry, 
neutral, angry and angry?]  
2. N A A A [?neutral, angry, angry, and an-
gry?] 
3. D A I A A A N A [?disapproval, angry, in-
sulting, angry, angry, angry, neutral, and an-
gry?] 
4. I A A N [?insulting, angry, angry and neu-
tral?] 
The dynamic algorithm is used to find the 
smallest edit distance between the test emotion-
al context [angry and angry] and the training 
and generated emotional context for the Mayid 
character for each sub-theme. In the above ex-
ample, the second and fourth emotional se-
quences have the smallest edit distance (dis-
tance = 2) to the test emotional context and the 
former suggests ?angry? as the affect conveyed 
in the current input (?go on den?) while the lat-
ter implies ?neutral? expressed in the current 
input. Thus we need to resort to the emotional 
context of other characters to justify the rec-
ommended affects. From the chatting log, we 
find that Lisa was ?angry? in her most recent 
input (the 1st input) while Elise was ?threaten-
ing? in her most recent input (the 5th input). 
Since the bully, Mayid, has a negative relation-
ships with Lisa (being ?angry?) and Elise (being 
?threatening?), the imperative input (?go on 
den?) may indicate ?angry? rather than ?neutral?. 
Therefore our processing adjusts the affect from 
?neutral? to ?angry? for the 6th input.  
In this way, by considering the linguistic con-
textual indicators, the potential emotional con-
text one character was in, relationships with 
others and recent emotional profiles of other 
characters, our affect detection component has 
been able to inference emotion based on context 
to mark up the rest of the above test example 
transcript (e.g. Mayid being ?angry? for the 10th 
input). However our processing could be fooled 
easily by various diverse ways for affective ex-
pressions and creative improvisation (test emo-
tional patterns not shown in the training and 
1486
generated sets). We intend to adopt better emo-
tion simulation tools, more linguistic hints, psy-
chological (context-based) emotional theories 
for further improvements. Also, our processing 
currently only focused on the school bullying 
scenario. We are on our way to extend the con-
text-based affect sensing to the Crohn?s disease 
scenario to further evaluate its efficiency. 
5 Evaluation and Conclusion 
We carried out user testing with 220 secondary 
school students from Birmingham and Darling-
ton schools for the improvisation of school bul-
lying and Crohn?s disease scenarios. Generally, 
our previous statistical results based on the col-
lected questionnaires indicate that the involve-
ment of the AI character has not made any sta-
tistically significant difference to users? en-
gagement and enjoyment with the emphasis of 
users? notice of the AI character?s contribution 
throughout. Briefly, the methodology of the 
testing is that we had each testing subject have 
an experience of both scenarios, one including 
the AI minor character only and the other in-
cluding the human-controlled minor character 
only. After the testing sessions, we obtained 
users? feedback via questionnaires and group 
debriefings. Improvisational transcripts were 
automatically recorded during the testing so that 
it allows further evaluation of the performance 
of the affect detection component.  
Therefore, we produce a new set of results for 
the evaluation of the updated affect detection 
component with metaphorical and context-based 
affect detection based on the analysis of some 
recorded transcripts of school bullying scenario. 
Generally two human judges (not engaged in 
any development stage) marked up the affect of 
150 turn-taking user input from the recorded 
another 4 transcripts from school bullying sce-
nario (different from those used for the training 
of Markov chains). In order to verify the effi-
ciency of the new developments, we provide 
Cohen?s Kappa inter-agreements for EMMA?s 
performance with and without the new devel-
opments for the detection of the most common-
ly used 12 affective states. In the school bully-
ing scenario, EMMA played a minor bit-part 
character (Lisa?s friend: Dave). The agreement 
for human judge A/B is 0.45. The inter-
agreements between human judge A/B and 
EMMA with and without the new developments 
are presented in Table 1.  
 
 Human 
Judge A 
Human 
Judge B 
EMMA (pre-
vious version) 
0.38 0.30 
EMMA (new 
version) 
0.40 0.32 
 
Table 1: Inter-agreements between human 
judges and EMMA with and without the new 
developments 
Although further work is needed, the new de-
velopments on metaphorical and contextual af-
fect sensing have improved EMMA?s perfor-
mance of affect detection in the test transcripts 
comparing with the previous version. 
The evaluation results indicated that most of 
the improvements (approximately 80%) are ob-
tained for negative affect detection based on the 
inference of context information. But there are 
still some cases: when the two human judges 
both believed that user inputs carried negative 
affective states (such as angry, threatening, dis-
approval etc), EMMA regarded them as neutral. 
One most obvious reason is that some of the 
previous pipeline processing (such as dealing 
with mis-spelling, acronyms etc, and syntactic 
processing from Rasp etc) failed to recover the 
standard user input or recognize the complex 
structure of the input which led to less interest-
ing and less emotional context and may affect 
the performance of contextual affect sensing. 
(The work of Sproat et al (2001) can point out 
helpful directions on this aspect.) Currently we 
achieved 69% average accuracy rate for the 
contextual affect sensing for the emotion inter-
pretation of all the human controlled characters 
in school bullying scenario. We also aim to ex-
tend the evaluation of the context-based affect 
detection using transcripts from other scenarios. 
Moreover, some of the improvements (nearly 
20%) in the updated affect sensing component 
are made by the metaphorical processing. How-
ever, since the test transcripts contained a very 
small number of metaphorical language pheno-
mena comparatively, we intend to use other re-
sources (e.g. The Wall Street Journal and other 
metaphorical databases (such as ATT-Meta, 
2008)) to further evaluate the new development 
on metaphorical affect sensing.   
1487
References 
ATT-Meta Project Databank: Examples of Usage of 
Metaphors of Mind. 2008. 
http://www.cs.bham.ac.uk/~jab/ATT-
Meta/Databank/.  
Aylett, A., Louchart, S. Dias, J., Paiva, A., Vala, M., 
Woods, S. and Hall, L.E. 2006. Unscripted Narra-
tive for Affectively Driven Characters. IEEE 
Computer Graphics and Applications 26(3). 42-
52.  
Briscoe, E. & Carroll, J. 2002. Robust Accurate Sta-
tistical Annotation of General Text. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation, Las Palmas, 
Gran Canaria. 1499-1504.  
Cavazza, M., Smith, C., Charlton, D., Zhang, L., 
Turunen, M. and Hakulinen, J. 2008. A ?Compa-
nion? ECA with Planning and Activity Modelling. 
In Proceedings of the 7th International Confe-
rence on Autonomous Agents and Multi-Agent 
Systems. Portugal, 1281-1284.  
Craggs, R. & Wood, M. 2004. A Two Dimensional 
Annotation Scheme for Emotion in Dialogue. In 
Proceedings of AAAI Spring Symposium: Explor-
ing Attitude and Affect in Text.  
Egges, A., Kshirsagar, S. & Magnenat-Thalmann, N. 
2003. A Model for Personality and Emotion Si-
mulation, In Proceedings of Knowledge-Based In-
telligent Information & Engineering Systems 
(KES2003), Lecture Notes in AI. Springer-Verlag: 
Berlin, 453-461.  
Esuli, A. and Sebastiani, F. 2006. Determining Term 
Subjectivity and Term Orientation for Opinion 
Mining. In Proceedings of EACL-06, 11th Confe-
rence of the European Chapter of the Association 
for Computational Linguistics, Trento, IT. 193-
200.  
Fainsilber, L. and Ortony, A. 1987. Metaphorical 
uses of language in the expression of emotions. 
Metaphor and Symbolic Activity, 2(4), 239-250. 
Fellbaum, C. 1998. WordNet, an Electronic Lexical 
Database. The MIT press.  
K?vecses, Z. 1998. Are There Any Emotion-Specific 
Metaphors? In Speaking of Emotions: Conceptua-
lization and Expression. Athanasiadou, A. and 
Tabakowska, E. (eds.), Berlin and New York: 
Mouton de Gruyter, 127-151.  
Liu, H. & Singh, P. 2004. ConceptNet: A practical 
commonsense reasoning toolkit. BT Technology 
Journal, Volume 22, Kluwer Academic Publish-
ers.  
Mateas, M. 2002. Interactive Drama, Art and Artifi-
cial Intelligence. Ph.D. Thesis. School of Com-
puter Science, Carnegie Mellon University.  
Rayson, P. 2003. Matrix: A statistical method and 
software tool for linguistic analysis through cor-
pus comparison. Ph.D. thesis, Lancaster Universi-
ty.  
Shaikh, M.A.M., Prendinger, H. & Mitsuru, I. 2007. 
Assessing sentiment of text by semantic depen-
dency and contextual valence analysis. In Pro-
ceeding of ACII 2007, 191-202.  
Sproat, R., Black, A., Chen, S., Kumar, S., Osten-
dorf, M. and Richards, C. 2001. Normalization of 
Non-standard Words. Computer Speech and Lan-
guage, 15(3), 287-333. 
Wallington, A.M., Agerri, R., Barnden, J.A., Lee, 
M.G. & Rumbell, T. 2008. Affect Transfer by 
Metaphor for an Intelligent Conversational Agent. 
In Procs of LREC 2008 Workshop on Sentiment 
Analysis: Emotion, Metaphor, Ontology and Ter-
minology, pp.107-113. Morocco. 
Zhang, L., Barnden, J.A. Hendley, R.J. Lee, M.G. 
Wallington, A.M. and Wen, Z. 2008a. Affect De-
tection and Metaphor in E-drama. Int. J. Continu-
ing Engineering Education and Life-Long Learn-
ing, Vol. 18, No. 2, 234-252.  
Zhang, L., Gillies, M. & Barnden, J.A. 2008b. EM-
MA: an Automated Intelligent Actor in E-drama. 
In Proceedings of International Conference on In-
telligent User Interfaces. 13th ?16th Jan 2008. 
Canary Islands, Spain. pp. 409-412.  
Zhang, L., Gillies, M., Dhaliwal, K., Gower, A., Ro-
bertson, D. & Crabtree, B. 2009. E-drama: Facili-
tating Online Role-play using an AI Actor and 
Emotionally Expressive Characters. International 
Journal of Artificial Intelligence in Education. 
Vol 19(1), pp.5-38. 
Zhe, X. & Boucouvalas, A.C. 2002. Text-to-Emotion 
Engine for Real Time Internet Communication. In 
Proceedings of International Symposium on 
Communication Systems, Networks and DSPs, 
Staffordshire University, UK, 164-168.  
 
 
1488
