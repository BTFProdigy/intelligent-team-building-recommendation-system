A Trigger Language Model-based IR System 
ZHANG Jun-lin   SUN Le   QU Wei-min   SUN Yu-fang  
 
Open System & Chinese Information Processing Center 
Institute of Software, The Chinese Academy of Sciences 
P.O.BOX 8718,Beijing 100080 
junlin01@iscas.cn
 
Abstract 
Language model based IR system proposed in 
recent 5 years has introduced the language 
model approach in the speech recognition 
area into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not 
the truth. Though statistical MT approach 
alleviates the situation by taking the 
synonymy factor into account, it never helps 
to judge the different meanings of the same 
word in varied context. In this paper we 
propose the trigger language model based IR 
system to resolve the problem. Firstly we 
compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the 
query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model 
to form the trigger language model based IR 
system. Experiments show that the 
performance of trigger language model based 
IR system has been improved greatly. The 
precision of trigger language model increased 
12% and recall increased nearly 10.8% 
compared with Ponte language model 
method. 
 
1 Introduction 
 
Using language models for information 
retrieval has been studied extensively 
recently(Jin et al2002 Lafferty and Zhai 2001 
Srikanth and Srihari 2002  Lavrenko and Croft 
2001 Liu and Croft 2002). The basic idea is to 
compute the conditional probability P(Q|D), i.e. 
the probability of generating a query Q given the 
observation of a document D. Several different 
methods have been applied to compute this 
conditional probability. In most approaches, the 
computation is conceptually decomposed into 
two distinct steps: (1) Estimating a document 
language model; (2) Computing the query 
likelihood using the estimated document model 
based on some query model. For example, Ponte 
and Croft emphasized the first step, and used 
several heuristics to smooth the Maximum 
Likelihood of the document language model, and 
assumed that the query is generated under a 
multivariate Bernoulli model (Ponte and Croft 
1998). The BBN method (Miller et al1999) 
emphasized the second step and used a two-state 
hidden Markov model as the basis for generating 
queries, which, in effect, is to smooth the MLE 
with linear interpolation, a strategy also adopted 
in Hiemstra and Kraaij (Hiemstra and  Kraaij 
1999). In Zhai and Lafferty (Zhai and  Lafferty 
2001), it has been found that the retrieval 
performance is affected by both the estimation 
accuracy of document language models and the 
appropriate modeling of the query, and a two 
stage smoothing method was suggested to 
explicitly address these two distinct steps. 
It?s not hard to see that the unigram 
language model IR method contains the 
following assumption: Each word appearing in 
the document set and query has nothing to do 
with any other word. Obviously this assumption 
is not true in reality. Though statistical MT 
approach (Berger and  Lafferty 1999 ) alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. Though 
the basic idea of using the triggered words to 
improve the performance of language model was 
proposed by Raymond almost 10 years ago 
(Raymond et al1993), Our method adopts a 
different approach for other objectivity in the IR 
field. Firstly we compute the mutual information 
of the words from training corpus and then 
design the algorithm to get the triggered words of 
the query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model to 
form the trigger language model based IR system. 
Experiments show that the performance of trigger 
language model based IR system has been 
improved greatly.   
In what follows, Section 2 describes trigger 
language model based IR system in detail. 
Section 3 is our evaluation about the model. 
Finally, Section 4 summarizes the work in this 
paper. 
2 Trigger Language Model based IR 
System 
 
2.1 Inter-relationship of Indexing Words 
  
In order to find out the inter-relationship of 
words in some specific context, we consider the 
co-occurring times of different words within 
fixed sized text window of the document. When 
the co-occurring time is large enough, we think 
that relationship is meaningful. Mutual 
Information is a common tool to be applied under 
this situation. So we compute the mutual 
information as following: 
 
)()()1(
),,(
)()(
)1(
),,(
),(
baw
wwba
w
b
w
a
ww
wba
ba
wNwNL
NLwwN
N
wN
N
wN
LN
LwwN
ww
???
?=
???
?
???
?????
?
???
?
??=?
             
(1) 
  
where  denotes the size of the vocabulary, 
 is the co-occurring times of 
word  and  within   sized window 
in training set.  is the count of the word 
 appearing in the training set and  is 
the count of word  appearing in the training 
set. 
wN
, wb L ),( a wwN
aw
aw
bw
(N
wL
)aw
bw
)( bwN
We use the corpus provided by IR task of 
NTCIR2 (NTCIR 2002) as the training set to 
compute the mutual information of words. This 
corpus contains nearly 100 thousands news 
articles encoding in BIG5 charset. We think the 
mutual information which is larger than 25 is 
meaningful. Considering the stop words in 
document or query are useless to represent the 
content, we remove 200 highest frequent words 
from the document before computation. Table 1 
shows some examples with higher mutual 
information. 
 
? ? 
(test) 
? ? ? (alphabet):1895 
??(rail):1353  
? ? (delimitation):758 
? ? ? ?
(windtunnel):473   
???(meter):421   
?? (test paper):403  
? ?
(missile) 
? ? ? ?
(antiaircraft):1063  
??(develop):708 
? ?? (long-range):472 
???(anti-tank):354 
?? 
(bribe) 
? ? (tax dodging):3462 
????(jobbery):2603 
????(FBI):1041 
???(voter):730 
??(zhanjiang):478 
???(Utah):427  
? ? ?
(truculency)
????(scrutator):710 
? ? ? ? (long-range 
missile):497  
????(terrorism):457 
? ? (biochemistry):390 
??(equipoise):327 
??(plague):334   
???(Bagdad):325   
 
      Table 1. Examples of Mutual Information 
2.2 Algorithm of Triggered Words by 
Query  
Generally speaking, a word always 
represents many different meanings and its exact 
meaning adopted in specific topic can be 
determined by the co-occurring words in its 
context. Different meaning of a word often lead 
to the different vocabulary set of related word. 
In order to find out the exact meaning of the 
words contained by the query in IR system, we 
design the algorithm to compute the triggered 
vocabularies of query. It is just these triggered 
words that show the exact meaning of the words 
in query in some specific context and help fix 
down the topic of query more clearly. The basic 
idea behind the algorithm is as following: By 
computing the mutual information, we can derive 
the relative words of a query word. All these 
words mean the semantically related vocabularies 
of the query word under different contexts. We 
propose that if the intersection of the derived 
related words of different words in query is not 
null, the words in the intersection is useful to 
judge the exact meaning of the words in query. 
At the same time, the more times an intersection 
word appears in related vocabulary set of 
different query word, the higher the weight of 
this word to fix down the topic of the query is. So 
we design the following algorithm to compute 
the triggered vocabulary set of query:     
 
Algorithm 1:Triggered vocabularies by query 
Input: Vocabulary set I of query word and its 
co-occurring words after removing the stop 
words in the query. 
},......,,......,,,{ 2211 ><><><><= nnii SqSqSqSqI
Output: Triggered vocabulary set T. 
 
Setp 1. Initialize the set ?=T . 
Setp 2. for(i =2;i<=n;i++) 
{ 
for(j=1;j<= ;j++) inC
{ 
2.1get the different 
combination },......,,,{ ,,2,2,1,1, ><><><= ijijjjjjj SqSqSqL
i
 
which contains  elements from set I ; 
2.2 if any vocabulary set )1(, ikS kj <=<  
in  contains no element, then we turn to 2.4 , 
otherwise we turn to 2.3; 
jL
2.3 Compute the intersection  of all 
vocabulary set  in . 
Here
jiT ,
< mw
)1(, ikS kj <=<
,......,, 221 ><> w
jL
>i },,{ 1, <=ji wT ??? ,
where
2
log i
w =? , ( iw <==<1 ). w?  is the 
word  weight decided by the length of ; jL
jiTT ,?=
w?
q
,,{ 11 ,,......, 22 <>>< mww ???
+? )|( jij dqp
><><=
=
other
ww
dq
q
ji
)
>m
)( i
cs
q
<wm,,,,{ 2211 ??
},.... )(Qli q
?
2.4 T , adopting the higher word 
weight during the merging process; 
} 
}  
Step 3. Output the triggered vocabulary set T ;    
      
2.3 Similarity Computation of Query and 
Document 
We use the similar strategy with Ponte 
language model method (Ponte and Croft 1998) 
to compute the similarity between the query and 
the document. That is, we firstly construct the 
simple language model according to the 
statistical information of vocabulary and then 
compute the generative probability of the query. 
The difference is that the trigger language model 
method takes the context information of a word 
into account. So we compute the triggered words 
set of query  according to algorithm 1.This 
way we get the triggered vocabulary set  
}><= mq wT . 
This set contains the words triggered by query 
and it is these triggered words that determine the 
exact meaning of the vocabularies in query 
among the several optional choices. This helps 
fix down the topic of query more clearly.   
Introducing the triggered words factor into the 
document language model, we can form the 
trigger language model based information 
retrieval system. 
The similarity of query and document can 
be computed as following: 
? ?
= =
=
)(
1
)(
1
()|(
Ql
i
dl
j
d
tf
CMQP   (2) 
??
??
?
???= Tddqdqp jjijji
0
}),......()(
1
)|( ?  (3) 
 
(1) ,......,{ 21 qqqQ =  denotes query 
and is the length of the query;  )(Ql
(2) denotes the trigger language model of 
document ;  
dM
d
(3) denotes a 
document in document set and is the length 
of the document; 
},....,....,{ )(21 dlj ddddd =
(l )d
(4) 
)(
)(
dl
df
C jj =
jd
 is the weight parameter of 
words  in a document. Here  means 
the account of the words  appearing in the 
document. 
)( jdf
jd
(5)  denotes the probability of  
being triggered by the document word .When 
2 words are same, the probability equals 1. If 
they are different and the word  belongs to 
the triggered vocabulary set of query, the 
probability equals the according parameter in the 
,otherwise the probability is 0? 
)|( ji dqp iq
jd
jd
qT
(6)
cs
qtf i )(
)iq
iq
is used for data smoothing; here 
 denotes times of query word  
appearing in document set and   denotes the 
total length of documents which contains the 
word . 
(tf iq
cs
 
3 Experiment Results  
3.1 Corpus 
 
The corpus we used to evaluate the 
performance of our proposed trigger language 
model IR system is the document set offered by 
the traditional Chinese Document set of NTCIR3 
for the IR task. The corpus consists of 381681 
news articles from Hong Kong and Taiwan with 
varied topics. After the word segmentation, the 
document set contains 150700953 words. Among 
them,127519 different words are the entries of 
the vocabulary. The average length of each 
document is 394. 
The 50 queries offered by NTCIR3 IR task 
are contained in a XML file and each query 
consists of following elements: Topic 
Number(NUM),Topic Title(TITLE),Topic 
question(DESC),Topic Narrative(NARR) and 
Topic Concepts(CONC). In order to make it 
easer to compare the performance of the different 
IR methods, we adopt the Topic Question field as 
the query and regard the top 1000 retrieval 
documents as the standard result of the 
experiment.    
 
3.2 Analysis of Experiment Results 
We design 3 relative experiments to 
evaluate the trigger language model IR method: 
vector space model, Ponte language model based 
method and the trigger language model approach. 
Precision and recall are two main evaluation 
parameters. As for the trigger model IR method, 
the optimal size of the text window is 20 content 
words and the mutual information over 25 is 
regarded as the meaningful information. 
Experiment results can be seen in table 2. 
The data of column %  in table 2 shows 
the performance improvement of Ponte language 
model compared with vector space model. The 
data tells us that the precision of language model 
based method increased 10% and recall increased 
nearly 13.7%. The data of column %?  in table 
2 shows the performance improvement of trigger 
language model compared with Ponte language 
model method. From the data we can see that the 
precision of trigger language model increased 
12% and recall increased nearly 10.8%. We can 
draw the conclusion that the trigger language 
model has improved the performance greatly. 
The performance comparison can be showed 
more clearly in figure 1. 
1?
2
    
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
Recall
Pr
ec
is
io
n
tfidf
Ponte
Language
Model
Trigger
Language
Model
 
 Figure 1. Precision-Recall of 3 methods 
 
  Tfidf Lm(ponte) Trigger lm % 1?  %  2?
Relevant: 3284 3284 3284 ----
- 
----
- 
Rel.ret: 1843 2096 2322 13.7 10.8 
Precision:      
0. 00 
0. 10 
0. 20 
0. 30 
0. 40 
0. 50 
0. 60 
0. 70 
0. 80 
0. 90 
1. 00 
Avg: 
0. 6016 
0. 4607 
0. 3812 
0. 3336 
0. 2738 
0. 2495 
0. 2179 
0. 1566 
0. 0978 
0. 0389 
0. 0019 
0.2377 
0.6109 
0.4844 
0.4123 
0.3757 
0.3255 
0.2854 
0.2313 
0.1716 
0.1041 
0.0474 
0.0025 
0.2610 
0. 7537 
0. 5314 
0. 4541 
0. 4094 
0. 3648 
0. 3237 
0. 2538 
0. 2011 
0. 1153 
0. 0435 
0. 0055 
0. 2933 
+2 
+5 
+8 
+12 
+18 
+14 
+6 
+9 
+6 
+21 
+31 
+10 
+23 
+10 
+10 
+9 
+12 
+13 
+9 
+17 
+10 
-8 
+120 
+12 
 
Table 2. Experiment results
 
4  Conclusion  
Language model based IR system proposed 
in recent 5 years has introduced the language 
model approach in the speech recognition area 
into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not the 
truth. Though statistical MT approach alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. . Firstly 
we compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the query 
in order to fix down the topic of query more 
clearly. We introduce the relative parameters into 
the document language model to form the trigger 
language model based IR system. Experiments 
show that the performance of trigger language 
model based IR system has been improved 
greatly. 
Acknowledgement 
This work is supported by Beijing New Star Plan 
of Technology & Science(NO.H020820790130) 
and the National Science Fund of China under 
contact 60203007. 
References  
Berger A. and  Lafferty J. (1999). Information 
retrieval as statistical translation. In Proceedings of 
SIGIR ?99. pp. 222-229. 
 
Jin R., Hauptmann A.G.  and Zhai C.(2002) Title 
Language Model for Information Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval. 
 
Hiemstra D. and  Kraaij W. (1999), Twenty-One at 
TREC-7: ad-hoc and cross-language track, In 
Proceedings of the seventh Text Retrieval 
Conference TREC-7, NIST Special Publication 
500-242, pages 227-238, 1999. 
 
Lafferty J. and Zhai. C. (2001) Document language 
models,query models and risk minimization for 
information retrieval. In Proceedings of the 24th 
ACM SIGIR Conference,pp.111-119. 
 
Lavrenko,V., and Croft,W.B.(2001) .Relevance based 
language models. In Proceedings of the 24th ACM 
SIGIR Conference.pp.120-127. 
 
Liu,X. and Croft,W.B.(2002).Passage Retrieval 
Based on Language Models. In Proceedings of the 
11th  International Conference on Information and 
Knowledge Management. Pp.375-382 
 
Miller D.,  Leek T.  and Schwartz  R. M. (1999). 
A hidden Markov model information retrieval 
system. Proceedings of SIGIR?1999, pp. 214-222. . 
 
NTCIR Workshop 
(research.nii.ac.jp/ntcir/index-en.html) 
  
Ponte J. and Croft W. B. (1998). A language 
modeling approach to information retrieval. In 
Proceedings of SIGIR? 1998, pp. 275-281. 
 
Raymond Lau, Roni Rosenfeld and Salim 
Roukos(1993) Trigger-based Language Models: A 
Maximum Entropy Apporach. Proceedings ICASSP 
'93, Minneapolis, MN, pp. II-45 - II-48. 
 
Srikanth M. and  Srihari. R(2002). Biterm 
Language Models for Document Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval.  
 
Zhai C. and Lafferty J.(2001). A study of smoothing 
methods for language models applied to ad hoc 
information retrieval. In Proceeding of SIGIR?01, 
2001, pp. 334-342. 
 
 
 
A Structured Prediction Approach for Statistical Machine Translation
Dakun Zhang*          Le Sun?          Wenbo Li* 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{dakun04,liwenbo02}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
We propose a new formally syntax-based 
method for statistical machine translation. 
Transductions between parsing trees are 
transformed into a problem of sequence 
tagging, which is then tackled by a search-
based structured prediction method. This 
allows us to automatically acquire transla-
tion knowledge from a parallel corpus 
without the need of complex linguistic 
parsing. This method can achieve compa-
rable results with phrase-based method 
(like Pharaoh), however, only about ten 
percent number of translation table is used. 
Experiments show that the structured pre-
diction approach for SMT is promising for 
its strong ability at combining words. 
1 Introduction 
Statistical Machine Translation (SMT) is attract-
ing more attentions than rule-based and example-
based methods because of the availability of large 
training corpora and automatic techniques. How-
ever, rich language structure is difficult to be inte-
grated in the current SMT framework. Most of the 
SMT approaches integrating syntactic structures 
are based on probabilistic tree transducers (tree-
to-tree model). This leads to a large increase in the 
model complexity (Yamada and Knight 2001; 
Yamada and Knight 2002; Gildea 2003; Galley et 
al. 2004; Knight and Graehl 2005; Liu et al 2006). 
However, formally syntax-based methods propose 
simple but efficient ways to parse and translate 
sentences (Wu 1997; Chiang 2005). 
In this paper, we propose a new model of SMT 
by using structured prediction to perform tree-to-
tree transductions. This model is inspired by Sa-
gae and Lavie (2005), in which a stack-based rep-
resentation of monolingual parsing trees is used. 
Our contributions lie in the extension of this rep-
resentation to bilingual parsing trees based on 
ITGs and in the use of a structured prediction 
method, called SEARN (Daum? III et al 2007), to 
predict parsing structures. 
Furthermore, in order to facilitate the use of 
structured prediction method, we perform another 
transformation from ITG-like trees to label se-
quence with the grouping of stack operations. 
Then the structure preserving problem in transla-
tion is transferred to a structured prediction one 
tackled by sequence labeling method such as in 
Part-of-Speech (POS) tagging. This transforma-
tion can be performed automatically without com-
plex linguistic information. At last, a modified 
search process integrating structure information is 
performed to produce sentence translation. Figure 
1 illustrates the process flow of our model. Be-
sides, the phrase extraction is constrained by ITGs. 
Therefore, in this model, most units are word 
based except that we regard those complex word 
alignments as a whole (i.e. phrase) for the simplic-
ity of ITG-like tree representations. 
B ilingual S en tences
G IZ A + +  T ra in ing
(B id irec tiona l)
W ord  A lignm en ts
(g row -d iag -fina l)
S truc tu red  Info rm ation
(T ra in ing  by  S E A R N )
L anguage  M odel
M ono lingual
S en tences
Search  e*
M ax im ize  P r(e)*P r(f|e )
Inpu t
Source L anguage
S en tence
O utpu t
T arge t L anguage
 S en tence
S tack -based  O pera tions
T rans la tion  M odel
IT G -like  T rees
 
Figure 1: Chart of model framework 
The paper is organized as follows: related work 
is show in section 2. The details of the transforma-
649
tion from word alignments to structured parsing 
trees and then to label sequence are given in sec-
tion 3. The structured prediction method is de-
scribed in section 4. In section 5, a beam search 
decoder with structured information is described. 
Experiments are given for three European lan-
guage pairs in section 6 and we conclude our pa-
per with some discussions. 
2 Related Work 
This method is similar to block-orientation model-
ing (Tillmann and Zhang 2005) and maximum 
entropy based phrase reordering model (Xiong et 
al. 2006), in which local orientations (left/right) of 
phrase pairs (blocks) are learned via MaxEnt clas-
sifiers. However, we assign shift/reduce labeling 
of ITGs taken from the shift-reduce parsing, and 
classifier is learned via SEARN. This paper is 
more elaborated by assigning detailed stack-
operations. 
The use of structured prediction to SMT is also 
investigated by (Liang et al 2006; Tillmann and 
Zhang 2006; Watanabe et al 2007). In contrast, 
we use SEARN to estimate one bilingual parsing 
tree for each sentence pair from its word corre-
spondences. As a consequence, the generation of 
target language sentences is assisted by this struc-
tured information. 
Turian et al (2006) propose a purely discrimi-
native learning method for parsing and translation 
with tree structured models. The word alignments 
and English parse tree were fed into the GenPar 
system (Burbank et al 2005) to produce binarized 
tree alignments. In our method, we predict tree 
structures from word alignments through several 
transformations without involving parser and/or 
tree alignments. 
3 Transformation 
3.1 Word Alignments and ITG-like Tree 
First, following Koehn et al (2003), bilingual sen-
tences are trained by GIZA++ (Och and Ney 2003) 
in two directions (from source to target and target 
to source). Then, two resulting alignments are re-
combined to form a whole according to heuristic 
rules, e.g. grow-diag-final. Second, based on the 
word alignment matrix, one unique parsing tree 
can be generated according to ITG constraints 
where the ?left-first? constraint is posed. That is to 
say, we always make the leaf nodes as the right 
sons as possible as they can. Here we present two 
basic operations for mapping tree items, one is in 
order and the other is in reverse order (see Figure 
2). Basic word alignments are in (a), while (b) is 
their corresponding alignment matrix. They can be 
described using ITG-like trees (c). 
f1 f1       f2
e1        *
e2                  * f1/e1 f2/e2
(1a) (1b) (1c)
f1       f2
e1                  *
e2        * f1/e2 f2/e1
(2a) (2b) (2c)
f1/e1 S
f2/e2 S,R+
(1d)
f1/e2 S
f2/e1 S,R-
(2d)
f2
f1 f2
e1 e2
e1 e2
 
Figure 2: Two basic representations for tree items 
 
Figure 3: ?inside-out? transpositions (a) and (b) with two 
typical complex sequences (c) and (d). In (c) and (d), word 
correspondence f2-e2 is also extracted as sub-alignments. 
The two widely known situations that cannot be 
described by ITGs are called ?inside-out? transpo-
sitions (Figure 3 a & b). Since they cannot be de-
composed in ITGs, we consider them as basic 
units. In this case, phrase alignment is used. In our 
model, more complex situations exist for the word 
correspondences are generated automatically from 
GIZA++. At the same time, we also keep the sub-
alignments in those complex situations in order to 
extend the coverage of translation options. The 
sub-alignments are restricted to those that can be 
described by the two basic operations. In other 
words, for our ITG-like tree, the nodes are mostly 
word pairs, except some indecomposable word 
sequences pairs. Figure 3 shows four typical com-
plex sequences viewed as phrases. 
Therefore, our ITG-like trees take some phrase 
alignments into consideration and we also keep 
the sub-alignments in these situations. Tree items 
in our model are restricted to minimum constitu-
ents for the simplicity of parsing tree generation. 
Then we extract those word pairs from tree items, 
instead of all the possible word sequences, as our 
translation table. In this way, we can greatly re-
duce the number of translation pairs to be consid-
eration. 
650
3.2 SHIFT and REDUCE Operations 
Sagae and Lavie (2005) propose a constituency-
based parsing method to determine sentence de-
pendency structures. This method is simple and 
efficient, which makes use of SHIFT and RE-
DUCE operations within a stack framework. This 
kind of representations can be easily learned by a 
classifier with linear time complexity. 
In their method, they build a parse tree of a sen-
tence one word at a time just as in a stack parser. 
At any time step, they either shift a new word on 
to the stack, or reduce the top two elements on the 
stack into a new non-terminal. 
Sagae and Lavie?s algorithms are designed for 
monolingual parsing problem. We extend it to 
represent our ITG-like tree. In our problem, each 
word pairs can be viewed as tree items (nodes). 
To handle our tree alignment problem, we need to 
define two REDUCE operations: REDUCE in 
order and REDUCE in reverse order. We define 
these three basic operations as follows: 
? S: SHIFT - push the current item onto the 
stack. 
? R+: REDUCE in order - pop the first two 
items from the stack, and combine them in 
the original order on the target side, then 
push back. 
? R-: REDUCE in reverse order - pop the 
first two items from the stack, and combine 
them in the reverse order on the target side, 
then push back. 
Using these operators, our ITG-like tree is 
transformed to serial stack operations. In Figure 2, 
(d) is such a representation for the two basic 
alignments. Therefore, the structure of word 
aligned sentences can be transformed to an opera-
tion sequence, which represents the bilingual pars-
ing correspondences. 
After that, we attach these operations to each 
corresponding tree item like a sequence labeling 
problem. We need to perform another ?grouping? 
step to make sure only one operation is assigned 
to each item, such as ?S,R+?, ?S,R-,R+?, etc. 
Then, those grouped operations are regarded as a 
whole and performed as one label. The number of 
this kind of labels is decided by the training cor-
pus1. Having defined such labels, the prediction of 
                                                 
1 This set of labels is quite small and only 16 for the French-
English training set with 688,031 sentences. 
tree structures is transformed to a label prediction 
one. That is, giving word pairs as input, we trans-
form them to their corresponding labels (stack 
operations) in the output. At the same time, tree 
transductions are encoded in those labels. Once all 
the ?labels? are performed, there should be only 
one element in the stack, i.e. the generating sen-
tence translation pairs. See Appendix A for a more 
complete example in Chinese-English with our 
defined operations. 
Another constraint we impose is to keep the 
least number of elements in stack at any time. If 
two elements on the top of the stack can be com-
bined, we combine them to form a single item. 
This constraint can avoid having too many possi-
ble operations for the last word pair, which may 
make future predictions difficult. 
4 Structured Prediction 
SEARN is a machine learning method proposed 
recently by Daum? III et al (2007) to solve struc-
tured prediction problems. It can produce a high 
prediction performance without compromising 
speed, simplicity and generality. By incorporating 
the search and learning process, SEARN can solve 
the complex problems without having to perform 
explicit decoding any more. 
In most cases, a prediction of input x in domain 
X into output y in domain Y, like SVM and deci-
sion trees, cannot keep the structure information 
during prediction. SEARN considers this problem 
as a cost sensitive classification one. By defining 
features and a loss function, it performs a cost 
sensitive learning algorithm to learn predictions. 
During each iteration, the optimal policy (decided 
by previous classifiers) generates new training 
examples through the search space. These data are 
used to adjust performance for next classifier. 
Then, iterations can keep this algorithm to per-
form better for prediction tasks. Structures are 
preserved for it integrates searching and learning 
at the same time.  
4.1 Parsing Tree Prediction 
For our problem, using SEARN to predict the 
stack-based ITG-like trees, given word alignments 
as input, can benefit from the advantages of this 
algorithm. With the structured learning method, 
we can account for the sentence structures and 
their correspondence between two languages at 
651
the same time. Moreover, it keeps the translating 
structures from source to target. 
As we have transformed the tree-to-tree transla-
tion problem into a sequence labeling one, all we 
need to solve is a tagging problem similar to a 
POS tagging (Daum? III et al 2006). The input 
sequence x is word pairs and output y is the group 
of SHIFT and REDUCE operations. For sequence 
labeling problem, the standard loss function is 
Hamming distance, which measures the difference 
between the true output and the predicting one: 
?=
t
tt yyyyHL )?,()?,( ?                 (1) 
where ? is 0 if two variables are equal, and 1 oth-
erwise. 
5 Decoder 
We use a left-to-right beam search decoder to find 
the best translation given a source sentence. Com-
pared with general phrase-based beam search de-
coder like Pharaoh (Koehn 2004), this decoder 
integrates structured information and does not 
need distortion cost and other costs (e.g. future 
costs) any more. Therefore, the best translation 
can be determined by: 
})()|({maxarg* )(elengthlm
e
epefpe ?=     (2) 
where ? is a factor of word length penalty. Simi-
larly, the translation probability  can be 
further decomposed into: 
)|( efp
?=
i
ii efefp )|()|( ?                  (3) 
and )|( ii ef?  represents the probability distribu-
tion of word pairs. 
Instead of extracting all possible phrases from 
word alignments, we consider those translation 
pairs from the nodes of ITG-like trees only. Like 
Pharaoh, we calculate their probability as a com-
bination of 5 constituents: phrase translation prob-
ability (in both directions), lexical translation 
probability (in both directions) and phrase penalty 
(default is set at 2.718). The corresponding weight 
is trained through minimum error rate method 
(Och 2003). Parameters of this part can be calcu-
lated in advance once tree structures are generated 
and can be stored as phrase translation table. 
5.1 Core Algorithm 
Another important question is how to preserve 
sentence structures during decoding. A left-to-
right monotonous search procedure is needed. 
Giving the source sentence, word translation can-
didates can be determined according to the trans-
lation table. Then, several rich features like cur-
rent and previous source words are extracted 
based on these translation pairs and source sen-
tence. After that, our structured prediction learn-
ing method will be used to predict the output ?la-
bels?, which produces a bilingual parsing tree. 
Then, a target output will be generated for the cur-
rent partial source sentence as soon as bilingual 
parsing trees are formed. The output of this part 
therefore contains syntactic information for struc-
ture. 
For instance, given the current source partial 
like ?f1 f2?, we can generate their translation 
word pair sequences with the translation table, 
like ?f1/e1 f2/e2?, ?f1/e3 f2/e4? and so on. The 
corresponding features are then able to be decided 
for the next predicting process. Once the output 
predictions (i.e. stack operations) are decided, the 
bilingual tree structures are formed at the same 
time. As a consequence, results of these opera-
tions are the final translations which we really 
need. 
At each stage of translation, language model 
parameters can be added to adjust the total costs 
of translation candidates and make the pruning 
process reasonable. The whole sentence is then 
processed by incrementally constructing the trans-
lation hypotheses. Lastly, the element in the last 
beam with the minimum cost is the final transla-
tion. In general, the translation process can be de-
scribed in the following way: 
 
5.2 Recombining and Pruning 
Different translation options can combine to form 
the same fragment by beam search decoder. Re-
combining is therefore needed here to reduce the 
search space. So, only the one with the lowest cost 
is kept when several fragments are identical. This 
recombination is a risk-free operation to improve 
searching efficiency. 
Another pruning method used in our system is 
histogram pruning. Only n-best translations are 
652
allowed for the same source part in each stack (e.g. 
n=100). In contrast with traditional beam search 
decoder, we generate our translation candidates 
from the same input, instead of all allowed word 
pairs elsewhere. Therefore the pruning is much 
more reasonable for each beam. There is no rela-
tive threshold cut off compared with Pharaoh. 
In the end, the complexities for decoding are 
the main concern of our method. In practice, how-
ever, it will not exceed the  (m for 
sentence length, N for stack size and Tn for al-
lowed translation candidates). This is based on the 
assumption that our prediction process (tackled by 
SEARN) is fed with three features (only one for-
mer item is associated), which makes it no need of 
full sentence predictions at each time. 
)**( TnNmO
6 Experiment 
We validate our method using the corpus from the 
shared task on NAACL 2006 workshop for statis-
tical machine translation2. The difference of our 
method lies in the framework and different phrase 
translation table. Experiments are carried on all 
the three language pairs (French-English, Ger-
man-English and Spanish-English) and perform-
ances are evaluated by the providing test sets. Sys-
tem parameters are adjusted with development 
data under minimum error rate training. 
For SEARN, three features are chosen to use: 
the current source word, the word before it and the 
current target word. As we do not know the real 
target word order before decoding, the corre-
sponding target word?s position cannot be used as 
features. Besides, we filter the features less than 5 
times to reduce the training complexities. 
The classifier we used in the training process is 
based on perceptron because of its simplicity and 
performance. We modified Daum? III?s script3 to 
fit our method and use the default 5 iterations for 
each perceptron-based training and 3 itertaions for 
SEARN. 
6.1 Results for different language pairs 
The  final  results  of  our  system,  named Amasis, 
and baseline system Pharaoh (Koehn and Monz 
2006) for three language pairs are listed in Table 1. 
The last three lines are the results of Pharaoh with 
phrase length from 1 to 3. However, the length of 
                                                 
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.cs.utah.edu/~hal/searn/SimpleSearn.tgz 
0
5000
10000
15000
20000
k
Pharaoh 15724573 12667210 19367713
Amasis 1522468 1715732 1572069
F-E G-E S-E
 
Figure 4: Numbers of translation table 
0.0%
5.0%
10.0%
15.0%
20.0%
25.0%
30.0%
35.0%
40.0%
Pharaoh 3.7% 5.1% 3.5%
Amasis 32.2% 33.0% 36.4%
F-E G-E S-E
 
Figure 5: Percent of single word translation pairs (only one 
word in the source side) 
F-E G-E S-E  
In Out In Out In Out
Amasis 27.44 18.41 23.02 15.97 27.51 23.35
Pharaoh1 20.54 14.07 17.53 12.13 23.23 20.24
Pharaoh2 27.71 19.41 23.36 15.77 28.88 25.28
Pharaoh3 30.01 20.77 24.40 16.58 30.58 26.51
Table 1: BLEU scores for different language pairs. In - In-
domain test, Out - Out-of-domain test. 
 
phrases for Amasis is determined by ITG-like tree 
nodes and there is no restriction for it. 
Even without producing higher BLEU scores 
than Pharaoh, our approach is still interesting for 
the following reasons. First, the number of phrase 
translation pairs is greatly reduced in our system. 
The ratio of translation table number in our 
method (Amasis) to Pharaoh, for French-English 
is 9.68%, for German-English is 13.54%, for 
Spanish-English is 8.12% (Figure 4). This means 
that our method is more efficient at combining 
words and phrases during translation. The reasons 
for the different ratio for the three languages are 
not very clear, maybe are related to the flexibility 
of word order of source language. Second, we 
count the single word translation pairs (only one 
word in the source side) as shown in Figure 5. 
There are significantly more single word transla-
tions in our method. However, the translation 
quality can be kept at the same level under this 
circumstance. Third, our current experimental re-
sults are produced with only three common fea-
tures (the corresponding current source and target 
word and the last source one) without any linguis-
tics information. More useful features are ex-
pected to be helpful like POS tags. Finally, the 
performance can be further improved if we use a 
more powerful classifier (such as SVM or ME) 
with more iterations. 
653
7 Conclusion 
Our method provides a simple and efficient way 
to solve the word ordering problem partially 
which is NP-hard (Knight 1999). It is word based 
except for those indecomposable word sequences 
under ITGs. However, it can achieve comparable 
results with phrase-based method (like Pharaoh), 
while much fewer translation options are used. 
For the structure prediction process, only 3 com-
mon features are preserved and perceptron-based 
classifiers are chosen for the use of simplicity. We 
argue that this approach is promising when more 
features and more powerful classifiers are used as 
Daum? III et al (2007) stated. 
Our contributions lie in the integration of struc-
ture prediction for bilingual parsing trees through 
serial transformations. We reinforce the power of 
formally syntax-based method by using structured 
prediction method to obtain tree-to-tree transduc-
tions by the transforming from word alignments to 
ITG-like trees and then to label sequences. Thus, 
the sentence structures can be better accounted for 
during translating. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108. We would like to 
thank anonymous reviewers for their detailed comments. 
Appendix A. A Complete Example in Chinese-English 
with Our Defined Operations 
Word alignments 
 
ITG-like tree 
 
SHIFT-REDUCE label sequence 
??/a   S 
??/to learn about  S 
??/Chinese  S,R+ 
??/music   S,R+ 
?/?   S,R+ 
? ?/great   S 
?/?   S,R+ 
??/way   S,R+,R-,R+ 
Stack status when operations finish 
?? ?? ?? ?? ? ? ? ? ??  
/ a great way to learn about Chinese music 
References 
A. Burbank, M. Carpuat, et al 2005. Final Report of the 2005 
Language Engineering Workshop on Statistical Machine 
Translation by Parsing. Johns Hopkins University 
D. Chiang. 2005. A Hierarchical Phrase-Based Model for 
Statistical Machine Translation. In ACL, pages 263-270. 
M. Galley, M. Hopkins, et al 2004. What's in a translation 
rule? In HLT-NAACL, Boston, MA. 
D. Gildea. 2003. Loosely Tree-Based Alignment for Machine 
Translation. In ACL, pages 80-87, Sapporo, Japan. 
H. Daum? III, J. Langford, et al 2007. Search-based Struc-
tured Prediction. Under review by the Machine Learning 
Journal. http://pub.hal3.name/daume06searn.pdf. 
H. Daum? III, J. Langford, et al 2006. Searn in Practice. 
http://pub.hal3.name/daume06searn-practice.pdf.  
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational Linguis-
tics 25(4): 607-615. 
K. Knight and J. Graehl. 2005. An Overview of Probabilistic 
Tree Transducers for Natural Language Processing. In 
CICLing, pages 1-24. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Models. In 
Proc. of AMTA, pages 115-124. 
P. Koehn and C. Monz. 2006. Manual and Automatic Evalua-
tion of Machine Translation between European Languages. 
In Proc. on the Workshop on Statistical Machine Transla-
tion, pages 102-121, New York City. 
P. Koehn, F. J. Och, et al 2003. Statistical Phrase-Based 
Translation. In HLT-NAACL, pages 127-133. 
P. Liang, A. Bouchard, et al 2006. An End-to-End 
Discriminative Approach to Machine Translation. In ACL. 
Y. Liu, Q. Liu, et al 2006. Tree-to-String Alignment Tem-
plate for Statistical Machine Translation. In ACL. 
F. J. Och. 2003. Minimum Error Rate Training in Statistical 
Machine Translation. In ACL, pages 160-167. 
F. J. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computational 
Linguistics 29(1): 19-51. 
K. Sagae and A. Lavie. 2005. A Classifier-Based Parser with 
Linear Run-Time Complexity. In IWPT, pages 125-132. 
C. Tillmann and T. Zhang. 2005. A Localized Prediction 
Model for Statistical Machine Translation. In ACL. 
C. Tillmann and T. Zhang. 2006. A Discriminative Global 
Training Algorithm for Statistical MT. in ACL. 
J. Turian, B. Wellington, et al 2006. Scalable Discriminative 
Learning for Natural Language Parsing and Translation. In 
Proceedings of NIPS, Vancouver, BC. 
T. Watanabe, J. Suzuki, et al 2007. Online Large-Margin 
Training for Statistical Machine Translation. In EMNLP. 
D. Wu. 1997. Stochastic Inversion Transduction Grammars 
and Bilingual Parsing of Parallel Corpora. Computational 
Linguistics 23(3): 377-404. 
D. Xiong, Q. Liu, et al 2006. Maximum Entropy Based 
Phrase Reordering Model for Statistical Machine Transla-
tion. In ACL, pages 521-528. 
K. Yamada and K. Knight. 2001. A Syntax-based Statistical 
Translation Model. In ACL, pages 523-530. 
K. Yamada and K. Knight. 2002. A Decoder for Syntax-
based Statistical MT. In ACL, pages 303-310. 
654
Two Step Chinese Named Entity Recognition Based on Conditional 
Random Fields Models 
Yuanyong Feng*          Ruihong Huang*         Le Sun? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{comerfeng,ruihong2}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, part-
of-speech and a small-vocabulary-
character-lists feature and heristic post-
process rules for MSRA NER open track 
under the framework of Conditional Ran-
dom Fields (CRFs) model. 
1  Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et al, 
2001) framework. It integrates multiple features 
based on single Chinese character or space sepa-
rated ASCII words. The early designed system 
(Feng et al, 2006) is used for the MSRA NER 
open track this year. The output of an external 
part-of-speech tagging tool and some carefully 
collected small-scale-character-lists are used as 
open knowledge. Some post process steps are also 
applied to complement the local limitation in 
model?s feature engineering. 
The remaining of this paper is organized as fol-
lows. Section 2 introduces Conditional Random 
Fields model. Section 3 presents the details of our 
system on Chinese NER integrating multiple fea-
tures. Section 4 describes the post-processings 
based on some heuristic rules. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2  Conditional Random Fields Model 
Conditional random fields are undirected graphical 
models for calculating the conditional probability 
for output vertices based on input ones. While 
sharing the same exponential form with maximum 
entropy models, they have more efficient proce-
dures for complete, non-greedy finite-state infer-
ence and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines the 
corresponding state sequence s? probability as fol-
lows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f
Z
?? ?
=
= ??
o
s o os s t
 
(1)
Where ? is the model parameter set, Zo is the nor-
malization factor over all state sequences, fk is an 
arbitrary feature function, and ?k is the learned fea-
ture weight. A feature function defines its value to 
be 0 in most cases, and to be 1 in some designated 
cases. For example, the value of a feature named 
?MAYBE-SURNAME? is 1 if and only if st-1 is 
OTHER, st is PER, and the t-th character in o is a 
common-surname. 
The inference and training procedures of CRFs 
can be derived directly from those equivalences in 
HMM. For instance, the forward variable ?t(si) 
defines the probability that state at time t being si 
at time t given the observation sequence o. As-
sumed that we know the probabilities of each 
possible value si for the beginning state  ?0(si), 
then we have 
1( ) ( )exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?=? ? o (2)
120
Sixth SIGHAN Workshop on Chinese Language Processing
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3  Chinese NER Using CRFs Model Inte-
grating Multiple Features 
Besides the text feature(TXT), simplified part-of-
speech (POS) feature, and small-vocabulary-
character-lists (SVCL) feature, which use in the 
early system (Feng et al, 2006), some new fea-
tures such as word boundary, adjoining state bi-
gram ? observation and early NE output are also 
combined under the unified CRFs framework. 
The text feature includes single Chinese charac-
ter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Unlike those in 
the earyly system, the POS tag set are merged into 
9 categories from the criterion of modern Chinese 
corpora construction (Yu, 1999), which contains 
39 tags.  
The third type of features are derived from the 
small-vocabulary-character lists which are essen-
tially same as the ones used in last year except 
with some additional items. Some examples of this 
list are given in Table 1. 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is 
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail charac-ter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail charac- ? ? ? ? ?, , , , , 
ter ?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
The fourth type of feature is word boundary. We 
use the B, I, E, U, and O to indicate Begining, In-
ner, Ending, and Uniq part of, or outside of a word 
given a word segmentation. The O case occurs 
when a token, for example the charactor ?&?, is 
ignored by the segmentator. We do not combine 
the boundary information with other features be-
cause we argue it is very limited and may cause 
errors. 
The last type of features is bigram state com-
bined with observations. We argue that observa-
toin (mainly is of named entity derived by early 
system or character text itself) and state transition 
are not conditionally independent and entails dedi-
cate considerings. 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an obser-
vation sequence is feed into the model. 
Each token state is a combination of the type of 
the named entity it belongs to and the boundary 
type it locates within. The entity types are person 
name (PER), location name (LOC), organization 
name (ORG), date expression (DAT), time expres-
sion (TIM), numeric expression (NUM), and not 
named entity (OTH). The boundary types are sim-
ply Beginning, Inside, and Outside (BIO).  
All above types of features are extracted from a 
varying length window. The main criteria is that 
wider window with smaller feature space and nar-
row window when the observation features are in a 
large range. 
The main feature set is shown the following. 
Character Texts(TXT): 
TXT-2, TXT-1, TXT0, TXT1, TXT2, 
TXT-1TXT0,  TXT1TXT0, TXT1TXT2 
simplified part-of-speech (POS): 
unigram: POS-4 ~ POS4 
121
Sixth SIGHAN Workshop on Chinese Language Processing
small-vocabulary-character-lists (SVCL): 
unigram: SVCL-2 ~ SVCL7 
bigram:SVCL0SVCL1, SVCL1SVCL2 
Word Boundary (WB): 
WB-1,WB0,WB1 
Named Entity (NE): 
unigram: NE-4 ~ NE4 
bigram:NE-2NE-1, NE-1NE0, NE0NE1, NE1NE2 
State Bigram (B) ? Observation: 
B, B-TXT0, B-NE-1, B-NE0, B-NE1 
Table 2.  The Main Feature Set 
4  Post Processing on Heuristic Rules 
Observing from the evaluation, our model has 
worse performance on ORG and PER than LOC. 
Furthermore, the analysis of the errors tells us that 
they are hard to be tackled with the improvement 
of the model itself. Therefore, we decided to do 
some post-process to correct certain types of tag-
ging errors of the unified model mainly concerning 
the two kinds of entities, ORG and PER.  
At the training phrase, we compare the tagging 
output of the model with the correct tags and col-
lect the falsely tagged instances. To identify the 
rules used in the post-process, we categorize the 
errors into several types, discriminate the types 
and encode them into the rules according to two 
principles: 
1) the rules are applied on the tagged sequences 
output by the unified model.  
2) The rules applied shouldn?t introduce more 
other errors. 
As a result, we have extracted eight rules, seven 
for ORG, one for PER. Generally, the rules work 
only on the local context of the examined tags, 
they correct some type of error by changing some 
tags when seeing certain pattern of context before 
or after the current tags in a limited distance. We 
want to give one rule as one example to explain 
the way they function. 
Example: {<LOC>}+<ORG> ==> <ORG>,  
After this rule is applied, one or more locations 
followed by a organization name will be tagged 
ORG. This is the case where there are a location 
name in a organization name. Besides, we can see 
since the location and latter part of the organiza-
tion name are tagged separately in the unified 
model, we may only resort to the post-process to 
get the right government boundary.  
5  Evaluation 
5.1  Results 
The evaluations in training phrase tell us the 
post-process can improve the performance by one 
percent. We are satisfied since we just applied 
eight rules. 
The formal eveluation results of our system are 
shown in Table 3. 
 R P F 
Overall 86.74 90.03 88.36 
PER 90.83 92.16 91.49 
LOC 89.89 91.66 90.77 
ORG 77.99 85.16 81.41 
Table 3. Formal Results on MSRA NER Open 
5.2  Errors  from NER Track 
The NER errors in our system are mainly of as 
follows: 
z Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding full 
names. Some common abbreviations has no corre-
sponding full names appeared in document. Here 
are some examples: 
R1: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
122
Sixth SIGHAN Workshop on Chinese Language Processing
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
z Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:?[???? LOC]?[???? PER]
?? 
K:?[???? PER]?[???? PER]
?? 
z Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For in-
stance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from failing 
to identify verbs and prepositions, such as: 
R:??????????? ????? 
K:[??????????? ORG]????
? 
6  Conclusions and Future Work 
We mainly described a Chinese named entity rec-
ognition system NER@ISCAS, which integrates 
text, part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER open track 
under the framework of Conditional Random 
Fields (CRFs) model. Although it provides a uni-
fied framework to integrate multiple flexible fea-
tures, and to achieve global optimization on input 
text sequence, the popular linear chained Condi-
tional Random Fields model often fails to catch 
semantic relations among reoccurred mentions and 
adjoining entities in a catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post process-
ing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108.  
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Human-
Machine Interface Technology Evaluation. 
Yuanyong Feng, Le Sun, Yuanhua Lv. 2006. 
Chinese Word Segmentation and Named Entity 
Recognition Based on Conditional Random 
Fields Models. Proceedings of SIGHAN-2006, 
Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia,. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Label-
ing Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
123
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 585?592,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Iterative Implicit Feedback Approach to Personalized Search 
 
Yuanhua Lv 1, Le Sun 2, Junlin Zhang 2, Jian-Yun Nie 3, Wan Chen 4, and Wei Zhang 2 
1, 2 Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
3 University of Montreal, Canada 
1 lvyuanhua@gmail.com 
2 {sunle, junlin01, zhangwei04}@iscas.cn 
3 nie@iro.umontreal.ca   4 chenwan@nus.edu.sg 
 
Abstract 
General information retrieval systems are 
designed to serve all users without con-
sidering individual needs. In this paper, 
we propose a novel approach to person-
alized search. It can, in a unified way, 
exploit and utilize implicit feedback in-
formation, such as query logs and imme-
diately viewed documents. Moreover, our 
approach can implement result re-ranking 
and query expansion simultaneously and 
collaboratively. Based on this approach, 
we develop a client-side personalized web 
search agent PAIR (Personalized Assis-
tant for Information Retrieval), which 
supports both English and Chinese. Our 
experiments on TREC and HTRDP col-
lections clearly show that the new ap-
proach is both effective and efficient. 
1 Introduction 
Analysis suggests that, while current information 
retrieval systems, e.g., web search engines, do a 
good job of retrieving results to satisfy the range 
of intents people have, they are not so well in 
discerning individuals? search goals (J. Teevan et 
al., 2005). Search engines encounter problems 
such as query ambiguity and results ordered by 
popularity rather than relevance to the user?s in-
dividual needs. 
To overcome the above problems, there have 
been many attempts to improve retrieval accuracy 
based on personalized information. Relevance 
Feedback (G. Salton and C. Buckley, 1990) is the 
main post-query method for automatically im-
proving a system?s accuracy of a user?s individual 
need. The technique relies on explicit relevance 
assessments (i.e. indications of which documents 
contain relevant information). Relevance feed-
back has been proved to be quite effective for 
improving retrieval accuracy (G. Salton and C. 
Buckley, 1990; J. J. Rocchio, 1971). However, 
searchers may be unwilling to provide relevance 
information through explicitly marking relevant 
documents (M. Beaulieu and S. Jones, 1998). 
Implicit Feedback, in which an IR system un-
obtrusively monitors search behavior, removes 
the need for the searcher to explicitly indicate 
which documents are relevant (M. Morita and Y. 
Shinoda, 1994). The technique uses implicit 
relevance indications, although not being as ac-
curate as explicit feedback, is proved can be an 
effective substitute for explicit feedback in in-
teractive information seeking environments (R. 
White et al, 2002). In this paper, we utilize the 
immediately viewed documents, which are the 
clicked results in the same query, as one type of 
implicit feedback information. Research shows 
that relative preferences derived from immedi-
ately viewed documents are reasonably accurate 
on average (T. Joachims et al, 2005). 
Another type of implicit feedback information 
that we exploit is users? query logs. Anyone who 
uses search engines has accumulated lots of click 
through data, from which we can know what 
queries were, when queries occurred, and which 
search results were selected to view. These query 
logs provide valuable information to capture us-
ers? interests and preferences. 
Both types of implicit feedback information 
above can be utilized to do result re-ranking and 
query expansion, (J. Teevan et al, 2005; Xuehua 
Shen. et al, 2005) which are the two general ap-
proaches to personalized search. (J. Pitkow et al, 
2002) However, to the best of our knowledge, 
how to exploit these two types of implicit feed-
back in a unified way, which not only brings col-
laboration between query expansion and result 
re-ranking but also makes the whole system more 
concise, has so far not been well studied in the 
previous work. In this paper, we adopt HITS al-
gorithm (J. Kleinberg, 1998), and propose a 
585
HITS-like iterative approach addressing such a 
problem. 
Our work differs from existing work in several 
aspects: (1) We propose a HITS-like iterative 
approach to personalized search, based on which, 
implicit feedback information, including imme-
diately viewed documents and query logs, can be 
utilized in a unified way. (2) We implement re-
sult re-ranking and query expansion simultane-
ously and collaboratively triggered by every 
click. (3) We develop and evaluate a client-side 
personalized web search agent PAIR, which 
supports both English and Chinese. 
The remaining of this paper is organized as 
follows. Section 2 describes our novel approach 
for personalized search. Section 3 provides the 
architecture of PAIR system and some specific 
techniques. Section 4 presents the details of the 
experiment. Section 5 discusses the previous 
work related to our approach. Section 6 draws 
some conclusions of our work. 
2 Iterative Implicit Feedback Approach 
We propose a HITS-like iterative approach for 
personalized search. HITS (Hyperlink-Induced 
Topic Search) algorithm, first described by (J. 
Kleinberg, 1998), was originally used for the 
detection of high-score hub and authority web 
pages. The Authority pages are the central web 
pages in the context of particular query topics. 
The strongest authority pages consciously do not 
link one another1 ? they can only be linked by 
some relatively anonymous hub pages. The mu-
tual reinforcement principle of HITS states that a 
web page is a good authority page if it is linked by 
many good hub pages, and that a web page is a 
good hub page if it links many good authority 
pages. A directed graph is constructed, of which 
the nodes represent web pages and the directed 
edges represent hyperlinks. After iteratively 
computing based on the reinforcement principle, 
each node gets an authority score and a hub score. 
In our approach, we exploit the relationships 
between documents and terms in a similar way to 
HITS. Unseen search results, those results which 
are retrieved from search engine yet not been 
presented to the user, are considered as ?authority 
pages?. Representative terms are considered as 
?hub pages?. Here the representative terms are the 
terms extracted from and best representing the 
implicit feedback information. Representative 
terms confer a relevance score to the unseen 
                                                          
1 For instance, There is hardly any other company?s Web 
page linked from ?http://www.microsoft.com/? 
search results ? specifically, the unseen search 
results, which contain more good representative 
terms, have a higher possibility of being relevant; 
the representative terms should be more repre-
sentative, if they occur in the unseen search re-
sults that are more likely to be relevant. Thus, 
also there is mutual reinforcement principle ex-
isting between representative terms and unseen 
search results. By the same token, we constructed 
a directed graph, of which the nodes indicate un-
seen search results and representative terms, and 
the directed edges represent the occurrence of the 
representative terms in the unseen search results. 
The following Table 1 shows how our approach 
corresponds to HITS algorithm. 
 
The Directed Graph 
Approaches
Nodes Edges 
HITS Authority Pages Hub Pages Hyperlinks
Our  
Approach
Unseen Search 
Results 
Representative 
Terms Occurrence
2
Table 1. Our approach versus HITS. 
 
Because we have already known that the rep-
resentative terms are ?hub pages?, and that the 
unseen search results are ?authority pages?, with 
respect to the former, only hub scores need to be 
computed; with respect to the latter, only author-
ity scores need to be computed. 
Finally, after iteratively computing based on 
the mutual reinforcement principle we can 
re-rank the unseen search results according to 
their authority scores, as well as select the repre-
sentative terms with highest hub scores to ex-
pand the query. Below we present how to con-
struct a directed graph to begin with. 
2.1 Constructing a Directed Graph 
We can view the unseen search results and the 
representative terms as a directed graph G = (V, E). 
A sample directed graph is shown in Figure 1: 
 
 
Figure 1. A sample directed graph. 
 
The nodes V correspond to the unseen search 
results (the rectangles in Figure 1) and the repre-
                                                          
2 The occurrence of the representative terms in the unseen 
search results. 
586
sentative terms (the circles in Figure 1); a di-
rected edge ?p?q?E? is weighed by the fre-
quency of the occurrence of a representative term 
p in an unseen search result q (e.g., the number 
put on the edge ?t1?r2? indicates that t1 occurs 
twice in r2). We say that each representative term 
only has an out-degree which is the number of the 
unseen search results it occurs in, as well as that 
each unseen search result only has an in-degree 
which is the count of the representative terms it 
contains. Based on this, we assume that the un-
seen search results and the representative terms 
respectively correspond to the authority pages 
and the hub pages ? this assumption is used 
throughout the proposed algorithm. 
2.2 A HITS-like Iterative Algorithm 
In this section, we present how to initialize the 
directed graph and how to iteratively compute the 
authority scores and the hub scores. And then 
according to these scores, we show how to re-rank 
the unseen search results and expand the initial 
query. 
Initially, each unseen search result of the query 
are considered equally authoritative, that is, 
0 0 0
1 2 | |
1 | |
Y
Yy y y= ?= =                  (1) 
Where vector Y indicates authority scores of the 
overall unseen search results, and |Y| is the size of 
such a vector. Meanwhile, each representative 
term, with the term frequency tfj in the history 
query logs that have been judged related to the 
current query, obtains its hub score according to 
the follow formulation: 
0
| |
1
X
j i
j itf tfx == ?                       (2) 
Where vector X indicates hub scores of the overall 
representative terms, and |X| is the size of the 
vector X. The nodes of the directed graph are 
initialized in this way. Next, we associate each 
edge with a weight: 
,( )ji i jw tft r? =                     (3) 
Where tfi,j indicates the term frequency of the 
representative term ti occurring in the unseen 
search result rj; ?w(ti? rj)? is the weight of edge 
that link from ti to rj. For instance, in Figure 1, 
w(t1? r2) = 2. 
After initialization, the iteratively computing of 
hub scores and authority scores starts. 
The hub score of each representative term is 
re-computed based on three factors: the authority 
scores of each unseen search result where this 
term occurs; the occurring frequency of this term 
in each unseen search result; the total occurrence 
of every representative term in each unseen search 
result. The formulation for re-computing hub 
scores is as follows: 
( 1)
:
:
( )
( )'
k ji
i
jnji
jn
k
jj
n
w
wt r
t r
t ryx t r
+
?  ?
?  ?
?= ?? ?
    (4) 
Where x`i(k+1) is the hub score of a representative 
term ti after (k+1)th iteration; yjk is the authority 
score of an unseen search result rj after kth itera-
tion; ??j: ti?rj? indicates the set of all unseen 
search results those ti occurs in; ??n: tn?rj? in-
dicates the set of all representative terms those rj 
contains. 
The authority score of each unseen search re-
sult is also re-computed relying on three factors: 
the hub scores of each representative term that 
this search result contains; the occurring fre-
quency of each representative term in this search 
result; the total occurrence of each representative 
term in every unseen search results. The formu-
lation for re-computing authority scores is as 
follows: 
( 1)
:
:
( )
( )'
k k
ij
miji
mi
ji
i
m
w
wt r
t r
t ry x t r
+
?  ?
?  ?
?= ?? ?
    (5) 
Where y`j(k+1) is the authority score of an unseen 
search result rj after (k+1)th iteration; xik  is the 
hub score of a representative term ti after kth it-
eration; ??i: ti?rj? indicates the set of all repre-
sentative terms those rj contains; ??m: ti?rm? 
indicates the set of all unseen search results those 
ti occurs in. 
After re-computation, the hub scores and the 
authority scores are normalized to 1. The formu-
lation for normalization is as follows: 
| | | |
1 1
and
' '
' '
j i
iY Xj
kkk k
y xy x
y x= =
=   =
? ?
              (6) 
The iteration, including re-computation and 
normalization, is repeated until the changes of the 
hub scores and the authority scores are smaller 
than some predefined threshold ? (e.g. 10-6). 
Specifically, after each repetition, the changes in 
authority scores and hub scores are computed 
using the following formulation: 
2 2( 1) ( 1)
| | | |
1 1
( ) ( )
k k k k
i ij j
Y x
j i
c y y x x
+ +
= =
= ? + ?? ?        (7) 
The iteration stops if c<?. Moreover, the itera-
tion will also stop if repetition has reached a 
587
predefined times k (e.g. 30). The procedure of the 
iteration is shown in Figure 2. 
As soon as the iteration stops, the top n unseen 
search results with highest authority scores are 
selected and recommended to the user; the top m 
representative terms with highest hub scores are 
selected to expand the original query. Here n is a 
predefined number (in PAIR system we set n=3, 
n is given a small number because using implicit 
feedback information is sometimes risky.) m is 
determined according to the position of the big-
gest gap, that is, if ti ? ti+1 is bigger than the gap 
of any other two neighboring ones of the top half 
representative terms, then m is given a value i. 
Furthermore, some of these representative terms 
(e.g. top 50% high score terms) will be again used 
in the next time of implementing the iterative 
algorithm together with some newly incoming 
terms extracted from the just now click. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The HITS-like iterative algorithm. 
 
3 Implementation 
3.1 System Design 
In this section, we present our experimental sys-
tem PAIR, which is an IE Browser Helper Object 
(BHO) based on the popular Web search engine 
Google. PAIR has three main modules: Result 
Retrieval module, User Interactions module, and 
Iterative Algorithm module. The architecture is 
shown in Figure 3. 
The Result Retrieval module runs in back-
grounds and retrieves results from search engine. 
When the query has been expanded, this module 
will use the new keywords to continue retrieving. 
The User Interactions module can handle three 
types of basic user actions: (1) submitting a query; 
(2) clicking to view a search result; (3) clicking 
the ?Next Page? link. For each of these actions, 
the system responds with: (a) exploiting and ex-
tracting representative terms from implicit feed-
back information; (b) fetching the unseen search 
results via Results Retrieval module; (c) sending 
the representative terms and the unseen search 
results to Iterative Algorithm module. 
 
 
Figure 3. The architecture of PAIR. 
 
The Iterative Algorithm module implements 
the HITS-like algorithm described in section 2. 
When this module receives data from User In-
teractions module, it responds with: (a) iteratively 
computing the hub scores and authority scores; (b) 
re-ranking the unseen search results and expand-
ing the original query. 
Some specific techniques for capturing and 
exploiting implicit feedback information are de-
scribed in the following sections. 
3.2 Extract Representative Terms from 
Query Logs 
We judge whether a query log is related to the 
current query based on the similarity between the 
query log and the current query text. Here the 
query log is associated with all documents that 
the user has selected to view. The form of each 
query log is as follows 
<query text><query time> [clicked documents]* 
The ?clicked documents? consist of URL, title 
and snippet of every clicked document. The rea-
son why we utilize the query text of the current 
query but not the search results (including title, 
snippet, etc.) to compute the similarity, is out of 
consideration for efficiency. If we had used the 
search results to determine the similarity, the 
computation could only start once the search en-
gine has returned the search results. In our method, 
instead, we can exploit query logs while search 
engine is doing retrieving. Notice that although 
our system only utilizes the query logs in the last 
24 hours; in practice, we can exploit much more 
because of its low computation cost with respect 
to the retrieval process performed in parallel.
Iterate (T, R, k, ?) 
T: a collection of m terms 
R: a collection of n search results 
k: a natural number 
?: a predefined threshold 
Apply (1) to initialize Y. 
Apply (2) to initialize X. 
Apply (3) to initialize W. 
For i = 1, 2?, k 
Apply (4) to (Xi-1, Yi-1) and obtain X`i. 
Apply (5) to (Xi-1, Yi-1) and obtain Y`i. 
Apply (6) to Normalize X`i and Y`i, and respectively 
obtain Xi and Yi. 
Apply (7) and obtain c. 
If c<?, then break. 
End 
Return (X, Y). 
588
Table 2. Sample results of re-ranking. The search results in boldface are the ones that our system rec-
ommends to the user. ?-3? and ?-2? in the right side of some results indicate the how their ranks descend. 
 
We use the standard vector space retrieval 
model (G. Salton and M. J. McGill, 1983) to 
compute the similarity. If the similarity between 
any query log and the current query exceeds a 
predefined threshold, the query log will be con-
sidered to be related to current query. Our system 
will attempt to extract some (e.g. 30%) represen-
tative terms from such related query logs ac-
cording to the weights computed by applying the 
following formulation: 
( )i i iw f idftt =                      (8) 
Where tfi and idfi respectively are the term fre-
quency and inverse document frequency of ti in 
the clicked documents of a related query log. 
This formulation means that a term is more rep-
resentative if it has a higher frequency as well as 
a broader distribution in the related query log. 
3.3 Extract Representative Terms from 
Immediately Viewed Documents 
The representative terms extracted from immedi-
ately viewed documents are determined based on 
three factors: term frequency in the immediately 
viewed document, inverse document frequency in 
the entire seen search results, and a discriminant 
value. The formulation is as follows:  
( ) ( )Ni ii i
r ddw d
x xtf idfx x= ? ?             (9) 
Where tfxidr is the term frequency of term xi in the 
viewed results set dr; tfxidr is the inverse document 
frequency of xi in the entire seen results set dN. 
And the discriminant value d(xi) of xi is computed 
using the weighting schemes F2 (S. E. Robertson 
and K. Sparck Jones, 1976) as follows: 
( ) ln
( ) ( )i
r Rd
n r N Rx = ? ?
                 (10) 
Where r is the number of the immediately viewed 
documents containing term xi; n is the number of 
the seen results containing term xi; R is the num-
ber of the immediately viewed documents in the 
query; N is the number of the entire seen results.  
3.4 Sample Results 
Unlike other systems which do result re-ranking 
and query expansion respectively in different 
ways, our system implements these two functions 
simultaneously and collaboratively ?  Query 
expansion provides diversified search results 
which must rely on the use of re-ranking to be 
moved forward and recommended to the user. 
 
 
Figure 4. A screen shot for query expansion. 
 
After iteratively computing using our approach, 
the system selects some search results with top 
highest authority scores and recommends them to 
the user. In Table 2, we show that PAIR suc-
cessfully re-ranks the unseen search results of 
?jaguar? respectively using the immediately 
Google result PAIR result  
query = ?jaguar? query = ?jaguar? After the 4th result being clicked 
query = ?jaguar? 
?car? ? query logs 
1 Jaguar www.jaguar.com/ 
Jaguar 
www.jaguar.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/ 
2 Jaguar CA - Jaguar Cars www.jaguar.com/ca/en/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/ 
Jaguar UK - R is for? 
www.jaguar-racing.com/ 
3 Jaguar Cars www.jaguarcars.com/ 
Jaguar Cars 
www.jaguarcars.com/ 
Jaguar 
www.jaguar.com/ 
4 Apple - Mac OS X www.apple.com/macosx/ 
Apple - Mac OS X 
www.apple.com/macosx/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/                      -2 
5 Apple - Support ? www.apple.com/support/... 
Amazon.com: Mac OS X 10.2? 
www.amazon.com/exec/obidos/... 
Jaguar Cars 
www.jaguarcars.com/                        -2 
6 Jaguar UK - Jaguar Cars www.jaguar.co.uk/ 
Mac OS X 10.2 Jaguar? 
arstechnica.com/reviews/os? 
Apple - Mac OS X 
www.apple.com/macosx/                     -2 
7 Jaguar UK - R is for? www.jaguar-racing.com/ 
Macworld: News: Macworld? 
maccentral.macworld.com/news/? 
Apple - Support ? 
www.apple.com/support/...                    -2 
8 Jaguar dspace.dial.pipex.com/? 
Apple - Support? 
www.apple.com/support/...                -3 
Jaguar 
dspace.dial.pipex.com/? 
9 Schr?dinger -> Home www.schrodinger.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/                       -3 
Schr?dinger -> Home 
www.schrodinger.com/ 
10 Schr?dinger -> Site Map www.schrodinger.com/... 
Jaguar UK - R is for? 
www.jaguar-racing.com/                  -3 
Schr?dinger -> Site Map 
www.schrodinger.com/... 
589
viewed documents and the query logs. Simulta-
neously, some representative terms are selected 
to expand the original query. In the query of 
?jaguar? (without query logs), we click some 
results about ?Mac OS?, and then we see that a 
term ?Mac? has been selected to expand the 
original query, and some results of the new query 
?jaguar Mac? are recommended to the user under 
the help of re-ranking, as shown in Figure 4. 
4 Experiment 
4.1 Experimental Methodology 
It is a challenge to quantitatively evaluate the 
potential performance improvement of the pro-
posed approach over Google in an unbiased way 
(D. Hawking et al, 1999; Xuehua Shen et al, 
2005). Here, we adopt a similar quantitative 
evaluation as what Xuehua Shen et al (2005) do 
to evaluate our system PAIR and recruit 9 stu-
dents who have different backgrounds to partici-
pate in our experiment. We use query topics from 
TREC 2005 and 2004 Hard Track, TREC 2004 
Terabyte track for English information retrieval,3 
and use query topics from HTRDP 2005 Evalua-
tion for Chinese information retrieval.4 The rea-
son why we utilize multiple TREC tasks rather 
than using a single one is that more queries are 
more likely to cover the most interesting topics 
for each participant. 
Initially, each participant would freely choose 
some topics (typically 5 TREC topics and 5 
HTRDP topics). Each query of TREC topics will 
be submitted to three systems: UCAIR 5 (Xue-
hua Shen et al, 2005), ?PAIR No QE? (PAIR 
system of which the query expansion function is 
blocked) and PAIR. Each query of HTRDP topics 
needs only to be submitted to ?PAIR No QE? and 
PAIR. We do not evaluate UCAIR using HTRDP 
topics, since it does not support Chinese. For each 
query topic, the participants use the title of the 
topic as the initial keyword to begin with. Also 
they can form some other keywords by them-
selves if the title alone fails to describe some de-
tails of the topic. There is no limit on how many 
queries they must submit. During each query 
process, the participant may click to view some 
results, just as in normal web search. 
Then, at the end of each query, search results 
from these different systems are randomly and 
anonymously mixed together so that every par-
                                                          
3 Text REtrieval Conference. http://trec.nist.gov/ 
4 2005 HTRDP Evaluation. http://www.863data.org.cn/ 
5 The latest version released on November 11, 2005. 
http://sifaka.cs.uiuc.edu/ir/ucair/ 
ticipant would not know where a result comes 
from. The participants would judge which of 
these results are relevant. 
At last, we respectively measure precision at 
top 5, top 10, top 20 and top 30 documents of 
these system. 
4.2 Results and Analysis 
Altogether, 45 TREC topics (62 queries in all) are 
chosen for English information retrieval. 712 
documents are judged as relevant from Google 
search results. The corresponding number of 
relevant documents from UCAIR, ?PAIR No QE? 
and PAIR respectively is: 921, 891 and 1040. 
Figure 5 shows the average precision of these four 
systems at top n documents among such 45 TREC 
topics. 
 
 
Figure 5. Average precision for TREC topics. 
 
45 HTRDP topics (66 queries in all) are chosen 
for Chinese information retrieval. 809 documents 
are judged as relevant from Google search results. 
The corresponding number of relevant documents 
from ?PAIR No QE? and PAIR respectively is: 
1198 and 1416. Figure 6 shows the average pre-
cision of these three systems at top n documents 
among such 45 HTRDP topics. 
 
 
Figure 6. Average precision for HTRDP topics. 
 
PAIR and ?PAIR No QE? versus Google 
We can see clearly from Figure 5 and Figure 6 
that the precision of PAIR is improved a lot 
comparing with that of Google in all measure-
590
ments. Moreover, the improvement scale in-
creases from precision at top 10 to that of top 30. 
One explanation for this is that the more implicit 
feedback information generated, the more repre-
sentative terms can be obtained, and thus, the 
iterative algorithm can perform better, leading to 
more precise search results. ?PAIR No QE? also 
significantly outperforms Google in these meas-
urements, however, with query expansion, PAIR 
can perform even better. Thus, we say that result 
re-ranking and query expansion both play an 
important role in PAIR. 
Comparing Figure 5 with Figure 6, one can see 
that the improvement of PAIR versus Google in 
Chinese IR is even larger than that of English IR. 
One explanation for this is that: before imple-
menting the iterative algorithm, each Chinese 
search result, including title and snippet, is seg-
mented into words (or phrases). And only the 
noun, verb and adjective of these words (or 
phrases) are used in next stages, whereas, we only 
remove the stop words for English search result. 
Another explanation is that there are some Chi-
nese web pages with the same content. If one of 
such pages is clicked, then, occasionally some 
repetition pages are recommended to the user. 
However, since PAIR is based on the search re-
sults of Google and the information concerning 
the result pages that PAIR can obtained is limited, 
which leads to it difficult to avoid the replica-
tions. 
PAIR and ?PAIR No QE? versus UCAIR 
In Figure 5, we can see that the precision of 
?PAIR No QE? is better than that of UCAIR 
among top 5 and top 10 documents, and is almost 
the same as that of UCAIR among top 20 and top 
30 documents. However, PAIR is much better 
than UCAIR in all measurements. This indicates 
that result re-ranking fails to do its best without 
query expansion, since the relevant documents in 
original query are limited, and only the re-ranking 
method alone cannot solve the ?relevant docu-
ments sparseness? problem. Thus, the query ex-
pansion method, which can provide fresh and 
relevant documents, can help the re-ranking 
method to reach an even better performance. 
Efficiency of PAIR 
The iteration statistic in evaluation indicates that 
the average iteration times of our approach is 22 
before convergence on condition that we set the 
threshold ? = 10-6. The experiment shows that the 
computation time of the proposed approach is 
imperceptible for users (less than 1ms.) 
5 Related Work 
There have been many prior attempts to person-
alized search. In this paper, we focus on the re-
lated work doing personalized search based on 
implicit feedback information. 
Some of the existing studies capture users? in-
formation need by exploiting query logs. For 
example, M. Speretta and S. Gauch (2005) build 
user profiles based on activity at the search site 
and study the use of these profiles to provide 
personalized search results. F. Liu et al (2002) 
learn user's favorite categories from his query 
history. Their system maps the input query to a set 
of interesting categories based on the user profile 
and confines the search domain to these catego-
ries. Some studies improve retrieval performance 
by exploiting users? browsing history (F. Tanud-
jaja and L. Mu, 2002; M. Morita and Y. Shinoda, 
1994) or Web communities (A. Kritikopoulos 
and M. Sideri, 2003; K. Sugiyama et al, 2004) 
Some studies utilize client side interactions, for 
example, K. Bharat (2000) automatically discov-
ers related material on behalf of the user by 
serving as an intermediary between the user and 
information retrieval systems. His system ob-
serves users interacting with everyday applica-
tions and then anticipates their information needs 
using a model of the task at hand. Some latest 
studies combine several types of implicit feed-
back information. J. Teevan et al (2005) explore 
rich models of user interests, which are built 
from both search-related information, such as 
previously issued queries and previously visited 
Web pages, and other information about the user 
such as documents and email the user has read 
and created. This information is used to re-rank 
Web search results within a relevance feedback 
framework. 
Our work is partly inspired by the study of 
Xuehua Shen et al (2005), which is closely re-
lated to ours in that they also exploit immediately 
viewed documents and short-term history queries, 
implement query expansion and re-ranking, and 
develop a client-side web search agents that per-
form eager implicit feedback. However, their 
work differs from ours in three ways: First, they 
use the cosine similarity to implement query ex-
pansion, and use Rocchio formulation (J. J. 
Rocchio, 1971) to re-rank the search results. 
Thus, their query expansion and re-ranking are 
computed separately and are not so concise and 
collaborative. Secondly, their query expansion is 
based only on the past queries and is imple-
mented before the query, which leads to that 
591
their query expansion does not benefit from 
user?s click through data. Thirdly, they do not 
compute the relevance of search results and the 
relativity of expanded terms in an iterative fash-
ion. Thus, their approach does not utilize the re-
lation among search results, among expanded 
terms, and between search results and expanded 
terms. 
6 Conclusions 
In this paper, we studied how to exploit implicit 
feedback information to improve retrieval accu-
racy. Unlike most previous work, we propose a 
novel HITS-like iterative algorithm that can 
make use of query logs and immediately viewed 
documents in a unified way, which not only 
brings collaboration between query expansion 
and result re-ranking but also makes the whole 
system more concise. We further propose some 
specific techniques to capture and exploit these 
two types of implicit feedback information. Us-
ing these techniques, we develop a client-side 
web search agent PAIR. Experiments in English 
and Chinese collections show that our approach 
is both effective and efficient. 
However, there is still room to improve the 
performance of the proposed approach, such as 
exploiting other types of personalized informa-
tion, choosing some more effective strategies to 
extract representative terms, studying the effects 
of the parameters used in the approach, etc. 
Acknowledgement 
We would like to thank the anonymous review-
ers for their helpful feedback and corrections, 
and to the nine participants of our evaluation ex-
periments. Additionally, this work is supported 
by the National Science Fund of China under 
contact 60203007. 
References 
A. Kritikopoulos and M. Sideri, 2003. The Compass 
Filter: Search engine result personalization using 
Web communities. In Proceedings of ITWP, pages 
229-240. 
D. Hawking, N. Craswell, P.B. Thistlewaite, and D. 
Harman, 1999. Results and challenges in web 
search evaluation. Computer Networks, 
31(11-16):1321?1330. 
F. Liu, C. Yu, and W. Meng, 2002. Personalized web 
search by mapping user queries to categories. In 
Proceedings of CIKM, pages 558-565. 
F. Tanudjaja and L. Mu, 2002. Persona: a contextual-
ized and personalized web search. HICSS. 
G. Salton and M. J. McGill, 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill. 
G. Salton and C. Buckley, 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297. 
J. J. Rocchio, 1971. Relevance feedback in informa-
tion retrieval. In The SMART Retrieval System :  
Experiments in Automatic Document Processing, 
pages 313?323. Prentice-Hall Inc. 
J. Kleinberg, 1998. Authoritative sources in a hyper-
linked environment. ACM, 46(5):604?632. 
J. Pitkow, H. Schutze, T. Cass, R. Cooley, D. 
Turnbull, A. Edmonds, E. Adar, and T. Breuel, 
2002. Personalized search. Communications of the 
ACM, 45(9):50-55. 
J. Teevan, S. T. Dumais, and E. Horvitz, 2005. Per-
sonalizing search via automated analysis of interests 
and activities. In Proceedings of SIGIR, pages 
449-456. 
K. Bharat, 2000. SearchPad: Explicit capture of 
search context to support Web search. Computer 
Networks, 33(1-6): 493-501. 
K. Sugiyama, K. Hatano, and M. Yoshikawa, 2004. 
Adaptive Web search based on user profile con-
structed without any effort from user. In Proceed-
ings of WWW, pages 675-684. 
M. Beaulieu and S. Jones, 1998. Interactive searching 
and interface issues in the okapi best match retrieval 
system. Interacting with Computers, 10(3):237-248. 
M. Morita and Y. Shinoda, 1994. Information filtering 
based on user behavior analysis and best match text 
retrieval. In Proceedings of SIGIR, pages 272?281. 
M. Speretta and S. Gauch, 2005. Personalizing search 
based on user search history. Web Intelligence, 
pages 622-628. 
R. White, I. Ruthven, and J. M. Jose, 2002. The use of 
implicit evidence for relevance feedback in web 
retrieval. In Proceedings of ECIR, pages 93?109. 
S. E. Robertson and K. Sparck Jones, 1976. Relevance 
weighting of search terms. Journal of the 
American Society for Information Science, 
27(3):129-146. 
T. Joachims, L. Granka, B. Pang, H. Hembrooke, and 
G. Gay, 2005. Accurately Interpreting Clickthrough 
Data as Implicit Feedback, In Proceedings of 
SIGIR, pages 154-161. 
Xuehua Shen, Bin Tan, and Chengxiang Zhai, 2005. 
Implicit User Modeling for Personalized Search. In 
Proceedings of CIKM, pages 824-831. 
592
Query Translation in Chinese-English Cross-Language 
Information Retrieval 
Zhang Yibo, Sun Le, Du Lin, Sun Yufang 
Chinese Information Processing Center, 
Institute of Software, Chinese Academy of Sciences, 
P.O.Box 8718, Beijing, 100080, P.R. China 
e-mail: zyb, lesun, !du, yfsun@sonata.iscas.ac.cn 
Abstract 
This paper proposed a new query translation 
method based on the mutual information 
matrices of terms in the Chinese and 
English corpora. Instead of looking up a 
? bilingual phrase dictionary, the 
compositional phrase (the translation of 
phrase can be derived from the translation 
of its components) in the query can be 
indirectly translated via a general-purpose 
Chinese-English dictionary look-up 
procedure. A novel selection method for 
translations of query terms is also presented 
in detail. Our query translation method 
ultimately constructs an English query in 
which each query term has a weight. The 
evaluation results show that the retrieval 
performance achieved by our query 
translation method is about 73% of 
monolingual information retrieval and is 
about 28% higher than that of simple word- 
by-word translation way. 
Introduction 
With the rapid growth of electronic documents 
and the great development of network in China, 
there are more and more people touching the 
Intemet, on which, however, English is the most 
popular language being used. It is difficult for 
most people in China to use English fluently, so 
they would like to use Chinese to express their 
queries to retrieval the relevant English 
documents. This situation motivates research in 
Cross Language Information Retrieval (CLIR). 
There are two approaches to CLIR, one is 
query translation; the other is translating 
original language documents to destination 
This research was supported by the National 
Science Fund of China for Distinguished Young 
Scholars under contact 69983009. 
language quivalents. Obviously, the latter is a 
very expensive task since there are so many 
documents in a collection and there is not yet a 
reliable machine translation system that can be 
used to process automatically. Most researchers 
are inclined to choose the query translation 
approach \[Oard. (1996)\]. Methods for query 
translation have focused on three areas: the 
employment of machine translation techniques, 
dictionary based translation \[Hull & 
Grefenstette (1996); Ballesteros & Croft (1996)\], 
parallel or comparable corpora for generating 
a translation model \[Davis & Dunning (1995); 
Sheridan & Ballerini (1996); Nie, Jian-Yun et 
a1.(1999)\]. Machine translation (MT) method 
has many obstacles to prevent its employment 
into CLIR such as deep syntactic and semantic 
analysis, user queries consisting of only one or 
two words, and an arduous task to build a MT 
system. Dictionary based query translation is the 
most popular method because of its easiness to 
perform. The main reasons leading to the great 
drops in CLIP,. effectiveness by this method are 
ambiguities caused by more than one translation 
of a query term and failures to translate phrases 
during query translation. Previous studies \[Hull 
& Grefenstette (1996); Ballesteros & Croft 
(1996)\] have shown that automatic word-by- 
word (WBW) query translation via machine 
readable dictionary (MKD) results in a 40-60% 
loss in effectiveness below that of monolingual 
retrieval. With regard to the use of parallel 
corpora translation method, the critiques one 
often raises concern the availability of reliable 
parallel text corpora. An alternative way is that 
making use of the comparable corpora because 
they are easier to be obtained and there are more 
and more bilingual even multilingual documents 
on the Internet. From analyzing a document 
collection, an associated word list can be 
yielded and it is often used to expansion the 
query in monolingual information retrieval \[Qiu 
& Frei(1993); Jing & Croft(1994)\]. 
104 
In this paper, a new query translation is 
presented by combination dictionary based 
method with the comparable corpora analyzing. 
Ambiguity problem and phrase information lost 
are attacked in dictionary based Chinese- 
English Cross-Language information Retrieval 
(CECLIR). The remainder of this paper is 
organized as follows: section 1 gives a method 
to calculate the mutual information matrices of 
Chinese-English comparable corpora. Section 2 
develops a scheme to select the translations of 
the Chinese query terms and introduces how the 
compositional phrase can be kept in our method. 
Section 3 presents a set of preliminary 
experiment on comparable corpora to evaluate 
our query translation method and gives some 
explanations. 
1 .Mutual information matrices 
calculation 
We hypothesize that the words in a sentence 
after being removed the stop words be 
associated with each other and work together to 
express a query requirement. The association 
relationship between two words can be 
indicated by their mutual information, which 
can be further used to discover phrases \[Church 
:& Hanks (1990)\]. If two words are independent 
with each other, their mutual information would 
be close to zero. On the other hand, if they are 
strongly related, the mutual information would 
be much greater than zero and they would be 
much like to be a phrase; if they occur 
complementarily, the mutual information would 
be negative. In conclusion? the bigger the 
mutual information of word pair, the more 
probable the word phrase would be a phrase. 
According to \[Fano (1961)\], we can define the 
mutual information M1 (tl,t z) of term t I and 
t z as formula (1). 
MI(q,t2) =log  z P(t~'t2) (1) 
P(t~)P(t2) 
Where 
P(tl, t z) is the co-occurrence probability of 
t~ and t~ in a Chinese sentence. The reason we 
select a Chinese sentence to be a window other 
than a fixed length window is that a full Chinese 
sentence can keep more linguistic information 
and consequently, it is more reasonable that we 
can regard t~ and t 2 to be a phrase when they 
co-occur in a sentence. P(t l) and P(t 2) are 
the occurrence probabilities of term t I and t 2 
in a sentence. These probabilities can be 
calculated by the occurrence of term t~ and t 2 
in the collection as equation (2), (3) and (4). 
P(tl) = n,__~_ (2) 
N 
P(t2) = n,2 (3) 
N 
P(tl, t2 ) = n,,,,~ (4) 
N 
Where 
nt~ , nt2 is the individual term frequency of 
term t I and t 2 respectively if either of them 
occur in a sentence of the collection, ntt,t ~ is 
the co-occurrence frequency of term t I and t 2 
i f  they are all in a sentence of the collection. N 
is the number of sentences of the collection. 
Replacing (1) with equation (2), (3) and (4), the 
mutual information of term t I and t 2 can be 
expressed by following formula. 
n,,. N 
MI(q,t 2) = log 2 '- (5) 
H h nt 2 
Table 2 and table 3 show the occurrence 
frequency values and mutual information values 
calculated by formula (5) for three Chinese 
compositional phrases and their corresponding 
English phrases respectively found in our 
comparable corpora. 
t 1 It 2 n,, n,2 n,,,,: MI 
~,\[-~l~f\]~ 106 84 45 9.28 
j~p l t~ 45 97 21 9.21 
~\ ]~\ [g~ 73 22 19 10.51 
Table 2: Mutual information of three Chinese 
phrases (N  = 123,000) 
tl I t2 nt~ ntz nt.t~ M1 
File I system 158 126 52 8.91 
User I management 59 112 18 8.97 
Graphic \[ interface 92 41 34 10.70 
Table 3: Mutual information of three English 
phrases (N  = 184,000) 
Anal)zing the Chinese-English comparable 
corpora in this way, we can get two mutual 
information value matrices to indicate which 
two terms (as to the Chinese collection, they are 
105 
almost Chinese words after segmentation) 
would be most possible to be a phrase. A word 
list associated to each Chinese query term can 
be obtained by looking up the mutual 
information value matrix of the Chinese corpus 
with a cutoff of M1 =1.50. As discussed 
above, the bigger the mutual information value 
between two terms, the more possible the two 
words would be a phrase. We can infer that the 
associated word list of the query term contains 
the terms that are the most possible components 
of a compositional phrase. In other words, the 
phrase information can be kept by this way. The 
Chinese query is translated into English via 
looking up the English senses of Chinese query 
term and words in its associated word list in a 
Chinese-English dictionary. The procedures 
how to select appropriate tranlations and to 
construct he English query are discussed in 
section 2. 
2 Translations selection and phrase 
keeping 
It is a naive method to translate a Chinese query 
only by looking up each Chinese term to get its 
English senses in a Chinese-English dictionary. 
This method, however, results in too many 
ambiguities during the query translation and 
offers no path to select appropriate ones among 
the translations. In addition, phrases in the query 
can not be translated effectively. Previous tudy 
has showed that failure to translate phrases 
greatly reduces the performance by up to 25% 
over automatic word-by-word (WBW) query 
translation \[Ballesteros & Croft (1996)\]. 
In our method, those English translations 
most likely co-occur with each other can be 
obtained via looking up the mutual information 
value matrix of the English corpus with a cutoff 
M1 = 1.50. In this way, the English senses of 
terms in the associated word list can provide a 
good context for the translation of the Chinese 
query term and give a significant clue for its 
translations selection. In addition, the 
information of two terms (either Chinese or 
English) to be a phrase can also be stored in the 
associated word list. In the following, we firstly 
describe our method to select translations in 
detail, and then we give an example to 
demonstrate how to keep the phrase information 
in our method. 
Supposing the Chinese query is expressed by 
(e 1 ,e~ ,.--, e, ). el, e2,... , e, are the segmented 
Chinese words of the query after removing the 
stop words. The translations of 
e m (m = 1,...,r)by looking up the Chinese- 
English bilingual dictionary can be ordered in 
descending by following formula. 
W(fm t ) = lOglO(Ot'i_Ml(f ~)+ fl "o_Ml(fm t )) (6) 
l~'llgmkl" t " " 
z z 
i _M i ( fm l ) = k=l j= l  / 
I~llrmkl (7) 
k=l  
r \]~1 l 
Z ZMI(f,~,J~ k) 
o_ MI(fm l ) = i=l,i?m k=l (8) 
r  lYd 
? i=l,i~m 
Where 
f~ is one sense of the English translation set 
F m of the word e,~ (l = 1,...,IF.b g. is the 
association word set of e m . The size of E m is 
le.I and its element is e~ (k = 1 .... ,le.I) 
F~ is the English translation set of emk, its 
element is f,,~. ct is the coefficient to 
emphasize the inner mutual information 
between the English sense f t of the single 
Chinese query term e m and the English sense 
f ,~ of the e m's association word emk. The 
first part of the formula (6) i _M I ( f~)  
reflects the probability of English translation 
f,~ and f ,~ to be a phrase. /3 is the 
coefficient o emphasize the outside mutual 
information between f,~ and the English sense 
~* of the other Chinese terms included in the 
query. The second part of the formula (6) 
o_  Ml ( f~)  reflects the relevant value between 
the English sense f ,~of  e m and the whole 
query concept. 
Our method of translations selection can be 
described as follows: if the weight of any 
translation of the Chinese query term is greater 
than 1.00, the sense is selected to construct the 
English query. If there is no weight of any 
translation of the Chinese query term greater 
than 1.00, the sense with biggest one is selected 
to construct the English query. In this way, we 
can make an English query by the following 
Boolean expression. 
106 
r / I F~I .  t "~ Query =21\[ XI ~glra'W(gra))) (9) 
Where o I is set element after the English Ora  
translation sense set F m which is detruncated 
by our translation selection method. 
In order to demonstrate he procedure of our 
method, we give an example and explain how 
the English translations are selected and how the 
phrase information is kept. Given a simple 
Chinese query " ~ fi' , '~ ~ , ~ ~ (user, 
management, command)" after segmentation 
and removing stop words, the associated word 
list of term " ~ ~ (user)" is " '~" 2~ 
(management) , 4-~ ,~ (information) , --f- ~" 
(manual)" and the associated word list of term 
"'~ ~E(management)" is "~ ~ (user), *J~(harrd 
disk), ~,Aq-(file)". We process the associated 
word "'~2~2(management)" of the query term 
"~q ~ (user)" in a special way by adding an 
appropriate value to their mutual information 
value to let theirs be the biggest in the 
associated word list, because the associated 
word " '~(management)"  also occurs in the 
original query. Similar way is done with the 
associated word "~ ~ (user)" of the query term 
"~ ~ (management)". In this way, , the 
compositional phrase " h~ ~ '~ JX (user 
management)" can be kept in both associated 
word list of term " I t / "  (user)" and term "~X 
(management)". 
When term ")~ ~"  is translated into English 
by looking up the general-purpose Chinese- 
English bilingual dictionary, we get its English 
sense set "user, consumer" ordered by the 
formula (6). When term "~'JE" is translated 
into English, we get its English sense set 
"management, administration, supervision, run" 
ordered by the formula (6). We can fred the first 
positions of the English translation set of the 
query term "~ P"  and term '"~'JX" are "user" 
and "management" respectively. From the point 
of view of translation, the phrase "user 
management" can be regarded as the English 
phrase translation of "~ # '~ ~" .  According to 
our translation selection and formula (9), we can 
construct the English Boolean query as follows, 
in which each query term has a weight. 
Query = (user, 1.86)and ((management, 1.83) 
or (administration, 1.63)) and (command, 1.92). 
3 Evaluation and discussion 
To evaluate our query translation method, we 
did a set of experiment to compare it to the 
word-by-word (WBW) translation method and 
manual translation method. In the word-by- 
word translation method, the Chinese queries 
are automatically segmented and the Chinese 
terms included in them are translated into 
English only by looking up the general-purpose 
Chinese-English bilingual dictionary. In the 
manual translation method, the Chinese queries 
are translated into English by a Ph.D. student. 
The segmentation we used is based on a small 
general-purpose Chinese-English bilingual 
dictionary that only contains 46,570 pairs in 
which each Chinese word has several English 
translations. The forward and backward 
maximum matching algorithm is used to 
segment he texts and find the combinatorial 
ambiguities. Of all the combinatorial 
ambiguities, 91.2% are removed with the word 
uni-gram prior probabilities. A stop word list of 
1210 elements is set up, which contains 
frequently used functional words as well as 
symbols \[Du & Sun (2000)\]. Our Chinese query 
translation process contains following steps: 
(1) Segment the Chinese query according to the 
method introduced above. 
(2) Get the associated word list of each Chinese 
term included in the query from the Chinese 
mutual information matrix. 
(3) Look up the English sense set of each 
Chinese term and its associated word in the 
general-purpose Chinese-English bilingual 
dictionary. 
(4) Select the English translation sense by the 
method introduced in section 2 (in formula 
(6) the coefficents tx and fl are selected 
by 1.0 and 0.5 respectively in our 
experiment) and construct the English query 
on the basis of the formula (9). 
The document collection used in our 
experiments consists of several Chinese and 
corresponding English computer manuals, 
which include Linux-HOWTO, PostgreSQL 
handbook, Mysql handbook, Linux kernel* and 
Linux Gazette 17 volumes (from July, 1998 to 
Dec., 1999)". In order get a large number 
document Chinese and English collections, we 
decomposed these manuals and let every 
document no more than 15 sentences. As a 
* http://www.linux forum.nct/books/index.html 
* *http://www.linuxgazette.com.cn 
107 
result, Chinese-English bilingual comparable 
corpora are obtained in which contain about 
8,200 Chinese documents and 12,500 English 
documents. We design 13 Chinese queries, the 
average length is about 7 single Chinese 
character (about hree Chinese words). All work 
in this study was performed on the Search2000 
information retrieval system \[Du & Zhang 
(2000)\], which can process both Chinese and 
English Boolean queries. 
Table 4 shows the precision and recall table for 
the three methods. The first column in table 4 
contains precision values averaged 13 queries and 
interpolated to eleven recall points from 0.0 to 1.0 
in steps of 0.1. The third column contains 
precision values achieved by our translation 
method (QT). 
Precision Precision Precision 
Recall (WBW) (Manual) (QT) 
at 0.00 
at 0.10 
at 0.20 
at 0.30 
at 0.40 
at 0.50 
at 0.60 
at 0.70 
at 0.80 
at 0.90 
at 1.00 
Avg. 
0.5831 0.8975 0.6642 
0.5132 0.7884 0.5825 
0.4036 0.6573 0.5174 
0.3771 0.6206 0.4728 
0.3128 0.5840 0.4163 
0.2816 0.5118 0.3838 
0.2143 0.4876 0.3104 
0.1641 0.3833 0.2645 
0.1110 0.2114 0.1702 
0.0741 0.1667 0.1020 
0.0212 0.0428 0.0342 
0.2778 0.4865 0.3562 
Table 4: The results of the three methods 
The results in table 4 suggest that in this case, 
the WBW query translation leads to a great drop 
in effectiveness of 42.90% below that for 
monolingual retrieval (manual translation 
method). The result of our query translation 
method greatly improves effectiveness by 
28.22% over the WBW method, and its 
effectiveness is about 73.21% of that for 
monolingual retrieval. Although phrase 
translation is not executed directly in our 
method, the phrase information is kept 
effectively in the associated word list. Therefore, 
the phrase can be well ~anslated. The associated 
word list also provides a good context for 
translation of the Chinese query terms 
(corresponding to the first part of formula (6) 
i _Ml ( f~t) )  and a good English translation is 
given a relatively high weight. The results in 
table 4 show that our query translation method 
can construct a good English query and indeed 
improve the effectiveness. 
Conclusion 
Automatic word-by-word query translation is an 
attractive method because it is easy to perform, 
resources are readily available, and performance 
is similar to that of other CLIP,. methods. 
However, there are a lot of ambiguities in 
translation of the query terms and failures to 
translate phrases correctly, which are mainly 
responsible for the large drops in effectiveness 
below monolingual retrieval performance. 
Aiming to tackle with these problems, we 
develop a new scheme for how to select 
translations in this paper. In addition, rather than 
using a bilingual phrase dictionary, we also put 
forward a new method to translate phrases 
indirectly by using the mutual information 
between two words in a full sentence and keep 
the phrase information in the associated word 
list effectively. As a result of our query 
translation method, an English query is 
constructed in which each query term has a 
weight. 
In this study, our method leads to improve 
the effectiveness by 28.22% over the word by 
word query translation method, but is still about 
27% below the monolingual retrieval 
performance. If query expansion is employed in 
our method, we expect he performance should 
be further improved. A shortcoming of our 
method is that the cost of calculation of the 
mutual information matrices is very large. We 
are currently exploring an algorithm to generate 
the matrices more efficiently and the selection 
of coefficients in formula (6) also needs further 
research. 
Acknowledgements 
The authors wish to express their appreciation to
those interpreters of computer manuals. Without 
theft selfless contribution, our experiment 
would be impossible. Thanks to the anonymous 
reviewers for their helpful comments. 
References 
Ballesteros, L. and Croft, W. B.(1996). Dictionary- 
based methods for cross-lingual information 
retrieval. In Proceedings of the 7 '~ International 
DEXA Conference on Database and Expert 
Systems Applications,pp.791-801 . . . .  
108 
Church, K. W. and Hanks, P. (1990). Word 
association norms, mutual information and 
lexicography. Computational Linguistics, 16(1), pp. 
22-29. 
Davis, M. and Dunning, T. (1995). Query translation 
using evolutionary programming for multi-lingual 
information retrieval. In Proceedings of the 4 'h 
Annual Conference on Evolutionary Programming, 
pp. 175-185. 
Du, Lin and Sun, Yufang. (2000). A new indexing 
method based on word proximity for Chinese text 
retrieval. Journal of Computer Science and 
Technology,15(3),pp.280-286. 
Du, Lin; Zhang, Yibo and Sun, Yufang. (2000). The 
Design and Implementation of WEB-Based 
Chinese Text Retrieval System Search2000, (in 
Chinese). In Proceedings of 2000 International 
Conference on Multilingual Information 
Processing,pp.44-50. 
Fano,. R. (1961). Transmission of Information: A 
statistical theory of Communications. MIT Press, 
Cambridge, MA. 
Hull, D. A. and Grefenstette, G. (1996). Querying 
across languages: A dictionary-based approach to 
multilingual informaiton retrieval. In Proceedings 
of the 19 th International Conference on Research 
and Development in Information Retrieval,pp.49- 
57. 
Jing, Yufeng and Croft, W. Bruce. (1994). An 
association thesaurus for information retrieval. In 
Proceedings of RIA 0 94,pp. 146-160. 
Nie, Jian-Yun; Brisebois M. and Ren, Xiaobo. 
(1996). On Chinese text retrieval. In Proceedings 
of the 19 'h Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval,pp.225-233. 
Oard, D. W. (1996). A survey of multilingual text 
retrieval. Technical Report UMIACS-TR-96- 
19,http://www.ee.umd.edu/medlab/filter/papers/sig 
ir96.ps. 
Qiu, Yonggang, and Frei , H. P. (1993). Concept 
based query expansion. In Proceedings of the 16 'h 
Annual International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval,pp. 160-169. 
Sheridan, P. and Ballenni, J. P. (1996). Experiments 
in multilingual information retrieval using the 
spider system. In Proceedings of the 19 ~h 
International Conference on Research and 
Development i  Information Retrieval,pp.58-65. 
109 
Word Alignment of English-Chinese Bilingual Corpus 
Based on Chunks 
Sun Le, Jin Youbing, Du Lin, Sun Yufang 
Chinese Information Processing Center 
Institute of Software 
Chinese Academy of Sciences 
Beijing 100080 
P. R. China 
lesun, ybjin, yfsun, ldu@sonata.iscas.ac.cn 
Abstract 
In this paper, a method for the word alignment 
of English-Chinese corpus based on chunks is 
proposed. The chunks of English sentences are 
identified firstly. Then the chunk boundaries of 
Chinese sentences are predicted by the 
translations of English chunks and heuristic 
information. The ambiguities of Chinese chunk 
boundaries are resolved by the coterminous 
words in English chunks. With the chunk 
aligned bilingual corpus, a translation relation 
probability is proposed to align words. Finally, 
we evaluate our system by real corpus and 
present the experiment results. 
Key Words: Word Alignment, Chunk Alignment, 
Bilingual Corpus, Lexicon Extraction 
1 Introduction 
With the easier access to bilingual corpora, there 
is a tendency in NLP community to process and 
refine the bilingual corpora, which can serve as 
the knowledge base in support of many NLP 
applications, such as automatic or human-aid 
translation, multilingual terminology and 
lexicography, multilingual information retrieval 
system, etc. 
Different NLP applications need different 
bilingual corpora, which are aligned at different 
level. They can be divided by the nature of the 
segment to section level, paragraph level, 
sentence level, phrase level, word level, byte 
level, etc. 
As for our applications, we choose the chunk 
level to do alignment based on following 
considerations. Firstly, our applications, which 
include an example-based machine translation 
system, a computer aid translation system and a 
multilingual information retrieval system, need 
the alignment below the sentence level, on 
which we can acquire bilingual word and phrase 
dictionaries and. other useful translation 
information. Secondly, the word level alignment 
between English and Chinese language is 
difficult to deal with. There are no cognate 
words. The change in Chinese word order and 
word POS always produce many null and 
mistake correspondences. Next, we observe the 
phenomenon that when we translate the English 
sentence to Chinese sentence, all the words in 
one English chunk tend to be translated as one 
block of Chinese words which are coterminous. 
The word orders within these blocks tend to 
keep with the English chunk, also. So there are 
stronger boundaries between chunks than 
between words when we translate texts. Finally, 
as we all known, chunk has been assigned 
syntactic structure (Steven Abney, 1991), which 
comprises a connected sub-graph of the 
sentence's parse tree. So it's possible to align 
sentence structure and obtain translation 
grammars based on chunks by parsing. 
Many researchers have studied the text 
alignment problem and a number of quite 
encouraging results have been reported to 
different level alignments. With 
sentence-aligned corpus ready in hand, we focus 
our attention on the intra-sentence alignment 
between the sentence pairs. In this paper, a 
method for the word alignment of 
English-Chinese corpus based on chunks is 
proposed. The chunks of English sentences are 
identified firstly. Then the chunk boundaries of 
Chinese sentences are predicted by the bilingual 
lexicon and synonymy Chinese dictionary and 
heuristic information. The ambiguities of 
Chinese chunk boundaries are resolved by the 
coterminous words in English chunks. With the 
110 
chunk aligned bilingual corpus, a translation 
relation probability is proposed to align words. 
Although this paper is related to 
English-Chinese word alignment, he idea can 
be used to any other language bilingual corpora. 
In the following sections, we first present a brief 
review of related work in word alignment. Then 
discuss our alignment algorithm based on 
chunks in detail. Following this is an analysis of 
our experimental results. Finally, we close our 
paper with a discussion of future work. 
2 Related Work 
There are basically two kinds of approaches on 
word alignment: he statistical-based approaches 
(Brown et. al., 1990; Gale & Church, 1991; 
Dagan et. al. 1993; Chang, 1994), and the 
lexicon-based approaches (Ker & Chang, 1997; 
Wang et. al., 1999). 
Several translation models based on word 
alignment are built by Brown et al (1990) in 
order to implement the English-French 
statistical machine translation. The probabilities, 
such as translation probability, fertility 
probability, distortion probability, are estimated 
by EM algorithm. The Z 2 measure is used by 
Gale & Church (1991) to align partial words. 
Dagan (1993) uses an improved Brown model to 
align the words for texts including OCR noise. 
They first align word partially by character 
string matching. Then use the translation model 
to align words. Chang (1994) uses the POS 
probability rather than translation probability in 
Brown model to align the English-Chinese POS 
tagged corpus. Ker & Chang (1997) propose an 
approach to align Chinese English corpus based 
on semantic class. There are two semantic 
classes are used in their model. One is the 
semantic class of Longman lexicon of 
contemporary English, the other is synonymy 
Chinese dictionary. The semantic lass rules of 
translation between Chinese and English are 
extracted from large-scale training corpus. Then 
Chinese and English words are aligned by these 
rules. Wang (1999) also uses the lexicons to 
align the Chinese English bilingual corpus. His 
model is based on bilingual lexicon, sense 
similarity and location distortion probability. 
The statistical-based approaches need complex 
training and are sensitive to training data. It's a 
pity that almost no linguistic knowledge is used 
in these approaches. The lexicon-based 
approaches seem simplify the word alignment 
problem and can't obtain much translation 
information above word level. To combine these 
two approaches in a better way is the direction 
in near future. In this paper we proposed a 
method to align the bilingual corpus base on 
chunks. The linguistic knowledge such as POS 
tag and Chunk tag are used in a simply 
statistical model. 
3 Alignment Algorithm 
3.1 Outline of Algorithm 
For our procedure in this paper, the bilingual 
corpus has been aligned at the sentence l vel, 
and the English language texts have been tagged 
with POS tag, and the Chinese language texts 
have been segmented and tagged with POS tag. 
We have available a bilingual lexicon which 
lists typical translation for many of the words in 
the corpus. We have available a synonymy 
Chinese dictionary, also. We identify the chunks 
of English sentences and then predict he chunk 
boundaries of Chinese sentences from the 
translation of every English chunks and 
heuristic information by use of the bilingual 
lexicon. The ambiguities of Chinese chunk 
boundaries are resolved by the coterminous 
words in English chunks. After produce the 
word candidate sets by statistical method, we 
calculate the translation relation probability 
between every word pair and select the best 
alignment forms. The detail algorithm for word 
alignment is given in table 1. 
Step 1: According to the definition of Chunk in 
English, separate the English sentence into 
a few chunks and labeled with order 
number from left to fight. 
Step 2: Try to find the Chinese translation of 
every English chunk created in step 1 by 
bilingual dictionary and synonymy Chinese 
dictionary. If the Chinese translation is fred, 
then label the Chinese words with the same 
number used for the English chunk in step 
1. 
Step 3: Disambiguate the multi-label Chinese 
words by the translation location of 
coterminous words within the same English 
chunk. 
Step 4: Separate the Chinese sentence into a few 
chunks by heuristic information. 
Step 5: Save all the alignment at chunk level in 
111 
whole corpus as a base for word alignment. 
Step 6: Produce the word candidate sets by 
statistical method. 
Step 7: Calculate the translation relation 
probability between every word and it's 
candidate translation words. 
Step 8: Select he best translation by comparing 
the total TRP value in different alignment 
forms. 
Table 1. Outline of Alignment Algorithm 
3.2 Chunk Identifying of English 
Sentence 
Following Steven Abney (1991), there are two 
separate stages in chunking parser, which is the 
chunker and the attacher. The chunker converts 
a stream of words into a stream of chunks, and 
the attacher converts the stream of chunks into a 
stream of sentences. So only the chunker is 
needed in this paper. It's a non-deterministic 
version of a LR parser. For detail about chunker 
and the used grammars, please see Abney 
(1991). Then the chunks in one sentence are 
labeled with order number from left to right. 
3.3 Chunk Boundary Prediction of 
Chinese Sentence 
We observe the phenomenon that when we 
translate the English sentence to Chinese 
sentence, all the words in one English chunk 
tend to be translated as one block of Chinese 
words that are coterminous. The word orders 
within these blocks tend to keep with the 
English chunk, also. There are three examples in 
figure 1. The first sentence pair is chosen from 
an example sentence of Abney (1991). The 
second sentence pair is from a computer 
handbook. In these sentence pair all English 
chunks can find the exactly Chinese Chunk. In 
the third sentence pair only one English chunk 
can't find the exactly Chinese chunk for this 
sentence is chosen from a story and the 
translation is not literally. 
In order to find the Chinese translation of every 
English chunk, we use the bilingual dictionary 
and synonymy Chinese dictionary to implement 
the matching. If the Chinese translation of any 
words within the English chunk is found, then 
label the Chinese word with the same number 
used for labeling the English chunk. 
If there are Chinese words, which are labeled 
simultaneously by two or more number of 
English chunks, we use the number of nearby 
Chinese words to disambiguate. For example, in 
figure 2, the first Chinese word /~ j  may be 
correspondent to the English chunk 5 or 7. We 
have known that the words in one English chunk 
tend to be translated as one block of Chinese 
words that are coterminous, So it's easy to 
decide the first Chinese word )x~ ffJ is 
correspondent to the English chunk 7, the second 
Chinese word )x~ ~ is correspondent to the 
English chunk 5. By the same way, we can find 
the correct ranslations of Chinese word ~ 
and ~ is English chunk 6 and chunk 8 
respectively. In Step 4 of figure 2, the Chinese 
words with the same label number are bracketed 
with in one chunk. Finally, we separate the 
Chinese sentence into a few chunks by heuristic 
information based on POS tag (especially the 
preposition, conjunction, and auxiliary words) 
and the grammatical knowledge-base of 
contemporary Chinese (Yu shi wen, 1998). 
\[The b~ald man\] \[was itting\] [on his suitcase\]. 
\[To a c c e ~ _ ~ _ ~ _ ~ c l i c k \ ]  Ion "Su2.p..9.~'l. 
\[I gathered\] \[from what hey said\] ,\[that an elder sister\] [of his\] \[ was coming \] \[to stay with them\],\[ and 
that s h e ~ \ ]  \[ that e v ~  
\ [~ ' f l ' \ ]~qb\ ] \ [~\ ] \ [~\ ] \ [  - ~ \ ] \ [~\ ] \ [~ l ' \ ]~- -~\ ] ,  \ [~ .R~\ ] \ [~_h\ ] \ [~ l J \ ] .  
Figure 1. Three Examples of Chunk Afignment 
112 
Step 1 English chunks with order number 
\[This product 1\] \[is designed 2\] for \[low-cost 3\], \[turnkey solutions 4\] and \[mission-critical 
applications 5\] that \[require 6\] \[a central application host 7\] and \[ do not require 8\] \[networking 9\]. 
Step 2 Label the translation of English chunk with it's order number 
i~(1) ~( I )  ~ ~j ~(6 /8 )  --~(7) ~,~,(7) ~(5 /7 )  5\]~01,(7) ~ ~(8) ~(6 /8)  I~ 
~(9)  {k~(3) ~2~:(3). ~ ~\ ] '~ '~ ~(4)  ~~(4)  ~ ~.~'~(5)  ' f~-~(5 /7)  ~ ~gJ- 
(2)? 
Step 3 Disambiguate the multi-label Chinese words 
i.~(1) ~:~(1) ~ ~ ~,~(6)  - -~(7)  @,~,(7) ~(7)  5\]E;~R(7) ~ ~(8)  ~(8)  ~\ ]~(9)  
t~ (3) ~.  (3). ~ ~ 9  ~: (4 )  ~ (4) 7A Y~'Pi(5) i~-  ~ (5) ~ ~2@~9 (2), 
Step 4. Separate the Chinese sentence into a few chunks 
~(3)3, E~ ~'~9 ~:  ~(4) \ ]  ~ \[Y~'I~ ~ ~(5) \ ]  ~ \[~,i-~9(2)\]0 
Figure 2. An Example for Chunk Alignment Algorithm from Step 1 to 4 
3.4 Calculation of Translation Relation 
Probability for Words 
With the alignments at chunk level of whole 
corpus, we propose a Translation Relation 
Probability (TRP) to implement the word 
alignment. The translation Relation probability 
of words are given by following equation: 
P~ - L:~ (1) 
L 'L  
Where f? is the frequency of English word in 
whole corpus; fc is the frequency of Chinese 
Word in whole corpus; f~ is calculated by 
follow equation: 
N /ln( 2Lay ) + ln(Lav) 
I L~i + Lci 
(2) 
Where Lmv is the average words number of all 
English chunks and all Chinese chunks which 
are related to the English word in whole Corpus; 
L~i is the word number of the English chunk in 
which the English candidate words co-occur 
with the Chinese words; ~ is the word number 
of the Chinese chunk in which the English 
candidate words co-occur with the Chinese 
words; N is the total number of chunks in which 
the English word co-occur with the Chinese 
word; 13?e is the penalty value to indicate the 
POS change between the Engfish word and the 
Chinese word. 
By this equation we connect he chunk length 
and POS change with the co-occurrence 
frequency. The less the chunk length, the higher 
the translation relation probability. For example, 
the chunk pak, which is composed by one 
English word and two Chinese words, is more 
reliable than the chunk pair, which is composed 
by four English words and four Chinese words. 
An example is given in figure 3. There are 5 
possible alignment forms in our consideration 
for this chunk, which includes three Engfish 
words and three Chinese words. Then calculate 
the total TRP value for every possible alignment 
word pairs in each alignment form by equation 
(1). After we get the total TRP value for each 
alignment form, we choose the biggest one. 
floppy disk drive 
III 
A 
floppy disk drive floppy disk drive 
B C 
floppy disk drive floppy disk drive 
X1 
D E 
Figure 3. The Possible Word Alignment Forms in One Chunk 
113 
4 Experimental Results 
4.1 System Architecture 
English 
corpus 
Chunk L Identifying 
~ lSentence  Aligned 
ingual Corp~ 
! 
Word Dictionary ~ "~ 
Segmented an  
tagged Chinese 
Corpus 
~"~Grammar Rule fo r~ 
-.. Chunk Constructing ~ 
Tagged 
Sentence 
~cUristic information f r~ 
hunk Constructing J 1 E y!  
~ ual Database ~-~ Translation Dictionary ) 
I I 
Source Text 
I 
Example Based Machine 
Translation System 
User's Languages Inquiry ~ Multilingual Information t Retrieval Results 
Retrieval System ~> 
Chunk 
Identifying 
Chunk 1 Tagged 
Sentence 
Target Text~ Computer Aid
? Translation System 
1 
Figure 4. System Architecture 
114 
4.2 Experiment Results 
We tested our system with an English-Chinese 
bilingual corpus, which is part of a computer 
handbook (Sco Unix handbook). There are 
about 2246 English sentence and 2169 Chinese 
sentence in this computer handbook after filter 
noisy figures and tables. Finally we extracted 
14,214 chunk pairs from the corpus. The 
accuracy for automatic chunk alignment is 
85.7%. The accuracy for word alignment based 
on correctly aligned chunk pairs is 93.6%. The 
errors mainly due to the following reasons: 
Chinese segmentation error, stop words noise, 
POS tag error. The parameter 13ec we used in 
equation (2) should be chosen from the training 
corpus. In table 2, the total TRP values of 
example in figure 3 are showed. The alignment 
form D is the best. 
(floppy I -~)  
(disk I ~)  
(drive I ~) 
(floppy \[ ~ ~)  
(disk drive I :~:) 
(floppy\[ ~) 
(disk drive I ~ ~) 
(floppy disk\[ .~7~) 
(drive I -Sg~ :~) 
(floppy disk \[ ~3~ ~J)  
(drivel ~)  
0.9444 
0.0212 
0.1722 
0.2857 
0.1765 
0.9444 
0.3529 
0.8333 
0.8947 
X 1/3 
X 1/3 
X 1/3 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
0.3429 X 1/2 
0.1722 X 1/2 
Total TRP of A =0.3792 
Total TRP of B =0.3194 
Total TRP of C =0.6485 
Total TRP of D =0.8640 
Total TRP of E =0.2576 
Table 2. Total TRP Value for Example in Figure 3 
5 Conclusions and Future Work 
With the more and more bilingual corpora, there 
is a tendency in NLP community to process and 
refine the bilingual corpora, which can serve as 
the knowledge base in support of many NLP 
applications. In this paper, a method for the 
word alignment of English-Chinese corpus 
based on chunks is presented. After identified 
the chunks of English sentences, we predict he 
chunk boundaries of Chinese sentences by the 
bilingual exicon, synonymy Chinese dictionary 
and heuristic information. The ambiguities of 
Chinese chunk boundaries are resolved by the 
coterminous words in English chunks. After 
produce the word candidate sets by statistical 
method, we calculate the translation relation 
probability between every word pair and select 
the best alignment forms. We evaluate our 
system by real corpus and present the results. 
Although the results we got are quite promising 
to bilingual English Chinese text, there are still 
much to do in near future. The corpus we use in 
our experinaent is a relative small corpus about 
computer handbook, in which the terms are 
translated with high consistency. We should 
extend our method to the large corpus of other 
domains without lost much accuracy. To 
increase the correct rate of Chinese word 
segmentation is important for our word 
alignment. To extract he corresponding syntax 
information of English Chinese bilingual corpus 
by shallow parsing is a direction for future work, 
also. 
Acknowledgements 
This research was. funded by Natural Science 
Foundation of China (Grant No. 69983009). 
The authors would like to thank the anonymous 
reviewers for their helpful comments. 
References 
Abney, Steven, 1991. Parsing by Chunks. In: Robert 
Berwick, Steven Abney and Carol Tenny (eds.), 
Pringciple-Based Parsing, Kluwer Academic 
Publishers 
Brown, P. F., Della Pietra, S. A., Della Pietra, V., J., 
and Mercer, R. L., 1993. The Mathematics of 
Statistical Machine Translation: Parameter 
Estimation. In.Computational Linguistics, 19(2), 
pp.263-311. .... 
Chang, J. S., and Chert, M. H. C. 1994 Using Partial 
115 
Aligned Parallel Text and Part-of-speech 
Information in Word Alignment. In Proceedings of 
the First Conference of the Association for 
Machine Translation in the Americas(AMTA94), 
pp 16-23 
Dagan, I. and Church, K. W. 1994 Termight: 
Identofying and Translating. Technical 
terminology. InProceedings ofEACL 
Fung, P., and Church, K. W., 1994. K-vec: A New 
Approach for Aligning Parallel Texts. In 
Proceedings of the 15th International Conference 
on Computational Linguistics (COLING94), 
Japan, pp. 1096-1102, 
Gale, W. A., and Church, K. W., 1991. A Program for 
Aligning Sentences in Bilingual Corpora. In 
Proceedings of the 29th Annual Meeting of the 
Association for Computational Linguistics 
(ACLgl), pp. 177-184 
Kay, M., and Roscheisen M., 1993. Text-Translation 
Alignment. Computational Linguistics, 
19/l,pp.121 
Ker, M. and Chang, J. S. 1997 A Class-Based 
Approach to Word Aligmnent. Computational 
Linguistics,23(2),pp 313-343 
Langlais, Ph., Simard, M., Veronis, J., Armstong, S., 
Bonhomme, P., Debili, F., Isabelle, P., Souissi, E., 
and Theron, P., 1998. Arcade: A cooperative 
research project on parallel text alignment 
evaluation. In First International Conference on 
Language Resources and Evaluation, Granada, 
Spain. 
Melamed, I. D. 1996. Automatic Detection of 
Omissions in Translations. In Proceedings of the 
16th International Conference on Computational 
Linguistics, Copenhagen, Denmark 
Sun, Le, Du, Lin, Sun, Yufang, Jin, Youbin 1999 
Sentence Alignment of English-Chinese Complex 
Bilingual Corpora. Proceeding of the workshop 
MAL99, 135-139 
Wang, Bin, Liu, Qun, and Zhang, Xiang, 1999 An 
Automatic Chinese-English Word Alignment 
System. Proceedings of ICMI99, ppl00-104, 
Hong Kong 
Wu, Daikai.and Xia, Xuanyin. 1995. Large-Scale 
Automatic Extraction of an English-Chinese 
translation Lexicon. Machine Translation, 
9:3--4,285-313 
Yu, Shiwen, Zhu, Xuefeng, Wang, Hui, Zhang 
Yunyun, 1998 The Grammatical Knowledge-base 
of Contemporary Chinese: A complete 
Specification. Tsinghua University Publishers 
116 
Constructing of a Large-Scale Chinese-English  
Parallel Corpus 
 
Le Sun, Song Xue, Weimin Qu, Xiaofeng Wang,Yufang Sun 
Chinese Information Processing Center 
Institute of Software, Chinese Academy of Sciences 
Beijing 100080, P. R. China 
lesun, bradxue, qwm, wxf, yfsun@sonata.iscas.ac.cn 
 
Abstract  
This paper describes the constructing of a 
large-scale (above 500,000 pair sentences) 
Chinese-English parallel corpus. The current 
status of Chinese corpora is overviewed with 
the emphasis on parallel corpus. The XML 
coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing. Finally, we show the design of 
the concordance of the parallel corpus and 
the prospect to further development. 
Introduction 
With the development of the corpus linguistics, 
more and more language resources have been 
established and used in language engineering 
research and applications. As we all know, there 
are different kinds of corpora for different kinds 
applications. For example, the Chinese 
Part-Of-Speech annotation corpus used to train 
program for Chinese word segmentation and 
POS tag, the Chinese tree bank used to Chinese 
syntax study, and so on.  
 
In this paper the constructing of a large-scale 
Chinese-English parallel corpus, which is totally 
above 500,000 pair sentences and the first year 
task is 100,000 pair sentences, is described. The 
applications of the large-scale Chinese-English 
parallel corpus put emphasis on the sentence 
template extracting for EBMT (Example-Based 
Machine Translation) and translation model 
training for SBMT (Statistical-Based Machine 
Translation). The latent applications may include 
the bilingual lexicon extraction, special term or 
phase extraction, bilingual teaching, 
Chinese-English contrastive study, etc.  
 
Numerous corpus data gathering efforts exit all 
of the world. The rapid multiplication of such 
efforts has made it critical to create a set of 
standards for encoding corpora. CES (Corpus 
Encoding Standard), which is conformant to the 
TEI Guideline for Electronic Text Encoding and 
Interchange of the Text Encoding Initiative (TEI 
2002), has been adopted by many corpus-based 
work. The XML Corpus Encoding Standard 
(XCES) is a part of the Guideline developed by 
the Expert Advisory Group on Language 
Engineering Standards (Ide, N., Bonhomme, P., 
Romary, L. 2000). The coding of our 
Chinese-English Parallel Corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. 
 
In the following section, we first present a brief 
review of the current status of Chinese corpora 
with the emphasis on parallel corpus. Then the 
XML coding principles for Chinese?English 
parallel corpus are discussed in detail. Following 
this is the sentence alignment algorithm used in 
this project with a computer-aided checking 
processing. Finally, we show the design of the 
concordance of the parallel corpus and the 
prospect to further development. 
1 Chinese Corpus Project Overview 
The Chinese Corpus constructing work started in 
1920?s, See Zhiwei Feng (2001). The 
machine-readable corpora established in 1980?s 
are listed as following: 
 Chinese Modern Literature Corpus 
(1979), 5.27 Million Chinese 
Characters, WuHan University; 
 Modern Chinese Corpus (1983), 20 
Million Chinese Characters, Beijing 
University of Aeronautics and 
Astronautics; 
 Middle School Chinese Book 
Corpus (1983), 1.06 Million Chinese 
Characters, Beijing Normal University; 
 Modern Chinese Word Frequency 
Corpus (1983), 1.82 million Chinese 
characters, Beijing Language & 
Culture University. 
 
The first national large-scale Chinese corpus 
project is proposed in 1991 by State Language 
Commission in China. The Chinese texts used in 
this corpus are selected carefully under the 
condition of times, genre, and field. Now the 
corpus is about 20 million Chinese characters. 
 
From 1992, there are several large-scale Chinese 
corpus constructed by different institutes. The 
most noticeable in them is the Chinese POS 
annotation corpus accomplished by Institute of 
Computational Linguistics, Peking University, 
with the cooperation with Fujitsu Company. The 
content of this corpus is people?s daily, one of 
the most popular newspapers in China. The 
Chinese texts are segmented and added POS tag 
with high precision. The total Chinese 
Characters are about 27 million.  
 
There are several Chinese corpora in Tsinghua 
University also. The corpus, which is used for 
Chinese segmentation study, includes 100 
million Chinese characters. The Hua Yu corpus 
(2 million Chinese characters) is a POS tagged 
field-balance corpus. And the 10 percent of this 
corpus has been used for constructing Chinese 
tree bank. 
 
These are also other valuable Chinese corpora 
established in ShanXi University, Harbin 
technical University, ShangHai Normal 
University, City University of Hong Kong, 
Taiwan Academia Sinica, University of 
Pennsylvania and so on. Please refer to Zhiwei 
Feng (2001) for detail.  
 
In October 2001, a national corpus project, that 
is, national 863 project about Chinese 
Information Processing Platform, is launched. 
It?s a cooperation project between five institutes 
in China, including Institute of Software, 
Chinese Academy of Sciences, Institute of 
Computational Linguistics, Peking University, 
Tsinghua University, Nanjing University and 
Institute of Language, State Language 
Commission. The content of corpora and 
intended scale in this project are showed in table 
1 in detail. The large-scale Chinese-English 
parallel corpus described in this paper is one of 
the scheming corpora in this project. 
 
The multilingual corpus is important for 
computational linguistics research and 
contrastive linguistics study. So there are many 
multilingual corpus have been established or 
being developed in many institutes in China 
mainland. The table 2 shows the 
Chinese-English parallel corpus had been 
constructed in Mainland China. There are also 
some bilingual corpora about other language pair, 
such as Chinese-Japanese, Chinese-German, etc.
 
 
Sub-Project Name Responsible Institute First-Year  Scale 
Scheming
 Scale 
Chinese Balance Corpus State Language Commission 70 MCC 150 MCC
Chinese-English Parallel Corpus  IOS, Chinese Academy of Science 100 TS 500 TS 
Chinese POS Annotation Corpus ICL, Peking University 7 MCC 30 MCC 
Chinese Tree Bank Tsinghua University 15 TS 60 TS 
Chinese Concept Dictionary ICL, Peking University 20 TC 60 TC 
Chinese Semantic Knowledge Base Tsinghua University 8 TW 24TW 
Table 1 The 863 Chinese corpus project 
 
MCC: Million Chinese Character       TS: Thousand Sentence 
TC: Thousand Concept                 TW: Thousand Word 
Institute  Corpus Describing  
 Scale 
ICL, Peking University Sentence & Phrase Alignment 5 TS 
Harbin Institute of Technology  Sentence, Phrase, Word Alignment Above 5 TS 
State Language Commission Computer Science and Plato Unknown 
Beijing Foreign Studies University Literary, Science and Civilization in 
China 
Unknown 
Northeastern University Sentence & Phrase Alignment Unknown 
IOS, Chinese Academy of Science Sentence Alignment  8 TS 
Table 2 The Chinese-English parallel corpus in Mainland 
 
It has been noticed by many scholars that we 
should build a principle for sharing language 
resource in research work and to avoid the waste 
in time and effort in repeated construction. 
 
2 Resource Collection 
Unlike single linguistic resource, the parallel 
resource for special language pair is limited no 
matter what language pair is. Although the 
Chinese and English both are most popular 
language in the world, we still encounter much 
difficult in obtaining parallel corpus resource 
from Internet for following reasons: 
There are seldom web pages in China 
provide the same content in English 
pages and in Chinese pages; 
The English news in web are translated 
freely other than literally with many 
content omission; 
Some bilingual texts are restricted and 
used only to member.  
 
After two years efforts, there are totally about 
16,000KB untagged Chinese-English parallel 
texts in hand. The genres of the resource we 
collected are showed in table 3. 
 
 
Chinese Genre About Percent
News 10% 
Literature 30% 
Government 
Report 25% 
Sciences & 
Technology 35% 
Table 3 The genre in parallel texts 
3 Coding 
3.1 General Principles 
The coding of the parallel corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. The eXtendible Make-up Language (XML) 
is used for the text coding. Textual features are 
marked by tags enclosed within angle brackets. 
For example, a title is marked by start tag <title> 
and an end tag </title >. Every element has some 
attributes to identifier of the element.  
 
The document type definition (DTD) for the 
texts in the corpus may differs in some respects 
from the TEI model. The general principle for 
coding are based on following consideration: 
Comply with TEI guide lines on the 
whole; 
Define the tag with clear meaning used 
by most people in china; 
Only used the attributes which can be 
easily and automatically get from source 
texts, except the alignment link, which is 
the key attribute in this corpus and 
several steps are used to keep high 
precise (See section 4 for detail); 
Try to keep all the interim resource in 
hand in case information loses, such as, 
the title tag in HTML files. 
 
The overall structure of a Chinese-English 
Parallel corpus is shown by this example: 
<article id=?UH001?> 
<Header type =?Unix Handbook?> 
</Header> 
<text> 
</text> 
</article> 
There are two main parts in a text: a header and 
the main text. Every text has an unique identifier 
that is, article id, in this case UH001 (indicating 
text 001 of the Unix Handbook) 
3.2 The header 
Each text is described by a header, which has 
four parts in accordance with the TEI guidelines: 
a file description, an encoding description, a 
profile description, and a revision description. 
The file description gives bibliographical 
information on the source text. The elements 
include title, author, www address (If the text is 
obtain from Internet), etc. The encoding 
description in our corpus is very brief, only the 
project name and the DTD file name are listed.  
 
The country or region use the language is 
indicated in the profile description. The 
description under <language> used in our corpus 
is in terms of labels like: Mainland Chinese 
(MaC), Hong Kong Chinese (HKC), Taiwan 
Chinese (TwC), Singapore Chinese (SiC), 
American English (AmE), British English (BrE), 
Canadian English (CaE), etc. 
 
Another tag used in the profile description is 
<textclass>. According to the parallel resource 
in hand, the texts are grouped into 4 genres (as 
show in table 3), such as, News , Literature, 
Science & Technical?Government Report. 
A series of changes are listed in the revise 
description and specified the change, the date of 
the change, the person responsible for the 
change, and the nature of the change. 
3.3 Text Units and Alignment Unit 
The corpus texts are segmented according to the 
natural units, such as: chapter, paragraph, 
sentence (S-unit), and word. The English words 
are simply marked by spacing as in ordinary 
written text. The Chinese words are not 
indicated by space in order to avoid the segment 
error. 
 
An ID is given to every paragraph to indicate the 
relative position in whole chapter. The sentence 
is called S-unit, the same as Johansson, Ebeling 
and Oksefjell (1999) to underline that they are 
not necessarily sentences in a grammatical sense. 
 
The sentence alignment type between Chinese 
S-unit and English S-unit maybe 1:1, 2:1, 3:1, 
1:2, 1:3,2:2, 3:2, 2:3. Links between parallel 
texts are showed by attributes of S-Alignment. 
One of the Chinese alignment unit (it may 
beyond one S-unit) are linked with the 
correspondence English alignment unit.   
3.4 Sample Text 
A sample text of our Chinese-English parallel 
corpus is showed in figure 1.  
 
 
Figure1 Sample Text  
4 Sentence Alignment 
4.1 Algorithm Overview 
The key attribute in this corpus is alignment link, 
which connect the one or more Chinese sentence 
with one or more correspond English sentence. 
In order to keep high precise in sentence 
alignment, several steps are used with the human 
and computer cooperation. 
 
The first step to extract structural information for 
parallel corpus is paragraph alignment and 
sentence alignment, that is noting which 
paragraph and sentence in one language 
correspond to which paragraph and sentence in 
another language.  
 
This problem has been studied by many 
researchers and a number of quite encouraging 
results have been reported. However, almost all 
bilingual corpora used in research are clear 
(nearly without sentence omission or insertion) 
and literal translation bilingual texts. The 
performance tends to deteriorate significantly 
when these approaches are applied to noisy 
complex corpora (with sentence omission or 
insertion, less literal translation). 
 
There are basically three kinds of approaches on 
sentence alignment: the length-based approach 
(Gale & Church 1991 and Brown et al 1991), 
the lexical approach (key & Roscheisen 1993), 
and the combination of them (Chen 1993, Wu 
1994 and Langlais 1998, etc.). 
 
The first published algorithms for aligning 
sentences in parallel texts are length-based 
approach proposed by Gale & Church (1991) 
and Brow et al(1991). Based on the observation 
that short sentences tend to be translated as short 
sentences and long sentences as long sentences, 
they calculate the most likely sentence 
correspondences as a function of the relative 
length of the candidates. The basic approach of 
Brow et al is similar to Gale and Church, but 
works by comparing sentence length in words 
rather than characters. While the idea is simple, 
the models can still be quite effective when used 
to clear and literal translated corpora. Once the 
algorithm had accidentally mis-aligned a pair 
sentence, it tends to be unable to correct itself 
and get back on track before the end of the 
paragraph. Use alone, length-based alignment 
algorithms are therefore neither very robust nor 
reliable. 
 
Kay & Roscheisen (1993) use a partial 
alignment of lexical items induce a maximum 
likelihood at sentence level. The method is 
reliable but time consuming.  
 
Chen (1993) combines the length-based 
approach and lexicon-based approach together. 
A translation model is used to estimate the cost 
of a certain alignment, and the best alignment is 
found by using dynamic programming as the 
length-based method. The method is robust, fast 
enough to be practical and more accurate than 
previous methods. 
 
The first sentence alignment model used to align 
English-Chinese bilingual texts is proposed by 
Wu (1994). For lack of cognates in 
English-Chinese, he used lexical cues to add the 
robust of his model. 
 
All of these works are test on nearly clear and 
literal translation bilingual corpora. 
 
There are seldom papers related to paragraph 
alignment. It's believed by most of the 
researchers that the paragraph alignment is an 
easier task than sentence alignment. Gale & 
Church (1991) suggest that the same 
length-based algorithm can be used to align 
paragraph also. 
 
4.2 The Alignment Steps 
Sentence alignment algorithm of our system can 
be outlined as follows: 
 
Step 1: Align sentence by the improved 
length-based algorithm.(Desicibed in Sun 
etc. 1999) 
Step 2: A lexicon checking process is added to 
judge all the alignment results in step 1. 
A score is given to every alignment pair 
(A Chinese word segmentation system is 
used in this process to find Chinese 
word). 
Step 3: The alignments whose score above a 
threshold C1 are judged as correct 
alignment. Remove these correct 
alignments from bilingual texts temporally. 
Step 4: The rest parts are aligned again by length 
based approach. 
Step 5: Repeat step 2, the score of every 
lignment is showed as a reference to human 
checking. 
 
4.3 Computer-Aided Checking 
It's obviously difficult to increase greatly the 
accuracy and robust of sentence alignment only by 
length based approach. So a lexicon checking 
process is added to our system. The alignment 
results obtained by length based approach are 
checked by an English-Chinese lexicon. A score SA 
is given to every alignment sentence pair. The 
score SA is calculated by following idea, that is, 
the twice number of correctly matched English 
words and Chinese words to the sum of number 
of English and Chinese words. In figure 2, the 
interface for human checking is showed in order 
to processes the noise Chinese-English parallel 
resource. 
 
4.4 Experiment Results 
 
We tested our alignment algorithm with part of a 
computer handbook (Sco Unix handbook). There 
are about 4681 English sentences and 4430 
Chinese sentences in this computer handbook 
after filter noisy figures and tables. The detail 
experiment result of automatic sentence 
alignment is show in table 4. The total precision 
is about 95%.  
 
 
Figure 2 Interface for Human Checking 
 
Class of 
Alignment 
No. of 
Aligned Sentence 
Pair 
No. of 
Correct 
Sentence Pair 
No. of 
Error 
Sentence Pair 
 
Precision 
1:1 2992 2957 35 98.83% 
1:2 238 211 27 88.66% 
2:1 414 352 62 85.02% 
2:2 113 97 16 85.84% 
1:3 35 24 11 68.57% 
3:1 75 49 26 65.33% 
2:3 13 6 7 46.15% 
3:2 22 16 6 72.72% 
3:3 6 3 3 50.00% 
0:1 3 2 1 66.67% 
1:0 7 4 3 75.00% 
Total 3918 3721 197 94.97% 
Table 4 The detail experiment result of automatic sentence alignment 
5 Bilingual Concordance Design 
We also designed a bilingual concordance tool 
used for discovering facts during the translation 
between Chinese and English. Besides a listing 
of the keywords with the contexts in which they 
appear, the correspondence translation sentence 
also be presented in this tool. The options may 
include bilingual concordances, sorting in a 
variety of orders, and producing basic text 
statistics. The intended interface is showed in 
figure 3.  
  
 
Figure 3 The Interface for bilingual Concordance 
 
6 Conclusion & Further Prospects 
In this paper, we introduce the developing project, 
that is, the constructing of a large-scale (above 
500,000 pair sentences) Chinese-English parallel 
corpus. The current status of Chinese corpora is 
overviewed with the emphasis on parallel corpus. 
The XML coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing in order to processes the noise 
Chinese-English parallel resource.. We show the 
design of the bilingual concordance for the 
parallel corpus, also.  
 
As a beginning project, there is still much room 
for further development. The parallel resource is 
relative rare, so the new ways, such as, data 
exchange with other researcher institute and 
translation company, should be launched to 
obtain more parallel resource which can be used 
to research society. The coding principle should 
be adjusted in real work. A coding rule in more 
detail should form in near future. We also intend 
to add the option for recommendation the 
correspondence translation word for input 
keywords in concordance tool.   
 
Acknowledgements 
This work is supported by China 863 project 
(Grant No. 2001AA114040) and the National 
Science Fund of China under contact 69983009. 
Our thanks go to all the project members from 
five institutes for discussion and the anonymous 
reviewers for kind suggestions. . 
 
References  
Catherine N. Bal (1997), Tutorial: 
Concordances and Corpora, 
http://www.georgetown.edu/cball 
/corpora/tutorial.html  
D. Wu, (1994) Aligning a Parallel 
English-Chinese Corpus Statistically with 
Lexical Criteria, In Proceedings of the 32nd 
Annual Meeting of the Association for 
Computational Linguistics (ACL'94), 
pp.80-87 
I. D. Melamed. (1996)  Automatic Detection of 
Omissions in Transaltions, In Proceedings of  
the 16th International Conference on 
Computational Linguistics, Copenhagen, 
Denmark 
ISLE, International Standards for Language 
Engineering 
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE
_Home_Page.htm 
J.S. Chang and M. H. Chen (1997)  An 
alignment method for noisy parallel corpora 
based on image processing techniques, In 
Proceedings of the 35th Meeting of the 
Association for Computational Linguistics, 
Madrid, pp. 297-304 
Kay M., and Roscheisen M. (1993). 
Text-Translation Alignment, Computational 
Linguistics, 19/1,pp.121-142 
Le Sun , Lin Du, Yufang Sun, Jin Youbin (1999) 
Sentence Alignment of English-Chinese 
Complex Bilingual Corpora. Proceeding of 
the workshop MAL'99, 135-139 
N. Ide, L. Romary (2001). A Common 
Framework for Syntactic Annotation 
Proceedings of ACL'2001, Toulouse, 298-305 
N. Ide, L. Romary, (2000) XML Support for 
Annotated Language Resources. Proceedings 
of the Workshop on Web-based Language 
Documentation and Description, 
Philadelphia, 148-153. 
N. Ide, P. Bonhomme,, L. Romary (2000). 
XCES: An XML-based Standard for 
Linguistic Corpora.. Proceedings of the 
Second Language Resources and Evaluation 
Conference (LREC), Athens, Greece, 825-30. 
P. F. Brown, J. C. Lai, and R. L. Mercer (1991) 
Aligning Sentences in Parallel Corpora, In 
Proceedings of the 29th Annual Meeting of 
the Association for Computational 
Linguistics (ACL'91), pp.169-176. 
P. Fung, and K. W. Church (1994)  K-vec: A 
New Approach for Aligning Parallel Texts, In 
Proceedings of the 15th International 
Conference on Computational Linguistics 
(COLING'94), Tokyo, Japan, pp. 1096-1102, 
Ph. Langlais, M. Simard, J. Veronis, 
S.Armstong, P. Bonhomme, F. Debili, P. 
Isabelle, E. Souissi, and P. Theron. (1998)  
Arcade: A cooperative research project on 
parallel text alignment evaluation. In First 
International Conference on Language 
Resources and Evaluation, Granada, Spain. 
S. F. Chen, (1993) Aligning Sentences in 
Bilingual Corpora Using Lexical Information. 
In Proceedings of the 31th Annual Meeting 
of the Association for Computational 
Linguistics, pp. 9-16 
Shiwen Yu, Xuefeng Zhu, Hui Wang, Yunyun 
Zhang (1998), The Grammatical 
Knowledge-base of Contemporary Chinese: 
A complete Specification. Tsinghua 
University Publishers 
Stig Johansson, Jarle Ebeling, Signe Oksefjell 
(1999), English-Norwegian Parallel 
Corpus:Manual, 
http://www.hf.uio.no/iba/prosjekt/ 
TEI (2002) The XML Version of the TEI Guidelines   http://www.hcu.ox.ac.uk/TEI/Guidelines/ 
W. A. Gale, and K. W. Church (1991) A Program 
for Aligning Sentences in Bilingual Corpora, 
In Proceedings of the 29th Annual Meeting of 
the Association for Computational Linguistics 
(ACL'91), pp. 177-184 
Zhiwei Feng (2001), The History and Current 
status of Chinese Corpus Research, 
International Conference on Chinese 
Computing ICCC2001, pp. 1-15 (In Chinese)  
 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 181?184,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition Based on 
Conditional Random Fields Models 
 
 
Yuanyong Feng Le Sun Yuanhua Lv 
Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
{yuanyong02, sunle, yuanhua04}@ios.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, 
part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER 
open track under the framework of Con-
ditional Random Fields (CRFs) model. 
The techniques used for the close NER 
and word segmentation tracks are also 
presented. 
1 Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et 
al., 2001) framework. It integrates multiple fea-
tures based on single Chinese character or space 
separated ASCII words. The early designed sys-
tem (Feng et al, 2005) is used for the MSRA 
NER open track this year. The output of an ex-
ternal part-of-speech tagging tool and some care-
fully collected small-scale-character-lists are 
used as outer knowledge. 
The close word segmentation and named en-
tity recognition tracks are also based on this sys-
tem by some adjustments.  
The remaining of this paper is organized as 
follows. Section 2 introduces Conditional Ran-
dom Fields model. Section 3 presents the details 
of our system on Chinese NER integrating mul-
tiple features. Section 4 describes the features 
extraction for close track. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2 Conditional Random Fields Model 
Conditional random fields are undirected graphi-
cal models for calculating the conditional prob-
ability for output vertices based on input ones. 
While sharing the same exponential form with 
maximum entropy models, they have more effi-
cient procedures for complete, non-greedy finite-
state inference and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines 
the corresponding state sequence s? probability as 
follows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f s s t
Z
?? ?
=
= ??
o
s o o
 
(1)
Where ? is the model parameter set, Zo is the 
normalization factor over all state sequences, fk is 
an arbitrary feature function, and ?k is the learned 
feature weight. A feature function defines its 
value to be 0 in most cases, and to be 1 in some 
designated cases. For example, the value of a 
feature named ?MAYBE-SURNAME? is 1 if 
and only if st-1 is OTHER, st is PER, and the t-th 
character in o is a common-surname. 
The inference and training procedures of 
CRFs can be derived directly from those equiva-
lences in HMM. For instance, the forward vari-
able ?t(si) defines the probability that state at 
time t being si at time t given the observation 
sequence o. Assumed that we know the proba-
bilities of each possible value si for the beginning 
state  ?0(si), then we have 
1( ) ( ) exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?= ? ? o (2)
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3 Chinese NER Using CRFs Model Inte-
grating Multiple Features for Open  
Track 
In our system the text feature, part-of-speech 
(POS) feature, and small-vocabulary-character-
lists (SVCL) feature are combined under a uni-
fied CRFs framework. 
181
The text feature includes single Chinese char-
acter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Our POS tag set 
follows the criterion of modern Chinese corpora 
construction (Yu, 1999), which contains 39 tags.  
The last feature is based on lists. We first list 
all digits and English letters in Chinese. Then 
most frequently used character feature in Chinese 
NER are collected, including 100 single charac-
ter surnames, 100 location tail characters, and 40 
organization tail characters. The total number of 
these items in our lists is less than 600. The lists 
altogether make up a list feature (SVCL). Some 
examples of this list are given in Table 1. 
 
 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an ob-
servation sequence is feed into the model. 
Each token state is a combination of the type 
of the named entity it belongs to and the bound-
ary type it locates within. The entity types are 
person name (PER), location name (LOC), or-
ganization name (ORG), date expression (DAT), 
time expression (TIM), numeric expression 
(NUM), and not named entity (OTH). The 
boundary types are simply Beginning, Inside, 
and Outside (BIO).  
4 Feature Extraction for Close Tracks 
In close tracks, only character and word list fea-
tures which are extracted from training data are 
applied for word segmentation. In NER track we 
also include a named entity list extracted from 
the training data.  
To extract the list feature, we simply search 
each text string among the list items in maximum 
length forward way. 
Taking the word segmentation task for in-
stance, when a text string c1c2?cn is given, we 
tag each character into a BIO-WL style. If 
cici+1?cj matches an item I of length j-i+1 and no 
other item I? of length k (k>j-i+1) in the list 
matches cici+1?cj?ck+i-1, then the characters are 
tagged as follows: 
 
ci ci+1 ? cj 
B-WL I-WL ? I-WL 
 
If no item in the list matches head subpart of 
the string, then ci is tagged as 0.  
The tagging operation iterates on the 
remaining part until all characters are tagged. 
5 Evaluation 
5.1 Results 
The system for our MSRA NER open track 
submission has some bugs and was trained on a 
much smaller training data set than the full set 
the organizer provided. The results are very low, 
see Table 2:  
 
Accuracy 96.28% 
Precision 83.20% 
Recall 67.03% 
FB1 74.24% 
Table 2. MSRA NER Open 
 
When we fixed the bug and retrained on the 
full training corpus, the result comes out to be as 
follows: 
 
Accuracy 98.24% 
Precision 89.38% 
Recall 83.07% 
FB1 86.11% 
Table 3. MSRA NER Open (retrained) 
 
All the submissions on close tracks are trained 
on 80% of the training corpora, the remaining 
20% parts are used for development. The results 
are shown in Table 4 and Table 5: 
 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail char-acter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail char-acter 
? ? ? ? ?, , , , , 
?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
182
Corpus Measure 
UPUC  CityU  CKIP MSRA
Recall 0.922 0.952 0.939 0.933
Precision 0.912 0.954 0.929 0.942
FB1 0.917 0.953 0.934 0.937
OOV  
Recall 0.680 0.747 0.606 0.640
IV Recall 0.945 0.960 0.954 0.943
Table 4. WS Close 
 
Measure MSRA CityU LDC 
Accuracy 92.44 97.80 93.82 
Precision 81.64 92.76 81.43 
Recall 31.24 81.81 59.53 
FB1 45.19 86.94 68.78 
Table 5. NER Close 
 
The reason for low measure on MSRA NER 
track exists in that we chose a much smaller 
training data file encoded in CP936 (about 7% of 
the full data set). This file may be an incomplete 
output when the organizer transfers from another 
encoding scheme. 
5.2 Errors  from NER Track 
The NER errors in our system are mainly as fol-
lows: 
? Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding 
full names. Some common abbreviations has no 
corresponding full names appeared in document. 
Here are some examples: 
R1:?????????? ? ? ?
??[???????????? ORG] 
[?? GPE]?[?? GPE]?????
???? 
K:?????????? [? GPE] 
[? GPE]???[?????????
??? ORG][?? GPE]?[?? 
GPE]????????? 
R: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
? Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:????[?? ?? PER]??[?
? PER] ???? 
K:????[?? PER][?? 
PER][?? PER][?? PER] ???
? 
R:?[???? LOC]?[???? 
PER]?? 
K:?[???? PER]?[???? 
PER]?? 
? Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For 
instance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from fail-
ing to identify verbs and prepositions, such as: 
R:[???? ? ???????? 
ORG]??????[??? ORG]??
???? 
K:[???? ORG]?[??????
?? ORG]??????[??? ORG]
?????? 
R:??????????? ????? 
K:[??????????? ORG]???
?? 
R:?? ?? 
K:[?? PER] ?? 
? Other Types: 
R:???? ??? ??????? 
K:????[??? PER]?????
?? 
R:????? ? ? ??? 
183
K:?????[? LOC][? LOC]?
?? 
6 Conclusions and Future Work 
We mainly described a Chinese named entity 
recognition system NER@ISCAS, which inte-
grates text, part-of-speech and a small-
vocabulary-character-lists feature for MSRA 
NER open track under the framework of Condi-
tional Random Fields (CRFs) model. Although it 
provides a unified framework to integrate multi-
ple flexible features, and to achieve global opti-
mization on input text sequence, the popular lin-
ear chained Conditional Random Fields model 
often fails to catch semantic relations among re-
occurred mentions and adjoining entities in a 
catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post proc-
essing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgment 
This work is supported by the National Science 
Fund of China under contract 60203007. 
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Hu-
man-Machine Interface Technology Evalua-
tion. 
Yuanyong Feng, Le Sun and Junlin Zhang. 2005. 
Early Results for Chinese Named Entity Rec-
ognition Using Conditional Random Fields 
Model, HMM and Maximum Entropy. IEEE 
Natural Language Processing & Knowledge 
Engineering. Beijing: Publishing House, 
BUPT. pp. 549~552. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and La-
beling Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
184
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 96?99,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
 A Syllable-based Name Transliteration System 
 
 
Xue Jiang 1, 2 
1Institute of Software, Chinese 
Academy of Science. 
Beijing China, 100190 
jiangxue1024@yahoo.com.cn 
Le Sun 1, Dakun Zhang 1 
2School of Software Engineering,  
Huazhong University of Science and 
Technology. Wuhan China, 430074 
sunle@iscas.ac.cn 
dakun04@iscas.ac.cn 
 
 
 
 
Abstract 
This paper describes the name entity transli-
teration system which we conducted for the 
?NEWS2009 Machine Transliteration Shared 
Task? (Li et al2009). We get the translitera-
tion in Chinese from an English name with 
three steps. We syllabify the English name 
into a sequence of syllables by some rules, 
and generate the most probable Pinyin se-
quence with the mapping model of English 
syllables to Pinyin (EP model), then we con-
vert the Pinyin sequence into a Chinese cha-
racter sequence with the mapping model of 
Pinyin to characters (PC model). And we get 
the final Chinese character sequence. Our 
system achieves an ACC of 0.498 and a 
Mean F-score of 0.786 in the official evalua-
tion result. 
1 Introduction 
The main subject of shared task is to translate 
English names (source language) to Chinese 
names (target language). Firstly, we fix some 
rules and syllabify the English names into a se-
quence of syllables by these rules, in the mean-
while, we convert the Chinese names into Pinyin 
sequence. Secondly, we construct an EP model 
referring to the method of phrase-based machine 
translation. In the next, we construct a 2-gram 
language model on characters and a chart reflect-
ing the using frequency of each character with 
the same pronunciation, both of which constitute 
the PC model converting Pinyin sequence into 
character sequence. When a Pinyin is mapped to 
several different characters, we can use them to 
make a choice. In our experiment, we adopt the 
corpus provided by NEWS2009 (Li et al2004) 
and the LDC Name Entity Lists 1 respectively to 
conduct two EP models, while the NEWS2009 
corpus for the PC model. The experiment indi-
cates that the larger a training corpus is, the more 
precise the transliteration is. 
2 Transliteration System Description 
Knowing from the definition of transliteration, 
we must make the translating result maintain the 
original pronunciation in source language. We 
found that most English letters and letter compo-
sitions? pronunciation are relatively fixed, so we 
can take a syllabification on an English name, 
therefore the syllable sequence can represent its 
pronunciation. In Chinese, Pinyin is used to 
represent a character?s pronunciation. Based on 
these analyses, we transliterate the English sylla-
ble sequence into a Pinyin sequence, and then 
translate the Pinyin sequence into characters. 
We suppose that the probability of a translitera-
tion from an English name to a Chinese name is 
denoted by P(Ch|En), the probability of a transla-
tion from an English syllable sequence to a Pi-
nyin  sequence is denoted by P(Py|En), and the 
probability of a translation from a Pinyin se-
quence to a characters is denoted by P(Ch|Py), 
then we can get the formula: 
P(Ch|En) = P(Ch|Py) * P(Py|En)      (1) 
The character sequence in candidates having 
the max value of P(Ch|En) is the best translitera-
tion(Wan and Verspoor, 1998). 
2.1 Syllabification of English Names 
English letters can be divided into vowel letters 
(VL) and consonant letters (CL). Usually, in a 
                                                 
1
: Chinese <-> English Name Entity Lists v 1.0, LDC Cata-
log No.: LDC2005T34 
96
word, a phonetic syllable can be constructed in a 
structure of CL+VL, CL+VL+CL, CL+VL+NL. 
To adapt for Chinese phonetic rule, we divide the 
continuous CLs into independent CLs(IC) and 
divide structure of CL+VL+CL into CL+VL and 
an IC. Take ?Ronald? as an example, it can be 
syllabified into ?Ro/na/l/d?, ?Ro? is CL+VL, 
?nal? is CL+VL+CL, and is divided into CL+VL 
and IC. ?d? is an independent CL(KUO et al 
2007). Of course there are some English names 
more complex to be syllabified, so we define 
seven rules for syllabification (JIANG et al 
2006): 
(1) Define English letter set as O, vowel set as 
V={a, e, i, o, u}, consonant set as C=O-V. 
(2) Replace all ?x? in a name with ?ks? before 
syllabification because it?s always pro-
nounced as  ?ks?. 
(3) The continuous VLs should be regarded as 
one VL. 
(4) There are some special cases in rule (3), 
the continuous VLs like ?oi?, ?io?, ?eo? 
are pronounced as two syllables, so they 
should be cut into two parts, so ?Wilhoit? 
will be syllabifyd into ?wi/l/ho/i/t?. 
(5) The continuous CLs should be cut into 
several independent CLs. If the last one is 
followed by some VLs, they will make up 
a syllable. 
(6) Some continuous CLs are pronounced as a 
syllable, such as ?ck?, ?th?, these CLs will 
not be syllabifyd and be regarded as a sin-
gle CL, ?Jack? is syllabifyd into ?Ja/ck?. 
(7) There are some other composition with the 
structure of VL+CL, such as ?ing?, ?er?, 
?an? and so on. If it?s a consonant behind 
these compositions in the name, we can 
syllabify it at the end of the composition, 
while if it?s a vowel behind them, we 
should double write the last letter and syl-
labify the word between the two same let-
ters. 
After syllabicating English names, we convert 
corresponding Chinese names into Pinyin. There 
are a few characters with multiple pronunciations 
in the training data, we find them out and ensure 
its pronunciation in a name manually.  
We record all of these syllables got from the 
training data set, if we meet a syllable out of vo-
cabulary when transliterating an English name, 
we will find a similar one with the shortest edit-
distance in the vocabulary to replace that. 
2.2 Mapping Model of English Syllables to 
Pinyins 
The EP model consists of a phrase-based ma-
chine translation model with a trigram language 
model.  
Given an English name f, we want to find its 
Chinese translation e, which maximize the condi-
tional probability )|Pr( fe , as shown below. 
)|Pr(maxarg* fee e?
    (2) 
Using Bayes rule, (1) can be decomposed into 
a Translation Model )|Pr( ef  and a Language 
Model )Pr(e  (Brown et al 1993), which can 
both be trained separately. These models are 
usually regarded as features and combined with 
scaling factors to form a log-linear model (Och 
and Ney 2002). It can then be written as: 
? ?
?
?
?
?
?
'
1
1
)],'(exp[
)],(exp[
              
)|()|Pr(
1
e
mm
M
m
mm
M
m
feh
feh
fepfe M
?
?
?
        (3) 
In our model, we use the following features: 
? phrase translation probability )|( fep  
? lexical weighting )|( felex  
? inverse phrase translation probability 
)|( efp  
? inverse lexical weighting )|( eflex   
? phrase penalty (always exp(1) = 2.718) 
? word penalty (target name length) 
? target language model, trigram 
The first five features can be seen as a whole 
phrase translation cost and used as one during 
decoding.  
In general, the translation process can be de-
scribed as follows: 
(1). Segmenting input English syllable se-
quence f into J syllables Jf 1  
(2). Translating each English syllable 
jf  
into several Pinyins 
jke  
(3). Selecting the N-best words 
nee ...1 , 
combined with reordering and Language 
Model and other features 
97
(4). Rescoring the translation word set with 
additional features to find the best one. 
We use SRI toolkit to train our trigram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman 1998). In the standard 
experiment, we use training data set provided by 
NEWS2009 (Li et al2004) to train this language 
model, in the nonstandard one, we use that and 
the LDC Name Entity Lists to train this language 
model. 
2.3 Mapping Model of Pinyins to Chinese 
Characters 
Since the Chinese characters used in people 
names are limited, most of the conversions from 
Pinyin to character are fixed. But some Pinyins 
still have several corresponding characters, and 
we should make a choice among these characters. 
To solve this problem, we conduct a PC model 
consisting a frequency chart which reflects the 
using frequency of each character at different 
positions in the names and a 2-gram language 
model with absolute discounting smoothing.  
A Chinese name is represented as C1C2?
Cn?Ci (1?i?n) is a Chinese character. C1 is at 
the first position, we call it FW; C2 ?Cn-1 are in 
the middle, we call them MW; Cn is at the last 
position, we call it LW. Usually, each character 
has different frequencies at these three positions. 
In the training data set of NEWS2009, Pinyin 
?luo? can be mapped to three characters: ???, 
???, and ???, each of them has different fre-
quencies at different positions. 
 
 FW MW LW 
? 0.677 0.647 0.501 
? 0.323 0.352 0.499 
? 0 0.001 0 
Table 1. Different frequencies at different positions 
 
From this table, we can see that at FW and 
MW position, ??? is more probable to be cho-
sen than the others, but sometimes ??? or ??? 
is the correct one. In order to ensure characters 
with lower frequency like ??? and ??? can be 
chosen firstly in a certain context, we conduct a 
2-gram language model.  
If a Pinyin can be mapped to several charac-
ters, the condition probability (P(Chi|py)) indicat-
ing that how possible a character should be cho-
sen is determined by the weighted average of its 
position frequency (P(Chi|pos)) and its probabili-
ty in the 2-gram language model (P(Chi|Chi-1)). 
P(Chi|py) = a*P(Chi|pos)+(1-a)*P(Chi|Chi-1)  (4) 
0 < a < 1. In our experiments, we set a = 0.1.  
2.4 Experiments and Results 
We carried out two experiments. The difference 
between them is the training data for EP model. 
The standard experiment adopts corpus provided 
by NEWS2009, while the nonstandard one 
adopts LDC Name Entity Lists. 
 
Corpora Name Num 
LDC2005T34 572213 
NEWS09_train_ench_31961 31961 
Table 2. Corpora used for training the EP model 
 
Considering that an English name may be 
translated to different Chinese names in different 
corpora, so we established a unique PC model 
with the training data set provided by 
NEWS2009 to avoid the model?s deviation 
caused by different corpora. 
The experimenting data is the development 
data set provided by NEWS2009 (Li et al2004), 
testing script is also provided by NEWS2009. 
First, we take a syllabification on testing 
names.  Then we use the EP model to generate 5-
best Pinyin sequences and their probabilities.  
For each Pinyin sequence, the PC model gives 3-
best character sequences and their probabilities. 
In the end, we sort the results by probabilities of 
character sequences and corresponding Pinyin 
sequences. 
The evaluation results are shown below. 
 
Metrics Standard Nonstandard 
ACC 0.490677 0.502417 
Mean F-score 0.782039 0.784203 
MRR 0.606424 0.611214 
MAP_ref 0.490677 0.502417 
MAP_10 0.189290 0.189782 
MAP_sys 0.191476 0.192129 
Table 3. Evaluation results of standard and             
nonstandard experiments 
It?s easy to see that nonstandard test is better 
than standard one on each metric. A larger cor-
pus does make a contribution to a more accurate 
model. 
98
For the official evaluation, we make two tests 
on the testing data set provided by NEWS2009 
(Li et al2004). The table 4 shows respectively 
the evaluation results of standard and nonstan-
dard tests given by NEWS2009. 
Metrics Standard Nonstandard 
ACC 0.498 0.500 
Mean F-score 0.786 0.786 
MRR 0.603 0.607 
MAP_ref 0.498 0.500 
MAP_10 0.187 0.189 
MAP_sys 0.189 0.191 
Table 4. Official evaluation results of standard and 
nonstandard tests 
3 Conclusion 
We construct a name entity transliteration system 
based on syllable. This system syllabifies Eng-
lish names by rules, then translates the syllables 
to Pinyin and Chinese characters by statistics 
model. We found that a larger corpus may im-
prove the transliteration. Besides, we can do 
something else to improve that. We need to fix 
more complex rules for syllabification. If we can 
get the name user?s gender from some features of 
the name itself, then translate the male and fe-
male names on different Chinese character sets, 
the results may be more precise. 
Acknowledgments 
This work was supported by the National 
Science Foundation of China (60736044, 
60773027), as well as 863 Hi-Tech Research and 
Development Program of China (2006AA010108 
-5, 2008AA01Z145).  
We also thank Haizhou Li, Min Zhang and 
Jian Su for providing the English-Chinese data. 
Reference 
Franz Josef Och and Hermann Ney. 2002. ?Discri-
minative Training and Maximum Entropy Models 
for Statistical Machine Translation?. In  Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Haizhou Li, A Kumaran, Min Zhang, Vladimir Per-
vouchine, "Whitepaper of NEWS 2009 Machine 
Transliteration Shared Task". In Proceedings of 
ACL-IJCNLP 2009 Named Entities Workshop 
(NEWS 2009), Singapore, 2009 
Haizhou Li, Min Zhang, Jian Su. 2004. ?A joint 
source channel model for machine transliteration?, 
In Proceedings of the 42nd ACL, 2004 
Jiang Long, Zhou Ming, and Chien Lee-feng. 2006. 
?Named Entity Translation with Web Mining and 
Transliteration?. Journal of Chinese Information 
Processing, 21(1):1629--1634. 
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 
2007.  ?A Phonetic Similarity Model for Automatic 
Extraction of Transliteration Pairs?. ACM Trans. 
Asian Language Information Processing, 6(2), Sep-
tember 2007. 
Peter F. Brown, Stephen A. Della Pietra, et al 1993. 
?The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation?. Computational Lin-
guistics 19(2): 263-311. 
Stanley F. Chen and Joshua Goodman. 1998. ?An 
empirical study of smoothing techniques for lan-
guage modeling?. Technical Report TR-10-98, Har-
vard University. 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
?Automatic English-Chinese name transliteration 
for development of multilingual resources?. In Pro-
ceedings of the 17th international conference on 
Computational linguistics, 2: 1352 ? 1356. 
99
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2280?2290, Dublin, Ireland, August 23-29 2014.
A Probabilistic Co-Bootstrapping Method for Entity Set Expansion 
 
 
Bei Shi,    Zhengzhong Zhang Le Sun,    Xianpei Han 
Institute of Software, 
 Chinese Academy of Sciences, 
 Beijing, China 
State Key Laboratory of Computer Science, 
Institute of Software,  
Chinese Academy of Sciences,  
Beijing, China 
{shibei, zhenzhong, sunle, xianpei}@nfs.iscas.ac.cn 
 
 
Abstract 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category. 
Unfortunately, traditional ESE methods usually have the expansion boundary problem and the semantic 
drift problem. To resolve the above two problems, this paper proposes a probabilistic Co-Bootstrapping 
method, which can accurately determine the expansion boundary using both the positive and the 
discriminant negative instances, and resolve the semantic drift problem by effectively maintaining and 
refining the expansion boundary during bootstrapping iterations. Experimental results show that our 
method can achieve a competitive performance. 
1 Introduction 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category 
from text corpus or Web. For example, given the capital seeds {Rome, Beijing, Paris}, an ESE system 
should extract all other capitals from Web, such as Ottawa, Moscow and London. ESE system has 
been used in many applications, e.g., dictionary construction (Cohen and Sarawagi, 2004), word sense 
disambiguation (Pantel and Lin, 2002), query refinement (Hu et al., 2009), and query suggestion (Cao 
et al., 2008). 
Due to the limited supervision provided by ESE (in most cases only 3-5 seeds are given), traditional 
ESE systems usually employ bootstrapping methods (Cucchiarelli and Velardi, 2001; Etzioni et al., 
2005; Pasca, 2007; Riloff and Jones, 1999; Wang and Cohen, 2008). That is, the entity set is 
iteratively expanded through a pattern generation step and an instance extraction step. Figure 1(a) 
demonstrates a simple bootstrapping process.? 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                        
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
* is the city of 
 
at the embassy in * 
* is the capital of 
at the hotel in * 
Chicago 
Berlin 
Pattern Generation Instance Extraction 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
Sydney 
Boston 
* is the city of 
 
at the embassy in * 
 
* is the capital of 
 
to cities such as * 
 
at the hotel in * 
Chicago 
Nokia * the official web site 
 
New York 
Pattern Generation Instance Co-Extraction 
Negative Positive 
 
(a) (b) 
Figure 1: A demo of Bootstrapping (a) and Co-Bootstrapping (b) 
2280
However, the traditional bootstrapping methods have two main drawbacks:  
1) The expansion boundary problem. That is, using only positive seeds (i.e., some example 
entities from the category we want to expand), it is difficult to represent which entities we want to 
expand and which we don?t want. For example, starting from positive seeds {Rome, Beijing, Paris}, 
we can expand entities at many different levels, e.g., all capitals, all cities, or even all locations. And 
all these explanations are reasonable.  
2) The semantic drift problem. That is, the expansion category may change gradually when noisy 
instances/patterns are introduced during the bootstrapping iterations. For example, in Figure 1 (a), the 
instance Rome will introduce a pattern ?* is the city of?, which will introduce many noisy city 
instances such as Milan and Chicago for the expansion of Capital. And these noisy cities in turn will 
introduce more city patterns and instances, and finally will lead to a semantic drift from Capital to 
City. 
In recent years, some methods (Curran et al, 2007; Pennacchiotti and Pantel, 2011) have exploited 
mutual exclusion constraint to resolve the semantic drift problem. These methods expand multiple 
categories simultaneously, and will determine the expansion boundary based on the mutually 
exclusive property of the pre-given categories. For instance, the exclusive categories Fruit and 
Company will be jointly expanded and the expansion boundary of {Apple, Banana, Cherry} will be 
limited by the expansion boundary of {Google, Microsoft, Apple Inc.}. These methods, however, still 
have the following two drawbacks: 
1) These methods require that the expanded categories should be mutually exclusive. However, in 
many cases the mutually exclusive assumption does not hold. For example, many categories hold a 
hyponymy relation (e.g., the categories City and Capital, because the patterns for Capital are also the 
patterns for City) or a high semantic overlap (e.g., the categories Movies and Novels, because some 
movies are directly based on the novels of the same title.). 
2) These methods require the manually determination of the mutually exclusive categories. 
Unfortunately, it is often very hard for even the experts to determine the categories which can define 
the expansion boundaries for each other. For example, in order to expand the category Chemical 
Element, it is difficult to predict its semantic drift towards Color caused by the ambiguous instances 
{Silver, Gold}. 
In this paper, to resolve the above problems, we propose a probabilistic Co-Bootstrapping method. 
The first advantage of our method is that we propose a method to better define the expansion boundary 
using both the positive and the discriminant negative seeds, which can both be automatically populated 
during the bootstrapping process. For instance, in Figure 1(b), in order to expand Capital, the 
Co-Bootstrapping algorithm will populate both positive instances from the positive seeds {Rome, 
Beijing, Paris}, and negative instances from the negative seeds {Boston, Sydney, New York}. In this 
way the expansion boundary of Capital can be accurately determined. 
The second advantage of our method is that we can maintain and refine the expansion boundary 
during bootstrapping iterations, so that the semantic drift problem can be effectively resolved. 
Specifically, we propose an effective scoring algorithm to estimate the probability that an extracted 
instance belongs to the target category. Based on this scoring algorithm, this paper can effectively 
select positive instances and discriminant negative instances. Therefore the expansion boundary can be 
maintained and refined through the above jointly expansion process. 
We have evaluated our method on the expansion of thirteen categories of entities. The experimental 
results show that our method can achieve 6%~15% P@200 performance improvement over the 
baseline methods. 
This paper is organized as follows. Section 2 briefly reviews related work. Section 3 defines the 
problem and proposes a probabilistic Co-Bootstrapping approach. Experiments are presented in 
Section 4. Finally, we conclude this paper and discuss some future work in Section 5. 
2 Related Work 
In recent years, ESE has received considerable attentions from both research (An et al., 2003; 
Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang and 
Cohen, 2008) and industry communities (e.g., Google Sets). Till now, most ESE systems employ 
bootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc. 
2281
The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem 
and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic 
drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et 
al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the 
assumption that high-ranked instances will be more likely to be the instances of the target category. 
The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and 
Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which 
expand multiple categories simultaneously and determine the expansion boundary based on the 
mutually exclusive property of the pre-given categories. 
3 The Co-Bootstrapping Method 
3.1 The Framework of Probabilistic Co-Bootstrapping 
Given the initial positive seeds and negative seeds, the goal of our method is to extract instances of a 
specific target semantic category. For demonstration, we will describe our method through the running 
example shown in Figure 1(b). 
Specifically, Figure 2 shows the framework of our method. The central tasks of our 
Co-Bootstrapping method are as follows: 
 
Figure 2: The framework of probabilistic Co-Bootstrapping 
1) Pattern Generation and Evaluation. This step generates and evaluates patterns using the 
statistics of the positive and the negative instances. Specifically, we propose three measures of pattern 
quality: the Generality (GE), the Precision of Extracted Instances (PE) and the Precision of Not 
Extracted Instances (PNE). 
2) Instance Co-Extraction. This step co-extracts the positive and the negative instances using 
highly confident patterns. Specifically, we propose an effective scoring algorithm to estimate the 
probability that an extracted instance belongs to the target category based on the statistics and the 
quality of the patterns which extract it. 
3) Seed Selection. This step selects the high ranked positive instances and discriminant negative 
instances to refine the expansion boundary by measuring how well a new instance can be used to 
define the expansion boundary. 
The above three steps will iterate until the number of extracted entities reaches a predefined 
threshold. We describe these steps as follows. 
3.2 Pattern Generation and Evaluation 
In this section, we describe the pattern generation and evaluation step. In this paper, each pattern is a 
4-grams lexical context of an entity. We use the Google Web 1T corpus?s (Brants and Franz, 2006) 
5-grams for both the pattern generation and the instance co-extraction in ESE. Our method generates 
patterns through two steps: 1) Generate candidate patterns by matching seeds with the 5-grams. 2) 
Evaluate the quality of the patterns. 
For the first step, we simply match each seed instance with all 5-grams, then we replace the 
matching instance with wildcard ?*? to generate the pattern. 
Extracted Positive (ep) London 
Extracted Negative (en) Shanghai, Milan 
Not Extracted Positive (nep) Tokyo 
Not Extracted Negative (nen) Chicago, Nokia 
 
Table 1: (a) shows the four classes of instances according to polarity and extraction. (b) shows the four 
classes of the instances given ?to cities such as *? 
Count Positive Negative 
Extracted Extracted Positive (ep) Extracted Negative (en) 
Not Extracted 
Not Extracted and Positive 
(nep) 
Not Extracted and Negative 
(nen) 
Pattern Generation and Evaluation 
Initial   
Seeds 
Pattern 
Positive Instance 
Discriminant Negative Instance    
Positive Instance  
Negative Instance 
Instance Co-Extraction 
Seeds Evaluation and Selection 
(a) (b) 
2282
For the second step, we propose three measures to evaluate the quality of a pattern, correspondingly 
the Generality (GE), the Precision of Extracted Instances (PE), and the Precision of Not Extracted 
Instances (PNE). Specifically, given a pattern, we observed that all instances can be categorized into 
four classes, according to whether they belong to the target category and whether they can be extracted 
by the pattern (shown in Table 1(a)). For example, given the pattern ?to cities such as *? in Figure 
1(b), the instances under its four classes are shown in Table 1 (b). 
The proposed three measures of the quality of a pattern can be computed as follows (In most cases, 
we cannot get the accurate number of ep, en, nep and nen. So this paper uses the corresponding known 
instances in the previous iteration to approximately compute ep, en, nep and nen): 
1) Generality (GE). The Generality of a pattern measures how many entities can be extracted by it. 
A more general pattern will cover more entities than a more specific pattern. Specifically, the GE of a 
pattern is computed as: 
 
That is, the proportion of the instances which can be extracted by the pattern in the previous iteration. 
2) Precision of Extracted Instances (PE). The PE measures how likely an instance extracted by a 
pattern will be positive. That is, a pattern with higher PE will be more likely to extract positive 
instances than a lower PE pattern. The PE is computed as: 
 
That is, the proportion of positive instances within all instances which can be extracted by the 
pattern in the previous iteration. 
3) Precision of Not Extracted Instances (PNE). The PNE measures how likely a not extracted 
instance is positive. Instances not extracted by a high PNE pattern will be more likely to be positive. 
PNE is computed as: 
 
Because the number of negative instances is usually much larger than the number of positive 
instances, we normalize the number of positive and negative instances in the formula. 
Table 2 shows these measures of some selected patterns evaluated using the Google Web 1T corpus. 
We can see that the above measures can effectively evaluate the quality of patterns. For instance, 
GE(?* is the city of?)=0.566 is larger than GE(?at the embassy in *?)=0.340, which is consistent with 
our intuition that the pattern ?* is the city of? is more general than ?at the embassy in *?. PE(?* is the 
capital of?)=0.928 is larger than PE(?* is the city of?)=0.269, which is consistent with our intuition 
that the instances extracted by ?* is the capital of? are more likely Capital than by?* is the city of?. 
 GE PE PNE 
at the embassy in * 0.340 0.833 0.312 
* is the capital of 0.321 0.928 0.224 
to cities such as * 0.426 0.875 0.566 
at the hotel in * 0.333 0.192 0.571 
* is the city of 0.566 0.269 0.592 
* the official web site 0.218 0.230 0.607 
Table 2: The GE, PE and PNE of some selected patterns 
3.3 Instance Co-Extraction 
In this section, we describe how to co-extract positive instances and discriminant negative instances. 
Given the generated patterns, the central task of this step is to measure the likelihood of an instance to 
be positive. The higher the likelihood, the more likely the instance belongs to the target category. To 
resolve the task, we propose a probabilistic method which predicts the probability of an instance to be 
positive, i.e., the Instance Positive Probability and we denote it as P+. Generally, the P+ is determined 
by both the statistics and the quality of patterns. We start with the observation that: 
2283
1) If an instance is extracted by a pattern with a high PE, the instance will have a high P+. 
2) If an instance is not extracted by a high PNE pattern, the instance will have a high P+. 
3) If an instance is extracted by many patterns with high PE and not extracted by many patterns 
with high PNE, the instance will have a high P+, and vice versa. 
Based on the above observations, the computation of P+ is as follows: 
The Situation of One Pattern 
For the situation that only one pattern exists, the P+ of an instance can be simply computed as: 
 
where e denotes an extracted instance and p denotes a pattern which extracts e. This formula means 
that if the instance is extracted by a pattern, the P+ is determined by the PE of the pattern. For 
example, in Figure 3 (a), the instance Tokyo is only extracted by the pattern ?at the embassy in *? and 
the P+ is determined by the PE of ?at the embassy in *?, i.e., P+(Tokyo)=PE(?at the embassy in *?). 
The above formula also means when the instance cannot be extracted by the only pattern, the P+ 
will be determined by the PNE of the pattern. For example, in Figure 3 (b), the instance Tokyo is not 
extracted by the only pattern ?at the hotel in *? and the P+ is only determined by the PNE of ?at the 
hotel in *?, that is, P+(Tokyo)=PNE(?at the hotel in *?). 
 
 
 
 
Figure 3: (a) Tokyo is extracted by ?at the embassy in *?. (b) Tokyo is not extracted by ?at the hotel 
in *?. (c) London is extracted by ?at the embassy in *? and not extracted by ?to cities such as *?. 
The Situation of Multiple Patterns 
In this section, we describe how to compute P+ in the situation of multiple patterns. Specifically, we 
assume that an instance is extracted by different patterns independently. Therefore, given all the 
pattern-instance relations (i.e., whether a specific pattern extracts a specific instance), the likelihood 
for an instance e being positive is computed as: 
 
where R+ is all the patterns which extract e, and R- is all the patterns which do not extract e. I+ is the 
set of all positive instances.  is the probability of the event ?pattern p extracts 
instance e and e is positive?. Using Bayes rule, this probability can be computed as: 
 
where  is the probability of the event ?p extracts an instance e?, its value is GE(p); 
 is the conditional probability that e is positive under the condition ?p extracts e?, 
and its value is PE(p). Finally  is computed as: 
 
 is the probability of the event ?p does not extract e and e is positive?, which can 
be computed as: 
 
 is the probability of p not extracting an instance e, and its value is 1-GE(p). 
 is the conditional probability that e is positive under the condition ?p does not 
extract e?, and its value is PNE(p). Then  is finally computed as: 
 
Tokyo at the embassy in * Tokyo  at the hotel in * London 
at the embassy in * 
to cities such as * 
(a) (b) (c) 
2284
For example, in Figure 3 (c), the instance London is extracted by the pattern ?at the embassy in *? 
and not extracted by the pattern ?to cities such as *?. In this situation, PosLikelihood(London)= 
[GE(?at the embassy in *?) ? PE(?at the embassy in?)] ? [(1-GE(?to cities such as *?)) ? PNE(?to 
cities such as *?)]. 
Using the same intuition and the same method, the likelihood of an instance being negative is 
computed as: 
 
where  is the probability of the event ?p extracts e and e is negative?, which is 
computed as: 
 
 is the probability of the event ?p does not extract e and e is negative?, which is 
computed as: 
 
For instance, in Figure 3 (c), NegLikelihood(London) = [GE(?at the embassy in *?) ? (1-PE(?at the 
embassy in?))] ? [(1-GE(?to cities such as *?)) ? (1-PNE(?to cities such as *?))]. 
Finally, the Instance Positive Probability, P+, is computed as:  
 
3.4 Seed Selection 
In this section, we describe how to select positive and discriminant negative instances at each iteration. 
To determine whether an instance is positive, we use a threshold of P+ to determine the polarity of 
instances, which can be empirically estimated from data. The instances which have much higher P+ 
than the threshold will be added to the set of positive instances. For example, London and Tokyo in 
Figure 1 (b) are selected as positive instances. 
To select discriminant negative instances, we observed that not all negative instances are the same 
useful for the expansion boundary determination. Intuitively, the discriminant negative instances are 
those negative instances which are highly overlapped with the positive instances. For instance, due to 
the lower overlap between categories Fruit and Capital, Apple is not a discriminant negative instance 
since it provides little information for the expansion boundary determination. Therefore, the instances 
near the threshold are used as the discriminant negative instances in the next iteration. (Notice that, the 
computation of GE, PE and PNE still uses all positive and negative instances, rather than only 
discriminant negative instances). For example, in Figure 1(b), Shanghai, Milan and Chicago are 
selected as discriminate negative instances, and Nokia will be neglected. Finally the boundary between 
Capital and City can be determined by the positive instances and the discriminant negative instances. 
4 Experiments 
4.1 Experimental Settings 
Category Description Category Description 
CAP Place: capital name FAC Facilities: names of man-made structures 
ELE chemical element ORG Organization: e.g. companies, governmental 
FEM Person: female first name GPE Place: Geo-political entities 
MALE Person: male first name LOC Locations other than GPEs 
LAST Person: last name DAT Reference to a date or period 
TTL Honorific title LANG Any named language 
NORP Nationality, Religion, Political(adjectival)   
Table 3: Target categories 
Corpus: In our experiments, we used the Google Web 1T corpus (Brants and Franz, 2006) as our 
expansion corpus. Specifically, we use the open source package LIT-Indexer (Ceylan and Mihalcea, 
2011) to support efficient wildcard querying for pattern generation and instance extraction. 
2285
Target Expansion Categories: We conduct our experiments on thirteen categories, which are shown 
in Table 3. Eleven of them are from Curran et al. (2007). Besides the eleven categories, to evaluate 
how well ESE systems can resolve the semantic drift problem, we use two additional categories 
(Capital and Chemical Element) which are high likely to drift into other categories. 
Evaluation Criteria: Following Curran et al (2007), we use precision at top n (P@N) as the 
performance metrics, i.e., the percentage of correct entities in the top n ranked entities for a given 
category. In our experiments, we use P@10, P@20, P@50, P@100 and P@200. Since the output is a 
ranked list of extracted entities, we also choose the average precision (AP) as the evaluation metric. In 
our experiments, the correctness of all extracted entities is manually judged. In our experiments, we 
present results to 3 annotators, and an instance will be considered as positive if 2 annotators label it as 
positive. We also provide annotators some supporting resources for better evaluation, e.g., the entity 
list of target type collected from Wikipedia. 
4.2 Experimental Results 
In this section, we analyze the effect of negative instances, categories boundaries, and seed selection 
strategies. We compare our method with the following two baseline methods: i) Only_Pos (POS): 
This is an entity set expansion system which uses only positive seeds. ii) Mutual_Exclusion (ME): 
This is a mutual exclusion bootstrapping based ESE method, whose expansion boundary is determined 
by the exclusion of the categories. 
We implement our method using two different settings: i) Hum_Co-Bootstrapping (Hum_CB): 
This is the proposed Co-Bootstrapping method in which the initial negative seeds are manually given. 
Specifically, we randomly select five positive seeds from the list of the category?s instances while the 
initial negative seeds are manually provided. ii) Feedback_Co-Bootstrapping (FB_CB): This is our 
proposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds:   
1) Expand the entity set using only the positive seeds for only first iteration. Return the top ten 
instances. 2) Select the negative instances in the top ten results of the first iteration as negative seeds. 
4.2.1. Overall Performance 
Several papers have shown that the experimental performance may vary with different seed choices 
(Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009). Therefore, we input the 
ESE system with five different positive seed settings for each category. Finally we average the 
performance on the five settings so that the impact of seed selection can be reduced. 
 P@10 P@20 P@50 P@100 P@200 MAP 
POS 0.84 0.74 0.55 0.41 0.34 0.42 
ME 0.83(0.90) 0.79(0.87) 0.68(0.78) 0.58(0.67) 0.51(0.59) - 
Hum_CB 0.97 0.95 0.83 0.71 0.57 0.78 
FB_CB 0.97 0.96 0.90 0.79 0.66 0.85 
Table 4: The overall experimental results 
Table 4 shows the overall experimental results. The results in parentheses are the known results of 
eleven categories (without CAP and ELE) shown in (Curran et al., 2007). MAP of ME is missed 
because there are no available results in (Curran et al., 2007). From Table 4, we can see that: 
1) Our method can achieve a significant performance improvement: Compared with the 
baseline POS, our method Hum_CB and FB_CB can respectively achieve a 23% and 32% 
improvement on P@200; Compared with the baseline ME, our method Hum_CB and FB_CB can 
respectively improve P@200 by 6% and 15%. 
2) By explicitly representing the expansion boundary, the expansion performance can be 
increased: Compared with the baseline POS, ME can achieve a 17% improvement on P@200, and our 
method Hum_CB can achieve a 23% improvement on P@200. 
3) The negative seeds can better determine the expansion boundary than mutually exclusive 
categories. Compared with ME, Hum_CB and FB_CB can respectively achieve a 6% and 15% 
improvement on P@200. We believe this is because using negative instances is a more accurate and 
more robust way for defining and maintaining the expansion boundary than mutually exclusive 
categories. 
2286
4) The system?s feedback is useful for selecting negative instances: Compared with Hum_CB, 
FB_CB method can significantly improve the P@200 by 9.0%. We believe this is because that the 
system?s feedback is a good indicator of the semantic drift direction. In contrast, it is usually difficult 
for human to determine which directions the bootstrapping will drift towards. 
4.2.2. Detailed Analysis: Expansion Boundary 
In Table 5, we show the top 20 positive and negative Capital instances (FB_CB setting). From Table 5, 
we can make the following observations: 1) Our method can effectively generate negative instances. 
In Table 5, the negative instances contain cities, states, countries and general terms, all of which have 
a high semantic overlap with Capital category. 2) The positive instances and negative instances 
generated by our Co-Bootstrapping method can discriminately determine the expansion boundary. For 
instance, the negative instances Kyoto can distinguish Capital from City; Australia and China can 
distinguish Capital from Country; 
Positive Instances 
London,  Paris,  Moscow,  Beijing,  Madrid,  Amsterdam,  Washington,  Tokyo,  Berlin,  Rome,  
Vienna,  Baghdad,  Athens,  Bangkok,  Cairo,  Dublin,  Brussels,  Prague,  San,  Budapest 
Negative Instances 
(with categories)  
City Kyoto,  Kong,  Newcastle,  Zurich,  Lincoln,  Albany,  Lyon,  LA,  Shanghai 
Country China,  Australia 
General downtown,  April 
State Hawaii,  Oklahoma,  Manhattan 
Other Hollywood,  DC,  Tehran,  Charlotte 
Table 5: Top 20 positive instances and negative instances (True positive instances are in bold) 
4.2.3. Detailed Analysis: Semantic Drift Problem 
POS 
Stockholm,  Tampa,  East,  West,  Springfield,  Newport, Cincinnati,  Dublin,  Chattanooga,  Savannah,  
Omaha,  Cambridge,  Memphis,  Providence,  Panama,  Miami,  Cape,  Victoria,  Milan,  Berlin 
ME 
London,  Prague,  Newport,  Cape,  Dublin,  Savannah,  Chattanooga,  Beijing,  Memphis,  Athens,  
Berlin,  Miami,  Plymouth,  Victoria,  Omaha,  Tokyo,  Portland,  Troy,  Anchorage,  Bangkok 
Hum_CB 
London,  Rome,  Berlin,  Paris,  Athens,  Moscow,  Tokyo,  Beijing,  Prague,  Madrid,  Vienna,  
Dublin,  Budapest,  Amsterdam,  Bangkok,  Brussels,  Sydney,  Cairo,  Washington,  Barcelona 
FB_CB 
London,  Paris,  Moscow,  Beijing,  Washington,  Tokyo,  Berlin,  Rome,  Vienna,  Baghdad,  
Athens,  Bangkok,  Cairo,  Brussels,  Prague,  San,  Budapest,  Amsterdam,  Dublin,  Madrid 
Table 6: Top 20 instances of all methods (True positive instances are in bold) 
To analyze how our method can resolve the semantic drift problem, Table 6 shows the top 20 positive 
Capital instances of different methods. From Table 6, we can make the following observations: i) 
Different methods can resolve the semantic drift problem to different extent: ME is better than POS, 
with 50% instances being positive, and our method is better than ME, with 95% instances being 
positive. ii) The Co-Bootstrapping method can effectively resolve the semantic drift problem: 25% of 
POS?s top 20 instances and 50% of ME?s top 20 instances are positive. In contrast, 90% of Hum_CB?s 
top 20 instances and 95% of FB_CB?s top 20 instances are positive respectively. It proves that 
Co-Bootstrapping method can better resolve the semantic drift problem than POS and ME. 
4.3 Parameter Optimization 
 
Figure 4: The MAP vs. threshold of P+ 
Our method has only one parameter: threshold of P+, which determines the instance?s polarity. 
Intuitively, a larger threshold of P+ will improve the precision of the positive instances but will regard 
some positive instances as negative instances mistakenly. As shown in Figure 4, our method can 
achieve the best MAP performance when the value of the threshold is 0.6. 
0
0.2
0.4
0.6
0.8
1
0.0 0.2 0.4 0.6 0.8 1.0
MAP
Threshold of P+ 
 
MAP 
2287
4.4 Comparison with State-of-the-Art Systems 
We also compare our method with three state-of-the-art systems: Google Sets1-- an ESE application 
provided by Google, SEAL2 -- a state-of-the-art ESE method proposed by Wang and Cohen (2008), 
and WMEB -- a state-of-the-art mutual exclusion based system proposed in McIntosh and Curran 
(2008). To make a fair comparison, we directly use the results before the adjustment which miss 
P@10 and P@50 in their original paper (McIntosh and Curran, 2008) and compared the performance 
of these systems on nine categories in (McIntosh and Curran, 2008). For each system, we conduct the 
experiment five times to reduce the impact of seeds selection. The average P@10, P@50, P@100 and 
P@200 are shown in Figure 5. 
 
Figure 5: The results compared with three state-of-the-art systems 
From the results shown in Figure 5, we can see that our probabilistic Co-Bootstrapping method can 
achieve state-of-the-art performance on all metrics: Compared with the well-known baseline Google 
Sets, our method can get a 42.0% improvement on P@200; Compared with the SEAL baseline, our 
method can get a 35.0% improvement on P@200; Compared with the WMEB method, our method can 
achieve a 6.2% improvement on P@100 and a 3.1% improvement on P@200. 
5 Conclusion and Future Work 
In this paper, we proposed a probabilistic Co-Bootstrapping method for entity set expansion. By 
introducing negative instances to define and refine the expansion boundary, our method can 
effectively resolve the expansion boundary problem and the semantic drift problem. Experimental 
results show that our method achieves significant performance improvement over the baselines, and 
outperforms three state-of-the-art ESE systems. Currently, our method did not take into account the 
long tail entity expansion, i.e., the instances which appear only a few times in the corpus, such as 
Saipan, Roseau and Suva for the Capital category. For future work, we will resolve the long tail 
entities in our Co-Bootstrapping method by taking the sparsity of instances/patterns into consideration. 
6 Acknowledgements 
We would like to thank three anonymous reviewers for invaluable comments and suggestions to 
improve our paper. This work is supported by the National Natural Science Foundation of China under 
Grants no. 61100152 and 61272324, and the National High Technology Development 863 Program of 
China under Grants no. 2013AA01A603. 
References 
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. 
In: Proceedings of the fifth ACM conference on Digital libraries (DL-00), Pages 85-94. 
Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 2003. Automatic acquisition of named entity tagged corpus 
from world wide web. In: Proceedings of ACL-03, Pages 165-168, Volume 2. 
Thorsten Brants and Alex Franz. 2006. Web 1t-5gram version1. http://www.ldc.upenn.edu/Catalog/ 
catalogEntry.jsp?catalogId=LDC2006T13 
                                                        
1 https://docs.google.com/spreadsheet/ 
2 http://www.boowa.com/ 
0.978 0.909 
0.848 
0.773 
0
0.2
0.4
0.6
0.8
1
P@10 P@50 P@100 P@200
Google Sets SEAL WMEB Co-Bootstrapping
2288
Sergey Brin. 1998. Extracting patterns and relations from the World Wide Web. In: Proceedings of the 
Workshop at the 6th International Conference on Extending Database Technology, Pages 172-183. 
Michael J. Cafarella, Doug Downey, Stephen Soderland, and Oren Etzioni. 2005. KnowItNow: Fast, Scalable 
Information Extraction from the Web. In: Proceedings of EMNLP-05, Pages 563-570. 
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. 2008. Context-aware 
query suggestion by mining click-through and session data. In Proceedings of KDD-08, pages 875?883. 
Hakan Ceylan and Rada Mihalcea. 2011. An Efficient Indexer for Large N-Gram Corpora. In: Proceedings of 
System Demonstrations of ACL-11, Pages 103-108. 
William W. Cohen and Sunita Sarawagi. 2004. Exploiting dictionaries in named entity extraction: combining 
semi-Markov extraction processes and data integration methods. In: Proceedings of KDD-04, Pages 89-98. 
Alessandro Cucchiarelli and Paola Velardi. 2001. Unsupervised Named Entity Recognition Using Syntactic and 
Semantic Contextual Evidence. In: Computational Linguistics, Pages 123-131, Volume 27. 
James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with Mutual Exclusion 
Bootstrapping. In: Proceedings of the 10th Conference of the Pacific Association for Computational 
Linguistics, Pages 172?180. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. 
Weld, and Alexander Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental 
Study. In: Artificial Intelligence, Pages 91-134, Volume 165. 
Jian Hu, Gang Wang, Fred Lochovsky, Jiantao Sun, and Zheng Chen. 2009. Understanding user?s query intent 
with Wikipedia. In Proceedings of WWW-09, Pages 471?480. 
Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using 
recursive patterns. In: Proceedings of ACL-10, Pages 1482?1491. 
Tara McIntosh and James R. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent 
lexicon and template acquisition. In: Proceedings of the Australasian Language Technology Association 
Workshop, Pages 97-105. 
Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. 
In: Proceedings of ACL-09, Pages 396-404. 
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In: Proceedings of KDD-08, Pages 
613-619. 
Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labeling Semantic Classes. In: Proceedings of 
HLT/NAACL, Pages 321-328, Volume 4. 
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically 
Harvesting Semantic Relations. In: Proceedings of ACL-06, Pages 113?120. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In: Proceedings of EMNLP-09, Pages 938-947. 
Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In: Proceedings of 
CIKM-07, Pages 683-690. 
Marco Pennacchiotti, Patrick Pantel. 2011. Automatically building training examples for entity extraction. In: 
Proceedings of CoNLL-11, Pages 163-171. 
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction using multi-level 
bootstrapping. In: Proceedings of AAAI-99, Pages 474-479. 
Partha P. Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 
2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In: Proceedings of 
EMNLP-08, Pages 582-590. 
Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction 
pattern contexts. In: Proceedings of ACL-02, Pages 214-221. 
Richard C. Wang and William W. Cohen. 2008. Iterative Set Expansion of Named Entities using the Web. In: 
Proceedings of ICDM-08, Pages 1091-1096. 
2289
Richard C. Wang and William W. Cohen. 2009. Automatic Set Instance Extraction using the Web. In: 
Proceedings of ACL-09, Pages 441-449. 
Vishnu Vvas, Patrick Pantel and Eric Crestan. 2009. Helping editors choose better seed sets for entity set 
expansion. In: Proceedings of CIKM-09, Pages 225-234 
Roman Yangarber, Winston Lin and Ralph Grishman. 2002. Unsupervised learning of generalized names. In: 
Proceedings of COLING-02, Pages 1-7. 
2290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 105?115, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An Entity-Topic Model for Entity Linking 
Xianpei Han        Le Sun 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
 
 
Abstract 
Entity Linking (EL) has received 
considerable attention in recent years. 
Given many name mentions in a document, 
the goal of EL is to predict their referent 
entities in a knowledge base. Traditionally, 
there have been two distinct directions of 
EL research: one focusing on the effects of 
mention?s context compatibility, assuming 
that ?the referent entity of a mention is 
reflected by its context?; the other dealing 
with the effects of document?s topic 
coherence, assuming that ?a mention?s 
referent entity should be coherent with the 
document?s main topics?. In this paper, we 
propose a generative model ? called entity-
topic model, to effectively join the above 
two complementary directions together. By 
jointly modeling and exploiting the context 
compatibility, the topic coherence and the 
correlation between them, our model can 
accurately link all mentions in a document 
using both the local information (including 
the words and the mentions in a document) 
and the global knowledge (including the 
topic knowledge, the entity context 
knowledge and the entity name knowledge). 
Experimental results demonstrate the 
effectiveness of the proposed model. 
1 Introduction 
Entity Linking (EL) has received considerable 
research attention in recent years (McNamee & 
Dang, 2009; Ji et al2010). Given many name 
mentions in a document, the goal of EL is to 
predict their referent entities in a given knowledge 
base (KB), such as the Wikipedia1. For example, as 
                                                          
1 www.wikipedia.org 
shown in Figure 1, an EL system should identify 
the referent entities of the three mentions WWDC, 
Apple and Lion correspondingly are the entities 
Apple Worldwide Developers Conference, Apple 
Inc. and Mac OS X Lion in KB. The EL problem 
appears in many different guises throughout the 
areas of natural language processing, information 
retrieval and text mining. For instance, in many 
applications we need to collect all appearances of a 
specific entity in different documents, EL is an 
effective way to resolve such an information 
integration problem. Furthermore, EL can bridge 
the mentions in documents with the semantic 
information in knowledge bases (e.g., Wikipedia 
and Freebase 2 ), thus can provide a solid 
foundation for knowledge-rich methods. 
 
Figure 1. A Demo of Entity Linking 
Unfortunately, the accurate EL is often hindered 
by the name ambiguity problem, i.e., a name may 
refer to different entities in different contexts. For 
example, the name Apple may refer to more than 
20 entities in Wikipedia, such as Apple Inc., Apple 
(band) and Apple Bank. Traditionally, there have 
been two distinct directions in EL to resolve the 
name ambiguity problem: one focusing on the 
effects of mention?s context compatibility and the 
other dealing with the effects of document?s topic 
coherence. EL methods based on context 
                                                          
2 www.freebase.com 
At the WWDC 
conference, 
Apple  
introduces its 
new operating 
system release - 
Lion. 
Document Knowledge Base
Apple Inc. 
MAC OS X Lion 
Steve Jobs 
iPhone
Apple Worldwide 
Developers 
Conference 
California
105
compatibility assume that ?the referent entity of a 
mention is reflected by its context?(Mihalcea & 
Cosomai, 2007; Zhang et al2010; Zheng et al
2010; Han & Sun, 2011; Kataria et al2011; Sen 
2012). For example, the context compatibility 
based methods will identify the referent entity of 
the mention Lion in Figure 1 is the entity Mac OS 
X Lion, since this entity is more compatible with its 
context words operating system and release than 
other candidates such as Lion(big cats) or 
Lion(band). EL methods based on topic coherence 
assume that ?a mention?s referent entity should be 
coherent with document?s main topics? (Medelyan 
et al2008; Milne & Witten, 2008; Kulkarni et al
2009; Han et al2011). For example, the topic 
coherence based methods will link the mention 
Apple in Figure 1 to the entity Apple Inc., since it 
is more coherent with the document?s topic MAC 
OS X Lion Release than other referent candidates 
such as Apple (band) or Apple Bank. 
In recent years, both of the above two EL 
directions have shown their effectiveness to some 
extent, and obviously they are complementary to 
each other. Therefore we believe that bring the 
above two directions together will enhance the EL 
performance. Traditionally, the above two 
directions are usually be brought together using a 
hybrid method (Zhang and Sim, 2011; Ratinov et 
al., 2011; Han et al2011), i.e., the context 
compatibility and the topic coherence are first 
separately modeled, then their EL evidence are 
combined through an additional model. For 
example, Zhang and Sim (2011) first models the 
context compatibility as a context similarity and 
the topic coherence as a similarity between the 
underlying topics of documents and KB entries, 
then these two similarities are combined through 
an additional SVM classifier for the final EL 
decision. 
The main drawback of these hybrid methods, 
however, is that they model the context 
compatibility and the topic coherence separately, 
which makes it difficult to capture the mutual 
reinforcement effect between the above two 
directions. That is, the topic coherence and the 
context compatibility are highly correlated and 
their evidence can be used to reinforce each other 
in EL decisions. For example, in Figure 1, if the 
context compatibility gives a high likelihood the 
mention Apple refers to the entity Apple Inc., then 
this likelihood will give more evidence for this 
document?s topic is about MAC OS X Lion, and it 
in turn will reinforce the topic coherence between 
the entity MAC OS X Lion and the document. In 
reverse, once we known the topic of this document 
is about MAC OS X Lion, the context compatibility 
between the mention Apple and the entity Apple 
Inc. can be improved as the importance of the 
context words operating system and release will be 
increased using the topic knowledge. In this way, 
we believe that modeling the above two directions 
jointly, rather than separately, will further improve 
the EL performance by capturing the mutual 
reinforcement effect between the context 
compatibility and the topic coherence. 
In this paper, we propose a method to jointly 
model and exploit the context compatibility, the 
topic coherence and the correlation between them 
for better EL performance. Specifically, we 
propose a generative probabilistic model ? called 
entity-topic model, which can uniformly model the 
text compatibility and the topic coherence as the 
statistical dependencies between the mentions, the 
words, the underlying entities and the underlying 
topics of a document by assuming that each 
document is generated according to the following 
two assumptions: 
1) Topic coherence assumption: All entities 
in a document should be centered around the main 
topics of the document. For example, the entity 
Apple Inc. tends to occur in documents about IT, 
but the entity Apple Bank  will more likely to occur 
in documents about bank or investment. 
2) Context compatibility assumption: The 
context words of a mention should be centered on 
its referent entity. For example, the words 
computer, phone and music tends to occur in the 
context of the entity Apple Inc., meanwhile the 
words loan, invest and deposit will more likely to 
occur in the context of the entity Apple Bank. 
In this way, the entity-topic model uniformly 
models the context compatibility, the topic 
coherence and the correlation between them as the 
dependencies between the observed information 
(the mentions and the words) in a document and 
the hidden information we want to know (the 
underlying topics and entities) through the global 
knowledge (including the topic knowledge, the 
entity name knowledge and the entity context 
knowledge). And the EL problem can now be 
decomposed into the following two inference tasks: 
106
1) Predicting the underlying topics and the 
underlying entities of a document based on the 
observed information and the global knowledge. 
We call such a task the prediction task; 
2) Estimating the global knowledge from data. 
Notice that the topic knowledge, the entity 
name knowledge and the entity context 
knowledge are all not previously given, thus we 
need to estimate them from data. We call such a 
task the knowledge discovery task. 
Because the accurate inference of the above two 
tasks is intractable in our entity-topic model, this 
paper also develops an approximate inference 
algorithm ? the Gibbs sampling algorithm to solve 
them. 
Contributions. The main contributions of this 
paper are summarized below: 
y We propose a generative probabilistic 
model, the entity-topic model, which can jointly 
model and exploit the context compatibility, the 
topic coherence and the correlation between them 
for better EL performance; 
y We develop a Gibbs sampling algorithm to 
solve the two inference tasks of our model: 1) 
Discovering the global knowledge from data; and 2) 
Collectively making accurate EL decisions. 
This paper is organized as follows. Section 2 
describes the proposed entity-topic model. Section 
3 demonstrates the Gibbs sampling algorithm. The 
experimental results are presented and discussed in 
Section 4. The related work is reviewed in Section 
5. Finally we conclude this paper in Section 6. 
2 The Entity-Topic Model for Entity 
Linking 
In this section, we describe the proposed entity-
topic model. In following we first demonstrate how 
to capture the context compatibility, the topic 
coherence and the correlation between them in the 
document generative process, then we incorporate 
the global knowledge generation into our model 
for knowledge estimation from data. 
2.1 Document Generative Process 
As shown in Section 1, we jointly model the 
context compatibility and the topic coherence as 
the statistical dependencies in the entity-topic 
model by assuming that all documents are 
generated in a topical coherent and context 
compatible way. In following we describe the 
document generative process. 
In our model, each document d is assumed 
composed of two types of information, i.e., the 
mentions and the words. Formally, we represent a 
document as: 
A document is a collection of M mentions and 
N words, denoted as d = {m1, ?, mM; w1, ?, 
wN}, with mi the ith mention and wj the jth word. 
For example, the document in Figure 1 is 
represented as d = {WWDC, Apple, Lion;   at, the, 
conference, ?}, where WWDC, Apple, Lion are 
the three mentions and the other are the words. 
To generate a document, our model relies on 
three types of global knowledge, including: 
y Topic Knowledge ?  (The entity 
distribution of topics): In our model, all entities in 
a document are generated based on its underlying 
topics, with each topic is a group of semantically 
related entities. Statistically, we model each topic 
as a multinomial distribution of entities, with the 
probability indicating the likelihood an entity to be 
extracted from this topic. For example, we may 
have a topic ?Apple Inc:= {Steve Jobs0.12, iPhone0.07, 
iPod0.08, ?}, indicating the likelihood of the entity 
Steve Jobs be extracted from this topic is 0.12, etc. 
y Entity Name Knowledge ?  (The name 
distribution of entities): In our model, all name 
mentions are generated using the name knowledge 
of its referent entity. Specifically, we model the 
name knowledge of an entity as a multinomial 
distribution of its names, with the probability 
indicating the likelihood this entity is mentioned 
by the name. For example, the name knowledge of 
the entity Apple Inc. may be ?Apple Inc: = {Apple0.51, 
Apple Computer Inc.0.10, Apple Inc.0.07, ?}, indicating 
that the entity Apple Inc. is mentioned by the name 
Apple with probability 0.51, etc. 
y Entity Context Knowledge ? (The context 
word distribution of entities): In our model, all 
context words of an entity?s mention are generated 
using its context knowledge. Concretely, we model 
the context knowledge of an entity as a 
multinomial distribution of words, with the 
probability indicating the likelihood a word 
appearing in this entity?s context. For example, we 
may have ?Apple Inc:= {phone0.07, computer0.10, IT0.06, 
phone0.002, ?}, indicating that the word computer 
appearing in the context of the entity Apple Inc. 
with probability 0.1, etc. 
107
 
Figure 2. The document generative process, with 
Dir(:), Mult(:) and Unif(:) correspondingly 
Dirichlet, Multinomial and Uniform distribution 
Given the entity list E = {e1, e2, ?, eE} in the 
knowledge base, the word list V = {w1, w2, ?, wv}, 
the entity name list K = {n1, n2, ?, nK} and the 
global knowledge described in above, the 
generation process of a document collection 
(corpus) D = {d1, d2, ?, dD} is shown in Figure 2.  
To demonstrate the generation process, we also 
demonstrate how the document in Figure 1 can be 
generated using our model in following steps: 
Step 1: The model generates the topic 
distribution of the document as ?d = {Apple Inc.0.45, 
Operating System(OS)0.55}; 
Step 2: For the three mentions in the document: 
i. According to the topic distribution  ?d, the 
model generates their topic assignments as 
z1=Apple Inc., z2 = Apple Inc., z3 = OS; 
ii. According to the topic knowledge ?Apple Inc. , 
?OS  and the topic assignments z1, z2, z3, the model 
generates their entity assignments as e1 = Apple 
Worldwide Developers Conference, e2 = Apple Inc., 
e3 = Mac OS X Lion; 
iii. According to the name knowledge of the 
entities Apple Worldwide Developers Conference, 
Apple Inc. and Mac OS X Lion, our model 
generates the three mentions as m1=WWDC, m2 = 
Apple, m3 = Lion; 
Step 3: For all words in the document: 
i. According to the referent entity set in 
document ed = {Apple Worldwide Developers 
Conference, Apple Inc., Mac OS X Lion}, the 
model generates the target entity they describes as 
a3=Apple Worldwide Developers Conference and 
a4=Apple Inc.; 
ii. According to their target entity and the 
context knowledge of these entities, the model 
generates the context words in the document. For 
example, according to the context knowledge of 
the entities Apple Worldwide Developers 
Conference, the model generates its context word 
w3 =conference, and according to the context 
knowledge of the entity Apple Inc., the model 
generates its context word w4 = introduces. 
Through the above generative process, we can 
see that all entities in a document are extracted 
from the document?s underlying topics, ensuring 
the topic coherence; and all words in a document 
are extracted from the context word distributions 
of its referent entities, resulting in the context 
compatibility. Furthermore, the generation of 
topics, entities, mentions and words are highly 
correlated, thus our model can capture the 
correlation between the topic coherence and the 
context compatibility. 
2.2 Global Knowledge Generative Process 
The entity-topic model relies on three types of 
global knowledge (including the topic knowledge, 
the entity name knowledge and the entity context 
knowledge) to generate a document. Unfortunately, 
all three types of global knowledge are unknown 
and thus need to be estimated from data. In this 
paper we estimate the global knowledge through 
Bayesian inference by also incorporating the 
knowledge generation process into our model. 
Specifically, given the topic number T, the entity 
number E, the name number K and the word 
number V, the entity-topic model generates the 
global knowledge as follows: 
1) ?j? ? Dir(?) 
For each topic z, our model samples its entity 
distribution ?z from an E-dimensional Dirichlet 
distribution with hyperparameter ?. 
2) ?j? ? Dir(?) 
For each entity e, our model samples its name 
distribution ?e from a K-dimensional Dirichlet 
distribution with hyperparameter ?. 
3) ?j? ? Dir(?) 
Given the topic knowledge ? , the entity name 
knowledge ? and the entity context knowledge ?: 
1. For each doc d in D, sample its topic distribution
?d ? Dir(?); 
2. For each of the Md mentions mi in doc d: 
a) Sample a topic assignment zi ? Mult(?d); 
b) Sample an entity assignment ei ? Mult(?zi);
c) Sample a mention mi ? Mult(?ei); 
3. For each of the Nd words wi in doc d: 
a) Sample a target entity it describes from d?s 
referent entities ai ? Unif(em1 ; em2 ;? ? ? ; emd);
b) Sample a describing word using ai?s context 
word distribution wi ? Mult(?ai). 
108
For each entity e, our model samples its context 
word distribution ?e  from a V-dimensional 
Dirichlet distribution with hyperparameter ?. 
Finally, the full entity-topic model is shown in 
Figure 3 using the plate representation. 
?
?
 
Figure 3. The plate representation of the entity-
topic model 
2.3 The Probability of a Corpus 
Using the entity-topic model, the probability of 
generating a corpus D={d1, d2, ?, dD} given 
hyperparameters ?, ?, ? and ? can be expressed as: 
P (D;?; ?; ?; ?) =
Y
d
P (md;wd;?; ?; ?; ?)
=
Y
d
X
ed
P (edj?; ?)P (mdjed; ?)P (wdjed; ?)
=
Z
?
P (?j?)
Z
?
P (?j?)
Y
d
X
ed
P (mdjed; ?)
?
Z
?
P (?j?)
X
ad
P (adjed)P (wdjad; ?)
?
Z
?
P (?j?)P (edj?; ?)d?d?d?d? (2:1)
 
where md  and ed  correspondingly the set of 
mentions and their entity assignments in document 
d, wd and ad correspondingly the set of words and 
their entity assignments in document d. 
3 Inference using Gibbs Sampling 
In this section, we describe how to resolve the 
entity linking problem using the entity-topic model. 
Overall, there were two inference tasks for EL: 
1) The prediction task. Given a document d, 
predicting its entity assignments (ed for mentions 
and ad  for words) and topic assignments ( zd ). 
Notice that here the EL decisions are just the 
prediction of per-mention entity assignments (ed). 
2) The knowledge discovery task. Given a 
corpus D={d1, d2, ?, dD}, estimating the global 
knowledge (including the entity distribution of 
topics ?, the name distribution ? and the context 
word distribution ? of entities) from data. 
Unfortunately, due to the heaven correlation 
between topics, entities, mentions and words (the 
correlation is also demonstrated in Eq. (2.1), where 
the integral is intractable due to the coupling 
between ? , ?, ? and ? ), the accurate inference of 
the above two tasks is intractable. For this reason, 
we propose an approximate inference algorithm ? 
the Gibbs sampling algorithm for the entity-topic 
model by extending the well-known Gibbs 
sampling algorithm for LDA (Griffiths & Steyvers, 
2004). In Gibbs sampling, we first construct the 
posterior distribution P (z; e;ajD) , then this 
posterior distribution is used to: 1) estimate ?, ?, ? 
and ?; and 2) predict the entities and the topics of 
all documents in D. Specifically, we first derive the 
joint posterior distribution from Eq. (2.1) as: 
P (z; e; ajD) / P (z)P (ejz)P (mje)P (aje)P (wja) 
where 
P (z) = (?(T?)?(?)T )
D
DY
d=1
Q
t ?(? +CDTdt )
?(T? + CDTd? )
       (3.1) 
is the probability of the joint topic assignment z to 
all mentions m in corpus D, and 
P (ejz) = (?(E?)?(?)E )
T
TY
t=1
Q
e ?(? + CTEte )
?(E? + CTEt? )
     (3.2) 
is the conditional probability of the joint entity 
assignments e to all mentions m in corpus D given 
all topic assignments z, and 
P (mje) = (?(K?)?(?)K )
E
EY
e=1
Q
m ?(? + CEMem )
?(K? + CEMe? )
   (3.3) 
is the conditional probability of all mentions m 
given all per-mention entity assignments e, and 
P (aje) =
DY
d=1
Y
e?ed
?CDEde
CDEd?
?CDAde              (3.4) 
is the conditional probability of the joint entity  
assignments a to all words w in corpus D given all 
per-mention entity assignments e, and 
109
P (wja) = (?(V ?)?(?)V )
E
EY
e=1
Q
w ?(? + CEWew )
?(V ? + CEWe? )
    (3.5) 
is the conditional probability of all words w given 
all per-word entity assignments a . In all above 
formulas, ?(:) is the Gamma function, CDTdt  is the 
times topic t has been assigned for all mentions in 
document d, CDTd? =
P
t CDTdt  is the topic number 
in document d, and CTEte , CEMem ,CDEde , CDAde , C
EW
ew  
have similar explanation. 
Based on the above joint probability, we 
construct a Markov chain that converges to the 
posterior distribution P (z; e;ajD) and then draw 
samples from this Markov chain for inference. For 
entity-topic model, each state in the Markov chain 
is an assignment (including topic assignment to a 
mention, entity assignment to a mention and entity 
assignment to a word). In Gibbs sampling, all 
assignments are sequentially sampled conditioned 
on all the current other assignments. So here we 
only need to derive the following three fully 
conditional assignment distributions: 
1) P (zi = tjz?i; e;a;D): the topic assignment 
distribution to a mention given the current 
other topic assignments z?i , the current 
entity assignments e and a; 
2) P (ei = ejz; e?i;a;D) : the entity 
assignment distribution to a mention given 
the current entity assignments of all other 
mentions e?i, the current topic assignments 
z  and the current entity assignments of 
context words a; 
3) P (ai = ejz; e; a?i;D) : the entity 
assignment distribution to a context word 
given the current entity assignments of all 
other context words a?i, the current topic 
assignments  z  and the current entity 
assignments e of mentions. 
Using the Formula 3.1-3.5, we can derive the 
above three conditional distributions as (where mi 
is contained in doc d): 
P (zi = tjz?i;e;a;D) /
CDT(?i)dt + ?
CDT(?i)d? + T?
?
CTE(?i)te + ?
CTE(?i)t? +E?
 
where the topic assignment to a mention is 
determined by the probability this topic appearing 
in doc d (the 1st term) and the probability the 
referent entity appearing in this topic (the 2nd term); 
P (ei = ejz; e?i;a;D) /
CTE(?i)te + ?
CTE(?i)t? +E?
?
CEM(?i)em + ?
CEM(?i)e? +K?
?
?CDE(?i)de + 1
CDE(?i)de
?CDAde  
where the entity assignment to a mention is 
determined by the probability this entity extracted 
from the assigned topic (the 1st term), the 
probability this entity is referred by the name m 
(the 2nd term) and the contextual words describing 
this entity in doc d (the 3rd term); 
P (ai = ejz; e;a?i;D) /
CDEde
CDEd?
?
CEW(?i)ew + ?
CEW(?i)e? + V ?
 
where the entity assignment to a word is 
determined by the number of times this entity has 
been assigned to mentions in doc d (the 1st term) 
and the probability the word appearing in the 
context of this entity (the 2nd term). 
Finally, using the above three conditional 
distributions, we iteratively update all assignments 
of corpus D until coverage, then the global 
knowledge is estimated using the final assignments, 
and the final entity assignments are used as the 
referents of their corresponding mentions. 
Inference on Unseen Documents. When 
unseen documents are given, we predict its entities 
and topics using the incremental Gibbs sampling 
algorithm described in (Kataria et al2011), i.e., 
we iteratively update the entity assignments and 
the topic assignments of an unseen document as 
the same as the above inference process, but with 
the previously learned global knowledge fixed. 
Hyperparameter setting. One still problem 
here is the setting of the hyperparameters ?, ?, ? 
and ?. For ? and ? , this paper empirically set the 
value of them to ? = 50=T  and ? = 0:1  as in 
Griffiths & Steyvers(2004). For ?, we notice that 
K? is the number of pseudo names added to each 
entity, when ? = 0  our model only mentions an 
entity using its previously used names. Observed 
that an entity typically has a fixed set of names, we 
set ? to a small value by setting K? = 1:0. For ?, 
we notice that V ? is the number of pseudo words 
added to each entity, playing the role of smoothing 
its context word distribution. As there is typically a 
relatively loose correlation between an entity and 
its context words, we set ?  to a relatively large 
value by fixing the total smoothing words added to 
each entity, a typical value is V ? = 2000. 
110
4 Experiments 
In this section, we evaluate our method and 
compare it with the traditional EL methods. We 
first explain the experimental settings in Section 
4.1-4.4, then discuss the results in Section 4.5. 
4.1 Knowledge Base 
In our experiments, we use the Jan. 30, 2010 
English version of Wikipedia as the knowledge 
base, which contains over 3 million entities. Notice 
that we also take the general concepts in Wikipedia 
(such as Apple, Video, Computer, etc.) as entities, 
so the entity in this paper may not strictly follow 
its definition. 
4.2 Data Sets 
There are two standard data sets for EL: IITB3 and 
TAC 2009 EL data set (McNamee & Dang, 2009), 
where IITB focuses on aggressive recall EL and 
TAC 2009 focuses on EL on salient mentions. Due 
to the collective nature of our method, we mainly 
used the IITB as the primary data set as the same 
as Kulkarni et al009) and Han et al011). But 
we also give the EL accuracies on the TAC 2009 in 
Sect. 4.5.4 as auxiliary results.  
Overall, the IITB data set contains 107 web 
documents. For each document, the name 
mentions? referent entities in Wikipedia are 
manually annotated to be as exhaustive as possible. 
In total, 17,200 name mentions are annotated, with 
161 name mentions per document on average. In 
our experiments, we use only the name mentions 
whose referent entities are contained in Wikipedia. 
4.3 Evaluation Criteria 
This paper adopted the same performance metrics 
used in the Kulkarni et al2009), which includes 
Recall, Precision and F1. Let M* be the golden 
standard set of the EL results (each EL result is a 
pair (m, e), with m the mention and e its referent 
entity), M be the set of EL results outputted by an 
EL system, then these metrics are computed as: 
Precision = jM\M
?j
jMj  
Recall = jM\M
?j
jM?j  
where two EL results are considered equal if and 
only if both their mentions and referent entities are 
equal. As the same as Kulkarni et al009), 
                                                          
3 http://www.cse.iitb.ac.in/~soumen/doc/QCQ/ 
Precision and Recall are averaged across 
documents and overall F1 is used as the primary 
performance metric by computing from average 
Precision and Recall. 
4.4 Baselines 
We compare our method with five baselines which 
are described as follows: 
Wikify!. This is a context compatibility based 
EL method using vector space model (Mihalcea & 
Csomai, 2007). Wikify! computes the context 
compatibility using the word overlap between the 
mention?s context and the entity?s Wikipedia entry. 
EM-Model. This is a statistical context 
compatibility based EL method described in Han 
& Sun(2011), which computes the compatibility by 
integrating the evidence from the entity popularity, 
the entity name knowledge and the context word 
distribution of entities. 
M&W. This is a relational topic coherence based 
EL method described in Milne & Witten(2008). 
M&W measures an entity?s topic coherence to a 
document as its average semantic relatedness to the 
unambiguous entities in the document. 
CSAW. This is an EL method which combines 
context compatibility and topic coherence using a 
hybrid method (Kulkarni et al2009), where 
context compatibility and topic coherence are first 
separated modeled as context similarity and the 
sum of all pair-wise semantic relatedness between 
the entities in the document, then the entities which 
can maximize the weighted sum of the context 
compatibility and the topic coherence are identified 
as the referent entities of the document. 
EL-Graph. This is a graph based hybrid EL 
method described in Han et al2011), which first 
models the context compatibility as text similarity 
and the topic coherence of an entity as its node 
importance in a referent graph which captures all 
mention-entity and entity-entity relations in a 
document, then a random walk algorithm is used to 
collectively find all referent entities of a document. 
Except for CSAW and EL-Graph, all other 
baselines are designed only to link the salient name 
mentions (i.e., key phrases) in a document. In our 
experiment, in order to compare the EL 
performances on also the non-salient name 
mentions, we push these systems? recall by 
reducing their respective importance thresholds of 
linked mentions. 
111
4.5 Experimental Results 
4.5.1 Overall Performance 
We compared our method with all the above five 
baselines. For our method, we estimate the global 
knowledge using all the articles in the Jan. 30, 
2010 English version of Wikipedia, and totally 
there were 3,083,158 articles. For each article, the 
mentions within it are detected using the methods 
described in Medelyan et al008) and all terms in 
an article are used as context words, so a term may 
both be a mention and a context word. The topic 
number of our model is T = 300  (will be 
empirically set in Sect 4.5.2). To train the entity-
topic model, we run 500  iterations of our Gibbs 
sampling algorithm to converge. The training time 
of our model is nearly one week on our server 
using 20 GB RAM and one core of 3.2 GHz CPU. 
Since the training can be done offline, we believe 
that the training time is not critical to the real-
world usage as the online inference on new 
document is very quick. Using the above settings, 
the overall results are shown in Table 1. 
 Precision Recall F1 
Wikify! 0.55 0.28 0.37 
EM-Model 0.82 0.48 0.61 
M&W 0.80 0.38 0.52 
CSAW 0.65 0.73 0.69 
EL-Graph 0.69 0.76 0.73 
Our Method 0.81 0.80 0.80 
Table 1. The overall results on IITB data set 
From the overall results in Table 1, we can see that: 
1) By jointly modeling and exploiting the 
context compatibility and the topic coherence, our 
method can achieve competitive performance: ?1  
compared with the context compatibility baselines 
Wikify! and EM-Model, our method 
correspondingly gets 43% and 19% F1 
improvement; ?2  compared with the topic 
coherence baselines M&W, our method achieves 
28% F1 improvement; ?  compared with the 
hybrid baselines CSAW and EL-Graph, our method 
correspondingly achieves 11% and 7% F1 
improvement. 
2) Compared with the context compatibility 
only and the topic coherence only methods, the 
main advantage of our method is that, rather than 
only achieved high entity linking precision on 
salient mentions, it can also effectively link the 
non-salient mentions in a document: this is 
demonstrated in our method?s significant Recall 
improvement: a 32~52% Recall improvement over 
baselines Wikify!, EM-Model and M&W. We 
believe this is because a document usually contains 
little evidence for EL decisions on non-salient 
mentions, so with either only context compatibility 
or only topic coherence the evidence is not enough 
for EL decisions on these non-salient mentions, 
and bring these two directions together is critical 
for the accurate EL on these mentions. 
3) Compared with the hybrid methods, the 
main advantage of our method is the improvement 
of EL precision (a 11~16% improvement over 
baselines CSAW and EL-Graph), we believe this is 
because: ?1 Our method can further capture the 
mutual reinforcement effect between the context 
compatibility and the topic coherence; ?2 The 
traditional hybrid methods usually determine the 
topic coherence of an entity to a document using 
all entities in the document, in comparison our 
method uses only the entities in the same topic, we 
believe this is more reasonable for EL decisions. 
4.5.2 Parameter Tuning 
One still parameter of our method is the topic 
number T. An appropriate T will distribute entities 
into well-organized topics, in turn it will capture 
the co-occurrence information of entities. Figure 4 
plots the F1 at different T values. We can see that 
the F1 is not very sensitive to the topic number and 
with T = 300  our method achieves its best F1 
performance. 
0 200 400 600 800 1000
0.770
0.775
0.780
0.785
0.790
0.795
0.800
F1
T  
Figure 4. The F1 vs. the topic number T 
4.5.3 Detailed Analysis 
In this section we analyze why and how our 
method works well in detail. Generally, we believe 
the main advantages of our method are: 
1) The effects of topic knowledge. One main 
advantage of our model is that the topic knowledge 
112
can provide a document-specific entity prior for EL. 
Concretely, using the topic knowledge and the 
topic distribution of documents, the prior for an 
entity appearing in a document d is highly related 
to the document?s topics: 
P (ejd) =Pz P (zjd)P (ejz) 
This prior is obviously more reasonable than the 
?information less prior? (i.e., all entities have equal 
prior) or ?a global entity popularity prior? (Han & 
Sun, 2011). To demonstrate, Table 2-3 show the 3 
topics where the Apple Inc. and the fruit Apple 
have the largest generation probability P(e|z) from 
these topics. We can see that the topic knowledge 
can provide a reasonable prior for entities 
appearing in a document: the Apple Inc. has a large 
prior in documents about Computer, Video and 
Software, and the fruit Apple has a large prior in 
documents about Wine, Food and Plant. 
Topic(Computer) Topic(Video) Topic(Software) 
Computer 
CPU 
Hardware 
Personal computer 
Video 
Mobile phone 
Mass media 
Music 
Computer software
Microsoft Windows
Linux 
Web browser 
Computer memory Television Operating system
Table 2. The 3 topics where the Apple Inc. has the 
largest P(e|z) 
Topic(Wine) Topic(Food) Topic(Plant) 
Wine 
Grape 
Vineyard 
Winery 
Food 
Restaurant 
Meat 
Cheese 
Plant 
Flower 
Leaf 
Tree 
Apple Vegetable Fruit 
Table 3. The 3 topics where the fruit Apple has the 
largest P(e|z) 
2) The effects of a fine-tuned context model. 
The second advantage of our model is that it 
provides a statistical framework for fine-tuning the 
context model from data. To demonstrate such an 
effect, Table 4 compares the EL performance of  
? the entity-topic model with no context model is 
used (No Context), i.e., we determine the referent 
entity of a mention by deleting the 3rd term of the 
formula P (ei = ejz;e?i;a;D) in Section 3; ? with 
the context model estimated using the entity?s 
Wikipedia page (Article Content), ? with the 
context model estimated using the 50 word 
window of all its mentions in Wikipedia (Mention 
Context) and; ?  with the context model in the 
original entity-topic model (Entity-Topic Model). 
From Table 4 we can see that a fine-tuned context 
model will result in a 2~7% F1 improvement. 
Context Model F1 
No Context 0.73 
Article Content 
Mention Context 
Entity-Topic Model 
0.75 
0.78 
0.80 
Table 4. The F1 using different context models 
3) The effects of joint model. The third 
advantage of our model is that it jointly model the 
context compatibility and the topic coherence, 
which bring two benefits: ?  the mutual 
reinforcement between the two directions can be 
captured in our model; ? the context compatibility 
and the topic coherence are uniformly modeled and 
jointly estimated, which makes the model more 
accurate for EL. 
4.5.4 EL Accuracies on TAC 2009 dataset 
We also compare our method with the top 5 EL 
systems in TAC 2009 and the two state-of-the-art 
systems (EM-Model and EL-Graph) on TAC 2009 
data set in Figure 5 (For EL-Graph and our method, 
a NIL threshold is used to detect whether the 
referent entity is contained in the knowledge base, 
if the knowledge base not contains the referent 
entity, we assign the mention to a NIL entity). 
From Figure 5, we can see that our method is 
competitive: 1) Our method can achieve a 3.4% 
accuracy improvement over the best system in 
TAC 2009; 2) Our method, EM-Model and EL-
Graph get very close accuracies (0.854, 0.86 and 
0.838 correspondingly), we believe this is because: 
?1  The mentions to be linked in TAC data set are 
mostly salient mentions; ?2  The influence of the 
NIL referent entity problem, i.e., the referent entity 
is not contained in the given knowledge base: Most 
referent entities (67.5%) on TAC 2009 are NIL 
entity and our method has no special handling on 
this problem, rather than other methods such as the 
EM-Model, which affects the overall performance 
of our method. 
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
 
Figure 5. The EL accuracies on TAC 2009 dataset 
113
5 Related Work 
In this section, we briefly review the related work 
of EL. Traditionally, the context compatibility 
based methods link a mention to the entity which 
has the largest compatibility with it. Cucerzan 
(2007) modeled the compatibility as the cosine 
similarity between the vector space representation 
of mention?s context and of entity?s Wikipedia 
entry. Mihalcea & Csomai (2007), Bunescu & 
Pasca (2006), Fader et al2009), Gottipati et 
al.(2011) and Zhang et al011) extended the 
vector space model with more information such as 
the entity category and the acronym expansion, etc. 
Han & Sun (2011) proposed a generative model 
which computes the compatibility using the 
evidences from entity?s popularity, name 
distribution and context word distribution. Kataria 
et al011) and Sen (2012) used a latent topic 
model to learn the context model of entities. Zheng 
et al2010), Dredze et al2010), Zhang et al
(2010), Zhou et al2010) and Ji & Chen(2011) 
employed the ranking techniques to further take 
relations between candidate entities into account. 
On the other side, the topic coherence based 
methods link a mention to the entity which are 
most coherent to the document containing it. 
Medelyan et al2008) measured the topic 
coherence of an entity to a document as the 
weighted average of its relatedness to the 
unambiguous entities in the document. Milne and 
Witten (2008) extended Medelyan et al2008)?s 
coherence by incorporating commonness and 
context quality. Bhattacharya and Getoor (2006) 
modeled the topic coherence as the likelihood an 
entity is generated from the latent topics of a 
document. Sen (2012) modeled the topic coherence 
as the groups of co-occurring entities. Kulkarni et 
al. (2009) modeled the topic coherence as the sum 
of all pair-wise relatedness between the referent 
entities of a document. Han et al011) and 
Hoffart et al011) modeled the topic coherence of 
an entity as its node importance in a graph which 
captures all mention-entity and entity-entity 
relations in a document. 
6 Conclusions and Future Work 
This paper proposes a generative model, the entity-
topic model, for entity linking. By uniformly 
modeling context compatibility, topic coherence 
and the correlation between them as statistical 
dependencies, our model provides an effective way 
to jointly exploit them for better EL performance. 
In this paper, the entity-topic model can only 
link mentions to the previously given entities in a 
knowledge base. For future work, we want to 
overcome this limit by incorporating an entity 
discovery ability into our model, so that it can also 
discover and learn the knowledge of previously 
unseen entities from a corpus for linking name 
mentions to these entities. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no.  
90920010 and 61100152. Moreover, we sincerely 
thank the reviewers for their valuable comments. 
References 
Adafre, S. F. & de Rijke, M. 2005. Discovering missing 
links in Wikipedia. In: Proceedings of the 3rd 
international workshop on Link discovery. 
Bhattacharya, I. and L. Getoor. 2006. A latent dirichlet 
model for unsupervised entity resolution. In: 
Proceedings of SIAM International Conference on 
Data Mining. 
Blei, D. M. and A. Y. Ng, et al2003). Latent dirichlet 
allocation. In: The Journal of Machine Learning 
Research 3: 993--1022. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In: 
Proceedings of EACL, vol. 6. 
Brown,  P., Pietra, S. D.,  Pietra, V. D., and Mercer, R.  
1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2), 263-31. 
Chen, S. F. & Goodman, J. 1999. An empirical study of 
smoothing techniques for language modeling.  In 
Computer Speech and Language, London; Orlando: 
Academic Press, c1986-, pp. 359-394. 
Cucerzan, S. 2007. Large-scale named entity 
disambiguation based on Wikipedia data. In:  
Proceedings of EMNLP-CoNLL, pp. 708-716. 
De Beaugrande, R. A. and W. U. Dressler. 1981. 
Introduction to text linguistics, Chapter V, Longman 
London. 
Dredze, M., McNamee, P., Rao, D., Gerber, A. & Finin, 
T. 2010. Entity Disambiguation for Knowledge Base 
Population. In: Proceedings of the 23rd International 
Conference on Computational Linguistics. 
114
Fader, A., Soderland, S., Etzioni, O. & Center, T. 2009. 
Scaling Wikipedia-based named entity 
disambiguation to arbitrary web text. In: Proceedings 
of  Wiki-AI Workshop at IJCAI, vol. 9. 
Gottipati, S., Jiang, J. 2011. Linking Entities to a 
Knowledge Base with Query Expansion. In: 
Proceedings of EMNLP. 
Griffiths, T. L. and M. Steyvers. 2004. Finding 
scientific topics. In: Proceedings of the National 
Academy of Sciences of the United States of 
America. 
Han, X., Sun, L. and Zhao J. 2011. Collective Entity 
Linking in Web Text: A Graph-Based Method. In: 
Proceedings of 34th Annual ACM SIGIR Conference. 
Han, X. and Sun, L. 2011. A Generative Entity-Mention 
Model for Linking Entities with Knowledge Base. In: 
Proceedings of ACL-HLT. 
Hoffart, J., Yosef, M. A., et al011. Robust 
Disambiguation of Named Entities in Text. In: 
Proceedings of EMNLP. 
Jelinek, Frederick and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of the Workshop 
on Pattern Recognition in Practice. 
Kataria, S. S., Kumar, K. S. and Rastogi, R. 2011. Entity 
Disambiguation with Hierarchical Topic Models. In: 
Proceedings of KDD. 
Kulkarni, S., Singh, A., Ramakrishnan, G. & 
Chakrabarti, S. 2009. Collective annotation of 
Wikipedia entities in web text. In: Proceedings of the 
15th ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 457-466. 
Li, X., Morie, P. & Roth, D. 2004. Identification and 
tracing of ambiguous names: Discriminative and 
generative approaches. In: Proceedings of the 
National Conference on Artificial Intelligence, pp. 
419-424. 
McNamee, P. & Dang, H. T. 2009.  Overview of the 
TAC 2009 Knowledge Base Population Track. In: 
Proceeding of Text Analysis Conference. 
Ji, H., et al010. Overview of the TAC 2010 knowledge 
base population track. In: Proceedings of Text 
Analysis Conference. 
Ji, H. and Chen, Z. 2011. Collaborative Ranking: A 
Case Study on Entity Linking. In: Proceedings of 
EMNLP. 
Milne, D. & Witten, I. H. 2008. Learning to link with 
Wikipedia. In: Proceedings of the 17th ACM 
conference on Conference on information and 
knowledge management. 
Milne, D., et al2006. Mining Domain-Specific 
Thesauri from Wikipedia: A case study. In Proc. of 
IEEE/WIC/ACM WI. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. In: Proceedings of the 
AAAI WikiAI workshop. 
Mihalcea, R. & Csomai, A. 2007. Wikify!: linking 
documents to encyclopedic knowledge. In: 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp. 233-242. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. Name 
discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Ratinov, L. and D. Roth, et al011. Local and Global 
Algorithms for Disambiguation to Wikipedia. In: 
Proceedings of ACL. 
Sen, P. 2012. Collective context-aware topic models for 
entity disambiguation. In Proceedings of WWW '12, 
New York, NY, USA, ACM. 
Zhang, W., Su, J., Tan, Chew Lim & Wang, W. T. 2010. 
Entity Linking Leveraging Automatically Generated 
Annotation. In: Proceedings of the 23rd International 
Conference on Computational Linguistics (Coling 
2010). 
Zheng, Z., Li, F., Huang, M. & Zhu, X. 2010. Learning 
to Link Entities with Knowledge Base. In: The 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Zhou, Y., Nie, L., Rouhani-Kalleh, O., Vasile, F. & 
Gaffney, S. 2010. Resolving Surface Forms to 
Wikipedia Topics. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010),  pp. 1335-1343. 
Zhang, W. and Sim, Y. C., et al2011. Entity Linking 
with Effective Acronym Expansion, Instance 
Selection and Topic Modeling?. In: Proceedings of 
IJCAI. 
 
115
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 945?954,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Generative Entity-Mention Model for Linking Entities with 
Knowledge Base 
Xianpei Han        Le Sun 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
 
Abstract 
Linking entities with knowledge base (entity 
linking) is a key issue in bridging the textual 
data with the structural knowledge base. Due to 
the name variation problem and the name 
ambiguity problem, the entity linking decisions 
are critically depending on the heterogenous 
knowledge of entities. In this paper, we propose 
a generative probabilistic model, called entity-
mention model, which can leverage 
heterogenous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task. 
In our model, each name mention to be linked 
is modeled as a sample generated through a 
three-step generative story, and the entity 
knowledge is encoded in the distribution of 
entities in document P(e), the distribution of 
possible names of a specific entity P(s|e), and 
the distribution of possible contexts of a 
specific entity P(c|e). To find the referent entity 
of a name mention, our method combines the 
evidences from all the three distributions P(e), 
P(s|e) and P(c|e). Experimental results show 
that our method can significantly outperform 
the traditional methods. 
1 Introduction 
In recent years, due to the proliferation of 
knowledge-sharing communities like Wikipedia 1 
and the many research efforts for the automated 
knowledge base population from Web like the 
Read the Web2 project, more and more large-scale 
knowledge bases are available. These knowledge 
bases contain rich knowledge about the world?s 
entities, their semantic properties, and the semantic 
relations between each other. One of the most 
notorious examples is Wikipedia: its 2010 English 
                                                        
1 http://www.wikipedia.org/ 
2 http://rtw.ml.cmu.edu/ 
version contains more than 3 million entities and 
20 million semantic relations. Bridging these 
knowledge bases with the textual data can facilitate 
many different tasks such as entity search, 
information extraction and text classification. For 
example, as shown in Figure 1, knowing the word 
Jordan in the document refers to a basketball 
player and the word Bulls refers to a NBA team 
would be helpful in classifying this document into 
the Sport/Basketball class. 
After a standout career at the University,
joined the in 1984.
Michael Jeffrey Jordan
NBA Player
Basketball Player
Chicago Bulls
NBA
Sport Organization
NBA Team
Knowledge Base
Employer-of
IS-A
IS-A IS-A
IS-A
IS-A
Part-of
Jordan
Bulls
 
Figure 1. A Demo of Entity Linking 
A key issue in bridging the knowledge base with 
the textual data is linking the entities in a 
document with their referents in a knowledge base, 
which is usually referred to as the Entity Linking 
task. Given a set of name mentions M = {m1, 
m2, ?, mk} contained in documents and a 
knowledge base KB containing a set of entities E = 
{e1, e2, ?, en}, an entity linking system is a 
function : M E? ?  which links these name 
mentions to their referent entities in KB. For 
example, in Figure 1 an entity linking system 
should link the name mention Jordan to the entity 
Michael Jeffrey Jordan and the name mention 
Bulls to the entity Chicago Bulls. 
The entity linking task, however, is not trivial 
due to the name variation problem and the name 
ambiguity problem. Name variation means that an 
entity can be mentioned in different ways such as 
full name, aliases, acronyms and misspellings. For 
945
example, the entity Michael Jeffrey Jordan can be 
mentioned using more than 10 names, such as 
Michael Jordan, MJ and Jordan. The name 
ambiguity problem is related to the fact that a 
name may refer to different entities in different 
contexts. For example, the name Bulls can refer to 
more than 20 entities in Wikipedia, such as the 
NBA team Chicago Bulls, the football team Belfast 
Bulls and the cricket team Queensland Bulls. 
Complicated by the name variation problem and 
the name ambiguity problem, the entity linking 
decisions are critically depending on the 
knowledge of entities (Li et al, 2004; Bunescu & 
Pasca, 2006; Cucerzan, 2007; Milne & Witten, 
2008 and Fader et al, 2009). Based on the previous 
work, we found that the following three types of 
entity knowledge can provide critical evidence for 
the entity linking decisions: 
? Popularity Knowledge. The popularity 
knowledge of entities tells us the likelihood of an 
entity appearing in a document. In entity linking, 
the entity popularity knowledge can provide a 
priori information to the possible referent entities 
of a name mention. For example, without any other 
information, the popularity knowledge can tell that 
in a Web page the name ?Michael Jordan? will 
more likely refer to the notorious basketball player 
Michael Jeffrey Jordan, rather than the less 
popular Berkeley professor Michael I. Jordan. 
? Name Knowledge. The name knowledge 
tells us the possible names of an entity and the 
likelihood of a name referring to a specific entity. 
For example, we would expect the name 
knowledge tells that both the ?MJ? and ?Michael 
Jordan? are possible names of the basketball 
player Michael Jeffrey Jordan, but the ?Michael 
Jordan? has a larger likelihood. The name 
knowledge plays the central role in resolving the 
name variation problem, and is also helpful in 
resolving the name ambiguity problem. 
? Context Knowledge. The context 
knowledge tells us the likelihood of an entity 
appearing in a specific context. For example, given 
the context ?__wins NBA MVP?, the name 
?Michael Jordan? should more likely refer to the 
basketball player Michael Jeffrey Jordan than the 
Berkeley professor Michael I. Jordan. Context 
knowledge is crucial in solving the name 
ambiguities. 
Unfortunately, in entity linking system, the 
modeling and exploitation of these types of entity 
knowledge is not straightforward. As shown above, 
these types of knowledge are heterogenous, 
making it difficult to be incorporated in the same 
model. Furthermore, in most cases the knowledge 
of entities is not explicitly given, making it 
challenging to extract the entity knowledge from 
data. 
To resolve the above problems, this paper 
proposes a generative probabilistic model, called 
entity-mention model, which can leverage the 
heterogeneous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task. In 
our model, each name mention is modeled as a 
sample generated through a three-step generative 
story, where the entity knowledge is encoded in 
three distributions: the entity popularity knowledge 
is encoded in the distribution of entities in 
document P(e), the entity name knowledge is 
encoded in the distribution of possible names of a 
specific entity P(s|e), and the entity context 
knowledge is encoded in the distribution of 
possible contexts of a specific entity P(c|e). The 
P(e), P(s|e) and P(c|e) are respectively called the 
entity popularity model, the entity name model and 
the entity context model. To find the referent entity 
of a name mention, our method combines the 
evidences from all the three distributions P(e), 
P(s|e) and P(c|e). We evaluate our method on both 
Wikipedia articles and general newswire 
documents. Experimental results show that our 
method can significantly improve the entity linking 
accuracy. 
Our Contributions. Specifically, the main 
contributions of this paper are as follows: 
1) We propose a new generative model, the 
entity-mention model, which can leverage 
heterogenous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task; 
2) By modeling the entity knowledge as 
probabilistic distributions, our model has a 
statistical foundation, making it different from 
most previous ad hoc approaches. 
This paper is organized as follows. The entity-
mention model is described in Section 2. The 
model estimation is described in Section 3. The 
experimental results are presented and discussed in 
Section 4. The related work is reviewed in Section 
5. Finally we conclude this paper in Section 6. 
946
2 The Generative Entity-Mention Model 
for Entity Linking 
In this section we describe the generative entity-
mention model. We first describe the generative 
story of our model, then formulate the model and 
show how to apply it to the entity linking task. 
2.1 The Generative Story 
In the entity mention model, each name mention is 
modeled as a generated sample. For demonstration, 
Figure 2 shows two examples of name mention 
generation. As shown in Figure 2, the generative 
story of a name mention is composed of three steps, 
which are detailed as follows: 
(i) Firstly, the model chooses the referent 
entity e of the name mention from the given 
knowledge base, according to the distribution of 
entities in document P(e). In Figure 2, the model 
chooses the entity ?Michael Jeffrey Jordan? for the 
first name mention, and the entity ?Michael I. 
Jordan? for the second name mention; 
(ii) Secondly, the model outputs the name s of 
the name mention according to the distribution of 
possible names of the referent entity P(s|e). In 
Figure 2, the model outputs ?Jordan? as the name 
of the entity ?Michael Jeffrey Jordan?, and the 
?Michael Jordan? as the name of the entity 
?Michael I. Jordan?; 
(iii) Finally, the model outputs the context c of 
the name mention according to the distribution of 
possible contexts of the referent entity P(c|e). In 
Figure 2, the model outputs the context ?joins 
Bulls in 1984? for the first name mention, and the 
context ?is a professor in UC Berkeley? for the 
second name mention. 
2.2 Model 
Based on the above generative story, the 
probability of a name mention m (its context is c 
and its name is s) referring to a specific entity e 
can be expressed as the following formula (here we 
assume that s and c are independent given e): 
( , , )P(m,e)= P s c e = P(e)P(s |e)P(c|e) 
This model incorporates the three types of entity 
knowledge we explained earlier: P(e) corresponds 
to the popularity knowledge, P(s|e) corresponds to 
the name knowledge and P(c|e) corresponds to the 
context knowledge. 
Knowledge Base
Michael Jeffrey Jordan
Michael I. Jordan
Jordan Michael Jordan
Jordan joins Bulls in
1984.
Michael Jordan is a
professor in UC Berkeley.
Entity
Name
Mention
 
Figure 2.  Two examples of name mention 
generation 
Given a name mention m, to perform entity 
linking, we need to find the entity e which 
maximizes the probability P(e|m). Then we can 
resolve the entity linking task as follows: 
( , )e argmax argmax ( ) ( | ) ( | )( ) ee
P m e P e P s e P c eP m? ?
 
Therefore, the main problem of entity linking is to 
estimate the three distributions P(e), P(s|e) and 
P(c|e), i.e., to extract the entity knowledge from 
data. In Section 3, we will show how to estimate 
these three distributions. 
Candidate Selection. Because a knowledge base 
usually contains millions of entities, it is time-
consuming to compute all P(m,e) scores between a 
name mention and all the entities contained in a 
knowledge base. To reduce the time required, the 
entity linking system employs a candidate selection 
process to filter out the impossible referent 
candidates of a name mention. In this paper, we 
adopt the candidate selection method of 
NLPR_KBP system (Han and Zhao, 2009), the 
main idea of which is first building a name-to-
entity dictionary using the redirect links, 
disambiguation pages, anchor texts of Wikipedia, 
then the candidate entities of a name mention are 
selected by finding its name?s corresponding entry 
in the dictionary. 
3 Model Estimation  
Section 2 shows that the entity mention model can 
decompose the entity linking task into the 
estimation of three distributions P(e), P(s|e) and 
P(c|e). In this section, we describe the details of the 
estimation of these three distributions. We first 
947
introduce the training data, then describe the 
estimation methods. 
3.1 Training Data 
In this paper, the training data of our model is a set 
of annotated name mentions M = {m1, m2, ?, mn}. 
Each annotated name mention is a triple m={s, e, 
c}, where s is the name, e is the referent entity and 
c is the context. For example, two annotated name 
mentions are as follows: 
? Jordan | Michael Jeffrey Jordan | ? wins his first NBA 
MVP in 1991. 
? NBA | National Basketball Association | ? is the pre-
eminent men's professional basketball league. 
In this paper, we focus on the task of linking 
entities with Wikipedia, even though the proposed 
method can be applied to other resources. We will 
only show how to get the training data from 
Wikipedia. In Wikipedia, a hyperlink between two 
articles is an annotated name mention (Milne & 
Witten, 2008): its anchor text is the name and its 
target article is the referent entity. For example, in 
following hyperlink (in Wiki syntax), the NBA is 
the name and the National Basketball Association 
is the referent entity. 
?He won his first [[National Basketball Association | 
NBA]] championship with the Bulls?  
Therefore, we can get the training data by 
collecting all annotated name mentions from the 
hyperlink data of Wikipedia. In total, we collected 
more than 23,000,000 annotated name mentions. 
3.2 Entity Popularity Model 
The distribution P(e) encodes the popularity 
knowledge as a distribution of entities, i.e., the 
P(e1) should be larger than P(e2) if e1 is more 
popular than e2. For example, on the Web the 
P(Michael Jeffrey Jordan) should be higher than 
the P(Michael I. Jordan). In this section, we 
estimate the distribution P(e) using a model called 
entity popularity model. 
Given a knowledge base KB which contains N 
entities, in its simplest form, we can assume that 
all entities have equal popularity, and the 
distribution P(e) can be estimated as: 
( ) 1P e N?  
However, this does not reflect well the real 
situation because some entities are obviously more 
popular than others. To get a more precise 
estimation, we observed that a more popular entity 
usually appears more times than a less popular 
entity in a large text corpus, i.e., more name 
mentions refer to this entity. For example, in 
Wikipedia the NBA player Michael Jeffrey Jordan 
appears more than 10 times than the Berkeley 
professor Michael I. Jordan. Based on the above 
observation, our entity popularity model uses the 
entity frequencies in the name mention data set M 
to estimate the distribution P(e) as follows: 
( ) 1( ) Count eP e M N
?? ?
 
where Count(e) is the count of the name mentions 
whose referent entity is e, and the |M| is the total 
name mention size. The estimation is further 
smoothed using the simple add-one smoothing 
method for the zero probability problem. For 
illustration, Table 1 shows three selected entities? 
popularity. 
Entity Popularity 
National Basketball Association 1.73*10-5 
Michael Jeffrey Jordan(NBA player) 8.21*10-6 
Michael I. Jordan(Berkeley Professor) 7.50*10-8 
Table 1. Three examples of entity popularity 
3.3 Entity Name Model 
The distribution P(s|e) encodes the name 
knowledge of entities, i.e., for a specific entity e, 
its more frequently used name should be assigned a 
higher P(s|e) value than the less frequently used 
name, and a zero P(s|e) value should be assigned 
to those never used names. For instance, we would 
expect the P(Michael Jordan|Michael Jeffrey 
Jordan) to be high, P(MJ|Michael Jeffrey Jordan) 
to be relative high and P(Michael I. 
Jordan|Michael Jeffrey Jordan) to be zero. 
Intuitively, the name model can be estimated by 
first collecting all (entity, name) pairs from the 
name mention data set, then using the maximum 
likelihood estimation: 
( , )( | ) ( , )
s
Count e sP s e Count e s??
 
where the Count(e,s) is the count of the name 
mentions whose referent entity is e and name is s. 
However, this method does not work well because 
it cannot correctly deal with an unseen entity or an 
unseen name. For example, because the name 
?MJ? doesn?t refer to the Michael Jeffrey Jordan in 
Wikipedia, the name model will not be able to 
identify ?MJ? as a name of him, even ?MJ? is a 
popular name of Michael Jeffrey Jordan on Web. 
948
To better estimate the distribution P(s|e), this 
paper proposes a much more generic model, called 
entity name model, which can capture the 
variations (including full name, aliases, acronyms 
and misspellings) of an entity's name using a 
statistical translation model. Given an entity?s 
name s, our model assumes that it is a translation 
of this entity?s full name f using the IBM model 1 
(Brown, et al, 1993). Let ?  be the vocabulary 
containing all words may be used in the name of 
entities, the entity name model assumes that a 
word in ? can be translated through the following 
four ways: 
1) It is retained (translated into itself); 
2) It is translated into its acronym; 
3) It is omitted(translated into the word NULL); 
4) It is translated into another word (misspelling 
or alias). 
In this way, all name variations of an entity are 
captured as the possible translations of its full 
name. To illustrate, Figure 3 shows how the full 
name ?Michael Jeffrey Jordan? can be transalted 
into its misspelling name ?Micheal Jordan?. 
 
Figure 3. The translation from Michael Jefferey 
Jordan to Micheal Jordan 
Based on the translation model, P(s|e) can be 
written as: 
01
( | )( 1)
fs
s
ll
i jl
ijf
P(s |e) t s fl
?
??
? ? ??
 
where ? is a normalization factor, f is the full name 
of entity e, lf is the length of f, ls is the length of the 
name s, si the i
th word of s, fj is the j
th word of f and 
t(si|fj) is the lexical translation probability which 
indicates the probability of a word fj in the full 
name will be written as si in the output name. 
Now the main problem is to estimate the lexical 
translation probability t(si|fj). In this paper, we first 
collect the (name, entity full name) pairs from all 
annotated name mentions, then get the lexical 
translation probability by feeding this data set into 
an IBM model 1 training system (we use the 
GIZA++ Toolkit3). 
Table 2 shows several resulting lexical 
translation probabilities through the above process. 
                                                        
3 http://fjoch.com/GIZA++.html 
We can see that the entity name model can capture 
the different name variations, such as the acronym 
(Michael?M), the misspelling (Michael?Micheal) 
and the omission (St. ? NULL). 
Full name word Name word Probability 
Michael Michael 0.77 
Michael M 0.008 
Michael Micheal 2.64*10
-4
 
Jordan Jordan 0.96 
Jordan J 6.13*10
-4
 
St. NULL 0.14 
Sir NULL 0.02 
Table 2. Several lexical translation probabilities 
3.4 Entity Context Model 
The distribution P(c|e) encodes the context 
knowledge of entities, i.e., it will assign a high 
P(c|e) value if the entity e frequently appears in the 
context c, and will assign a low P(c|e) value if the 
entity e rarely appears in the context c. For 
example, given the following two contexts: 
C1: __wins NBA MVP. 
C2: __is a researcher in machine learning. 
Then P(C1|Michael Jeffrey Jordan) should be high 
because the NBA player Michael Jeffrey Jordan 
often appears in C1 and the P(C2|Michael Jeffrey 
Jordan) should be extremely low because he rarely 
appears in C2. 
__ wi s NBA MVP.
__is a professor in UC
Berkeley.
Michael Jeffrey Jordan
(NBA Player)
NBA=0.03
MVP=0.008
Basketball=0.02
player=0.005
win=0.00008
professor=0
...
Michael I. Jordan
(Berkeley Professor)
professor=0.003
Berkeley=0.002
machine learning=0.1
researcher = 0.006
NBA = 0
MVP=0
...
 
Figure 4. Two entity context models 
To estimate the distribution P(c|e), we propose a 
method based on language modeling, called entity 
context model. In our model, the context of each 
name mention m is the word window surrounding 
m, and the window size is set to 50 according to 
the experiments in (Pedersen et al, 2005). 
Specifically, the context knowledge of an entity e 
is encoded in an unigram language model: 
{ ( )}e eM P t?  
where Pe(t) is the probability of the term t 
appearing in the context of e. In our model, the 
term may indicate a word, a named entity 
(extracted using the Stanford Named Entity 
Michael Jeffrey Jordan 
Micheal Jordan NULL 
Full Name 
Name 
949
Recognizer 4 ) or a Wikipedia concept (extracted 
using the method described in (Han and Zhao, 
2010)). Figure 4 shows two entity context models 
and the contexts generated using them. 
Now, given a context c containing n terms 
t1t2?tn, the entity context model estimates the 
probability P(c|e) as: 
1 2 1 2( | ) ( ... | ) ( ) ( ).... ( )n e e e e nP c e P t t t M P t P t P t? ? 
So the main problem is to estimate Pe(t), the 
probability of a term t appearing in the context of 
the entity e. 
Using the annotated name mention data set M, 
we can get the maximum likelihood estimation of 
Pe(t) as follows: 
_
( )( ) ( )
e
e ML
e
t
Count tP t Count t??
 
where Counte(t) is the frequency of occurrences of 
a term t in the contexts of the name mentions 
whose referent entity is e. 
Because an entity e?s name mentions are usually 
not enough to support a robust estimation of Pe(t) 
due to the sparse data problem (Chen and 
Goodman, 1999), we further smooth Pe(t) using the 
Jelinek-Mercer smoothing method (Jelinek and 
Mercer, 1980): 
_( ) ( ) (1 ) ( )e e ML gP t P t P t? ?? ? ?
 
where Pg(t) is a general language model which is 
estimated using the whole Wikipedia data, and the 
optimal value of ? is set to 0.2 through a learning 
process shown in Section 4. 
3.5 The NIL Entity Problem 
By estimating P(e), P(s|e) and P(c|e), our method 
can effectively link a name mention to its referent 
entity contained in a knowledge base. 
Unfortunately, there is still the NIL entity problem 
(McNamee and Dang, 2009), i.e., the referent 
entity may not be contained in the given 
knowledge base. In this situation, the name 
mention should be linked to the NIL entity. 
Traditional methods usually resolve this problem 
with an additional classification step (Zheng et al 
2010): a classifier is trained to identify whether a 
name mention should be linked to the NIL entity. 
Rather than employing an additional step, our 
entity mention model seamlessly takes into account 
the NIL entity problem. The start assumption of 
                                                        
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
our solution is that ?If a name mention refers to a 
specific entity, then the probability of this name 
mention is generated by the specific entity?s model 
should be significantly higher than the probability 
it is generated by a general language model?. 
Based on the above assumption, we first add a 
pseudo entity, the NIL entity, into the knowledge 
base and assume that the NIL entity generates a 
name mention according to the general language 
model Pg, without using any entity knowledge; 
then we treat the NIL entity in the same way as 
other entities: if the probability of a name mention 
is generated by the NIL entity is higher than all 
other entities in Knowledge base, we link the name 
mention to the NIL entity. Based on the above 
discussion, we compute the three probabilities of 
the NIL entity: P(e), P(s|e) and P(c|e) as follows: 
1P(NIL) M N? ?
 
( )gt sP(s | NIL) P t???
 
( )gt cP(c | NIL) P t???
 
4 Experiments 
In this section, we assess the performance of our 
method and compare it with the traditional 
methods. In following, we first explain the 
experimental settings in Section 4.1, 4.2 and 4.3, 
then evaluate and discuss the results in Section 4.4. 
4.1 Knowledge Base 
In our experiments, we use the Jan. 30, 2010 
English version of Wikipedia as the knowledge 
base, which contains over 3 million distinct entities. 
4.2 Data Sets 
To evaluate the entity linking performance, we 
adopted two data sets: the first is WikiAmbi, which 
is used to evaluate the performance on Wikipedia 
articles; the second is TAC_KBP, which is used to 
evaluate the performance on general newswire 
documents. In following, we describe these two 
data sets in detail. 
WikiAmbi: The WikiAmbi data set contains 1000 
annotated name mentions which are randomly 
selected from Wikipedia hyperlinks data set (as 
shown in Section 3.1, the hyperlinks between 
Wikipedia articles are manually annotated name 
mentions). In WikiAmbi, there were 207 distinct 
950
names and each name contains at least two 
possible referent entities (on average 6.7 candidate 
referent entities for each name) 5 . In our 
experiments, the name mentions contained in the 
WikiAmbi are removed from the training data. 
TAC_KBP: The TAC_KBP is the standard data 
set used in the Entity Linking task of the TAC 
2009 (McNamee and Dang, 2009). The TAC_KBP 
contains 3904 name mentions which are selected 
from English newswire articles. For each name 
mention, its referent entity in Wikipedia is 
manually annotated. Overall, 57% (2229 of 3904) 
name mentions?s referent entities are missing in 
Wikipedia, so TAC_KBP is also suitable to 
evaluate the NIL entity detection performance. 
The above two data sets can provide a standard 
testbed for the entity linking task. However, there 
were still some limitations of these data sets: First, 
these data sets only annotate the salient name 
mentions in a document, meanwhile many NLP 
applications need all name mentions are linked. 
Second, these data sets only contain well-formed 
documents, but in many real-world applications the 
entity linking often be applied to noisy documents 
such as product reviews and microblog messages. 
In future, we want to develop a data set which can 
reflect these real-world settings. 
4.3 Evaluation Criteria 
We adopted the standard performance metrics used 
in the Entity Linking task of the TAC 2009 
(McNamee and Dang, 2009). These metrics are: 
? Micro-Averaged Accuracy (Micro-
Accuracy): measures entity linking accuracy 
averaged over all the name mentions; 
? Macro-Averaged Accuracy (Macro-
Accuracy): measures entity linking accuracy 
averaged over all the target entities. 
As in TAC 2009, we used Micro-Accuracy as the 
primary performance metric. 
4.4 Experimental Results 
We compared our method with three baselines: (1) 
The first is the traditional Bag of Words based 
method (Cucerzan, 2007): a name mention?s 
referent entity is the entity which has the highest 
cosine similarity with its context ? we denoted it as 
BoW; (2) The second is the method described in 
                                                        
5 This is because we want to create a highly ambiguous test 
data set 
(Medelyan et al, 2008), where a name mention?s 
referent entity is the entity which has the largest 
average semantic relatedness with the name 
mention?s unambiguous context entities ? we 
denoted it as TopicIndex. (3) The third one is the 
same as the method described in (Milne & Witten, 
2008), which uses learning techniques to balance 
the semantic relatedness, commoness and context 
quality ? we denoted it as Learning2Link. 
4.4.1 Overall Performance 
We conduct experiments on both WikiAmbi and 
TAC_KBP datasets with several methods: the 
baseline BoW; the baseline TopicIndex; the 
baseline Learning2Link; the proposed method 
using only popularity  knowledge (Popu), i.e., the 
P(m,e)=P(e); the proposed method with one 
component of the model is ablated(this is used to 
evaluate the independent contributions of the three 
components), correspondingly Popu+Name(i.e., 
the P(m,e)=P(e)P(s|e)), Name+Context(i.e., the 
P(m,e)=P(c|e)P(s|e)) and Popu+Context (i.e., the 
P(m,e)=P(e)P(c|e)); and the full entity mention 
model (Full Model). For all methods, the 
parameters were configured through 10-fold cross 
validation. The overall performance results are 
shown in Table 3 and 4. 
 Micro-Accuracy Macro-Accuracy 
BoW 0.60 0.61 
TopicIndex 0.66 0.49 
Learning2Link 0.70 0.54 
Popu 0.39 0.24 
Popu + Name 0.50 0.31 
Name+Context 0.70 0.68 
Popu+Context 0.72 0.73 
Full Model 0.80 0.77 
Table 3. The overall results on WikiAmbi dataset 
 Micro-Accuracy Macro-Accuracy 
BoW 0.72 0.75 
TopicIndex 0.80 0.76 
Learning2Link 0.83 0.79 
Popu 0.60 0.53 
Popu + Name 0.63 0.59 
Name+Context 0.81 0.78 
Popu+Context 0.84 0.83 
Full Model 0.86 0.88 
Table 4. The overall results on TAC-KBP dataset 
From the results in Table 3 and 4, we can make the 
following observations: 
1) Compared with the traditional methods, 
our entity mention model can achieve a significant 
951
performance improvement: In WikiAmbi and 
TAC_KBP datasets, compared with the BoW 
baseline, our method respectively gets 20% and 
14% micro-accuracy improvement; compared with 
the TopicIndex baseline, our method respectively 
gets 14% and 6% micro-accuracy improvement; 
compared with the Learning2Link baseline, our 
method respectively gets 10% and 3% micro-
accuracy improvement. 
2) By incorporating more entity knowledge, 
our method can significantly improve the entity 
linking performance: When only using the 
popularity knowledge, our method can only 
achieve 49.5% micro-accuracy. By adding the 
name knowledge, our method can achieve 56.5% 
micro-accuracy, a 7% improvement over the Popu. 
By further adding the context knowledge, our 
method can achieve 83% micro-accuracy, a 33.5% 
improvement over Popu and a 26.5% improvement 
over Popu+Name. 
3) All three types of entity knowledge 
contribute to the final performance improvement, 
and the context knowledge contributes the most: 
By respectively ablating the popularity knowledge, 
the name knowledge and the context knowledge, 
the performance of our model correspondingly 
reduces 7.5%, 5% and 26.5%. 
NIL Entity Detection Performance. To 
compare the performances of resolving the NIL 
entity problem, Table 5 shows the micro-
accuracies of different systems on the TAC_KBP 
data set (where All is the whole data set, NIL only 
contains the name mentions whose referent entity 
is NIL, InKB only contains the name mentions 
whose referent entity is contained in the 
knowledge base). From Table 5 we can see that our 
method can effectively detect the NIL entity 
meanwhile retaining the high InKB accuracy. 
 All NIL InKB 
BoW 0.72 0.77 0.65 
TopicIndex 0.80 0.91 0.65 
Learning2Link 0.83 0.90 0.73 
Full Model 0.86 0.90 0.79 
Table 5.  The NIL entity detection performance on 
the TAC_KBP data set 
4.4.2 Optimizing Parameters 
Our model needs to tune one parameter: the 
Jelinek-Mercer smoothing parameter ? used in the 
entity context model. Intuitively, a smaller ? 
means that the general language model plays a 
more important role. Figure 5 plots the tradeoff.  In 
both WikiAmbi and TAC_KBP data sets,  Figure 5 
shows that a ? value 0.2 will result in the best 
performance. 
 
Figure 5. The micro-accuracy vs. ? 
4.4.3 Detailed Analysis 
To better understand the reasons why and how the 
proposed method works well, in this Section we 
analyze our method in detail.  
The Effect of Incorporating Heterogenous 
Entity Knowledge. The first advantage of our 
method is the entity mention model can 
incorporate heterogeneous entity knowledge. The 
Table 3 and 4 have shown that, by incorporating 
heterogenous entity knowledge (including the 
name knowledge, the popularity knowledge and 
the context knowledge), the entity linking 
performance can obtain a significant improvement. 
 
Figure 6. The performance vs. training mention 
size on WikiAmbi data set 
The Effect of Better Entity Knowledge 
Extraction. The second advantage of our method 
is that, by representing the entity knowledge as 
probabilistic distributions, our model has a 
statistical foundation and can better extract the 
entity knowledge using more training data through 
the entity popularity model, the entity name model 
and the entity context model. For instance, we can 
train a better entity context model P(c|e) using 
more name mentions. To find whether a better 
952
entity knowledge extraction will result in a better 
performance, Figure 6 plots the micro-accuray 
along with the size of the training data on name 
mentions for P(c|e) of each entity e.  From Figure 
6, we can see that when more training data is used, 
the performance increases. 
4.4.4 Comparision with State-of-the-Art 
Performance 
We also compared our method with the state-of-
the-art entity linking systems in the TAC 2009 
KBP track (McNamee and Dang, 2009). Figure 7 
plots the comparison with the top five 
performances in TAC 2009 KBP track. From 
Figure 7, we can see that our method can 
outperform the state-of-the-art approaches: 
compared with the best ranking system, our 
method can achieve a 4% performance 
improvement. 
 
Figure 7.  A comparison with top 5 TAC 2009 
KBP systems 
5 Related Work 
In this section, we briefly review the related work. 
To the date, most entity linking systems employed 
the context similarity based methods. The essential 
idea was to extract the discriminative features of an 
entity from its description, then link a name 
mention to the entity which has the largest context 
similarity with it. Cucerzan (2007) proposed a Bag 
of Words based method, which represents each 
target entity as a vector of terms, then the 
similarity between a name mention and an entity 
was computed using the cosine similarity measure. 
Mihalcea & Csomai (2007), Bunescu & Pasca 
(2006), Fader et al (2009) extended the BoW 
model by incorporating more entity knowledge 
such as popularity knowledge, entity category 
knowledge, etc.  Zheng et al (2010), Dredze et al 
(2010), Zhang et al (2010) and Zhou et al (2010) 
employed the learning to rank techniques which 
can further take the relations between candidate 
entities into account. Because the context 
similarity based methods can only represent the 
entity knowledge as features, the main drawback of 
it was the difficulty to incorporate heterogenous 
entity knowledge. 
Recently there were also some entity linking 
methods based on inter-dependency. These 
methods assumed that the entities in the same 
document are related to each other, thus the 
referent entity of a name mention is the entity 
which is most related to its contextual entities.  
Medelyan et al (2008) found the referent entity of 
a name mention by computing the weighted 
average of semantic relatedness between the 
candidate entity and its unambiguous contextual 
entities. Milne and Witten (2008) extended 
Medelyan et al (2008) by adopting learning-based 
techniques to balance the semantic relatedness, 
commoness and context quality. Kulkarni et al 
(2009) proposed a method which collectively 
resolves the entity linking tasks in a document as 
an optimization problem. The drawback of the 
inter-dependency based methods is that they are 
usually specially designed to the leverage of 
semantic relations, doesn?t take the other types of 
entity knowledge into consideration.  
6 Conclusions and Future Work 
This paper proposes a generative probabilistic 
model, the entity-mention model, for the entity 
linking task. The main advantage of our model is it 
can incorporate multiple types of heterogenous 
entity knowledge. Furthermore, our model has a 
statistical foundation, making the entity knowledge 
extraction approach different from most previous 
ad hoc approaches. Experimental results show that 
our method can achieve competitive performance. 
In our method, we did not take into account the 
dependence between entities in the same document. 
This aspect could be complementary to those we 
considered in this paper. For our future work, we 
can integrate such dependencies in our model. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60773027, 60736044, 90920010, 61070106 and 
61003117, and the National High Technology 
Development 863 Program of China under Grants 
no. 2008AA01Z145. Moreover, we sincerely thank 
the reviewers for their valuable comments. 
953
References  
Adafre, S. F. & de Rijke, M. 2005. Discovering missing 
links in Wikipedia. In: Proceedings of the 3rd 
international workshop on Link discovery. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In: 
Proceedings of EACL, vol. 6. 
Brown,  P., Pietra, S. D.,  Pietra, V. D., and Mercer, R.  
1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2), 263-31. 
Chen, S. F. & Goodman, J. 1999. An empirical study of 
smoothing techniques for language modeling.  In 
Computer Speech and Language, London; Orlando: 
Academic Press, c1986-, pp. 359-394. 
Cucerzan, S. 2007. Large-scale named entity 
disambiguation based on Wikipedia data. In:  
Proceedings of EMNLP-CoNLL, pp. 708-716. 
Dredze, M., McNamee, P., Rao, D., Gerber, A. & Finin, 
T. 2010. Entity Disambiguation for Knowledge Base 
Population. In: Proceedings of the 23rd International 
Conference on Computational Linguistics. 
Fader, A., Soderland, S., Etzioni, O. & Center, T. 2009. 
Scaling Wikipedia-based named entity 
disambiguation to arbitrary web text. In: Proceedings 
of  Wiki-AI Workshop at IJCAI, vol. 9. 
Han, X. & Zhao, J. 2009. NLPR_KBP in TAC 2009 
KBP Track: A Two-Stage Method to Entity Linking. 
In: Proceeding of Text Analysis Conference. 
Han, X. & Zhao, J. 2010. Structural semantic 
relatedness: a knowledge-based method to named 
entity disambiguation. In: Proceedings of the 48th 
Annual Meeting of the Association for 
Computational Linguistics. 
Jelinek, Frederick and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of the Workshop 
on Pattern Recognition in Practice. 
Kulkarni, S., Singh, A., Ramakrishnan, G. & 
Chakrabarti, S. 2009. Collective annotation of 
Wikipedia entities in web text. In: Proceedings of the 
15th ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 457-466. 
Li, X., Morie, P. & Roth, D. 2004. Identification and 
tracing of ambiguous names: Discriminative and 
generative approaches. In: Proceedings of the 
National Conference on Artificial Intelligence, pp. 
419-424. 
McNamee, P. & Dang, H. T. 2009.  Overview of the 
TAC 2009 Knowledge Base Population Track. In: 
Proceeding of Text Analysis Conference. 
Milne, D. & Witten, I. H. 2008. Learning to link with 
Wikipedia. In: Proceedings of the 17th ACM 
conference on Conference on information and 
knowledge management. 
Milne, D., et al  2006. Mining Domain-Specific 
Thesauri from Wikipedia: A case study. In Proc. of 
IEEE/WIC/ACM WI. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. In: Proceedings of the 
AAAI WikiAI workshop. 
Mihalcea, R. & Csomai, A. 2007. Wikify!: linking 
documents to encyclopedic knowledge. In: 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp. 233-242. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. Name 
discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Zhang, W., Su, J., Tan, Chew Lim  & Wang, W. T. 
2010. Entity Linking Leveraging Automatically 
Generated Annotation. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010). 
Zheng, Z., Li, F., Huang, M. & Zhu, X. 2010. Learning 
to Link Entities with Knowledge Base. In: The 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Zhou, Y., Nie, L., Rouhani-Kalleh, O., Vasile, F. & 
Gaffney, S. 2010. Resolving Surface Forms to 
Wikipedia Topics. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010),  pp. 1335-1343. 
954
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 61?67,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
A Feature-Enriched Tree Kernel for Relation Extraction 
 
          Le Sun      and      Xianpei Han 
State Key Laboratory of Computer Science 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{sunle, xianpei}@nfs.iscas.ac.cn 
  
 
Abstract 
Tree kernel is an effective technique for rela-
tion extraction. However, the traditional syn-
tactic tree representation is often too coarse or 
ambiguous to accurately capture the semantic 
relation information between two entities. In 
this paper, we propose a new tree kernel, 
called feature-enriched tree kernel (FTK), 
which can enhance the traditional tree kernel 
by: 1) refining the syntactic tree representation 
by annotating each tree node with a set of dis-
criminant features; and 2) proposing a new 
tree kernel which can better measure the syn-
tactic tree similarity by taking all features into 
consideration. Experimental results show that 
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution 
tree kernel. 
1 Introduction 
Relation Extraction (RE) aims to identify a set of 
predefined relations between pairs of entities in 
text. In recent years, relation extraction has re-
ceived considerable research attention. An effec-
tive technique is the tree kernel (Zelenko et al, 
2003; Zhou et al, 2007; Zhang et al, 2006; Qian 
et al, 2008), which can exploit syntactic parse tree 
information for relation extraction. Given a pair of 
entities in a sentence, the tree kernel-based RE 
method first represents the relation information 
between them using a proper sub-tree (e.g., SPT ? 
the sub-tree enclosed by the shortest path linking 
the two involved entities). For example, the three 
syntactic tree representations in Figure 1. Then the 
similarity between two trees are computed using a 
tree kernel, e.g., the convolution tree kernel pro-
posed by Collins and Duffy (2001). Finally, new 
relation instances are extracted using kernel based 
classifiers, e.g., the SVM classifier. 
Unfortunately, one main shortcoming of the 
traditional tree kernel is that the syntactic tree rep-
resentation usually cannot accurately capture the 
 
Figure 1. The ambiguity of possessive structure 
relation information between two entities. This is 
mainly due to the following two reasons: 
1) The syntactic tree focuses on representing 
syntactic relation/structure, which is often too 
coarse or ambiguous to capture the semantic re-
lation information. In a syntactic tree, each node 
indicates a clause/phrase/word and is only labeled 
with a Treebank tag (Marcus et al, 1993). The 
Treebank tag, unfortunately, is usually too coarse 
or too general to capture semantic information. 
For example, all the three trees in Figure 1 share 
the same possessive syntactic structure, but ex-
press quite different semantic relations: where 
?Mary?s brothers? expresses PER-SOC Family 
relation, ?Mary?s toys? expresses Possession rela-
tion, and ?New York?s airports? expresses PHYS-
Located relation. 
2) Some critical information may lost during 
sub-tree representation extraction. For example, 
in Figure 2, when extracting SPT representation, 
all nodes outside the shortest-path will be pruned, 
such as the nodes [NN plants] and [POS ?s] in tree 
T1. In this pruning process, the critical infor-
mation ?word town is the possessor of the posses-
sive phrase the town?s plants? will be lost, which 
in turn will lead to the misclassification of the 
DISC relation between one and town. 
This paper proposes a new tree kernel, referred 
as feature-enriched tree kernel (FTK), which can 
effectively resolve the above problems by enhanc-
ing the traditional tree kernel in following ways: 
1) We refine the syntactic tree representa-
tion by annotating each tree node with a set of dis-
criminant features. These features are utilized to 
NP
NP NN
NN POS
Mary 's
brothers
(a) (b) (c)
NP
NP NN
NN POS
Mary 's
toys
NP
NP NN
NN POS
NY 's
airports
61
better capture the semantic relation information 
between two entities. For example, in order to dif-
ferentiate the syntactic tree representations in Fig-
ure 1, FTK will annotate them with several fea-
tures indicating ?brother is a male sibling?, ?toy 
is an artifact?, ?New York is a city?, ?airport is 
facility?, etc. 
2) Based on the refined syntactic tree repre-
sentation, we propose a new tree kernel ? feature-
enriched tree kernel, which can better measure the 
similarity between two trees by also taking all fea-
tures into consideration. 
 
Figure 2. SPT representation extraction 
We have experimented our method on the ACE 
2004 RDC corpus. Experimental results show that 
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution tree 
kernel based method. 
This paper is organized as follows. Section 2 
describes the feature-enriched tree kernel. Section 
3 presents the features we used. Section 4 dis-
cusses the experiments. Section 5 briefly reviews 
the related work. Finally Section 6 concludes this 
paper. 
2 The Feature-Enriched Tree Kernel 
In this section, we describe the proposed feature-
enriched tree kernel (FTK) for relation extraction. 
2.1 Refining Syntactic Tree Representation 
As described in above, syntactic tree is often too 
coarse or too ambiguous to represent the semantic 
relation information between two entities. To re-
solve this problem, we refine the syntactic tree 
representation by annotating each tree node with 
a set of discriminant features. 
 
Figure 3. Syntactic tree enriched with features 
Specifically, for each node  in a syntactic tree 
, we represent it as a tuple: 
 
where  is its phrase label (i.e., its Treebank tag), 
and  is a feature vector which indicates the 
characteristics of node , which is represented as: 
 
where fi is a feature and is associated with a weight 
. The feature we used includes charac-
teristics of relation instance, phrase properties and 
context information (See Section 3 for details). 
For demonstration, Figure 3 shows the feature-
enriched version of tree T2 and tree T4 in Figure 
2. We can see that, although T2 and T4 share the 
same syntactic structure, the annotated features 
can still differentiate them. For example, the NP5 
node in tree T2 and the NP5 node in tree T4 are 
differentiated using their features Possessive-
Phrase and PPPhrase, which indicate that NP5 in 
T2 is a possessive phrase, meanwhile NP5 in T4 is 
a preposition phrase. 
2.2 Feature-Enriched Tree Kernel 
This section describes how to take into account 
the annotated features for a better tree similarity. 
In Collins and Duffy?s convolution tree kernel 
(CTK), the similarity between two trees T1 and T2 
is the number of their common sub-trees: 
 
Using this formula, CTK only considers whether 
two enumerated sub-trees have the identical syn-
tactic structure (the indicator  is 1 if the 
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
POS
's
NN
plants
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
Prune
NP
NP PP
CD
one
IN
E1
of
NP
NP
DT NN
the teams
E2
PP
IN
in USA
NP
NN
Prune
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
teams
E2
DT
the
(T1)
(T2)
(T3) (T4)
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
(T2)
PossessivePhrase, RootPath:NP-PP,
Contain_Arg2_GPE, ...
Possessor, Contain_Arg2_GPE,
RootPath:NP-PP-NP,
EndWithPOS, ...
EntType:GPE, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:town, WN:district, WN:region,
WN:location, Match_Arg2_GPE ...
PPPhrase, RootPath:NP-PP,
Contain_Arg2_ORG, ...
PP_Head, RootPath:NP-PP-NP,
Contain_Arg2_ORG,...
EntType:ORG, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:team, WN:social_unit,
WN:group, WN:organization,
Match_Arg2_ORG ...
Feature Vector
1
2 3
4
5
6 7 8
9 10 11
12
14
13
15
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
team
E2
DT
the
(T4) 1
2 3
4
5
6 7 8
9 10
12
14
11
13
15
62
two sub-trees  and  have the identical syntac-
tic structure and 0 otherwise). Such an assumption 
makes CTK can only capture the syntactic struc-
ture similarity between two trees, while ignoring 
other useful information. 
To resolve the above problem, the feature-en-
riched tree kernel (FTK) compute the similarity 
between two trees as the sum of the similarities 
between their common sub-trees: 
 
where  is the similarity between enumer-
ated sub-trees  and , which is computed as: 
 
where  is the same indicator function as in 
CTK; is a pair of aligned nodes between 
 and , where  and  are correspondingly in 
the same position of tree  and ;  is the 
set of all aligned node pairs;  is the 
feature vector similarity between node  and , 
computed as the dot product between their feature 
vectors  and . 
Notice that, if all nodes are not annotated with 
features,  will be equal to . In this 
perspective, we can view  as a similarity 
adjusted version of , i.e.,  only 
considers whether two nodes are equal, in contrast 
 further considers the feature similarity 
 between two nodes. 
The Computation of FTK. As the same as 
CTK, FTK can be efficiently computed as: 
 
where  is the set of nodes in tree , and 
 evaluates the sum of the similarities of 
common sub-trees rooted at node  and node , 
which is recursively computed as follows: 
1) If the production rules of  and  are differ-
ent,  = 0; 
2) If both  and  is pre-terminal nodes, 
; 
Otherwise go to step 3; 
3) Calculate  recursively as: 
?(n1;n2) = ?? (1 + sim(n1; n2))
?
#ch(n1)X
k=1
(1 + ?(ch(n1; k); ch(n2; k))
 
3 Features for Relation Extraction 
This section presents the features we used to en-
rich the syntactic tree representation. 
3.1 Instance Feature 
Relation instances of the same type often share 
some common characteristics. In this paper, we 
add the following instance features to the root 
node of a sub-tree representation: 
1) Syntactico-Semantic structure. A fea-
ture indicates whether a relation instance has the 
following four syntactico-semantic structures in 
(Chan & Roth, 2011) ? Premodifiers, Possessive, 
Preposition, Formulaic and Verbal. 
2) Entity-related information of argu-
ments. Features about the entity information of 
arguments, including: a) #TP1-#TP2: the concat 
of the major entity types of arguments; b) #ST1-
#ST2: the concat of the sub entity types of argu-
ments; c) #MT1-#MT2: the concat of the mention 
types of arguments. 
3) Base phrase chunking features. Fea-
tures about the phrase path between two argu-
ments and the phrases? head before and after the 
arguments, which are the same as the phrase 
chunking features in (Zhou, et al, 2005). 
3.2 Phrase Feature 
As discussed in above, the Treebank tag is too 
coarse to capture the property of a phrase node. 
Therefore, we enrich each phrase node with fea-
tures about its lexical pattern, its content infor-
mation, and its lexical semantics: 
1) Lexical Pattern. We capture the lexical 
pattern of a phrase node using the following fea-
tures: a) LP_Poss: A feature indicates the node is 
a possessive phrase; b) LP_PP: A feature indi-
cates the node is a preposition phrase; c) LP_CC: 
A feature indicates the node is a conjunction 
phrase; d) LP_EndWithPUNC: A feature indicates 
the node ends with a punctuation; e) LP_EndWith-
POSS: A feature indicates the node ends with a 
possessive word. 
2) Content Information. We capture the 
property of a node?s content using the following 
features: a) MB_#Num: The number of mentions 
contained in the phrase; b) MB_C_#Type: A fea-
ture indicates that the phrase contains a mention 
with major entity type #Type; c) MW_#Num: The 
number of words within the phrase. 
3) Lexical Semantics. If the node is a pre-
terminal node, we capture its lexical semantic by 
adding features indicating its WordNet sense in-
formation. Specifically, the first WordNet sense 
of the terminal word, and all this sense?s hyponym 
senses will be added as features. For example, 
WordNet senses {New York#1, city#1, district#1, 
63
region#1, ?} will be added as features to the [NN 
New York]  node in Figure 1. 
3.3 Context Information Feature 
The context information of a phrase node is criti-
cal for identifying the role and the importance of 
a sub-tree in the whole relation instance. This pa-
per captures the following context information: 
1) Contextual path from sub-tree root to 
the phrase node. As shown in Zhou et al (2007), 
the context path from root to the phrase node is an 
effective context information feature. In this paper, 
we use the same settings in (Zhou et al, 2007), i.e., 
each phrase node is enriched with its context paths 
of length 1, 2, 3. 
2) Relative position with arguments. We 
observed that a phrase?s relative position with the 
relation?s arguments is useful for identifying the 
role of the phrase node in the whole relation in-
stance. To capture the relative position infor-
mation, we define five possible relative positions 
between a phrase node and an argument, corre-
sponding match, cover, within, overlap and other. 
Using these five relative positions, we capture the 
context information using the following features: 
a) #RP_Arg1Head_#Arg1Type: a feature in-
dicates the relative position of a phrase node with 
argument 1?s head phrase, where #RP is the rela-
tive position (one of match, cover, within, overlap, 
other), and #Arg1Type is the major entity type of 
argument 1. One example feature may be 
Match_Arg1Head_LOC. 
b) #RP_Arg2Head_#Arg2Type: The relative 
position with argument 2?s head phrase; 
c) #RP_Arg1Extend_#Arg1Type: The rela-
tive position with argument 1?s extended phrase; 
d) #PR_Arg2Extend_#Arg2Type: The rela-
tive position with argument 2?s extended phrase. 
Feature weighting. Currently, we set al fea-
tures with an uniform weight , which is 
used to control the relative importance of the fea-
ture in the final tree similarity: the larger the fea-
ture weight, the more important the feature in the 
final tree similarity. 
4 Experiments 
4.1 Experimental Setting 
To assess the feature-enriched tree kernel, we 
evaluate our method on the ACE RDC 2004 cor-
pus using the same experimental settings as (Qian 
et al, 2008). That is, we parse all sentences using 
the Charniak?s parser (Charniak, 2001), relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence. 
In our experiments, we implement the feature-en-
riched tree kernel by extending the SVMlight (Joa-
chims, 1998) with the proposed tree kernel func-
tion (Moschitti, 2004). We apply the one vs. oth-
ers strategy for multiple classification using SVM. 
For SVM training, the parameter C is set to 2.4 for 
all experiments, and the tree kernel parameter ? is 
tuned to 0.2 for FTK and 0.4 (the optimal param-
eter setting used in Qian et al(2008)) for CTK. 
4.2 Experimental Results 
4.2.1 Overall performance 
We compare our method with the standard convo-
lution tree kernel (CTK) on the state-of-the-art 
context sensitive shortest path-enclosed tree rep-
resentation (CSPT, Zhou et al, 2007). We exper-
iment our method with four different feature set-
tings, correspondingly: 1) FTK with only instance 
features ? FTK(instance); 2) FTK with only 
phrase features ? FTK(phrase); 3) FTK with only 
context information features ? FTK(context); and 
4) FTK with all features ? FTK. The overall per-
formance of CTK and FTK is shown in Table 1, 
the F-measure improvements over CTK are also 
shown inside the parentheses. The detailed perfor-
mance of FTK on the 7 major relation types of 
ACE 2004 is shown in Table 2. 
 P(%) R(%) F 
CTK 77.1 61.3 68.3 (-------) 
FTK(instance) 78.5 64.6 70.9 (+2.6%) 
FTK(phrase) 78.3 64.2 70.5 (+2.2%) 
FTK(context) 80.1 67.5 73.2 (+4.9%) 
FTK 81.2 67.4 73.7 (+5.4%) 
Table 1. Overall Performance 
Relation Type P(%) R(%) F Impr 
EMP-ORG 84.7 82.4 83.5 5.8% 
PER-SOC 79.9 70.7 75.0 1.0% 
PHYS 73.3 64.4 68.6 7.0% 
ART 83.6 57.5 68.2 1.7% 
GPE-AFF 74.7 56.6 64.4 4.3% 
DISC 81.6 48.0 60.5 6.6% 
OTHER-AFF 74.2 36.8 49.2 1.0% 
Table 2. FTK on the 7 major relation types and 
their F-measure improvement over CTK 
From Table 1 and 2, we can see that: 
1) By refining the syntactic tree with discri-
minant features and incorporating these features 
into the final tree similarity, FTK can significantly 
improve the relation extraction performance: 
compared with the convolution tree kernel base-
line CTK, our method can achieve a 5.4% F-meas-
ure improvement. 
64
2) All types of features can improve the per-
formance of relation extraction: FTK can corre-
spondingly get 2.6%, 2.2% and 4.9% F-measure 
improvements using instance features, phrase fea-
tures and context information features. 
3) Within the three types of features, context 
information feature can achieve the highest F-
measure improvement. We believe this may be-
cause: ?  The context information is useful in 
providing clues for identifying the role and the im-
portance of a sub-tree; and ? The context-free as-
sumption of CTK is too strong, some critical in-
formation will lost in the CTK computation. 
4) The performance improvement of FTK 
varies significantly on different relation types: in 
Table 2, most performance improvement gains 
from the EMP-ORG, PHYS, GPE-AFF and DISC 
relation types. We believe this may because the 
discriminant features will better complement the 
syntactic tree for capturing EMP-ORG, PHYS, 
GPE-AFF and DISC relation. On contrast the fea-
tures may be redundant to the syntactic infor-
mation for other relation types. 
System P(%) R(%) F 
Qian et al, (2008): composite kernel 83.0 72.0 77.1 
Zhou et al, (2007): composite kernel 82.2 70.2 75.8 
Ours: FTK with CSPT 81.2 67.4 73.7 
Zhou et al, (2007): context sensitive 
CTK with CSPT 
81.1 66.7 73.2 
Ours: FTK with SPT 81.1 66.2 72.9 
Jiang & Zhai (2007): MaxEnt classi-
fier with features 
74.6 71.3 72.9 
Zhang et al, (2006): composite kernel  76.1 68.4 72.1 
Zhao & Grishman, (2005): Composite 
kernel 
69.2 70.5 70.4 
Zhang et al, (2006): CTK with SPT 74.1 62.4 67.7 
Table 3. Comparison of different systems on the 
ACE RDC 2004 corpus 
4.2.2 Comparison with other systems 
Finally, Table 3 compares the performance of our 
method with several other systems. From Table 3, 
we can see that FTK can achieve competitive per-
formance: ? It achieves a 0.8% F-measure im-
provement over the feature-based system of Jiang 
& Zhai (2007); ? It achieves a 0.5% F-measure 
improvement over a state-of-the-art tree kernel: 
context sensitive CTK with CSPT of Zhou et al, 
(2007); ? The F-measure of our system is slightly 
lower than the current best performance on ACE 
2004 (Qian et al, 2008) ? 73.7 vs. 77.1, we believe 
this is because the system of (Qian et al, 2008) 
adopts two extra techniques: composing tree ker-
nel with a state-of-the-art feature-based kernel and 
using a more proper sub-tree representation. We 
believe these two techniques can also be used to 
further improve the performance of our system. 
5 Related Work 
This section briefly reviews the related work. A 
classical technique for relation extraction is to 
model the task as a feature-based classification 
problem (Kambhatla, 2004; Zhou et al, 2005; 
Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & 
Roth, 2011), and feature engineering is obviously 
the key for performance improvement. As an al-
ternative, tree kernel-based method implicitly de-
fines features by directly measuring the similarity 
between two structures (Bunescu and Mooney, 
2005; Bunescu and Mooney, 2006; Zelenko et al 
2003; Culotta and Sorensen, 2004; Zhang et al, 
2006). Composite kernels were also be used (Zhao 
and Grishman, 2005; Zhang et al, 2006). 
The main drawback of the current tree kernel is 
that the syntactic tree representation often cannot 
accurately capture the relation information. To re-
solve this problem, Zhou et al (2007) took the an-
cestral information of sub-trees into consideration; 
Reichartz and Korte (2010) incorporated depend-
ency type information into a tree kernel; Plank and 
Moschitti (2013) and Liu et al (2013) embedded 
semantic information into tree kernel. Bloehdorn 
and Moschitti (2007a, 2007b) proposed Syntactic 
Semantic Tree Kernels (SSTK), which can cap-
ture the semantic similarity between leaf nodes. 
Moschitti (2009) proposed a tree kernel which 
specify a kernel function over any pair of nodes 
between two trees, and it was further extended and 
applied in other tasks in (Croce et al, 2011; Croce 
et al, 2012; Mehdad et al, 2010). 
6 Conclusions and Future Work 
This paper proposes a feature-enriched tree kernel, 
which can: 1) refine the syntactic tree representa-
tion; and 2) better measure the similarity between 
two trees. For future work, we want to develop a 
feature weighting algorithm which can accurately 
measure the relevance of a feature to a relation in-
stance for better RE performance. 
Acknowledgments 
This work is supported by the National Natural 
Science Foundation of China under Grants no. 
61100152 and 61272324, and the Open Project of 
Beijing Key Laboratory of Internet Culture and 
Digital Dissemination Research under Grants no. 
ICDD201204.  
65
References 
Agichtein, E. and Gravano, L. 2000. Snowball: Ex-
tracting relations from large plain-text collections. 
In: Proceedings of the 5th ACM Conference on Dig-
ital Libraries, pp. 85?94. 
Plank, B. and Moschitti, A. 2013. Embedding Semantic 
Similarity in Tree Kernels for Domain Adaptation of 
Relation Extraction?. In: Proceedings of ACL 2013. 
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, 
M. and Etzioni, O. 2007. Open information extrac-
tion from the Web. In: Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence, 
pp. 2670?2676. 
Bunescu, R. and Mooney, R. 2005. A shortest path de-
pendency kernel for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods 
in Natural Language Processing, pp.724?731. 
Bloehdorn, S. and Moschitti, A. 2007a. Combined Syn-
tactic and Semantic Kernels for Text Classification. 
In: Proceedings of the 29th European Conference on 
Information Retrieval (ECIR). 
Bloehdorn, S. and Moschitti, A. 2007b. Structure and 
semantics for expressive text kernels. In: Proceeding 
of ACM 16th Conference on Information and 
Knowledge Management (CIKM). 
Bunescu, R. and Mooney. R., 2006. Subsequence ker-
nels for relation extraction. In: Advances in Neural 
Information Processing Systems 18, pp. 171?178. 
Charniak, E., 2001. Immediate-head parsing for lan-
guage models. In: Proceedings of the 39th Annual 
Meeting on Association for Computational Linguis-
tics, pp. 124-131. 
Chan, Y. S. and Roth, D. 2010. Exploiting background 
knowledge for relation extraction. In: Proceedings 
of the 23rd International Conference on Computa-
tional Linguistics, pp. 152?160. 
Chan, Y. S. and Roth, D. 2011. Exploiting syntactico-
semantic structures for relation extraction. In: Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pp. 551?560. 
Croce, D., Moschitti, A. and Basili, R. 2011. Struc-
tured lexical similarity via convolution kernels on 
dependency trees. In: Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language 
Processing, pp. 1034?1046. 
Croce, D., Moschitti, A., Basili, R. and Palmer, M. 
2012. Verb Classification using Distributional Sim-
ilarity in Syntactic and Semantic Structures. In: Pro-
ceedings of ACL 2012, pp. 263-272. 
Culotta, A. and Sorensen, J. 2004. Dependency tree 
kernels for relation extraction. In: Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics, pp. 423?429. 
Grishman, R. and Sundheim, B. 1996. Message under-
standing conference-6: A brief history. In: Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pp. 466?471. 
Collins, M. and Duffy, N., 2001. Convolution Kernels 
for Natural Language. In: Proceedings of NIPS 
2001. 
Liu, D., et al 2013. Incorporating lexical semantic 
similarity to tree kernel-based Chinese relation ex-
traction. In: Proceedings of Chinese Lexical Seman-
tics 2013. 
Jiang, J. and Zhai, C. 2007. A systematic exploration of 
the feature space for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pp. 113?120. 
Joachims, T.  1998.  Text  Categorization  with  Su 
pport Vector  Machine:  learning  with  many  rele-
vant  features. ECML-1998: 137-142. 
Kambhatla, N. 2004. Combining lexical, syntactic, and 
semantic features with maximum entropy models for 
extracting relations. In: the Proceedings of 42st An-
nual Meeting of the Association for Computational 
Linguistics, pp. 178?181. 
Krause, S., Li, H., Uszkoreit, H., & Xu, F. 2012. Large-
scale learning of relation-extraction rules with dis-
tant supervision from the web. In: Proceedings of 
ISWC 2012, pp. 263-278. 
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. 
1993. Building a large annotated corpus of English: 
The Penn Treebank. Computational linguistics, 
19(2), 313-330. 
Moschitti, A. 2004. A study on Convolution Kernels for 
Shallow Semantic Parsing. In: Proceedings of the 
42-th Conference on Association for Computational 
Linguistic (ACL-2004). 
Moschitti, A. 2009. Syntactic and semantic kernels for 
short text pair categorization. In: Proceedings of the 
12th Conference of the European Chapter of the 
ACL (EACL 2009), pp. 576?584. 
Mehdad, Y., Moschitti, A. and Zanzotto, F. 2010. Syn-
tactic/Semantic Structures for Textual Entailment 
Recognition. In: Proceedings of Human Language 
Technology - North American chapter of the Asso-
ciation for Computational Linguistics. 
Mintz, M., Bills, S., Snow, R. and Jurafsky D. 2009. 
Distant supervision for relation extraction without 
labeled data. In: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association 
for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pp. 1003?1011. 
66
Qian L., Zhou G., Kong F., Zhu Q., and Qian P., 2008. 
Exploiting constituent dependencies for tree kernel 
based semantic relation extraction. In: Proceedings 
of the 22nd International Conference on Computa-
tional Linguistics, pp. 697-704. 
Reichartz, F. and H. Korte, et al  2010. Semantic rela-
tion extraction with kernels over typed dependency 
trees. In: Proceedings of the 16th ACM SIGKDD 
international conference on Knowledge discovery 
and data mining. 
Zelenko, D., Aone, C., and Richardella, A. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3:1083?1106. 
Zhang, M., Zhang, J., and Su, J. 2006. Exploring syn-
tactic features for relation extraction using a convo-
lution tree kernel. In: Proceedings of the Human 
Language Technology Conference and the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 288?295. 
Zhang, M., Zhang, J., Su, J. and Zhou, G. 2006. A com-
posite kernel to extract relations between entities 
with both flat and structured features. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting 
of the Association for Computational Linguistics, 
pages 825?832. 
Zhao, S. and Grishman, R. 2005. Extracting relations 
with integrated information using kernel methods. 
In Proceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
419?426. 
Zhou, G., Su, J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 427?
434. 
Zhou, G. and Zhang M. 2007. Extracting relation in-
formation from text documents by exploring various 
types of knowledge. Information Processing & Man-
agement 43(4): 969--982. 
Zhou, G., et al 2007. Tree kernel-based relation ex-
traction with context-sensitive structured parse tree 
information. In: Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, pp. 728?736. 
67
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718?724,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Semantic Consistency: A Local Subspace Based Method for Distant 
Supervised Relation Extraction 
 
 Xianpei Han    and       Le Sun 
State Key Laboratory of Computer Science 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
  
Abstract 
One fundamental problem of distant supervi-
sion is the noisy training corpus problem. In 
this paper, we propose a new distant supervi-
sion method, called Semantic Consistency, 
which can identify reliable instances from 
noisy instances by inspecting whether an in-
stance is located in a semantically consistent 
region. Specifically, we propose a semantic 
consistency model, which first models the lo-
cal subspace around an instance as a sparse 
linear combination of training instances, then 
estimate the semantic consistency by exploit-
ing the characteristics of the local subspace. 
Experimental results verified the effectiveness 
of our method. 
1 Introduction 
Relation extraction aims to identify and categorize 
relations between pairs of entities in text. Due to 
the time-consuming annotation process, one criti-
cal challenge of relation extraction is the lack of 
training data. To address this limitation, a promis-
ing approach is distant supervision (DS), which 
can automatically gather labeled data by heuristi-
cally aligning entities in text with those in a 
knowledge base (Mintz et al, 2009). The under-
lying assumption of distant supervision is that 
every sentence that mentions two entities is likely 
to express their relation in a knowledge base. 
Relation Instance Label 
S1: Jobs was the founder of Apple Founder-of, CEO-of 
S2: Jobs joins Apple Founder-of, CEO-of 
Figure 1. Labeled instances by distant supervi-
sion, using relations CEO-of(Steve Jobs, Apple 
Inc.) and Founder-of(Steve Jobs, Apple Inc.) 
The distant supervision assumption, unfortu-
nately, can often fail and result in a noisy training 
corpus. For example, in Figure 1 DS assumption 
will wrongly label S1 as a CEO-of instance and S2 
as instance of Founder-of and CEO-of. The noisy 
training corpus in turn will lead to noisy extrac-
tions that hurt extraction accuracy (Riedel et al, 
2010). 
 
Figure 2. The regions the two instances in Figure 
1 located, where: 1) S1 locates in a semantically 
consistent region; and 2) S2 locates in a semanti-
cally inconsistent region 
To resolve the noisy training corpus problem, 
this paper proposes a new distant supervision 
method, called Semantic Consistency, which can 
effectively identify reliable instances from noisy 
instances by inspecting whether an instance is lo-
cated in a semantically consistent region. Figure 2 
shows two intuitive examples. We can see that, 
semantic consistency is an effective way to iden-
tify reliable instances. For example, in Figure 2 S1 
is highly likely a reliable Founder-of instance be-
cause its neighbors are highly semantically con-
sistent, i.e., most of them express the same rela-
tion type ? Founder-of. On contrast S2 is highly 
likely a noisy instance because its neighbors are 
semantically inconsistent, i.e., they have a diverse 
relation types. The problem now is how to model 
the semantic consistency around an instance. 
To model the semantic consistency, this paper 
proposes a local subspace based method. Specifi-
cally, given sufficient training instances, our 
method first models each relation type as a linear 
subspace spanned by its training instances. Then, 
the local subspace around an instance is modeled 
and characterized by seeking the sparsest linear 
combination of training instances which can re-
construct the instance. Finally, we estimate the se-
mantic consistency of an instance by exploiting 
the characteristics of its local subspace. 
+
+
+
+
+
?
??
?
?
S2
? S1 ?
+:  CEO-of
?:  Founder-of
+
?
+
?
? ?
+    :  Manager-of
   :  CTO-of
718
This paper is organized as follows. Section 2 
reviews related work. Section 3 describes the pro-
posed method. Section 4 presents the experiments. 
Finally Section 5 concludes this paper. 
2 Related Work 
This section briefly reviews the related work. Cra-
ven and Kumlien (1999), Wu et al (2007) and 
Mintz et al(2009) were several pioneer work of 
distant supervision. One main problem of DS as-
sumption is that it often will lead to false positives 
in training data. To resolve this problem, Bunescu 
and Mooney (2007), Riedel et al (2010) and Yao 
et al (2010) relaxed the DS assumption to the at-
least-one assumption and employed multi-in-
stance learning techniques to identify wrongly la-
beled instances. Takamatsu et al (2012) proposed 
a generative model to eliminate noisy instances. 
Another research issue of distant supervision is 
that a pair of entities may participate in more than 
one relation. To resolve this problem, Hoffmann 
et al (2010) proposed a method which can com-
bine a sentence-level model with a corpus-level 
model to resolve the multi-label problem. 
Surdeanu et al (2012) proposed a multi-instance 
multi-label learning approach which can jointly 
model all instances of an entity pair and all their 
labels. Several other research issues also have 
been addressed. Xu et al (2013), Min et al (2013) 
and Zhang et al (2013) try to resolve the false 
negative problem raised by the incomplete 
knowledge base problem. Hoffmann et al (2010) 
and Zhang et al (2010) try to improve the extrac-
tion precision by learning a dynamic lexicon. 
3 The Semantic Consistency Model for 
Relation Extraction 
In this section, we describe our semantic con-
sistency model for relation extraction. We first 
model the subspaces of all relation types in the 
original feature space, then model and character-
ize the local subspace around an instance, finally 
estimate the semantic consistency of an instance 
and exploit it for relation extraction. 
3.1 Testing Instance as a Linear Combina-
tion of Training Instances 
In this paper, we assume that there exist k distinct 
relation types of interest and each relation type is 
represented with an integer index from 1 to k. For 
ith relation type, we assume that totally ni training 
instances Vi = fvi;1;vi;2; :::;vi;nig have been 
collected using DS assumption. And each instance 
is represented as a weighted feature vector, such 
as the features used in (Mintz, 2009) or (Surdeanu 
et al, 2012), with each feature is TFIDF weighted 
by taking each instance as an individual document. 
To model the subspace of ith relation type in 
the original feature space, a variety of models 
have been proposed to discover the underlying 
patterns of Vi. In this paper, we make a simple and 
effective assumption that the instances of a single 
relation type can be represented as the linear 
combination of other instances of the same rela-
tion type. This assumption is well motived in rela-
tion extraction, because although there is nearly 
unlimited ways to express a specific relation, in 
many cases basic principles of economy of ex-
pression and/or conventions of genre will ensure 
that certain systematic ways will be used to ex-
press a specific relation (Wang et al, 2012). For 
example, as shown in (Hearst, 1992), the IS-A re-
lation is usually expressed using several regular 
patterns, such as ?such NP as {NP ,}* {(or | and)} 
NP? and ?NP {, NP}* {,} or other NP?. 
Based on the above assumption, we hold many 
instances for each relation type and directly use 
these instances to model the subspace of a relation 
type. Specifically, we represent an instance y of 
ith type as the linear combination of training in-
stances associated with ith type: 
y = ?i;1vi;1 + ?i;2vi;2 + ::: + +?i;nivi;ni   (1) 
for some scalars , with j = 1, 2, ?,ni. For ex-
ample, we can represent the CEO-of instance 
?Jobs was the CEO of Apple? as the following lin-
ear combination of CEO-of instances: 
? 0.8: Steve Ballmer is the CEO of Microsoft 
? 0.2: Rometty was served as the CEO of IBM 
For simplicity, we arrange the given ni training in-
stances of ith relation type as columns of a matrix 
Ai = [vi;1;vi;2; :::;vi;ni], then we can write the 
matrix form of Formula 1 as: 
y = Aixi                           (2) 
where xi = [?i;1; :::; ?i;ni] is the coefficient vec-
tor. In this way, the subspace of a relation type is 
the linear subspace spanned by its training in-
stances, and if we can find a valid xi, we can ex-
plain y as a valid instance of ith relation type. 
3.2 Local Subspace Modeling 
via Sparse Representation 
Based on the above model, the local subspace of 
an instance is modeled as the linear combination 
of training instances which can reconstruct the in-
stance. Specifically, to model the local subspace, 
we first concatenate the n training instances of all 
k relation types: 
A = [A1;A2; :::; Ak] 
719
Then the local subspace around y is modeled by 
seeking the solution of the following formula: 
y = Ax                           (3) 
However, because of the redundancy of train-
ing instances, Formula 3 usually has more than 
one solution. In this paper, following the idea in 
(Wright et al, 2009) for robust face recognition, 
we use the sparsest solution (i.e., how to recon-
struct an instance using minimal training in-
stances), which have been shown is both discrimi-
nant and robust to noisiness. Concretely, we seek 
the sparse linear combination of training instances 
to reconstruct y by solving: 
(l1) : x? = arg min kxk1 s.t. kAx?yk2 ? "  (4) 
where x= [?1;1; :::;?1;n1; :::;?i;1;?i;2; :::;?i;ni; :::] 
is a coefficient vector which identifies the span-
ning instances of y?s local subspace, i.e., the in-
stances whose ??,? ? 0 . In practice, the training 
corpus may be too large to direct solve Formula 4. 
Therefore, this paper uses the K-Nearest-Neigh-
bors (KNN) of y (1000 nearest neighbors in this 
paper) to construct the training instance matrix A 
for each y, and KNN can be searched very effi-
ciently using specialized algorithms such as the 
LSH functions in (Andoni & Indyk, 2006). 
Through the above semantic decomposition, 
we can see that, the entries of x can encode the 
underlying semantic information of instance y. 
For ith relation type, let  be a new vector 
whose only nonzero entries are the entries in x that 
are associated with ith relation type, then we can 
compute the semantic component corresponding 
to ith relation type as . In this way a 
testing instance y will be decomposed into k se-
mantic components, with each component corre-
sponds to one relation type (with an additional 
noise component ): 
y= y1 + :::+yi + :::+yk + ?        (5) 
S1 = 0:8?
2
6
4
was
co-founder
of
...
3
7
5+ 0:2?
2
6
4
Jobs
Apple
the
...
3
7
5
 
 
S2 = 0:1
?join
...
?
+ 0:1
?join
...
?
+ 0:1
?join
...
?
+ ...
 
Figure 3. The semantic decomposition of the two 
instances in Figure 1 
Figure 3 shows an example of semantic decom-
position. We can see that, the semantic decompo-
sition can effectively summarize the semantic 
consistency information of y?s local subspace: if 
the instances around an instance have diverse re-
lation types (S2 for example), its information will 
be scattered on many different semantic compo-
nents. On contrast if the instances around an in-
stance have consistent relation types (S1 for ex-
ample), most of its information will concentrate 
on the corresponding relation type. 
3.3 Semantic Consistency based 
Relation Extraction 
This section describes how to estimate and exploit 
the semantic consistency for relation extraction. 
Specifically, given y?s semantic decomposition: 
y= y1 + :::+yi + :::+yk + ? 
we observe that if instance y locates at a semantic 
consistent region, then all its information will con-
centrate on a specific component yi, with all other 
components equal to zero vector 0. However, 
modeling errors, expression ambiguity and noisy 
features will lead to small nonzero components. 
Based on the above discussion, we define the se-
mantic consistency of an instance as the semantic 
concentration degree of its decomposition: 
Definition 1(Semantic Consistency). For an in-
stance y, its semantic consistency with ith relation 
type is: 
Consistency(y; i) = kyik2P
i kyik2 + k?k2
 
where Consistency(y, i)  and will be 1.0 if 
all information of y is consistent with ith relation 
type; on contrast it will be 0 if no information in y 
is consistent with ith relation type. 
Semantic Consistency based Relation Ex-
traction. To get accurate extractions, we deter-
mine the relation type of y based on both: 1) How 
much information in y is related to ith type; and 2) 
its semantic consistency score with ith type, i.e., 
whether y is a reliable instance of ith type. 
To measure how much information in y is re-
lated to ith relation type, we compute the propor-
tio  of common information between y and yi: 
sim(y;yi) = y ? yiy ? y
                    (6) 
Then the likelihood for a testing instance y ex-
pressing ith relation type is scored by summariz-
ing both its information and semantic consistency: 
rel(y; i) = sim(y;yi)?Consistency(y; i) 
and y will be classified into ith relation type if its 
likelihood is larger than a threshold: 
rel(y; i) ? ?i                       (7) 
where  is a relation type specific threshold 
learned from training dataset. 
Founder-of CEO-of 
Founder-of noise 
CTO-of 
720
Multi-Instance Evidence Combination. It is 
often that an entity pair will match more than one 
sentence. To exploit such redundancy for more 
confident extraction, this paper first combines the 
evidence from different instances by combing 
their underlying components. That is, given the 
matched m instances Y={y1, y2, ?, ym} for an en-
tity pair (e1, e2), we first decompose each instance 
as yj = yj1 + ::: + yjk + ?, then the entity-pair 
level decomposition y = y1 + :::+yk + ? is ob-
tained by summarizing semantic components of 
different instances: yi =P1?j?myji. Finally, the 
likelihood of an entity pair expressing ith relation 
type is scored as: 
rel(Y; i) = sim(y;yi)Consistency(y; i)log(m+1) 
where  is a score used to encourage 
extractions with more matching instances. 
3.4 One further Issue for Distant Supervi-
sion: Training Instance Selection 
The above model further provides new insights 
into one issue for distant supervision: training in-
stance selection. In this paper, we select informa-
tive training instances by seeking a most compact 
subset of instances which can span the whole sub-
space of a relation type. That is, all instances of 
ith type can be represented as a linear combination 
of these selected instances. 
However, finding the optimal subset of training 
instances is difficult, as there exist 2N possible so-
lutions for a relation type with N training instances. 
Therefore, this paper proposes an approximate 
training instance selection algorithm as follows: 
1) Computing the centroid of ith relation type as                
vi = P1?j?ni vi;j 
2) Finding the set of training instances which 
can most compactly span the centroid by 
solving: 
(l1) : xi = arg min kxk1 s.t. kAix? vik2 ? " 
3) Ranking all training instances according to 
their absolute coefficient weight value ; 
4) Selecting top p percent ranked instances as 
final training instances. 
The above training instance selection has two 
benefits. First, it will select informative instances 
and remove redundant instances: an informative 
instance will receive a high  value because 
many other instances can be represented using it; 
and if two instances are redundant, the sparse so-
lution will only retain one of them. Second, most 
of the wrongly labeled training instances will be 
filtered, because these instances are usually not 
regular expressions of ith type, so they appear 
only a few times and will receive a small . 
4 Experiments 
In this section, we assess the performance of our 
method and compare it with other methods. 
Dataset. We assess our method using the KBP 
dataset developed by Surdeanu et al (2012). The 
KBP is constructed by aligning the relations from 
a subset of English Wikipedia infoboxes against a 
document collection that merges two distinct 
sources: (1) a 1.5 million documents collection 
provided by the KBP shared task(Ji et al, 2010; Ji 
et al, 2011); and (2) a complete snapshot of the 
June 2010 version of Wikipedia. Totally 183,062 
training relations and 3,334 testing relations are 
collected. For tuning and testing, we used the 
same partition as Surdeanu et al (2012): 40 que-
ries for development and 160 queries for formal 
evaluation. In this paper, each instance in KBP is 
represented as a feature vector using the features 
as the same as in (Surdeanu et al, 2012). 
Baselines. We compare our method with four 
baselines as follows: 
? Mintz++. This is a traditional DS assump-
tion based model proposed by Mintz et al(2009).  
? Hoffmann. This is an at-least-one as-
sumption based multi-instance learning method 
proposed by Hoffmann et al (2011). 
? MIML. This is a multi-instance multi-la-
bel model proposed by Surdeanu et al (2012).  
? KNN. This is a classical K-Nearest-
Neighbor classifier baseline. Specifically, given 
an entity pair, we first classify each matching in-
stance using the labels of its 5 (tuned on training 
corpus) nearest neighbors with cosine similarity, 
then all matching instances? classification results 
are added together. 
Evaluation. We use the same evaluation set-
tings as Surdeanu et al (2012). That is, we use the 
official KBP scorer with two changes: (a) relation 
mentions are evaluated regardless of their support 
document; and (b) we score only on the subset of 
gold relations that have at least one mention in 
matched sentences. For evaluation, we use 
Mintz++, Hoffmann, and MIML implementation 
from Stanford?s MIMLRE package (Surdeanu et 
al., 2012) and implement KNN by ourselves. 
4.1 Experimental Results 
4.1.1 Overall Results 
We conduct experiments using all baselines and 
our semantic consistency based method. For our 
721
method, we use top 10% weighted training in-
stances. All features occur less than 5 times are 
filtered. All l1-minimization problems in this pa-
per are solved using the augmented Lagrange 
multiplier algorithm (Yang et al, 2010), which 
has been proven is accurate, efficient, and robust. 
To select the classification threshold  for ith re-
lation type, we use the value which can achieve 
the best F-measure on training dataset (with an ad-
ditional restriction that precision should > 10%). 
 
Figure 4. Precision/recall curves in KBP dataset 
System Precision Recall F1 
Mintz++ 0.260 0.250 0.255 
Hoffmann 0.306 0.198 0.241 
MIML 0.249 0.314 0.278 
KNN 0.261 0.295 0.277 
Our method 0.286 0.342 0.311 
Table 1.  The best F1-measures in KBP dataset 
Figure 4 shows the precision/recall curves of 
different systems, and Table 1 shows their best 
F1-measures. From these results, we can see that: 
1) The semantic consistency based method 
can achieve robust and competitive performance: 
in KBP dataset, our method correspondingly 
achieves 5.6%, 7%, 3.3% and 3.4% F1 improve-
ments over the Mintz++, Hoffmann, MIML and 
KNN baselines. We believe this verifies that the 
semantic consistency around an instance is an ef-
fective way to identify reliable instances. 
2) From Figure 4 we can see that our method 
achieves a consistent improvement on the high-re-
call region of the KBP curves (when recall > 0.1). 
We believe this is because by modeling the se-
mantic consistency using the local subspace 
around each testing instance, our method can bet-
ter solve the classification of long tail instances 
which are not expressed using salient patterns. 
3) The local subspace around an instance 
can be effectively modeled as a linear subspace 
spanned by training instances. From Table 1 we 
can see that both our method and KNN baseline 
(where the local subspace is spanned using its k 
nearest neighbors) achieve competitive perfor-
mance: even the simple KNN baseline can achieve 
a competitive performance (0.277 in F1). This re-
sult shows: a) the effectiveness of instance-based 
subspace modeling; and b) by partitioning sub-
space into many local subspaces, the subspace 
model is more adaptive and robust to model prior. 
4) The sparse representation is an effective 
way to model the local subspace using training in-
stances. Compared with KNN baseline, our 
method can achieve a 3.4% F1 improvement. We 
believe this is because: (1) the discriminative na-
ture of sparse representation as shown in (Wright 
et al, 2009); and (2) the sparse representation 
globally seeks the combination of training in-
stances to characterize the local subspace, on con-
trast KNN uses only its nearest neighbor in the 
training data, which is more easily affected by 
noisy training instances(e.g., false positives). 
4.1.2 Training Instance Selection Results 
To demonstrate the effect of training instance se-
lection, Table 2 reports our method?s performance 
using different proportions of training instances. 
Proportion 5% 10% 20% 100% 
Best F1 0.282 0.311 0.305 0.280 
Table 2. The best F1-measures using different 
proportions of top weighted training instances 
From Table 2, we can see that: ? Our training in-
stance selection algorithm is effective: our method 
can achieve performance improvement using only 
top weighted instances. ? The training instances 
are highly redundant: using only 10% weighted 
instances can achieve a competitive performance. 
5 Conclusion and Future Work 
This paper proposes a semantic consistency 
method, which can identify reliable instances 
from noisy instances for distant supervised rela-
tion extraction. For future work, we want to de-
sign a more effective instance selection algorithm 
and embed it into our extraction framework. 
Acknowledgments 
This work is supported by the National Natural 
Science Foundation of China under Grants no. 
61100152 and 61272324, and the National High 
Technology Development 863 Program of China 
under Grants no. 2013AA01A603. 
722
Reference 
Andoni, Alexandr, and Piotr Indyk . 2006. Near-opti-
mal hashing algorithms for approximate nearest 
neighbor in high dimensions. In: Foundations of 
Computer Science, 2006,  pp. 459-468. 
Bunescu, Razvan, and Raymond Mooney. 2007. 
Learning to extract relations from the web using 
minimal supervision. In: ACL 2007, pp. 576. 
Craven, Mark, and Johan Kumlien. 1999. Constructing 
biological knowledge bases by extracting infor-
mation from text sources. In : Proceedings of AAAI 
1999. 
Downey, Doug, Oren Etzioni, and Stephen Soderland. 
2005. A probabilistic model of redundancy in infor-
mation extraction, In: Proceeding of IJCAI 2005. 
Gupta, Rahul, and Sunita Sarawagi. 2011. Joint train-
ing for open-domain extraction on the web: exploit-
ing overlap when supervision is limited. In: Pro-
ceedings of WSDM 2011, pp. 217-226. 
Hearst, Marti A. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In: Proceedings of 
COLING 1992, pp. 539-545. 
Hoffmann, Raphael, Congle Zhang, and Daniel S. 
Weld. 2010. Learning 5000 relational extractors. In: 
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, 2010, pp. 
286-295. 
Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke 
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction 
of overlapping relations. In: Proceedings of ACL 
2011, pp. 541-550. 
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010 
knowledge base population track. In: Proceedings 
of the Text Analytics Conference. 
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the TAC 2011 
knowledge base population track.  In Proceedings of 
the Text Analytics Conference. 
Krause, Sebastian, Hong Li, Hans Uszkoreit, and Feiyu 
Xu. 2012. Large-Scale learning of relation-extrac-
tion rules with distant supervision from the web. In: 
ISWC 2012, pp. 263-278. 
Mintz, Mike, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In: Proceedings ACL- 
AFNLP 2009, pp. 1003-1011. 
Min, Bonan, Ralph Grishman, Li Wan, Chang Wang, 
and David Gondek. 2013. Distant Supervision for 
Relation Extraction with an Incomplete Knowledge 
Base. In: Proceedings of NAACL-HLT 2013,pp. 
777-782. 
Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun. 
2012. New york university 2012 system for kbp slot 
filling. In: Proceedings of TAC 2012. 
Nguyen, Truc-Vien T., and Alessandro Moschitti. 
2011. Joint distant and direct supervision for rela-
tion extraction. In: Proceedings of IJCNLP 2011, pp. 
732-740. 
Riedel, Sebastian, Limin Yao, and Andrew McCallum. 
2010. Modeling relations and their mentions with-
out labeled text. In: Machine Learning and 
Knowledge Discovery in Databases, 2010, pp. 148-
163. 
Riedel, Sebastian, Limin Yao, Andrew McCallum, and 
Benjamin M. Marlin. 2013. Relation Extraction 
with Matrix Factorization and Universal Schemas. 
In: Proceedings of NAACL-HLT 2013, pp. 74-84. 
Roth, Benjamin, and Dietrich Klakow. 2013. Combin-
ing Generative and Discriminative Model Scores for 
Distant Supervision. In: Proceedings of ACL 2013, 
pp. 24-29. 
Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati, 
and Christopher D. Manning. 2012. Multi-instance 
multi-label learning for relation extraction. In: Pro-
ceedings of EMNLP-CoNLL 2012, pp. 455-465. 
Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. 
2012. Reducing wrong labels in distant supervision 
for relation extraction. In: ACL 2012,pp. 721-729. 
Wang, Chang, Aditya Kalyanpur, James Fan, Branimir 
K. Boguraev, and D. C. Gondek. 2012. Relation ex-
traction and scoring in DeepQA. In: IBM Journal of 
Research and Development, 56(3.4), pp. 9-1. 
Wang, Chang, James Fan, Aditya Kalyanpur, and Da-
vid Gondek. 2011. Relation extraction with relation 
topics. In: Proceedings of EMNLP 2011, pp. 1426-
1436. 
Wright, John, Allen Y. Yang, Arvind Ganesh, Shankar 
S. Sastry, and Yi Ma. 2009. Robust face recognition 
via sparse representation. In: Pattern Analysis and 
Machine Intelligence, IEEE Transactions on, 31(2), 
210-227 
Wu, Fei, and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In: Proceedings of CIKM 
2007,pp. 41-50. 
Xu, Wei, Raphael Hoffmann Le Zhao, and Ralph 
Grishman. 2013. Filling Knowledge Base Gaps for 
Distant Supervision of Relation Extraction. In: Pro-
ceedings of Proceedings of 2013, pp. 665-670. 
Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, and 
Yi Ma. 2010. Fast l1-Minimization Algorithms and 
An Application in Robust Face Recognition: A Re-
view. In: Proceedings of ICIP 2010. 
Yao, Limin, Sebastian Riedel, and Andrew McCallum. 
2010. Collective cross-document relation extraction 
723
without labelled data. In: Proceedings of EMNLP 
2010, pp. 1013-1023. 
Zhang, Congle, Raphael Hoffmann, and Daniel S. 
Weld. 2012. Ontological smoothing for relation ex-
traction with minimal supervision. In: Proceedings 
of AAAI 2012, pp. 157-163. 
Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan, 
Jun, Chen, Zheng and Sui, Zhifang. 2013. Towards 
Accurate Distant Supervision for Relational Facts 
Extraction. In: Proceedings of ACL 2013, pp. 810-
815. 
724
Overview of the Chinese Word Sense Induction Task at CLP2010 
Le Sun 
Institute of Software 
Chinese Academy of 
Sciences 
sunle@iscas.ac.cn 
Zhenzhong Zhang 
Institute of Software, Graduate 
University Chinese Academy of 
Sciences 
zhenzhong@nfs.iscas.ac.cn
Qiang Dong 
Canada Keentime Inc. 
dongqiang@keenage.
com 
 
Abstract 
In this paper, we describe the Chinese 
word sense induction task at CLP2010. 
Seventeen teams participated in this task 
and nineteen system results were 
submitted. All participant systems are 
evaluated on a dataset containing 100 
target words and 5000 instances using 
the standard cluster evaluation. We will 
describe the participating systems and 
the evaluation results, and then find the 
most suitable method by comparing the 
different Chinese word sense induction 
systems. 
1 Introduction 
Word Sense Disambiguation (WSD) is an 
important task in natural language proceeding 
research and is critical to many applications 
which require language understanding. In 
traditional evaluations, the supervised methods 
usually can achieve a better WSD performance 
than the unsupervised methods. But the 
supervised WSD methods have some drawbacks: 
Firstly, they need large annotated dataset which 
is expensive to manually annotate (Agirre and 
Aitor, 2007). Secondly, the supervised WSD 
methods   are based on the ?fixed-list of 
senses? paradigm, i.e., the senses of a target 
word are represented as a closed list coming 
from a manually constructed dictionary (Agirre 
et al, 2006). Such a ?Fixed-list of senses? 
paradigm suffers from the lack of explicit and 
topic relations between word senses, are usually 
cannot reflect the exact context of the target 
word (Veronis, 2004). Furthermore, because the 
?fixed-list of senses? paradigm make the fix 
granularity assumption of the senses distinction, 
it may not be suitable in different situations 
(Samuel and Mirella, 2009). Thirdly, since most 
supervised WSD methods assign senses based 
on dictionaries or other lexical resources, it will 
be difficult to adapt them to new domains or 
languages when such resources are scare 
(Samuel and Mirella, 2009).  
To overcome the deficiencies of the 
supervised WSD methods, many unsupervised 
WSD methods have been developed in recent 
years, which can induce word senses directly 
from the unannotated dataset, i.e., Word Sense 
Induction (WSI). In this sense, WSI could be 
treat as a clustering task, which groups the 
instances of the target word according to their 
contextual similarity, with each resulting cluster 
corresponding to a specific ?word sense? or 
?word use? of the target word (in the task of 
WSI, the term ?word use? is more suitable than 
?word sense?(Agirre and Aitor, 2007)).  
Although traditional clustering techniques can 
be directly employed in WSI, in recent years 
some new methods have been proposed to 
enhance the WSI performance, such as the 
Bayesian approach (Samuel and Mirella, 2009) 
and the collocation graph approach (Ioannis and 
Suresh, 2008). Both the traditional and the new 
methods can achieve a good performance in the 
task of English word sense induction. However, 
the methods work well in English may not be 
suitable for Chinese due to the difference 
between Chinese and English.  So it is both 
important and critical to provide a standard 
testbed for the task of Chinese word sense 
induction (CWSI), in order to compare the 
performance of different Chinese WSI methods 
and find the methods which are suitable for the 
Chinese word sense induction task.  
In this paper, we describe the Chinese word 
sense induction task at CLP2010. The goal of 
this task is to provide a standard testbed for 
Chinese WSI task. By comparing the different 
Chinese WSI methods, we can find the suitable 
methods for the Chinese word sense induction 
task.  
This paper is organized as follow. Section 2 
describes the evaluation dataset in detail. Section 
3 demonstrates the evaluation criteria. Section 3 
describes the participated systems and their 
results. The conclusions are drawn in section 4. 
2 Dataset 
Two datasets are provided to the participants: 
the trial dataset and the test dataset. 
The trial dataset contains 50 Chinese words, 
and for each Chinese word, a set of 50 word 
instances are provided. All word instances are 
extracted from the Web and the newspapers like 
the Xinhua newspaper and the Renmin 
newspaper, and the HowNet senses of target 
words were manually annotated (Dong). Figure 
1 shows an example of the trial data without 
hand-annotated tag. Figure 2 shows an example 
of the trial data with hand-annotated tag. In 
Figure 1, the tag ?snum=2? indicates that the 
target word ???? has two different senses in 
this dataset. In each instance, the target word is 
marked between the tag ?<head>? and the tag 
?</head>?. In Figure 2, all instances between the 
tag ?<sense s=S0>? and the tag ?</sense>? are 
belong to the same sense class.  
 
 
Figure 1: Example of the trial data without 
hand-annotated tag. 
 
The case of the test dataset is similar to the 
trial dataset, but with little different in the 
number of target words. The test dataset contains 
100 target words (22 Chinese words containing 
one Chinese character and 78 Chinese words  
containing two or more Chinese ideographs). 
Figure 3 shows an example of a system?s output. 
In Figure 3, the first column represents the 
identifiers of target word, the second column 
represents the identifiers of instances, and the 
third column represents the identifiers of the 
resulting clusters and their weight (1.0 by default) 
generated by Chinese WSI systems. 
 
 
Figure 2: Example of the trial data with 
hand-annotated tag. 
 
 
Figure 3: Example of the output format. 
3 Evaluation Metric 
As described in Section 1, WSI could be 
conceptualized as a clustering problem. So we 
can measure the performance of WSI systems 
using the standard cluster evaluation metrics. As 
the same as Zhao and Karypis(2005), we use the 
FScore measure as the primary measure for 
assessing different WSI methods. The FScore is 
used in a similar way as at Information Retrieval 
field.  
In this case, the results of the WSI systems are 
treated as clusters of instances and the gold 
standard senses are classes. Then the precision 
of a class with respect to a cluster is defined as 
the number of their mutual instances divided by 
the total cluster size, and the recall of a class 
with respect to a cluster is defined as the number 
of their mutual instances divided by the total 
class size. The detailed definition is as bellows.  
Let the size of a particular class sr is nr, the 
size of a particular cluster hj is nj and the size of 
their common instances set is nr,j.,then the 
precision can be defined as: 
,( , ) r jr j
j
n
P s h
n
=  
The recall can be defined as: 
,( , ) r jr j
r
n
R s h
n
=  
Then FScore of this class and cluster is defined 
to be: 
2 ( , ) ( , )
( , )
( , ) ( , )
r j r j
r j
r j r j
P s h R s h
F s h
P s h R s h
? ?= +  
The FScore of a class sr, F(sr), is the maximum 
F(sr, hj) value attained by any cluster, and it is 
defined as: 
 ( ) max( ( , ))
j
r r jh
F s F s h=  
Finally, the FScore of the entire clustering 
solution is defined as the weighted average 
FScore of all class: 
1
( )q r r
r
n F s
FScore
n=
?=?  
where q is the number of classes and n is the size 
of the instance set for particular target word. 
Table 1 shows an example of a contingency 
table of classes and clusters, which can be used 
to calculate FScore. 
 
 Cluster 1 Cluster 2 
Class 1 100 500 
Class 2 400 200 
Table 1: A contingency table of classes and 
clusters 
 
Using this contingency table, we can calculate 
the FScore of this example is 0.7483. It is easy 
to know the FScore of a perfect clustering 
solution will be equal to one, where each cluster 
has exactly the same instances as one of the 
classes, and vice versa. This means that the 
higher the FScore, the better the clustering 
performance. 
Purity and entropy (Zhao and Karypis, 2005) 
are also used to measure the performance of the 
clustering solution. Compared to FScore, they 
have some disadvantages. FScore uses two 
complementary concepts, precision and recall, to 
assess the quality of a clustering solution. 
Precision indicates the degree of the instances 
that make up a cluster, which belong to a single 
class. On the other hand, recall indicates the 
degree of the instances that make up a class, 
which belong to a single cluster. But purity and 
entropy only consider one factor and discard 
another. So we use FScore measure to assess a 
clustering solution. 
For the sake of completeness, we also employ 
the V-Measure to assess different clustering 
solutions. V-Measure assesses a cluster solution 
by considering its homogeneity and its 
completeness (Rosenberg and Hirschberg, 2007). 
Homogeneity measures the degree that each 
cluster contains data points which belong to a 
single Gold Standard class. And completeness 
measures the degree that each Gold Standard 
class contains data points assigned to a single 
cluster (Rosenberg and Hirschberg, 2007). In 
general, the larger the V-Measure, the better the 
clustering performance. More details can be 
referred to (Rosenberg and Hirschberg, 2007). 
4 Results 
In this section we describe the participant 
systems and present their results. 
Since the size of test data may not be large 
enough to distinguish word senses, participants 
were provided the total number of the target 
word?s senses. And participants were also 
allowed to use extra resources without 
hand-annotated. 
4.1 Participant teams and systems 
There were 17 teams registered for the WSI task 
and 12 teams submitted their results. Totally 19 
participant system results were submitted (One 
was submitted after the deadline). 10 teams 
submitted their technical reports. Table 2 
demonstrates the statistics of the participant 
information.  
The methods used by the participated systems 
were described as follows: 
FDU: This system first extracted the triplets 
for target word in each instance and got the 
intersection of all related words of these triplets 
using Baidu web search engine. Then the triplets 
and their corresponding intersections were used 
to construct feature vectors of the target word?s 
instances. After that, sequential Information 
Bottleneck algorithm was used to group 
instances into clusters. 
BUPT: Three clustering algorithms- the 
k-means algorithm, the Expectation- 
maximization algorithm and the Locally 
Adaptive Clustering algorithm were employed to 
cluster instances, where all instances were 
represented using some combined features. In 
the end the Group-average agglomerative 
clustering was used to cluster the consensus 
matrix M, which was obtained from the  
 
 
Name of Participant Team Result Report
Natural Language Processing Laboratory at Northeastern University (NEU) ? ? 
Beijing University of Posts and Telecommunications (BUPT) ? ? 
Beijing Institute of Technology (BIT) ?  
Shanghai Jiao Tong University (SJTU)   
Laboratory of Intelligent Information Processing and Application 
Institutional at Leshan Teachers? College (LSTC) 
? ? 
 Natural Language Processing Laboratory at Soochow University (SCU) ? ? 
Fudan University (FDU) ? ? 
Institute of Computational Linguistics at Peking University 1 (PKU1) ? ? 
Beijing University of Information Science and Technology (BUIST) ?  
Tsinghua University Research Institute of Information Technology, 
Speech and Language Technologies R&D Center (THU) 
  
Information Retrieval Laboratory at Dalian University of Technology 
(DLUT) 
? ? 
Institute of Computational Linguistics at Peking University 2 (PKU2) ? ? 
City University of HK (CTU)   
Institute of Software Chinese Academy of Sciences (ISCAS) ? ? 
Cognitive Science Department at Xiamen University (XMU) ? ? 
Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)   
National Taipei University of Technology (NTUT)   
Table 2: The registered teams. ??? means that the team submitted the result or the report. 
 
adjacency matrices of the individual clusters 
generated by the three single clustering 
algorithms mentioned above. 
LSTC: This team extracted the five neighbor 
words and their POSs around the target word as 
features. Then the k-means algorithm was used 
to cluster the instances of each target word. 
NEU: The ?Global collocation? and the 
?local collocation? were extracted as features. A 
constraint hierarchical clustering algorithm was 
used to cluster the instances of each target 
word. 
XMU: The neighbor words of the target 
word were extracted as features and TongYiCi 
CiLin1 was employed to measure the similarity 
between instances. The word instances are 
???????????????????????????????????????? ?????????????????????
1? http://www.ir?lab.org/?
clustered using the improved hierarchical 
clustering algorithm based on parts of speech. 
DLUT: This team used the information gain 
to determine the size of the feature window. 
TongYiCi CiLin was used to solve the data 
sparseness problem.  The word instances are 
clustered using an improvement k-means 
algorithm where k-initial centers were selected 
based on maximum distance. 
ISCAS: This team employed k-means 
clustering algorithm to cluster the second order 
co-occurrence vectors of contextual words. 
TongYiCi CiLin and singular value 
decomposition method were used to solve the 
problem of data sparseness. Please note that this 
system was submitted by the organizers. The 
organizers have taken great care in order to  
 
guaranty all participants are under the same 
conditions. 
PKU2: This team used local tokens, local 
bigram feature and topical feature to represent 
words as vectors. Spectral clustering method 
was used to cluster the instances of each target 
word. 
PKU1: This team extracted three types of 
features to represent instances as feature vectors. 
Then the clustering was done by using k-means 
algorithm. 
SCU: All words except stop words in 
instances were extracted to produce the feature 
vectors, based on which the similarity matrix 
were generated. After that, the spectral 
clustering algorithm was applied to group 
instances into clusters. 
4.2 Official Results 
In this section we present the official results of 
the participant systems (ISCAS* was submitted 
by organizers; BUIST** was submitted after the 
deadline). We also provide the result of a 
baseline -- 1c1w, which group all instances of a 
target word into a single cluster. 
Table 3 shows the FScore of the main 
systems submitted by participant teams on the 
test dataset. Table 4 shows the FScore and 
V-Measure of all participant systems. Systems 
were ranked according to their FScore. 
 
Systems Rank FScore 
BUPT_mainsys 1 0.7933 
PKU1_main_system 2 0.7812 
FDU 3 0.7788 
DLUT_main_system 4 0.7729 
PKU2 5 0.7598 
ISCAS* 6 0.7209 
SCU 7 0.7108 
NEU_WSI_1 8 0.6715 
XMU 9 0.6534 
BIT 10 0.6366 
1c1w 11 0.6147 
BUIST** 12 0.5972 
LSTC 13 0.5789 
Table 3: FScore of main systems on the test 
dataset including one baseline -1c1w. 
 
 
 
 
Systems Rank FScore V- 
Measure
BUPT_mainsys 1 0.7933 0.4628 
BUPT_LAC 2 0.7895 0.4538 
BUPT_EM 3 0.7855 0.4356 
BUPT_kmeans 4 0.7849 0.4472 
PKU1_main_system 5 0.7812 0.4300 
FDU 6 0.7788 0.4196 
DLUT_main_system 7 0.7729 0.5032 
PKU1_agglo 8 0.7651 0.4096 
PKU2 9 0.7598 0.4078 
ISCAS* 10 0.7209 0.3174 
SCU 11 0.7108 0.3131 
NEU_WSI_1 12 0.6715 0.2331 
XMU 13 0.6534 0.1954 
NEU_WSI_0 14 0.6520 0.1947 
BIT 15 0.6366 0.1713 
1c1w 16 0.6147 0.0 
DLUT_RUN2 17 0.6067 0.1192 
BUIST** 18 0.5972 0.1014 
DLUT_RUN3 19 0.5882 0.0906 
LSTC 20 0.5789 0.0535 
Table 4: FScore and V-Measure of all systems, 
including one baseline. 
 
From the results shown in Table 3 and 4, we 
can see that: 
1)  As described in section 4.1, most 
systems use traditional clustering 
methods. For example, the teams using 
the k-means algorithm contain BUPT, 
LSTC, PKU1, DLUT and ISCAS. The 
teams using the spectral clustering 
algorithm contain SCU and PKU2. The 
team XMU and NEU use hierarchical 
clustering algorithm. The results shows 
that if provided with the number of 
target word senses, traditional methods 
can achieve a good performance. But we 
also notice that even the same method 
can have a different performance. This 
seems to indicate that features which are 
predictive of word senses are important 
to the task of CWSI. 
2)  Most systems outperform the 1c1w 
baseline, which indicates these systems 
are able to induce correct senses of 
target words to some extent.   
3)  The rank of FScore is much the same as 
that of V-Measure but with little 
difference. This may be because that the 
two evaluation measures both assess 
quality of a clustering solution by 
considering two different aspects, where 
precision corresponds to homogeneity 
and recall corresponds to completeness. 
But when assessing the quality of a 
clustering solution, the FScore only 
considers the contributions from the 
classes which are most similar to the 
clusters while the V-Measure considers 
the contributions from all classes. 
 
Systems Characters Words 
BUPT_mainsys 0.6307 0.8392 
BUPT_LAC 0.6298 0.8346 
BUPT_EM 0.6191 0.8324 
BUPT_kmeans 0.6104 0.8341 
PKU1_main_system 0.6291 0.8240 
FDU 0.6964 0.8020 
DLUT_main_system 0.5178 0.8448 
PKU1_agglo 0.5946 0.8132 
PKU2 0.6157 0.8004 
ISCAS* 0.5639 0.7651 
SCU 0.5715 0.7501 
NEU_WSI_1 0.5786 0.6977 
XMU 0.5290 0.6885 
NEU_WSI_0 0.5439 0.6825 
BIT 0.5328 0.6659 
DLUT_RUN2 0.5196 0.6313 
BUIST** 0.5022 0.6240 
DLUT_RUN3 0.5066 0.6113 
LSTC 0.4648 0.6110 
1c1w 0.4611 0.6581 
Table 5: FScore of all systems on the dataset 
only containing either single characters or 
words respectively. 
 
A Chinese word can be constituted by single 
or multiple Chinese characters. Senses of 
Chinese characters are usually determined by 
the words containing the character. In order to 
compare the WSI performance on different 
granularity of words, we add 22 Chinese 
characters into the test corpus. Table 5 shows 
the results of the participant systems 
correspondingly on the corpus which only 
contains the 22 Chinese characters and the 
corpus which only contains the 78 Chinese 
words. 
From Table 5, we can see that: 
1) The FScore of systems on the corpus 
only containing single characters is 
significantly lower than that on the 
corpus only containing words. We 
believe this is because: 1) The Single 
Chinese characters usually contains 
more senses than Chinese words; 2) 
Their senses are not determined directly 
by their contexts but by the words 
containing them. Compared to the 
number of instances, the number of 
words containing the single character is 
large. So it is difficult to distinguish 
different senses of single characters 
because of the data sparseness.  
2) We noticed that all systems outperform 
the 1c1w baseline on the corpus only 
containing single characters but there 
are some systems? FScore are lower 
than the baseline on the corpus only 
containing words. It may be because the 
large number of characters? senses and 
the FScore favored the words which 
have small number of senses.  
5 Conclusions 
In this paper we describe the design and the 
results of CLP2010 back-off task 4-Chinese 
word sense induction task. 17 teams registered 
to this task and 12 teams submitted their results. 
In total there were 19 participant systems (One 
of them was submitted after the deadline). And 
10 teams submitted their technical reports. All 
systems are evaluated on a corpus containing 
100 target words and 5000 instances using 
FScore measure and V-Measure. Participants 
are also provided with the number of senses and 
allowed to use resources without 
hand-annotated. 
The evaluation results have shown that most 
of the participant systems achieve a better 
performance than the 1c1w baseline. We also 
notice that it is more difficult to distinguish 
senses of Chinese characters than words. For 
future work, in order to test the performances of 
Chinese word sense induction systems under 
different conditions, corpus from different 
fields will be constructed and the number of 
target word senses will not be provided and will 
leave as an open task to the participant systems. 
Acknowledgments 
This work has been partially funded by National 
Natural Science Foundation of China under 
grant #60773027, #60736044 and #90920010 
and by ?863? Key Projects #2006AA010108, 
?863? Projects #2008AA01Z145. We would 
like to thank Dr. Han Xianpei and Zhang Weiru 
for their detailed comments. We also want to 
thank the annotators for their hard work on 
preparing the trial and test dataset. 
References 
Andrew Rosenberg and Julia Hirschberg. 2007. 
V-Measure: A conditional entropy-based external 
cluster evaluation measure. In Proceedings of the 
2007 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL), 
pages 410?420. 
Eneko Agirre, David Mart??nez, Oier L?opez de 
Lacalle,and Aitor Soroa. 2006. Two graph-based 
algorithms for state-of-the-art WSD. In 
Proceedings of the 2006 Conference on Empirical 
Methods in Natural Language Processing, pages 
585?593, Sydney, Australia. 
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 
task2: Evaluating word sense induction and 
discrimination systems. In Proceedings of 
SemEval-2007. Association for Computational 
Llinguistics, pages 7-12, Prague. 
Ioannis P. Klapaftis and Suresh Manandhar, 2008. 
Word Sense Induction Using Graphs of 
Collocations. In Proceeding of the 2008 
conference on 18th European Conference on 
Artificial Intelligence, Pages: 298-302. 
Jean. V?eronis. 2004. Hyperlex: lexical cartography 
for information retrieval. Computer Speech & 
Language,18(3):223.252. 
Samuel Brody and Mirella Lapata, 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
103-111, Athens, Greece. 
Ying Zhao and George Karypis. 2005. Hierarchical 
clustering algorithms for document datasets. Data 
Mining and Knowledge Discovery,10(2):141.168. 
Zhendong  Dong, 
http://www.keenage.com/zhiwang/e_zhiwang.html 
ISCAS?A System for Chinese Word Sense Induction Based on 
K-means Algorithm 
  Zhenzhong Zhang*           Le Sun? Wenbo Li? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
zhenzhong@nfs.iscas.ac.cn 
?Institute of Software 
Chinese Academy of Sciences 
{sunle,wenbo02}@iscas.ac.cn
 
Abstract 
This paper presents an unsupervised 
method for automatic Chinese word 
sense induction. The algorithm is based 
on clustering the similar words according 
to the contexts in which they occur. First, 
the target word which needs to be 
disambiguated is represented as the 
vector of its contexts. Then, reconstruct 
the matrix constituted by the vectors of 
target words through singular value 
decomposition (SVD) method, and use 
the vectors to cluster the similar words. 
Our system participants in CLP2010 
back off task4-Chinese word sense 
induction. 
1 Introduction 
It has been shown that using word senses instead 
of surface word forms could improve 
performance on many nature language 
processing tasks such as information extraction 
(Joyce and Alan, 1999), information retrieval 
(Ozlem et al, 1999) and machine translation 
(David et al, 2005). Historically, word senses 
are represented as a fixed-list of definitions 
coming from a manually complied dictionary. 
However, there seem to be some disadvantages 
associated with such fixed-list of senses 
paradigm. Since dictionaries usually contain 
general definitions and lack explicit semantic, 
they can?t reflect the exact content of the context 
where the target word appears. Another 
disadvantage is that the granularity of sense 
distinctions is fixed, so it may not be entirely 
suitable for different applications. 
In order to overcome these limitations, some 
techniques like word sense induction (WSI) have 
been proposed for discovering words? senses 
automatically from the unannotated corpus. The 
word sense induction algorithms are usually base 
on the Distributional Hypothesis, proposed by 
(Zellig, 1954), which showed that words with 
similar meanings appear in similar contexts 
(Michael, 2009). And the hypothesis is also 
popularized with the phrase ?a word characte-
rized by the company it keeps? (John, 1957). 
This concept shows us a method to automatical-
ly discover senses of words by clustering the 
target words with similar contexts (Lin, 1998). 
The word sense induction can be regarded as an 
unsupervised clustering problem. First, select 
some features to be used when comparing simi-
larity between words. Second, represent disam-
biguated words as vectors of selected features 
according to target words? contexts. Third, clus-
ter the similar words using the vectors. But 
compared with European languages such as Eng-
lish, Chinese language has its own characteris-
tics. For example, Chinese ideographs have 
senses while the English alphabets don?t have. 
So the methods which work well in English may 
not be entirely suitable for Chinese. 
  This paper proposes a method for Chinese 
word sense induction, which contains two stage 
processes: features selecting and context cluster-
ing. Chinese ideographs and Chinese words 
which have two or more Chinese ideographs are 
used different strategies when selecting features. 
The vectors of target word?s instances are put 
together to constitute a matrix, whose row is in-
stances and column is features. Reconstruct the 
matrix through singular value decomposition to 
get a new vector for each instance. Then, K-
means clustering algorithm is employed to clus-
ter the vectors of disambiguated words? contexts. 
Each cluster to which some instances belong to 
identifies a sense of corresponding target word. 
Our system participants in CLP2010 back off 
task4 - Chinese word sense induction. 
The remainder of this paper is organized as 
follows. Section 2 presents the Chinese word 
senses induction algorithm. Section 3 presents 
the evaluation sheme and the results of our 
system. Section 4 gives some discussions and 
conclusions. 
2 Chinese Word Senses Induction 
This section will present the strategies of select-
ing features for disambiguated Chinese words 
and k-means algorithm for clustering vectors of 
the contexts.  
2.1 Features Selection 
Since the input instances of target words are un-
structured, it's necessary to select features and 
transform them into structured format to fit the 
automatic clustering algorithm. Following the 
example in (Ted, 2007), words are chosen as 
features to represent the contexts where target 
words appear. A word w in the context of the 
target word can be represented as a vector whose 
ith component is the average of the calculated 
conditional probabilities of w and wj.  
The target words are usually removed from 
the corpus in the task of English word sense in-
duction. But Chinese language is very different 
from European languages such as English. Chi-
nese ideographs usually have meanings of their 
own while English   alphabets don?t have. In 
Chinese word senses induction tasks, the target 
word may be a Chinese word which could have 
one or more Chinese ideographs or a Chinese 
ideograph. And the meaning of Chinese ideo-
graphs is determined by the Chinese word where 
it appears. The following example shows us this 
case. 
z ??????????????? 162
???? 
z ?????????????????
??????????? 
In this example, the target word is Chinese 
ideograph ??? displayed in italic in the con-
texts. In the first context, its meaning is paddy 
which is determined by the Chinese word ??
? ?, and similarly in the second context its 
meaning is valley determined by ????. Since 
the meaning of the Chinese ideograph ??? is 
determined by the word where it appears, it may 
not be appropriate to remove it from the con-
texts simply while the others of the word are left. 
Different strategies are employed to remove tar-
get words.  If the target word contains two or 
more Chinese ideographs, it will be removed 
from the context. Otherwise it will be kept. 
  To solve the problem of data sparseness, we 
extracted extra 100 instances for each target 
word from Sogou Data and also used the 
thesauruses (TongYiCi CiLin of HIT) to reduce 
the dimensionality of the word space (feature 
space). Two filtering heuristics are applied when 
selecting features. The first one is the minimum 
frequency p1 of words, and the second one is the 
maximum frequency p2 of words. 
Each selected word (feature) should be as-
signed a weight, which indicates the relative fre-
quency of two co-occurring words. Using condi-
tional probabilities for weighting for object/verb 
and subject/verb pairs is better than point-wise 
mutual information (Philipp et al, 2005). So we 
used conditional probabilities for weighting 
words pairs. Let numi,j denote the number of the 
instances where the word i and word j co-occur , 
and numi denote the number of the instances in 
which the word i appears. Then the jth compo-
nent of the vector of the word i can be calculated 
using the following equation. 
,
( | ) ( | )
2i j
p j i p i j
w
+=  
Where 
  
,( | ) i j
j
n u m
p i j
n u m
=  
The contexts of each target word are represented 
as the centroid of the vectors of the words occur-
ring in the target contexts. Figure 1 shows an 
example of context vector, where the Chinese 
word ???? co-occurs with Chinese words ??
??and ????. 
 
Figure 1: An example of  a context vector for 
????, calculated as the centroid of vectors of 
???? and ????. 
2.2 Clustering Algorithm 
K-means algorithm is applied to cluster the vec-
tors of the target word. It assigns each element to 
one of K clusters according to which centroid 
the element is close to by the similarity function. 
The cosine function is used to measure the simi-
larity between two vectors V and W: 
1
2 2
1 1
( , )
| | | |
n
i i
i
n n
i i
i i
VW
V W
sim V W
V W
V W
=
= =
?= =?
?
? ?
 
where n is the number of features in each vector. 
Before clustering the vectors of instances, we 
put together the vectors of instances in the cor-
pus and obtain a co-occurrence matrix of in-
stances and words. Singular value decomposi-
tion is applied to reduce the dimensionality of 
the resulting multidimensional space and finds 
the major axes of variation in the word space 
(Golub and Van Loan, 1989). After the reduc-
tion, the similarity between two instances can be 
measured using the cosine function mentioned as 
above between the corresponding vectors. The 
clustering algorithm stops when the centroid of 
each cluster does not change or the iteration of 
the algorithm exceed a user-defined threshold p3. 
And the number of the clusters is determined by 
the corpus where the target word appears. Each 
cluster to which some instances belong 
represents one senses of the target word 
represented by the vector. 
We also employed a graph-based clustering 
algorithm -Chinese Whispers (CW) (Chris, 2006) 
to deal with the task of Chinese WSI. CW does 
not require any input parameters and has a good 
performance in WSI (Chris, 2006). For more 
details about CW algorithm please refer to 
(Chris, 2006). We first constructed a graph, 
whose vertexes were instances of target word 
and edges? weight was the similarity of the cor-
responding two vertexes. Then we removed the 
edges with minimum weight until the percentage 
of the kept edges? sum respect the total was be-
low a threshold p4. CW algorithm was employed 
to cluster the graph and each clusters represented 
a sense of target word. 
3 Evaluation 
This section presents the evaluation scheme, set 
of parameters and the result of our system. 
3.1 Evaluation Scheme 
We use standard cluster evaluation methods to 
measure the performance of our WSI system. 
Following the former practice (Zhao and Kary-
pis, 2005), we consider the FScore measure for 
assessing WSI methods. The FScore is used in a 
similar fashion to Information Retrieval exercis-
es. 
Let we assume that the size of a particular 
class sr is nr, the size of a particular cluster hj is 
nj and the size of their common instances set is 
nr,j. The precision can be calculated as follow: 
,( , ) r jr j
j
n
P s h
n
=  
The recall value can be defined as: 
,( , ) r jr j
r
n
R s h
n
=  
Then FScore of this class and cluster is defined 
to be: 
2 ( , ) ( , )
( , )
( , ) ( , )
r j r j
r j
r j r j
P s h R s h
F s h
P s h R s h
? ?= +  
The FScore of class sr, F(sr), is the maximum 
F(sr, hj) value attained by any cluster, and it is 
defined as: 
 ( ) max( ( , ))
j
r r jh
F s F s h=  
Finally, the FScore of the entire clustering solu-
tion is defined as the weighted average FScore 
of each class: 
1( )q r r
r
n F s
FScore
n=
?=?  
  Where q is the number of classes and n is the 
total number of the instances where target word 
appears. 
3.2 Tuning the Parameters 
We tune the parameters of our system on the 
training data. But because of time restrictions, 
we do not optimize these parameters. The max-
imum frequency of a word (p2) and the maxi-
mum number of the K-means? iteration (p3) are 
tuned on the training data. The minimum fre-
quency of a word (p1) was set to two following 
our intuition. The last parameter K -the number 
of the clusters is determined by the test data in 
which the target word appears. When tuning pa-
rameters, we first fixed the parameter p3 and 
found the best value of parameter p2, which 
could lead to the best performance. The results 
have been shown in Table 1 and Table 2. 
 
Parameters FScore 
P3=300,p2=35 0.7502 
P3=400,p2=40 0.7523 
P3=500,p2=40 0.7582 
Table 1: The results of K-means with SVD 
 
Parameters FScore 
P3=300,p2=40 0.7454 
P3=400,p2=40 0.7493 
P3=500,p2=45 0.7404 
Table 2: The results of K-means 
 
The performance of CW algorithm is shown 
in Table 3. The parameter p4 is a threshold for 
pruning graph as describing in section 2.2.  
Parameter FScore 
P4=0.55 0.6325 
P4=0.6 0.6321 
P4=0.65 0.6278 
P4=0.7 0.6393 
P4=0.75 0.6289 
P4=0.8 0.6345 
P4=0.85 0.6326 
P4=0.9 0.6342 
P4=0.95 0.6355 
Table 3: The results of CW. 
The result shows that the K-means algorithm 
has a better performance than CW. That may 
because CW can?t use the information of the 
number of clusters, but K-means could. Another 
problem for CW is that the size of corpus is 
small and the constructed graph can?t reflect the 
inherent relation between the instances.  
Based on the result of experiments, we em-
ployed K-means algorithm for our system and 
the parameters is shown in Table 4. 
 
Parameters Value
P1: Minimum frequency of a word 2 
P2: Maximum frequency of a word 40 
P3: Maximum number of K-means ite-
ration 
500 
K: the number of the cluster - 
Table 4: Parameters for the system. The last pa-
rameter K is provided by the test data. 
3.3 Result 
Our system participants in the CLP2010 back-
off task4 and disambiguate 100 target words, 
total 5000 instances. The F-score of our system 
on the test data is 0.7209 against the F-score 
0.7933 of the best system. 
4 Conclusion 
We have presented a model for Chinese word 
sense induction. Different strategies are applied 
to deal with Chinese ideographs and Chinese 
words that contain two or more Chinese ideo-
graphs. After selecting the features ?words, sin-
gular value decomposition is used to find the 
major axes of variation in the feature space and 
reconstruct the vector of each context. Then we 
employ k-means cluster algorithm to cluster the 
vectors of contexts. Result shows that our sys-
tem is able to induce correct senses. One draw-
back of our system is that it overlooks the infre-
quent senses because of lacking enough data. 
And our system only uses the information of 
word co-occurrences. So in the future we would 
like to integrate different kinds of information 
such as topical information, syntactic informa-
tion and semantic information, and see if we 
could get a better result. 
Acknowledgement 
This work has been partially funded by National 
Natural Science Foundation of China under 
grant #60773027, #60736044 and #90920010 
and by ?863? Key Projects #2006AA010108, 
?863? Projects #2008AA01Z145. We would like 
to thank anonymous reviewers for their detailed 
comments. 
References 
Chris Biemann, 2006.  Chinese whispers - an efficient 
graph clustering algorithm and its application to 
natural language processing problems, In Pro-
ceedings of TextGraphs, pp. 73?80, New York, 
USA. 
David Vickrey, Luke Biewald, Marc Teyssley, and 
Daphne Koller. 2005. Word-sense disambiguation 
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, 
pages 771-778, Vancouver, British Columbia, 
Canada 
Dekang Lin. 1998. Automatic retrieval and clustering 
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics, 
volume 2, pages 768-774, Montreal, Quebec, Can-
ada 
Golub, G. H. and Van Loan, C. F. 1989. Matrix 
Computations. The John Hopkins University Press, 
Baltimore, MD 
John, R., Firth. 1957. A Synopsis of Linguistic Theory 
1930-1955, pages 1-32. 
Joyce Yue Chai and Alan W. Biermann. 1999. The 
use of word sense disambiguation in an informa-
tion extraction system. In Proceedings of the six-
teenth national conference on Artificial intelli-
gence and the eleventh Innovative applications of 
artificial intelligence conference innovative appli-
cations of artificial intelligence, pages 850-855, 
Orlando, Florida, United States. 
Michael Denkowski. 2009. A Survey of Techniques 
for Unsupervised Word Sense Induction. 
Ozlem Uzuner, Boris Katz, and Deniz Yuret. 1999. 
Word sense disambiguation for information re-
trieval. In Proceedings of the sixteenth national 
conference on Artificial intelligence and the ele-
venth Innovative applications of artificial intelli-
gence conference innovative applications of artifi-
cial intelligence, page 985, Orlando, Florida, Unit-
ed States. 
Philipp Cimiano, Andreas Hotho, and Steffen Staab, 
2005.  Learning concept hierarchies from text cor-
pora using formal concept analysis, Journal of Ar-
tificial Intelligence Research (JAIR), 24, 305?339. 
Ted Pedersen, 2007. Umnd2: Senseclusters applied to 
the sense induction task of senseval-4. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations, pages 394?397, Prague, Czech 
Republic. 
Zellig Harris. 1954. Distributional Structure, pages 
146-162.  
Ying Zhao and George Karypis. 2005. Hierarchical 
clustering algorithms for document datasets. Data 
Mining and Knowledge Discovery, 10(2):141.168. 
