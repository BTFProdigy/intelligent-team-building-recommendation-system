Automatic Text Categorization by Unsupervised Learning 
Youngjoong Ko 
Department of Colnputer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, 121-742, Korea 
kyj @nlpzodiac.sogang.ac.kr, 
Jungyun Seo 
Depmlment of Computer Science, 
Sogang University 
1 Sinsu-dong, Mapo-gu 
Seoul, 121-742, Korea 
seoiy @ccs.sogang.ac.kr 
Abstract 
The goal of text categorization is to classify 
documents into a certain number of pre- 
defined categories. The previous works in 
this area have used a large number of 
labeled training doculnents for supervised 
learning. One problem is that it is difficult to 
create the labeled training documents. While 
it is easy to collect the unlabeled ocuments, 
it is not so easy to manually categorize them 
for creating traiuing documents. In this 
paper, we propose an unsupervised learning 
method to overcome these difficulties. The 
proposed lnethod divides the documents into 
sentences, and categorizes each sentence 
using keyword lists of each category and 
sentence simihuity measure. And then, it 
uses the categorized sentences for refining. 
The proposed method shows a similar 
degree of performance, compared with the 
traditional supervised learning inethods. 
Therefore, this method can be used in areas 
where low-cost ext categorization is needed. 
It also can be used for creating training 
documents. 
Introduction 
With the rapid growth of the internet, the 
availability of on-line text information has been 
considerably increased. As a result, text 
categorization has become one of the key 
techniques fox handling and organizing text data. 
Automatic text categorization in the previous 
works is a supervised learning task, defined as 
assigning category labels (pro-defined) to text 
documents based on the likelihood suggested by 
a training set of labeled doculnents. However, 
the previous learning algorithms have some 
problems. One of them is that they require a 
large, often prohibitive, number of labeled 
training documents for the accurate learning. 
Since the application area of automatic text 
categorization has diversified froln newswire 
articles and web pages to electronic mails and 
newsgroup postings, it is a difficult task to 
create training data for each application area 
(Nigam K. et al, 1998). 
In this paper, we propose a new automatic text 
categorization lnethod based on unsupervised 
learning. Without creating training documents 
by hand, it automatically creates training 
sentence sets using keyword lists of each 
category. And then, it uses them for training and 
classifies text documents. The proposed method 
can provide basic data fox" creating training 
doculnents from collected documents, and can 
be used in an application area to classify text 
documents in low cost. We use the 2 / statistic 
(Yang Y. et al, 1998) as a feature selection 
method and the naive Bayes classifier 
(McCailum A. et al, 1998) as a statistical text 
classifier. The naive Bayes classifier is one of 
the statistical text classifiers that use word 
frequencies as features. Other examples include 
k-nearest-neighbor (Yang Y. et al, 1994), 
TFIDF/Roccio (Lewis D.D. et al, 1996), 
support vector machines (Joachilns T. et al, 
1998) and decision tree (Lewis D.D. et al, 
1994). 
1 Proposal: A text categorization scheme 
The proposed system consists of three modules 
as shown in Figure 1; a module to preprocess 
collected ocuments, a module to create training 
sentence sets, and a module to extract features 
and to classify text doculnents. 
453 
............. 1\] 
i 
J 
Text (~ 
/ 
/ " 
,L',.~g,,r} 
i, I ) 
.1 
.l&ll ego 13', 
Ax~J~llJll~ ', /, 
I( ",'11 ello I'1\] 
Figurel : Architecture for the proposed system 
1.1 Preprocessing 
First, the html tags and special characters in the 
collected ocuments are removed. And then, the 
contents of the documents are segmented into 
sentences. We extract content words for each 
sentence using only nouns. In Korean, there are 
active-predicative common nouns which become 
verbs when they am combined with verb- 
derivational suffixes (e.g., ha-ta 'do', toy-la 
'become', etc.). There are also stative- 
predicative common nouns which become 
adjectives when they are combined with 
adjective-derivational suffixes such as ha. These 
derived verbs and adjectives are productive in 
Korean, and they are classified as nouns 
according to the Korean POS tagger. Other 
verbs and adjectives are not informative in many 
cases. 
1.2 Creating training sentence sets 
Because the proposed system does not have 
training documents, training sentence sets for 
each category corresponding to the training 
documents have to be created. We define 
keywords for each category by hand, which 
contain special features of each category 
sufficiently. To choose these keywords, we first 
regard category names and their synonyms as 
keywords. And we include several words that 
have a definite meaning of each category. The 
average number of keywords for each category 
is 3. (Total 141 keywords for 47 categories) 
Table 1 lists the examples of keywords for 
each category. 
Table 1: Examples of keywords for each category 
Category Keywords 
ye-hayng (trip), 
kwan-kwang 
(sightseeing) 
Um-ak(music) 
Cong-kyo 
(religion) 
Pang-song 
(broadcasting) 
ye-hayng (trip), 
kwan-kwang (sightseeing) 
Um-ak (music) 
Cong-kyo (religion), 
chen-cwu-kyo(Catholicism) 
ki-tok-kyo(Christianity), 
pwul-kyo(Buddhism) 
Pang-song (broadcasting), TV thal- 
ley-pi-cyen(television), la-ti-o(radio) 
Next, the sentences which contain pre-defined 
keywords of each category in their content 
words are chosen as the initial representative 
sentences. The remaining sentences am called 
unclassified sentences. We scale up the 
representative sentence sets by assigning the 
unclassified sentences to their related category. 
This assignment has been done through 
measuring similarities of the unclassified 
sentences to the representative sentences. We 
will elaborate this process in the next two 
subsections. 
1.2.1 Extracting and verifying representative 
sentences 
We define the representative s ntence as what 
contains pre-defined keywords of the category in 
its content words. But there exist error sentences 
in the representative sentences. They do not 
have special features of a category even though 
they contain the keywords of the category. To 
relnove such error sentences, we can rank the 
representative sentences by computing the 
weight of each sentence as follows: 
1) Word weights are computed using Term 
Frequency (TF) and Inverse Category Frequency 
(ICF) (Cho K. et al, 1997). 
@ The within-category word frequency(TF~j), 
TFij = the number of times words ti occurs 
in the j th category (1) 
? In Information Retrival, Inverse Document 
Frequency (IDF) are used generally. But a 
sentence is a processing unit in the 
proposed method. Therefore, the document 
frequency cannot be counted. Also, since 
ICF was defined by Cho K. et al (1997) 
454 
and its efficiency was verified, we use it in 
tile proposed method. ICF is computed as 
follows: 
ICF i -- Iog(M ) - I og(CF  i ) (2) 
? 
where CF is tile number of categories that 
contain t;, and M is tile total number of 
categories. 
Tile Colnbination (TFICF) of the above (9 
and ?, i.e., weight w~ i of word t; in ./tit 
category is computed as follows: 
wij --- TFii x ,cl~,. 
: Tl ' i j  X ( log(M)  - l og(CF  i ) ) (3) 
2) Using word weights (%) computed in 1), a 
sentence weight (We) in jth category are 
computed as follows: 
W Ij q- W2j +...-F WNj 
W!/ = (4) 
N 
where N is the total number of words in a 
sentence. 
3) The representative s ntences of each category 
are sorted in the decreasing order of weight, 
which was computed in 2). And then, the lop 
70% of tile representative s ntences are selected 
and used in our experiment. It is decided 
empirically. 
1.2.2 Extending representative s ntence sets 
To extend lhe representative s ntence sets, the 
unclassified sentences are classified into their 
related category through measuring similarities 
of the unclassified sentences to the 
representative s ntences. 
(l) Measurement of word and sentence 
similarities 
As similar words tend to appear in similar 
contexts, we compute the similarity by using 
contextual information (Kim H. et al, 1999; 
Karov Y. et al, 1999). In this paper, words and 
sentences play COlnplementary roles. That is, a 
sentence is represented by the set of words it 
contains, and a word by the set of sentences in 
which it appears. Sentences are simihu" to the 
extent that they contain similar words, and 
words are similar to the extent that they appear 
in similar sentences. This definition is circular. 
Titus, it is applied iteratively using two matrices 
as shown in Figure 2. in this paper, we set the 
number of iterations as 3, as is recommended by 
Karov Y. et al (1999). 
S i rn i la r~~ Wod " Sente,,C; : 
i A i d 
Figure 2: llerative computation of word and 
sentence similarities 
In Figure 2, each category has a word 
silnilarity matrix WSM,, and a sentence similarity 
matrix SSM,,. In each iteration n, we update 
WSM,, whose rows and columns are labeled by 
all content words encountered in the 
rcpresentatwe sentences of each category and 
input unclassified sentences. In that lnatrix, the 
cell (i j) hokls a value between 0 and l, 
indicating the extent to which the ith word is 
contextually similar to the jth word. Also, we 
keep and update a SSM,,, which holds similarities 
among sentences. The rows of SSM,, correspond 
to the unclassified sentences and the cohmms to 
the representative s ntences. In this paper, the 
number of input sentences of row and column in 
SSM is limited to 200, considering execution 
time and memory allocation. 
To compute tile similarities, we initialize 
WSM, to the identity matrix. That is, each word 
is fully similar (1) to itself and completely 
dissimilar (0) to other words. The following 
steps are iterated until the changes in the 
similarity values are small enough. 
1. Update the sentence similarity lnatrix SSM,,, 
using the word similarity matrix WSM,. 
2. Update the word similarity matrix WSM,,, 
using the sentence similarity matrix SSM,. 
(2) Affinity formulae 
qb simplify tile symmetric iterative treatment of 
similarity between words and sentences, we 
del'ine an auxiliary relation between words and 
sentences as affinity. A woM W is assumed to 
have a certain affinity to every sentence, which 
455 
is a real number between 0 and 1. It reflects the 
contextual relationships between W and the 
words of the sentence. If W belongs to a 
sentence S, its affinity to S is 1. If W is totally 
unrelated to S, the affinity is close to 0. If W is 
contextually similar to the words of S, its affinity 
to S is between 0 and 1. In a similar manner, a 
sentence S has some affinity to every word, 
reflecting the similarity of S to the sentences 
involving that word. 
Affinity formulae are defined as follows 
(Karov Y. et al, 1999). In these formulae, W ~ S 
means that a word belongs to a sentence: 
aft,, (W, S) = max w, es sire,, (W , W i ) 
aff,, (S, W) = max w~s; sire,, (S, S~ ) 
(5) 
(6) 
In the above formulae, n denotes the iteration 
number, and the similarity values are defined by 
WSM,, and SSM,,. Every word has some affinity 
to the sentence, and the sentence can be 
represented by a vector indicating the affinity of 
each word to it. 
(3) Similarity formulae 
The similarity of Wj to W2 is the average affinity 
of the sentences that include W~ to 1+'2, and the 
similarity of a sentence S~ to $2 is a weighted 
average of the affinity of the words in S~ to Se. 
Similarity formulae are defined as follows 
(Karov Y. et al, 1999): 
sim,,+l (Sj, S 2 ) = Z weight(W, S1 ). qlJ',, (W, S 2 ) (7) 
WE ,~'~ 
if W I =W 2 
sim,,+l (W l , W 2 ) = 1 
C/,?e 
sim"+l (Wl' W2 ) = Z weight(S, W l ). aft,, (S, W 2 ) (8) 
W~eS 
The weights in Formula 7 are computed 
following the methodology in the next section. 
The sum of weights in Formula 8, which is a 
reciprocal number of sentences that contain W, 
is !. These values are used to update the 
corresponding entries of WSM and SSM,,. 
(4) Word weights 
In Formula 7, the weight of a word is a product 
of three factors. It excludes the words that are 
expected to be given unreliable similarity values. 
The weights are not changed in their process of 
iterations. 
l. Global frequency: Frequent words in total 
sentences are less informative of sense and of 
sentence similarity. For example, a word like 
'phil-yo(necessity)' frequently appears in any 
sentence. The formula is as follows (Karov Y. 
et al, 1999): 
max{0,1 freq(W) "1 
max 5, freq(x) J
(9) 
In (9), max52\[req(x) is the sum of the five 
highest frequencies in total sentences. 
2.Log-likelihood faclor: In general, the words 
that are indicative of the sense appear in 
representative s ntences more frequently than 
in total sentences. The log-likelihood factor 
captures this tendency. It is computed as 
follows (Karov Y. et al, 1999): 
log pr(w; l w) (lO) 
Pr(Wi ) 
In (10), Pr(Wi) is estimated from the 
frequency of Wi in the total sentences, and 
Pr(WilW) fi'om the frequency of Wi in 
representative sentences. To avoid poor 
estimation for words with a low count in 
representative s ntences, we nmltiply the log- 
likelihood by (11) where count(Wi) is the 
number of occurrences of Wi in representative 
sentences. For the words which do not appear 
in representative s ntences, we assign weight 
(1.0) to them. And the other words are 
assigned weight that adds 1.0 to computed 
value: 
c?unt(Wi) t (11) min. 1, 3 
3.Part of ,q~eech: Each part of speech is 
assigned a weight. We assign weight (1.0) to 
proper noun, non-predicative common noun, 
and foreign word, and assign weight (0.6) to 
active-predicative common noun and stative- 
predicative common noun. 
456 
The total weight of a word is the product of the 
above t'actors, each norlnalized by the sum of 
factors of the words in a sentence as follows 
(Karov Y. et al, 1999): 
,&ctor(Wi, S) 
weight 
? J 'actor(Wi, S)  
IVieS 
(12) 
In (12), factor(W, S) is the weight before 
normalization. 
(5) Assigning unclassified sentences to a 
category 
We first computed similarities of the 
unclassified sentences to the representative 
sentences. And then, we decided a Silnilarity 
value o1' each unclassified sentence for each 
category using two alternate ways. 
1 tl sint(X,ci)=-- ? Sil!l (X,Sj) (13) 
tiE(' I1 . ( ,'~'jcR,., 
j=  J 
sinl(X,ci)=nlnxl.siul(X Si)} (14) 
(:'~(" I SjcRc, 
In (13) and (14), i) X is au unclassified sentence, 
ii) C = {c l,c2 ..... c,,,} is a category set, and iii) 
R,,,={&,Sa ...... S',,} is a representative sentence 
set of category c.. 
Each unclassified sentence is assigned to a 
category which has a maxinmln similarity wflue. 
But there exist unclassified sentences which do 
not belong to any category. To remove these 
unclassified sentences, we set up a threshold 
value using normal distribution of similarity 
values as follows: 
max{sim(X,c i )  } >_ tt + 017 (15) 
ciEC 
In (15), i) X is an unclassified sentence, ii) It is 
an average of similarity wflues, iii) o is a 
standard eviation of similarity wdues, and iv) 0 
is a numerical wdue corresponding to 
threshold(%) in normal distribution table. 
1.3 Feature selection and text classifier 
1.3.1 Feature Selection 
The size of the vocabulary used in our 
experiment is selected by ranking words 
according to their Z 2 statislic with respect o the 
category. Using the two-way contingency table 
of a word t and a category c - i) A is the number 
of times t and c co-occur, ii) B is the number of 
times t occurs without c, iii) C is the number of 
times c occurs without t, iv) D is the number ot' 
times ueither c nor t occurs, and vi) N is the total 
number of sentences - the word-goodness 
measure is defined as follows (Yang Y. et al, 
1997): 
Z2(t,c) = N?(AD-CB)2 (16) 
(A + C)(B + D)(A + B)(C + D) 
To measure the goodness of a word in a 
global feature selection, we combine the 
category-specific s ores of a word as follows: 
I I I  9 2 ZF,,.,~, (t) = n~a,x{ Z .= (t, (:~)} (17) 
1.3.2 Text classifier 
The method that we use for classifying 
documents is uaivc Bayes, with minor 
modifications based on Kullback-Leibler 
Divergence (Craven M. et al, 1999). The basic 
idea in naive Bayes approaches i to use the joint 
probabilities of words and categories to estimate 
the probabilities of categories given a document. 
Given a document d for chtssit'ication, we 
calculate the probabilities of each category c as 
follows: 
Pr(cld) Pr(c) Pr(d lc) 7' _ _ P r (c )H Pr(t i Ic.) N(rM~ 
Pr(d) i< 
T !c)) 
,, ;=, Id) ) 
In tile above l'ormula, i) 11, is the number of 
words in d, ii) N(t~ld) is the frequency of woM t i 
in clocument d, iii) 7" is the size of tile 
vocabulary, and iv) t~ is tile ith word in the 
vocabulary. Pr(tAc ) thus represents the 
probability that a randomly drawn woM from a 
randolnly drawn docmnent in category c will be 
the word 6 Pr(tild) represents the proportion of 
woMs in docmnent d that are word t c Each 
probability is estimated by formulae (19) and 
(20), which are called the expected likelihood 
457 
estimator (Li H. et al, 1997). The category 
predicted by the method for a given document is 
simply the category with the greatest score. This 
method performs exactly the same 
classifications as naive Bayes does, but produces 
classification scores that are less extreme. 
N(ti, c) + 0.5 
Pr(ti \[ c) = T( (l 9) 
Z N(t i' c) + 0.5 x T c 
j=l 
Pr(t i id) = N( t . i ,d )+O.5xT ,  z (20) 
0 if N(t i ,d) :  0 
2 Evaluation of experiment 
2.1 Performance measures 
In this paper, a document is assigned to only one 
category. We use the standard definition of 
recall, precision, and F, measure as performance 
measures. For evaluating performance average 
across categories, we use the micro-averaging 
method. F~ measure is defined by the following 
formula (Yang Y. et al, 1997): 
2 q) 
F 1 ( r ,  p )  - (21 ) 
r+ p 
where r represents recall and p precision. It 
balances recall and precision in a way that gives 
them equal weight. 
2.2 Experiment settings 
We used total 47 categories in our experiment. 
They consist of 2,286 documents to be collected 
in web. We did not use tag information of web 
documents. And a so-called bag of words or 
unigraln representation was used. Table 2 shows 
the settings of experiment data in detail. 
Table 2: Setting experiment data 
........................... i i avg# avg # of I #of '  #of  I .... of doc, sen. . . . . .  d0c  sen: ...... ~ .......... ~ inacat, inadoc. 
Training 1 ,383 67,506 29.4 48.8 
Set (60%) 
903 Test Set 56,446 19.2 62.5 (40%) 
2.3 Prinmry results 
2.3.1 Results of the different combinations of 
similarity value decisions and thresholds 
We evaluated our method according to the 
different combinations of similarity value 
decisions and thresholds in section 1.2.2. We 
used thresholds of top 5%, top 10%, top 15%, 
top20% in formula (15), and tested the two 
options, average and maximum in formulae (13) 
and (14). We limited our vocabulary to 2,000 
words in this experiment. 
+Close  Test(max) -~N-~Close Test(avg) 
+Open Test(max) ---')(~Open Test(avo) 
0.73 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
0.72 
UL 0.71 
0.7 
0.69 
E 0.68 
0.67 
0.66 
5% 10% 15% 20% 
Threshold(%) 
Figure 3: Results of the different combinations of 
similarity wdue decisions and thresholds 
Figure 3 shows results according to the two 
options in each threshold. Here, the result using 
maxinmm was better than that using average 
with regrad to all thresholds. The results of top 
10% and top 15% were best. Therefore, we used 
the maximum in the decision of similarity value 
and top 15% in threshold in our experiments. 
2.3.2 The proposed system vs. the system by 
supervised learning 
For the fair evaluation, we embodied a 
traditional system by supervised learning using 
the same feature selection method (2/ statistic) 
and classifier (naive Bayes Classifier), as used in 
the proposed system. And we tested these 
systems and compared their performance: 
458 
+method by supervised learning ' -~t r -p roposed  method 
0.8 .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
0.770 
0.75 
0.725 
E. o7 
>~ 0.675 
0.05 
E 0.0 
0675 
0.!i5 
0,525 
6.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Vooabulary Size 
lqgure 4: Comparison of the proposed system and the 
syslem by supervised learning 
Figure 4 displays the performance urves for the 
proposed system and the system by supervised 
learning. The best F~ score of the proposed 
system is 71.8% and that of the system by 
supervised learning is 75.6%. Therefore, the 
difference between them is only 3.8%. 
Conclusion 
This paper has described a new automatic text 
categorization method. This method 
automatically created training sets using 
keyword lists of each category and used them 
for training. And then, it classified text 
documents. This could be a significant method 
in text learning because of the high cost of hand- 
labeling training docmnents and the awfilability 
of huge volumes of unlabeled ocuments. The 
experiment results showed that with respect o 
performance, the difference between the 
proposed method and the method by supervised 
learning is insignificant. Therefore, this method 
can be used in areas where low-cost text 
categorization is required, and can be used for 
creating training data. 
This study awaits further research. First, a 
more scientific approach for defining keyword 
lists should be investigated. Next, if we use a 
word sense disambiguation systeln in the 
extraction step of representative s ntences, we 
would be able to achieve a better performance. 
Acknowledgments  
This work was supported by KOSEF under 
Grant No. 97-0102-03-01-3. We wish to thank 
Jeoung-seok Kiln for his valuable COlnments to 
the earlier version of this paper. 
Refel-elices 
Cho K. and Kim J. (1997) Automatic Text 
Categorization on Hierarchical Category Structure 
by using ICF(Inverted Category Frequency) 
Weighting. In Proceedings of KISS coqference, 
pp.507-510. 
Craven M., DiPasquo D., Freitag I)., McCallum A., 
Mitchell T., Nigam K. and Slauery S. (1999) 
l,earning to Conslruct Knowledge Bases from lhe 
World Wide Web. to appear in Artificial 
httelligence. 
Joachims T. (1998)Text Categorization with Supporl 
Vector Machines: Learning with Many Relevant 
Features. In European Conference on Machine 
Learning(ECML). 
Karov Y. and 17,dehnan S. (1998) Similarity-based 
Word Sense l)isambiguation. Computational 
Linguistics, Vol 24, No I, pp. 41-60. 
Kim H., KEY., Park S. and See J. (1999) hfformal 
Requirements Analysis St,1)porling System for 
Htlulall Engineer. 111 Proceedings of Conference m~ 
IEEE- SMC99. Vol 3, pp. 1013-1018. 
Lewis D.D. and Ringuette M. (t994) A comparison 
of '\['wo 1,earning Algorithms for Text categorizalion. 
In Proceeding of the 3 ''~ Ammal &,ml~osium o~ 
Document Attalysis and h{/brmation Retrieval. 
Lewis I).D., Schapire P,.E., Calhm J.P. and Papka 
P,.(1996) Training Algorilhms for IJnear Text 
Classifiers. In Proceedings of the 19" htter~tatiomtl 
Coqference on Research and Deveh)l)ment in 
h!/btwtation Retrieval (SIGIR'96), pp. 289-297. 
Li H. and Yamanishi K. (1997) Document 
Classification Using a Finite Mixture Model. The 
Association for Co,qmtatiomtl Littguistics, 
ACE '97. 
McCallum A. and Nigram K. (1998) A comparison 
of Event Models for Naive Bayes Text 
Classification. AAAI '98 workshop on Leanting for 
7kvt Categorization. 
Nigam K., McCallum A., Thrun S. and Mitchell T. 
(1998) Learning to Classify Text from Labeled and 
Unlabeled l)oeuments. In Proceedings of 15" 
National Conference on Artificial httelligence 
(AAAI-98). 
Yang Y. (1999) An ewduation of statistical 
approaches to text categol"ization, ht.formation 
Retrieval Journal, May. 
Yaug Y. (1994) Expert netword: Effective and 
efficient learning fi'om human decisions in text 
catego,izatin and retriewfl. In 17" Ammal 
lnternatiomd A CM SIG1R Conference on Research 
and Development in hi formation Retrieval 
(SIGIR'94), pp. 13-22. 
Yang Y. and Pederson J.O. (1997) A comparative 
study on feature selection in text categorization, ht
Proceedings of the 14" International Conference on 
Machine Learning. 
459 
Text Categorization using Feature Projections
Youngjoong Ko
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
kyj@nlpzodiac.sogang.ac.kr,
Jungyun Seo
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
seojy@ccs.sogang.ac.kr
Abstract
This paper proposes a new approach for text
categorization, based on a feature projection
technique. In our approach, training data are
represented as the projections of training
documents on each feature. The voting for a
classification is processed on the basis of
individual feature projections. The final
classification of test documents is
determined by a majority voting from the
individual classifications of each feature.
Our empirical results show that the proposed
approach, Text Categorization using Feature
Projections (TCFP), outperforms k-NN,
Rocchio, and Na?ve Bayes. Most of all,
TCFP is about one hundred times faster than
k-NN. Since TCFP algorithm is very simple,
its implementation and training process can
be done very easily. For these reasons,
TCFP can be a useful classifier in the areas,
which need a fast and high-performance text
categorization task.
Introduction
An issue of text categorization is to classify
documents into a certain number of pre-defined
categories. Text categorization is an active
research area in information retrieval and
machine learning. A wide range of supervised
learning algorithms has been applied to this
issue, using a training data set of categorized
documents. The Na?ve Bayes (McCalum et al,
1998; Ko et al, 2000), Nearest Neighbor (Yang
et al, 2002), and Rocchio (Lewis et al, 1996)
are well-known algorithms.
Among these learning algorithms, we focus
on the Nearest Neighbor algorithm. In particular,
the k-Nearest Neighbor (k-NN) classifier in text
categorization is one of the state-of-the-art
methods including Support Vector Machine
(SVM) and Boosting algorithms. Since the
Nearest Neighbor algorithm is much simpler
than the other algorithms, the k-NN classifier is
intuitive and easy to understand, and it learns
quickly. But the weak point of k-NN is too slow
at running time. The main computation is the
on-line scoring of all training documents, in
order to find the k nearest neighbors of a test
document. In order to reduce the scaling
problem in on-line ranking, a number of
techniques have been studied in the literature.
Techniques such as instance pruning technique
(Wilson et al, 2000) and projection (Akkus et al,
1996) are well known.
The instance pruning technique is one of the
most straightforward ways to speed
classification in a nearest neighbor system. It
reduces time necessary and storage requirements
by removing instances from the training set. A
large number of such reduction techniques have
been proposed, including the Condensed Nearest
Neighbor Rule (Hart, 1968), IB2 and IB3 (Aha et
al., 1991), and the Typical Instance Based
Learning (Zhang, 1992). These and other
reduction techniques were surveyed in depth in
(Wilson et al, 1999), along with several new
reduction techniques called DROP1-DROP5. Of
these, DROP4 had the best performance.
Another trial to overcome this problem exists
on feature projections. Akkus and Guvenir
presented a new approach to classification based
on feature projections (Akkus et al, 1996). They
called their resulting algorithm k-Nearest
Neighbor on Feature Projections (k-NNFP). In
this approach, the classification knowledge is
represented as the sets of projections of training
data on each feature dimension. The
classification of an instance is based on a voting
by the k nearest neighbors of each feature in a
test instance. The resulting system allowed the
classification to be much faster than that of
k-NN and its performance were comparable with
k-NN.
In this paper, we present a particular
implementation of text categorization using
feature projections. When we applied the feature
projection technique to text categorization, we
found several problems caused by the special
properties of text categorization problem. We
describe these problems in detail and propose a
new approach to solve them. The proposed
system shows the better performance than k-NN
and it is much faster than k-NN.
The rest of this paper is organized as follows.
Section 1 simply presents k-NN and k-NNFP
algorithm. Section 2 explains a new approach
using feature projections. In section 3, we
discuss empirical results in our experiments.
Section 4 is devoted to an analysis of time
complexity and strong points of the new
proposed classifier. The final section presents
conclusions.
1. k-NN and k-NNFP Algorithm
In this section, we simply describe k-NN and
k-NNFP algorithm.
1.1 k-NN Algorithm
As an instance-based classification method,
k-NN has been known as an effective approach
to a broad range of pattern recognition and text
classification problems (Duda et al, 2001; Yang,
1994). In k-NN algorithm, a new input instance
should belong to the same class as their k nearest
neighbors in the training data set. After all the
training data is stored in memory, a new input
instance is classified with the class of k nearest
neighbors among all stored training instances.
For the distance measure and the document
representation, we use the conventional vector
space model in text categorization; each
document is represented as a vector of term
weights, and similarity between two documents
is measured by the cosine value of the angle
between the corresponding vectors (Yang et al,
2002).
Let a document d with n terms (t) be
represented as the feature vector:
>=< ),(),...,,(),,( 21 dtwdtwdtwd n
rrrr (1)
We compute the weight vectors for each
document using one of the conventional TF-IDF
schemes (Salton et al, 1988). The weight of
term t in document d is calculated as follows:
d
nNdttfdtw tr
rr )/log()),(log1(),( ?+= (2)
where
i) ),( dtw
r
is the weight of term t in document d
r
ii) ),( dttf
r
is the within-document Term Frequency (TF)
iii) )/log(
t
nN is the Inverted Document Frequency
(IDF)
iv) N is the number of documents in the training set
v) nt is the number of training documents in which t
occurs
vi) ? ?= dt dtwd r rr 2),( is the 2-norm of vector dr
Given an arbitrary test document d, the k-NN
classifier assigns a relevance score to each
candidate category cj using the following
formula:
?
??
?=
jk DdRd
j dddcs
I
rr
rrr
)(
),cos(),( (3)
where )(dRk
r
denotes a set of the k nearest
neighbors of document d and Dj is a set of
training documents in class cj.
1.2 k-Nearest Neighbor on Feature Projection
(k-NNFP) Algorithm
The k-NNFP is a variant of k-NN method. The
main difference is that instances are projected on
their features in the n-dimensional space (see
figure 1) and distance between two instances is
calculated according to a single feature. The
distance between two instances di and dj with
regard to m-th feature tm is distm(tm(i), tm(j)) as
follows:
),(),())(),(( jmimmm dtwdtwjtitdistm
rr
?= (4)
where )(itm denotes m-th feature t in a instance
id
r
.
The classification on a feature is done
according to votes of the k-nearest neighbors of
that feature in a test instance. The final
classification of the test instance is determined
by a majority voting from individual
classification of each feature. If there are n
features, this method returns n? k votes whereas
k-NN method returns k votes.
2. A New Approach of Text
Categorization on Feature Projections
First of all, we show an example of feature
projections in text categorization for more easy
understanding. We then enumerate the problems
to be duly considered when the feature
projection technique is applied to text
categorization. Finally, we propose a new
approach using feature projections to overcome
these problems.
2.1 An Example of Feature Projections in
Text Categorization
We give a simple example of the feature
projections in text categorization. To simplify
our description, we suppose that all documents
have just two features (f1 and f2) and two
categories (c1 and c2). The TF-IDF value by
formula (2) is used as the weight of a feautre.
Each document is normalized as a unit vector
and each category has three instances:
{ }3211 ,, dddc = and { }6542 ,, dddc = . Figure 1
shows how document vectors in conventional
vector space are transformed into feature
projections and stored on each feature dimension.
The result of feature projections on a term (or
feature) can be seen as a set of weights of
documents for the term. Since a term with 0.0
weight is useless, the size of the set equals to the
DF value of the term.







	














	



	

			
	

	

		
	

















	








	


 



 



 



 



 

	
	
	

	
	
	



	
	
Figure 1. Feature representation on feature
projections
2.2 Problems in Applying Feature Projections
to Text Categorization
There are three problems: (1) the diversity of the
Document Frequency (DF) values of terms, (2)
the property of using TF-IDF value of a term as
the weight of the feature, and (3) the lack of
contextual information.
2.2.1 The diversity of the Document Frequency
values of terms
Table 1 shows a distribution of the DF values of
the terms in Newsgroup data set. The numerical
values of Table 1 are calculated from training
data set with 16,000 documents and 10,000
features chosen by feature selection. The k in
fourth column means the number of nearest
neighbors selected in k-NNFP; the k in k-NNFP
was set to 20 in our experiments.
Table 1. A distribution of the DF values of the terms
in Newsgroup data set
Average
DF
maximum
DF
Minimum
DF
The # of
features
DF < k (20)
54.59 8,407 4 6,489
According to Table 1, more than a half of the
features have the DF values less than k (20).
This result is also explained by Zipf?s law. The
problem is that some features have the DF
values less than k while other features have the
DF values much greater than k. For a feature that
has a DF value less than k, all the elements of
the feature projections on the feature could and
should participate for voting. In this case, the
number of elements chosen for voting is less
than k. For other features, only maximum k
elements among the elements of the feature
projections should be chosen for voting.
Therefore, we need to normalize the voting ratio
for each feature. As shown in formula (5), we
use a proportional voting method to normalize
the voting ratio.
2.2.2 The property of using TF-IDF value of a
term as weight of a feature
The TF-IDF value of a term is their presumed
value for identifying the content of a document
(Salton et al, 1983). On feature projections,
elements with a high TF-IDF value for a feature
become more useful classification criterions for
the feature than any elements with low TF-IDF
values. Thus we use only elements with TF-IDF
values above the average TF-IDF value for
voting. The selected elements also participate for
proportional voting with the same importance as
TF-IDF value of each element. The voting ratio
of each category cj in a feature tm(i) of a test
document id
r
is calculated by the following
formula:
??
??
?=
mmmm
jj
Ilt
lm
Ilt
mlmm dtwltcydtwitcr
)()(
),())(,(),())(,(
rr (5)
In above formula, Im denotes a set of elements
selected for voting and { }1.0))(,( ?ltcy mj is a
function; if the category for a element )(ltm is
equal to jc , the output value is 1. Otherwise, the
output value is 0.
2.2.3 The lack of contextual information
Since each feature votes separately on feature
projections, contextual information is missed.
We use the idea of co-occurrence frequency for
applying contextual information to our
algorithm.
To calculate a co-occurrence frequency value
between two terms ti and tl, we count the number
of documents that include both terms. It is
separately calculated in each category of training
data. Finally, the co-occurrence frequency value
of two terms is obtained by a maximum value
among co-occurrence frequency values in each
category as follows:
{ }),,(max),( jli
c
li cttcottco
j
= (6)
where ),( li ttco denotes a co-occurrence
frequency value of ti and tl, and
),,( jli cttco denotes a co-occurrence frequency
value of ti and tl in a category cj.
TF-IDF values of two terms ti and tj, which
occur in a test document d, are modified by
reflecting the co-occurrence frequency value.
That is, the terms with a high co-occurrence
frequency value and a low category frequency
value could have higher term weights as follows:
where i) tw(ti,d) denotes a modified term weight
assigned to term ti, ii) cf denotes the category
frequency, the number of categories in which ti
and tj co-occur, and iii) ),(max jttco i is the
maximum value among all co-occurrence
frequency values.
Finally, in order to apply these improvements
(formulae (5) and (7)) to our algorithm, we
calculate the voting score of each category jc
in mt of a test document id
r
as the following
formula:
))(,(),())(,( itcrdttwitcs mimm jj ?=
r
(8)
Here, since the modified TF-IDF value of a
feature in a test document has to be also
considered as an important factor, it is used for
voting score instead of the simple voting value
(1).
(7))),(log(max1
)),(log(1
)log(1
11),(),( ???
?
???
?
???
?
???
?
+
+
????
?
???
?
+
+?=
ji
ji
ii ttco
ttco
cfdtwdttw
rr
2.3 A New Text Categorization Algorithm
using Feature Projections
A new text categorization algorithm using
feature projections, named TCFP, is described
in the following:
In training phase, our algorithm needs only a
very simple process; the training documents are
projected on their each feature and numerical
values for the proportional voting (formula (5))
are calculated.
3. Empirical Evaluation
3.1 Data Sets and Experimental Settings
To test our proposed approach, we used two
different data sets. For fair evaluation, we used
the five-fold cross-validation method. Therefore,
all results of our experiments are averages of
five runs.
The Newsgroups data set, collected by Ken
Lang, contains about 20,000 articles evenly
divided among 20 UseNet discussion groups
(McCalum et al, 1998). After removing words
that occur only once or on a stop word list, the
average vocabulary from five training data has
51,325 words (with no stemming). The second
data set comes from the WebKB project at
CMU (Yang et al, 2002). We use the four most
populous entity-representing categories: course,
faculty, project, and student. The resulting
data set consists of 4,198 pages with a
vocabulary of 18,742 words. It is an uneven data
set; the largest category has 1,641 pages and the
smallest one has 503 pages.
We applied statistical feature selection at a
preprocessing stage for each classifier, using a
2? statistics (Yang et al, 1997).
To compare TCFP to other algorithms for
speeding classification, we implemented
k-NNFP and k-NN with reduction. We used
DROP4 as reduction technique (Wilson et al,
1999). By DROP4, only 26% of the original
training documents in both data sets was
retained. The k in k-NNFP was set to 20 and
the k in k-NN with reduction was set to 30. In
addition, we implement other classifiers: Naive
Bayes, k-NN, and Rocchio classifier. The k in
k-NN was set to 30 and and ?=16 and ?=4 were
used in Rocchio classifier.
As performance measures, we followed the
standard definition of recall, precision, and F1
measure. For evaluating performance average
across categories, we used the micro-averaging
method.
3.2 Experimental Results
3.2.1 Comparison of TCFP and k-NN (and
other algorithms for speeding classification )
Figure 2 and Table 2 show results from TCFP,
k-NN, k-NN with reduction, and k-NNFP. In
addition, we added other type of TCFP to our
experiment. It was TCFP without contextual
information (not using formula (7)).








    	    
 









 	


 
	

Figure 2. Comparison of TCFP , k-NN, k-NNFP, and
k-NN with reduction
test document: d
r
=<t1,t2,?,tn>, category set:
C={c1,c2,?,cm}
begin
for each category cj
vote[cj] =0
for each feature ti
tw(ti,d) is calculated by formula (7)
/* majority voting*/
for each feature ti
for each category cj
vote[cj]=vote[cj]+tw(ti,d)?r(cj,ti)
by formula (8)
for each category cj
prediction = ][maxarg j
c
cvote
j
return prediction
end
Table 2. The top micro-average F1 of each classifier
TCFP
TCFP
without
context
k-NN k-NNFP
k-NN
with
reduction
85.41 85.14 85.15 81.93 81.34
As a result, TCFP achieved the highest
micro-average F1 score. Also, TCFP without
contextual information presented the nearly
same performance as k-NN. Although, over all
vocabulary sizes, TCFP without contextual
information achieved little lower performance
than TCFP, it also can be useful classifier for its
simplicity and the fast running time(see Table 5).
3.2.2 Comparison with other classifiers
The comparisons with other classifiers are
shown in Figure 3 and Table 3. In this
experiment, we used Na?ve Bayes, and Rocchio
classifier.









    	    
 









	




   	


Figure 3. Comparison with other classifiers
Table 3. The top micro-average F1 of each classifier
TCFP k-NN NB Rocchio
85.41 85.15 82.51 81.68
The result shows that TCFP produced the higher
performance than the other classifiers.
3.2.3 Comparison of performances in an
uneven data set, WebKB.
In the above experiments, the Newsgroup data
set, which is an evenly divided data set, was
used. If we use an uneven data set, we can face a
problem. The cause of the problem is that a
category of the larger size has more voting
candidates than a category of the smaller size.
We simply modified the majority voting score
calculated in TCFP algorithm by the following
formula:
{ } ??
???
?
?= ),(/),(max][][ ji
c
jj cdnumcdnumcvotecvote
i
(9)
where num(d,cj) denotes the number of training
document in category cj.
The results of the modified algorithm are
shown in Table 4. As we can see in this table, the
modified TCFP algorithm performed similarly
on the uneven data set, WebKB; the modified
TCFP algorithm achieved the highest score.
Table 4. The top micro-average F1 of each classifier
TCFP k-NN NB Rocchio k-NNFP
k-NN
with
reduction
86.6 84.83 85.22 85.98 82.78 81.34
3.2.4 Run-time observation
Table 5 shows the average running times in CPU
seconds for each classifier on the Newsgroup
data. Note that we included only testing phase
with 4,000 documents.
Table 5. Average running time of each classifier
TCFP
without
context
Rocchio NB TCFP
k-NN
with
reduction
k-NN
0.69 0.8 1.22 1.38 37.97 142.5
Since the computations depend on the
vocabulary sizes, we calculated the above
numerical value by averaging running times
from 1,000 to 10,000 terms. In Table 5, the
running time of TCFP is similar to other faster
classifiers: Rocchio and Na?ve Bayes. Also it is
about one hundred times faster than that of k-NN.
Note that TCFP without contextual information
is the fastest classifier.
4. Discussions
First of all, time complexities between k-NN and
TCFP are compared. Using the inverted-file
indexing of training documents, the time
complexity of k-NN is O(m2l/n) (Yang, 1994),
where m is the number of unique words in the
document, l is the number of training documents,
and n is the number of unique terms in the
training collection. TCFP has the time
complexity of O(m2). Even more, the time
complexity of TCFP without contextual
information is O(mc), where c is the number of
categories. That is, the classification of TCFP
requires a simple calculation in proportion to the
number of unique terms in the test document.
On the other hand, in k-NN, a search in the
whole training space must be done for each test
document.
The other strong points of TCFP are the
simplicity of algorithm and high-performance.
Since the algorithm of TCFP is very simple like
k-NN, TCFP can be implemented quite easily
and its training phase can also be a simple
process. In our experiments, we achieved the
better performance than k-NN. We analyze that
our algorithm is more robust from irrelevant
features than k-NN. When a document contains
irrelevant features, the angle of the document
vector is changed in k-NN. In TCFP, however,
the irrelevant features contribute to only voting
of the features. Hence TCFP decreases the bad
effect of the irrelevant features.
Conclusions
In this paper, a new type of text categorization,
TCFP, has been presented. This algorithm has
been compared with k-NN and other classifiers.
Since each feature in TCFP individually
contributes to the classification process, TCFP is
robust from irrelevant features. By the simplicity
of TCFP algorithm, its implementation and
training process can be done very easily. The
experimental results show that, on the
performance, TCFP is superior to Rocchio,
Na?ve Bayes, and k-NN. Moreover, it
outperforms other classifiers for speeding
classification such as k-NNFP and k-NN with
reduction. In running time observation, TCFP is
about one hundred times faster than k-NN.
Therefore, we can use TCFP in the areas, which
require a fast and high-performance text
classifier.
References
Aha, D. W., Dennis K., and Marc K. A. (1991)
Instance-Based Learning Algorithms. Machine
Learning, vol. 6, pp. 37-66.
Akkus A. and Guvenir H.A. (1996) K Nearest
Neighbor Classification on Feature Projections. In
Proceedings of ICML? 96, Itally, pp. 12-19.
Duda R.O., Hart P.E., and Stork D.G. (2001) Pattern
Classification. John Wiley & Sons, Second Edition.
Hart, P. E. (1968) The Condensed Nearest Neighbor
Rule. Institute of Electrical and Electronics
Engineers Transactions on Information Theory. Vol.
14, pp. 515-516.
Ko Y. and Seo J. (2000) Automatic Text
Categorization by Unsupervised Learning. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING), pp. 453-459.
Lewis D.D., Schapire R.E., Callan J.P., and Papka R.
(1996) Training Algorithms for Linear Text
Classifiers. In Proceedings of the 19th International
Conference on Research and Development in
Information Retrieval (SIGIR?96), pp.289-297.
McCallum A. and Nigam K. (1998) A Comparison of
Event Models for Na?ve Bayes Text Classification.
AAAI ?98 workshop on Learning for Text
Categorization. pp. 41-48.
Salton G. and McGill M.J. (1983) Introduction to
Modern Information Retrieval. McGraw-Hill, Inc.
Salton G. and Buckley C. (1988) Term weighting
approaches in automatic text retrieval. Information
Processing and Management, 24:513-523.
Wilson D. R. and Martinez T. R. (2000) An
Integrated Instance-based Learning Algorithm,
Computational Intelligence, Volume 16, Number 1,
pp. 1-28.
Wilson, D. R. and Martinez T. R. (2000) Reduction
Techniques for Exemplar-Based Learning
Algorithms. Machine Learning, vol. 38, no. 3, pp.
257-286.
Yang Y. (1994) Expert network: Effective and
efficient learning from human decisions in text
categorization and retrieval. In Proceedings of 17th
International ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR?94), pp 13-22.
Yang Y. and Pedersen J.P. (1997) Feature selection
in statistical learning of text categorization. In The
Fourteenth International Conference on Machine
Learning, pages 412-420.
Yang Y., Slattery S., and Ghani R. (2002) A study of
approaches to hypertext categorization, Journal of
Intelligent Information Systems, Volume 18,
Number 2.
Zhang, J. (1992) Selecting Typical Instances in
Instance-Based Learning. Proceedings of the Ninth
International Conference on Machine Learning.
Automatic Text Categorization using the Importance of Sentences
Youngjoong Ko, Jinwoo Park, and Jungyun Seo
Department of Computer Science,
Sogang University
1 Sinsu-dong, Mapo-gu
Seoul, 121-742, Korea
{kyj,jwpark}@nlpzodiac.sogang.ac.kr, seojy@ccs.sogang.ac.kr
Abstract
Automatic text categorization is a problem
of automatically assigning text documents to
predefined categories. In order to classify
text documents, we must extract good
features from them. In previous research, a
text document is commonly represented by
the term frequency and the inverted
document frequency of each feature. Since
there is a difference between important
sentences and unimportant sentences in a
document, the features from more important
sentences should be considered more than
other features. In this paper, we measure the
importance of sentences using text
summarization techniques. Then a document
is represented as a vector of features with
different weights according to the
importance of each sentence. To verify our
new method, we conducted experiments on
two language newsgroup data sets: one
written by English and the other written by
Korean. Four kinds of classifiers were used
in our experiments: Na?ve Bayes, Rocchio,
k-NN, and SVM. We observed that our new
method made a significant improvement in
all classifiers and both data sets.
Introduction
The goal of text categorization is to classify
documents into a certain number of pre-defined
categories. Text categorization is an active
research area in information retrieval and
machine learning. A wide range of supervised
learning algorithms has been applied to this
problem using a training data set of categorized
documents. For examples, there are the Na?ve
Bayes (McCallum et al, 1998; Ko et al, 2000),
Rocchio (Lewis et al, 1996), Nearest Neighbor
(Yang et al, 2002), and Support Vector
Machines (Joachims, 1998).
A text categorization task consists of a
training phase and a text classification phase.
The former includes the feature extraction
process and the indexing process. The vector
space model has been used as the conventional
method for text representation (Salton et al,
1983). This model represents a document as a
vector of features using Term Frequency (TF)
and Inverted Document Frequency (IDF). This
model simply counts TF without considering
where the term occurs. But each sentence in a
document has different importance for
identifying the content of the document. Thus,
by assigning a different weight according to the
importance of the sentence to each term, we can
achieve better results. For this problem, several
techniques have been studied. First, term
weights were differently weighted by the
location of a term, so that the structural
information of a document was applied to term
weights (Murata et al, 2000). But this method
supposes that only several sentences, which are
located at the front or the rear of a document,
have the important meaning. Hence it can be
applied to only documents with fixed form such
as articles. The next technique used the title of a
document in order to choose the important terms
(Mock et al, 1996). The terms in the title were
handled importantly. But a drawback of this
method is that some titles, which do not contain
well the meaning of the document, can rather
increase the ambiguity of the meaning. This case
often comes out in documents with a informal
style such as Newsgroup and Email. To
overcome these problems, we have studied text
summarization techniques with great interest.
Among text summarization techniques, there are
statistical methods and linguistic methods
(Radev et al, 2000; Marcu et al, 1999). Since
the former methods are simpler and faster than
the latter methods, we use the former methods to
be applied to text categorization. Therefore, we
employ two kinds of text summarization
techniques; one measures the importance of
sentences by the similarity between the title and
each sentence in a document, and the other by
the importance of terms in each sentence.
In this paper, we use two kinds of text
summarization techniques for classifying
important sentences and unimportant sentences.
The importance of each sentence is measured by
these techniques. Then term weights in each
sentence are modified in proportion to the
calculated sentence importance. To test our
proposed method, we used two different
newsgroup data sets; one is a well known data
set, the Newsgroup data set by Ken Lang, and
the other was gathered from Korean UseNet
discussion group. As a result, our proposed
method showed the better performance than
basis system in both data sets.
The rest of this paper is organized as follows.
Section 1 explains the proposed text
categorization system in detail. In section 2, we
discuss the empirical results in our experiments.
Section 3 is devoted to the analysis of our
method. The final section presents conclusions
and future works.
1. The Proposed Text Categorization
System
The proposed system consists of two modules as
shown in Figure 1: one module for training
phase and the other module for text
classification phase. The each process of Figure
1 is explained in the following sections.


	








? 
	

		




	




Learning with Unlabeled Data for Text Categorization Using Bootstrapping  
and Feature Projection Techniques 
Youngjoong Ko 
Dept. of Computer Science, Sogang Univ. 
Sinsu-dong 1, Mapo-gu 
Seoul, 121-742, Korea 
kyj@nlpzodiac.sogang.ac.kr 
Jungyun Seo 
Dept. of Computer Science, Sogang Univ. 
Sinsu-dong 1, Mapo-gu 
Seoul, 121-742, Korea 
    seojy@ccs.sogang.ac.kr 
 
Abstract 
A wide range of supervised learning 
algorithms has been applied to Text 
Categorization. However, the supervised 
learning approaches have some problems. One 
of them is that they require a large, often 
prohibitive, number of labeled training 
documents for accurate learning. Generally, 
acquiring class labels for training data is costly, 
while gathering a large quantity of unlabeled 
data is cheap. We here propose a new 
automatic text categorization method for 
learning from only unlabeled data using a 
bootstrapping framework and a feature 
projection technique. From results of our 
experiments, our method showed reasonably 
comparable performance compared with a 
supervised method. If our method is used in a 
text categorization task, building text 
categorization systems will become 
significantly faster and less expensive. 
1 Introduction 
Text categorization is the task of classifying 
documents into a certain number of pre-defined 
categories. Many supervised learning algorithms 
have been applied to this area. These algorithms 
today are reasonably successful when provided 
with enough labeled or annotated training 
examples.  For example, there are Naive Bayes 
(McCallum and Nigam, 1998), Rocchio (Lewis et 
al., 1996), Nearest Neighbor (kNN) (Yang et al, 
2002), TCFP (Ko and Seo, 2002), and Support 
Vector Machine (SVM) (Joachims, 1998). 
However, the supervised learning approach has 
some difficulties. One key difficulty is that it 
requires a large, often prohibitive, number of 
labeled training data for accurate learning. Since a 
labeling task must be done manually, it is a 
painfully time-consuming process. Furthermore, 
since the application area of text categorization has 
diversified from newswire articles and web pages 
to E-mails and newsgroup postings, it is also a 
difficult task to create training data for each 
application area (Nigam et al, 1998). In this light, 
we consider learning algorithms that do not require 
such a large amount of labeled data. 
While labeled data are difficult to obtain, 
unlabeled data are readily available and plentiful. 
Therefore, this paper advocates using a 
bootstrapping framework and a feature projection 
technique with just unlabeled data for text 
categorization. The input to the bootstrapping 
process is a large amount of unlabeled data and a 
small amount of seed information to tell the learner 
about the specific task. In this paper, we consider 
seed information in the form of title words 
associated with categories. In general, since 
unlabeled data are much less expensive and easier 
to collect than labeled data, our method is useful 
for text categorization tasks including online data 
sources such as web pages, E-mails, and 
newsgroup postings.  
To automatically build up a text classifier with 
unlabeled data, we must solve two problems; how 
we can automatically generate labeled training 
documents (machine-labeled data) from only title 
words and how we can handle incorrectly labeled 
documents in the machine-labeled data. This paper 
provides solutions for these problems. For the first 
problem, we employ the bootstrapping framework. 
For the second, we use the TCFP classifier with 
robustness from noisy data (Ko and Seo, 2004). 
How can labeled training data be automatically 
created from unlabeled data and title words? 
Maybe unlabeled data don?t have any information 
for building a text classifier because they do not 
contain the most important information, their 
category. Thus we must assign the class to each 
document in order to use supervised learning 
approaches. Since text categorization is a task 
based on pre-defined categories, we know the 
categories for classifying documents. Knowing the 
categories means that we can choose at least a 
representative title word of each category. This is 
the starting point of our proposed method. As we 
carry out a bootstrapping task from these title 
words, we can finally get labeled training data. 
Suppose, for example, that we are interested in 
classifying newsgroup postings about specially 
?Autos? category. Above all, we can select 
?automobile? as a title word, and automatically 
extract keywords (?car?, ?gear?, ?transmission?, 
?sedan?, and so on) using co-occurrence 
information. In our method, we use context (a 
sequence of 60 words) as a unit of meaning for 
bootstrapping from title words; it is generally 
constructed as a middle size of a sentence and a 
document. We then extract core contexts that 
include at least one of the title words and the 
keywords. We call them centroid-contexts because 
they are regarded as contexts with the core 
meaning of each category. From the centroid-
contexts, we can gain many words contextually co-
occurred with the title words and keywords: 
?driver?, ?clutch?, ?trunk?, and so on. They are 
words in first-order co-occurrence with the title 
words and the keywords. To gather more 
vocabulary, we extract contexts that are similar to 
centroid-contexts by a similarity measure; they 
contain words in second-order co-occurrence with 
the title words and the keywords. We finally 
construct context-cluster of each category as the 
combination of centroid-contexts and contexts 
selected by the similarity measure. Using the 
context-clusters as labeled training data, a Naive 
Bayes classifier can be built. Since the Naive 
Bayes classifier can label all unlabeled documents 
for their category, we can finally obtain labeled 
training data (machine-labeled data).  
When the machine-labeled data is used to learn a 
text classifier, there is another difficult in that they 
have more incorrectly labeled documents than 
manually labeled data. Thus we develop and 
employ the TCFP classifiers with robustness from 
noisy data. 
The rest of this paper is organized as follows. 
Section 2 reviews previous works. In section 3 and 
4, we explain the proposed method in detail. 
Section 5 is devoted to the analysis of the 
empirical results. The final section describes 
conclusions and future works. 
 
2 Related Works 
In general, related approaches for using unlabeled 
data in text categorization have two directions; 
One builds classifiers from a combination of 
labeled and unlabeled data (Nigam, 2001; Bennett 
and Demiriz, 1999), and the other employs 
clustering algorithms for text categorization 
(Slonim et al, 2002). 
Nigam studied an Expected Maximization (EM) 
technique for combining labeled and unlabeled 
data for text categorization in his dissertation. He 
showed that the accuracy of learned text classifiers 
can be improved by augmenting a small number of 
labeled training data with a large pool of unlabeled 
data.  
Bennet and Demiriz achieved small 
improvements on some UCI data sets using SVM. 
It seems that SVMs assume that decision 
boundaries lie between classes in low-density 
regions of instance space, and the unlabeled 
examples help find these areas. 
Slonim suggested clustering techniques for 
unsupervised document classification. Given a 
collection of unlabeled data, he attempted to find 
clusters that are highly correlated with the true 
topics of documents by unsupervised clustering 
methods. In his paper, Slonim proposed a new 
clustering method, the sequential Information 
Bottleneck (sIB) algorithm. 
 
3 The Bootstrapping Algorithm for Creating 
Machine-labeled Data 
The bootstrapping framework described in this 
paper consists of the following steps. Each module 
is described in the following sections in detail. 
 
1. Preprocessing: Contexts are separated from 
unlabeled documents and content words are 
extracted from them. 
2. Constructing context-clusters for training: 
- Keywords of each category are created 
- Centroid-contexts are extracted and verified 
- Context-clusters are created by a similarity  
measure 
3. Learning Classifier: Naive Bayes classifier are 
learned by using the context-clusters 
 
3.1 Preprocessing 
The preprocessing module has two main roles: 
extracting content words and reconstructing the 
collected documents into contexts. We use the Brill 
POS tagger to extract content words (Brill, 1995).  
Generally, the supervised learning approach with 
labeled data regards a document as a unit of 
meaning. But since we can use only the title words 
and unlabeled data, we define context as a unit of 
meaning and we employ it as the meaning unit to 
bootstrap the meaning of each category. In our 
system, we regard a sequence of 60 content words 
within a document as a context. To extract contexts 
from a document, we use sliding window 
techniques (Maarek et al, 1991). The window is a 
slide from the first word of the document to the last 
in the size of the window (60 words) and the 
interval of each window (30 words). Therefore, the 
final output of preprocessing is a set of context 
vectors that are represented as content words of 
each context. 
 
3.2 Constructing Context-Clusters for 
Training 
At first, we automatically create keywords from a 
title word for each category using co-occurrence 
information. Then centroid-contexts are extracted 
using the title word and keywords. They contain at 
least one of the title and keywords. Finally, we can 
gain more information of each category by 
assigning remaining contexts to each context-
cluster using a similarity measure technique; the 
remaining contexts do not contain any keywords or 
title words. 
3.2.1 Creating Keyword Lists 
The starting point of our method is that we have 
title words and collected documents. A title word 
can present the main meaning of each category but 
it could be insufficient in representing any 
category for text categorization. Thus we need to 
find words that are semantically related to a title 
word, and we define them as keywords of each 
category. 
The score of semantic similarity between a title 
word, T, and a word, W, is calculated by the cosine 
metric as follows: 
 
??
?
==
=
?
?=
n
i i
n
i i
n
i ii
wt
wt
WTsim
1
2
1
2
1),(               (1) 
 
where ti and wi represent the occurrence (binary 
value: 0 or 1) of words T and W in i-th document 
respectively, and n is the total number of 
documents in the collected documents. This 
method calculates the similarity score between 
words based on the degree of their co-occurrence 
in the same document.  
Since the keywords for text categorization must 
have the power to discriminate categories as well 
as similarity with the title words, we assign a word 
to the keyword list of a category with the 
maximum similarity score and recalculate the score 
of the word in the category using the following 
formula: 
 
)),(),((),(),( maxsecmaxmaxmax WTsimWTsimWTsimcWScore ond?+=  (2) 
 
where Tmax is the title word with the maximum 
similarity score with a word W, cmax is the category 
of the title word Tmax, and Tsecondmax is other title 
word with the second high similarity score with the 
word W. 
This formula means that a word with high 
ranking in a category has a high similarity score 
with the title word of the category and a high 
similarity score difference with other title words. 
We sort out words assigned to each category 
according to the calculated score in descending 
order. We then choose top m words as keywords in 
the category. Table 1 shows the list of keywords 
(top 5) for each category in the WebKB data set. 
 
Table 1. The list of keywords in the WebKB data set 
Category Title Word Keywords 
course course assignments, hours, instructor, class, fall 
faculty professor associate, ph.d, fax, interests, publications 
project project system, systems, research, software, information 
student student graduate, computer, science, page, university 
 
3.2.2 Extracting and Verifying Centroid-Contexts 
We choose contexts with a keyword or a title word 
of a category as centroid-contexts. Among 
centroid-contexts, some contexts could not have 
good features of a category even though they 
include the keywords of the category. To rank the 
importance of centroid-contexts, we compute the 
importance score of each centroid-context. First of 
all, weights (Wij) of word wi in j-th category are 
calculated using Term Frequency (TF) within a 
category and Inverse Category Frequency (ICF) 
(Cho and Kim, 1997) as follows:  
 
))log()(log( iijiijij CFMTFICFTFW ??=?=     (3) 
 
where CFi is the number of categories that contain 
wi and M is the total number of categories. 
Using word weights (Wij) calculated by formula 
3, the score of a centroid-context (Sk) in j-th 
category (cj) is computed as follows: 
 
N
WWW
cSScore Njjjjk
+++= ...),( 21            (4) 
 
where N is the  number of words in the centroid-
context. 
As a result, we obtain a set of words in first-
order co-occurrence from centroid-contexts of each 
category. 
3.2.3 Creating Context-Clusters 
We gather the second-order co-occurrence 
information by assigning remaining contexts to the 
context-cluster of each category. For the assigning 
criterion, we calculate similarity between 
remaining contexts and centroid-contexts of each 
category. Thus we employ the similarity measure 
technique by Karov and Edelman (1998). In our 
method, a part of this technique is reformed for our 
purpose and remaining contexts are assigned to 
each context-cluster by that revised technique. 
 
1) Measurement of word and context similarities 
As similar words tend to appear in similar contexts, 
we can compute the similarity by using contextual 
information. Words and contexts play 
complementary roles. Contexts are similar to the 
extent that they contain similar words, and words 
are similar to the extent that they appear in similar 
contexts (Karov and Edelman, 1998). This 
definition is circular. Thus it is applied iteratively 
using two matrices, WSM and CSM. 
Each category has a word similarity matrix 
WSMn and a context similarity matrix CSMn. In 
each iteration n, we update WSMn, whose rows and 
columns are labeled by all content words 
encountered in the centroid-contexts of each 
category and input remaining contexts. In that 
matrix, the cell (i,j) holds a value between 0 and 1, 
indicating the extent to which the i-th word is 
contextually similar to the j-th word. Also, we keep 
and update a CSMn, which holds similarities 
among contexts. The rows of CSMn correspond to 
the remaining contexts and the columns to the 
centroid-contexts. In this paper, the number of 
input contexts of row and column in CSM is 
limited to 200, considering execution time and 
memory allocation, and the number of iterations is 
set as 3.  
To compute the similarities, we initialize WSMn 
to the identity matrix. The following steps are 
iterated until the changes in the similarity values 
are small enough. 
1. Update the context similarity matrix CSMn, 
using the word similarity matrix WSMn. 
2. Update the word similarity matrix WSMn, using the 
context similarity matrix CSMn. 
2) Affinity formulae 
To simplify the symmetric iterative treatment of 
similarity between words and contexts, we define 
an auxiliary relation between words and contexts 
as affinity.  
Affinity formulae are defined as follows (Karov 
and Edelman, 1998): 
 
                  ),(max),( inXWn WWsimXWaff i?=   (5) 
 (6)                    ),(max),( jnXWn XXsimWXaff j?=
In the above formulae, n denotes the iteration 
number, and the similarity values are defined by 
WSMn and CSMn. Every word has some affinity to 
the context, and the context can be represented by 
a vector indicating the affinity of each word to it. 
 
3) Similarity formulae 
The similarity of W1 to W2 is the average affinity of 
the contexts that include W1 to W2, and the 
similarity of a context X1 to X2 is a weighted 
average of the affinity of the words in X1 to X2. 
Similarity formulae are defined as follows: 
 
  ),(),(),( 21211
1
XWaffXWweightXXsim n
XW
n ?= ?
?
+ (7) 
   (8)  
 ),(),(),(  
1),(   
  
21211
211
21
1
WXaffWXweightWWsim
else
WWsim
WWif 
n
XW
n
n
?=
=
=
?
?
+
+
The weights in formula 7 are computed as 
reflecting global frequency, log-likelihood factors, 
and part of speech as used in (Karov and Edelman, 
1998). The sum of weights in formula 8, which is a 
reciprocal number of contexts that contain W1, is 1. 
 
4) Assigning remaining contexts to a category 
We decided a similarity value of each remaining 
context for each category using the following 
method: 
       ),(),(   ??
?
??
?=
??
j
CCSiCc
SXsimavercXsim
icji
     (9) 
 
In formula 9, i) X is a remaining context, ii) 
{ }mcccC ,...,, 21= is a category set, and iii) { }nc SSi ,...,1=CC is 
a controid-contexts set of category ci. 
Each remaining context is assigned to a category 
which has a maximum similarity value. But there 
may exist noisy remaining contexts which do not 
belong to any category. To remove these noisy 
remaining contexts, we set up a dropping threshold 
using normal distribution of similarity values as 
follows (Ko and Seo, 2000): 
 
                         } ),( max{
Cci
??? +?
? i
cXsim (10) 
 
where i) X is a remaining context, ii) ? is an 
average of similarity values , iii) ? is a 
standard deviation of similarity values, and iv) ? is 
a numerical value corresponding to the threshold 
(%) in normal distribution table.  
),( iCc cXsimi?
Finally, a remaining context is assigned to the 
context-cluster of any category when the category 
has a maximum similarity above the dropping 
threshold value. In this paper, we empirically use a 
15% threshold value from an experiment using a 
validation set. 
3.3 Learning the Naive Bayes Classifier Using 
Context-Clusters 
In above section, we obtained labeled training data: 
context-clusters. Since training data are labeled as 
the context unit, we employ a Naive Bayes 
classifier because it can be built by estimating the 
word probability in a category, but not in a 
document. That is, the Naive Bayes classifier does 
not require labeled data with the unit of documents 
unlike other classifiers.  
We use the Naive Bayes classifier with minor 
modifications based on Kullback-Leibler 
Divergence (Craven et al, 2000). We classify a 
document di according to the following formula: 
 
 
?
?
=
=
???
?
???
?
+?
?=
||
1
||
1
),(
)?;|(
)?;|(
log)?;|(
)?;(log
                   
)?;|()?|(
)?|(
)?;|()?|(
)?;|(
V
t it
jt
it
j
V
t
dwN
jtj
i
jij
ij
dwP
cwP
dwP
n
cP
cwPcP
dP
cdPcP
dcP i
?
???
???
???
  (11) 
 
 
where i) n is the number of words in document di, 
ii) wt is the t-th word in the vocabulary, iii) N(wt,di) 
is the frequency of word wt in document di. 
Here, the Laplace smoothing is used to estimate 
the probability of word wt in class cj and the 
probability of class cj as follows: 
 
?=+
+
=
||
1
),(||
),(1
)?;|(
V
t ct
ct
jt
j
j
GwNV
GwN
cwP ?           (12) 
?+
+
=
i
i
j
c c
c
j
GC
G
cP
||||
||1
)?|( ?                    (13) 
 
where  is the count of the number of times 
word w
),(
jct GwN
t occurs in the context-cluster ( ) of 
category c
jcG
j. 
 
4 Using a Feature Projection Technique for 
Handling Noisy Data of Machine-labeled 
Data 
We finally obtained labeled data of a documents 
unit, machine-labeled data. Now we can learn text 
classifiers using them. But since the machine-
labeled data are created by our method, they 
generally include far more incorrectly labeled 
documents than the human-labeled data. Thus we 
employ a feature projection technique for our 
method. By the property of the feature projection 
technique, a classifier (the TCFP classifier) can 
have robustness from noisy data (Ko and Seo, 
2004). As seen in our experiment results, TCFP 
showed the highest performance among 
conventional classifiers in using machine-labeled 
data. 
 
The TCFP classifier with robustness from noisy 
data 
Here, we simply describe the TCFP classifier using 
the feature projection technique (Ko and Seo, 
2002; 2004). In this approach, the classification 
knowledge is represented as sets of projections of 
training data on each feature dimension. The 
classification of a test document is based on the 
voting of each feature of that test document. That 
is, the final prediction score is calculated by 
accumulating the voting scores of all features.  
First of all, we must calculate the voting ratio of 
each category for all features. Since elements with 
a high TF-IDF value in projections of a feature 
must become more useful classification criteria for 
the feature, we use only elements with TF-IDF 
values above the average TF-IDF value for voting. 
And the selected elements participate in 
proportional voting with the same importance as 
the TF-IDF value of each element. The voting ratio 
of each category cj in a feature tm is calculated by 
the following formula: 
 
 
??
??
?=
mmmm
jj
Ilt
lm
Ilt
mlmm dtwltcydtwtcr
)()(
),())(,(),(),(
rr
   (14) 
 
In formula 14, w ),( dtm
r
is the weight of term tm in 
document d, Im denotes a set of elements selected 
for voting and  is a function; if the 
category for an element t  is equal to c , the 
output value is 1. Otherwise, the output value is 0.  
{ }1.0?
)(lm
))(,( ltcy mj
j
Next, since each feature separately votes on 
feature projections, contextual information is 
missing. Thus we calculate co-occurrence 
frequency of features in the training data and 
modify TF-IDF values of two terms ti and tj in a 
test document by co-occurrence frequency between 
them; terms with a high co-occurrence frequency 
value have higher term weights.  
Finally, the voting score of each category c in 
the m-th feature t
j
m of a test document d is 
calculated by the following formula: 
 
))(1log(),(),(),( 2 mmmm ttcrdttwtcvs jj ?+??=
r
   (15) 
 
where tw(tm,d) denotes a modified term weight by 
the co-occurrence frequency and denotes 
the calculated ?
)(2 mt?
m
2 statistics value of . t
 
 
 Table 2. The top micro-avg F1 scores and  precision-recall breakeven points of each method.  
 OurMethod 
(basis) 
OurMethod
(NB) 
OurMethod
(Rocchio) 
OurMethod
(kNN) 
OurMethod 
(SVM) 
OurMethod
(TCFP) 
Newsgroups 79.36 83.46 83 79.95 82.49 86.19 
WebKB 73.63 73.22 75.28 68.04 73.74 75.47 
Reuters 88.62 88.23 86.26 85.65 87.41 89.09 
 
The outline of the TCFP classifier is as follow: 
 
5 Empirical Evaluation 
5.1 Data Sets and Experimental Settings 
To test our method, we used three different kinds 
of data sets: UseNet newsgroups (20 Newsgroups), 
web pages (WebKB), and newswire articles 
(Reuters 21578). For fair evaluation in 
Newsgroups and WebKB, we employed the five-
fold cross-validation method. 
The Newsgroups data set, collected by Ken 
Lang, contains about 20,000 articles evenly 
divided among 20 UseNet discussion groups 
(McCallum and Nigam, 1998). In this paper, we 
used only 16 categories after removing 4 
categories: three miscellaneous categories 
(talk.politics.misc, talk.religion.misc, and 
comp.os.ms-windows.misc) and one duplicate 
me
T B 
pro et 
con ty 
com
T et 
con es 
fro in 
(N us 
cat
A
eac
app
sta
cla
A
sta
me
across categories, we used the micro-averaging 
method (Yang et al, 2002). Results on Reuters are 
reported as precision-recall breakeven points, 
which is a standard information retrieval measure 
for binary classification (Joachims, 1998). 
1. input : test document: d
r
 =<t1,t2,?,tn> 
2. main process 
For each feature ti 
           tw(ti,d) is calculated  
 
For each feature ti 
          For each category cj 
                vote[cj]=vote[cj]+vs(cj,ti) by Formula 15
 
prediction = ][maxarg j
c
cvote
j
 
Title words in our experiment are selected 
according to category names of each data set (see 
Table 1 as an example). 
5.2 Experimental Results 
5.2.1 Observing the Performance According to 
the Number of Keywords 
First of all, we determine the number of keywords 
in our method using the validation set.  The 
number of keywords is limited by the top m-th 
keyword from the ordered list of each category. 
Figure 1 displays the performance at different 
number of keywords (from 0 to 20) in each data set. 
 
40
45
50
55
60
65
70
75
80
85
0 1 2 3 4 5 8 10 13 15 18 20
The number of keywords
M
i
cro
-a
v
g
. 
F1
Newsgroups WebKB Reuters 
 
Figure 1. The comparison of performance according to 
the number of keywords 
 
We set the number of keywords to 2 in 
Newsgroups, 5 in WebKB, and 3 in Reuters 
empirically. Generally, we recommend that the 
number of keywords be between 2 and 5. 
5.2.2 Comparing our Method Using TCFP with 
those Using other Classifiers 
In this section, we prove the superiority of TCFP 
over the other classifiers (SVM, kNN, Naive Bayes aning category (comp.sys. ibm.pc.hardware).  
he second data set comes from the WebK
ject at CMU (Craven et al, 2000). This data s
tains web pages gathered from universi
puter science departments.  
he Reuters 21578 Distribution 1.0 data s
sists of 12,902 articles and 90 topic categori
m the Reuters newswire. Like other study 
igam, 2001), we used the ten most populo
egories to identify the news topic.  bout 25% documents from training data of 
h data set are selected for a validation set. We 
lied a statistical feature selection method (?2 
tistics) to a preprocessing stage for each 
ssifier (Yang and Pedersen, 1997). 
s performance measures, we followed the 
ndard definition of recall, precision, and F1 
asure. For evaluation performance average 
(NB), Roccio) in training data with much noisy 
data such as machine-labeled data. As shown in 
Table 2, we obtained the best performance in using 
TCFP at all three data sets.  
Let us define the notations. OurMethod(basis) 
denotes the Naive Bayes classifier using labeled 
contexts and OurMethod(NB) denotes the Naive 
Bayes classifier using machine-labeled data as 
training data. The same manner is applied for other 
classifiers. 
OurMethod(TCFP) achieved more advanced 
scores than OurMethod(basis): 6.83 in 
Newsgroups, 1.84 in WebKB, and  0.47 in Reuters. 
5.2.3 Comparing with the Supervised Naive 
Bayes Classifier 
For this experiment, we consider two possible 
cases for labeling task. The first task is to label a 
part of collected documents and the second is to 
label all of them. As the first task, we built up a 
new training data set; it consists of 500 different 
documents randomly chosen from appropriate 
categories like the experiment in (Slonim et al, 
2002). As a result, we report performances from 
two kinds of Naive Bayes classifiers which are 
learned from 500 training documents and the 
whole training documents respectively. 
 
Table 3. The comparison of our method and the 
supervised NB classifier 
 OurMethod 
(TCFP) 
NB 
(500) 
NB 
(All) 
Newsgroups 86.19 72.68 91.72 
WebKB 75.47 74.1 85.29 
Reuters 89.09 82.1 91.64 
 
In Table 3, the results of our method are higher 
than those of NB(500) and are comparable to those 
of NB(All) in all data sets. Especially, the result in 
Reuters reached 2.55 close to that of NB(All) 
though it used the whole labeled training data. 
5.2.4 Enhancing our Method from Choosing 
Keywords by Human 
The main problem of our method is that the 
performance depends on the quality of the 
keywords and title words. As we have seen in 
Table 3, we obtained the worst performance in the 
WebKB data set. In fact, title words and keywords 
of each category in the WebKB data set alo have 
high frequency in other categories. We think these 
factors contribute to a comparatively poor 
performance of our method. If keywords as well as 
title words are supplied by humans, our method 
may achieve higher performance. However, 
choosing the proper keywords for each category is 
a much difficult task. Moreover, keywords from 
developers, who have insufficient knowledge about 
an application domain, do not guarantee high 
performance. In order to overcome this problem, 
we propose a hybrid method for choosing 
keywords. That is, a developer obtains 10 
candidate keywords from our keyword extraction 
method and then they can choose proper keywords 
from them. Table 4 shows the results from three 
data sets. 
Table 4. The comparison of our method and enhancing 
method 
 OurMethod 
(TCFP) 
Enhancing 
(TCFP)) Improvement
Newsgroups 86.19 86.23 +0.04 
WebKB 75.47 77.59 +2.12 
Reuters 89.09 89.52 +0.43 
 
As shown in Table 4, especially we could achieve 
significant improvement in the WebKb data set. 
Thus we find that the new method for choosing 
keywords is more useful in a domain with 
confused keywords between categories such as the 
WebKB data set. 
5.2.5 Comparing with a Clustering Technique 
In related works, we presented two approaches 
using unlabeled data in text categorization; one 
approach combines unlabeled data and labeled data, 
and the other approach uses the clustering 
technique for text categorization. Since our method 
does not use any labeled data, it cannot be fairly 
compared with the former approaches. Therefore, 
we compare our method with a clustering 
technique. Slonim et al (2002) proposed a new 
clustering algorithm (sIB) for unsupervised 
document classification and verified the superiority 
of his algorithm. In his experiments, the sIB 
algorithm was superior to other clustering 
algorithms. As we set the same experimental 
settings as in Slonim?s experiments and conduct 
experiments, we verify that our method 
outperforms ths sIB algorithm. In our experiments, 
we used the micro-averaging precision as 
performance measure and two revised data sets: 
revised_NG, revised_Reuters. These data sets were 
revised in the same way according to Slonim?s 
paper as follows:  
In revised_NG, the categories of Newsgroups were 
united with respect to 10 meta-categories: five comp 
categories, three politics categories, two sports 
categories, three religions categories, and two 
transportation categories into five big meta-
categories.  
The revised_Reuters used the 10 most frequent 
categories in the Reuters 21578 corpus under the 
ModApte split.  
As shown in Table 5, our method shows 6.65 
advanced score in revised_NG and 3.2 advanced 
score in revised_Reuters. 
 
Table 5. The comparison of our method and sIB 
 sIB OurMethod 
(TCFP) Improvement
revised_NG 79.5 86.15 +6.65 
revised_Reuters 85.8 89 +3.2 
6 Conclusions and Future Works 
This paper has addressed a new unsupervised or 
semi-unsupervised text categorization method. 
Though our method uses only title words and 
unlabeled data, it shows reasonably comparable 
performance in comparison with that of the 
supervised Naive Bayes classifier. Moreover, it 
outperforms a clustering method, sIB. Labeled data 
are expensive while unlabeled data are inexpensive 
and plentiful. Therefore, our method is useful for 
low-cost text categorization. Furthermore, if some 
text categorization tasks require high accuracy, our 
method can be used as an assistant tool for easily 
creating labeled training data.  
Since our method depends on title words and 
keywords, we need additional studies about the 
characteristics of candidate words for title words 
and keywords according to each data set. 
 
Acknowledgement  
This work was supported by grant No. R01-2003-
000-11588-0 from the basic Research Program of 
the KOSEF 
 
References  
K. Bennett and A. Demiriz, 1999, Semi-supervised 
Support Vector Machines, Advances in Neural 
Information Processing Systems 11, pp. 368-374. 
E. Brill, 1995, Transformation-Based Error-driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging, Computational 
Linguistics, Vol.21, No. 4. 
K. Cho and J. Kim, 1997, Automatic Text 
Categorization on Hierarchical Category Structure by 
using ICF (Inverse Category Frequency) Weighting, 
In Proc. of KISS conference, pp. 507-510. 
M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. 
Mitchell, K. Nigam, and S. Slattery, 2000, Learning 
to construct knowledge bases from the World Wide 
Web, Artificial Intelligence, 118(1-2), pp. 69-113. 
T. Joachims, 1998, Text Categorization with Support 
Vector Machines: Learning with Many Relevant 
Features. In Proc. of ECML, pp. 137-142. 
Y. Karov and S. Edelman, 1998, Similarity-based Word 
Sense Disambiguation, Computational Linguistics, 
Vol. 24, No. 1, pp. 41-60. 
Y. Ko and J. Seo, 2000, Automatic Text Categorization 
by Unsupervised Learning, In Proc. of 
COLING?2000, pp. 453-459. 
Y. Ko and J. Seo, 2002, Text Categorization using 
Feature Projections, In Proc. of COLING?2002, pp. 
467-473. 
Y. Ko and J. Seo, 2004, Using the Feature Projection 
Technique based on the Normalized Voting Method 
for Text Classification, Information Processing and 
Management, Vol. 40, No. 2, pp. 191-208. 
D.D. Lewis, R.E. Schapire, J.P. Callan, and R. Papka, 
1996, Training Algorithms for Linear Text 
Classifiers. In Proc. of SIGIR?96, pp.289-297. 
Y. Maarek, D. Berry, and G. Kaiser, 1991, An 
Information Retrieval Approach for Automatically 
Construction Software Libraries, IEEE Transaction 
on Software Engineering, Vol. 17, No. 8, pp. 800-
813. 
A. McCallum and K. Nigam, 1998, A Comparison of 
Event Models for Naive Bayes Text Classification. 
AAAI ?98 workshop on Learning for Text 
Categorization, pp. 41-48. 
K. P. Nigam, A. McCallum, S. Thrun, and T. Mitchell, 
1998, Learning to Classify Text from Labeled and 
Unlabeled Documents, In Proc. of AAAI-98. 
K. P. Nigam, 2001, Using Unlabeled Data to Improve 
Text Classification, The dissertation for the degree of 
Doctor of Philosophy. 
N. Slonim, N. Friedman, and N. Tishby, 2002, 
Unsupervised Document Classification using 
Sequential Information Maximization, In Proc. of 
SIGIR?02, pp. 129-136. 
Y. Yang and J. P. Pedersen. 1997, Feature selection in 
statistical leaning of text categorization. In Proc. of 
ICML?97, pp. 412-420. 
Y. Yang, S. Slattery, and R. Ghani. 2002, A study of 
approaches to hypertext categorization, Journal of 
Intelligent Information Systems, Vol. 18, No. 2. 
 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 731 ? 741, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Improving Korean Speech Acts Analysis by Using 
Shrinkage and Discourse Stack 
Kyungsun Kim1, Youngjoong Ko2, and Jungyun Seo3
1
 Information Retrieval Division, Diquest.Inc, Seocho-dong,  
Seocho-gu, Seoul, 137-070, Korea 
kksun@diquest.com
2
 Dept. of Computer Engineering, Dong-A University, 840,  
Hadan 2-dong, Saha-gu,  Busan, 604-714, Korea 
yjko@dau.ac.kr
3
 Dept. of Computer Science and Interdisciplinary Program of Integrated Biotechnology, 
Sogang University, Seoul, 121-742, Korea 
seojy@sogang.ac.kr
Abstract. A speech act is a linguistic action intended by a speaker. It is impor-
tant to analyze the speech act for the dialogue understanding system because the 
speech act of an utterance is closely tied with the user?s intention in the utter-
ance. This paper proposes to use a speech acts hierarchy and a discourse stack 
for improving the accuracy of classifiers in speech acts analysis. We first adopt 
a hierarchical statistical technique called shrinkage to solve the data sparseness 
problem. In addition, we use a discourse stack in order to easily apply discourse 
structure information to the speech acts analysis. From the results of experi-
ments, we observed that the proposed model made a significant improvement 
for Korean speech acts analysis. Moreover, we found that it can be more useful 
when training data is insufficient. 
1   Introduction 
To understand a natural language dialogue, a dialogue system must be able to make 
out the speaker?s intentions indicated by utterances. Since the speech act of an utter-
ance is very important in understanding a speaker?s intentions, it is an essential part of 
a dialogue system. However, it is difficult to infer the speech act from a surface utter-
ance because the utterance may represent more than one speech act according to the 
context [5][7]. 
Various machine learning models have been used to efficiently classify speech acts 
such as MEM (Maximum Entropy Model) [1], HMM (Hidden Markov Model) with 
Decision Tree [8][11], Neural Network Model [5]. And there are also studies on 
methods of automatically selecting efficient features with useful information for 
speech acts analysis [5][10]. Since the machine learning models can efficiently ana-
lyze a large quantity of data and consider many different feature interactions, they can 
provide a means of associating features of utterances with particular speech acts. 
Generally, it is hard to create enough the number of examples for each speech act 
in the training examples. Thus this situation has been one of the main causes for  
errors occurred in speech acts analysis. That is, the sparse data problem from low 
732 K. Kim, Y. Ko, and J. Seo 
frequency of some speech acts has commonly occurred in the previous research [8]. 
Due to the problem, the accuracy of each speech act in previous research tends to be 
proportional to the frequency of each speech act in the training data. Therefore, we 
first focus on how to scale up statistical learning methods to solve the sparseness 
problem of training data in speech acts analysis. Then we propose to construct the 
commonly-available hierarchies of speech acts and apply a well-understood technique 
from Statistics called shrinkage to our speech acts analysis system. It provides im-
proved estimates of parameters that would otherwise be uncertain due to limited 
amounts of training data [3]. The technique uses a hierarchy to shrink parameter esti-
mates in data sparse children toward the estimates of the data-rich ancestors in ways 
that are probably optimal under the appropriate conditions [9]. We employ a simple 
form of shrinkage that creates new parameter estimates for a child by a linear interpo-
lation of all hierarchy nodes from the child to the root.  
In addition, discourse structure information can be used to identify the speech acts 
of utterances [1]. But most previous research has used only speech acts of previous 
utterances without considering discourse structure information to determine the speech 
act of current utterance. Therefore, in order to use discourse structure information for 
analyzing speech acts, we design a simple discourse stack. By using the discourse 
stack, the discourse structure information is easily applied to speech acts analysis. 
In this paper, we propose a new speech acts analysis model to improve the per-
formance by using shrinkage and discourse structure information. From the results of 
experiments, the proposed system showed significant improvement in comparison 
with previous research. 
The rest of this paper is organized as follows. Section 2 explains the proposed 
speech acts analysis system in detail. In section 3, we discuss the empirical results in 
our experiments. The final section presents conclusions. 
2   The Proposed Speech Acts Analysis System 
The proposed system consists of two modules as shown in Fig. 1: one module to  
extract  features  from  training  data  and  the  other module to build up a hierarchy of  
 	

 	

 	

 	







	

  	

		
	
 	

	
 	
Fig. 1. The overview of the proposed system 
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 733
speech acts and estimate weights of each feature on the hierarchy by shrinkage. Each 
process of Fig. 1 is explained in the following sections. 
2.1   Feature Extraction 
2.1.1   Sentence Features Extraction 
We assume that clue words and a sequence of POS tags in an utterance provide very 
effective information for analyzing the speech act of the current utterance. We extract 
informative features for speech acts analysis using a Morphological analyzer; they are 
called the sentence features. The sentence features consist of content words annotated 
with POS tags and POS bi-grams of all words in an utterance. Fig. 2 shows an exam-
ple of sentence feature extraction. 
Input:
 	.
(My name is HongKildong.)
Morphological analyzer
The result of morphological analysis:

/np /j   /ncn /j /nq /jcp 	/ef ./s.
(My/np name/ncn is/jcp HongKildong/nq ./s.)
Feature extractor
Content Words:

/np /ncn /nq /jcp
(My/np name/ncn HongKilgong/nq is/jcp)
POS bi-grams:
np-j j-ncn ncn-j j-nq, nq-jcp jcp-ef ef-s.
Fig. 2 An example of sentence feature extraction
2.1.2   Context Features Extraction 
Most previous research uses the speech act of previous utterance as context feature 
(CF1 in Table 1) [5][8]. Since discourse structure information represents the relation-
ship between two consecutive utterances, it is efficient to use discourse structure  
For each utterance  
Begin 
if(Move a sub-dialogue?)  
Use speech acts of previous utterance and Sub-dialogue Start (SS) 
Push speech acts of current utterance. 
else if(Return from a sub-dialogue?)  
Use speech acts that pop in discourse stack and Sub-dialogue End (SE) 
else  
Use speech acts of previous utterance and Dialogue Continue (DC) 
End
734 K. Kim, Y. Ko, and J. Seo 
information for speech acts analysis [1]. Especially, the speech act of seventh utter-
ance in Table 1 (UID: 7) is tied with that of second utterance (UID: 2). In our system, 
we first design a discourse stack to easily detect discourse structure information and 
extract the discourse structure information from the discourse stack for context fea-
tures. Context features of our system consist of speech acts of previous utterance and 
markers of discourse structure information (CF2 in Table 1). An algorithm for dis-
course stack is described as the following:  
Table 1. An example of Context Feature
* UID: ID of utterances, DS: Discourse Structure, CF1: Using speech acts of previous utterances as features 
(Context Feature Type1), CF2: Using Discourse Structure Information by Discourse Stack as features 
(Context Feature Type2), Speech acts and discourse structure information were annotated by human. 
2.2   The Feature Weight Calculation by Shrinkage in a Hierarchy of Speech Acts 
Data sparseness is a common problem in mechanical learning fields. For speech acts 
analysis, the problem becomes more serious because it is a time-consuming and diffi-
cult task to collect dialogue examples and construct dialogue training data tagged with 
a lot of information for various application areas. Therefore, we apply the shrinkage 
technique to solve this data sparseness problem in speech acts analysis. The shrinkage 
technique was verified in its efficiency for text classification tasks learned with insuf-
ficient training data. Therefore, we first build up a hierarchy of speech acts to estimate 
the weight of features for each speech act by the shrinkage technique.  
2.2.1   The Hierarchy Construction for Speech Acts 
To model a dialogue system, the dialogue grammar has commonly used and it has 
observed  that  dialogues  consist  of adjacency pairs of the types of utterances such as  
UID DS Utterance
Speech 
Acts
CF1 CF2 
1 1 
????????????
(I would like to reserve a room) 
Inform 
Dialog-
start
Dialog-start,
NULL
2 1.1 
?????????
(What kind of room do you want?) 
Ask-ref Inform
Inform,  
SS
3 1.1.1 
????????????
(What kind of room do you have?) 
Ask-ref Ask-ref
Ask-ref,  
SS
4 1.1.1 
????????????.
(We have single and double rooms) 
Response Ask-ref
Ask-ref,  
DC
5 1.1.2 
???????
(How much are those rooms?) 
Ask-ref 
Re-
sponse
Response,  
DC
6 1.1.2 
?????????????????.
(Singles cost 30,000 won and doubles cost 
40,000 won.) 
Response Ask-ref
Ask-ref,  
DC
7 1.1 
?????????.
(A single room, please) 
Response
Re-
sponse
Ask-ref,  
SE
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 735
Table 2. The Hierarchy of Speech Acts 
 Parent Child 
Ask-if 
Ask-ref 
Ask-confirm 
Offer 
Suggest 
Type1: Utterances of 
request type 
Request 
Accept 
Response 
Reject 
Type2: Utterances of 
response type 
Acknowledge 
Expressive 
Promise Type3: Utterances with a 
speaker emotion 
Closing 
Opening 
Introducing-oneself 
Correct 
Root 
Type4: Utterances of 
usually life 
Inform 
request-type and response-type [2][8]. Therefore, our speech acts hierarchy is built up 
according to this grammar. Table 2 shows the structure of our speech acts hierarchy. 
2.2.2   Mixture Weighting Model by Shrinkage in a Hierarchy of Speech Acts 
The shrinkage technique estimates the probability of a word as the weighted sum of 
the maximum-likelihood estimates from leaf to root in a hierarchy [9]. This estimate 
process can give us a possibility to resolve the data sparseness problem in some 
speech acts with insufficient examples. Fig.3 shows that the shrinkage-based estimate 
of the probability of a feature (? /np?) given a speech act class (?Accept?) is calcu-
lated from a weighted sum of the maximum-likelihood estimates from leaf to root. 
ROOT
 
 
?? 		

TYPE1
 
 
?? 

TYPE2
 
 
?? 

ACCEPT
 
 
?? 

? ? ?
 
 
 
 

 
 
	
 
 
	

?? 

1
type1.accept  ?? 


2
type1.accept   


3
type1.accept   		

Fig. 3. An example of the shrinkage-based estimate of the probability of features 
736 K. Kim, Y. Ko, and J. Seo 
Let }?,...,?,?{ 21 kjjj ???  be k such estimates, where jkj ?? =?  is the estimate at the leaf, 
and k-1 is the depth of speech acts ts in a hierarchy of Speech Acts. The interpolation 
weights among the ancestors of speech acts ts are written },...,,{ 21 kjjj ??? , where 
11 =? = ijki ? . We write j?

 for the new estimate of the speech act-conditioned feature 
probabilities based on shrinkage. The new estimate for the probability of feature 
tf given speech act js is as follows: 
11211 ?
...
??);( jtkjjtjjtjjjtjt sfP ???????? +++==

. (1)
We derive empirically optimal weights using the following iterative procedure: 
2.3   The SVM Classifier 
Support Vector Machines (SVM) is one of the state-of-the-art classifiers for classifi-
cation tasks [6][12]. Since SVM has shown the high performance in various research 
areas, we also employ it in our method. In our method, we use the linear models of-
fered by SVMlight [4] and jt?

, which are calculated by formula (1), are used as the 
feature weights of speech acts for the SVM classifier.  
Initialize:  
Set the j? ?s to some initial values, say k
i
j
1
=?
Iterate: 
1. Calculate the degree to which each estimate predicts the features tf  in the held-out 
feature set, jH , from speech acts js  : 
? ?? ??
==
jtjt Hw m
m
jt
m
j
i
jt
i
j
Hw
t
i
j
i
j fP ??
??
??
?
?
)generate toused was?(                (2) 
2. Compensate the degree for loss that is caused by large variation of each degree : 
m
m
m
jj
i
j
i
?
+=
???                                                      (3) 
3. Derive new weights by normalizing the s'? :
?
=
m
m
j
i
ji
j ?
??                                                       (4) 
Terminate: Upon convergence of the likelihood function
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 737
Table 3. The part of mixture weights learned by shrinkage-based estimation 
Speech Acts Mixture Weights # training 
documents Root Parent Child Root Parent Child 
Ask-ref 0.289 0.32 0.39 Type1
Suggest 0.257 0.275 0.467 
Type2 Expressive 0.263 0.335 0.4 
Type3 Reject 0.259 0.269 0.47 
250 Root 
Type4 Inform 0.297 0.336 0.366 
Ask-ref 0.282 0.295 0.422 Type1 Suggest 0.217 0.22 0.562 
Type2 Expressive 0.229 0.279 0.49 
Type3 Reject 0.212 0.215 0.571 
8349 Root 
Type4 Inform 0.26 0.332 0.406 
3   Empirical Evaluation 
3.1   Experimental Data 
We used the Korean dialogue corpus which has used in previous research [1][5][8]. 
This corpus was transcribed from recordings in real fields such as hotel reservation, 
airline reservation and tour reservation and consists of 528 dialogues, 10,285 utter-
ances (19.48 utterances per dialogue). Each utterance in dialogues is manually anno-
tated with a speaker (SP), a speech act (SA) and a discourse structure (DS). This an-
notated dialogue corpus has 17 types of speech acts. Table 4 shows a part of the anno-
tated dialog corpus and Table 5 shows the distribution of speech acts in the annotated 
dialogue corpus. 
Table 4. A part of the annotated dialogue corpus
Tag Values 
SP Customer 
KS ??????????????????????.
EN
I?m a student and registered for a language course at University of Geor-
gia in U.S. 
SA Introducing-oneself 
DS [2] 
SP Customer 
KS ????????????????.
EN I have some questions about lodgings. 
SA Request 
DS [2] 
738 K. Kim, Y. Ko, and J. Seo 
Table 5. The distribution of speech acts in corpus
Speech act type Ratio (%) Speech act type Ratio (%) 
Accept 2.49 Introducing-oneself 6.75 
Acknowledge 5.75 Offer 0.4 
Ask-confirm 3.16 Opening 6.58 
Ask-if 5.36 Promise 2.42 
Ask-ref 13.39 Reject 1.07 
Closing 3.39 Request 4.96 
Correct 0.03 Response 24.73 
Expressive 5.64 Suggest 1.98 
Inform 11.9 Total 100 
We divided the annotated dialogue corpus into the training data with 428 dia-
logues, 8,349 utterances (19.51 utterances per dialogue), and the testing data with 100 
dialogues, 1,936 utterances (19.36 utterances per dialogue). 
3.2   Primary Experimental Results 
3.2.1   The Performances of Speech Acts Analysis Model Using Shrinkage and 
Discourse Stack 
In order to verify the proposed method, we made four kinds of speech acts analysis 
systems which use different kind of features. The Baseline System used default fea-
tures such as sentence features and context features [5]. The Second system (Type 1) 
was built up to verify the shrinkage technique. Its features were the same as those of 
the first system but they were weighted by the shrinkage technique. The third System 
(Type 2) used the discourse structure information from the proposed discourse stack 
without shrinkage. Finally, the fourth system (Type 3) combined the discourse struc-
ture information and the shrinkage technique. 
Table 6 shows the results of four speech acts analysis systems. As shown in Table 6, 
the performances of the proposed systems (Type 1,2,3) are better than the baseline 
system. The proposed system of Type 3 reported the best performance. 
3.2.2   The Improvement of the Proposed System Using the Shrinkage Technique 
in Sparse Data 
Here, we verify the facts that the shrinkage technique can improve the speech acts 
analysis when training data is sparse. We first compare the system with shrinkage 
(Type 3) and the system without shrinkage (Type 2). Fig. 4 shows the changes of 
performance in each number of training data from 250 to 8439. The proposed system 
with shrinkage obtains the better performance over all intervals in Fig. 4. Especially, 
the shrinkage technique provides more improvement when the amount of training data 
is small. This is a proof that the shrinkage technique can become an effective solution 
for sparse data problem from insufficient training data. 
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 739
Table 6. The results of four speech acts analysis systems (precision %)
Speech acts Baseline      
System 
Proposed        
System 
(Type1) 
Proposed        
System 
(Type2) 
Proposed     
System 
(Type3) 
Accept 36.00% 50.00% 38.00% 50.00% 
Acknowledge 91.30% 91.30% 92.75% 95.65% 
ask-confirm 92.68% 96.34% 93.90% 95.12% 
ask-if 84.16% 86.14% 86.14% 89.11% 
ask-ref 89.88% 91.05% 90.66% 91.44% 
Closing 60.00% 61.43% 67.14% 71.43% 
Correct 0.00% 0.00% 0.00% 0.00% 
Expressive 85.84% 83.19% 87.61% 83.19% 
Inform 70.00% 70.00% 76.00% 75.60% 
Introducing-oneself 98.58% 98.58% 97.87% 98.58% 
Offer 12.50% 12.50% 12.50% 12.50% 
Opening 97.60% 96.80% 96.80% 96.80% 
Promise 92.50% 92.50% 87.50% 90.00% 
Reject 68.18% 72.73% 68.18% 68.18% 
Request 71.43% 73.81% 70.24% 69.05% 
Response 96.49% 96.07% 96.07% 96.07% 
Suggest 56.76% 56.76% 56.76% 62.16% 
TOTAL 85.18% 85.85% 86.31% 87.04% 
 
 
 
 
 
 
 
 
 
             	    	

 	
 	 	
	
Fig. 4. The performance according to different number of training data 
We then compare performances between the system of Type 2 and the system of 
Type 3 according to distribution of each speech act. As shown in Fig. 5, the pro-
posed system (Type 3) with the shrinkage technique shows higher performance in 
speech acts with insufficient examples such as ?Accept?, ?Closing?, ?Promise? and 
?Suggest?.  
740 K. Kim, Y. Ko, and J. Seo 
 
 
 
 
 
 
 
 
 	
 


 











 

	



	

 





 















	











 













	
 













	


 



























 













 






	
	

 


 
 



 	
  	
  

 

Fig. 5. The comparison of the performances for the shrinkage technique according to the distri-
bution of speech acts 
3.2.3   The Comparison of Performance with  Speech Acts Analysis Models 
Table 7 shows results from the proposed model and previous speech acts analysis 
models: the maximum entropy model (MEM) [1], the decision tree model (DTM) [8], 
and the neural network model (NNM) [5]. We report the performance of each system 
when using the same test data set as that of this paper. As a result, the proposed model 
achieved the highest performance. 
Table 7. The experimental results of the proposed model and other previous models
Model Precision (%) 
MEM 83.4% 
DTM 81.7% 
NNM 85.2% 
The propose model 87.0% 
In the experiment, it is difficult to compare the proposed model directly with the 
other models because input features are different respectively. Even though direct 
comparisons are impossible, we think that the proposed model is more robust and 
efficient than MEM and DTM. In MEM and DTM, they used many kinds of high 
level linguistic knowledge than ours such as sentence type, tense, modality and so on. 
Nevertheless, the performances of them are lower than that of the proposed model. 
Moreover, the proposed model is more effective than NNM because the performance 
of the proposed model is better than that of NNM in spite of using same features. 
4   Conclusions 
In this paper, we proposed the new speech analysis model to improve speech acts 
analysis by using the shrinkage technique and the discourse stack. We first made a 
Other
 Improving Korean Speech Acts Analysis by Using Shrinkage and Discourse Stack 741
hierarchy of speech acts by dialogue grammar for shrinkage and then estimate the 
probability of each feature on the hierarchy by the shrinkage technique. In experimen-
tal results, the proposed model is more effective for classifying speech acts. Espe-
cially, the shrinkage technique achieved more improvement when training data is 
sparse. Therefore, the shrinkage technique can be applied to the real applications that 
suffer from the data sparseness problem. We also proposed to use the discourse stack 
for easily extracting discourse structure information. As a result, the proposed model 
with shrinkage and the discourse stack showed the better performance than other 
speech acts analysis models. 
Acknowledgement 
This research was supported as a Brain Neuroinformatics Research Program spon-
sored by the Ministry of Commerce, Industry and Energy of Korea. 
References 
1. Choi, W., Cho, J. and Seo, J.: Analysis System of speech acts and Discourse Structures 
Using Maximum Entropy Model, In Proceedings of COLING-ACL99, (1999), 230-237 
2. Grosz, B.: Discourse and Dialogue, In Survey of the State of the Art in Human Language 
Technology, Center for Spoken Language Understanding, (1995), 227-254 
3. James, W. and Stein, C.: Estimation with Quadratic Loss, In Proceedings of the Fourth 
Berkeley Symposium on Mathematical Statistics and Probability 1, University of Califor-
nia Press, 361-379 
4. Joachims, T.: Text Categorization with Support Vector Machines: Learning with Many 
Relevant Features. In European conference on machine learning (ECML), (1998), 137-142 
5. Kim, K., Kim, H. and Seo, J.: A Neural Network Model with Feature Selection for Korean 
Speech Act Classification, International Journal of Neural System, VOL. 14 NO. 6, 
(2004), 407-414 
6. Ko, Y., Park, J, Seo, J.: Improving Text Categorization Using the Importance of Sen-
tences, Information Processing & Management, Vol. 40, No. 1, (2004), 65-79 
7. Lee, J., Kim, G., and Seo, J.: A Dialogue Analysis Model with Statistical Speech Act Proc-
essing for Dialogue Machine Translation, In Proceedings of ACL Workshop on Spoken 
Language Translation, (1997), 10-15 
8. Lee, S. and Seo, J.: A Korean Speech Act Analysis System Using Hidden Markov Model 
with Decision Trees, International Journal of Computer Processing of Oriental Languages. 
VOL. 15, NO. 3, (2002), 231-243 
9. MacCallum, A., Rosenfeld, R., Mitchell, T. and Ng, A.Y.: Improving Text Classification 
by Shrinkge in a Hierarchy of Classes, In Proceedings of the International Conference on 
Machine Learning. (1998) 
10. Samuel, K., Caberry, S., and Vijay-Shanker, K.: Automatically Selecting Useful Phrases 
for Dialogue Act Tagging, In Proceedings of the Fourth Conference of the Pacific Associa-
tion for Computational Linguistics, (1999) 
11. Tanaka, H. and Yokoo, A.: An Efficient Statistical Speech Act Type Tagging System for 
Speech Translation Systems, In Proceedings of COLING-ACL99, (1999), 381-388 
12. Vapnik, V.: The Nature of Statistical Learning Theory, Springer Verlag, New York, 
(1995)  
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 153?156,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Extracting Comparative Sentences from Korean Text Documents Us-
ing Comparative Lexical Patterns and Machine Learning Techniques 
 
Seon Yang 
Department of Computer Engineering, 
Dong-A University,  
840 Hadan 2-dong, Saha-gu, 
Busan 604-714 Korea 
syang@donga.ac.kr 
Youngjoong Ko 
Department of Computer Engineering, 
Dong-A University,  
840 Hadan 2-dong, Saha-gu,  
Busan 604-714 Korea 
yjko@dau.ac.kr 
  
Abstract 
 
This paper proposes how to automatically 
identify Korean comparative sentences from 
text documents. This paper first investigates 
many comparative sentences referring to pre-
vious studies and then defines a set of compar-
ative keywords from them. A sentence which 
contains one or more elements of the keyword 
set is called a comparative-sentence candidate. 
Finally, we use machine learning techniques to 
eliminate non-comparative sentences from the 
candidates. As a result, we achieved signifi-
cant performance, an F1-score of 88.54%, in 
our experiments using various web documents. 
 
1 Introduction 
Comparing one entity with other entities is one 
of the most convincing ways of evaluation (Jin-
dal and Liu, 2006). A comparative sentence for-
mulates an ordering relation between two entities 
and that relation is very useful for many applica-
tion areas. One key area is for the customers. For 
example, a customer can make a decision on 
his/her final choice about a digital camera after 
reading other customers' product reviews, e.g., 
?Digital Camera X is much cheaper than Y 
though it functions as good as Y!? Another one 
is for manufacturers. All the manufacturers have 
an interest in the articles saying how their prod-
ucts are compared with competitors? ones.  
Comparative sentences often contain some 
comparative keywords. A sentence may express 
some comparison if it contains any comparative 
keywords such as ??? ([bo-da]: than)?, ??? 
([ga-jang]: most)?, ??? ([da-reu]: different)?, 
?? ([gat]: same)?. But many sentences also ex-
press comparison without those keywords.  Simi-
larly, although some sentences contain some 
keywords, they cannot be comparative sentences. 
By these reasons, extracting comparative sen-
tences is not a simple or easy problem. It needs 
more complicated and challenging processes 
than only searching out some keywords for ex-
tracting comparative sentences. 
Jindal and Liu (2006) previously studied to 
identify English comparative sentences. But the 
mechanism of Korean as an agglutinative lan-
guage and that of English as an inflecting lan-
guage have seriously different aspects. One of 
the greatest differences related to our work is that 
there are Part-of-Speech (POS) Tags for compar-
ative and superlative in English1, whereas, unfor-
tunately, the POS tagger of Korean does not pro-
vide any comparative and superlative tags be-
cause the analysis of Korean comparative is 
much more difficult than that of English. The 
major challenge of our work is therefore to iden-
tify comparative sentences without comparative 
and superlative POS Tags. 
We first survey previous studies about the Ko-
rean comparative syntax and collect the corpus 
of Korean comparative sentences from the Web. 
As we refer to previous studies and investigate 
real comparative sentences form the collected 
corpus, we can construct the set of comparative 
keywords and extract comparative-sentence can-
didates; the sentences which contain one or more 
element of the keyword set are called compara-
tive-sentence candidates. Then we use some ma-
chine learning techniques to eliminate non-
comparative sentences from those candidates. 
The final experimental results in 5-fold cross 
                                                 
1 JJR: adjective and comparative, JJS: adjective and superla-
tive, RBR: adverb and comparative, and RBS: adverb and 
superlative 
153
validation show the overall precision of 88.68% 
and the overall recall of 88.40%. 
The remainder of the paper is organized as fol-
lows. Section 2 describes the related work. In 
section 3, we explain comparative keywords and 
comparative-sentence candidates. In section 4, 
we describe how to eliminate non-comparative 
sentences from the candidates extracted in pre-
ceding section. Section 5 presents the experimen-
tal results. Finally, we discuss conclusions and 
future work in section 6 
 
2 Related Work 
We have not found any direct work on automati-
cally extracting Korean comparative sentences. 
There is only one study by Jindal and Liu (2006) 
that is related to English. They used comparative 
and superlative POS tags and additional some 
keywords to search English comparative sen-
tences. Then they used Class Sequential Rules 
and Na?ve Bayesian learning method. Their ex-
periment showed a precision of 79% and recall 
of 81%.  
Our research is closely related to linguistics. 
Ha (1999) described Korean comparative con-
structions with a linguistic view. Oh (2003) dis-
cussed the gradability of comparatives. Jeong 
(2000) classified the adjective superlative by the 
type of measures. 
Opinion mining is also related to our work. 
Many comparative sentences also contain the 
speaker?s opinions and especially comparison is 
one of the most powerful tools for evaluation. 
We have surveyed many studies about opinion 
mining (Lee et al, 2008; Kim and Hovy, 2006; 
Wilson and Wiebe, 2003; Riloff and Wiebe, 
2003; Esuli and Sebastiani, 2006).  
Maximum Entropy Model is used in our tech-
nique. Berger et al (1996) described Maximum 
Entropy approach to National Language 
Processing. In our experiments, we used Zhang?s 
Maximum Entropy Model Toolkit (2004). Na?ve 
Bayesian classifier is used to prove the perfor-
mance of MEM (McCallum and Nigam (1998)). 
 
3 Extracting Comparative-sentence 
Candidates 
In this section, we define comparative keywords 
and extract comparative-sentence candidates by 
using those keywords.  
3.1 Comparative keyword 
First of all, we classify comparative sentences 
into six types and then we extract single compar-
ative keywords from each type as follows: 
 
Table 1. The six types of comparative sentences 
 Type Single-keyword Examples 
1 Equality ??  ([gat]: same)? 
2 Similarity ????  ([bi-seut-ha]: similar)?
3 Difference ???  ([da-reu]: different)? 
4 Greater or 
lesser 
??? ([bo-da]: than)? 
5 Superlative ???  ([ga-jang]: most)? 
6 Predicative No single-keywords 
 
We can easily find such keywords from the vari-
ous sentences in first five types, while we cannot 
find any single keyword in the sentences of type 
6.  
 
Ex1) ?X?? ???? ????????, Y ?? 
??????.? ([X-gum-eui won-jae-ryo-neun 
cho-san-vi-nil-su-ji-in-de, Y-gum-eun cheon-
yeon-chi-kl-i-da]: Raw material of gum X is po-
lyvinyl acetate, but that of Y is natural chicle.)2 
 
And we can find many non-comparative sen-
tences which contain some keywords. The fol-
lowing example (Ex2) shows non-comparative 
though it contains ?? ([gat]: It means 'same', but 
it sometimes means 'think?)?. 
 
Ex2) ?? ??? ?? ?? ? ? ???.? ([Nae 
sang-gak-en nae-il bi-ga ol geot gat-a-yo]: I 
think it will rain tomorrow.) 
 
Thus all the sentences can be divided into four 
categories as follows: 
 
Table 2.  The four categories of the sentences 
Single-keyword Contain  Not contain  
Comparative  
Sentences  
S1 S2 
Non-comparative 
Sentences 
S3 S4 (unconcerned 
group) 
 
                                                 
2 In fact, type 6 can be sorted as non-comparative from lin-
guistic view. But the speaker is probably saying that Y is 
better than X. This is very important comparative data as an 
opinion. Therefore, we also regard the sentences containing 
implicit comparison as comparative sentences 
154
Our final goal is to find an effective method to 
extract S1 and S2, but single-keyword searching 
just outputs S1 and S3. In order to capture S2, we 
added long-distance-words sequences to the set 
of single-keywords. For example, we could ex-
tract ?<? [neun], ?? [in-de], ? [eun], ?? [i-
da]>? as a long-distance-words sequence from 
Ex1-sentence. It means that the sentence is 
formed as < S V but S V> in English (S: subject 
phrase, V: verb phrase). Thus we defined com-
parative keyword in this paper as follows: 
 
Definition (comparative keyword): A compara-
tive keyword is formed as a word or a phrase or 
a long-distance-words sequence. When a com-
parative keyword is contained in any sentence, 
the sentence is most likely to be a comparative 
sentence. (We will use an abbreviation ?CK?.) 
 
3.2 Comparative-sentence Candidates 
We finally set up a total of 177 CKs by human 
efforts. In the previous work, Jindal and Liu 
(2006) defined 83 keywords and key phrases in-
cluding comparative or superlative POS tags in 
English; they did not use any long-distance-
words sequence.  
Keyword searching process can detect most of 
comparative sentences (S1, S2 and S3)3  from 
original text documents. That is, the recall is high 
but the precision is low. We here defined a com-
parative-sentence candidate as a sentence which 
contains one or more elements of the set of CKs. 
Now we need to eliminate the incorrect sen-
tences (S3) from those captured sentences. First, 
we divided the set of CKs into two subsets de-
noted by CKL1 and CKL2 according to the pre-
cision of each keyword; we used 90% of the pre-
cision as a threshold value. The average preci-
sion of comparative-sentence candidates with a 
CKL1 keyword is 97.44% and they do not re-
quire any additional process. But that of compar-
ative-sentence candidates with a CKL2 keyword 
is 29.34% and we decide to eliminate non-
comparative sentences only from comparative 
sentence candidates with a CKL2 keyword. 
 
4 Eliminating Non-comparative Sen-
tences from the Candidates  
 
                                                 
3 As you can see in the experiment section, keyword search-
ing captures 95.96% comparative sentences.  
To effectively eliminate non-comparative sen-
tences from comparative sentence candidates 
with a CKL2 keyword, we employ machine 
learning techniques (MEM and Na?ve Bayes). 
For feature extraction from each comparative-
sentence candidate, we use continuous words 
sequence within the radius of 3 (the window size 
of 7) of each keyword in the sentence; we expe-
rimented with radius options of 2, 3, and 4 and 
we achieved the best performance in the radius 
of 3. After determining the radius, we replace 
each word with its POS tag; in order to reflect 
various expressions of each sentence, POS tags 
are more proper than lexical information of ac-
tual words. However, since CKs play the most 
important role to discriminate comparative sen-
tences, they are represented as a combination of 
their actual keyword and POS tag. Thus our fea-
ture is formed as ?X ? y?. (?X? means a se-
quence and ?y? means a class; y1 denotes com-
parative and y2 denotes non-comparative). For 
instance,  ?<pv etm nbn ?/pa ep ef sf >4? y2? is 
one of the features from the sentence of Ex2 in 
section 3.1. 
5 Experimental Results  
Three trained human annotators compiled a cor-
pus of 277 online documents from various do-
mains. They discussed their disagreements and 
they finally annotated 7,384 sentences. Table 3 
shows the number of comparative sentences and 
non-comparative sentences in our corpus. 
 
Table 3. The numbers of annotated sentences 
Total Comparative Non-comparative 
7,384 2,383 (32%) 5,001 (68%) 
 
Before evaluating our proposed method, we 
conducted some experiments by machine learn-
ing techniques with all the unigrams of total ac-
tual words as baseline systems; they do not use 
any CKs. The precision, recall and F1-score of 
the baseline systems are shown at Table 4. 
 
Table 4. The results of baseline systems (%) 
Baseline
System 
Precision Recall F1-score 
NB 35.98 91.62 51.66 
MEM 78.17 63.34 69.94 
 
The final overall results using the 5-fold cross 
validation are shown in Table 5 and Figure 1. 
                                                 
4 The labels such as ?pv?, ?etm?, ?nbn?, etc. are Korean POS 
Tags 
155
 
Table 5. The results of our proposed method (%) 
Method Preci-
sion 
Recall F1-score 
CKs only 68.39 95.96 79.87 
CKs + NB 85.42 88.59 86.67 
CKs + MEM 88.68 88.40 88.54 
 
 
Fig. 1 The results of our proposed method (%) 
 
As shown in Table 5 and Figure 1, both of MEM 
and NB is shown good performance but the F1-
score of MEM is little higher than that of NB. By 
applying machine learning technique to our me-
thod, we can achieve high precision while we 
can preserve high recall. 
 
6 Conclusions and Future Work 
In this paper, we have presented how to extract 
comparative sentences from Korean text docu-
ments by keyword searching process and ma-
chine learning techniques. Our experimental re-
sults showed that our proposed method can be 
effectively used to identify comparative sen-
tences. Since the research of comparison mining 
is currently in the beginning step in the world, 
our proposed techniques can contribute much to 
text mining and opinion mining research. 
In our future work, we plan to classify com-
parative types and to extract comparative rela-
tions from identified comparative sentences.  
 
Acknowledgement  
This paper was supported by the Korean Re-
search Foundation Grant funded by the Korean 
Government (KRF-2008-331-D00553) 
 
References  
Adam L. Berger et al 1996. A Maximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-71. 
Andrea Esuli and Fabrizio Sebastiani. 2006. Deter-
mining Term Subjectivity and Term Orientation for 
Opinion Mining. European Chapter of the Associa-
tion for Computational Linguistics, 193-200. 
 Andrew McCallum and Kamal Nigam. 1998. A 
Comparison of Event Models for Na?ve Bayes Text 
Classification. Association for Advancement of Ar-
tificial Intelligence, 41-48. 
Dong-joo Lee et al 2008. Opinion Mining of Cus-
tomer Feedback Data on the Web. International 
Conference on Ubiquitous Information Manage-
ment and Community, 247-252. 
Ellen Riloff and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Em-
pirical Methods in Natural Language Processing. 
Gil-jong Ha. 1999. Korean Modern Comparative Syn-
tax, Pijbook Press, Seoul, Korea. 
Gil-jong Ha. 1999. Research on Korean Equality 
Comparative Syntax, Association for Korean Lin-
guistics, 5:229-265. 
In-su Jeong. 2000. Research on Korean Adjective 
Superlative Comparative Syntax. Korean Han-min-
jok Eo-mun-hak, 36:61-86. 
Kyeong-sook Oh. 2004. The Difference between 
?Man-kum? Comparative and ?Cheo-rum? Compar-
ative. Society of Korean Semantics, 14:197-221. 
 Nitin Jindal and Bing Liu. 2006. Identifying Com-
parative Sentences in Text Documents, Association 
for Computing Machinery/Special Interest Group 
on Information Retrieval, 244-251. 
Nitin Jindal and Bing Liu. 2006. Mining Comparative 
Sentences and Relations, Association for Ad-
vancement of Artificial Intelligence, 1331-1336. 
Soomin Kim and Eduard Hovy. 2006. Automatic De-
tection of Opinion Bearing Words and Sentences. 
Computational Linguistics/Association for Compu-
tational Linguistics. 
Theresa Wilson and Janyce Wiebe. 2003. Annotating 
Opinions in the World Press. Special Interest 
Group in Discourse and Dialoque/Association for 
Computational Linguistics. 
Zhang Le. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++. http://homepages.inf.ed.ac. 
uk/s0450736/maxent_toolkit.html. 
156
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1636?1644,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Comparative Entities and Predicates from Texts Using 
Comparative Type Classification  
 
 
Seon Yang Youngjoong Ko 
Department of Computer Engineering, Department of Computer Engineering, 
Dong-A University, Dong-A University, 
Busan, Korea Busan, Korea 
seony.yang@gmail.com yjko@dau.ac.kr 
 
 
 
 
 
 
Abstract 
The automatic extraction of comparative in-
formation is an important text mining 
problem and an area of increasing interest. 
In this paper, we study how to build a 
Korean comparison mining system. Our 
work is composed of two consecutive tasks: 
1) classifying comparative sentences into 
different types and 2) mining comparative 
entities and predicates. We perform various 
experiments to find relevant features and 
learning techniques. As a result, we achieve 
outstanding performance enough for 
practical use.  
1 Introduction 
Almost every day, people are faced with a situation 
that they must decide upon one thing or the other. 
To make better decisions, they probably attempt to 
compare entities that they are interesting in. These 
days, many web search engines are helping people 
look for their interesting entities. It is clear that 
getting information from a large amount of web 
data retrieved by the search engines is a much 
better and easier way than the traditional survey 
methods. However, it is also clear that directly 
reading each document is not a perfect solution. If 
people only have access to a small amount of data, 
they may get a biased point of view. On the other 
hand, investigating large amounts of data is a time-
consuming job. Therefore, a comparison mining 
system, which can automatically provide a 
summary of comparisons between two (or more) 
entities from a large quantity of web documents, 
would be very useful in many areas such as 
marketing.  
We divide our work into two tasks to effectively 
build a comparison mining system. The first task is 
related to a sentence classification problem and the 
second is related to an information extraction 
problem. 
  
Task 1. Classifying comparative sentences into 
one non-comparative class and seven 
comparative classes (or types); 1) Equality, 2) 
Similarity, 3) Difference, 4) Greater or lesser, 5) 
Superlative, 6) Pseudo, and 7) Implicit 
comparisons. The purpose of this task is to 
efficiently perform the following task. 
Task 2. Mining comparative entities and 
predicates taking into account the characteristics 
of each type. For example, from the sentence 
?Stock-X is worth more than stock-Y.? belonging 
to ?4) Greater or lesser? type, we extract ?stock-
X? as a subject entity (SE), ?stock-Y? as an 
object entity (OE), and ?worth? as a comparative 
predicate (PR).  
  
These tasks are not easy or simple problems as 
described below.  
  
 Classifying comparative sentences (Task 1): For 
the first task, we extract comparative sentences 
from text documents and then classify the 
extracted comparative sentences into seven 
1636
comparative types. Our basic idea is a keyword 
search. Since Ha (1999a) categorized dozens of 
Korean comparative keywords, we easily build an 
initial keyword set as follows: 
  
? ?ling = {?? ([gat]: same)?, ??? ([bo-da]: than)?, 
??? ([ga-jang]: most)?, ?}  
  
In addition, we easily match each of these 
keywords to a particular type anchored to Ha?s 
research, e.g., ?? ([gat]: same)? to ?1) Equality?, 
??? ([bo-da]: than)? to ?4) Greater or lesser?. 
However, any method that depends on just these 
linguistic-based keywords has obvious limitations 
as follows: 
  
1)  ?ling is insufficient to cover all of the actual 
comparison expressions. 
2) There are many non-comparative sentences 
that contain some elements of ?ling. 
3) There is no one-to-one relationship between 
keyword types and sentence types. 
  
Mining comparative entities and predicates 
(Task 2): Our basic idea for the second task is 
selecting candidates first and finding answers from 
the candidates later. We regard each of noun words 
as a candidate for SE/OE, and each of adjective (or 
verb) words as a candidate for PR. However, this 
candidate detection has serious problems as 
follows:  
  
4) There are many actual SEs, OEs, and PRs that 
consist of multiple words. 
5) There are many sentences with no OE, 
especially among superlative sentences. It 
means that the ellipsis is frequently occurred in 
superlative sentences. 
  
We focus on solving the above five problems. 
We perform various experiments to find relevant 
features and proper machine learning techniques. 
The final experimental results in 5-fold cross 
validation show the overall accuracy of 88.59% for 
the first task and the overall accuracy of 86.81% 
for the second task. 
The remainder of the paper is organized as 
follows. Section 2 briefly introduces related work. 
Section 3 and Section 4 describe our first task and 
second task in detail, respectively. Section 5 
reports our experimental results and finally Section 
6 concludes. 
2 Related Work 
Linguistic researchers focus on defining the syntax 
and semantics of comparative constructs. Ha 
(1999a; 1999b) classified the structures of Korean 
comparative sentences into several classes and 
arranged comparison-bearing words from a 
linguistic perspective. Since he summarized the 
modern Korean comparative studies, his research 
helps us have a linguistic point of view. We also 
refer to Jeong (2000) and Oh (2004). Jeong 
classified adjective superlatives using certain 
measures, and Oh discussed the gradability of 
comparatives. 
In computer engineering, we found five previous 
studies related to comparison mining. Jindal and 
Liu (2006a; 2006b) studied to mine comparative 
relations from English text documents. They used 
comparative and superlative POS tags, and some 
additional keywords. Their methods applied Class 
Sequential Rules and Label Sequential Rules. 
Yang and Ko (2009; 2011) studied to extract 
comparative sentences in Korean text documents. 
Li et al (2010) studied to mine comparable entities 
from English comparative questions that users 
posted online. They focused on finding a set of 
comparable entities given a user?s input entity.  
Opinion mining is also related to our work 
because many comparative sentences also contain 
the speaker?s opinion/sentiment. Lee et al (2008) 
surveyed various techniques that have been 
developed for the key tasks of opinion mining. 
Kim and Hovy (2006) introduced a methodology 
for analyzing judgment opinion. Riloff and Wiebe 
(2003) presented a bootstrapping process that 
learns linguistically rich extraction patterns for 
subjective expressions.  
In this study, three learning techniques are 
employed: the maximum entropy method (MEM) 
as a representative probabilistic model, the support 
vector machine (SVM) as a kernel model, and 
transformation-based learning (TBL) as a rule-
based model. Berger et al (1996) presented a 
Maximum Entropy Approach to natural language 
processing. Joachims (1998) introduced SVM for 
text classification. Various TBL studies have been 
performed. Brill (1992; 1995) first introduced TBL 
and presented a case study on part-of-speech 
1637
tagging. Ramshaw and Marcus (1995) applied 
TBL for locating chunks in tagged texts. Black and 
Vasilakopoulos (2002) used a modified TBL 
technique for Named Entity Recognition.  
3 Classifying Comparative Sentences 
(Task 1) 
We first classify the sentences into comparatives 
and non-comparatives by extracting only 
comparatives from text documents. Then we 
classify the comparatives into seven types.  
3.1 Extracting comparative sentences from 
text documents 
Our strategy is to first detect Comparative 
Sentence candidates (CS-candidates), and then 
eliminate non-comparative sentences from the 
candidates. As mentioned in the introduction 
section, we easily construct a linguistic-based 
keyword set, ?ling. However, we observe that ?ling 
is not enough to capture all the actual comparison 
expressions. Hence, we build a comparison lexicon 
as follows: 
  
? Comparison Lexicon = ?ling U {Additional 
keywords that are frequently used for actual 
comparative expressions} 
  
This lexicon is composed of three parts. The first 
part includes the elements of ?ling and their 
synonyms. The second part consists of idioms. For 
example, an idiom ?X? ?? ??? [X-ga meon-jeo 
u-seot-da]? commonly means ?The winner is X? 
while it literally means ?X laughed first?. The last 
part consists of long-distance-words sequences, 
e.g., ?<X? [X-neun], ?? [ji-man], Y? [Y-neun], ? 
[da]>?. This sequence means that the sentence is 
formed as < S(X) + V + but + S(Y) + V > in 
English (S: subject phrase; V: verb phrase; X, Y: 
proper nouns). We could regard a word, ??? ([ji-
man]: but),? as a single keyword. However, this 
word also captures numerous non-comparative 
sentences. Namely, the precision value can fall too 
much due to this word. By using long-distance-
words sequences instead of single keywords, we 
can keep the precision value from dropping 
seriously low. 
The comparison lexicon finally has a total of 
177 elements. We call each element ?CK? 
hereafter. Note that our lexicon does not include 
comparative/superlative POS tags. Unlike English, 
there is no Korean comparative/superlative POS 
tag from POS tagger commonly. Our lexicon 
covers 95.96% of the comparative sentences in our 
corpus. It means that we successfully defined a 
comparison lexicon for CS-candidate detection. 
However, the lexicon shows a relatively low 
precision of 68.39%. While detecting CS-
candidates, the lexicon also captures many non-
comparative sentences, e.g., following Ex1: 
   
? Ex1. ???? ??? ?? ? ??.? ([nai-il-eun ju-
sik-i o-reul-geot gat-da]: I think stock price will 
rise tomorrow.)  
  
This sentence is a non-comparative sentence even 
though it contains a CK, ??[gat].? This CK 
generally means ?same,? but it often expresses 
?conjecture.? Since it is an adjective in both cases, 
it is difficult to distinguish the difference. 
To effectively filter out non-comparative 
sentences from CS-candidates, we use the 
sequences of ?continuous POS tags within a radius 
of 3 words from each CK? as features. Each word 
in the sequence is replaced with its POS tag in 
order to reflect various expressions. However, as 
CKs play the most important role, they are 
represented as a combination of their lexicalization 
and POS tag, e.g., ??/pa1.? Finally, the feature has 
the form of ?X ? y? (?X? means a sequence and 
?y? means a class; y1: comparative, y2: non-
comparative). For instance, ?<pv etm nbn ?/pa ef 
sf 2  >? y2? is one of the features from Ex1 
sentence. Finally, we achieved an f1-score of 
90.23% using SVM. 
3.2 Classifying comparative sentences into 
seven types 
As we extract comparative sentences successfully, 
the next step is to classify the comparatives into 
different types. We define seven comparative types 
and then employ TBL for comparative sentence 
classification.  
We first define six broad comparative types 
based on modern Korean linguistics: 1) Equality, 
2) Similarity, 3) Difference, 4) Greater or lesser, 
5) Superlative, 6) Pseudo comparisons. The first 
five types can be understood intuitively, whereas 
                                                          
1 The POS tag ?pa? means ?the stem of an adjective?.  
2 The labels such as ?pv?, ?etm? are Korean POS Tags.  
1638
the sixth type needs more explanation. ?6) Pseudo? 
comparison includes comparative sentences that 
compare two (or more) properties of one entity 
such as ?Smartphone-X is a computer rather than a 
phone.? This type of sentence is often classified 
into ?4) Greater or lesser.? However, since this 
paper focuses on comparisons between different 
entities, we separate ?6) Pseudo? type from ?4) 
Greater or lesser? type.  
The seventh type is ?7) Implicit? comparison.  It 
is added with the goal of covering literally 
?implicit? comparisons. For example, the sentence 
?Shopping Mall X guarantees no fee full refund, 
but Shopping Mall Y requires refund-fee? does not 
directly compare two shopping malls. It implicitly 
gives a hint that X is more beneficial to use than Y. 
It can be considered as a non-comparative sentence 
from a linguistic point of view. However, we 
conclude that this kind of sentence is as important 
as the other explicit comparisons from an 
engineering point of view.  
After defining the seven comparative types, we 
simply match each sentences to a particular type 
based on the CK types; e.g., a sentence which 
contains the word ??? ([ga-jang]: most)? is 
matched to ?Superlative? type. However, a method 
that uses just the CK information has a serious 
problem. For example, although we easily match 
the CK ??? ([bo-da]: than)? to ?Greater or lesser? 
without doubt, we observe that the type of CK 
itself does not guarantee the correct type of the 
sentence as we can see in the following three 
sentences: 
  
? Ex2. ?X? ??? Y?? ??? ???? ??.? ([X-
eui pum-jil-eun Y-bo-da jo-chi-do na-ppeu-ji-do an-
ta]: The quality of X is neither better nor worse 
than that of Y.) ? It can be interpreted as ?The 
quality of X is similar to that of Y.? (Similarity) 
? Ex3. ?X? Y?? ??? ??.? ([X-ga Y-bo-da pum-
jil-I jo-ta]:  The quality of X is better than that of 
Y.) ?  It is consistent with the CK type 
(Greater or lesser) 
? Ex4. ?X? ?? ?? ????? ??? ??.? ([X-
neun  da-reun eo-tteon ka-me-ra-bo-da pum-jil-i  jo-
ta]: X is better than any other cameras in 
quality.) ? It can be interpreted as ?X is the 
best camera in quality.? (Superlative) 
   
If we only rely on the CK type, we should label the 
above three sentences as ?Greater or lesser?. 
However, each of these three sentences belongs to 
a different type. This fact addresses that many CKs 
could have an ambiguity problem just like the CK 
of ??? ([bo-da]: than).?  
To solve this ambiguity problem, we employ 
TBL. We first roughly annotate the type of 
sentences using the type of CK itself. After this 
initial annotating, TBL generates a set of error-
driven transformation rules, and then a scoring 
function ranks the rules. We define our scoring 
function as Equation (1): 
  
Score(ri) = Ci - Ei                      (1) 
  
Here, ri is the i-th transformation rule, Ci is the 
number of corrected sentences after ri is applied, 
and Ei is the number of the opposite case. The 
ranking process is executed iteratively. The 
iterations stop when the scoring function reaches a 
certain threshold. We finally set up the threshold 
value as 1 after tuning. This means that we use 
only the rules whose score is 2 or more. 
4 Mining Comparative Entities and 
Predicates (Task 2) 
This section explains how to extract comparative 
entities and predicates. Our strategy is to first 
detect Comparative Element candidates (CE-
candidates), and then choose the answer among the 
candidates.  
In this paper, we only present the results of two 
types: ?Greater or lesser? and ?Superlative.? As 
we will see in the experiment section, these two 
types cover 65.8% of whole comparative sentences. 
We are still studying the other five types and plan 
to report their results soon. 
4.1 Comparative elements 
We extract three kinds of comparative elements in 
this paper: SE, OE and PR 
  
? Ex5. ?X??? Y???? ?? ???.? ([X-pa-i-ga 
Y-pa-i-bo-da ssa-go mas-it-da]: Pie X is cheaper 
and more delicious than Pie Y.) 
? Ex6. ??? ??? ? Z ? ?? ?????.? ([dai-
seon hu-bo-deul jung Z-ga ga-jang mit-eum-jik-
ha-da]: ?Z is the most trustworthy among the 
presidential candidates.?) 
  
1639
In Ex5 sentence, ?X?? (Pie X)? is a SE, ?Y?? 
(Pie Y)? is an OE, and ??? ??? (cheaper and 
more delicious)? is a PR. In Ex6 sentence, ?Z? is a 
SE, ??? ??? (the presidential candidates)? is an 
OE, and ?????? (trustworthy)? is a PR.  
Note that comparative elements are not limited 
to just one word. For example, ??? ??? 
(cheaper and more delicious)? and ??? ??? (the 
presidential candidates)? are composed of multiple 
words. After investigating numerous actual 
comparison expressions, we conclude that SEs, 
OEs, and PRs should not be limited to a single 
word.  It can miss a considerable amount of 
important information to restrict comparative 
elements to only one word. Hence, we define as 
follows: 
  
? Comparative elements (SE, OE, and PR) are 
composed of one or more consecutive words. 
  
It should also be noted that a number of superlative 
sentences are expressed without OE. In our corpus, 
the percentage of the Superlative sentences without 
any OE is close to 70%. Hence, we define as 
follows: 
  
? OEs can be omitted in the Superlative sentences. 
  
4.2 Detecting CE-candidates 
As comparative elements are allowed to have 
multiple words, we need some preprocessing steps 
for easy detection of CE-candidates. We thus apply 
some simplification processes. Through the 
simplification processes, we represent potential 
SEs/OEs as one ?N? and potential PRs as one ?P?. 
The following process is one of the simplification 
processes for making ?N? 
  
- Change each noun (or each noun compound) to 
a symbol ?N?. 
  
And, the following two example processes are for 
?P?. 
  
- Change ?pa (adjective)? and ?pv (verb)? to a 
symbol ?P?. 
- Change ?P + ecc (a suffix whose meaning is 
?and?) + P? to one ?P?, e.g., ?cheaper and 
more delicious? is tagged as one ?P?. 
  
In addition to the above examples, several 
processes are performed. We regard all the ?N?s as 
CE-candidates for SE/OE and all the ?P?s as CE-
candidates for PR. It is possible that a more 
analytic method is used instead of this 
simplification task, e.g., by a syntactic parser. We 
leave this to our future work.  
4.3 Finding final answers  
We now generate features. The patterns that 
consist of POS tags, CKs, and ?P?/?N? sequences 
within a radius of 4 POS tags from each ?N? or 
?P? are considered as features.  
  
Original  
sentence 
?X??? Y???? ?? ???.? 
(Pie X is cheaper and more 
delicious than Pie Y.) 
After POS 
tagging 
X??/nq + ?/jcs + Y??/nq + 
??/jca + ?/pa + ?/ecc + ??/pa + 
?/ef +./sf 
After 
simplification 
process 
X??/N(SE) + ?/jcs +  
Y??/N(OE) + ??/jca + 
?????/P(PR) + ./sf 
Patterns for  
SE  
<N(SE), jcs, N, ??/jca,P>, ?, 
<N(SE), jcs> 
Patterns for 
OE  
<N, jcs, N(OE), ??/jca,P, sf>, ?, 
<N(OE), ??/jca > 
Patterns for  
PR  
<N, jcs, N, ??/jca,P(PR), sf>, ?, 
<P(PR), sf> 
  
Table 1: Feature examples for mining comparative 
elements 
  
Table 1 lists some examples. Since the CKs play 
an important role, they are represented as a 
combination of their lexicalization and POS tag. 
After feature generation, we calculate each 
probability value of all CE-candidates using SVM. 
For example, if a sentence has three ?P?s, one ?P? 
with the highest probability value is selected as the 
answer PR. 
5 Experimental Evaluation  
5.1 Experimental Settings 
The experiments are conducted on 7,384 sentences 
collected from the web by three trained human 
labelers. Firstly, two labelers annotated the corpus. 
A Kappa value of 0.85 showed that it was safe to 
say that the two labelers agreed in their judgments. 
1640
Secondly, the third labeler annotated the 
conflicting part of the corpus. All three labelers 
discussed any conflict, and finally reached an 
agreement. Table 2 lists the distribution of the 
corpus. 
  
Comparative  
Types 
Sentence 
Portion 
Non-comparative: 5,001 (67.7%) 
Comparative: 2,383 (32.3%) 
Total (Corpus) 7,384 (100%) 
Among  
Comparative 
Sentences 
 
1) Equality 3.6% 
2) Similarity 7.2% 
3) Difference 4.8% 
4) Greater or lesser 54.5% 
5) Superlative 11.3% 
6) Pseudo  1.3% 
7) Implicit 17.5% 
Total (Comparative) 100% 
  
Table 2: Distribution of the corpus 
  
5.2 Classifying comparative sentences  
Our experimental results for Task 1 showed an f1-
score of 90.23% in extracting comparative 
sentences from text documents and an accuracy of 
81.67% in classifying the comparative sentences 
into seven comparative types.  
The integrated results showed an accuracy of 
88.59%. Non-comparative sentences were regarded 
as an eighth comparative type in this integrated 
result. It means that we classify entire sentences 
into eight types (seven comparative types and one 
non-comparative type). 
5.2.1   Extracting comparative sentences. 
Before evaluating our proposed method for 
comparative sentence extraction, we conducted 
four experiments with all of the lexical unigrams 
and bigrams using MEM and SVM. Among these 
four cases, SVM with lexical unigrams showed the 
highest performance, an f1-score of 79.49%. We 
regard this score as our baseline performance.  
Next, we did experiments using all of the 
continuous lexical sequences and using all of the 
POS tags sequences within a radius of n words 
from each CK as features (n=1,2,3,4,5). Among 
these ten cases, ?the POS tags sequences within a 
radius of 3? showed the best performance. Besides, 
as SVM showed the better performance than MEM 
in overall experiments, we employ SVM as our 
proposed learning technique. Table 3 summarizes 
the overall results. 
  
Systems Precision Recall F1-score 
baseline 87.86 72.57 79.49 
comparison lexicon 
only 
68.39 95.96 79.87 
comparison lexicon  
& SVM  
(proposed) 
92.24 88.31 90.23 
  
Table 3: Final results in comparative sentence 
extraction (%) 
  
As given above, we successfully detected CS-
candidates with considerably high recall by using 
the comparison lexicon. We also successfully 
filtered the candidates with high precision while 
still preserving high recall by applying machine 
learning technique. Finally, we could achieve an 
outstanding performance, an f1-score of 90.23%. 
5.2.2   Classifying comparative sentences into 
seven types. 
Like the previous comparative sentence extraction 
task, we also conducted experiments for type 
classification using the same features (continuous 
POS tags sequences within a radius of 3 words 
from each CK) and the same learning technique 
(SVM). Here, we achieved an accuracy of 73.64%. 
We regard this score as our baseline performance.  
Next, we tested a completely different technique, 
the TBL method. TBL is well-known to be 
relatively strong in sparse problems. We observed 
that the performance of type classification can be 
influenced by very subtle differences in many 
cases. Hence, we think that an error-driven 
approach can perform well in comparative type 
classification. Experimental results showed that 
TBL actually performed better than SVM or MEM.  
In the first step, we roughly annotated the type 
of a sentence using the type of the CK itself. Then, 
we generated error-driven transformation rules 
from the incorrectly annotated sentences. 
Transformation templates we defined are given in 
Table 4. Numerous transformation rules were 
generated on the basis of the templates. For 
example, ?Change the type of the current sentence 
from ?Greater or lesser? to ?Superlative? if this 
sentence holds the CK of ??? ([bo-da]: than)?, 
1641
and the second preceding word of the CK is tagged 
as mm? is a transformation rule generated by the 
third template. 
  
Change the type of the current sentence from x to y if 
this sentence holds the CK of k, and ? 
1. the preceding word of k is tagged z. 
2. the following word of k is tagged z. 
3. the second preceding word of k is tagged z. 
4. the second following word of k is tagged z. 
5. the preceding word of k is tagged z, and the 
following word of k is tagged w. 
6. the preceding word of k is tagged z, and the 
second preceding word of k is tagged w. 
7. the following word of k is tagged z, and the 
second following word of k is tagged w. 
  
Table 4: Transformation templates 
  
For evaluation of threshold values, we 
performed experiments with three options as given 
in Table 5.  
  
Threshold 0 1 2 
Accuracy 79.99 81.67 80.04 
  
Table 5: Evaluation of threshold option (%); 
Threshold n means that the learning iterations continues while 
Ci-Ei ? n+1 
  
We achieved the best performance with the 
threshold option 1. Finally, we classified 
comparative sentences into seven types using TBL 
with an accuracy of 81.67%.  
5.2.3   Integrated results of Task 1 
We sum up our proposed method for Task 1 as two 
steps as follows; 
  
 1) The comparison lexicon detects CS-candidates 
in text documents, and then SVM eliminates 
the non-comparative sentences from the 
candidates. Thus, all of the sentences are 
divided into two classes: a comparative class 
and a non-comparative class. 
 2) TBL then classifies the sentences placed in the 
comparative class in the previous step into 
seven comparative types.  
  
The integrated results showed an overall accuracy 
of 88.59% for the eight-type classification. To 
evaluate the effectiveness of our two-step 
processing, we performed one-step processing 
experiments using SVM and TBL. Table 6 shows a 
comparison of the results.  
  
Processing Accuracy 
One-step 
processing 
(classifying eight 
types at a time) 
comparison 
lexicon & SVM 
75.64 
comparison 
lexicon & TBL 
72.49 
Two-step processing  
(proposed) 
88.59 
  
Table 6: Integrated results for Task 1 (%) 
  
As shown above, Task 1 was successfully divided 
into two steps.  
5.3 Mining comparative entities and 
predicates 
For the mining task of comparative entities and 
predicates, we used 460 comparative sentences 
(Greater or lesser: 300, Superlative: 160). As 
previously mentioned, we allowed multiple-word 
comparative elements. Table 7 lists the portion of 
multiple-word comparative elements.  
  
Multi-word rate SE OE PR 
Greater or lesser 30.0 31.3 8.3 
Superlative 24.4 
9.4 
(32.6) 
8.1 
  
Table 7: Portion (%) of multiple-word comparative 
elements 
   
As given above, each multiple-word portion, 
especially in SEs and OEs, is quite high. This fact 
proves that it is absolutely necessary to allow 
multiple-word comparative elements. Relatively 
lower rate of 9.4% in Superlative-OEs is caused by 
a number of omitted OEs. If sentences that do not 
have any OEs are excluded, the portion of 
multiple-words becomes 32.6% as written in 
parentheses. 
Table 8 shows the effectiveness of simplification 
processes. We calculated the error rates of CE-
candidate detection before and after simplification 
processes.  
  
1642
Simplification 
processes 
SE OE PR 
Greater or 
lesser 
Before 34.7 39.3 10.0 
After 4.7 8.0 1.7 
Superlative 
Before 26.3 
85.0 
(38.9) 
9.4 
After 1.9 
75.6 
(6.3) 
1.3 
  
Table 8: Error rate (%) in CE-candidate detection 
  
Here, the first value of 34.7% means that the real 
SEs of 104 sentences (among total 300 Greater or 
lesser sentences) were not detected by CE-
candidate detection before simplification processes. 
After the processes, the error rate decreased to 
4.7%. The significant differences between before 
and after indicate that we successfully detect CE-
candidates through the simplification processes. 
Although the Superlative-OEs still show the 
seriously high rate of 75.6%, it is also caused by a 
number of omitted OEs. If sentences that do not 
have any OEs are excluded, the error rate is only 
6.3% as written in parentheses.  
The final results for Task 2 are reported in Table 
9. We calculated each probability of CE-candidates 
using MEM and SVM. Both MEM and SVM 
showed outstanding performance; there was no 
significant difference between the two machine 
learning methods (SVM and MEM). Hence, we 
only report the results of SVM. Note that many 
sentences do not contain any OE. To identify such 
sentences, if SVM tagged every ?N? in a sentence 
as ?not OE?, we tagged the sentence as ?no OE?.  
  
Final Results SE OE PR 
Greater or lesser 86.00 89.67 92.67 
Superlative 84.38 71.25 90.00 
Total 85.43 83.26 91.74 
  
Table 9: Final results of Task 2 (Accuracy, %) 
  
As shown above, we successfully extracted the 
comparative entities and predicates with 
outstanding performance, an overall accuracy of 
86.81%.  
6 Conclusions and Future Work 
This paper has studied a Korean comparison 
mining system. Our proposed system achieved an 
accuracy of 88.59% for classifying comparative 
sentences into eight types (one non-comparative 
type and seven comparative types), and an 
accuracy of 86.81% for mining comparative 
entities and predicates. These results demonstrated 
that our proposed method could be used effectively 
in practical applications. Since the comparison 
mining is an area of increasing interest around the 
world, our study can contribute greatly to text 
mining research. 
In our future work, we have the following plans. 
Our first plan is to complete the mining process on 
all the types of sentences. The second one is to 
conduct more experiments for obtaining better 
performance. The final one is about an integrated 
system. Since we perform Task 1 and Task 2 
separately, we need to build an end-to-end system.  
Acknowledgment  
This research was supported by Basic Science 
Research Program through the National Research 
Foundation of Korea (NRF) funded by the 
Ministry of Education, Science and Technology 
(2010-0015613) 
References  
Adam L. Berger, Stephen A. Della Pietra and Vicent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational 
Linguistics, 22(1):39-71. 
William J. Black and Argyrios Vasilakopoulos. 2002. 
Language-Independent named Entity Classification 
by modified Transformation-based Learning and by 
Decision Tree Induction. In Proceedings of 
CoNLL?02, 24:1-4. 
Eric Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of ANLP?92, 152-155. 
Eric Brill. 1995. Transformation-based Error-Driven 
Learning and Natural language Processing: A Case 
Study in Part-of-Speech tagging. Computational 
Linguistics, 543-565. 
Gil-jong Ha. 1999a. Korean Modern Comparative 
Syntax, Pijbook Press, Seoul, Korea. 
Gil-jong Ha. 1999b. Research on Korean Equality 
Comparative Syntax, Association for Korean 
Linguistics, 5:229-265. 
In-su Jeong. 2000. Research on Korean Adjective 
Superlative Comparative Syntax. Korean Han-min-
jok Eo-mun-hak, 36:61-86. 
1643
Nitin Jindal and Bing Liu. 2006. Identifying 
Comparative Sentences in Text Documents, In 
Proceedings of SIGIR?06, 244-251. 
Nitin Jindal and Bing Liu. 2006. Mining Comparative 
Sentences and Relations, In Proceedings of AAAI?06, 
1331-1336. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
relevant Features. In Proceedings of ECML?98, 137-
142 
Soomin Kim and Eduard Hovy. 2006. Automatic 
Detection of Opinion Bearing Words and Sentences. 
In Proceedings of ACL?06. 
Dong-joo Lee, OK-Ran Jeong and Sang-goo Lee. 2008. 
Opinion Mining of Customer Feedback Data on the 
Web. In Proceedings of ICUIMC?08, 247-252. 
Shasha Li, Chin-Yew Lin, Young-In Song and Zhoujun 
Li. 2010. Comparable Entity Mining from 
Comparative Questions. In Proceedings of ACL?10, 
650-658. 
Kyeong-sook Oh. 2004. The Difference between ?Man-
kum? Comparative and ?Cheo-rum? Comparative. 
Society of Korean Semantics, 14:197-221. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text 
Chunking using Transformation-Based Learning. In 
Proceedings of NLP/VLC?95, 82-94. 
Ellen Riloff and Janyce Wiebe. 2003. Learning 
Extraction Patterns for Subjective Expressions. In 
Proceedings of EMNLP?03. 
Seon Yang and Youngjoong Ko. 2009. Extracting 
Comparative Sentences from Korean Text 
Documents Using Comparative Lexical Patterns and 
Machine Learning Techniques. In Proceedings of 
ACL-IJNLP:Short Papers, 153-156 
Seon Yang and Youngjoong Ko. 2011. Finding relevant 
features for Korean comparative sentence extraction. 
Pattern Recognition Letters, 32(2):293-296 
1644
