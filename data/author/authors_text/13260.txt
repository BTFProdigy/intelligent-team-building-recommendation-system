Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 600?608,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Prenominal Modifier Ordering via Multiple Sequence Alignment
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
Producing a fluent ordering for a set of
prenominal modifiers in a noun phrase
(NP) is a problematic task for natural lan-
guage generation and machine translation
systems. We present a novel approach
to this issue, adapting multiple sequence
alignment techniques used in computa-
tional biology to the alignment of modi-
fiers. We describe two training techniques
to create such alignments based on raw
text, and demonstrate ordering accuracies
superior to earlier reported approaches.
1 Introduction
Natural language generation and machine trans-
lation systems must produce text which not only
conforms to a reasonable grammatical model,
but which also sounds smooth and natural to
a human consumer. Ordering prenominal mod-
ifiers in noun phrases is particularly difficult
in these applications, as the rules underlying
these orderings are subtle and not well under-
stood. For example, the phrase ?big red ball?
seems natural, while ?red big ball? seems more
marked, suitable only in specific contexts. There
is some consensus that the order of prenom-
inal modifiers in noun phrases is governed in
part by semantic constraints, but there is no
agreement on the exact constraints necessary to
specify consistent orderings for any given set of
modifiers. General principles of modifier order-
ing based on semantic constraints also fall short
on larger domains, where it is not always clear
how to map prenominal modifiers to proposed
semantic groups.
With the recent advantages of large corpora
and powerful computational resources, work
on automatically ordering prenominal modifiers
has moved away from approaches based on gen-
eral principles, and towards learning ordering
preferences empirically from existing corpora.
Such approaches have several advantages: (1)
The predicted orderings are based on prior evi-
dence from ?real-world? texts, ensuring that they
are therefore reasonably natural. (2) Many (if
not all) prenominal modifiers can be ordered.
(3) Expanding the training data with more and
larger corpora often improves the system with-
out requiring significant manual labor.
In this paper, we introduce a novel approach
to prenominal modifier ordering adapted from
multiple sequence alignment (MSA) techniques
used in computational biology. MSA is generally
applied to DNA, RNA, and protein sequences,
aligning three or more biological sequences in or-
der to determine, for example, common ancestry
(Durbin et al, 1999; Gusfield, 1997; Carrillo and
Lipman, 1988). MSA techniques have not been
widely applied in NLP, but have produced some
promising results for building a generation map-
ping dictionary (Barzilay and Lee, 2002), para-
phrasing (Barzilay and Lee, 2003), and phone
recognition (White et al, 2006).
We believe that multiple sequence alignment
is well-suited for aligning linguistic sequences,
and that these alignments can be used to predict
prenominal modifier ordering for any given set
of modifiers. Our technique utilizes simple fea-
tures within the raw text, and does not require
any semantic information. We achieve good per-
formance using this approach, with results com-
petitive with earlier work (Shaw and Hatzivas-
siloglou, 1999; Malouf, 2000; Mitchell, 2009) and
higher recall and F-measure than that reported
in Mitchell (2009) when tested on the same cor-
pus.
600
2 Related work
In one of the first attempts at automatically or-
dering prenominal modifiers, Shaw and Hatzi-
vassiloglou (1999) present three empirical meth-
ods to order a variety of prenominal modifier
types. Their approach provides ordering deci-
sions for adjectives, gerunds (such as ?running?
in ?running man?), and past participles (such
as ?heated? in ?heated debate?), as well as for
modifying nouns (such as ?baseball? in ?base-
ball field?). A morphology module transforms
plural nouns and comparative/superlative forms
into their base forms, increasing the frequency
counts for each modifier. We will briefly re-
cap their three methods, which are categorized
as the direct evidence method, the transitivity
method, and the clustering method.
Given prenominal modifiers a and b in a train-
ing corpus, the direct evidence method com-
pares frequency counts of the ordered sequences
<a,b> and <b,a>. This approach works well,
but is limited by data sparsity; groups of two or
more modifiers before a noun are relatively in-
frequent in traditional corpora, and finding the
same pair of modifiers together more than once
is particularly rare.
To overcome this issue, Shaw and Hatzi-
vassiloglou?s transitivity and clustering meth-
ods make inferences about unseen orderings
among prenominal modifiers. In the transitiv-
ity method, given three modifiers a,b,c, where a
precedes b and b precedes c, the model concludes
that a precedes c. The clustering method calcu-
lates a similarity score between modifiers based
on where the modifiers occur in relation to the
other modifiers in the corpus. Those modifiers
that are most similar are clustered together, and
ordering decisions can be made between modi-
fiers in separate clusters. All three approaches
are designed to order pairs of modifiers; it is un-
clear how to extend these approaches to order
groups larger than a pair.
Shaw and Hatzivassiloglou find that NPs with
only adjectives as modifiers (including gerunds
and past participles) are considerably easier to
order than those which contain both adjectives
and nouns. They also find large differences in
accuracy across domains; their systems achieve
much lower overall accuracy on financial text
(the Wall Street Journal (WSJ) corpus (Marcus
et al, 1999)) than on medical discharge sum-
maries.
Looking at all modifier pairs, the authors
achieve their highest prediction accuracy of
90.7% using the transitivity technique on a med-
ical corpus. We do not have access to this cor-
pus, but we do have access to the WSJ corpus,
which provides a way to compare our methods.
On this corpus, their model produces predic-
tions for 62.5% of all modifier pairs and achieves
83.6% accuracy when it is able to make a predic-
tion. Random guessing on the remainder yields
an overall accuracy of 71.0%.
Malouf (2000) also examines the problem of
prenominal modifier ordering. He too proposes
several statistical techniques, achieving results
ranging from 78.3% to 91.9% accuracy. He
achieves his best results by combining memory-
based learning and positional probability to
modifiers from the first 100 million tokens of
the BNC. However, this evaluation is limited to
the ordering of prenominal adjectives, which is a
considerably simpler task than ordering all types
of prenominal modifiers. Malouf?s approaches
are also limited to ordering pairs of modifiers.
Mitchell (2009) proposes another approach,
grouping modifiers into classes and ordering
based on those classes. A modifier?s class is as-
signed based on its placement before a noun,
relative to the other modifiers it appears with.
Classes are composed of those modifiers that
tend to be placed closer to the head noun, those
modifiers that tend to be placed farther from the
head noun, etc., with each class corresponding
to a general positional preference. Unlike earlier
work, these classes allow more than one ordering
to be proposed for some pairs of modifiers.
Combining corpora of various genres,
Mitchell?s system achieves a token precision
of 89.6% (see Section 4 for discussion and
comparison of various evaluation metrics).
However, the model only makes predictions for
74.1% of all modifier pairs in the test data, so
recall is quite low (see Tables 4 and 6).
Overall, previous work in noun-phrase order-
601
ing has produced impressive accuracies in some
domains, but currently available systems tend
to adapt poorly to unseen modifiers and do not
generalize well to unseen domains.
3 Methods
3.1 Multiple Sequence Alignment
Multiple sequence alignment algorithms align
sequences of discrete tokens into a series of
columns. They attempt to align identical or
easily-substitutable tokens within a column, in-
serting gaps when such gaps will result in a bet-
ter alignment (more homogeneous token assign-
ments within each column). For example, con-
sider the simple alignment shown in Table 1.
The two sequences ?GAACTGAT? and ?AAGT-
GTAT? are aligned to maximize the number of
identical items that appear in the same column,
substituting tokens (column 3), and inserting
gaps (columns 1 and 6)1.
A full MSA is generally constructed by itera-
tively aligning each new sequence with an identi-
cal or similar sequence already in the MSA (so-
called ?progressive alignment?). The costs of
token substitution are often taken from a hand-
tuned substitution matrix. A cost may also be
associated with inserting a gap into the exist-
ing MSA (a ?gap penalty?). Once the full MSA
has been constructed, a Position Specific Score
Matrix (PSSM) can be induced, in which each
token (including a special gap token) is assigned
a separate alignment cost for each column. An
unseen sequence can then be aligned with the
full MSA by Viterbi search.
Predicting sequence ordering within a noun
phrase is a natural application for MSA tech-
niques, and it seems reasonable to propose that
aligning an unseen set of modifiers with such an
MSA model will yield acceptable orderings. Ta-
ble 2 illustrates how MSA may be applied to
modifiers before a noun. Given an NP preceded
by modifiers hungry, big, and Grizzly, alignment
of the modifiers with NPs seen in the training
corpus determines the prenominal ordering big
hungry Grizzly. We then align every permuta-
1See Durbin et al (1999) for details on standard align-
ment techniques.
G A C T G - A T
- A G T G T A T
1 2 3 4 5 6 7 8
Table 1: Alignment of the two DNA sequences
?GAACTGAT? and ?AAGTGTAT?.
small clumsy black bear
big - black cow
two-story - brown house
big clumsy - bull
small fuzzy brown duck
large - green house
big hungry Grizzly bear
Table 2: Example noun-phrase alignment.
tion of the NP and choose the best-scoring align-
ment.
The vocabulary for a linguistic alignment is
large enough to render a hand-tuned substitu-
tion matrix impractical, so we instead construct
a cost function based on features of the token
under consideration and those of the other to-
kens already aligned in a column.
We know of no prior work on methods for
training such an alignment. We present and
compare two training methods, each of which
produces competitive ordering accuracies. Both
training methods share the feature-set described
in Table 3. In each case, we train an MSA by
aligning each instance in the training data.
3.2 Maximum Likelihood Training
In our alignment approach, the features listed in
Table 3 are grouped into several classes. All ob-
served words are a class, all observed stems are
a class (Porter, 1980), and so on. We treat each
indicator feature as a separate class, and make
the assumption that classes are independent of
one another. This assumption is clearly false,
but serves as a reasonable first approximation,
similar to the independence assumption in Na??ve
Bayesian analysis. After aligning each instance,
we estimate the probability of a feature appear-
ing in a column as the simple maximum like-
lihood estimate given the observed occurrences
602
Identity Features
Word Token
Stem Word stem, derived by the Porter Stemmer
Length ?Binned? length indicators: 1, 2, 3, 4, 5-6, 7-8, 9-12, 13-18, >18 characters
Indicator Features
Capitalized Token begins with a capital
All-caps Entire token is capitalized
Hyphenated Token contains a hyphen
Numeric Entire token is numeric (e.g. 234)
Initial Numeric Token begins with a numeral (e.g. 123, 2-sided)
Endings Token ends with -al, -ble, -ed, -er, -est, -ic, -ing, -ive, -ly
Table 3: Description of the feature-set.
within its class.2 This produces a new PSSM
with which to align the next instance.
Our problem differs from alignment of biolog-
ical sequences in that we have little prior knowl-
edge of the similarity between sequences. ?Sim-
ilarity? can be defined in many ways; for bio-
logical sequences, a simple Levenshtein distance
is effective, using a matrix of substitution costs
or simple token identity (equivalent to a ma-
trix with cost 0 on the diagonal and 1 every-
where else). These matrices are constructed and
tuned by domain experts, and are used both in
choosing alignment order (i.e., which sequence
to align next) and during the actual alignment.
When aligning biological sequences, it is cus-
tomary to first calculate the pairwise distance
between each two sequences and then introduce
new sequences into the MSA in order of simi-
larity. In this way, identical sequences may be
aligned first, followed by less similar sequences
(Durbin et al, 1999).
However, we have no principled method of de-
termining the ?similarity? of two words in an NP.
We have no a priori notion of what the cost
of substituting ?two-story? for ?red? should be.
Lacking this prior knowledge, we have no opti-
mal alignment order and we must in effect learn
the substitution costs as we construct the MSA.
Therefore, we choose to add instances in the or-
der they occur in the corpus, and to iterate over
the entire MSA, re-introducing each sequence.
2We treat two special symbols for gaps and unknown
words as members of the word class.
This allows a word to ?move? from its original
column to a column which became more likely
as more sequences were aligned. Each iteration
is similar to a step in the EM algorithm: create a
model (build up an MSA and PSSM), apply the
model to the data (re-align all sequences), and
repeat. Randomly permuting the training cor-
pus did not change our results significantly, so
we believe our results are not greatly dependent
on the initial sequence order.
Instead of assigning substitution costs, we
compute the cost of aligning a word into a par-
ticular column, as follows:
C = The set of i feature classes, Ci ? C
j = Features 1 . . . |Ci| from class Ci
cnt(i, j, k) = The count of instances of
feature j from class
i in column k
?i = Laplace smoothing count
for feature class Ci
A = The number of aligned instances
f(w, i, j) =
?
??
??
1 if word w has feature j from
Ci,
0 otherwise
These help define feature positional probabilities
for column k:
p(i, j, k) =
cnt(i, j, k) + ?i
A+ ?i ? |Ci|
(1)
603
That is, the probability of feature j from class
i occurring in column k is a simple maximum-
likelihood estimate ? count the number of times
we have already aligned that feature in the col-
umn and divide by the number of sequences
aligned. We smooth that probability with sim-
ple Laplace smoothing.
We can now calculate the probability of align-
ing a word w into column k by multiplying the
product of the probabilities of aligning each of
the word?s features. Taking the negative log to
convert that probability into a cost function:
c(w, k) = ?
|C|?
i=1
|Ci|?
j=1
log (p(i, j, k) ? f(w, i, j)) (2)
Finally, we define the cost of inserting a new
column into the alignment to be equal to the
number of columns in the existing alignment,
thereby increasingly penalizing each inserted
column until additional columns become pro-
hibitively expensive.
i(j) = I ? Length of existing alignment (3)
The longest NPs aligned were 7 words, and
most ML MSAs ended with 12-14 columns.
We experimented with various column insertion
costs and values for the smoothing ? and found
no significant differences in overall performance.
3.3 Discriminative Training
We also trained a discriminative model, us-
ing the same feature-set. Discriminative train-
ing does not require division of the features
into classes or the independence assumption dis-
cussed in Section 3.2. We again produced a cost
vector for each column. We fixed the alignment
length at 8 columns, allowing alignment of the
longest instances in our test corpus.
Our training data consists of ordered se-
quences, but the model we are attempting to
learn is a set of column probabilities. Since we
have no gold-standard MSAs, we instead align
the ordered NPs with the current model and
treat the least cost alignment of the correct or-
dering as the reference for training.
We trained this model using the averaged per-
ceptron algorithm (Collins, 2002). A percep-
tron learns from classifier errors, i.e., when it
misorders an NP. At each training instance, we
align all possible permutations of the modifiers
with the MSA. If the least cost alignment does
not correspond to the correct ordering of the
modifiers, we update the perceptron to penal-
ize features occurring in that alignment and to
reward features occurring in the least cost align-
ment corresponding to the correct ordering, us-
ing standard perceptron updates.
Examining every permutation of the NP in-
volves a non-polynomial cost, but the sequences
under consideration are quite short (less than
1% of the NPs in our corpus have more than 3
modifiers, and the longest has 6; see Table 7). So
exhaustive search is practical for our problem; if
we were to apply MSA to longer sequences, we
would need to prune heavily.3
4 Evaluation
We trained and tested on the same corpus used
by Mitchell (2009), including identical 10-fold
cross-validation splits. The corpus consists of
all NPs extracted from the Penn Treebank,
the Brown corpus, and the Switchboard corpus
(Marcus et al, 1999; Kucera and Francis, 1967;
Godfrey et al, 1992). The corpus is heavily
biased toward WSJ text (74%), with approxi-
mately 13% of the NPs from each of the other
corpora.
We evaluated our system using several related
but distinct metrics, and on both modifier pairs
and full NPs.
We define:
T = The set of unique orderings found in the
test corpus
P = The set of unique orderings predicted by
the system
Type Precision (|P ? T|/|P|) measures the
probability that a predicted ordering is ?reason-
able? (where ?reasonable? is defined as orderings
which are found in the test corpus).
3The same issue arises when evaluating candidate or-
derings; see Section 4.
604
Token Accuracy Type Precision Type Recall Type F-measure
Mitchell N/A 90.3% (2.2) 67.2% (3.4) 77.1%
ML MSA 85.5% (1.0) 84.6% (1.1) 84.7% (1.1) 84.7%
Perceptron MSA 88.9% (0.7) 88.2% (0.8) 88.1% (0.8) 88.2%
Table 4: Results on the combined WSJ, Switchboard, and Brown corpus; averages and standard deviations
over a 10-fold cross validation. Winning scores are in bold.
Type Recall (|P?T|/|T|) measures the per-
centage of ?reasonable? orderings which the sys-
tem recreates.
Note that these two metrics differ only in no-
tation from those used by Mitchell (2009).
We also define a third metric, Token Accu-
racy, which measures accuracy on each individ-
ual ordering in the test corpus, rather than on
unique orderings. This penalizes producing or-
derings which are legal, but uncommon. For ex-
ample, if {a,b} occurs eight times in the test cor-
pus as <a,b> and two times as <b,a>, we will
be limited to a maximum accuracy of 80% (pre-
suming our system correctly predicts the more
common ordering). However, even though sug-
gesting <b,a> is not strictly incorrect, we gen-
erally prefer to reward a system that produces
more common orderings, an attribute not em-
phasized by type-based metrics. Our test cor-
pus does not contain many ambiguous pairings,
so our theoretical maximum token accuracy is
99.8%.
We define:
o1..N = All modifier orderings in the
test data
pred(oi) = The predicted ordering for
modifiers in oi
ai =
{
1 if pred(oi) = oi,
0 otherwise
Token Accuracy =
N?
i=0
ai
N
4.1 Pairwise Ordering
Most earlier work has focused on ordering pairs
of modifiers. The results in Table 4 are di-
rectly comparable to those found in Mitchell
(2009). Mitchell?s earlier approach does not gen-
erate a prediction when the system has insuffi-
cient evidence, and allows generation of multiple
predictions given conflicting evidence. In the-
ory, generating multiple predictions could im-
prove recall, but in practice her system appears
biased toward under-predicting, favoring preci-
sion. Our approach, in contrast, forces predic-
tion of a single ordering for each test instance,
occasionally costing some precision (in particu-
lar in cross-domain trials; see Table 5), but con-
sistently balancing recall and precision.
Our measurement of Token Accuracy is com-
parable to the accuracy measure reported in
Shaw and Hatzivassiloglou (1999) and Malouf
(2000) (although we evaluate on a different cor-
pus). Their approaches produce a single order-
ing for each test instance evaluated, so for each
incorrectly ordered modifier pair, there is a cor-
responding modifier pair in the test data that
was not predicted.
Shaw and Hatzivassiloglou found financial
text particularly difficult to order, and reported
that their performance dropped by 19% when
they included nouns as well as adjectives. Mal-
ouf?s system surpasses theirs, achieving an accu-
racy of 91.9%. However, his corpus was derived
from the BNC ? he did not attempt to order fi-
nancial text ? and he ordered only adjectives as
modifiers. In contrast, our test corpus consists
mainly of WSJ text, and we test on all forms
of prenominal modifiers. We believe this to be
a considerably more difficult task, so our peak
performance of 88.9% would appear to be ? at
worst ? quite competitive.
Table 5 presents an evaluation of cross-
domain generalization, splitting the same cor-
pus by genre ? Brown, Switchboard, and WSJ.
In each trial, we train on two genres and test on
605
Training Testing Token Type Type Type
Corpora Corpus Accuracy Precision Recall F-measure
Mitchell
Brown+WSJ Swbd N/A 94.2% 58.2% 72.0%
Swbd+WSJ Brown N/A 87.0% 51.2% 64.5%
Swbd+Brown WSJ N/A 82.4% 27.2% 40.9%
ML MSA
Brown+WSJ Swbd 74.6% 74.7% 75.3% 75.0%
Swbd+WSJ Brown 75.3% 74.7% 74.9% 74.8%
Swbd+Brown WSJ 70.2% 71.6% 71.8% 71.7%
Perceptron MSA
Brown+WSJ Swbd 77.2% 78.2% 77.6% 77.9%
Swbd+WSJ Brown 76.4% 76.7% 76.4% 76.5%
Swbd+Brown WSJ 77.9% 77.5% 77.3% 77.4%
Table 5: Cross-domain generalization.
Token Accuracy Token Precision Token Recall Token F-measure
Mitchell N/A 94.4% 78.6% (1.2) 85.7%
ML MSA 76.9% (1.6) 76.5% (1.4) 76.5% (1.4) 76.50%
Perceptron MSA 86.7% (0.9) 86.7% (0.9) 86.7% (0.9) 86.7%
Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To
compare directly with Mitchell (2009), we report token precision and recall instead of type. Our system
always proposes one and only one ordering, so token accuracy, precision, and recall are identical.
the third.4 Our results mirror those in the previ-
ous trials ? forcing a prediction costs some pre-
cision (vis-a-vis Mitchell?s 2009 system), but our
recall is dramatically higher, resulting in more
balanced performance overall.
4.2 Full NP Ordering
We now extend our analysis to ordering en-
tire NPs, a task we feel the MSA approach
should be particularly suited to, since (unlike
pairwise models) it can model positional prob-
abilities over an entire NP. To our knowledge,
the only previously reported work on this task
is Mitchell?s (2009). We train this model on
the full NP instead of on modifier pairs; this
makes little difference in pairwise accuracy, but
improves full-NP ordering considerably.
As seen in Table 6, both MSA models perform
quite well, the perceptron-trained MSA again
outperforming the maximum likelihood model.
However, we were somewhat disappointed in the
performance on longer sequences. We expected
the MSA to encode enough global information
4Note that the WSJ corpus is much larger than the
other two, comprising approximately 84% of the total.
Modifiers Frequency Token Pairwise
Accuracy Accuracy
2 89.1% 89.7% 89.7%
3 10.0% 64.5% 84.4%
4 0.9% 37.2% 80.7%
Table 7: Descriminative model performance on NPs
of various lengths, including pairwise measures.
to perform accurate full sequence ordering, but
found the accuracy drops off dramatically on
NPs with more modifiers. In fact, the accu-
racy on longer sequences is worse than we would
expect by simply extending a pairwise model.
For instance, ordering three modifiers requires
three pairwise decisions. We predict pairwise
orderings with 88% accuracy, so we would ex-
pect no worse than (.88)3, or 68% accuracy on
such sequences. However, the pairwise accu-
racy declines on longer NPs, so it underperforms
even that theoretical minimum. Sparse training
data for longer NPs biases the model strongly
toward short sequences and transitivity (which
our model does not encode) may become impor-
tant when ordering several modifiers.
606
5 Ablation Tests
We performed limited ablation testing on the
discriminative model, removing features individ-
ually and comparing token accuracy (see Table
8). We found that few of the features provided
great benefit individually; the overall system
performance remains dominated by the word.
The word and stem features appear to cap-
ture essentially the same information; note that
performance does not decline when the word
or stem features are ablated, but drops dras-
tically when both are omitted. Performance de-
clines slightly more when ending features are ab-
lated as well as words and stems, so it appears
that ? as expected ? the information captured
by ending features overlaps somewhat with lex-
ical identity. The effects of individual features
are all small and none are statistically signifi-
cant.
Feature(s) Gain/Loss
Word 0.0
Stem 0.0
Capitalization -0.1
All-Caps 0.0
Numeric -0.2
Initial-numeral 0.0
Length -0.1
Hyphen 0.0
-al 0.0
-ble -0.4
-ed -0.4
-er 0.0
-est -0.1
-ic +0.1
-ing 0.0
-ive -0.1
-ly 0.0
Word and stem -22.9
Word, stem, and endings -24.2
Table 8: Ablation test results on the discriminative
model.
6 Summary and Future Directions
We adapted MSA approaches commonly used
in computational biology to linguistic problems
and presented two novel methods for training
such alignments. We applied these techniques
to the problem of ordering prenominal modi-
fiers in noun phrases, and achieved performance
competitive with ? and in many cases, superior
to ? the best results previously reported.
In our current work, we have focused on rel-
atively simple features, which should be adapt-
able to other languages without expensive re-
sources or much linguistic insight. We are inter-
ested in exploring richer sources of features for
ordering information. We found simple morpho-
logical features provided discriminative clues for
otherwise ambiguous instances, and believe that
richer morphological features might be helpful
even in a language as morphologically impover-
ished as English. Boleda et al (2005) achieved
promising preliminary results using morphology
for classifying adjectives in Catalan.
Further, we might be able to capture some
of the semantic relationships noted by psycho-
logical analyses (Ziff, 1960; Martin, 1969) by
labeling words which belong to known seman-
tic classes (e.g., colors, size denominators, etc.).
We intend to explore deriving such labels from
resources such as WordNet or OntoNotes.
We also plan to continue exploration of MSA
training methods. We see considerable room
for refinement in generative MSA models; our
maximum likelihood training provides a strong
starting point for EM optimization, conditional
likelihood, or gradient descent methods. We are
also considering applying maximum entropy ap-
proaches to improving the discriminative model.
Finally (and perhaps most importantly), we
expect that our model would benefit from ad-
ditional training data, and plan to train on a
larger, automatically-parsed corpus.
Even in its current form, our approach im-
proves the state-of-the-art, and we believe MSA
techniques can be a useful tool for ordering
prenominal modifiers in NLP tasks.
7 Acknowledgements
This research was supported in part by NSF
Grant #IIS-0811745. Any opinions, findings,
conclusions or recommendations expressed in
this publication are those of the authors and do
not necessarily reflect the views of the NSF.
607
References
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence align-
ment. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing
- Volume 10, pages 164?171, Philadelphia. Asso-
ciation for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL), vol-
ume 15, pages 201?31, Edmonton, Canada. As-
sociation for Computational Linguistics.
Gemma Boleda, Toni Badia, and Sabine Schulte
im Walde. 2005. Morphology vs. syntax in adjec-
tive class acquisition. In Proceedings of the ACL-
SIGLEX Workshop on Deep Lexical Acquisition,
pages 77?86, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Humberto Carrillo and David Lipman. 1988. The
multiple sequence alignment problem in biol-
ogy. SIAM Journal on Applied Mathematics,
48(5):1073?1082, October.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8,
Philadelphia, July. Association for Computational
Linguistics.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1999. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, West
Nyack, NY, July.
John J. Godfrey, Edward C. Holliman, and Jane
McDaniel. 1992. SWITCHBOARD: telephone
speech corpus for research and development. In
Acoustics, Speech, and Signal Processing, IEEE
International Conference on, volume 1, pages 517?
520, Los Alamitos, CA, USA. IEEE Computer So-
ciety.
Dan Gusfield. 1997. Algorithms on Strings, Trees
and Sequences: Computer Science and Computa-
tional Biology. Cambridge University Press, West
Nyack, NY, May.
H. Kucera and W. N Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
Robert Malouf. 2000. The order of prenominal ad-
jectives in natural language generation. In Pro-
ceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 85?92,
Hong Kong, October. Association for Computa-
tional Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.
J. E. Martin. 1969. Semantic determinants of pre-
ferred adjective order. Journal of Verbal Learning
& Verbal Behavior. Vol, 8(6):697?704.
Margaret Mitchell. 2009. Class-Based ordering of
prenominal modifiers. In Proceedings of the 12th
European Workshop on Natural Language Gener-
ation (ENLG 2009), pages 50?57, Athens, Greece,
March. Association for Computational Linguis-
tics.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
James Shaw and Vasileios Hatzivassiloglou. 1999.
Ordering among premodifiers. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 135?143, Col-
lege Park, Maryland, USA, June. Association for
Computational Linguistics.
Christopher White, Izhak Shafran, and Jean luc
Gauvain. 2006. Discriminative classifiers for
language recognition. In Proceedings of the
2006 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP),
pages 213?216, Toulouse, France. IEEE.
Paul Ziff. 1960. Semantic Analysis. Cornell Univer-
sity Press, Ithaca, New York.
608
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 440?449,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Beam-Width Prediction for Efficient Context-Free Parsing
Nathan Bodenstab? Aaron Dunlop? Keith Hall? and Brian Roark?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?Google, Inc., Zurich, Switzerland
{bodensta,dunlopa,roark}@cslu.ogi.edu kbhall@google.com
Abstract
Efficient decoding for syntactic parsing has
become a necessary research area as statisti-
cal grammars grow in accuracy and size and
as more NLP applications leverage syntac-
tic analyses. We review prior methods for
pruning and then present a new framework
that unifies their strengths into a single ap-
proach. Using a log linear model, we learn
the optimal beam-search pruning parameters
for each CYK chart cell, effectively predicting
the most promising areas of the model space
to explore. We demonstrate that our method
is faster than coarse-to-fine pruning, exempli-
fied in both the Charniak and Berkeley parsers,
by empirically comparing our parser to the
Berkeley parser using the same grammar and
under identical operating conditions.
1 Introduction
Statistical constituent parsers have gradually in-
creased in accuracy over the past ten years. This
accuracy increase has opened the door to automati-
cally derived syntactic information within a number
of NLP tasks. Prior work incorporating parse struc-
ture into machine translation (Chiang, 2010) and Se-
mantic Role Labeling (Tsai et al, 2005; Punyakanok
et al, 2008) indicate that such hierarchical structure
can have great benefit over shallow labeling tech-
niques like chunking and part-of-speech tagging.
Although syntax is becoming increasingly impor-
tant for large-scale NLP applications, constituent
parsing is slow ? too slow to scale to the size of
many potential consumer applications. The exhaus-
tive CYK algorithm has computational complexity
O(n3|G|) where n is the length of the sentence and
|G| is the number of grammar productions, a non-
negligible constant. Increases in accuracy have pri-
marily been accomplished through an increase in
the size of the grammar, allowing individual gram-
mar rules to be more sensitive to their surround-
ing context, at a considerable cost in efficiency.
Grammar transformation techniques such as linguis-
tically inspired non-terminal annotations (Johnson,
1998; Klein and Manning, 2003b) and latent vari-
able grammars (Matsuzaki et al, 2005; Petrov et al,
2006) have increased the grammar size |G| from a
few thousand rules to several million in an explic-
itly enumerable grammar, or even more in an im-
plicit grammar. Exhaustive search for the maximum
likelihood parse tree with a state-of-the-art grammar
can require over a minute of processing for a sin-
gle sentence of 25 words, an unacceptable amount
of time for real-time applications or when process-
ing millions of sentences. Deterministic algorithms
for dependency parsing exist that can extract syntac-
tic dependency structure very quickly (Nivre, 2008),
but this approach is often undesirable as constituent
parsers are more accurate and more adaptable to new
domains (Petrov et al, 2010).
The most accurate constituent parsers, e.g., Char-
niak (2000), Petrov and Klein (2007a), make use
of approximate inference, limiting their search to
a fraction of the total search space and achieving
speeds of between one and four newspaper sen-
tences per second. The paradigm for building state-
of-the-art parsing models is to first design a model
structure that can achieve high accuracy and then,
after the model has been built, design effective ap-
proximate inference methods around that particu-
lar model; e.g., coarse-to-fine non-terminal hierar-
chies for a given model, or agenda-based methods
440
that are empirically tuned to achieve acceptable ef-
ficiency/accuracy operating points. While both of
the above mentioned papers use the CYK dynamic
programming algorithm to search through possible
solutions, their particular methods of approximate
inference are quite distinct.
In this paper, we examine a general approach to
approximate inference in constituent parsing that
learns cell-specific thresholds for arbitrary gram-
mars. For each cell in the CYK chart, we sort all
potential constituents in a local agenda, ordered by
an estimate of their posterior probability. Given fea-
tures extracted from the chart cell context ? e.g.,
span width; POS-tags and words surrounding the
boundary of the cell ? we train a log linear model
to predict how many constituents should be popped
from the local agenda and added to the chart. As
a special case of this approach, we simply pre-
dict whether the number to add should be zero or
greater than zero, in which case the method can be
seen as a cell-by-cell generalization of Roark and
Hollingshead?s (2008; 2009) tagger-derived Chart
Constraints. More generally, instead of a binary
classification decision, we can also use this method
to predict the desired cell population directly and
get cell closure for free when the classifier predicts
a beam-width of zero. In addition, we use a non-
symmetric loss function during optimization to ac-
count for the imbalance between over-predicting or
under-predicting the beam-width.
A key feature of our approach is that it does
not rely upon reference syntactic annotations when
learning to search. Rather, the beam-width predic-
tion model is trained to learn the rank of constituents
in the maximum likelihood trees.1 We will illus-
trate this by presenting results using a latent-variable
grammar, for which there is no ?true? reference la-
tent variable parse. We simply parse sections 2-21
of the WSJ treebank and train our search models
from the output of these trees, with no prior knowl-
edge of the non-terminal set or other grammar char-
acteristics to guide the process. Hence, this ap-
1Note that we do not call this method ?unsupervised? be-
cause all grammars used in this paper are induced from super-
vised data, although our framework can also accommodate un-
supervised grammars. We emphasize that we are learning to
search using only maximum likelihood trees, not that we are
doing unsupervised parsing.
Figure 1: Inside (grey) and outside (white) representations of
an example chart edge Ni,j .
proach is broadly applicable to a wide range of sce-
narios, including tuning the search to new domains
where domain mismatch may yield very different ef-
ficiency/accuracy operating points.
In the next section, we present prior work on
approximate inference in parsing, and discuss how
our method to learn optimal beam-search param-
eters unite many of their strengths into a single
framework. We then explore using our approach to
open or close cells in the chart as an alternative to
Roark and Hollingshead (2008; 2009). Finally, we
present results which combine cell closure and adap-
tive beam-width prediction to achieve the most effi-
cient parser.
2 Background
2.1 Preliminaries and notation
Let S = w1 . . . w|S| represent an input string of
|S| words. Let wi,j denote the substring from word
wi+1 to wj ; i.e., S = w0,|S|. We use the term chart
edge to refer to a non-terminal spanning a specific
substring of the input sentence. Let Ni,j denote the
edge labeled with non-terminalN spanning wi,j , for
example NP3,7. We define an edge?s figure-of-merit
(FOM) as an estimate of the product of its inside
(?) and outside (?) scores, conceptually the relative
merit the edge has to participate in the final parse
tree (see Figure 1). More formally:
?(Ni,j) = P (w0,i, Ni,j , wj,n)
?(Ni,j) = P (wi,j |N)
FOM(Ni,j) = ??(Ni,j)??(Ni,j)
441
With bottom-up parsing, the true inside probability
is accumulated and ?(Ni,j) does not need to be esti-
mated, improving the FOMs ability to represent the
true inside/outside distribution.
In this paper, we use a modified version of the
Caraballo and Charniak Boundary FOM (1998)
for local edge comparison, which computes ??(Ni,j)
using POS forward-backward scores and POS-to-
nonterminal constituent boundary transition proba-
bilities. Details can be found in (?).
We also note that in this paper we only use
the FOM scoring function to rank constituents in
a local agenda. Alternative approaches to rank-
ing competitors are also possible, such as Learning
as Search Optimization (Daume? and Marcu, 2005).
The method we present in this paper to learn the op-
timal beam-search parameters is applicable to any
ranking function, and we demonstrate this by com-
puting results with both the Boundary FOM and
only the inside probability in Section 6.
2.2 Agenda-based parsing
Agenda-based parsers maintain a global agenda of
edges, ranked by FOM score. At each iteration, the
highest-scoring edge is popped off of the agenda,
added to the chart, and combined with other edges
already in the chart. The agenda-based approach
includes best-first parsing (Bobrow, 1990) and A*
parsing (Klein and Manning, 2003a), which differ
in whether an admissible FOM estimate ??(Ni,j) is
required. A* uses an admissible FOM, and thus
guarantees finding the maximum likelihood parse,
whereas an inadmissible heuristic (best-first) may
require less exploration of the search space. Much
work has been pursued in both admissible and in-
admissible heuristics for agenda parsing (Caraballo
and Charniak, 1998; Klein and Manning, 2003a;
Pauls et al, 2010).
In this paper, we also make use of agendas, but
at a local rather than a global level. We maintain an
agenda for each cell, which has two significant ben-
efits: 1) Competing edges can be compared directly,
avoiding the difficulty inherent in agenda-based ap-
proaches of comparing edges of radically differ-
ent span lengths and characteristics; and 2) Since
the agendas are very small, the overhead of agenda
maintenance ? a large component of agenda-based
parse time ? is minimal.
2.3 Beam-search parsing
CYK parsing with a beam-search is a local pruning
strategy, comparing edges within the same chart cell.
The beam-width can be defined in terms of a thresh-
old in the number of edges allowed, or in terms of
a threshold on the difference in probability relative
to the highest scoring edge (Collins, 1999; Zhang et
al., 2010). For the current paper, we use both kinds
of thresholds, avoiding pathological cases that each
individual criteria is prone to encounter. Further, un-
like most beam-search approaches we will make use
of a FOM estimate of the posterior probability of an
edge, defined above, as our ranking function. Fi-
nally, we will learn log linear models to assign cell-
specific thresholds, rather than relying on a single
search parameter.
2.4 Coarse-to-Fine Parsing
Coarse-to-fine parsing, also known as multiple pass
parsing (Goodman, 1997; Charniak, 2000; Char-
niak and Johnson, 2005), first parses the input sen-
tence with a simplified (coarse) version of the tar-
get (fine) grammar in which multiple non-terminals
are merged into a single state. Since the coarse
grammar is quite small, parsing is much faster than
with the fine grammar, and can quickly yield an es-
timate of the outside probability ?(?) for use in sub-
sequent agenda or beam-search parsing with the fine
grammar. This approach can also be used iteratively
with grammars of increasing complexity (Petrov and
Klein, 2007a).
Building a coarse grammar from a fine gram-
mar is a non-trivial problem, and most often ap-
proached with detailed knowledge of the fine gram-
mar being used. For example, Goodman (1997)
suggests using a coarse grammar consisting of reg-
ular non-terminals, such as NP and VP, and then
non-terminals augmented with head-word informa-
tion for the more accurate second-pass grammar.
Such an approach is followed by Charniak (2000) as
well. Petrov and Klein (2007a) derive coarse gram-
mars in a more statistically principled way, although
the technique is closely tied to their latent variable
grammar representation.
To the extent that our cell-specific threshold clas-
sifier predicts that a chart cell should contain zero
edges or more than zero edges, it is making coarse
442
predictions about the unlabeled constituent structure
of the target parse tree. This aspect of our work is
can be viewed as a coarse-to-fine process, though
without considering specific grammatical categories
or rule productions.
2.5 Chart Constraints
Roark and Hollingshead (2008; 2009) introduced
a pruning technique that ignores entire chart cells
based on lexical and POS features of the input sen-
tence. They train two finite-state binary taggers:
one that allows multi-word constituents to start at
a word, and one that allows constituents to end at a
word. Given these tags, it is straightforward to com-
pletely skip many chart cells during processing.
In this paper, instead of tagging word positions to
infer valid constituent spans, we classify chart cells
directly. We further generalize this cell classification
to predict the beam-width of the chart cell, where a
beam-width of zero indicates that the cell is com-
pletely closed. We discuss this in detail in the next
section.
3 Open/Closed Cell Classification
3.1 Constituent Closure
We first look at the binary classification of chart cells
as either open or closed to full constituents, and pre-
dict this value from the input sentence alone. This
is the same problem that Roark and Hollingshead
(2008; 2009) solve with Chart Constraints; however,
where they classify lexical items as either beginning
or ending a constituent, we classify individual chart
cells as open or closed, an approach we call Con-
stituent Closure. Although the number of classifi-
cations scales quadratically with our approach, the
total parse time is still dominated by the O(n3|G|)
parsing complexity and we find that the added level
of specificity reduces the search space significantly.
To learn to classify a chart cell spanning words
wi+1 . . . wj of a sentence S as open or closed to full
constituents, we first map cells in the training corpus
to tuples:
?(S, i, j) = (x, y) (1)
where x is a feature-vector representation of the
chart cell and y is the target class 1 if the cell con-
tains an edge from the maximum likelihood parse
tree, 0 otherwise. The feature vector x is encoded
with the chart cell?s absolute and relative span width,
as well as unigram and bigram lexical and part-of-
speech tag items from wi?1 . . . wj+2.
Given feature/target tuples (x, y) for every chart
cell in every sentence of a training corpus ? , we train
a weight vector ? using the averaged perceptron al-
gorithm (Collins, 2002) to learn an open/closed bi-
nary decision boundary:
?? = argmin
?
?
(x,y)??(?)
L?(H(? ? x), y) (2)
where H(?) is the unit step function: 1 if the inner
product ? ?x > 0, and 0 otherwise; and L?(?, ?) is an
asymmetric loss function, defined below.
When predicting cell closure, all misclassifica-
tions are not equal. If we leave open a cell which
contains no edges in the maximum likelihood (ML)
parse, we incur the cost of additional processing, but
are still able to recover the ML tree. However, if we
close a chart cell which contains an ML edge, search
errors occur. To deal with this imbalance, we intro-
duce an asymmetric loss functionL?(?, ?) to penalize
false-negatives more severely during training.
L?(h, y) =
?
??
??
0 if h = y
1 if h > y
? if h < y
(3)
We found the value ? = 102 to give the best per-
formance on our development set, and we use this
value in all of our experiments.
Figures 2a and 2b compare the pruned charts of
Chart Constraints and Constituent Closure for a sin-
gle sentence in the development set. Note that both
of these methods are predicting where a complete
constituent may be located in the chart, not partial
constituents headed by factored nonterminals within
a binarized grammar. Depending on the grammar
factorization (right or left) we can infer chart cells
that are restricted to only edges with a factored left-
hand-side non-terminal. In Figure 2 these chart cells
are colored gray. Note that Constituent Closure re-
duces the number of completely open cells consider-
ably vs. Chart Constraints, and the number of cells
open to factored categories somewhat.
443
3.2 Complete Closure
Alternatively, we can predict whether a chart cell
contains any edge, either a partial or a full con-
stituent, an approach we call Complete Closure.
This is a more difficult classification problem as par-
tial constituents occur in a variety of contexts. Nev-
ertheless, learning this directly allows us to remove a
large number of internal chart cells from considera-
tion, since no additional cells need to be left open to
partial constituents. The learning algorithm is iden-
tical to Equation 2, but training examples are now
assigned a positive label if the chart cell contains any
edge from the binarized maximum likelihood tree.
Figure 2c gives a visual representation of Complete
Closure for the same sentence; the number of com-
pletely open cells increases somewhat, but the total
number of open cells (including those open to fac-
tored categories) is greatly reduced.
We compare the effectiveness of Constituent Clo-
sure, Complete Closure, and Chart Constraints, by
decreasing the percentage of chart cells closed un-
til accuracy over all sentences in our development
set start to decline. For Constituent and Complete
Closure, we also vary the loss function, adjusting
the relative penalty between a false-negative (clos-
ing off a chart cell that contains a maximum like-
lihood edge) and a false-positive. Results show that
using Chart Constrains as a baseline, we prune (skip)
33% of the total chart cells. Constituent Closure im-
proves on this baseline only slightly (36%), but we
see our biggest gains with Complete Closure, which
prunes 56% of all chart cells in the development set.
All of these open/closed cell classification meth-
ods can improve the efficiency of the exhaustive
CYK algorithm, or any of the approximate infer-
ence methods mentioned in Section 2. We empir-
ically evaluate them when applied to CYK parsing
and beam-search parsing in Section 6.
4 Beam-Width Prediction
The cell-closing approaches discussed in Section 3
make binary decisions to either allow or completely
block all edges in each cell. This all-on/all-off tactic
ignores the characteristics of the local cell popula-
tion, which, given a large statistical grammar, may
contain hundred of edges, even if very improbable.
Retaining all of these partial derivations forces the
(a) Chart Constraints (Roark and Hollingshead, 2009)
(b) Constituent Closure (this paper)
(c) Complete Closure (this paper)
Figure 2: Comparison of Chart Constraints (Roark and
Hollingshead, 2009) to Constituent and Complete Closure for a
single example sentence. Black cells are open to all edges while
grey cells only allow factored edges (incomplete constituents).
search in larger spans to continue down improbable
paths, adversely affecting efficiency. We can further
improve parsing speed in these open cells by lever-
aging local pruning methods, such as beam-search.
When parsing with a beam-search, finding the op-
timal beam-width threshold(s) to balance speed and
accuracy is a necessary step. As mentioned in Sec-
444
tion 2.3, two variations of the beam-width are of-
ten considered: a fixed number of allowed edges,
or a relative probability difference from the highest
scoring local edge. For the remainder of this pa-
per we fix the relative probability threshold for all
experiments and focus on adapting the number of
allowed edges per cell. We will refer to this number-
of-allowed-edges value as the beam-width, notated
by b, and leave adaptation of the relative probability
difference to future work.
The standard way to tune the beam-width is a sim-
ple sweep over possible values until accuracy on
a heldout data set starts to decline. The optimal
point will necessarily be very conservative, allowing
outliers (sentences or sub-phrases with above aver-
age ambiguity) to stay within the beam and produce
valid parse trees. The majority of chart cells will
require much fewer than b entries to find the max-
imum likelihood (ML) edge, yet, constrained by a
constant beam-width, the cell will continue to be
filled with unfruitful edges, exponentially increasing
downstream computation.
For example, when parsing with the Berkeley
latent-variable grammar and Boundary FOM, we
find we can reduce the global beam-width b to 15
edges in each cell before accuracy starts to decline.
However we find that 73% of the ML edges are
ranked first in their cell and 96% are ranked in the
top three. Thus, in 24 of every 25 cells, 80% of the
edges are unnecessary (12 of the top 15). Clearly,
it would be advantageous to adapt the beam-width
such that it is restrictive when we are confident in
the FOM ranking and more forgiving in ambiguous
contexts.
To address this problem, we learn the optimal
beam-width for each chart cell directly. We define
Ri,j as the rank of the ML edge in the chart cell
spanning wi+1 . . . wj . If no ML edge exists in the
cell, then Ri,j = 0. Given a global maximum beam-
width b, we train b different binary classifiers, each
using separate mapping functions ?k, where the tar-
get value y produced by ?k is 1 if Ri,j > k and 0
otherwise.
The same asymmetry noted in Section 3 applies
in this task as well. When in doubt, we prefer to
over-predict the beam-width and risk an increase in
processing time opposed to under-predicting at the
expense of accuracy. Thus we use the same loss
function L?, this time training several classifiers:
??k = argmin
?
?
(x,y)??k(?)
L?(H(? ? x), y) (4)
Note that in Equation 4 when k = 0, we re-
cover the open/closed cell classification of Equa-
tion 2, since a beam width of 0 indicates that the
chart cell is completely closed.
During decoding, we assign the beam-width
for chart cell spanning wi+1 . . . wj given models
?0, ?1, ...?b?1 by finding the lowest value k such that
the binary classifier ?k classifiesRi,j ? k. If no such
k exists, R?i,j is set to the maximum beam-width
value b:
R?i,j = argmin
k
?k ? xi ? 0 (5)
In Equation 5 we assume there are b unique clas-
sifiers, one for each possible beam-width value be-
tween 0 and b? 1, but this level of granularity is not
required. Choosing the number of classification bins
to minimize total parsing time is dependent on the
FOM function and how it ranks ML edges. With the
Boundary FOM we use in this paper, 97.8% of ML
edges have a local rank less than five and we find that
the added cost of computing b decision boundaries
for each cell is not worth the added specificity. We
searched over possible classification bins and found
that training four classifiers with beam-width deci-
sion boundaries at 0, 1, 2, and 4 is faster than 15 in-
dividual classifiers and more memory efficient, since
each model ?k has over 800,000 parameters. All
beam-width prediction results reported in this paper
use these settings.
Figure 3 is a visual representation of beam-width
prediction on a single sentence of the development
set using the Berkeley latent-variable grammar and
Boundary FOM. In this figure, the gray scale repre-
sents the relative size of the beam-width, black being
the maximum beam-width value, b, and the lightest
gray being a beam-width of size one. We can see
from this figure that very few chart cells are classi-
fied as needing the full 15 edges, apart from span-1
cells which we do not classify.
445
Figure 3: Visualization of Beam-Width Prediction for a single example sentence. The grey scale represents the size of the predicted
beam-width: white is 0 (cell is skipped) and black is the maximum value b (b=15 in this example).
5 Experimental Setup
We run all experiments on the WSJ treebank (Mar-
cus et al, 1999) using the standard splits: section
2-21 for training, section 22 for development, and
section 23 for testing. We preprocess the treebank
by removing empty nodes, temporal labels, and spu-
rious unary productions (X?X), as is standard in
published works on syntactic parsing.
The pruning methods we present in this paper can
be used to parse with any grammar. To achieve state-
of-the-art accuracy levels, we parse with the Berke-
ley SM6 latent-variable grammar (Petrov and Klein,
2007b) where the original treebank non-terminals
are automatically split into subclasses to optimize
parsing accuracy. This is an explicit grammar con-
sisting of 4.3 million productions, 2.4 million of
which are lexical productions. Exhaustive CYK
parsing with the grammar takes more than a minute
per sentence.
Accuracy is computed from the 1-best Viterbi
(max) tree extracted from the chart. Alternative de-
coding methods, such as marginalizing over the la-
tent variables in the grammar or MaxRule decod-
ing (Petrov and Klein, 2007a) are certainly possible
in our framework, but it is unknown how effective
these methods will be given the heavily pruned na-
ture of the chart. We leave investigation of this to
future work. We compute the precision and recall
of constituents from the 1-best Viterbi trees using
the standard EVALB script (?), which ignores punc-
tuation and the root symbol. Accuracy results are
reported as F-measure (F1), the harmonic mean be-
tween precision and recall.
We ran all timing tests on an Intel 3.00GHz pro-
cessor with 6MB of cache and 16GB of memory.
Our parser is written in Java and publicly available
at http://nlp.csee.ogi.edu.
6 Results
We empirically demonstrate the advantages of our
pruning methods by comparing the total parse time
of each system, including FOM initialization, chart
cell classification, and beam-width prediction. The
parse times reported for Chart Constraints do not in-
clude tagging times as we were provided with this
pre-tagged data, but tagging all of Section 22 takes
less than three seconds and we choose to ignore this
contribution for simplicity.
Figure 4 contains a timing comparison of the three
components of our final parser: Boundary FOM ini-
tialization (which includes the forward-backward al-
gorithm over ambiguous part-of-speech tags), beam-
446
Figure 4: Timing breakdown by sentence length for major
components of our parser.
width prediction, and the final beam-search, includ-
ing 1-best extraction. We bin these relative times
with respect to sentence length to see how each com-
ponent scales with the number of input words. As
expected, theO(n3|G|) beam-search begins to dom-
inate as the sentence length grows, but Boundary
FOM initialization is not cheap, and absorbs, on
average, 20% of the total parse time. Beam-width
prediction, on the other hand, is almost negligible
in terms of processing time even though it scales
quadratically with the length of the sentence.
We compare the accuracy degradation of beam-
width prediction and Chart Constraints in Figure 5
as we incrementally tighten their respective prun-
ing parameters. We also include the baseline beam-
search parser with Boundary FOM in this figure
to demonstrate the accuracy/speed trade-off of ad-
justing a global beam-width alone. In this figure
we see that the knee of the beam-width prediction
curve (Beam-Predict) extends substantially further
to the left before accuracy declines, indicating that
our pruning method is intelligently removing a sig-
nificant portion of the search space that remains un-
pruned with Chart Constraints.
In Table 1 we present the accuracy and parse time
for three baseline parsers on the development set:
exhaustive CYK parsing, beam-search parsing using
only the inside score ?(?), and beam-search parsing
using the Boundary FOM. We then apply our two
cell-closing methods, Constituent Closure and Com-
plete Closure, to all three baselines. As expected,
the relative speedup of these methods across the var-
ious baselines is similar since the open/closed cell
classification does not change across parsers. We
Figure 5: Time vs. accuracy curves comparing beam-width
prediction (Beam-Predict) and Chart Constraints.
also see that Complete Closure is between 22% and
31% faster than Constituent Closure, indicating that
the greater number of cells closed translates directly
into a reduction in parse time. We can further apply
beam-width prediction to the two beam-search base-
line parsers in Table 1. Dynamically adjusting the
beam-width for the remaining open cells decreases
parse time by an additional 25% when using the In-
side FOM, and 28% with the boundary FOM.
We apply our best model to the test set and report
results in Table 2. Beam-width prediction, again,
outperforms the baseline of a constant beam-width
by 65% and the open/closed classification of Chart
Constraints by 49%. We also compare beam-width
prediction to the Berkeley Coarse-to-Fine parser.
Both our parser and the Berkeley parser are written
in Java, both are run with Viterbi decoding, and both
parse with the same grammar, so a direct compari-
son of speed and accuracy is fair.2
7 Conclusion and Future Work
We have introduced three new pruning methods, the
best of which unites figure-of-merit estimation from
agenda-based parsing, local pruning from beam-
search parsing, and unlabeled constituent structure
2We run the Berkeley parser with the default search param-
eterization to achieve the fastest possible parsing time. We note
that 3 of 2416 sentences fail to parse under these settings. Using
the ?-accurate? option provides a valid parse for all sentences,
but increases parsing time of section 23 to 0.293 seconds per
sentence with no increase in F-score. We assume a back-off
strategy for failed parses could be implemented to parse all sen-
tences with a parsing time close to the default parameterization.
447
Parser Sec/Sent F1
CYK 70.383 89.4
CYK + Constituent Closure 47.870 89.3
CYK + Complete Closure 32.619 89.3
Beam + Inside FOM (BI) 3.977 89.2
BI + Constituent Closure 2.033 89.2
BI + Complete Closure 1.575 89.3
BI + Beam-Predict 1.180 89.3
Beam + Boundary FOM (BB) 0.326 89.2
BB + Constituent Closure 0.279 89.2
BB + Complete Closure 0.199 89.3
BB + Beam-Predict 0.143 89.3
Table 1: Section 22 development set results for CYK and
Beam-Search (Beam) parsing using the Berkeley latent-variable
grammar.
prediction from coarse-to-fine parsing and Chart
Constraints. Furthermore, our pruning method is
trained using only maximum likelihood trees, allow-
ing it to be tuned to specific domains without labeled
data. Using this framework, we have shown that we
can decrease parsing time by 65% over a standard
beam-search without any loss in accuracy, and parse
significantly faster than both the Berkeley parser and
Chart Constraints.
We plan to explore a number of remaining ques-
tions in future work. First, we will try combin-
ing our approach with constituent-level Coarse-to-
Fine pruning. The two methods prune the search
space in very different ways and may prove to be
complementary. On the other hand, our parser cur-
rently spends 20% of the total parse time initializing
the FOM, and adding additional preprocessing costs,
such as parsing with a coarse grammar, may not out-
weigh the benefits gained in the final search.
Second, as with Chart Constraints we do not
prune lexical or unary edges in the span-1 chart cells
(i.e., chart cells that span a single word). We ex-
pect pruning entries in these cells would notably re-
duce parse time since they cause exponentially many
chart edges to be built in larger spans. Initial work
constraining span-1 chart cells has promising results
(Bodenstab et al, 2011) and we hope to investigate
its interaction with beam-width prediction even fur-
ther.
Parser Sec/Sent F1
CYK 64.610 88.7
Berkeley CTF MaxRule 0.213 90.2
Berkeley CTF Viterbi 0.208 88.8
Beam + Boundary FOM (BB) 0.334 88.6
BB + Chart Constraints 0.244 88.7
BB + Beam-Predict (this paper) 0.125 88.7
Table 2: Section 23 test set results for multiple parsers using
the Berkeley latent-variable grammar.
Finally, the size and structure of the grammar is
the single largest contributor to parse efficiency. In
contrast to the current paradigm, we plan to inves-
tigate new algorithms that jointly optimize accuracy
and efficiency during grammar induction, leading to
more efficient decoding.
Acknowledgments
We would like to thank Kristy Hollingshead for
her valuable discussions, as well as the anony-
mous reviewers who gave very helpful feedback.
This research was supported in part by NSF Grants
#IIS-0447214, #IIS-0811745 and DARPA grant
#HR0011-09-1-0041. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or DARPA.
References
Robert J. Bobrow. 1990. Statistical agenda parsing. In
DARPA Speech and Language Workshop, pages 222?
224.
Nathan Bodenstab, Kristy Hollingshead, and Brian
Roark. 2011. Unary constraints for efficient context-
free parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
Portland, Oregon.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24:275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
448
chapter of the Association for Computational Linguis-
tics conference, pages 132?139, Seattle, Washington.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 1443?1452.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD dissertation, Uni-
versity of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1?8,
Philadelphia.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of the
22nd international conference on Machine learning,
ICML ?05, pages 169?176, New York, NY, USA.
Joshua Goodman. 1997. Global thresholding and
Multiple-Pass parsing. Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 11?25.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2003a. A* pars-
ing. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL ?03), pages 40?47, Edmonton, Canada.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics - ACL ?05, pages
75?82, Ann Arbor, Michigan.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34:513?553.
Adam Pauls, Dan Klein, and Chris Quirk. 2010. Top-
down k-best a* parsing. In In proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics Short Papers, ACLShort ?10, pages 200?204,
Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433?440, Syd-
ney, Australia.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, editors,
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 745?
752, Manchester, UK.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity Context-Free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647?655, Boulder, Colorado.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of ME and
SVM via integer linear programming. In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning, CONLL ?05, pages 233?236,
Morristown, NJ, USA.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast Lexicalised-Grammar parsing.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 1472?1479, Bei-
jing, China.
449
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 236?241,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Modeling for Prenominal Modifier Ordering
Margaret Mitchell
University of Aberdeen
Aberdeen, Scotland, U.K.
m.mitchell@abdn.ac.uk
Aaron Dunlop
Oregon Health & Science University
Portland, OR
dunlopa@cslu.ogi.edu
Brian Roark
Oregon Health & Science University
Portland, OR
roark@cslu.ogi.edu
Abstract
In this paper, we argue that ordering prenom-
inal modifiers ? typically pursued as a su-
pervised modeling task ? is particularly well-
suited to semi-supervised approaches. By
relying on automatic parses to extract noun
phrases, we can scale up the training data
by orders of magnitude. This minimizes
the predominant issue of data sparsity that
has informed most previous approaches. We
compare several recent approaches, and find
improvements from additional training data
across the board; however, none outperform
a simple n-gram model.
1 Introduction
In any given noun phrase (NP), an arbitrary num-
ber of nominal modifiers may be used. The order of
these modifiers affects how natural or fluent a phrase
sounds. Determining a natural ordering is a key task
in the surface realization stage of a natural language
generation (NLG) system, where the adjectives and
other modifiers chosen to identify a referent must be
ordered before a final string is produced. For ex-
ample, consider the alternation between the phrases
?big red ball? and ?red big ball?. The phrase ?big
red ball? provides a basic ordering of the words big
and red. The reverse ordering, in ?red big ball?,
sounds strange, a phrase that would only occur in
marked situations. There is no consensus on the ex-
act qualities that affect a modifier?s position, but it is
clear that some modifier orderings sound more natu-
ral than others, even if all are strictly speaking gram-
matical.
Determining methods for ordering modifiers
prenominally and investigating the factors underly-
ing modifier ordering have been areas of consider-
able research, including work in natural language
processing (Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000; Mitchell, 2009; Dunlop et al, 2010), lin-
guistics (Whorf, 1945; Vendler, 1968), and psychol-
ogy (Martin, 1969; Danks and Glucksberg, 1971).
A central issue in work on modifier ordering is how
to order modifiers that are unobserved during sys-
tem development. English has upwards of 200,000
words, with over 50,000 words in the vocabulary of
an educated adult (Aitchison, 2003). Up to a quar-
ter of these words may be adjectives, which poses a
significant problem for any system that attempts to
categorize English adjectives in ways that are useful
for an ordering task. Extensive in-context observa-
tion of adjectives and other modifiers is required to
adequately characterize their behavior.
Developers of automatic modifier ordering sys-
tems have thus spent considerable effort attempting
to make reliable predictions despite sparse data, and
have largely limited their systems to order modifier
pairs instead of full modifier strings. Conventional
wisdom has been that direct evidence methods such
as simple n-gram modeling are insufficient for cap-
turing such a complex and productive process.
Recent approaches have therefore utilized in-
creasingly sophisticated data-driven approaches.
Most recently, Dunlop et al (2010) used both dis-
criminative and generative methods for estimat-
ing class-based language models with multiple-
sequence alignments (MSA). Training on manually
curated syntactic corpora, they showed excellent in-
domain performance relative to prior systems, and
decent cross-domain generalization.
However, following a purely supervised training
approach for this task is unduly limiting and leads
to conventional assumptions that are not borne out
in practice, such as the inapplicability of simple n-236
gram models. NP segmentation is one of the most
reliable annotations that automatic parsers can now
produce, and may be applied to essentially arbitrary
amounts of unlabeled data. This yields orders-of-
magnitude larger training sets, so that methods that
are sensitive to sparse data and/or are domain spe-
cific can be trained on sufficient data.
In this paper, we compare an n-gram language
model and a hidden Markov model (HMM) con-
structed using expectation maximization (EM) with
several recent ordering approaches, and demonstrate
superior performance of the n-gram model across
different domains, particularly as the training data
size is scaled up. This paper presents two important
results: 1) N-gram modeling performs better than
previously believed for this task, and in fact sur-
passes current class-based systems.1 2) Automatic
parsers can effectively provide essentially unlimited
training data for learning modifier ordering prefer-
ences. Our results point the way to larger scale data-
driven approaches to this and related tasks.
2 Related Work
In one of the earliest automatic prenominal mod-
ifier ordering systems, Shaw and Hatzivassiloglou
(1999) ordered pairs of modifiers, including adjec-
tives, nouns (?baseball field?); gerunds, (?running
man?); and participles (?heated debate?). They
described a direct evidence method, a transitivity
method, and a clustering method for ordering these
different kinds of modifiers, with the transitivity
technique returning the highest accuracy of 90.67%
on a medical text. However, when testing across
domains, their accuracy dropped to 56%, not much
higher than random guessing.
Malouf (2000) continued this work, ordering
prenominal adjective pairs in the BNC. He aban-
doned a bigram model, finding it achieved only
75.57% prediction accuracy, and instead pursued
statistical and machine learning techniques that are
more robust to data sparsity. Malouf achieved an
accuracy of 91.85% by combining three systems.
However, it is not clear whether the proposed or-
dering approaches extend to other kinds of modi-
fiers, such as gerund verbs and nouns, and he did
not present analysis of cross-domain generalization.
1But note that these approaches may still be useful, e.g.,
when the goal is to construct general modifier classes.
Dataset 2 mods 3 mods 4 mods
WSJ 02-21 auto 10,070 1,333 129
WSJ 02-21 manu 9,976 1,311 129
NYT 1,616,497 191,787 18,183
Table 1: Multi-modifier noun phrases in training data
Dataset 2 mods 3 mods 4 mods
WSJ 22-24 1,366 152 20
SWBD 1,376 143 19
Brown 1,428 101 9
Table 2: Multi-modifier noun phrases in testing data
Later, Mitchell (2009) focused on creating a class-
based model for modifier ordering. Her system
mapped each modifier to a class based on the fre-
quency with which it occurs in different prenominal
positions, and ordered unseen sequences based on
these classes. Dunlop et al (2010) used a Multiple
Sequence Alignment (MSA) approach to order mod-
ifiers, achieving the highest accuracy to date across
different domains. In contrast to earlier work, both
systems order full modifier strings.
Below, we evaluate these most recent systems,
scaling up the training data by several orders of mag-
nitude. Our results indicate that an n-gram model
outperforms previous systems, and generalizes quite
well across different domains.
3 Corpora
Following Dunlop et al (2010), we use the Wall St.
Journal (WSJ), Switchboard (SWBD) and Brown
corpus sections of the Penn Treebank (Marcus et al,
1993) as our supervised training and testing base-
lines. For semi-supervised training, we automati-
cally parse sections 02-21 of the WSJ treebank using
cross-validation methods, and scale up the amount
of data used by parsing the New York Times (NYT)
section of the Gigaword (Graff and Cieri, 2003) cor-
pus using the Berkeley Parser (Petrov and Klein,
2007; Petrov, 2010).
Table 1 lists the NP length distributions for each
training corpus. The WSJ training corpus yields just
under 5,100 distinct modifier types (without normal-
izing for capitalization), while the NYT data yields
105,364. Note that the number of NPs extracted
from the manual and automatic parses of the WSJ
are quite close. We find that the overlap between the
two groups is well over 90%, suggesting that extract-237
ing NPs from a large, automatically parsed corpus
will provide phrases comparable to manually anno-
tated NPs.
We evaluate across a variety of domains, includ-
ing (1) the WSJ sections 22-24, and sections com-
mensurate in size of (2) the SWBD corpus and (3)
the Brown corpus. Table 2 lists the NP length distri-
butions for each test corpus.
4 Methods
In this section, we present two novel prenominal
modifier ordering approaches: a 5-gram model and
an EM-trained HMM. In both systems, modifiers
that occur only once in the training data are given the
Berkeley parser OOV class labels (Petrov, 2010).
In Section 5, we compare these approaches to the
one-class system described in Mitchell (2010) and
the discriminative MSA described in Dunlop et al
(2010). We refer the interested reader to those pa-
pers for the details of their learning algorithms.
4.1 N-Gram Modeling
We used the SRILM toolkit (Stolcke, 2002) to build
unpruned 5-gram models using interpolated mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998). In the testing phase, each possible permuta-
tion is assigned a probability by the model, and the
highest probability sequence is chosen.
We explored building n-gram models based on
entire observed sequences (sentences) and on ex-
tracted multiple modifier NPs. As shown in Table
3, we found a very large (12% absolute) accuracy
improvement in a model trained with just NP se-
quences. This is likely due to several factors, in-
cluding the role of the begin string symbol <s>,
which helps to capture word preferences for occur-
ring first in a modifier sequence; also the behav-
ior of modifiers when they occur in NPs may dif-
fer from how they behave in other contexts. Note
that the full-sentence n-gram model performs sim-
ilarly to Malouf?s bigram model; although the re-
sults are not directly comparable, this may explain
the common impression that n-gram modeling is not
effective for modifier ordering. We find that syntac-
tic annotations are critical for this task; all n-gram
results presented in the rest of the paper are trained
on extracted NPs.
Training data for n-gram model Accuracy
Full sentences 75.9
Extracted multi-modifier NPs 88.1
Table 3: Modifier ordering accuracy on WSJ sections 22-
24, trained on sections 2-21
4.2 Hidden Markov Model
Mitchell?s single-class system and Dunlop et. al?s
MSA approach both group tokens into position clus-
ters. The success of these systems suggests that a
position-specific class-based HMM might perform
well on this task. We use EM (Dempster et al, 1977)
to learn the parameterizations of such an HMM.
The model is defined in terms of state transition
probabilities P(c? | c), i.e., the probability of transi-
tioning from a state labeled c to a state labeled c?;
and state observation probabilities P(w | c), i.e.,
the probability of emitting word w from a particu-
lar class c. Since the classes are predicting an or-
dering, we include hard constraints on class tran-
sitions. Specifically, we forbid a transition from a
class closer to the head noun to one farther away.
More formally, if the subscript of a class indicates
its distance from the head, then for any i, j, P(ci |
cj) = 0 if i ? j; i.e., ci is stipulated to never occur
closer to the head than cj .
We established 8 classes and an HMM Markov
order of 1 (along with start and end states) based
on performance on a held-out set (section 00 of the
WSJ treebank). We initialize the model with a uni-
form distribution over allowed transition and emis-
sion probabilities, and use add-? regularization in
the M-step of EM at each iteration. We empirically
determined ? smoothing values of 0.1 for emissions
and 500 for transitions. Rather than training to full
convergence of the corpus likelihood, we stop train-
ing when there is no improvement in ordering accu-
racy on the held-out dataset for five iterations, and
output the best scoring model.
Because of the constraints on transition probabil-
ities, straightforward application of EM leads to the
transition probabilities strongly skewing the learn-
ing of emission probabilities. We thus followed a
generalized EM procedure (Neal and Hinton, 1998),
updating only emission probabilities until no more
improvement is achieved, and then training both
emission and transition probabilities. Often, we238
WSJ Accuracy SWBD Accuracy Brown Accuracy
Training data Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA
WSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5
auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6
NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.4
20% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.1
50% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6
all 92.4 80.2 89.3 92.1 86.4 74.5 81.4 83.4 82.3 69.3 79.3 82.0
NYT+WSJ auto 93.7 81.1 89.7 92.2 86.3 74.5 81.3 83.4 82.3 69.3 79.3 81.8
Table 4: Results on WSJ sections 22-24, Switchboard test set, and Brown test set for n-gram model (Ngr), Mitchell?s
single-class system (1-cl), HMM and MSA systems, under various training conditions.
find no improvement with the inclusion of transition
probabilities, and they are left uniform. In this case,
test ordering is determined by the class label alone.
5 Empirical results
Several measures have been used to evaluate the
accuracy of a system?s modifier ordering, includ-
ing both type/token accuracy, pairwise accuracy, and
full string accuracy. We evaluate full string ordering
accuracy over all tokens in the evaluation set. For
every NP, if the model?s highest-scoring ordering is
identical to the actual observed order, it is correct;
otherwise, it is incorrect. We report the percentage
of orders correctly predicted.
We evaluate under a variety of training conditions,
on WSJ sections 22-24, as well as the testing sec-
tions from the Switchboard and Brown corpus por-
tions of the Penn Treebank. We perform no domain-
specific tuning, so the results on the Switchboard
and Brown corpora demonstrate cross-domain appli-
cability of the approaches.
5.1 Manual parses versus automatic parses
We begin by comparing the NPs extracted from
manual parses to those extracted from automatic
parses. We parsed Wall Street Journal sections 02
through 21 using cross-validation to ensure that the
parses are as errorful as when sentences have never
been observed by training.
Table 4 compares models trained on these two
training corpora, as evaluated on the manually-
annotated test set. No system?s accuracy degrades
greatly when using automatic parses, indicating that
we can likely derive useful training data by automat-
ically parsing a large, unlabeled training corpus.
5.2 Semi-supervised models
We now evaluate performance of the models on the
scaled up training data. Using the Berkeley parser,
we parsed 169 million words of NYT text from the
English Gigaword corpus (Graff and Cieri, 2003),
extracted the multiple modifier NPs, and trained our
various models on this data. Rows 3-6 of Table
4 show the accuracy on WSJ sections 22-24 after
training on 10%, 20%, 50% and 100% of this data.
Note that this represents approximately 150 times
the amount of training data as the original treebank
training data. Even with just 10% of this data (a
15-fold increase in the training data), we see across
the board improvements. Using all of the NYT data
results in approximately 5% absolute performance
increase for the n-gram and MSA models, yielding
roughly commensurate performance, over 92% ac-
curacy. Although we do not have space to present
the results in this paper, we found further improve-
ments (over 1% absolute, statistically significant) by
combining the four models, indicating a continued
benefit of the other models, even if none of them
best the n-gram individually.
Based on these results, this task is clearly
amenable to semi-supervised learning approaches.
All systems show large accuracy improvements.
Further, contrary to conventional wisdom, n-gram
models are very competitive with recent high-
accuracy frameworks. Additionally, n-gram models
appear to be domain sensitive, as evidenced by the
last row of Table 4, which presents results when the
1.8 million NPs in the NYT corpus are augmented
with just 11 thousand NPs from the WSJ (auto) col-
lection. The n-gram model still outperforms the
other systems, but improves by well over a percent,
while the class-based HMM and MSA approaches239
are relatively static. (The single-class system shows
some domain sensitivity, improving nearly a point.)
5.3 Cross-domain evaluation
With respect to cross-domain applicability, we see
that, as with the WSJ evaluation, the MSA and n-
gram approaches are roughly commensurate on the
Brown corpus; but the n-gram model shows a greater
advantage on the Switchboard test set when trained
on the NYT data. Perhaps this is due to higher re-
liance on conventionalized collocations in the spo-
ken language of Switchboard. Finally, it is clear
that the addition of the WSJ data to the NYT data
yields improvements only for the specific newswire
domain ? none of the results change much for these
two new domains when the WSJ data is included
(last row of the table).
We note that the improvements observed when
scaling the training corpus with in-domain data per-
sist when applied to very diverse domains. Interest-
ingly, n-gram models, which may have been consid-
ered unlikely to generalize well to other domains,
maintain their superior performance in each trial.
6 Discussion
In this paper, we demonstrated the efficacy of scal-
ing up training data for prenominal modifier or-
dering using automatic parses. We presented two
novel systems for ordering prenominal modifiers,
and demonstrated that with sufficient data, a simple
n-gram model outperforms position-specific models,
such as an EM-trained HMM and the MSA approach
of Dunlop et al (2010). The accuracy achieved by
the n-gram model is particularly interesting, since
such models have previously been considered inef-
fective for this task. This does not obviate the need
for a class based model ? modifier classes may in-
form linguistic research, and system combination
still yields large improvements ? but points to new
data-rich methods for learning such models.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
References
Jean Aitchison. 2003. Words in the mind: an intro-
duction to the mental lexicon. Blackwell Publishing,
Cornwall, United Kindgom, third edition. p. 7.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
Joseph H. Danks and Sam Glucksberg. 1971. Psycho-
logical scaling of adjective order. Journal of Verbal
Learning and Verbal Behavior, 10(1):63?67.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Se-
ries B, 39(1):1?38.
Aaron Dunlop, Margaret Mitchell, and Brian Roark.
2010. Prenominal modier ordering via multiple se-
quence alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL (HLT-NAACL 2010), pages 600?
608, Los Angeles, CA, USA. Association for Compu-
tational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. Linguistic Data Consortium, Philadelphia, PA,
USA.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings of
the 38th ACL (ACL 2000), pages 85?92, Hong Kong.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
J. E. Martin. 1969. Semantic determinants of preferred
adjective order. Journal of Verbal Learning and Verbal
Behavior, 8(6):697?704.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 50?57, Athens, Greece. Associa-
tion for Computational Linguistics.
Margaret Mitchell. 2010. A flexible approach to class-
based ordering of prenominal modifiers. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, volume 5980 of Lec-
ture Notes in Computer Science. Springer, Berlin /
Heidelberg.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. Kluwer Academic Publish-
ers, Dordrecht.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American240
Chapter of the ACL (HLT-NAACL 2007), pages 404?
411, Rochester, NY, USA. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Berkeley parser. GNU General Pub-
lic License v.2.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
ACL (ACL 1999), pages 135?143, College Park, Mary-
land. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP
2002), volume 2, pages 901?904.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton, The Netherlands.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1?11.
241
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 797?802,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Transforming trees into hedges and parsing with ?hedgebank? grammars
Mahsa Yarmohammadi
?
, Aaron Dunlop
?
and Brian Roark
?
?
Oregon Health & Science University, Portland, Oregon
?
Google, Inc., New York
yarmoham@ohsu.edu, {aaron.dunlop,roarkbr}@gmail.com
Abstract
Finite-state chunking and tagging meth-
ods are very fast for annotating non-
hierarchical syntactic information, and are
often applied in applications that do not
require full syntactic analyses. Scenar-
ios such as incremental machine transla-
tion may benefit from some degree of hier-
archical syntactic analysis without requir-
ing fully connected parses. We introduce
hedge parsing as an approach to recover-
ing constituents of length up to some max-
imum span L. This approach improves ef-
ficiency by bounding constituent size, and
allows for efficient segmentation strategies
prior to parsing. Unlike shallow parsing
methods, hedge parsing yields internal hi-
erarchical structure of phrases within its
span bound. We present the approach and
some initial experiments on different infer-
ence strategies.
1 Introduction
Parsing full hierarchical syntactic structures is
costly, and some NLP applications that could ben-
efit from parses instead substitute shallow prox-
ies such as NP chunks. Models to derive such
non-hierarchical annotations are finite-state, so in-
ference is very fast. Still, these partial annota-
tions omit all but the most basic syntactic segmen-
tation, ignoring the abundant local structure that
could be of utility even in the absence of fully con-
nected structures. For example, in incremental (si-
multaneous) machine translation (Yarmohammadi
et al, 2013), sub-sentential segments are trans-
lated independently and sequentially, hence the
fully-connected syntactic structure is not generally
available. Even so, locally-connected source lan-
guage parse structures can inform both segmen-
tation and translation of each segment in such a
translation scenario.
One way to provide local hierarchical syntactic
structures without fully connected trees is to fo-
cus on providing full hierarchical annotations for
structures within a local window, ignoring global
constituents outside that window. We follow the
XML community in naming structures of this type
hedges (not to be confused with the rhetorical de-
vice of the same name), due to the fact that they are
like smaller versions of trees which occur in se-
quences. Such structures may be of utility to var-
ious structured inference tasks, as well as within
a full parsing pipeline, to quickly constrain subse-
quent inference, much as finite-state models such
as supertagging (Bangalore and Joshi, 1999) or
chart cell constraints (Roark and Hollingshead,
2008; Roark et al, 2012) are used.
In this paper, we consider the problem of hedge
parsing, i.e., discovering every constituent of
length up to some span L. Similar constraints
have been used in dependency parsing (Eisner
and Smith, 2005; Dreyer et al, 2006), where the
use of hard constraints on the distance between
heads and dependents is known as vine parsing.
It is also reminiscent of so-called Semi-Markov
models (Sarawagi and Cohen, 2004), which allow
finite-state models to reason about segments rather
than just tags by imposing segment length limits.
In the XML community, trees and hedges are used
for models of XML document instances and for
the contents of elements (Br?uggemann-Klein and
Wood, 2004). As far as we know, this paper is
the first to consider this sort of partial parsing ap-
proach for natural language.
We pursue this topic via tree transformation,
whereby non-root non-terminals labeling con-
stituents of span > L in the tree are recursively
elided and their children promoted to attach to
their parent. In such a way, hedges are sequen-
tially connected to the top-most non-terminal in
the tree, as demonstrated in Figure 1. After apply-
ing such a transform to a treebank, we can induce
grammars and modify parsing to search as needed
to recover just these constituents.
In this paper, we propose several methods to
797
a)
33
,1 13
116
LQYHVWRUV
IRU
$'-3
--
WUHDFKHURXV
9%
UHPDLQ
93
0'
ZLOO
93
'7 -- 11
WKH KLJK\LHOG PDUNHW
13
,1
RI
33
--
PXFK
13
13
116
$QDO\VWV
13
9%3
DUH
--
FRQFHUQHG
,1
WKDW


6
93
6
$'-3
6%$5
b)
33
,1 13
116
LQYHVWRUV
IRU
$'-3
--
WUHDFKHURXV
9%
UHPDLQ
93
0'
ZLOO
93
'7 -- 11
WKH KLJK\LHOG PDUNHW
13
,1
RI
33
--
PXFK
13
13
116
$QDO\VWV
13 9%3
DUH
--
FRQFHUQHG
,1
WKDW


6
Figure 1: a) Full parse tree, b) Hedge parse tree with maximum constituent span of 7 (L = 7).
parse hedge constituents and examine their accu-
racy/efficiency tradeoffs. This is compared with
a baseline of parsing with a typically induced
context-free grammar and transforming the result
via the hedge transform, which provides a ceiling
on accuracy and a floor on efficiency. We inves-
tigate pre-segmenting the sentences with a finite-
state model prior to hedge parsing, and achieve
large speedups relative to hedge parsing the whole
string, though at a loss in accuracy due to cas-
cading segmentation errors. In all cases, we find
it crucial that our ?hedgebank? grammars be re-
trained to match the conditions during inference.
2 Methods
In this section, we present the details of our ap-
proach. First, we present the simple tree transform
from a full treebank parse tree to a (root attached)
sequence of hedges. Next, we discuss modifica-
tions to inference and the resulting computational
complexity gains. Finally, we discuss segmenting
to further reduce computational complexity.
2.1 Hedge Tree Transform
The hedge tree transform converts the original
parse tree into a hedge parse tree. In the resulting
hedge parse tree, every child of the top-most node
spans at most L words. To transform an original
tree to a hedge tree, we remove every non-terminal
with span larger than L and attach its children to its
parent. We label span length on each node by re-
cursively summing the span lengths of each node?s
children, with terminal items by definition having
span 1. A second top-down pass evaluates each
node before evaluating its children, and removes
nodes spanning > L words. For example, the span
of the non-root S, SBAR, ADJP, and VP nodes in
Figure 1(a) have spans between 10 and 13, hence
are removed in the tree in Figure 1(b).
If we apply this transform to an entire tree-
bank, we can use the transformed trees to induce
a PCFG for parsing. Figure 2 plots the percentage
of constituents from the original WSJ Penn tree-
bank (sections 2-21) retained in the transformed
version, as we vary the maximum span length pa-
rameter L. Over half of constituents have span 3 or
less (which includes frequent base noun phrases);
L = 7 covers approximately three quarters of the
original constituents, and L = 15 over 90%. Most
experiments in this paper will focus on L = 7,
which is short enough to provide a large speedup
yet still cover a large fraction of constituents.
2.2 Hedge Parsing
As stated earlier, our brute-force baseline ap-
proach is to parse the sentence using a full context-
free grammar (CFG) and then hedge-transform the
result. This method should yield a ceiling on
798
P
c
t
.
o
f
c
o
n
s
t
i
t
u
e
n
t
s
r
e
t
a
i
n
e
d
0 5 10 15 20
50
60
70
80
90
100
Maximum ?span ?size ?(L)
P
e
r
c
e
n
t
a
g
e
 ?
o
f
 ?
c
o
n
s
t
i
t
u
e
n
t
s
 ?
r
e
t
a
i
n
e
d
c
e
s
s
t
o
c
o
m
p
a
r
e
d
w
i
t
h
g
r
a
m
m
a
r
s
t
r
a
i
n
e
d
o
n
t
r
a
n
s
-
f
o
r
m
e
d
t
r
e
e
s
;
b
u
t
i
t
w
i
l
l
b
e
s
l
o
w
e
r
t
o
p
a
r
s
e
.
W
e
a
i
m
t
o
d
r
a
m
a
t
i
c
a
l
l
y
i
m
p
r
o
v
e
e
f
fi
c
i
e
n
c
y
u
p
o
n
t
h
i
s
b
a
s
e
l
i
n
e
w
h
i
l
e
l
o
s
i
n
g
a
s
l
i
t
t
l
e
a
c
c
u
r
a
c
y
a
s
p
o
s
s
i
b
l
e
.
S
i
n
c
e
w
e
l
i
m
i
t
t
h
e
s
p
a
n
o
f
n
o
n
-
t
e
r
m
i
n
a
l
l
a
-
b
e
l
s
,
w
e
c
a
n
c
o
n
s
t
r
a
i
n
t
h
e
s
e
a
r
c
h
p
e
r
f
o
r
m
e
d
b
y
t
h
e
p
a
r
s
e
r
,
g
r
e
a
t
l
y
r
e
d
u
c
e
t
h
e
C
Y
K
p
r
o
c
e
s
s
i
n
g
t
i
m
e
.
I
n
e
s
s
e
n
c
e
,
w
e
p
e
r
f
o
r
m
n
o
w
o
r
k
i
n
c
h
a
r
t
c
e
l
l
s
s
p
a
n
-
n
i
n
g
m
o
r
e
t
h
a
n
L
w
o
r
d
s
,
e
x
c
e
p
t
f
o
r
t
h
e
c
e
l
l
s
a
l
o
n
g
t
h
e
p
e
r
i
p
h
e
r
y
o
f
t
h
e
c
h
a
r
t
,
w
h
i
c
h
a
r
e
j
u
s
t
u
s
e
d
t
o
c
o
n
n
e
c
t
t
h
e
h
e
d
g
e
s
t
o
t
h
e
r
o
o
t
.
C
o
n
s
i
d
e
r
t
h
e
fl
a
t
t
r
e
e
i
n
F
i
g
u
r
e
1
.
F
o
r
u
s
e
b
y
a
C
Y
K
p
a
r
s
i
n
g
a
l
-
g
o
r
i
t
h
m
,
t
r
e
e
s
a
r
e
b
i
n
a
r
i
z
e
d
p
r
i
o
r
t
o
g
r
a
m
m
a
r
i
n
-
d
u
c
t
i
o
n
,
r
e
s
u
l
t
i
n
g
i
n
s
p
e
c
i
a
l
n
o
n
-
t
e
r
m
i
n
a
l
s
c
r
e
a
t
e
d
b
y
b
i
n
a
r
i
z
a
t
i
o
n
.
O
t
h
e
r
t
h
a
n
t
h
e
s
y
m
b
o
l
a
t
t
h
e
r
o
o
t
o
f
t
h
e
t
r
e
e
,
t
h
e
o
n
l
y
c
o
n
s
t
i
t
u
e
n
t
s
w
i
t
h
s
p
a
n
l
e
n
g
t
h
g
r
e
a
t
e
r
t
h
a
n
L
i
n
t
h
e
b
i
n
a
r
i
z
e
d
t
r
e
e
w
i
l
l
b
e
l
a
b
e
l
e
d
w
i
t
h
t
h
e
s
e
s
p
e
c
i
a
l
b
i
n
a
r
i
z
a
t
i
o
n
n
o
n
-
t
e
r
m
i
n
a
l
s
.
F
u
r
-
t
h
e
r
,
i
f
t
h
e
b
i
n
a
r
i
z
a
t
i
o
n
s
y
s
t
e
m
a
t
i
c
a
l
l
y
g
r
o
u
p
s
t
h
e
l
e
f
t
m
o
s
t
o
r
t
h
e
r
i
g
h
t
m
o
s
t
c
h
i
l
d
r
e
n
u
n
d
e
r
t
h
e
s
e
n
e
w
n
o
n
-
t
e
r
m
i
n
a
l
s
(
t
h
e
m
o
s
t
c
o
m
m
o
n
s
t
r
a
t
e
g
y
)
,
t
h
e
n
c
o
n
s
t
i
t
u
e
n
t
s
w
i
t
h
s
p
a
n
g
r
e
a
t
e
r
t
h
a
n
L
w
i
l
l
e
i
t
h
e
r
b
e
g
i
n
a
t
t
h
e
fi
r
s
t
w
o
r
d
(
l
e
f
t
m
o
s
t
g
r
o
u
p
i
n
g
)
o
r
e
n
d
a
t
t
h
e
l
a
s
t
w
o
r
d
(
r
i
g
h
t
m
o
s
t
g
r
o
u
p
i
n
g
)
,
f
u
r
t
h
e
r
c
o
n
-
s
t
r
a
i
n
i
n
g
t
h
e
n
u
m
b
e
r
o
f
c
e
l
l
s
i
n
t
h
e
c
h
a
r
t
r
e
q
u
i
r
i
n
g
w
o
r
k
.
C
o
m
p
l
e
x
i
t
y
o
f
p
a
r
s
i
n
g
w
i
t
h
a
f
u
l
l
C
Y
K
p
a
r
s
e
r
i
s
O
(
n
3
|
G
|
)
w
h
e
r
e
n
i
s
t
h
e
l
e
n
g
t
h
o
f
i
n
p
u
t
a
n
d
|
G
|
i
s
t
h
e
g
r
a
m
m
a
r
s
i
z
e
c
o
n
s
t
a
n
t
.
I
n
c
o
n
t
r
a
s
t
,
c
o
m
p
l
e
x
-
i
t
y
o
f
p
a
r
s
i
n
g
w
i
t
h
a
h
e
d
g
e
c
o
n
s
t
r
a
i
n
e
d
C
Y
K
i
s
r
e
-
d
u
c
e
d
t
o
O
(
(
n
L
2
+
n
2
)
|
G
|
)
.
T
o
s
e
e
t
h
a
t
t
h
i
s
i
s
t
h
e
c
a
s
e
,
c
o
n
s
i
d
e
r
t
h
a
t
t
h
e
r
e
a
r
e
O
(
n
L
)
c
e
l
l
s
o
f
s
p
a
n
L
o
r
l
e
s
s
,
a
n
d
e
a
c
h
h
a
s
a
m
a
x
i
m
u
m
o
f
L
m
i
d
p
o
i
n
t
s
,
w
h
i
c
h
a
c
c
o
u
n
t
s
f
o
r
t
h
e
fi
r
s
t
t
e
r
m
.
B
e
y
o
n
d
t
h
e
s
e
,
t
h
e
r
e
a
r
e
O
(
n
)
r
e
m
a
i
n
i
n
g
a
c
t
i
v
e
c
e
l
l
s
w
i
t
h
O
(
n
)
p
o
s
s
i
b
l
e
m
i
d
p
o
i
n
t
s
,
w
h
i
c
h
a
c
c
o
u
n
t
s
f
o
r
t
h
e
s
e
c
o
n
d
t
e
r
m
.
N
o
t
e
,
h
o
w
e
v
e
r
,
t
h
a
t
t
h
e
w
o
r
k
i
n
t
h
e
s
e
l
a
t
t
e
r
c
e
l
l
s
m
a
y
b
e
l
e
s
s
,
s
i
n
c
e
t
h
e
s
e
t
o
f
p
o
s
s
i
b
l
e
n
o
n
-
t
e
r
m
i
n
a
l
s
i
s
r
e
d
u
c
e
d
.
I
t
i
s
p
o
s
s
i
b
l
e
t
o
p
a
r
s
e
w
i
t
h
a
s
t
a
n
d
a
r
d
l
y
i
n
d
u
c
e
d
P
C
F
G
u
s
i
n
g
t
h
i
s
s
o
r
t
o
f
h
e
d
g
e
c
o
n
s
t
r
a
i
n
e
d
p
a
r
s
-
i
n
g
t
h
a
t
o
n
l
y
c
o
n
s
i
d
e
r
s
a
s
u
b
s
e
t
o
f
t
h
e
c
h
a
r
t
c
e
l
l
s
,
a
n
d
s
p
e
e
d
u
p
s
a
r
e
a
c
h
i
e
v
e
d
,
h
o
w
e
v
e
r
t
h
i
s
i
s
c
l
e
a
r
l
y
n
o
n
-
o
p
t
i
m
a
l
,
s
i
n
c
e
t
h
e
m
o
d
e
l
i
s
i
l
l
-
s
u
i
t
e
d
t
o
c
o
m
-
b
i
n
i
n
g
h
e
d
g
e
s
i
n
t
o
fl
a
t
s
t
r
u
c
t
u
r
e
s
a
t
t
h
e
r
o
o
t
o
f
t
h
e
t
r
e
e
.
T
h
i
s
r
e
s
u
l
t
s
i
n
d
e
g
r
a
d
a
t
i
o
n
o
f
p
a
r
s
i
n
g
p
e
r
-
f
o
r
m
a
n
c
e
b
y
t
e
n
s
o
f
p
o
i
n
t
s
o
f
F
-
m
e
a
s
u
r
e
v
e
r
s
u
s
s
t
a
n
d
a
r
d
p
a
r
s
i
n
g
.
I
n
s
t
e
a
d
,
i
n
a
l
l
s
c
e
n
a
r
i
o
s
w
h
e
r
e
a
t
h
e
c
h
a
r
t
i
s
c
o
n
s
t
r
a
i
n
e
d
t
o
s
e
a
r
c
h
f
o
r
h
e
d
g
e
s
,
w
e
l
e
a
r
n
a
g
r
a
m
m
a
r
f
r
o
m
a
h
e
d
g
e
t
r
a
n
s
f
o
r
m
e
d
t
r
e
e
-
b
a
n
k
,
m
a
t
c
h
e
d
t
o
t
h
e
m
a
x
i
m
u
m
l
e
n
g
t
h
a
l
l
o
w
e
d
b
y
t
h
e
p
a
r
s
e
r
,
w
h
i
c
h
w
e
c
a
l
l
a
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
.
A
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
i
s
a
f
u
l
l
y
f
u
n
c
t
i
o
n
a
l
P
C
F
G
a
n
d
i
t
c
a
n
b
e
u
s
e
d
w
i
t
h
a
n
y
s
t
a
n
d
a
r
d
p
a
r
s
i
n
g
a
l
-
g
o
r
i
t
h
m
,
i
.
e
.
,
t
h
e
s
e
a
r
e
n
o
t
g
e
n
e
r
a
l
l
y
fi
n
i
t
e
-
s
t
a
t
e
e
q
u
i
v
a
l
e
n
t
m
o
d
e
l
s
.
H
o
w
e
v
e
r
,
u
s
i
n
g
t
h
e
B
e
r
k
e
-
l
e
y
g
r
a
m
m
a
r
l
e
a
r
n
e
r
(
s
e
e
S
e
c
t
i
o
n
3
)
,
w
e
fi
n
d
t
h
a
t
h
e
d
g
e
b
a
n
k
g
r
a
m
m
a
r
s
a
r
e
t
y
p
i
c
a
l
l
y
s
m
a
l
l
e
r
t
h
a
n
t
r
e
e
b
a
n
k
g
r
a
m
m
a
r
s
,
a
l
s
o
c
o
n
t
r
i
b
u
t
i
n
g
t
o
p
a
r
s
i
n
g
s
p
e
e
d
u
p
v
i
a
t
h
e
g
r
a
m
m
a
r
c
o
n
s
t
a
n
t
.
A
u
n
i
q
u
e
p
r
o
p
e
r
t
y
o
f
h
e
d
g
e
c
o
n
s
t
i
t
u
e
n
t
s
c
o
m
-
p
a
r
e
d
t
o
c
o
n
s
t
i
t
u
e
n
t
s
i
n
t
h
e
o
r
i
g
i
n
a
l
p
a
r
s
e
t
r
e
e
s
i
s
t
h
a
t
t
h
e
y
a
r
e
s
e
q
u
e
n
t
i
a
l
l
y
c
o
n
n
e
c
t
e
d
t
o
t
h
e
T
O
P
n
o
d
e
.
T
h
i
s
p
r
o
p
e
r
t
y
e
n
a
b
l
e
s
u
s
t
o
c
h
u
n
k
t
h
e
s
e
n
t
e
n
c
e
i
n
t
o
s
e
g
m
e
n
t
s
t
h
a
t
c
o
r
r
e
s
p
o
n
d
t
o
c
o
m
-
p
l
e
t
e
h
e
d
g
e
s
,
a
n
d
p
a
r
s
e
t
h
e
s
e
g
m
e
n
t
s
i
n
d
e
p
e
n
-
d
e
n
t
l
y
(
a
n
d
s
i
m
u
l
t
a
n
e
o
u
s
l
y
)
i
n
s
t
e
a
d
o
f
p
a
r
s
i
n
g
t
h
e
e
n
t
i
r
e
s
e
n
t
e
n
c
e
.
I
n
s
e
c
t
i
o
n
2
.
3
,
w
e
p
r
e
s
e
n
t
o
u
r
a
p
-
p
r
o
a
c
h
t
o
h
e
d
g
e
s
e
g
m
e
n
t
a
t
i
o
n
.
N
o
t
e
t
h
a
t
p
a
r
s
i
n
g
s
e
g
m
e
n
t
s
w
i
t
h
a
g
r
a
m
m
a
r
t
r
a
i
n
e
d
o
n
w
h
o
l
e
s
t
r
i
n
g
s
Maximum span size (L)
Figure 2: Percentage of constituents retained at various
span length parameters
hedge-parsing accuracy, as it has access to rich
contextual information (as compared to grammars
trained on transformed trees). Naturally, inference
will be slow; we aim to improve efficiency upon
this baseline while minimizing accuracy loss.
Since we limit the span of non-terminal la-
bels, we can constrain the search performed by the
parser, greatly reduce the CYK processing time. In
essence, we perform no work in chart cells span-
ning more than L words, except for the cells along
the periphery of the chart, which are just used to
connect the hedges to the root. Consider the flat
tree in Figure 1(b). For use by a CYK parsing al-
gorithm, trees are binarized prior to grammar in-
duction, resulting in special non-terminals created
by binarization. Other than the symbol at the root
of the tree, the only constituents with span length
greater than L in the binarized tree will be labeled
with these special binarization non-terminals. Fur-
ther, if the binarization systematically groups the
leftmost or the rightmost children under these new
non-terminals (the most common strategy), then
constituents with span greater than L will either
begin at the first word (leftmost grouping) or end
at the last word (rightmost), further constraining
the number of cells in the chart requiring work.
Complexity of parsing with a full CYK parser is
O(n
3
|G|) where n is the length of input and |G| is
the grammar size constant. In contrast, complex-
ity of parsing with a hedge constrained CYK is re-
duced to O((nL
2
+n
2
)|G|). To see that this is the
case, consider that there are O(nL) cells of span L
or less, and each has a maximum of L midpoints,
which accounts for the first term. Beyond these,
there are O(n) remaining active cells with O(n)
possible midpoints, which accounts for the second
term. Note also that these latter cells (spanning
> L words) may be less expensive, as the set of
possible non-terminals is reduced to only those in-
troduced by binarization.
It is possible to parse with a standardly induced
PCFG using this sort of hedge constrained pars-
ing that only considers a subset of the chart cells,
and speedu s are achiev d, however this is clearly
non-optimal, since the mod l is ill-suited to com-
bining hedges into flat structures at the root of the
tree. Space constraints preclude inclusion of tri-
als with this method, b t the net result is a se-
vere degradation in accuracy (tens of points of F-
measure) versus standar parsing. Thus, we train
a grammar in a matched condition, which we call
it a hedgebank grammar. A hedgebank gram-
mar is a fully functional PCFG which is learned
from a hedge transformed treebank. A hedgebank
grammar can be used with any standard parsing
algorithm, i.e., these are not generally finite-state
equivalent models. However, using the Berke-
ley grammar learner (see ?3), we find that hedge-
bank grammars are typically smaller than tree-
bank grammars, reducing the grammar constant
and contributing to faster inference.
A unique property of hedge constituents com-
pared to constituents in the original parse trees
is that they are sequentially connected to the top-
most node. This property enables us to chunk the
sentence into segments that correspond to com-
plete hedges, and parse the segments indepen-
dently (and simultaneously) instead of parsing the
entire sentence. In section 2.3, we present our ap-
proach to hedge segmentation.
In all scenarios where the chart is constrained
to search for hedges, we learn a hedgebank gram-
mar, which is matched to the maximum length al-
lowed by the parser. In the pre-segmentation sce-
nario, we first decompose the hedge transformed
treebank into its hedge segments and then learn a
hedgebank grammar from the new corpus.
2.3 Hedge Segmentation
In this section we present our segmentation model
which takes the input sentence and chunks it into
appropriate segments for hedge parsing. We treat
this as a binary classification task which decides
if a word can begin a new hedge. We use hedge
segmentation as a finite-state pre-processing step
for hedge context-free parsing.
Our task is to learn which words can begin
(B) a hedge constituent. Given a set of labeled
pairs (S,H) where S is a sentence of n words
w
1
. . . w
n
and H is its hedge parse tree, word w
b
belongs to B if there is a hedge constituent span-
ning w
b
. . . w
e
for some e ? b and w
b
belongs to
?
B
otherwise. To predict the hedge boundaries more
accurately, we grouped consecutive unary or POS-
799
tag hedges together under a new non-terminal la-
beled G. Unlabeled segmentation tags for the
words in the example sentence in Figure 1(b) are:
?Analysts/B are/
?
B concerned/
?
B that/
?
B much/B
of/
?
B the/
?
B high-yield/
?
B market/
?
B will/B
remain/
?
B treacherous/
?
B for/
?
B investors/
?
B ./B?
In addition to the simple unlabeled segmentation
with B and
?
B tags, we try a labeled segmenta-
tion with B
C
and
?
B
C
tags where C is hedge con-
stituent type. We restrict the types to the most im-
portant types ? following the 11 chunk types an-
notated in the CoNLL-2000 chunking task (Sang
and Buchholz, 2000) ? by replacing all other types
with a new type OUT. Thus, ?Analysts? is labeled
B
G
; ?much?, B
NP
; ?will?, B
VP
and so on.
To automatically predict the class of each word
position, we train a multi-class classifier from la-
beled training data using a discriminative linear
model, learning the model parameters with the av-
eraged perceptron algorithm (Collins, 2002). We
follow Roark et al (2012) in the features they used
to label words as beginning or ending constituents.
The segmenter extracts features from word and
POS-tag input sequences and hedge-boundary tag
output sequences. The feature set includes tri-
grams of surrounding words, trigrams of surround-
ing POS tags, and hedge-boundary tags of the pre-
vious words. An additional orthographical fea-
ture set is used to tag rare
1
and unknown words.
This feature set includes prefixes and suffixes of
the words (up to 4 characters), and presence of
a hyphen, digit, or an upper-case character. Re-
ported results are for a Markov order-2 segmenter,
which includes features with the output classes of
the previous two words.
3 Experimental Results
We ran all experiments on the WSJ Penn Tree-
bank corpus (Marcus et al, 1999) using section
2-21 for training, section 24 for development, and
section 23 for testing. We performed exhaustive
CYK parsing using the BUBS parser
2
(Bodenstab
et al, 2011) with Berkeley SM6 latent-variable
grammars (Petrov and Klein, 2007) learned by the
Berkeley grammar trainer with default settings.
We compute accuracy from the 1-best Viterbi
tree extracted from the chart using the standard
EVALB script. Accuracy results are reported as
precision, recall and F1-score, the harmonic mean
between the two. In all trials, we evaluate accuracy
with respect to the hedge transformed reference
1
Rare words occur less than 5 times in the training data.
2
https://code.google.com/p/bubs-parser
Hedge Parsing Acc/Eff
Parser P R F1 w/s
Full w/full CYK 88.8 89.2 89.0 2.4
Hedgebank 87.6 84.4 86.0 25.7
Table 1: Hedge parsing results on section 24 for L = 7.
treebank, i.e., we are not penalizing the parser for
not discovering constituents longer than the max-
imum length. Segmentation accuracy is reported
as an F1-score of unlabeled segment bracketing.
We ran timing tests on an Intel 2.66GHz proces-
sor with 3MB of cache and 2GB of memory. Note
that segmentation time is negligible compared to
the parsing time, hence is omitted in reported time.
Efficiency results are reported as number of words
parsed per second (w/s).
Table 1 presents hedge parsing accuracy on
the development set for the full parsing baseline,
where the output of regular PCFG parsing is trans-
formed to hedges and evaluated, versus parsing
with a hedgebank grammar, with no segmenta-
tion of the strings. We find an order of magnitude
speedup of parsing, but at the cost of 3 percent F-
measure absolute. Note that most of that loss is
in recall, indicating that hedges predicted in that
condition are nearly as reliable as in full parsing.
Table 2 shows the results on the development
set when segmenting prior to hedge parsing. The
first row shows the result with no segmentation,
the same as the last row in Table 1 for ease of ref-
erence. The next row shows behavior with per-
fect segmentation. The final two rows show per-
formance with automatic segmentation, using a
model that includes either unlabeled or labeled
segmentation tags, as described in the last section.
Segmentation accuracy is better for the model with
labels, although overall that accuracy is rather low.
We achieve nearly another order of magnitude
speedup over hedge parsing without segmentation,
but again at the cost of nearly 5 percent F1.
Table 3 presents results of our best configura-
tions on the eval set, section 23. The results show
the same patterns as on the development set. Fi-
nally, Figure 3 shows the speed of inference, la-
Table 2: Hedge segmentation and parsing results on section
24 for L = 7.
Segmen- Seg Hedge Parsing Acc/Eff
tation F1 P R F1 w/s
None n/a 87.6 84.4 86.0 25.7
Oracle 100 91.3 88.9 90.1 188.6
Unlabeled 80.6 77.2 75.3 76.2 159.1
Labeled 83.8 83.1 79.5 81.3 195.8
800
Segmentation Grammar
Segmentation Acc Hedge Parsing Acc/Eff
P R F1 P R F1 w/s
None Full w/full CYK n/a 90.3 90.3 90.3 2.7
None Hedgebank n/a 88.3 85.3 86.8 26.2
Labeled Hedgebank 84.0 86.6 85.3 85.1 81.1 83.0 203.0
Table 3: Hedge segmentation and parsing results on test data, section 23, for L = 7.
W
o
r
d
s
p
e
r
s
e
c
o
n
d
Full ?Parsing
Hedge ?No ?Seg
Hedge ?With ?Seg
0 5 10 15 20
0
200
400
600
800
Maximum ?span ?size ?(L)
W
o
r
d
s
 ?
p
a
r
s
e
d
 ?
p
e
r
 ?
s
e
c
o
n
d
W
o
r
d
s
p
e
r
s
e
c
o
n
d
Full ?Parsing
Hedge ?No ?Seg
Hedge ?With ?Seg
0 5 10 15 20
0
200
400
600
800
Maximum ?span ?size ?(L)
W
o
r
d
s
 ?
p
a
r
s
e
d
 ?
p
e
r
 ?
s
e
c
o
n
d
H
e
d
g
e
P
r
e
c
i
s
i
o
n
0 5 10 15 20
75
80
85
90
95
Maximum ?span ?size ?(L)
H
e
d
g
e
 ?
P
r
e
c
i
s
i
o
n
H
e
d
g
e
R
e
c
a
l
l
0 5 10 15 20
75
80
85
90
95
Maximum ?span ?size ?(L)
H
e
d
g
e
 ?
R
e
c
a
l
l
Figure 4: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3  20.
Maximum span size (L) axi um span size (L)
a) b)
Figure 3: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3?20.
beled precision and labeled recall of annotating
hedge constituents on the test set as a function
of the maximum span parameter L, versus the
baseline parser. Keep in mind that the number
of reference constituents increases as L increases,
hence both precision and recall can decrease as
the parameter grows. Segmentation achieves large
speedups for smaller L values, but the accuracy
degradation is consistent, pointing to the need for
improved segmentation.
4 Conclusion and Future Work
We proposed a novel partial parsing approach for
applications that require a fast syntactic analysis
of the input beyond shallow bracketing. The span-
limit parameter allows tuning the annotation of in-
ternal structure as appropriate for the application
domain, trading off annotation complexity against
inference time. These properties make hedge pars-
ing potentially very useful for incremental text or
speech processing, such as streaming text analysis
or simultaneous translation.
One interesting characteristic of these anno-
tations is that they allow for string segmenta-
tion prior to inference, provided that the segment
boundaries do not cross any hedge boundaries. We
found that baseline segmentation models did pro-
vide a significant speedup in parsing, but that cas-
cading errors remain a problem.
There are many directions of future work to
pursue here. First, the current results are all for
exhaustive CYK parsing, and we plan to per-
form a detailed investigation of the performance
of hedgebank parsing with prioritization and prun-
ing methods of the sort available in BUBS (Bo-
denstab et al, 2011). Further, this sort of annota-
tion seems well suited to incremental parsing with
beam search, which has been shown to achieve
high accuracies even for fully connected parsing
(Zhang and Clark, 2011). Improvements to the
transform (e.g., grouping items not in hedges un-
der non-terminals) and to the segmentation model
(e.g., increasing precision at the expense of recall)
could improve accuracy without greatly reducing
efficiency. Finally, we intend to perform an ex-
trinsic evaluation of this parsing in an on-line task
such as simultaneous translation.
Acknowledgments
This work was supported in part by NSF grant
#IIS-0964102. Any opinions, findings, conclu-
sions or recommendations expressed in this pub-
lication are those of the authors and do not neces-
sarily reflect the views of the NSF.
801
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and
Brian Roark. 2011. Beam-width prediction for ef-
ficient context-free parsing. In Proceedings of the
49th Annual Meeting ACL: HLT, pages 440?449.
Anne Br?uggemann-Klein and Derick Wood. 2004.
Balanced context-free grammars, hedge grammars
and pushdown caterpillar automata. In Extreme
Markup Languages.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1?8.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking
for speed and precision. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 201?205.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the Ninth International Workshop on
Parsing Technology (IWPT), pages 30?41.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of the 22nd national conference on Artificial intelli-
gence, pages 1663?1666.
Brian Roark and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 745?751.
Brian Roark, Kristy Hollingshead, and Nathan Boden-
stab. 2012. Finite-state chart constraints for reduced
complexity context-free parsing pipelines. Compu-
tational Linguistics, 38(4):719?753.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of Conference on Com-
putational Natural Language Learning (CoNLL),
pages 127?132.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 1185?1192.
Mahsa Yarmohammadi, Vivek K. Rangarajan Sridhar,
Srinivas Bangalore, and Baskaran Sankaran. 2013.
Incremental segmentation and decoding strategies
for simultaneous translation. In Proceedings of the
6th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 1032?1036.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
802
