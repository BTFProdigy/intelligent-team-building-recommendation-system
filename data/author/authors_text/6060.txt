Statistical Dependency Parsing of Turkish
Gu?ls?en Eryig?it
Department of Computer Engineering
Istanbul Technical University
Istanbul, 34469, Turkey
gulsen@cs.itu.edu.tr
Kemal Oflazer
Faculty of Engineering and Natural Sciences
Sabanci University
Istanbul, 34956, Turkey
oflazer@sabanciuniv.edu
Abstract
This paper presents results from the first
statistical dependency parser for Turkish.
Turkish is a free-constituent order lan-
guage with complex agglutinative inflec-
tional and derivational morphology and
presents interesting challenges for statisti-
cal parsing, as in general, dependency re-
lations are between ?portions? of words
? called inflectional groups. We have
explored statistical models that use dif-
ferent representational units for parsing.
We have used the Turkish Dependency
Treebank to train and test our parser
but have limited this initial exploration
to that subset of the treebank sentences
with only left-to-right non-crossing depen-
dency links. Our results indicate that the
best accuracy in terms of the dependency
relations between inflectional groups is
obtained when we use inflectional groups
as units in parsing, and when contexts
around the dependent are employed.
1 Introduction
The availability of treebanks of various sorts have
fostered the development of statistical parsers
trained with the structural data in these tree-
banks. With the emergence of the important role
of word-to-word relations in parsing (Charniak,
2000; Collins, 1996), dependency grammars have
gained a certain popularity; e.g., Yamada and Mat-
sumoto (2003) for English, Kudo and Matsumoto
(2000; 2002), Sekine et al (2000) for Japanese,
Chung and Rim (2004) for Korean, Nivre et al
(2004) for Swedish, Nivre and Nilsson (2005) for
Czech, among others.
Dependency grammars represent the structure
of the sentences by positing binary dependency
relations between words. For instance, Figure 1
Figure 1: Dependency Relations for a Turkish and
an English sentence
shows the dependency graph of a Turkish and
an English sentence where dependency labels are
shown annotating the arcs which extend from de-
pendents to heads.
Parsers employing CFG-backbones have been
found to be less effective for free-constituent-
order languages where constituents can easily
change their position in the sentence without
modifying the general meaning of the sentence.
Collins et al (1999) applied the parser of Collins
(1997) developed for English, to Czech, and found
that the performance was substantially lower when
compared to the results for English.
2 Turkish
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1994). At the syntax
level, the unmarked constituent order is SOV, but
constituent order may vary freely as demanded by
the discourse context. Essentially all constituent
orders are possible, especially at the main sen-
tence level, with very minimal formal constraints.
In written text however, the unmarked order is
dominant at both the main sentence and embedded
clause level.
Turkish morphotactics is quite complicated: a
given word form may involve multiple derivations
and the number of word forms one can generate
from a nominal or verbal root is theoretically in-
finite. Derivations in Turkish are very produc-
tive, and the syntactic relations that a word is in-
89
volved in as a dependent or head element, are de-
termined by the inflectional properties of the one
or more (possibly intermediate) derived forms. In
this work, we assume that a Turkish word is rep-
resented as a sequence of inflectional groups (IGs
hereafter), separated by ?DBs, denoting derivation
boundaries, in the following general form:
root+IG1 + ?DB+IG2 + ?DB+? ? ? + ?DB+IGn.
Here each IGi denotes relevant inflectional fea-
tures including the part-of-speech for the root and
for any of the derived forms. For instance, the de-
rived modifier sag?lamlas?t?rd?g??m?zdaki1
would be represented as:2
sag?lam(strong)+Adj
+?DB+Verb+Become
+?DB+Verb+Caus+Pos
+?DB+Noun+PastPart+A3sg+P3sg+Loc
+?DB+Adj+Rel
The five IGs in this are the feature sequences sep-
arated by the ?DB marker. The first IG shows the
part-of-speech for the root which is its only inflec-
tional feature. The second IG indicates a deriva-
tion into a verb whose semantics is ?to become?
the preceding adjective. The third IG indicates
that a causative verb with positive polarity is de-
rived from the previous verb. The fourth IG in-
dicates the derivation of a nominal form, a past
participle, with +Noun as the part-of-speech and
+PastPart, as the minor part-of-speech, with
some additional inflectional features. Finally, the
fifth IG indicates a derivation into a relativizer ad-
jective.
A sentence would then be represented as a se-
quence of the IGs making up the words. When a
word is considered as a sequence of IGs, linguis-
tically, the last IG of a word determines its role
as a dependent, so, syntactic relation links only
emanate from the last IG of a (dependent) word,
and land on one of the IGs of a (head) word on
the right (with minor exceptions), as exemplified
in Figure 2. And again with minor exceptions, the
dependency links between the IGs, when drawn
above the IG sequence, do not cross.3 Figure 3
from Oflazer (2003) shows a dependency tree for
a Turkish sentence laid on top of the words seg-
mented along IG boundaries.
With this view in mind, the dependency rela-
tions that are to be extracted by a parser should be
relations between certain inflectional groups and
1Literally, ?(the thing existing) at the time we caused
(something) to become strong?.
2The morphological features other than the obvious part-
of-speech features are: +Become: become verb, +Caus:
causative verb, +PastPart: Derived past participle,
+P3sg: 3sg possessive agreement, +A3sg: 3sg number-
person agreement, +Loc: Locative case, +Pos: Positive Po-
larity, +Rel: Relativizing Modifier.
3Only 2.5% of the dependencies in the Turkish treebank
(Oflazer et al, 2003) actually cross another dependency link.
Figure 2: Dependency Links and IGs
not orthographic words. Since only the word-
final inflectional groups have out-going depen-
dency links to a head, there will be IGs which do
not have any outgoing links (e.g., the first IG of the
word bu?yu?mesi in Figure 3). We assume that such
IGs are implicitly linked to the next IG, but nei-
ther represent nor extract such relationships with
the parser, as it is the task of the morphological
analyzer to extract those. Thus the parsing mod-
els that we will present in subsequent sections all
aim to extract these surface relations between the
relevant IGs, and in line with this, we will employ
performance measures based on IGs and their re-
lationships, and not on orthographic words.
We use a model of sentence structure as de-
picted in Figure 4. In this figure, the top part repre-
sents the words in a sentence. After morphological
analysis and morphological disambiguation, each
word is represented with (the sequence of) its in-
flectional groups, shown in the middle of the fig-
ure. The inflectional groups are then reindexed
so that they are the ?units? for the purposes of
parsing. The inflectional groups marked with ?
are those from which a dependency link will em-
anate from, to a head-word to the right. Please
note that the number of such marked inflectional
groups is the same as the number of words in the
sentence, and all of such IGs, (except one corre-
sponding to the distinguished head of the sentence
which will not have any links), will have outgoing
dependency links.
In the rest of this paper, we first give a very brief
overview a general model of statistical depen-
dency parsing and then introduce three models for
dependency parsing of Turkish. We then present
our results for these models and for some addi-
tional experiments for the best performing model.
We then close with a discussion on the results,
analysis of the errors the parser makes, and con-
clusions.
3 Parser
Statistical dependency parsers first compute the
probabilities of the unit-to-unit dependencies, and
then find the most probable dependency tree T ?
among the set of possible dependency trees. This
90
Bu eski ev+de +ki g?l+?n b?yle b?y? +me+si herkes+i ?ok etkile+di
Mod
Det
Mod
Subj
Mod
Subj
Obj
Mod
bu
+Det
eski
+Adj
ev
+Noun
+A3sg
+Pnon
+Loc
+Adj g?l
+Noun
+A3sg
+Pnon
+Gen
b?yle
+Adv
b?y?
+Verb
+Noun
+Inf
+A3sg
+P3sg
+Nom
herkes
+Pron
+A3pl
+Pnon
+Acc
?ok
+Adv
etkile
+Verb
+Past
+A3sg
This               old             house-at+that-is         rose's            such                     grow +ing              everyone        very      impressed
Such growing of the rose in this old house impressed everyone very much.
+?s indicate morpheme boundaries. The rounded rectangles show the words while the inflectional groups within
the words that have more than 1 IG are emphasized with the dashed rounded rectangles. The inflectional features
of each inflectional group as produced by the morphological analyzer are listed below.
Figure 3: Dependency links in an example Turkish sentence.
w1
  ##
IG1

IG2

? ? ? IG?g1

IG1 IG2 ? ? ? IG?g1
w2
  $$
IG1

IG2 ? ? ? IG?g2

IGg1+1 ? ? ? IG?g1+g2
. . .
. . .
wn
  ##
IG1 IG2 ? ? ? IG?gn

? ? ? IG??n
?i =
Pi
k=1 gk
Figure 4: Sentence Structure
can be formulated as
T ? = argmax
T
P (T, S)
= argmax
T
n?1
?
i=1
P (dep (wi, wH(i)) |S)(1)
where in our case S is a sequence of units (words,
IGs) and T , ranges over possible dependency
trees consisting of left-to-right dependency links
dep (wi, wH(i)) with wH(i) denoting the head unit
to which the dependent unit, wi, is linked to.
The distance between the dependent units plays
an important role in the computation of the depen-
dency probabilities. Collins (1996) employs this
distance ?i,H(i) in the computation of word-to-
word dependency probabilities
P (dep (wi, wH(i)) |S) ? (2)
P (link(wi, wH(i)) |?i,H(i))
suggesting that distance is a crucial variable when
deciding whether two words are related, along
with other features such as intervening punctua-
tion. Chung and Rim (2004) propose a different
method and introduce a new probability factor that
takes into account the distance between the depen-
dent and the head. The model in equation 3 takes
into account the contexts that the dependent and
head reside in and the distance between the head
and the dependent.
P (dep (wi, wH(i)) |S) ? (3)
P (link(wi, wH(i))) |?i ?H(i)) ?
P (wi links to some head
H(i) ? i away|?i)
Here ?i represents the context around the depen-
dent wi and ?H(i), represents the context around
the head word. P (dep (wi, wH(i)) |S) is the prob-
ability of the directed dependency relation be-
tween wi and wH(i) in the current sentence, while
P (link(wi, wH(i)) |?i ?H(i)) is the probability of
seeing a similar dependency (with wi as the depen-
dent, wH(i) as the head in a similar context) in the
training treebank.
For the parsing models that will be described
below, the relevant statistical parameters needed
have been estimated from the Turkish treebank
(Oflazer et al, 2003). Since this treebank is rel-
atively smaller than the available treebanks for
other languages (e.g., Penn Treebank), we have
91
opted to model the bigram linkage probabilities
in an unlexicalized manner (that is, by just taking
certain morphosyntactic properties into account),
to avoid, to the extent possible, the data sparseness
problem which is especially acute for Turkish. We
have also been encouraged by the success of the
unlexicalized parsers reported recently (Klein and
Manning, 2003; Chung and Rim, 2004).
For parsing, we use a version of the Backward
Beam Search Algorithm (Sekine et al, 2000) de-
veloped for Japanese dependency analysis adapted
to our representations of the morphological struc-
ture of the words. This algorithm parses a sentence
by starting from the end and analyzing it towards
the beginning. Bymaking the projectivity assump-
tion that the relations do not cross, this algorithm
considerably facilitates the analysis.
4 Details of the Parsing Models
In this section we detail three models that we have
experimented with for Turkish. All three models
are unlexicalized and differ either in the units used
for parsing or in the way contexts modeled. In
all three models, we use the probability model in
Equation 3.
4.1 Simplifying IG Tags
Our morphological analyzer produces a rather rich
representation with a multitude of morphosyntac-
tic and morphosemantic features encoded in the
words. However, not all of these features are nec-
essarily relevant in all the tasks that these analyses
can be used in. Further, different subsets of these
features may be relevant depending on the func-
tion of a word. In the models discussed below, we
use a reduced representation of the IGs to ?unlex-
icalize? the words:
1. For nominal IGs,4 we use two different tags
depending on whether the IG is used as a de-
pendent or as a head during (different stages
of ) parsing:
? If the IG is used as a dependent, (and,
only word-final IGs can be dependents),
we represent that IG by a reduced tag
consisting of only the case marker, as
that essentially determines the syntactic
function of that IG as a dependent, and
only nominals have cases.
? If the IG is used as a head, then we use
only part-of-speech and the possessive
agreement marker in the reduced tag.
4These are nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives,
past and future participles.
2. For adjective IGs with present/past/future
participles minor part-of-speech, we use the
part-of-speech when they are used as depen-
dents and the part-of-speech plus the the pos-
sessive agreement marker when used as a
head.
3. For other IGs, we reduce the IG to just the
part-of-speech.
Such a reduced representation also helps alleviate
the sparse data problem as statistics from many
word forms with only the relevant features are
conflated.
We modeled the second probability term on the
right-hand side of Equation 3 (involving the dis-
tance between the dependent and the head unit) in
the following manner. First, we collected statis-
tics over the treebank sentences, and noted that,
if we count words as units, then 90% of depen-
dency links link to a word that is less than 3 words
away. Similarly, if we count distance in terms of
IGs, then 90% of dependency links link to an IG
that is less than 4 IGs away to the right. Thus we
selected a parameter k = 4 for Models 1 and 3 be-
low, where distance is measured in terms of words,
and k = 5 for Model 2 where distance is measured
in terms of IGs, as a threshold value at and beyond
which a dependency is considered ?distant?. Dur-
ing actual runs,
P (wi links to some head H(i) ? i away|?i)
was computed by interpolating
P1(wi links to some head H(i) ? i away|?i)
estimated from the training corpus, and
P2(wi links to some head H(i) ? i away)
the estimated probability for a length of a link
when no contexts are considered, again estimated
from the training corpus. When probabilities are
estimated from the training set, all distances larger
than k are assigned the same probability. If even
after interpolation, the probability is 0, then a very
small value is used. This is a modified version of
the backed-off smoothing used by Collins (1996)
to alleviate sparse data problems. A similar inter-
polation is used for the first component on the right
hand side of Equation 3 by removing the head and
the dependent contextual information all at once.
4.2 Model 1 ? ?Unlexicalized? Word-based
Model
In this model, we represent each word by a re-
duced representation of its last IG when used as a
dependent,5 and by concatenation of the reduced
5Remember that other IGs in a word, if any, do not have
any bearing on how this word links to its head word.
92
representation of its IGs when used as a head.
Since a word can be both a dependent and a head
word, the reduced representation to be used is dy-
namically determined during parsing.
Parsing then proceeds with words as units rep-
resented in this manner. Once the parser links
these units, we remap these links back to IGs to
recover the actual IG-to-IG dependencies. We al-
ready know that any outgoing link from a depen-
dent will emanate from the last IG of that word.
For the head word, we assume that the link lands
on the first IG of that word.6
For the contexts, we use the following scheme.
A contextual element on the left is treated as a de-
pendent and is modeled with its last IG, while a
contextual element on the right is represented as
if it were a head using all its IGs. We ignore any
overlaps between contexts in this and the subse-
quent models.
In Figure 5 we show in a table the sample sen-
tence in Figure 3, the morphological analysis for
each word and the reduced tags for representing
the units for the three models. For each model, we
list the tags when the unit is used as a head and
when it is used as a dependent. For model 1, we
use the tags in rows 3 and 4.
4.3 Model 2 - IG-based Model
In this model, we represent each IG with re-
duced representations in the manner above, but
do not concatenate them into a representation for
the word. So our ?units? for parsing are IGs.
The parser directly establishes IG-to-IG links from
word-final IGs to some IG to the right. The con-
texts that are used in this model are the IGs to
the left (starting with the last IG of the preceding
word) and the right of the dependent and the head
IG.
The units and the tags we use in this model are
in rows 5 and 6 in the table in Figure 5. Note
that the empty cells in row 4 corresponds to IGs
which can not be syntactic dependents as they are
not word-final.
4.4 Model 3 ? IG-based Model with
Word-final IG Contexts
This model is almost exactly like Model 2 above.
The two differences are that (i) for contexts we
only use just the word-final IGs to the left and the
right ignoring any non-word-final IGs in between
(except for the case that the context and the head
overlap, where we use the tag of the head IG in-
6This choice is based on the observation that in the tree-
bank, 85.6% of the dependency links land on the first (and
possibly the only) IG of the head word, while 14.4% of the
dependency links land on an IG other than the first one.
stead of the final IG); and (ii) the distance function
is computed in terms of words. The reason this
model is used is that it is the word final IGs that
determine the syntactic roles of the dependents.
5 Results
Since in this study we are limited to parsing sen-
tences with only left-to-right dependency links7
which do not cross each other, we eliminated the
sentences having such dependencies (even if they
contain a single one) and used a subset of 3398
such sentences in the Turkish Treebank. The gold
standard part-of-speech tags are used in the exper-
iments. The sentences in the corpus ranged be-
tween 2 words to 40 words with an average of
about 8 words;8 90% of the sentences had less
than or equal to 15 words. In terms of IGs, the
sentences comprised 2 to 55 IGs with an average
of 10 IGs per sentence; 90% of the sentences had
less than or equal to 15 IGs. We partitioned this
set into training and test sets in 10 different ways
to obtain results with 10-fold cross-validation.
We implemented three baseline parsers:
1. The first baseline parser links a word-final IG
to the first IG of the next word on the right.
2. The second baseline parser links a word-final
IG to the last IG of the next word on the
right.9
3. The third baseline parser is a deterministic
rule-based parser that links each word-final
IG to an IG on the right based on the approach
of Nivre (2003). The parser uses 23 unlexi-
calized linking rules and a heuristic that links
any non-punctuation word not linked by the
parser to the last IG of the last word as a de-
pendent.
Table 1 shows the results from our experiments
with these baseline parsers and parsers that are
based on the three models above. The three mod-
els have been experimented with different contexts
around both the dependent unit and the head. In
each row, columns 3 and 4 show the percentage of
IG?IG dependency relations correctly recovered
for all tokens, and just words excluding punctu-
ation from the statistics, while columns 5 and 6
show the percentage of test sentences for which
all dependency relations extracted agree with the
7In 95% of the treebank dependencies, the head is the
right of the dependent.
8This is quite normal; the equivalents of function words
in English are embedded as morphemes (not IGs) into these
words.
9Note that for head words with a single IG, the first two
baselines behave the same.
93
Figure 5: Tags used in the parsing models
relations in the treebank. Each entry presents the
average and the standard error of the results on the
test set, over the 10 iterations of the 10-fold cross-
validation. Our main goal is to improve the per-
centage of correctly determined IG-to-IG depen-
dency relations, shown in the fourth column of the
table. The best results in these experiments are ob-
tained with Model 3 using 1 unit on both sides of
the dependent. Although it is slightly better than
Model 2 with the same context size, the difference
between the means (0.4?0.2) for each 10 iterations
is statistically significant.
Since we have been using unlexicalized models,
we wanted to test out whether a smaller training
corpus would have a major impact for our current
models. Table 2 shows results for Model 3 with no
context and 1 unit on each side of the dependent,
obtained by using only a 1500 sentence subset of
the original treebank, again using 10-fold cross
validation. Remarkably the reduction in training
set size has a very small impact on the results.
Although all along, we have suggested that de-
termining word-to-word dependency relationships
is not the right approach for evaluating parser per-
formance for Turkish, we have nevertheless per-
formed word-to-word correctness evaluation so
that comparison with other word based approaches
can be made. In this evaluation, we assume that a
dependency link is correct if we correctly deter-
mine the head word (but not necessarily the cor-
rect IG). Table 3 shows the word based results for
the best cases of the models in Table 1.
We have also tested our parser with a pure word
model where both the dependent and the head are
represented by the concatenation of their IGs, that
is, by their full morphological analysis except the
root. The result for this case is given in the last row
of Table 3. This result is even lower than the rule-
based baseline.10 For this model, if we connect the
10Also lower than Model 1 with no context (79.1?1.1)
dependent to the first IG of the head as we did in
Model 1, the IG-IG accuracy excluding punctua-
tions becomes 69.9?3.1, which is also lower than
baseline 3 (70.5%).
6 Discussions
Our results indicate that all of our models perform
better than the 3 baseline parsers, even when no
contexts around the dependent and head units are
used. We get our best results with Model 3, where
IGs are used as units for parsing and contexts are
comprised of word final IGs. The highest accuracy
in terms of percent of correctly extracted IG-to-IG
relations excluding punctuations (73.5%) was ob-
tained when one word is used as context on both
sides of the the dependent.11 We also noted that
using a smaller treebank to train our models did
not result in a significant reduction in our accu-
racy indicating that the unlexicalized models are
quite effective, but this also may hint that a larger
treebank with unlexicalized modeling may not be
useful for improving link accuracy.
A detailed look at the results from the best per-
forming model shown in in Table 4,12 indicates
that, accuracy decrases with the increasing sen-
tence length. For longer sentences, we should em-
ploy more sophisticated models possibly including
lexicalization.
A further analysis of the actual errors made by
the best performing model indicates almost 40%
of the errors are ?attachment? problems: the de-
pendent IGs, especially verbal adjuncts and argu-
ments, link to the wrong IG but otherwise with the
same morphological features as the correct one ex-
cept for the root word. This indicates we may have
to model distance in a more sophisticated way and
11We should also note that early experiments using differ-
ent sets of morphological features that we intuitively thought
should be useful, gave rather low accuracy results.
12These results are significantly higher than the best base-
line (rule based) for all the sentence length categories.
94
Percentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Baseline 1 NA 59.9 ?0.3 63.9 ?0.7 21.4 ?0.6 24.0 ?0.7
Baseline 2 NA 58.3 ?0.2 62.2 ?0.8 20.1 ?0.0 22.6 ?0.6
Baseline 3 NA 69.6 ?0.2 70.5 ?0.8 31.7 ?0.7 36.6 ?0.8
Model 1 None 69.8 ?0.4 71.0 ?1.3 32.7 ?0.6 36.2 ?0.7
(k=4) Dl=1 69.9 ?0.4 71.1 ?1.2 32.9 ?0.5 36.4 ?0.6
Dl=1 Dr=1 71.3 ?0.4 72.5 ?1.2 33.4 ?0.8 36.7 ?0.8
Hl=1 Hr=1 64.7 ?0.4 65.5 ?1.3 25.4 ?0.6 28.7 ?0.8
Both 71.4 ?0.4 72.6 ?1.1 34.2 ?0.7 37.2 ?0.6
Model 2 None 70.5 ?0.3 71.9 ?1.0 32.1 ?0.9 36.3 ?0.9
(k=5) Dl=1 71.3 ?0.3 72.7 ?0.9 33.8 ?0.8 37.4 ?0.7
Dl=1 Dr=1 71.9 ?0.3 73.1 ?0.9 34.8 ?0.7 38.0 ?0.7
Hl=1 Hr=1 57.4 ?0.3 57.6 ?0.7 23.5 ?0.6 25.8 ?0.6
Both 70.9 ?0.3 72.2 ?0.9 34.2 ?0.8 37.2 ?0.9
Model 3 None 71.2 ?0.3 72.6 ?0.9 34.4 ?0.7 38.1 ?0.7
(k=4) Dl=1 71.2 ?0.4 72.6 ?1.1 34.5 ?0.7 38.3 ?0.6
Dl=1 Dr=1 72.3 ?0.3 73.5 ?1.0 35.5 ?0.9 38.7 ?0.9
Hl=1 Hr=1 55.2 ?0.3 55.1 ?0.7 22.0 ?0.6 24.1 ?0.6
Both 71.1 ?0.3 72.4 ?0.9 35.5 ?0.8 38.4 ?0.9
The Context column entries show the context around the dependent and the head unit. Dl=1 and Dr=1 indicate
the use of 1 unit left and the right of the dependent respectively. Hl=1 and Hr=1 indicate the use of 1 unit left and
the right of the head respectively. Both indicates both head and the dependent have 1 unit of context on both sides.
Table 1: Results from parsing with the baseline parsers and statistical parsers based on Models 1-3.
Percentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Model 3 None 71.0 ?0.6 72.2 ?1.5 34.4 ?1.0 38.1 ?1.1
(k=4, 1500 Sentences) Dl=1 Dr=1 71.6 ?0.4 72.6 ?1.1 35.1 ?1.3 38.4 ?1.5
Table 2: Results from using a smaller training corpus.
Percentage of Word-Word
Relations Correct
Parsing Model Context Words only
Baseline 1 NA 72.1 ?0.5
Baseline 2 NA 72.1 ?0.5
Baseline 3 NA 80.3 ?0.7
Model 1 (k=4) Both 80.8 ?0.9
Model 2 (k=5) Dl=1 Dr=1 81.0 ?0.7
Model 3 (k=4) Dl=1 Dr=1 81.2 ?1.0
Pure Word Model None 77.7 ?3.5
Table 3: Results from word-to-word correctness evaluation.
Sentence Length l (IGs) % Accuracy
1 < l ? 10 80.2 ?0.5
10 < l ? 20 70.1 ?0.4
20 < l ? 30 64.6 ?1.0
30 < l 62.7 ?1.3
Table 4: Accuracy over different length sentences.
95
perhaps use a limited lexicalization such as includ-
ing limited non-morphological information (e.g.,
verb valency) into the tags.
7 Conclusions
We have presented our results from statistical de-
pendency parsing of Turkish with statistical mod-
els trained from the sentences in the Turkish tree-
bank. The dependency relations are between
sub-lexical units that we call inflectional groups
(IGs) and the parser recovers dependency rela-
tions between these IGs. Due to the modest size
of the treebank available to us, we have used
unlexicalized statistical models, representing IGs
by reduced representations of their morphological
properties. For the purposes of this work we have
limited ourselves to sentences with all left-to-right
dependency links that do not cross each other.
We get our best results (73.5% IG-to-IG link ac-
curacy) using a model where IGs are used as units
for parsing and we use as contexts, word final IGs
of the words before and after the dependent.
Future work involves a more detailed under-
standing of the nature of the errors and see how
limited lexicalization can help, as well as investi-
gation of more sophisticated models such as SVM
or memory-based techniques for correctly identi-
fying dependencies.
8 Acknowledgement
This research was supported in part by a research
grant from TUBITAK (The Scientific and Techni-
cal Research Council of Turkey) and from Istanbul
Technical University.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In 1st Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Seattle, Washington.
Hoojung Chung and Hae-Chang Rim. 2004. Un-
lexicalized dependency parser for variable word or-
der languages based on local contextual pattern.
In Computational Linguistics and Intelligent Text
Processing (CICLing-2004), Seoul, Korea. Lecture
Notes in Computer Science 2945.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505?518, University of Maryland.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
the 34th AnnualMeeting of the Association for Com-
putational Linguistics, Santa Cruz, CA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan.
Taku Kudo and Yuji Matsumoto. 2000. Japanese
dependency analysis based on support vector ma-
chines. In Joint Sigdat Conference On Empirical
Methods In Natural Language Processing and Very
Large Corpora, Hong Kong.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Sixth Conference on Natural Language Learning,
Taipei, Taiwan.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 99?106,
Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In 8th Confer-
ence on Computational Natural Language Learning,
Boston, Massachusetts.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of 8th
International Workshop on Parsing Technologies,
pages 23?25, Nancy, France, April.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotatedCorpora. Kluwer Acad-
emic Publishers.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kemal Oflazer. 2003. Dependency parsing with an
extended finite-state approach. Computational Lin-
guistics, 29(4).
Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isa-
hara. 2000. Backward beam search algorithm for
dependency analysis of Japanese. In 17th Inter-
national Conference on Computational Linguistics,
pages 754 ? 760, Saarbru?cken, Germany.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In 8th International Workshop of Parsing
Technologies, Nancy, France.
96
Bootstrapping Morphological Analyzers 
by Combining Human Elicitation and 
Machine Learning 
Kemal Oflazer* 
Sabancl University 
Marjorie McShane* 
New Mexico State University 
Sergei Nirenburg* 
New Mexico State University 
This paper presents a semiautomatic technique for developing broad-coverage finite-state mor- 
phological analyzers for use in natural language processing applications. It consists of three 
components--elicitation f linguistic information from humans, a machine learning bootstrap- 
ping scheme, and a testing environment. The three components are applied iteratively until a 
threshold of output quality is attained. The initial application of this technique is for the mor- 
phology of low-density languages in the context of the Expedition project at NMS U Computing 
Research Laboratory. This elicit-build-test technique compiles lexical and inflectional information 
elicited from a human into a finite-state transducer lexicon and combines this with a sequence 
of morphographemic rewrite rules that is induced using transformation-based l arning from 
the elicited examples. The resulting morphological nalyzer is then tested against a test set, 
and any corrections are fed back into the learning procedure, which then builds an improved 
analyzer. 
1. Introduction 
The Expedition project at NMSU Computing Research Laboratory is devoted to the 
fast "ramp-up" of machine translation systems from less studied, so-called low-density 
languages, into English. One of the components hat must be acquired and built dur- 
ing this process is a morphological nalyzer for the source language. Since language 
informants are not expected or required to be well-versed in computational linguistics 
in general, or in recent approaches tobuilding morphological nalyzers (e.g., Kosken- 
niemi 1983; Antworth 1990; Karttunen, Kaplan, and Zaenen 1992; Karttunen 1994) and 
the operation of state-of-the-art finite-state tools (e.g., Karttunen 1993; Karttunen and 
Beesley 1992; Karttunen et al 1996; Mohri, Pereira, and Riley 1998; van Noord 1999; 
van Noord and Gerdemann 1999) in particular, the generation of the morphological 
analyzer component has to be accomplished semiautomatically. The informant will 
be guided through a knowledge licitation procedure using the elicitation component 
of Expedition, the Boas system. As this task is not easy, we expect hat the develop- 
ment of the morphological nalyzer will be an iterative process, whereby the human 
informant will revise and/or refine the information previously elicited based on the 
feedback from test runs of the nascent analyzer. 
* Faculty of Engineering and Natural Sciences, Orhanh, 81474 Tuzla, Istanbul, TURKEY 
t Computing Research Laboratory, Las Cruces, NM 88003 
Computational Linguistics Volume 27, Number 1 
The work reported in this paper describes the process of building and refining mor- 
phological analyzers using data elicited from human informants and machine learning. 
The main use of machine learning in our current approach is in the automatic learning 
of formal rewrite or replace rules for morphographemic changes derived from the ex- 
amples provided by the informant. The subtask of accounting for morphographemic 
changes is perhaps one of the more complicated aspects of building an analyzer; by 
automating it, we expect o improve productivity. 
After a review of related work, we very briefly describe the Boas project, of which 
the current work is a part. Subsequent sections describe the details of the approach, 
the architecture of the morphological analyzer, the elicited descriptive data, and the 
computational processes performed on this data, including segmentation and the in- 
duction of morphographemic rules. We then provide a detailed example of applying 
this approach to developing a morphological nalyzer for Polish. Finally, we provide 
some conclusions and ideas for future work. 
2. Related Work 
Machine learning techniques are widely employed in many aspects of language pro- 
cessing. The availability of large, annotated corpora has fueled a significant amount of 
work in the application of machine learning techniques to language processing prob- 
lems, such as part-of-speech tagging, grammar induction, and sense disambiguation, 
as witnessed by recent workshops and journal issues dedicated to this topic. 1 The cur- 
rent work attempts to contribute to this literature by describing a human-supervised 
machine learning approach to the induction of morphological analyzers--a problem 
that, surprisingly, has received little attention. 
There have been a number of studies on inducing morphographemic rules from a 
list of inflected words and a root word list. Johnson (1984) presents a scheme for in- 
ducing phonological rules from surface data, mainly in the context of studying certain 
aspects of language acquisition. The premise is that languages have a finite number of 
alternations to be handled by morphographemic rules and a fixed number of contexts 
in which they appear; so if there is enough data, phonological rewrite rules can be 
generated to account for the data. Rules are ordered by some notion of "surfaciness", 
and at each stage the most surfacy rule--the rule with the most transparent context-- 
is selected. Golding and Thompson (1985) describe an approach for inducing rules of 
English word formation from a corpus of root forms and the corresponding inflected 
forms. The procedure described there generates a sequence of transformation rules, 2 
each specifying how to perform a particular inflection. 
More recently, Theron and Cloete (1997) have presented a scheme for obtaining 
two-level morphology rules from a set of aligned segmented and surface pairs. They 
use the notion of string edit sequences, assuming that only insertions and deletions 
are applied to a root form to get the inflected form. They determine the root form 
associated with an inflected form (and consequently the suffixes and prefixes) by ex- 
haustively matching the inflected form against all root words. The motivation is that 
"real" suffixes will appear frequently in the corpus of inflected forms. Once common 
suffixes and prefixes are identified, the segmentation for an inflected word can be 
determined by choosing the segmentation with the most frequently occurring affix 
segments; the remainder is then considered the root. While this procedure seems to 
1 For instance, the CoNLL (Computational Natural Language Learning) Workshops, recent special issues 
of Machine Learning Journal (Vol. 34 Issue 1/3, Feb. 1999) and AIMagazine (Vol. 18, No. 4, 1997). 
2 Not in the sense in which it is used in transformation-based learning (Brill 1995). 
60 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
be reasonable for a small root word list, the potential for "noisy" or incorrect align- 
ments is quite high when the corpus of inflected forms is large and the procedure 
is not given any prior knowledge of possible segmentations. As a result, automati- 
cally selecting the "correct" segmentation becomes nontrivial. An additional compli- 
cation is that allomorphs how up as distinct affixes and their counts in segmentations 
are not accumulated, which might lead to actual segmentations being missed due to 
fragmentation. The rules are not induced via a learning scheme: aligned pairs are 
compressed into a special data structure and traversals over this data structure gener- 
ate morphographemic rules. Theron and Cloete have experimented with pluralization 
in Afrikaans, and the resulting system has shown about 94% accuracy on unseen 
words. 
Goldsmith (1998) has used an unsupervised learning method based on the mini- 
mum description length principle to learn the "morphology" of a number of languages. 
What is learned is a set of root words and affixes, and common inflectional-pattern 
classes. The system requires just a corpus of words in a language. In the absence of 
any root word list to use as a scaffolding, the shortest forms that appear frequently 
are assumed to be roots, and observed surface forms are then either generated by the 
concatenative affixation of suffixes or by rewrite rules. 3 Since the system has no notion 
of what the roots and their part-of-speech values really are, and what morphological 
information is encoded by the affixes, this information eeds to be retrofitted manually 
by a human, who has to weed through a large number of noisy rules. We feel that this 
approach, while quite novel, can be used to build real-world morphological nalyzers 
only after substantial modifications are made. 
3. The BOAS Project 
Boas (Nirenburg 1998; Nirenburg and Raskin 1998) is a semiautomatic knowledge 
elicitation system that guides a team of two people (a language informant and a 
programmer) through the process of developing the static knowledge sources required 
to produce a moderate-quality, broad-coverage MT system from any "low-density" 
language into English. Boas contains knowledge about human language phenomena 
and various realizations of these phenomena in a number of specific languages, as 
well as extensive pedagogical support, making the system a kind of "linguist in a 
box," intended to help nonprofessional users with the task. In the spirit of the goal- 
driven, "demand-side" approach to computational pplications of language processing 
(Nirenburg and Raskin 1999), the process of acquiring this knowledge has been split 
into two steps: (i) acquiring the descriptive, declarative knowledge about a language 
and (ii) deriving operational knowledge (content for the processing engines) from this 
descriptive knowledge. 
An important goal that we strive to achieve regarding these descriptive and op- 
erational pieces of information, be they elicited from human informants or acquired 
via machine learning, is that they be transparent, human-readable, and, where neces- 
sary, human-maintainable and human-extendable, contrary to the opaque and unin- 
terpretable representations acquired by various statistical learning paradigms. 
Before proceeding any further, we would also like to make explicit the aims and 
limitations of our approach. Our main goal is to significantly expedite the develop- 
ment of a morphological nalyzer. It is clear that for inflectional languages where each 
3 Some of these rules may not make sense, but they are necessary to account for the data: for instance, a 
rule like insert a word f inal y after the root " eas " is used to generate easy. 
61 
Computational Linguistics Volume 27, Number 1 
root word can be associated with a finite number of word forms, one can, with a lot of 
work, generate a list of word forms with associated morphological features encoded, 
then use this as a lookup table to analyze word forms in input texts. Since this pro- 
cess is time consuming, expensive, and error-prone, it is something we would like to 
avoid. We prefer to capture general morphophonological and morphographemic phe- 
nomena using sample paradigms as the basis of lexical abstractions. This reduces the 
acquisition process to assigning citation forms to one of the established paradigms; 
the automatic generation process described below does the rest of the work. 4 This 
process is still imperfect, as we expect human informants to err in making their 
paradigm abstractions and to overlook details and exceptions. So, the whole pro- 
cess is an iterative one, with convergence to a wide-coverage analyzer coming slowly 
at the beginning (where morphological phenomena nd lexicon abstractions are be- 
ing defined and tested), but significantly speeding up once wholesale lexical acqui- 
sition starts. Since the generation of the operational content (data files to be used 
by the morphological analyzer engine) from the elicited descriptions i  expected to 
take only a few minutes, feedback on operational performance can be provided very 
quickly. 
Human languages have many diverse morphological phenomena nd it is not 
our intent at this point to have a universal architecture that can accommodate any 
and all phenomena. Rather, we propose an extensible approach that can accommo- 
date additional functionality in future incarnations of Boas. We also intend to limit 
morphological processing to single tokens and to deal with multitoken phenomena, 
such as partial or full word reduplications, with additional machinery that we do not 
discuss here. 
4. The Elicit-Build-Test Loop 
In this paper we concentrate on operational content in the context of building a mor- 
phological analyzer. To determine this content, we integrate the information provided 
by the informant with automatically derived information. The whole process is an 
iterative one, as illustrated in Figure 1: the elicited information is transformed into 
the operational data required by the generic morphological analyzer engine and the 
resulting analyzer is then tested on a test corpus, s'6 Any discrepancies between the 
output of the analyzer and the test corpus are then analyzed and potential sources 
of errors are given as feedback to the elicitation process. Currently, this feedback is 
limited to identifying problems in handling morphographemic processes (such as for 
instance the change of word-final -y to -i when the suffix -est is added). 
The box in Figure 1 labeled Morphological Analyzer Generation is the main com- 
ponent, which takes in the elicited information and generates a series of regular ex- 
pressions for describing the morphological lexicon and morphographemic rules. The 
morphographemic rules describing changes in spelling as a result of affixation opera- 
tions are induced from the examples provided by using transformation-based l arning 
(Brill 1995; Satta and Henderson 1997). The result is an ordered set of contextual re- 
place or rewrite rules, much like those used in phonology. 
4 We use the term citation form to refer to the word form that is used to look up a given inflected form 
in a dictionary. It may  be the root or stem form that affixation is applied to, or it may  have additional 
morphological markers to indicate its citation form status. 
5 We currently use XRCE finite-state tools as our target environment (Karttunen et al 1996). 
6 The test corpus is either elicited from the human informant or compiled from on-line resources for the 
language in question. 
62 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
Corpus  1 
~ompilatiol~ 
Start 
I Human Elicitation 1~ 
Process 
I Description f Morphology 
(paradigms~ examples~ exceptions~ etc.) 
i Test I Err?rs, I Corpus Omissions 
Figure 1 
I Morphological Analyzer 1 
Generation 
\[ Content for Morphological Analyzer Engine \] 
(lexicons~ morphographemic rules) 
\[" Comparison ~ ~l 
~'| with Test Corpus \[ 
\[,(MA Engine, Test Engine)J "\[ 
The elicit-build-test paradigm for bootstrapping a morphological nalyzer. 
4.1 Morphological Analyzer Architecture 
We adopt the general approach advocated by Karttunen (1994) and build the morpho- 
logical analyzer as the combination of several finite-state transducers, ome of which 
are constructed irectly from the elicited information, and others of which are con- 
structed from the output of the machine learning stage. Since the combination of the 
transducers i computed at compile-time, there are no run-time overheads. The ba- 
sic architecture of the morphological analyzer is depicted in Figure 2. The analyzer 
consists of the union of transducers, each of which implements the morphological 
analysis process for one paradigm. Each transducer is the composition of a number of 
components. These components (from bottom to top) are described below: 
. 
. 
The bottom component is an ordered sequence of morphographemic 
rules that are learned via transformation-based l arning from the sample 
inflectional paradigms provided by the human informant. These rules are 
then composed into one finite-state transducer (Kaplan and Kay 1994). 
The citation form and affix lexicon contains the citation forms and the 
affixes. We currently assume that all affixation is concatenative and that 
the lexicon is described by a regular expression of the sort 
\[ Pref ixes  \]* \[ Citat ionForms \] \[ Suf f ixes \].7 
7 We currently assume that we have at most one prefix and at most one suffix, but this is not a 
fundamental limitation. The elicitation of morphotactics foran agglutinating language like Turkish or 
Finnish requires a significantly more sophisticated licitation machinery. 
63 
Computational Linguistics Volume 27, Number 1 
Lemma+Morphological Features (e.g., happy+Adj+Super) 
. - - - ~ ZZ-_Z,Z,Z-,--22-7.--7.2--ZZZZZZZZZ--Z'2~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7~2-=2=2-=2- :22: :2=:=:==-==:==:=:2=:=: :~- - .  
i J Feature Constraints Feature Constraints o ', 
I Surfacy-to-Feature 
Mapping 1 
O 
I Lexical & Surfacy Constraints I 
U 000 U 
O 
Surfacy-to-FeatureMapping ) 
,, 0 
? 1' Lexical & Surfacy Constraints 
O 
I Morpheme-to-Surf icy-Feature 1 r Morpheme-to-Surfacy-Feature 
Mapping J Mapping J 
O o 
nl 1 Citation Form and Affix Lexico itation Form and Affix Lexicon 
O O 
\[,I Morphographemic Rules lJ !\[ Morphographemic Rules 11 
Paradigm 1 Paradigm n 
" ' '4  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  I " *  
Surface Form (e.g., happiest) 
Figure 2 
General architecture of the morphological nalyzer. 
. 
. 
. 
The morpheme to surfacy feature mapping essentially maps 
morphemes to feature names but retains some encoding of the surface 
morpheme. Thus, al lomorphs that encode the same feature would be 
mapped to different surfacy features. 
The lexical and surfacy constraints pecify any conditions to constrain 
the possibly overgenerating morphotactics of the citation form and 
morpheme lexicons. These constraints can be encoded using the citation 
forms and the surfacy features generated by the previous mapping. The 
use of surfacy features also enables reference to zero morphemes, which 
otherwise could not be used. For instance, if in some paradigm a certain 
prefix does not co-occur with a certain suffix, or always occurs with 
some other suffix, or if a certain citation form in that paradigm has 
exceptional behavior with respect o one or more of the affixes, or if the 
affixal aUomorph that goes with a certain citation form depends on the 
properties of the citation form, these are encoded at this level as 
finite-state constraints. 
The surfacy feature to feature mapping module maps the surfacy 
representation f the affixes to symbolic feature names; as a result, no 
surface information remains except for the citation form. Thus, for 
instance, al lomorphs that encode the same feature and map to different 
surfacy features now map to the same feature symbol. 
64 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
. The feature constraints specify constraints among the symbolic features. 
They are different means of constraining morphotactics than the one 
provided by lexical and surfacy constraints. At this level, one refers to 
and constrains symbolic morphosyntactic features as opposed to surfacy 
features. This may provide a more natural or convenient abstraction, 
especially for languages with long-distance morphotactic constraints. 
These six finite-state transducers are composed to yield a transducer for the paradigm. 
The union of the transducers for all paradigms produces one (possibly large) trans- 
ducer for morphological nalysis, where surface strings applied at the lower end pro- 
duce all possible analyses at the upper end. 
4.2 Information Elicited from Human Informants 
The Boas environment guides the language informant through a series of questions 
leading up to paradigm delineation. The informant indicates the parameters for which 
a given part of speech inflects (e.g., Case, Number), the relevant values for those pa- 
rameters (e.g., Nominative, Accusative; Singular, Plural), and the licit combinations 
of parameter values (e.g., Nominative Singular, Nominative Plural). The informant 
then posits any number of paradigms, whose members are expected to show sim- 
ilar patterns of inflection. It is assumed that all citation forms that belong to the 
same paradigm take essentially the same set of inflectional affixes (perhaps ubject 
to morphophonological v riations). It is expected that the citation forms and/or the 
affixes may undergo systematic or idiosyncratic morphographemic changes. It is also 
assumed that certain citation forms in a given paradigm may behave in some excep- 
tional way (for instance, contrary to all other citation forms, a given citation form 
may not have one of the inflected forms.) A paradigm description provides the full 
inflectional pattern for one characteristic or distinguished citation form and additional 
examples for any other citation forms whose inflectional forms undergo nonstandard 
morphographemic changes. If necessary, any lexical and feature constraints can be 
encoded. Currently the provisions we have for such constraints are limited to writing 
regular expressions (albeit at a much higher level than standard regular expressions); 
however, capturing such constraints using a more natural language (e.g., Ranta 1998) 
can be incorporated into future versions. 
4.3 Elicited Descriptive Data 
Figure 3 presents the encoding of the information elicited for one paradigm of a Polish 
morphological nalyzer, which will be covered in detail ater, s
The data elicited using the user interface component of Boas is converted into 
a description text file with various components delineated by SGML-like tags. The 
components in the description are as follows: 
? The <LANGUAGE-DESCRIPTION... >component lists information about he 
language and specifies its vowels and consonants, and other orthographic 
symbols that do not fall into those two groups. 
? A paradigm description starts with the tag <PARADIGM NAME=... >, which 
lists the name of the paradigm, its part-of-speech ategory, and any 
8 Our actual system works using unicode character representation. But unicode input and output are not 
yet supported in the XRCE xfst tool, hence we employ an ASCII external representation f r the unicode 
characters during off-line testing. Inthe following examples, however, we have opted to represent the 
actual characters a they should appear on screen. 
65 
Computational Linguistics Volume 27, Number 1 
<LANGUAGE-DESCRIPTION TYPE = "morphology" 
NAME = "Polish" 
ALPHABET = "a~bcdde~fghijklhnnfio6pqrs~tuvwxyz~z" 
VOWELS = "age@io6uy" 
CONSONANTS= "bcddfghjkl~mnfipqrs~tvwxz~z" 
OTHER = ""> 
<PARADIGM NAME="MasclnUStart"  POS = "Noun" FEATURES="Mascul ine"> 
<PRIMARY-EXAMPLE> 
<INF-GROUP> 
<PRIMARY-CIT-FORM FORM = "telefon"> 
<INF-FORM FORM = "telefon" FEATURE = "Nom. Sg."> 
<INF-FORM FORM = "tslefon" FEATURE ="Acc .  Sg."> 
<INF-FORM FORM = "telefonach" FEATURE = "Loc.Pl ."> 
<INF-FORM FORM = "telefonami" FEATURE = "Instr .P l ."> 
</ INF-GROUP> 
</PRIMARY-EXAMPLE > 
<EXAMPLE> 
<INF-GROUP> 
</INF-GROUP> 
</EXAMPLE> 
<LEXICON> 
</LEXICON> 
</PARADIGM> 
<CIT-FORM FORM = "akcent"> 
<INF-FORM FORM = "akcent" FEATURE = "Nom. Sg."> 
<INF-FORM FORM = "akcencie" FEATURE = "Loc.Sg."> 
<CIT-FORM FORM = "stron"> 
<CIT-FORM FORM = "klub"> 
<CIT-FORM FORM = "sklep"> 
</LANGUAGE-DESCRIPTION> 
Figure 3 
Sample paradigm description generated by Boas elicitation. 
additional morphosyntactic features that are common to all citation 
forms in this paradigm. In the example in Figure 3, the paradigm is for 
masculine nouns. Everything up to the </PARADIGM> tag is part of the 
descriptive data for the paradigm. This descriptive data consists of a 
primary example, a series of zero or more additional examples, and the 
lexicon. 
The primary example is given between the <PRIMARY-EXAMPLE> and 
</PRIMARY-EXAMPLE> tags. The description is given as a sequence of one 
or more inflection groups between <INF-GROUP> and </INF-GROUP> tags. 
In some instances, a given lexical item can use different citation forms in 
different inflectional forms. For example, one citation form might be 
used in the present tense and another in the past tense; or one might be 
66 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
used with multisyllable affixes and another with single-syllable affixes. 
Thus, a given lexical item can have multiple citation forms, each of 
which gets associated with a mutually exclusive subset of inflectional 
forms. All the citation forms for a given lexical item, plus all its 
inflectional forms, are represented in an inflection group. If the 
association of citation forms with inflectional forms is predictable (as 
indicated by the language informant), the subsets of inflectional forms 
are processed separately; if not, we assume that all citation forms can be 
used in all inflectional forms and hence overgenerate. Manual constraints 
can later be added, if necessary, to constrain this overgeneration. 
Additional examples are provided between <EXAMPLE> and </EXAMPLE> 
tags. Examples contain ew citation forms plus any inflectional forms 
that are not predictable based on the primary example. Each example is 
considered an inflectional group and is enclosed within the 
corresponding tags. 
The citation forms given in the primary example and any additional 
examples are considered to be a part of the citation form lexicon of the 
paradigm definition. Any additional citation forms in this paradigm are 
listed between the <LEXICON> and </LEXICON> tags. 
5. Generating the Morphological Analyzer 
The morphological nalyzer is a finite-state transducer that is actually the union of 
the transducers for each paradigm definition in the description provided. Thus, the 
elicited data is processed one paradigm at a time. For each paradigm we proceed as 
follows: 
. 
. 
. 
The elicited primary citation form and associated inflected forms are 
processed to find the "best" segmentation f the forms into stem and 
affixes. 9Although we allow for inflectional forms to have both a prefix 
and a suffix (one of each), we expect only suffixation to be employed by 
the inflecting languages with which we are dealing (Sproat 1992). 
Once the affixes are determined, we segment the inflected forms for the 
primary example and any additional examples provided, and pair them 
with the corresponding surface forms. The segmented forms are now 
based on the citation form plus the affixes (not the stem). The reason is 
that we expect he morphological nalyzer to generate the citation form 
for further access to lexical databases to be used in the applications. The 
resulting segmented form-surface form pairs make up the example base 
of the paradigm. 
The citation forms given in the primary example, in additional examples, 
and explicitly in the lexicon definition of the elicited data, along with the 
mapping from suffix strings to the corresponding morphosyntactic 
features, are compiled (by our morphological nalyzer generating 
system) into suitable regular expressions (expressed using the regular 
9 The stern is considered to be that part of the citation form onto which affixes are attached, and in our 
context i has no function except for determining the affix strings. 
67 
Computational Linguistics Volume 27, Number 1 
. 
. 
expression language of the XRCE finite-state tools \[Karttunen tal. 
1996\]). l? 
The example base of the paradigm generated in step 2 is then used by a 
learning algorithm to generate a sequence of morphographemic rules 
(Kaplan and Kay 1994) that handle the morphographemic phenomena. 
The regular expressions for the lexicon in step 3 and the regular 
expressions for the morphographemic rules induced in step 4 are then 
compiled into finite-state transducers and combined by composition to 
generate the finite-state morphological nalyzer for the paradigm. 
The resulting finite-state transducers for each paradigm are then unioned to give 
the transducer for the complete set of paradigms. 
5.1 Determining Segmentation and Affixes 
The suffixes and prefixes in a paradigm are determined by segmenting the inflected 
forms provided for the primary example. This process is complicated by the fact that 
the citation form may not correspond to the stem--it may contain a morphological in- 
dication that it is the citation form. Furthermore, since the language informant provides 
only a small number of examples, tatistically motivated approaches like the one sug- 
gested by Theron and Cleoete (1997) are not applicable. We have experimented with a 
number of approaches and have found that the following approach works quite well. 
Using the notion of description length (Rissanen 1989), we try to find a stem and 
a set of affixes that account for all the inflected forms of the primary example. Let 
C = (cl, c2 . . . . .  ccl be the character string for the citation form in the primary example 
(ci are symbols in the alphabet of the language). Let Sk = (cl, c2 . . . . .  Ckl, 1 < k <_ c 
be a (string) prefix of C length k. We assume that the stem onto which morphological 
affixes are attached is Sk for some k. 11 The set of inflectional forms given in the primary 
J J ,fill (f//are alphabet example are {F1, F2,..., El}, with each Fj = ~f~,f~ . . . .  symbols in the 
of the language and lj is the length of the jth form). The function ed(v,w) (ed for 
edit distance), where v and w are strings, measures the minimum number of symbol 
insertions and deletions (but not substitutions) that can be applied to v to obtain w 
(Damerau 1964). 12 We define 
j=f 
d(Sk) = k + ~_~ ed(Sk, Fj) 
j=l  
as a measure of the information eeded to account for all the inflected forms. The first 
term above, k, is the length of the stem. The second term, the summation, measures 
how many symbols must be inserted and deleted to obtain the inflected form. The 
Sk with the minimum d(Sk) is then chosen as the stem S. Creating segmentations 
based on stem S proceeds as follows: To determine the affixes in each inflected form 
Fj = ~f~,f~ . . . . .  f/i/, we compute the projection of the stem Pj = ~f~ . . . .  ,f/el in Fj, as that 
10 Note that other finite state tools could also be used (e.g., Mohri, Pereira and Riley 1998; van Noord 
1999). 
11 The stem can also be an arbitrary substring of C, not just some initial prefix. Our approach can 
certainly extend to that. 
12 The function ed(...) assumes that vowels only align with other vowels or are elided, and consonants 
only align with consonants or are elided. 
68 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
substring of Fj whose alignment with S provides the minimum edit distance, that is, 
P j = argmin ed ( S, ~f~ . . . . . .  /d,>) 
(f~ ..... ,fd,>,l<_b'<e'<lj 
Then we select he substring ~f~ . . . . .  f~-l> of Fj (if it exists) as the prefix and ... ,J~} q~+l" 
(if it exists) as the suffix. If there are multiple substrings of Fj that give the same 
(minimum) edit distance when aligned with S, we prefer the longer substring. We 
then create 
f~_l + C + 9<~+1 . . . . .  . . . . .  
as an aligned segmented form-surface form pair and add it to the example base that 
we will use in the learning stage. Note that we now use the citation form C, and not 
the stem S, as a part of the segmented form. 
Thus, at the end of the process we generate pairs of inflected forms and their 
corresponding segmented forms to be used in the derivation of the morphographemic 
rules. These pairs come from both the inflected forms given in the primary example 
and from any additional examples given. 
For example, suppose we have the following primary example: 
<PRIMARY-EXAMPLE> 
<INF-GROUP> 
<PRIMARY-C IT -FORM FORM = "strona"> 
<INF-FORM FORM = "strona" FEATURE = "Nom. Sg."> 
<INF-FORM FORM = "strong" FEATURE = "Acc. Sg."> 
<INF-FORM FORM = "strony" FEATURE = "Gen. Sg."> 
<INF-FORM FORM = "stronie" FEATURE = "Dat .Sg."> 
<INF-FORM FORM = "stronie" FEATURE ="Loc .  Sg."> 
<INF-FORM FORM = "strong" FEATURE =" Ins t r .  Sg."> 
<INF-FORM FORM = "strony" FEATURE = "Nom. P l . "> 
<INF-FORM FORM = "strony" FEATURE = "Acc .P I . "> 
<INF-FORM FORM = "stron" FEATURE = "Gen. P l . "> 
<INF-FORM FORM = "stronom" FEATURE = "Dat. P l . "> 
<INF-FORM FORM = "stronach" FEATURE = "Loc.P l . "> 
<INF-FORM FORM = "stronami"  FEATURE = " Inst r .P l . "> 
</ INF-GROUP> 
</PR IMARY-EXAMPLE> 
For this example, stems Sk: s, st, str, stro, stron, strona, are considered. Table 1 
tabulates d(Sk) considering all the unique inflected forms above. It can be seen that 
the value of d(Ss) is minimum for $5 = S = stron. We then determine suffixes based 
on this stem selection. The suffixes are given in this table under k = 5, where the stem 
S = stron perfectly aligns with the initial substring stron in each inflected form Fj, with 
0 edit distance. 
The segmented form-surface form pairs in Table 2 are then generated from the 
alignment of the stem with each surface form. 
5.2 Learning Segmentation and Morphographemic Rules 
The citation form and the affix information elicited and extracted by the process de- 
scribed above are used to construct regular expressions for the lexicon component 
69 
Computational Linguistics Volume 27, Number 1 
Table 1 
Stems Sk and the corresponding d(Sk). 
k=l  k=2 
Stems Considered, Sk 
k=3 k=4 k=5 k=6 
Form Fj s st str stro stron 
strona 5 4 3 2 1 
stron~ 5 4 3 2 1 
strony 5 4 3 2 1 
stronie 6 5 4 3 2 
stron G 5 4 3 2 1 
stron 4 3 2 1 0 
stronom 6 5 4 3 2 
stronach 7 6 5 4 3 
stronami 7 6 5 4 3 
Suffix 
-a 
-? 
-y 
-ie 
-om 
-ach 
-ami 
strona 
0 
2 
2 
3 
2 
1 
3 
2 
2 
d(Sk) 51 43 35 27 19 
Table 2 
The segmented and surface pair examples obtained. 
Segmented Surface 
strona+a strona 
strona+~ stron~ 
strona+y strony 
strona+ie stronie 
strona+ G stron G 
strona+ stron 
strona+om stronom 
strona+ach stronach 
strona+ami stronami 
of each paradigm. 13The example segmentations are fed into the learning module to 
induce morphographemic rules. 
5.2.1 Generat ing Candidate Rules f rom Examples. The preprocessing stage yields 
a list of pairs of segmented lexical forms and surface forms. The segmented forms 
contain the citation forms and affixes; the affix boundaries are marked by the + symbol. 
This list is then processed by a transformation-based l arning paradigm (Brill 1995; 
Satta and Henderson 1997), as illustrated in Figure 4. The basic idea is that we consider 
the list of segmented words as our input and find transformation rules (expressed as 
contextual rewrite rules) to incrementally transform this list into the list of surface 
forms. The transformation we choose at every iteration is the one that makes the list 
of segmented forms closest o the list of surface forms. 
The first step in the learning process is an initial alignment of pairs using a stan- 
dard dynamic programming scheme. The only constraints in the alignment are: (i) a + 
in the segmented lexical form is always aligned with an empty string on the surface 
side, notated by 0; (ii) a consonant on one side is always aligned with a consonant or 
0 on the other side, and likewise for vowels; (iii) the alignment must correspond to 
13 The result of this process i a script for the XRCE finite-state ool xfst. Large-scale l xicons can be more 
efficiently compiled by the XRCE tool lexc. We currently do not generate l xc scripts, but it is trivial to 
do so. 
70 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
Segmented 
Forms 
I 
Figure 4 
l 
(incrementally) 
transformed 
segmented 
forms 
Learner 
Surface forms 
(Truth) 
Transformation-based learning of morphographemic rules. 
the minimum edit distance between the original lexical and surface forms. 14 From this 
point on, we will use a simple example from English to clarify our points. 
Assume that we have the pairs (un+happy+est, unhappiest)  and (shop+ed, 
shopped) in our example base. We align these and determine the total number of 
"errors" in the segmented forms that we have to fix to make all segmented forms 
match the corresponding surface forms. The initial alignment produces the aligned 
pairs: 
un + happy + est shopO+ ed 
un 0 happi 0 est shopp 0 ed 
with a total of five errors. From each segmented pair we generate rewrite rules of the 
sort is 
u -> i \[\] Le f tContext ,  RightContext ; 
where u(pper) is a symbol in the segmented form, l(ower) is a symbol in the surface 
form. Rules are generated only from those aligned symbol pairs that are different. 
Lef tContext  and RightContext are simple regular expressions describing contexts 
in the segmented side (up to some small length), also taking into account he word 
boundaries. For instance, from the first aligned-pair example, this procedure would 
generate rules such as the following (depending on the amount of left and right context 
allowed): 
y -> i 
y -> i 
y -> i 
+ -> 0 
+ -> 0 
+ -> 0 
+ -> 0 
p_  y->i  
p_+es  y -> i  
p_  + e s t # y -> i 
# U n 
e s t 
est# . .  
est# Ppy  - 
+ -> 0 
p_+e 
p_+est  
p p_+e 
#un _ hap  
14 We arbitrarily choose one if there are multiple legitimate alignments. 
15 We use the XRCE finite-state ools regular expression syntax (Karttunen et al 1996). For the sake of 
readability, we will ignore the escape symbol (%) that should precede any special characters (e.g., ?) 
used in these rules. 
71 
Computational Linguistics Volume 27, Number 1 
The # symbol denotes a word boundary and is intended to capture any word-initial 
and word-final phenomena. The segmentation rules (+ -> 0) require at least some 
minimal eft or right context (usually longer than the minimal context for other rules 
in order to produce more accurate segmentation decisions). We disallow contexts that 
consist only of a morpheme boundary, as such contexts are usually not informative. 
It should be noted that these rules transform a segmented form into a surface form 
(contrary to what may be expected for analysis). This lets us capture situations where 
multiple segmented forms map to the same surface form, which occurs when the 
language has morphological mbiguity. Thus, in a reverse lookup, a given surface 
form may be interpreted in multiple ways, if applicable. 
Since we have many examples of aligned pairs in our example base, it is likely that 
a given rule will be generated from many pairs. For instance, if the pairs (stop+ed, 
stopped) and (tr ip+ed, tr ipped) were also in the list, the gemination rule 0 -> p 
I I p - + e d (along with certain others) will also be generated from these examples. 
We count how many times a rule is generated and associate this number with the rule 
as its promise, meaning that it promises to fix this many "errors" if it is selected to 
apply to the current list of segmented forms. 
5.2.2 Genera l i z ing  Ru les .  The candidate rules generated by the processes described 
above refer to specific strings of symbols as left and right contexts. It is, however, 
possible to obtain more generalized rules by classifying the symbols in the alphabet 
into phonologically relevant groups, like vowels and consonants. The benefit of this 
approach is that the number of rules thus induced is typically smaller, and more 
unseen cases can be covered. 
For instance, in addition to a rule like 0 -> p I I p - + e,  the rules 
0 -> p 
0 -> p 
CONSONANTS _ 
p _ + VOWELS 
+ e 
0 -> p CONSONANTS _ + VOWELS 
can be generated, where symbols such as CONSONANTS and VOWELS stand for regu- 
lar expressions denoting the union of relevant symbols in the alphabet. The promise 
scores of the generalized rules are found by adding the promise scores of the origi- 
nal rules generating them. Generalization substantially increases the number of can- 
didate rules to be considered during each iteration, but this is not a very serious 
issue, as the number of examples per paradigm is expected to be quite small. The 
rules thus learned would be the most general set of rules that do not conflict with 
the evidence in the examples. It is possible to use a more refined set of classes that 
correspond to subclasses of vowels (e.g., high vowels) and consonants (e.g., frica- 
tives) but these will substantially increase the number of candidate rules at every 
iteration and will have an impact on the iteration time unless examples are chosen 
carefully. 
5.2.3 Selecting Rules. At each iteration, all the rules along with their promise scores 
are generated from the current state of the example pairs. The rules generated are then 
ranked based on their promise scores, with the top rule having the highest promise. 
Among rules with the same promise score, we rank more general rules higher, with 
generality being based on context subsumption (i.e., preference goes to rules using 
shorter contexts and/or referring to classes of symbols, like vowels or consonants). 
All segmentation rules go to the bottom of the list, though within this group, rules 
are still ranked based on decreasing promise and context generality. The reasoning 
72 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
for treating the segmentation rules separately and later in the process is that affixa- 
tion boundaries constitute contexts for all morphographemic  changes; therefore they 
should not be eliminated if there are any (more) morphographemic  phenomena to 
process. 
Starting with the top-ranked rule, we test each rule on the segmented compo- 
nent of the pairs. A finite-state ngine emulates the replace rules to see how much 
the segmented forms are "fixed." The first rule that fixes as many "errors" as it 
promises to fix, and does not generate an interim example base with generation 
ambiguity, is selected. 16 The issue of generation ambiguity refers to cases where the 
same segmented forms are paired with distinct surface forms. 17 In such cases, find- 
ing a rule that fixes both pairs is not possible, so in choosing rules, we avoid any 
rules whose tentative application generates an interim example base with such am- 
biguities. In this way, we can account for all the discrepancies between the sur- 
face and segmented forms without falling into a local minima. Although we do not 
have formal proof that this simple heuristic avoids such local minima situations, in 
our experimentation with a large number  of cases we have never seen such an in- 
stance. 
The complete procedure for rule learning can now be given as follows: 
- Align surface and segmented forms in the example base; 
- Compute total Error; 
- while(Error > O) { 
-Generate all possible rewrite rules subject to context size limits; 
-Rank Rules ; 
-whi le  ( there  are more ru les  and a ru le  has not yet  been se lec ted)  { 
- Tentatively apply the next rule to all the segmented forms; 
- Re-align the resulting segmented forms with the 
corresponding surface forms to see how many 
''errors'' have been fixed; 
- If the number of errors fixed is equal to what the rule 
promised to fix AND the result does not have generation 
ambiguity, select this rule; 
} 
-Commit the changes performed by the rule on the segmented forms 
to the example base; 
-Reduce Error by the promise score of the selected rule; 
This procedure ventually generates an ordered sequence of two ordered groups 
of rewrite rules. The first group of rules is for any morphographemic  phenomena 
in the given set of examples, and the second group of rules handles segmentation. 
All these rules are composed in the order in which they are generated to construct 
the Morphographemic Rules transducer at the bottom of each parad igm (see Fig- 
ure 2). 
16 Note that a rule may actually introduce unintended errors in other pairs, since context checking is 
done only on the segmented form side; therefore what a rule delivers may be different than what it 
promises, as promise scores also depend on the surface side. 
17 Consider a state of the example base where some segmented lexical form L is paired with different 
surface forms $1 and $2, that is, we have pairs (L, $1) and (L, $2) in our example base. Any rule that 
will bring L closer to $1 will also change L of the second pair and potentially make it impossible to 
bring it closer to $2. 
73 
Computational Linguistics Volume 27, Number 1 
5.3 Identifying Errors and Providing Feedback 
Once the Morphographemic Rules transducers are compiled and composed with the 
lexicon transducer that is generated automatically from the elicited information, we 
obtain an analyzer for the paradigm. The analyzer for the paradigm can be tested by 
using the xfst environment of the XRCE finite-state tools. This environment provides 
machinery for testing the output of the analyzer by generating all forms involving 
a specific citation form, a specific morphosyntactic feature, or the like. This kind of 
testing has proved quite sufficient for our purposes. 
When the full analyzer is generated by unioning all the analyzers for each para- 
digm, one can do a more comprehensive t st against a test corpus to see what surface 
forms in the test corpus are not recognized by the generated analyzer. Apart from 
revealing obvious deficiencies in coverage (e.g., missing citation forms in the lexicon), 
such testing provides feedback about minor human errors--the failure to cover cer- 
tain morphographemic phenomena, or the incorrect assignment of citation forms to 
paradigms, for example. 
Our approach is as follows: we use the resulting morphological analyzer with an 
error-tolerant finite-state recognizer engine (Oflazer 1996). Using this engine, we try to 
find words recognized by the analyzer that are (very) close to a rejected (correct) word 
in the test corpus, essentially performing a reverse spelling correction. If the rejection 
is due to a small number of errors (1 or 2), the erroneous words recognized by the 
recognizer are aligned with the corresponding correct words from the test corpus. 
These aligned pairs can then be analyzed to see what the problems may be. 
5.4 Applicability to Infixing, Circumfixing, and Agglutinating Languages 
The machine learning procedure for inducing rewrite rules is not language dependent. 
It is applicable to any language whose lexical representation is a concatenation of
free and bound morphemes (or portions thereof). All this stage requires is a set of 
pairs of lexical and surface representations of the examples compiled for the example 
base. 
We have tested the rule learning component above on several other languages in- 
cluding Turkish, an agglutinating language, using an example base with lexical forms 
produced by a variant of the two-level morphology-based finite-state morphological 
analyzer described in Oflazer (1994). The lexical representation for Turkish also in- 
volved meta symbols (such as H for high vowels, D for dentals, etc.), which would 
be resolved with the appropriate surface symbol by the rules learned. For instance, 
vowel harmony rules would learn to resolve H as one of ~, i ,  u, ii in the appropriate 
context. 
Furthermore, the version of the rule learning (sub)system used for Turkish also 
made use of context-bound morphophonological distinctions that are not elicited in 
Boas, such as high vowels, low unrounded vowels, dentals, etc. The rules generated 
were the most general set of rules that did not conflict with the example base. There 
were many examples in the example base that involved multiple suffixes, not just 
one, as in the inflecting languages we address in this paper. It was quite satisfying 
to observe that the system could learn rules for dealing with vowel harmony, de- 
voicing, and so on. A caveat is that if there were too many examples and too many 
morphophonological classes, the number of candidate rules to be tried increased ex- 
ponentially. This could be alleviated to a certain extent by a careful selection of the 
example base. 
Thus, the rule-learning component is applicable to agglutinative, and also to in- 
fixing and circumfixing languages, provided there is a proper representation of the 
lexical and surface forms. However, for infixing languages it could be very problem- 
74 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
atic to have a linear representation f the infixation, with the lexical root being split 
in two and the morphotactics picking up the first part, the infix, and the second part. 
To prevent overgeneration, the infix lexicon might have to be replicated for each root, 
to enforce the fact that the two parts of the stem go together. 18The case for circumfix- 
ation is simpler since the number of such morphemes i  assumed to be much smaller 
than the number of stems, so the circumfixing morphemes can be split up into two 
lexicons and treated as a prefix-suffix combination. The co-occurrence r strictions for 
the respective pairs can then be manually enforced with finite-state constraints that 
can be added to the lexical and surfacy constraints section of the analyzer (see Fig- 
ure 2). 
Thus, in all three cases, learning the rules is not a problem provided the example 
base is in the requisite linear representation. On the other hand, this approach as such 
is inapplicable to languages like Arabic, which have radically different word formation 
processes (for which a number of other finite-state approaches have been proposed; 
(see, for example, Beesley \[1996\] and Kiraz \[2000\]). 
On the other hand, in contrast o acquiring the rewrite rules, eliciting the mor- 
photactics and the affix lexicons for an agglutinating language (semi)automatically 
is a very different process and is yet to be addressed. There are three parts to this 
problem: 
. 
2. 
3. 
Determining the boundaries of free and bound morphemes, accounting 
for any morphographemic variations; 
Determining the order of morphemes; 
Determining the "semantics" of the morphemes, that is, the features they 
encode. 
These are complicated by a number of additional issues uch as zero morphemes, local 
and long-distance o-occurrence r strictions (e.g., for allomorph selection), exceptions, 
productive derivations, circular derivations, and morphemes with the same surface 
forms but a totally different morphotactic position and function. Also, in languages 
that have a phenomenon like vowel harmony, such as Turkish, even if all harmonic 
allomorphs of a certain suffix are somehow automatically grouped into a lexicon with- 
out any further abstraction, severe overgeneration would result, unless the all root and 
suffix lexicons were split or replicated along vowel lines. In such cases, a human in- 
formant (who possesses a certain familiarity with morphographemics and issues of 
overgeneration) may have to resort o manual abstraction of the morpheme represen- 
tations. Then the process of acquiring the features for inflectional and derivational 
morphemes could proceed. 
6. Bootstrapping a Polish Analyzer 
This section presents a quite extensive xample of bootstrapping a morphological n- 
alyzer for Polish by iteratively providing examples and testing the morphological n- 
alyzer systematically. The idea of this exercise was to have a relatively limited number 
of paradigms that bunched words showing slight inflectional variations. 19For reasons 
18 This is much like what one encounters when dealing with reduplication in the FS framework. Also 
note that his is a lexicon issue and not a rule issue. 
19 Nonexpert language informants u ing Boas will be encouraged to split, rather than bunch, paradigms, 
for the sake of simplicity. 
75 
Computational Linguistics Volume 27, Number 1 
of space, the exposition is limited to developing four paradigms, of which one will be 
covered in detail. The paradigms here cover only a subset of masculine norms, and 
do not treat feminine or neuter nouns at all; however, they cover all the problems that 
would be found in words of those genders. 
For purposes of testing the learner off-line (i.e., outside the Boas environment), we 
tried to keep to a minimum the number of inflected forms given for each additional 
citation form. This was a learner-oriented task and intended to determine how robust 
the learner could become with a minimum of input. When using the Boas interface, the 
language informant will not have the option of selectively providing inflected forms. 
The interface works as follows: the informant gives all forms of the primary example 
and lists other citation forms that he or she thinks belong to the given paradigm. Hav- 
ing learned rules from the primary example, the learner generates all the inflectional 
forms for each citation form provided. The informant hen corrects all mistakes and 
the learner elearns the rules. So, the informant never has the opportunity to say "Well, 
I know the learner can't predict the locative singular for this word, so I will supply 
it overtly from the outset." The informant will just have to wait for the learner to get 
the given forms wrong and then correct hem. Any other approach would make for 
a complex interface and would require a sophisticated language informant--not what 
we are expecting. 
Polish is a highly inflectional West Slavic language that is written using extended 
Latin characters (six consonants and three vowels have diacritics). Certain phonemes 
are written using combinations of letters: e.g., sz, cz, and szcz represent phonetic ~, 
G and ~,  respectively. R? Polish nominals inflect for seven cases: Nominative (Nom.), 
Accusative (Acc.), Genitive (Gen.), Dative (Dat.), Locative (Loc.), Instrumental (Instr.), 
and Vocative (Voc.); and two numbers: Singular (Sg.) and Plural (P1.). 21 The complex- 
ity of Polish declension derives from four sources: (i) certain stem-final consonants 
mutate during inflection; these are called "alternating" consonants, and are contrasted 
with so-called "nonalternating" consonants (alternating/nonalternating is a crucial 
diagnostic for paradigm delineation in Polish); (ii) certain letters are spelled differ- 
ently depending on whether they are word-final or word-internal (e.g., word-final 
-d is written -si when followed by a vocalic ending); (iii) final-syllable vowels are 
added/deleted in some (not entirely predictable) words; and (iv) declension is not 
entirely phonologically driven--semantics and idiosyncrasy affect inflectional end- 
ings. 
The following practical simplifications have been made for testing purposes: 
Words that are normally capitalized (like names) are not capitalized here. 
Some inflectional form(s) that might not be semantically valid (e.g., 
plurals for collectives) were disregarded. Thus a bit of overgeneration 
still remains but can be removed with some additional effort. 
6.1 Paradigm 1 
The process starts with the description of Paradigm 1, which describes alternating 
inanimate masculine nouns with genitive singular in -u and no vowel shifts. The 
20 We actually treat hese as single symbols during learning. Such symbols are indicated in the 
description file in a special section that we have omitted in Figure 3. 
21 The Vocative case was not included in these tests because it is not expected tooccur widely in the 
journalistic prose for which the system is being built. 
76 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
following pr imary example for the 
Case 
Nom.  
Acc. 
Gen. 
Dat. 
Loc. 
Instr. 
citation form telefon is given in full: 
Number 
Singular Plural 
telefon telefony 
telefon telefony 
telefonu telefon6w 
telefonowi telefonom 
telefonie telefonach 
telefonem telefonami 
All inflectional forms in this paradigm are trivial except: 
? The Loc.Sg. depends on the final consonant and induces orthographic 
alternations for some alternating consonants: 22
Final Consonant(s) 
b, p, f, w, m, n, s, z 
t, d, st, zm 
Lr, st 
g, k, ch 
Loc.Sg. Ending 
-ie 
- ie 
-e 
-u 
Consonant Alternations 
t-,c, d--,dz, st--+gc, zm--*;~m 
f~l, r--*rz, sl--*gl 
? Instr.Sg. and Nom.P1. depend on the final consonant; two velars have an 
idiosyncratic ending: 
Final Consonant(s) 
b, p, f, w, m, n, s, z 
t, d, st zm, L r, st, ch 
g,k 
Instr.Sg. 
Ending 
-em 
- iem 
Nom.P1. 
Ending 
-y 
-i 
The following examples were provided in addition to the inflectional forms of the 
pr imary example in order to show Loc.Sg. endings and accompanying consonant al- 
ternations that could not be predicted based on the pr imary example: 
1. t~c: akcent (Nom.Sg.), akcencie (Loc.Sg.) 
2. d --* dz: wyktad (Nom.Sg.), wyktadzie (Loc.Sg.) 
3. st ---~dc: most (Nom.Sg.), modcie (Loc.Sg.) 
4. zm---~m: komunizm (Nom.Sg.), komuni~mie (Loc.Sg.) 
5. t-*l: artykut (Nom.Sg.), artykule (Loc.Sg.) 
6. r--*rz: teatr (Nom.Sg.), teatrze (Loc.Sg.) 
7. st~sl: pomyst (Nom.Sg.), pomydle (Loc.Sg.) 
The following additional examples were provided to show velar pecularities: 
8. g: pociqg (Nom.Sg.), pociqgu (Loc.Sg.), pociqgiem (Instr.Sg.), pociqgi (Nom.Pl.) 
22 Strictly speaking, the consonants b,p,f, w, m, n, s, and z alternate as well in the Loc.Sg., since 
alternating/nonalternating is a phonological distinction, ot a graphotactic one. The softening of these 
consonants i  indicated by the -i that precedes the canonical Loc.Sg. ending -e. However, for our 
purposes it is more straightforward toconsider the Loc.Sg. ending for these consonants -ie with no 
accompanying graphotactic alternation. 
77 
Computational Linguistics Volume 27, Number 1 
Table 3 
Summary of runs for Paradigm 1. 
Citation Additional Run 1 Additional Run 2 Additional Run 3 
Key Forms Examples Results Examples Results Examples Results 
0 telefon, stron, x/ 
paragraf, 
~piew, sklep, 
ttum, adres, 
obraz 
1 akcent, bilet Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. x/ 
Loc.Sg. oblique forms lnstr.Sg. 
2 wyklad, sad Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. x/ 
Loc.Sg. oblique forms Instr.Sg. 
3 most, list Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. V' 
Loc.Sg. oblique forms Instr.Sg. 
4 komunizm, Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. ~/ 
socjalizm Loc.Sg. oblique forms Instr.Sg. 
5 artykut, Nom.Sg. mutates all Nora.P1. mutates Instr.Sg. x/ 
kawat Loc.Sg. oblique forms Instr.Sg. 
6 teatr, numer Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. x/ 
Loc.Sg. oblique forms Instr.Sg. 
7 pomysl, Nom.Sg. mutates all Nom.P1. mutates Instr.Sg. x/ 
zmyst Loc.Sg. oblique forms Instr.Sg. 
8 poci~g, brzeg Nom.Sg. x/ 
Loc.Sg. 
Instr.Sg. 
Nora.P1. 
9 bank, krok Nom.Sg. missed velar- Loc.Sg. of v / 
krok; 
Loc.Sg. specific Loc.Sg.; Add btysk to 
Instr.Sg. gave *krokie lexicon for 
Nom.P1. not kroku testing 
10 dach, wirch Nom.Sg. missed velar- Loc.Sg. of wrong add v / 
wirch; 
specific Loc.Sg.; Add ~miech Instr.Sg. Instr.Sg. 
gave *wirchie to lexicon for wirch, of 
not wirchu for testing ~miech wirch 
Loc.Sg. 
. 
10. 
k: bank (Nom.Sg.), banku (Loc.Sg.), bankiem (Instr.Sg.), banki (Nom.Pl.) 
ch: dach (Nom.Sg.), dachu (Loc.Sg.) 
Table 3 summarizes the first three runs for this paradigm, which were sufficient o 
create a relatively robust set of morphological rules that required only slight amend- 
ment and further testing in two additional runs. For this and subsequent such tables 
we use the following conventions: Key 0 shows the primary citation form and addi- 
tional citation forms whose inflectional patterns hould be fully covered by the rules 
generated for the primary example. The other key numbers correspond to the addi- 
tional examples given above. Boldface citation forms under the lexicon column are 
those for which some additional inflectional examples were given. The citation forms 
given in plain text are for testing purposes. Oblique cases refer to the Genitive, Dative, 
Locative, and Instrumental cases. 
The original assumption for Paradigm 1 was that it would be sufficient o pro- 
vide one unmutated form (the Nom.Sg.) plus the mutated form (the Loc.Sg.) for words 
ending in mutating consonants. This led to overgeneralization of the alternation; there- 
78 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
fore, another unmutated form had to be added as a "control." Adding the Nom.P1. 
forms fixed most oblique forms for all the words, but it left the Instr.Sg. mutated. 
This appears to be because the inflectional ending for the Loc.Sg. (which mutates) and 
the Instr.Sg. (which does not) both begin in -e for the words in question. Adding the 
Instr.Sg. overtly counters overgeneralization of the alternation. The source of the velar 
errors is not immediately evident. 
Supplementary testing was carried out after the above-mentioned words were all 
correct. Correct forms were produced for all new words showing consonant mutations 
and velar peculiarities: amolot, przyklad, pretekst, podziaL kolor, dtug, lek, gmach. One error 
for a nonmutating word (in Key 0) occurred. This word, herb, ends in a different 
consonant than the primary example and produced the wrong Loc.Sg. form. This was 
later added overtly and more words with other nonmutating consonants (postcp, puf, 
gniew, film, opis, raz) were tested; all were covered correctly. 
6.2 Paradigm 2 
The paradigm implemented next was Paradigm 2: alternating inanimate masculine 
nouns with genitive singular in -u and vowel shifts. The following primary example 
for the citation form gr6b was given in full: 
Case 
Nom. 
Acc. 
Gen. 
Dat. 
Loc. 
Instr. 
Number 
Singular Plural 
gr6b groby 
gr6b groby 
grobu grob6w 
grobowi grobom 
grobie grobach 
grobem grobami 
This paradigm is just like Paradigm 1, except hat there are vowel shifts that are 
not entirely graphotactically predictable; therefore, words showing these shifts must be 
classed separately. The vowel shifts occur in all inflectional forms except he Nom.Sg. 
and the Acc.Sg., which are identical. The following vowel shifts occurred in the cases 
we considered (~b indicates vowel deletion). 
Vowel in Vowel in 
Nom.Sg./Acc.Sg. Other Forms 
6 o 
e 
ie ~b 
a e ~ 
This shift only occurs in Loc.Sg. 
The following consonant alternations are also observed in this paradigm: 
Consonant in 
Most Forms 
d 
dz 
t 
r 
Consonant in 
Loc.Sg. 
dz  
~dz 
1 
r z  
Based on the experience of Paradigm 1, the Instr.Sg. forms for all words with 
consonant alternation were provided as examples at the outset o avoid the overgen- 
eralization of the alternation. The velar pecularities are still in effect and must be dealt 
with explicitly. 
79 
Computational Linguistics Volume 27, Number 1 
The following examples were given to exemplify vowel shifts with an unmutating 
consonant: 
1. e --* q~ shift with n: sen(Nom.Sg.), snie (Loc.Sg.) 
The following examples were employed to show vowel shifts in combination with 
various consonant alternations in the Loc.Sg. forms: 
. 
. 
, 
5. 
6. 
d ~ o and d --* dz: samoch6d (Nom.Sg.), samochodzie (Loc.Sg.), samochodem 
(Instr.Sg.) 
a --~ e and zd ~ ~dz: dojazd (Nom.Sg.), doje~dzie (Loc.Sg.), dojazdem 
(Instr.Sg.) 
d --+ o and t --* h st6t (Nom.Sg.), stole (Loc.Sg.), stotem (Instr.Sg.) 
e -* ~ and r --~ rz: puder (Nom.Sg.), pudrze (Loc.Sg.), pudrem (Instr.Sg.) 
ie --~ ~ and r ~ rz: cukier (Nom.Sg.), cukrze (Loc.Sg.), cukrem (Instr.Sg.) 
Finally, the following examples were given to show velar peculiarities: 
. 
. 
e --, ~ with k: budynek (Nom.Sg.), budynku (Loc.Sg.), budynkiem (Instr.Sg.), 
budynki (Nom.P1.) 
d --* 0 with g: r6g (Nom.Sg.), rogu (Loc.Sg.), rogiem (Instr.Sg.), rogi 
(Nom.P1.) 
At the end of first run for this paradigm only one of the eight groups above 
was covered completely. All vowel shifts for all groups came out right. However, the 
Nom.P1. and Acc.P1. endings were incorrectly generalized as -i instead of -y, probably 
because two "exceptional" velar examples (in -i) were provided in contrast o one 
"regular" nonvelar example (in -y). Adding the Nom.P1. forms of three nonvelar words 
fixed this error. The results for velars were perfect except for the loss of z in 10 of 12 
forms of obowiqzek. Adding the Nom.P1. form obowi~zki f xed this. For st6t and d6t, the 
consonant alternation was incorrectly extended to Gen.Sg. Adding the Gen.Sg. form 
of st6t fixed this error for both words. At the end of the second run, all groups were 
correctly learned. 
Supplementary testing after the above-mentioned words were correct included the 
words naw6z, doch6d, poz6r, rozbi6r, gr6d, rozch6d, nar6d, wtorek, kierunek; all forms were 
correct. 
6.3 Parad igm 3 
Paradigm 3 contains alternating "man" nouns--that is, masculine nouns referring to 
human men. The following primary example for the citation form pasierb was given 
in full: 
Case 
Nom. 
acc. 
Gen. 
Dat. 
Loc. 
InstE 
Number 
Singular 
pasierb 
pasierba 
pasierba 
pasierbowi 
pasierbie 
pasierbem 
Plural 
pasierbowie 
pasierbi 
pasierb6w 
pasierb6w 
pasierbom 
pasierbach 
pasierbami 
80 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
In this paradigm, all of the consonant alternations encountered above are still in 
effect and some word-final consonants undergo additional alternations in the Nom.P1. 
The velar peculiarities remain in effect. One additional complication i this paradigm 
is that there may be multiple Nom.P1. forms for a given citation form (e.g., pasierbowie 
and pasierbi are both acceptable Nora.P1. forms for pasierb). Furthermore, -i/-y are allo- 
morphs in complementary distribution (i.e., the second Nom.P1. form in this paradigm 
is realized with -y for certain word-final consonants). 
Stem-Final 
Consonant 
b, f, w, m, n, z, t 
p, ch 
d,t  
r,k,g 
Nom.P1. 
Ending 
-owie or -i or both 
-i only 
-owie only 
-owie or -y or both 
Since the analyzer needs only to analyze (and not generate) forms, there is no need 
to split this paradigm into five different ones to account for each Nom.P1. possibility: 
-owie,-owie/-i,-i, -owie/-y, -y. We simply permit overgeneration, allowing each word to 
have two Nom.P1. forms: the correct one of the -i/-y allomorphs and -owie. Further, 
since the analyzer has no way to predict which of the -i/-y allomorphs i used with a 
given word-final consonant, explicit examples of each word-final consonant must be 
provided. 
These considerations lead to splitting the citation forms for this paradigm into 
14 groups, which represent the primary example plus 13 inflectional groups added 
as supplementary examples. The Nom.Sg., Loc.Sg., and both (or applicable) Nom.P1. 
forms were provided for all groups apart from the primary example. After the first 
run, 13 of 14 groups were correctly covered. The remaining roup was handled cor- 
rectly in two additional runs: two more inflectional forms of the example in word-final 
r had to be provided to counter overgeneralization of the r --* rz alternation. 
Supplementary testing after the above-mentioned words were correct included 
the citation forms drab, piastun, kasztelan, faraon, w6jt, mnich, biedak, norweg, wtoch. The 
following errors were encountered: 
norweg got the Acc.Sg./Gen.Sg. form *norweda instead of norwega. 
Adding the correct Acc.Sg. form fixed this problem. 
wtoch got the Nom.P1. form *wtoci instead of wtosi. This form was added 
overtly. 
mnich got the Nom.P1. form *mnici instead of mnisi. This form was added 
overtly. 
After these final additions, wtoch and mnich ended up with the Acc.Sg./Gen.Sg. 
forms *wtosa and *mnisa instead of wtocha and mnicha (i.e., the alternation was overgen- 
eralized again). Overtly adding the correct Acc.Sg. form wtocha solved this problem 
for both words and all forms were now correct. 
6.4 Paradigm 4 
Paradigm 4 was for nonalternating inanimate masculine nouns with genitive singular 
in -a and no vowel shifts. The following declension for bicz was provided as the 
81 
Computational Linguistics Volume 27, Number 1 
primary example: 
Case 
Nom. 
Acc. 
Gen. 
Dat. 
Loc. 
Instr. 
Number 
Singular Plural 
bicz bicze 
bicz bicze 
bicza biczy 
biczowi biczom 
biczu biczach 
biczem biczami 
A spelling rule of Polish comes into play in this paradigm: letters that take a 
diacritic word-finally or when followed by a consonant are spelled with no diacritic 
plus an -i when followed by a vowel. For instance: ~+u --* niu, ~+owi --* niowi, d+u --* 
ciu, d+owi --+ ciowi. Some, but not all, word-final letters in this paradigm have diacritics. 
In addition, in this paradigm, Gen.Sg. endings depend on the final consonant: they 
can be -6w (for j, ch, szcz), -i (for L ~, ~) or -y (for cz, sz, rz, ~). In many instances, more 
than one form is possible, but this test covers only the most common form for each 
stem-final consonant. 
The citation forms in this paradigm broke down into 10 groups based on the final 
consonant. The Nom.Sg., Gen.Pl., and Instr.P1. forms were provided for the 9 groups 
(the tenth is the primary example, for which all forms were provided). Eight of the 
10 groups were handled correctly after the first run. The spelling-rule related to -i 
required some extra forms to be learned correctly. Otherwise, everything came out as 
predicted. Supplementary testing included the citation forms klawisz, b~bel, strumie~, 
tach, cyrkularz; all inflectional forms were produced correctly. 
7. Performance Issues 
Generating a morphological nalyzer once the descriptive data is given can be carried 
out very fast. Each paradigm can be processed within tens of seconds on a fast work- 
station, including the few tens of iterations of rule learning from the examples. A new 
version of the analyzer can be generated within minutes and tested rapidly on any test 
data. Thus, none of the processes described in this paper constitutes a bottleneck in the 
elicitation process. Figure 5 provides ome relevant information from the runs of the 
first paradigm in Polish described above. The top graph shows, for different runs, the 
number of distinct rules generated from the aligned segmented form--surface-form 
pairs generated from the examples provided, using a rule format with at most five 
symbols in each of the left and right contexts. The bottom graph shows, for differ- 
ent runs, the total number of rules generated and generalized--again, with the same 
context size as above. 
There are a few interesting things about these graphs. As expected, when more 
examples are added, the number of rules and the number of iterations needed for 
convergence usually increases. All curves have a steeper initial segment and a steeper 
final segment. The steep initial segments result from the initial selection of rules that 
fix the largest number of "errors" between the segmented and surface forms. Once 
those rules are found, the curves flatten as a number of morphographemic rules are 
selected, each dealing with a very small number of errors. Finally, when all the mor- 
phographemic changes are accounted for, the segmentation rules kick in and each such 
rule fixes a large number of segmentation "errors," so that a few general rules deal 
with all such cases. 
82 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
Rules generated in each i te ra t ion  o f  the learner i n  sequent ia l  runs 
- - - ? - - -Run  1 
- - - I I - - -Run  2 
& Run 3 
1000 ? 
900  ? 
800  ? 
700  
600 
500 ? 
= 
-5 
400  ? 
300  ? 
200  
100 
0 
\ 
? \ 
? , L 
" ' ? ,  " I L  
" ? - - . ? ,  l i -  ~ ~ll- ~. iB.  i .  
~-- -e* - .~? . . ?  " i t -  ~ ~il-- ~ . i .  ~ . i -  
;'.-,, . 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  23  
Learning i teration 
Rules general ized in each i te ra t ion  o f  the learner i n  sequent ia l  runs  
- - -e - - -Run  1 
- - -m- - -Run  2 
A Run 3 
20000,  
18000,  
16000,  
14000,  
~ 12000,  
m 
lO000,  
8000,  
6000,  
4000,  
2000,  
0 
~ #1-. - +an- :--+HI- --+l+.,.. - ~ _ . .  0 
~ -i-. ~ -il~,.~, L
" '~- - - t - .  ~-ap .  
" ' t - . .e .  ~'i l- ~ L 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  21 22 23  
Learning Iteration 
Figure 5 
Rule statistics for processing Paradigm 1. 
8. Summary  and Conc lus ions  
We have presented the highlights of our approach for automatically generating finite- 
state morphological nalyzers from information elicited from human informants. Our 
approach uses transformation-based learning to induce morphographemic rules from 
examples and combines these rules with the lexicon information elicited to compile 
the morphological nalyzer. There are other opportunities for using machine learning 
in this process. For instance, one of the important issues in wholesale acquisition of 
83 
Computational Linguistics Volume 27, Number 1 
open-class items is that of determining which paradigm a given citation form belongs 
to. From the examples given during the acquisition phase, it is possible to induce a 
classifier that can perform this selection to aid the language informant. 
We believe that we have presented a viable approach to the automatic generation 
of a natural language processor. Since this approach involves a human informant 
working in an elicit-generate-test loop, the noise and opaqueness of other induction 
schemes can be avoided. 
We also feel that the task of analyzing a set of incorrectly generated forms and 
automatically offering a diagnosis of what may have gone wrong and what additional 
examples can be supplied as remedies is, in itself, an important aspect of this work. 
Although we have only scratched the surface of this topic here, we consider it a fruitful 
extension of the work described in this paper. 
Acknowledgments 
This research was supported in part by 
Contract MDA904-97-C-3976 from the U.S. 
Department of Defense. We also thank 
XRCE for providing the finite-state tools. 
Most of this work was done while the first 
author was visiting NMSU Computing 
Research Laboratory during the 1998-1999 
academic year, on leave from Bilkent 
University, Ankara, Turkey. 
References 
Antworth, Evan L. 1990. PC-KIMMO: A 
two-level processor for Morphological Analysis. 
Occasional Publications in Academic 
Computing, Number 16. Summer 
Institute of Linguistics, Dallas, TX. 
Beesley, Kenneth R. 1996. Arabic finite-state 
morphological nalysis and generation. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 89-94, Copenhagen, 
Denmark. 
Brill, Eric. 1995. Transformation-based 
error-driven learning and natural 
language processing: A case study in 
part-of-speech tagging. Computational 
Linguistics, 21(4):543-566, December. 
Damerau, F. J. 1964. A technique for 
computer detection and correction of 
spelling errors. Communications of the 
Association for Computing Machinery, 
7(3):171-176. 
Golding, Andrew and Henry S. Thompson. 
1985. A morphology component for 
language programs. Linguistics, 
23:263-284. 
Goldsmith, John. 1998. Unsupervised 
learning of the morphology of a natural 
language. Unpublished manuscript, 
available at http:/ /humanit ies.  
uchicago, edu/f aculty/goldsmith/ 
index, html. 
Johnson, Mark. 1984. A discovery procedure 
for certain phonological rules. In 
Proceedings ofl Oth International Conference 
on Computational Linguistics (COLING'84), 
pages 344-347, Stanford, CA, USA. 
Kaplan, Ronald M. and Martin Kay. 1994. 
Regular models of phonological rule 
systems. Computational Linguistics, 
20(3):331-378, September. 
Karttunen, Lauri. 1993. Finite-state l xicon 
compiler. Technical Report, XEROX, Palo 
Alto Research Center, April. 
Karttunen, Lauri. 1994. Constructing lexical 
transducers. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING '94), volume 1, 
pages 406-411, Kyoto, Japan. 
Karttunen, Lauri and Kenneth R. Beesley. 
1992. Two-level rule compiler. Technical 
Report, XEROX Palo Alto Research 
Center. 
Karttunen, Lauri, Jean-Pierre Chanod, 
Gregory Grefenstette, and Anne Schiller. 
1996. Regular expressions for language 
engineering. Natural Language Engineering, 
2(4):305-328. 
Karttunen, Lauri, Ronald M. Kaplan, and 
Annie Zaenen. 1992. Two-level 
morphology with composition. In 
Proceedings ofthe 14th International 
Conference on Computational Linguistics, 
volume 1, pages 141-148, Nantes, France. 
Kiraz, George Anton. 2000. Multitiered 
nonlinear morphology using multitape 
finite automata: A case study on Syriac 
and Arabic. Computational Linguistics, 
26(1):77-105. 
Koskenniemi, Kimmo. 1983. Two-level 
morphology: A general computational 
model for word form recognition and 
production. Publication No. 11, 
Department of General Linguistics, 
University of Helsinki. 
Mohri, Mehryar, Fernando Pereira, and 
Michael Riley. 1998. A rational design for 
a weighted finite-state transducer library. 
84 
Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers 
In Lecture Notes in Computer Science, 1436. 
Springer Verlag. 
Nirenburg, Sergei. 1998. Universal grammar 
and lexis for quick ramp-up of MT 
systems. In Proceedings ofthe First 
International Conference on Language 
Resources and Evaluation, pages 739-746, 
Spain. 
Nirenburg, Sergei and Victor Raskin. 1998. 
Project Boas: "A Linguist in a Box" as a 
multi-purpose language resource. In 
COLING-ACL "98: 36th Annual Meeting of 
the Association for Computational Linguistics 
and 17th International Conference on 
Computational Linguistics, pages 975-979, 
Montreal, Quebec Canada. 
Nirenburg, Sergei and Victor Raskin. 1999. 
Supply-side and demand-side l xical 
semantics. In Evelyne Viegas, editor, 
Depth and Breadth of Semantic Lexicons. Text, 
Speech, and Language Technology Series. 
Kluwer, Dordrecht and Boston. 
Oflazer, Kemal. 1994. Two-level description 
of Turkish morphology. Literary and 
Linguistic Computing, 9(2):137-148. 
Oflazer, Kemal. 1996. Error-tolerant 
finite-state recognition with applications 
to morphological nalysis and spelling 
correction. Computational Linguistics, 
22(1):73-90, March. 
Ranta, Aarne. 1998. A multilingual natural 
language interface to regular expressions. 
In Lauri Karttunen and Kemal Oflazer, 
editors, Proceedings ofthe International 
Workshop on Finite State Methods in Natural 
Language Processing, FSMNLP'98, 
pages 79-90. 
Rissanen, Jorma. 1989. Stochastic Complexity 
in Statistical Inquiry. World Scientific 
Publishing. 
Satta, Giorgio and John C. Henderson. 1997. 
String transformation learning. In 
Proceedings ofACL/EACL'97. 
Sproat, Richard. 1992. Morphology and 
Computation. M1T Press. 
Theron, Pieter and Ian Cloete. 1997. 
Automatic acquisition of two-level 
morphological rules. In Proceedings ofthe 
5th Conference on Applied Natural Language 
Processing. 
van Noord, Gertjan. 1999. FSA6: Finite state 
automata utilities (version 6) manual. 
Available at http://odur.let.rug.nl/van- 
noord/Fsa/Manual/. 
van Noord, Gertjan and Dale Gerdemann. 
1999. An extendible regular expression 
compiler for finite-state approaches in
natural anguage processing. In 
Proceedings ofWIA 99. 
85 

c? 2003 Association for Computational Linguistics
Dependency Parsing with an Extended
Finite-State Approach
Kemal Oflazer?
Sabanc? University
This article presents a dependency parsing scheme using an extended finite-state approach. The
parser augments input representation with ?channels? so that links representing syntactic depen-
dency relations among words can be accommodated and iterates on the input a number of times
to arrive at a fixed point. Intermediate configurations violating various constraints of projective
dependency representations such as no crossing links and no independent items except senten-
tial head are filtered via finite-state filters. We have applied the parser to dependency parsing of
Turkish.
1. Introduction
Finite-state machines have been used for many tasks in language processing, such as
tokenization, morphological analysis, and parsing. Recent advances in the develop-
ment of sophisticated tools for building finite-state systems (e.g., XRCE Finite State
Tools [Karttunen et al 1996], AT&T Tools [Mohri, Pereira, and Riley 1998], and Fi-
nite State Automata Utilities [van Noord 1997]) have fostered the development of
quite complex finite-state systems for natural language processing. In the last sev-
eral years, there have been a number of studies on developing finite-state parsing
systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefen-
stette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et
al. 1997). Another stream of work in using finite-state methods in parsing is based
on approximating context-free grammars with finite-state grammars, which are then
processed by efficient methods for such grammars (Black 1989; Pereira and Wright
1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been
a number of approaches to natural language parsing using extended finite-state ap-
proaches in which a finite-state engine is applied multiple times to the input, or various
derivatives thereof, until some termination condition is reached (Abney 1996; Roche
1997).
This article presents an approach to dependency parsing using a finite-state ap-
proach. The approach is similar to those of Roche and Abney in that all three use an
extended finite-state scheme to parse the input sentences. Our contributions can be
summarized as follows:
? Our approach differs from Roche?s and Abney?s in that it is based on the
dependency grammar approach and at the output produces an encoding
of the dependency structure of a sentence. The lexical items and the
dependency relations are encoded in an intertwined manner and
manipulated by grammar rules, as well as structural and linguistic
? Faculty of Engineering and Natural Sciences, Sabanc? University, Orhanl?, 34956, Tuzla, Istanbul,
Turkey. E-mail: oflazer@sabanciuniv.edu.
516
Computational Linguistics Volume 29, Number 4
constraints implemented as finite-state filters, to arrive at parses. The
output of the parser is a finite-state transducer that compactly packs all
the ambiguities as a lattice.
? As our approach is an all-parses approach with no statistical component,
we have used Lin?s (1995) proposal for ranking the parses based on the
total link length and have obtained promising results. For over 48% of
the sentences, the correct parse was among the dependency trees with
the smallest total link length.
? Our approach can employ violable constraints for robust parsing so that
when the parser fails to link all dependents to a head, one can use
lenient filtering to allow parses with a small number of unlinked
dependents to be output.
? The rules for linking dependents to heads can specify constraints on the
intervening material between them, so that, for instance, certain links
may be prevented from crossing barriers such as punctuation or lexical
items with certain parts of speech or morphological properties (Collins
1996; Giguet and Vergne 1997; Tapanainen and Ja?rvinen 1997).
We summarize in Figure 1 the basic idea of our approach. This figure presents in a
rather high-level fashion, for a Turkish and an English sentence, the input and output
representation for the approach to be presented. For the purposes of this summary, we
assume that none of the words in the sentences have any morphological ambiguity
and that their morphological properties are essentially obvious from the glosses. We
represent the input to the parser as a string of symbols encoding the words with some
additional delimiter markers. Panel (a) of Figure 1 shows this input representation for
a Turkish sentence, on the top right, and panel (b) shows it for an English sentence.
The parser operates in iterations. In the first iteration, the parser takes the input
string encoding the sentence and manipulates it to produce the intermediate string
in which we have three dependency relations encoded by additional symbols (high-
lighted with boldface type) injected into the string. The partial dependency trees en-
coded are depicted to the left of the intermediate strings. It should be noted that the
sets of dependency relations captured in the first iteration are different for Turkish
and English. In the Turkish sentence, two determiner links and one object link are
encoded in parallel, whereas in the English sentence, two determiner links and one
subject link are encoded in parallel. The common property of these links is that they
do not ?interfere? with each other.
The second iteration of the parser takes the output of the first iteration and manip-
ulates it to produce a slightly longer string in which symbols encoding a new subject
(object) link are injected into the Turkish (English) string. (We again highlight these
symbols with boldface type.) Note that in the English string the relative positions of
the link start and end symbols indicate that this is a right-to-left link. The dependency
structures encoded by these strings are again on their left. After the second iteration,
there are no further links that can be added, since in each case there is only one word
left without any outgoing links and it happens to be the head of the sentence.
The article is structured as follows: After a brief overview of related work, we
summarize dependency grammars and aspects of Turkish relevant to this work. We
provide a summary of concepts from finite-state transducers so that subsequent sec-
tions can be self-contained. We continue by describing the representation that we have
employed for encoding dependency structures, along with the encoding of depen-
dency linking rules operating on these representations and configurational constraints
517
Oflazer Dependency Parsing
Figure 1
Dependency parsing by means of iterative manipulations of strings encoding dependency
structures.
518
Computational Linguistics Volume 29, Number 4
for filtering them. We then describe the parser and its operational aspects, with details
on how linguistically motivated constraints for further filtering are implemented. We
briefly provide a scheme for a robust-parsing extension of our approach using the
lenient composition operation. We then provide results from a prototype implementa-
tion of the parser and its application to dependency parsing of Turkish. We close with
remarks and conclusions.
2. Overview of Related Work
Although finite-state methods have been applied to parsing by many researchers,
extended finite-state techniques were initially used only by Roche (1997), Abney (1996),
and the FASTUS group (Hobbs et al 1997). In the context of dependency parsing with
finite-state machines, Elworthy (2000) has recently proposed a finite-state parser that
produces a dependency output.
Roche (1997) presents a top-down approach for parsing context-free grammars
implemented with finite-state transducers. The transducers are based on a syntactic
dictionary comprising patterns of lexical and nonlexical items. The input is initially
bracketed with sentence markers at both ends and then fed into a transducer for
bracketing according to bracketing rules for each of the patterns in the dictionary.
The output of the transducer is fed back to the input, and the constituent structure is
iteratively refined. When the output of the transducer reaches a fixed point, that is,
when no additional brackets can be inserted, parsing ends.
Abney (1996) presents a finite-state parsing approach in which a tagged sentence
is parsed by transducers that progressively transform the input into sequences of sym-
bols representing phrasal constituents. In this approach, the input sentence is assumed
to be tagged with a part-of-speech tagger. The parser consists of a cascade of stages.
Each stage is a finite-state transducer implementing rules that bracket and transduce
the input to an output containing a mixture of unconsumed terminal symbols and
nonterminal symbols for the phrases recognized. The output of a stage goes to the
next stage in the cascade, which further brackets the input using yet other rules.
Each cascade typically corresponds to a level in a standard X-bar grammar. After a
certain (fixed) number of cascades, the input is fully bracketed, with the structures
being indicated by labels on the brackets. Iterations in Roche?s approach roughly cor-
respond to cascades in Abney?s approach. The grammar, however, determines the
number of levels or cascades in Abney?s approach: that is, structure is fixed. A work
along the lines of Abney?s is that of Kokkinakis and Kokkinakis (1999) for parsing
Swedish.
Elworthy (2000) presents a finite-state parser that can produce a dependency struc-
ture from the input. The parser utilizes standard phrase structure rules that are anno-
tated with ?instructions? that associate the components of the phrases recognized with
dependency grammar?motivated relations. A head is annotated with variables associ-
ating it with its dependents. These variables are filled in by the instructions associated
with the rules. These variables are copied or percolated ?up? the rules according to
special instructions. The approach resembles a unification-based grammar in which
instead of unification, dependency relation features are passed from a dependent to
its head. The rules for recognizing phrases and implementing their instructions are
implemented as finite-state transducers.
Another notable system for finite-state parsing is FASTUS (Hobbs et al 1997). FAS-
TUS uses a five-stage cascaded system, with each stage consisting of nondeterministic
finite-state machines. FASTUS is mainly for information extraction applications. The
early stages recognize complex multiword units such as proper names and colloca-
519
Oflazer Dependency Parsing
tions and build upon these by grouping them into phrases. Later stages are geared
toward recognizing event patterns and building event structures.
3. Dependency Syntax
Dependency approaches to syntactic representation use the notion of syntactic relation
to associate surface lexical items. Melc?uk (1988) presents a comprehensive exposition
of dependency syntax. Computational approaches to dependency syntax have recently
become quite popular (e.g., a workshop dedicated to computational approaches to de-
pendency grammars was held at COLING/ACL?98). Ja?rvinen and Tapanainen (1998;
Tapanainen and Ja?rvinen 1997) have demonstrated an efficient wide-coverage depen-
dency parser for English. The work of Sleator and Temperley (1991) on link grammar,
essentially a lexicalized variant of dependency grammar, has also proved to be interest-
ing in regard to a number of aspects. Dependency-based statistical language modeling
and parsing have also become quite popular in statistical natural language processing
(Lafferty, Sleator, and Temperley 1992; Eisner 1996; Chelba et al 1997; Collins 1996;
Collins et al 1999).
Robinson (1970) gives four axioms for well-formed dependency structures that
have been assumed in almost all computational approaches. These state that, in a
dependency structure of a sentence,
1. one and only one word is independent, that is, not linked to some other
word;
2. all others depend directly on some word;
3. no word depends on more than one other; and
4. if a word A depends directly on word B, and some word C intervenes
between them (in linear order), then C depends directly on A or on B, or
on some other intervening word.
This last condition of projectivity (or various extensions of it; see, e.g., Lai and Huang
[1994]) is usually assumed by most computational approaches to dependency gram-
mars as a constraint for filtering configurations and has also been used as a simplifying
condition in statistical approaches for inducing dependencies from corpora (e.g., Yu?ret
1998).1
4. Turkish
Turkish is an agglutinative language in which a sequence of inflectional and deriva-
tional morphemes get affixed to a root (Oflazer 1993). At the syntax level, the un-
marked constituent order is Subject-Object-Verb, but constituent order may vary as
demanded by the discourse context. Essentially all constituent orders are possible,
especially at the main sentence level, with very minimal formal constraints. In writ-
ten text, however, the unmarked order is dominant at both the main-sentence and
embedded-clause level.
Turkish morphophonology is characterized by a number of processes such as
vowel harmony (vowels in suffixes, with very minor exceptions, agree with previous
1 See section 6 for how projectivity is checked and section 6.5 on the implications of checking for
projectivity during parsing with both right-to-left and left-to-right dependency links.
520
Computational Linguistics Volume 29, Number 4
vowels in certain aspects), consonant agreement, and vowel and consonant ellipsis.
The morphotactics are quite complicated: A given word form may involve multiple
derivations (as we show shortly). The number of word forms one can generate from
a nominal or verbal root is theoretically infinite (see, e.g., Hankamer, [1989]).
Derivations in Turkish are very productive, and the syntactic relations that a word
is involved in as a dependent or head element are determined by the inflectional
properties of the one or more (possibly intermediate) derived forms. In this work, we
assume that a Turkish word is represented as a sequence of inflectional groups (IGs),
separated by ^DBs, denoting derivation boundaries, in the following general form:
root+IG1 + ?DB+IG2 + ?DB+? ? ? + ?DB+IGn.
Here each IGi denotes relevant inflectional features including the part of speech for
the root, for the first IG, and for any of the derived forms. For instance, the derived
modifier sag?lamlas?t?rd?g??m?zdaki2 would be represented as3
sag?lam+Adj+^DB+Verb+Become+^DB+Verb+Caus+Pos
+^DB+Noun+PastPart+A3sg+P3sg+Loc
+^DB+Adj
The five IGs in this are
1. sag?lam(strong)+Adj
2. +Verb+Become
3. +Verb+Caus+Pos
4. +Noun+PastPart+A3sg+P3sg+Loc
5. +Adj
The first shows the root word along with its part of speech, which is its only inflec-
tional feature. The second IG indicates a derivation into a verb whose semantics is
?to become? the preceding adjective. The +Become can be thought of as a minor part-
of-speech tag. The third IG indicates that a causative verb with positive polarity is
derived from the previous verb. The fourth IG indicates the derivation of a nominal
form, a past participle, with +Noun as the part of speech and +PastPart. It has has
other inflectional features: +A3sg for third-person singular, +P3sg for third-person sin-
gular possessive agreement, and +Loc for locative case. Finally the fifth IG indicates a
derivation into an adjective.
A sentence would then be represented as a sequence of the IGs making up the
words. An interesting observation that we can make about Turkish is that, when a
word is considered as a sequence of IGs, syntactic relation links emanate only from
the last IG of a (dependent) word and land on one of the IGs of a (head)word on the
2 Literally, ?(the thing existing) at the time we caused (something) to become strong?. Obviously this is
not a word that one would use everyday. Turkish words found in typical text average three to four
morphemes including the stem, with an average of about 1.7 derivations per word.
3 The morphological features other than the obvious part-of-speech features are +Become: become verb,
+Caus: causative verb, PastPart: derived past participle, P3sg: third-person singular possessive
agreement, A3sg: third-person singular number-person agreement, +Zero: zero derivation with no overt
morpheme, +Pnon: no possessive agreement, +Loc: locative case, +Pos: positive polarity.
521
Oflazer Dependency Parsing
Figure 2
Links and inflectional groups.
Figure 3
Dependency links in an example Turkish sentence.
right (with minor exceptions), as exemplified in Figure 2. A second observation is that,
with minor exceptions, the dependency links between the IGs, when drawn above the
IG sequence, do not cross.4 Figure 3 shows a dependency tree for a Turkish sentence
laid on top of the words segmented along IG boundaries. It should be noted that all
IGs that link to the same head IG comprise a constituent, and the legality of a link
depends primarily on the inflectional features of the IGs it connects.
For the purposes of this article we can summarize aspects of Turkish as follows:
? The IGs are the ?words.? That is, we treat a chunk of (free and bound)
inflectional morphemes as the units that we relate with dependency
links.
? Within a word, the IGs are linearly dependent on the next IG, if any. We
would not, however, show and deal with these explicitly, but rather deal
only with the dependency link emanating from the last IG in each word,
which is the syntactic head of the word (whereas the first IG which
contains the root is the semantic head).
4 Such cases would be violating the projectivity constraint. The only examples of such crossing that we
know are certain discontinuous noun phrases in which an adverbial modifier of the matrix verb
intervenes between a specifier and the rest of the noun phrase. Since the specifier links to the head
noun but the adverbial links to the verb, the links have to cross.
522
Computational Linguistics Volume 29, Number 4
? For all practical purposes the syntactic dependency links go from left to
right, that is, the core structure is subject-object-verb, and modifiers
precede their heads.
5. Finite-State Transducers
The exposition in the subsequent sections will make extensive use of concepts from
finite-state transducers. In this section, we provide a brief overview of the main rel-
evant concepts; the reader is referred to recent expositions (e.g., Roche and Schabes
[1997]; also, Hopcroft and Ullman [1979] provides a detailed exposition of finite-state
machines and regular languages.)
Finite-state transducers are finite-state devices with transitions labeled by pairs of
symbols (u:l), u denoting the ?upper? symbol and l denoting the ?lower? symbol.
These symbols come from a finite alphabet. Additionally, either u or l (but not both)
can be the  symbol, denoting the empty string. A finite-state transducer T maps
between two regular languages: U, the ?upper? language, and L, the ?lower? language.
The mapping is bidirectional, and in general, a string in one of the languages may
map to one or more strings in the other language. The transductions in both directions
are valid only if the string on the input side takes the finite-state transducer to a final
state.
The behavior of finite-state transducers can also be described using regular ex-
pressions over an alphabet of symbols of the form (u:l) (including symbols :l and
u:), in complete analogy to regular expressions for finite-state recognizers. Since the
notational mechanisms provided by the basic definition of regular expressions (con-
catenation, union, and Kleene star [Hopcroft and Ullman 1979]) are quite restricted and
low level, developers of finite-state transducer manipulation systems have augmented
the notational capabilities with operations at a much higher level of abstraction, much
closer to the operations used by the computational linguistics application (see, e.g.,
Karttunen et al, [1996]; see also http://www.xrce.xerox.com/competencies/content-
analysis/fsCompiler/fssyntax.html, and also van Noord, [1997]).
Finite-state transducers are closed under union, but in contrast to finite-state rec-
ognizers, they are not closed under difference and intersection operations (Kaplan and
Kay 1994). On the other hand, finite-state transducers are closed under the operation
of composition, which is very much an analog of function composition in algebra. Let
T1 be a transducer that maps between regular languages U1 and L1, and let T2 be a
transducer that maps between regular languages U2 and L2. The composition T of T1
and T2, denoted by T1 ? T2, is the transducer that maps between U = T?11 (L1 ? U2)
and L = T2(L1 ? U2).5 That is, the resulting mapping is defined only for the respective
images, in T?11 and T2, of the intersection L1 ? U2. A pair of strings (x, y) ? T1 ? T2 if
and only if ?z such that (x, z) ? T1 and (z, y) ? T2. Note that the composition operation
is order dependent; T1 ? T2 is not the same mapping as T2 ? T1. Figure 4 summarizes
the main points of the composition operation for finite-state transducers.
6. Finite-State Dependency Parsing
Our approach is based on constructing a graphic representation of the dependency
structure of a sentence, including the lexical items and the labeled directed arcs en-
5 Notationally, for a transducer T, we take T(U) to mean the transduction from the upper to the lower
language and T?1 to mean the transduction from the lower to the upper language.
523
Oflazer Dependency Parsing
Figure 4
Composition operation for finite-state transducers.
Figure 5
Physical representation and logical view of channels and dependency links.
coding the dependency relations. In constructing this dependency graph, these labeled
links are represented by additional symbols that are laid out within the symbols rep-
resenting the lexical items and their morphological features.
The approach relies on augmenting the input with ?channels? that (logically) re-
side above the IG sequence and ?laying? links representing dependency relations in
these channels, as depicted in Figure 5(a). The input to the parser is a representation of
the sentence as a sequence (or a lattice, if morphological disambiguation has not been
performed) of IGs with some additional symbols to delineate certain boundaries.6 The
6 The lattice of all morphological analyses of the words in a sentence can be encoded by a finite-state
acceptor. In fact, one gets such an acceptor when a nondeterministic morphological analysis finite-state
transducer is applied to the input. Further, finite-state acceptors are assumed to be coerced into identity
transducers that map the input strings they accept to identical output strings. This coercion is necessary
so that filters defined as acceptors can be used as transducers with the composition operators.
524
Computational Linguistics Volume 29, Number 4
Figure 6
Channel symbol slots around an inflectional group.
parser operates in a number of iterations: At each iteration of the parser, a new empty
channel is ?stacked? on ?top? of the input, and any possible links are established us-
ing these channels.7 Parsing terminates when no new links can be established within
the most recent channel added, that is, when a fixed point is reached. In this respect,
this approach is similar to that of Roche (1997). An abstract view of this is presented
in panels (a) through (c) of Figure 5.
6.1 Representing Channels and Syntactic Relations
The sequence (or the lattice) of IGs is produced by a morphological analysis transducer,
with each IG initially being augmented by two pairs of delimiter symbols, as <(IG)>.
The ( and ) pair separates the morphological features from the channel representation
symbols, while < and > separate the representations of consecutive IGs. Word-final IGs
(IGs from which links will emanate) are further augmented with a special marker @.
Channels are represented by pairs of matching symbols that are inserted between
the <. . . ( and the ). . . > delimiter symbols. Symbols for new channels (upper channels
in Figure 5) are stacked so that the symbols for the topmost channels are those closest to
the (. . . ), and in this way dependency links do not cross when drawn (see Figure 6).
At any time, the number of channel symbols on both sides of an IG is the same.
Multiple dependency links can occupy mutually exclusive segments of a channel as
long as they do not interfere with each other; that is, each channel may accommodate
many dependency links whenever possible.8
How a certain segment of channel is used is indicated by various symbols sur-
rounding the IGs, within the < and > delimiters:
? The channel symbol 0 indicates that the channel segment is not used by
any dependency link and thus is empty.
? The channel symbol 1 indicates that the channel is used by a link that
starts at some IG on the left and ends at some IG on the right. That is,
the link is just ?crossing over? this IG.
? When a link starts from a word-final IG, then a link start symbol is used
on the right side of the word-final IG (i.e., between ) and >).
? When a link terminates on an IG, then a link end symbol denoting the
syntactic relation is used on the left side of the IG (i.e., between < and ().
7 The beginnings and the ends of the arrows in the figure indicate the dependent and head IGs,
respectively.
8 The exposition here is for only left-to-right links. See section 6.5 for a dependency representation for
both left-to-right and right-to-left links.
525
Oflazer Dependency Parsing
The following syntactic relations are currently encoded in the channels:
1. Subject (s/S)
2. Object (o/O)
3. Modifier (adverbs/adjectives) (m/M) )
4. Possessor (p/P)
5. Classifier (c/C)
6. Determiner (d/D)
7. Dative adjunct (t/T)
8. Ablative adjunct (f/F)
9. Locative adjunct (l/L)
10. Instrumental adjunct (i/I)
The lowercase symbol in each case is used to indicate the start of a link, and the
uppercase symbols indicate the end of a link. Both kinds of symbols are used to
encode configurational and linguistic constraints on IGs, as we show later.
For instance, with three channels, the dependency structure of the IGs of bu eski
evdeki gu?lu?n (of the rose at this old house) in Figure 3 would be represented as
<000(bu+Det@)0d0><010(eski+Adj@)01m><MD0(ev+Noun+A3sg+Pnon+Loc)000>
<000(+Adj@)00m><M00(gu?l+Noun+A3sg+Pnon+Gen@)0p0>)
The M and the D to the left of the first IG of evdeki (third IG above) indicate the
incoming modifier and determiner links from the first two IGs, matching the start
symbols m and d in the second and the first IGs. The m--M pair encodes the modifier
link from eski (old) to evde (at house), and the d--D pair encodes the determiner link
from bu (this) to evde. The last IG above has an M on the left side matching the m in
the IG to the left. This m--M pair encodes the modifier relation between +ki and gu?lu?n
(of the rose). The last IG above has an outgoing possessor link marked by the p on
its right side, indicating that it is a genitive-marked possessor of some other IG to the
right.
We should note, however, that the (morphological) relations between IGs that
make up a single word are not at all a concern here and are not considered to be
syntactic dependency relations. Thus they are never explicitly shown or encoded ex-
cept by virtue of their being sequentially placed in the input. The only links that we
explicitly encode are those links emanating from a word-final IG and landing on some
other IG.
6.2 Components of the Parser
The basic strategy of a parser iteration is to recognize, by means of a rule (encoded
as a regular expression), a dependent IG and a head IG and link them by modifying
the ?topmost? channel between the two. Once we identify the dependent IG and the
head IG (in a manner to be described shortly), we proceed as follows:
1. We create an empty channel by injecting 0s to just outside of the (...)
pairs:
526
Computational Linguistics Volume 29, Number 4
<...0(IDdep@)0...>...<...0(IG)0...>...<...0(IGhead)0...>
2. We put temporary braces (of the type for the dependency link to be
established?we use MOD below for a modifier link for expository
purposes) to the right of the dependent IG and to the left of the head IG:
<...0(IGdep{MOD@)0...>...<...0(IG)0...>...<...0MOD}(IGhead)0...>
3. We mark the start, intermediate, and end IGs of the link with the
appropriate symbols encoding the relation thus established by the braces:
<...0(IGdep{MOD@)m...>...<...1(IG)1...>...<...MMOD}(IGhead)0...>
4. We remove the temporary braces, and we have the string in item 1, with
some symbols modified:
<...0(IGdep@)m...>...<...1(IG)1...>...<...M(IGhead)0...>
The second step above deserves some more attention, as it has additional functions
besides identifying the relevant IGs. One should be careful to avoid generating strings
that are either illegal or redundant or cannot lead to a valid parse at the end of the
iterations. Thus, the second step makes sure that
1. The last channel in the segment to be bracketed is free.
2. The dependent is not already linked at one of the lower channels (since
an IG can be the dependent of only one other IG).
3. None of the channels directly underneath the segment have any links
coming into or going out of the projection, in those channels, of the
segment bracketed. This makes sure that there are no crossing links. It is
obviously okay to have links that start and terminate in lower channels
within the projection of the bracketed segment.9
4. There are no channels below the current channel that are unused in the
segment to be bracketed (if there are, then this link could have been
made there.)
5. The link to be established does not trap an unlinked word-final IG. If
there is such an IG, its future link would have to cross the link to be
established in the current segment.
The last three of these constraints are depicted in Figure 7.
6.3 Rules for Establishing Dependency Links
The components of a dependency link are recognized using regular expressions. These
regular expressions identify the dependent IG, the head IG, and the IGs in between to
be skipped over, and they temporarily bracket the input segment including these IGs.
9 It is actually possible to place a crossing link by laying it in a special channel below the IG sequence so
that it would not interfere with the other links. This would necessitate additional delimiter symbols
and would unnecessarily further complicate the presentation.
527
Oflazer Dependency Parsing
Figure 7
Configurations to be avoided during parsing.
These regular expressions correspond to ?grammar rules,? and a collection of these
rules comprise the dependency grammar.
A typical rule looks like the following:10
[ LR [ML IGMiddle MR]* RL ] (->) "{Rel" ... "Rel}" || IGDep IGHead
This rule is an example of a XRCE optional-replace rule that nondeterministically
inserts the curly braces on the right-hand side (the symbols on both sides of the
ellipsis) into the output string in the lower language, around any part of the input
string in the upper language that matches its left-hand side, provided the left-hand
side is contextually constrained on the left by IGDep and on the right by IGHead. This
replace rule can nondeterministically make multiple nonoverlapping replacements.11
The left-hand side of this rule (to the left of (->)) has three components: The
first part, LR, specifies the constraints on the right-hand side of the dependent IG.
The second part, [ML IGMiddle MR]*, defines any middle IGs that will be ignored
10 We use the XRCE regular expression language syntax; the [ and ] act like parentheses used as
grouping operators in the language. See http://www.xrce.xerox.com/competencies/content-
analysis/fsCompiler/fssyntax.html for details.
11 An earlier implementation of the parser used a slightly different optional-replace rule that did not
make use of the contextual constraint, as the new format was not included in the toolkit available to
the author. A typical rule there looked like [[LL IGDep LR] [ML IGMiddle MR]* [RL IGHead RR]]
(->) "{Rel" ... "Rel}". Although for the purposes of writing the dependency grammar, the old rule
format was more transparent, its use necessitated some extra complexity in various other components
of the parser. The old rule format has been abandoned in favor of the new format. I thank an
anonymous reviewer for suggesting the use of this new rule format.
528
Computational Linguistics Volume 29, Number 4
and skipped over,12 and the third part, RL, specifies the contraints on the left-hand
side channel symbols of the head IG. The head and dependent IG patterns IGDep and
IGHead are specified as left and right contextual constraints on the pattern of the three
components specified on the left-hand side.
This rule (optionally) brackets (with {Rel and Rel}) any occurrence of pattern LR
[ML IGMiddle MR] RL provided the pattern IGDep is to the left of LR and the pattern
IGHead is to the right of RL.13 After the (optional) bracketing, the brace {Rel occurs
between IGDep and LR, and the brace Rel} occurs between RL and IGHead. Each rule
has its own brace symbol depending on the relationship of the dependent and the
head. The optionality is necessary because a given IG may be related to multiple IGs
as a result of syntactic ambiguities, and all such links have to be produced to arrive
at the final set of parses. It should also be noted that there are rules that deviate from
the template above in that the segment to be skipped may be missing, or may contain
barrier patterns that should not be skipped over, etc.
The symbols L(eft)R(ight), M(iddle)L, MR, and RL are regular expressions that
encode constraints on the bounding channel symbols that are used to enforce some of
the configurational constraints described earlier. Let
RightChannelSymbols = [ "1" | "0" | "s" | "o" | "m" | "p" |
"c" | "d" | "t" | "l" | "f" | "i" ];
and
LeftChannelSymbols = [ "1" | "0" | "S" | "O" | "M" | "P" |
"C" | "D" | "T" | "L" | "F" | "I" ];
These four regular expressions are defined as follows:
1. The regular expression LR = ["@" ")" "0" ["0"]* ">" ] checks that
a. The matching IG is a word-final IG (has a @ marker)
b. The right-side topmost channel is empty (channel symbol
nearest to ) is 0)
c. The IG is not linked to any other in any of the lower channels
d. No links in any of the lower channels cross into this segment
(that is, there are no 1s in lower channels.)
These conditions imply that the only channel symbol that may appear in
the right side of a dependent IG is 0.
2. The regular expression ML = ["<" LeftChannelSymbols* "0" "(" ]
ensures that the topmost channel is empty, but it does not constrain the
symbols in the lower channels, if any, as there may be other links ending
at the matching IG.
3. Similarly, the regular expression
MR = [ ")" "0" RightChannelSymbols* ">" ] also ensures that the
topmost channel is empty, but it does not constrain the symbols in the
lower channels, if any, as there may be other links starting at the
matching IG.
12 It is possible that the pattern to be skipped over can be specified by more complex patterns.
13 We use the symbols {Rel and Rel} as generic bracketing delimiters.
529
Oflazer Dependency Parsing
4. The regular expression
RL =[ "<" [LeftChannelSymbols* - $1] "0" "(" ] also ensures that
the topmost channel is empty. Note that since the matching IG is the IG
of the head, multiple dependency links may end at the matching IG, so
there are no constraints on the symbols in the lower channels, but there
cannot be any 1s on the left side, since that would imply a lower link
crossing to the right side.
For instance, the rule
[ LR [ ML AnyIG MR ]* RL ] (->) "{SBJ" ... "SBJ}" ||
NominativeNominalA3pl _ FiniteVerbA3sgA3pl;
is used to bracket a segment starting right after a plural nominative nominal, as subject
of a finite verb somewhere on the right, with either +A3sg or +A3pl number-person
agreement (allowed in Turkish). In this rule, the regular expression Nominative-
NominalA3pl is defined as follows:
[ (RootWord) ["+Noun" |"+Pron"] (NominalType) "+A3pl"
PossessiveAgreement "+Nom"]
and it matches any nominal IG (including any derived nominals) with nominative
case and +A3pl agreement. There are a number of points to note in this expression:
1. (. . . ) indicates optionality: The RootWord, a regular expression matching
a sequence of one or more characters in the Turkish alphabet, is optional,
since this may be a derived noun for which the root would be in a
previous IG.
2. NominalType, another optional component, is a regular expression
matching possible minor part-of-speech tags for nouns and pronouns.
3. PossessiveAgreement is a regular expression that matches all possible
possessive agreement markers.
4. The nominal has third-person plural agreement and nominative case.
The order of the components of this regular expression corresponds to the order of
the morphological feature symbols produced by the morphological analyzer for a
nominal IG. The regular expression FiniteVerbA3sgA3pl matches any finite-verb IG
with either +A3sg or +A3pl number-person agreement. The regular expression AnyIG
matches any IG.
All the rules in the dependency grammar written in the form described are grouped
together into a parallel bracketing regular expression defined as follows:
Bracket = [
[LR [ML IGMiddle1 MR]* RL] (->) "{Rel1" ... "Rel1}"
|| IGDep1 IGHead1,,
[LR [ML IGMiddle2 MR]* RL] (->) "{Rel2" ... "Rel2}"
|| IGDep2 IGHead2,,
. . .
[LR [ML IGMiddlen MR]* RL] (->) "{Reln" ... "Reln}"
|| IGDepn IGHeadn
];
530
Computational Linguistics Volume 29, Number 4
where left-hand-side patterns and dependent and head IGs are specified in accordance
with the rule format given earlier. {Reli and Reli} are pairs of braces; there is a distinct
pair for each syntactic relation to be identified by these rules (and not necessarily a
unique one for each rule). This set of rules will produce all possible bracketings of the
input IG sequence, subject to the constraints specified by the patterns. This overgen-
eration is then filtered (mostly at compile time and not at parse time) by a number of
additional configurational and linguistic constraints that are discussed shortly.
6.4 Constructing the Parsing Transducer
In this section, we describe the components of the parsing transducer. As stated earlier,
links are established in a number of iterations. Each iteration mainly consists of an
application of a parsing transducer followed by a filtering transducer that eliminates
certain redundant partial parse configurations.14
The parsing transducer consists of a transducer that inserts an empty channel
followed by transducers that implement steps 2 to 4 described at the beginning of
section 6.2.
We can write the following regular expression for the parser transducer as15
Parser = AddChannel
.o.
Bracket
.o.
FilterEmptySegments
.o.
MarkChannels
.o.
RemoveBraces;
The transducer AddChannel is a simple transducer that adds a pair of 0 channel sym-
bols around the (...) in the IGs. It implements step 1 in section 6.2. The transducer
Bracket was defined in the previous section. It implements step 2 described in sec-
tion 6.2.
Since the bracketing rules are nondeterministic, they will generate many config-
urations in which certain segments in the stacked channels will not be used. A rule
may attempt to establish a link in the topmost channel even though the correspond-
ing segment is not utilized in a previous channel (e.g., the corresponding segment of
one of the previous channels may be all 0s). One needs to eliminate such redundant
configurations after each iteration to prevent their proliferation at later iterations of
the parser. Checking whether the segment just underneath the topmost channel is
empty has worked perfectly in our experiments in that none of the parses selected
had any empty segments that were not detected by this test. The regular expression
FilterEmptySegments filters these configurations, an example of which is depicted in
Figure 7(b).16
14 We use the term configuration to denote an encoding of a (partial) dependency parse as a string of
symbols, as described in section 6.1.
15 The .o. operator denotes the composition operation (denoted using ? in section 5) for finite-state
transducers in the XRCE regular expression language.
16 Incorporating this configurational constraint into the bracketing phase implies that each rule will have
a check encoding the fact ?it is not the case that all symbols in the channel immediately below are 0,?
which makes all rules a bit awkward. Incorporating this as a separate postbracketing constraint is
simpler. Since all compositions are done at compile time, no additional penalty is incurred.
531
Oflazer Dependency Parsing
The transducer MarkChannels implements step 3 in section 6.2. This transducer
modifies the channel symbols to mark a link:
? The new (topmost) right channel symbol in the IG just to the right of the
opening brace is modified to a link start symbol (one of the symbols s,
o, m, p, c, d, t, l, f, i).
? The new (topmost) right channel symbols on both sides of all the IGs
fully bracketed by the braces (all IGs except the dependent and head
IGs) are modified to 1; this is necessary so that that segment of the
channel can be claimed and used for detecting crossing links.
? The new (topmost) left channel symbol in the IG just to the left of the
closing brace is modified to a link end symbol (one of the symbols S, O,
M, P, C, D, T, L, F, I).
Finally, the transducer RemoveBraces removes the braces.17 It should be noted that the
transducer for Parse is computed off-line at compile time, so that no composition is
done at parse time.
The parsing scheme described above is bottom up, in that the links between closest
head-dependent pairs are established first, in the lowest channel. Subsequent longer-
distance links are established in later stages as long as they do not conflict with links
established in lower channels. It is also conceivable that one could employ a top-down
parsing scheme linking all pairs that are far apart, again checking for configurational
constraints. If full nondeterminism is maintained in the bracketing step, it really does
not matter whether one uses bottom-up or top-down parsing. Bottom-up parsing,
however, offers certain advantages in that various (usually linguistically motivated)
constraints that have to hold between nearby pairs or pairs that have to be immediately
sequential can be enforced at an earlier stage by using simpler regular expressions.
These constraints help prune the intermediate parse strings.
6.5 Dependency Structures with Both Left-to-Right and Right-to-Left Links
Although the formulation up until now has been one for dependency structures in-
volving left-to-right dependency links, the approach presented above can handle a
dependency grammar with both left-to-right and right-to-left links. In this section, we
will outline the details of the changes that would be needed in such a formulation
but will then go ahead with the left-to-right implementation, as that forms the basis
of our implementation for Turkish, for which left-to-right links suffice for all practical
purposes.
Incorporating the right-to-left links into the approach would require the following
modifications to the formulation:
1. The right-to-left links would use the same representation as the
left-to-right links, except they would be distinguished by the symbols
marking the links at the dependent and head IG sites. With the
left-to-right links described so far, lowercase link symbols on the right
side of an IG mark the dependent IG and uppercase symbols on the left
17 The details of the regular expressions for these transducers are rather uninteresting. They are
essentially upper-side to lower-side contextual-replace regular expressions. For instance, RemoveBraces
maps all brace symbols on the upper side to  on the lower side.
532
Computational Linguistics Volume 29, Number 4
Figure 8
Bidirectional dependency links.
side of the IG mark the head IG (which follows the dependent IG in
linear order). We would still use the same conventions for right-to-left
links, except that we could have head and dependent IG markers on
both sides of the channel representation. This is shown graphically in
Figure 8. So a right-to-left link would have a lowercase link mark on the
left side of the dependent IG and an uppercase link mark on the right
side of the head IG to the left of the dependent IG.
2. With both left-to-right and right-to-left rules, we would need two
different rule formats. The rule format for left-to-right links would be
slightly different from the format given earlier:
[ LRl [ML IGMiddle MR]* RLl ] (->) "{Rel-left-to-right"
... "Rel-left-to-right}"
|| LL IGDep IGHead The
rule format for right-to-left links would be:
[ LRr [ML IGMiddle MR]* RLr ] (->) "{Rel-right-to-left"
... "Rel-right-to-left}"
|| IGHead IGDep RR
3. Since nothing in the format of the rules indicates the direction of the
link, the direction would need to be indicated by the type of braces that
are used to (temporarily) mark the segment claimed for the link. For
instance, for a left-to-right rule to link a subject IG to a verb IG, we
would use braces {SBJ-left-to-right and SBJ-left-to-right} and for
a right-to-left rule (for the same kind of relation), we would use symbols
{SBJ-right-to-left and SBJ-right-to-left}. The transducer that
inserts the appropriate markers for links (MarkChannels in section 6.4)
would then execute the appropriate action based on the type and the
direction indication of the delimiting braces. For left-to-right braces it
will insert the (lowercase) link start symbol to the right side of the left
brace and the (uppercase) link end symbol to the left side of the right
brace. For right-to-left braces, it will insert the link start symbol to the
left side of the right brace and the link end symbol to the right side of
the left brace.
4. The regular expressions checking the channel symbols around the
dependent and head IGs would be different for the two types of rules.
This is basically necessitated by the fact that since the IGs could now
have links outgoing from both sides, checks have to be made on both
sides:
? LRl in left-to-right rules would check that the dependent IG is a
word-final IG and is not already linked and that no links are
533
Oflazer Dependency Parsing
crossing in or out. So it would function like LR, described in
section 6.3.
? LL, just to the left of the IGDep pattern, would also make sure
that the IG is not linked, via a right-to-left link, to an IG further
to the left.
? RLl would function just like RL, described in section 6.3.
? LRr, which, for right-to-left rules, would be constraining the left
channel symbols of the head IG, would need only to ensure that
the top channel is available for a link and that no other links are
crossing in and out.
? RLr would ensure that the dependent IG is not linked to any IG
to the left and that there are no links crossing, and that the top
channel is available.
? RR, just to the right of IGDep in the right-to-left rule, would
make sure that the dependent IG is not linked to any IG to the
right and would additionally check that the top channel is
available and that no links are crossing.
There is, however, a potential problem for a grammar with both left-to-right and
right-to-left links. Robinson?s axioms (see section 3) do not seem to disallow cyclic
dependency links (unless the antisymmetry is interpreted to apply over the transitive
closure of the ?depends on? relationship), but configurations involving cycles are not
assumed to correspond to legitimate dependency structures.
When both left-to-right and right-to-left links exists in a grammar, it is conceivable
that two left-to-right rules may separately posit two left-to-right links, so that IG A
links to IG B, IG B (or the word-final IG of the word to which B belongs) links to IG
C, and later in a subsequent iteration, a right-to-left rule posits a link from IG C (or
the word-final IG of the word to which C belongs) to IG A, where IG A precedes IG B,
which precedes IG C in linear order. An implementation for a grammar would have
to recognize such circular structures and eliminate them. It is possible to filter some of
these cyclic configurations using a finite-state filter, but some will have to be checked
later by a non-finite-state procedure.
If all but one of the links forming a cycle are established in the same channel (e.g.,
following the example above, the links from IG A to IG B and from IG B to IG C
are established in the next-to-the-topmost channel), the cycle-forming link has to be
established in the (current) topmost channel (that is, the right-to-left link from IG C
to IG A has to be established there; otherwise the configuration will be filtered by the
rule that says links have to established as the earliest possible channel). In order for
a cycle to form in this case, IGs A, B, and C with have to be in sequential words, and
the cycle-inducing link and the other links in the ?other? direction will all be side by
side. A set of simple regular expressions can recognize if a series of pairs of link start
and link end symbols in one direction all appearing in the next-to-top channel (i.e.,
the second symbol to the left and right of ( and ), respectively, are surrounded by a
link end?link start pair for the cycle-inducing link in the other direction) and kill any
such configurations.
If, however, cycles are induced by links appearing in more than two different chan-
nels, then there is no elegant way of recognizing these in the finite-state framework,
and such cases would have to be checked through other means.
534
Computational Linguistics Volume 29, Number 4
6.6 Iterative Application of the Parser
Full parsing consists of iterative applications of the Parser transducer until a fixed
point is reached. It should be noted that in general, different dependency parses of a
sentence may use different numbers of channels, so all these parses have to be collected
during each iteration.
Let Sentence be a transducer that represents the word sequence. The pseudocode
for iterative applications of the parser is given as follows:
# Map sentence to a transducer representing a lattice of IGs
M = [Sentence .o. MorphologicalAnalyzer];
# Initialize Parses
Parses = { };
i = 0;
while (M.l != { } && i < MaxIterations) {
# Parse and filter the current M
X = M .o. Parse .o. SyntacticFilter;
# Extract any configurations which correspond to parses
Partial = X .o. OnlyOneUnlinked;
# and union with the Parses transducer
Parses = [Parses | Partial];
# filter any stale configurations
M = [X - Partial] .o. TopChannelNotEmpty;
i = i+1;
}
Leaving the details of the transducer SyntacticFilter, a filter that eliminates config-
urations violating various linguistically motivated constraints, to a later section, this
pseudocode works as follows: First, the sentence coded in Sentence is composed with
the MorphologicalAnalyzer, which performs full morphological analysis of the tokens
in the sentence along with some very conservative local morphological disambigua-
tion. The resulting transducer encodes the sentence as a lattice representing all relevant
morphological ambiguities. It is also possible to disambiguate the sentence prior to
parsing with a tagger and present the parser with a fully disambiguated sentence.
During each iteration, M encodes as a transducer, the valid partial-dependency con-
figurations. First X is computed by applying the Parse and SyntacticFilter trans-
ducers, in that order, to M. At this point, there may be some complete parses, that is,
configurations that have all except one of their word-final IGs linked (e.g., a parse in
which every IG is linked to the next IG would use only the first channel, and such
a parse would be generated right after the first iteration.) The transducer X encoding
the result of one iteration of parsing is filtered by OnlyOneUnlinked, defined as
OnlyOneUnlinked = ~[[ $[ "<" LeftChannelSymbols*
"(" AnyIG "@" ")"
["0" | 1]* ">" ]]^ > 1 ];
This would be read as ?It is not the case that there is more than one instance of word-
final IGs whose right channel symbols do not contain any outgoing link marker.?18
This filter lets only those configurations that have all their required links established,
18 Note that this constraint is for a grammmar with left-to-right links.
535
Oflazer Dependency Parsing
that is, all word-final IGs, except one, are linked (only one word-final IG has all of its
right channel symbols as 0s and 1s.) Any such parses in Partial are unioned with
Parses (initially empty) and removed from X to give the M for the next iteration. Any
configurations among the remaining ones (with no links in the most recently added
channel, because of optionality in bracketing) are filtered, since these will violate the
empty-channel constraint (see Figure 7(b)). This is achieved by means of composition,
with the transducer TopChannelNotEmpty defined as follows:
TopChannelNotEmpty = ~[ ["<" LeftChannelSymbols* "0"
"(" AnyIG ("@") ")" "0"
RightChannelSymbols* ">" ] *] ;
This filter would be read as ?It is not the case that all topmost channel symbols in a
configuration are all 0s.? Thus configurations in which all most recent channel symbols
are 0 are filtered. If the lower language of M (denoted by M.l) becomes empty at this
point (or we exceed the number of maximum number of iterations), the iteration exits,
with Parses containing the relevant result configurations. MaxIterations is typically
small. In the worst case, the number of iterations one would need would equal the
number of word-final IGs, but in our experiments parsing has converged in five or six
iterations, and we have used eight as the maximum.
6.7 Handling Coordinating Conjunctions
Headless constructions such as coordinating conjunctions have been one of the weaker
points of dependency grammar approaches. Our implementation of coordinate con-
junction constructs essentially follows the formulation of Ja?rvinen and Tapanainen
(1998). For a sequence of IGs like
D1 . . .C . . .D2 . . .C . . . . . .Dk . . .H
where Di are the dependent IGs that are coordinated and C represents the conjunction
IGs (for, (comma), and, and or), and H is the head IG, we effectively thread a ?long
link? (possibly spanning multiple channels) from D1 to H. If the link between Dk and
H is labeled L, then dependent Di links to the following C with link L, and this C links
to Di+1 with L. This is conceptually equivalent to the following: The ?logical? link
with label L from conjoined dependent X and Y to their head Z is implemented with
three actual links of type L: X?and, and?Y, and Y?Z. If there are additional conjunctions
and conjuncts, we continue to add (as required) one link of type L per word: Linking
conjoined dependents (W and X and Y) to Z is implemented with links W?and, and?X,
X?and, and?Y, and Y?Z.
One feature of Turkish simplifies this threading a bit: The left conjunct IG has to
immediately precede the conjunction IG. The rules that do not involve conjunctions
establish the link between Dk and H. For each such rule, we need two simple rules: The
first rule links a dependent Di, (i < k), to the conjunction immediately following. Since
the link type is almost always determined by the inflectional features of the dependent,
this linking can be done (ambiguously in a very few cases in which dependent features
do not uniquely determine the link type). The second rule links the conjunction to the
right conjunct. Note that this applies only to conjunct IGs that have already been
linked to from their left conjunct. Since the outermost link symbol on the left side of a
conjunction IG identifies the relation (because the left conjunct is immediately to the
left of this IG), the link emanating from the conjunction to the right can be made to
land on an IG that agrees with the left conjunct in relevant features.
536
Computational Linguistics Volume 29, Number 4
Figure 9
Link configurations for conjunction ambiguity.
When we have two groups of conjoined constructs
D1 C . . .D2 C . . .Dk . . .H1 C . . .H2 C . . .Hl
the rightmost conjunct of the first group, Dk, will alternately attach to H1, H2, . . . , Hl.
In the first case, the complete first conjunction group links Dk to H1, but not to the rest.
In the second case, the complete first conjunction group links to the conjunction of H1
and H2, which are then conjoined with H3 through Hl. In the last case, the complete
first group links to the complete conjunction of H1, H2, . . . , to Hl. In all cases, the
links from H1 all the way to Hl are independently threaded. A number of additional
constraints also filter situations in which a conjoined head has both conjoined and
locally attached dependents of the same type, by checking that the left channel symbols
for these are not interleaved with other symbols. Figure 9 provides an example for
this kind of conjunction ambiguity. In this implementation we have not attempted to
handle circumscribing conjunctions such as the equivalents of either . . . or.
6.8 Enforcing Syntactic Constraints
The rules linking the IGs are overgenerating in that they may generate configurations
that may violate some general or language-specific constraints. For instance, more than
one subject or one object may attach to a verb, more than one determiner or possessor
may attach to a nominal, an object may attach to a verb that is then passivized in the
next IG, or a nominative personal pronoun may be linked as a direct object (which is
not possible in Turkish).
Some of the constraints preventing these configurations can be encoded in the
bracketing rule patterns. For instance, a rule for linking a nominal IG to a verb IG
as a subject may check, using a suitable regular expression, the left-hand channel
symbols of the verb IG to make sure that it does not already contain an incoming
subject link. There are also a number of situations in which the determination of a link
depends on a pattern that is outside the sequence of the IGs from dependent to the
head IG specified in a bracketing rule (but nevertheless in the same word in which
the head IG is located). For instance, in Turkish, present participles are considered
537
Oflazer Dependency Parsing
modifiers derived from verbs. The verb part is the head of the sentential clause with
a subject gap. Thus if a nominal IG attaches to a verb IG as a subject, but the verb
IG is followed by another IG indicating that it is a present participle, then we should
kill this configuration, since such verbs are not allowed to have subjects. It is also
possible to incorporate almost all lexicalized argument structure?related constraints
for dealing with intransitive and transitive verbs, provided the lexicon component
(the morphological analyzer in our case) produces such lexically determined features.
We have chosen not to encode such constraints in the general format of the rules
and to implement them instead as filters that eliminate configurations produced by
the parsing. We have observed that this makes the linking rules more perspicuous and
easier to maintain.
Each constraint is implemented as a finite-state filter that operates on the outputs
of the Parse transducer by checking the symbols denoting the relations. For instance,
we can define the following regular expression for filtering out configurations in which
two determiners are attached to the same IG:
AtMostOneDeterminer =
[ "<" [ ~[[$"D"]^>1] & LeftChannelSymbols* ] "(" AnyIG ("@") ")"
RightChannelSymbols+ ">" ]*;
This regular expression constrains the form of the configurations generated by parsing.
Note that this transducer lets through a sequence of zero or more IGs, none of which
have more than one D symbol (indicating an incoming determiner link) among the
left channel symbols. The crucial portion at the beginning of the regular expression
says: ?For any IG, it is not the case that there is more than one substring containing
D among the left channel symbols of that IG (that is, the intersection of the symbols
between < and ( with LeftChannelSymbols does not contain more than one D).?
We can provide the following finite-state filter as an example in which the violat-
ing configurations can be found by checking IGs following the head IG. For instance,
the configurations in which subjects are linked to verbs which are then derived into
present participles would be filtered by a finite-state filter like
NoSubjectForPresentPart = ~$[ "<" $["S"] & LeftChannelSymbols*
"(" Verb ")"
RightChannelSymbols* ">"
"<" LeftChannelSymbols*
"(" PresentParticipleIG ("@") ")"
RightChannelSymbols* ">" ]
which says that the configuration does not contain, among the left-side channel sym-
bols, a verb IG with a subject marker followed by a present participle IG.
The following are examples of the constraints that we have encoded as finite-state
filters:
? At most one subject can link to a verb.
? At most one direct object can link to a verb.
? At most one dative (locative, ablative, instrumental) adjunct can link to a
verb.
? Finite verbs derived from nouns and adjectives with zero suffixes do not
have objects (as they are the equivalents of be verbs).
538
Computational Linguistics Volume 29, Number 4
? Reflexive verbs do not get any overt objects.
? A postposition must always have an object to its immediate left (hence
such a dependent nominal cannot link to some other head.)19
All syntactic constraints can be formulated similar to those given in the list. All such
constraints Cons1, Cons2 . . . ConsN can then be composed to give one transducer that
enforces all of these:
SyntacticFilter = [ Cons1 .o. Cons2 .o. Cons3 .o. ... .o. ConsN]
In the current implementation we use a total of 28 such constraints.
6.9 Robust Parsing
It is possible that either because of grammar coverage, or because of ungrammati-
cal input, a parse with only one unlinked word-final IG may not be found. In such
cases, Parses in the pseudocode for parsing presented in section 6.6 would be empty.
One might, however, opt to accept parses with k > 1 unlinked word-final IGs when
there are no parses with < k unlinked word-final IGs (for some small k). This can
be achieved by using Karttunen?s lenient composition operator (Karttunen 1998). Le-
nient composition, notated as .O., is used with a generator?filter combination. When
a generator transducer, G, is leniently composed with a filter transducer, F, the result-
ing transducer, G .O. F, has the following behavior when an input is applied: If any
of the outputs of G in response to an input string satisfy the filter F, then G .O. F
produces just these as output. Otherwise, G .O. F outputs what G outputs.
Let Unlinked i denote a regular expression that accepts parse configurations with
no more than i unlinked word-final IGs. For instance, for i = 2, this would be defined
as follows:
Unlinked_2 = ~[[$[ "<" LeftChannelSymbols* "(" AnyIG "@" ")"
["0" | 1]* ">"]]^ > 2 ];
which rejects configurations having more than two word-final IGs whose right channel
symbols contain only 0s and 1s (i.e., they do not link to some other IG as a dependent).
We can augment the pseudocode given in section 6.6 as follows:
if (Parses == { }) {
PartialParses = M .O. Unlinked_1 .O. Unlinked_2 .O. Unlinked_3;
}
This will have the parser produce outputs with up to three unlinked word-final IGs
when there are no outputs with a smaller number of unlinked word-final IGs. Thus, it is
possible to recover some of the partial-dependency structures when a full-dependency
structure is not available for some reason. The caveat would be, however, that since
Unlinked 1 is a very strong constraint, any relaxation would increase the number of
outputs substantially. We have used this approach quite productively during the de-
velopment of the dependency linking rules to discover coverage gaps in our grammar.
19 In fact, the morphological analyzer produces, for each postposition, a marker denoting the case of the
preceding nominal as a subcategorization feature. This is used in a semilexicalized fashion while
linking nominals to their head postpositions.
539
Oflazer Dependency Parsing
7. Experiments with Dependency Parsing of Turkish
Our implementation work has mainly consisted of developing and implementing the
representation and finite-state techniques involved here, along with a nontrivial gram-
mar component; we have not attempted to build a wide-coverage parser that is ex-
pected to work on an arbitrary test corpus. Although we have built the grammar
component manually using a very small set of sentences, it is conceivable that fu-
ture work on inducing (possibly statistical) dependency grammars will exploit de-
pendency treebanks, which are slowly becoming available (Hajic? 1998; Oflazer et al
2003).
The grammar has two major components. The morphological analyzer is a full-
coverage analyzer built using XRCE finite-state tools, slightly modified to generate
outputs as a sequence of IGs for a sequence of words. When an input sentence (again
represented as a transducer denoting a sequence of words) is composed with the mor-
phological analyzer (see the pseudocode given in section 6.6), a transducer for the
lattice representing all IGs for all morphological ambiguities (remaining after a light
morphological disambiguation) is generated. The dependency relations are described
by a set of about 60 rules much like the ones exemplified earlier. These rules were
developed using a small set of 30 sentences. The rules were almost all nonlexical,
establishing links of the types listed earlier. There is an additional set of 28 finite-
state constraints that impose various syntactic and structural constraints. The resulting
Parser transducer has 13,290 states and 186,270 transitions, and the SyntacticFilter
transducer has 3,800 states and 134,491 transitions. The combined transducer for mor-
phological analysis and (very limited) disambiguation has 100,103 states and 243,533
arcs.
The dependency grammar and the finite-state dependency parser were tested on
a set of 200 Turkish sentences, including the 30 that were used for developing and
testing the grammar. These sentences had 4 to 43 words, with an average of about
18 words. Table 1 presents our results for parsing this set of 200 sentences. This table
presents the minimum, the maximum, and the average of the number of words and
IGs per sentence, the number of parser iterations and the number of parses generated.
(The number of iterations includes the last iteration where no new links are added.)
There were 22 sentences among the 200 that had quite a number of verbal adjuncts
that function as modifiers. These freely attach to any verb IG, creating an analog of
the PP attachment problem and giving rise to a very large number of parses. The last
row in the table gives the minimum, maximum and the average number of parses
when such sentences were not considered.
To impose a ranking on the parses generated based on just structural properties
of the dependency tree, we employed Lin?s (1996) notion of structural complexity. We
measured the total link length (TLL) in a dependency parse counting the IGs the links
pass over in the linear representation and ordered the dependency parses based on
the TLL of the dependency tree. We classified the sentences into six groups:
1. Sentences that had a single minimum TLL parse which was correct. There were
a total of 39 sentences (19.5%) in this group.
2. Sentences that had more than one parse with the same minimum TLL and the
correct parse was among these parses. There were 58 sentences (29.0%) in this
group. Thus for a total of 97 (48.5%) sentences, the correct parse was
found among the parses with the minimum TLL. In these cases the
average number of parses with the minimum TLL was about 6
540
Computational Linguistics Volume 29, Number 4
(minimum 1 parse and maximum 38 parses with the same minimum
TLL).
3. Sentences for which the correct parse was not among the minimum TLL parses but
was among the next-largest TLL group. There were 29 (14.5%) sentences in
this group.
4. Sentences for which the correct parse was not among the smallest and the
next-smallest TLL groups, but among the next three smallest TLL groups. There
were a total of 26 (13%) sentences in this group.
5. Sentences for which the parser generated parses, but the correct parse was not
among the first five groups. There were 26 (13%) such sentences. For these,
we did not check any further and assumed there were no correct parses.
The parses that were generated usually used other (morphological)
ambiguities of the lexical item to arrive at a parse.
6. Sentences for which no parses could be found, usually as a result of the lack of
coverage of the dependency grammar and the morphological analyzer. There
were 22 (11%) sentences in this group.
It seems that for quite a number of sentences (groups 1?3 in the list), a relatively
small number of parses have to be processed further with any additional lexical and/or
statistical constraints to extract the correct parse. Although to obtain the statistics in
items 1?6, we had to extract the full set of parse strings from the transducer that
encoded the parses compactly, one does not have to do this. The parses with the
shortest link length can be found by treating the resulting parse lattice transducer as a
directed acyclic graph and finding the path with the minimum number of 1 symbols on
it from the start state node to the final state node using one of the standard shortest-
path algorithms (e.g., Dijsktra?s algorithm [Cormen, Leiserson, and Rivest 1990]).20
This is because paths from the start state to the final state are string encodings of the
dependency trees. The 1 symbols in the representation add up to the total link length of
the encoded dependency tree. Since the representation of the tree is quite convoluted,
the 1s in a block of 1s in the string representation all belong to different links stacked
on top of each other. Thus we ?count? the length of the links in an ?interleaved?
fashion. On the other hand, Dijkstra?s algorithm may not be very useful, since one
may need to extract the k shortest paths to select from, perhaps, later, with more-
informed criteria than link length, such as lexical and statistical information. For this
we may use an algorithm which finds the k shortest paths between a source and a
sink node in a directed graph (e.g., Eppstein 1998).
The complete parser, including about 60 linking rules and the 28 syntactic con-
straints, is defined using about 240 regular expressions coded using XRCE regular
expression language. These regular expressions compile in about one minute on Pen-
tium III 700 MHz (running Linux) into the Parser and SyntacticFilter transducers.
The parser iterations are handled by a script interpreted by the XRCE finite state tool,
xfst.
Parsing takes about a second per sentence, including lookup in the morphological
analyzer, which is performed with a composition. With manually completely morpho-
logically disambiguated input, parsing is essentially instantaneous.21
20 I thank an anonymous reviewer for suggesting this.
21 We performed a simple experiment with 14 sentences that were manually morphologically
541
Oflazer Dependency Parsing
Table 1
Statistics from parsing 200 Turkish sentences.
Minimum Maximum Average
Words/sentence 4 43 18.2
IGs/sentence 4 59 22.5
Parser iterations 3 8 5.0
Parses/sentence 1 12,400 408.7
Parses/sentence 1 285 55.4
excluding the 22 sentences
with > 1,000 parses
Input Sentence: Du?nya Bankas? Tu?rkiye Direkto?ru? hu?ku?metin izledig?i ekonomik pro-
gram?n sonucunda o?nemli ad?mlar?n at?ld?g??n? so?yledi.
English: The World Bank Turkey director said that as a result of the economic program
followed by the government, important steps were taken.
Parser output after three iterations:
Parse1:
<000(dUnya+Noun+A3sg+Pnon+Nom@)00c><C00(banka+Noun+A3sg+P3sg+Nom@)0c0>
<010(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)01c><CC0(direktOr+Noun+A3sg+P3sg+Nom@)s00>
<001(hUkUmet+Noun+A3sg+Pnon+Gen@)10s><S01(izle+Verb+Pos)100><001(+Adj+PastPart+P3sg@)1m0>
<011(ekonomik+Adj@)11m><MM1(program+Noun+A3sg+Pnon+Gen@)10p>
<P01(sonuC+Noun+A3sg+P3sg+Loc@)1l0>
<011(Onem+Noun)110><011(+Adj+With@)11m><M11(adIm+Noun+A3pl+Pnon+Gen@)11s>
<S11(at+Verb)110><011(+Verb+Pass+Pos)110><011(+Noun+PastPart+A3sg+P3sg+Acc@)11o>
<OLS(sOyle+Verb+Pos+Past+A3sg@)000>
Parse2:
<000(dUnya+Noun+A3sg+Pnon+Nom@)00c><C00(banka+Noun+A3sg+P3sg+Nom@)0c0>
<010(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)01c><CC0(direktOr+Noun+A3sg+P3sg+Nom@)s00>
<001(hUkUmet+Noun+A3sg+Pnon+Gen@)10s><S01(izle+Verb+Pos)100><001(+Adj+PastPart+P3sg@)1m0>
<011(ekonomik+Adj@)11m><MM1(program+Noun+A3sg+Pnon+Gen@)10p>
<P01(sonuC+Noun+A3sg+P3sg+Loc@)1l0>
<011(Onem+Noun)110><011(+Adj+With@)11m><M11(adIm+Noun+A3pl+Pnon+Gen@)11s>
<SL1(at+Verb)100><001(+Verb+Pass+Pos)100><001(+Noun+PastPart+A3sg+P3sg+Acc@)10o>
<O0S(sOyle+Verb+Pos+Past+A3sg@)000>small
Figure 10
Sample input and output of the parser. The only difference in the two parses is in the locative
adjunct attachment (to verbs at and so?yle). The IGs that differ in the two parses are
<S11(at+Verb)110> versus <SL1(at+Verb)100>, and <OLS(sOyle+Verb+Pos+Past+A3sg@)000>
versus <O0S(sOyle+Verb+Pos+Past+A3sg@)000>.
Figure 10 presents the input and the output of the parser for a sample Turkish
sentence: Du?nya Bankas? Tu?rkiye Direkto?ru? hu?ku?metin izledig?i ekonomik program?n sonucunda
o?nemli ad?mlar?n at?ld?g??n? so?yledi. (The World Bank Turkey director said that as a result
of the economic program followed by the government, important steps were taken.)
Figure 11 shows the output of the parser processed with a Perl script to provide a
more human-readable presentation:
disambiguated. For this set of sentences, there were about 7 parses per sentence. The average number
of parses for these sentences when all their morphological ambiguities were considered was 15. When
the two sentences with the highest number of parses were removed from this set, the corresponding
numbers were 3 parses per sentence and 11 parses per sentence.
542
Computational Linguistics Volume 29, Number 4
s----------------------------------------------------
c---------------C s m---------------M
c---C c c---CC s s---S m m--MM p-
...... ...... ........ ......... ........ ..... ........ ......... ........
dUnya banka tUrkiye direktOr hUkUmet izle ekonomik program
Noun Noun Noun Noun Noun Verb Adj Adj@ Noun
A3sg A3sg Prop A3sg A3sg Pos PastPart A3sg
Pnon P3sg A3sg P3sg Pnon P3sg@ Pnon
Nom@ Nom@ Pnon Nom@ Gen@ Gen@
Nom@
-----------------------------------------------------------------------------------S
l-----------------------------------------L S
--P l m---M s---SL o---O S
......... ......... ......... ......... ....... ...... ......... ......
sonuC Onem adIm at sOyle
Noun Noun Adj Noun Verb Verb Noun Verb
A3sg With@ A3pl Pass PastPart Pos
P3sg Pnon Pos A3sg Past
Loc@ Gen@ P3sg A3sg@
Acc@
Figure 11
Dependency tree for the second parse.
8. Discussion and Conclusions
We have presented the architecture and implementation of a dependency parser using
an extended finite state. Although the emphasis has been on the description of the
approach, we have developed a dependency grammar for Turkish and have used it to
experiment with a small sample of 200 Turkish sentences. We have also employed a
scheme for ranking dependency parses using the total link length of the dependency
trees, as originally suggested by Lin (1996), with quite promising results. It is possible
to use algorithms for extracting k shortest paths to extract parses from the transducer,
which compactly encodes all dependency parses, and further to rank a much smaller
set of parses using lexical and statistical information whenever available.
Another interesting point that we have noted, especially during the development
of the grammar, is that the grammar rules do not have to pay any real attention to the
sequence of the IGs that do not have anything to do with the current rule (with a very
few exceptions in some special cases in which the rules have to check that links do
not cross a ?barrier?). This means that that the grammar of the IG sequence is really
localized to the morphological analyzer and that for the most part the dependency
grammar does not have to ?know? about the sequencing of the IGs within a word.
In addition to the reductionistic disambiguator that we have used just prior to
parsing, we have implemented a number of heuristics to limit the number of poten-
tially spurious configurations that result from optionality in bracketing, mainly by
enforcing obligatory bracketing for sequential dependent-head pairs (e.g., the comple-
ment of a postposition is immediately before it, or for conjunctions, the left conjunct
is always the previous IG). Such heuristics force such dependencies to appear in the
first channel and hence prune many potentially useless configurations popping up in
later iterations. Although we have not performed any significant experiments with the
robust parsing technique that we describe in the article, it has been very instrumental
during the process of debugging the grammar. During debugging, when the actual
parser did not deliver any results after a certain number of iterations, we generated
partial parses with up to four unlinked word-final IGs to see where we were having
problems with the coverage and added new linking rules.
543
Oflazer Dependency Parsing
Acknowledgments
This work was partially supported by grant
EEEAG-199E027 from TU?BI?TAK (The
Turkish Council on Scientific and Technical
Research). A portion of this work was done
while the author was visiting the
Computing Research Laboratory at New
Mexico State University. The author thanks
Lauri Karttunen of Xerox PARC for making
available XRCE finite-state tools. Mercan
Karahan, currently of Purdue University,
helped substantially with the
implementation of the parser and with the
experimentation. Comments by anonymous
reviewers helped substantially to improve
the article.
References
Abney, Steven. 1996. Partial parsing via
finite-state cascades. Journal of Natural
Language Engineering, 2(4):337?344.
Ait-Mokhtar, Salah and Jean-Pierre Chanod.
1997. Incremental finite-state parsing. In
Proceedings of ANLP?97, pages 72?79, April.
Black, Alan. 1989. Finite state machines
from feature grammars. In Proceedings of
International Workshop on Parsing
Technologies, pages 277?285.
Chanod, Jean-Pierre and Pasi Tapanainen.
1996. A robust finite-state grammar for
French. In John Carroll and Ted Briscoe,
editors, Proceedings of the ESSLLI?96
Workshop on Robust Parsing, pages 16?25,
August.
Chelba, Ciprian, David Engle, Frederick
Jelinek, Victor Jimenez, Sanjeev
Khudanpur, Lidia Mangu, Harry Printz,
Eric Ristad, Ronald Rosenfeld, Andreas
Stolcke, and Dekai Wu. 1997. Structure
and estimation of a dependency language
model. In Processings of Eurospeech?97.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184?191.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillman. 1999. A statistical
parser for Czech. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 505?512,
June.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge, MA.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics (COLING-96), pages 340?345,
August.
Elworthy, David. 2000. A finite state parser
with dependency structure output. In
Proceedings of International Workshop on
Parsing Technologies.
Eppstein, David. 1998. Finding k-shortest
paths. Siam Journal on Computing,
28(2):652?673.
Giguet, Emmanuel and Jacques Vergne.
1997. From part-of-speech tagging to
memory-based deep syntactic analysis. In
Proceedings of the International Workshop on
Parsing Technologies, pages 77?88.
Grefenstette, Gregory. 1996. Light parsing as
finite-state filtering. In ECAI ?96 Workshop
on Extended Finite State Models of Language,
August.
Grimley-Evans, Edmund. 1997.
Approximating context-free grammars
with a finite-state calculus. In Proceedings
of ACL-EACL?97, pages 452?459.
Hajic?, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In Eva Hajicova,
editor, Issues in Valency and Meaning:
Studies in Honour of Jarmila Panenova.
Karolinum?Charles University Press,
Prague, pages 106?132.
Hankamer, Jorge. 1989. Morphological
parsing and the lexicon. In
W. Marslen-Wilson, editor, Lexical
Representation and Process. MIT Press,
Cambridge, MA, pages 392?408.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded finite state transducer for
extracting information from natural
language text. In Emmanuel Roche and
Yves Schabes, editors, Finite State Language
Processing. MIT Press, Cambridge, MA,
pages 386?406.
Hopcroft, John E. and Jeffrey D. Ullman.
1979. Introduction to Automata Theory,
Languages, and Computation.
Addison-Wesley, Reading, MA.
Ja?rvinen, Timo and Pasi Tapanainen. 1998.
Towards an implementable dependency
grammar. In Proceedings of
COLING/ACL?98 Workshop on Processing
Dependency-Based Grammars, pages 1?10.
Johnson, Mark. 1998. Finite state
approximation of constraint-based
grammars using left-corner grammar
transforms. In Proceedings of
COLING-ACL?98, pages 619?623, August.
Kaplan, Ronald M. and Martin Kay. 1994. Reg-
ular models of phonological rule systems.
Computational Linguistics, 20(3):331?378.
544
Computational Linguistics Volume 29, Number 4
Karttunen, Lauri. 1998. The proper
treatment of optimality theory in
computational linguistics. In Lauri
Karttunen and Kemal Oflazer, editors,
Proceedings of the International Workshop on
Finite State Methods in Natural Language
Processing (FSMNLP), June.
Karttunen, Lauri, Jean-Pierre Chanod,
Gregory Grefenstette, and Anne Schiller.
1996. Regular expressions for language
engineering. Natural Language Engineering,
2(4):305?328.
Kokkinakis, Dimitrios and Sofie Johansson
Kokkinakis. 1999. A cascaded finite state
parser for syntactic analysis of Swedish.
In Proceedings of EACL?99.
Koskenniemi, Kimmo. 1990. Finite-state
parsing and disambiguation. In
Proceedings of the 13th International
Conference on Computational Linguistics
(COLING?90), pages 229?233.
Koskenniemi, Kimmo, Pasi Tapanainen, and
Atro Voutilainen. 1992. Compiling and
using finite-state syntactic rules. In
Proceedings of the 14th International
Conference on Computational Linguistics,
COLING-92, pages 156?162.
Lafferty, John, Daniel Sleator, and Davy
Temperley. 1992. Grammatical trigrams: A
probabilistic model of link grammars. In
Proceedings of the 1992 AAAI Fall Symposium
on Probablistic Approaches to Natural
Language.
Lai, Bong Yeung Tom and Changning
Huang. 1994. Dependency grammar and
the parsing of Chinese sentences. In
Proceedings of the 1994 Joint Conference of 8th
ACLIC and 2nd PaFoCol.
Lin, Dekang. 1995. A dependency-based
method for evaluation of broad-coverage
parsers. In Proceedings of IJCAI?95.
Lin, Dekang. 1996. On the structural
complexity of natural language sentences.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING-96).
Melc?uk, Igor A. 1988. Dependency Syntax:
Theory and Practice. State University of
New York Press, Albany, NY.
Mohri, Mehryar, Fernando C. N. Pereira,
and Michael Riley. 1998. A rational design
for a weighted finite-state transducer
library. In Derick Wood and Sheng Yu,
editors, Proceedings of the Second
International Workshop on Implementing
Automata (WIA ?97), volume 1436 of
Lecture Notes in Computer Science.
Springer-Verlag, Berlin, pages 144?158.
Nederhof, Mark-Jan. 1998. Context-free
parsing through regular approximation.
In Lauri Karttunen and Kemal Oflazer,
editors, Proceedings of International
Workshop on Finite State Methods in Natural
Language Processing, pages 13?24, Ankara,
Turkey.
Nederhof, Mark-Jan. 2000. Practical
experiments with regular approximation
of context-free languages. Computational
Linguistics, 26(1):17?44.
Oflazer, Kemal. 1993. Two-level description
of Turkish morphology. In Proceedings of
the Sixth Conference of the European Chapter
of the Association for Computational
Linguistics, April. (A full version appears
in Literary and Linguistic Computing, 9(2),
1994).
Oflazer, Kemal, Bilge Say, Dilek Zeynep
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In Anne
Abeille?, editor, Treebanks. Kluwer
Academic Publishers, Dordrecht, the
Netherlands.
Pereira, Fernando C. N., and Rebecca N.
Wright. 1997. Finite state approximation
of phrase structure grammars. In
Emmanuel Roche and Yves Schabes,
editors, Finite State Language Processing.
MIT Press, Cambridge, MA.
Robinson, Jane J. 1970. Dependency
structures and transformational rules.
Language, 46(2):259?284.
Roche, Emmanuel. 1997. Parsing with finite
state transducers. In Emmanuel Roche
and Yves Schabes, editors, Finite?State
Language Processing, chap. 8. MIT Press,
Cambrigde, MA.
Roche, Emmanuel and Yves Schabes,
editors. 1997. Finite State Language
Processing. MIT Press, Cambridge, MA.
Sleator, Daniel and Davy Temperley. 1991.
Parsing English with a link grammar.
Technical Report CMU-CS-91-196,
Computer Science Department, Carnegie
Mellon University.
Tapanainen, Pasi and Timo Ja?rvinen. 1997.
A non-projective dependency parser. In
Proceedings of ANLP?97, pages 64?71, April.
van Noord, Gertjan. 1997. FSA utilities: A
toolbox to manipulate finite-state
automata. In Derick Wood,
Darrell Raymond, and Sheng Yu, editors,
Automata Implementation, volume 1260 of
Lecture Notes in Computer Science.
Springer-Verlag, Berlin.
Yu?ret, Deniz. 1998. Discovery of Linguistic
Relations Using Lexical Attraction. Ph.D.
thesis, Department of Electrical
Engineering and Computer Science,
Massachusetts Institute of Technology,
Cambridge, MA.
Dependency Parsing of Turkish
Gu?ls?en Eryig?it?
Istanbul Technical University
Joakim Nivre?? ?
Va?xjo? University, Uppsala University
Kemal Oflazer?
Sabanc? University
The suitability of different parsing methods for different languages is an important topic in
syntactic parsing. Especially lesser-studied languages, typologically different from the languages
for which methods have originally been developed, pose interesting challenges in this respect. This
article presents an investigation of data-driven dependency parsing of Turkish, an agglutinative,
free constituent order language that can be seen as the representative of a wider class of languages
of similar type. Our investigations show that morphological structure plays an essential role in
finding syntactic relations in such a language. In particular, we show that employing sublexical
units called inflectional groups, rather than word forms, as the basic parsing units improves
parsing accuracy. We test our claim on two different parsing methods, one based on a probabilis-
tic model with beam search and the other based on discriminative classifiers and a deterministic
parsing strategy, and show that the usefulness of sublexical units holds regardless of the parsing
method. We examine the impact of morphological and lexical information in detail and show that,
properly used, this kind of information can improve parsing accuracy substantially. Applying
the techniques presented in this article, we achieve the highest reported accuracy for parsing the
Turkish Treebank.
1. Introduction
Robust syntactic parsing of natural language is an area in which we have seen tremen-
dous development during the last 10 to 15 years, mainly on the basis of data-driven
methods but sometimes in combination with grammar-based approaches. Despite this,
most of the approaches in this field have only been tested on a relatively small set
of languages, mostly English but to some extent also languages like Chinese, Czech,
Japanese, and German.
? Department of Computer Engineering, Istanbul Technical University, 34469 Istanbul, Turkey.
E-mail: gulsen.cebiroglu@itu.edu.tr.
?? School of Mathematics and Systems Engineering, Va?xjo? University, 35260 Va?xjo?, Sweden.
E-mail: joakim.nivre@msi.vxu.se.
? Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden.
? Faculty of Engineering and Natural Sciences, Sabanc? University, 34956 Istanbul, Turkey.
E-mail: oflazer@sabanciuniv.edu.
Submission received: 5 October 2006; revised submission received: 3 April 2007; accepted for publication:
16 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
An important issue in this context is to what extent our models and algorithms
are tailored to properties of specific languages or language groups. This issue is es-
pecially pertinent for data-driven approaches, where one of the claimed advantages
is portability to new languages. The results so far mainly come from studies where a
parser originally developed for English, such as the Collins parser (Collins 1997, 1999),
is applied to a new language, which often leads to a significant decrease in the measured
accuracy (Collins et al 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and
Manning 2003; Corazza et al 2004). However, it is often quite difficult to tease apart the
influence of different features of the parsing methodology in the observed degradation
of performance.
A related issue concerns the suitability of different kinds of syntactic representation
for different types of languages. Whereas most of the work on English has been based
on constituency-based representations, partly influenced by the availability of data
resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it
has been argued that free constituent order languages can be analyzed more adequately
using dependency-based representations, which is also the kind of annotation found,
for example, in the Prague Dependency Treebank of Czech (Hajic? et al 2001). Recently,
dependency-based parsing has been applied to 13 different languages in the shared
task of the 2006 Conference on Computational Natural Language Learning (CoNLL)
(Buchholz and Marsi 2006).
In this article, we focus on dependency-based parsing of Turkish, a language that
is characterized by rich agglutinative morphology, free constituent order, and predom-
inantly head-final syntactic constructions. Thus, Turkish can be viewed as the repre-
sentative of a class of languages that are very different from English and most other
languages that have been studied in the parsing literature. Using data from the recently
released Turkish Treebank (Oflazer et al 2003), we investigate the impact of different
design choices in developing data-driven parsers. There are essentially three sets of
issues that are addressed in these experiments.
 The first set includes issues relating to the treatment of morphology in
syntactic parsing, which becomes crucial when dealing with languages
where the most important clues to syntactic functions are often found in
the morphology rather than in word order patterns. Thus, for Turkish, it
has previously been shown that parsing accuracy can be improved by
taking morphologically defined units rather than word forms as the basic
units of syntactic structure (Eryig?it and Oflazer 2006). In this article, we
corroborate this claim showing that it holds in both approaches we
explore. We also study the impact of different morphological feature
representations on parsing accuracy.
 The second set of issues concerns lexicalization, a topic that has been very
prominent in the parsing literature lately. Whereas the best performing
parsers for English all make use of lexical information, the real benefits of
lexicalization for English as well as other languages remains controversial
(Dubey and Keller, 2003; Klein and Manning 2003; Arun and Keller 2005).
 The third set concerns the basic parsing methodology, including both
parsing algorithms and learning algorithms. We first introduce a statistical
parser using a conditional probabilistic model which is very sensitive to
the selected representational features and thus clearly exposes the ones
358
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
with crucial importance for parsing Turkish. We then implement our
models on a deterministic classifier-based parser using discriminative
learning, which is one of the best performing dependency parsers
evaluated on a wide range of different languages.
Additionally we address the following issues:
 We investigate learning curves and provide an error analysis for the best
performing parser.
 For most of our experiments we use as input the gold-standard tags from
the treebank. However, in our last experiments we evaluate the impact of
automatic statistical morphological disambiguation on the performance of
our best performing parser.
The rest of the article is structured as follows. Section 2 gives a very brief introduc-
tion to Turkish morphology and syntax and discusses the representation of morpholog-
ical information and syntactic dependency relations in the Turkish Treebank. Section 3
is devoted to methodological issues, in particular the data sets and evaluation metrics
used in experiments. The following two sections present two different dependency
parsers trained and evaluated on the Turkish Treebank: a probabilistic parser (Section 4)
and a classifier-based parser (Section 5). Section 6 investigates the impact of lexicaliza-
tion and morphological information on the two parsers, and Section 7 examines their
learning curves. Section 8 presents an error analysis for the best performing parser,
and Section 9 analyzes the degradation in parsing performance when using automatic
morphological disambiguation. Section 10 discusses related work, and Section 11 sum-
marizes the main conclusions from our study.
2. Turkish: Morphology and Dependency Relations
Turkish displays rather different characteristics compared to the more well-studied
languages in the parsing literature. Most of these characteristics are also found in
many agglutinative languages such as Basque, Estonian, Finnish, Hungarian, Japanese,
and Korean.1 Turkish is a flexible constituent order language. Even though in written
texts the constituent order predominantly conforms to the SOV order, constituents may
freely change their position depending on the requirements of the discourse context
(Erguvanl? 1979; Hoffman 1994). However, from a dependency structure point of view,
Turkish is predominantly (but not exclusively) head final.
Turkish has a very rich agglutinative morphological structure. Nouns can give rise
to about 100 inflected forms and verbs to many more. Furthermore, Turkish words may
be formed through very productive derivations, increasing substantially the number of
possible word forms that can be generated from a root word. It is not uncommon to find
up to four or five derivations in a single word. Previous work on Turkish (Hakkani-
Tu?r, Oflazer, and Tu?r 2002; Oflazer 2003; Oflazer et al 2003; Eryig?it and Oflazer 2006)
has represented the morphological structure of Turkish words by splitting them into
inflectional groups (IGs). The root and derivational elements of a word are represented
1 We, however, do not necessarily suggest that the morphological sublexical representation that we use for
Turkish later in this article is applicable to these languages.
359
Computational Linguistics Volume 34, Number 3
by different IGs, separated from each other by derivational boundaries (DB). Each IG is
then annotated with its own part of speech and any inflectional features as illustrated
in the following example:2
araban?zdayd?
(?it was in your car?)
araban?zda DB yd?
araba+Noun+A3sg+P2pl+Loc
? ?? ?
IG1
DB +Verb+Zero+Past+A3sg
? ?? ?
IG2
?in your car? ?it was?
In this example, the root of the word araban?zdayd? is araba (?car?) and its part of speech is
noun. From this, a verb is derived in a separate IG. So, the word is composed of two IGs
where the first one, araban?zda (?in your car?), is a noun in locative case and in second
plural possessive form, and the second one is a verbal derivation from this noun in past
tense and third person singular form.
2.1 Dependency Relations in Turkish
Because most syntactic information is mediated by morphology, it is not sufficient
for the parser to only find dependency relations between orthographic words;3 the
correct IGs involved in the relations should also be identified. We can motivate this
with the following very simple example: In the phrase spor araban?zdayd? (?it was in
your sports car?), the adjective spor (?sports?) should be connected to the first IG of
the second word. It is the word araba (?car?) which is modified by the adjective, not
the derived verb form araban?zdayd? (?it was in your car?). So a parser should not just
say that the first word is a dependent of the second but also state that the syntactic
relation is between the last IG of the first word and the first IG of the second word, as
shown:
spor
Mod

araban?zda DB yd?
In Figure 1 we see a complete dependency tree for a Turkish sentence laid on top of the
words segmented along IG boundaries. The rounded rectangles show the words and
IGs within words are marked with dashed rounded rectangles. The first thing to note
in this figure is that the dependency links always emanate from the last IG of a word,
because that IG determines the role of that word as a dependent. The dependency links
land on one of the IGs of a (head) word (almost always to the right). The non-final IGs
(e.g., the first IG of the word okuldaki in Figure 1) may only have incoming dependency
2 +A3sg = 3sg number agreement, +P2pl = 2pl possessive agreement, +Loc = Locative Case.
3 For the same reason, Bozsahin (2002) uses morphemes as sublexical constituents in a CCG framework.
Because the lexicon was organized in terms of morphemes each with its own CCG functor, the grammar
had to account for both the morphotactics and the syntax at the same time.
360
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 1
Dependency links in an example Turkish sentence.
?+?s indicate morpheme boundaries. The rounded rectangles show words, and IGs within words
that have more than one IG are indicated by the dashed rounded rectangles. The inflectional
features of each IG as produced by the morphological analyzer are listed below the IG.
links and are assumed to be morphologically linked to the next IG to the right (but we
do not explicitly show these links).4
The noun phrase formed by the three words o?g?rencilerin en ak?ll?s? in this example
highlights the importance of the IG-based representation of syntactic relations. Here
in the word ak?ll?s?, we have three IGs: The first contains the singular noun ak?l (?intelli-
gence?), the second IG indicates the derivation into an adjective ak?ll? (?intelligence-with?
? ?intelligent?). The preceding word en (?most?), an intensifier adverb, is linked to this IG
as a modifier (thus forming ?most intelligent?). The third IG indicates another derivation
into a noun (?a singular entity that is most intelligent?). This last IG is the head of a
dependency link emanating from the word o?g?rencilerin with genitive case-marking (?of
the students? or ?students? ?) which acts as the possessor of the last noun IG of the third
word ak?ll?s?. Finally, this word is the subject of the verb IG of the last word, through its
last IG.
2.2 The Turkish Treebank
We have used the Turkish Treebank (Oflazer et al 2003), created by the Middle East
Technical University and Sabanc? University, in the experiments we report in this ar-
ticle. The Turkish Treebank is based on a small subset of the Metu Turkish Corpus
(www.ii.metu.edu.tr/?corpus/corpus.html), a balanced collection of post-1990 Turk-
ish text from 10 genres. The version that has been used in this article is the version used
in the CoNLL-X shared task publicly available from www.ii.metu.edu.tr/?corpus/
treebank.html.
This treebank comprises 5,635 sentences in which words are represented with IG-
based gold-standard morphological representation and dependency links between IGs.
4 It is worth pointing out that arrows in this representation point from dependents to heads, because
representations with arrows in the opposite direction also exist in the literature.
361
Computational Linguistics Volume 34, Number 3
The average number of IGs per word is 1.26 in running text, but the figure is higher for
open class words and 1 for high frequency function words which do not inflect. Of all
the dependencies in the treebank, 95% are head-final5 and 97.5% are projective.6
Even though the number of sentences in the Turkish Treebank is in the same range
as for many other available treebanks for languages such as Danish (Kromann 2003),
Swedish (Nivre, Nilsson, and Hall 2006), and Bulgarian (Simov, Popova, and Osenova
2002), the number of words is considerably smaller (54K as opposed to 70?100K for the
other treebanks). This corresponds to a relatively short average sentence length in the
treebank of about 8.6 words, which is mainly due to the richness of the morphological
structure, because often one word in Turkish may correspond to a whole sentence in
another language.
3. Dependency Parsing of Turkish
In the following sections, we investigate different approaches to dependency parsing
of Turkish and show that using parsing units smaller than words improves the parsing
accuracy. We start by describing our evaluation metrics and the data sets used, and
continue by presenting our baseline parsers: two naive parsers, which link a dependent
to an IG in the next word, and one rule-based parser. We then present our data-driven
parsers in the subsequent sections: a statistical parser using a conditional probabilistic
model (from now on referred to as the probabilistic parser) in Section 4 and a deter-
ministic classifier-based parser using discriminative learning (from now on referred to
as the classifier-based parser) in Section 5.
3.1 Data Sets and Evaluation Metrics
Our experiments are carried out on the entire treebank and all our results are reported
for this data set. We use ten-fold cross-validation for the evaluation of the parsers, except
for the baseline parsers which do not need to be trained. We divide the treebank data
into ten equal parts and in each iteration use nine parts as training data and test the
parser on the remaining part.
We report the results as mean scores of the ten-fold cross-validation, with standard
error. The main evaluation metrics that we use are the unlabeled attachment score
(ASU) and labeled attachment score (ASL), namely, the proportion of IGs that are
attached to the correct head (with the correct label for ASL). A correct attachment is
one in which the dependent IG (the last IG in the dependent word) is not only attached
to the correct head word but also to the correct IG within the head word. Where relevant, we
also report the (unlabeled) word-to-word score (WWU), which only measures whether
a dependent word is connected to (some IG in) the correct head word. It should be
clear from the discussion in Section 2.1 and from Figure 1 that the IG-to-IG evaluation
is the right one to use for Turkish even though it is more stringent than word-to-
word evaluation. Dependency links emanating from punctuation are excluded in all
5 Half of the head-initial dependencies are actually not real head-initial structures; these are caused by
some enclitics (addressed in detail in the following sections) which can easily be recovered with some
predefined rules.
6 A dependency between a dependent i and a head j is projective if and only if all the words or IGs that
occur between i and j in the linear order of the sentence are dominated by j. A dependency analysis with
only projective dependencies corresponds to a constituent analysis with only continuous constituents.
362
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
evaluation scores. Non-final IGs of a word are assumed to link to the next IG within the
word, but these links, referred to as InnerWord links, are not considered as dependency
relations and are excluded in evaluation scoring.
3.2 Baseline Parsers
We implemented three baseline parsers to assess the performance of our probabilistic
and classifier-based parsers. The first baseline parser attaches each word (from the last
IG) to the first IG of the next word while the second parser attaches each word to the
final IG of the next word. Obviously these two baseline parsers behave the same when
the head word has only one IG. The final punctuation of each sentence is assumed to
be the root of the sentence and it is not connected to any head. The first two lines of Ta-
ble 1 give the unlabeled attachment scores of these parsers. We observe that attaching
the link to the first IG instead of the last one gives better results.
The third baseline parser is a rule-based parser that uses a modified version of the
deterministic parsing algorithm by Nivre (2006). This parsing algorithm, which will be
explained in detail in Section 5, is a linear-time algorithm that derives a dependency
graph in one left-to-right pass over the input, using a stack to store partially processed
tokens and a list to store remaining input tokens in a way similar to a shift-reduce
parser. In the rule-based baseline parser, the next parsing action is determined according
to 31 predefined hand-written rules (Eryig?it 2006; Eryig?it, Adal?, and Oflazer 2006).
The rules determine whether or not to connect the units (words or IGs) on top of the
stack and at the head of the input list (regardless of dependency labels). It can be
seen that the rule-based parser provides an improvement of about 15 percentage points
compared to the relatively naive simpler baseline parsers which cannot recover head-
initial dependencies.
4. Probabilistic Dependency Parser
A well-studied approach to dependency parsing is a statistical approach where the
parser takes a morphologically tagged and disambiguated sentence as input, and
outputs the most probable dependency tree by using probabilities induced from the
training data. Such an approach comprises three components:
1. A parsing algorithm for building the dependency analyses (Eisner 1996;
Sekine, Uchimoto, and Isahara 2000)
2. A conditional probability model to score the analyses (Collins 1996)
Table 1
Unlabeled attachment scores and unlabeled word-to-word scores for the baseline parsers.
Parsing Model ASU WWU
Attach-to-next (first IG) 56.0 63.3
Attach-to-next (last IG) 54.1 63.3
Rule-based 70.5 79.3
363
Computational Linguistics Volume 34, Number 3
3. Maximum likelihood estimation to make inferences about the underlying
probability models (Collins 1996; Chung and Rim 2004)
4.1 Methodology
The aim of our probabilistic model is to assign a probability to each candidate depen-
dency link by using the frequencies of similar dependencies computed from a training
set. The aim of the parsing algorithm is then to explore the search space in order to find
the most probable dependency tree. This can be formulated with Equation (1) where S
is a sequence of n units (words or IGs) and T ranges over possible dependency trees
consisting of dependency links dep(ui,uH(i) ), with uH(i) denoting the head unit to which
the dependent unit ui is linked and the probability of a given tree is the product of the
dependency links that it comprises.
T? = argmax
T
P(T|S) = argmax
T
n?1
?
i=1
P(dep (ui,uH(i) ) |S) (1)
The observation that 95% of the dependencies in the Turkish treebank are head-
final dependencies motivated us to employ the backward beam search dependency
parsing algorithm by Sekine, Uchimoto, and Isahara (2000) (originally designed for
Japanese, another head-final language), adapted to our morphological representation
with IGs. This algorithm parses a sentence starting from the end moving towards the
beginning, trying at each step to link the dependents to a unit to the right. It uses a
beam which keeps track of the most probable dependency structures for the partially
processed sentence. However, in order to handle head-initial dependencies, it employs
three predefined lexicalized rules7 (also used in our rule-based baseline parser). For
every new word, the parser starts by checking if any of the rules apply. If so, it constructs
a right-to-left link whereH(i) < i and directly assigns 1.0 as the dependency probability
(P(dep (ui,uH(i) ) |S) = 1.0). If none of the rules apply, it instead uses probabilities for
head-final dependencies.
For the probability model, we adopt the approach by Chung and Rim (2004), which
itself is a modified version of the statistical model used in Collins (1996).8 In this model
in Equation (2), the probability of a dependency link P(dep (ui,uH(i) ) |S) linking ui to a
head uH(i) is approximated with the product of two probabilities:
P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i ?H(i) ) (2)
P(ui links to some head dist(i,H(i)) away |?i)
7 The rules check for enclitics such as de, ki, mi, written on the right side of and separately from the word
they attach to, for the verb deg?il, which gives a negative meaning to the word coming before it and for
nominals which do not have any verbs on their right side.
8 The statistical model in Collins (1996) is actually used in a phrase-structure-based parsing approach, but
it uses the same idea of computing probabilities between dependents and head units. We also tried to
employ the statistical model of Collins, where the distance measure ?i,H(i) is included in the link
probability formula (P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i,H(i) )) , but we obtained worse results
with this.
364
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
In this equation,
 P(link(ui,uH(i) ) |?i ?H(i) ) is the probability of seeing the same dependency
within a similar context where ?i represents the context around the
dependent ui and ?H(i) represents the context around the head uH(i), and
 P(ui links to some head dist(i,H(i)) away |?i) is the probability of seeing
the dependent linking to some head a distance dist(i,H(i)) away, in the
context ?i.
In all of the following models, dist(i,H(i)) is taken as the number of actual word
boundaries between the dependent and the head unit regardless of whether full words
or IGs were used as units of parsing.9
To alleviate the data sparseness, we use the interpolation of other estimates while
calculating the probabilities in Equation (2).10 We use a strategy similar to Collins (1996)
and we interpolate with estimates based on less context:
P(x|y) ? ? ? P1(x|y) + (1 ? ?) ? P2(x) (3)
where ? = ?/(?+ 1) and ? is the count of the x occurrences
During the actual runs, the smoothed probability P(link(ui,uH(i) ) |?i ?H(i) ) is estimated
by interpolating two unsmoothed empirical estimates extracted from the treebank:
P1(link(ui,uH(i) ) |?i ?H(i) ) and P2(link(ui,uH(i) )). A similar approach was employed for
P(ui links to some head dist(i,H(i)) away |?i) and it is estimated by interpolating
P1(ui links to some head dist(i,H(i)) away |?i) and P2(ui links to some head dist(i,H(i))
away). If even after interpolation, the probability is 0, then a very small value is
used. Further, distances larger than a certain threshold value were assigned the same
probability, as explained later.
4.2 The Choice of Parsing Units
In the probabilistic dependency parsing experiments, we experimented with three dif-
ferent ways of choosing and representing the units for parsing:11
1. Word-based model #1: In this model, the units of parsing are the actual words
and each word is represented by a combination of the representations of all the IGs
that make it up. Note that although all IGs are used in representing a word, not all the
information provided by an IG has to be used, as we will see shortly. This representation,
however, raises the following question: If we use the words as the parsing units and
9 We also tried other distance functions, for example, the number of IGs between dependent and head
units, but this choice fared better than the alternatives.
10 We tried many other different interpolation and backoff models where we tried to remove the neighbors
one by one or the inflectional features. But we obtained the best results with a two-level interpolation by
removing the contextual information all at once.
11 Clarifying examples of these representations will be provided in the immediately following section.
365
Computational Linguistics Volume 34, Number 3
find the dependencies between these, how can we translate these to the dependencies
between the IGs, since our goal is to find dependencies between IGs? The selection
of the IG of the dependent word is an easy decision, as it is the last IG in the word.
The selection of the head IG is obviously more difficult. Because such a word-based
model will not provide much information about the underlying IGs structure, we will
have to make some assumptions about the head IG. The observation that 85.6% of the
dependency links in the treebank land on the first (and possibly the only) IG of the
head word and the fact that our first baseline model (attaching to the first IG) gives
better performance than our second baseline model (attaching to the last IG), suggest
that after identifying the correct word, choosing the first IG as the head IG may be a
reasonable heuristic. Another approach to determining the correct IG in the head word
could be to develop a post-processor which selects this IG using additional rules. Such
a post-processor could be worth developing if the WWU accuracy obtained with this
model proves to be higher than all of the other models, that is, if this is the best way
of finding the correct dependencies between words without considering which IGs are
connected. However, as we will see in Section 4.4, this model does not give the best
WWU.
2. Word-based model #2: This model is just like the previous model but we rep-
resent a word using its final IGs rather than the concatenation of all their IGs when it
is used as a dependent. The representation is the same as in Word-based model #1 when
the word is a head. This results in a dynamic selection of the representation during
parsing as the representation of a word will be determined according to its role at that
moment. The representation of the neighboring units in context will again be selected
with respect to the word in question: any context unit on the left will be represented
with its dependent representation (just the last IG) and any neighbor on the right will
be represented with its representation as a head. The selection of the IG in the head
word is the same as in the first model.
3. IG-based model: In this model, we use IGs as units in parsing. We split the IG-
based representation of each word and reindex these IGs in order to use them as single
units in parsing. Figure 2 shows this transfer to the IG-based model. We still, however,
need to know which IGs are word-final as they will be the dependent IGs (shown in
the figure by asterisks). The contextual elements that are used in this model are the
IGs to the left (starting with the last IG of the preceding word) and to the right of the
dependent and the head IG.
Figure 2
Mapping from word-based to IG-based representation of a sentence.
366
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
4.3 Reduced Dynamic Representations for IGs
In all three models, it is certainly possible to use all the information supplied by the
full morphological analysis in representing the IGs.12 This includes the root words
themselves, major and minor parts of speech,13 number and person agreement markers,
possessive agreement markers, case markers, tense, aspect, mood markers, and other
miscellaneous inflectional and semantic markers especially for derivations. Not all of
these features may be relevant to the parsing task, and further, different features may
be relevant depending on whether the IG is being used as a dependent or a head. Also,
in order to alleviate the data sparseness problem that may result from the relatively
modest size of the treebank, an ?unlexicalized? representation that does not contain the
root word needs to be considered so that statistics from IGs that are otherwise the same
except for the root word (if any) can be conflated.14 After some preliminary experimen-
tation, we decided that a reduced representation for IGs that is dynamically selected
depending on head or dependent status would give us the best performance. We explain
the representation of the IGs and the parameters that we used in the three models.
 When used as a dependent (or part of a dependent word in models 1
and 2) during parsing;
? Nominal IGs (nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives, past
and future participles) are represented only with the case marker,
because that essentially determines the syntactic function of that IG
as a dependent, and only nominals have cases.
? Any other IG is just represented with its minor part of speech.
 When used as a head (or part of a head word in models 1 and 2);
? Nominal IGs and adjective IGs with participle minor part of
speech15 are represented with the minor part of speech and the
possessive agreement marker.
? Any other IG is just represented with its minor part of speech.
Figures 3?5 show, for the first three words in Figure 1, the unlexicalized reduced
representations that are used in the three models when units are used as dependents
and heads during parsing.
12 See Figure 1 for a sample of such information.
13 A minor part-of-speech category is available for some major part-of-speech categories: pronouns are
further divided into personal pronouns, demonstrative pronouns, interrogative pronouns, and so on. The
minor part-of-speech category always implies the major part of speech. For derived IGs, the minor part of
speech mostly indicates a finer syntactic or semantic characterization of the derived word. When no
minor part of speech is available the major part of speech is used.
14 Remember that only the first IG in a word has the root word.
15 These are modifiers derived from verbs. They have adjective as their major part of speech and
past/future participle as their minor part of speech. They are the only types of IGs that have possessive
agreement markers other than nominals.
367
Computational Linguistics Volume 34, Number 3
Figure 3
Reduced IG representation for Word-based model #1.
Figure 4
Reduced IG representation for Word-based model #2.
4.4 Experimental Results
In this section, we first evaluate the performance of the models described in Section 4.2.
We then investigate the impact of different choices of morphological features on the best
performing IG-based model. In addition to the parsing model, the parser is given the
following parameters:
 the number of left and right neighbors of the dependent (Dl, Dr) to define
the dependent context ?i,
16
 the number of left and right neighbors of the head (Hl, Hr) to define the
head context ?H(i),
 the size of the beam (beamsize), and
16 In terms of parsing units, the number of words for word-based models and the number of IGs for
IG-based models.
368
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 5
Reduced IG representation for IG-based model.
 the distance threshold value beyond which P(ui links to some head
dist(i,H(i)) away |?i) is assigned the same probability.
Table 2 gives the ASU scores for the word-based and IG-based models for the
best combinations of contexts used for each case. We also provide WWU scores for
comparison, but again stress that the main evaluation criterion is the ASU score. For
all three models, the beamsize value is selected as 3 and distance threshold is selected
as 6.17 It can be seen that the performance of the word-based models is lower than
our rule-based baseline parser (Table 1) with ASU = 70.5, even though it is better than
the first two rather naive baselines. On the other hand, the IG-based model outper-
forms all of the baseline parsers and word-based models. It should also be noted that
the IG-based model improves not only the ASU accuracy but also the word-to-word
accuracy compared, to the word-based models. Thus, the IG-based model not only
helps to recover the relations between correct IGs but also to find the correct head
word.
In Table 3, we also present results from experiments employing different represen-
tations for the IGs. A more detailed investigation about the use of limited lexicalization
and inflectional features will be presented later in Section 6. Here, we will see what
would have happened if we had used alternative reduced IG representations compared
to the representation described earlier, which is used in the best performing IG-based
model.
Table 3 gives the results for each change to the representational model. One can
see that none of these representational changes improves the performance of the best
performing model. Only employing major part-of-speech tags (#1) actually comes close,
and the difference is not statistically significant. Lexicalization of the model results in
a drastic decrease in performance: Using the surface form (#6) gives somewhat better
17 As stated earlier in Section 4.1, our distance function is calculated according to the word boundaries
between the dependent and the head units. In the treebank, 95% of the dependency links link to a word
that is less than six words away. Thus all the distances larger than or equal to six are conflated into the
same small probability.
369
Computational Linguistics Volume 34, Number 3
Table 2
Unlabeled attachment scores and unlabeled word-to-word scores for the probabilistic parser.
Parsing Model (parameters) ASU WWU
Word-based model #1 (Dl=1, Dr=1, Hl=1, Hr=1) 68.1?0.4 77.1?0.7
Word-based model #2 (Dl=1, Dr=1, Hl=1, Hr=1) 68.3?0.3 77.6?0.5
IG-based model (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3 79.0?0.7
results than using root information (#5). Also, dynamic selection of tags seems to help
performance (#3) but using all available inflectional information performs significantly
worse possibly due to data sparseness.
5. Classifier-Based Dependency Parser
Our second data-driven parser is based on a parsing strategy that has achieved a high
parsing accuracy across a variety of different languages (Nivre et al 2006, 2007). This
strategy consists of the combination of the following three techniques:
1. Deterministic parsing algorithms for building dependency graphs (Kudo
and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003)
Table 3
Unlabeled attachment scores for different choices for morphological features.
Model ASU
IG-based model
# (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3
1 Using major part of speech 71.2?0.2
instead of minor part of speech
2 Using only minor part of speech and 68.3?0.2
no other inflectional features
3 Using minor part of speech for all 71.0?0.3
types of IGs together with case and
possessive markers for nominals
and possessive marker for adjectives
(but no dynamic selection)
4 Using all inflectional features in 46.5?0.4
addition to minor part of speech
5 Adding root information to the best 53.7?0.2
performing IG-based model
6 Adding surface form information to the best 54.4?0.2
performing IG-based model
370
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
2. History-based models for predicting the next parser action (Black et al
1992; Magerman 1995; Ratnaparkhi 1997; Collins 1999)
3. Discriminative classifiers to map histories to parser actions (Kudo and
Matsumoto 2002; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson
2004)
A system of this kind employs no grammar but relies completely on inductive learning
from treebank data for the analysis of new sentences, and on deterministic parsing
for disambiguation. This combination of methods guarantees that the parser is robust,
never failing to produce an analysis for an input sentence, and efficient, typically
deriving this analysis in time that is linear in the length of the sentence.
In the following sections, we will first present the parsing methodology and then
results that show that the IG-based model again outperforms the word-based model. We
will then explore how we can further improve the accuracy by exploiting the advantages
of this parser. All experiments are performed using the freely available implementation
MaltParser.18
5.1 Methodology
For the experiments in this article, we use a variant of the parsing algorithm proposed
by Nivre (2003, 2006), a linear-time algorithm that derives a labeled dependency graph
in one left-to-right pass over the input, using a stack to store partially processed tokens
and a list to store remaining input tokens. However, in contrast to the original arc-eager
parsing strategy, we use an arc-standard bottom-up algorithm, as described in Nivre
(2004). Like many algorithms used for dependency parsing, this algorithm is restricted
to projective dependency graphs.
The parser uses two elementary data structures, a stack ? of partially analyzed
tokens and an input list ? of remaining input tokens. The parser is initialized with an
empty stack and with all the tokens of a sentence in the input list; it terminates as soon
as the input list is empty. In the following, we use subscripted indices, starting from 0,
to refer to particular tokens in ? and ?. Thus, ?0 is the token on top of the stack ? (the
top token) and ?0 is the first token in the input list ? (the next token); ?0 and ?0 are
collectively referred to as the target tokens, because they are the tokens considered as
candidates for a dependency relation by the parsing algorithm.
There are three different parsing actions, or transitions, that can be performed in
any non-terminal configuration of the parser:
 Shift: Push the next token onto the stack.
 Left-Arcr: Add a dependency arc from the next token to the top token,
labeled r, then pop the stack.
 Right-Arcr: Add a dependency arc from the top token to the next token,
labeled r, then replace the next token by the top token at the head of the
input list.
18 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
371
Computational Linguistics Volume 34, Number 3
In order to perform deterministic parsing in linear time, we need to be able to predict
the correct parsing action (including the choice of a dependency type r for Left-Arcr
and Right-Arcr) at any point during the parsing of a sentence. This is what we use a
history-based classifier for.
The features of the history-based model can be defined in terms of different linguis-
tic features of tokens, in particular the target tokens. In addition to the target tokens,
features can be based on neighboring tokens, both on the stack and in the remaining
input, as well as dependents or heads of these tokens in the partially built dependency
graph. The linguistic attributes available for a given token are the following:
 Lexical form (root) (LEX)
 Part-of-speech category (POS)
 Inflectional features (INF)
 Dependency type to the head if available (DEP)
To predict parser actions from histories, represented as feature vectors, we use sup-
port vector machines (SVMs), which combine the maximum margin strategy introduced
by Vapnik (1995) with the use of kernel functions to map the original feature space
to a higher-dimensional space. This type of classifier has been used successfully in
deterministic parsing by Kudo and Matsumoto (2002), Yamada and Matsumoto (2003),
and Sagae and Lavie (2005), among others. To be more specific, we use the LIBSVM
library for SVM learning (Chang and Lin 2001), with a polynomial kernel of degree 2,
with binarization of symbolic features, and with the one-versus-one strategy for multi-
class classification.19
This approach has some advantages over the probabilistic parser, in that
 it can process both left-to-right and right-to-left dependencies due to its
parsing algorithm,
 it assigns dependency labels simultaneously with dependencies and can
use these as features in the history-based model, and
 it does not necessarily require expert knowledge about the choice of
linguistically relevant features to use in the representations because SVM
training involves implicit feature selection.
However, we still exclude sentences with non-projective dependencies during train-
ing.20 Because the classifier-based parser not only builds dependency structures but also
assigns dependency labels, we give ASL scores as well as ASU scores.
19 Experiments have also been performed using memory-based learning (Daelemans and Bosch 2005). They
were found to give lower parsing accuracy.
20 Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to
learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by
adding an extra dependency label, we did not observe any improvement when applying the
pseudo-projective processing of Nivre and Nilsson (2005), which is reported to improve accuracy for
other languages.
372
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
5.2 Experimental Results
In this section, our first aim is to confirm the claim that using IGs as the units in parsing
improves performance. For this purpose, we start by using models similar to those
described in the previous section. We use an unlexicalized feature model where the
parser uses only the minor POS and the DEP of tokens and compare the results with the
probabilistic parser. We then show in the second part how we can improve accuracy by
exploiting the morphological structure of Turkish and taking advantage of the special
features of this parser.
5.2.1 Comparison with the Probabilistic Parser. In order to compare with the results of the
previous section, we adopt the same strategy that we used earlier in order to present
inflectional groups. We employ two representation models:
 Word-based model, where each word is represented by the concatenation
of its IGs,
 IG-based model, where the units are inflectional groups.
We take the minor POS category plus the case and possessive agreement markers for
nominals and participle adjectives to make up the POS feature of each IG.21 However,
we do not employ dynamic selection of these features and just use the same strategy
for both dependents and the heads. The reason is that, in this parser, we do not make
the assumption that the head is always on the right side of the dependent, but also
try to find head-initial dependencies, and the parser does not know at a given stage
if a unit is a candidate head or dependent. In the IG-based model, InnerWord relations
(Figure 5), which are actually determined by the morphological analyzer, are processed
deterministically without consulting the SVM classifiers.22
The feature model (Feature Model #1) to be used in these experiments is shown
in Figure 6. This feature model uses five POS features, defined by the POS of the two
topmost stack tokens (?0, ?1), the first two tokens of the remaining input (?0, ?1), and
the token which comes just after the topmost stack token in the actual sentence (?0 + 1).
The dependency type features involve the top token on the stack (?0), its leftmost and
rightmost dependent (l(?0), r(?0)), and the leftmost dependent of the next input token
(l(?0)).
The results for this feature model and the two representation models can be seen
in Table 4. We again see that the IG-based model outperforms the word-based model.
When we compare the unlabeled (ASU) scores with the results of the probabilistic parser
(from Table 2), we see that we do not obtain any improvements neither for the IG-based
model nor for the word-based model. This is probably the combined effect of not using
21 Thus, we are actually combining some inflectional features with the part-of-speech category and use
them together in the POS feature.
22 Because only the first IG of a word carries the stem information (and the remaining IGs has null ? ?
values for this field), a lexicalized model can easily determine the InnerWord links without need for a
deterministic model. For the unlexicalized models, it is necessary to process InnerWord relations
deterministically in order to get the full benefit of IG-based parsing, because the classifiers cannot
correctly predict these relations without lexical information (Eryig?it, Nivre, and Oflazer 2006). However,
for the lexicalized models, adding deterministic InnerWord processing has no impact at all on parsing
accuracy, but it reduces training and parsing time by reducing the number of training instances for the
SVM classifiers.
373
Computational Linguistics Volume 34, Number 3
Figure 6
Feature models for the classifier-based parser.
Table 4
Unlabeled and labeled attachment scores for the unlexicalized classifier-based parser.
Parsing Model ASU ASL
Word-based model 67.1?0.3 57.8?0.3
IG-based model 70.6?0.2 60.9?0.3
the lexical information for head-initial dependencies that we use in our rules in the
probabilistic parser, and of not using dynamic selection.23
5.2.2 Exploiting the Advantages of the Classifier-Based Parser. To exploit the advantages
of the classifier-based parser, we now describe a setting which does not rely on any
linguistic knowledge on the selection of inflectional features and lets the classifier of the
parser select the useful combinations of the features. As SVMs can perform such tasks
successfully, we now explore different representations of the morphological data in the
IG-based model to see if the performance can be improved.
As shown in earlier examples, the inflectional information available for a given
token normally consists of a complex combination of atomic features such as +A3sg,
+Pnon, and +Loc. Eryig?it, Nivre, and Oflazer (2006) showed that adding inflectional
features as atomic values to the feature models was better than taking certain subsets
with linguistic intuition and trying to improve on them. Thus we now present results
with the feature model where the POS component only comprises the minor part of
speech and the INF comprises all the other inflectional features provided by the tree-
bank without any reduction. We investigate the impact of this approach first with an
unlexicalized model (Feature Model #2 in Figure 6) and then with a lexicalized model
(Feature Model #3 in Figure 6) where we investigate two different kinds of lexicalization:
one using just the root information and one using the complete surface form as lexical
features.
Table 5 gives the results for both unlexicalized and lexicalized models with INF
features included in the feature model. We can see the benefit of using inflectional
features separately and split into atomic components, by comparing the first line of
the table with the best results for the IG-based model in Table 4. We can also note
23 Actually, the equivalent of this IG-based model is the probabilistic model #3 in Table 3 (with no dynamic
selection), which does not do significantly better than this classifier-based model.
374
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 5
Unlabeled and labeled attachment scores for enhancements of the IG-based model.
Feature Model ASU ASL
Feature Model #2 (no lexicalization) 72.4?0.2 63.1?0.3
Feature Model #3 (lex. with surface forms) 75.7?0.2 66.6?0.3
Feature Model #3 (lex. with roots) 76.0?0.2 67.0?0.3
the improvement that lexicalized models bring:24 In contrast to the probabilistic parser,
lexicalization using root information rather than surface form gives better performance,
even though the difference is not statistically significant. The improvement in ASU
score is 3.9 percentage points for the lexicalized model (with root) over the IG-based
model of the probabilistic parser with ASU=72.1?0.3. A similar case can be observed for
WWU accuracies: Including INF and lexicalization with roots gives WWU=82.7?0.5 on
the entire treebank, which provides an improvement of 3.3 percentage points over the
IG-based model of the probabilistic parser (withWWU=79.0?0.7).
6. The Impact of Inflectional Features and Lexicalization
In the previous sections, we presented our parsers using optimized parameters and
feature representations. We have observed that using complete inflectional features and
lexicalized models improves the accuracy of the classifier-based parser significantly,
whereas for the probabilistic parser adding these features has a negative impact on
accuracy. In this section, we investigate the influence of different inflectional features
and lexical information on both parsers using the best performing IG-based models,
in order to get a more fine-grained picture. The results of the experiments with the
classifier-based parser are not strictly comparable to those of other experiments, because
the training data have here been divided into smaller sets (based on the major part of
speech category of the next token) as a way of reducing SVM training times without a
significant decrease in accuracy. For the probabilistic parser, we have not used dynamic
selection while investigating the impact of inflectional features.
6.1 Inflectional Features
In order to see the influence of inflectional features, we tested six different sets, where
each set includes the previous one and adds some more inflectional features. The
following list describes each set in relation to the previous one:
Set 1 No inflectional features except for minor part of speech
Set 2 Set 1 + case and possessive markers for nominals, possessive markers for partici-
ple adjectives
Set 3 Set 2 + person/number agreement features for nominals and verbs
Set 4 Set 3 + all inflectional features for nominals
24 The unlabeled exact match score (that is, the percentage of sentences for which all dependencies are
correctly determined) for this best performing model is 37.5% upon IG-based evaluation and 46.5% upon
word-based evaluation.
375
Computational Linguistics Volume 34, Number 3
Set 5 Set 4 + all inflectional features for verbs
Set 6 Set 5 + all inflectional features
Figure 7 shows the results for both the probabilistic and the classifier-based parser.
The results shown in Figures 7b confirm the importance of case and possessive features,
which was presupposed in the manual selection of features in Section 4. Besides these,
the number/person agreement features available for nominals and verbs are also impor-
tant inflectional features even though they do not provide any statistically significant
increase in accuracy (except for ASU in Figure 7b [Set 3]). Another point that merits
attention is the fact that the labeled accuracy is affected more by the usage of inflectional
features compared to unlabeled accuracy. The difference between Set 1 and Set 2 (in
Figure 7b) is nearly 4 percentage points for ASU and 10 percentage points for ASL. It
thus appears that inflectional features are especially important in order to determine the
type of the relationship between the dependent and head units. This is logical because
in Turkish it is usually not the word order that determines the roles of the constituents
in a sentence, but the inflectional features (especially the case markers). We again see
from these figures that the classifier-based parser does not suffer from sparse data even
if we use the full set of inflectional features (Set 6) provided by the treebank, whereas the
probabilistic parser starts having this problem even with Set 3 (Figure 7a). The problem
gets worse when we add the complete set of inflectional features.
6.2 Lexicalization
In order to get a more fine-grained view of the role of lexicalization, we have investi-
gated the effect of lexicalizing IGs from different major part-of-speech categories. We
expand this analysis into POS categories where relevant. The results are shown in Ta-
ble 6, where the first column gives the part-of-speech tag of the lexicalized units, and
the second and third columns give the total frequency and the frequency of distinct roots
for that part-of-speech tag. We again see that the probabilistic parser suffers from sparse
data especially for part-of-speech tags that appear with a high number of distinct roots.
We cannot observe any increase with the lexicalization of any category. The situation is
different for the classifier-based parser. None of the individual lexicalizations causes a
decrease. We see that the lexicalization of nouns causes a significant increase in accuracy.
Figure 7
Accuracy for feature sets 1?6:
a) Unlabeled accuracy for probabilistic parser
b) Unlabeled and labeled accuracy for classifier-based parser
376
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 6
Unlabeled and labeled attachment scores for limited lexicalization (n = count, d = number of
distinct roots).
Probabilistic Classifier-based
n d ASU ASU ASL
None - - 72.1?0.3 72.8?0.2 63.2?0.3
Adjectives 6446 735 68.7?0.2 72.9?0.2 63.2?0.3
Adverbs 3033 221 69.8?0.3 73.1?0.2 63.4?0.3
Conjunctions 2200 44 67.8?0.4 74.1?0.2 64.2?0.3
Determiners 1998 13 71.8?0.3 72.8?0.2 63.3?0.3
Duplications 11 9 72.0?0.3 72.8?0.2 63.2?0.3
Interjections 100 34 72.0?0.3 72.8?0.2 63.2?0.3
Nouns 21860 3935 53.7?0.3 73.9?0.2 64.6?0.3
Numbers 850 226 71.4?0.3 72.9?0.2 63.3?0.3
Post-positions 1250 46 70.9?0.3 72.9?0.2 63.2?0.3
Pronouns 2145 28 72.0?0.2 72.8?0.2 63.2?0.3
Punctuations 10420 16 72.1?0.3 73.4?0.2 63.7?0.3
Questions 228 6 71.9?0.2 72.8?0.2 63.2?0.3
Verbs 14641 1256 59.9?0.4 72.9?0.2 63.8?0.3
Lexicalization of verbs also gives a noticeable increase in the labeled accuracy even
though this is not statistically significant. A further investigation on the minor parts of
speech of nouns25 shows that only common nouns have this positive effect, whereas the
lexicalization of proper nouns does not improve accuracy. We see that the lexicalization
of conjunctions also improves the accuracy significantly. This improvement can be at-
tributed to the enclitics (such as de, ki,mi, written on the right side of and separately from
the word they attach to), which give rise to head-initial dependencies. These enclitics,
which are annotated as conjunctions in the treebank, can be differentiated from other
conjunctions by lexicalization which makes it very easy to connect them to their head
on the left.
Because we did not observe any improvement in the probabilistic parser, we con-
tinued further experimentation only with the classifier-based parser. We tried partially
lexicalized models by lexicalizing various combinations of certain POS categories (see
Figure 8). The results show that, whereas lexicalization certainly improves parsing
accuracy for Turkish, only the lexicalization of conjunctions and nouns together has
an impact on accuracy. Similarly to the experiments on inflectional features, we again
see that the classifier-based parser has no sparse data problem even if we use a totally
lexicalized model.
Although the effect of lexicalization has been discussed in several studies recently
(Dubey and Keller 2003; Klein and Manning 2003; Arun and Keller 2005), it is often
investigated as an all-or-nothing affair, except for a few studies that analyze the distri-
butions of lexical items, for example, Bikel (2004) and Gildea (2001). The results for
25 IGs with a noun part-of-speech tag other than common nouns are marked with an additional minor part
of speech that indicates whether the nominal is a proper noun or a derived form?one of future
participle, past participle, infinitive, or a form involving a zero-morpheme derivation. These latter four
do not contain any root information.
377
Computational Linguistics Volume 34, Number 3
Figure 8
Unlabeled and labeled attachment scores for incrementally extended lexicalization for the
classifier-based parser.
Turkish clearly show that the effect of lexicalization is not uniform across syntactic
categories, and that a more fine-grained analysis is necessary to determine in what
respects lexicalization may have a positive or negative influence. For some models
(especially those suffering from sparse data), it may even be a better choice to use some
kind of limited lexicalization instead of full lexicalization, although the experiments
in this article do not show any example of that. The results from the previous section
suggests that the same is true for morphological information, but this time showing that
limited addition of inflectional features (instead of using them fully) helps to improve
the accuracy of the probabilistic parser.
7. The Impact of Training Set Size
In order to see the influence of the training set size on the performance of our parsers,
we designed the experiments shown in Figure 9, where the x-axis shows the number
of cross validation subsets that we used for training in each step. Figure 9 gives the
ASU scores for the probabilistic parser (unlexicalized except for head-initial rules) and
the classifier-based parser (unlexicalized and lexicalized). We observe that the relative
improvement with growing training set size is largest for the classifier-based lexicalized
model with a relative difference of 5.2?0.2 between using nine training subsets and one
training subset, whereas this number is 4.6?0.3 for the unlexicalized classifier-based
model and 2.5?0.2 for the unlexicalized probabilistic model. We can state that despite its
lower accuracy, the probabilistic model is less affected by the size of the training data.
We can see from this chart that the relative ranking of the models remain the same,
except for sizes 1?3, where the probabilistic parser does better (or no worse than) the
unlexicalized classifier-based models. Another conclusion may be that classifier-based
models are better at extracting information with the increasing size of the data in hand,
whereas the probabilistic model cannot be improved very much with the increasing size
of the data. We can observe this situation especially in the lexicalized model which is
improved significantly between size = 6 subsets and size = 9 subsets, whereas there is
no significant improvement on the unlexicalized models within this interval.
378
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
68
69
70
71
72
73
74
75
76
77
1 2 3 4 5 6 7 8 9
# cross validation sets used in training
probabilistic unlex.
classifier-based unlex.
classifier-based lex.
Figure 9
Unlabeled attachment score for different training set sizes.
8. Error Analysis
In this section, we present a detailed error analysis on the results of our best per-
forming parser. We first evaluate our results on different dependency types. We then
investigate the error distribution in terms of distance between the head assigned by
the parser and the actual head. Finally, we look at the error distribution in relation
to sentence length. In the analysis, the results are aggregated over all ten folds of the
cross-validation.
8.1 Accuracy per Dependency Type
Table 7 gives theASU, labeled precision, labeled recall and labeled F-score for individual
dependency types. The table is sorted according to the ASU results, and the average
distance between head and dependent is given for each type.
We see that the parser cannot find labeled dependencies for the types that have
fewer than 100 occurrences in the treebank, with the single exception of RELATIVIZER,
the enclitic ki (conjunction), written separately from the word it attaches to. Because this
dependency type always occurs with the same particle, there is no sparse data problem.
If we exclude the low-frequency types, we can divide the results into three main
groups. The first group consists of determiners, particles, and nominals that have an
ASU score over 79% and link to nearby heads. The second group mainly contains
subjects, objects, and different kinds of adjuncts, with a score in the range 55?79% and
a distance of 1.8?4.6 IGs to their head. This is the group where inflectional features are
most important for finding the correct dependency. The third group contains distant
dependencies with a much lower accuracy. These are generally relations like sentence
modifier, vocative, and apposition, which are hard to find for the parser because they
cannot be differentiated from other nominals used as subjects, objects, or normal mod-
ifiers. Another construction that is hard to parse correctly is coordination, which may
require a special treatment.
379
Computational Linguistics Volume 34, Number 3
Table 7
Attachment score (ASU), labeled precision (P), labeled recall (R) and labeled F-score for each
dependency type in the treebank (n = count, dist = dependency length).
Label n dist ASU P R F
SENTENCE 7,252 1.5 90.5 87.4 89.2 88.3
DETERMINER 1,952 1.3 90.0 84.6 85.3 85.0
QUESTION.PARTICLE 288 1.3 86.1 80.0 76.4 78.2
INTENSIFIER 903 1.2 85.9 80.7 80.3 80.5
RELATIVIZER 85 1.2 84.7 56.6 50.6 53.4
CLASSIFIER 2,048 1.2 83.7 74.6 71.7 73.1
POSSESSOR 1,516 1.9 79.4 81.6 73.6 77.4
NEGATIVE.PARTICLE 160 1.4 79.4 76.4 68.8 72.4
OBJECT 7,956 1.8 75.9 63.3 62.5 62.9
MODIFIER 11,685 2.6 71.9 66.5 64.8 65.7
DATIVE.ADJUNCT 1,360 2.4 70.8 46.4 50.2 48.2
FOCUS.PARTICLE 23 1.1 69.6 0.0 0.0 0.0
SUBJECT 4,479 4.6 68.6 50.9 56.2 53.4
ABLATIVE.ADJUNCT 523 2.5 68.1 44.0 54.5 48.7
INSTRUMENTAL.ADJUNCT 271 3.0 62.7 29.8 21.8 25.2
ETOL 10 4.2 60.0 0.0 0.0 0.0
LOCATIVE.ADJUNCT 1,142 4.2 56.9 43.3 48.4 45.7
COORDINATION 814 3.4 54.1 53.1 49.8 51.4
S.MODIFIER 594 9.6 50.8 42.2 45.8 43.9
EQU.ADJUNCT 16 3.7 50.0 0.0 0.0 0.0
APPOSITION 187 6.4 49.2 49.2 16.6 24.8
VOCATIVE 241 3.4 42.3 27.2 18.3 21.8
COLLOCATION 51 3.3 41.2 0.0 0.0 0.0
ROOT 16 - 0.0 0.0 0.0 0.0
Total 43,572 2.5 76.0 67.0 67.0 67.0
8.2 Error Distance
When we evaluate our parser based on the dependency direction, we obtain an ASU
of 72.2 for head-initial dependencies and 76.2 for head-final ones. Figure 10a and
Figure 10b give the error distance distributions for head-initial and head-final depen-
dencies based on the unlabeled performance of the parser. The x-axis in the figures gives
the difference between indexes of the assigned head IG and the real head IG.
As stated previously, the head-initial dependencies constitute 5% of the entire de-
pendencies in the treebank. Figure 10a shows that for head-initial dependencies the
parser has a tendency to connect the dependents to a head closer than the real head
or in the wrong direction. When we investigate these dependencies, we see that 70%
of them are connected to a head adjacent to the dependent and the parser finds 90% of
these dependencies correctly. Thus, we can say that the parser has no problem in finding
adjacent head-initial dependencies. Moreover, 87% of the errors where the error distance
is equal to 1 (Figure 10a)26 are due to the dependents being connected to the wrong IG
26 Meaning that the actual head and assigned head are adjacent.
380
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 10
Error distance distributions a) for head-initial dependencies b) for head-final dependencies.
of the correct head word. When we investigate the ability of the parser in finding the
dependency direction, we see that our parser has a high precision value (91%) and a
relatively lower recall value (80%).
The parser is 100% successful in finding the direction of head-final dependencies.
Furthermore, the errors that it makes while determining the correct head have a roughly
normal distance distribution, as can be seen from Figure 10b.27 We can see from the same
figure that 57% of the errors fall within the interval of ?2 IGs away from the actual
head.
8.3 Sentence Length
Figure 11 shows the distribution of errors over sentences of different lengths. The
x-axis shows the sentence length (measured in number of dependencies), the y-axis
shows the error count, and the z-axis shows the number of sentences. As expected,
the distribution is dominated by short sentences with few errors (especially sentences
of up to seven dependencies with one error). The mean number of errors appears to
be a linear function of sentence length, which would imply that the error probability
27 Error distances with less than 40 occurrences are excluded from the figure.
381
Computational Linguistics Volume 34, Number 3
Figure 11
Error distribution versus sentence length.
per word does not increase with sentence length. This is interesting in that it seems to
indicate that the classifier-based parser does not suffer from error propagation despite
its greedy, deterministic parsing strategy.
9. The Impact of Morphological Disambiguation
In all of the experiments reported herein, we have used the gold-standard tags provided
by the treebank. Another point that deserves investigation is therefore the impact of
using tags automatically assigned by a morphological disambiguator, in other words
the accuracy of the parser on raw text. The role of morphological disambiguators for
highly inflectional languages is far more complex than assigning a single main POS
category (e.g., Noun, Verb, Adj) to a word, and also involves assigning the correct mor-
phological information which is crucial for higher level applications. The complexity of
morphological disambiguation in an agglutinative language like Turkish is due to the
large number of morphological feature tag combinations that can be assigned to words.
The number of potential morphological tag combinations in Turkish for all practical
purposes is very large due to productively derived forms.28
The two subsequent examples, for the words kalemi and asmadan, expose the two
phenomena that a Turkish morphological disambiguator should deal with. The outputs
of the morphological analyzer are listed below the words. The first example shows that
all three possible analyses of the word kalemi have ?Noun? as the POS category but they
differ in that they have different stems and inflectional features. In the second example
28 For the treebank data, the number of distinct combinations of morphological features is 718 for the
word-based model of the classifier-based parser and 108 for the IG-based model.
382
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
we see that the possible analyses also have different IG segmentations; the first two
analyses of the word asmadan consists of two IGs whereas the last one has one IG.
kalemi
kale +Noun+A3sg+P1sg+Acc (?my castle? in accusative form)
kalem +Noun+A3sg+P3sg+Nom (?his pencil?)
kalem +Noun+A3sg+Pnon+Acc (?the pencil? in accusative form)
asmadan
as +Verb+Pos DB +Adverb+WithoutHavingDoneSo (?without having hanged (it)?)
as +Verb+Pos DB +Noun+Inf2+A3sg+Pnon+Abl (?from hanging (it)?)
asma +Noun+A3sg+Pnon+Abl (?from the vine?)
The task of the morphological disambiguator is to choose one of the possible mor-
phological analyses and thus to find the correct inflectional features including parts
of speech, and the IG structure. We first used the two-level morphological analyzer of
Oflazer (1994) to analyze all the words in the treebank.29 This morphological analyzer
simultaneously produces the IG segmentation and the relevant features encoded in all
analyses of a word form. We then used the morphological disambiguator of Yu?ret and
Tu?re (2006), which has a reported accuracy of 96% for Turkish.
When tested on our treebank data, the accuracy of the morphological disambiguator
is 88.4%, including punctuation (which is unambiguous) and using a lookup table for
the words that are not recognized by the morphological analyzer.30 The lower accuracy
of the morphological disambiguator on the treebank can be due to different selections
in the annotation process of the morphological disambiguator training data (Yu?ret and
Tu?re 2006), which is totally different from the treebank data.
In order to investigate the impact of morphological disambiguation errors, we used
our best IG-based model and a lexicalized word-based model with our classifier-based
parser.31 We again evaluated our parsing models with ASU, ASL, and WWU scores.
There is no problem when evaluating withWWU scores because this metric only takes
into account whether the head word assigned to a dependent is correct or not, which
means that any errors of the morphological disambiguator can be ignored. Similarly, in
calculating ASU and ASL scores for the word-based model, dependencies are assumed
to be connected to the first IG of the head word without taking into consideration any
errors in tags caused by the morphological disambiguator. But when evaluating with
the ASU and ASL scores for the IG-based model, one problem that may appear is that
the disambiguator may have assigned a totally different IG structure to the head word,
compared to the gold standard (cf. the three analyses of the word asmadan). In this case,
we accept a dependency link to be correct if the dependent is connected to the correct
head word and the head IG has the same POS category as the gold-standard. This is
reasonable because we know that some of the errors in inflectional features do not affect
the type of dependency very much. For example, if we put the adjective ku?c?u?k (?small?)
29 We noted that 39% of the words were ambiguous and 17% had more than two distinct morphological
analyses.
30 The words not recognized by the morphological analyzer are generally proper nouns, numbers, and
some combined words that are created in the development stage of the treebank and constitute 6.2% of
the whole treebank. If these words are excluded, the accuracy of the tagger is 84.6%.
31 For this model, we added LEX features for ?0, ?0, ?1 to the feature model of our word-based model in
Table 4.
383
Computational Linguistics Volume 34, Number 3
Table 8
Impact of morphological disambiguation on unlabeled and labeled attachment scores and
word-to-word scores.
ASU ASL WWU
Word-based Gold standard 71.2?0.3 62.3?0.3 82.1?0.9
Tagged 69.5?0.3 59.3?0.3 80.2?0.9
IG-based Gold standard 76.0?0.2 67.0?0.3 82.7?0.5
Tagged 73.3?0.3 63.2?0.3 80.6?0.7
in front of the example given previously (ku?c?u?k kalemi), then the choice of morphological
analysis of the noun has no impact on the fact that the adjective should be connected
to the noun with dependency type ?MODIFIER?. Moreover, most of the errors in POS
categories will actually prevent the parser from finding the correct head word, which
can be observed from the drop inWWU accuracy.
Table 8 shows that the IG-based model and the word-based model are equally
affected by the tagging errors and have a drop in accuracy within similar ranges. (It
can also be seen that, even with automatically tagged data, the IG-based model gives
better accuracy than the word-based model.) We can say that the use of an automatic
morphological analyzer and disambiguator causes a drop in the range of 3 percentage
points for unlabeled accuracy and 4 percentage points for labeled accuracy (for both
word-based and IG-based models).
10. Related Work
The first results on the Turkish Treebank come from Eryig?it and Oflazer (2006) where the
authors used only a subset of the treebank sentences containing exclusively head-final
and projective dependencies. The parser used in that paper is a preliminary version of
the probabilistic parser used in this article. The first results on the entire treebank appear
in Nivre et al (2007), where the authors use memory-based learning to predict parser
actions, and in Eryig?it, Adal?, and Oflazer (2006), which introduces the rule-based parser
used in this article.
The Turkish Treebank has recently been parsed by 17 research groups in the CoNLL-
X shared task on multilingual dependency parsing (Buchholz and Marsi 2006), where it
was seen as the most difficult language by the organizers and most of the groups.32 The
following quote is taken from Buchholz and Marsi (page 161): ?The most difficult data
set is clearly the Turkish one. It is rather small, and in contrast to Arabic and Slovene,
which are equally small or smaller, it covers 8 genres, which results in a high percentage
of new FORM and LEMMA values in the test set.?
The results for Turkish are given in Table 9. Our classifier-based parser obtained
the best results for Turkish (with ASU=75.8 and ASL=65.7) and also for Japanese, which
is the only agglutinative and head-final language in the shared task other than Turkish
(Nivre et al 2006). The groups were asked to find the correct IG-to-IG dependency links.
When we look at the results, we observe that most of the best performing parsers use
32 The Turkish data used in the shared task is actually a modified version of the treebank used in this article;
some conversions are made on punctuation structures in order to keep consistency between all languages.
384
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 9
CoNLL-X shared task results on Turkish (taken from Table 5 in Buchholz and Marsi [2006]).
Teams ASU ASL
Nivre et al (2006) 75.8 65.7
Johansson and Nugues (2006) 73.6 63.4
McDonald, Lerman, and Pereira (2006) 74.7 63.2
Corston-Oliver and Aue (2006) 73.1 61.7
Cheng, Asahara, and Matsumoto (2006) 74.5 61.2
Chang, Do, and Roth (2006) 73.2 60.5
Yu?ret (2006) 71.5 60.3
Riedel, C?ak?c?, and Meza-Ruiz (2006) 74.1 58.6
Carreras, Surdeanu, and Marquez (2006) 70.1 58.1
Wu, Lee, and Yang (2006) 69.3 55.1
Shimizu (2006) 68.8 54.2
Bick (2006) 65.5 53.9
Canisius et al (2006) 64.2 51.1
Schiehlen and Spranger (2006) 61.6 49.8
Dreyer, Smith, and Smith (2006) 60.5 46.1
Liu et al (2006) 56.9 41.7
Attardi (2006) 65.3 37.8
one of the parsing algorithms of Eisner (1996), Nivre (2003), or Yamada and Matsumoto
(2003) together with a learning method based on the maximum margin strategy. We
can also see that a common property of the parsers which fall below the average
(ASL=55.4) is that they do not make use of inflectional features, which is crucial for
Turkish.33
Another recent study that has promising results is C?ak?c? and Baldridge (2006),
where the authors use the MSTParser (McDonald, Lerman, and Pereira 2006), also used
in the CoNLL-X shared task (line 3 in Table 9). Following the work of Eryig?it and Oflazer
(2006) and Nivre et al (2006), they use the stem information and the case information
for nominals and they also report an increase in performance by using these features.
Similar to one of the models (?INF as a single feature?) in Eryig?it, Nivre, and Oflazer
(2006), where the feature names of the suffixes provided by the morphological analyzer
are concatenated and used as a feature to the classifier, they use the surface forms of
the suffixes as a whole. We can say that the models in this article cover this approach in
that each suffix is used as a single feature name (which is shown to perform better than
using them concatenated to each other in Eryig?it, Nivre, and Oflazer [2006]). Because in
Turkish, the same suffixes take different forms under vowel harmony34 and the surface
forms of some different suffixes are structurally ambiguous,35 using them with their
feature names is actually more meaningful. C?ak?c? and Baldridge (2006) report a word-
to-word accuracy of 84.9%, which seems competitive, but unfortunately from this we
33 Actually, there are two parsers (Bick 2006 and Attardi 2006 in Table 9) in this group which try to use parts
of the inflectional features under special circumstances.
34 For example, in the words ev+de (?at home?) and okul+da (?at school?), the suffixes -de and -da are the same
locative case suffixes (+Loc) but they take different forms due to vowel harmony.
35 For example, in the word ev+in, the surface morpheme -inmay indicate both a second singular possessive
suffix (+P2sg) which will give the word the meaning of ?your house? and a genitive case (+Gen) which
will give the word the meaning of ?of the house?, as the underlying lexicalmorphemes are different.
385
Computational Linguistics Volume 34, Number 3
are not able to gauge the IG-to-IG accuracy which we have argued is the right metric
to use for Turkish, and their results are not comparable to any of the results in the
literature, because they have not based their experiments on any of the official releases
of the treebank. In addition, they use an evaluation metric different from the ones in
the literature in that they only excluded some of the punctuations from the evaluation
score.
11. Conclusions
In this article, we have investigated a number of issues in data-driven dependency pars-
ing of Turkish. One of the main results is that IG-based models consistently outperform
word-based models. This result holds regardless of whether we evaluate accuracy on
the word level or on the IG level; it holds regardless of whether we use the probabilistic
parser or the classifier-based parser; and it holds even if we take into account the
problem caused by errors in automatic morphological analysis and disambiguation.
Another important conclusion is that the use of morphological information can
increase parsing accuracy substantially. Again, this result has been obtained both for the
probabilistic and the classifier-based parser, although the probabilistic parser requires
careful manual selection of relevant features to counter the effect of data sparseness.
A similar result has been obtained with respect to lexicalization, although in this case
an improvement has only been demonstrated for the classifier-based parser, which is
probably due to its greater resilience to data sparseness.
By combining the deterministic classifier-based parsing approach with an adequate
use of IG-based representations, morphological information, and lexicalization, we have
been able to achieve the highest reported accuracy for parsing the Turkish Treebank.
Acknowledgments
We are grateful for the financial support
from TUBITAK (The Scientific and Technical
Research Council of Turkey) and Istanbul
Technical University. We want to thank Johan
Hall and Jens Nilsson in the language
technology group at Va?xjo? University for
their contributions to the classifier-based
parser framework (MaltParser) within which
we developed the classifier-based parser for
Turkish. We also want to thank Deniz Yu?ret
for providing us with his morphological
disambiguator, and Es?ref Adal? for his
valuable comments. Finally, we want to
thank our three anonymous reviewers for
insightful comments and suggestions
that helped us improve the final version of
the article.
References
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of ACL?05, pages 302?313,
Ann Arbor, MI.
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of CONLL-X,
pages 166?170, New York, NY.
Bick, Eckhard. 2006. Lingpars, a linguistically
inspired, language-independent machine
learner for dependency treebanks. In
Proceedings of CONLL-X, pages 171?175,
New York, NY.
Bikel, Daniel M. 2004. A distributional
analysis of a lexicalized statistical parsing
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, pages 182?189, Barcelona.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied to
the Chinese treebank. In Proceedings of the
2nd Chinese Language Processing Workshop,
pages 1?6, Hong Kong.
Black, Ezra, Frederick Jelinek, John D.
Lafferty, David M. Magerman, Robert L.
Mercer, and Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the 5th DARPA Speech and
Natural Language Workshop, pages 31?37,
New York, NY.
386
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Bozs?ahin, Cem. 2002. The combinatory
morphemic lexicon. Computational
Linguistics, 28(2):145?186.
Buchholz, Sabine and Erwin Marsi.
2006. CONLL-X shared task on
multilingual dependency parsing. In
Proceedings of CONLL-X, pages 149?164,
New York, NY.
C?ak?c?, Ruket and Jason Baldridge. 2006.
Projective and non-projective Turkish
parsing. In Proceedings of the 5th
International Treebanks and Linguistic
Theories Conference, pages 43?54, Prague.
Canisius, Sander, Toine Bogers, Antal
van den Bosch, Jeroen Geertzen, and Erik
Tjong Kim Sang. 2006. Dependency
parsing by inference over high-recall
dependency predictions. In Proceedings of
CONLL-X, pages 176?180, New York, NY.
Carreras, Xavier, Mihai Surdeanu, and Lluis
Marquez. 2006. Projective dependency
parsing with perceptron. In Proceedings of
CONLL-X, pages 181?185, New York, NY.
Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vector
Machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Chang, Ming-Wei, Quang Do, and Dan Roth.
2006. A pipeline model for bottom-up
dependency parsing. In Proceedings of
CONLL-X, pages 186?190, New York, NY.
Cheng, Yuchang, Masayuki Asahara, and
Yuji Matsumoto. 2006. Multi-lingual
dependency parsing at NAIST. In
Proceedings of CONLL-X, pages 191?195,
New York, NY.
Chung, Hoojung and Hae-Chang Rim. 2004.
Unlexicalized dependency parser for
variable word order languages based on
local contextual pattern. In Proceedings of
the 5th International Conference on Intelligent
Text Processing and Computational
Linguistics, pages 109?120, Seoul.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of ACL?96,
pages 184?191, Santa Cruz, CA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL?97, pages 16?23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael, Jan Hajic, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of ACL?99,
pages 505?518, College Park, MD.
Corazza, Anna, Alberto Lavelli, Giorgio
Satta, and Roberto Zanoli. 2004. Analyzing
an Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the 3rd
Workshop on Treebanks and Linguistic
Theories, pages 39?50, Tu?bingen.
Corston-Oliver, Simon and Anthony Aue.
2006. Dependency parsing with reference
to Slovene, Spanish and Swedish. In
Proceedings of CONLL-X, pages 196?200,
New York, NY.
Daelemans, Walter and Antal Vanden
Bosch. 2005.Memory-Based Language
Processing. Cambridge University Press,
Cambridge.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In Proceedings of CONLL-X,
pages 201?205, New York, NY.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings
of ACL?03, pages 96?103, Sapporo.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 340?345, Copenhagen.
Erguvanl?, Eser Emine. 1979. The Function
of Word Order in Turkish Grammar.
Ph.D. thesis, UCLA.
Eryig?it, Gu?ls?en. 2006. Tu?rkc?enin Bag?l?l?k
Ayr?s?t?rmas? (Dependency Parsing of Turkish).
Ph.D. thesis, Istanbul Technical University.
Eryig?it, Gu?ls?en, Es?ref Adal?, and Kemal
Oflazer. 2006. Tu?rkc?e cu?mlelerin kural
tabanl? bag?l?l?k analizi [Rule-based
dependency parsing of Turkish sentences].
In Proceedings of the 15th Turkish Symposium
on Artificial Intelligence and Neural
Networks, pages 17?24, Mug?la.
Eryig?it, Gu?ls?en, Joakim Nivre, and Kemal
Oflazer. 2006. The incremental use of
morphological information and
lexicalization in data-driven dependency
parsing. In Computer Processing of Oriental
Languages, Beyond the Orient: The Research
Challenges Ahead, pages 498?507,
Singapore.
Eryig?it, Gu?ls?en and Kemal Oflazer. 2006.
Statistical dependency parsing of Turkish.
In Proceedings of EACL?06, pages 89?96,
Trento.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 167?202,
Pittsburgh, PA.
387
Computational Linguistics Volume 34, Number 3
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Hladka?.
2001. Prague dependency treebank 1.0
(final production label). CDROM CAT:
LDC2001T10., ISBN 1-58563-212-0.
Hakkani-Tu?r, Dilek, Kemal Oflazer, and
Go?khan Tu?r. 2002. Statistical
morphological disambiguation for
agglutinative languages. Journal of
Computers and Humanities, 36(4):381?410.
Hoffman, Beryl. 1994. Generating context
appropriate word orders in Turkish. In
Proceedings of the Seventh International
Workshop on Natural Language Generation,
pages 117?126, Kennebunkport, ME.
Johansson, Richard and Pierre Nugues. 2006.
Investigating multilingual dependency
parsing. In Proceedings of CONLL-X,
pages 206?210, New York, NY.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of ACL?03, pages 423?430,
Sapporo.
Kromann, Matthias T. 2003. The Danish
dependency treebank and the underlying
linguistic theory. In Proceedings of the 2nd
Workshop on Treebanks and Linguistic
Theories, pages 217?220, Va?xjo?.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Conference on Computational Natural
Language Learning, pages 63?69, Taipei.
Levy, Roger and Christopher Manning. 2003.
Is it harder to parse Chinese, or the
Chinese treebank? In Proceedings of ACL?03,
pages 439?446, Sapporo.
Liu, Ting, Jinshan Ma, Huijia Zhu, and
Sheng Li. 2006. Dependency parsing based
on dynamic local optimization. In
Proceedings of CONLL-X, pages 211?215,
New York, NY.
Magerman, David M. 1995. Statistical
decision-tree models for parsing. In
Proceedings of ACL?95, pages 276?283,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings of
CONLL-X, pages 216?220, New York, NY.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing. In
Proceedings of the Workshop on Incremental
Parsing: Bringing Engineering and Cognition
Together, pages 50?57, Barcelona.
Nivre, Joakim. 2006. Inductive Dependency
Parsing. Springer, Dordrecht.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it, Sandra
Ku?bler, Stetoslav Marinov, and Erwin
Marsi. 2007. Maltparser: A
language-independent system for
data-driven dependency parsing. Natural
Language Engineering Journal, 13(2):95?135.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?ls?en Eryig?it, and Stetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of CONLL-X,
pages 221?225, New York, NY.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
Proceedings of ACL?05, pages 99?106, Ann
Arbor, MI.
Nivre, Joakim, Jens Nilsson, and Johan Hall.
2006. Talbanken05: A Swedish treebank
with phrase structure and dependency
annotation. In Proceedings of LREC,
pages 1392?1395, Genoa.
Oflazer, Kemal. 1994. Two-level description
of Turkish morphology. Literary and
Linguistic Computing, 9(2):137?148.
Oflazer, Kemal. 2003. Dependency parsing
with an extended finite-state approach.
Computational Linguistics, 29(4):515?544.
Oflazer, Kemal, Bilge Say, Dilek Z.
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?,
editor, Treebanks: Building and Using Parsed
Corpora. Kluwer, London, pages 261?277.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1?10,
Providence, RI.
Riedel, Sebastian, Ruket C?ak?c?, and
Ivan Meza-Ruiz. 2006. Multi-lingual
dependency parsing with incremental
integer linear programming. In
Proceedings of CONLL-X, pages 226?230,
New York, NY.
388
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear run-time
complexity. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 125?132, Vancouver.
Schiehlen, Michael and Kristina Spranger.
2006. Language independent probabilistic
context-free parsing bolstered by machine
learning. In Proceedings of CONLL-X,
pages 231?235, New York, NY.
Sekine, Satoshi, Kiyotaka Uchimoto, and
Hitoshi Isahara. 2000. Backward beam
search algorithm for dependency analysis
of Japanese. In Proceedings of the 17th
International Conference on Computational
Linguistics, pages 754?760, Saarbru?cken.
Shimizu, Nobuyuki. 2006. Maximum
spanning tree algorithm for non-projective
labeled dependency parsing. In
Proceedings of CONLL-X, pages 236?240,
New York, NY.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of
the World. Lincom-Europa, Munich,
pages 135?142.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer, New
York, NY.
Wu, Yu-Chieh, Yue-Shi Lee, and Jie-Chi
Yang. 2006. The exploration of
deterministic and efficient dependency
parsing. In Proceedings of CONLL-X,
pages 241?245, New York, NY.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy.
Yu?ret, Deniz. 2006. Dependency parsing as a
classification problem. In Proceedings of
CONLL-X, pages 246?250, New York, NY.
Yu?ret, Deniz and Ferhan Tu?re. 2006.
Learning morphological disambiguation
rules for Turkish. In Proceedings of
HLT/NAACL?06, pages 328?334,
New York, NY.
389

This article has been cited by:
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 153?160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Morphology-Syntax Interface for Turkish LFG
?Ozlem C?etinog?lu
Faculty of Engineering and Natural Sciences
Sabanc? University
34956, Istanbul, Turkey
ozlemc@su.sabanciuniv.edu
Kemal Oflazer
Faculty of Engineering and Natural Sciences
Sabanc? University
34956, Istanbul, Turkey
oflazer@sabanciuniv.edu
Abstract
This paper investigates the use of sublexi-
cal units as a solution to handling the com-
plex morphology with productive deriva-
tional processes, in the development of
a lexical functional grammar for Turkish.
Such sublexical units make it possible to
expose the internal structure of words with
multiple derivations to the grammar rules
in a uniform manner. This in turn leads to
more succinct and manageable rules. Fur-
ther, the semantics of the derivations can
also be systematically reflected in a com-
positional way by constructing PRED val-
ues on the fly. We illustrate how we use
sublexical units for handling simple pro-
ductive derivational morphology and more
interesting cases such as causativization,
etc., which change verb valency. Our pri-
ority is to handle several linguistic phe-
nomena in order to observe the effects of
our approach on both the c-structure and
the f-structure representation, and gram-
mar writing, leaving the coverage and
evaluation issues aside for the moment.
1 Introduction
This paper presents highlights of a large scale lex-
ical functional grammar for Turkish that is being
developed in the context of the ParGram project1
In order to incorporate in a manageable way, the
complex morphology and the syntactic relations
mediated by morphological units, and to handle
lexical representations of very productive deriva-
tions, we have opted to develop the grammar using
sublexical units called inflectional groups.
Inflectional groups (IGs hereafter) represent the
inflectional properties of segments of a complex
1http://www2.parc.com/istl/groups/nltt/
pargram/
word structure separated by derivational bound-
aries. An IG is typically larger than a morpheme
but smaller than a word (except when the word has
no derivational morphology in which case the IG
corresponds to the word). It turns out that it is
the IGs that actually define syntactic relations be-
tween words. A grammar for Turkish that is based
on words as units would have to refer to informa-
tion encoded at arbitrary positions in words, mak-
ing the task of the grammar writer much harder.
On the other hand, treating morphemes as units in
the grammar level implies that the grammar will
have to know about morphotactics making either
the morphological analyzer redundant, or repeat-
ing the information in the morphological analyzer
at the grammar level which is not very desirable.
IGs bring a certain form of normalization to the
lexical representation of a language like Turkish,
so that units in which the grammar rules refer to
are simple enough to allow easy access to the in-
formation encoded in complex word structures.
That IGs delineate productive derivational pro-
cesses in words necessitates a mechanism that re-
flects the effect of the derivations to semantic rep-
resentations and valency changes. For instance,
English LFG (Kaplan and Bresnan, 1982) repre-
sents derivations as a part of the lexicon; both
happy and happiness are separately lexicalized.
Lexicalized representations of adjectives such as
easy and easier are related, so that both lexicalized
and phrasal comparatives would have the same
feature structure; easier would have the feature
structure
(1)  





PRED ?easy?
ADJUNCT

PRED ?more?

DEG-DIM pos
DEGREE comparative






Encoding derivations in the lexicon could be ap-
plicable for languages with relatively unproduc-
tive derivational phenomena, but it certainly is not
153
possible to represent in the grammar lexicon,2 all
derived forms as lexemes for an agglutinative lan-
guage like Turkish. Thus one needs to incorpo-
rate such derivational processes in a principled
way along with the computation of the effects on
derivations on the representation of the semantic
information.
Lexical functional grammar (LFG) (Kaplan and
Bresnan, 1982) is a theory representing the syn-
tax in two parallel levels: Constituent structures
(c-structures) have the form of context-free phrase
structure trees. Functional structures (f-structures)
are sets of pairs of attributes and values; attributes
may be features, such as tense and gender, or func-
tions, such as subject and object. C-structures de-
fine the syntactic representation and f-structures
define more semantic representation. Therefore
c-structures are more language specific whereas
f-structures of the same phrase for different lan-
guages are expected to be similar to each other.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related work both on
Turkish, and on issues similar to those addressed
in this paper. Section 3 motivates and presents IGs
while Section 4 explains how they are employed
in a LFG setting. Section 5 summarizes the ar-
chitecture and the current status of the our system.
Finally we give conclusions in Section 6.
2 Related Work
Gu?ngo?rdu? and Oflazer (1995) describes a rather
extensive grammar for Turkish using the LFG
formalism. Although this grammar had a good
coverage and handled phenomena such as free-
constituent order, the underlying implementation
was based on pseudo-unification. But most cru-
cially, it employed a rather standard approach to
represent the lexical units: words with multiple
nested derivations were represented with complex
nested feature structures where linguistically rel-
evant information could be embedded at unpre-
dictable depths which made access to them in rules
extremely complex and unwieldy.
Bozs?ahin (2002) employed morphemes overtly
as lexical units in a CCG framework to account
for a variety of linguistic phenomena in a pro-
totype implementation. The drawback was that
morphotactics was explicitly raised to the level of
the sentence grammar, hence the categorial lexi-
con accounted for both constituent order and the
morpheme order with no distinction. Oflazer?s de-
pendency parser (2003) used IGs as units between
which dependency relations were established. An-
other parser based on IGs is Eryig?it and Oflazer?s
2We use this term to distinguish the lexicon used by the
morphological analyzer.
(2006) statistical dependency parser for Turkish.
C?ak?c? (2005), used relations between IG-based
representations encoded within the Turkish Tree-
bank (Oflazer et al, 2003) to automatically induce
a CCG grammar lexicon for Turkish.
In a more general setting, Butt and King (2005)
have handled the morphological causative in Urdu
as a separate node in c-structure rules using LFG?s
restriction operator in semantic construction of
causatives. Their approach is quite similar to ours
yet differs in an important way: the rules explicitly
use morphemes as constituents so it is not clear if
this is just for this case, or all morphology is han-
dled at the syntax level.
3 Inflectional Groups as Sublexical Units
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1994). At the syntax
level, the unmarked constituent order is SOV, but
constituent order may vary freely as demanded by
the discourse context. Essentially all constituent
orders are possible, especially at the main sen-
tence level, with very minimal formal constraints.
In written text however, the unmarked order is
dominant at both the main sentence and embedded
clause level.
Turkish morphotactics is quite complicated: a
given word form may involve multiple derivations
and the number of word forms one can generate
from a nominal or verbal root is theoretically in-
finite. Turkish words found in typical text aver-
age about 3-4 morphemes including the stem, with
an average of about 1.23 derivations per word,
but given that certain noninflecting function words
such as conjuctions, determiners, etc. are rather
frequent, this number is rather close to 2 for in-
flecting word classes. Statistics from the Turkish
Treebank indicate that for sentences ranging be-
tween 2 words to 40 words (with an average of
about 8 words), the number of IGs range from 2
to 55 IGs (with an average of 10 IGs per sentence)
(Eryig?it and Oflazer, 2006).
The morphological analysis of a word can be
represented as a sequence of tags corresponding
to the morphemes. In our morphological analyzer
output, the tag ?DB denotes derivation boundaries
that we also use to define IGs. If we represent the
morphological information in Turkish in the fol-
lowing general form:
root+IG
 
  DB+IG

  DB+        DB+IG
 
.
then each IG
 
denotes the relevant sequence of in-
flectional features including the part-of-speech for
the root (in IG
 
) and for any of the derived forms.
A given word may have multiple such representa-
tions depending on any morphological ambiguity
brought about by alternative segmentations of the
154
Figure 1: Modifier-head relations in the NP eski
kitaplar?mdaki hikayeler
word, and by ambiguous interpretations of mor-
phemes.
For instance, the morphological analysis of
the derived modifier cezaland?r?lacak (lit-
erally, ?(the one) that will be given punishment?)
would be :3
ceza(punishment)+Noun+A3sg+Pnon+Nom
?DB+Verb+Acquire
?DB+Verb+Caus
?DB+Verb+Pass+Pos
?DB+Adj+FutPart+Pnon
The five IGs in this word are:
1. +Noun+A3sg+Pnon+Nom
2. +Verb+Acquire
3. +Verb+Caus
4. +Verb+Pass+Pos
5. +Adj+FutPart+Pnon
The first IG indicates that the root is a singular
noun with nominative case marker and no posses-
sive marker. The second IG indicates a deriva-
tion into a verb whose semantics is ?to acquire?
the preceding noun. The third IG indicates that a
causative verb (equivalent to ?to punish? in En-
glish), is derived from the previous verb. The
fourth IG indicates the derivation of a passive verb
with positive polarity from the previous verb. Fi-
nally the last IG represents a derivation into future
participle which will function as a modifier in the
sentence.
The simple phrase eski kitaplar?mdaki hikayeler
(the stories in my old books) in Figure 1 will help
clarify how IGs are involved in syntactic relations:
Here, eski (old) modifies kitap (book) and not
hikayeler (stories),4 and the locative phrase eski
3The morphological features other than the obvious part-
of-speech features are: +A3sg: 3sg number-person agree-
ment, +Pnon: no possesive agreement, +Nom: Nominative
case, +Acquire: acquire verb, +Caus: causative verb,
+Pass: passive verb, +FutPart: Derived future participle,
+Pos: Positive Polarity.
4Though looking at just the last POS of the words one
sees an +Adj +Adj +Noun sequence which may imply
that both adjectives modify the noun hikayeler
kitaplar?mda (in my old books) modifies hikayeler
with the help of derivational suffix -ki. Morpheme
boundaries are represented by ?+? sign and mor-
phemes in solid boxes actually define one IG. The
dashed box around solid boxes is for word bound-
ary. As the example indicates, IGs may consist of
one or more morphemes.
Example (2) shows the corresponding f-
structure for this NP. Supporting the dependency
representation in Figure 1, f-structure of adjective
eski is placed as the adjunct of kitaplar?mda, at
the innermost level. The semantics of the relative
suffix -ki is shown as ?rel  OBJ? where the f-
structure that represents the NP eski kitaplar?mda
is the OBJ of the derived adjective. The new f-
structure with a PRED constructed on the fly, then
modifies the noun hikayeler. The derived adjective
behaves essentially like a lexical adjective. The ef-
fect of using IGs as the representative units can be
explicitly seen in c-structure where each IG cor-
responds to a separate node as in Example (3).5
Here, DS stands for derivational suffix.
(2)
 















PRED ?hikaye?
ADJUNCT
 









PRED ?rel kitap?
OBJ
 




PRED ?kitap?
ADJUNCT

PRED ?eski?
ATYPE attributive

CASE loc, NUM pl





ATYPE attributive










CASE NOM, NUM PL
















(3) NP
 
 
 
 




AP
 
 
 
 




NP






AP
A
eski
NP
N
kitaplar?mda
DS
ki
NP
N
hikayeler
Figure 2 shows the modifier-head relations for
a more complex example given in Example (4)
where we observe a chain/hierarchy of relations
between IGs
(4) mavi
blue
renkli
color-WITH
elbiselideki
dress-WITH-LOC-REL
kitap
book
5Note that placing the sublexical units of a word in sepa-
rate nodes goes against the Lexical Integrity principle of LFG
(Dalrymple, 2001). The issue is currently being discussed
within the LFG community (T. H. King, personal communi-
cation).
155
?the book on the one with the blue colored
dress?
Figure 2: Syntactic Relations in the NP mavi ren-
kli elbiselideki kitap
Examples (5) and (6) show respectively the con-
stituent structure (c-structure) and the correspond-
ing feature structure (f-structure) for this noun
phrase. Within the tree representation, each IG
corresponds to a separate node. Thus, the LFG
grammar rules constructing the c-structures are
coded using IGs as units of parsing. If an IG con-
tains the root morpheme of a word, then the node
corresponding to that IG is named as one of the
syntactic category symbols. The rest of the IGs
are given the node name DS (to indicate deriva-
tional suffix), no matter what the content of the IG
is.
The semantic representation of derivational suf-
fixes plays an important role in f-structure con-
struction. In almost all cases, each derivation that
is induced by an overt or a covert affix gets a OBJ
feature which is then unified with the f-structure of
the preceding stem already constructed, to obtain
the feature structure of the derived form, with the
PRED of the derived form being constructed on
the fly. A PRED feature thus constructed however
is not meant to necessarily have a precise lexical
semantics. Most derivational suffixes have a con-
sistent (lexical) semantics6, but some don?t, that
is, the precise additional lexical semantics that the
derivational suffix brings in, depends on the stem
it is affixed to. Nevertheless, we represent both
cases in the same manner, leaving the determina-
tion of the precise lexical semantics aside.
If we consider Figure 2 in terms of dependency
relations, the adjective mavi (blue) modifies the
noun renk (color) and then the derivational suf-
fix -li (with) kicks in although the -li is attached
to renk only. Therefore, the semantics of the
phrase should be with(blue color), not blue
with(color). With the approach we take, this
difference can easily be represented in both the f-
structure as in the leftmost branch in Example (5)
6e.g., the ?to acquire? example earlier
and the c-structure as in the middle ADJUNCT
f-structure in Example (6). Each DS in c-structure
gives rise to an OBJject in c-structure. More pre-
cisely, a derived phrase is always represented as
a binary tree where the right daughter is always
a DS. In f-structure DS unifies with the mother f-
structure and inserts PRED feature which subcat-
egorizes for a OBJ. The left daughter of the bi-
nary tree is the original form of the phrase that is
derived, and it unifies with the OBJ of the mother
f-structure.
(5)
NP






AP






NP






AP






NP






AP




NP




AP
A
mavi
NP
N
renk
DS
li
NP
N
elbise
DS
li
DS
de
DS
ki
NP
N
kitap
4 Inflectional Groups in Practice
We have already seen how the IGs are used to con-
struct on the fly PRED features that reflect the
lexical semantics of the derivation. In this section
we describe how we handle phenomena where the
derivational suffix in question does not explicitly
affect the semantic representation in PRED fea-
ture but determines the semantic role so as to unify
the derived form or its components with the appro-
priate external f-structure.
4.1 Sentential Complements and Adjuncts,
and Relative Clauses
In Turkish, sentential complements and adjuncts
are marked by productive verbal derivations into
nominals (infinitives, participles) or adverbials,
while relative clauses with subject and non-subject
(object or adjunct) gaps are formed by participles
which function as adjectivals modifying a head
noun.
Example (7) shows a simple sentence that will
be used in the following examples.
156
(6)  




































PRED ?kitap?
ADJUNCT
 






























PRED ?rel zero-deriv?
OBJ
 

























PRED ?zero-deriv with?
OBJ
 



















PRED ?with elbise?
OBJ
 














PRED ?elbise?
ADJUNCT
 








PRED ?with renk?
OBJ
 



PRED ?renk?
ADJUNCT

PRED ?mavi?

CASE nom, NUM sg, PERS 3




ATYPE attributive









CASE nom, NUM sg, PERS 3















ATYPE attributive




















CASE loc, NUM sg, PERS 3


























ATYPE attributive































CASE NOM, NUM SG, PERS 3





































(7) K?z
Girl-NOM
adam?
man-ACC
arad?.
call-PAST
?The girl called the man?
In (8), we see a past-participle form heading a
sentential complement functioning as an object for
the verb so?yledi (said).
(8) Manav
Grocer-NOM
k?z?n
girl-GEN
adam?
man-ACC
arad?g??n?
call-PASTPART-ACC
so?yledi.
say-PAST
?The grocer said that the girl called the man?
Once the grammar encounters such a sentential
complement, everything up to the participle IG is
parsed, as a normal sentence and then the partici-
ple IG appends nominal features, e.g., CASE, to
the existing f-structure. The final f-structure is for
a noun phrase, which now is the object of the ma-
trix verb, as shown in Example (9). Since the par-
ticiple IG has the right set of syntactic features of
a noun, no new rules are needed to incorporate the
derived f-structure to the rest of the grammar, that
is, the derived phrase can be used as if it is a sim-
ple NP within the rules. The same mechanism is
used for all kinds of verbal derivations into infini-
tives, adverbial adjuncts, including those deriva-
tions encoded by lexical reduplications identified
by multi-word construct processors.
(9)  






























PRED ?so?yle manav, ara?
SUBJ

PRED ?manav?
CASE nom, NUM sg, PERS 3

OBJ
 















PRED ?ara k z, adam?
SUBJ

PRED ?k z?
CASE gen, NUM sg, PERS 3

OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3

CHECK

PART pastpart
	
CASE acc, NUM sg, PERS 3, VTYPE main
CLAUSE-TYPE nom
















TNS-ASP

TENSE past
	
NUM SG, PERS 3, VTYPE MAIN































Relative clauses also admit to a similar mech-
anism. Relative clauses in Turkish are gapped
sentences which function as modifiers of nominal
heads. Turkish relative clauses have been previ-
ously studied (Barker et al, 1990; Gu?ngo?rdu? and
Engdahl, 1998) and found to pose interesting is-
sues for linguistic and computational modeling.
Our aim here is not to address this problem in its
generality but show with a simple example, how
our treatment of IGs encoding derived forms han-
dle the mechanics of generating f-structures for
such cases.
Kaplan and Zaenen (1988) have suggested a
general approach for handling long distance de-
pendencies. They have extended the LFG notation
and allowed regular expressions in place of sim-
ple attributes within f-structure constraints so that
phenomena requiring infinite disjunctive enumer-
ation can be described with a finite expression. We
basically follow this approach and once we derive
the participle phrase we unify it with the appro-
priate argument of the verb using rules based on
functional uncertainty. Example (10) shows a rel-
ative clause where a participle form is used as a
modifier of a head noun, adam in this case.
(10) Manav?n
Grocer-GEN
k?z?n
girl-GEN
[]

obj-gap
arad?g??n?
call-PASTPART-ACC
so?yledig?i
say-PASTPART
adam

man-NOM
?The man the grocer said the girl called?
This time, the sentence is parsed with a gap with
an appropriate functional uncertainty constraint,
and when the participle IG is encountered the sen-
tence f-structure is derived into an adjective and
the gap in the derived form, the object here, is
then unified with the head word as marked with
co-indexation in Example (11).
The example sentence (10) includes Example
(8) as a relative clause with the object extracted,
hence the similarity in the f-structures can be ob-
served easily. The ADJUNCT in Example (11)
157
is almost the same as the whole f-structure of Ex-
ample (9), differing in TNS-ASP and ADJUNCT-
TYPE features. At the grammar level, both the rel-
ative clause and the complete sentence is parsed
with the same core sentence rule. To understand
whether the core sentence is a complete sentence
or not, the finite verb requirement is checked.
Since the requirement is met by the existence of
TENSE feature, Example (8) is parsed as a com-
plete sentence. Indeed the relative clause also in-
cludes temporal information as ?pastpart? value of
PART feature, of the ADJUNCT f-structure, de-
noting a past event.
(11)  





































PRED ?adam?  
ADJUNCT
 































PRED ?so?yle manav, ara?
SUBJ

PRED ?manav?
CASE gen, NUM sg, PERS 3
	
OBJ
 














PRED ?ara kz, adam?
SUBJ

PRED ?kz?
CASE gen, NUM sg, PERS 3
	
OBJ

PRED ?adam?

 
CHECK

PART pastpart

CASE acc, NUM sg, PERS 3, VTYPE main
CLAUSE-TYPE nom















CHECK

PART pastpart

NUM sg, PERS 3, VTYPE main
ADJUNCT-TYPE relative
































CASE NOM, NUM SG, PERS 3






































4.2 Causatives
Turkish verbal morphotactics allows the produc-
tion multiply causative forms for verbs.7 Such
verb formations are also treated as verbal deriva-
tions and hence define IGs. For instance, the mor-
phological analysis for the verb arad? (s/he called)
is
ara+Verb+Pos+Past+A3sg
and for its causative aratt? (s/he made (someone
else) call) the analysis is
ara+Verb?DB+Verb+Caus+Pos+Past+A3sg.
In Example (12) we see a sentence and its
causative form followed by respective f-structures
for these sentences in Examples (13) and (14). The
detailed morphological analyses of the verbs are
given to emphasize the morphosyntactic relation
between the bare and causatived versions of the
verb.
(12) a. K?z
Girl-NOM
adam?
man-ACC
arad?.
call-PAST
?The girl called the man?
b. Manav
Grocer-NOM
k?za
girl-DAT
adam?
man-ACC
aratt?.
call-CAUS-PAST
?The grocer made the girl call the man?
7Passive, reflexive, reciprocal/collective verb formations
are also handled in morphology, though the latter two are not
productive due to semantic constraints. On the other hand
it is possible for a verb to have multiple causative markers,
though in practice 2-3 seem to be the maximum observed.
(13)  













PRED ?ara k z, adam?
SUBJ

PRED ?k z?
CASE nom, NUM sg, PERS 3

OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3

TNS-ASP

TENSE past
	
NUM SG, PERS 3,VTYPE MAIN














(14)  






























PRED ?caus manav, k z, adam, ara k z , adam?
SUBJ

PRED ?manav?
	
OBJ

PRED ?k z?
	
 
OBJTH

PRED ?adam?
	

XCOMP
 









PRED ?ara k z , adam?
SUBJ

PRED ?k z?
CASE dat, NUM sg, PERS 3

 
OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3


VTYPE main










TNS-ASP

TENSE past
	
NUM SG, PERS 3,VTYPE MAIN































The end-result of processing an IG which has a
verb with a causative form is to create a larger f-
structure whose PRED feature has a SUBJect, an
OBJect and a XCOMPlement. The f-structure of
the first verb is the complement in the f-structure
of the causative form, that is, its whole structure is
embedded into the mother f-structure in an encap-
sulated way. The object of the causative (causee
- that who is caused by the causer ? the sub-
ject of the causative verb) is unified with the sub-
ject the inner f-structure. If the original verb is
transitive, the object of the original verb is fur-
ther unified with the OBJTH of the causative
verb. All of grammatical functions in the inner
f-structure, namely XCOMP, are also represented
in the mother f-structure and are placed as argu-
ments of caus since the flat representation is re-
quired to enable free word order in sentence level.
Though not explicit in the sample f-structures,
the important part is unifying the object and for-
mer subject with appropriate case markers, since
the functions of the phrases in the sentence are de-
cided with the help of case markers due to free
word order. If the verb that is causativized sub-
categorizes for an direct object in accusative case,
after causative formation, the new object unified
with the subject of the causativized verb should
be in dative case (Example 15). But if the verb
in question subcategorizes for a dative or an abla-
tive oblique object, then this object will be trans-
formed into a direct object in accusative case after
causativization (Example 16). That is, the causati-
vation will select the case of the object of the
causative verb, so as not to ?interfere? with the ob-
ject of the verb that is causativized. In causativized
intransitive verbs the causative object is always in
accusative case.
158
(15) a. adam
man-NOM
kad?n?
woman-ACC
arad?.
call-PAST
?the man called the woman?
b. adama
man-DAT
kad?n?
woman-ACC
aratt?.
call-CAUS-PAST
?(s/he) made the man call the woman?
(16) a. adam
man-NOM
kad?na
woman-DAT
vurdu.
hit-PAST
?the man hit the woman?
b. adam?
man-ACC
kad?na
woman-DAT
vurdurdu.
hit-CAUS-PAST
?(s/he) made the man hit the woman?
All other derivational phenomena can be solved in
a similar way by establishing the appropriate se-
mantic representation for the derived IG and its
effect on the semantic representation.
5 Current Implementation
The implementation of the Turkish LFG gram-
mar is based on the Xerox Linguistic Environ-
ment (XLE) (Maxwell III and Kaplan, 1996), a
grammar development platform that facilitates the
integration of various modules, such as tokeniz-
ers, finite-state morphological analyzers, and lex-
icons. We have integrated into XLE, a series of
finite state transducers for morphological analysis
and for multi-word processing for handling lexi-
calized, semi-lexicalized collocations and a lim-
ited form of non-lexicalized collocations.
The finite state modules provide the rele-
vant ambiguous morphological interpretations for
words and their split into IGs, but do not provide
syntactically relevant semantic and subcategoriza-
tion information for root words. Such information
is encoded in a lexicon of root words on the gram-
mar side.
The grammar developed so far addresses many
important aspects ranging from free constituent or-
der, subject and non-subject extractions, all kinds
of subordinate clauses mediated by derivational
morphology and has a very wide coverage NP sub-
grammar. As we have also emphasized earlier, the
actual grammar rules are oblivious to the source of
the IGs, so that the same rule handles an adjective
- noun phrase regardless of whether the adjective
is lexical or a derived one. So all such relations in
Figure 28 are handled with the same phrase struc-
ture rule.
The grammar is however lacking the treatment
of certain interesting features of Turkish such as
suspended affixation (Kabak, 2007) in which the
inflectional features of the last element in a co-
ordination have a phrasal scope, that is, all other
8Except the last one which requires some additional treat-
ment with respect to definiteness.
coordinated constituents have certain default fea-
tures which are then ?overridden? by the features
of the last element in the coordination. A very sim-
ple case of such suspended affixation is exempli-
fied in (17a) and (17b). Note that although this is
not due to derivational morphology that we have
emphasized in the previous examples, it is due to
a more general nature of morphology in which af-
fixes may have phrasal scopes.
(17) a. k?z
girl
adam
man-NOM
ve
and
kad?n?
woman-ACC
arad?.
call-PAST
?the girl called the man and the woman?
b. k?z
girl
[adam
[man
ve
and
kad?n]-?
woman]-ACC
arad?.
call-PAST
?the girl called the man and the woman?
Suspended affixation is an example of a phe-
nomenon that IGs do not seem directly suitable
for. The unification of the coordinated IGs have to
be done in a way in which non-default features of
the final constituent is percolated to the upper node
in the tree as is usually done with phrase struc-
ture grammars but unlike coordination is handled
in such grammars.
6 Conclusions and Future Work
This paper has described the highlights of our
work on developing a LFG grammar for Turkish
employing sublexical constituents, that we have
called inflectional groups. Such a sublexical con-
stituent choice has enabled us to handle the very
productive derivational morphology in Turkish in
a rather principled way and has made the grammar
more or less oblivious to morphological complex-
ity.
Our current and future work involves extending
the coverage of the grammar and lexicon as we
have so far included in the grammar lexicon only
a small subset of the root lexicon of the morpho-
logical analyzer, annotated with the semantic and
subcategorization features relevant to the linguis-
tic phenomena that we have handled. We also in-
tend to use the Turkish Treebank (Oflazer et al,
2003), as a resource to extract statistical informa-
tion along the lines of Frank et al (2003) and
O?Donovan et al (2005).
Acknowledgement
This work is supported by TUBITAK (The Scien-
tific and Technical Research Council of Turkey)
by grant 105E021.
159
References
Chris Barker, Jorge Hankamer, and John Moore, 1990.
Grammatical Relations, chapter Wa and Ga in Turk-
ish. CSLI.
Cem Bozs?ahin. 2002. The combinatory morphemic
lexicon. Computational Linguistics, 28(2):145?186.
Miriam Butt and Tracey Holloway King. 2005.
Restriction for morphological valency alternations:
The Urdu causative. In Proceedings of The 10th
International LFG Conference, Bergen, Norway.
CSLI Publications.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73?78, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press, New York.
Gu?ls?en Eryig?it and Kemal Oflazer. 2006. Statisti-
cal dependency parsing for turkish. In Proceedings
of EACL 2006 - The 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy. Association for Computa-
tional Linguistics.
Anette Frank, Louisa Sadler, Josef van Genabith, and
Andy Way. 2003. From treebank resources to LFG
f-structures:automatic f-structure annotation of tree-
bank trees and CFGs extracted from treebanks. In
Anne Abeille, editor, Treebanks. Kluwer Academic
Publishers, Dordrecht.
Zelal Gu?ngo?rdu? and Elisabeth Engdahl. 1998. A rela-
tional approach to relativization in Turkish. In Joint
Conference on Formal Grammar, HPSG and Cate-
gorial Grammar, Saarbru?cken, Germany, August.
Zelal Gu?ngo?rdu? and Kemal Oflazer. 1995. Parsing
Turkish using the Lexical Functional Grammar for-
malism. Machine Translation, 10(4):515?544.
Bar?s? Kabak. 2007. Turkish suspended affixation. Lin-
guistics, 45. (to appear).
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. MIT Press, Cambridge, MA.
Ronald M. Kaplan and Annie Zaenen. 1988. Long-
distance dependencies, constituent structure, and
functional uncertainty. In M. Baitin and A. Kroch,
editors, Alternative Conceptions of Phrase Struc-
ture. University of Chicago Press, Chicago.
John T. Maxwell III and Ronald M. Kaplan. 1996.
An efficient parser for LFG. In Miriam Butt and
Tracy Holloway King, editors, The Proceedings of
the LFG ?96 Conference, Rank Xerox, Grenoble.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the Penn-II and Penn-III Treebanks. Computational
Linguistics, 31(3):329?365.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer Aca-
demic Publishers.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137?148.
Kemal Oflazer. 2003. Dependency parsing with an
extended finite-state approach. Computational Lin-
guistics, 29(4):515?544.
160
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 189?192,
Prague, June 2007. c?2007 Association for Computational Linguistics
Machine Translation between Turkic Languages
A. Cu?neyd TANTUG?
Istanbul Technical University
Istanbul, Turkey
tantug@itu.edu.tr
Es?ref ADALI
Istanbul Technical University
Istanbul, Turkey
adali@itu.edu.tr
Kemal OFLAZER
Sabanci University
Istanbul, Turkey
oflazer@sabanciuniv.edu
Abstract
We present an approach to MT between Tur-
kic languages and present results from an
implementation of a MT system from Turk-
men to Turkish. Our approach relies on am-
biguous lexical and morphological transfer
augmented with target side rule-based re-
pairs and rescoring with statistical language
models.
1 Introduction
Machine translation is certainly one of the tough-
est problems in natural language processing. It is
generally accepted however that machine transla-
tion between close or related languages is simpler
than full-fledged translation between languages that
differ substantially in morphological and syntactic
structure. In this paper, we present a machine trans-
lation system from Turkmen to Turkish, both of
which belong to the Turkic language family. Tur-
kic languages essentially exhibit the same charac-
teristics at the morphological and syntactic levels.
However, except for a few pairs, the languages are
not mutually intelligible owing to substantial diver-
gences in their lexicons possibly due to different re-
gional and historical influences. Such divergences
at the lexical level along with many but minor diver-
gences at morphological and syntactic levels make
the translation problem rather non-trivial. Our ap-
proach is based on essentially morphological pro-
cessing, and direct lexical and morphological trans-
fer, augmented with substantial multi-word process-
ing on the source language side and statistical pro-
cessing on the target side where data for statistical
language modelling is more readily available.
2 Related Work
Studies on machine translation between close
languages are generally concentrated around
certain Slavic languages (e.g., Czech?Slovak,
Czech?Polish, Czech?Lithuanian (Hajic et al,
2003)) and languages spoken in the Iberian Penin-
sula (e.g., Spanish?Catalan (Canals et al, 2000),
Spanish?Galician (Corbi-Bellot et al, 2003) and
Spanish?Portugese (Garrido-Alenda et al, 2003).
Most of these implementations use similar modules:
a morphological analyzer, a part-of-speech tagger,
a bilingual transfer dictionary and a morphological
generator. Except for the Czech?Lithuanian
system which uses a shallow parser, syntactic
parsing is not necessary in most cases because of
the similarities in word orders. Also, the lexical
semantic ambiguity is usually preserved so, none of
these systems has any module for handling the lex-
ical ambiguity. For Turkic languages, Hamzaog?lu
(1993) has developed a system from Turkish to
Azerbaijani, and Alt?ntas? (2000) has developed a
system from Turkish to Crimean Tatar.
3 Turkic Languages
Turkic languages, spoken by more than 180 million
people, constitutes subfamily of Ural-Altaic lan-
guages and includes languages like Turkish, Azer-
baijani, Turkmen, Uzbek, Kyrghyz, Kazakh, Tatar,
Uyghur and many more. All Turkic languages have
very productive inflectional and derivational agglu-
tinative morphology. For example the Turkish word
evlerimizden has three inflectional morphemes at-
tached to a noun root ev (house), for the plural form
with second person plural possessive agreement and
ablative case:
189
evlerimizden (from our houses)
ev+ler+imiz+den
ev+Noun+A3pl+P1sg+Abl
All Turkic languages exhibit SOV constituent or-
der but depending on discourse requirements, con-
stituents can be in any order without any substan-
tial formal constraints. Syntactic structures between
Turkic languages are more or less parallel though
there are interesting divergences due to mismatches
in multi-word or idiomatic constructions.
4 Approach
Our approach is based on a direct morphological
transfer with some local multi-word processing on
the source language side, and statistical disambigua-
tion on the target language side. The main steps of
our model are:
1. Source Language (SL) Morphological Analysis
2. SL Morphological Disambiguation
3. Multi-Word Unit (MWU) Recognizer
4. Morphological Transfer
5. Root Word Transfer
6. Statistical Disambiguation and Rescoring (SLM)
7. Sentence Level Rules (SLR)
8. Target Language (TL) Morphological Generator
Steps other than 3, 6 and 7 are the minimum
requirements for a direct morphological translation
model (henceforth, the baseline system). The MWU
Recognizer, SLM and SLR modules are additional
modules for the baseline system to improve the
translation quality.
Source language morphological analysis may pro-
duce multiple interpretation of a source word, and
usually, depending on the ambiguities brought about
by multiple possible segmentations into root and
suffixes, there may be different root words of pos-
sibly different parts-of-speech for the same word
form. Furthermore, each root word thus produced
may map to multiple target root words due to word
sense ambiguity. Hence, among all possible sen-
tences that can be generated with these ambigui-
ties, the most probable one is selected by using var-
ious types of SLMs that are trained on target lan-
guage corpora annotated with disambiguated roots
and morphological features.
MWU processing in Turkic languages involves
more than the usual lexicalized collocations and
involves detection of mostly unlexicalized intra-
word morphological patterns (Oflazer et al, 2004).
Source MWUs are recognized and marked during
source analysis and the root word transfer module
maps these either to target MWU patterns, or di-
rectly translates when there is a divergence.
Morphological transfer is implemented by a set of
rules hand-crafted using the contrastive knowledge
between the selected language pair.
Although the syntactic structures are very simi-
lar between Turkic languages, there are quite many
minor situations where target morphological fea-
tures marking features such as subject-verb agree-
ment have to be recovered when such features are
not present in the source. Furthermore, occasion-
ally certain phrases have to be rearranged. Finally, a
morphological generator produces the surface forms
of the lexical forms in the sentence.
5 Turkmen to Turkish MT System
The first implementation of our approach is from
Turkmen to Turkish. A general diagram of our MT
system is presented in Figure 1. The morphologi-
cal analysis on the Turkmen side is performed by
a two-level morphological analyzer developed using
Xerox finite state tools (Tantug? et al, 2006). It takes
a Turkmen word and produces all possible morpho-
logical interpretations of that word. A simple ex-
periment on our test set indicates that the average
Turkmen word gets about 1.55 analyses. The multi-
word recognition module operates on the output of
the morphological analyzer and wherever applica-
ble, combines analyses of multiple tokens into a new
analysis with appropriate morphological features.
One side effect of multi-word processing is a small
reduction in morphological ambiguity, as when such
units are combined, the remaining morphological in-
terpretations for these tokens are deleted.
The actual transfer is carried out by transferring
the morphological structures and word roots from
the source language to the target language maintain-
ing any ambiguity in the process. These are imple-
mented with finite state transducers that are com-
piled from replace rules written in the Xerox regular
expression language.1 A very simple example of this
transfer is shown in Figure 2.2
1The current implementation employs 28 replace rules for
morphological feature transfer and 19 rules for sentence level
processing.
2+Pos:Positive polarity, +A3sg: 3rd person singular agree-
ment, +Inf1,+Inf2: infinitive markers, +P3sg, +Pnon: pos-
sessive agreement markers, +Nom,+Acc: Nominative and ac-
190
Figure 1: Main blocks of the translation system
o?smegi
?
Source Morphological Analysis
?
o?s+Verb+Pos?DB+Noun+Inf1+A3sg+P3sg+Nom
o?s+Verb+Pos?DB+Noun+Inf1+A3sg+Pnon+Acc
?
Source-to-Target Morphological Feature Transfer
?
o?s+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom
o?s+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc
?
Source-to-Target Root word Transfer
?
ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom
ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc
bu?yu?+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom
bu?yu?+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc
?
Target Morphological Generation
?
ilerlemesi (the progress of (something))
ilerlemeyi (the progress (as direct object))
bu?yu?mesi (the growth of (something))
bu?yu?meyi (the growth (as direct object))
Figure 2: Word transfer
In this example, once the morphological analy-
sis is produced, first we do a morphological feature
transfer mapping. In this case, the only interesting
mapping is the change of the infinitive marker. The
source root verb is then ambiguously mapped to two
verbs on the Turkish side. Finally, the Turkish sur-
face form is generated by the morphological gen-
erator. Note that all the morphological processing
details such as vowel harmony resolution (a mor-
phographemic process common to all Turkic lan-
guages though not in identical ways) are localized
to morphological generation.
Root word transfer is also based on a large trans-
cusative case markers.
ducer compiled from bilingual dictionaries which
contain many-to-many mappings. During mapping
this transducer takes into account the source root
word POS.3 In some rare cases, mapping the word
root is not sufficient to generate a legal Turkish lex-
ical structure, as sometimes a required feature on
the target side may not be explicitly available on the
source word to generate a proper word. In order to
produce the correct mapping in such cases, some ad-
ditional lexicalized rules look at a wider context and
infer any needed features.
While the output of morphological feature trans-
fer module is usually unambiguous, ambiguity arises
during the root word transfer phase. We attempt to
resolve this ambiguity on the target language side
using statistical language models. This however
presents additional difficulties as any statistical lan-
guage model for Turkish (and possibly other Turkic
languages) which is built by using the surface forms
suffers from data sparsity problems. This is due
to agglutinative morphology whereby a root word
may give rise to too many inflected forms (about a
hundred inflected forms for nouns and much more
for verbs; when productive derivations are consid-
ered these numbers grow substantially!). Therefore,
instead of building statistical language models on
full word forms, we work with morphologically an-
alyzed and disambiguated target language corpora.
For example, we use a language model that is only
based on the (disambiguated) root words to disam-
biguate ambiguous root words that arise from root
3Statistics on the test set indicate that on the average each
source language root word maps to about 2 target language root
words.
191
word transfer. We also employ a language model
which is trained on the last set of inflectional fea-
tures of morphological parses (hence does not in-
volve any root words.)
Although word-by-word translation can produce
reasonably high quality translations, but in many
cases, it is also the source of many translation errors.
To alleviate the shortcomings of the word-by-word
translation approach, we resort to a series of rules
that operate across the whole sentence. Such rules
operate on the lexical and surface representation of
the output sentence. For example, when the source
language is missing a subject agreement marker on
a verb, this feature can not be transferred to the tar-
get language and the target language generator will
fail to generate the appropriate word. We use some
simple heuristics that try to recover the agreement
information from any overt pronominal subject in
nominative case, and that failing, set the agreement
to 3rd person singular. Some sentence level rules
require surface forms because this set of rules usu-
ally make orthographic changes affected by previous
word forms. In the following example, suitable vari-
ants of the clitics de and mi must be selected so that
vowel harmony with the previous token is preserved.
o de go?rdu? mi? ? o da go?rdu? mu??
(did he see too?)
A wide-coverage Turkish morphological analyzer
(Oflazer, 1994) made available to be used in reverse
direction to generate the surface forms of the trans-
lations.
6 Results and Evaluation
We have tracked the progress of our changes to
our system using the BLEU metric (Papineni et al,
2004), though it has serious drawbacks for aggluti-
native and free constituent order languages.
The performance of the baseline system (all steps
above, except 3, 6, and 7) and systems with ad-
ditional modules are given in Table 1 for a set of
254 Turkmen sentences with 2 reference translations
each. As seen in the table, each module contributes
to the performance of the baseline system. Further-
more, a manual investigation of the outputs indicates
that the actual quality of the translations is higher
than the one indicated by the BLEU score.4 The er-
rors mostly stem from the statical language models
4There are many translations which preserve the samemean-
ing with the references but get low BLEU scores.
not doing a good job at selecting the right root words
and/or the right morphological features.
System BLEU Score
Baseline 26.57
Baseline + MWU 28.45
Baseline + MWU + SLM 31.37
Baseline + MWU + SLM + SLR 33.34
Table 1: BLEU Scores
7 Conclusions
We have presented an MT system architecture be-
tween Turkic languages using morphological trans-
fer coupled with target side language modelling and
results from a Turkmen to Turkish system. The re-
sults are quite positive but there is quite some room
for improvement. Our current work involves im-
proving the quality of our current system as well as
expanding this approach to Azerbaijani and Uyghur.
Acknowledgments
This work was partially supported by Project 106E048 funded
by The Scientific and Technical Research Council of Turkey.
Kemal Oflazer acknowledges the kind support of LTI at
Carnegie Mellon University, where he was a sabbatical visitor
during the academic year 2006 ? 2007.
References
A. Cu?neyd Tantug?, Es?ref Adal?, Kemal Oflazer. 2006. Com-
puter Analysis of the Turkmen Language Morphology. Fin-
TAL, Lecture Notes in Computer Science, 4139:186-193.
A. Garrido-Alenda et al 2003. Shallow Parsing for
Portuguese-Spanish Machine Translation. in TASHA 2003:
Workshop on Tagging and Shallow Processing of Por-
tuguese, Lisbon, Portugal.
A. M. Corbi-Bellot et al 2005. An open-source shallow-
transfer machine translation engine for the Romance lan-
guages of Spain. in 10th EAMT conference ?Practical ap-
plications of machine translation?, Budapest, Hungary.
Jan Hajic, Petr Homola, Vladislav Kubon. 2003. A simple
multilingual machine translation system. MT Summit IX.
I?lker Hamzaog?lu. 1993. Machine translation from Turkish to
other Turkic languages and an implementation for the Azeri
language. MSc Thesis, Bogazici University, Istanbul.
Kemal Alt?ntas?. 2000. Turkish to Crimean Tatar Machine
Translation System. MSc Thesis, Bilkent University, Ankara.
Kemal Oflazer. 1994. Two-level description of Turkish mor-
phology. Literary and Linguistic Computing, 9(2).
Kemal Oflazer, O?zlem C?etinog?lu, Bilge Say. 2004. Integrat-
ing Morphology with Multi-word Expression Processing in
Turkish. The ACL 2004 Workshop on Multiword Expres-
sions: Integrating Processing.
Kishore Papineni et al 2002. BLEU : A Method for Automatic
Evaluation of Machine Translation. Association of Compu-
tational Linguistics, ACL?02.
Raul Canals-Marote et al 2000. interNOSTRUM: a Spanish-
Catalan Machine Translation System. Machine Translation
Review, 11:21-25.
192
Vi-xfst: A Visual Regular Expression Development Environment for Xerox
Finite State Tool
Kemal Oflazer and Yasin Y?lmaz
Human Language and Speech Technology Laboratory
Sabanc University
Istanbul, Turkey
oflazer@sabanciuniv.edu, yyilmaz@su.sabanciuniv.edu
Abstract
This paper describes Vi-xfst, a visual interface and
a development environment, for developing finite
state language processing applications using the Xe-
rox Finite State Tool, xfst. Vi-xfst lets a user con-
struct complex regular expressions via a drag-and-
drop visual interface, treating simpler regular ex-
pressions as ?Lego Blocks.? It also enables the vi-
sualization of the structure of the regular expres-
sion components, providing a bird?s eye view of
the overall system, enabling a user to easily under-
stand and track the structural and functional rela-
tionships among the components involved. Since
the structure of a large regular expression (built in
terms of other regular expressions) is now transpar-
ent, users can also interact with regular expressions
at any level of detail, easily navigating among them
for testing. Vi-xfst also keeps track of dependen-
cies among the regular expressions at a very fine-
grained level. So when a certain regular expression
is modified as a result of testing, only the depen-
dent regular expressions are recompiled resulting in
an improvement in development process time, by
avoiding file level recompiles which usually causes
redundant regular expression compilations.
1 Introduction
Finite state machines are widely used in many lan-
guage processing applications to implement com-
ponents such as tokenizers, morphological analyz-
ers/generators, shallow parsers, etc. Large scale fi-
nite state language processing systems built using
tools such as the Xerox Finite State Tool (Kart-
tunen et al, 1996; Karttunen et al, 1997; Beesley
and Karttunen, 2003), van Noord?s Prolog-based
tool (van Noord, 1997), the AT&T weighted finite
state machine suite (Mohri et al, 1998) or the IN-
TEX System (Silberztein, 2000), involve tens or
hundreds of regular expressions which are compiled
into finite state transducers that are interpreted by
the underlying run-time engines of the (respective)
tools.
Developing such large scale finite state systems is
currently done without much of a support for the
?software engineering? aspects. Regular expres-
sions are constructed manually by the developer
with a text-editor and then compiled, and the result-
ing transducers are tested. Any modifications have
to be done afterwards on the same text file(s) and the
whole project has to be recompiled many times in a
development cycle. Visualization, an important aid
in understanding and managing the complexity of
any large scale system, is limited to displaying the
finite state machine graph (e.g., Gansner and North
(1999), or the visualization functionality in INTEX
(Silberztein, 2000)). However, such visualization
(sort of akin to visualizing the machine code of a
program written in a high-level language) may not
be very helpful, as developers rarely, and possibly
never, think of such large systems in terms of states
and transitions. The relationship between the reg-
ular expressions and the finite state machines they
are compiled into are opaque except for the simplest
of regular expressions. Further, the size of the re-
sulting machines, in terms of states and transitions,
is very large, usually in the thousands to hundreds
of thousands states, if not more, making such visu-
alization meaningless. On the other hand, it may
prove quite useful to visualize the structural compo-
nents of a set of regular expressions and how they
are put together, much in the spirit of visualizing the
relationships amongst the data objects and/or mod-
ules in a large program. However such visualiza-
tion and other maintenance operations for large fi-
nite state projects spanning over many files, depend
on tracking the structural relationships and depen-
dencies among the regular expressions, which may
prove hard or inconvenient when text-editors are the
only development tool.
This paper presents a visual interface and develop-
ment environment, Vi-xfst (Y?lmaz, 2003), for the
Xerox Finite State Tool, xfst, one of the most sophis-
ticated tools for constructing finite state language
processing applications (Karttunen et al, 1997).
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
Vi-xfst enables incremental construction of com-
plex regular expressions via a drag-and-drop inter-
face, treating simpler regular expressions as ?Lego
Blocks?. Vi-xfst also enables the visualization of the
structure of the regular expression components, so
that the developer can have a bird?s eye view of the
overall system, easily understanding and tracking
the relationships among the components involved.
Since the structure of a large regular expression
(built in terms of other regular expressions) is now
transparent, the developer can interact with regular
expressions at any level of detail, easily navigating
among them for testing and debugging. Vi-xfst also
keeps track of the dependencies among the regular
expressions at a very fine-grained level. So, when
a certain regular expression is modified as a result
of testing or debugging, only the dependent regu-
lar expressions are recompiled. This results is an
improvement in development time, by avoiding file
level recompiles which usually causes substantial
redundant regular expression compilations.
In the following sections, after a short overview of
the Xerox xfst finite state machine development en-
vironment, we describe salient features of Vi-xfst
through some simple examples.
2 Overview of xfst
xfst is a sophisticated command-line-oriented inter-
face developed by Xerox Research Centre Europe,
for building large finite state transducers for lan-
guage processing applications. Users of xfst employ
a high-level regular expression language which pro-
vides an extensive palette of high-level operators.1
Such regular expressions are then compiled into fi-
nite state transducers and interpreted by a run-time
engine built into the tool. xfst also provides a further
set of commands for combining, testing and inspect-
ing the finite state transducers produced by the regu-
lar expression compiler. Transducers may be loaded
onto a stack maintained by the system, and the top-
most transducer on the stack is available for testing
or any further operations. Transducers can also be
saved to files which can later be reused or used by
other programs in the Xerox finite state suite.
Although xfst provides quite useful debugging facil-
ities for testing finite state networks, it does not pro-
vide additional functionality beyond the command-
1Details of the operators are available at http://www.
xrce.xerox.com/competencies/content-analysis/
fsCompiler/fssyntax.html and http://www.xrce.
xerox.com/competencies/content-analysis/
fsCompiler/fssyntax-explicit.html.
line interface to alleviate the complexity of develop-
ing large scale projects. Building a large scale finite
state transducer-based application such as a mor-
phological analyzer or a shallow finite state parser,
consisting of tens to hundreds of regular expres-
sions, is also a large software engineering undertak-
ing. Large finite state projects can utilize the make
functionality in Linux/Unix/cygwin environments,
by manually entering (file level) dependencies be-
tween regular expressions tered into a makele. The
make program then invokes the compiler at the shell
level on the relevant files by tracking the modifica-
tion times of files. Since whole files are recompiled
at a time even when a very small change is made,
there may be redundant recompilations that may in-
crease the development time.
3 Vi-xfst ? a visual interface to xfst
As a development environment, Vi-xfst has two im-
portant features that improve the development pro-
cess of complex large scale finite state projects with
xfst.
1. It enables the construction of regular expres-
sions by combining previously defined regular
expressions via a drag-and-drop interface.
2. As regular expressions are built by combining
other regular expressions, Vi-xfst keeps track of
the topological structure of the regular expres-
sion ? how component regular expressions re-
late to each other. It derives and maintains the
dependency relationships of a regular expres-
sion to its components, and via transitive clo-
sure, to the components they depend on. This
structure and dependency relations can then be
used to visualize a regular expression at vari-
ous levels of detail, and also be used in very
fine-grained recompilations when some regu-
lar expressions are modified.
3.1 Using Vi-xfst
In this section, we describe important features Vi-
xfst through some examples.2 The first example is
for a simple date parser described in Karttunen et
al. (1996). This date parser is implemented in xfst
using the following regular expressions:3
2The examples we provide are rather simple ones, as length
restrictions do not allow us to include large figures to visualize
complex finite state projects.
3The define command defines a named regular expres-
sion which can then be subsequently referred to in later regular
expressions. | denotes the union operator. 0 (without quotes)
denotes the empty string traditionally represented by  in the lit-
erature. The quotes " are used to literalize sequence of symbols
which have special roles in the regular expression language.
define 1to9 [1|2|3|4|5|6|7|8|9];
define Day [Monday|Tuesday|Wednesday|
Thursday|Friday|
Saturday|Sunday];
define Month [January|February|March|
April|May|June|July|
August|September|October|
November|December];
define def2 [1|2];
define def4 [3];
define def5 ["0"|1];
define Empty [ 0 ]
define def16 [ (Day ", ")];
define SPACE [" "];
define 0To9 ["0" | 1To9];
define Date [1to9|[def2 0To9]|
[def4 def5]];
define Year [1to9 [[0To9 [[[0To9
[0To9|Empty]]
|Empty]|Empty]]
|Empty]];
define DateYear [(", " Year)];
define LeftBracket [ "[" ];
define RightBracket [ "]" ];
define AllDates [Day|[def16 Month
SPACE Date DateYear]];
define AllDatesParser
[AllDates @->
LeftBracket ... RightBracket];
read regex AllDatesParser;
The most important regular expression above is
AllDates, a pattern that describes a set of
calendar dates. It matches date expressions
such as Sunday, January 23, 2004 or just
Monday. The subsequent regular expression
AllDatesParser uses the longest match down-
ward bracket operator (the combination of @-> and
...) to define a transducer that puts [ and ]
around the longest matching patterns in the input
side of the transducer.
Figure 1 shows the state of the screen of Vi-xfst just
after the AllDatesParser regular expression is
constructed. In this figure, the left side window
shows, under the Definitions tab, the regular
expressions defined. The top right window shows
the template for the longest match regular expres-
sion slots filled by drag and drop from the list on the
left. The AllDatesParser regular expression is
entered by selecting the longest-match downward
bracket operator (depicted with the icon @-> with
... underneath) from the palette above, which then
inserts a template that has empty slots ? three in this
case. The user then ?picks up? regular expressions
from the left and drops them into the appropriate
slots. When the regular expression is completed, it
can be sent to the xfst process for compilation. The
bottom right window, under the Messages tab,
shows the messages received from the xfst process
running in the background during the compilation
of this and the previous regular expressions.
Figure 2 shows the user testing a regular expression
loaded on to the stack of the xfst. The left win-
dow under the Networks tab, shows the networks
pushed on to the xfst stack. The bottom right win-
dow under Test tab lists a series of input, one of
which can be selected as the input string and then
applied up or down to the topmost network on the
stack.4 The result of application appears on the
bottom pane on the right. In this case, we see the
input with the brackets inserted around the longest
matching date pattern, Sunday, January 23,
2004 in this case.
3.2 Visualizing regular expression structure
When developing or testing a large finite state trans-
ducer compiled from a regular expression built as a
hierarchy of smaller regular expressions, it is very
helpful, especially during development, to visualize
the overall structure of the regular expression to eas-
ily see how components relate to each other.
Vi-xfst provides a facility for viewing the structure
of a regular expression at various levels of detail.
To illustrate this, we use a simple cascade of trans-
ducers simulating a coke machine dispensing cans
of soft drink when the right amount of coins are
dropped in.5 The regular expressions for this ex-
ample are:6
define N [ n ];
define D [ d ];
define Q [ q ];
define DefPLONK [ PLONK ];
define CENT [ c ];
define SixtyFiveCents [ [ [ CENT ]^65 ]
.x.
DefPLONK ];
define CENTS [[N .x. [[ CENT ]^5 ]|
[D .x. [[ CENT ]^10 ]]]|
[Q .x. [[ CENT ]^25 ]]];
define BuyCoke [ [ [ CENTS ]* ]
.o.
SixtyFiveCents ];
4xfst only allows the application of inputs to the topmost
network on the stack.
5See http://www.xrce.xerox.com/competencies/
content-analysis/fsCompiler/fsexamples.html for
this example.
6The additional operators in this example are: .x. repre-
senting the cross-product and .o. representing the composi-
tion of transducers, and caret operator ( ? )denoting the repeated
concatenation of its left argument as many times as indicated by
its right argument.
Figure 1: Constructing a regular expression via the drag-and-drop interface
The last regular expression here BuyCoke defines
a transducer that consist of the composition of two
other transducers. The transducer [ CENTS ]*
maps any sequence of symbols n, d, and q repre-
senting, nickels, dimes and quarters, into the appro-
priate number of cents, represented as a sequence
of c symbols. The transducer SixtyFiveCents
maps a sequence of 65 c symbols to the symbol
PLONK representing a can of soft drink (falling).
Figure 3 shows the simplest visualization of the
BuyCoke transducer in which only the top level
components of the compose operator (.o.) are dis-
played. The user can navigate among the visible
regular expressions and ?zoom? into any regular ex-
pressions further, if necessary. For instance, Figure
4 shows the rendering of the same transducer af-
ter the top transducer is expanded where we see the
union of three cross-product operators, while Figure
5 shows the rendering after both components are ex-
panded. When a regular expression laid out, the user
can select any of the regular expressions displayed
and make that the active transducer for testing (that
is, push it onto the top of the xfst transducer stack)
Figure 3: Simplest view of a regular expression
and rapidly navigate among the regular expressions
without having to remember their names and loca-
tions in the files.
As we re-render the layout of a regular expression,
we place the components of the compose and cross-
product operators in a vertical layout, and others in
Figure 2: Testing a regular expression.
Figure 4: View after the top regular expression is
expanded.
a horizontal layout and determine the best layout
of the components to be displayed in a rectangular
bounding box. It is also possible to render the up-
ward and downward replace operators in a vertical
layout, but we have opted to render them in a hori-
zontal layout (as in Figure 1). The main reason for
this is that although the components of the replace
part of such an expression can be placed vertically,
the contexts need to be placed in a horizontal layout.
A visualization of a complex network employing a
different layout of the replace rules is shown in Fig-
ure 6 with the Windows version of Vi-xfst. Here we
see a portion of a Number-to-English mapping net-
work7 where different components are visualized at
different structural resolutions.
3.3 Interaction of Vi-xfst with xfst
Vi-xfst interacts with xfst via inter-process commu-
nication. User actions on the Vi-xfst side get trans-
lated to xfst commands and get sent to xfst which
maintains the overall state of the system in its own
universe. Messages and outputs produced by xfst
are piped back to Vi-xfst, which are then parsed
7Due to Lauri Karttunen; see http://www.cis.
upenn.edu/~cis639/assign/assign8.html for the
xfst script for this transducer. It maps numbers like 1234 into
English strings like One thousand two hundred and thirty four.
Figure 6: Mixed visualization of a complex network
Figure 5: View after both regular expressions are
expanded.
and presented back to the user. If a direct API is
available to xfst, it would certainly be possible to
implement tighter interface that would provide bet-
ter error-handling and slightly improved interaction
with the xfst functionality.
All the files that Vi-xfst produces for a project are
directly compatible with and usable by xfst; that is,
as far as xfst is concerned, those files are valid reg-
ular expression script files. Vi-xfst maintains all the
additional bookkeeping as comments in these files
and such information is meaningful only to Vi-xfst
and used when a project is re-loaded to recover all
dependency and debugging information originally
computed or entered. Currently, Vi-xfst has some
primitive facilities for directly importing hand gen-
erated files for xfst to enable manipulation of al-
ready existing projects.
4 Selective regular expression compilation
Selective compilation is one of the simple facili-
ties available in many software development envi-
ronments. A software development project uses se-
lective compilation to compile modules that have
been modified and those that depend (transitively) in
some way (via say header file inclusion) to the mod-
ified modules. This selective compilation scheme,
typically known as the make operation, depends on
a manually or automatically generated makele cap-
turing dependencies. It can save time during devel-
opment as only the relevant files are recompiled af-
ter a set of modifications.
In the context of developing large scale finite state
language processing application, we encounter the
same issue. During testing, we recognize that a cer-
tain regular expression is buggy, fix it, and then have
to recompile all others that use that regular expres-
sion as a component. It is certainly possible to use
make and recompile the appropriate regular expres-
sion files. But, this has two major disadvantages:
? The user has to manually maintain the make-
le that captures the dependencies and invokes
the necessary compilation steps. This may be
a non-trivial task for a large project.
? When even a singular regular expression is
modified, the file the regular expression resides
in, and all the other files containing regular ex-
pressions that (transitively) depend on that file,
have to be recompiled. This may waste a con-
siderable amount of time as many other regu-
lar expressions that do not need to be recom-
piled, are compiled just because they happen
to reside in the same file with some other reg-
ular expression. Since some regular expres-
sions may take a considerable amount of time
to compile, this unnecessarily slows down the
development process.
Vi-xfst provides a selective compilation functional-
ity to address this problem by automatically keeping
track of the regular expression level dependencies as
they are built via the drag-and-drop interface. This
dependency can then be exploited by Vi-xfst when a
recompile needs to be done.
Figure 7 shows the directed acyclic dependency
graph of the regular expressions in Section 3.1, ex-
tracted as the regular expressions are being defined.
A node in this graph represents a regular expression
that has been defined, and when there is an arc from
a node to another node, it indicates that the regu-
lar expression at the source of the arc directly de-
pends on the regular expression at the target of the
arc. For instance, in Figure 7, the regular expres-
sion AllDates directly depends on the regular ex-
pressions Date, DateYear, Month, SPACE, and
def16.
Figure 7: The dependency graph for the regular ex-
pressions of the DateParser.
After one or more regular expressions are modified,
we first recompile (by sending a dene command to
xfst) those regular expressions, and then recompile
all regular expressions starting with immediate de-
pendents and traversing systematically upwards to
the regular expressions of all ?top? nodes on which
no other regular expressions depend, making sure
that
? all regular expressions that a regular expres-
sion depends on and have to be recompiled, are
recompiled before that regular expression is re-
compiled, and
? every regular expression that needs to be re-
compiled is recompiled only once.
To achieve these, we compute the subgraph of the
dependency graph that has all the nodes correspond-
ing to the modified regular expressions and any
other regular expressions that transitively depends
on these regular expressions. Then, a topological
sort of the resulting subgraph gives a possible linear
ordering of the regular expression compilations.
For instance for the dependency subgraph in Fig-
ure 7, if the user modifies the definition of
the network 1to9, the dependency subgraph
of the regular expressions that have to be re-
compiled would be the one shown in Figure
8. A (reverse) topological sort of this depen-
Figure 8: The dependency subgraph graph induced
by the regular expression 1to9.
dency subgraph gives us one of the possible or-
ders for recompiling only the relevant regular
expressions as: 1to9, 0To9, Date, Year,
DateYear, AllDates, AllDatesParser
5 Conclusions and future work
We have described Vi-xfst, a visual interface and a
development environment for the development of
large finite state language processing application
components, using the Xerox Finite State Tool xfst.
In addition to a drag-and-drop user interface for
constructing regular expressions in a hierarchical
manner, Vi-xfst can visualize the structure of a reg-
ular expression at different levels of detail. It also
keeps track of how regular expressions depend on
each other and uses this dependency information for
selective compilation of regular expressions when
one or more regular expressions are modified dur-
ing development.
The current version of Vi-xfst lacks certain features
that we plan to add in the future versions. One im-
portant functionality that we plan to add is user cus-
tomizable operator definitions so that new regular
expression operators can be added by the user as op-
posed to being fixed at compile-time. The user can
define the relevant aspects (slots, layout) of an oper-
ator in a configuration file which can be read at the
program start-up time. Another important feature
is the importing of libraries of regular expressions
much like symbol libraries in drawing programs and
the like.
The interface of Vi-xfst to the xfst itself is localized
to a few modules. It is possible to interface with
other finite state tools by rewriting these modules
and providing user-definable operators.
6 Acknowledgments
We thank XRCE for providing us with the xfst and
other related programs in the finite state suite.
References
Kenneth R. Beesley and Lauri Karttunen. 2003. Fi-
nite State Morphology. CSLI Publications, Stan-
ford University.
Emden R. Gansner and Stephen C. North. 1999. An
open graph visualization system and its applications
to software engineering. Software ? Practice and
Experience.
Lauri Karttunen, Jean-Pierre Chanod, Gregory
Grefenstette, and Anne Schiller. 1996. Regular ex-
pressions for language engineering. Natural Lan-
guage Engineering, 2(4):305?328.
Lauri Karttunen, Tamas Gaal, and Andre Kempe.
1997. Xerox Finite-State Tool. Technical report,
Xerox Research Centre Europe.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 1998. A rational design for a weighted finite?
state transducer library. In Lecture Notes in Com-
puter Science, 1436. Springer Verlag.
Max Silberztein. 2000. Intex: An fst toolbox. The-
oretical Computer Science, 231(1):33?46, January.
Gertjan van Noord. 1997. FSA utilities: A toolbox
to manipulate finite state automata. In D. Raymond,
D. Wood, and S. Yu, editors, Automata Implemen-
tation, number 1260 in Lecture Notes in Computer
Science. Springer Verlag.
Yasin Y?lmaz. 2003. Vi-XFST: A visual interface
for Xerox Finite State Toolkit. Master?s thesis, Sa-
banc? University, July.
Integrating Morphology with Multi-word Expression Processing in Turkish
Kemal Oflazer and ?zlem ?etinog?lu
Human Language and Speech Technology Laboratory
Sabanc University
Istanbul, Turkey
{oflazer,ozlemc}@sabanciuniv.edu
Bilge Say
Informatics Institute
Middle East Technical University
Ankara, Turkey
bsay@ii.metu.edu.tr
Abstract
This paper describes a multi-word expression pro-
cessor for preprocessing Turkish text for various
language engineering applications. In addition to
the fairly standard set of lexicalized collocations
and multi-word expressions such as named-entities,
Turkish uses a quite wide range of semi-lexicalized
and non-lexicalized collocations. After an overview
of relevant aspects of Turkish, we present a descrip-
tion of the multi-word expressions we handle. We
then summarize the computational setting in which
we employ a series of components for tokenization,
morphological analysis, and multi-word expression
extraction. We finally present results from runs over
a large corpus and a small gold-standard corpus.
1 Introduction
Multi-word expression extraction is an important
component in language processing that aims to
identify segments of input text where the syntactic
structure and the semantics of a sequence of words
(possibly not contiguous) are usually not composi-
tional. Idiomatic forms, support verbs, verbs with
specific particle or pre/post-position uses, morpho-
logical derivations via partial or full word duplica-
tions are some examples of multi-word expressions.
Further, expressions such as time-date expressions
or proper nouns which can be described with sim-
ple (usually finite state) grammars, and whose inter-
nal structure is of no real importance to the overall
analysis of the sentence, can also be considered un-
der this heading. Marking multi-word expressions
in text usually reduces (though not significantly)
the number of actual tokens that further processing
modules use as input, although this reduction may
depend on the domain the text comes from. It can
also reduce the multiplicative ambiguity as morpho-
logical interpretations of tokens are reduced when
they are coalesced into multi-word expressions with
usually a single interpretation.
Turkish presents some interesting issues for multi-
word expression processing as it makes substan-
tial use of support verbs with lexicalized direct or
oblique objects subject to various morphological
constraints. It also uses partial and full reduplica-
tion of forms of various parts-of-speech, across their
whole domain to form what we call non-lexicalized
collocations, where it is the duplication and contrast
of certain morphological patterns that signal a col-
location rather than the specific root words used.
In this paper, we describe a multi-word expression
processor for preprocessing Turkish text for vari-
ous language engineering applications. In the next
section after a very short overview of relevant as-
pects of Turkish, we present a rather comprehen-
sive description of the multi-word expressions we
handle. We then summarize the structure of the
multi-word expression processor which employs a
series of components for tokenization, morpholog-
ical analysis, conservative non-statistical morpho-
logical disambiguation, and multi-word expression
extraction. We finally present results from runs over
a large corpus and a small gold-standard corpus.
1.1 Related Work
Recent work on multi-word expression extraction,
use three basic approaches: statistical, rule-based,
and hybrid. Statistical approaches require a corpus
that contains significant numbers of occurrences of
multi-word expressions. But even if the corpus con-
sists of millions of words, usually, the frequencies
of multi-word expressions are too low for statisti-
cal extraction. Baldwin and Villavicencio (2002)
indicate that ?two-thirds of verb-particle construc-
tions occur at most three times in the overall corpus,
meaning that any extraction method must be able
to handle extremely sparse data.? They use a rule-
based method to extract multi-word expressions in
the form of a head verb and a single obligatory
preposition employing a tagger augmented with an
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 64-71
existing chunking system with which they first iden-
tify the particle chunked and then turn back for the
verb part of the construction.
Piao et al (2003) employ their semantic field an-
notator USAS, containing 37,000 words and a tem-
plate list of 16,000 multi-word units, all constructed
manually from various resources, in order to extract
multi-word expressions. The evaluation indicates a
high precision (over 90%) but the estimated recall is
about 40%. Deeper investigation on the corpus has
indicated that two-thirds of the multi-word expres-
sions occur in the corpus once or twice, verifying
the fact that the statistical methods filtering low fre-
quencies would fail.
Urizar et al (2000) describe a Basque terminol-
ogy extraction system which covered multi-word
term extraction as a subset. As Basque is a highly
inflected agglutinative language like Turkish, mor-
phological information is exploited to better define
multi-word patterns. Their lemmatizer/tagger EU-
SLEM, consists of a tokenizer followed by two sub-
systems for the treatment of single word and multi-
word expressions, and a disambiguator. The pro-
posed term extraction tool uses the tagged input as
the input of a shallow parsing phase which consists
of regular expressions representing morphosyntac-
tic patterns. The final step uses statistical measures
to eliminate incorrect candidates.
The basic disadvantages of rule-based approaches
are that they usually lack flexibility, and it is a
time-consuming and never ending process to try to
cover a high percentage of the multi-word expres-
sions in a language with rules and predefined lists.
The LINGO group which defines multi-word ex-
pressions as ?a pain in the neck for NLP? (Sag et al,
2002), suggests hybrid approaches using rule based
approaches to identify possible multi-word expres-
sions out of a corpus and using statistical methods
to enhance the results obtained.
2 Multi-word expressions in Turkish
Turkish is an Ural-Altaic language, having aggluti-
native word structures with productive inflectional
and derivational processes. Most derivational phe-
nomena take place within a word form, but there are
certain derivations involving partial or full redupli-
cations that are best considered under the notion of
multi-word expressions.
Turkish word forms consist of morphemes concate-
nated to a root morpheme or to other morphemes,
much like beads on a string. Except for a very
few exceptional cases, the surface realizations of
the morphemes are conditioned by various morpho-
phonemic processes such as vowel harmony, vowel
and consonant elisions. The morphotactics of word
forms can be quite complex when multiple deriva-
tions are involved. For instance, the derived mod-
ifier sag?lamlas?t?rd?g??m?zdaki1 would be
represented as:2
saglam+Adj
?DB+Verb+Become
?DB+Verb+Caus+Pos
?DB+Adj+PastPart+P1sg
?DB+Noun+Zero+A3sg+Pnon+Loc
?DB+Adj
This word starts out with an adjective root and af-
ter five derivations, ends up with the final part-of-
speech adjective which determines its role in the
sentence.
Turkish employs multi-word expressions in essen-
tially four different forms:
1. Lexicalized Collocations where all compo-
nents of the collocations are fixed,
2. Semi-lexicalized Collocations where some
components of the collocation are fixed and
some can vary via inflectional and derivational
morphology processes and the (lexical) seman-
tics of the collocation is not compositional,
3. Non-lexicalized Collocations where the collo-
cation is mediated by a morphosyntactic pat-
tern of duplicated and/or contrasting compo-
nents ? hence the name non-lexicalized, and
4. Multi-word Named-entities which are multi-
word proper names for persons, organizations,
places, etc.
2.1 Lexicalized Collocations
Under the notion of lexicalized collocations, we
consider the usual fixed multi-word expressions
1Literally, ?(the thing existing) at the time we caused (some-
thing) to become strong?. Obviously this is not a word that one
would use everyday. Turkish words (excluding noninflecting
frequent words such as conjunctions, clitics, etc.) found in typ-
ical text average about 10 letters in length.
2Please refer to the list of morphological features given in
Appendix A for the semantics of some of the non-obvious sym-
bols used here.
whose resulting syntactic function and semantics
are not readily predictable from the structure and
the morphological properties of the constituents.
Here are some examples of the multi-word expres-
sions that we consider under this grouping:3 ,4
(1) hi? olmazsa
? hi?(never)+Adverb
ol(be)+Verb+Neg+Aor+Cond+A3sg
? hi?_olmazsa+Adverb
?at least? (literally ?if it never is?)
(2) ipe sapa gelmez
? ip(rope)+Noun+A3sg+Pnon+Dat
sap(handle)+Noun+A3sg+Pnon+Dat
gel(come)+Verb+Neg+Aor+A3sg
? ipe_sapa_gelmez+Adj
?worthless? (literally ?(he) does not come to rope
and handle?)
2.2 Semi-lexicalized Collocations
Multi-word expressions that are considered under
this heading are compound and support verb forma-
tions where there are two or more lexical items the
last of which is a verb or is a derivation involving
a verb. These are formed by a lexically adjacent,
direct or oblique object, and a verb, which for the
purposes of syntactic analysis, may be considered
as single lexical item: e.g., sayg dur- (literally to
stand (in) respect ? to pay respect), kafay ye- (lit-
erally to eat the head ? to get mentally deranged),
etc.5 Even though the other components can them-
selves be inflected, they can be assumed to be fixed
for the purposes of the collocation, and the colloca-
tion assumes its morphosyntactic features from the
last verb which itself may undergo any morpholog-
ical derivation or inflection process. For instance in
(3) kafay ye-
? kafa(head)+Noun+A3sg+Pnon+Acc
ye(eat)+Verb...
3In every group we first list the morphological features of
all the tokens, one on every line (with the glosses for the roots),
and then provide the morphological features of the multi-word
construct and then provide glosses and literal meanings.
4Please refer to the list of morphological features given in
Appendix A for the semantics of some of the non-obvious sym-
bols used here.
5Here we just show the roots of the verb with - denoting the
rest of the suffixes for any inflectional and derivational markers.
? kafay_ye+Verb...
?get mentally deranged? ( literally ?eat the head?)
the first part of the collocation, the accusative
marked noun kafay, is the fixed part and the part
starting with the verb ye- is the variable part which
may be inflected and/or derived in myriads of ways.
For example the following are some possible forms
of the collocation:
? kafay? yedim ?I got mentally deranged?
? kafay? yiyeceklerdi ?they were about to get
mentally deranged?
? kafay? yiyenler ?those who got mentally de-
ranged?
? kafay? yedi

gi ?the fact that (s/he) got mentally
deranged?
Under certain circumstances, the ?fixed? part may
actually vary in a rather controlled manner subject
to certain morphosyntactic constraints, as in the id-
iomatic verb:
(4) kafa(y) ?ek-
? kafa(head)+Noun+A3sg+Pnon+Acc
?ek(pull)+Verb...
? kafa_?ek+Verb...
?consume alcohol? (but literally ?to pull the head?)
(5) kafalar ?ek-
? kafa+Noun+A3pl+Pnon+Acc
?ek+Verb...
? kafa_?ek+Verb...
?consume alcohol? (but literally ?to pull the
heads?)
where the fixed part can be in the nominative or the
accusative case, and if it is in the accusative case, it
may be marked plural, in which case the verb has to
have some kind of plural agreement (i.e., first, sec-
ond or third person plural), but no possessive agree-
ment markers are allowed.
In their simplest forms, it is sufficient to recognize
a sequence of tokens one of whose morphologi-
cal analyses matches the corresponding pattern, and
then coalesce these into a single multi-word expres-
sion representation. However, some or all variants
of these and similar semi-lexicalized collocations
present further complications brought about by the
relative freeness of the constituent order in Turkish,
and by the interaction of various clitics with such
collocations.6
When such multi-word expressions are coalesced
into a single morphological entity, the ambiguity in
morphological interpretation is reduced as we see in
the following example:
(6) devam etti
? devam(continuation)+Noun+A3sg
+Pnon+Nom
*deva(therapy)+Noun+A3sg+P1sg+Nom
et(make)+Verb+Pos+Past+A3sg
*et(meat)+Noun+A3sg+Pnon+Nom
?DB+Verb+Past+A3sg
? devam_et+Verb+Pos+Past+A3sg
?(he) continued? (literally ?made a continuation?)
Here, when this semi-lexicalized collocation is rec-
ognized, other morphological interpretations of the
components (marked with a * above) can safely be
removed, contributing to overall morphological am-
biguity reduction.
2.3 Non-lexicalized Collocations
Turkish employs quite a number of non-lexicalized
collocations where the sentential role of the collo-
cation has (almost) nothing to do with the parts-of-
speech and the morphological features of the indi-
vidual forms involved. Almost all of these colloca-
tions involve partial or full duplications of the forms
involved and can actually be viewed as morphologi-
cal derivational processes mediated by reduplication
across multiple tokens.
The morphological feature representations of such
multi-word expressions follow one of the patterns:
1) ? ?
2) ? Z ?,
3) ? + X ? + Y
4) ?1 + X ?2 + X
where ? is the duplicated string comprising the
root, its part-of-speech and possibly some additional
morphological features encoded by any suffixes. X
and Y are further duplicated or contrasted morpho-
logical patterns and Z is a certain clitic token. In
6The question and the emphasis clitics which are written as
separate tokens, can occasionally intervene between the com-
ponents of a semi-lexicalized collocation. We omit the details
of these due to space restrictions.
duplications of type 4, it is possible that ?1 is dif-
ferent from ?2.
Below we present list of the more interesting non-
lexicalized expressions along with some examples
and issues.
? When a noun appears in duplicate following the
first pattern above, the collocation behaves like a
manner adverb, modifying a verb usually to the
right. Although this pattern does not necessarily
occur with every possible noun, it may occur with
many (countable) nouns without much of a further
semantic restriction. Such a sequence has to be co-
alesced into a representation indicating this deriva-
tional process as we see below.
(7) ev ev (? ?)
? ev(house)+Noun+A3sg+Pnon+Nom
ev+Noun+A3sg+Pnon+Nom
? ev+Noun+A3sg+Pnon+Nom?DB+Adverb+By
?house by house? (literally ?house house?)
? When an adjective appears in duplicate, the col-
location behaves like a manner adverb (with the se-
mantics of -ly adverbs in English), modifying a verb
usually to the right. Thus such a sequence has to
be coalesced into a representation indicating this
derivational process.
(8) yava?s yava?s (? ?)
? yava?s(slow)+Adj
yava?s+Adj
? yava?s+Adj?DB+Adverb+Ly
?slowly? (literally ?slow slow?)
This kind of duplication can also occur when the
adjective is a derived adjective as in
(9) hzl hzl (? ?)
? hz(speed)+Noun+A3sg+Pnon+Nom
?DB+Adj+With
hz+Noun+A3sg+Pnon+Nom
?DB+Adj+With
? hz+Noun+A3sg+Pnon+Nom
?DB+Adj+With?DB+Adverb+Ly
?rapidly? (literally ?with-speed with-speed?)
? Turkish has a fairly large set of onomatopoeic
words which always appear in duplicate and func-
tion as manner adverbs. The words by themselves
have no other usage and literal meaning, and mildly
resemble sounds produced by natural or artificial
objects. In these cases, the root word almost al-
ways is reduplicated but need not be, but both words
should be of the part-of-speech category +Dup that
we use to mark such roots.
(10) harl hurul (?1 + X ?2 + X )
? harl+Dup
hurul+Dup
? harl_hurul+Adverb+Resemble
?making rough noises? (no literal meaning)
? Duplicated verbs with optative mood and third
person singular agreement function as manner ad-
verbs, indicating that another verb is executed in a
manner indicated by the duplicated verb:
(11) ko?sa ko?sa (? ?)
? ko?s(run)+Verb+Pos+Opt+A3sg
ko?s(run)+Verb+Pos+Opt+A3sg
? ko?s+Verb+Pos+?DB+Adverb+ByDoingSo
?by running? (literally ?let him run let him run?)
? Duplicated verbs in aorist mood with third person
agreement and first positive then negative polarity,
function as temporal adverbs with the semantics ?as
soon as one has verbed?
(12) uyur uyumaz (? + X ? + Y )
? uyu+Verb+Pos+Aor+A3sg
uyu+Verb+Neg+Aor+A3sg
? uyu+Verb+Pos+?DB+Adverb+AsSoonAs
?as soon as (he) sleeps? ( literally ?(he) sleeps (he)
does not sleep?)
It should be noted that for most of the non-
lexicalized collocations involving verbs (like (11)
and (12) above), the verbal portion before the in-
flectional marking mood can have additional deriva-
tional markers and all such markers have to dupli-
cate.
(13) sa

glamla?strr sa

glamla?strmaz (?+X ?+Y )
? saglam+Adj?DB+Verb+Become
?DB+Verb+Caus?DB+Verb+Pos+Aor+A3sg
saglam+Adj?DB+Verb+Become
?DB+Verb+Caus?DB+Verb+Neg+Aor+A3sg
? saglam+Adj?DB+Verb+Become+
?DB+Verb+Caus+Pos
?DB+Adverb+AsSoonAs
?as soon as (he) fortifies (causes to become strong)?
Another interesting point is that non-lexicalized col-
locations can interact with semi-lexicalized collo-
cations since they both usually involve verbs. For
instance, when the verb of the semi-lexicalized col-
location example in (5) is duplicated in the form of
the non-lexicalized collocation in (12), we get
(14) kafalar? ?eker ?ekmez
In this case, first the non-lexicalized collocation has
to be coalesced into
(15) kafalar? ?ek+Verb+Pos
?DB+Adverb+AsSoonAs
and then the semi-lexicalized collocation kicks in,
to give
(16) kafa_?ek+Verb+Pos
?DB+Adverb+AsSoonAs
(?as soon as (we/you/they) get drunk?)
Finally, the following non-lexicalized collocation
involving adjectival forms involving duplication and
a question clitic is an example of the last type of
non-lexicalized collocation.
(17) g?zel mi g?zel (? Z ?)
? g?zel+Adj
mi+Ques
g?zel+Adj
? g?zel+Adj+Very
?very beautiful? (literally ?beautiful (is it?) beauti-
ful?)
2.4 Named-entities
Another class of multi-word expressions that we
process is the class of multi-word named-entities
denoting persons, organizations and locations. We
essentially treat these just like the semi-lexicalized
collocation discussed earlier, in that, when such
named-entities are used in text, all but the last com-
ponent are fixed and the last component will usually
undergo certain morphological processes demanded
by the syntactic context as in
Figure 1: The architecture of the multi-word expres-
sion extraction processor
(18) T?rkiye B?y?k Millet Meclisi?nde ....7
Here, the last component is case marked and this
represents a case marking on the whole named-
entity. We package this as
(19) T?rkiye_B?y?k_Millet_Meclisi
+Noun+Prop+A3sg+Pnon+Loc
To recognize these named entities we use a rather
simple approach employing a rather extensive
database of person, organization and place names,
developed in the context of a previous project, in-
stead of using a more sophisticated named-entity
extraction scheme.
3 The Structure of the Multi-word
Expression Processor
Our multi-word expression processor is a multi-
stage system as depicted in Figure 1. The first
component is a standard tokenizer which splits in-
put text into constituent tokens. These then go into
7In the Turkish Grand National Assembly.
a wide-coverage morphological analyzer (Oflazer,
1994) implemented using Xerox finite state technol-
ogy (Karttunen et al, 1997), which generates, for all
tokens, all possible morphological analyses. This
module also performs unknown processing by pos-
tulating possible noun roots and then trying to parse
the rest of a word as a sequence of possible Turk-
ish suffixes. The morphological analysis stage also
performs a very conservative non-statistical mor-
phological disambiguation to remove some very un-
likely parses based on unambiguous contexts. Fig-
ure 2 shows a sample Turkish text that comes out of
morphological processing, about to go into multi-
word expression extraction.
Kistin kist+Noun+A3sg+P2sg+Nom
kist+Noun+A3sg+Pnon+Gen
saglgm saglk+Noun+A3sg+P1sg+Acc
sag+Adj?DB+Noun+Ness+
A3sg+P1sg+Acc
skntya sknt+Noun+A3sg+Pnon+Dat
sokacak sok+Verb+Pos+Fut+A3sg
sok+Verb+Pos?DB+Adj
+FutPart+Pnon
herhangi herhangi+Adj
bir bir+Det
bir+Num+Card
bir+Adj
bir+Adverb
etkisi etki+Noun+A3sg+P3sg+Nom
s?z s?z+Noun+A3sg+Pnon+Nom
konusu konu+Noun+A3sg+P3sg+Nom
degil deg+Verb?DB+Verb+Pass
+Pos+Imp+A2sg
degil+Conj
degil+Verb+Pres+A3sg
. .+Punc
Figure 2: Output of the morphological analyzer
The multi-word expression extraction processor has
three stages with the output of one stage feeding into
the next stage:
1. The first stage handles lexicalized collocations
and multi-word named entities.
2. The second stage handles non-lexicalized col-
locations.
3. The third stage handles semi-lexicalized col-
locations. The reason semi-lexicalized collo-
cations are handled last, is that any duplicate
verb formations have to be processed before
compound verbs are combined with their lexi-
calized complements (cf. examples (14) ? (16)
above).
The output of the multi-word expression extraction
processor for the relevant segments in Figure 2 is
given in Figure 3.
The multi-word expression extraction processor has
been implemented in Perl. The rule bases for
the three stages are maintained separately and then
compiled offline into regular expressions which are
then used by Perl at runtime.
...
skntya_sokacak skntya_sok+Verb
+Pos+Fut+A3sg
skntya_sok+Verb
+Pos?DB+Adj
+FutPart+Pnon
herhangi_bir herhangi_bir+Det
...
s?z_konusu s?z_konu+Noun+A3sg
+P3sg+Nom
...
Figure 3: Output of the multi-word expression ex-
traction processor
Table 1 presents statistics on the current rule base
of our multi-word expression extraction processor:
For named entity recognition, we use a list of about
Rule Type Number of Rules
Lexicalized Colloc. 363
Semi-lexicalized Colloc. 731
Non-lexicalized Colloc. 16
Table 1: Rules base statistics
60,000 first and last names, a list of about 16,000
multi-word organization and place names.
4 Evaluation
To improve and evaluate our multi-word expression
extraction processor, we used two corpora of news
text. We used a corpus of about 730,000 tokens to
incrementally test and improve our semi-lexicalized
rule base, by searching for compound verb forma-
tions, etc. Once such rules were extracted, we tested
our processor on this corpus, and on a small corpus
of about 4200 words to measure precision and re-
call. Table 2 provides some statistics on these cor-
pora.
Table 3 shows the result of multi-word expression
extraction on the large (training) corpus. It should
be noted that we only mark multi-word named-
entities, not all. Thus many references to persons by
Corpus Number of Avg. Analyses
Tokens per Token
Large Corpus 729,955 1.760
Small Corpus 4,242 1.702
Table 2: Corpora Statistics
their last name are not marked, hence the low num-
ber of named-entities extracted.8 As a result of
MW Type Number Extracted
Lexicalized Colloc. 3,883
Semi-lexicalized Colloc. 9,173
Non-lexicalized Colloc. 220
Named-Entities 4,480
Total 17,750
Table 3: Multi-word expression extraction statistics
on the large corpus
this extraction, the average number of morphologi-
cal parses per token go from 1.760 down to 1.745.
Table 4 shows the result of multi-word expression
extraction on the small corpus. We also manu-
MW Type Number Extracted
Lexicalized Colloc. 15
Semi-lexicalized Colloc. 62
Non-lexicalized Colloc. 0
Named-Entities 99
Total 176
Table 4: Multi-word expression extraction statistics
on the small corpus
ally marked up the small corpus into a gold-standard
corpus to test precision and recall. The results in
Table 4 correspond to an overall recall of 65.2%
and a precision of 98.9%, over all classes of multi-
word expressions. When we consider all classes
except named-entities, we have a recall of 60.1%
and a precision of 100%. An analysis of the er-
rors and missed multi-word expressions indicates
that the test corpus had a certain variant of a com-
pound verb construction that we had failed to ex-
tract from the larger corpus we used for compil-
ing rules. Failing to extract the multi-word expres-
sions for that compound verb accounted for most
of the drop in recall. Since we are currently using
a rather naive named-entity extraction scheme,9 re-
8Since this is a very large corpus, we have no easy way of
obtaining accurate precision and recall figures.
9As opposed to a general purpose statistical NE extractor
that we have developed earlier (T?r et al, 2003).
call is rather low as there are quite a number of for-
eign multi-word named-entities (persons and orga-
nizations mostly) that do not exist in our database
of named-entities. On the other hand, since named-
entity extraction for English is a relatively mature
technology, we can easily integrate an existing tool
to improve our recall.
5 Conclusions
This paper has described a multi-word expression
extraction system for Turkish for handling vari-
ous types of multi-word expressions such as semi-
lexicalized and non-lexicalized collocations which
depend on the recognition of certain morphologi-
cal patterns across tokens. Our results indicate that
with about 1100 rules (most of which were extracted
from a large ?training corpus? searching for patterns
involving a certain small set of support verbs), we
were able get alost 100% precision and around
60% recall on a small ?test? corpus. We expect that
with additional rules from dictionaries and other
sources we will improve recall significantly.
6 Acknowledgments
We thank Orhan Bilgin for helping us compile the
multi-word expressions.
References
Timothy Baldwin and Aline Villavicencio. 2002.
Extracting the unextractable: A case study on
verb-particles. In Proceedings of the Sixth Confer-
ence on Computational Natural Language Learning
(CoNLL 2002), pages 99?105.
Lauri Karttunen, Tamas Gaal, and Andre Kempe.
1997. Xerox Finite-State Tool. Technical report,
Xerox Research Centre Europe.
Kemal Oflazer. 1994. Two-level description of
Turkish morphology. Literary and Linguistic Com-
puting, 9(2):137?148.
Scott S. L. Piao, Paul Rayson, Dawn Archer, An-
drew Wilson, and Tony McEnery. 2003. Extracting
multiword expressions with a semantic tagger. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15.
G?khan T?r, Dilek Zeynep Hakkani-T?r, and Ke-
mal Oflazer. 2003. A statistical information extrac-
tion systems for Turkish. Natural Language Engi-
neering, 9(2).
R. Urizar, N. Ezeiza, and I. Alegria. 2000. Mor-
phosyntactic structure of terms in Basque for auto-
matic terminology extraction. In Proceedings of the
ninth EURALEX International Congress.
A Morphosyntactic Features For Turkish
This section lists the features and their semantics for
the morphological representations used in the text.
?DB marks a derivation boundary.
? Parts-of-speech:+Noun, +Adjective, +Adverb,
+Verb, +Dup (for onomatopoeic words which al-
ways appear in duplicate), +Question (yes/no ques-
tion marker clitic), +Number, +Determiner
? Agreement: +A[1-3][sg-pl], e.g., +A3pl.
? Possessive agreement: +P[1-3][sg-pl] and
+Pnon, e.g., +P1sg
? Case: +Nominative, +Accusative, +Locative,
+Ablative, +Instrumental, +Genitive, +Dative.
? Miscellaneous Verbal Features: +Causative,
+Passive, +Positive Polarity, +Negative Polar-
ity, +Optative Mood, +Aorist Aspect, +Become,
+Conditional Mood, +Imperative Mood, +Past
tense
? Miscellaneous POS Subtypes: Adverbs: +By
(as in ?house by house?), +ByDoingSo, (as in
?he came by running?), +Resemble (as in ?he
made sounds resembling ..?), +Ly (as in ?slowly?)
+AsSoonAs (as in ?he came down as soon as he
woke up?); Adjectives: +With (as in ?the book
with red cover?), +FutPart ? future participle ?
as in (?the boy who will come?); Nouns:+Proper
Noun, +Ness (as in ?sick-ness?), +FutPart ?
future participle fact ? as in (?I know that he will
come?) ; Numbers: +Cardinal
Proceedings of the Workshop on Statistical Machine Translation, pages 7?14,
New York City, June 2006. c?2006 Association for Computational Linguistics
Initial Explorations in English to Turkish Statistical Machine Translation
?Ilknur Durgar El-Kahlout
Faculty of Enginering
and
Natural Sciences
Sabanc? University
Istanbul, 34956, Turkey
ilknurdurgar@su.sabanciuniv.edu
Kemal Oflazer
Faculty of Engineering
and
Natural Sciences
Sabanc? University
Istanbul, 34956, Turkey
oflazer@sabanciuniv.edu
Abstract
This paper presents some very prelimi-
nary results for and problems in develop-
ing a statistical machine translation sys-
tem from English to Turkish. Starting with
a baseline word model trained from about
20K aligned sentences, we explore various
ways of exploiting morphological struc-
ture to improve upon the baseline sys-
tem. As Turkish is a language with com-
plex agglutinative word structures, we ex-
periment with morphologically segmented
and disambiguated versions of the parallel
texts in order to also uncover relations be-
tween morphemes and function words in
one language with morphemes and func-
tions words in the other, in addition to re-
lations between open class content words.
Morphological segmentation on the Turk-
ish side also conflates the statistics from
allomorphs so that sparseness can be al-
leviated to a certain extent. We find
that this approach coupled with a simple
grouping of most frequent morphemes and
function words on both sides improve the
BLEU score from the baseline of 0.0752
to 0.0913 with the small training data. We
close with a discussion on why one should
not expect distortion parameters to model
word-local morpheme ordering and that a
new approach to handling complex mor-
photactics is needed.
1 Introduction
The availability of large amounts of so-called par-
allel texts has motivated the application of statisti-
cal techniques to the problem of machine translation
starting with the seminal work at IBM in the early
90?s (Brown et al, 1992; Brown et al, 1993). Statis-
tical machine translation views the translation pro-
cess as a noisy-channel signal recovery process in
which one tries to recover the input ?signal? e, from
the observed output signal f.1
Early statistical machine translation systems used
a purely word-based approach without taking into
account any of the morphological or syntactic prop-
erties of the languages (Brown et al, 1993). Lim-
itations of basic word-based models prompted re-
searchers to exploit morphological and/or syntac-
tic/phrasal structure (Niessen and Ney, (2004),
Lee,(2004), Yamada and Knight (2001), Marcu and
Wong (2002), Och and Ney (2004),Koehn et al
(2003), among others.)
In the context of the agglutinative languages sim-
ilar to Turkish (in at least morphological aspects) ,
there has been some recent work on translating from
and to Finnish with the significant amount of data
in the Europarl corpus. Although the BLEU (Pap-
ineni et al, 2002) score from Finnish to English is
21.8, the score in the reverse direction is reported
as 13.0 which is one of the lowest scores in 11 Eu-
ropean languages scores (Koehn, 2005). Also, re-
ported from and to translation scores for Finnish are
the lowest on average, even with the large number of
1Denoting English and French as used in the original IBM
Project which translated from French to English using the paral-
lel text of the Hansards, the Canadian Parliament Proceedings.
7
sentences available. These may hint at the fact that
standard alignment models may be poorly equipped
to deal with translation from a poor morphology lan-
guage like English to an complex morphology lan-
guage like Finnish or Turkish.
This paper presents results from some very pre-
liminary explorations into developing an English-to-
Turkish statistical machine translation system and
discusses the various problems encountered. Start-
ing with a baseline word model trained from about
20K aligned sentences, we explore various ways of
exploiting morphological structure to improve upon
the baseline system. As Turkish is a language with
agglutinative word structures, we experiment with
morphologically segmented and disambiguated ver-
sions of the parallel text, in order to also uncover
relations between morphemes and function words in
one language with morphemes and functions words
in the other, in addition to relations between open
class content words; as a cursory analysis of sen-
tence aligned Turkish and English texts indicates
that translations of certain English words are actu-
ally morphemes embedded into Turkish words. We
choose a morphological segmentation representa-
tion on the Turkish side which abstracts from word-
internal morphological variations and conflates the
statistics from allomorphs so that data sparseness
can be alleviated to a certain extent.
This paper is organized as follows: we start with
the some of the issues of building an SMT system
into Turkish followed by a short overview Turk-
ish morphology to motivate its effect on the word
alignment problem with English. We then present
results from our explorations with a baseline sys-
tem and with morphologically segmented parallel
aligned texts, and conclude after a short discussion.
2 Issues in building a SMT system for
Turkish
The first step of building an SMT system is the com-
pilation of a large amount of parallel texts which
turns out to be a significant problem for the Turkish
and English pair. There are not many sources of such
texts and most of what is electronically available
are parallel texts diplomatic or legal domains from
NATO, EU, and foreign ministry sources. There
is also a limited amount data parallel news corpus
available from certain news sources. Although we
have collected about 300K sentence parallel texts,
most of these require significant clean-up (from
HTML/PDF sources) and we have limited our train-
ing data in this paper to about 22,500 sentence sub-
set of these parallel texts which comprises the sub-
set of sentences of 40 words or less from the 30K
sentences that have been cleaned-up and sentence
aligned.2  3
The main aspect that would have to be seri-
ously considered first for Turkish in SMT is the
productive inflectional and derivational morphol-
ogy. Turkish word forms consist of morphemes
concatenated to a root morpheme or to other mor-
phemes, much like ?beads on a string? (Oflazer,
1994). Except for a very few exceptional cases,
the surface realizations of the morphemes are con-
ditioned by various local regular morphophonemic
processes such as vowel harmony, consonant assim-
ilation and elisions. Further, most morphemes have
phrasal scopes: although they attach to a partic-
ular stem, their syntactic roles extend beyond the
stems. The morphotactics of word forms can be
quite complex especially when multiple derivations
are involved. For instance, the derived modifier
sag?lamlas?t?rd?g??m?zdaki 4 would be bro-
ken into surface morphemes as follows:
sag?lam+las?+t?r+d?g?+?m?z+da+ki
Starting from an adjectival root sag?lam, this word
form first derives a verbal stem sag?lamlas?, meaning
?to become strong?. A second suffix, the causative
surface morpheme +t?r which we treat as a verbal
derivation, forms yet another verbal stem meaning
?to cause to become strong? or ?to make strong (for-
tify)?. The immediately following participle suffix
2We are rapidly increasing our cleaned-up text and expect to
clean up and sentence align all within a few months.
3As the average Turkish word in running text has between
2 and 3 morphemes we limited ourselves to 40 words in the
parallel texts in order not to exceed the maximum number of
words recommended for GIZA++ training.
4Literally, ?(the thing existing) at the time we caused (some-
thing) to become strong?. Obviously this is not a word that one
would use everyday, but already illustrates the difficulty as one
Turkish ?word? would have to be aligned to a possible discon-
tinues sequence of English words if we were to attempt a word
level alignment. Turkish words (excluding noninflecting fre-
quent words such as conjunctions, clitics, etc.) found in typical
running text average about 10 letters in length. The average
number of bound morphemes in such words is about 2.
8
+d?g?, produces a participial nominal, which inflects
in the normal pattern for nouns (here, for 1  per-
son plural possessor which marks agreement with
the subject of the verb, and locative case). The final
suffix, +ki, is a relativizer, producing a word which
functions as a modifier in a sentence, modifying a
noun somewhere to the right.
However, if one further abstracts from the mor-
phophonological processes involved one could get a
lexical form
sag?lam+lAs?+DHr+DHk+HmHz+DA+ki
In this representation, the lexical morphemes ex-
cept the lexical root utilize meta-symbols that stand
for a set of graphemes which are selected on the
surface by a series of morphographemic processes
which are rooted in morphophonological processes
some of which are discussed below, but have noth-
ing whatsoever with any of the syntactic and se-
mantic relationship that word is involved in. For
instance, A stands for back and unrounded vowels
a and e, in orthography, H stands for high vow-
els ?, i, u and u?, and D stands for d and t, repre-
senting alveolar consonants. Thus, a lexical mor-
pheme represented as +DHr actually represents 8
possible allomorphs, which appear as one of +d?r,
+dir, +dur, +du?r, +t?r, +tir, +tur, +tu?r depending
on the local morphophonemic context. Thus at this
level of representation words that look very differ-
ent on the surface, look very similar. For instance,
although the words masas?nda ?on his table? and def-
terinde ?in his notebook? in Turkish look quite dif-
ferent, the lexical morphemes except for the root
are the same: masas?nda has the lexical structure
masa+sH+ndA, while defterinde has the lexical
structure defter+sH+ndA.
The use of this representation is particularly im-
portant for Turkish for the following reason. Allo-
morphs which differ because of local word-internal
morphographemic and morphotactical constraints
almost always correspond to the same words or units
in English when translated. When such units are
considered by themselves as the units in alignment,
statistics get fragmented and the model quality suf-
fers. On the other hand, this representation if di-
rectly used in a standard SMT model such as IBM
Model 4, will most likely cause problems, since
now, the distortion parameters will have to take on
the task of generating the correct sequence of mor-
phemes in a word (which is really a local word-
internal problem to be solved) in addition to gen-
erating the correct sequence of words.
3 Aligning English?Turkish Sentences
If an alignment between the components of paral-
lel Turkish and English sentences is computed, one
obtains an alignment like the one shown in Figure
1, where it is clear that Turkish words may actually
correspond to whole phrases in the English sentence.
Figure 1: Word level alignment between a Turkish
and an English sentence
One major problem with this situation is that even
if a word occurs many times in the English side,
the actual Turkish equivalent could be either miss-
ing from the Turkish part, or occur with a very low
frequency, but many inflected variants of the form
could be present. For example, Table 1 shows the
occurrences of different forms for the root word
faaliyet ?activity? in the parallel texts we experi-
mented with. Although, many forms of the root
word appear, none of the forms appear very fre-
quently and one may even have to drop occurrences
of frequency 1 depending on the word-level align-
ment model used, further worsening the sparseness
problem.5
To overcome this problem and to get the max-
imum benefit from the limited amount of parallel
texts, we decided to perform morphological analy-
sis of both the Turkish and the English texts to be
able to uncover relationships between root words,
suffixes and function words while aligning them.
5A noun root in Turkish may have about hundred inflected
forms and substantially more if productive derivations are con-
sidered, meanwhile verbs can have thousands of inflected and
derived forms if not more.
9
Table 1: Forms of the word faaliyet ?activity?
Wordform Count Gloss
faaliyet 3 ?activity?
faaliyete 1 ?to the activity?
faaliyetinde 1 ?in its activity?
faaliyetler 3 ?activities?
faaliyetlere 6 ?to the activities?
faaliyetleri 7 ?their activities?
faaliyetlerin 7 ?of the activities?
faaliyetlerinde 1 ?in their activities?
faaliyetlerine 5 ?to their activities?
faaliyetlerini 1 ?their activities (acc.)?
faaliyetlerinin 2 ?of their activities?
faaliyetleriyle 1 ?with their activities?
faaliyette 2 ?in (the) activity?
faaliyetteki 1 ?that which is in activity?
Total 41
On the Turkish side, we extracted the lexical mor-
phemes of each word using a version of the mor-
phological analyzer (Oflazer, 1994) that segmented
the Turkish words along morpheme boundaries and
normalized the root words in cases they were de-
formed due to a morphographemic process. So
the word faaliyetleriyle when segmented into lexical
morphemes becomes faaliyet +lAr +sH +ylA. Am-
biguous instances were disambiguated statistically
(Ku?lekc?i and Oflazer, 2005).
Similarly, the English text was tagged using Tree-
Tagger (Schmid, 1994), which provides a lemma
and a POS for each word. We augmented this pro-
cess with some additional processing for handling
derivational morphology. We then dropped any tags
which did not imply an explicit morpheme (or an
exceptional form). The complete set of tags that are
used from the Penn-Treebank tagset is shown in Ta-
ble 2.6 To make the representation of the Turkish
texts and English texts similar, tags are marked with
a ?+? at the beginning of all tags to indicate that such
tokens are treated as ?morphemes.? For instance,
the English word activities was segmented as activ-
6The tagset used by the TreeTagger is a refinement of Penn-
Treebank tagset where the second letter of the verb part-of-
speech tags distinguishes between ?be? verbs (B), ?have? verbs
(H) and other verbs (V),(Schmid, 1994).
ity +NNS. The alignments we expected to obtain are
depicted in Figure 2 for the example sentence given
earlier in Figure 1.
Table 2: The set of tags used to mark explicit mor-
phemes in English
Tag Meaning
JJR Adjective, comparative
JJS Adjective, superlative
NNS Noun, plural
POS Possessive ending
RBR Adverb, comparative
RBS Adverb, superlative
VB Verb, base form
VBD Verb, past tense
VBG Verb, gerund or present participle
VBN Verb, past participle
VBP Verb, non3rd person singular present
VBZ Verb, 3rd person singular present
Figure 2: ?Morpheme? alignment between a Turkish
and an English sentence
4 Experiments
We proceeded with the following sequence of exper-
iments:
(1) Baseline: As a baseline system, we used a
pure word-based approach and used Pharaoh Train-
ing tool (2004), to train on the 22,500 sentences, and
decoded using Pharaoh (Koehn et al, 2003) to ob-
tain translations for a test set of 50 sentences. This
gave us a baseline BLEU score of 0.0752.
(2) Morpheme Concatenation: We then trained
the same system with the morphemic representation
10
of the parallel texts as discussed above. The de-
coder now produced the translations as a sequence
of root words and morphemes. The surface words
were then obtained by just concatenating all the
morphemes following a root word (until the next
root word) taking into just morphographemic rules
but not any morphotactic constraints. As expected
this ?morpheme-salad? produces a ?word-salad?, as
most of the time wrong morphemes are associated
with incompatible root words violating many mor-
photactic constraints. The BLEU score here was
0.0281, substantially worse than the baseline in (1)
above.
(3) Selective Morpheme Concatenation: With
a small script we injected a bit of morphotactical
knowledge into the surface form generation process
and only combined those morphemes following a
root word (in the given sequence), that gave rise to
a valid Turkish word form as checked by a morpho-
logical analyzer. Any unused morphemes were ig-
nored. This improved the BLEU score to 0.0424
which was still below the baseline.
(4) Morpheme Grouping: Observing that certain
sequence of morphemes in Turkish texts are trans-
lations of some continuous sequence of functional
words and tags in English texts, and that some mor-
phemes should be aligned differently depending on
the other morphemes in their context, we attempted
a morpheme grouping. For example the morpheme
sequence +DHr +mA marks infinitive form of a
causative verb which in Turkish inflects like a noun;
or the lexical morpheme sequence +yAcAk +DHr
usually maps to ?it/he/she will?. To find such groups
of morphemes and functional words, we applied a
sequence of morpheme groupings by extracting fre-
quently occuring n-grams of morphemes as follows
(much like the grouping used by Chiang (2005): in a
series of iterations, we obtained high-frequency bi-
grams from the morphemic representation of paral-
lel texts, of either morphemes, or of previously such
identified morpheme groups and neighboring mor-
phemes until up to four morphemes or one root 3
morpheme could be combined. During this process
we ignored those combinations that contain punctu-
ation or a morpheme preceding a root word. A simi-
lar grouping was done on the English side grouping
function words and morphemes before and after root
words.
The aim of this process was two-fold: it let fre-
quent morphemes to behave as a single token and
help Pharaoh with identification of some of the
phrases. Also since the number of tokens on both
sides were reduced, this enabled GIZA++ to produce
somewhat better alignments.
The morpheme level translations that were ob-
tained from training with this parallel texts were then
converted into surface forms by concatenating the
morphemes in the sequence produced. This resulted
in a BLEU score of 0.0644.
(5) Morpheme Grouping with Selective Mor-
pheme Concatenation: This was the same as (4)
with the morphemes selectively combined as in (3).
The BLEU score of 0.0913 with this approach was
now above the baseline.
Table 3 summarizes the results in these five exper-
iments:
Table 3: BLEU scores for experiments (1) to (4)
Exp. System BLEU
(1) Baseline 0.0752
(2) Morph. Concatenation. 0.0281
(3) Selective Morph. Concat. 0.0424
(4) Morph. Grouping and Concat. 0.0644
(5) Morph. Grouping + (3) 0.0913
In an attempt to factor out and see if the transla-
tions were at all successful in getting the root words
in the translations we performed the following: We
morphologically analyzed and disambiguated the
reference texts, and reduced all to sequences of root
words by eliminating all the morphemes. We per-
formed the same for the outputs of (1) (after mor-
phological analysis and disambiguation), (2) and (4)
above, that is, threw away the morphemes ((3) is
the same as (2) and (5) same as (4) here). The
translation root word sequences and the reference
root word sequences were then evaluated using the
BLEU (which would like to label here as BLEU-r
for BLEU root, to avoid any comparison to previous
results, which will be misleading. These scores are
shown in Figure 4.
The results in Tables 3 and 4 indicate that with the
standard models for SMT, we are still quite far from
even identifying the correct root words in the trans-
11
Table 4: BLEU-r scores for experiments (1), (2) and
(4)
Exp. System BLEU
(1) Baseline 0.0955
(2) Morph. Concatenation. 0.0787
(4) Morph. Grouping 0.1224
lations into Turkish, let alne getting the morphemes
and their sequences right. Although some of this
may be due to the (relatively) small amount of paral-
lel texts we used, it may also be the case that splitting
the sentences into morphemes can play havoc with
the alignment process by significantly increasing the
number of tokens per sentence especially when such
tokens align to tokens on the other side that is quite
far away.
Nevertheless the models we used produce some
quite reasonable translations for a small number of
test sentences. Table 5 shows the two examples of
translations that were obtained using the standard
models (containing no Turkish specific manipula-
tion except for selective morpheme concatenation).
We have marked the surface morpheme boundaries
in the translated and reference Turkish texts to in-
dicate where morphemes are joined for exposition
purposes here ? they neither appear in the reference
translations nor in the produced translations!
5 Discussion
Although our work is only an initial exploration
into developing a statistical machine translation sys-
tem from English to Turkish, our experiments at
least point out that using standard models to deter-
mine the correct sequence of morphemes within the
words, using more powerful mechanisms meant to
determine the (longer) sequence of words in sen-
tences, is probably not a good idea. Morpheme or-
dering is a very local process and the correct se-
quence should be determined locally though the ex-
istence of morphemes could be postulated from sen-
tence level features during the translation process.
Furthermore, insisting on generating the exact se-
quence of morphemes could be an overkill. On
the other hand, a morphological generator could
take a root word and a bag of morphemes and
Table 5: Some good SMT results
Input: international terrorism also remains to be an important
issue .
Baseline: ulus+lararas? tero?rizm de o?nem+li kal+m?s?+t?r . bir
konu ol+acak+t?r
Selective Morpheme Concatenation: ulus+lararas? tero?rizm
de ol+ma+ya devam et+mek+te+dir o?nem+li bir sorun+dur .
Morpheme Grouping: ulus+lararas? tero?rizm de o?nem+li bir
sorun ol+ma+ya devam et+mek+te+dir .
Reference Translation: ulus+lararas? tero?rizm de o?nem+li bir
sorun ol+ma+ya devam et+mek+te+dir .
Input: the initiation of negotiations will represent the
beginning of a next phase in the process of accession
Baseline: mu?zakere+ler+in go?r+u?s?+me+ler yap+?l+acak bir
der+ken as?ama+n?n hasar+? su?rec+i bas?lang?c+?+n? 15+?i
Selective Morpheme Concatenation: initiation mu?zakere+ler
temsil ed+il+me+si+nin bas?lang?c+? bir as?ama+s?+nda kat?l?m
su?rec+i+nin ertesi
Morpheme Grouping: mu?zakere+ler+in bas?la+ma+s?+n?n
bas?lang?c+?+n? temsil ed+ecek+tir kat?l?m su?rec+i+nin bir
sonra+ki as?ama
Reference Translation: mu?zakere+ler+in bas?la+ma+s? ,
kat?l?m su?rec+i+nin bir sonra+ki as?ama+s?+n?n bas?lang?c+?+n?
temsil ed+ecek+tir
generate possible legitimate surface words by tak-
ing into account morphotactic constraints and mor-
phographemic constraints, possibly (and ambigu-
ously) filling in any morphemes missing in the trans-
lation but actually required by the morphotactic
paradigm. Any ambiguities from the morphologi-
cal generation could then be filtered by a language
model.
Such a bag-of-morphemes approach suggests that
we do not actually try to determine exactly where the
morphemes actually go in the translation but rather
determine the root words (including any function
words) and then associate translated morphemes
with the (bag of the) right root word. The resulting
sequence of root words and their bags-of-morpheme
can be run through a morphological generator which
can handle all the word-internal phenomena such as
proper morpheme ordering, filling in morphemes or
even ignoring spurious morphemes, handling local
morphographemic phenomena such as vowel har-
mony, etc. However, this approach of not placing
morphemes into specific position in the translated
output but just associating them with certain root
words requires that a significantly different align-
ment and decoding models be developed.
Another representation option that could be em-
12
ployed is to do away completely with morphemes on
the Turkish side and just replace them with morpho-
logical feature symbols (much like we did here for
English). This has the advantage of better handling
allomorphy ? all allomorphs including those that are
not just phonological variants map to the same fea-
ture, and homograph morphemes which signal dif-
ferent features map to different features. So in a
sense, this would provide a more accurate decom-
position of the words on the Turkish side, but at the
same time introduce a larger set of features since
default feature symbols are produced for any mor-
phemes that do not exist on the surface. Removing
such redundant features from such a representation
and then using reduced features could be an interest-
ing avenue to pursue. Generation of surface words
would not be a problem since, our morphological
generator does not care if it is input morphemes or
features.
6 Conclusions
We have presented the results of our initial explo-
rations into statistical machine translation from En-
glish to Turkish. Using a relatively small parallel
corpus of about 22,500 sentences, we have exper-
imented with a baseline word-to-word translation
model using the Pharaoh decoder. We have also ex-
perimented with a morphemic representation of the
parallel texts and have aligned the sentences at the
morpheme level. The decoder in this cases produces
root word and morpheme sequences which are then
selectively concatenated into surface words by pos-
sibly ignoring some morphemes which are redun-
dant or wrong. We have also attempted a simple
grouping of root words and morphemes to both help
the alignment by reducing the number of tokens in
the sentences and by already identifying some pos-
sible phrases. This grouping of morphemes and the
use of selective morpheme concatenation in produc-
ing surface words has increased the BLEU score
for our test set from 0.0752 to 0.0913. Current
ongoing work involves increasing the parallel cor-
pus size and the development of bag-of-morphemes
modeling approach to translation to separate the
sentence level word sequencing from word-internal
morpheme sequencing.
7 Acknowledgements
This work was supported by T ?UB?ITAK (Turkish
Scientific and Technological Research Foundation)
project 105E020 ?Building a Statistical Machine
Translation for Turkish and English?.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, John D. Lafferty, and Robert L. Mercer.
1992. Analysis, statistical transfer, and synthesis in
machine translation. In Proceeding of TMI: Fourth
International Conference on Theoretical and Method-
ological Issues in MT, pages 83?100.
Peter F Brown, Stephen A Della Pietra, Vincent J
Della Pietra, and Robert L Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL.
Philip Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT Summit X, Phuket,
Thailand.
M. Oguzhan Ku?lekc?i and Kemal Oflazer. 2005. Pro-
nunciation disambiguation in turkish. In Pinar Yolum,
Tunga Gu?ngo?r, Fikret S. Gu?rgen, and Can C. ?Ozturan,
editors, ISCIS, volume 3733 of Lecture Notes in Com-
puter Science, pages 636?645. Springer.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004 - Companion Volume, pages 57?60.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-02), Philadelphia.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic information. Computational Linguis-
tics, 30(2):181?204.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
13
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Kishore Papineni, Todd Ward Salim Roukos, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 311?318, Philadel-
phia, July. Association for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics, pages 00?00, Toulouse.
14
Statistical Morphological Disambiguation for 
Agglutinative Languages 
Di lek  Z. Hakkani-Tfir,  Kemal  Oflazer, GSkhan T i i r  
\ ])el)aIt lncn(;  of COmlmtcr  Engin(',ering, 
B i lkent  Univers i ty,  
Ankara ,  06533, TU l l .KEY  
{hakkani, ko, tur}~cs,  b i lkent ,  edu. t r  
Abstract 
In this 1)aper, we present sta.tistical models for 
morphological disambiguation i Tm'kish. Turkish 
presents an interesting problem for statistical ,nodcls 
since the potential tag set size is very large because 
of the productive, derivational morl/hology. \Ve pro- 
pose to handle this by breaking Ul) 1;11(; morhosyn- 
tactic tags into inflectional groups, each of which 
contains the inflectional features ti)r each (internm- 
diate) derived tbrm. Our statistical models score the 
probability of each morhosyntactic tag by consider- 
ing statistics over the individual inflection groups 
in a trigram model. Among the three models that 
we have deveh)l)ed and tested, (;11(; simplest model 
ignoring the lo(:al mort)hota(:ties within words l)er- 
tbrms the best. Ollr })('.st; rigram model 1)erfornls 
with 93.95% accuracy on otir test data getting all 1;11o 
lllorhosyllta(;ti(; aild semantic fc.atul'es correct. If we 
are just interested in syntactically relevant features 
alld igilore a very sinall set of semantic features, then 
(;tie accuracy increases to 95.07%. 
1 I n t roduct ion  
Re(:ent advances in (:onltmter har(lware and avail- 
al)ility of very large corpora have made (;t1(` - al)pli- 
cation of s(;atistical techniques to natural language 
processing a t)asible and a very at)pealing resem'ch 
area. Many useflll results have 1)cell obtained by 
applyilig these techniques to English (and similar 
languages) in parsing, word sense dismnbiguation, 
part-of  speech (POS) tagging, speech recognition, 
et;c. However, languages like Turkish, Czech, Hun- 
garian and Finnish, displ W a substantially different 
behavior than English. Unlike English, these lan- 
guages have agglutinative or inflective morphology 
and relatively free constituent order. Such languages 
have. received little previous attention in statistical 
processing. 
In this lmper, we t)resent our work on modeling 
Turkish using statistical methods, and present re- 
suits on morphological disainbiguation. The meth- 
ods developed here are certainly al)plicable to other 
agglutinative languages, especially those involving 
productive derivational phenomena. The Iml)er is 
organized as follows: After a brief overview of re- 
lated previous work, we smnma.rize relevant aspects 
of Turkish and present details of various statistical 
models for nlorlfliological disanfl/iguation for Turk- 
ish. We then present results and analyses fronl our 
experiments. 
2 Related  Work  
There has been a large numl)er of studies in tag- 
ging and mori)hological disambiguation using vari- 
ous techniques. POS taggiug systems have used ei- 
ther a statistical or a rule-based approach, hi the 
statistical api)roach, a large corpus \]ms been used to 
train a t)rotmbilistic model wlfieh then has been used 
to tag new text, assigning the most likely tag for a 
given word in a given context (e.g., Church (1.988), 
Cutting el; al. (1992)). In the rule-based approach, 
a large mmfl>e.r of hand-craft;ed linguisiic constraints 
are used to elinfinate impossible tags or morpho- 
logical t)arse.s tbr a given word in a given context 
(Ka.rlsson et al, 1995). Brill (1995a) has presented 
a transfl)rnmtioi>based lea.rning at)l)roach, whi(:h in- 
duces disanlbiguation rules from tagged corpora. 
Morphologi(:al disanlbiguation i inflecting or ag- 
glutinative languages with COlnl)lex morphology in- 
volves more than determining the major or minor 
Imrts-of-sl)cech of the lexiea.l items. Typically, roof  
phology marks a mlmber of inflectional or deriva- 
tioiml features and this involves ambiguity. For in- 
stance, a given word nlay be chopl)ed up in difl'erent 
ways into mort)heroes , a given mort)heine may inark 
different features depending on the morphotactics, 
or lexicalized variants of derived words may interact 
with productively derived versions (see Ottazer and 
Tiir (1997) for the difl'erent kinds of morphological 
ambiguities in Turkish.) We assume that all syn- 
tactically relevant fcat'urcs of word forms have to be 
determined correctly for morphological disambigua.- 
tion. 
In this context, there have l)een some interesting 
previous studies for difl'erent languages. Levinger 
ct al. (1995) have reported on an approach that 
learns morpholexical probabilities fi'om an mltagged 
eorlms mid have. used the resulting infornlation in 
285 
morphological disambiguation in Hebrew. Haji~: 
and Hla(lk~i (1998) have used ntaximunl entropy 
modeling approach for morphological dismnbigua- 
tion in Czech. Ezeiza et al (1998) have combined 
stochastic and rule-based isambiguation methods 
for Basque. Megyesi (19991 has adapted Brill's POS 
tagger with extended lexical templates to Itungar- 
tan. 
Previous ai)proaches to morphological dismnbi- 
guation of Turkish text; had employed a constraint- 
based approach (Otlazer and KuruSz, 1994; Oflazer 
and Tiir, 1996; Oflazer and Tiir, 1997). Although 
results obtained earlier in these at)preaches were rea- 
sonable, the fact that tim constraint rules were hand 
crafted posed a rather serious impediment o the 
generality and improvement of these systems. 
3 Turk i sh  
Turkish is a flee constituent order language. Tlm 
order of the constituents may clmnge freely accord- 
ing to tim discourse context and the syntactic role of 
the constituents i indicated by their case marking. 
Turkish has agglutinative morphology with produc- 
tive inflectional and derixmtional suflixations. The 
number of word forms one can derive from a Turkish 
root; form mW be in the millions (ttankmner, 19891. 
Hence, the number of distinct word forms, i.e., the 
vocabulary size, can be very large. For instance, Ta- 
ble 1 shows the size of the vocabulary for I and 10 
million word corl)ora of Turkish, collected from on- 
line newspaI)ers. This large vocabulary is the reason 
Corpus  s ize  Vocabu lary  s ize  
1M words 106,547 
10M words 41.7,775 
Table 1: Vocabulary sizes for two Turkish corpora. 
for a serious data sparseness problem and also sig- 
nificantly increases the number of parameters to be 
estimated even for a bigram language model. The 
size of the vocabulary also causes the perplexity to 
be large (although this is not an issue in morpho- 
logical disambiguation). Table 2 lists tlm training 
and test set perplexities of trigram language models 
trained on 1 and 10 million word corpora for ri51rkish. 
For each corpus, tile first cohmm is the perplexity 
for the data the language model is trained on, and 
the second column is the pert)lexity for previously 
unseen test data of 1 million words. Another ma- 
jor reason for the high perplexity of Turkish is the 
high percentage of out-of  vocabulary words (words 
in the test; data which did not occur in the training 
data); this results from the productivity of the word 
tbrmation process. 
Training Training Set Test Set (1M words) 
Data Perplexity Perplexity 
1M words 66.13 t449.81 
10M words 94.08 1084.13 
Table 2: The pert)lexity of Turkish corpora using 
word-based trigram language models. 
The issue of large vocabulary brought in by pro- 
ductive inflectional and derivational processes also 
makes tagset design an important issue. 111 lan- 
guages like English, the nunlber of POS tags that can 
be assigned to the words in a text; is rather linfited 
(less than 100, though some researchers have used 
large tag sets to refine g,:anularity, but they are still 
small compared to Turkish.) But, such a finite tagset 
al)proach for languages like Turkish may lead to an 
inevitable loss of information. The reason for this 
is that the lnorphological features of intermediate 
derivations can contain markers for syntactic rela- 
tionshil)s. Thus, leaving out this information witlfin 
a fixed-tagset scheme may prevent crucial syntac- 
tic information fl'om being represented (Oilazer et 
al., 1999). For examl)le , it; is not clear what POS 
tag sllould be assigned to the word sa.~lamlaqtwmak 
(below), without losing any information, the cate- 
gory of the root; (Adjective), tile final category of 
the word as a whole (Noun) or one of the interme- 
diate categories (Verb). 1 
s a\[flam +laq + t,r + ma~: 
saglam+kdj ^DB+yerb+Become^DB 
+gerb+Caus+Pos ^DB+Noun+Inf +A3sg+Pnon+lqom 
to ca'ass (,s'ometh, i.ng) to become stron 9 / 
to strength, or,/fortify (somcth, ing) 
Ignoring the fact that the root; word is an adjec- 
tive may sever any relationslfips with a.n adverbial 
modifier modi~ying the root. Thus instead of a sim- 
I)le POS tag, wc use the full rno~Tflt, oIogical a'nahtscs 
of the words, rcprcscntcd as a combination of \]'ca- 
tures (including any dcrivational markers) as their 
morphosyntactic tags. For instance in the exami)le 
above, we would use everything including the root; 
form as the morphosyntactic tag. 
In order to alleviate the data sparseness probleln 
we break down the flfll tags. We represent each word 
as a sequence of inflectional groups (IGs hereafter), 
separated by "DBs denoting derivation boundaries, 
as described by Oflazer (1999). Thus a morphologi- 
cal parse would be represented in the following gen- 
eral tbrm: 
tThe morphological features other than the POSs are: 
+Become: become verb, +Cans: causative verb, +Pos: Positive 
polarity, +Inf: marker that derives an infinitive form fl'om a 
verb, +Aasg: 3sg number-person agreement, +Pnon: No pos- 
sessive agreement, m~d +Nora: Nominative case. "DB's mark 
derivational boundaries. 
286 
Full ~\[hgs (No roots) 
hfltectional GrouI)s 
Possil)le 
OO 
9,129 
O1)served 
10,531 
2,194 
%fl)le 3: Numbers of q2tgs and iGs 
root+IGi  ~DB+IG2 ~DB+- - -^DB+IG. 
where  IGi denotes relevant inflectional fea l ;urcs  o f  
the inflectional groul)s, including the 1)art-ofsl)eech 
for the root or any of the derived forms. 
For exalnlfle , the infinitive, tbtm s(u.~lamla.#'trmak 
given above would be ret)resented with the adjective 
reading of the root sa.rflam mM the tbllowing 4 IGs: 
1. Adj 
2. Verb+Become 
3. Verb+Caus+Pos 
d. Noun+Inf+A3sg+Pnon+Nom 
Table 3 1)rovides a ( ' ,oml )ar i son  of the mnnl)er dis- 
l, in(:t full morl)hosyntactic tags (ignoring the root 
words in this case) mid IGs, generativ(dy 1)ossil)le 
a.nd observed in a (:ortms of 1M words (considering 
a\]\[ ambiguities). One can see thai; the' nmnber ob- 
served till tags ignoring the root words is very high, 
significantly higher than quoted tbr Czech by Ita.ji5 
and Itladk5 (1998). 
4 Stat i s t i ca l  Morpho log ica l  
D isambiguat ion  
Morphoh)gica.1 disambiguation is the prol)lcun of 
tinding the. corresponding s(;qucnce, of morl/hological 
parses (including l;he root), 7' = t~ ~ = l l ,12, . . . , l , , ,  
given a sequence of words 1? = 'w~' = 1u 1 , 'W2, ...~ 'lU n. 
Our at)proach ix to model the (listrilmtion of lilOr- 
phological I)arscs give, n the words, using a hidden 
Markov model, and then to seek the variable 7', I.hat 
maxilnizes .I'(TII'V): 
:/' T P (W)  ) 
= ~.~, , laxP(T)  ? P (WlT) (2 )  
T 
The term P(W)  ix a constant for all choices of T, and 
can thus be ignored when choosing the most prob- 
able 7'. \Ve C~lll further simplify the t)roblem using 
the aSSUlnlil;ion that words arc indc'i)endent of each 
other given their tags. In Tm'ldsh we can use the 
additional simplification that \ ] ' (wi l t i )  = l since l,i 
illcludes tim root fbrm and all morphosyntactic t~a- 
tures to uniquely determine the word f'orm. 2 Since 
2'l'hat is, we assume that  there is no morphological gen- 
eration ambiguity. This  is ahnost  always true. There are 
a tb.w word fin'ms like flelirkcne and horde, which have the 
m o, l , -~/so  z'( ' , , , ; ItT) : P(*,,~I*~) = 1, w(; ~u~ ,v r i , , :  
7 b 
P(WIT) = I \ ]P(w; I t~')  = 1 
i - -  \] 
a, l ld 
~/rgl~.,x P(SqW) = arg?11Hix \ ] ) (T )  (3) 
7' !1' 
N o w 
P(~') = P(t , , l t~' - '  ) x P ( t , - i l t~  '-2) x . . .  
xP(t~lt l)  ? l)(t l)  
Simplifying fin%her with the trigram tag model, we 
get: 
P(T) = P(t,dt,,_.,.,Z,_,) ? 
l ' ( t , ,_ l  It,,-:~, t,,-.,) x . . .  
v(l,:~l~,,~._,) ? e(t~lt. , )  ? z'(~,) 
= flP(l, ilti_e,ti_l) (4) 
i=1 
win, to  w,, d~,ti,,; P(z., I t - , ,  z.0) = I"(z:1), p(z. ,  I*,,, 1,1 ) = 
P(tuItl) to simplif3, the notation. 
If we consider morl)hoh)gi(:al analyses as a se- 
(t11011(:(2 o f  root  ;111(l \ ]Gs ,  each parse t,i can \])e rcp- 
? res(;nted as (I i ,  IGi ,  , . . . ,  IG i , , , ) ,  where ni is the 
nuinber ()t" IG's in the, in, word.:~ This rel)resental.ioil 
changes the l)ro})lem as shown in Figure 1 wher(', the, 
chain rule has been used to factor out the individual 
comt)oncnts. 
This f(irtlttll~ltioll stil l suffers from 1:,t1(! data spat'so- 
ll(}SS 1)roblcll l .  To  allo, v ia tc  l;\]lis~ wc ill~ll,\[c thQ fo lk iw-  
ing siml)lifying assumlitions: 
1. A root wor(1 del)ends only on l;he roots of the 
1)revious words, alld ix indet)cndent of the inflec- 
tional and derivationa\] productions on thein: 
l ' (n l (n -~,  IG~_.,4,..., 1G~_.,,,, .,), 
(r i -a,  I6 ' i _~,~, . . . , /G~_~, , , _ , ) )  = 
P("~l"~-~,n-,) (~') 
The intention here is that this will be useflll 
in tile disambigua?ion of the root word when a 
given form has mori)hological parses with dif- 
fiwent root words. So, tbr instance, for disam- 
biguating the surface, form adam with the fol- 
lowing two parses: 
same morphological parses with the word forms gclirkcn and 
heretic, respectively but are i)ronounced (and writte.n) sl lghlly 
differently. These. m'e rarely seen in written te.xts, and can 
thus l)e. ignored. 
aln our training and W.st data, the nmnbcr  of 1Gs in a word 
form is on the average 1.6, the.refore, ni is usually 1 or 2. We. 
have seen, occasionally, word tbrms with 5 or 6 inflectional 
groups. 
287 
I)(ti\[t1-1) 
z 
I )(t i It i -2,t i-1) 
P(  (ri, IGi,l . . . IGi,n~ )\[(ri-2, IGi-2,1. . . IGi-'2,ni_2 ), (ri-1, IG i - l ,1 .  . . IG i - i  , , i - ,  )) 
P(ri l(r i -2, \ [Gi -2,1. . .  IGi-~,,,_2), (ri-1, IG i -La . . .  IGi-1,,zi_, )) x 
P ( IG i j  \[(ri-2, \[Gi-'e,1...IGi-'e,ni_2), (I'i-1, IOi-l,1 ...\[Gi_l,ni_, ), I'i) x 
. . .  X 
P( IGi,m \[(ri- u, IG i- 2,1 ...\[Gi-2,,~,_=), (r i- l  , IG~-I,1 ...\[Gi-l,,,_~ ), ri, IGi,1, .,., i Gi ,m-l )  
Figure 1: Equation tbr morphological disambiguation 
(a) adam+Noun+A3sg+Pnon+Nom (man) 
(b) ada+Noun+A3sg+Plsg+Nom ( y island) 
in the iloun phrase k'trm~z~ kazakh adam (the 
man with a red sweater), only the roots (along 
with the part -of  speech of the root) of the pre- 
vious words will be used to select the right root. 
Note that tile selection of the root  has some im- 
pact on what the next IG in the word is, but we 
assuine that IGs are determined by the syntac- 
tic context and not by the root. 
2. An interesting observation that we can make 
about q_hrkish is that when a word is consid- 
ere(l as a sequence of IGs, syntactic relations 
are between the last IG of a (dependent) word 
and with some (including the last) IG of the 
(head) word on the right (with nfinor eXCel)- 
tions) (Oflazer, 1999). 
Based on these assumptions and the equation in Fig- 
ure 1, we define three models, all of which are based 
on word level trigrams: 
1. Mode l  1: The presence of IGs in a word only 
depends on the final IGs of the previous words. 
This model ignores any morphotactical relation 
between an IG and any previous IG in the same 
word. 
2. Mode l  2: The presence of IGs in a word only 
depends on the final IGs of the previous words 
and the previous IG in tile same word. In 
this model, we consider morphotactical rela- 
tions and assume that an IG (except the first 
one) in a word form has some dependency on 
tile previous IG. Given that on the average a
word has about 1.6 IGs, IG bigrams should be 
sufficient. 
3. Mode l  3: This is the same as Model 2, except 
that the dependence with the previous IG in a 
word is assumed to be indelmndent of the de- 
pendence on the final IGs of the previous words. 
This allows the formulation to separate the con- 
tributions of the morphotactics and syntax. 
The equations for these models are shown in Figure 
2. We also have built a baseline model based on 
when tags are decomposed into inflectional groups. 
tile standard definition of the tagging problem in 
Equation 2. For the baseline, we have assumed that 
the part of the morphological nalysis after the root 
word is the tag in the conventional sense (and the 
assumption that P(wi\]ti) = 1 no longer holds). 
5 Experiments and Results 
To evaluate our models, we f rst  trained our models 
and then tried to morphologically disambiguate our 
test data. For statistical modeling we used SRILM 
- the SRI language modeling toolkit (Stolcke, 1999). 
Both the test data and training data were 
collected from the web resources of a Turkish 
daily newspN)er. The tokens were analyzed using 
the morphological analyzer, developed by Oflazer 
(1994). The mnbiguity of the training data was 
then reduced fl'om 1.75 to 1.55 using a preprocessor, 
that disambiguates lexicalized and non-lexicalized 
collocations and removes certain obviously impossi- 
ble parses, and trigs to analyze unknown words with 
all unkllown word processor. The training data con- 
sists of the unambiguous sequences (US) consisting 
of about 650K tokens in a corpus of i million tokens, 
and two sets of manually dismnbiguated corpora of 
12,000 and 20,000 tokens. Tile idea of using unam- 
biguous sequences is similar to Brill's work on un- 
supervised learning of disambiguation rules for POS 
tagging (199517). 
The test data consists of 2763 tokens, 935 (~34?/0) 
of which have more than one morphological nalysis 
after preprocessing. The ambiguity of the test data 
was reduced from 1.74 to 1.53 after prct)rocessing. 
As our evaluation metric, we used accuracy de- 
fined as follows: 
# of correct parses 
x 100 accuracy = # o.f tokens 
The accuracy results are given in Table 4. For all 
cases, our models pertbrmed better than baseline tag 
model. As expected, the tag model suffered consid- 
erably from data sparseness. Using all of our train- 
ing data., we achieved an accuracy of 93.95%, wlfich 
is 2.57% points better titan tile tag model trained us- 
ing the same amount of data. Models 2 and 3 gave 
288 
In all three models we assume that  roots and IGs are indel)cn(tenl.  
Mode l  1:  This  model  assumes that  un IG in ~ word depends on the last IGs of the two previous words. 
P(IGi,t:\[(ri-~, 1Gi-2,~ ... Gi-'~,,~_.2), ( r i -1 ,  IG i - l ,~ , . . . ,  IGi-~,,,~_~ ), ri, IGi ,~, . . . ,  IGi,t,-~) 
) I" l 1 ( G~,~.IIG~-~,,,,_~,-/Gi-l,ni_l ) 
Ther(;fore, 
l ) ( t i \ [ t i -~ , t i -1 )  
II i 
k=. l  
(0) 
Mode l  2: The  model  a ssmn(~s that  in addi t ion to th(~ dcl)(mdonci(;s in Model  1, an IG also (lot)ends on tim 
prev ious  IG  in the s~mm word. 
P(1Gi,~.l(ri_.~, IGi-~,~ ...IGi-~,~,i .,), (ri-~ 
Th(,r(',for(;, 
IG i _~, , , . . . , IG i  1,,~__,),ri,lGi,1,...,lGi,~.--1) = 
? \ [ I  ~'(~c~,~.l~(;~--~,,,,_~, ~(;~-~,,,,_,, IG~,k._, ) (7) 
k=l  
Mode l  3: This  is same as Model  2, except the mort)hotact ic  and syntact ic  dt'4)t;ntlenci(;s arc considered to 
bc independent .  
\])(\]Gi,kl(l"i-2,lGi-2,1....\[Gi-2,,,i . ,),(ri- l , . ld/i-. i ,1,...,\]Gi-l,,,~_ 1) , r i , lG i , l , . . . , l -Gi ,k- l )  = 
) ~ 
\])(IGi l.\[IGi..2,,,~ .,, t ' (~ i_ l , , , ,  .l (~r i , t , . - - l )  
r.l?lwr(;forc, 
P(ti\[ti-2,l. i- j) = l)(~' i \ ] r i_ .~,r i_ l )  x \] (1(,,,11. ,. IG,_u), . . . .  , , , IO i - i , , , , _ , )  x 
Ic--I 
In order to simpli\[y the uotation, wc lmve dctlncd the follc, v:ing: 
) ,t ? 
I (IGi,t. llGi,k_l 
= l'(1Gi,~.llGi ._,,,. , . IGi  j , , , ,_,) x P(IGi,~,.) 
I'(IGi,t. IIGi,,~-I) ) 
F(','~l','-,,','o) = IX',',) 
j~(,.:+.(,,,.,) = r,(, .+.,) 
l(m,,~l?(,_, . . . .  , ,mo,,,o) = J (.rc~,~,,) 
, , , = 17 , ,. I (IG2,t\[IGo ...... ICt,,,, ) (IG2,11IG,,,,,) 
1 ( IGi, l \ [ IGi-2, ,~i_~, IGi-1, ,~i_ l ,  IG i ,o )  
P(IGI,I~I IG- I  .... 1, IGo,~o, IGx,k-1) 
P( IG2,t\[IGo,~o, IG1,~1, IG'~,t-l ) 
= P( IG i , l l lG i _ .2 , , , i _ , , IG i_ l , , , i _~)  
= P(IGI,~IlGI,,,_I) 
= I ( IG2,t\[ IG1,,~I, IG2,t_I)  
P(IG2,~\[IGI, , . , IG2,o) = P(IG2,,I IG~ .... ) 
P( fG i , l l \ [a i ,o )  = I ; '  ( ~r (.T~i, 1 ) 
for k = 1,2,.. . , 'hi,  1 = 1,2, ...,n~, and i = 1,2, ..., 'n. 
F igure 2: Equut ions  for Modcls \], 2, and  3. 
289 
Training Data ~Lag Model Model 1 Model 1 Model 2 Model 3 
(Baseline) (Bigram) 
Unambiguous equences (US) 86 .75% 88 .21% 89 .06% 87 .01% 87.19% 
US + 12,000 words 91.34% 93 .52% 93 .34% 92 .d3% 92.72% 
US + 32,000 words 91.34% 93 .95% 93 .56% 92 .87% 92.94% 
Table 4: Accuracy results for difli;rent models. 
similar results, Model 2 suffered from data sparse- 
hess slightly more than Model 3, as expected. 
Surprisingly, the bigram version of Model I (i.e., 
Equation (7), but with bigrams in root and IG mod- 
els), also performs quite well. If we consider just the 
syntactically relevant morl)hological features and ig- 
nore any senlantic features that we mark in ulorphol- 
ogy, the accuracy increases a bit flirt, her. These stem 
ti'om two properties of %lrkish: Most Turkish root 
words also have a proper noun reading, when writ- 
ten with the first letter cai)italized. 4 We (:ount it; as 
an error if the tagger does not get the correct 1)roper 
noun marking, for a proper noun. But this is usua\]ly 
impossil)le especially at the begimfing of sentences 
where the tagger can not exploit caI)italization and 
has to back-off to a lower-order model. In ahnost all 
of such cases, all syntactically relevant morl)hosyn- 
tactic features except the proper noun marking are 
actually correct. Another imi)ortant ease is the pro- 
noun o, which has t)oth personal prollottll (s/he) and 
demonstrative 1)ronoun readings (it) (in addition to 
a syntactically distinct determiner reading (that)). 
Resolution of this is always by semantic cousi(ler- 
atious. When we count as (:orreet m~y errors in- 
volving such selnantic marker cases, we get an ac- 
curacy of 95.07% with the best (',as(; (cf. 93.91% 
of the Model 1). This is slightly 1)etter than the 
precision figures that is reported earlier on morpho- 
logical disambiguation of Turkish using constraint- 
based techniques (Oflazer and T/Jr, 1997). Our re- 
suits are slightly better than the results on Czech 
of Haji~ and Hla(lkg (1998). Megyesi (1999) reports 
a 95.53% accuracy on Hungarian (a language whose 
features relevant o this task are very close to those 
of Turkish), with just the POS tags 1)eing correct. In 
our model this corresponds to the root and the POS 
tag of the last IG 1)eing correct and the accuracy 
of our best model with this assumi)tion is 96.07%. 
When POS tags and subtags are considered, the re- 
ported accuracy for Hungarian is 91.94% while the 
corresl)onding accuracy in our case is 95.07%. We. 
can also note that the results presented by Ezeiza 
et al (1998) for Basque are better titan ours. The 
main reason tbr this is that they eml)loy a much 
more sot)histicated (comt)ared to our t)reprocessor) 
din fact, any word form is a i)otential first name or a last 
na I I10 .  
constraint-grammar based system which imI)roves 
t)recision without reducing recall. Statistical tech- 
niques applied at'~er this disaml)iguation yield a bet- 
ter accuracy compared to starting from a more am- 
1)iguous initial state. 
Since our models assmned that we have indepen- 
dent models for disambiguating the root words, and 
the IOs, we ran experiments to see the contribution 
of the individual models. Table 5 summm'izes the ac- 
curacy results of the individual models for the best 
case (Model 1 in Table 4.) 
Mode l  Accuracy 
IG Model 92.08% 
Root Model 80.36% 
Combined Model 93.95% 
Table 5: The contril)ution of the individual models 
ibr the 1)est case. 
There are quite a number of classes of words which 
are always ambiguous and the t)reprocessing that 
we have employed in creating the unambiguous se- 
quences ca.n never resolve these cases. Tlms sta- 
tistical models trained using only the unambiguous 
sequences as the training data do not handle these 
ambiguous cases at all. This is why the accuracy 
results with only unalnbiguous equences are sig- 
nificantly lower (row 1 in Table 4). The manually 
dismnl)iguated training sets have such mnbiguities 
resolved, so those models perform much better. 
An analysis of the errors indicates the following: 
In 15% of the errors, the last IG of the word is in- 
correct )ut the root and the rest of the IOs, if any, 
are correct. In 3% of the errors, the last IG of the 
word is correct but the either the root or SOlne of 
the previous IGs are incorrect. In 82% of the errors, 
neither the last IG nor any of the previous IOs are 
correct. Along a different dimension, in about 51% 
of the errors, the root and its part-of-speech are not 
determined correctly, while in 84% of the errors, the 
root and the tirst IG combination is not correctly 
determined. 
290 
6 Conclusions 
W(; have 1)resented an ai)l)roach t() slatisti(:al mod- 
eling fl)r agglutinativ(: lmlguages, esi)(;(:ially those 
having l)roducl;ive d(;rivational 1)\]:(ulomena. ()ur ai)- 
l)roa('h essentia.lly involves l)re.al:ing u t) the full m(/r- 
t)hological ana.lysis across (l(~'rivational boundaries 
mid l;reai;ing the (:Onll)On(mt;s ;Is sul)tags, and l;helt 
determining the corre(:l; se(tuenc( ', of tags via sl;al;is- 
tical l;echniques. This, to our knowl(~.(lge, is th('. first 
detailed attempt in statistical modeling of agghttina- 
rive langua.g('~s and (:an cerl;aJnly l)('. al)plied to other 
such lmLguages like ltmlgari:m ;rod Fimfish with 1)re - 
duetive derivational morl)hology. 
7 Acknowledgments 
We tlmnk Andreas Stoleke of SIH STAI{ lml) tbr pro- 
vi(ling us wil;h the \]anguag(; mod('.ling t;oolkit and ti)r 
very helpflfl dis(:ussions on I;his work. IAz Stlriberg 
of SRI STAR Labs, and Bilge Say of Middle East 
Te(:hnical University hfformal;i(:s \]nstitul;e, 1)rovided 
hell)rid insights and (:ommenl;s. 
References 
F, ri(: Brill. 1995a. Trat:slbrmal.i()n-1)as(~d err()r- 
(h:iv(m l('m'ning mid ha.rural angm~gc pro(:(:ssing: 
A case stu(ly in 1)art-ofsl)(~e(:h lagging. Compul, a- 
tional Lin9'u, istics , 21 (4):543 566, l)ceeinl)er. 
Eric Brill. 19951). Unsupervised learning of disam- 
Mguation rule's for l)art of st)(w~(:h (;agging. In l)~v -
eeedings of th, e Third l,Vorksh, op on Very \],a~:(I(: 
Corpora, Carat)ridge, MA, .hme. 
K(umeth W. Church. 1.988. A sto(:hasti(: parts 1)ro- 
p~r}/lll all(\] ;* llOllll phrase  l)ars(}r for lllll(}sl;li(;\[;(}(1 
t( :t .  In l'roeeediv,9s of I,h,e ,5'eemM Co',@re'm:e 
on Applied Nat'm'al Lang'ua9e l)~vcessi'n9, Austin, 
~lZ'xas. 
1)oug Cutting, Julim: Kupiec, Jan Pedersen, and 
l)enelope Sibun. 1992. A practical imri;-of-st)eech 
tagger, llt: Proceedings oJ" l,h,e Third Co'nfercnce 
on Applied Natural Language .l)~vees.sin9, Trenl;o, 
Italy. 
N. Ezeiza, I. Alegria, J. M. Arriola, R. Urizar, and 
I. Aduriz. 1998. Colnbining stochastic and rule- 
I)ased m(;l, hods for dismnl)iguation i agglutina, 
rive bmgu;~ges. In Proceedin9s of the 36 tl' An- 
nual Meeting of th, e Association for Computa- 
tional Linguistics and 17 th International Co~@r- 
enec on Computational Linguistics, pages 379 
384, Montreal, Quebec, Canada, August. 
Jan Haji~ and Barl)ora HladkS. 1998. Tagging 
hfllective languages: Predictiol: of morphologi- 
cal categories fl)r a rich, smtcCured t,~gseI;. In 
Proceedings of COLING/ACL '98, pages 483-490, 
Montreal, Canada, Augusl;. 
Jorge Hankmner, 1989. Lezieal Ilepresentation and 
Process, chapter Mort)hological Parsing and the 
Lexicon. The MIT Press. 
l,?ed Karlsson, Atro Voutilainen, Juha \]teikkilii, 
and Arto Anl:tila. 1995. Constraint Gramm, ar-A 
Language Inde.pcndent System for I>arsi'n9 Unre- 
.~trieted Tezt. Mouton de Gruyter. 
Moshc I~oving;cr, Uzzi Ornan, and Aton Itai. 1995. 
Learning morpho-lexical probabilities fl'om an 
untagged corpus with an application to He- 
tirew. Comp'utational Lin9uisties, 21(3):383 404, 
Sei)t(nnb(~u'. 
Be:{l~ Megyesi. 1999. Iml)roving Brill's POS tag- 
ger f(/r a.n agglutinative language. In Pascal(', 
Fung and Joe Zhou, editors, \])roeeedin9s of th, e 
,\]oint ,5'IGDAT' Confere.'nee on E'm, pirieal Methods 
in Natural La'n9uage I'rocessin 9 and VeUl Lmyle 
Co,7~ora , pages 275-284, College \]?ark, Maryland, 
USA, June. 
Kema.1 ()llazer and Ilker Km'uSz. 1994. Tagging and 
morphological disambiguation f ~lStrkish text. In 
l'roeeedinw of the 4 u' Applied Nal,'m'al Language 
Processing Co~@renee, t)ages 144 149. ACL, ()c- 
Iol)er. 
Iiemal ()flazer m~d GSktmn '.l'{ir. 1996. Coml)in- 
ing hand-t:rafl;(~d rules a.nd unsul)('.rvis(~(l l ;arn- 
ing in (:onstraint-t)as('.d morphological disaml)igua- 
ti(m. In 1,3ri(: Brill and Kemw.th Chur(:h, editors, 
\]"lvceedin9.s of tll.e A(fL-SIGDAT Co'nil'fence on 
\],)mpirieal Meth.ods i'n Nal,'m'al Lan.qv, age P~veess- 
in 9. 
t(cmal Otlazer and GSklmn Tilt. 1997. Mor- 
1)h()logi(:al disaml)iguation l)y vol;ing (:onstra.in(;s. 
In Proeeeding.s of the 35 u' A'n'n'ual Meeting of 
the A,s'sociatio'n for Computational Li'nguistie.s 
(A(H,'OT/lC~ACL'97), Madrid, Sl/ain, July. 
\](.cmal ()tlaz(!r, 1)iM: Z. Hal:kani-Tiir, mid G6khan 
T/Jr. 1999. l)(;sign ior a Turkish :;reebank. In l',v- 
e('edin9 s of Workshop on Ling'uisl, ieally l'nt, e~Tn'el, r',d 
ColTmra , al, I~,A CL '99, Bergen, Norway, .June.. 
Kcmal Otlazer. 1994. Two-level description of Turk- 
ish morphology. Literary and l, inguistie Co'reput- 
ing, 9(2):137 148. 
K(.'m~d ()tlazer. 1.999. Del)endency pa.rsing with an 
extended iinite state al)proach. It: Proceedings of 
the 371,h Ann'ual Meeting of the Association for 
Co'm,irutational Linguistics, College Park, Mary- 
land, .hme. 
Andreas Stolcke. 1999. SRILM--the S1H language 
mo(teling toolkit, h t tp  ://www. speech, s r i .  corn/ 
proj ec ts / s r i lm/ .  
291 
Introduction to the Special Issue on 
Finite-State Methods in NLP 
Lauri  Kar t tunew 
Xerox Research Centre Europe 
Kemal  Of lazer t
Bilkent University 
The idea for this special issue came up during the preparations of the International 
Workshop on Finite-State Methods in Natural Language Processing, that was held at 
Bilkent University in Ankara, Turkey in the summer of 1998. The number of the sub- 
missions had exceeded our initial expectations and we were able to select quite a good 
set of papers from those submitted. Further, the workshop and the preceding tutorial 
by Kenneth Beesley, on finite-state methods, was attended by quite a large number of 
participants. This led us to believe that interest in the theory and applications of finite- 
state machinery was alive and well, and that some of the papers from this workshop 
along with further additional submissions could make a very good special issue for 
this journal. The five papers in this issue are the result of this process. 
The last decade has seen a quite a substantial surge in the use of finite-state meth- 
ods in all aspects of natural anguage applications. Fueled by the theoretical contribu- 
tions of Kaplan and Kay (1994), Mohri's recent contributions on the use of finite-state 
techniques in various NLP problems (Mohri 1996, 1997), the success of finite-state ap- 
proaches especially in computational morphology, for example, Koskenniemi (1983), 
Karttunen (1983), and Karttunen, Kaplan, and Zaenen (1992), and, finally, the avail- 
ability of state-of-the-art tools for building and manipulating large-scale finite-state 
systems (Karttunen 1993; Karttunen and Beesley 1992; Karttunen et al 1996; Mohri, 
Pereira, and Riley 1998; van Noord 1999), recent years have seen many successful 
applications of finite-state approaches in tagging, spell checking, information extrac- 
tion, parsing, speech recognition, and text-to-speech applications. This is a remarkable 
comeback considering that in the dawn of modern linguistics (Chomsky 1957), finite- 
state grammars were dismissed as fundamentally inadequate. As a result, most of the 
work in computational linguistics in the past few decades has been focused on far 
more powerful formalisms. 
Recent publications on finite-state technology include two collections of papers 
(Roche and Schabes 1997; Kornai 1999) with contributions covering a wide range of 
these topics. This special issue, we hope, will add to these contributions. 
The five papers in this collection cover many aspects of finite-state theory and 
applications. The papers Treatment of Epsilon Moves in Subset Construction by van Noord 
and Incremental Construction of Minimal Acyclic Finite-State Automata nd Transducers by 
Daciuk, Watson, Watson, and Mihov, address two fundamental aspects in the con- 
struction of finite-state recognizers. Van Noord presents results for various methods 
for producing a deterministic automaton with no epsilon transitions from a nondeter- 
ministic automaton with a large number of epsilon transitions, especially those result- 
ing from finite-state approximations of context-free and more powerful formalisms. 
Daciuk et al present a new method for constructing minimal, deterministic, acyclic 
6, chemin de Maupertuis, 38240, Meylan, France 
t Bilkent, TR-06533, Ankara, Turkey 
Q 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
finite-state machines from a list of input strings, in a single pass. Practical Experiments 
with Regular Approximations of Context-free Languages by Nederhof, presents evaluations 
of various regular approximation algorithms on actual grammars, providing insights 
into pros and cons of such algorithms. Multitiered Nonlinear Morphology Using Multitape 
Finite Automata: A Case Study on Semitic by Kiraz presents the formalism and implemen- 
tation of an approach for dealing with nonlinear phenomena found in the morphology 
of semitic languages and compares his approach with other systems that have been 
proposed for the same languages. Finally, Learning Dependency Translation Models as Col- 
lections of Finite-State Head Transducers by Alshawi, Bangalore, and Douglas, presents 
an application of the finite-state transducer f amework in a machine translation task 
where weighted finite-state head transducers induced from a corpus of aligned par- 
allel sentences are used to recursively map headwords from the source to the target 
language. 
Our guest editorial board for this issue included Ken Beesley, Eric Brill, Eva 
Ejerhed, George Kiraz, AndrOs Kornai, Mehryar Mohri, Mark-Jan Nederhof, Mar- 
tin Kay, Ron Kaplan, and Atro Voutilainen; we received additional help from many 
other eviewers. Julia Hirschberg, editor-in-chief o Computational Linguistics, helped us 
through all aspects of the selection process, guiding us around many intricate issues. 
We thank the guest editorial board, the additional reviewers, and Julia for their superb 
contributions. We hope you find this special issue well worth the effort. 
References 
Chomsky, Noam. 1957. Syntactic Structures. 
Mouton, The Hague. 
Kaplan, Ronald M. and Martin Kay.1994. 
Regular models of phonological rule 
systems. Computational Linguistics, 
20(3):331-378. 
Karttunen, Lauri. 1983. KIMMO: A general 
morphological processor. Texas Linguistic 
Forum, 22:163-186. 
Karttunen, Lauri. 1993. Finite-state l xicon 
compiler. Technical Report, XEROX Palo 
Alto Research Center, April. Available at 
http://www.xrce.xerox.com/research 
/mltt/fsSoft. 
Karttunen, Lauri and Kenneth. R. Beesley. 
1992. Two-level rule compiler. Technical 
Report, XEROX Palo Alto Research 
Center. Available at 
http://www.xrce.xerox.com/research/ 
mltt/fsSoft. 
Karttunen, Lauri, Jean-Pierre Chanod, 
Gregory Grefenstette, and Anne Schiller. 
1996. Regular expressions for language 
engineering. Natural Language Engineering, 
2(4):305-328. 
Karttunen, Lauri, Ronald M. Kaplan, and 
Annie Zaenen. 1992. Two-level 
morphology with composition. In
COLING-92: Papers Presented tothe 15 th \[sic\] 
International Conference on Computational 
Linguistics, volume 1, pages 141-148, 
Nantes, France. International Committee 
on Computational Linguistics. 
Kornai, AndrOs, editor. 1999. Extented Finite 
State Models of Language. Cambridge 
University Press, Cambridge, England. 
Koskenniemi, Kimmo. 1983. Two-level 
morphology: A general computational 
model for word form recognition and 
production. Publication No: 11, 
Department of General Linguistics, 
University of Helsinki. 
Mohri, Mehryar. 1996. On some applications 
of finite-state automata heory to natural 
language processing. Natural Language 
Engineering, 2:1-20. 
Mohri, Mehryar. 1997. Finite-state 
transducers in language and speech 
processing. Computational Linguistics, 
23(2):269-311, June. 
Mohri, Mehryar, Fernando C.N. Pereira, and 
Michael Riley. 1998. A rational design for 
a weighted finite-state transducer library. 
In Derick Wood and Sheng Yu, editors, 
Automata Implementation. Lecture Notes in 
Computer Science, Number 1436. 
Springer Verlag, pages 144-158. 
Roche, Emmanuel and Yves Schabes, 
editors. 1997. Finite-State Language 
Processing. MIT Press, Cambridge, MA. 
van Noord, Gertjan. 1999. FSA6: Finite state 
automata utilities (version 6) manual. 
Available at http://odur.let.rug.nl/van- 
noord / Fsa /Manual. 
Proceedings of the Second Workshop on Statistical Machine Translation, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploring Different Representational Units in English-to-Turkish Statistical
Machine Translation
Kemal Oflazer?,?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
oflazer@sabanciuniv.edu
I?lknur Durgar El-Kahlout?
? Faculty of Engineering and Natural Sciences
Sabanc? University
Istanbul, Tuzla, 34956, Turkey
ilknurdurgar@su.sabanciuniv.edu
Abstract
We investigate different representational
granularities for sub-lexical representation
in statistical machine translation work from
English to Turkish. We find that (i) rep-
resenting both Turkish and English at the
morpheme-level but with some selective
morpheme-grouping on the Turkish side of
the training data, (ii) augmenting the train-
ing data with ?sentences? comprising only
the content words of the original training
data to bias root word alignment, (iii) re-
ranking the n-best morpheme-sequence out-
puts of the decoder with a word-based lan-
guage model, and (iv) using model iteration
all provide a non-trivial improvement over
a fully word-based baseline. Despite our
very limited training data, we improve from
20.22 BLEU points for our simplest model
to 25.08 BLEU points for an improvement
of 4.86 points or 24% relative.
1 Introduction
Statistical machine translation (SMT) from English-
to-Turkish poses a number of difficulties. Typo-
logically English and Turkish are rather distant lan-
guages: while English has very limited morphology
and rather fixed SVO constituent order, Turkish is an
agglutinative language with a very rich and produc-
tive derivational and inflectional morphology, and a
very flexible (but SOV dominant) constituent order.
Another issue of practical significance is the lack of
large scale parallel text resources, with no substan-
tial improvement expected in the near future.
In this paper, we investigate different represen-
tational granularities for sub-lexical representation
of parallel data for English-to-Turkish phrase-based
SMT and compare them with a word-based base-
line. We also employ two-levels of language mod-
els: the decoder uses a morpheme based LM while it
is generating an n-best list. The n-best lists are then
rescored using a word-based LM.
The paper is structured as follows: We first briefly
discuss issues in SMT and Turkish, and review re-
lated work. We then outline how we exploit mor-
phology, and present results from our baseline and
morphologically segmented models, followed by
some sample outputs. We then describe discuss
model iteration. Finally, we present a comprehen-
sive discussion of our approach and results, and
briefly discuss word-repair ? fixing morphologicaly
malformed words ? and offer a few ideas about the
adaptation of BLEU to morphologically complex
languages like Turkish.
2 Turkish and SMT
Our previous experience with SMT into Turkish
(Durgar El-Kahlout and Oflazer, 2006) hinted that
exploiting sub-lexical structure would be a fruitful
avenue to pursue. This was based on the observation
that a Turkish word would have to align with a com-
plete phrase on the English side, and that sometimes
these phrases on the English side could be discontin-
uous. Figure 1 shows a pair of English and Turkish
sentences that are aligned at the word (top) and mor-
pheme (bottom) levels. At the morpheme level, we
have split the Turkish words into their lexical mor-
phemes while English words with overt morphemes
have been stemmed, and such morphemes have been
marked with a tag.
The productive morphology of Turkish implies
potentially a very large vocabulary size. Thus,
sparseness which is more acute when very modest
25
Figure 1: Word and morpheme alignments for a pair of English-Turkish sentences
parallel resources are available becomes an impor-
tant issue. However, Turkish employs about 30,000
root words and about 150 distinct suffixes, so when
morphemes are used as the units in the parallel texts,
the sparseness problem can be alleviated to some ex-
tent.
Our approach in this paper is to represent Turk-
ish words with their morphological segmentation.
We use lexical morphemes instead of surface mor-
phemes, as most surface distinctions are man-
ifestations of word-internal phenomena such as
vowel harmony, and morphotactics. With lexi-
cal morpheme representation, we can abstract away
such word-internal details and conflate statistics for
seemingly different suffixes, as at this level of repre-
sentation words that look very different on the sur-
face, look very similar.1 For instance, although the
words evinde ?in his house? and masas?nda ?on his
table? look quite different, the lexical morphemes
except for the root are the same: ev+sH+ndA vs.
masa+sH+ndA.
We should however note that although employ-
ing a morpheme based representations dramatically
reduces the vocabulary size on the Turkish side, it
also runs the risk of overloading distortion mecha-
nisms to account for both word-internal morpheme
sequencing and sentence level word ordering.
The segmentation of a word in general is not
unique. We first generate a representation that con-
tains both the lexical segments and the morpho-
logical features encoded for all possible segmenta-
1This is in a sense very similar to the more general problem
of lexical redundancy addressed by Talbot and Osborne (2006)
but our approach does not require the more sophisticated solu-
tion there.
tions and interpretations of the word. For the word
emeli for instance, our morphological analyzer gen-
erates the following with lexical morphemes brack-
eted with (..):
(em)em+Verb+Pos(+yAlH)?DB+Adverb+Since
since (someone) sucked (something)
(emel)emel+Noun+A3sg(+sH)+P3sg+Nom
his/her ambition
(emel)emel+Noun+A3sg+Pnon(+yH)+Acc
ambition (as object of a transitive verb)
These analyses are then disambiguated with a sta-
tistical disambiguator (Yu?ret and Tu?re, 2006) which
operates on the morphological features.2 Finally, the
morphological features are removed from each parse
leaving the lexical morphemes.
Using morphology in SMT has been recently ad-
dressed by researchers translation from or into mor-
phologically rich(er) languages. Niessen and Ney
(2004) have used morphological decomposition to
improve alignment quality. Yang and Kirchhoff
(2006) use phrase-based backoff models to translate
words that are unknown to the decoder, by morpho-
logically decomposing the unknown source word.
They particularly apply their method to translating
from Finnish ? another language with very similar
structural characteristics to Turkish. Corston-Oliver
and Gamon (2004) normalize inflectional morphol-
ogy by stemming the word for German-English
word alignment. Lee (2004) uses a morphologically
analyzed and tagged parallel corpus for Arabic-
English SMT. Zolmann et al (2006) also exploit
morphology in Arabic-English SMT. Popovic and
Ney (2004) investigate improving translation qual-
2This disambiguator has about 94% accuracy.
26
ity from inflected languages by using stems, suffixes
and part-of-speech tags. Goldwater and McClosky
(2005) use morphological analysis on Czech text to
get improvements in Czech to English SMT. Re-
cently, Minkov et al (2007) have used morphologi-
cal postprocessing on the output side using structural
information and information from the source side, to
improve SMT quality.
3 Exploiting Morphology
Our parallel data consists mainly of documents in
international relations and legal documents from
sources such as the Turkish Ministry of Foreign Af-
fairs, EU, etc. We process these as follows: (i) We
segment the words in our Turkish corpus into lex-
ical morphemes whereby differences in the surface
representations of morphemes due to word-internal
phenomena are abstracted out to improve statistics
during alignment.3 (ii) We tag the English side us-
ing TreeTagger (Schmid, 1994), which provides a
lemma and a part-of-speech for each word. We then
remove any tags which do not imply an explicit mor-
pheme or an exceptional form. So for instance, if
the word book gets tagged as +NN, we keep book
in the text, but remove +NN. For books tagged as
+NNS or booking tagged as +VVG, we keep book
and +NNS, and book and +VVG. A word like went is
replaced by go +VVD.4 (iii) From these morpholog-
ically segmented corpora, we also extract for each
sentence, the sequence of roots for open class con-
tent words (nouns, adjectives, adverbs, and verbs).
For Turkish, this corresponds to removing all mor-
phemes and any roots for closed classes. For En-
glish, this corresponds to removing all words tagged
as closed class words along with the tags such as
+VVG above that signal a morpheme on an open
class content word. We use this to augment the train-
ing corpus and bias content word alignments, with
the hope that such roots may get a chance to align
without any additional ?noise? from morphemes and
other function words.
From such processed data, we compile the data
sets whose statistics are listed in Table 1. One can
note that Turkish has many more distinct word forms
(about twice as many as English), but has much less
3So for example, the surface plural morphemes +ler and
+lar get conflated to +lAr and their statistics are hence com-
bined.
4Ideally, it would have been very desirable to actually do
derivational morphological analysis on the English side, so that
one could for example analyze accession into access plus a
marker indicating nominalization.
Turkish Sent. Words (UNK) Uniq. Words
Train 45,709 557,530 52,897
Train-Content 56,609 436,762 13,767
Tune 200 3,258 1,442
Test 649 10,334 (545) 4,355
English
Train 45,709 723,399 26,747
Train-Content 56,609 403,162 19,791
Test 649 13,484 (231) 3,220
Morph- Uniq. Morp./ Uniq. Uniq.
Turkish emes Morp. Word Roots Suff.
Train 1,005,045 15,081 1.80 14,976 105
Tune 6,240 859 1.92 810 49
Test 18,713 2,297 1.81 2,220 77
Table 1: Statistics on Turkish and English training
and test data, and Turkish morphological structure
number of distinct content words than English.5 For
language models in decoding and n-best list rescor-
ing, we use, in addition to the training data, a mono-
lingual Turkish text of about 100,000 sentences (in
a segmented and disambiguated form).
A typical sentence pair in our data looks like
the following, where we have highlighted the con-
tent root words with bold font, coindexed them to
show their alignments and bracketed the ?words?
that evaluation on test would consider.
? T: [kat1 +hl +ma] [ortakl?k2 +sh +nhn]
[uygula3 +hn +ma +sh] [,] [ortakl?k4]
[anlas?ma5 +sh] [c?erc?eve6 +sh +nda]
[izle7 +hn +yacak +dhr] [.]
? E: the implementation3 of the acces-
sion1 partnership2 will be monitor7
+vvn in the framework6 of the
association4 agreement5 .
Note that when the morphemes/tags (starting with
a +) are concatenated, we get the ?word-based?
version of the corpus, since surface words are di-
rectly recoverable from the concatenated represen-
tation. We use this word-based representation also
for word-based language models used for rescoring.
We employ the phrase-based SMT framework
(Koehn et al, 2003), and use the Moses toolkit
(Koehn et al, 2007), and the SRILM language mod-
elling toolkit (Stolcke, 2002), and evaluate our de-
coded translations using the BLEU measure (Pap-
ineni et al, 2002), using a single reference transla-
tion.
5The training set in the first row of 1 was limited to sen-
tences on the Turkish side which had at most 90 tokens (roots
and bound morphemes) in total in order to comply with require-
ments of the GIZA++ alignment tool. However when only the
content words are included, we have more sentences to include
since much less number of sentences violate the length restric-
tion when morphemes/function word are removed.
27
Moses Dec. Parms. BLEU BLEU-c
Default 16.29 16.13
dl = -1, -weight-d = 0.1 20.16 19.77
Table 2: BLEU results for baseline experiments.
BLEU is for the model trained on the training set
BLEU-C is for the model trained on training set augmented with
the content words.
3.1 The Baseline System
As a baseline system, we trained a model using
default Moses parameters (e.g., maximum phrase
length = 7), using the word-based training corpus.
The English test set was decoded with both default
decoder parameters and with the distortion limit (-dl
in Moses) set to unlimited (-1 in Moses) and distor-
tion weight (-weight-d in Moses) set to a very low
value of 0.1 to allow for long distance distortions.6
We also augmented the training set with the con-
tent word data and trained a second baseline model.
Minimum error rate training with the tune set did not
provide any tangible improvements.7 Table 2 shows
the BLEU results for baseline performance. It can
be seen that adding the content word training data
actually hampers the baseline performance.
3.2 Fully Morphologically Segmented Model
We now trained a model using the fully morpho-
logically segmented training corpus with and with-
out content word parallel corpus augmentation. For
decoding, we used a 5-gram morpheme-based lan-
guage model with the hope of capturing local mor-
photactic ordering constraints, and perhaps some
sentence level ordering of words.8 We then decoded
and obtained 1000-best lists. The 1000-best sen-
tences were then converted to ?words? (by concate-
nating the morphemes) and then rescored with a 4-
gram word-based language model with the hope of
enforcing more distant word sequencing constraints.
For this, we followed the following procedure: We
6We arrived at this combination by experimenting with the
decoder to avoid the almost monotonic translation we were get-
ting with the default parameters.
7We ran MERT on the baseline model and the morphologi-
cally segmented models forcing -weight-d to range a very small
around 0.1, but letting the other parameters range in their sug-
gested ranges. Even though the procedure came back claiming
that it achieved a better BLEU score on the tune set, running
the new model on the test set did not show any improvement at
all. This may have been due to the fact that the initial choice
of -weight-d along with -dl set to 1 provides such a drastic
improvement that perturbations in the other parameters do not
have much impact.
8Given that on the average we have almost two bound mor-
phemes per ?word? (for inflecting word classes), a morpheme
5-gram would cover about 2 ?words?.
tried various linear combinations of the word-based
language model and the translation model scores on
the tune corpus, and used the combination that per-
formed best to evaluate the test corpus. We also ex-
perimented with both the default decoding parame-
ters, and the modified parameters used in the base-
line model decoding above.
The results in Table 3 indicate that the default de-
coding parameters used by the Moses decoder pro-
vide a very dismal results ? much below the baseline
scores. We can speculate that as the constituent or-
ders of Turkish and English are very different, (root)
words may have to be scrambled to rather long dis-
tances along with the translations of functions words
and tags on the English side, to morphemes on the
Turkish side. Thus limiting maximum distortion
and penalizing distortions with the default higher
weight, result in these low BLEU results. Allowing
the decoder to consider longer range distortions and
penalizing such distortions much less with the mod-
ified decoding parameters, seem to make an enor-
mous difference in this case, providing close to al-
most 7 BLEU points improvement.9
We can also see that, contrary to the case with
the baseline word-based experiments, using the ad-
ditional content word corpus for training actually
provides a tangible improvement (about 6.2% rel-
ative (w/o rescoring)), most likely due to slightly
better alignments when content words are used.10
Rescoring the 1000-best sentence output with a 4-
gram word-based language model provides an addi-
tional 0.79 BLEU points (about 4% relative) ? from
20.22 to 21.01 ? for the model with the basic train-
ing set, and an additional 0.71 BLEU points (about
3% relative) ? from 21.47 to 22.18? for the model
with the augmented training set. The cumulative im-
provement is 1.96 BLEU points or about 9.4% rela-
tive.
3.3 Selectively Segmented Model
A systematic analysis of the alignment files pro-
duced by GIZA++ for a small subset of the train-
ing sentences showed that certain morphemes on the
9The ?morpheme? BLEU scores are much higher (34.43
on the test set) where we measure BLEU using decoded mor-
phemes as tokens. This is just indicative and but correlates with
word-level BLEU which we report in Table 3, and can be used
to gauge relative improvements to the models.
10We also constructed phrase tables only from the actual
training set (w/o the content word section) after the alignment
phase. The resulting models fared slightly worse though we do
not yet understand why.
28
Moses Dec. Parms. BLEU BLEU-c
Default 13.55 NA
dl = -1, -weight-d = 0.1 20.22 21.47
dl = -1, -weight-d = 0.1
+ word-level LM rescoring 21.01 22.18
Table 3: BLEU results for experiments with fully
morphologically segmented training set
Turkish side were almost consistently never aligned
with anything on the English side: e.g., the com-
pound noun marker morpheme in Turkish (+sh) does
not have a corresponding unit on the English side
since English noun-noun compounds do not carry
any overt markers. Such markers were never aligned
to anything or were aligned almost randomly to to-
kens on the English side. Since we perform deriva-
tional morphological analysis on the Turkish side
but not on the English side, we noted that most ver-
bal nominalizations on the English side were just
aligned to the verb roots on the Turkish side and
the additional markers on the Turkish side indicat-
ing the nominalization and agreement markers etc.,
were mostly unaligned.
For just these cases, we selectively attached such
morphemes (and in the case of verbs, the interven-
ing morphemes) to the root, but otherwise kept other
morphemes, especially any case morphemes, still by
themselves, as they almost often align with preposi-
tions on the English side quite accurately.11
This time, we trained a model on just the content-
word augmented training corpus, with the better per-
forming parameters for the decoder and again did
1000-best rescoring.12 The results for this experi-
ment are shown in Table 4. The resulting BLEU
represents 2.43 points (11% relative) improvement
over the best fully segmented model (and 4.39 points
21.7% compared to the very initial morphologically
segmented model). This is a very encouraging result
that indicates we should perhaps consider a much
more detailed analysis of morpheme alignments to
uncover additional morphemes with similar status.
Table 5 provides additional details on the BLEU
11It should be noted that what to selectively attach to the root
should be considered on a per-language basis; if Turkish were
to be aligned with a language with similar morphological mark-
ers, this perhaps would not have been needed. Again one per-
haps can use methods similar to those suggested by Talbot and
Osborne (2006).
12Decoders for the fully-segmented model and selectively
segmented model use different 5-gram language models, since
the language model corpus should have the same selectively
segmented units as those in the training set. However, the word-
level language models used in rescoring are the same.
Moses Dec. Parms. BLEU-c
dl = -1, -weight-d = 0.1
+ word-level LM rescoring 22.18
(Full Segmentation (from Table 3))
dl = -1, -weight-d = 0.1 23.47
dl = -1, -weight-d = 0.1
+ word-level LM rescoring 24.61
Table 4: BLEU results for experiments with selec-
tively segmented and content-word augmented train-
ing set
Range Sent. BLEU-c
1 - 10 172 44.36
1 - 15 276 34.63
5 - 15 217 33.00
1 - 20 369 28.84
1 - 30 517 27.88
1 - 40 589 24.90
All 649 24.61
Table 5: BLEU Scores for different ranges of
(source) sentence length for the result in Table 4
scores for this model, for different ranges of (En-
glish source) sentence length.
4 Sample Rules and Translations
We have extracted some additional statistics from
the translations produced from English test set. Of
the 10,563 words in the decoded test set, a total of
957 words (9.0 %) were not seen in the training cor-
pus. However, interestingly, of these 957 words, 432
(45%) were actually morphologically well-formed
(some as complex as having 4-5 morphemes!) This
indicates that the phrase-based translation model
is able to synthesize novel complex words.13 In
fact, some phrase table entries seem to capture
morphologically marked subcategorization patterns.
An example is the phrase translation pair
after examine +vvg ?
+acc incele+dhk +abl sonra
which very much resembles a typical structural
transfer rule one would find in a symbolic machine
translation system
PP(after examine +vvg NPeng) ?
PP(NPturk+acc incele+dhk +abl sonra)
in that the accusative marker is tacked to the
translation of the English NP.
Figure 2 shows how segments are translated to
Turkish for a sample sentence. Figure 3 shows the
translations of three sentences from the test data
13Though whether such words are actually correct in their
context is not necessarily clear.
29
c?ocuk [[ child ]]
hak+lar+sh +nhn [[ +nns +pos right ]]
koru+hn+ma+sh [[ protection ]]
+nhn [[ of ]]
tes?vik et+hl+ma+sh [[ promote ]]
+loc [[ +nns in ]] ab [[ eu ]]
ve ulus+lararasi standart +lar
[[ and international standard +nns ]]
+dat uygun [[ line with ]] +dhr . [[ .]]
Figure 2: Phrasal translations selected for a sample
sentence
Inp.: 1 . everyone?s right to life shall be protected by law .
Trans.: 1 . herkesin yas?ama hakk? kanunla korunur.
Lit.: everyone?s living right is protected with law .
Ref.: 1 . herkesin yas?am hakk? yasan?n korumas? alt?ndad?r .
Lit.: everyone?s life right is under the protection of the law.
Inp.: promote protection of children?s rights in line with eu and
international standards .
Trans.: c?ocuk haklar?n?n korunmas?n?n ab ve uluslararas?
standartlara uygun s?ekilde gelis?tirilmesi.
Lit.: develop protection of children?s rights in accordance with
eu and international standards .
Ref.: ab ve uluslararas? standartlar dog?rultusunda c?ocuk
haklar?n?n korunmas?n?n tes?vik edilmesi.
Lit.: in line with eu and international standards pro-
mote/motivate protection of children?s rights .
Inp.: as a key feature of such a strategy, an accession partner-
ship will be drawn up on the basis of previous european council
conclusions.
Trans.: bu stratejinin kilit unsuru bir kat?l?m ortakl?g?? bel-
gesi haz?rlanacak kadar?n temelinde , bir o?nceki avrupa konseyi
sonuc?lar?d?r .
Lit.: as a key feature of this strategy, accession partnership doc-
ument will be prepared ??? based are previous european council
resolutions .
Ref.: bu stratejinin kilit unsuru olarak , daha o?nceki ab zirve
sonuc?lar?na dayan?larak bir kat?l?m ortakl?g?? olus?turulacakt?r.
Lit.: as a key feature of this strategy an accession partnership
based on earlier eu summit resolutions will be formed .
Figure 3: Some sample translations
along with the literal paraphrases of the translation
and the reference versions. The first two are quite
accurate and acceptable translations while the third
clearly has missing and incorrect parts.
5 Model Iteration
We have also experimented with an iterative ap-
proach to use multiple models to see if further im-
provements are possible. This is akin to post-editing
(though definitely not akin to the much more so-
phisticated approach in described in Simard et al
(2007)). We proceeded as follows: We used the
selective segmentation based model above and de-
coded our English training data ETrain and English
test data ETest to obtain T1Train and T1Test re-
Step BLEU
From Table 4 24.61
Iter. 1 24.77
Iter. 2 25.08
Table 6: BLEU results for two model iterations
spectively. We then trained the next model using
T1Train and TTrain, to build a model that hopefully
will improve upon the output of the previous model,
T1Test, to bring it closer to TTest. This model when
applied to T1Train and T1Test produce T2Train and
T2Test respectively.
We have not included the content word corpus
in these experiments, as (i) our few very prelimi-
nary experiments indicated that using a morpheme-
based models in subsequent iterations would per-
form worse than word-based models, and (ii) that for
word-based models adding the content word training
data was not helpful as our baseline experiments in-
dicated. The models were tested by decoding the
output of the previous model for original test data.
For word-based decoding in the additional iterations
we used a 3-gram word-based language model but
reranked the 1000-best outputs using a 4-gram lan-
guage model. Table 6 provides the BLEU results for
these experiments corresponding to two additional
model iterations.
The BLEU result for the second iteration, 25.08,
represents a cumulative 4.86 points (24% relative)
improvement over the initial fully morphologically
segmented model using only the basic training set
and no rescoring.
6 Discussion
Translation into Turkish seems to involve processes
that are somewhat more complex than standard sta-
tistical translation models: sometimes words on the
Turkish side are synthesized from the translations
of two or more (SMT) phrases, and errors in any
translated morpheme or its morphotactic position
render the synthesized word incorrect, even though
the rest of the word can be quite fine. If we just
extract the root words (not just for content words
but all words) in the decoded test set and the ref-
erence set, and compute root word BLEU, we ob-
tain 30.62, [64.6/35.7/23.4/16.3]. The unigram pre-
cision score shows that we are getting almost 65% of
the root words correct. However, the unigram pre-
cision score with full words is about 52% for our
best model. Thus we are missing about 13% of the
words although we seem to be getting their roots
30
correct. With a tool that we have developed, BLEU+
(Tantug? et al, 2007), we have investigated such mis-
matches and have found that most of these are ac-
tually morphologically bogus, in that, although they
have the root word right, the morphemes are either
not the applicable ones or are in a morphotactically
wrong position. These can easily be identified with
the morphological generator that we have. In many
cases, such morphologically bogus words are one
morpheme edit distance away from the correct form
in the reference file. Another avenue that could be
pursued is the use of skip language models (sup-
ported by the SRILM toolkit) so that the content
word order could directly be used by the decoder.14
At this point it is very hard to compare how our re-
sults fare in the grand scheme of things, since there
is not much prior results for English to Turkish SMT.
Koehn (2005) reports on translation from English to
Finnish, another language that is morphologically as
complex as Turkish, with the added complexity of
compounding and stricter agreement between mod-
ifiers and head nouns. A standard phrase-based sys-
tem trained with 941,890 pairs of sentences (about
20 times the data that we have!) gives a BLEU score
of 13.00. However, in this study, nothing specific for
Finnish was employed, and one can certainly em-
ploy techniques similar to presented here to improve
upon this.
6.1 Word Repair
The fact that there are quite many erroneous words
which are actually easy to fix suggests some ideas to
improve unigram precision. One can utilize a mor-
pheme level ?spelling corrector? that operates on
segmented representations, and corrects such forms
to possible morphologically correct words in or-
der to form a lattice which can again be rescored
to select the contextually correct one.15 With the
BLEU+ tool, we have done one experiment that
shows that if we could recover all morphologically
bogus words that are 1 and 2 morpheme edit dis-
tance from the correct form, the word BLEU score
could rise to 29.86, [60.0/34.9/23.3/16.] and 30.48
[63.3/35.6/23.4/16.4] respectively. Obviously, these
are upper-bound oracle scores, as subsequent candi-
date generation and lattice rescoring could make er-
14This was suggested by one of the reviewers.
15It would however perhaps be much better if the decoder
could be augmented with a filter that could be invoked at much
earlier stages of sentence generation to check if certain gener-
ated segments violate hard-constraints (such as morphotactic
constraints) regardless of what the statistics say.
rors, but nevertheless they are very close to the root
word BLEU scores above.
Another path to pursue in repairing words is to
identify morphologically correct words which are
either OOVs in the language model or for which
the language model has low confidence. One can
perhaps identify these using posterior probabilities
(e.g., using techniques in Zens and Ney (2006)) and
generate additional morphologically valid words
that are ?close? and construct a lattice that can be
rescored.
6.2 Some Thoughts on BLEU
BLEU is particularly harsh for Turkish and the mor-
pheme based-approach, because of the all-or-none
nature of token comparison, as discussed above.
There are also cases where words with different
morphemes have very close morphosemantics, con-
vey the relevant meaning and are almost inter-
changeable:
? gel+hyor (geliyor - he is coming) vs. gel+makta
(gelmekte - he is (in a state of) coming) are essentially
the same. On a scale of 0 to 1, one could rate these at
about 0.95 in similarity.
? gel+yacak (gelecek - he will come) vs. gel+yacak+dhr
(gelecektir - he will come) in a sentence final position.
Such pairs could be rated perhaps at 0.90 in similarity.
? gel+dh (geldi - he came (past tense)) vs. gel+mhs (gelmis?
- he came (hearsay past tense)). These essentially mark
past tense but differ in how the speaker relates to the event
and could be rated at perhaps 0.70 similarity.
Note that using stems and their synonyms as used
in METEOR (Banerjee and Lavie, 2005) could also
be considered for word similarity.
Again using the BLEU+ tool and a slightly dif-
ferent formulation of token similarity in BLEU com-
putation, we find that using morphological similar-
ity our best score above, 25.08 BLEU increases to
25.14 BLEU, while using only root word synonymy
and very close hypernymy from Wordnet, gives us
25.45 BLEU. The combination of rules and Wordnet
match gives 25.46 BLEU. Note that these increases
are much less than what can (potentially) be gained
from solving the word-repair problem above.
7 Conclusions
We have presented results from our investigation
into using different granularity of sub-lexical rep-
resentations for English to Turkish SMT. We have
found that employing a language-pair specific rep-
resentation somewhere in between using full word-
forms and fully morphologically segmented repre-
sentations and using content words as additional
31
data provide a significant boost in BLEU scores,
in addition to contributions of word-level rescoring
of 1000-best outputs and model iteration, to give a
BLEU score of 25.08 points with very modest par-
allel text resources. Detailed analysis of the errors
point at a few directions such as word-repair, to im-
prove word accuracy. This also suggests perhaps
hooking into the decoder, a mechanism for imposing
hard constraints (such as morphotactic constraints)
during decoding to avoid generating morphologi-
cally bogus words. Another direction is to introduce
exploitation of limited structures such as bracketed
noun phrases before considering full-fledged syntac-
tic structure.
Acknowledgements
This work was supported by TU?BI?TAK ? The Turk-
ish National Science and Technology Foundation
under project grant 105E020. We thank the anony-
mous reviewer for some very useful comments and
suggestions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved correlation
with human judgments. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?72, Ann
Arbor, Michigan, June.
Simon Corston-Oliver and Michael Gamon. 2004. Normaliz-
ing German and English inflectional morphology to improve
statistical word alignment. In Proceedings of AMTA, pages
48?57.
I?lknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial ex-
plorations in English to Turkish statistical machine transla-
tion. In Proceedings on the Workshop on Statistical Machine
Translation, pages 7?14, New York City, June.
Sharon Goldwater and David McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In Proceedings
of Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Process-
ing, pages 676?683, Vancouver, British Columbia, Canada,
October.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL?07) ? Com-
panion Volume, June.
Philip Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit X, Phuket, Thailand.
Young-Suk Lee. 2004. Morphological analysis for statistical
machine translation. In Proceedings of HLT-NAACL 2004 -
Companion Volume, pages 57?60.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007.
Generating complex morphology for machine translation. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), Prague, Czech Re-
public, June.
Sonja Niessen and Hermann Ney. 2004. Statistical machine
translation with scarce resources using morpho-syntatic in-
formation. Computational Linguistics, 30(2):181?204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, University
of Pennsylvania.
Maja Popovic and Hermann Ney. 2004. Towards the use of
word stems and suffixes for statistical machine translation.
In Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC), pages 1585?1588,
May.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. In Proceedings of International Confer-
ence on New Methods in Language Processing.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007. Statis-
tical phrase-based post-editing. In Proceedings of NAACL,
April.
Andreas Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proceedings of the Intl. Conf. on Spoken
Language Processing.
David Talbot and Miles Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In Proceedings of the 21st
International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational
Linguistics, pages 969?976, Sydney, Australia, July.
Cu?neyd Tantug?, Kemal Oflazer, and I?lknur Durgar El-Kahlout.
2007. BLEU+: a tool for fine-grained BLEU computation.
in preparation.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected languages.
In Proceedings of EACL, pages 41?48.
Deniz Yu?ret and Ferhan Tu?re. 2006. Learning morphological
disambiguation rules for Turkish. In Proceedings of the Hu-
man Language Technology Conference of the NAACL, Main
Conference, pages 328?334, New York City, USA, June.
Richard Zens and Hermann Ney. 2006. N-gram posterior prob-
abilities for statistical machine translation. In Proceedings
on the Workshop on Statistical Machine Translation, pages
72?77, New York City, June. Association for Computational
Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.
2006. Bridging the inflection morphology gap for Arabic
statistical machine translation. In Proceedings of the Human
Language Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pages 201?204, New York City,
USA, June.
32
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207?213,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Human Judgment Corpus and a Metric for Arabic MT Evaluation
Houda Bouamor, Hanan Alshikhabobakr, Behrang Mohit and Kemal Oflazer
Carnegie Mellon University in Qatar
{hbouamor,halshikh,behrang,ko}@cmu.edu
Abstract
We present a human judgments dataset
and an adapted metric for evaluation of
Arabic machine translation. Our medium-
scale dataset is the first of its kind for Ara-
bic with high annotation quality. We use
the dataset to adapt the BLEU score for
Arabic. Our score (AL-BLEU) provides
partial credits for stem and morphologi-
cal matchings of hypothesis and reference
words. We evaluate BLEU, METEOR and
AL-BLEU on our human judgments cor-
pus and show that AL-BLEU has the high-
est correlation with human judgments. We
are releasing the dataset and software to
the research community.
1 Introduction
Evaluation of Machine Translation (MT) contin-
ues to be a challenging research problem. There
is an ongoing effort in finding simple and scal-
able metrics with rich linguistic analysis. A wide
range of metrics have been proposed and evaluated
mostly for European target languages (Callison-
Burch et al., 2011; Mach?a?cek and Bojar, 2013).
These metrics are usually evaluated based on their
correlation with human judgments on a set of MT
output. While there has been growing interest in
building systems for translating into Arabic, the
evaluation of Arabic MT is still an under-studied
problem. Standard MT metrics such as BLEU (Pa-
pineni et al., 2002) or TER (Snover et al., 2006)
have been widely used for evaluating Arabic MT
(El Kholy and Habash, 2012). These metrics use
strict word and phrase matching between the MT
output and reference translations. For morpholog-
ically rich target languages such as Arabic, such
criteria are too simplistic and inadequate. In this
paper, we present: (a) the first human judgment
dataset for Arabic MT (b) the Arabic Language
BLEU (AL-BLEU), an extension of the BLEU
score for Arabic MT evaluation.
Our annotated dataset is composed of the output
of six MT systems with texts from a diverse set of
topics. A group of ten native Arabic speakers an-
notated this corpus with high-levels of inter- and
intra-annotator agreements. Our AL-BLEU met-
ric uses a rich set of morphological, syntactic and
lexical features to extend the evaluation beyond
the exact matching. We conduct different exper-
iments on the newly built dataset and demonstrate
that AL-BLEU shows a stronger average correla-
tion with human judgments than the BLEU and
METEOR scores. Our dataset and our AL-BLEU
metric provide useful testbeds for further research
on Arabic MT and its evaluation.
1
2 Related Work
Several studies on MT evaluation have pointed out
the inadequacy of the standard n-gram based eval-
uation metrics for various languages (Callison-
Burch et al., 2006). For morphologically complex
languages and those without word delimiters, sev-
eral studies have attempted to improve upon them
and suggest more reliable metrics that correlate
better with human judgments (Denoual and Lep-
age, 2005; Homola et al., 2009).
A common approach to the problem of mor-
phologically complex words is to integrate some
linguistic knowledge in the metric. ME-
TEOR (Denkowski and Lavie, 2011), TER-
Plus (Snover et al., 2010) incorporate limited lin-
guistic resources. Popovi?c and Ney (2009) showed
that n-gram based evaluation metrics calculated on
POS sequences correlate well with human judg-
ments, and recently designed and evaluated MPF,
a BLEU-style metric based on morphemes and
POS tags (Popovi?c, 2011). In the same direc-
1
The dataset and the software are available at:
http://nlp.qatar.cmu.edu/resources/
AL-BLEU
207
tion, Chen and Kuhn (2011) proposed AMBER,
a modified version of BLEU incorporating re-
call, extra penalties, and light linguistic knowl-
edge about English morphology. Liu et al. (2010)
propose TESLA-M, a variant of a metric based
on n-gram matching that utilizes light-weight lin-
guistic analysis including lemmatization, POS tag-
ging, and WordNet synonym relations. This met-
ric was then extended to TESLA-B to model
phrase synonyms by exploiting bilingual phrase
tables (Dahlmeier et al., 2011). Tantug et al.
(2008) presented BLEU+, a tool that implements
various extension to BLEU computation to allow
for a better evaluation of the translation perfor-
mance for Turkish.
To the best of our knowledge the only human
judgment dataset for Arabic MT is the small cor-
pus which was used to tune parameters of the ME-
TEOR metric for Arabic (Denkowski and Lavie,
2011). Due to the shortage of Arabic human judg-
ment dataset, studies on the performance of eval-
uation metrics have been constrained and limited.
A relevant effort in this area is the upper-bound es-
timation of BLEU and METEOR scores for Ara-
bic MT output (El Kholy and Habash, 2011). As
part of its extensive functionality, the AMEANA
system provides the upper-bound estimate by an
exhaustive matching of morphological and lexical
features between the hypothesis and the reference
translations. Our use of morphological and lex-
ical features overlaps with the AMEANA frame-
work. However, we extend our partial matching
to a supervised tuning framework for estimating
the value of partial credits. Moreover, our human
judgment dataset allows us to validate our frame-
work with a large-scale gold-standard data.
3 Human judgment dataset
We describe here our procedure for compiling a
diverse Arabic MT dataset and annotating it with
human judgments.
3.1 Data and systems
We annotate a corpus composed of three datasets:
(1) the standard English-Arabic NIST 2005 cor-
pus, commonly used for MT evaluations and com-
posed of news stories. We use the first English
translation as the source and the single corre-
sponding Arabic sentence as the reference. (2) the
MEDAR corpus (Maegaard et al., 2010) that con-
sists of texts related to the climate change with
four Arabic reference translations. We only use
the first reference in this study. (3) a small dataset
of Wikipedia articles (WIKI) to extend our cor-
pus and metric evaluation to topics beyond the
commonly-used news topics. This sub-corpus
consists of our in-house Arabic translations of
seven English Wikipedia articles. The articles are:
Earl Francis Lloyd, Western Europe, Citizenship,
Marcus Garvey, Middle Age translation, Acadian,
NBA. The English articles which do not exist in
the Arabic Wikipedia were manually translated by
a bilingual linguist.
Table 1 gives an overview of these sub-corpora
characteristics.
NIST MEDAR WIKI
# of Documents 100 4 7
# of Sentences 1056 509 327
Table 1: Statistics on the datasets.
We use six state-of-the-art English-to-Arabic
MT systems. These include four research-oriented
phrase-based systems with various morphological
and syntactic features and different Arabic tok-
enization schemes and also two commercial off-
the-shelf systems.
3.2 Annotation of human judgments
In order conduct a manual evaluation of the six
MT systems, we formulated it as a ranking prob-
lem. We adapt the framework used in the WMT
2011 shared task for evaluating MT metrics on
European language pairs (Callison-Burch et al.,
2011) for Arabic MT. We gather human ranking
judgments by asking ten annotators (each native
speaker of Arabic with English as a second lan-
guage) to assess the quality of the English-Arabic
systems, by ranking sentences relative to each
other, from the best to the worst (ties are allowed).
We use the Appraise toolkit (Federmann, 2012)
designed for manual MT evaluation. The tool dis-
plays to the annotator, the source sentence and
translations produced by various MT systems. The
annotators received initial training on the tool and
the task with ten sentences. They were presented
with a brief guideline indicating the purpose of the
task and the main criteria of MT output evaluation.
Each annotator was assigned to 22 ranking
tasks. Each task included ten screens. Each screen
involveed ranking translations of ten sentences. In
total, we collected 22, 000 rankings for 1892 sen-
208
tences (22 tasks?10 screens?10 judges). In each
annotation screen, the annotator was shown the
source-language (English) sentences, as well as
five translations to be ranked. We did not provide
annotators with the reference to avoid any bias in
the annotation process. Each source sentence was
presented with its direct context. Rather than at-
tempting to get a complete ordering over the sys-
tems, we instead relied on random selection and a
reasonably large sample size to make the compar-
isons fair (Callison-Burch et al., 2011).
An example of a source sentence and its five
translations to be ranked is given in Table 2.
3.3 Annotation quality and analysis
In order to ensure the validity of any evaluation
setup, a reasonable of inter- and intra-annotator
agreement rates in ranking should exist. To mea-
sure these agreements, we deliberately reassigned
10% of the tasks to second annotators. More-
over, we ensured that 10% of the screens are re-
displayed to the same annotator within the same
task. This procedure allowed us to collect reliable
quality control measure for our dataset.
?
inter
?
intra
EN-AR 0.57 0.62
Average EN-EU 0.41 0.57
EN-CZ 0.40 0.54
Table 3: Inter- and intra-annotator agreement
scores for our annotation compared to the aver-
age scores for five English to five European lan-
guages and also English-Czech (Callison-Burch et
al., 2011).
We measured head-to-head pairwise agreement
among annotators using Cohen?s kappa (?) (Co-
hen, 1968), defined as follows:
? =
P (A)? P (E)
1? P (E)
where P(A) is the proportion of times annotators
agree and P(E) is the proportion of agreement by
chance.
Table 3 gives average values obtained for inter-
annotator and intra-annotator agreement and com-
pare our results to similar annotation efforts in
WMT-13 on different European languages. Here
we compare against the average agreement for En-
glish to five languages and also from English to
one morphologically rich language (Czech).
4
Based on Landis and Koch (1977) ? interpre-
tation, the ?
inter
value (57%) and also compar-
ing our agreement scores with WMT-13 annota-
tions, we believe that we have reached a reliable
and consistent annotation quality.
4 AL-BLEU
Despite its well-known shortcomings (Callison-
Burch et al., 2006), BLEU continues to be the
de-facto MT evaluation metric. BLEU uses an
exact n-gram matching criterion that is too strict
for a morphologically rich language like Arabic.
The system outputs in Table 2 are examples of
how BLEU heavily penalizes Arabic. Based on
BLEU, the best hypothesis is from Sys
5
which has
three unigram and one bigram exact matches with
the reference. However, the sentence is the 4
th
ranked by annotators. In contrast, the output of
Sys
3
(ranked 1
st
by annotators) has only one ex-
act match, but several partial matches when mor-
phological and lexical information are taken into
consideration.
We propose the Arabic Language BLEU (AL-
BLEU) metric which extends BLEU to deal with
Arabic rich morphology. We extend the matching
to morphological, syntactic and lexical levels with
an optimized partial credit. AL-BLEU starts with
the exact matching of hypothesis tokens against
the reference tokens. Furthermore, it considers the
following: (a) morphological and syntactic feature
matching, (b) stem matching. Based on Arabic lin-
guistic intuition, we check the matching of a sub-
set of 5 morphological features: (i) POS tag, (ii)
gender (iii) number (iv) person (v) definiteness.
We use the MADA package (Habash et al., 2009)
to collect the stem and the morphological features
of the hypothesis and reference translation.
Figure 1 summarizes the function in which we
consider partial matching (m(t
h
, t
r
)) of a hypoth-
esis token (t
h
) and its associated reference token
(t
r
). Starting with the BLEU criterion, we first
check if the hypothesis token is same as the ref-
erence one and provide the full credit for it. If
the exact matching fails, we provide partial credit
for matching at the stem and morphological level.
The value of the partial credits are the sum of
the stem weight (w
s
) and the morphological fea-
4
We compare against the agreement score for annotations
performed by WMT researchers which are higher than the
WMT annotations on Mechanical Turk.
209
Source France plans to attend ASEAN emergency summit.
Reference .

?

KPA??@
	
?AJ


?B@

??

?
P?
	
?k ?
	
Q

?

K A?
	
Q
	
?
frnsaA tEtzm HDwr qmp AaAlaAsyaAn AaAlTaAr}ip
Hypothesis
Systems Rank
Annot
BLEU Rank
BLEU
AL-BLEU Rank
AL?BLEU
Sys
1
2 0.0047 2 0.4816 1

?

KPA??@
	
?AJ


?

B@

??

?
P?
	
?m
?
A?
	
Q
	
? ??
	
m
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162?173,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Recall-Oriented Learning of Named Entities in Arabic Wikipedia
Behrang Mohit? Nathan Schneider? Rishav Bhowmick? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?P.O. Box 24866, Doha, Qatar ?Pittsburgh, PA 15213, USA
{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the problem of NER in Arabic
Wikipedia, a semisupervised domain adap-
tation setting for which we have no labeled
training data in the target domain. To fa-
cilitate evaluation, we obtain annotations
for articles in four topical groups, allow-
ing annotators to identify domain-specific
entity types in addition to standard cate-
gories. Standard supervised learning on
newswire text leads to poor target-domain
recall. We train a sequence model and show
that a simple modification to the online
learner?a loss function encouraging it to
?arrogantly? favor recall over precision?
substantially improves recall and F1. We
then adapt our model with self-training
on unlabeled target-domain data; enforc-
ing the same recall-oriented bias in the self-
training stage yields marginal gains.1
1 Introduction
This paper considers named entity recognition
(NER) in text that is different from most past re-
search on NER. Specifically, we consider Arabic
Wikipedia articles with diverse topics beyond the
commonly-used news domain. These data chal-
lenge past approaches in two ways:
First, Arabic is a morphologically rich lan-
guage (Habash, 2010). Named entities are ref-
erenced using complex syntactic constructions
(cf. English NEs, which are primarily sequences
of proper nouns). The Arabic script suppresses
most vowels, increasing lexical ambiguity, and
lacks capitalization, a key clue for English NER.
Second, much research has focused on the use
of news text for system building and evaluation.
Wikipedia articles are not news, belonging instead
to a wide range of domains that are not clearly
1The annotated dataset and a supplementary document
with additional details of this work can be found at:
http://www.ark.cs.cmu.edu/AQMAR
delineated. One hallmark of this divergence be-
tween Wikipedia and the news domain is a dif-
ference in the distributions of named entities. In-
deed, the classic named entity types (person, or-
ganization, location) may not be the most apt for
articles in other domains (e.g., scientific or social
topics). On the other hand, Wikipedia is a large
dataset, inviting semisupervised approaches.
In this paper, we describe advances on the prob-
lem of NER in Arabic Wikipedia. The techniques
are general and make use of well-understood
building blocks. Our contributions are:
? A small corpus of articles annotated in a new
scheme that provides more freedom for annota-
tors to adapt NE analysis to new domains;
? An ?arrogant? learning approach designed to
boost recall in supervised training as well as
self-training; and
? An empirical evaluation of this technique as ap-
plied to a well-established discriminative NER
model and feature set.
Experiments show consistent gains on the chal-
lenging problem of identifying named entities in
Arabic Wikipedia text.
2 Arabic Wikipedia NE Annotation
Most of the effort in NER has been fo-
cused around a small set of domains and
general-purpose entity classes relevant to those
domains?especially the categories PER(SON),
ORG(ANIZATION), and LOC(ATION) (POL),
which are highly prominent in news text. Ara-
bic is no exception: the publicly available NER
corpora?ACE (Walker et al 2006), ANER (Be-
najiba et al 2008), and OntoNotes (Hovy et al
2006)?all are in the news domain.2 However,
2OntoNotes contains news-related text. ACE includes
some text from blogs. In addition to the POL classes, both
corpora include additional NE classes such as facility, event,
product, vehicle, etc. These entities are infrequent and may
not be comprehensive enough to cover the larger set of pos-
162
History Science Sports Technology
dev: Damascus Atom Rau?l Gonza?les Linux
Imam Hussein Shrine Nuclear power Real Madrid Solaris
test: Crusades Enrico Fermi 2004 Summer Olympics Computer
Islamic Golden Age Light Christiano Ronaldo Computer Software
Islamic History Periodic Table Football Internet
Ibn Tolun Mosque Physics Portugal football team Richard Stallman
Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System
Claudio Filippone (PER) 	??J. ?J

	
? ?K
X???; Linux (SOFTWARE) ??
	
JJ
?; Spanish
League (CHAMPIONSHIPS) ?


	
GAJ.?B@ ?

P?Y?@; proton (PARTICLE) 	??K?QK. ; nuclear
radiation (GENERIC-MISC) ?


??
	
J? @ ?A? ?B@; Real Zaragoza (ORG)

???

Q??? ?AK
P
Table 1: Translated titles
of Arabic Wikipedia arti-
cles in our development
and test sets, and some
NEs with standard and
article-specific classes.
Additionally, Prussia and
Amman were reserved
for training annotators,
and Gulf War for esti-
mating inter-annotator
agreement.
appropriate entity classes will vary widely by do-
main; occurrence rates for entity classes are quite
different in news text vs. Wikipedia, for instance
(Balasuriya et al 2009). This is abundantly
clear in technical and scientific discourse, where
much of the terminology is domain-specific, but it
holds elsewhere. Non-POL entities in the history
domain, for instance, include important events
(wars, famines) and cultural movements (roman-
ticism). Ignoring such domain-critical entities
likely limits the usefulness of the NE analysis.
Recognizing this limitation, some work on
NER has sought to codify more robust invento-
ries of general-purpose entity types (Sekine et al
2002; Weischedel and Brunstein, 2005; Grouin
et al 2011) or to enumerate domain-specific
types (Settles, 2004; Yao et al 2003). Coarse,
general-purpose categories have also been used
for semantic tagging of nouns and verbs (Cia-
ramita and Johnson, 2003). Yet as the number
of classes or domains grows, rigorously docu-
menting and organizing the classes?even for a
single language?requires intensive effort. Ide-
ally, an NER system would refine the traditional
classes (Hovy et al 2011) or identify new entity
classes when they arise in new domains, adapting
to new data. For this reason, we believe it is valu-
able to consider NER systems that identify (but
do not necessarily label) entity mentions, and also
to consider annotation schemes that allow annota-
tors more freedom in defining entity classes.
Our aim in creating an annotated dataset is to
provide a testbed for evaluation of new NER mod-
els. We will use these data as development and
sible NEs (Sekine et al 2002). Nezda et al(2006) anno-
tated and evaluated an Arabic NE corpus with an extended
set of 18 classes (including temporal and numeric entities);
this corpus has not been released publicly.
testing examples, but not as training data. In ?4
we will discuss our semisupervised approach to
learning, which leverages ACE and ANER data
as an annotated training corpus.
2.1 Annotation Strategy
We conducted a small annotation project on Ara-
bic Wikipedia articles. Two college-educated na-
tive Arabic speakers annotated about 3,000 sen-
tences from 31 articles. We identified four top-
ical areas of interest?history, technology, sci-
ence, and sports?and browsed these topics un-
til we had found 31 articles that we deemed sat-
isfactory on the basis of length (at least 1,000
words), cross-lingual linkages (associated articles
in English, German, and Chinese3), and subjec-
tive judgments of quality. The list of these arti-
cles along with sample NEs are presented in ta-
ble 1. These articles were then preprocessed to
extract main article text (eliminating tables, lists,
info-boxes, captions, etc.) for annotation.
Our approach follows ACE guidelines (LDC,
2005) in identifying NE boundaries and choos-
ing POL tags. In addition to this traditional form
of annotation, annotators were encouraged to ar-
ticulate one to three salient, article-specific en-
tity categories per article. For example, names
of particles (e.g., proton) are highly salient in the
Atom article. Annotators were asked to read the
entire article first, and then to decide which non-
traditional classes of entities would be important
in the context of article. In some cases, annotators
reported using heuristics (such as being proper
3These three languages have the most articles on
Wikipedia. Associated articles here are those that have been
manually hyperlinked from the Arabic page as cross-lingual
correspondences. They are not translations, but if the associ-
ations are accurate, these articles should be topically similar
to the Arabic page that links to them.
163
Token position agreement rate 92.6% Cohen?s ?: 0.86
Token agreement rate 88.3% Cohen?s ?: 0.86
Token F1 between annotators 91.0%
Entity boundary match F1 94.0%
Entity category match F1 87.4%
Table 2: Inter-annotator agreement measurements.
nouns or having an English translation which is
conventionally capitalized) to help guide their de-
termination of non-canonical entities and entity
classes. Annotators produced written descriptions
of their classes, including example instances.
This scheme was chosen for its flexibility: in
contrast to a scenario with a fixed ontology, anno-
tators required minimal training beyond the POL
conventions, and did not have to worry about
delineating custom categories precisely enough
that they would extend straightforwardly to other
topics or domains. Of course, we expect inter-
annotator variability to be greater for these open-
ended classification criteria.
2.2 Annotation Quality Evaluation
During annotation, two articles (Prussia and Am-
man) were reserved for training annotators on
the task. Once they were accustomed to anno-
tation, both independently annotated a third ar-
ticle. We used this 4,750-word article (Gulf War,

?J

	
K A

J? @ i. J
?
	
m?'@ H. Qk) to measure inter-annotator
agreement. Table 2 provides scores for token-
level agreement measures and entity-level F1 be-
tween the two annotated versions of the article.4
These measures indicate strong agreement for
locating and categorizing NEs both at the token
and chunk levels. Closer examination of agree-
ment scores shows that PER and MIS classes have
the lowest rates of agreement. That the mis-
cellaneous class, used for infrequent or article-
specific NEs, receives poor agreement is unsur-
prising. The low agreement on the PER class
seems to be due to the use of titles and descriptive
terms in personal names. Despite explicit guide-
lines to exclude the titles, annotators disagreed on
the inclusion of descriptors that disambiguate the
NE (e.g., the father in H.

B@ ??K. h. Qk. : George
Bush, the father).
4The position and boundary measures ignore the distinc-
tions between the POLM classes. To avoid artificial inflation
of the token and token position agreement rates, we exclude
the 81% of tokens tagged by both annotators as not belong-
ing to an entity.
History: Gulf War, Prussia, Damascus, Crusades
WAR CONFLICT ? ? ?
Science: Atom, Periodic table
THEORY ? CHEMICAL ? ?
NAME ROMAN ? PARTICLE ? ?
Sports: Football, Rau?l Gonza?les
SPORT ? CHAMPIONSHIP ?
AWARD ? NAME ROMAN ?
Technology: Computer, Richard Stallman
COMPUTER VARIETY ? SOFTWARE ?
COMPONENT ?
Table 3: Custom NE categories suggested by one or
both annotators for 10 articles. Article titles are trans-
lated from Arabic. ? indicates that both annotators vol-
unteered a category for an article; ? indicates that only
one annotator suggested the category. Annotators were
not given a predetermined set of possible categories;
rather, category matches between annotators were de-
termined by post hoc analysis. NAME ROMAN indi-
cates an NE rendered in Roman characters.
2.3 Validating Category Intuitions
To investigate the variability between annotators
with respect to custom category intuitions, we
asked our two annotators to independently read
10 of the articles in the data (scattered across our
four focus domains) and suggest up to 3 custom
categories for each. We assigned short names to
these suggestions, seen in table 3. In 13 cases,
both annotators suggested a category for an article
that was essentially the same (?); three such cat-
egories spanned multiple articles. In three cases
a category was suggested by only one annotator
(?).5 Thus, we see that our annotators were gen-
erally, but not entirely, consistent with each other
in their creation of custom categories. Further, al-
most all of our article-specific categories corre-
spond to classes in the extended NE taxonomy of
(Sekine et al 2002), which speaks to the reason-
ableness of both sets of categories?and by exten-
sion, our open-ended annotation process.
Our annotation of named entities outside of the
traditional POL classes creates a useful resource
for entity detection and recognition in new do-
mains. Even the ability to detect non-canonical
types of NEs should help applications such as QA
and MT (Toral et al 2005; Babych and Hart-
ley, 2003). Possible avenues for future work
include annotating and projecting non-canonical
5When it came to tagging NEs, one of the two annota-
tors was assigned to each article. Custom categories only
suggested by the other annotator were ignored.
164
NEs from English articles to their Arabic coun-
terparts (Hassan et al 2007), automatically clus-
tering non-canonical types of entities into article-
specific or cross-article classes (cf. Frietag, 2004),
or using non-canonical classes to improve the
(author-specified) article categories in Wikipedia.
Hereafter, we merge all article-specific cate-
gories with the generic MIS category. The pro-
portion of entity mentions that are tagged as MIS,
while varying to a large extent by document, is
a major indication of the gulf between the news
data (<10%) and the Wikipedia data (53% for the
development set, 37% for the test set).
Below, we aim to develop entity detection mod-
els that generalize beyond the traditional POL en-
tities. We do not address here the challenges of
automatically classifying entities or inferring non-
canonical groupings.
3 Data
Table 4 summarizes the various corpora used in
this work.6 Our NE-annotated Wikipedia sub-
corpus, described above, consists of several Ara-
bic Wikipedia articles from four focus domains.7
We do not use these for supervised training data;
they serve only as development and test data. A
larger set of Arabic Wikipedia articles, selected
on the basis of quality heuristics, serves as unla-
beled data for semisupervised learning.
Our out-of-domain labeled NE data is drawn
from the ANER (Benajiba et al 2007) and
ACE-2005 (Walker et al 2006) newswire cor-
pora. Entity types in this data are POL cate-
gories (PER, ORG, LOC) and MIS. Portions of the
ACE corpus were held out as development and
test data; the remainder is used in training.
4 Models
Our starting point for statistical NER is a feature-
based linear model over sequences, trained using
the structured perceptron (Collins, 2002).8
In addition to lexical and morphological9 fea-
6Additional details appear in the supplement.
7We downloaded a snapshot of Arabic Wikipedia
(http://ar.wikipedia.org) on 8/29/2009 and pre-
processed the articles to extract main body text and metadata
using the mwlib package for Python (PediaPress, 2010).
8A more leisurely discussion of the structured percep-
tron and its connection to empirical risk minimization can
be found in the supplementary document.
9We obtain morphological analyses from the MADA tool
(Habash and Rambow, 2005; Roth et al 2008).
Training words NEs
ACE+ANER 212,839 15,796
Wikipedia (unlabeled, 397 docs) 1,110,546 ?
Development
ACE 7,776 638
Wikipedia (4 domains, 8 docs) 21,203 2,073
Test
ACE 7,789 621
Wikipedia (4 domains, 20 docs) 52,650 3,781
Table 4: Number of words (entity mentions) in data sets.
tures known to work well for Arabic NER (Be-
najiba et al 2008; Abdul-Hamid and Darwish,
2010), we incorporate some additional features
enabled by Wikipedia. We do not employ a
gazetteer, as the construction of a broad-domain
gazetteer is a significant undertaking orthogo-
nal to the challenges of a new text domain like
Wikipedia.10 A descriptive list of our features is
available in the supplementary document.
We use a first-order structured perceptron; none
of our features consider more than a pair of con-
secutive BIO labels at a time. The model enforces
the constraint that NE sequences must begin with
B (so the bigram ?O, I? is disallowed).
Training this model on ACE and ANER data
achieves performance comparable to the state of
the art (F1-measure11 above 69%), but fares much
worse on our Wikipedia test set (F1-measure
around 47%); details are given in ?5.
4.1 Recall-Oriented Perceptron
By augmenting the perceptron?s online update
with a cost function term, we can incorporate a
task-dependent notion of error into the objective,
as with structured SVMs (Taskar et al 2004;
Tsochantaridis et al 2005). Let c(y,y?) denote
a measure of error when y is the correct label se-
quence but y? is predicted. For observed sequence
x and feature weights (model parameters) w, the
structured hinge loss is `hinge(x,y,w) =
max
y?
(
w>g(x,y?) + c(y,y?)
)
?w>g(x,y)
(1)
The maximization problem inside the parentheses
is known as cost-augmented decoding. If c fac-
10A gazetteer ought to yield further improvements in line
with previous findings in NER (Ratinov and Roth, 2009).
11Though optimizing NER systems for F1 has been called
into question (Manning, 2006), no alternative metric has
achieved widespread acceptance in the community.
165
tors similarly to the feature function g(x,y), then
we can increase penalties for y that have more
local mistakes. This raises the learner?s aware-
ness about how it will be evaluated. Incorporat-
ing cost-augmented decoding into the perceptron
leads to this decoding step:
y? ? arg max
y?
(
w>g(x,y?) + c(y,y?)
)
, (2)
which amounts to performing stochastic subgradi-
ent ascent on an objective function with the Eq. 1
loss (Ratliff et al 2006).
In this framework, cost functions can be for-
mulated to distinguish between different types of
errors made during training. For a tag sequence
y = ?y1, y2, . . . , yM ?, Gimpel and Smith (2010b)
define word-local cost functions that differently
penalize precision errors (i.e., yi = O ? y?i 6= O
for the ith word), recall errors (yi 6= O? y?i = O),
and entity class/position errors (other cases where
yi 6= y?i). As will be shown below, a key problem
in cross-domain NER is poor recall, so we will
penalize recall errors more severely:
c(y,y?) =
M?
i=1
?
?
?
0 if yi = y?i
? if yi 6= O ? y?i = O
1 otherwise
(3)
for a penalty parameter ? > 1. We call our learner
the ?recall-oriented? perceptron (ROP).
We note that Minkov et al(2006) similarly ex-
plored the recall vs. precision tradeoff in NER.
Their technique was to directly tune the weight
of a single feature?the feature marking O (non-
entity tokens); a lower weight for this feature will
incur a greater penalty for predicting O. Below
we demonstrate that our method, which is less
coarse, is more successful in our setting.12
In our experiments we will show that injecting
?arrogance? into the learner via the recall-oriented
loss function substantially improves recall, espe-
cially for non-POL entities (?5.3).
4.2 Self-Training and Semisupervised
Learning
As we will show experimentally, the differences
between news text and Wikipedia text call for do-
main adaptation. In the case of Arabic Wikipedia,
12The distinction between the techniques is that our cost
function adjusts the whole model in order to perform better
at recall on the training data.
Input: labeled data ??x(n),y(n)??Nn=1; unlabeled
data ?x?(j)?Jj=1; supervised learner L;
number of iterations T ?
Output: w
w? L(??x(n),y(n)??Nn=1)
for t = 1 to T ? do
for j = 1 to J do
y?(j) ? arg maxy w
>g(x?(j),y)
w? L(??x(n),y(n)??Nn=1 ? ??x?
(j), y?(j)??Jj=1)
Algorithm 1: Self-training.
there is no available labeled training data. Yet
the available unlabeled data is vast, so we turn to
semisupervised learning.
Here we adapt self-training, a simple tech-
nique that leverages a supervised learner (like the
perceptron) to perform semisupervised learning
(Clark et al 2003; Mihalcea, 2004; McClosky
et al 2006). In our version, a model is trained
on the labeled data, then used to label the un-
labeled target data. We iterate between training
on the hypothetically-labeled target data plus the
original labeled set, and relabeling the target data;
see Algorithm 1. Before self-training, we remove
sentences hypothesized not to contain any named
entity mentions, which we found avoids further
encouragement of the model toward low recall.
5 Experiments
We investigate two questions in the context of
NER for Arabic Wikipedia:
? Loss function: Does integrating a cost func-
tion into our learning algorithm, as we have
done in the recall-oriented perceptron (?4.1),
improve recall and overall performance on
Wikipedia data?
? Semisupervised learning for domain adap-
tation: Can our models benefit from large
amounts of unlabeled Wikipedia data, in addi-
tion to the (out-of-domain) labeled data? We
experiment with a self-training phase following
the fully supervised learning phase.
We report experiments for the possible combi-
nations of the above ideas. These are summarized
in table 5. Note that the recall-oriented percep-
tron can be used for the supervised learning phase,
for the self-training phase, or both. This leaves us
with the following combinations:
? reg/none (baseline): regular supervised learner.
? ROP/none: recall-oriented supervised learner.
166
Figure 1: Tuning the recall-oriented cost parame-
ter for different learning settings. We optimized
for development set F1, choosing penalty ? = 200
for recall-oriented supervised learning (in the plot,
ROP/*?this is regardless of whether a stage of
self-training will follow); ? = 100 for recall-
oriented self-training following recall-oriented su-
pervised learning (ROP/ROP); and ? = 3200 for
recall-oriented self-training following regular super-
vised learning (reg/ROP).
? reg/reg: standard self-training setup.
? ROP/reg: recall-oriented supervised learner, fol-
lowed by standard self-training.
? reg/ROP: regular supervised model as the initial la-
beler for recall-oriented self-training.
? ROP/ROP (the ?double ROP? condition): recall-
oriented supervised model as the initial labeler for
recall-oriented self-training. Note that the two
ROPs can use different cost parameters.
For evaluating our models we consider the
named entity detection task, i.e., recognizing
which spans of words constitute entities. This
is measured by per-entity precision, recall, and
F1.13 To measure statistical significance of differ-
ences between models we use Gimpel and Smith?s
(2010) implementation of the paired bootstrap re-
sampler of (Koehn, 2004), taking 10,000 samples
for each comparison.
5.1 Baseline
Our baseline is the perceptron, trained on the
POL entity boundaries in the ACE+ANER cor-
pus (reg/none).14 Development data was used to
select the number of iterations (10). We per-
formed 3-fold cross-validation on the ACE data
and found wide variance in the in-domain entity
detection performance of this model:
P R F1
fold 1 70.43 63.08 66.55
fold 2 87.48 81.13 84.18
fold 3 65.09 51.13 57.27
average 74.33 65.11 69.33
(Fold 1 corresponds to the ACE test set described
in table 4.) We also trained the model to perform
POL detection and classification, achieving nearly
identical results in the 3-way cross-validation of
ACE data. From these data we conclude that our
13Only entity spans that exactly match the gold spans are
counted as correct. We calculated these scores with the
conlleval.pl script from the CoNLL 2003 shared task.
14In keeping with prior work, we ignore non-POL cate-
gories for the ACE evaluation.
baseline is on par with the state of the art for Ara-
bic NER on ACE news text (Abdul-Hamid and
Darwish, 2010).15
Here is the performance of the baseline entity
detection model on our 20-article test set:16
P R F1
technology 60.42 20.26 30.35
science 64.96 25.73 36.86
history 63.09 35.58 45.50
sports 71.66 59.94 65.28
overall 66.30 35.91 46.59
Unsurprisingly, performance on Wikipedia data
varies widely across article domains and is much
lower than in-domain performance. Precision
scores fall between 60% and 72% for all domains,
but recall in most cases is far worse. Miscella-
neous class recall, in particular, suffers badly (un-
der 10%)?which partially accounts for the poor
recall in science and technology articles (they
have by far the highest proportion of MIS entities).
5.2 Self-Training
Following Clark et al(2003), we applied self-
training as described in Algorithm 1, with the
perceptron as the supervised learner. Our unla-
beled data consists of 397 Arabic Wikipedia ar-
ticles (1 million words) selected at random from
all articles exceeding a simple length threshold
(1,000 words); see table 4. We used only one iter-
ation (T ? = 1), as experiments on development
data showed no benefit from additional rounds.
Several rounds of self-training hurt performance,
15Abdul-Hamid and Darwish report as their best result a
macroaveraged F1-score of 76. As they do not specify which
data they used for their held-out test set, we cannot perform
a direct comparison. However, our feature set is nearly a
superset of their best feature set, and their result lies well
within the range of results seen in our cross-validation folds.
16Our Wikipedia evaluations use models trained on
POLM entity boundaries in ACE. Per-domain and overall
scores are microaverages across articles.
167
SELF-TRAINING
SUPERVISED none reg ROP
reg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97
ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16
Table 5: Entity detection precision, recall, and F1 for each learning setting, microaveraged across the 24 articles
in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular
vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self-
training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.
baseline
entities words recall
PER 1081 1743 49.95
ORG 286 637 23.92
LOC 1019 1413 61.43
MIS 1395 2176 9.30
overall 3781 5969 35.91
Figure 2: Recall improve-
ment over baseline in the test
set by gold NER category,
counts for those categories in
the data, and recall scores for
our baseline model. Markers
in the plot indicate different
experimental settings corre-
sponding to cells in table 5.
an effect attested in earlier research (Curran et al
2007) and sometimes known as ?semantic drift.?
Results are shown in table 5. We find that stan-
dard self-training (the middle column) has very
little impact on performance.17 Why is this the
case? We venture that poor baseline recall and the
domain variability within Wikipedia are to blame.
5.3 Recall-Oriented Learning
The recall-oriented bias can be introduced in ei-
ther or both of the stages of our semisupervised
learning framework: in the supervised learn-
ing phase, modifying the objective of our base-
line (?5.1); and within the self-training algorithm
(?5.2).18 As noted in ?4.1, the aim of this ap-
proach is to discourage recall errors (false nega-
tives), which are the chief difficulty for the news
text?trained model in the new domain. We se-
lected the value of the false positive penalty for
cost-augmented decoding, ?, using the develop-
ment data (figure 1).
The results in table 5 demonstrate improve-
ments due to the recall-oriented bias in both
stages of learning.19 When used in the super-
17In neither case does regular self-training produce a sig-
nificantly different F1 score than no self-training.
18Standard Viterbi decoding was used to label the data
within the self-training algorithm; note that cost-augmented
decoding only makes sense in learning, not as a prediction
technique, since it deliberately introduces errors relative to a
correct output that must be provided.
19In terms of F1, the worst of the 3 models with the ROP
supervised learner significantly outperforms the best model
with the regular supervised learner (p < 0.005). The im-
vised phase (bottom left cell), the recall gains
are substantial?nearly 9% over the baseline. In-
tegrating this bias within self-training (last col-
umn of the table) produces a more modest im-
provement (less than 3%) relative to the base-
line. In both cases, the improvements to recall
more than compensate for the amount of degra-
dation to precision. This trend is robust: wher-
ever the recall-oriented perceptron is added, we
observe improvements in both recall and F1. Per-
haps surprisingly, these gains are somewhat addi-
tive: using the ROP in both learning phases gives
a small (though not always significant) gain over
alternatives (standard supervised perceptron, no
self-training, or self-training with a standard per-
ceptron). In fact, when the standard supervised
learner is used, recall-oriented self-training suc-
ceeds despite the ineffectiveness of standard self-
training.
Performance breakdowns by (gold) class, fig-
ure 2, and domain, figure 3, further attest to the
robustness of the overall results. The most dra-
matic gains are in miscellaneous class recall?
each form of the recall bias produces an improve-
ment, and using this bias in both the supervised
and self-training phases is clearly most success-
ful for miscellaneous entities. Correspondingly,
the technology and science domains (in which this
class dominates?83% and 61% of mentions, ver-
provements due to self-training are marginal, however: ROP
self-training produces a significant gain only following reg-
ular supervised learning (p < 0.05).
168
Figure 3: Supervised
learner precision vs.
recall as evaluated
on Wikipedia test
data in different
topical domains. The
regular perceptron
(baseline model) is
contrasted with ROP.
No self-training is
applied.
sus 6% and 12% for history and sports, respec-
tively) receive the biggest boost. Still, the gaps
between domains are not entirely removed.
Most improvements relate to the reduction of
false negatives, which fall into three groups:
(a) entities occurring infrequently or partially
in the labeled training data (e.g. uranium);
(b) domain-specific entities sharing lexical or con-
textual features with the POL entities (e.g. Linux,
titanium); and (c) words with Latin characters,
common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.
An alternative?and simpler?approach to
controlling the precision-recall tradeoff is the
Minkov et al(2006) strategy of tuning a single
feature weight subsequent to learning (see ?4.1
above). We performed an oracle experiment to
determine how this compares to recall-oriented
learning in our setting. An oracle trained with
the method of Minkov et aloutperforms the three
models in table 5 that use the regular perceptron
for the supervised phase of learning, but under-
performs the supervised ROP conditions.20
Overall, we find that incorporating the recall-
oriented bias in learning is fruitful for adapting to
Wikipedia because the gains in recall outpace the
damage to precision.
6 Discussion
To our knowledge, this work is the first sugges-
tion that substantively modifying the supervised
learning criterion in a resource-rich domain can
reap benefits in subsequent semisupervised appli-
cation in a new domain. Past work has looked
20Tuning the O feature weight to optimize for F1 on our
test set, we found that oracle precision would be 66.2, recall
would be 39.0, and F1 would be 49.1. The F1 score of our
best model is nearly 3 points higher than the Minkov et al
style oracle, and over 4 points higher than the non-oracle
version where the development set is used for tuning.
at regularization (Chelba and Acero, 2006) and
feature design (Daume? III, 2007); we alter the
loss function. Not surprisingly, the double-ROP
approach harms performance on the original do-
main (on ACE data, we achieve 55.41% F1, far
below the standard perceptron). Yet we observe
that models can be prepared for adaptation even
before a learner is exposed a new domain, sacri-
ficing performance in the original domain.
The recall-oriented bias is not merely encour-
aging the learner to identify entities already seen
in training. As recall increases, so does the num-
ber of new entity types recovered by the model:
of the 2,070 NE types in the test data that were
never seen in training, only 450 were ever found
by the baseline, versus 588 in the reg/ROP condi-
tion, 632 in the ROP/none condition, and 717 in
the double-ROP condition.
We note finally that our method is a simple
extension to the standard structured perceptron;
cost-augmented inference is often no more ex-
pensive than traditional inference, and the algo-
rithmic change is equivalent to adding one addi-
tional feature. Our recall-oriented cost function
is parameterized by a single value, ?; recall is
highly sensitive to the choice of this value (fig-
ure 1 shows how we tuned it on development
data), and thus we anticipate that, in general, such
tuning will be essential to leveraging the benefits
of arrogance.
7 Related Work
Our approach draws on insights from work in
the areas of NER, domain adaptation, NLP with
Wikipedia, and semisupervised learning. As all
are broad areas of research, we highlight only the
most relevant contributions here.
Research in Arabic NER has been focused on
compiling and optimizing the gazetteers and fea-
169
ture sets for standard sequential modeling algo-
rithms (Benajiba et al 2008; Farber et al 2008;
Shaalan and Raza, 2008; Abdul-Hamid and Dar-
wish, 2010). We make use of features identi-
fied in this prior work to construct a strong base-
line system. We are unaware of any Arabic NER
work that has addressed diverse text domains like
Wikipedia. Both the English and Arabic ver-
sions of Wikipedia have been used, however, as
resources in service of traditional NER (Kazama
and Torisawa, 2007; Benajiba et al 2008). Attia
et al(2010) heuristically induce a mapping be-
tween Arabic Wikipedia and Arabic WordNet to
construct Arabic NE gazetteers.
Balasuriya et al(2009) highlight the substan-
tial divergence between entities appearing in En-
glish Wikipedia versus traditional corpora, and
the effects of this divergence on NER perfor-
mance. There is evidence that models trained
on Wikipedia data generalize and perform well
on corpora with narrower domains. Nothman
et al(2009) and Balasuriya et al(2009) show
that NER models trained on both automatically
and manually annotated Wikipedia corpora per-
form reasonably well on news corpora. The re-
verse scenario does not hold for models trained
on news text, a result we also observe in Arabic
NER. Other work has gone beyond the entity de-
tection problem: Florian et al(2004) addition-
ally predict within-document entity coreference
for Arabic, Chinese, and English ACE text, while
Cucerzan (2007) aims to resolve every mention
detected in English Wikipedia pages to a canoni-
cal article devoted to the entity in question.
The domain and topic diversity of NEs has been
studied in the framework of domain adaptation
research. A group of these methods use self-
training and select the most informative features
and training instances to adapt a source domain
learner to the new target domain. Wu et al(2009)
bootstrap the NER leaner with a subset of unla-
beled instances that bridge the source and target
domains. Jiang and Zhai (2006) and Daume? III
(2007) make use of some labeled target-domain
data to tune or augment the features of the source
model towards the target domain. Here, in con-
trast, we use labeled target-domain data only for
tuning and evaluation. Another important dis-
tinction is that domain variation in this prior
work is restricted to topically-related corpora (e.g.
newswire vs. broadcast news), whereas in our
work, major topical differences distinguish the
training and test corpora?and consequently, their
salient NE classes. In these respects our NER
setting is closer to that of Florian et al(2010),
who recognize English entities in noisy text, (Sur-
deanu et al 2011), which concerns information
extraction in a topically distinct target domain,
and (Dalton et al 2011), which addresses English
NER in noisy and topically divergent text.
Self-training (Clark et al 2003; Mihalcea,
2004; McClosky et al 2006) is widely used
in NLP and has inspired related techniques that
learn from automatically labeled data (Liang et
al., 2008; Petrov et al 2010). Our self-training
procedure differs from some others in that we use
all of the automatically labeled examples, rather
than filtering them based on a confidence score.
Cost functions have been used in non-
structured classification settings to penalize cer-
tain types of errors more than others (Chan and
Stolfo, 1998; Domingos, 1999; Kiddon and Brun,
2011). The goal of optimizing our structured NER
model for recall is quite similar to the scenario ex-
plored by Minkov et al(2006), as noted above.
8 Conclusion
We explored the problem of learning an NER
model suited to domains for which no labeled
training data are available. A loss function to en-
courage recall over precision during supervised
discriminative learning substantially improves re-
call and overall entity detection performance, es-
pecially when combined with a semisupervised
learning regimen incorporating the same bias.
We have also developed a small corpus of Ara-
bic Wikipedia articles via a flexible entity an-
notation scheme spanning four topical domains
(publicly available at http://www.ark.cs.
cmu.edu/AQMAR).
Acknowledgments
We thank Mariem Fekih Zguir and Reham Al Tamime
for assistance with annotation, Michael Heilman for
his tagger implementation, and Nizar Habash and col-
leagues for the MADA toolkit. We thank members of
the ARK group at CMU, Hal Daume?, and anonymous
reviewers for their valuable suggestions. This publica-
tion was made possible by grant NPRP-08-485-1-083
from the Qatar National Research Fund (a member of
the Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
170
References
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity
recognition. In Proceedings of the 2010 Named En-
tities Workshop, pages 110?115, Uppsala, Sweden,
July. Association for Computational Linguistics.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010.
An automatically built named entity lexicon for
Arabic. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias, ed-
itors, Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the 7th
International EAMT Workshop on MT and Other
Language Technology Tools, EAMT ?03.
Dominic Balasuriya, Nicky Ringland, Joel Nothman,
Tara Murphy, and James R. Curran. 2009. Named
entity recognition in Wikipedia. In Proceedings
of the 2009 Workshop on The People?s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 10?18, Suntec, Singapore, August.
Association for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jose? Miguel
Bened??Ruiz. 2007. ANERsys: an Arabic named
entity recognition system based on maximum en-
tropy. In Alexander Gelbukh, editor, Proceedings
of CICLing, pages 143?153, Mexico City, Mexio.
Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized
feature sets. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 284?293, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Philip K. Chan and Salvatore J. Stolfo. 1998. To-
ward scalable learning with non-uniform class and
cost distributions: a case study in credit card fraud
detection. In Proceedings of the Fourth Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 164?168, New York City, New
York, USA, August. AAAI Press.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. Computer Speech and Language, 20(4):382?
399.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
168?175.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49?55.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1?
8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual
Exclusion Bootstrapping. In Proceedings of PA-
CLING, 2007.
Jeffrey Dalton, James Allan, and David A. Smith.
2011. Passage retrieval for incorporating global
evidence in sequence labeling. In Proceedings of
the 20th ACM International Conference on Infor-
mation and Knowledge Management (CIKM ?11),
pages 355?364, Glasgow, Scotland, UK, October.
ACM.
Hal Daume? III. 2007. Frustratingly easy domain
adaptation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 256?263, Prague, Czech Republic,
June. Association for Computational Linguistics.
Pedro Domingos. 1999. MetaCost: a general method
for making classifiers cost-sensitive. Proceedings
of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining,
pages 155?164.
Benjamin Farber, Dayne Freitag, Nizar Habash, and
Owen Rambow. 2008. Improving NER in Arabic
using a morphological tagger. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), pages
2509?2514, Marrakech, Morocco, May. European
Language Resources Association (ELRA).
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity detection
and tracking. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, Proceedings of the Hu-
man Language Technology Conference of the North
171
American Chapter of the Association for Compu-
tational Linguistics: HLT-NAACL 2004, page 18,
Boston, Massachusetts, USA, May. Association for
Computational Linguistics.
Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection ro-
bustness to noisy input. In Proceedings of EMNLP
2010, pages 335?345, Cambridge, MA, October.
Association for Computational Linguistics.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 262?269, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2010a. Softmax-
margin CRFs: Training log-linear models with loss
functions. In Proceedings of the Human Language
Technologies Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733?736, Los Angeles, California,
USA, June.
Kevin Gimpel and Noah A. Smith. 2010b.
Softmax-margin training for structured log-
linear models. Technical Report CMU-LTI-
10-008, Carnegie Mellon University. http:
//www.lti.cs.cmu.edu/research/
reports/2010/cmulti10008.pdf.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: from guidelines to evaluation,
an overview. In Proceedings of the 5th Linguis-
tic Annotation Workshop, pages 92?100, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
573?580, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan and Claypool Pub-
lishers.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by ex-
ploiting comparable and parallel corpora. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing (RANLP ?07),
Borovets, Bulgaria.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the Human Language Technology Conference of
the NAACL (HLT-NAACL), pages 57?60, New York
City, USA, June. Association for Computational
Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Peas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466?1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2006. Exploit-
ing domain structure for named entity recognition.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL (HLT-NAACL), pages
74?81, New York City, USA, June. Association for
Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007.
Exploiting Wikipedia as external knowledge for
named entity recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698?707, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That?s what
she said: double entendre identification. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 89?94, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction)
Arabic annotation guidelines for entities, version
5.3.3. Linguistic Data Consortium, Philadelphia.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Con-
ference on Machine Learning (ICML), pages 592?
599, Helsinki, Finland.
Chris Manning. 2006. Doing named entity recogni-
tion? Don?t optimize for F1. http://nlpers.
blogspot.com/2006/08/doing-named-
entity-recognition-dont.html.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In HLT-NAACL
2004 Workshop: Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004),
Boston, Massachusetts, USA.
172
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: adjusting the recall-precision trade-off
for entity extraction. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 93?96,
New York City, USA, June. Association for Com-
putational Linguistics.
Luke Nezda, Andrew Hickl, John Lehmann, and Sar-
mad Fayyaz. 2006. What in the world is a Shahab?
Wide coverage named entity recognition for Arabic.
In Proccedings of LREC, pages 41?46.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2009),
pages 612?620, Athens, Greece, March. Associa-
tion for Computational Linguistics.
PediaPress. 2010. mwlib. http://code.
pediapress.com/wiki/wiki/mwlib.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705?713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. Subgradient methods for maxi-
mum margin structured learning. In ICML Work-
shop on Learning in Structured Output Spaces,
Pittsburgh, Pennsylvania, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona
Diab, and Cynthia Rudin. 2008. Arabic morpho-
logical tagging, diacritization, and lemmatization
using lexeme models and feature ranking. In Pro-
ceedings of ACL-08: HLT, pages 117?120, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Pro-
ceedings of LREC.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Nigel Collier, Patrick Ruch, and Adeline
Nazarenko, editors, COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 107?110, Geneva, Switzerland, Au-
gust. COLING.
Khaled Shaalan and Hafsa Raza. 2008. Arabic
named entity recognition from diverse text types. In
Advances in Natural Language Processing, pages
440?451. Springer.
Mihai Surdeanu, David McClosky, Mason R. Smith,
Andrey Gusev, and Christopher D. Manning. 2011.
Customizing an information extraction system to
a new domain. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004. Max-margin Markov networks. In Sebastian
Thrun, Lawrence Saul, and Bernhard Scho?lkopf,
editors, Advances in Neural Information Processing
Systems 16. MIT Press.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question an-
swering using named entity recognition. Natu-
ral Language Processing and Information Systems,
3513/2005:181?191.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484, September.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multi-
lingual training corpus. LDC2006T06, Linguistic
Data Consortium, Philadelphia.
Ralph Weischedel and Ada Brunstein. 2005.
BBN pronoun coreference and entity type cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523?1532, Singapore, August.
Association for Computational Linguistics.
Tianfang Yao, Wei Ding, and Gregor Erbach. 2003.
CHINERS: a Chinese named entity recognition sys-
tem for the sports domain. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing, pages 55?62, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
173
Erratum
In Section 5 of the article ?Dependency Parsing of Turkish? by Gu?ls?en Eryig?it, Joakim
Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357?389), some abbreviations
were misinterpreted during the copyediting process.
The third sentence of Section 5.2 should be as follows: ?We use an unlexicalized
feature model where the parser uses only the minor part-of-speech category (as POS)
and dependency type of tokens (as DEP) and compare the results with the probabilistic
parser.?
The first sentence of the second paragraph of Section 5.2.1 should start as follows:
?We take the minor part-of-speech category. . . .?
The ?POS? abbreviation used on page 20 should be read as ?minor part-of-
speech,? and the ?POS? abbreviations on pages 21, 26, 27, and 28 should be read as
?part-of-speech.?
? 2008 Association for Computational Linguistics
This article has been cited by:
Proceedings of NAACL-HLT 2013, pages 32?40,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Simultaneous Word-Morpheme Alignment for
Statistical Machine Translation
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
Current word alignment models for statisti-
cal machine translation do not address mor-
phology beyond merely splitting words. We
present a two-level alignment model that dis-
tinguishes between words and morphemes, in
which we embed an IBM Model 1 inside an
HMM based word alignment model. The
model jointly induces word and morpheme
alignments using an EM algorithm. We eval-
uated our model on Turkish-English parallel
data. We obtained significant improvement of
BLEU scores over IBM Model 4. Our results
indicate that utilizing information from mor-
phology improves the quality of word align-
ments.
1 Introduction
All current state-of-the-art approaches to SMT rely
on an automatically word-aligned corpus. However,
current alignment models do not take into account
the morpheme, the smallest unit of syntax, beyond
merely splitting words. Since morphology has not
been addressed explicitly in word alignment models,
researchers have resorted to tweaking SMT systems
by manipulating the content and the form of what
should be the so-called ?word?.
Since the word is the smallest unit of translation
from the standpoint of word alignment models, the
central focus of research on translating morphologi-
cally rich languages has been decomposition of mor-
phologically complex words into tokens of the right
granularity and representation for machine transla-
tion. Chung and Gildea (2009) and Naradowsky and
Toutanova (2011) use unsupervised methods to find
word segmentations that create a one-to-one map-
ping of words in both languages. Al-Onaizan et al
(1999), C?mejrek et al (2003), and Goldwater and
McClosky (2005) manipulate morphologically rich
languages by selective lemmatization. Lee (2004)
attempts to learn the probability of deleting or merg-
ing Arabic morphemes for Arabic to English trans-
lation. Niessen and Ney (2000) split German com-
pound nouns, and merge German phrases that cor-
respond to a single English word. Alternatively,
Yeniterzi and Oflazer (2010) manipulate words of
the morphologically poor side of a language pair
to mimic having a morphological structure similar
to the richer side via exploiting syntactic structure,
in order to improve the similarity of words on both
sides of the translation.
We present an alignment model that assumes in-
ternal structure for words, and we can legitimately
talk about words and their morphemes in line with
the linguistic conception of these terms. Our model
avoids the problem of collapsing words and mor-
phemes into one single category. We adopt a two-
level representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment in the scope of a given word
alignment. The model jointly induces word and
morpheme alignments using an EM algorithm.
We develop our model in two stages. Our initial
model is analogous to IBM Model 1: the first level
is a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. In this manner,
we embed one IBM Model 1 in the scope of another
IBM Model 1. At the second stage, by introducing
distortion probabilities at the word level, we develop
an HMM extension of the initial model.
We evaluated the performance of our model on the
32
Turkish-English pair both on hand-aligned data and
by running end-to-end machine translation experi-
ments. To evaluate our results, we created gold word
alignments for 75 Turkish-English sentences. We
obtain significant improvement of AER and BLEU
scores over IBM Model 4. Section 2.1 introduces
the concept of morpheme alignment in terms of its
relation to word alignment. Section 2.2 presents
the derivation of the EM algorithm and Section 3
presents the results of our experiments.
2 Two-level Alignment Model (TAM)
2.1 Morpheme Alignment
Following the standard alignment models of Brown
et al (1993), we assume one-to-many alignment for
both words and morphemes. A word alignment aw
(or only a) is a function mapping a set of word po-
sitions in a source language sentence to a set of
word positions in a target language sentence. A mor-
pheme alignment am is a function mapping a set of
morpheme positions in a source language sentence
to a set of morpheme positions in a target language
sentence. A morpheme position is a pair of integers
(j, k), which defines a word position j and a relative
morpheme position k in the word at position j. The
alignments below are depicted in Figures 1 and 2.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
Figure 1 shows a word alignment between two sen-
tences. Figure 2 shows the morpheme alignment be-
tween same sentences. We assume that all unaligned
morphemes in a sentence map to a special null mor-
pheme.
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the fol-
lowing conditions: If the morpheme alignment am
maps a morpheme of e to a morpheme of f , then the
word alignment aw maps e to f . If the word align-
ment aw maps e to f , then the morpheme alignment
am maps at least one morpheme of e to a morpheme
of f . If the word alignment aw maps e to null, then
all of its morphemes are mapped to null. In sum, a
morpheme alignment am and a word alignment aw
are compatible if and only if:
? j, k,m, n ? N+, ? s, t ? N+
[am(j, k) = (m,n)? aw(j) = m] ?
[aw(j) = m? am(j, s) = (m, t)] ?
[aw(j) = null? am(j, k) = null] (1)
Please note that, according to this definition of com-
patibility, ?am(j, k) = null? does not necessarily im-
ply ?aw(j) = null?.
A word alignment induces a set of compati-
ble morpheme alignments. However, a morpheme
alignment induces a unique word alignment. There-
fore, if a morpheme alignment am and a word align-
ment aw are compatible, then the word alignment is
aw is recoverable from the morpheme alignment am.
The two-level alignment model (TAM), like
IBM Model 1, defines an alignment between words
of a sentence pair. In addition, it defines a mor-
pheme alignment between the morphemes of a sen-
tence pair.
The problem domain of IBM Model 1 is defined
over alignments between words, which is depicted
as the gray box in Figure 1. In Figure 2, the smaller
boxes embedded inside the main box depict the new
problem domain of TAM. Given the word align-
ments in Figure 1, we are presented with a new
alignment problem defined over their morphemes.
The new alignment problem is constrained by the
given word alignment. We, like IBM Model 1, adopt
a bag-of-morphemes approach to this new problem.
We thus embed one IBM Model 1 into the scope of
another IBM Model 1, and formulate a second-order
interpretation of IBM Model 1.
TAM, like IBM Model 1, assumes that words and
morphemes are translated independently of their
context. The units of translation are both words and
morphemes. Both the word alignment aw and the
morpheme alignment am are hidden variables that
need to be learned from the data using the EM algo-
rithm.
In IBM Model 1, p(e|f), the probability of trans-
lating the sentence f into e with any alignment is
computed by summing over all possible word align-
ments:
p(e|f) =
?
a
p(a, e|f)
33
Figure 1: Word alignment Figure 2: Morpheme alignment
In TAM, the probability of translating the sentence
f into e with any alignment is computed by sum-
ming over all possible word alignments and all pos-
sible morpheme alignments that are compatible with
a given word alignment aw:
p(e|f) =
?
aw
p(aw, e|f)
?
am
p(am, e|aw, f) (2)
where am stands for a morpheme alignment. Since
the morpheme alignment am is in the scope of a
given word alignment aw, am is constrained by aw.
In IBM Model 1, we compute the probability of
translating the sentence f into e by summing over
all possible word alignments between the words of f
and e:
p(e|f) = R(e, f)
|e|?
j=1
|f |?
i=0
t(ej |fi) (3)
where t(ej | fi) is the word translation probability
of ej given fi. R(e, f) substitutes
P (le|lf )
(lf+1)
le for easy
readability.1
In TAM, the probability of translating the sen-
tence f into e is computed as follows:
Word
R(e, f)
|e|?
j=1
|f |?
i=0
(
t(ej |fi)
R(ej , fi)
|ej |?
k=1
|fi|?
n=0
t(ekj |f
n
i )
)
Morpheme
where fni is the n
th morpheme of the word at po-
sition i. The right part of this equation, the con-
tribution of morpheme translation probabilities, is
1le = |e| is the number of words in sentence e and lf = |f |.
in the scope of the left part. In the right part, we
compute the probability of translating the word fi
into the word ej by summing over all possible mor-
pheme alignments between the morphemes of ej and
fi. R(ej , fi) is equivalent to R(e, f) except for the
fact that its domain is not the set of sentences but
the set of words. The length of words ej and fi in
R(ej , fi) are the number of morphemes of ej and fi.
The left part, the contribution of word transla-
tion probabilities alone, equals Eqn. 3. Therefore,
canceling the contribution of morpheme translation
probabilities reduces TAM to IBM Model 1. In
our experiments, we call this reduced version of
TAM ?word-only? (IBM). TAM with the contribu-
tion of both word and morpheme translation proba-
bilities, as the equation above, is called ?word-and-
morpheme?. Finally, we also cancel out the con-
tribution of word translation probabilities, which is
called ?morpheme-only?. In the ?morpheme-only?
version of TAM, t(ej |fi) equals 1. Bellow is the
equation of p(e|f) in the morpheme-only model.
p(e|f) =
R(e, f)
|e|?
j=1
|f |?
i=0
|ej |?
k=1
|fi|?
n=0
R(ej , fi)t(e
k
j |f
n
i ) (4)
Please note that, although this version of the two-
level alignment model does not use word translation
probabilities, it is also a word-aware model, as mor-
pheme alignments are restricted to correspond to a
valid word alignment according to Eqn. 1. When
presented with words that exhibit no morphology,
the morpheme-only version of TAM is equivalent to
IBM Model 1, as every single-morpheme word is it-
self a morpheme.
Deficiency and Non-Deficiency of TAM We
present two versions of TAM, the word-and-
34
morpheme and the morpheme-only versions. The
word-and-morpheme version of the model is defi-
cient whereas the morpheme-only model is not.
The word-and-morpheme version is deficient, be-
cause some probability is allocated to cases where
the morphemes generated by the morpheme model
do not match the words generated by the word
model. Moreover, although most languages exhibit
morphology to some extent, they can be input to the
algorithm without morpheme boundaries. This also
causes deficiency in the word-and-morpheme ver-
sion, as single morpheme words are generated twice,
as a word and as a morpheme.
Nevertheless, we observed that the deficient ver-
sion of TAM can perform as good as the non-
deficient version of TAM, and sometimes performs
better. This is not surprising, as deficient word align-
ment models such as IBM Model 3 or discriminative
word alignment models work well in practice.
Goldwater and McClosky (2005) proposed a mor-
pheme aware word alignment model for language
pairs in which the source language words corre-
spond to only one morpheme. Their word alignment
model is:
P (e|f) =
K?
k=0
P (ek|f)
where ek is the kth morpheme of the word e. The
morpheme-only version of our model is a general-
ization of this model. However, there are major dif-
ferences in their and our implementation and exper-
imentation. Their model assumes a fixed number of
possible morphemes associated with any stem in the
language, and if the morpheme ek is not present, it
is assigned a null value.
The null word on the source side is also a null
morpheme, since every single morpheme word is it-
self a morpheme. In TAM, the null word is the null
morpheme that all unaligned morphemes align to.
2.2 Second-Order Counts
In TAM, we collect counts for both word translations
and morpheme translations. Unlike IBM Model 1,
R(e, f) = P (le|lf )
(lf+1)
le does not cancel out in the counts
of TAM. To compute the conditional probability
P (le|lf ), we assume that the length of word e (the
number of morphemes of word e) varies according
to a Poisson distribution with a mean that is linear
with length of the word f .
P (le|lf ) = FPoisson(le, r ? lf )
=
exp(?r ? lf )(r ? lf )le
le!
FPoisson(le, r ? lf ) expresses the probability that there
are le morphemes in e if the expected number of
morphemes in e is r ? lf , where r =
E[le]
E[lf ]
is the rate
parameter. Since lf is undefined for null words, we
omit R(e, f) for null words.
We introduce T (e|f), the translation probability
of e given f with all possible morpheme alignments,
as it will occur frequently in the counts of TAM:
T (e|f) = t(e|f)R(e, f)
|e|?
k=1
|f |?
n=0
t(ek|fn)
The role of T (e|f) in TAM is very similar to the
role of t(e|f) in IBM Model 1. In finding the Viterbi
alignments, we do not take max over the values in
the summation in T (e|f).
2.2.1 Word Counts
Similar to IBM Model 1, we collect counts for
word translations over all possible alignments,
weighted by their probability. In Eqn. 5, the count
function collects evidence from a sentence pair
(e, f) as follows: For all words ej of the sentence e
and for all word alignments aw(j), we collect counts
for a particular input word f and an output word e
iff ej = e and faw(j) = f .
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
T (e|f)
|f |?
i=0
T (e|fi)
(5)
2.2.2 Morpheme Counts
As for morpheme translations, we collect counts
over all possible word and morpheme alignments,
weighted by their probability. The morpheme count
function below collects evidence from a word pair
(e, f) in a sentence pair (e, f) as follows: For all
words ej of the sentence e and for all word align-
ments aw(j), for all morphemes ekj of the word ej
and for all morpheme alignments am(j, k), we col-
lect counts for a particular input morpheme g and an
35
output morpheme h iff ej = e and faw(j) = f and
h = ekj and g = fam(j,k).
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
T (e|f)
|f |?
i=0
T (e|fi)
t(h|g)
|f |?
i=1
t(h|f i)
The left part of the morpheme count function is the
same as the word-counts in Eqn. 5. Since it does not
contain h or g, it needs to be computed only once for
each word. The right part of the equation is familiar
from the IBM Model 1 counts.
2.3 HMM Extension
We implemented TAM with the HMM extension
(Vogel et al, 1996) at the word level. We redefine
p(e|f) as follows:
R(e, f)
?
aw
|e|?
j=1
(
p(s(j ) |C (faw (j?1 ))) t(ej |faw(j))
R(ej , faw(j))
?
am
|ej |?
k=1
t(ekj |fam(j,k))
)
where the distortion probability depends on the rel-
ative jump width s(j) = aw(j ? 1) ? aw(j),
as opposed to absolute positions. The distortion
probability is conditioned on class of the previous
aligned word C (faw(j?1)). We used the mkcls
tool in GIZA (Och and Ney, 2003) to learn the word
classes.
We formulated the HMM extension of TAM only
at the word level. Nevertheless, the morpheme-only
version of TAM also has an HMM extension, as it
is also a word-aware model. To obtain the HMM
extension of the morpheme-only version, substitute
t(ej |faw(j)) with 1 in the equation above.
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions. We
learn the probabilities of jumping to a null position
from the data. To compute the jump probability from
a null position, we keep track of the nearest previous
source word that does not align to null, and use the
position of the previous non-null word to calculate
the jump width. For this reason, we use a total of
2lf ? 1 words for the HMM model, the positions
> lf stand for null positions between the words of f
(Och and Ney, 2003). We do not allow null to null
jumps. In sum, we enforce the following constraints:
P (i+ lf + 1|i
?) = p(null|i?)
P (i+ lf + 1|i
? + lf + 1) = 0
P (i|i? + lf + 1) = p(i|i
?)
In the HMM extension of TAM, we perform
forward-backward training using the word counts in
Eqn. 5 as the emission probabilities. We calculate
the posterior word translation probabilities for each
ej and fi such that 1 ? j ? le and 1 ? i ? 2lf ? 1
as follows:
?j(i) =
?j(i)?j(i)
2lf?1?
m=1
?j(m)?j(m)
where ? is the forward and ? is the backward prob-
abilities of the HMM. The HMM word counts, in
turn, are the posterior word translation probabilities
obtained from the forward-backward training:
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?j(aw(j))
Likewise, we use the posterior probabilities in HMM
morpheme counts:
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
?j(aw(j))
t(h|g)
|f |?
i=1
t(h|f i)
The complexity of the HMM extension of TAM is
O(n3m2), where n is the number of words, and m
is the number of morphemes per word.
2.4 Variational Bayes
Moore (2004) showed that the EM algorithm is par-
ticularly susceptible to overfitting in the case of rare
words when training IBM Model 1. In order to pre-
vent overfitting, we use the Variational Bayes ex-
tension of the EM algorithm (Beal, 2003). This
36
(a) Kas?m 1996?da, Tu?rk makamlar?, I?c?is?leri Bakanl?g?? bu?nyesinde bir kay?p kis?ileri arama birimi olus?turdu.
(b) Kas?m+Noun 1996+Num?Loc ,+Punc Tu?rk+Noun makam+Noun?A3pl?P3sg ,+Punc I?c?is?i+Noun?A3pl?
P3sg Bakanl?k+Noun?P3sg bu?nye+Noun?P3sg?Loc bir+Det kay?p+Adj kis?i+Noun?A3pl?Acc ara+Verb?
Inf2 birim+Noun?P3sg olus?+Verb?Caus?Past .+Punc
(c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of the
Interior.
(d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN?ity+N|N.?NNS set+VB?VBD up+RP
a+DT miss+VB?VBG+JJ person+NN?NNS search+NN unit+NN within+IN the+DT minister+NN?
y+N|N. of+IN the+DT interior+NN .+.
(e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ
persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+.
Figure 3: Turkish-English data examples
amounts to a small change to the M step of the orig-
inal EM algorithm. We introduce Dirichlet priors ?
to perform an inexact normalization by applying the
function f(v) = exp(?(v)) to the expected counts
collected in the E step, where ? is the digamma
function (Johnson, 2007).
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
We set ? to 10?20, a very low value, to have the ef-
fect of anti-smoothing, as low values of ? cause the
algorithm to favor words which co-occur frequently
and to penalize words that co-occur rarely.
3 Experimental Setup
3.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences, which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such as
the Turkish Ministry of Foreign Affairs, EU, etc. We
followed a heavily supervised approach in morpho-
logical analysis. The Turkish data was first morpho-
logically parsed (Oflazer, 1994), then disambiguated
(Sak et al, 2007) to select the contextually salient in-
terpretation of words. In addition, we removed mor-
phological features that are not explicitly marked by
an overt morpheme ? thus each feature symbol be-
yond the root part-of-speech corresponds to a mor-
pheme. Line (b) of Figure 3 shows an example of
a segmented Turkish sentence. The root is followed
by its part-of-speech tag separated by a ?+?. The
derivational and inflectional morphemes that follow
the root are separated by ???s. In all experiments,
we used the same segmented version of the Turkish
data, because Turkish is an agglutinative language.
For English, we used the CELEX database
(Baayen et al, 1995) to segment English words into
morphemes. We created two versions of the data:
a segmented version that involves both derivational
and inflectional morphology, and an unsegmented
POS tagged version. The CELEX database provides
tags for English derivational morphemes, which in-
dicate their function: the part-of-speech category the
morpheme attaches to and the part-of-speech cate-
gory it returns. For example, in ?sparse+ity? = ?spar-
sity?, the morpheme -ity attaches to an adjective to
the right and returns a noun. This behavior is repre-
sented as ?N|A.? in CELEX, where ?.? indicates the
attachment position. We used these tags in addition
to the surface forms of the English morphemes, in
order to disambiguate multiple functions of a single
surface morpheme.
The English sentence in line (d) of Figure 3 ex-
hibits both derivational and inflectional morphology.
For example, ?author+ity+s?=?authorities? has both
an inflectional suffix -s and a derivational suffix -ity,
whereas ?person+s? has only an inflectional suffix -s.
For both English and Turkish data, the dashes in
Figure 3 stand for morpheme boundaries, therefore
the strings between the dashes are treated as indi-
37
Words Morphemes
Tokens Types Tokens Types
English Der+Inf 1,033,726 27,758 1,368,188 19,448
English POS 1,033,726 28,647 1,033,726 28,647
Turkish Der+Inf 812,374 57,249 1,484,673 16,713
Table 1: Data statistics
visible units. Table 1 shows the number of words,
the number of morphemes and the respective vocab-
ulary sizes. The average number of morphemes in
segmented Turkish words is 2.69, and the average
length of segmented English words is 1.57.
3.2 Experiments
We initialized our baseline word-only model with 5
iterations of IBM Model 1, and further trained the
HMM extension (Vogel et al, 1996) for 5 iterations.
We call this model ?baseline HMM? in the discus-
sions. Similarly, we initialized the two versions of
TAM with 5 iterations of the model explained in
Section 2.2, and then trained the HMM extension of
it as explained in Section 2.3 for 5 iterations.
To obtain BLEU scores for TAM models and
our implementation of the word-only model, i.e.
baseline-HMM, we bypassed GIZA++ in the Moses
toolkit (Och and Ney, 2003). We also ran GIZA++
(IBM Model 1?4) on the data. We translated 1000
sentence test sets.
4 Results and Discussion
We evaluated the performance of our model in two
different ways. First, we evaluated against gold
word alignments for 75 Turkish-English sentences.
Second, we used the word Viterbi alignments of our
algorithm to obtain BLEU scores.
Table 2 shows the AER (Och and Ney, 2003) of
the word alignments of the Turkish-English pair and
the translation performance of the word alignments
learned by our models. We report the grow-diag-
final (Koehn et al, 2003) of the Viterbi alignments.
In Table 2, results obtained with different versions
of the English data are represented as follows: ?Der?
stands for derivational morphology, ?Inf? for inflec-
tional morphology, and ?POS? for part-of-speech
tags. ?Der+Inf? corresponds to the example sen-
tence in line (d) in Figure 3, and ?POS? to line (e).
?DIR? stands for models with Dirichlet priors, and
?NO DIR? stands for models without Dirichlet pri-
ors. All reported results are of the HMM extension
of respective models.
Table 2 shows that using Dirichlet priors hurts
the AER performance of the word-and-morpheme
model in all experiment settings, and benefits the
morpheme-only model in the POS tagged experi-
ment settings.
In order to reduce the effect of nondeterminism,
we run Moses three times per experiment setting,
and report the highest BLEU scores obtained. Since
the BLEU scores we obtained are close, we did a sig-
nificance test on the scores (Koehn, 2004). Table 2
visualizes the partition of the BLEU scores into sta-
tistical significance groups. If two scores within the
same column have the same background color, or the
border between their cells is removed, then the dif-
ference between their scores is not statistically sig-
nificant. For example, the best BLEU scores, which
are in bold, have white background. All scores in a
given experiment setting without white background
are significantly worse than the best score in that ex-
periment setting, unless there is no border separating
them from the best score.
In all experiment settings, the TAM Models per-
form better than the baseline-HMM. Our experi-
ments showed that the baseline-HMM benefits from
Dirichlet priors to a larger extent than the TAM mod-
els. Dirichlet priors help reduce the overfitting in
the case of rare words. The size of the word vo-
cabulary is larger than the size of the morpheme
vocabulary. Therefore the number of rare words is
larger for words than it is for morphemes. Conse-
quently, baseline-HMM, using only the word vocab-
38
                                                
BLEU
EN to TR
BLEU
TR to EN
AER
Der+Inf POS Der+Inf POS Der+Inf POS
NO
 
DIR
TAM
Morph only 22.57 22.54 29.30 29.45 0.293 0.276
Word & Morph 21.95 22.37 28.81 29.01 0.286 0.282
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.78 21.38 28.22 28.02 0.381 0.375
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
DIR
TAM
Morph only 22.18 22.52 29.32 29.98 0.304 0.256
Word & Morph 22.43 21.62 29.21 29.11 0.338 0.317
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.69 21.95 28.76 29.13 0.381 0.377
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
Table 2: AER and BLEU Scores
ulary, benefits from the use of Dirichlet priors more
than the TAM models.
In four out of eight experiment settings, the
morpheme-only model performs better than the
word-and-morpheme version of TAM. However,
please note that our extensive experimentation
with TAM models revealed that the superiority
of the morpheme-only model over the word-and-
morpheme model is highly dependent on segmenta-
tion accuracy, degree of segmentation, and morpho-
logical richness of languages.
Finally, we treated morphemes as words and
trained IBM Model 4 on the morpheme segmented
versions of the data. To obtain BLEU scores, we
had to unsegment the translation output: we con-
catenated the prefixes to the morpheme to the right,
and suffixes to the morpheme to the left. Since this
process creates malformed words, the BLEU scores
obtained are much lower than the scores obtained by
IBM Model 4, the baseline and the TAM Models.
5 Conclusion
We presented two versions of a two-level alignment
model for morphologically rich languages. We ob-
served that information provided by word transla-
tions and morpheme translations interact in a way
that enables the model to be receptive to the par-
tial information in rarely occurring words through
their frequently occurring morphemes. We obtained
significant improvement of BLEU scores over IBM
Model 4. In conclusion, morphologically aware
word alignment models prove to be superior to their
word-only counterparts.
Acknowledgments Funded by NSF award IIS-
0910611. Kemal Oflazer acknowledges the gen-
erous support of the Qatar Foundation through
Carnegie Mellon University?s Seed Research pro-
gram. The statements made herein are solely the
responsibility of this author(s), and not necessarily
that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
39
Technical report, Final Report, JHU Summer Work-
shop.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2) [CD-ROM].
Linguistic Data Consortium, University of Pennsylva-
nia [Distributor], Philadelphia, PA.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90, Morristown, NJ, USA.
Association for Computational Linguistics.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In EMNLP-CoNLL, pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL, pages 57?
60.
Robert C. Moore. 2004. Improving IBM word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich Hidden Semi-Markov Models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 895?904, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sonja Niessen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Computa-
tional Linguistics, pages 1081?1085, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kemal Oflazer. 1994. Two-level description of Turkish
morphology. Literary and Linguistic Computing, 9(2).
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118,
Berlin, Heidelberg. Springer-Verlag.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
ACL, pages 454?464, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
40
Proceedings of NAACL-HLT 2013, pages 439?444,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dudley North visits North London:
Learning When to Transliterate to Arabic
Mahmoud Azab Houda Bouamor
Carnegie Mellon University
P.O. Box 24866, Doha, Qatar
{mazab, hbouamor, behrang, ko}@qatar.cmu.edu
Behrang Mohit Kemal Oflazer
Abstract
We report the results of our work on automat-
ing the transliteration decision of named en-
tities for English to Arabic machine trans-
lation. We construct a classification-based
framework to automate this decision, evalu-
ate our classifier both in the limited news and
the diverse Wikipedia domains, and achieve
promising accuracy. Moreover, we demon-
strate a reduction of translation error and
an improvement in the performance of an
English-to-Arabic machine translation sys-
tem.
1 Introduction
Translation of named entities (NEs) is important
for NLP applications such as Machine Translation
(MT) and Cross-lingual Information Retrieval. For
MT, NEs are major subset of the out-of-vocabulary
terms (OOVs). Due to their diversity, they cannot
always be found in parallel corpora, dictionaries or
gazetteers. Thus, state-of-the-art of MT needs to
handle NEs in specific ways. For instance, in the
English-Arabic automatic translation example given
in Figure 1, the noun ?North? has been erroneously
translated to ? ?J
?A?
??@ /Al$mAlyp ? (indicating the
north direction in English) instead of being translit-
erated to ? HP?	K / nwrv?.
As shown in Figure 1, direct translation of in-
vocabulary terms could degrade translation quality.
Also blind transliteration of OOVs does not neces-
sarily contribute to translation adequacy and may ac-
tually create noisy contexts for the language model
and the decoder.
English Input: Dudley North was an English merchant.
SMT output: . ?K

	Q
?m.Proceedings of NAACL-HLT 2013, pages 661?667,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
Nathan Schneider? Behrang Mohit? Chris Dyer? Kemal Oflazer? Noah A. Smith?
School of Computer Science
Carnegie Mellon University
?Pittsburgh, PA 15213, USA
?Doha, Qatar
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
1 Introduction
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including person and artifact (for nouns)
and motion and weather (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al, 2006), annotated datasets
(Maamouri et al, 2004; Hovy et al, 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
. HA

?J
J.?

J? @
	
Y
	
? @?
	
K ?? ?? ?
	
?? ?


	
?
	
Y
	
? @?
	
J? @ QK
Y? ??j

JK

Ann-A Gloss Ann-B
controls
communication
manager
communicationthe-windows
in
attribute configuration relation
shape and-layout shape
communication
windows
communicationthe-applications
?The window manager controls the configuration and
layout of application windows.?
Figure 1: A sentence from the ?X Window System? ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al, 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This ?MT-in-the-
middle? approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(?2), then the approaches we explored (?3), and fi-
nally present experimental results and analysis in ?4.
2 Task and Resources
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
661
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al, 2012), which augmented a named
entity corpus (Mohit et al, 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., ?Islamic Golden Age?), science (?Atom?),
sports (?Real Madrid?), and technology (?Linux?).
Schneider et al (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
communication (which includes kinds of software)
occurred in the technology domain, whereas most
substances were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators? (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as ?supersense
tags? arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al, 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al, 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al
(2012), but we exclude two test set documents??Light? and
?Ibn Tolun Mosque??due to preprocessing issues.
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic?English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al, 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al, 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al, 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al, 1993)
and achieves 77% F1 in-domain.
3 Methods
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
3.1 Heuristic Tagging with a Lexicon
Using the lexicon built from AWN and OntoNotes
(see ?2), our heuristic approach works as follows:
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al, 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al, 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
662
E? person location artifact substance Automatic English supersense tagging
e? 1 2 3 4 5 6 7 8 9 English sentence
a 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)
N P N A N N Arabic POS tagging
A? person location artifact Projected supersense tagging
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
{1?person11, 4?location
4
3, {5, 6}?artifact
7
6}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) ? senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).
3.2 Unsupervised Sequence Models
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in ?2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al (2010), replicating a feature-
set used previously for NER (Mohit et al, 2012)?
including context tokens, character n-grams, and
POS?and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike??true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
3.3 MT-in-the-Middle
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al,
2001; Smith and Smith, 2004; Hwa et al, 2005; Mi-
halcea et al, 2007; Burkett and Klein, 2008; Burkett
et al, 2010; Das and Petrov, 2011; Kim et al, 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic?English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect?especially for
Wikipedia text, which is distant from the domain
of the translation system?s training data?but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
1. Preprocess the input Arabic sentence a to
match the decoder?s model of Arabic.
2. Translate the sentence, recovering not just
the English output e? but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output E?.
4. Project the supersense tags back to the Ara-
bic sentence, yielding A?: Each Arabic token
a ? a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in E? containing some word aligned
to a. Then, for each English supersense men-
663
Coverage Ann-A Ann-B
Nouns All Tokens Mentions P R F1 P R F1
Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
MT-in-the-middle
(?3.3)
QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair ?a?, e?? ? z, we con-
sider every word in a? as aligned to every word
in e? (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
4 Experiments and Analysis
Table 1 compares the techniques (?3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions?unlabeled recall
is substantially improved, and unlabeled precision is
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline?s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see ?2), and also well below the 70% inter-
annotator F1 reported by Schneider et al (2012). We
therefore examine the weaknesses of our approach
for Arabic.
4.1 MT for Projection
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences?5 per article in
the corpus?were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
8BLEU (Papineni et al, 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
664
. ????@ ?


	
? @Yg.

?Q

	??

?
	
Jj ??@

?J.k. ??

?@?
	
K ??k ??mProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 454?464,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical
Machine Translation from English to Turkish
Reyyan Yeniterzi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
reyyan@cs.cmu.edu
Kemal Oflazer
Computer Science
Carnegie Mellon University-Qatar
PO Box 24866, Doha, Qatar
ko@cs.cmu.edu
Abstract
We present a novel scheme to apply fac-
tored phrase-based SMT to a language pair
with very disparate morphological struc-
tures. Our approach relies on syntac-
tic analysis on the source side (English)
and then encodes a wide variety of local
and non-local syntactic structures as com-
plex structural tags which appear as ad-
ditional factors in the training data. On
the target side (Turkish), we only per-
form morphological analysis and disam-
biguation but treat the complete complex
morphological tag as a factor, instead of
separating morphemes. We incrementally
explore capturing various syntactic sub-
structures as complex tags on the En-
glish side, and evaluate how our transla-
tions improve in BLEU scores. Our max-
imal set of source and target side trans-
formations, coupled with some additional
techniques, provide an 39% relative im-
provement from a baseline 17.08 to 23.78
BLEU, all averaged over 10 training and
test sets. Now that the syntactic analy-
sis on the English side is available, we
also experiment with more long distance
constituent reordering to bring the English
constituent order close to Turkish, but find
that these transformations do not provide
any additional consistent tangible gains
when averaged over the 10 sets.
1 Introduction
Statistical machine translation into a morphologi-
cally complex language such as Turkish, Finnish
or Arabic, involves the generation of target words
with the proper morphology, in addition to prop-
erly ordering the target words. Earlier work on
translation from English to Turkish (Oflazer and
Durgar-El-Kahlout, 2007; Oflazer, 2008; Durgar-
El-Kahlout and Oflazer, 2010) has used an ap-
proach which relied on identifying the contextu-
ally correct parts-of-speech, roots and any mor-
phemes on the English side, and the complete se-
quence of roots and overt derivational and inflec-
tional morphemes for each word on the Turkish
side. Once these were identified as separate to-
kens, they were then used as ?words? in a stan-
dard phrase-based framework (Koehn et al, 2003).
They have reported that, given the typical com-
plexity of Turkish words, there was a substantial
percentage of words whose morphological struc-
ture was incorrect: either the morphemes were
not applicable for the part-of-speech category of
the root word selected, or the morphemes were
in the wrong order. The main reason given for
these problems was that the same statistical trans-
lation, reordering and language modeling mecha-
nisms were being employed to both determine the
morphological structure of the words and, at the
same time, get the global order of the words cor-
rect. Even though a significant improvement of a
standard word-based baseline was achieved, fur-
ther analysis hinted at a direction where morphol-
ogy and syntax on the Turkish side had to be dealt
with using separate mechanisms.
Motivated by the observation that many lo-
cal and some nonlocal syntactic structures in En-
glish essentially map to morphologically complex
words in Turkish, we present a radically different
approach which does not segment Turkish words
into morphemes, but uses a representation equiv-
alent to the full word form. On the English side,
we rely on a full syntactic analysis using a depen-
dency parser. This analysis then lets us abstract
and encode many local and some nonlocal syn-
tactic structures as complex tags (dynamically, as
opposed to the static complex tags as proposed by
Birch et al (2007) and Hassan et al (2007)). Thus
454
we can bring the representation of English syntax
closer to the Turkish morphosyntax.
Such an approach enables the following: (i)
Driven by the pattern of morphological structures
of full word forms on the Turkish side represented
as root words and complex tags, we can iden-
tify and reorganize phrases on the English side,
to ?align? English syntax to Turkish morphology
wherever possible. (ii) Continuous and discontin-
uous variants of certain (syntactic) phrases can be
conflated during the SMT phrase extraction pro-
cess. (iii) The length of the English sentences can
be dramatically reduced, as most function words
encoding syntax are now abstracted into complex
tags. (iv) The representation of both the source and
the target sides of the parallel corpus can now be
mostly normalized. This facilitates the use of fac-
tored phrase-based translation that was not pre-
viously applicable due to the morphological com-
plexity on the target side and mismatch between
source and target morphologies.
We find that with the full set of syntax-to-
morphology transformations and some additional
techniques we can get about 39% relative im-
provement in BLEU scores over a word-based
baseline and about 28% improvement of a factored
baseline, all experiments being done over 10 train-
ing and test sets. We also find that further con-
stituent reordering taking advantage of the syntac-
tic analysis of the source side, does not provide
tangible improvements when averaged over the 10
data sets.
This paper is organized as follows: Sec-
tion 2 presents the basic idea behind syntax-
to-morphology alignment. Section 3 describes
our experimental set-up and presents results from
a sequence of incremental syntax-to-morphology
transformations, and additional techniques. Sec-
tion 4 summarizes our constituent reordering ex-
periments and their results. Section 5 presents a
review of related work and situates our approach.
We assume that the reader is familiar with the
basics of phrase-based statistical machine transla-
tion (Koehn et al, 2003) and factored statistical
machine translation (Koehn and Hoang, 2007).
2 Syntax-to-Morphology Mapping
In this section, we describe how we map between
certain source language syntactic structures and
target words with complex morphological struc-
tures. At the top of Figure 1, we see a pair of
(syntactic) phrases, where we have (positionally)
aligned the words that should be translated to each
other. We can note that the function words on and
Figure 1: Transformation of an English preposi-
tional phrase
their are not really aligned to any of the Turkish
words as they really correspond to two of the mor-
phemes of the last Turkish word.
When we tag and syntactically analyze the En-
glish side into dependency relations, and morpho-
logically analyze and disambiguate the Turkish
phrase, we get the representation in the middle of
Figure 1, where we have co-indexed components
that should map to each other, and some of the
syntactic relations that the function words are in-
volved in are marked with dependency links.1
The basic idea in our approach is to take various
function words on the English side, whose syntac-
tic relationships are identified by the parser, and
then package them as complex tags on the related
content words. So, in this example, if we move
the first two function words from the English side
and attach as syntactic tags to the word they are in
dependency relation with, we get the aligned rep-
resentation at the bottom of Figure 1.2,3 Here we
can note that all root words and tags that corre-
spond to each other are nicely structured and are
in the same relative order. In fact, we can treat
each token as being composed of two factors: the
roots and the accompanying tags. The tags on the
Turkish side encode morphosyntactic information
encoded in the morphology of the words, while the
1The meanings of various tags are as follows: Depen-
dency Labels: PMOD - Preposition Modifier; POS - Pos-
sessive. Part-of-Speech Tags for the English words: +IN -
Preposition; +PRP$ - Possessive Pronoun; +JJ - Adjective;
+NN - Noun; +NNS - Plural Noun. Morphological Feature
Tags in the Turkish Sentence: +A3pl - 3rd person plural;
+P3sg - 3rd person singular possessive; +Loc - Locative case.
Note that we mark an English plural noun as +NN NNS to in-
dicate that the root is a noun and there is a plural morpheme
on it. Note also that economic is also related to relations but
we are not interested in such content words and their rela-
tions.
2We use to prefix such syntactic tags on the English side.
3The order is important in that we would like to attach the
same sequence of function words in the same order so that
the resulting tags on the English side are the same.
455
(complex) tags on the English side encode local
(and sometimes, non-local) syntactic information.
Furthermore, we can see that before the transfor-
mations, the English side has 4 words, while af-
terwards it has only 2 words. We find (and elab-
orate later) that this reduction in the English side
of the training corpus, in general, is about 30%,
and is correlated with improved BLEU scores. We
believe the removal of many function words and
their folding into complex tags (which do not get
involved in GIZA++ alignment ? we only align the
root words) seems to improve alignment as there
are less number of ?words? to worry about during
that process.4
Another interesting side effect of this represen-
tation is the following. As the complex syntac-
tic tags on the English side are based on syntactic
relations and not necessarily positional proximity,
the tag for relations in a phrase like in their cul-
tural, historical and economic relations would be
exactly the same as above. Thus phrase extrac-
tion algorithms can conflate all constructs like in
their . . . economic relations as one phrase, regard-
less of the intervening modifiers, assuming that
parser does its job properly.
Not all cases can be captured as cleanly as the
example above, but most transformations capture
local and nonlocal syntax involving many function
words and then encode syntax with complex tags
resembling full morphological tags on the Turk-
ish side. These transformations, however, are not
meant to perform sentence level constituent re-
ordering on the English side. We explore these
later.
We developed set of about 20 linguistically-
motivated syntax-to-morphology transformations
which had variants parameterized depending on
what, for instance, the preposition or the adverbial
was, and how they map to morphological struc-
ture on the Turkish side. For instance, one general
rule handles cases like while . . . verb and if . . . verb
etc., mapping these to appropriate complex tags.
It is also possible that multiple transformations
can apply to generate a single English complex
tag: a portion of the tag can come from a verb
complex transformation, and another from an ad-
verbial phrase transformation involving a marked
such as while. Our transformations handle the fol-
lowing cases:
? Prepositions attach to the head-word of their
4Fraser (2009) uses the first four letters of German words
after morphological stripping and compound decomposition
to help with alignment in German to English and reverse
translation.
complement noun phrase as a component in
its complex tag.
? Possessive pronouns attach to the head-word
they specify.
? The possessive markers following a noun
(separated by the tokenizer) attached to the
noun.
? Auxiliary verbs and negation markers attach
to the lexical verb that they form a verb com-
plex with.
? Modals attach to the lexical verb they modify.
? Forms of be used as predicates with adjecti-
val or nominal dependents attach to the de-
pendent.
? Forms of be or have used to form passive
voice with past participle verbs, and forms of
be used with -ing verbs to form present con-
tinuous verbs, attach to the verb.
? Various adverbial clauses formed with if,
while, when, etc., are reorganized so that
these markers attach to the head verb of the
clause.
As stated earlier, these rules are linguistically mo-
tivated and are based on the morphological struc-
ture of the target language words. Hence for dif-
ferent target languages these rules will be differ-
ent. The rules recognize various local and nonlo-
cal syntactic structures in the source side parse tree
that correspond to complex morphological of tar-
get words and then remove source function words
folding them into complex tags. For instance, the
transformations in Figure 1 are handled by scripts
that process Malt Parser?s dependency structure
output and that essentially implement the follow-
ing sequence of rules expressed as pseudo code:
1) if (<Y>+PRP$ POS <Z>+NN<TAG>)
then {
APPEND <Y>+PRP$ TO <Z>+NN<TAG>
REMOVE <Y>+PRP$
}
2) if (<X>+IN PMOD <Z>+NN<TAG>)
then {
APPEND <X>+IN TO <Z>+NN<TAG>
REMOVE <X>+IN
}
Here <X>, <Y> and <Z> can be considered as Pro-
log like-variables that bind to patterns (mostly root
words), and the conditions check for specified de-
pendency relations (e.g., PMOD) between the left
and the right sides. When the condition is satis-
fied, then the part matching the function word is
removed and its syntactic information is appended
to form the complex tag on the noun (<TAG> would
either match null string or any previously ap-
pended function word markers.)5
5We outline two additional rules later when we see a more
complex example in Figure 2.
456
There are several other rules that handle more
mundane cases of date and time constructions (for
which, the part of the date construct which the
parser attaches a preposition, is usually different
than the part on the Turkish side that gets inflected
with case markers, and these have to be reconciled
by overriding the parser output.)
The next section presents an example of a sen-
tence with multiple transformations applied, after
discussing the preprocessing steps.
3 Experimental Setup and Results
3.1 Data Preparation
We worked on an English-Turkish parallel corpus
which consists of approximately 50K sentences
with an average of 23 words in English sentences
and 18 words in Turkish sentences. This is the
same parallel data that has been used in earlier
SMT work on Turkish (Durgar-El-Kahlout and
Oflazer, 2010). Let?s assume we have the follow-
ing pair of parallel sentences:
E: if a request is made orally the authority must
make a record of it
T: istek so?zlu? olarak yap?lm?s?sa yetkili makam bunu
kaydetmelidir
On the English side of the data, we use the Stan-
ford Log-Linear Tagger (Toutanova et al, 2003),
to tag the text with Penn Treebank Tagset. On
the Turkish side, we perform a full morphological
analysis, (Oflazer, 1994), and morphological dis-
ambiguation (Yuret and Tu?re, 2006) to select the
contextually salient interpretation of words. We
then remove any morphological features that are
not explicitly marked by an overt morpheme.6 So
for both sides we get,
E: if+IN a+DT request+NN is+VBZ made+VBN orally+RB
the+DT authority+NN must+MD make+VB a+DT record+NN
of+IN it+PRP
T: istek+Noun so?zlu?+Adj olarak+Verb+ByDoingSo
yap+Verb+Pass+Narr+Cond yetkili+Adj makam+Noun
bu+Pron+Acc kaydet+Verb+Neces+Cop
Finally we parse the English sentences using
MaltParser (Nivre et al, 2007), which gives us
labeled dependency parses. On the output of the
parser, we make one more transformation. We re-
place each word with its root, and possibly add an
additional tag for any inflectional information con-
veyed by overt morphemes or exceptional forms.
This is done by running the TreeTagger (Schmid,
1994) on the English side which provides the roots
in addition to the tags, and then carrying over this
information to the parser output. For example,
is is tagged as be+VB VBZ, made is tagged as
make+VB VBN, and a word like books is tagged
6For example, the morphological analyzer outputs +A3sg
to mark a singular noun, if there is no explicit plural mor-
pheme. Such markers are removed.
as book+NN NNS (and not as books+NNS). On
the Turkish side, each marker with a preceding
+ is a morphological feature. The first marker
is the part-of-speech tag of the root and the re-
mainder are the overt inflectional and derivational
markers of the word. For example, the analy-
sis kitap+Noun+P2pl+A3pl+Gen for a word
like kitap-lar-?n?z-?n7 (of your books)
represents the root kitap (book), a Noun, with
third person plural agreement A3pl, second per-
son plural possessive agreement, P2pl and geni-
tive case Gen.
The sentence representations in the middle part
of Figure 2 show these sentences with some of the
dependency relations (relevant to our transforma-
tions) extracted by the parser, explicitly marked as
labeled links. The representation at the bottom of
this figure (except for the co-indexation markers)
corresponds to the final transformed form of the
parallel training and test data. The co-indexation
is meant to show which root words on one side
map to which on the other side. Ultimately we
would want the alignment process to uncover the
root word alignments indicated here. We can also
note that the initial form of the English sentence
has 14 words and the final form after transforma-
tions, has 7 words (with complex tags).8
3.2 Experiments
We evaluated the impact of the transformations
in factored phrase-based SMT with an English-
Turkish data set which consists of 52712 parallel
sentences. In order to have more confidence in the
impact of our transformations, we randomly gen-
erated 10 training, test and tune set combinations.
For each combination, the latter two were 1000
sentences each and the remaining 50712 sentences
were used as training sets.9,10
We performed our experiments with the Moses
toolkit (Koehn et al, 2007). In order to encourage
long distance reordering in the decoder, we used
a distortion limit of -1 and a distortion weight of
7- shows surface morpheme boundaries.
8We could give two more examples of rules to process
the if-clause in the example in Figure 2. These rules would
be applied sequentially: The first rule recognizes the pas-
sive construction mediated by be+VB<AGR> forming a verb
complex (VC) with <Y>+VB_VBN and appends the former
to the complex tag on the latter and then deletes the former
token. The second rule then recognizes <X>+IN relating to
<Y>+VB<TAGS>with VMOD and appends the former to the
complex tag on the latter and then deletes the former token.
9The tune set was not used in this work but reserved for
future work so that meaningful comparisons could be made.
10It is possible that the 10 test sets are not mutually exclu-
sive.
457
Figure 2: An English-Turkish sentence pair with multiple transformations applied
0.1.11 We did not use MERT to further optimize
our model.12
For evaluation, we used the BLEU metric (Pap-
ineni et al, 2001). Each experiment was repeated
over the 10 data sets. Wherever meaningful, we
report the average BLEU scores over 10 data sets
along with the maximum and minimum values and
the standard deviation.
11These allow and do not penalize unlimited distortions.
12The experience with MERT for this language pair has
not been very positive. Earlier work on Turkish indicates that
starting with default Moses parameters and applying MERT
to the resulting model does not even come close to the per-
formance of the model with those two specific parameters set
as such (distortion limit -1 and distortion weight 0.1), most
likely because the default parameters do not encourage the
range of distortions that are needed to deal with the con-
stituent order differences. Earlier work on Turkish also shows
that even when the weight-d parameter is initialized with this
specific value, the space explored for distortion weight and
other parameters do not produce any improvements on the
test set, even though MERT claims there are improvements
on the tune set.
The other practical reasons for not using MERT were
the following: at the time we performed this work, the
discussion thread at http://www.mail-archive.
com/moses-support@mit.edu/msg01012.html
indicated that MERT was not tested on multiple factors.
The discussion thread at http://www.mail-archive.
com/moses-support@mit.edu/msg00262.html
claimed that MERT does not help very much with factored
models. With these observations, we opted not to experiment
with MERT with the multiple factor approach we employed,
given that it would be risky and time consuming to run
MERT needed for 10 different models and then not neces-
sarily see any (consistent) improvements. MERT however
is orthogonal to the improvements we achieve here and can
always be applied on top of the best model we get.
3.2.1 The Baseline Systems
As a baseline system, we built a standard phrase-
based system, using the surface forms of the words
without any transformations, and with a 3-gram
LM in the decoder. We also built a second baseline
system with a factored model. Instead of using just
the surface form of the word, we included the root,
part-of-speech and morphological tag information
into the corpus as additional factors alongside the
surface form.13 Thus, a token is represented with
three factors as Surface|Root|Tags where
Tags are complex tags on the English side, and
morphological tags on the Turkish side.14
Moses lets word alignment to align over any of
the factors. We aligned our training sets using only
the root factor to conflate statistics from different
forms of the same root. The rest of the factors are
then automatically assumed to be aligned, based
on the root alignment. Furthermore, in factored
models, we can employ different language models
for different factors. For the initial set of experi-
ments we used 3-gram LMs for all the factors.
For factored decoding, we employed a model
whereby we let the decoder translate a surface
form directly, but if/when that fails, the decoder
can back-off with a generation model that builds
a target word from independent translations of the
root and tags.
13In Moses, factors are separated by a ?|? symbol.
14Concatenating Root and Tags gives the Surface
form, in that the surface is unique given this concatenation.
458
The results of our baseline models are given in
top two rows of Table 1. As expected, the word-
based baseline performs worse than the factored
baseline. We believe that the use of multiple lan-
guage models (some much less sparse than the sur-
face LM) in the factored baseline is the main rea-
son for the improvement.
3.2.2 Applying Syntax-to-Morphology
Mapping Transformations
To gauge the effects of transformations separately,
we first performed them in batches on the En-
glish side. These batches were (i) transforma-
tions involving nouns and adjectives (Noun+Adj),
(ii) transformations involving verbs (Verb), (iii)
transformations involving adverbs (Adv), and
(iv) transformations involving verbs and adverbs
(Verb+Adv).
We also performed one set of transformations
on the Turkish side. In general, English preposi-
tions translate as case markers on Turkish nouns.
However, there are quite a number of lexical post-
positions in Turkish which also correspond to En-
glish prepositions. To normalize these with the
handling of case-markers, we treated these postpo-
sitions as if they were case-markers and attached
them to the immediately preceding noun, and then
aligned the resulting training data (PostP).15
The results of these experiments are presented
in Table 1. We can observe that the com-
bined syntax-to-morphology transformations on
the source side provide a substantial improvement
by themselves and a simple target side transfor-
mation on top of those provides a further boost
to 21.96 BLEU which represents a 28.57% rel-
ative improvement over the word-based baseline
and a 18.00% relative improvement over the fac-
tored baseline.
Experiment Ave. STD Max. Min.
Baseline 17.08 0.60 17.99 15.97
Factored Baseline 18.61 0.76 19.41 16.80
Noun+Adj 21.33 0.62 22.27 20.05
Verb 19.41 0.62 20.19 17.99
Adv 18.62 0.58 19.24 17.30
Verb+Adv 19.42 0.59 20.17 18.13
Noun+Adj 21.67 0.72 22.66 20.38
+Verb+Adv
Noun+Adj+Verb 21.96 0.72 22.91 20.67
+Adv+PostP
Table 1: BLEU scores for a variety of transforma-
tion combinations
We can see that every transformation improves
15Note than in this case, the translations would be gener-
ated in the same format, but we then split such postpositions
from the words they are attached to, during decoding, and
then evaluate the BLEU score.
the baseline system and the highest performance is
attained when all transformations are performed.
However when we take a closer look at the indi-
vidual transformations performed on English side,
we observe that not all of them have the same ef-
fect. While Noun+Adj transformations give us an
increase of 2.73 BLEU points, Verbs improve the
result by only 0.8 points and improvement with
Adverbs is even lower. To understand why we
get such a difference, we investigated the corre-
lation of the decrease in the number of tokens on
both sides of the parallel data, with the change in
BLEU scores. The graph in Figure 3 plots the
BLEU scores and the number of tokens in the two
sides of the training data as the data is modified
with transformations. We can see that as the num-
ber of tokens in English decrease, the BLEU score
increases. In order to measure the relationship
between these two variables statistically, we per-
formed a correlation analysis and found that there
is a strong negative correlation of -0.99 between
the BLEU score and the number of English tokens.
We can also note that the largest reduction in the
number of tokens comes with the application of
the Noun+Adj transformations, which correlates
with the largest increase in BLEU score.
It is also interesting to look at the n-gram pre-
cision components of the BLEU scores (again av-
eraged). In Table 2, we list these for words (ac-
tual BLEU), roots (BLEU-R) to see how effective
we are in getting the root words right, and mor-
phological tags, (BLEU-M), to see how effective
we are in getting just the morphosyntax right. It
1-gr. 2-gr. 3-gr. 4-gr.
BLEU 21.96 55.73 27.86 16.61 10.68
BLEU-R 27.63 68.60 35.49 21.08 13.47
BLEU-M 27.93 67.41 37.27 21.40 13.41
Table 2: Details of Word, Root and Morphology
BLEU Scores
seems we are getting almost 69% of the root words
and 68% of the morphological tags correct, but
not necessarily getting the combination equally as
good, since only about 56% of the full word forms
are correct. One possible way to address is to use
longer distance constraints on the morphological
tag factors, to see if we can select them better.
3.2.3 Experiments with higher-order
language models
Factored phrase-based SMT allows the use of mul-
tiple language models for the target side, for dif-
ferent factors during decoding. Since the number
of possible distinct morphological tags (the mor-
phological tag vocabulary size) in our training data
459
Figure 3: BLEU scores vs number of tokens in the training sets
(about 3700) is small compared to distinct num-
ber of surface forms (about 52K) and distinct roots
(about 15K including numbers), it makes sense to
investigate the contribution of higher order n-gram
language models for the morphological tag factor
on the target side, to see if we can address the ob-
servation in the previous section.
Using the data transformed with Noun+Adj-
+Verb+Adv+PostP transformations which previ-
ously gave us the best results overall, we experi-
mented with using higher order models (4-grams
to 9-grams) during decoding, for the morphologi-
cal tag factor models, keeping the surface and root
models at 3-gram. We observed that for all the 10
data sets, the improvements were consistent for up
to 8-gram. The BLEU with the 8-gram for only
the morphological tag factor averaged over the 10
data sets was 22.61 (max: 23.66, min: 21.37, std:
0.72) compared to the 21.96 in Table 1. Using a 4-
gram root LM, considerably less sparse than word
forms but more sparse that tags, we get a BLEU
score of 22.80 (max: 24.07, min: 21.57, std: 0.85).
The details of the various BLEU scores are shown
in the two halves of Table 3. It seems that larger
n-gram LMs contribute to the larger n-gram preci-
sions contributing to the BLEU but not to the uni-
gram precision.
3-gram root LM 1-gr. 2-gr. 3-gr. 4-gr.
BLEU 22.61 55.85 28.21 17.16 11.36
BLEU-R 28.21 68.67 35.80 21.55 14.07
BLEU-M 28.68 67.50 37.59 22.02 14.22
4-gram root LM 1-gr. 2-gr. 3-gr. 4-gr.
BLEU 22.80 55.85 28.39 17.34 11.54
BLEU-R 28.48 68.68 35.97 21.79 14.35
BLEU-M 28.82 67.49 37.63 22.17 14.40
Table 3: Details of Word, Root and Morphology
BLEU Scores, with 8-gram tag LM and 3/4-gram
root LMs
3.2.4 Augmenting the Training Data
In order to alleviate the lack of large scale parallel
corpora for the English?Turkish language pair, we
experimented with augmenting the training data
with reliable phrase pairs obtained from a previous
alignment. Phrase table entries for the surface fac-
tors produced by Moses after it does an alignment
on the roots, contain the English (e) and Turkish (t)
parts of a pair of aligned phrases, and the proba-
bilities, p(e|t), the conditional probability that the
English phrase is e given that the Turkish phrase
is t, and p(t|e), the conditional probability that
the Turkish phrase is t given the English phrase is
e. Among these phrase table entries, those with
p(e|t) ? p(t|e) and p(t|e) + p(e|t) larger than
some threshold, can be considered as reliable mu-
tual translations, in that they mostly translate to
each other and not much to others. We extracted
460
from the phrase table those phrases with 0.9 ?
p(e|t)/p(t|e) ? 1.1 and p(t|e) + p(e|t) ? 1.5
and added them to the training data to further bias
the alignment process. The resulting BLEU score
was 23.78 averaged over 10 data sets (max: 24.52,
min: 22.25, std: 0.71).16
4 Experiments with Constituent
Reordering
The transformations in the previous section do
not perform any constituent level reordering, but
rather eliminate certain English function words as
tokens in the text and fold them into complex syn-
tactic tags. That is, no transformations reorder
the English SVO order to Turkish SOV,17 for in-
stance, or move postnominal prepositional phrase
modifiers in English, to prenominal phrasal mod-
ifiers in Turkish. Now that we have the parses
of the English side, we have also investigated a
more comprehensive set of reordering transforma-
tions which perform the following constituent re-
orderings to bring English constituent order more
in line with the Turkish constitent order at the top
and embedded phrase levels:
? Object reordering (ObjR), in which the ob-
jects and their dependents are moved in front
of the verb.
? Adverbial phrase reordering (AdvR), which
involve moving post-verbal adverbial phrases
in front of the verb.
? Passive sentence agent reordering (PassAgR),
in which any post-verbal agents marked by
by, are moved in front of the verb.
? Subordinate clause reordering (SubCR)
which involve moving postnominal relative
clauses or prepositional phrase modifers in
front of any modifiers of the head noun.
Similarly any prepositional phrases attached
to verbs are moved to in front of the verb.
We performed these reorderings
on top of the data obtained with the
Noun+Adj+Verb+Adv+PostP transformations
earlier in Section 3.2.2 and used the same decoder
parameters. Table 4 shows the performance
obtained after various combination of reordering
operations over the 10 data sets. Although there
were some improvements for certain cases, none
16These experiments were done on top of the model in
3.2.3 with a 3-gram word and root LMs and 8-gram tag LM.
17Although Turkish is a free-constituent order language,
SOV is the dominant order in text.
of reordering gave consistent improvements for
all the data sets. A cursory examinations of
the alignments produced after these reordering
transformations indicated that the resulting root
alignments were not necessarily that close to
being monotonic as we would have expected.
Experiment Ave. STD Max. Min.
Baseline 21.96 0.72 22.91 20.67
ObjR 21.94 0.71 23.12 20.56
ObjR+AdvR 21.73 0.50 22.44 20.69
ObjR+PassAgR 21.88 0.73 23.03 20.51
ObjR+SubCR 21.88 0.61 22.77 20.92
Table 4: BLEU scores of after reordering transfor-
mations
5 Related Work
Statistical Machine Translation into a morpholog-
ically rich language is a challenging problem in
that, on the target side, the decoder needs to gen-
erate both the right sequence of constituents and
the right sequence of morphemes for each word.
Furthermore, since for such languages one can
generate tens of hundreds of inflected variants,
standard word-based alignment approaches suf-
fer from sparseness issues. Koehn (2005) applied
standard phrase-based SMT to Finnish using the
Europarl corpus and reported that translation to
Finnish had the worst BLEU scores.
Using morphology in statistical machine trans-
lation has been addressed by many researchers for
translation from or into morphologically rich(er)
languages. Niessen and Ney (2004) used mor-
phological decomposition to get better alignments.
Yang and Kirchhoff (2006) have used phrase-
based backoff models to translate unknown words
by morphologically decomposing the unknown
source words. Lee (2004) and Zolmann et al
(2006) have exploited morphology in Arabic-
English SMT. Popovic and Ney (2004) investi-
gated improving translation quality from inflected
languages by using stems, suffixes and part-of-
speech tags. Goldwater and McClosky (2005)
use morphological analysis on the Czech side to
get improvements in Czech-to-English statistical
machine translation. Minkov et al (2007) have
used morphological postprocessing on the target
side, to improve translation quality. Avramidis and
Koehn (2008) have annotated English with addi-
tional morphological information extracted from a
syntactic tree, and have used this in translation to
Greek and Czech. Recently, Bisazza and Federico
(2009) have applied morphological segmentation
in Turkish-to-English statistical machine transla-
tion and found that it provides nontrivial BLEU
461
score improvements.
In the context of translation from English to
Turkish, Durgar-El Kahlout and Oflazer (2010)
have explored different representational units of
the lexical morphemes and found that selectively
splitting morphemes on the target side provided
nontrivial improvement in the BLEU score. Their
approach was based on splitting the target Turk-
ish side, into constituent morphemes while our ap-
proach in this paper is the polar opposite: we do
not segment morphemes on the Turkish side but
rather join function words on the English side to
the related content words. Our approach is some-
what similar to recent approaches that use com-
plex syntactically-motivated complex tags. Birch
et al (2007) have integrated more syntax in a
factored translation approach by using CCG su-
pertags as a separate factor and have reported
a 0.46 BLEU point improvement in Dutch-to-
English translations. Although they used su-
pertags, these were obtained not via syntactic anal-
ysis but by supertagging, while we determine, on
the fly, the appropriate syntactic tags based on syn-
tactic structure. A similar approach based on su-
pertagging was proposed by Hassan et al (2007).
They used both CCG supertags and LTAG su-
pertags in Arabic-to-English phrase-based transla-
tion and have reported about 6% relative improve-
ment in BLEU scores. In the context of reorder-
ing, one recent work (Xu et al, 2009), was able
to get an improvement of 0.6 BLEU points by us-
ing source syntactic analysis and a constituent re-
ordering scheme like ours for English-to-Turkish
translation, but without using any morphology.
6 Conclusions
We have presented a novel way to incorporate
source syntactic structure in English-to-Turkish
phrase-based machine translation by parsing the
source sentences and then encoding many local
and nonlocal source syntactic structures as addi-
tional complex tag factors. Our goal was to ob-
tain representations of source syntactic structures
that parallel target morphological structures, and
enable us to extend factored translation, in appli-
cability, to languages with very disparate morpho-
logical structures.
In our experiments over a limited amount train-
ing data, but repeated with 10 different training
and test sets, we found that syntax-to-morphology
mapping transformations on the source side sen-
tences, along with a very small set of transforma-
tions on the target side, coupled with some ad-
ditional techniques provided about 39% relative
improvement in BLEU scores over a word-based
baseline and about 28% improvement of a factored
baseline. We also experimented with numerous
additional syntactic reordering transformation on
the source to further bring the constituent order in
line with the target order but found that these did
not provide any tangible improvements when av-
eraged over the 10 different data sets.
It is possible that the techniques presented in
this paper may be less effective if the available
data is much larger, but we have reasons to be-
lieve that they will still be effective then also. The
reduction in size of the source language side of
the training corpus seems to be definitely effective
and there no reason why such a reduction (if not
more) will not be observed in larger data. Also,
the preprocessing of English prepositional phrases
and many adverbial phrases usually involve rather
long distance relations in the source side syntactic
structure18 and when such structures are coded as
complex tags on the nominal or verbal heads, such
long distance syntax is effectively ?localized? and
thus can be better captured with the limited win-
dow size used for phrase extraction.
One limitation of the approach presented here
is that it is not directly applicable in the reverse
direction. The data encoding and set-up can di-
rectly be employed to generate English ?transla-
tion? expressed as a sequence of root and complex
tag combinations, but then some of the complex
tags could encode various syntactic constructs. To
finalize the translation after the decoding step, the
function words/tags in the complex tag would then
have to be unattached and their proper positions
in the sentence would have to be located. The
problem is essentially one of generating multiple
candidate sentences with the unattached function
words ambiguously positioned (say in a lattice)
and then use a second language model to rerank
these sentences to select the target sentence. This
is an avenue of research that we intend to look at
in the very near future.
Acknowledgements
We thank Joakim Nivre for providing us with the
parser. This publication was made possible by the
generous support of the Qatar Foundation through
Carnegie Mellon University?s Seed Research pro-
gram. The statements made herein are solely the
responsibility of the authors.
18For instance, consider the example in Figure 2 involving
if with some additional modifiers added to the intervening
noun phrase.
462
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statis-
tical machine translation. In Proceedings of ACL-
08/HLT, pages 763?770, Columbus, Ohio, June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored translation models.
In Proceedings of SMT Workshop at the 45th ACL.
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English sta-
tistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation, Tokyo, Japan, December.
I?lknur Durgar-El-Kahlout and Kemal Oflazer. 2010.
Exploiting morphology and local word reordering in
English to Turkish phrase-based statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing. To Appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 115?119, Athens,
Greece, March. Association for Computational Lin-
guistics.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of HLT/EMNLP-2005, pages
676?683, Vancouver, British Columbia, Canada,
October.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th ACL, pages 288?
295, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL-2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th ACL?demonstration
session, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit X.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT/NAACL-2004 ? Companion Volume, pages 57?
60.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th ACL, pages
128?135, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic information. Computational Lin-
guistics, 30(2):181?204.
Joakim Nivre, Hall Johan, Nilsson Jens, Chanev
Atanas, Gu?ls?en Eryig?it, Sandra Ku?bler, Marinov
Stetoslav, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering
Journal, 13(2):99?135.
Kemal Oflazer and I?lknur Durgar-El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of Statistical Machine Translation
Workshop at the 45th Annual Meeting of the
Association for Computational Linguistics, pages
25?32.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137?148.
Kemal Oflazer. 2008. Statistical machine translation
into a morphologically complex language. In Pro-
ceedings of the Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLing),
pages 376?387.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th ACL, pages 311?318.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical ma-
chine translation. In Proceedings of the 4th LREC,
pages 1585?1588, May.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT/NAACL-2003, pages 252?
259.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings HLT/NAACL-2009, pages 245?253, June.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL-2006,
pages 41?48.
463
Deniz Yuret and Ferhan Tu?re. 2006. Learning mor-
phological disambiguation rules for Turkish. In
Proceedings of HLT/NAACL-2006, pages 328?334,
New York City, USA, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of HLT/NAACL-2006 ? Companion
Volume, pages 201?204, New York City, USA, June.
464
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 176?180,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transforming Standard Arabic to Colloquial Arabic 
 
 Emad Mohamed, Behrang Mohit and Kemal Oflazer 
 
Carnegie Mellon University - Qatar 
Doha, Qatar 
emohamed@qatar.cmu.edu, behrang@cmu.edu, ko@cs.cmu.edu  
 
 
Abstract 
We present a method for generating Colloquial 
Egyptian Arabic (CEA) from morphologically dis-
ambiguated Modern Standard Arabic (MSA). 
When used in POS tagging, this process improves 
the accuracy from 73.24% to 86.84% on unseen 
CEA text, and reduces the percentage of out-of-
vocabulary words from 28.98% to 16.66%. The 
process holds promise for any NLP task targeting 
the dialectal varieties of Arabic; e.g., this approach 
may provide a cheap way to leverage MSA data 
and morphological resources to create resources 
for colloquial Arabic to English machine transla-
tion. It can also considerably speed up the annota-
tion of Arabic dialects. 
 
1. Introduction 
Most of the research on Arabic is focused on Mod-
ern Standard Arabic. Dialectal varieties have not 
received much attention due to the lack of dialectal 
tools and annotated texts (Duh and Kirchoff, 
2005). In this paper, we present a rule-based me-
thod to generate Colloquial Egyptian Arabic (CEA) 
from Modern Standard Arabic (MSA), relying on 
segment-based part-of-speech tags. The transfor-
mation process relies on the observation that di-
alectal varieties of Arabic differ mainly in the use 
of affixes and function words while the word stem 
mostly remains unchanged. For example, given the 
Buckwalter-encoded MSA sentence ?AlAxwAn 
Almslmwn lm yfwzwA fy AlAntxbAt? the rules pro-
duce ?AlAxwAn Almslmyn mfAzw$ f AlAntxAbAt? 
(????????? ? ?????? ???????? ??????, The Muslim Bro-
therhood did not win the elections). The availabili-
ty of segment-based part-of-speech tags is essential 
since many of the affixes in MSA are ambiguous. 
For example, lm could be either a negative particle 
or a question work, and the word AlAxwAn could 
be either made of two segments (Al+<xwAn, the 
brothers), or three segments (Al+>xw+An, the two 
brothers). 
    We first introduce the transformation rules, and 
show that in many cases it is feasible to transform 
MSA to CEA, although there are cases that require 
much more than POS tags.  We then provide a typ-
ical case in which we utilize the transformed text 
of the Arabic Treebank (Bies and Maamouri, 2003) 
to build a part-of-speech tagger for CEA. The tag-
ger improves the accuracy of POS tagging on au-
thentic Egyptian Arabic by 13% absolute (from 
73.24% to 86.84%) and reduces the percentage of 
out-of-vocabulary words from 28.98% to 16.66%. 
  
  2. MSA to CEA Conversion Rules 
Table 1 shows a sentence in MSA and its CEA 
counterpart. Both can be translated into: ?We did 
not write it for them.? MSA has three words while 
CEA is more synthetic as the preposition and the 
negative particle turn into clitics.  Table 1 illu-
strates the end product of one of the Imperfect 
transformation rules, namely the case where the 
Imperfect Verb is preceded by the negative particle 
lm. 
 
 Arabic Buckwalter 
MSA ??? ?????? ?? lm nktbhA lhn 
CEA ?????????? mktbnhlhm$ 
English We did not write it for them 
Table 1: a sentence in MSA and CEA 
 
Our 103 rules cover nominals (number and case 
affixes), verbs (tense, number, gender, and modali-
ty), pronouns (number and gender), and demon-
strative pronouns (number and gender).  
    The rules also cover certain lexical items as 400 
words in MSA have been converted to their com-
176
mon CEA counterparts.  Examples of lexical con-
versions include ZlAm and Dlmp (darkness), rjl 
and rAjl (man), rjAl and rjAlp (men), and kvyr and 
ktyr (many), where the first word is the MSA ver-
sion and the second is the CEA version.  
   Many of the lexical mappings are ambiguous. 
For example, the word rjl can either mean man or 
leg. When it means man, the CEA form is rAjl, but 
the word for leg is the same in both MSA and 
CEA. While they have different vowel patterns 
(rajul and rijol respectively), the vowel informa-
tion is harder to get correctly than POS tags. The 
problem may arise especially when dealing with 
raw data for which we need to provide POS tags 
(and vowels) so we may be able to convert it to the 
colloquial form. Below, we provide two sample 
rules:  
The imperfect verb is used, inter alia, to express 
the negated past, for which CEA uses the perfect 
verb. What makes things more complicated is that 
CEA treats negative particles and prepositional 
phrases as clitics. An example of this is the word 
mktbthlhm$ (I did not write it for them) in Table 1 
above. It is made of the negative particle m, the 
stem ktb (to write), the object pronoun h, the pre-
position l, the pronoun hm (them) and the negative 
particle $. Figure 1, and the following steps show 
the conversions of lm nktbhA lhm to 
mktbnhAlhm$: 
1. Replace the negative word lm with one of 
the prefixes m, mA or the word mA. 
2. Replace the Imperfect Verb prefix with its 
Perfect Verb suffix counterpart.  For exam-
ple, the IV first person singular subject pre-
fix > turns into t in the PV. 
3. If the verb is followed by a prepositional 
phrase headed by the preposition l that con-
tains a pronominal object, convert the pre-
position to a prepositional clitic. 
4. Transform the dual to plural and the plural 
feminine to plural masculine. 
5. Add the negative suffix $ (or the variant $y, 
which is less probable) 
As alluded to in 1) above, given that colloquial 
orthography is not standardized, many affixes and 
clitics can be written in different ways. For exam-
ple, the word mktbnhlhm$, can be written in 24 
ways. All these forms are legal and possible, as 
attested by their existence in a CEA corpus (the 
Arabic Online Commentary Dataset v1.1), which 
we also use for building a language model later. 
Figure 1: One negated IV form in MSA can generate 24 
(3x2x2x2) possible forms in CEA 
 
MSA possessive pronouns inflect for gender, num-
ber (singular, dual, and plural), and person. In 
CEA, there is no distinction between the dual and 
the plural, and a single pronoun is used for the 
plural feminine and masculine. The three MSA 
forms ktAbhm, ktAbhmA and ktAbhn (their book 
for the masculine plural, the dual, and the feminine 
plural respectively) all collapse to ktAbhm.  
 
Table 2 has examples of some other rules we have 
applied.  We note that the stem, in bold, hardly 
changes, and that the changes mainly affect func-
tion segments. The last example is a lexical rule in 
which the stem has to change. 
 
Rule MSA CEA 
Future swf  yktb Hyktb/hyktb 
Future_NEG ln >ktb m$ hktb/ m$ Hktb 
IV yktbwn byktbw/ bktbw/ bktbwA 
Passive ktb Anktb/ Atktb 
NEG_PREP lys mnhn mmnhm$ 
Lexical trkhmA sAbhm 
Table 2: Examples of Conversion Rules. 
 
3. POS Tagging Egyptian Arabic 
We use the conversion above to build a POS tagger 
for Egyptian Arabic. We follow Mohamed and 
Kuebler (2010) in using whole word tagging, i.e., 
without any word segmentation. We use the Co-
lumbia Arabic Treebank 6-tag tag set: PRT (Par-
ticle), NOM (Nouns, Adjectives, and Adverbs), 
PROP (Proper Nouns), VRB (Verb), VRB-pass 
(Passive Verb), and PNX (Punctuation) (Habash 
and Roth, 2009). For example, the word 
wHnktblhm (and we will write to them, ?????????) 
receives the tag PRT+PRT+VRB+PRT+NOM. 
This results in 58 composite tags, 9 of which occur 
5 times or less in the converted ECA training set. 
177
    We converted two sections of the Arabic Tree-
bank (ATB): p2v3 and p3v2. For all the POS tag-
ging experiments, we use the memory-based POS 
tagger (MBT) (Daelemans et al, 1996) The best 
results, tuned on a dev set,  were obtained, in non-
exhaustive search,  with the Modified Value Dif-
ference Metric as a distance metric and with k  (the 
number of nearest neighbors) = 25. For known 
words, we use the IGTree algorithm and 2 words to 
the left, their POS tags, the focus word and its list 
of possible tags, 1 right context word and its list of 
possible tags as features. For unknown words, we 
use the IB1 algorithm and the word itself, its first 5 
and last 3 characters, 1 left context word and its 
POS tag, and 1 right context word and its list of 
possible tags as features. 
     
3.1. Development and Test Data 
As a development set, we use 100 user-contributed 
comments (2757 words) from the website ma-
srawy.com, which were judged to be highly collo-
quial. The test set contains 192 comments (7092 
words) from the same website with the same crite-
rion. The development and test sets were hand-
annotated with composite tags as illustrated above 
by two native Arabic-speaking students. 
The test and development sets contained spel-
ling errors (mostly run-on words). The most com-
mon of these is the vocative particle yA, which is 
usually attached to following word (e.g. yArAjl, 
(you man, ??????)). It is not clear whether it should 
be treated as a proclitic, since it also occurs as a 
separate word, which is the standard way of writ-
ing. The same holds true for the variation between 
the letters * and z, (? and ? in Arabic) which are 
pronounced exactly the same way in CEA to the 
extent that the substitution may not be considered a 
spelling error. 
 
3.2. Experiments and Results 
We ran five experiments to test the effect of MSA 
to CEA conversion on POS tagging: (a) Standard, 
where we train the tagger on the ATB MSA data, 
(b) 3-gram LM, where for each MSA sentence we 
generate all transformed sentences (see Section 2.1 
and Figure 1) and pick the most probable sentence 
according to a trigram language model built from 
an 11.5 million words of user contributed 
comments.1 This corpus is highly dialectal 
                                                 
1Available from  http://www.cs.jhu.edu/~ozaidan/AOC 
Egyptian Arabic, but like all similar collections, it 
is diglossic and demonstrates a high degree of 
code-switching between MSA and CEA. We use 
the SRILM toolkit (Stolcke, 2002) for language 
modeling and sentence scoring, (c) Random, 
where we choose a random sentence from all the 
correct sentences generated for each MSA 
sentence, (d) Hybrid, where we combine the data 
in a) with the best settings (as measured on the dev 
set) using the converted colloquial data (namely 
experiment c). Hybridization is necessary since 
most Arabic data in blogs and comments are a mix 
of MSA and CEA, and (e) Hybrid + dev, where 
we enrich the Hybrid training set with the dev data.  
  We use the following metrics for evaluation: 
KWA: Known Word Accuracy (%), UWA: 
Unknown Word Accuracy (%), TA: Total Accuracy 
(%), and UW: unknown words (%) in the 
respective set in the respective experiment. Table 
3(a) presents the results on the development set 
while Table 3(b) the results on the test set.  
 
Experiment KWA UWA TA UW 
(a) Standard 92.75 39.68 75.77 31.99 
(b) 3-gram LM 89.12 43.46 76.21 28.29 
(c) Random 92.36 43.51 79.25 26.84 
(d) Hybrid 94.13 52.22 84.87 22.09 
Table 3(a): POS results on the development set.   
 
We notice that randomly selecting a sentence from 
the correct generated sentences yields better results 
than choosing the most probable sentence accord-
ing to a language model. The reason for this may 
be that randomization guarantees more coverage of 
the various forms. We have found that the vocabu-
lary size (the number of unique word types) for the 
training set generated for the Random experiment 
is considerably larger than the vocabulary size for 
the 3-gram LM experiment (55367 unique word 
types in Random versus 51306 in 3-gram LM), 
which results in a drop of 4.6% absolute in the per-
centage of unknown words: 27.31% versus 
22.30%). This drop in the percentage of unknown 
words may indicate that generating all possible 
variations of CEA may be more useful than using a 
language model in general. Even in a CEA corpus 
of 35 million words, one third of the words gener-
ated by the rules are not in the corpus, while many 
178
of these are in both the test set and the develop-
ment set. 
 
Experiment KWA UWA TA UW 
(a) Standard 89.03 40.67 73.24 28.98 
(b) 3-gram LM 84.33 47.70 74.32 27.31 
(c) Random 90.24 48.90 79.67 22.70 
(d) Hybrid 92.22 53.92 83.81 19.45 
(e) Hybrid+dev 94.87 56.46 86.84 16.66 
Table 3(b): POS results on the test set 
 
    We also notice that the conversion alone im-
proves tagging accuracy from 75.77% to 79.25% 
on the development set, and from 73.24% to 
79.67% on the test set. Combining the original 
MSA and the best scoring converted data (Ran-
dom) raises the accuracies to 84.87% and 83.81% 
respectively.  The percentage of unknown words 
drops from 29.98% to 19.45% in the test set when 
we used the hybrid data. The fact that the percen-
tage of unknown words drops further to 16.66% in 
the Hybrid+dev experiment points out the authen-
tic colloquial data contains elements that have not 
been captured using conversion alone.    
 
4. Related Work 
To the best of our knowledge, ours is the first work 
that generates CEA automatically from morpholog-
ically disambiguated MSA, but Habash et al 
(2005) discussed root and pattern morphological 
analysis and generation of Arabic dialects within 
the MAGED morphological analyzer. MAGED 
incorporates the morphology, phonology, and or-
thography of several Arabic dialects. Diab et al 
(2010) worked on the annotation of dialectal Arab-
ic through the COLABA project, and they used the 
(manually) annotated resources to facilitate the 
incorporation of the dialects in Arabic information 
retrieval. 
  Duh and Kirchhoff (2005) successfully designed 
a POS tagger for CEA that used an MSA morpho-
logical analyzer and information gleaned from the 
intersection of several Arabic dialects.  This is dif-
ferent from our approach for which POS tagging is 
only an application.  Our focus is to use any exist-
ing MSA data to generate colloquial Arabic re-
sources that can be used in virtually any NLP task. 
   At a higher level, our work resembles that of 
Kundu and Roth (2011), in which they chose to 
adapt the text rather than the model. While they 
adapted the test set, we do so at the training set 
level. 
 
5. Conclusions and Future Work 
We have a presented a method to convert Modern 
Standard Arabic to Egyptian Colloquial Arabic 
with an example application to the POS tagging 
task. This approach may provide a cheap way to 
leverage MSA data and morphological resources to 
create resources for colloquial Arabic to English 
machine translation, for example. 
     While the rules of conversion were mainly 
morphological in nature, they have proved useful 
in handling colloquial data. However, morphology 
alone is not enough for handling key points of dif-
ference between CEA and MSA. While CEA is 
mainly an SVO language, MSA is mainly VSO, 
and while demonstratives are pre-nominal in MSA, 
they are post-nominal in CEA. These phenomena 
can be handled only through syntactic conversion.  
We expect that converting a dependency-based 
treebank to CEA can account for many of the phe-
nomena part-of-speech tags alone cannot handle 
  We are planning to extend the rules to other lin-
guistic phenomena and dialects, with possible ap-
plications to various NLP tasks for which MSA 
annotated data exist. When no gold standard seg-
ment-based POS tags are available, tools that pro-
duce segment-based annotation can be used, e.g.   
segment-based POS tagging (Mohamed and Kueb-
ler, 2010) or MADA (Habash et al 2009), although 
these are not expected to yield the same results as 
gold standard part-of-speech tags. 
 
Acknowledgements  
This publication was made possible by a NPRP 
grant (NPRP 09-1140-1-177) from the Qatar Na-
tional Research Fund (a member of The Qatar 
Foundation). The statements made herein are sole-
ly the responsibility of the authors.  
    We thank the two native speaker annotators and 
the anonymous reviewers for their instructive and 
enriching feedback. 
 
 
  
179
 References 
    Bies, Ann and Maamouri, Mohamed  (2003). Penn 
Arabic Treebank guidelines. Technical report, LDC, 
University of Pennsylvania. 
    Buckwalter, T. (2002). Arabic Morphological Analyz-
er (AraMorph). Version 1.0. Linguistic Data Consor-
tium, catalog number LDC2002L49 and ISBN 1-58563-
257- 0 
    Daelemans, Walter and van den Bosch, Antal ( 2005). 
Memory Based Language Processing. Cambridge Uni-
versity Press.   
    Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and 
Steven Gillis (1996). MBT: A memory-based part of 
speech tagger-generator. In Eva Ejerhed and Ido Dagan, 
editors, Proceedings of the 4th Workshop on Very Large 
Corpora, pages 14?27, Copenhagen, Denmark. 
   Diab, Mona; Habash, Nizar; Rambow, Owen; Altan-
tawy, Mohamed, and Benajiba, Yassine. COLABA: 
Arabic Dialect Annotation and Processing. LREC 2010. 
    Duh, K. and Kirchhoff, K. (2005). POS Tagging of 
Dialectal Arabic: A Minimally Supervised Approach. 
Proceedings of the ACL Workshop on Computational 
Approaches to Semitic Languages, Ann Arbor, June 
2005. 
    Habash, Nizar; Rambow, Own and Kiraz, George 
(2005). Morphological analysis and generation for 
Arabic dialects. Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages, pages 
17?24, Ann Arbor, June 2005 
    Habash, Nizar and Roth, Ryan. CATiB: The Colum-
bia Arabic Treebank. Proceedings of the ACL-IJCNLP 
2009 Conference Short Papers, pages 221?224, Singa-
pore, 4 August 2009. c 2009 ACL and AFNLP 
    Habash, Nizar, Owen Rambow and Ryan Roth. MA-
DA+TOKAN: A Toolkit for Arabic Tokenization, Dia-
critization, Morphological Disambiguation, POS Tag-
ging, Stemming and Lemmatization. In Proceedings of 
the 2nd International Conference on Arabic Language 
Resources and Tools (MEDAR), Cairo, Egypt, 2009 
   Kundu, Gourab abd Roth, Don (2011). Adapting Text 
instead of the Model: An Open Domain Approach. Pro-
ceedings of the Fifteenth Conference on Computational 
Natural Language Learning, pages 229?237,Portland, 
Oregon, USA, 23?24 June 2011 
    Mohamed, Emad. and Kuebler, Sandra (2010). Is 
Arabic Part of Speech Tagging Feasible Without Word 
Segmentation? Proceedings of HLT-NAACL 2010, Los 
Angeles, CA. 
    Stolcke, A. (2002). SRILM - an extensible language 
modeling toolkit. In Proc. of ICSLP, Denver, Colorado 
180
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?258,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coarse Lexical Semantic Annotation with Supersenses:
An Arabic Case Study
Nathan Schneider? Behrang Mohit? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?Doha, Qatar ?Pittsburgh, PA 15213, USA
{nschneid@cs.,behrang@,ko@cs.,nasmith@cs.}cmu.edu
Abstract
?Lightweight? semantic annotation of text
calls for a simple representation, ideally with-
out requiring a semantic lexicon to achieve
good coverage in the language and domain.
In this paper, we repurpose WordNet?s super-
sense tags for annotation, developing specific
guidelines for nominal expressions and ap-
plying them to Arabic Wikipedia articles in
four topical domains. The resulting corpus
has high coverage and was completed quickly
with reasonable inter-annotator agreement.
1 Introduction
The goal of ?lightweight? semantic annotation of
text, particularly in scenarios with limited resources
and expertise, presents several requirements for a
representation: simplicity; adaptability to new lan-
guages, topics, and genres; and coverage. This
paper describes coarse lexical semantic annotation
of Arabic Wikipedia articles subject to these con-
straints. Traditional lexical semantic representations
are either narrow in scope, like named entities,1 or
make reference to a full-fledged lexicon/ontology,
which may insufficiently cover the language/domain
of interest or require prohibitive expertise and ef-
fort to apply.2 We therefore turn to supersense tags
(SSTs), 40 coarse lexical semantic classes (25 for
nouns, 15 for verbs) originating in WordNet. Previ-
ously these served as groupings of English lexicon
1Some ontologies like those in Sekine et al (2002) and BBN
Identifinder (Bikel et al, 1999) include a large selection of
classes, which tend to be especially relevant to proper names.
2E.g., a WordNet (Fellbaum, 1998) sense annotation effort
reported by Passonneau et al (2010) found considerable inter-
annotator variability for some lexemes; FrameNet (Baker et
al., 1998) is limited in coverage, even for English; and Prop-
Bank (Kingsbury and Palmer, 2002) does not capture semantic
relationships across lexemes. We note that the Omega ontol-
ogy (Philpot et al, 2003) has been used for fine-grained cross-
lingual annotation (Hovy et al, 2006; Dorr et al, 2010).
Q.

J?K

considers
H. A

J?
book
?

	
JJ
k.
Guinness
?A

P?

C?
for-records

?J
?AJ


?? @
the-standard
COMMUNICATION
	
?

@
that

???Ag.
university
	
?@?Q


?? @
Al-Karaouine
ARTIFACT
?


	
?
in
?A
	
?
Fez
H. Q
	??? @
Morocco
LOCATION
?Y

?

@
oldest

???Ag.
university
GROUP
?


	
?
in
??A?? @
the-world
LOCATION
IJ
k
where
??

'
was
A?D?J
?

A

K
established
ACT
?


	
?
in

?
	
J?
year
859 ?


XCJ
?
AD
TIME
.
?The Guinness Book of World Records considers the
University of Al-Karaouine in Fez, Morocco, established
in the year 859 AD, the oldest university in the world.?
Figure 1: A sentence from the article ?Islamic Golden
Age,? with the supersense tagging from one of two anno-
tators. The Arabic is shown left-to-right.
entries, but here we have repurposed them as target
labels for direct human annotation.
Part of the earliest versions of WordNet, the
supersense categories (originally, ?lexicographer
classes?) were intended to partition all English noun
and verb senses into broad groupings, or semantic
fields (Miller, 1990; Fellbaum, 1990). More re-
cently, the task of automatic supersense tagging has
emerged for English (Ciaramita and Johnson, 2003;
Curran, 2005; Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009), as well as for Italian (Picca et al,
2008; Picca et al, 2009; Attardi et al, 2010) and
Chinese (Qiu et al, 2011), languages with WordNets
mapped to English WordNet.3 In principle, we be-
lieve supersenses ought to apply to nouns and verbs
in any language, and need not depend on the avail-
ability of a semantic lexicon.4 In this work we focus
on the noun SSTs, summarized in figure 2 and ap-
plied to an Arabic sentence in figure 1.
SSTs both refine and relate lexical items: they
capture lexical polysemy on the one hand?e.g.,
3Note that work in supersense tagging used text with fine-
grained sense annotations that were then coarsened to SSTs.
4The noun/verb distinction might prove problematic in some
languages.
253
Crusades ?Damascus ? Ibn Tolun Mosque ? Imam Hussein Shrine ? Islamic Golden Age ? Islamic History ?Ummayad Mosque 434s 16,185t 5,859m
Atom ? Enrico Fermi ? Light ? Nuclear power ? Periodic Table ? Physics ? Muhammad al-Razi 777s 18,559t 6,477m
2004 Summer Olympics ?Christiano Ronaldo ?Football ?FIFA World Cup ?Portugal football team ?Rau?l Gonza?les ?Real Madrid 390s 13,716t 5,149m
Computer ? Computer Software ? Internet ? Linux ? Richard Stallman ? Solaris ? X Window System 618s 16,992t 5,754m
Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts
of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239
mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?.
disambiguating PERSON vs. POSSESSION for the
noun principal?and generalize across lexemes on
the other?e.g., principal, teacher, and student can
all be PERSONs. This lumping property might be
expected to give too much latitude to annotators; yet
we find that in practice, it is possible to elicit reason-
able inter-annotator agreement, even for a language
other than English. We encapsulate our interpreta-
tion of the tags in a set of brief guidelines that aims
to be usable by anyone who can read and understand
a text in the target language; our annotators had no
prior expertise in linguistics or linguistic annotation.
Finally, we note that ad hoc categorization
schemes not unlike SSTs have been developed for
purposes ranging from question answering (Li and
Roth, 2002) to animacy hierarchy representation for
corpus linguistics (Zaenen et al, 2004). We believe
the interpretation of the SSTs adopted here can serve
as a single starting point for diverse resource en-
gineering efforts and applications, especially when
fine-grained sense annotation is not feasible.
2 Tagging Conventions
WordNet?s definitions of the supersenses are terse,
and we could find little explicit discussion of the
specific rationales behind each category. Thus,
we have crafted more specific explanations, sum-
marized for nouns in figure 2. English examples
are given, but the guidelines are intended to be
language-neutral. A more systematic breakdown,
formulated as a 43-rule decision list, is included
with the corpus.5 In developing these guidelines
we consulted English WordNet (Fellbaum, 1998)
and SemCor (Miller et al, 1993) for examples and
synset definitions, occasionally making simplifying
decisions where we found distinctions that seemed
esoteric or internally inconsistent. Special cases
(e.g., multiword expressions, anaphora, figurative
5For example, one rule states that all man-made structures
(buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs.
language) are addressed with additional rules.
3 Arabic Wikipedia Annotation
The annotation in this work was on top of a small
corpus of Arabic Wikipedia articles that had al-
ready been annotated for named entities (Mohit et
al., 2012). Here we use two different annotators,
both native speakers of Arabic attending a university
with English as the language of instruction.
Data & procedure. The dataset (table 1) consists of
the main text of 28 articles selected from the topical
domains of history, sports, science, and technology.
The annotation task was to identify and categorize
mentions, i.e., occurrences of terms belonging to
noun supersenses. Working in a custom, browser-
based interface, annotators were to tag each relevant
token with a supersense category by selecting the to-
ken and typing a tag symbol. Any token could be
marked as continuing a multiword unit by typing <.
If the annotator was ambivalent about a token they
were to mark it with the ? symbol. Sentences were
pre-tagged with suggestions where possible.6 Anno-
tators noted obvious errors in sentence splitting and
grammar so ill-formed sentences could be excluded.
Training. Over several months, annotators alter-
nately annotated sentences from 2 designated arti-
cles of each domain, and reviewed the annotations
for consistency. All tagging conventions were deve-
loped collaboratively by the author(s) and annotators
during this period, informed by points of confusion
and disagreement. WordNet and SemCor were con-
sulted as part of developing the guidelines, but not
during annotation itself so as to avoid complicating
the annotation process or overfitting to WordNet?s
idiosyncracies. The training phase ended once inter-
annotator mention F1 had reached 75%.
6Suggestions came from the previous named entity annota-
tion of PERSONs, organizations (GROUP), and LOCATIONs, as
well as heuristic lookup in lexical resources?Arabic WordNet
entries (Elkateb et al, 2006) mapped to English WordNet, and
named entities in OntoNotes (Hovy et al, 2006).
254
O NATURAL OBJECT natural feature or nonliving object in
nature barrier reef nest neutron star
planet sky fishpond metamorphic rock Mediterranean cave
stepping stone boulder Orion ember universe
A ARTIFACT man-made structures and objects bridge
restaurant bedroom stage cabinet toaster antidote aspirin
L LOCATION any name of a geopolitical entity, as well as
other nouns functioning as locations or regions
Cote d?Ivoire New York City downtown stage left India
Newark interior airspace
P PERSON humans or personified beings; names of social
groups (ethnic, political, etc.) that can refer to an individ-
ual in the singular Persian deity glasscutter mother
kibbutznik firstborn worshiper Roosevelt Arab consumer
appellant guardsman Muslim American communist
G GROUP groupings of people or objects, including: orga-
nizations/institutions; followers of social movements
collection flock army meeting clergy Mennonite Church
trumpet section health profession peasantry People?s Party
U.S. State Department University of California population
consulting firm communism Islam (= set of Muslims)
$ SUBSTANCE a material or substance krypton mocha
atom hydrochloric acid aluminum sand cardboard DNA
H POSSESSION term for an entity involved in ownership or
payment birthday present tax shelter money loan
T TIME a temporal point, period, amount, or measurement
10 seconds day Eastern Time leap year 2nd millenium BC
2011 (= year) velocity frequency runtime latency/delay
middle age half life basketball season words per minute
curfew industrial revolution instant/moment August
= RELATION relations between entities or quantities
ratio scale reverse personal relation exponential function
angular position unconnectedness transitivity
Q QUANTITY quantities and units of measure, including
cardinal numbers and fractional amounts 7 cm 1.8 million
12 percent/12% volume (= spatial extent) volt real number
square root digit 90 degrees handful ounce half
F FEELING subjective emotions indifference wonder
murderousness grudge desperation astonishment suffering
M MOTIVE an abstract external force that causes someone
to intend to do something reason incentive
C COMMUNICATION information encoding and transmis-
sion, except in the sense of a physical object
grave accent Book of Common Prayer alphabet
Cree language onomatopoeia reference concert hotel bill
broadcast television program discussion contract proposal
equation denial sarcasm concerto software
? COGNITION aspects of mind/thought/knowledge/belief/
perception; techniques and abilities; fields of academic
study; social or philosophical movements referring to the
system of beliefs Platonism hypothesis
logic biomedical science necromancy hierarchical structure
democracy innovativeness vocational program woodcraft
reference visual image Islam (= Islamic belief system) dream
scientific method consciousness puzzlement skepticism
reasoning design intuition inspiration muscle memory skill
aptitude/talent method sense of touch awareness
S STATE stable states of affairs; diseases and their symp-
toms symptom reprieve potency
poverty altitude sickness tumor fever measles bankruptcy
infamy opulence hunger opportunity darkness (= lack of light)
@ ATTRIBUTE characteristics of people/objects that can be
judged resilience buxomness virtue immateriality
admissibility coincidence valence sophistication simplicity
temperature (= degree of hotness) darkness (= dark coloring)
! ACT things people do or cause to happen; learned pro-
fessions meddling malpractice faith healing dismount
carnival football game acquisition engineering (= profession)
E EVENT things that happens at a given place and time
bomb blast ordeal miracle upheaval accident tide
R PROCESS a sustained phenomenon or one marked by
gradual changes through a series of states
oscillation distillation overheating aging accretion/growth
extinction evaporation
X PHENOMENON a physical force or something that hap-
pens/occurs electricity suction tailwind tornado effect
+ SHAPE two and three dimensional shapes
D FOOD things used as food or drink
B BODY human body parts, excluding diseases and their
symptoms
Y PLANT a plant or fungus
N ANIMAL non-human, non-plant life
Science chemicals, molecules, atoms, and subatomic
particles are tagged as SUBSTANCE
Sports championships/tournaments are EVENTs
(Information) Technology Software names, kinds, and
components are tagged as COMMUNICATION (e.g. kernel,
version, distribution, environment). A connection is a RE-
LATION; project, support, and a configuration are tagged
as COGNITION; development and collaboration are ACTs.
Arabic conventions Masdar constructions (verbal
nouns) are treated as nouns. Anaphora are not tagged.
Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME,
short description, and examples. Some examples and longer descriptions have been omitted due to space constraints.
Below: A few domain- and language-specific elaborations of the general guidelines.
255
Figure 3: Distribution of supersense mentions by
domain (left), and counts for tags occurring over
800 times (below). (Counts are of the union of the
annotators? choices, even when they disagree.)
tag num tag num
ACT (!) 3473 LOCATION (G) 1583
COMMUNICATION (C) 3007 GROUP (L) 1501
PERSON (P) 2650 TIME (T) 1407
ARTIFACT (A) 2164 SUBSTANCE ($) 1291
COGNITION (?) 1672 QUANTITY (Q) 1022
Main annotation. After training, the two annota-
tors proceeded on a per-document basis: first they
worked together to annotate several sentences from
the beginning of the article, then each was inde-
pendently assigned about half of the remaining sen-
tences (typically with 5?10 shared to measure agree-
ment). Throughout the process, annotators were en-
couraged to discuss points of confusion with each
other, but each sentence was annotated in its entirety
and never revisited. Annotation of 28 articles re-
quired approximately 100 annotator-hours. Articles
used in pilot rounds were re-annotated from scratch.
Analysis. Figure 3 shows the distribution of SSTs in
the corpus. Some of the most concrete tags?BODY,
ANIMAL, PLANT, NATURAL OBJECT, and FOOD?
were barely present, but would likely be frequent
in life sciences domains. Others, such as MOTIVE,
POSSESSION, and SHAPE, are limited in scope.
To measure inter-annotator agreement, 87 sen-
tences (2,774 tokens) distributed across 19 of the ar-
ticles (not including those used in pilot rounds) were
annotated independently by each annotator. Inter-
annotator mention F1 (counting agreement over en-
tire mentions and their labels) was 70%. Excluding
the 1,397 tokens left blank by both annotators, the
token-level agreement rate was 71%, with Cohen?s
? = 0.69, and token-level F1 was 83%.7
We also measured agreement on a tag-by-tag ba-
sis. For 8 of the 10 most frequent SSTs (fig-
ure 3), inter-annotator mention F1 ranged from 73%
to 80%. The two exceptions were QUANTITY at
63%, and COGNITION (probably the most heteroge-
neous category) at 49%. An examination of the con-
fusion matrix reveals four pairs of supersense cate-
gories that tended to provoke the most disagreement:
COMMUNICATION/COGNITION, ACT/COGNITION,
ACT/PROCESS, and ARTIFACT/COMMUNICATION.
7Token-level measures consider both the supersense label
and whether it begins or continues the mention.
The last is exhibited for the first mention in figure 1,
where one annotator chose ARTIFACT (referring to
the physical book) while the other chose COMMU-
NICATION (the content). Also in that sentence, an-
notators disagreed on the second use of university
(ARTIFACT vs. GROUP). As with any sense anno-
tation effort, some disagreements due to legitimate
ambiguity and different interpretations of the tags?
especially the broadest ones?are unavoidable.
A ?soft? agreement measure (counting as matches
any two mentions with the same label and at least
one token in common) gives an F1 of 79%, show-
ing that boundary decisions account for a major por-
tion of the disagreement. E.g., the city Fez, Mo-
rocco (figure 1) was tagged as a single LOCATION
by one annotator and as two by the other. Further
examples include the technical term ?thin client?,
for which one annotator omitted the adjective; and
?World Cup Football Championship?, where one an-
notator tagged the entire phrase as an EVENT while
the other tagged ?football? as a separate ACT.
4 Conclusion
We have codified supersense tags as a simple an-
notation scheme for coarse lexical semantics, and
have shown that supersense annotation of Ara-
bic Wikipedia can be rapid, reliable, and robust
(about half the tokens in our data are covered
by a nominal supersense). Our tagging guide-
lines and corpus are available for download at
http://www.ark.cs.cmu.edu/ArabicSST/.
Acknowledgments
We thank Nourhen Feki and Sarah Mustafa for assistance
with annotation, as well as Emad Mohamed, CMU ARK
members, and anonymous reviewers for their comments.
This publication was made possible by grant NPRP-08-
485-1-083 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements made
herein are solely the responsibility of the authors.
256
References
Giuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,
Alessandro Lenci, Simonetta Montemagni, and Maria
Simi. 2010. A resource and tool for super-sense
tagging of Italian texts. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics (COLING-
ACL ?98), pages 86?90, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages 168?
175, Sapporo, Japan, July.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June.
Bonnie J. Dorr, Rebecca J. Passonneau, David Farwell,
Rebecca Green, Nizar Habash, Stephen Helmreich,
Eduard Hovy, Lori Levin, Keith J. Miller, Teruko
Mitamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text corpora:
a new framework for annotation and evaluation. Nat-
ural Language Engineering, 16(03):197?243.
Sabri Elkateb, William Black, Horacio Rodr??guez, Musa
Alkhalifa, Piek Vossen, Adam Pease, and Christiane
Fellbaum. 2006. Building a WordNet for Arabic.
In Proceedings of The Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 29?34, Genoa, Italy.
Christiane Fellbaum. 1990. English verbs as a semantic
net. International Journal of Lexicography, 3(4):278?
301, December.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL (HLT-
NAACL), pages 57?60, New York City, USA, June.
Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Las Palmas, Canary Islands,
May.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1?7, Taipei, Taiwan, August. Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology (HLT ?93), HLT ?93, pages 303?308,
Plainsboro, NJ, USA, March. Association for Compu-
tational Linguistics.
George A. Miller. 1990. Nouns in WordNet: a lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264, December.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A. Smith. 2012.
Recall-oriented learning of named entities in Arabic
Wikipedia. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 162?173, Avi-
gnon, France, April. Association for Computational
Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
semantic constraints for estimating supersenses with
CRFs. In Proceedings of the Ninth SIAM International
Conference on Data Mining, pages 485?496, Sparks,
Nevada, USA, May. Society for Industrial and Applied
Mathematics.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
257
Andrew G. Philpot, Michael Fleischman, and Eduard H.
Hovy. 2003. Semi-automatic construction of a general
purpose ontology. In Proceedings of the International
Lisp Conference, New York, NY, USA, October.
Davide Picca, Alfio Massimiliano Gliozzo, and Mas-
similiano Ciaramita. 2008. Supersense Tagger
for Italian. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
Piperidis, and Daniel Tapias, editors, Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), pages 2386?2390, Marrakech, Mo-
rocco, May. European Language Resources Associa-
tion (ELRA).
Davide Picca, Alfio Massimiliano Gliozzo, and Simone
Campora. 2009. Bridging languages by SuperSense
entity tagging. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 136?142, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Likun Qiu, Yunfang Wu, Yanqiu Shao, and Alexander
Gelbukh. 2011. Combining contextual and struc-
tural information for supersense tagging of Chinese
unknown words. In Computational Linguistics and In-
telligent Text Processing: Proceedings of the 12th In-
ternational Conference on Computational Linguistics
and Intelligent Text Processing (CICLing?11), volume
6608 of Lecture Notes in Computer Science, pages 15?
28. Springer, Berlin.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC-02), Las Pal-
mas, Canary Islands, May.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
258
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 719?724,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Typesetting for Improved Readability using
Lexical and Syntactic Information
Ahmed Salama
ahmedsaa@qatar.cmu.edu
Kemal Oflazer
Carnegie Mellon University ? Qatar
Doha, Qatar
ko@cs.cmu.edu
Susan Hagan
hagan@cmu.edu
Abstract
We present results from our study of which
uses syntactically and semantically moti-
vated information to group segments of
sentences into unbreakable units for the
purpose of typesetting those sentences in
a region of a fixed width, using an other-
wise standard dynamic programming line
breaking algorithm, to minimize ragged-
ness. In addition to a rule-based base-
line segmenter, we use a very modest size
text, manually annotated with positions of
breaks, to train a maximum entropy clas-
sifier, relying on an extensive set of lexi-
cal and syntactic features, which can then
predict whether or not to break after a cer-
tain word position in a sentence. We also
use a simple genetic algorithm to search
for a subset of the features optimizing F1,
to arrive at a set of features that deliv-
ers 89.2% Precision, 90.2% Recall (89.7%
F1) on a test set, improving the rule-based
baseline by about 11 points and the classi-
fier trained on all features by about 1 point
in F1.
1 Introduction and Motivation
Current best practice in typography focuses on
several interrelated factors (Humar et al, 2008;
Tinkel, 1996). These factors include typeface se-
lection, the color of the type and its contrast with
the background, the size of the type, the length of
the lines of type in the body of the text, the media
in which the type will live, the distance between
each line of type, and the appearance of the jus-
tified or ragged right side edge of the paragraphs,
which should maintain either the appearance of a
straight line on both sides of the block of type (jus-
tified) or create a gentle wave on the ragged right
side edge.
This paper addresses one aspect of current ?best
practice,? concerning the alignment of text in a
paragraph. While current practice values that gen-
tle ?wave,? which puts the focus on the elegant
look of the overall paragraph, it does so at the
expense of meaning-making features. Meaning-
making features enable typesetting to maintain the
integrity of phrases within sentences, giving those
interests equal consideration with the overall look
of the paragraph. Figure 1 (a) shows a text frag-
ment typeset without any regard to natural breaks
while (b) shows an example of a typesetting that
we would like to get, where many natural breaks
are respected.
While current practice works well enough for
native speakers, fluency problems for non-native
speakers lead to uncertainty when the beginning
and end of English phrases are interrupted by the
need to move to the next line of the text before
completing the phrase. This pause is a poten-
tial problem for readers because they try to inter-
pret content words, relate them to their referents
and anticipate the role of the next word, as they
encounter them in the text (Just and Carpenter,
1980). While incorrect anticipation might not be
problematic for native speakers, who can quickly
re-adjust, non-native speakers may find inaccu-
rate anticipation more troublesome. This prob-
lem could be more significant because English
as a second language (ESL) readers are engaged
not only in understanding a foreign language, but
also in processing the ?anticipated text? as they
read a partial phrase, and move to the next line
in the text, only to discover that they anticipated
meaning incorrectly. Even native speakers with
less skill may experience difficulty comprehend-
ing text and work with young readers suggests that
?[c]omprehension difficulties may be localized at
points of high processing demands whether from
syntax or other sources? (Perfetti et al, 2005). As
ESL readers process a partial phrase, and move to
719
the next line in the text, instances of incorrectly
anticipated meaning would logically increase pro-
cessing demands to a greater degree. Additionally,
as readers make meaning, we assume that they
don?t parse their thoughts using the same phrasal
divisions ?needed to diagram a sentence.? Our per-
spective not only relies on the immediacy assump-
tion, but also develops as an outgrowth of other
ways that we make meaning outside of the form or
function rules of grammar. Specifically, Halliday
and Hasan (1976) found that rules of grammar do
not explain how cohesive principals engage read-
ers in meaning making across sentences. In order
to make meaning across sentences, readers must
be able to refer anaphorically backward to the pre-
vious sentence, and cataphorically forward to the
next sentence. Along similar lines, readers of a
single sentence assume that transitive verbs will
include a direct object, and will therefore specu-
late about what that object might be, and some-
times get it wrong.
Thus proper typesetting of a segment of text
must explore ways to help readers avoid incor-
rect anticipation, while also considering those mo-
ments in the text where readers tend to pause in
order to integrate the meaning of a phrase. Those
decisions depend on the context. A phrasal break
between a one-word subject and its verb tends to
be more unattractive, because the reader does not
have to make sense of relationships between the
noun/subject and related adjectives before moving
on to the verb. In this case, the reader will be more
likely to anticipate the verb to come. However,
a break between a subject preceded by multiple
adjectives and its verb is likely to be more use-
ful to a reader (if not ideal), because the relation-
ships between the noun and its related adjectives
are more likely to have thematic importance lead-
ing to longer gaze time on the relevant words in
the subject phrase (Just and Carpenter, 1980).
We are not aware of any prior work for bring-
ing computational linguistic techniques to bear on
this problem. A relatively recent study (Levasseur
et al, 2006) that accounted only for breaks at
commas and ends of sentences, found that even
those breaks improved reading fluency. While the
participants in that study were younger (7 to 9+
years old), the study is relevant because the chal-
lenges those young participants face, are faced
again when readers of any age encounter new and
complicated texts that present words they do not
know, and ideas they have never considered.
On the other hand, there is ample work on the
basic algorithm to place a sequence of words in a
typesetting area with a certain width, commonly
known as the optimal line breaking problem (e.g.,
Plass (1981), Knuth and Plass (1981)). This prob-
lem is quite well-understood and basic variants are
usually studied as an elementary example applica-
tion of dynamic programming.
In this paper we explore the problem of learn-
ing where to break sentences in order to avoid the
problems discussed above. Once such unbreak-
able segments are identified, a simple application
of the dynamic programming algorithm for opti-
mal line breaking, using unbreakable segments as
?words?, easily typesets the text to a given width
area.
2 Text Breaks
The rationale for content breaks is linked to our in-
terest in preventing inaccurate anticipation, which
is based on the immediacy assumption. The imme-
diacy assumption (Just and Carpenter, 1980) con-
siders, among other things, the reader?s interest in
trying to relate content words to their referents as
soon as possible. Prior context also encourages
the reader to anticipate a particular role or case
for the next word, such as agent or the manner
in which something is done.Therefore, in defin-
ing our breaks, we consider not only the need to
maintain the syntactic integrity of phrases, such
as the prepositional phrase, but also the semantic
integrity across syntactical divisions. For exam-
ple, semantic integrity is important when transitive
verbs anticipate direct objects. Strictly speaking,
we define a bad break as one that will cause (i)
unintended anaphoric collocation, (ii) unintended
cataphoric collocation, or (iii) incorrect anticipa-
tion.
Using these broad constraints, we derived a set
of about 30 rules that define acceptable and non-
acceptable breaks, with exceptions based on con-
text and other special cases. Some of the rules are
very simple and are only related to the word posi-
tion in the sentence:
? Break at the end of a sentence.
? Keep the first and last words of a sentence
with the rest of it.
The rest of the rule set are more complex and de-
pend on the structure of the sentence in question,
720
sanctions and UN charges of gross rights abuses. Military tensions on the
Korean peninsula have risen to their highest level for years, with the
communist state under the youthful Kim threatening nuclear war in response
to UN sanctions imposed after its third atomic test last month. It has also
(a) Text with standard typesetting
from US sanctions and UN charges of gross rights abuses. Military tensions
on the Korean peninsula have risen to their highest level for years,
with the communist state under the youthful Kim threatening nuclear war
in response to UN sanctions imposed after its third atomic test last month.
(b) Text with syntax-directed typesetting
Figure 1: Short fragment of text with standard typesetting (a) and with syntax and semantics motivated
typesetting (b), both in a 75 character width.
e.g.:
? Keep a single word subject with the verb.
? Keep an appositive phrase with the noun it
renames.
? Do not break inside a prepositional phrase.
? Keep marooned prepositions with the word
they modify.
? Keep the verb, the object and the preposition
together in a phrasal verb phrase.
? Keep a gerund clause with its adverbial com-
plement.
There are exceptions to these rules in certain cases
such as overly long phrases.
3 Experimental Setup
Our data set consists of a modest set of 150 sen-
tences (3918 tokens) selected from four different
documents and manually annotated by a human
expert relying on the 30 or so rules. The annota-
tion consists of marking after each token whether
one is allowed to break at that position or not.1
We developed three systems for predicting
breaks: a rule-based baseline system, a maximum-
entropy classifier that learns to classify breaks us-
ing about 100 lexical, syntactic and collocational
features, and a maximum entropy classifier that
uses a subset of these features selected by a sim-
ple genetic algorithm in a hill-climbing fashion.
We evaluated our classifiers intrinsically using the
usual measures:
1We expect to make our annotated data available upon the
publication of the paper.
? Precision: Percentage of the breaks posited
that were actually correct breaks in the gold-
standard hand-annotated data. It is possible
to get 100% precision by putting a single
break at the end.
? Recall: Percentage of the actual breaks cor-
rectly posited. It is possible to get 100% re-
call by positing a break after each token.
? F1: The geometric mean of precision and re-
call divided by their average.
It should be noted that when a text is typeset into
an area of width of a certain number of characters,
an erroneous break need not necessarily lead to an
actual break in the final output, that is an error may
not be too bad. On the other hand, a missed break
while not hurting the readability of the text may
actually lead to a long segment that may eventu-
ally worsen raggedness in the final typesetting.
Baseline Classifier We implemented a subset of
the rules (those that rely only on lexical and part-
of-speech information), as a baseline rule-based
break classifier. The baseline classifier avoids
breaks:
? after the first word in a sentence, quote or
parentheses,
? before the last word in a sentence, quote or
parentheses, and
? between a punctuation mark following a
word or between two consecutive punctua-
tion marks.
It posits breaks (i) before a word following a
punctuation, and (ii) before prepositions, auxil-
iary verbs, coordinating conjunctions, subordinate
conjunctions, relative pronouns, relative adverbs,
conjunctive adverbs, and correlative conjunctions.
721
Maximum Entropy Classifier We used the
CRF++ Tool2 but with the option to run it only
as a maximum entropy classifier (Berger et al,
1996), to train a classifier. We used a large set
of about 100 features grouped into the following
categories:
? Lexical features: These features include the
token and the POS tag for the previous, cur-
rent and the next word. We also encode
whether the word is part of a compound noun
or a verb, or is an adjective that subcatego-
rizes a specific preposition in WordNet, (e.g.,
familiar with).
? Constituency structure features: These are
unlexicalized features that take into account
in the parse tree, for a word and its previous
and next words, the labels of the parent, the
grandparent and their siblings, and number of
siblings they have. We also consider the label
of the closest common ancestor for a word
and its next word.
? Dependency structure features: These are un-
lexicalized features that essentially capture
the number of dependency relation links that
cross-over a given word boundary. The moti-
vation for these comes from the desire to limit
the amount of information that would need to
be carried over that boundary, assuming this
would be captured by the number of depen-
dency links over the break point.
? Baseline feature: This feature reflects
whether the rule-based baseline break classi-
fier posits a break at this point or not.
We use the following tools to process the sen-
tences to extract some of these features:
? Stanford constituency and dependency
parsers, (De Marneffe et al, 2006; Klein and
Manning, 2002; Klein and Manning, 2003),
? lemmatization tool in NLTK (Bird, 2006),
? WordNet for compound nouns and verbs
(Fellbaum, 1998).
2Available at http://crfpp.googlecode.com/
svn/trunk/doc/index.html.
Baseline ME-All ME-GA
Precision 77.9 87.3 89.2
Recall 80.4 90.2 90.2
F1 79.1 88.8 89.7
Table 1: Results from Baseline and Maximum En-
tropy break classifiers
Maximum Entropy Classifier with GA Feature
Selection We used a genetic algorithm on a de-
velopment data set, to select a subset of the fea-
tures above. Basically, we start with a randomly
selected set of features and through mutation and
crossover try to obtain feature combinations that
perform better over the development set in terms
of F1 score. After a few hundred generations of
this kind of hill-climbing, we get a subset of fea-
tures that perform the best.
4 Results
Our current evaluation is only intrinsic in that we
measure our performance in getting the break and
no-break points correctly in a test set. The results
are shown in Table 1. The column ME-All shows
the results for a maximum entropy classifier us-
ing all the features and the column ME-GA shows
the results for a maximum entropy classifier using
about 50 of the about 100 features available, as se-
lected by the genetic algorithm.
Our best system delivers 89.2% precision and
90.2% recall (with 89.7% F1), improving the rule-
based baseline by about 11 points and the classifier
trained on all features by about 1 point in F1.
After processing our test set with the ME-GA
classifier, we can feed the segments into a stan-
dard word-wrapping dynamic programming algo-
rithm (along with a maximum width) and obtain a
typeset version with minimum raggedness on the
right margin. This algorithm is fast enough to use
even dynamically when resizing a window if the
text is displayed in a browser on a screen. Fig-
ure 1 (b) displays an example of a small fragment
of text typeset using the output of our best break
classifier. One can immediately note that this type-
setting has more raggedness overall, but avoids the
bad breaks in (a). We are currently in the process
of designing a series of experiments for extrinsic
evaluation to determine if such typeset text helps
comprehension for secondary language learners.
722
4.1 Error Analysis
An analysis of the errors our best classifier makes
(which may or may not be translated into an actual
error in the final typesetting) shows that the major-
ity of the errors basically can be categorized into
the following groups:
? Incorrect breaks posited for multiword collo-
cations (e.g., act* of war,3 rule* of law, far
ahead* of, raining cats* and dogs, etc.)
? Missed breaks after a verb (e.g., calls | an act
of war, proceeded to | implement, etc.)
? Missed breaks before or after prepositions or
adverbials (e.g., the day after | the world re-
alized, every kind | of interference)
We expect to overcome such cases by increasing
our training data size significantly by using our
classifier to break new texts and then have a hu-
man annotator to manually correct the breaks.
5 Conclusions and Future Work
We have used syntactically motivated information
to help in typesetting text to facilitate better under-
standing of English text especially by secondary
language learners, by avoiding breaks which may
cause unnecessary anticipation errors. We have
cast this as a classification problem to indicate
whether to break after a certain word or not, by
taking into account a variety of features. Our best
system maximum entropy framework uses about
50 such features, which were selected using a ge-
netic algorithm and performs significantly better
than a rule-based break classifier and better than a
maximum entropy classifier that uses all available
features.
We are currently working on extending this
work in two main directions: We are designing
a set of experiments to extrinsically test whether
typesetting by our system improves reading ease
and comprehension. We are also looking into a
break labeling scheme that is not binary but based
on a notion of ?badness? ? perhaps quantized into
3-4 grades, that would allow flexibility between
preventing bad breaks and minimizing raggedness.
For instance, breaking a noun-phrase right after an
initial the may be considered very bad. On the
other hand, although it is desirable to keep an ob-
ject NP together with the preceding transitive verb,
3* indicates a spurious incorrect break, | indicates a
missed break.
breaking before the object NP, could be OK, if not
doing so causes an inordinate amount of ragged-
ness. Then the final typesetting stage can optimize
a combination of raggedness and the total ?bad-
ness? of all the breaks it posits.
Acknowledgements
This publication was made possible by grant
NPRP-09-873-1-129 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
Susan Hagan acknowledges the generous support
of the Qatar Foundation through Carnegie Mellon
University?s Seed Research program. The state-
ments made herein are solely the responsibility
of this author(s), and not necessarily those of the
Qatar Foundation.
References
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the COLING/ACL, pages
69?72. Association for Computational Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. The MIT Press.
M. A. K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman, London.
I. Humar, M. Gradisar, and T. Turk. 2008. The impact
of color combinations on the legibility of a web page
text presented on crt displays. International Journal
of Industrial Ergonomics, 38(11-12):885?899.
Marcel A. Just and Patricia A. Carpenter. 1980. A the-
ory of reading: From eye fixations to comprehen-
sion. Psychological Review, 87:329?354.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15(2003):3?10.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
723
Donald E Knuth and Michael F. Plass. 1981. Break-
ing paragraphs into lines. Software: Practice and
Experience, 11(11):1119?1184.
Valerie Marciarille Levasseur, Paul Macaruso,
Laura Conway Palumbo, and Donald Shankweiler.
2006. Syntactically cued text facilitates oral
reading fluency in developing readers. Applied
Psycholinguistics, 27(3):423?445.
C. A. Perfetti, N. Landi, and J. Oakhill. 2005. The ac-
quisition of reading comprehension skill. In M. J.
Snowling and C. Hulme, editors, The science of
reading: A handbook, pages 227?247. Blackwell,
Oxford.
Michael Frederick Plass. 1981. Optimal Pagina-
tion Techniques for Automatic Typesetting Systems.
Ph.D. thesis, Stanford University.
K. Tinkel. 1996. Taking it in: What makes type easier
to read. Adobe Magazine, pages 40?50.
724
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 494?502,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Multi-rate HMMs for Word Alignment
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
We apply multi-rate HMMs, a tree struc-
tured HMM model, to the word-alignment
problem. Multi-rate HMMs allow us to
model reordering at both the morpheme
level and the word level in a hierarchical
fashion. This approach leads to better ma-
chine translation results than a morpheme-
aware model that does not explicitly model
morpheme reordering.
1 Introduction
We present an HMM-based word-alignment
model that addresses transitions between mor-
pheme positions and word positions simultane-
ously. Our model is an instance of a multi-scale
HMM, a widely used method for modeling dif-
ferent levels of a hierarchical stochastic process.
In multi-scale modeling of language, the deepest
level of the hierarchy may consist of the phoneme
sequence, and going up in the hierarchy, the next
level may consist of the syllable sequence, and
then the word sequence, the phrase sequence, and
so on. By the same token, in the hierarchical word-
alignment model we present here, the lower level
consists of the morpheme sequence and the higher
level the word sequence.
Multi-scale HMMs have a natural application in
language processing due to the hierarchical nature
of linguistic structures. They have been used for
modeling text and handwriting (Fine et al, 1998),
in signal processing (Willsky, 2002), knowledge
extraction (Skounakis et al, 2003), as well as in
other fields of AI such as vision (Li et al, 2006;
Luettgen et al, 1993) and robotics (Theocharous
et al, 2001). The model we propose here is most
similar to multi-rate HMMs (C?etin et al, 2007),
which were applied to a classification problem in
industrial machine tool wear.
The vast majority of languages exhibit morphol-
ogy to some extent, leading to various efforts in
machine translation research to include morphol-
ogy in translation models (Al-Onaizan et al, 1999;
Niessen and Ney, 2000; C?mejrek et al, 2003;
Lee, 2004; Chung and Gildea, 2009; Yeniterzi and
Oflazer, 2010). For the word-alignment problem,
Goldwater and McClosky (2005) and Eyigo?z et al
(2013) suggested word alignment models that ad-
dress morphology directly.
Eyigo?z et al (2013) introduced two-level align-
ment models (TAM), which adopt a hierarchi-
cal representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment. TAMs jointly induce word
and morpheme alignments using an EM algorithm.
TAMs can align rarely occurring words through
their frequently occurring morphemes. In other
words, they use morpheme probabilities to smooth
rare word probabilities.
Eyigo?z et al (2013) introduced TAM 1, which is
analogous to IBM Model 1, in that the first level is
a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. By introducing
distortion probabilities at the word level, Eyigo?z
et al (2013) defined the HMM extension of TAM
1, the TAM-HMM. TAM-HMM was shown to
be superior to its single-level counterpart, i.e., the
HMM-based word alignment model of Vogel et al
(1996).
The alignment example in Figure 1 shows a
Turkish word aligned to an English phrase. The
morphemes of the Turkish word are aligned to
the English words. As the example shows, mor-
phologically rich languages exhibit complex re-
ordering phenomena at the morpheme level, which
is left unutilized in TAM-HMMs. In this paper,
we add morpheme sequence modeling to TAMs
to capture morpheme level distortions. The ex-
ample also shows that the Turkish morpheme or-
494
Figure 1: Turkish word aligned to an English
phrase.
der is the reverse of the English word order. Be-
cause this pattern spans several English words, it
can only be captured by modeling morpheme re-
ordering across word boundaries. We chose multi-
rate HMMs over other hierarchical HMM mod-
els because multi-rate HMMs allow morpheme se-
quence modeling across words over the entire sen-
tence.
It is possible to model the morpheme sequence
by treating morphemes as words: segmenting the
words into morphemes, and using word-based
word alignment models on the segmented data.
Eyigo?z et al (2013) showed that TAM-HMM per-
forms better than treating morphemes as words.
Since the multi-rate HMM allows both word
and morpheme sequence modeling, it is a gener-
alization of TAM-HMM, which allows only word
sequence modeling. TAM-HMM in turn is a gen-
eralization of the model suggested by Goldwater
and McClosky (2005) and TAM 1. Our results
show that multi-rate HMMs are superior to TAM-
HMMs. Therefore, multi-rate HMMs are the best
two-level alignment models proposed so far.
2 Two-level Alignment Model (TAM)
The two-level alignment model (TAM) takes the
approach of assigning probabilities to both word-
to-word translations and morpheme-to-morpheme
translations simultaneously, allowing morpheme-
level probabilities to guide alignment for rare word
pairs. TAM is based on a concept of alignment
defined at both the word and morpheme levels.
2.1 Morpheme Alignment
A word alignment aw is a function mapping a set
of word positions in a target language sentence e
to a set of word positions in a source language sen-
tence f , as exemplified in Figure 2. A morpheme
alignment am is a function mapping a set of mor-
pheme positions in a target language sentence to
a set of morpheme positions in a source language
sentence. A morpheme position is a pair of inte-
gers (j, k), which defines a word position j and a
relative morpheme position k in the word at posi-
tion j, as shown in Figure 3. The word and mor-
pheme alignments below are depicted in Figures 2
and 3.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the
following conditions: If the morpheme alignment
am maps a morpheme of e to a morpheme of f ,
then the word alignment aw maps e to f . If the
word alignment aw maps e to f , then the mor-
pheme alignment am maps at least one morpheme
of e to a morpheme of f . If the word align-
ment aw maps e to null, then all of its morphemes
are mapped to null. Figure 3 shows a morpheme
alignment that is compatible with, i.e., restricted
by, the word alignment in Figure 2. The smaller
boxes embedded inside the main box in Figure 3
depict the embedding of the morpheme level in-
side the word level in two-level alignment models
(TAM).
2.2 TAM 1
We call TAM without sequence modeling TAM 1,
because it defines an embedding of IBM Model 1
(Brown et al, 1993) for morphemes inside IBM
Model 1 for words. In TAM 1, p(e|f), the prob-
ability of translating the sentence f into e is com-
puted by summing over all possible word align-
ments and all possible morpheme alignments that
are compatible with a given word alignment aw:
Word Morpheme
Rw
|e|?
j=1
|f |?
i=0
?
?t(ej |fi) Rm
|ej |?
k=1
|fi|?
n=0
t(ekj |fni )
?
?
(1)
where fni is the nth morpheme of the word at po-
sition i. The probability of translating the word fi
into the word ej is computed by summing over all
possible morpheme alignments between the mor-
phemes of ej and fi. Rw substitutes P (le|lf )(lf+1)le for
easy readability.1 Rm is equivalent to Rw except
1le = |e| is the number of words in sentence e and lf =
|f |.
495
Figure 2: Word alignment Figure 3: Morpheme alignment
for the fact that its domain is not the set of sen-
tences but the set of words. The length of a word is
the number of morphemes in the word. The length
of words ej and fi in R(ej , fi) are the number of
morphemes of ej and fi. We assume that all un-
aligned morphemes in a sentence map to a special
null morpheme.
TAM 1 with the contribution of both word and
morpheme translation probabilities, as in Eqn. 1, is
called ?word-and-morpheme? version of TAM 1.
The model is technically deficient probabilisti-
cally, as it models word and morpheme transla-
tion independently, and assigns mass to invalid
word/morpheme combinations. We can also de-
fine the ?morpheme-only? version of TAM 1 by
canceling out the contribution of word translation
probabilities and assigning 1 to t(ej |fi) in Eqn. 1.
Please note that, although this version of the two-
level alignment model does not use word transla-
tion probabilities, it is also a word-aware model, as
morpheme alignments are restricted to correspond
to a valid word alignment. As such, it also allows
for word level sequence modeling by HMMs. Fi-
nally, canceling out the contribution of morpheme
translation probabilities reduces TAM 1 to IBM
Model 1. Just as IBM Model 1 is used for initial-
ization before HMM-based word-alignment mod-
els (Vogel et al, 1996; Och and Ney, 2003), TAM
Model 1 is used to initialize its HMM extensions,
which are described in the next section.
3 Multi-rate HMM
Like other multi-scale HMM models such as hi-
erarchical HMM?s (Fine et al, 1998) and hidden
Markov trees (Crouse et al, 1998), the multi-rate
HMM characterizes the inter-scale dependencies
by a tree structure. As shown in Figure 5, scales
are organized in a hierarchical manner from coarse
to fine, which allows for efficient representation of
both short- and long-distance context simultane-
ously.
We found that 51% of the dependency relations
in the Turkish Treebank (Oflazer et al, 2003) are
between the last morpheme of a dependent word
and the first morpheme (the root) of the head word
that is immediately to its right, which is exempli-
fied below. The following examples show English
sentences in Turkish word/morpheme order. The
pseudo Turkish words are formed by concatena-
tion of English morphemes, which are indicated
by the ?+? between the morphemes.
? ? I will come from X.
? X+ABL come+will+I
? ? I will look at X.
? X+DAT look+will+I
In English, the verb ?come? subcategorizes for
a PP headed by ?from? in the example above.
In the pseudo Turkish version of this sentence,
?come? subcategorizes for a NP marked with abla-
tive case (ABL), which corresponds to the prepo-
sition ?from?. Similarly, ?look? subcategorizes for
a PP headed by ?at? in English, and a NP marked
with dative case (DAT) in Turkish. Just as the verb
and the preposition that it subcategorizes for are
frequently found adjacent to each other in English,
the verb and the case that it subcategorizes for are
frequently found adjacent to each other in Turk-
ish. Thus, we have a pattern of three correspond-
ing morphemes appearing in reverse order in En-
glish and Turkish, spanning two words in Turkish
and three words in English. In order to capture
such regularities, we chose multi-rate HMMs over
other hierarchically structured HMM models be-
cause, unlike other models, multi-rate HMMs al-
low morpheme sequence modeling across words
over the entire sentence. This allows us to capture
morpheme-mediated syntactic relations between
words (Eryig?it et al, 2008), as exemplified above.
Morpheme sequence modeling across words is
shown in Figure 4 by the arrows after the nodes
496
Figure 4: Multi-rate HMM graph.
representing fam(0,2) and fam(1,2). The circles
represent the words and morphemes of the source
language, the squares represent the words and
morphemes of the target language. e0,2 is the last
morpheme of word e0, and e1,0 is the first mor-
pheme of the next word e1. fam(1,0) is conditioned
on fam(0,2), which is in the previous word.
In order to model the morpheme sequence
across words, we define the function prev(j, k),
which maps the morpheme position (j, k) to the
previous morpheme position:
prev(j, k) =
{
(j, k ? 1) if k > 1
(j ? 1, |ej?1|) if k = 1
If a morpheme is the first morpheme of a word,
then the previous morpheme is the last morpheme
of the previous word.
3.1 Transitions
3.1.1 Morpheme transitions
Before introducing the morpheme level transition
probabilities, we first restrict morpheme level tran-
sitions according to the assumptions of our model.
We consider only the morpheme alignment func-
tions that are compatible with a word alignment
function. If we allow unrestricted transitions be-
tween morphemes, then this would result in some
morpheme alignments that do not allow a valid
word alignment function.
To avoid this problem, we restrict the transi-
tion function as follows: at each time step, we
allow transitions between morphemes in sentence
f if the morphemes belong to the same word.
This restriction reduces the transition matrix to a
block diagonal matrix. The block diagonal matrix
Ab below is a square matrix which has blocks of
square matrices A1 ? ? ?An on the main diagonal,
and the off-diagonal values are zero.
Ab =
?
????
A0 0 ? ? ? 0
0 A1 ? ? ? 0... ... . . . ...
0 0 ? ? ? An
?
????
The square blocks A0, . . . ,An have the dimen-
sions |f0|, . . . , |fn|, the length of the words in sen-
tence f . In each step of the forward-backward al-
gorithm, multiplying the forward (or backward)
probability vectors with the block diagonal ma-
trix restricts morpheme transitions to occur only
within the words of sentence f .
In order to model the morpheme sequence
across words, we also allow transitions between
morphemes across the words in sentence f . How-
ever, we allow cross-word transitions only at cer-
tain time steps: between the last morpheme of a
word in sentence e and the first morpheme of the
next word in sentence e. This does not result in
morpheme alignments that do not allow a valid
word alignment function. Instead of the block di-
agonal matrix Ab, we use a transition matrix A
which is not necessarily block diagonal, to model
morpheme transitions across words.
In sum, we multiply the forward (or backward)
probability vectors with either the transition ma-
trix Ab or the transition matrix A, depending on
whether the transition is occurring at the last mor-
pheme of a word in e. We introduce the function
?(p, q, r, s) to indicate whether a transition is al-
lowed from source position (p, q) to source posi-
497
tion (r, s) when advancing one target position:
?(p, q, r, s) =
{
1 if p = r or s = 1
0 otherwise
Morpheme transition probabilities have four
components. First, the ? function as described
above. Second, the jump width:
J (p, q, r, s) = abs(r, s)? abs(p, q)
where abs(j, k) maps a word-relative morpheme
position to an absolute morpheme position, i.e., to
the simple left-to-right ordering of a morpheme in
a sentence. Third, the morpheme class of the pre-
vious morpheme:2
M(p, q) = Class(f qp )
Fourth, as the arrow from faw(0) to fam(0,0) in Fig-
ure 4 shows, there is a conditional dependence on
the word class that the morpheme is in:
W(r) = Class(fr)
Putting together these components, the morpheme
transitions are formulated as follows:
p(am(j, k) = (r, s) | am(prev(j, k)) = (p, q)) ?
p
(
J (p, q, r, s)|M(p, q),W(r)
)
?(p, q, r, s)
(2)
The block diagonal matrix Ab consists of mor-
pheme transition probabilities.
3.1.2 Word transitions
In the multi-rate HMM, word transition probabili-
ties have two components. First, the jump width:
J (p, r) = r ? p
Second, the word class of the previous word:
W(p) = Class(fp)
The jump width is conditioned on the word class
of the previous word:
p(aw(j) = r | aw(j ? 1) = p) ?
p(J (p, r) | W(p)) (3)
The transition matrix A, which is not necessarily
block diagonal, consists of values which are the
product of a morpheme transition probability, as
defined in Eqn. 2, and a word transition probabil-
ity, as defined in Eqn. 3.
2We used the mkcls tool in GIZA (Och and Ney, 2003)
to learn the word and the morpheme classes.
3.2 Probability of translating a sentence
Finally, putting together Eqn. 1, Eqn. 2 and Eqn. 3,
we formulate the probability of translating a sen-
tence p(e|f) as follows:
Rw
?
aw
|e|?
j=1
(
t(ej |faw(j))p(aw(j)|aw(j?1))
Rm
?
am
|ej |?
k=1
t(ej,k|fam(j,k))
p(am(j,k)|am(prev(j,k)))
)
Rw is the same as it is in Eqn. 1, whereas
Rm = P (le|lf ). If we cancel out morpheme tran-
sitions by setting p(am(j, k)|am(prev(j, k))) =
1/|fam(j,k)|, i.e., with a uniform distribution, then
we get TAM with only word-level sequence mod-
eling, which we call TAM-HMM.
The complexity of the multi-rate HMM is
O(m3n3), where n is the number of words, and
m is the number of morphemes per word. TAM-
HMM differs from multi-rate HMM only by the
lack of morpheme-level sequence modeling, and
has complexity O(m2n3).
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions.
We learn the probabilities of jumping to a null po-
sition from the data. To compute the transition
probability from a null position, we keep track of
the nearest previous source word (or morpheme)
that does not align to null, and use the position of
the previous non-null word to calculate the jump
width. In order to keep track of the previous non-
null word, we insert a null word between words
(Och and Ney, 2003). Similarly, we insert a null
morpheme after every non-null morpheme.
3.3 Counts
We use Expectation Maximization (EM) to learn
the word and morpheme translation probabili-
ties, as well as the transition probabilities of the
reordering model. This is done with forward-
backward training at the morpheme level, collect-
ing translation and transition counts for both the
word and the morphemes from the morpheme-
level trellis.
In Figure 5, the grid on the right depicts the
morpheme-level trellis. The grid on the left is
the abstraction of the word-level trellis over the
498
Figure 5: Multi-rate HMM trellis
morpheme-level trellis. For each target word e and
for each source word f , there is a small HMM trel-
lis with dimensions |e|?|f | inside the morpheme-
level trellis, as shown by the shaded area inside the
grid on the right. We collect counts for words by
summing over the values in the small HMM trellis
associated with the words.
3.3.1 Translation counts
Morpheme translation counts We compute ex-
pected counts over the morpheme-level trellis.
The morpheme translation count function below
collects expected counts for a morpheme pair
(h, g) in a sentence pair (e, f):
cm(h|g; e, f) =
?
(j,k)
s.t.
h=ekj
?
(p,q)
s.t.
g=fqp
?j,k(p, q)
where ?j,k(p, q) stands for the posterior mor-
pheme translation probabilities for source position
(p, q) and target position (i, j) that are computed
with the forward-backward algorithm.
Word translation counts For each target word
e and source word f , we collect word transla-
tion counts by summing over posterior morpheme
translation probabilities that are in the small trellis
associated with e and f .
Since ? allows only within-word transitions to
occur inside the small trellis, the posterior proba-
bility of observing the word e given the word f
is preserved across time points within the small
trellis associated with e and f . In other words,
the sum of the posterior probabilities in each col-
umn of the small trellis is the same. Therefore, we
collect word translation counts only from the last
morphemes of the words in e.
The word translation count function below col-
lects expected counts from a sentence pair (e, f)
for a particular source word f and target word e:
cw(e|f ; e, f) =
?
j
s.t.
e=ej
?
p
s.t.
f=fp
?
1?q?|f |
?j,|e|(p, q)
3.3.2 Transition counts
Morpheme transition counts For all target po-
sitions (j, k) and all pairs of source positions (p, q)
and (r, s), we compute morpheme transition pos-
teriors:
?j,k((p, q), (r, s))
using the forward-backward algorithm. These
expected counts are accumulated to esti-
mate the morpheme jump width probabilities
p
(
J (p, q, r, s)|M(p, q),W(r)
) used in Eqn. 2.
Word transition counts We compute posterior
probabilities for word transitions by summing over
morpheme transition posteriors between the mor-
phemes of the words fl and fn:
?j(p, r) =
?
1?q?|fp|
?
1?s?|fr|
?j,|ej |((p, q), (r, s))
Like the translation counts, the transition counts
are collected from the last morphemes of words
in e. These expected counts are accumulated
to estimate the word jump width probabilities
p(J (p, r) | W(p)) used in Eqn. 3.
Finally, Rm = P (le|lf ) does not cancel out in
the counts of the multi-rate HMM. To compute the
conditional probability P (le|lf ), we assume that
the length of word e varies according to a Poisson
distribution with a mean that is linear with length
of the word f (Brown et al, 1993).
3.4 Variational Bayes
In order to prevent overfitting, we use the Varia-
tional Bayes extension of the EM algorithm (Beal,
2003). This amounts to a small change to the
M step of the original EM algorithm. We in-
troduce Dirichlet priors ? to perform an inexact
normalization by applying the function f(v) =
exp(?(v)) to the expected counts collected in the
E step, where ? is the digamma function (John-
son, 2007). The M-step update for a multinomial
parameter ?x|y becomes:
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
499
Multi-rate
HMM
TAM-HMM WORD
Word-
Morph 
Morph 
only IBM 4 Baseline
BLEU TR to EN 30.82 29.48 29.98 29.13 27.91EN to TR 23.09 22.55  22.54 21.95 21.82
AER 0.254 0.255 0.256 0.375 0.370
Table 1: AER and BLEU Scores
We set ? to 10?20, a very low value, to have the
effect of anti-smoothing, as low values of ? cause
the algorithm to favor words which co-occur fre-
quently and to penalize words that co-occur rarely.
We used Dirichlet priors on morpheme translation
probabilities.
4 Experiments and Results
4.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such
as the Turkish Ministry of Foreign Affairs, EU,
etc. The Turkish data was first morphologically
parsed (Oflazer, 1994), then disambiguated (Sak
et al, 2007) to select the contextually salient inter-
pretation of words. In addition, we removed mor-
phological features that are not explicitly marked
by an overt morpheme. For English, we use part-
of-speech tagged data. The number of English
words is 1,033,726 and the size of the English vo-
cabulary is 28,647. The number of Turkish words
is 812,374, the size of the Turkish vocabulary is
57,249. The number of Turkish morphemes is
1,484,673 and the size of the morpheme vocab-
ulary is 16,713.
4.2 Experiments
We initialized our implementation of the single
level ?word-only? model, which we call ?baseline?
in Table 1, with 5 iterations of IBM Model 1, and
further trained the HMM extension (Vogel et al,
1996) for 5 iterations. Similarly, we initialized
TAM-HMM and multi-rate HMM with 5 iterations
of TAM 1 as explained in Section 2.2. Then we
trained TAM-HMM and the multi-rate HMM for 5
iterations. We also ran GIZA++ (IBM Model 1?4)
on the data. We translated 1000 sentence test sets.
We used Dirichlet priors in both IBM Model 1
and TAM 1 training. We experimented with using
Dirichlet priors on the HMM extensions of both
IBM-HMM and TAM-HMM. We report the best
results obtained for each model and translation di-
rection.
We evaluated the performance of our model in
two different ways. First, we evaluated against
gold word alignments for 75 Turkish-English sen-
tences. Table 1 shows the AER (Och and Ney,
2003) of the word alignments; we report the grow-
diag-final (Koehn et al, 2003) of the Viterbi align-
ments. Second, we used the Moses toolkit (Koehn
et al, 2007) to train machine translation systems
from the Viterbi alignments of our various models,
and evaluated the results with BLEU (Papineni et
al., 2002).
In order to reduce the effect of nondetermin-
ism, we run Moses three times per experiment set-
ting, and report the highest BLEU scores obtained.
Since the BLEU scores we obtained are close,
we did a significance test on the scores (Koehn,
2004). In Table 1, the colors partition the table
into equivalence classes: If two scores within the
same row have different background colors, then
the difference between their scores is statistically
significant. The best scores in the leftmost column
were obtained from multi-rate HMMs with Dirich-
let priors only during the TAM 1 training. On the
contrary, the best scores for TAM-HMM and the
baseline-HMM were obtained with Dirichlet pri-
ors both during the TAM 1 and the TAM-HMM
500
training. In Table 1, as the scores improve grad-
ually towards the left, the background color gets
gradually lighter, depicting the statistical signifi-
cance of the improvements. The multi-rate HMM
performs better than the TAM-HMM, which in
turn performs better than the word-only models.
5 Conclusion
We presented a multi-rate HMM word alignment
model, which models the word and the morpheme
sequence simultaneously. We have tested our
model on the Turkish-English pair and showed
that our model is superior to the two-level word
alignment model which has sequence modeling
only at the word level.
Acknowledgments Partially funded by NSF
award IIS-0910611. Kemal Oflazer acknowledges
the generous support of the Qatar Foundation
through Carnegie Mellon University?s Seed Re-
search program. The statements made herein are
solely the responsibility of this author(s), and not
necessarily that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, Final Report, JHU Summer Work-
shop.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
O?zgu?r C?etin, Mari Ostendorf, and Gary D. Bernard.
2007. Multirate coupled Hidden Markov Models
and their application to machining tool-wear clas-
sification. IEEE Transactions on Signal Processing,
55(6):2885?2896, June.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90.
Matthew Crouse, Robert Nowak, and Richard Bara-
niuk. 1998. Wavelet-based statistical signal pro-
cessing using Hidden Markov Models. IEEE Trans-
actions on Signal Processing, 46(4):886?902.
Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357?389.
Elif Eyigo?z, Daniel Gildea, and Kemal Oflazer. 2013.
Simultaneous word-morpheme alignment for statis-
tical machine translation. In NAACL.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical Hidden Markov model: Analysis
and applications. Machine Learning, 32(1):41?62,
July.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In EMNLP-CoNLL, pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In HLT-NAACL, pages
57?60.
Jia Li, Robert Gray, and Richard Olshen. 2006.
Multiresolution image classification by hierarchi-
cal modeling with two-dimensional Hidden Markov
Models. IEEE Transactions on Information Theory,
46(5):1826?1841, September.
Mark R. Luettgen, William C. Karl, Alan S. Willsky,
and Robert R. Tenney. 1993. Multiscale representa-
tions of Markov Random Fields. IEEE Transactions
on Signal Processing, 41(12):3377?3396.
Sonja Niessen and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
COLING, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
Models. Computational Linguistics, 29(1):19?51.
501
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r. 2003. Building a Turkish treebank. In
A. Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, pages 261?277. Kluwer, London.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02), pages 311?
318.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118.
Marios Skounakis, Mark Craven, and Soumya Ray.
2003. Hierarchical Hidden Markov Models for in-
formation extraction. In International Joint Con-
ference on Artificial Intelligence, volume 18, pages
427?433.
Georgios Theocharous, Khashayar Rohanimanesh, and
Sridhar Maharlevan. 2001. Learning hierarchi-
cal observable Markov decision process Models for
robot navigation. In ICRA 2001, volume 1, pages
511?516.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING, pages 836?841.
Alan S. Willsky. 2002. Multiresolution Markov Mod-
els for signal and image processing. In Proceedings
of the IEEE, pages 1396?1458.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from English to Turkish.
In ACL 2010, pages 454?464.
502
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
CMUQ@QALB-2014: An SMT-based System
for Automatic Arabic Error Correction
Serena Jeblee
1
, Houda Bouamor
2
, Wajdi Zaghouani
2
and Kemal Oflazer
2
1
Carnegie Mellon University
sjeblee@cs.cmu.edu
2
Carnegie Mellon University in Qatar
{hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.edu
Abstract
In this paper, we describe the CMUQ sys-
tem we submitted to The ANLP-QALB 2014
Shared Task on Automatic Text Correction
for Arabic. Our system combines rule-based
linguistic techniques with statistical language
modeling techniques and machine translation-
based methods. Our system outperforms the
baseline and reaches an F-score of 65.42% on
the test set of QALB corpus. This ranks us 3rd
in the competition.
1 Introduction
The business of text creation and editing represents a
large market where NLP technologies might be applied
naturally (Dale, 1997). Today?s users of word proces-
sors get surprisingly little help in checking spelling,
and a small number of them use more sophisticated
tools such as grammar checkers, to provide help in en-
suring that a text remains grammatically accurate after
modification. For instance, in the Arabic version of Mi-
crosoft Word, the spelling checker for Arabic, does not
give reasonable and natural proposals for many real-
word errors and even for simple probable errors (Had-
dad and Yaseen, 2007).
With the increased usage of computers in the pro-
cessing of natural languages comes the need for cor-
recting errors introduced at different stages. Natu-
ral language errors are not only made by human op-
erators at the input stage but also by NLP systems
that produce natural language output. Machine trans-
lation (MT), or optical character recognition (OCR),
often produce incorrect output riddled with odd lexi-
cal choices, grammar errors, or incorrectly recognized
characters. Correcting human/machine-produced er-
rors, or post-editing, can be manual or automated. For
morphologically and syntactically complex languages,
such as Modern Standard Arabic (MSA), correcting
texts automatically requires complex human and ma-
chine processing which makes generation of correct
candidates a challenging task.
For instance, the Automatic Arabic Text Correction
Shared Task is an interesting testbed to develop and
evaluate spelling correction systems for Arabic trained
either on naturally occurring errors in texts written by
humans (e.g., non-native speakers), or machines (e.g.,
MT output). In such tasks, participants are asked to
implement a system that takes as input Modern Stan-
dard Arabic texts with various spelling errors and au-
tomatically correct them. In this paper, we describe
the CMUQ system we developed to participate in the
The First Shared Task on Automatic Text Correction
for Arabic (Mohit et al., 2014). Our system combines
rule-based linguistic techniques with statistical lan-
guage modeling techniques and machine translation-
based methods. Our system outperforms the baseline,
achieves a better correction quality and reaches an F-
score of 62.96% on the development set of QALB cor-
pus (Zaghouani et al., 2014) and 65.42% on the test set.
The remainder of this paper is organized as follows.
First, we review the main previous efforts for automatic
spelling correction, in Section 2. In Section 3, we de-
scribe our system, which consists of several modules.
We continue with our experiments on the shared task
2014 dev set (Section 4). Then, we give an analysis of
our system output in Section 5. Finally, we conclude
and hint towards future improvement of the system, in
Section 6.
2 Related Work
Automatic error detection and correction include auto-
matic spelling checking, grammar checking and post-
editing. Numerous approaches (both supervised and
unsupervised) have been explored to improve the flu-
ency of the text and reduce the percentage of out-
of-vocabulary words using NLP tools, resources, and
heuristics, e.g., morphological analyzers, language
models, and edit-distance measure (Kukich, 1992;
Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan
et al., 2003; Haddad and Yaseen, 2007; Hassan et al.,
2008; Habash, 2008; Shaalan et al., 2010). There has
been a lot of work on error correction for English (e.g.,
(Golding and Roth, 1999)). Other approaches learn
models of correction by training on paired examples
of errors and their corrections, which is the main goal
of this work.
For Arabic, this issue was studied in various direc-
tions and in different research work. In 2003, Shaalan
et al. (2003) presented work on the specification and
classification of spelling errors in Arabic. Later on,
Haddad and Yaseen (2007) presented a hybrid ap-
proach using morphological features and rules to fine
137
tune the word recognition and non-word correction
method. In order to build an Arabic spelling checker,
Attia et al. (2012) developed semi-automatically, a dic-
tionary of 9 million fully inflected Arabic words us-
ing a morphological transducer and a large corpus.
They then created an error model by analyzing error
types and by creating an edit distance ranker. Finally,
they analyzed the level of noise in different sources of
data and selected the optimal subset to train their sys-
tem. Alkanhal et al. (2012) presented a stochastic ap-
proach for spelling correction of Arabic text. They used
a context-based system to automatically correct mis-
spelled words. First of all, a list is generated with pos-
sible alternatives for each misspelled word using the
Damerau-Levenshtein edit distance, then the right al-
ternative for each misspelled word is selected stochas-
tically using a lattice search, and an n-gram method.
Shaalan et al. (2012) trained a Noisy Channel Model
on word-based unigrams to detect and correct spelling
errors. Dahlmeier and Ng (2012a) built specialized de-
coders for English grammatical error correction. More
recently, (Pasha et al., 2014) created MADAMIRA,
a system for morphological analysis and disambigua-
tion of Arabic, this system can be used to improve the
accuracy of spelling checking system especially with
Hamza spelling correction.
In contrast to the approaches described above, we
use a machine translation (MT) based method to train
an error correction system. To the best of our knowl-
edge, this is the first error correction system for Arabic
using an MT approach.
3 Our System
Our system is a pipeline that consists of several dif-
ferent modules. The baseline system uses a spelling
checking module, and the final system uses a phrase-
based statistical machine translation system. To
preproces the text, we use the provided output of
MADAMIRA (Pasha et al., 2014) and a rule-based
correction. We then do a rule-based post-processing
to fix the punctuation.
3.1 Baseline Systems
For the baseline system, we try a common spelling
checking approach. We first pre-process the data us-
ing the features from MADAMIRA (see Feature 14
Replacement), then we use a noisy channel model for
spelling checking.
Feature 14 Replacement
The first step in the pipeline is to extract
MADAMIRA?s 14th feature from the .column file
and replace each word in the input text with this form.
MADAMIRA uses morphological disambiguation and
SVM analysis to select the most likely fully diacritized
Arabic word for the input word. The 14th feature
represents the undiacritized form of the most likely
word. This step corrects many Hamza placement or
omission errors, which makes a good base for other
correction modules.
Spelling Correction
The spelling checker is based on a noisy channel model
- we use a word list and language model to determine
the most probable correct Arabic word that could have
generated the incorrect form that we have in the text.
For detecting spelling errors we use the AraComLex
word list for spelling checking (Attia et al., 2012),
which contains about 9 million Arabic words.
1
We
look up the word from the input sentence in this list,
and attempt to correct those that are not found in the
list. We also train a mapping of incorrect words and
possible corrections from the edits in the training data.
If the word is in this map, the list of possible correc-
tions from the training data becomes the candidate list.
If the word is not in the trained map, the candidate list
is created by generating a list of words with common
insertions, substitutions, and deletions, according to the
list in (Attia et al., 2012). Each candidate is generated
by performing these edits and has a weight according to
the edit distance weights in the list. We then prune the
candidate list by keeping only the lowest weight words,
and removing candidates that are not found in the word
list. The resulting sentence is scored with a 3-gram lan-
guage model built with KenLM (Heafield et al., 2013)
on the correct side of the training data. The top one
sentence is then kept and considerd as the ?corrected?
one.
This module handles spelling errors of individual
words; it does not handle split/merge errors or word
reordering. The spelling checker sometimes attempts
to correct words that were already correct, because
the list does not contain named entities or translitera-
tions, and it does not contain all possible correct Arabic
words. Because the spelling checker module decreased
the overall performance, it is not included in our final
system.
3.2 Final System
Feature 14 Replacement
The first step in our final system is Feature 14 Replace-
ment, as described above.
Rule-based Clitic Correction
With the resulting data, we apply a set of rules to reat-
tach clitics that may have been split apart from the base
word. After examining the train dataset, we realized
that 95% of word merging cases involve ??? attach-
ment. When found by themselves, the clitics are at-
tached to either the previous word or next word, based
on whether they generally appear as prefixes or suf-
fixes. The clitics handled by this module are specified
in Table 2.
We also remove extra characters by replacing a se-
quence of 3 or more of the same character with a single
1
http://sourceforge.net/projects/
arabic-wordlist/
138
Dev
Exact Match No Punct
Precision Recall F1 Precision Recall F1
Feature 14 0.7746 0.3210 0.4539 0.8100 0.5190 0.6326
Feature 14 + Spelling checker (baseline) 0.4241 0.3458 0.3810 0.4057 0.4765 0.4382
Feature 14 + Clitic Rules 0.7884 0.3642 0.4983 0.8149 0.5894 0.6841
Feature 14 + Phrase-based MT 0.7296 0.5043 0.5964 0.7797 0.6397 0.7028
Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473
Test
Feature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135
Table 1: System results on the dev set (upper part) and on the test set (lower part).
Attach clitic to... Clitics
Beginning of next word {?, ?@, H
.
,
	
?, ?}
End of previous word {?, A?, A
	
K, ?


	
G, ?


, ??, @}
Table 2: Clitics handled by the rule-based module.
instance of that character (e.g. !!!!!!! would be replaced
with !).
Statistical Phrase-based Model
We use the Moses toolkit (Koehn et al., 2007) to
create a statistical phrase-based machine translation
model built on the best pre-processed data, as described
above. We treat this last step as a translation prob-
lem, where the source language is pre-processed in-
correct Arabic text, and the reference is correct Ara-
bic. Feature 14 extraction, rule-based correction, and
character de-duplication are applied to both the train
and dev sets. All but the last 1,000 sentences of the
train data are used at the training set for the phrase-
based model, the last 1,000 sentences of the train data
are used as a tuning set, and the dev set is used for
testing and evaluation. We use fast align, the aligner
included with the cdec decoder (Dyer et al., 2010) as
the word aligner with grow-diag as the symmetrization
heuristic (Och and Ney, 2003), and build a 5-gram lan-
guage model from the correct Arabic training data with
KenLM (Heafield et al., 2013). The system is evaluated
with BLEU (Papineni et al., 2002) and then scored for
precision, recall, and F1 measure against the dev set
reference.
We tested several different reordering window sizes
since this is not a standard translation task, so we may
want shorter distance reordering. Although 7 is the de-
fault size, we tested 7, 5, 4, 3, and 0, and found that a
window of size 4 produces the best result according to
BLEU score and F1 measure.
4 Experiments and Results
We train and evaluate our system with the train-
ing and development datasets provided for the shared
task and the m2Scorer (Dahlmeier and Ng, 2012b).
These datasets are extracted from the QALB corpus
of human-edited Arabic text produced by native speak-
ers, non-native speakers and machines (Zaghouani et
al., 2014).
We conducted a small scale statistical study on the
950K tokens training set used to build our system. We
realized that 306K tokens are affected by a correction
action which could be a word edit, insertion, deletion,
split or merge. 169K tokens were edited to correct the
spelling errors and 99K tokens were inserted (mostly
punctuation marks). Furthermore, there is a total of
6,7K non necessary tokens deleted and 10.6K attached
tokens split and 18.2 tokens merged. Finally, there are
only 427 tokens moved in the sentence and 1563 mul-
tiple correction action.
We experiment with different configurations and
reach the sweet spot of performance when combining
the different modules.
4.1 Results
To evaluate the performance of our system on the de-
velopment data, we compare its output to the reference
(gold annotation). We then compute the usual mea-
sures of precision, recall and f-measure. Results for
various system configurations on the dev and test sets
are given in Table 1. Using the baseline system con-
sisting in replacing words by their non diacritized form
(Feature 14), we could correct 51.9% of the errors oc-
curring in the dev set, when punctuation is not consid-
ered. This result drops when we consider the punctua-
tion errors which seem to be more complex to correct:
Only 32.1% of the errors are corrected in the dev set. It
is important to notice that adding the clitic rules to the
Feature 14 baseline yields an improvement of + 5.15 in
F-measure. We reach the best F-measure value when
using the phrase-based MT system after pre-processing
the data and applying the Feature 14 and clitic rules.
Using this combination we were able to correct 68.5%
of the errors (excluding punctuation) on the develop-
ment set with a precision of 82.2% and 74.38% on the
test set. When we consider the punctuation, 53.89%
of the errors of different types were corrected on the
dev set and 56.35% on the test set with a precision of
75.71% and 77.97%, respectively.
139
5 Error Analysis and Discussion
When building error correction systems, minimizing
the number of cases where correct words are marked
as incorrect is often regarded as more important than
covering a high number of errors. Therefore, a higher
precision is often preferred over higher recall. In order
to understand what was affecting the performance, we
took a closer look at our system output and translation
tables to present some samples of errors that our system
makes on development set.
5.1 Out-of-vocabulary Words
This category includes words that are not seen by our
system during the training which is a common problem
in machine translation systems. In our system, most of
out-of-vocabulary words were directly transferred un-
changed from source to target. For example the word

?J


??

????
	
? @ was not corrected to

?J


??

???
?
@.
5.2 Unnecessary Edits
In some cases, our system made some superfluous edits
such as adding the definite article in cases where it is
not required such as :
Source

?
	
JK


Y?
?
@
	
?AJ


?

@
Hypothesis

?
	
JK


Y?
?
@
	
?AJ


?

B@
Reference (unchanged)

?
	
JK


Y?
?
@
	
?AJ


?

@
Table 3: An example of an unnecessary addition of the
definite article.
5.3 Number Normalization
We observed that in some cases, the system did not nor-
malize the numbers such as in the following case which
requires some knowledge of the real context to under-
stand that these numbers require normalization.
Source

H@?A
	
?J


? 450000
Hypothesis

H@?A
	
?J


? 450000
Reference

H@?A
	
?J


? 450
Table 4: An example of number normalization.
5.4 Hamza Spelling
Even though our system corrected most of the Hamza
spelling errors, we noticed that in certain cases they
were not corrected, especially when the words without
the Hamza were valid entries in the dictionary. These
cases are not always easy to handle since only context
and semantic rules can handle them.
5.5 Grammatical Errors
In our error analysis we encountered many cases of un-
corrected grammatical errors. The most frequent type
Source

?J


	
J???@ X@?
Hypothesis

?J


	
J???@ X@?
Reference

?J


	
J???@ X

@?
Table 5: A sentence where the Hamza was not added
above the Alif in the first word because both versions
are valid dictionary entries.
is the case endings correction such as correcting the
verbs in jussive mode when there is a prohibition par-
ticle (negative imperative) like the (B) in the following
examples :
Source ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Hypothesis ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Reference ??E


XAK



@
??
?
	
??K
.
Q?
	
?


B
Table 6: An example of a grammatical error.
5.6 Unnecessary Word Deletion
According to the QALB annotation guidelines, ex-
tra words causing semantic ambiguity in the sentence
should be deleted. The decision to delete a given word
is usually based on the meaning and the understanding
of the human annotator, unfortunately this kind of er-
rors is very hard to process and our system was not able
to delete most of the unnecessary words.
Source Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Hypothesis Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Reference Q
	
k

@ A
	
J



?? A?
	
?? Y?D

?
	
J? ??
Table 7: An example of word deletion.
5.7 Adding Extra Words
Our analysis revealed cases of extra words introduced
to some sentences, despite the fact that the words added
are coherent with the context and could even improve
the overall readability of the sentence, they are uncred-
ited correction since they are not included in the gold
standard. For example :
Source ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Hypothesis Qm
?
'@ ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Reference ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Table 8: An example of the addition of extra words.
5.8 Merge and Split Errors
In this category, we show some sample errors of neces-
sary word splits and merge not done by our system. The
140
word Y?K
.
A???
	
k should have been split as Y?K
.
A???
	
k
and the word YK
.
B should have been merged to appear
as one word as in YK
.
B.
5.9 Dialectal Correction Errors
Dialectal words are usually converted to their Modern
Standard Arabic (MSA) equivalent in the QALB cor-
pus, since dialectal words are rare, our system is unable
to detect and translate the dialectal words to the MSA
as in the expression
	
?K


	
P I
.
? that is translated in the
gold standard to
	
?K


	
P
Q



	
?.
6 Conclusion
We presented our CMUQ system for automatic Ara-
bic text correction. Our system combines rule-based
linguistic techniques with statistical language model-
ing techniques and a phrase-based machine transla-
tion method. We experiment with different configu-
rations. Our experiments have shown that the system
we submitted outperforms the baseline and we reach
an F-score of 74.73% on the development set from
the QALB corpus when punctuation is excluded, and
65.42% on the test set when we consider the punctu-
ation errors . This placed us in the 3rd rank. We be-
lieve that our system could be improved in numerous
ways. In the future, we plan to finalize a current mod-
ule that we are developing to deal with merge and split
errors in a more specific way. We also want to focus in
a deeper way on the word movement as well as punc-
tuation problems, which can produce a more accurate
system. We will focus as well on learning further error
correction models from Arabic Wikipedia revision his-
tory, as it contains natural rewritings including spelling
corrections and other local text transformations.
Acknowledgements
This publication was made possible by grants NPRP-
09-1140-1-177 and NPRP-4-1058- 1-168 from the
Qatar National Research Fund (a member of the Qatar
Foundation). The statements made herein are solely the
responsibility of the authors.
References
Mohamed I. Alkanhal, Mohamed Al-Badrashiny, Man-
sour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012. Automatic Stochastic Arabic Spelling Correc-
tion With Emphasis on Space Insertions and Dele-
tions. IEEE Transactions on Audio, Speech & Lan-
guage Processing, 20(7):2111?2122.
Mohammed Attia, Pavel Pecina, Younes Samih,
Khaled Shaalan, and Josef van Genabith. 2012. Im-
proved Spelling Error Detection and Correction for
Arabic. In Proceedings of COLING 2012: Posters,
pages 103?112, Mumbai, India.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A Beam-
Search Decoder for Grammatical Error Correction.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 568?578, Jeju Island, Korea.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Bet-
ter Evaluation for Grammatical Error Correction. In
NAACL HLT ?12 Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 568?572.
Robert Dale. 1997. Computer Assistance in Text Cre-
ation and Editing. In Survey of the state of the art
in Human Language Technology, chapter 7, pages
235?237. Cambridge University Press.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A Decoder, Alignment, and Learning Frame-
work for Finite-state and Context-free Translation
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12, Uppsala, Sweden.
A. R. Golding and D. Roth. 1999. A Winnow Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1-3):107?130.
Nizar Habash. 2008. Four Techniques for Online Han-
dling of Out-of-Vocabulary Words in Arabic-English
Statistical Machine Translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57?60, Colum-
bus, Ohio.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-words in Arabic: a Hybrid
Approach. International Journal of Computer Pro-
cessing of Oriental Languages, 20(04):237?257.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction using
Finite State Automata. In Proceedings of the Third
International Joint Conference on Natural Language
Processing (IJCNLP 2008), pages 913?918, Hyder-
abad, India.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modfied
Kneser-Ney Language Model Estimation. In In Pro-
ceedings of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
141
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys
(CSUR), 24(4):377?439.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, page 1951.
Kemal Oflazer. 1996. Error-Tolerant Finite-State
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73?89.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Association for Computational Linguis-
tics, Philadelphia, Pennsylvania.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC?14), pages 1094?1101, Reykjavik, Iceland.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Proceedings of the 4th Conference on Lan-
guage Engineering, Egyptian Society of Language
Engineering (ELSE), Cairo, Egypt.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
Approach for Analyzing and Correcting Spelling Er-
rors for Non-native Arabic Learners. In Proceedings
of The 7th International Conference on Informatics
and Systems, INFOS2010, the special track on Nat-
ural Language Processing and Knowledge Mining,
pages 28?30, Cairo, Egypt.
Khaled Shaalan, Mohammed Attia, Pavel Pecina,
Younes Samih, and Josef van Genabith. 2012.
Arabic Word Generation and Modelling for Spell
Checking. In Proceedings of the Eighth Inter-
national Conference on Language Resources and
Evaluation (LREC-2012), pages 719?725, Istanbul,
Turkey.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), Reykjavik, Iceland.
Chiraz Zribi and Mohammed Ben Ahmed. 2003. Ef-
ficient Automatic Correction of Misspelled Arabic
Words Based on Contextual Information. In Pro-
ceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, pages
770?777, Oxford, UK.
142
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196?206,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Domain and Dialect Adaptation
for Machine Translation into Egyptian Arabic
Serena Jeblee
1
, Weston Feely
1
, Houda Bouamor
2
Alon Lavie
1
, Nizar Habash
3
and Kemal Oflazer
2
1
Carnegie Mellon University
{sjeblee, wfeely, alavie}@cs.cmu.edu
2
Carnegie Mellon University in Qatar
hbouamor@qatar.cmu.edu, ko@cs.cmu.edu
3
New York University Abu Dhabi
nizar.habash@nyu.edu
Abstract
In this paper, we present a statistical ma-
chine translation system for English to Di-
alectal Arabic (DA), using Modern Stan-
dard Arabic (MSA) as a pivot. We cre-
ate a core system to translate from En-
glish to MSA using a large bilingual par-
allel corpus. Then, we design two separate
pathways for translation from MSA into
DA: a two-step domain and dialect adap-
tation system and a one-step simultane-
ous domain and dialect adaptation system.
Both variants of the adaptation systems are
trained on a 100k sentence tri-parallel cor-
pus of English, MSA, and Egyptian Arabic
generated by a rule-based transformation.
We test our systems on a held-out Egyp-
tian Arabic test set from the 100k sen-
tence corpus and we achieve our best per-
formance using the two-step domain and
dialect adaptation system with a BLEU
score of 42.9.
1 Introduction
While MSA is the shared official language of cul-
ture, media and education in the Arab world, it is
not the native language of any speakers of Ara-
bic. Most native speakers are unable to produce
sustained spontaneous discourse in MSA - they
usually resort to repeated code-switching between
their dialect and MSA (Abu-Melhim, 1991). Ara-
bic speakers are quite aware of the contextual fac-
tors and the differences between their dialects and
MSA, although they may not always be able to
pinpoint exact linguistic differences. In the con-
text of natural language processing (NLP), some
Arabic dialects have started receiving increas-
ing attention, particularly in the context of ma-
chine translation (Zbib et al., 2012; Salloum and
Habash, 2013; Salloum et al., 2014; Al-Mannai
et al., 2014) and in terms of data collection (Cot-
terell and Callison-Burch, 2014; Bouamor et al.,
2014; Salama et al., 2014) and basic enabling
technologies (Habash et al., 2012; Pasha et al.,
2014). However, the focus is on a small number
of iconic dialects, (e.g., Egyptian). The Egyptian
media industry has traditionally played a dominant
role in the Arab world, making the Egyptian di-
alect the most widely understood and used dialect.
DA is now emerging as the language of informal
communication online. DA differs phonologically,
lexically, morphologically, and syntactically from
MSA. And while MSA has an established stan-
dard orthography, the dialects do not: people write
words reflecting their phonology and sometimes
use roman script. Thus, MSA tools cannot ef-
fectively model DA; for instance, over one-third
of Levantine verbs cannot be analyzed using an
MSA morphological analyzer (Habash and Ram-
bow, 2006). These differences make the direct use
of MSA NLP tools and applications for handling
dialects impractical.
In this work, we design an MT system for En-
glish to Egyptian Arabic translation by using MSA
as an intermediary step. This includes different
challenges from those faced when translating into
English. Because MSA is the formal written va-
riety of Arabic, there is an abundance of written
data, including parallel corpora from sources like
the United Nations and newspapers, as well as var-
ious treebanks. Using these resources, many re-
searchers have created fairly reliable MSA trans-
lation systems. However, these systems are not
designed to deal with the other Arabic variants.
Egyptian Arabic is much closer to MSA than
it is to English, so one can get a system bet-
196
ter performance by translating first into MSA and
then translating from MSA to Egyptian Arabic,
which are far more similar. Our approach consists
of a core MT system trained on a large amount
of out-of-domain English-MSA parallel data, fol-
lowed by an adaptation system. We design and im-
plement two adaptation systems: a two-step sys-
tem first adapts to in-domain MSA and then sep-
arately adapts from MSA to Egyptian Arabic, and
a one-step system that adapts directly from out-of-
domain MSA to in-domain Egyptian Arabic.
Our research contributions are summarized as
follows:
(a) We build a machine translation system to
translate into, rather than out of, dialectal Ara-
bic (from English), using MSA as a pivot
point.
(b) We apply a domain adaptation technique to
improve the MSA results on our in-domain
dataset.
(c) We automatically generate the Egyptian side
of a 100k tri-parallel corpus covering MSA,
English and Egyptian.
(d) We use this domain adaptation technique to
adapt MSA into dialectal Arabic.
The remainder of this paper is structured as fol-
lows. We first review the main previous efforts
for dealing with DA in NLP, in Section 2. In Sec-
tion 3,we give a general description about using
phrase-based MT as an adaptation system. Sec-
tion 4 presents the dataset used in the different ex-
periments. Our approach for translating English
text into Egyptian Arabic is explained in Section 5.
Section 6 presents our experimental setup and the
results obtained. Then, we give an analysis of our
system output in Section 7. Finally, we conclude
and describe our future work in Section 8.
2 Related work
Machine translation (MT) for dialectal Arabic
(DA) is quite challenging given the limited re-
sources to build rule-based models or train statis-
tical models for MT. While there has been a con-
siderable amount of work in the context of stan-
dard Arabic NLP (Habash, 2010), DA is impov-
erished in terms of available tools and resources
compared to MSA, e.g., there are few parallel DA-
English corpora (Zbib et al., 2012; Bouamor et
al., 2014). The majority of DA resources are for
speech recognition, although more and more re-
sources for machine translation and enabling tech-
nologies such as morphological analyzers are be-
coming available for specific dialects (Habash et
al., 2012; Habash et al., 2013).
For Arabic and its dialects, several researchers
have explored the idea of exploiting existing MSA
rich resources to build tools for DA NLP. Differ-
ent research work successfully translated DA to
MSA as a bridge to translate to English (Sawaf,
2010; Salloum and Habash, 2013), or to enhance
the performance of Arabic-based information re-
trieval systems (Shatnawi et al., 2012). Among the
efforts on translation from DA to MSA, Abo Bakr
et al. (2008) introduced a hybrid approach to trans-
fer a sentence from Egyptian Arabic to MSA.
Sajjad et al. (2013) used a dictionary of Egyp-
tian/MSA words to transform Egyptian to MSA
and showed improvement in the quality of ma-
chine translation. A similar but rule-based work
was done by Mohamed et al. (2012). Boujel-
bane et al. (2013) and Hamdi et al. (2014) built
a bilingual dictionary using explicit knowledge
about the relation between Tunisian Arabic and
MSA. These works are limited to a dictionary or
rules that are not available for all dialects. Zbib
et al. (2012) used crowdsourcing to translate sen-
tences from Egyptian and Levantine into English,
and thus built two bilingual corpora. The dialec-
tal sentences were selected from a large corpus
of Arabic web text. Then, they explored several
methods for dialect/English MT. Their best Egyp-
tian/English system was trained on dialect/English
parallel data. They argued that differences in genre
between MSA and DA make bridging through
MSA of limited value. For this reason, while piv-
oting through MSA, it is important to consider the
domain and add an additional step: domain adap-
tation.
The majority of previous efforts in DA MT has
been focusing on translating from dialectal Arabic
into other languages (mainly MSA or English). In
contrast, in this work we focus on building a sys-
tem to translate from English and MSA into DA.
Furthermore, to the best of our knowledge, this is
the first work in which we adapt the domain in ad-
dition to the dialect (Egyptian specifically).
3 Using Phrase-Based MT as an
Adaptation System
For commercial use, MT output is usually post-
edited by a human translator in order to fix the er-
rors generated by the MT system. This is often
faster and cheaper than having a human translate
197
the document from scratch. However, we can ap-
ply statistical phrase-based MT to create an auto-
matic machine post-editor (what we refer to in this
paper as an adaptation system) to improve the out-
put of an MT system, and make it more closely
resemble the references. Simard et al. (2007) used
a phrase-based MT system as an automatic post-
editor for the output of a commercial rule-based
MT system, showing that it produced better results
than both the rule-based system alone and a sin-
gle pass phrase-based MT system. This technique
is also useful for adapting to a specific domain or
dataset. Isabelle et al. (2007) used a statistical MT
system to automatically post-edit the output of a
generic rule-based MT system, to avoid manually
customizing a system dictionary and to reduce the
amount of manual post-editing required.
For our adaptation systems, we build a core
phrase-based MT system with a large amount of
out-of-domain data, which allows us to have better
coverage of the target language. For an adaptation
system, we then build a second phrase-based MT
system by translating the in-domain train, tune,
and test sets through the core translation system,
then using that data to build the second system.
This system uses only in-domain data: parallel
MT output from the core and the references. In
this system, instead of learning to translate one
language into another, the model learns to trans-
late erroneous MT output into more fluent output
of the same language, which more closely resem-
bles the references.
In this work, we apply this technique for
domain and dialect adaptation, treating Egyp-
tian Arabic as the target language, and the MT-
generated MSA as the erroneous MT output. We
use this approach to adapt to the domain of the
MSA data, and also to adapt to the Egyptian di-
alect. What we refer to as the ?one-step? system is
a core system plus one adaptation system, whereas
the ?two-step? system consists of the core plus two
subsequent adaptation systems. We describe the
systems in more detail in Section 5.
4 Data
For the core English to MSA system, we use
the 5 million parallel sentences of English and
MSA from NIST 2012 as the training set. The
tuning set consists of 1,356 sentences from the
NIST 2008 Open Machine Translation Evalua-
tion (MT08) data (NIST Multimodal Information
Group, 2010a), and the test set consists of 1,313
sentences from NIST MT09 (NIST Multimodal
Information Group, 2010b).
We use a 5-gram MSA language model built
using the SRILM toolkit (Stolcke, 2002) on 260
million words of MSA from the Arabic Gigaword
(Parker et al., 2011). All our MSA parallel data
and monolingual MSA language modeling data
were tokenized with MADA v3.1 (Habash and
Rambow, 2005) using the ATB (Arabic Treebank)
tokenization scheme.
For the adaptation systems, we build a 100k
tri-parallel corpus Egyptian-MSA-English corpus.
The MSA and English parts are extracted from
the NIST corpus distributed by the Linguistic Data
Consortium. The Egyptian sentences are obtained
automatically by extending Mohamed et al. (2012)
method for generating Egyptian Arabic from mor-
phologically disambiguated MSA sentences. This
rule-based method relies on 103 transformation
rules covering essentially nouns, verbs and pro-
nouns as well as certain lexical items. For each
MSA sentence, this method provides more than
one possible candidate, in its original version, the
Egyptian sentence kept was chosen randomly. We
extend the selection algorithm by scoring the dif-
ferent sentences using a language model. For
this, we use SRILM with modified Kneser-Ney
smoothing to build a 5-gram language model. The
model is trained on a corpus including articles ex-
tracted from the Egyptian version of Wikipedia
1
and the Egyptian side of the AOC corpus (Zaidan
and Callison-Burch, 2011). We chose to include
Egyptian Wikipedia for the formal level of sen-
tences in it different from the regular DA written
in blogs or microblogging websites (e.g., Twitter)
and closer to the ones generated by our system.
We split this data into train, tune, and test sets
of 98,027, 960, and 961 sentences respectively,
after removing duplicates across sets. The MSA
corpus was tokenized using MADA and the Egyp-
tian Arabic data was tokenized with MADA-ARZ
v0.4 (Habash et al., 2013), both using the ATB to-
kenization scheme, with alif/ya normalization.
5 System Design
Figure 1 shows a diagram of our three English to
Egyptian Arabic MT systems: (1) the baseline MT
system, (2) the one-step adaptation MT system,
and (3) the two-step adaptation MT system. We
describe each system below.
1
http://arz.wikipedia.org/
198
System Design
Egyptian ArabicEnglish
English
English Egyptian Arabic
Egyptian Arabic
MSA
Translation
Translation
Translation
Domain & Dialect Adaptation
Domain Adaptation Dialect AdaptationIn-domain MSAMSA
100K sent.
100K sent. 100K sent.
100K sent.5M sent.
5M sent.
Baseline MT System
One-Step Adaptation MT System
Two-Step Adaptation MT System
Figure 1: An overview of the different system architectures.
Baseline System
Our baseline system is a single phrase-based En-
glish to Egyptian Arabic MT system, built using
Moses (Koehn et al., 2007) on the 100k corpus de-
scribed in Section 4. This system does not include
any MSA data, nor does it have an adaptation sys-
tem; it is a typical, one-pass MT system that trans-
lates English directly into Egyptian Arabic. We
will show that using adaptation systems improves
the results significantly.
Core System
We base our systems on a core system built us-
ing Moses with the NIST data, a large amount of
parallel English-MSA data from different sources
than our in-domain data (the 100k dataset). Our
core system is also built using Moses. We use
this core system to translate the English side of our
100k train, tune, and test sets into MSA, the output
of which we refer to as MSA?. This MSA? data is
what we use as the source side for the adaptation
systems.
One-Step Adaptation System
To adapt to the domain and dialect of the 100k cor-
pus, we first build a single adaptation system that
translates the MSA? output of the core directly into
Egyptian Arabic using the 100k corpus. The train-
ing data consists of parallel MSA? (the output of
the core) and the Egyptian Arabic from the 100k
dataset. With this system, we can take an English
test set, translate it through the core to get MSA?
output, which we can translate through the adap-
tation system to get Egyptian Arabic.
Two-Step Adaptation System
We also build a two-step adaptation system that
consists of two adaptation steps: one to adapt the
MSA output of the core system to the domain of
the MSA in the 100k corpus, and a second system
to translate the MSA output of the domain adap-
tation system into Egyptian Arabic. We use the
first adaptation system to translate the MSA? train,
tune, and test sets (the output of the core, which is
out-of-domain MSA), into in-domain MSA. This
system is trained on the MSA? output parallel with
the MSA references from the 100k dataset. We
refer to the output of this system as MSA?, be-
cause it has been translated from English into out-
of-domain MSA (MSA?), and then from out-of-
domain MSA to in-domain MSA.
The first adaptation system is used to translate
the MSA? train, tune, and test sets into MSA?.
Then we use these MSA? sets with their parallel
Egyptian Arabic from the 100k dataset to build the
second adaptation system from in-domain MSA to
Egyptian Arabic. We do not use the dialect trans-
formation from (Mohamed et al., 2012) because it
is designed to work with gold-standard annotation
of the MSA text, which we do not have.
System Variants
Since MSA and Egyptian are more similar to each
other than they are to English, we tried several dif-
ferent reordering window sizes to find the optimal
reordering distance for adapting MSA to Egyptian
Arabic, including the typical reordering window
of length 7, a smaller window of length 4, and no
reordering at all. We found a reordering window
199
size of 7 to work best for all our systems, except
for the one-step adaptation system, where no re-
ordering produced the best result.
We also tested two different heuristics for sym-
metrizing the word alignments: grow-diag and
grow-diag-final-and (Och and Ney, 2003). We
found that using grow-diag as our symmetriza-
tion heuristic produced slightly better scores on
the 100k datasets. For the baseline and adaptation
systems we built 5-gram language models with
KenLM (Heafield et al., 2013) using the target side
of the training set, and for the core system we used
the large MSA language model described in sec-
tion 4. We use KenLM because it has been shown
(Heafield, 2011) to be faster and use less memory
than SRILM (Stolcke, 2002) and IRSTLM (Fed-
erico et al., 2008).
6 Evaluation and Results
For evaluation we use multeval (Clark et al.,
2011) to calculate BLEU (Papineni et al.,
2002), METEOR (Denkowski and Lavie, 2011),
TER (Snover et al., 2006), and length of the test set
for each system. We evaluate the core and adap-
tation systems on the MSA and Egyptian sides of
the test set drawn from the 100k corpus, which we
refer to as the 100k sets. The data used for evalua-
tion is a genuine Egyptian Arabic generated from
MSA, just like the data the systems were trained
on. It is not practical to evaluate on naturally-
generated Egyptian Arabic in this case because the
domain of our datasets is very formal, since most
of the text comes from news sources, and dialectal
Arabic is generally used in informal situations.
2
Below we report BLEU scores from our evalua-
tion using tokenized and detokenized system out-
put. We separate our results into the baseline sys-
tem results, the results of the core, the results of
the adaptation systems, and a comparison section.
We specify scores of intermediate system output,
such as MSA, as BLEU (A), and the scores of fi-
nal system output as BLEU (B). For error analysis,
we use METEOR X-ray (Denkowski and Lavie,
2011) to visualize the alignments of our system
results with the references and each other.
For all MT systems we used grow-diag as our
symmetrization heuristic. For each system, we re-
port only the BLEU score of the best reordering
window variant, which is specified in the caption
2
It is important to note that the Egyptian Arabic data we
use is more MSA-like than typical Egyptian because it was
generated directly from MSA.
below each table. The difference in scores be-
tween the different reordering window sizes (7, 4,
and 0) we tried for the adaptation systems was not
large (between 0 and 0.7 BLEU). In the following
tables we present the best results for each adapta-
tion system, which is a reordering window size of
7 for all systems, except for the phrase-based one-
step domain and dialect adaptation system, which
performs better with no reordering (0.2 BLEU bet-
ter than a window of 7, 0.6 BLEU better than a
window of 4), but these small differences in BLEU
scores are within noise. The greatest difference
in scores from the reordering windows was in the
two-step systems domain adaptation step (MSA to
MSA) on top of the phrase-based core, where a re-
ordering window of 7 was 0.7 BLEU better than a
window of 0.
6.1 Baseline System Results
BLEU (B)
Tokenized Detokenized
100k EGY Tune 22.6 22.3
100k EGY Test 21.5 21.1
Table 1: Baseline results (English ? EGY) with a
reordering window size of 7.
The baseline system demonstrates the results of
building a basic MT system directly from English
to Egyptian Arabic. The goal of the core and adap-
tation systems is to achieve better scores than this
initial approach.
6.2 Core System Results
In Table 2, we report BLEU scores for our core
system on its own tuning set, NIST MT08, and
NIST MT09 as a held-out MSA test set. We
also report scores on the tune and test sets used
to build our adaptation systems, both MSA and
Egyptian Arabic. This is not the final system out-
put, but rather these scores are for intermediate
output only, which becomes the input for our ada-
patation systems.
We notice that unsurprisingly the core system
performs much better on the 100k MSA test set
than on the 100k Egyptian Arabic test set, which
is to be expected because the core system is not
trained on any Egyptian Arabic data. This shows
the impact that the dialectal differences make on
MT output. The results on the Egyptian test
set here are the result of evaluating MSA output
against Egyptian Arabic references.
200
BLEU (A)
Tokenized Detokenized
NIST MT08 (Tune) 23.6 22.8
NIST MT09 (Test) 29.3 28.5
100k MSA Tune 39.8 39.3
100k MSA Test 39.4 39.0
100k EGY Tune 28.1 28.1
100k EGY Test 27.7 27.7
Table 2: Core system (English ? MSA) results
using a reordering window size of 7.
6.3 Adaptation System Results
The adaptation systems take as input the MSA out-
put of the core and attempt to improve the scores
on the Egyptian test set by adapting to the domain
of the 100k dataset, as well as to Egyptian Arabic,
in either one or two steps.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 40.8 40.5
100k EGY Test 40.3 40.1
Table 3: One-Step Adaptation system (MSA? ?
Egyptian Arabic) results using a reordering win-
dow size of 0.
Table 3 shows the results of the single adapta-
tion system, which adapts directly from the MSA
output of the core to Egyptian Arabic. These
BLEU scores are already much better than the core
systems performance on the same test sets, im-
proving from 28.1 BLEU to 40.5 BLEU on the
Egyptian Arabic tuning set (a difference of 12.4
BLEU) and improving from 22.7 BLEU to 40.1
BLEU on the Egyptian Arabic test set (a differ-
ence of 17.4 BLEU).
Tables 4 and 5 below illustrate the results of the
first and second steps of the two-step adaptation
system: Table 4 contains the results of the first do-
main adaptation step from out-of-domain MSA to
in-domain MSA and Table 5 contains the results of
the second dialect adaptation step from in-domain
MSA to Egyptian Arabic.
An example of our system output for an English
sentence is given in Table 6. Its METEOR X-ray
alignment is illustrated in Figure 2.
6.4 System Comparisons on 100k Test Sets
In Table 7, we compare the results from the core
and the results from the first step of the two-step
BLEU (A)
Tokenized Detokenized
100k MSA Tune 45.2 44.6
100k MSA Test 44.8 44.2
100k EGY Tune 32.2 32.2
100k EGY Test 32.0 32.0
Table 4: Domain Adaptation system (MSA? ?
MSA?) for Two-Step Adaptation System Results
using a reordering window size of 7.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 43.3 43.2
100k EGY Test 43.1 42.9
Table 5: Dialect Adaptation system (MSA? ?
Egyptian) for Two-Step Adaptation System Re-
sults using a reordering window size of 7.
? ? ????? ??????
?
????? ?????? ?????? ?? ??????? ??????
??
????? ????? ??????? ? ???????????? ? ???????????? ? ????????? ? ????????????? ????????? ? ???? ????? ? ?? ????????????? ? ????????????? ? ?????????? ? ?????Segment 314
P: 0.700 vs 0.900 : 0.200R: 0.700 vs 0.900 : 0.200Frag: 0.214 vs 0.085 : 0.129-Score: 0.550 vs 0.823 : 0.273
Figure 2: METEOR X-ray alignment of the sen-
tence in table 6. The left side is the output of the
one-step system, the right side is the output of the
two-step system, and the top is the reference. The
shaded cells represent matches between the refer-
ence and the one-step system, and the dots repre-
sent matches between the reference and the two-
step system.
adaptation system on the MSA test set and we
see that adapting to the domain improves BLEU
scores on MSA.
Since our goal is to improve the output for
1
One-Step System: Core + Domain and Dialect Adapta-
tion (MSA? ? EGY)
2
Two Step Adaptation System (Step 1): Core + Domain
Adaptation (MSA? ? MSA?)
3
Two Step Adaptation System (Step 2): Core + Domain
Adaptation (MSA??MSA?) + Dialect Adaptation (MSA??
EGY)
201
English UN closes old office in Liberia in preparation for new mission
Egyptian Reference

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

?K
.
A??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlsAbq fy lybyryp AAstEdAdA lmhmp jdydp
1-Step System

?YK


Yg
.

????
?
@X @Y?

J?@ AK


Q



J
.
J


? ?


	
?

??
?

Y

??@ I
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktb AAlqdymp fy lybyryA AAstEdAdA lmhmp jdydp
2-Step System (step2)

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

??
?

Y

??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlqdymp fy lybyryp AAstEdAdA lmhmp jdydp
Table 6: An example of system output from the Egyptian test set.
BLEU (A)
Tokenized Detokenized
Core (English ? MSA?) 39.4 39.0
Core + Domain Adaptation (MSA? ? MSA?) 44.8 44.2
Table 7: Comparison of results on 100k MSA test set.
BLEU (A/B)
Tokenized Detokenized
Baseline (English ? EGY) 21.5 (B) 21.1
Core (English ?MSA
?
) 27.7 (A) 27.7
One-Step Adaptation System
1
40.3 (B) 40.1
Two-Step Adaptation System (Step 1)
2
32.0 (A) 32.0
Two-Step Adaptation System (Step 2)
3
43.1 (B) 42.9
Table 8: Comparison of results on 100k EGY test data.
BLEU (B) METEOR TER Length
Baseline System 21.1 38.5 66.1 102.7
One-Step System 40.1 53.4 51.3 100.0
Two-Step System: Step 2 (Dialect) 42.9 55.2 50.4 100.1
Table 9: Detokenized BLEU, METEOR, TER, and length scores for the best system results.
Egyptian Arabic, we examine the improvement of
scores through different steps of the system in Ta-
ble 8. These scores are all based on the same
Egyptian Arabic references, even though some of
the systems are designed to produce MSA output.
It is important to note that although the first step
of the two-step adaptation system (domain adap-
tation) is still producing MSA output, it performs
better on the Egyptian test set than the out-of-
domain MSA core. The domain adaptation sys-
tem built on top of the core performs better than
the core alone on the 100k corpus MSA test set
(+5.2 BLEU), as well as the 100k corpus Egyptian
Arabic test set (+4.3 BLEU). The best score we
achieve on the 100k corpus MSA test set is 44.2
BLEU, from the core plus the domain adaptation
system.
Table 9 shows the other detokenized scores
from multeval (Clark et al., 2011) from the final
output on the EGY test set from each system, and
Table 10 shows BLEU-1 through BLEU-4 scores
on the same detokenized results, which shows an
improvement at different n-gram levels in unigram
coverage from the baseline system to the adapta-
tion systems.
Overall, the two-step adaptation system built on
top of the core performs 15.2 BLEU better than
the core alone on the 100k corpus Egyptian Ara-
bic test set and the one-step adaptation system per-
forms 12.4 BLEU better than the core on the same
test set. The best score on the 100k EGY test set
is from the two-step adaptation system with 42.9
BLEU, which outperforms the one-step adaptation
system by 2.8 BLEU points. We consider possible
causes of these results in section 7.
202
BLEU-1 BLEU-2 BLEU-3 BLEU-4
Baseline System 53.4 26.6 15.3 9.1
One-Step System 64.3 43.5 33.5 27.1
Two-Step System: Step 2 (Dialect) 65.2 46.0 36.8 30.7
Table 10: Detokenized BLEU (B) scores on the 100k EGY test set at different n-gram levels.
English US , Indonesia commit to closer trade , investment ties
Egyptian Reference

?

K?@

?K


PA?

J

?@?

?K


PAm
.
