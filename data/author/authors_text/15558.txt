Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 971?981,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Confidence in Structured-Prediction using Confidence-Weighted Models
Avihai Mejer
Department of Computer Science
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
Confidence-Weighted linear classifiers (CW)
and its successors were shown to perform
well on binary and multiclass NLP prob-
lems. In this paper we extend the CW ap-
proach for sequence learning and show that it
achieves state-of-the-art performance on four
noun phrase chucking and named entity recog-
nition tasks. We then derive few algorith-
mic approaches to estimate the prediction?s
correctness of each label in the output se-
quence. We show that our approach provides
a reliable relative correctness information as
it outperforms other alternatives in ranking
label-predictions according to their error. We
also show empirically that our methods output
close to absolute estimation of error. Finally,
we show how to use this information to im-
prove active learning.
1 Introduction
In the past decade structured classification has seen
much interest by the machine learning community.
After the introduction of conditional random fields
(CRFs) (Lafferty et al, 2001), and maximum mar-
gin Markov networks (Taskar et al, 2003), which
are batch algorithms, new online method were in-
troduced. For example the passive-aggressive algo-
rithm was adapted to chunking (Shimizu and Haas,
2006), parsing (McDonald et al, 2005b), learning
preferences (Wick et al, 2009) and text segmenta-
tion (McDonald et al, 2005a). These new online
algorithms are fast to train and simple to implement,
yet they generate models that output merely a pre-
diction with no additional information, as opposed
to probabilistic models like CRFs or HMMs.
In this work we fill this gap proposing few al-
ternatives to compute confidence in the output of
discriminative non-probabilistic algorithms. As be-
fore, our algorithms output the highest-scoring la-
beling. However, they also compute additional la-
belings, that are used to compute the per word con-
fidence in its labelings. We build on the recently
introduced confidence-weighted learning (Dredze et
al., 2008; Crammer et al, 2009b) and induce a dis-
tribution over labelings from the distribution main-
tained over weight-vectors.
We show how to compute confidence estimates
in the label predicted per word, such that the con-
fidence reflects the probability that the label is not
correct. We then use this confidence information
to rank all labeled words (in all sentences). This
can be thought of as a retrieval of the erroneous
words, which can than be passed to human anno-
tator for an examination, either to correct these mis-
takes or as a quality control component. Next, we
show how to apply our techniques to active learning
over sequences. We evaluate our methods on four
NP chunking and NER datasets and demonstrate the
usefulness of our methods. Finally, we report the
performance of obtained by CW-like adapted to se-
quence prediction, which are comparable with cur-
rent state-of-the-art algorithms.
2 Confidence-Weighted Learning
Consider the following online binary classification
problem that proceeds in rounds. On the ith round
the online algorithm receives an input xi ? Rd and
971
applies its current rule to make a prediction y?i ? Y ,
for the binary set Y = {?1,+1}. It then receives
the correct label yi ? Y and suffers a loss `(yi, y?i).
At this point, the algorithm updates its prediction
rule with the pair (xi, yi) and proceeds to the next
round. A summary of online algorithms can be
found in (Cesa-Bianchi and Lugosi, 2006).
Online confidence-weighted (CW) learning
(Dredze et al, 2008; Crammer et al, 2008),
generalized the passive-aggressive (PA) update
principle to multivariate Gaussian distributions
over the weight vectors - N (?,?) - for binary
classification. The mean ? ? Rd contains the
current estimate for the best weight vector, whereas
the Gaussian covariance matrix ? ? Rd?d captures
the confidence in this estimate. More precisely,
the diagonal elements ?p,p, capture the confidence
in the value of the corresponding weight ?p ; the
smaller the value of ?p,p, is, the more confident
is the model in the value of ?p. The off-diagonal
elements ?p,q for p 6= q capture the correlation
between the values of ?p and ?q. When the data
is of large dimension, such as in natural language
processing, a model that maintains a full covariance
matrix is not feasible and we back-off to diagonal
covariance matrices.
CW classifiers are trained according to a PA rule
that is modified to track differences in Gaussian dis-
tributions. At each round, the new mean and co-
variance of the weight vector distribution is chosen
to be the solucion of an optimization problem (see
(Crammer et al, 2008) for details). This particu-
lar CW rule may over-fit by construction. A more
recent alternative scheme called AROW (adaptive
regularization of weight-vectors) (Crammer et al,
2009b) replaces the guaranteed prediction at each
round with the a more relaxed objective (see (Cram-
mer et al, 2009b)). AROW has been shown to
perform well in practice, especially for noisy data
where CW severely overfits.
The solution for the updates of CW and AROW
share the same general form,
?i+1 =?i+?i?iyixi ; ?
?1
i+1 =?
?1
i+1+?ixix
>
i , (1)
where the difference between CW and AROW is the
specific instance-dependent rule used to set the val-
ues of ?i and ?i.
Algorithm 1 Sequence Labeling CW/AROW
Input: Joint feature mapping ?(x,y) ? Rd
Initial variance a > 0
Tradeoff Parameter r > 0 (AROW)
or Confidence parameter ? (CW)
Initialize: ?0 = 0 , ?0 = aI
for i = 1, 2 . . . do
Get xi ? X
Predict best labeling
y?i = arg maxz ?i?1 ??(xi, z)
Get correct labeling yi ? Y
|xi|
Define ?i,y,y? = ?(x,yi)??(x, y?i)
Compute ?i and ?i (Eq. (3) for CW ;
Eqs. (4),?i = 1/r) for AROW)
Set ?i = ?i?1 + ?i?i?1?i,y,y?
Set ??1i = ?
?1
i?1 + ?i?i,y,y??
>
i,y,y?
end for
3 Sequence Labeling
In the sequence labeling setting, instances x be-
long to a general input space X and conceptually are
composed of a finite number n of components, such
as words of a sentence. The number of components
n = |x| varies between instances. Each part of an
instance is labelled from a finite set Y , |Y| = K.
That is, a labeling of an entire instance belongs to
the product set y ? Y ? Y . . .Y (n times).
We employ a general approach (Collins, 2002;
Crammer et al, 2009a) to generalize binary clas-
sification and use a joined feature mapping of an
instance x and a labeling y into a common vector
space, ?(x,y) ? Rd.
Given an input instance x and a model ? ? Rd
we predict the labeling with the highest score, y? =
arg maxz ? ??(x, z). A brute-force approach eval-
uates the value of the score ? ??(x, z) for each pos-
sible labeling z ? Yn, which is not feasible for large
values of n. Instead, we follow standard factoriza-
tion and restrict the joint mapping to be of the form,
?(x,y) =
?n
p=1 ?(x, yp)+
?n
q=2 ?(x, yq, yq?1).
That is, the mapping is a sum of mappings, each tak-
ing into consideration only a label of a single part, or
two consecutive parts. The time required to compute
the max operator is linear in n and quadratic in K
using the dynamic-programming Viterbi algorithm.
After the algorithm made a prediction, it uses
972
the current labeled instance (xi,yi) to update the
model. We now define the update rule both for a
version of CW and for AROW for strucutred learn-
ing, staring with CW. Given the input parameter ?
of CW we denote by ?? = 1 + ?2/2, ??? = 1 + ?2.
We follow a similar argument as in the single up-
date of (Crammer et al, 2009a, sec. 5.1) to se-
quence labeling by a reduction to binary classifica-
tion. We first define the difference between the fea-
ture vector associated with the current labeling yi
and the feature vector associated with some label-
ing z to be, ?i,y,z = ?(x,yi) ? ?(x, z) , and
in particular, when we use the prediction y?i we get,
?i,y,y? = ?(x,yi)??(x, y?i) . The CW update is,
?i = ?i?1 + ?i?i?1?i,y,y?
??1i = ?
?1
i?1 + ?i?i,y,y??
>
i,y,y? , (2)
where the two scalars ?i and ?i are set using the
update rule defined by (Crammer et al, 2008) for
binary classification,
vi = ?
>
i,y,y??i?1?i,y,y? , mi = ?i?1 ??i,y,y? (3)
?i = max
{
0,
1
vi???
(
?mi?
? +
?
m2i
?4
4
+ vi?2???
)}
?i =
?i?
?
v+i
, v+i =
1
4
(
??ivi?+
?
?2i v
2
i ?
2 + 4vi
)2
We turn our attention and describe a mod-
ification of AROW for sequence prediction.
Replacing the binary-hinge loss in (Crammer
et al, 2009b, Eqs. (1,2)) the first one with the
corresponding multi-class hinge loss for structured
problems we obtain, 12 (?i??)
>??1i (?i??) +
1
2r (max {0,maxz 6=y {d(y, z)? ? ? (?i,y,z)}})
2,
where, d(y, z) =
?|x|
q=1 1yq 6=zq , is the hamming
distance between the two label sequences y and z.
The last equation is hard to optimize since the max
operator is enumerating over exponential number of
possible labellings z. We thus approximate the enu-
meration over all possible z with the predicted label
sequence y?i and get, 12 (?i??)
>??1i (?i??) +
1
2r
(
max
{
0, d(yi, y?i)? ? ?
(
?i,y,y?
)})2
. Com-
puting the optimal value of the last equation we get
an update of the form of the first equation of Eq. (2)
where
?i =
max
{
0, d(yi, y?i)? ?i?1 ?
(
?i,y,y?
)}
r + ?>i,y,y??i?1?i,y,y?
. (4)
Dataset Sentences Words Features
NP chunking 11K 259K 1.35M
NER English 17.5K 250K 1.76M
NER Spanish 10.2K 317.6K 1.85M
NER Dutch 21K 271.5K 1.76M
Table 1: Properties of datasets.
AROW CW 5-best PA Perceptron
NP chunking 0.946 0.947 0.946 **0.944
NER English 0.878 0.877 * 0.870 * 0.862
NER Dutch 0.791 0.787 0.784 * 0.761
NER Spanish 0.775 0.774 0.773 * 0.756
Table 2: Averaged F-measure of methods. Statistical sig-
nificance (t-test) are with respect to AROW, where * in-
dicates 0.001 and ** indicates 0.01
We proceed with the confidence paramters in
(Crammer et al, 2009b, Eqs. (1,2)), which takes into
considiration the change of confidence due to the up-
date. The effective features vector that is used to
update the mean parameters is ?i,y,y?, and thus the
structured update is, 12 log
(
det ?i
det ?
)
+12Tr
(
??1i?1?
)
+
1
2r?
>
i,y,y???i,y,y? . Solving the above equation we
get an update of the form of the second term of
Eq. (2) where ?i = 1r . The pseudo-code of CW and
AROW for sequence problems appears in Alg. 1.
4 Evaluation
For the experiments described in this paper we used
four large sequential classification datasets taken
from the CoNLL-2000, 2002 and 2003 shared tasks:
noun-phrase (NP) chunking (Kim et al, 2000),
and named-entity recognition (NER) in Spanish,
Dutch (Tjong and Sang, 2002) and English (Tjong
et al, 2003). The properties of the four datasets
are summarized in Table 1. We followed the feature
generation process of (Sha and Pereira, 2003).
Although our primary goal is estimating confi-
dence in prediction and not the actual performance
itself, we first report the results of using AROW and
CW for sequence learning. We compared the perfor-
mance CW and AROW of Alg. 1 with two standard
online baseline algorithms: Averaged-Perceptron al-
gorithm and 5-best PA (the value of five was shown
to be optimal for various tasks (Crammer et al,
2005)). The update rule described in Alg. 1 assumes
a full covariance matrix, which is not feasible in our
973
0.934 0.936 0.938 0.94 0.942 0.9440.936
0.9380.94
0.9420.944
0.9460.948
Recall
Precisio
n
 
 
PerceptronPACWAROW
(a) NP Chunking
0.81 0.82 0.83 0.84 0.85 0.860.82
0.830.84
0.850.86
0.870.88
0.89
Recall
Precisio
n
 
 
PerceptronPACWAROW
(b) NER English
0.68 0.7 0.72 0.74 0.760.7
0.720.74
0.760.78
0.8
Recall
Precisio
n
 
 
PerceptronPACWAROW
(c) NER Dutch
0.7 0.72 0.74 0.760.7
0.72
0.74
0.76
0.78
Recall
Precisio
n
 
 
PerceptronPACWAROW
(d) NER Spanish
Figure 1: Precision and Recall on four datasets (four panels). Each connected set of ten points corresponds to the performance of
a specific algorithm after each of the 10 iterations, increasing from bottom-left to top-right.
Prec Recall F-meas % Err
CW 0.945 0.942 0.943 2.34%
NP chunking
CRF 0.938 0.934 0.936 2.66%
CW 0.838 0.826 0.832 3.38%
NER English
CRF 0.823 0.820 0.822 3.53%
CW 0.803 0.755 0.778 2.05%
NER Dutch
CRF 0.775 0.753 0.764 2.09%
CW 0.738 0.720 0.729 4.09%
NER Spanish
CRF 0.751 0.730 0.740 2.05%
Table 3: Precision, Recall, F-measure and percentage of
mislabeled words results of CW vs. CRF
setting. Three options are possible: compute a full ?
and then take its diagonal elements; compute a full
inverse ?, take its diagonal elements and then com-
pute its inverse; assume that ? is diagonal and com-
pute the optimal update for this choice. We found
the first method to work best, and thus employ it
from now on.
The hyper parameters (r for AROW, ? for CW, C
for PA) were tuned for each task by a single run over
a random split of the data into a three-fourths train-
ing set and a one-fourth test set. We used parameter
averaging with all methods.
For each of the four datasets we used 10-fold
cross validation. All algorithms (Perceptron, PA,
CW and AROW) are online, and as mentioned above
work in rounds. For each of the ten folds, each of the
four algorithm performed ten (10) iterations over the
training set and the performance (Recall, Precision
and F-measure) was evaluated on the test set after
each iteration.
The F-measure of the four algorithms after 10 it-
erations over the four datasets is summarized in Ta-
ble 2. The general trend is that AROW slightly out-
performs CW, which is better than PA that is bet-
ter than the Perceptron. The difference between
AROW and the Perceptron is significant, and be-
tween AROW and PA is significant in two datasets.
The difference between AROW and CW is not sig-
nificant although it is consistent.
We further investigate the convergence properties
of the algorithms in Fig. 1. The figure shows the re-
call and precision results after each training round
averaged across the 10 folds. Each panel summa-
rizes the results on a single dataset, and in each panel
a single set of connected points corresponds to one
algorithm. Points in the left-bottom of the plot cor-
respond to early iterations and points in the right-top
correspond to later iterations. Long segments indi-
cate a big improvement in performance between two
consecutive iterations.
Few points are in order. First, high (in the y-axis)
values indicate better precision and right (in the x-
axis) values indicate better recall. Second, the per-
formance of all algorithms is converging in about 10
iterations as indicated by the fact the points in the
top-right of the plot are close to each other. Third,
the long segments in the bottom-left for the Percep-
tron algorithm indicate that this algorithm benefits
more from more than one pass compared with the
other. Fourth, on the three NER datasets after 10 it-
erations AROW gets slightly higher precision values
than CW, while CW gets slightly higher recall val-
ues than AROW. This is indicated by the fact that
the top-right red square is left and above to the top-
right blue circle. Finally, in two datasets, PA get
slightly better recall than CW and AROW, but pay-
ing in terms of precision and overall F-measure per-
formance.
In addition to online algorithms we also com-
pared the performance of CW with the CRF algo-
974
NP chunking NER English NER Spanish NER Dutch00.05
0.10.15
0.20.25
0.30.35
0.40.45
0.50.55
 
 
CRFKD?Fixed (K=50)KD?PC (K=50)DeltaWKBV (K=30)KBV (K=30)Random
(a) AvgP CW & CRF
NP chunking NER English NER Spanish NER Dutch0
0.02
0.04
0.06
0.08
0.1
Root Me
an Squa
red Error
 in Confi
dence
 
 
CRFKD?Fixed (K=50)KD?PC (K=50)WKBV (K=30)
(b) RMSE CW & CRF
NP chunking NER English NER Spanish NER Dutch00.05
0.10.15
0.20.25
0.30.35
0.40.45
0.50.55
 
 
KD?Fixed (K=50)DeltaWKBV (K=30)KBV (K=30)Random
(c) AvgP PA
NP chunking NER English NER Spanish NER Dutch0
0.02
0.04
0.06
0.08
0.1
Root Me
an Squa
red Error
 in Confid
ence
 
 
KD?Fixed (K=50)WKBV (K=30)
(d) RMSE PA
Figure 2: Two left panels: average precision of rankings of
the words of the test-set according to confidence in the predic-
tion of seven methods (left to right bars in each group): CRF,
KD-Fixed, KD-PC, Delta, WKBV, KBV and random ordering,
when training with the CW algorithm (top) and the PA algo-
rithm (bottom). Two right panels: The root-mean-squared-error
of four methods that output absolute valued confidence: CRF,
KD-Fixed, KD-PC and WKBV.
rithm which is a batch algorithm. We used Mal-
let toolkit (McCallum, 2002) for CRF implementa-
tion. For feature generation we used a combination
of standard methods provided with Mallet toolkit
(called pipes). We chose a combination yielding a
feature set that is close as possible to the feature
set we used in our system but it was not a perfect
match, CRF generated about 20% fewer features in
all datasets. Nevertheless, any other combination of
pipes we tried only hurt CRF performance. The pre-
cision, recall, F-measure and percentage of misla-
beled words of CW algorithm compared with CRF
measured over a single split of the data into a three-
fourths training set and a one-fourth test set is sum-
marized in Table 3. We see that in three of the four
datasets CW outperforms CRF and in one dataset
CRF performs better. Some of the performance dif-
ferences may be due to the differences in features.
5 Confidence in the Prediction
Most large-margin-based training algorithms output
models that their prediction is a single labeling of
the input, with no additional confidence information
about the correctness of that prediction. This situ-
ation is acceptable when the output of the system
is used anyway, irrespectively of its quality. This
situation is not acceptable when the output of the
system is used as an input of another system that is
sensitive to correctness of the specific prediction or
that integrates various input sources. In such cases,
additional confidence information about the correct-
ness of these feeds for specific input can be used
to improve the total output quality. Another case
where such information is useful, is when there is
additional agent that is validating the output of the
system. The confidence information can be used
to direct the check into small number of suspected
predictions as opposed to random check, which may
miss errors if their rate is small.
Some methods only provide relative confidence
information. This information can be used to rank
all predictions according to their confidence score,
which can be used to direct a quality control com-
ponent to detect errors in the prediction. Note,
the confidence score is meaningless by itself and
in fact, any monotonic transformation of the con-
fidence scores yield equivalent confidence informa-
tion. Other methods are providing confidence in the
predicted output as an absolute information, that is,
the probability of a prediction to be correct. We re-
fer to these probabilistic outputs in a frequentists ap-
proach. When taking a large set of events (predic-
tions) with similar probability confidence value ? of
being correct, we expect that about ? fraction of the
predictions in the group will be correct.
Algorithms: All of our methods to evaluate confi-
dence, except two (Delta and CRF below), share the
same conceptual approach and work in two stages.
First, a method generates a set of K possible label-
ings for the input sentence (instead of a single pre-
diction). Then, the confidence in a predicted label-
ing for a specific word is defined to be the proportion
of labelings which are consistent with the predicted
label. Formally, let z(i) for i = 1 . . .K be the K
labelings for some input x, and let y? be the actual
prediction for the input. (We do not assume that
y? = z(i) for some i). The confidence in the label
y?p of word p = 1 . . . |x| is defined to be
?p = |{i : y?p = z
(i)
p }|/K . (5)
975
1000 2000 3000 4000 50000
200400
600800
1000
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(a) NP Chunking
1000 2000 3000 4000 50000200
400600
8001000
12001400
1600
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(b) NER English
1000 2000 3000 4000 50000200
400600
8001000
12001400
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(c) NER Dutch
1000 2000 3000 4000 50000
500
1000
1500
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(d) NER Spanish
1000 2000 3000 4000 5000?100?50
050
100150
200250
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(e) NP Chunking
1000 2000 3000 4000 5000?100?50
050
100150
200250
300
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(f) NER English
1000 2000 3000 4000 5000?100
?50
0
50
100
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(g) NER Dutch
1000 2000 3000 4000 5000?100?50
050
100150
200
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(h) NER Spanish
Figure 3: Total number of detected erroneous words vs. the number of ranked words (top panels), and relative to the Delta method
(bottom panels). In other words, the lines in the bottom panels are the number of additional erroneous words detected compared to
Delta method. All methods builds on the same weight-vector except CRF (see text).
We tried four approaches to generate the set of K
possible labelings. The first method is valid only
for methods that induce a probability distribution
over predicted labels. In this case, we draw K la-
belings from this distribution. Specifically, we ex-
ploit the Gaussian distribution over weight vectors
w ? N (?,?) maintained by AROW and CW, by
inducing a distribution over labelings given an in-
put. The algorithm samples K weight vectors ac-
cording to this Gaussian distribution and outputs the
best labeling with respect to each weight vector. For-
mally, we define the set Z = {z(i) : z(i) =
arg maxzw ??(x, z) where w ? N (?,?)}
The predictions of algorithms that use the mean
weight vector y? = arg maxz ? ??(x, z) are invari-
ant to the value of the input ? (as noted by (Cram-
mer et al, 2008)). However for the purpose of con-
fidence estimation the specific value of ? has a huge
affect. Small eigenvalue of ? yield that all the ele-
ments of Z will be the same, while large values yield
random elements in the set, ignoring the input.
One possible simple option is to run the algorithm
few times, with few possible initializations of ? and
choose one using the training set. However since the
actual predictions of all these versions is the same
(invariance to scaling, see (Crammer et al, 2008))
in practice we run the algorithm once initializing
? = I . Then, after the training is completed, we
try few scalings of the final covariance s? for some
positive scalar s, and choose the best value s using
the training set. We refer to this method as KD-PC
for K-Draws by Parameters Confidence.
The second method to estimate confidence fol-
lows the same conceptual steps, except that we used
an isotropic covariance matrix, ? = sI for some
positive scale information s. As before, the value
of s was tuned on the training set. We denote this
method KD-Fixed for K Draws by Fixed Stan-
dard Deviation. This method is especially appeal-
ing, since it can be used in combination with training
algorithms that do not maintain confidence informa-
tion, such as the Perceptron or PA.
Our third and fourth methods are deterministic
and do not involve a stochastic process. We mod-
ified the Viterbi algorithm to output the K distinct
labelings with highest score (computed using the
mean weight vector in case of CW or AROW). The
third method assigns uniform importance to each
of the K labelings ignoring the actual score val-
ues. We call this method KBV, for K-best Viterbi.
We thus propose the fourth method in which we de-
fine an importance weight ?i to each labeling z(i)
and evaluate confidence using the weights, ?p =(?
i s.t. y?p=z
(i)
p
?i
)
/ (
?
i ?i) , where we set the
weights to be their score value clipped at zero from
below ?i = max{0,? ? ?(x, z(i))}. (In practice,
976
top score was always positive.) We call this method
WKBV for weighted K-best Viterbi.
In addition to these four methods we propose a
fifth method that is based on the margin and does
not share the same conceptual structure of the previ-
ous methods. This method provide confidence score
that is only relative and not absolute, namely its out-
put can be used to compare the confidence in two
labelings, yet there is no semantics defined over the
scores. Given an input sentence to be labeledx and a
model we define the confidence in the prediction as-
sociated with the pthword to be the difference in the
highest score and the closest score, where we set the
label of that word to anything but the label with the
highest score. Formally, as before we define the best
labeling y? = arg maxz ? ? ?(x, z), then the score
of word p is defined to be, ???(x, y?)?maxu6=y?p ??
?(x, z|zp=u) , where we define the labeling z|zp=u
to be the labeling that agrees with z on all words,
except the pth word, where we define its label to
be u. We refer to this method as Delta where the
confidence information is a difference, aka as delta,
between two score values.
Finally, as an additional baseline, we used a sixth
method based on the confidence values for single
words produced by CRF model. We considered the
marginal probability of the word p to be assigned the
predicted label y?p to be the confide value, this prob-
ability is calculated using the forward-backwards al-
gorithm. This method is close in spirit to the Delta
method as the later can be thought of computing
marginals (in score, rather than probability). It also
close to the K-Draws methods, as both CRF and K-
Draws induce a distribution over labels. For CRF we
can compute the marginals explicitly, while for the
Gaussian models generated by CW (or AROW) the
marginals can not be computed expliclity, and thus a
sample based estimation (K-Draws) is used.
Experimental Setting: We evaluate the above
methods as follows. We trained a classifier using
the CW algorithm running for ten (10) iterations on
three-fourth of the data and applied it to the remain-
ing one-fourth to get a labeling of the test set. There
are between 49K ? 54K words to be labeled in
all tasks, except NER Dutch where there are about
74K words. The fraction of words for which the
trained model makes a mistake ranges between 2%
(for NER Dutch) to 4.1% for NER Spanish.
We set the value of the hyper parameter ? to its
optimal value obtained in the experiments reported
in the previous section. The size ofK of the number
of labelings used in the four first methods (KD-PC,
KD-Fixed, KBV, WKBV) and the weighting scalar
s used in KD-PC and KD-Fixed were tuned for each
dataset on a single evaluation on subset of the train-
ing set according to the best measured average pre-
cision. For the parameter s we tried about 20 values
in the range 0.01 to 1.0, and for the number of labels
K we tried the values in 10, 20 . . . 80. The optimal
values are K = 50 for KD-PC and KD-Fixed, and
K = 30 for KBV and WKBV. We noticed that KD-
PC and KD-Fixed were robust to larger values of K,
while the performance of KBV and WKBV was de-
graded significantly for large values of K.
We also trained CRF on the same training sets and
applied it to label and assign confidence values to
all the words in the test sets. The fraction of mis-
labeled words produced by the CRF model and the
CW model is summarized in Table 3.
Relative Confidence: For each of the datasets,
we first trained a model using the CW algorithm and
applied each of the confidence methods on the out-
put, ranking from low to high all the words of the
test set according to the confidence in the prediction
associated with them. Ideally, the top ranked words
are the ones for which the classifier made a mistake
on. This task can be thought of as a retrieval task of
the erroneous words.
The average precision is the average of the pre-
cision values computed at all ranks of erroneous
words. The average precision for ranking the words
of the test-set according the confidence in the predic-
tion of seven methods appears in the top-left panel of
Fig. 2. (left to right bars in each group : CRF, KD-
Fixed, KD-PC, Delta, WKBV, KBV and random or-
dering.) We see that when ordering the words ran-
domly, the average precision is about the frequency
of erroneous word, which is the lowest average pre-
cision. Next are the two methods based on the best
Viterbi labelings, where the weighted approach out-
performing the non-weighted version. Thus, taking
the actual score value into consideration improves
the ability to detect erroneous words. Next in per-
formance is Delta, the margin-induced method. The
977
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(a) NP Chunking
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(b) NER English
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(c) NER Dutch
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(d) NER Spanish
Figure 4: Predicted error in each bin vs. the actual frequency of mistakes in each bin. Best performance is obtained by methods
close to the line y = x (black line) for four tasks. Four methods are compared: weighted K-Viterbi (WKBV), K-draws PC
(KD-PC) and K-draws fixed covariance (KD-Fixed) and CRF.
two best performing among the CW based methods
are KD-Fixed and KD-PC, where the former is bet-
ter in three out of four datasets. When compared
to CRF we see that in two cases CRF outperforms
the K-Draws based methods and in the other two
cases it performs equally. We found the relative suc-
cess of KD-Fixed compared to KD-PC surprising,
as KD-Fixed does not take into consideration the ac-
tual uncertainty in the parameters learned by CW,
and in fact replaced it with a fixed value across all
features. Since this method does not need to as-
sume a confidence-based learning approach we re-
peated the experiment, training a model with the
passive-aggressive algorithm, rather than CW. All
confidence estimation methods can be used except
the KD-PC, which does take the confidence infor-
mation into consideration. The results appear in
the bottom-left panel of Fig. 2, and basically tell
the same story, KD-Fixed outperform the margin
based method (Delta), and the Viterbi based meth-
ods (KBV, WKBV).
To better understand the behavior of the various
methods we plot the total number of detected erro-
neous words vs. the number of ranked words (first
5, 000 ranked words) in the top panels of Fig. 3. The
bottom panels show the relative additional number
of words each methods detects on top of the margin-
based Delta method. Clearly, KD-Fixed and KD-
PC detect erroneous words better than the other CW
based methods, finding about 100 more words than
Delta (when ranking 5, 000 words) which is about
8% of the total number of erroneous words.
Regarding CRF, it outperforms the K-Draws
methods in NER English and NP chunking datasets,
finding about 150 more words, CRF performed
equally for NER Dutch, and performed worse for
NER Spanish finding about 80 less words. We em-
phasize that all methods except CRF were based on
the same exact weight vector, ranking the same pre-
dations, while CRF used an alternative weight vector
that yields different number of erroneous words.
In details, we observe some correlation between
the percentage or erroneous words in the entire set
and the number of erroneous words detected among
the first 5, 000 ranked words. For NP chunking
and NER English datasets, CRF has more erroneous
words compared to CW and it detects more erro-
neous words compared to K-Draws. For NER Dutch
dataset CRF and CW have almost same number of
erroneous words and almost same number of erro-
neous words detected, and finally in NER Spanish
dataset CRF has fewer erroneous words and it de-
tected less erroneous words. In other words, where
there are more erroneous words to find (e.g. CRF in
NP chunking), the task of ranking erroneous words
is easier, and vice-versa.
We hypothesize that part of the performance dif-
ferences we see between the K-Draws and CRF
methods is due to the difference in the number of
erroneous words in the ranked set.
This ranking view can be thought of marking sus-
pected words to be evaluated manually by a human
annotator. Although in general it may be hard for a
human to annotate a single word with no need to an-
notate its close neighbor, this is not the case here. As
the neighbor words are already labeled, and pretty
reliably, as mentioned above.
Absolute Confidence: Our next goal is to eval-
uate how reliable are the absolute confidence val-
ues output by the proposed methods. As before, the
confidence estimation methods (KD-PC, KD-Fixed,
978
KBV, WKBV and CRF) were applied on the entire
set of predicted labels. (Delta method is omitted as
the confidence score it produces is not in [0, 1]).
For each of the four datasets and the five algo-
rithms we grouped the words according to the value
of their confidence. Specifically, we used twenty
(20) bins dividing uniformly the confidence range
into intervals of size 0.05. For each bin, we com-
puted the fraction of words predicted correctly from
the words assigned to that bin. Ultimately, the value
of the computed frequency should be about the cen-
ter value of the interval of the bin. Formally, bin
indexed j contains words with confidence value in
the range [(j ? 1)/20, j/20) for j = 1 . . . 20. Let bj
be the center value of bin j, that is bj = j/20?1/40.
The frequency of correct words in bin j, denoted
by cj is the fraction of words with confidence ? ?
[(j?1)/20, j/20) that their assigned label is correct.
Ultimately, these two values should be the same,
bj = cj , meaning that the confidence information
is a good estimator of the frequency of correct la-
bels. Methods for which cj > bj are too pessimistic,
predicting too high frequency of erroneous labels,
while methods for which cj < bj are too optimistic,
predicting too low frequency of erroneous words.
The results are summarized in Fig 4, one panel
per dataset, where we plot the value of the center-
of-bin bj vs. the frequency of correct prediction cj ,
connecting the points associated with a single algo-
rithm. Four algorithms are shown: KD-PC, KD-
Fixed, WKBV and CRF. We omit the results of the
KBV approach - they were substantially inferior to
all other methods. Best performance is obtained
when the resulting line is close to the line y = x.
From the plots we observe that WKBV is too pes-
simistic as its corresponding line (blue square) is
above the line y = x. CRF method is too optimistic,
its corresponding line is below the line y = x.
The KD-Fixed method is too pessimistic on NER-
Dutch and too optimistic on NER-English. The best
method is KD-PC which, surprisingly, tracks the line
x = y pretty closely. We hypothesis that its superi-
ority is because it makes use of the uncertainty infor-
mation captured in the covariance matrix ? which is
part of the Gaussian distribution.
Finally, these bins plots does not reflect the fact
that different bins were not populated uniformly, the
bins with higher values were more heavily popu-
lated. We thus plot in the top-right of Fig. 2 the
root mean-square error in predicting the bin center
value given by
?(?
j nj(bj ? cj)
2
)
/
(?
j nj
)
,
where nj is the number of words in the jth bin.
We observed a similar trend to the one appeared in
the previous figure. WKBV is the least-performing
method, then KD-Fixed and CRF, and then KD-PC
which achieved lowest RMSE in all four datasets.
Similar plot but when using PA for training appear
in the bottom-right panel of Fig. 2. In this case we
also see that KD-Fixed is better than WKBV, even
though both methods were not trained with an algo-
rithm that takes uncertainty information into consid-
eration, like CW.
The success of KD-PC and KD-Fixed in evaluat-
ing confidence led us to experiment with using sim-
ilar techniques for inference. Given an input sen-
tence, the inference algorithm samplesK times from
the Gaussian distribution and output the best label-
ing according to each sampled weight vector. Then
the algorithm predicts for each word the most fre-
quent label. We found this method inferior to infer-
ence with the mean parameters. This approach dif-
fers from the one used by (Crammer et al, 2009a),
as they output the most frequent labeling in a set,
while the predicted label of our algorithm may not
even belong to the set of predictions.
6 Active Learning
Encouraged by the success of the KD-PC and KD-
Fixed algorithms in estimating the confidence in the
prediction we apply these methods to the task of ac-
tive learning. In active learning, the algorithm is
given a large set of unlabeled data and a small set
of labeled data and works in iterations. On each it-
eration, the overall labeled data at this point is used
to build a model, which is then used to choose new
subset of examples to be annotated.
In our setting, we have a large set of unlabeled
sentences and start with a small set of 50 annotated
sentences. The active learning algorithm is then us-
ing the CW algorithm to build a model, which in turn
is used to rank sentences. The new data items are
then annotated and accumulated to the set of labeled
data points, ready for the next round. Many active
learning algorithms are first computing a prediction
for each of the unlabeled-data examples, which is
979
then used to choose new examples to be labeled. In
our case the goal is to label sentences, which are
expensive to label. We thus applied the following
setting. First, we chose a subset of 9K sentences
as unlabeled training set, and another subset of size
3K for evaluation. After obtaining a model, the al-
gorithm labels random 1, 000 sentences and chose a
subset of 10 sentences using the active learning rule,
which we will define shortly. After repeating this
process 10 times we then evaluate the current model
using the test data and proceed to choose new un-
labeled examples to be labeled. Each method was
applied to pick 5, 000 sentences to be labeled.
In the previous section, we used the confidence
estimation algorithms to choose individual words to
be annotated by a human. This setting is realistic
since most words in each sentence were already clas-
sified (correctly). However, when moving to active
learning, the situation changes. Now, all the words
in a sentence are not labeled, thus a human may need
to label additional words than the one in target, in or-
der to label the target word. We thus experimented
with the following protocol. On each iteration, the
algorithm defines the score of an entire sentence to
be the score of the least confident word in the sen-
tence. Then the algorithm chooses the least confi-
dent sentence, breaking ties by favoring shorter sen-
tences (assuming they contain relatively more infor-
mative words to be labeled than long sentences).
We evaluated five methods, KD-PC and KD-
Fixed mentioned above. The method that ranks
a sentence by the difference in score between the
top- and second-best labeling, averaged over the
length of sentence, denoted by MinMargin (Tong
and Koller, 2001). A similar approach, motivated
by (Dredze and Crammer, 2008), normalizes Min-
Margin score using the confidence information ex-
tracted from the Gaussian covariance matrix, we call
this method MinConfMargin. Finally, We also eval-
uated an approach that picks random sentences to be
labeled, denoted by RandAvg (averaged 5 times).
The averaged cumulative F-measure vs. num-
ber of words labeled is presented in Figs. 5,6. We
can see that for short horizon (small number of sen-
tences) the MinMargin is worse (in three out of four
data sets), while MinConfMargin is worse in NP
Chunking. Then there is no clear winner, but the
KD-Fixed seems to be the best most of the time. The
2000 4000 6000 8000 100000.84
0.850.86
0.870.88
0.890.9
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(a) NP Chunking
2000 4000 6000 8000 100000.35
0.40.45
0.50.55
0.6
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(b) NER English
1040.9
0.9050.91
0.9150.92
0.9250.93
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(c) NP Chunking
1040.62
0.640.66
0.680.7
0.720.74
0.76
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(d) NER English
Figure 5: Averaged cumulative F-score vs. total number of
words labeled. The top panels show the results for up to 10, 000
labeled words, while the bottom panels show the results for
more than 10k labeled words.
bottom panels show the results for more than 10k
training words. Here, the random method perform-
ing the worst, while KD-PC and KD-Fixed are the
best, and as shown in (Dredze and Crammer, 2008),
MinConfMargin outperforming MinMargin.
Related Work: Most previous work has fo-
cused on confidence estimation for an entire exam-
ple or some fields of an entry (Culotta and McCal-
lum, 2004) using CRFs. (Kristjansson et al, 2004)
show the utility of confidence estimation is extracted
fields of an interactive information extraction system
by high-lighting low confidence fields for the user.
(Scheffer et al, 2001) estimate confidence of sin-
gle token label in HMM based information extrac-
tion system by a method similar to the Delta method
we used. (Ueffing and Ney, 2007) propose several
methods for word level confidence estimation for the
task of machine translation. One of the methods they
use is very similar to the weighted and non-weighted
K-best Viterbi methods we used with the proper ad-
justments to the machine translation task.
Acknowledgments
The resrach is supported in part by German-Israeli
Foundation grant GIF-2209-1912. KC is a Horev
Fellow, supported by the Taub Foundations. The re-
viewers thanked for their constructive comments.
980
2000 4000 6000 8000 100000.3
0.350.4
0.450.5
0.55
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(a) NER Dutch
2000 4000 6000 8000 100000.320.34
0.360.38
0.40.42
0.440.46
0.48
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(b) NER Spanish
1040.58
0.60.62
0.640.66
0.680.7
0.72
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(c) NER Dutch
104 1050.480.5
0.520.54
0.560.58
0.60.62
0.64
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(d) NER Spanish
Figure 6: See Fig. 5
References
[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi and
G. Lugosi. 2006. Prediction, Learning, and Games.
Cambridge University Press, New York, NY, USA.
[Collins2002] M. Collins. 2002. Discriminative training
methods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP.
[Crammer et al2005] K. Crammer, R. Mcdonald, and
F. Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. Tech. report, Dept. of
CIS, U. of Penn.
[Crammer et al2008] K. Crammer, M. Dredze, and
F. Pereira. 2008. Exact confidence-weighted learning.
In NIPS 22.
[Crammer et al2009a] K. Crammer, M. Dredze, and
A. Kulesza. 2009a. Multi-class confidence weighted
algorithms. In EMNLP.
[Crammer et al2009b] K. Crammer, A. Kulesza, and
M. Dredze. 2009b. Adaptive regularization of
weighted vectors. In NIPS 23.
[Culotta and McCallum2004] A. Culotta and A. McCal-
lum. 2004. Confidence estimation for information ex-
traction. In HLT-NAACL, pages 109?112.
[Dredze and Crammer2008] M. Dredze and K. Crammer.
2008. Active learning with confidence. In ACL.
[Dredze et al2008] M. Dredze, K. Crammer, and
F. Pereira. 2008. Confidence-weighted linear
classification. In ICML.
[Kim et al2000] E.F. Tjong Kim, S. Buchholz, and
K. Sang. 2000. Introduction to the conll-2000 shared
task: Chunking.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In AAAI, pages 412?418.
[Lafferty et al2001] J. Lafferty, A. McCallum, and
F. Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data.
[McCallum2002] Andrew McCallum. 2002. MALLET:
A machine learning for language toolkit. http://
mallet.cs.umass.edu.
[McDonald et al2005a] R.T. McDonald, K. Crammer,
and F. Pereira. 2005a. Flexible text segmentation with
structured multilabel classification. In HLT/EMNLP.
[McDonald et al2005b] Ryan T. McDonald, Koby Cram-
mer, and Fernando C. N. Pereira. 2005b. Online large-
margin training of dependency parsers. In ACL.
[Scheffer et al2001] Tobias Scheffer, Christian Deco-
main, and Stefan Wrobel. 2001. Active hidden
markov models for information extraction. In IDA,
pages 309?318, London, UK. Springer-Verlag.
[Sha and Pereira2003] Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proc. of HLT-NAACL, pages 213?220.
[Shimizu and Haas2006] N. Shimizu and A. Haas. 2006.
Exact decoding for jointly labeling and chunking se-
quences. In COLING/ACL, pages 763?770.
[Taskar et al2003] B. Taskar, C. Guestrin, and D. Koller.
2003. Max-margin markov networks. In nips.
[Tjong and Sang2002] Erik F. Tjong and K. Sang. 2002.
Introduction to the conll-2002 shared task: Language-
independent named entity recognition. In CoNLL.
[Tjong et al2003] E.F. Tjong, K. Sang, and F. De Meul-
der. 2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL, pages 142?147.
[Tong and Koller2001] S. Tong and D. Koller. 2001.
Support vector machine active learning with applica-
tions to text classification. In JMLR, pages 999?1006.
[Ueffing and Ney2007] Nicola Ueffing and Hermann Ney.
2007. Word-level confidence estimation for machine
translation. Comput. Linguist., 33(1):9?40.
[Wick et al2009] M. Wick, K. Rohanimanesh, A. Cu-
lotta, and A. McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In NIPS Workshop
on Advances in Ranking.
981
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 488?497,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Training Dependency Parser Using Light Feedback
Avihai Mejer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
We introduce lightly supervised learning for
dependency parsing. In this paradigm, the al-
gorithm is initiated with a parser, such as one
that was built based on a very limited amount
of fully annotated training data. Then, the al-
gorithm iterates over unlabeled sentences and
asks only for a single bit of feedback, rather
than a full parse tree. Specifically, given an
example the algorithm outputs two possible
parse trees and receives only a single bit indi-
cating which of the two alternatives has more
correct edges. There is no direct information
about the correctness of any edge. We show
on dependency parsing tasks in 14 languages
that with only 1% of fully labeled data, and
light-feedback on the remaining 99% of the
training data, our algorithm achieves, on av-
erage, only 5% lower performance than when
training with fully annotated training set. We
also evaluate the algorithm in different feed-
back settings and show its robustness to noise.
1 Introduction
Supervised learning is a dominant paradigm in ma-
chine learning in which a prediction model is built
based on examples, each of which is composed of in-
puts and a corresponding full annotation. In the task
of parsing, examples are composed of sentences in
some language and associated with full parse trees.
These parse trees are often generated by human an-
notators. The annotation process is complex, slow
and prone to mistakes as for each sentence a full cor-
rect feedback is required.
We describe light-feedback learning which suits
learning problems with complex or structured out-
put, like parsing. After building an initial classi-
fier, our algorithm reduces the work of the annota-
tor from a full annotation of the input sentence to
a single bit of information. Specifically, it provides
the annotator with two alternative parses of the in-
put sentence and asks for the single bit indicating
which of the alternatives is better. In 95% of the
sentences both alternatives are identical except for a
single word. See Fig. 2 for an illustration. Thus,
the work of the annotator boils down to deciding
for some specific word in the sentence which of two
possible words should be that word?s head.
We show empirically, through simulation, that us-
ing only 1% of the training set with full annotation,
and the remaining 99% with light annotation, our al-
gorithm achieves an average accuracy of about 80%,
only 5% less than a parser built with full annotated
training data. These results are averaged over 14
languages. With additional simple relaxations, our
algorithm achieves average accuracy of 82.5%, not
far from the performance of an algorithm observing
full annotation of the data. We also evaluate our al-
gorithm under few noise settings, showing that it is
resistant to noise, with a decrease of only 1.5% in
accuracy under about 10% feedback noise. We defer
a discussion of related work to the end of the paper.
2 Dependency Parsing and Parsers
Dependency parsing of a sentence is an intermediate
between shallow-parsing, in which a given sentence
is annotated with its part-of-speech, and between a
full structure over the sentence, such as the ones de-
488
fined using context-free grammar. Given a sentence
with n words a parse tree is defined by constructing
a single directed edge outgoing from each word to
its head, that is the word it depends on according to
syntactic or semantic rules. Additionally, one of the
words of the sentence must be labeled as the root of
the tree. The choice of edges is restricted to induce
trees, i.e. graphs with no loops.
Dependency parsers, such as the MSTParser of
(McDonald et al, 2005), construct directed edges
between words of a given sentence to their argu-
ments. We focus on non-projective parsing with
non-typed (unlabeled) edges. MSTParser produces
a parse tree for a sentence by constructing a full di-
rected graph over the words of the sentence with
weighted edges, and then outputting the maximal
spanning tree (MST) of the graph. Given a true parse
tree (aka as gold labeling) and a predicted parse tree
y?, we evaluate the latter by counting the number of
words that are in agreement with the true labeling.
The MSTParser maintains a linear model for set-
ting the weights of the edges of the full graph. Given
the input sentence x the parser sets the weight of
the edge between words xi and xj to be s(i, j) =
w?f(x, i, j) using a feature function f that maps the
input x and a pair of possibly connected words into
Rd. Example features are the distance between the
two words, words identity and words part-of-speech.
The goal of the learning algorithm is to choose a
proper value of w such that the induced tree for each
sentence x will have high accuracy.
Online Learning: MSTParser is training a model
by processing one example at a time using online
learning. On each round the algorithm receives a
new sentence x and the set of correct edges y. It
then computes the score-value of all possible di-
rected edges, s(i, j) = w ? f (x, i, j) for words i, j
using the current parameters w. The algorithm is
computing the best dependency tree y? of this input
x defined to be the MST of the weighted complete
directed graph induced from the matrix {s(i, j)}. It
then uses the discrepancy between the true parse tree
y and the predicted parse tree y? to modify the weight
vector.
MSTParser specifically employs the MIRA algo-
rithm (Crammer et al, 2006) to update the weight
vector w using a linear update,
w?w+?
?
?
?
(i,j)?y
f(x, i, j)?
?
(i,j)?y?
f(x, i, j)
?
? (1)
for input-dependent scalar ? that is defined by the
algorithm. By construction, correct edges (i, j), that
appear both in the true parse tree y and the predicted
parse tree y?, are not affecting the update, as the
terms in the two sums of Eq. (1) cancel each other.
3 Online Learning with Light Feedback
Supervised learning is a very common paradigm in
machine learning, where we assume having access
to the correct full parse tree of every input sentence.
Many algorithms, including MSTParser, explicitly
assume this kind of feedback. Supervised learn-
ing algorithms achieve good performance in depen-
dency parsing, but they come with a price. Human
annotators are required to fully parse each and ev-
ery sentence in the corpora, a long, tedious and ex-
pensive process, which is also prone to mistakes.
For example, the first phase of the famous penn tree
bank project (Marcus et al, 1993) lasted three years,
in which annotators corrected outputs of automated
machines in a rate of 475 words per hour. For super-
vised learning to be successful, typically a large set
of thousands instances is required, which translates
to a long and expensive annotation phase.
Binary or multi-class prediction tasks, such as
spam filtering or document classification, are sim-
ple in the sense that the label associated with each
instance or input is simple. It is either a single bit
indicating whether the input email is spam or not,
or one of few values from a fixed predefined set if
topics. Dependency parsing is more complex as a
decision is required for every word of the sentence,
and additionally there is a global constraint of the
parse being a tree.
In binary classification or multi-class problems it
is only natural to either annotate (or label) an exam-
ple, or not, since the labels are atomic, they cannot
be decomposed to smaller components. The situa-
tion is different in structured tasks such as depen-
dency parsing (or sequence labeling) where each in-
stance is constructed of many elements that each
needs to be annotated. While there are relations and
489
coupling between the elements it is possible to anno-
tate an instance only partially, such as provide a de-
pendency edge only to several words in a sentence.
We take this approach to the extreme, and con-
sider (for now) that for each sentence only a single
bit of labeling will be provided. The choice of what
bit to require is algorithm and example dependent.
We propose using a light feedback scheme in order
to significantly reduce annotation effort for depen-
dency parsing. First, a base or initial model will be
learned from a very small set of fully annotated ex-
amples, i.e. sentences with full dependency infor-
mation known. Then, in a second training stage the
algorithm works in rounds. On each round the al-
gorithm is provided with a new non-annotated sen-
tence which it annotates, hopefully making the right
decision for most of the words. Then the algorithm
chooses subset of the words (or segments) to be an-
notated by humans. These words are the ones that al-
gorithm estimates to be the hardest, or that their true
label would resolve any ambiguity that is currently
existing with the parsing of the input sentence.
Although such partial annotation task may be eas-
ier and faster to annotate, we realize that even partial
annotation if not limited enough can require eventu-
ally similar effort as annotating the entire sentence.
For example, if for a 25-words sentence annotation
is requested for 5 words scattered over the entire sen-
tence, providing this annotation may require the an-
notator to basically parse the entire sentence.
We thus further restrict the possible feedback re-
quested from the annotator. Specifically, given a
new sentence our algorithm outputs two possible an-
notations, or parse trees, y?A and y?B , and asks for
a single bit from the annotator, indicating whether
parse A is better or parse B is better. We do not
ask the annotator to parse the actual sentence, or de-
cide what is the correct parse, but only to state which
of the parses is quantitatively better. Formally, we
say that parse y?A is better if it contains more correct
edges than y?B . The annotator is asked for a single
bit, and thus must state one of the two parses, even
if both parses are equally good. We denote this la-
beling paradigm as binary, as the annotator provides
binary feedback.
The two parses our algorithms presents to the an-
notator are the highest ranking parse and the sec-
ond highest ranking parse according to the current
model. That is, the parse it would output for x and
the best alternative. The feedback required from the
annotator is only which of the two parses is better,
the annotator does not explicitly indicate which of
the edges are labeled correctly or incorrectly, and
furthermore, the annotator does not provide any ex-
plicit information about the correct edge of any of
the words.
In general, the two alternative parses presented
to the annotator may be significantly different from
each other; they may disagree on the edges of many
words. In this case the task of deciding which of
them is better may be as hard as annotating the en-
tire sentence, and then comparing the resulting an-
notation to both alternatives. In practice, however,
due to our choice of features (as functions of the two
words) and model (linear), and since our algorithm
chooses the two parse-trees ranked highest and sec-
ond highest, the difference between the two alterna-
tives is very small. In fact, we found empirically
that, on average, in 95% of the sentences, they differ
in the labeling of only a single word. That is, both
y?A and y?B agree on all words, except some word xi,
for which the first alternative assigns to some word
xj and the second alternative assign to other word
xk. This is due to the fact that the score of the parses
are additive in the edges. Therefore, the parse tree
ranked second highest is obtained from the highest-
ranked parse tree, where for a single word the edge is
replaced, such that the difference between scores is
minimized. For the remaining 5% of the sentences,
replacing an edge as described causes a loop in the
graph induced over words, and thus more than a sin-
gle edge is modified. To minimize the potential labor
of the annotator we simply ignore these cases, and
present the annotator only two alternatives which are
different in a single edge. We refer to this setting or
scenario as single.
To conclude, given a new non-annotated sentence
x the algorithm uses its current model w to out-
put two annotations y?A and y?B which are different
only on a single word and ask the annotator which
is better. The annotator should decide to which of
two possible words xj and xk to connect the word
xi in question. The annotator then feeds the algo-
rithm a single bit, i.e. a binary labeling, which rep-
resents which alternative is better, and the algorithm
updates its internal model w. Although it may be the
490
Input data A set of n unlabeled sentences {xi}ni=1
Input parameters Initial weight vector learned from
fully annotated data u; Number of Iterations over the un-
labeled data T
Initialize w ? u
For t = 1, . . . , T
? For i = 1, . . . , n
? Compute the two configurations y?A and y?B
with highest scores of xi using w
? Ask for feedback : y?A vs. y?B
? Get feedback ? ? {+1,?1}
(or ? ? {+1, 0,?1} in Sec. 5)
? Compute the value of ? using the MIRA algo-
rithm ( or just set ? = 1 for simplicity)
? Update
w+??
?
(i,j)?(y?A/y?B?y?B/y?A)
(?1)[[(i,j)?y?B ]]f(x, i, j)
Output: Weight vector w
Figure 1: The Light-Feedback learning algorithm
case that both alternatives are equally good (or bad),
which occurs only when both assign the wrong word
to xi, that is not xj nor xk are the correct dependents
of xi, the annotator is still required to respond with
one alternative, even though a wrong edge is recom-
mended. Although this setting may induce noise, we
consider it since a human annotator, that is asked to
provide a quick light feedback, will tend to choose
one of the two proposed options, the one that seems
more reasonable, even if it is not correct. We refer to
this combined setting of receiving a binary feedback
only about a single word as Binary-Single. Below
we discuss alternative models where the annotator
may provide additional information, which we hy-
pothesize, would be for the price of labor.
Finally, given the light-feedback ? from the anno-
tator, where ? = +1 if the first parse y?A is preferred
over the second parse y?B , and ? = ?1 otherwise,
we employ a single online update,
w ?w + ??
?
?
?
(i,j)?y?A
f(x, i, j)?
?
(i,j)?y?B
f(x, i, j)
?
?
Pseudocode of the algorithm appears in Fig. 1.
From the last equation we note that the update de-
pends only on the edges that are different between
Figure 2: Example of single edge feedback. The solid blue ar-
rows describe the proposed parse and the two dashed red arrows
are the requested light feedback.
the two alternatives. This provides us the flexibil-
ity of what to show the annotator. One extreme is
to provide the annotator with (almost) a full depen-
dency parse tree, that both alternatives agree on, as
well as the dilemma. This provides the annotator
some context to assist of making a right decision and
fast. The other extreme, is to provide the annotator
only the edges for which the algorithm is not sure
about, omitting any edges both alternatives agree on.
This may remove labeling noise induced by erro-
neous edges both alternatives mistakenly agree on.
Formally, these options are equivalent, and the de-
cision which to use may even be dependent on the
individual annotator.
An example of a light-feedback request is shown
in Fig. 2. The sentence is 12 words long and
the parser succeeded to assign correct edges for 11
words. It was uncertain whether there was a ?sale by
first boston corps? - having the edge ?by?sale? (in-
correct), or there was an ?offer by first boston corps?
- having the edge ?by?offered? (correct). In this
example, a human annotator can easily clarify the
dilemma.
4 Evaluation
We evaluated the light feedback model using 14 lan-
guages: English (the Penn Tree Bank) and the re-
maining 13 were used in CoNLL 2006 shared task1.
The number of training sentences in the training
datasets is ranging is between about 1.5?57K, with
an average of about 14K sentences and 50K?700K
words. The test sets contain an average of ? 590
sentences and ?10K words for all datasets. The av-
erage number of words per sentence vary from 6 in
Chinese to 37 in Arabic.
1Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, Ger-
man, Japanese, Portuguese, Slovene, Spanish, Swedish and
Turkish . See http://nextens.uvt.nl/?conll/
491
Experimental Setup For each of the languages
we split the data into two parts of relative fraction of
p and 1?p for p = 10%, 5% and 1% and performed
training in two stages. First, we used the smaller set
to build a parser using standard supervised learning
procedure. Specifically, we used MSTParser and ran
the MIRA online learning algorithm for 5 iterations.
This process yielded our initial parser. Second, the
larger portion, which is the remaining of the training
set, was used to improve the initial parser using the
light feedback algorithm described above. Our algo-
rithm iterates over the sentences of the larger subset
and each sentence was parsed by the current parser
(parameterized by w) and asked for a preference be-
tween two specific parses for that sentence. Given
this feedback, the algorithm updated its model and
proceeded for the next sentence. The true parse of
these sentences was only used to simulate light feed-
back and it was never provided to the algorithm. The
performance of all the trained parsers was evaluated
on a fixed test set. We performed five iterations of
the larger subset during the light feedback training.
4.1 Results
The results of the light-feedback training after only
a single iteration are given in the two left plots of
Fig. 3. One plot shows the performance averaged
over all languages, and second plot show the results
for English. The black horizontal line shows the ac-
curacy achieved by training the parser on the entire
annotated data using the MIRA algorithm for 10 it-
erations. The predicted edge accuracy of the parser
trained on the entire annotated dataset ranges from
77% on Turkish to 93% on Japanese, with an aver-
age of 85%. This is our skyline.
The blue bars in each plot shows the accuracy of
a parser trained with only a fraction of the dataset
- 10% (left group), via 5% (middle) to 1% (right
group). As expected reducing the amount of training
data causes degradation in performance, from an ac-
curacy of about 76.3% (averaged over all languages)
via 75% to 70.1% when training only with 1% of the
data. These performance levels are our baselines,
one per specific amount of fully annotated data and
lightly annotated data.
The red bar in each pair, shows the contribution
of a single training epoch with light-feedback on the
performance. We see that training with light feed-
back improves the performance of the final parser.
Most noticeably, is when using only 1% of the fully
annotated data for initial training, and the remaining
99% of the training data with light feedback. The
accuracy on test set improves from 70.1% to 75.6%,
an absolute increase of 5.5%. These results are av-
eraged over all languages, individual results for En-
glish are also shown. In most languages, including
those not shown, these trends remain: when reduc-
ing the fraction of data used for fully supervised
training the performance decreases, and light feed-
back improves it, most substantially for the smallest
fraction of 1%.
We also evaluated the improvement in accuracy
on the test set by allowing more than a single itera-
tion over the larger fraction of the training set. The
results are summarized in two right plots of Fig. 3,
accuracy averaged over all languages (left), and for
English (right). Each line refers to a different ratio
of split between full supervised learning and light
feedback learning - blue for 90%, green for 95% and
red for 99%. The x-axis is the number of light feed-
back iterations, from zero up to five. The y-axis is
the accuracy. In general more iterations translates to
improvement in performance. For example, build-
ing a parser with only 1% of the training data yields
70.1% accuracy on the test set, a single iteration of
light-feedback on the remaining 99% improves the
performance to 75.6%, each of the next iterations
improves the accuracy by about 1? 2% up to an ac-
curacy of about 80%, which is only 5% lower than
the skyline. We note again, that the skyline was ob-
tained by using full feedback on the entire training
set, while our parser used at most five bits of feed-
back per sentence from the annotator, one bit per it-
eration.
As noted above, on each sentence, and each it-
eration, our algorithm presents a parsing query or
?dilemma?: should word a be assigned to word b
or word c. These queries are generated indepen-
dently of the previous queries shown, and in fact the
same query may be presented again in a later iter-
ation although already shown in an early one. We
thus added a memory storage of all queries to the
algorithm. When a query is generated by the algo-
rithm, it first checks if an annotation of it already
exists in memory. If this is the case, then no query is
issued to the annotator, and the algorithm simulates
492
90 95 9970
7274
7678
8082
8486
Average
%LightFeedback
Accurac
y on tes
t set
 
 
w/o light feedbackw light feedbackwith ALL data 90 95 9978
8082
8486
8890
English
%LightFeedback
Accurac
y on tes
t set
 
 
w/o light feedbackw light feedbackwith ALL data 0 1 2 3 4 570
7274
7678
8082
8486
Average
Iteration
Accurac
y on test
 set
 
 
909599with ALL data 0 1 2 3 4 578
8082
8486
8890
English
Iteration
Accurac
y on test
 set
 
 
909599with ALL data
Figure 3: Two left plots: Evaluation in Binary-Single light feedback setting. Averaged accuracy over all languages (left) and for
English. The horizontal black line shows the accuracy when training the parser on the entire annotated training data - ?skyline?.
Each pair of bars shows the results for a parser trained with small amount of fully annotated data (left blue bar) and a parser that
was then trained with a single iteration of light feedback on the remaining training data (right red bar). Two right plots: Evaluation
of training with up to five iterations of binary-single light feedback. The plots show the average accuracy (left), and for English.
Each line refers to a different ratio of split between full supervised learning and light feedback learning. The x-axis is the number
of iterations of light feedback, from zero to five.
a query and response using the stored information.
The fraction of new queries, that were actually
presented to the annotator, when light-training with
99% of the training set, is shown in the left panel
of Fig. 4. Each line corresponds to one language.
The languages are ordered in the legend according
to the average number of words per sentence: from
Chinese (6) to Arabic (37). Each point shows the
fraction of new queries (from the total number of
sentences with light-feedback) (y-axis) vs. the itera-
tion (x-axis). Two trends are observed. First, in later
iterations there are less and less new queries (or need
for an actual interaction with the annotator). By def-
inition, all queries during the first iteration are new,
and the fraction of new queries after five iteration
ranges from about 20% (Japanese and Chinese) to a
bit less than 80% (Arabic).
The second trend is across the average number of
words per sentence, the larger this number is, the
more new queries there are in multiple iterations.
For example, in Arabic (37 words per sentence) and
Spanish (28) about 80% of the light-training sen-
tences induce new queries in the fifth iteration, while
in Chinese (6) and Japanese (8) only about 20%.
As expected, longer sentences require, on average,
more queries before getting their parse correctly.
We can also compare the performance improve-
ment achieved by light feedbacks with the per-
formance achieved by using the same amount of
labeled-edges using fully annotated sentences in
standard supervised training. The average sentence
length across all languages is 18 words. Thus, re-
ceiving feedback regarding a single word in a sen-
tence equals to about 1/18 ? 5.5% of the informa-
tion provided by a fully annotated sentence. There-
fore, we may view the light-feedback provided for
99% of the dataset as about equal to additional 5.5%
of fully annotated data.
From the second plot from the right of Fig. 3, we
see that by training with 1% of fully annotated data
and a single iteration of light feedback over the re-
maining 99% of the data, the parser performance
is 75.6% (square markers at x = 1), compared to
75% obtained by training with 5% of fully anno-
tated data (diamond markers at x = 0). A second
iteration of light feedback on 99% of the dataset
can be viewed as additional . 5% of labeled data
(accounting for repeating queries). After the sec-
ond light feedback iteration, the parser performance
is 77.8% (square markers at x = 2), compared to
76.3% achieved when training with 10% of fully an-
notated data (circle markers at x = 0). Similar rela-
tions can be observed for English in the right plot of
Fig. 3. From these observations, we learn that on av-
erage, for about the same amount of labeled edges,
light feedback learning gains equal, or even better,
performance compared with fully labeled sentences.
5 Light Feedback Variants
Our current model is restrictive in two ways: first,
the algorithm does not pass to the annotators exam-
ples for which the disagreements is larger than one
word; and second, the annotator must prefer one of
the two alternatives. Both restrictions were set to
make the annotators? work easier. We now describe
the results of experiments in which one or even both
493
1 2 3 4 50
0.10.2
0.30.4
0.50.6
0.70.8
0.91
IterationsF
raction
 of Ne
w Edg
es Que
ried by
 the An
notato
r
 
 
chinese ( 6)japanese ( 8)turkish (12)dutch (14)bulgarian (15)swedish (15)slovene (16)danish (18)portuguese (20)english (23)spanish (28)arabic (37) 90 95 990.78
0.790.8
0.810.82
0.830.84
0.85
% of Light feedback data
Accura
cy on 
test se
t
 
 
Binary?SingleBinary?MultiTernary?SingleTernary?Multi 0 5 10 15 20 25 300.7
0.720.74
0.760.78
0.80.82
0.84
% of feedback noise
Accura
cy on t
est set
 
 Binary?MultiTernary?Multi
Figure 4: Left: the fraction of new queries presented to the annotator after each of the five iterations (x-axis) for all 14 languages,
when light-training with 99% of the entire training data. Middle: comparison of the accuracy achieved using the four light feedback
models using different fraction of the data for light feedback stage. The results are averaged over all the languages. Right: Effect of
light-feedback noise on the accuracy of the trained model. Results are averaged over all languages for two light feedback settings,
the ternary-multi and binary-multi. The plots show the performance measured on test set according the amount of feedback noise
added. The black line is the baseline of the initial parser trained on 1% of annotated data.
restrictions are relaxed, which may make the work
of the annotator harder, but as we shall see, improves
performance.
Our first modification is to allow the algorithm to
pass the annotator also queries on two alternatives
y?A and y?B that differ on more than a single edge.
As mentioned before, we found empirically that this
arises in only ?5% of the instances. In most cases
the two alternatives differ in two edges, but in some
cases the alternatives differ in up to five edges. Typ-
ically when the alternatives differ on more than a
single edge, the words in question are close to each
other in the sentence (in terms of word-distance)
and are syntactically related to each other. For ex-
ample, if changing the edge (i, j) to (i, k) forms a
loop in the dependency graph then also another edge
(k, l) must be changed to resolve the loop, so the
two edges different between the alternatives are re-
lated. Nevertheless, even if the two alternatives are
far from being similar, the annotator is still required
to provide only a binary feedback, indicating a strict
preference between the two alternatives. We refer to
this model as Binary-Multi, for binary feedback and
possibly multiple different edge between the alter-
natives.
Second, we enrich the number of possible re-
sponses of the annotator from two to three, giving
the annotator the option to respond that the two al-
ternatives y?A and y?B are equally good (or bad), and
no one should be preferred by the other. In this case
we set ? = 0 in the algorithm of Fig. 1, and as can
be seen in the pseudocode, this case does not modify
the weight vector w associated with the parser. Such
feedback will be received when both parses have the
same number of errors. (We can also imagine a hu-
man annotator using the equal feedback to indicate
?don?t know?). For the common case of single edge
difference between the two parses, this means that
both proposed edges are incorrect. Since there are
three possible responds we call this setting ternary.
This setting can be combined with the previous one
and thus we have in fact two new settings. The third
setting is when only single edges are presented to the
annotator, yet three possible responds are optional.
We call this setting Ternary-Single . The fourth, is
when the two alternatives may differ in more than a
single edge and three possible responds are optional
- Ternary-Multi setting.
The accuracy, averaged over all 14 languages, af-
ter 5 light feedback iterations, for all four settings
is shown in the middle plate of Fig. 4. Each of
the three groups summarizes the results for differ-
ent split of the training set to full training and light-
training: 90%, 95% and 99% (left to right; portion
of light training). The horizontal black line shows
the accuracy skyline (85% obtained by using all the
training set in full supervised learning). Each bar in
each group shows the results for one of the four set-
tings: Binary-Single, Binary-Multi, Ternary-Single
and Ternary-Multi. We focus our discussion in the
99% split. The averaged accuracy using Binary-
Single feedback setting is about 80% (left bar). Re-
laxing the type of input to include alternatives that
differ on more than one edge, improves accuracy by
1.4% (second bar from left). Slightly greater im-
provement is shown when relaxing the type of feed-
494
back, from binary to ternary (third bar from left).
Finally, relaxing both constraints yields an improve-
ment of additional 1% to an averaged accuracy of
82.5% which is only 2.5% lower than the skyline.
Moving to the other splits of 95, 90% we observe
that relaxing the feedback from binary to ternary im-
proves the accuracy more than requiring to provide a
preference of parses that differ on more than a single
word.
6 Noisy Light Feedback
In the last section we discussed relaxations that re-
quires slightly more effort from the annotator to gain
higher test accuracy. The intent of the light feed-
back is to build a high-accuracy parser, yet faster
and with less human effort compared with full su-
pervised learning, or alternatively, allow collecting
feedbacks from non-experts. We now evaluate the
effect of light-feedback noise, which may be a con-
sequence of asking the annotator to perform quick
(and rough) light-feedback. We experiment with two
settings in which the feedback of the annotator is
either binary or ternary, in the multi settings, when
99% of the training-data is used for light-feedback.
These settings refer to the second and fourth bar in
the right group of the middle plate of Fig. 4.
We injected independent feedback errors to a frac-
tion of  of the queries, where  is ranging between
0? 30%. In the Binary-Multi setting, we flipped the
binary preference with probability . For example, if
y?A is better than y?B then with probability  the light
feedback was the other way around. In the Ternary-
Multi setting we changed the correct feedback to one
of the other two possible feedbacks with probabil-
ity , the specific alternative chosen was chosen uni-
formly. E.g., if indeed y?A is preferred over y?B , then
with probability /2 the feedback was that y?B is pre-
ferred and with probability /2 that both are equal.
The accuracy vs. noise level for both settings is
presented in the right panel of Fig. 4. The black line
shows the baseline performance after training an ini-
tial parser on 1% of annotated data. Performance
of the parser trained using the Binary-Multi setting
drops by only 1% from 81.4% to 80.4% at error rate
of 5% and eventually as the feedback noise increase
to 30% the performance drops to 70% - the perfor-
mance level achieved by the initial trained model.
The accuracy of the parser trained in the richer
Ternary-Multi setting suffers only 1% performance
decrease at error rate of 10%, and eventually 5% de-
crease from 82.5% to 77.5% as the feedback noise
increase to 30%, still a 7.5% improvement over the
initial trained parser.
We hypothesize that learning with ternary feed-
back is more robust to noise, as in half of the noisy
feedbacks when there is a strict preference between
the alternatives, the effect of the noise is not to
update the model and practically ignore the input.
Clearly, this is preferable than the other outcome
of the noise, that forces the algorithm to make the
wrong update with respect to the true preference.
We also experimented with sentence depended
noise by training a secondary parser on a subset of
the training set, and emulating the feedback-bit us-
ing its output. Its averaged test error (=noise level)
is 22%. Yet, the accuracy obtained by our algo-
rithm with it is 77%, about the same as achieving
with 30% random annotation noise. We hypothesize
this is since the light-feedbacks are requested specif-
ically on the edges harder to predict, where the error
rate is higher than the 22% average error rate of the
secondary parser.
7 Related work
Weak-supervision, semi-supervised and active
learning (e.g. (Chapelle et al, 2006), (Tong and
Koller, 2001)) are general approaches related to the
light-feedback approach. These approaches build
on access to a small set of labeled examples and a
large set of unlabeled examples.
The work of Hall et al (2011) is the most simi-
lar to the light feedback settings we propose. They
apply an automatic implicit feedback approach for
improving the performance of dependency parsers.
The parser produces the k-best parse trees and an
external system that uses these parse trees provides
feedback as a score for each of the parses. In our
work, we focus on minimal updates by both restrict-
ing the number of compared parses to two, and hav-
ing them being almost identical (up to a single edge).
Hwa (1999) investigates training a phrase struc-
ture parser using partially labeled data in several set-
tings. In one of the settings, a parser is first trained
using a large fully labeled dataset from one domain
495
and then adapted to another domain using partial la-
beling. The parts of the data that are labeled are se-
lected in one of two approaches. In the first approach
phrases are randomly selected to be annotated. In
the second approach the phrases are selected accord-
ing to their linguistic categories based on predefined
rules. In both cases, the true phrases are provided. In
our work, we train the initial parser on small subset
of the data from the same domain. Additionally, the
feedback queries are selected dynamically according
to the edges estimated to be hardest for the parser.
Finally, we request only limited feedback and the
true parse is never provided directly.
Chang et al (2007) use a set of domain specific
rules as automatic implicit feedback for training in-
formation extraction system. For example, they use
a set of 15 simple rules to specify the expected for-
mats of fields to be extracted from advertisements.
The light feedback regarding a prediction is the
number of rules that are broken. That feedback is
used to update the prediction model.
Baldridge and Osborne (2004) learns an HPSG
parser using active learning to choose sentences to
be annotated from a large unlabeled pool. Then,
like our algorithm the annotator is presented with a
proposed parse with several local alternatives sub-
parse-trees. Yet, the annotator manually provides
the correct parse, if it is not found within the pro-
posed alternatives. Kristjansson et al (2004) em-
ploy similar approach of combining active learning
with corrective feedback for information extraction.
Instances with lowest confidence using the current
model are chosen to be annotated. Few alternative
labels are shown to the user, yet again, the correct
labeling is added manually if needed. The alterna-
tives shown to the user are intended to reduce the
effort of obtaining the right label, but eventually the
algorithm receives the correct prediction. Our al-
gorithm is passive about examples (and active only
about subset of the labels), while their algorithm
uses active learning to also choose examples. We
plan to extend our work in this direction. Addition-
ally, in these works, the feedback requests involve
many alternatives and providing the true annotation,
in oppose to the limited binary or ternary feedback.
Yet our results show that despite of these limitations
the trained parser achieved performance nor far from
the performance of a parser training using the entire
annotated dataset.
Finally, our setting is related to bandits (Cesa-
Bianchi and Lugosi, 2006) where the feedback is
extremely limited, a binary success-failure bit.
8 Summary
We showed in a series of experimental simulations
that using light-feedback it is possible to train a de-
pendency parser that achieves parsing performance
not far from standard supervised training. Further-
more, very little amount of fully annotated data,
even few tens of sentences, is sufficient for build-
ing an initial parser which can then be significantly
improved using light-feedbacks.
While light-feedback training and standard super-
vised training with about the same number of to-
tal annotated edges may achieve close performance,
we still view it as a possible alternative training
framework. The reduction of the general annota-
tion task into focused and small feedback requests,
opens possibilities for receiving these feedbacks be-
yond expert labeling. In our ongoing work we study
feedbacks from a large group of non-experts, and
possibly even automatically. Additionally, we inves-
tigate methods for selecting light-feedback queries
that are not necessarily derived from the highest
scoring parse and the best alternative parse. For ex-
ample, selecting queries that would be easy to an-
swer by non-experts.
496
References
[Baldridge and Osborne2004] J. Baldridge and M. Os-
borne. 2004. Active learning and the total cost of
annotation. In EMNLP.
[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi and
G. Lugosi. 2006. Prediction, Learning, and Games.
Cambridge University Press, New York, NY, USA.
[Chang et al2007] Ming-Wei Chang, Lev Ratinov, and
Dan Roth. 2007. Guiding semi-supervision with
constraint-driven learning. In In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics.
[Chapelle et al2006] O. Chapelle, B. Scho?lkopf, and
A. Zien, editors. 2006. Semi-Supervised Learning.
MIT Press, Cambridge, MA.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
[Hall et al2011] Keith Hall, Ryan McDonald, Jason
Katz-Brown, and Michael Ringgaard. 2011. Training
dependency parsers by jointly optimizing multiple ob-
jectives. In In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
[Hwa1999] Rebecca Hwa. 1999. Supervised grammar
induction using training data with limited constituent
information. CoRR, cs.CL/9905001.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In AAAI, pages 412?418.
[Marcus et al1993] Mitchell P. Marcus, Mary Ann
Marcinkiewicz, and Beatrice Santorini. 1993. Build-
ing a large annotated corpus of english: the penn tree-
bank. Comput. Linguist., 19:313?330, June.
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
HLT/EMNLP.
[Tong and Koller2001] S. Tong and D. Koller. 2001.
Support vector machine active learning with applica-
tions to text classification. In JMLR, pages 999?1006.
497
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 573?576,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Are You Sure? Confidence in Prediction of Dependency Tree Edges
Avihai Mejer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
We describe and evaluate several methods for
estimating the confidence in the per-edge cor-
rectness of a predicted dependency parse. We
show empirically that the confidence is asso-
ciated with the probability that an edge is se-
lected correctly and that it can be used to de-
tect incorrect edges very efficiently. We eval-
uate our methods on parsing text in 14 lan-
guages.
1 Introduction
Dependency parsers construct directed edges be-
tween words of a given sentence to their arguments
according to syntactic or semantic rules. We use
MSTParser of McDonald et al (2005) and focus
on non-projective dependency parse trees with non-
typed (unlabeled) edges. MSTParser produces a
parse tree for a sentence by constructing a full, di-
rected and weighted graph over the words of the
sentence, and then outputting the maximal spanning
tree (MST) of the graph. A linear model is em-
ployed for computing the weights of the edges using
features depending on the two words the edge con-
nects. Example features are the distance between the
two words, words identity and words part-of-speech.
MSTParser is training a model using online learning
and specifically the MIRA algorithm (Crammer et
al., 2006). The output of MSTParser is the highest
scoring parse tree, it is not accompanied by any ad-
ditional information about its quality.
In this work we evaluate few methods for estimat-
ing the confidence in the correctness of the predic-
tion of a parser. This information can be used in
several ways. For example, when using parse trees
as input to another system such as machine transla-
tion, the confidence information can be used to cor-
rect inputs with low confidence. Another example
is to guide manual validation to outputs which are
more likely to be erroneous, saving human labor.
We adapt methods proposed by Mejer and Cram-
mer (2010) in order to produce per-edge confidence
estimations in the prediction. Specifically, one ap-
proach is based on sampling, and another on a gen-
eralization of the concept of margin. Additionally,
we propose a new method based on combining both
approaches, and show that is outperforms both.
2 Confidence Estimation In Prediction
MSTParser produces the highest scoring parse trees
using the trained linear model with no additional
information about the confidence in the predicted
tree. In this work we compute per-edge confidence
scores, that is, a numeric confidence value, for
all edges predicted by the parser. Larger score
values indicate higher confidence. We use three
confidence estimation methods that were proposed
for sequence labeling (Mejer and Crammer, 2010),
adapted here for dependency parsing. A fourth
method, described in Sec. 3, is a combination of the
two best performing methods.
The first method, named Delta, is a margin-based
method. For computing the confidence of each edge
the method generates an additional parse-tree, which
is the best parse tree that is forced not to contain the
specific edge in question. The confidence score of
the edge is defined as the difference in the scores be-
573
tween the two parse trees. The score of a tree is the
sum of scores of the edges it contains. These con-
fidence scores are always positive, yet not limited
to [0, 1]. Delta method does not require parameter
tuning.
The second method, named Weighted K-Best
(WKB), is a deterministic method building on prop-
erties of the inference algorithm. Specifically,
we use k-best Maximum Spanning Tree algorithm
(Hall, 2007) to produce the K parse trees with the
highest score. This collection of K-trees is used to
compute the confidence in a predicted edge. The
confidence score is defined to be the weighted-
fraction of parse trees that contain the edge. The
contribution of different trees to compute this frac-
tion is proportional to their absolute score, where the
tree with the highest score has the largest contribu-
tion. Only trees with positive scores are included.
The computed score is in the range [0, 1]. The value
of K was tuned using a development set (optimiz-
ing the average-precision score of detecting incor-
rect edges, see below) and for most datasets K was
set to a value between 10? 20.
The third method, K Draws by Fixed Standard
Deviation (KD-Fix) is a probabilistic method. Here
we sample K weight vectors using a Gaussian dis-
tribution, for which the mean parameters are the
learned model and isotropic covariance matrix with
fixed variance s2. The value s is tuned on a develop-
ment set (optimizing the average-precision score of
detecting incorrect edges). The confidence of each
edge is the probability of this edge induced from the
distribution over parameters. We approximate this
quantity by sampling K parse trees, each obtained by
finding the MST when scores are computed by one
of K sampled models. Finally, the confidence score
of each edge predicted by the model is defined to
be the fraction of parse trees among the K trees that
contain this edge. Formally, the confidence score is
? = j/K where j is the number of parse trees that
contain this edge (j ? {0. . .K}) so the score is in
the range [0, 1]. We set K = 50.
Finally, we describe below a fourth method,
we call KD-Fix+Delta, which is a weighted-linear
combination of KD-Fix and Delta.
3 Evaluation
We evaluated the algorithms using 13 languages
used in CoNLL 2006 shared task1, and the English
Penn Treebank. The number of training sentences is
between 1.5-72K, with an average of 20K sentences
and 50K-1M words. The test sets contain ? 400
sentences and ?6K words for all datasets, except
English with 2.3K sentences and 55K words. Pa-
rameter tuning was performed on development sets
with 200 sentences per dataset. We trained a model
per dataset and used it to parse the test set. Pre-
dicted edge accuracy of the parser ranges from 77%
on Turkish to 93% on Japanese, with an average of
85%. We then assigned each predicted edge a confi-
dence score using the various confidence estimation
methods.
Absolute Confidence: We first evaluate the accu-
racy of the actual confidence values assigned by all
methods. Similar to (Mejer and Crammer, 2010) we
grouped edges according to the value of their con-
fidence. We used 20 bins dividing the confidence
range into intervals of size 0.05. Bin indexed j
contains edges with confidence value in the range
[ j?120 ,
j
20 ] , j = 1..20. Let bj be the center value of
bin j and let cj be the fraction of edges predicted
correctly from the edges assigned to bin j. For a
good confidence estimator we expect bj ? cj .
Results for 4 datasets are presented in Fig. 1. Plots
show the measured fraction of correctly predicted
edges cj vs. the value of the center of bin bj . Best
performance is obtained when a line corresponding
to a method is close to the line y = x. Results are
shown for KD-Fix and WKB; Delta is omitted as it
produces confidence scores out of [0, 1]. In two of
the shown plots (Chinese and Swedish) KD-Fix (cir-
cles) follows closely the expected accuracy line. In
another plot (Danish) KD-Fix is too pessimistic with
line above y = x and in yet another case (Turkish) it
is too optimistic. The distribution of this qualitative
behavior among the 14 datasets is: too optimistic
in 2 datasets, too pessimistic in 7 and close to the
line y = x in 5 datasets. The confidence scores
produced by the WKB are in general worse than
KD-Fix, too optimistic in some confidence range
1Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, Ger-
man, Japanese, Portuguese, Slovene, Spanish, Swedish and
Turkish . See http://nextens.uvt.nl/?conll/
574
0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Chinese
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Swedish
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Turkish
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Danish
 
 
KD?FixWKB
Figure 1: Evaluation of KD-Fix and WKB by comparing predicted accuracy vs. actual accuracy in each bin on 4 datasets. Best
performance is obtained for curves close to the line y=x (black line). Delta method is omitted as its output is not in the range [0, 1].
KD WKB Delta KD-Fix Random
Fix +Delta
Avg-Prec 0.535 0.304 0.518 0.547 0.147
Prec @10% 0.729 0.470 0.644 0.724 0.145
Prec @90% 0.270 0.157 0.351 0.348 0.147
RMSE 0.084 0.117 - - 0.458
Table 1: Row 1: Average precision in ranking all edges ac-
cording confidence values. Rows 2-3: Precision in detection of
incorrect edges when detected 10% and 90% of all the incorrect
edges. Row 4: Root mean square error. All results are averaged
over all datasets.
and too pessimistic in another range. We computed
the root mean square-error (RMSE) in predicting the
bin center value given by
?
(
?
j nj(bj?cj)
2)/(
?
j nj) ,
where nj is the number of edges in the jth bin.
The results, summarized in the 4th row of Table 1,
support the observation that KD-Fix performs better
than WKB, with smaller RMSE.
Incorrect Edges Detection: The goal of this task
is to efficiently detect incorrect predicted-edges.
We ranked all predicted edges of the test-set (per
dataset) according to their confidence score, order-
ing from low to high. Ideally, erroneous edges by
the parser are ranked at the top. A summary of
the average precision, computed at all ranks of erro-
neous edges, (averaged over all datasets, due to lack
of space), for all confidence estimation methods is
summarized in the first row of Table 1. The aver-
age precision achieved by random ordering is about
equal to the error rate for each dataset. The Delta
method improves significantly over both the random
ordering and WKB. KD-Fix achieves the best per-
formance in 12 of 14 datasets and the best average-
performance. These results are consistent with the
results obtained for sequence labeling by Mejer and
Crammer (2010).
Average precision summarizes the detection of
all incorrect edges into a single number. More re-
fined analysis is encapsulated in Precision-Recall
(PR) plots, showing the precision as more incorrect
edges are detected. PR plots for three datasets are
shown in Fig. 2. From these plots (applied also to
other datasets, omitted due to lack of space) we ob-
serve that in most cases KD-Fix performs signifi-
cantly better than Delta in the early detection stage
(first 10-20% of the incorrect edges), while Delta
performs better in late detection stages (last 10-20%
of the incorrect edges). The second and third rows of
Table 1 summarize the precision after detecting only
10% incorrect edges and after detecting 90% of the
incorrect edges, averaged over all datasets. For ex-
ample, in Czech and Portuguese plots of Fig. 2, we
observe an advantage of KD-Fix for low recall and
an advantage of Delta in high recall. Yet for Ara-
bic, for example, KD-Fix outperforms Delta along
the entire range of recall values.
KD-Fix assigns at most K distinct confidence val-
ues to each edge - the number of models that agreed
on that particular edge. Thus, when edges are ranked
according to the confidence, all edges that are as-
signed the same value are ordered randomly. Fur-
thermore, large fraction of the edges, ? 70 ? 80%,
are assigned one of the top-three scores (i.e. K-2,
K-1, K). As a results, the precision performance of
KD-Fix drops sharply for recall values of 80% and
above. On the other hand, we hypothesize that the
lower precision of Delta at low recall values (dia-
mond in Fig. 2) is because by definition Delta takes
into account only two parses, ignoring additional
possible parses with score close to the highest score.
This makes Delta method more sensitive to small
differences in score values compared to KD-Fix.
Based on this observation, we propose combin-
ing both KD-Fix and Delta. Our new method sets
the confidence score of an edge to be a weighted
mean of the score values of KD-Fix and Delta, with
weights a and 1-a, respectively. We use a value
575
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Czech
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Portuguese
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Arabic
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
0 20 40 60 800.20.3
0.40.5
0.60.7
K
Average 
Precision
 
 
ArabicChineseDanishDutchSloveneSpanish
Figure 2: (Best shown in color.) Three left plots: Precision in detection of incorrect edges as recall increases. Right plot: Effect of
K value on KD-Fix method performance (for six languages, the remaining languages follow similar trend, omitted for clarity).
a ? 1, so if the confidence value of two edges ac-
cording to KD-Fix is different, the contribution of
the score from Delta is negligible, and the final score
is very close as score of only KD-Fix. On the other
hand, if the score of KD-Fix is the same, as hap-
pens for many edges at high recall values, then Delta
breaks arbitrary ties. In other words, the new method
first ranks edges according to the confidence score
of KD-Fix, then among edges with equal KD-Fix
confidence score a secondary order is employed us-
ing Delta. Not surpassingly, we name this method
KD-Fix+Delta. This new method enjoys the bene-
fits of the two methods. From the first row of Table 1
we see that it achieves the highest average-precision
averaged over the 14 datasets. It improves average-
precision over KD-Fix in 12 of 14 datasets and over
Delta in all 14 datasets. From the second and third
row of the table, we see that it has Precision very
close to KD-Fix for recall of 10% (0.729 vs. 0.724),
and very close to Delta for recall of 90% (0.351 vs.
0.348). Moving to Fig. 2, we observe that the curve
associated with the new method (red ticks) is in gen-
eral as high as the curves associated with KD-Fix
for low values of recall, and as high as the curves
associated with Delta for large values of recall.
To illustrate the effectiveness of the incorrect
edges detection process, Table 2 presents the num-
ber of incorrect edges detected vs. number of edges
inspected for the English dataset. The test set for this
task includes 55K words and the parser made mis-
take on 6, 209 edges, that is, accuracy of 88.8%. We
see that using the ranking induced by KD-Fix+Delta
method, inspection of 550, 2750 and 5500 edges
(1, 5, 10% of all edges), allows detection of 6.6 ?
46% of all incorrect edges, over 4.5 times more ef-
fective than random validation.
Edges inspected Incorrect edges detected
(% of total edges) (% of incorrect edges)
550 (1%) 412 (6.6%)
2,750 (5%) 1,675 (27%)
5,500 (10%) 2,897 (46%)
Table 2: Number of incorrect edges detected, and the corre-
sponding percentage of all mistakes, after inspecting 1 ? 10%
of all edges, using ranking induced by KD-Fix+Delta method.
Effect of K value on KD-Fix method perfor-
mance The right plot of Fig. 2 shows the average-
precision of detecting incorrect edges on the test set
using the KD-Fix method for K values ranging be-
tween 2 and 80. We see that even with K = 2,
only two samples per sentence, the average preci-
sion results are much better than random ranking in
all tasks. AsK is increased the results improve until
reaching maximal results at K ? 30. Theoretical
calculations, using concentration inequalities, show
that accurate estimates based on the sampling proce-
dure requires K ? 102 ? 103. Yet, we see that for
practical uses, smaller K values by 1 ? 2 order of
magnitude is suffice.
References
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
[Hall2007] Keith Hall. 2007. k-best spanning tree pars-
ing. In In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
HLT/EMNLP.
[Mejer and Crammer2010] A. Mejer and K. Crammer.
2010. Confidence in structured-prediction using
confidence-weighted models. In EMNLP.
576
