Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Researcher affiliation extraction from homepages
Istva?n Nagy1, Richa?rd Farkas1,2, Ma?rk Jelasity2
Nagy.Istvan@gmail.com, {rfarkas,jelasity}@inf.u-szeged.hu
1 University of Szeged, Department of Informatics
A?rpad te?r 2., H-6720 Szeged, Hungary
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
Aradi ve?rtanuk tere 1., H-6720 Szeged, Hungary
Abstract
Our paper discusses the potential use of
Web Content Mining techniques for gath-
ering scientific social information from the
homepages of researchers. We will intro-
duce our system which seeks [affiliation,
position, start year, end year] information
tuples on these homepages along with pre-
liminary experimental results. We believe
that the lessons learnt from these experi-
ments may be useful for further scientific
social web mining.
1 Introduction
Scientific social network analysis (Yang et al,
2009; Said et al, 2008) seeks to discover global
patterns in the network of researchers working in
a particular field. Common approaches uses bibli-
ographic/scholarly data as the basis for this anal-
ysis. In this paper, we will discuss the poten-
tial of exploiting other resources as an informa-
tion source, such as the homepages of researchers.
The homepage of a researcher contains several
useful pieces of scientific social information like
the name of their supervisor, affiliations, academic
ranking and so on.
The information on homepages may be present
in a structured or natural text form. Here we
shall focus on the detection and analysis of full
text regions of the homepages as they may con-
tain a huge amount of information while requires
more sophisticated analysis than that for struc-
tured ones. We will show that this kind of Web-
based Relation Extraction requires different tech-
niques than the state-of-the-art seed-based ap-
proaches as it has to acquire information from the
long-tail of the World Wide Web.
As a case study, we chose one particular sci-
entific social information type and sought to ex-
tract information tuples concerning the previous
and current affiliations of the researcher in ques-
tion. We defined ?affiliation? as the current and
previous physical workplaces and higher edu-
cational institutes of the researcher in question.
Our aim is to use this kind of information to
discover collegial relationships and workplace-
changing behaviour which may be complementary
to the items of information originating from bibli-
ographic databases.
Based on a manually annotated corpus we car-
ried out several information extraction experi-
ments. The architecture of the complex system
and the recognised problems will be discussed in
Section 3, while our empirical results will be pre-
sented in Section 4. In the last two sections we
will briefly discuss our results and then draw our
main conclusions.
2 Related work
The relationship to previous studies will be dis-
cussed from a scientific social network analysis as
an application point of view and from a Web Con-
tent Mining point of view as well.
2.1 Researcher affiliation extraction
Scientific social network analysis has become a
growing area in recent years ((Yang et al, 2009;
Robardet and Fleury, 2009; Said et al, 2008)
just to name a few in recent studies). Its goal is
to provide a deeper insight into a research field
or into the personal connections among fields by
analysing relationships among researchers. The
existing studies use the co-authorship (e.g. (New-
man, 2001; Baraba?si et al, 2002)) or/and the cita-
tion (Goodrum et al, 2001; Teufel et al, 2006) in-
formation ? generally by constructing a graph with
nodes representing researchers ? as the basis for
their investigations.
Apart from publication-related relationships
? which are presented in structured scholarly
datasets ?, useful scientific social information can
1
be gathered from theWWW. Take, for instance the
homepage of a researchers where they summarise
their topic of interest, list supervisors and students,
nationality, age, memberships and so on. Our goal
is to develop an automatic Web Content Mining
systemwhich crawls the homepages of researchers
and extracts useful social information from them.
A case study will be outlined here, where the
previous and current affiliations of the researcher
in question were gathered automatically. Having
a list of normalised affiliations for each researcher
of a field (i) we ought to be able to discover col-
legial relationships (whether they worked with the
same group at the same time) which may differ
from the co-authorship relation and (ii) we hope
to be able to answer questions like ?Do American
or European researchers change their workplace
more often??.
2.2 Information extraction from homepages
From a technology point of view our procedure
is a Web Content Mining tool, but it differs from
the popular techniques used nowadays. The aim
of Web Content Mining (Liu and Chen-Chuan-
Chang, 2004) is to extract useful information from
the natural language-written parts of websites.
The first attempts on Web Content Mining be-
gan with the Internet around 1998-?99 (Adelberg,
1998; Califf and Mooney, 1999; Freitag, 1998;
Kosala and Blockeel, 2000). They were expert
systems with hand-crafted rules or induced rules
used in a supervised manner and based on labeled
corpora.
The next generation of approaches on the other
hand work in weakly-supervised settings (Etzioni
et al, 2005; Sekine, 2006; Bellare et al, 2007).
Here, the input is a seed list of target information
pairs and the goal is to gather a set of pairs which
are related to each other in the same manner as the
seed pairs. These pairs may contain related enti-
ties (for example, country - capital city in (Etzioni
et al, 2005) and celebrity partnerships in (Cheng
et al, 2009)) or form an entity-attribute pair (like
Nobel Prize recipient - year in (Feiyu Xu, 2007))
or may be concerned with retrieving all available
attributes for entities (Bellare et al, 2007; Pas?ca,
2009). These systems generally download web
pages which contain the seed pairs then learn syn-
tactical/semantical rules from the sentences of the
pairs (they generally use the positive instances for
one case as negative instances for another case).
According to these patterns, they can download a
new set of web pages and parse them to acquire
new pairs.
These seed-based systems exploit the redun-
dancy of the WWW. They are based on the hy-
pothesis that important information can be found
at several places and in several forms on the Web,
hence a few accurate rules can be used to collect
the required lists. Their goal is to find and recog-
nise (at least) one occurrence of the target infor-
mation and not to find their every occurrence on
the Web. But this is not the case in our scenario.
Several pieces of social information for the re-
searchers are available just on their homepages (or
nowhere). Thus here we must capture each men-
tion of the information. The weakly-supervised
(redundancy-based) systems can build on high-
precision and lower recall information extraction,
while we have to have target a perfect recall. For
the evaluation of such a system we constructed a
manually annotated corpus of researchers? home-
pages. This corpus was also used as a training cor-
pus for the preliminary information extraction ex-
periments described in this paper.
3 The architecture of the system
The general task of our system is to gather sci-
entific social information from the homepages of
researchers. In the use case presented in this pa-
per, the input is a set of researchers? names who
work in a particular research field (later on, this
list can be automatically gathered, for example,
from a call for papers) and the output is a list of
affiliations for each researcher. Here the affiliation
is a tuple of affiliation, position type and start/end
dates. We think that the lessons learnt from affili-
ation extraction will be useful for the development
of a general social information extraction system.
The system has to solve several subproblems
which will be described in the following subsec-
tions.
3.1 Locating the homepage of the researcher
Homepage candidates can be efficiently found by
using web search engine queries for the given
name. In our case study the homepage of the
researcher (when it existed) were among the top
10 responses of the Google API1 in each case.
However, selecting the correct homepage from
the top 10 responses is a harder task. Among
1http://code.google.com/apis/
2
these sites there are (i) publication-related ones
(books/articles written by the researchers, call for
papers), sites of the institute/group associated with
the researcher and (ii) homepages of people shar-
ing the same name.
In our preliminary experiments, we ignored
these two basic problems and automatically parsed
each website. However in the future we plan to
develop a two-stage approach to solve them. In
the first stage a general homepage detection model
? a binary classification problem with classes
homepage/non-homepage ? will be applied.
In the second stage we will attempt to automati-
cally extract textual clues for the relations among
the researchers (e.g. the particular field they work
in) from the homepage candidates and utilise these
cues for name disambiguation along with other bi-
ographical cues. For a survey of state-of-the-art
name disambiguation, see (Artiles et al, 2009).
3.2 Locating the relevant parts of the site
The URL got from the search engine usually
points to the main page of the homepage site. An
ideal system should automatically find every page
which might contain scientific social information
likeCurriculum Vitae, Research interests, Projects
etc. This can be done by analysing the text of the
links or even the linked page. In our case study we
simply parsed the pages to a depth of 1 (i.e. the
main page and each page which was linked from
it).
The located web pages usually have their con-
tent arranged in sections. The first step of infor-
mation extraction may be a relevant section se-
lection module. For example, in the affiliation
extraction task the Positions Held and Education
type sections are relevant while Selected Papers
is not. Having several relevant sections with their
textual positions, an automatic classification sys-
tem can filter out a huge number of probably irrel-
evant sections. In our experiments, we statistically
collected a few ?relevant keywords? and filtered
out sections and paragraphs which did not contain
any of these keywords.
3.3 Extracting information tuples
Pieces of scientific social information are usually
present on the homepages and in the CVs even in
an itemised (structured) form or in a natural lan-
guage full text form. Information extraction is
performed from the structured parts of the docu-
ments by automatically constructed rules based on
the HTML tags and keywords. This field is called
Wrapper Induction (Kushmerick, 2000).
We shall focus on the information extraction
from raw texts here because we found that more
pages express content in textual form than in a
structured one in the researchers? homepages of
our case study and this task still has several un-
solved problems. We mentioned above that sci-
entific social information extraction has to cap-
ture each occurrence of the target information.
We manually labeled homepages for the evalua-
tion of these systems. We think that the DOM
structure of the homepages (e.g. formatting tags,
section headers) could provide useful information,
hence the labeling was carried out in their origi-
nal HTML form (Farkas et al, 2008). In our pre-
liminary experiments we also used this corpus to
train classification models (they were evaluated in
a one-researcher-leave-out scheme). The purpose
of these supervised experiments was to gain an in-
sight into the nature of the problem, but we suggest
that a real-world system for this task should work
in a weakly-supervised setting.
3.4 Normalisation
The output of the extraction phase outlined above
is a list of affiliations for each researcher in the
form that occurred in the documents. However, for
scientific social network analysis, several normal-
isation steps should be performed. For example,
for collegial relationship extraction, along with
the matching of various transliteration of research
groups (like Massachusetts Institute of Technology
and MIT AI Lab), we have to identify the appropri-
ate institutional level where two researchers prob-
ably still have a personal contact as well.
4 Experiments
Now we will present the affiliation corpus which
was constructed manually for evaluation purposes
along with several preliminary experiments on af-
filiation extraction.
4.1 The affiliation corpus
We manually constructed a web page corpus
containing HTML documents annotated for pub-
licly available information about researchers. We
downloaded 455 sites, 5282 pages for 89 re-
searchers (who form the Programme Committee
of the SASO07 conference2), and two indepen-
2http://projects.csail.mit.edu/saso2007/tmc.html
3
dent annotators carried out their manual labeling
in the original (HTML) format of the web pages,
following an annotation guideline (Farkas et al,
2008). All the labels that were judged inconsis-
tent were collected together from the corpus for a
review by the two annotators and the chief annota-
tor. We defined a three-level deep annotation hier-
archy with 44 classes (labels). The wide range of
the labels and the inter-annotator agreement both
suggest that the automatic reproduction of this full
labelling is a hard task.
We selected one particular information class,
namely affiliation from our class hierarchy for our
case study. We defined ?affiliation? as the current
and previous physical workplaces and higher ed-
ucational institutes of the researcher in question
as we would like to use this kind of information
to discover collegial relationships and workplace-
changing behaviour. Here institutes related to re-
view activities, awards, or memberships are not re-
garded as affiliations. We call position the tu-
ple of <affiliation, position types,
years>, as for example in <National Depart-
ment of Computer Science and Operational Re-
search at the University of Montreal, adjunct Pro-
fessor, {1995, 2002}>3. Among the four slots
just the affiliation slot is mandatory (it is
the head) as the others are usually missing in real
homepages.
The problem of finding the relevant pages of a
homepage site originating from a seed URL was
not addressed in this study. We found that pages
holding affiliation information was the one re-
trieved by Google in 135 cases and directly linked
to the main page in 50 cases. We found affilia-
tion information for all of the 89 researchers of our
case study in the depth of 1, but we did not check
whether deeper crawling could have yielded new
information.
The affiliation information (like every piece of
scientific social information) can be present on
web pages in an itemised or natural text format.
We manually investigated our corpus and found
that the 47% of the pages contained affiliation in-
formation exclusively in a textual form, 24% ex-
clusively in an itemised form and 29% were hy-
brid. Information extraction from these two for-
mats requires different methods. We decided to
address the problem of affiliation extraction just
3the example is extracted from
http://bcr2.uwaterloo.ca/?rboutaba/biography2.htm
by using the raw text parts of the homepages.
We partitioned each downloaded page at HTML
breaking tags and kept the parts (paragraphs)
which were regarded as ?raw text?. Here we used
the following rule: a textual paragraph has to be
longer than 40 characters and contain at least one
verb. Certainly this rule is far from perfect (para-
graphs describing publication and longer items of
lists are still present), but it seems to be a reason-
able one as it extracts paragraphs even from ?hy-
brid? pages. We found 86,735 paragraphs in the
5282 downloaded pages and used them in experi-
ments in a raw txt format (HTML tags were re-
moved).
Table 4.1 summarises the size-related figures
for the part of this textual corpus which contains
affiliation information (these paragraphs contain
manually labeled information). The corpus is
freely available for non-commercial use4.
# researchers 59
# pages 103
# paragraph 151
# sentences 181
# affiliation 374
# position type 326
# year 212
Table 1: The size of the textual corpus which con-
tains affiliation information.
4.2 The multi-stage model of relation
extraction
Our relation extraction system follows the archi-
tecture described in the previous section. We fo-
cus on the relevant part location and information
extraction steps in this study. We applied simple
rules to recognise the relevant parts of the home-
pages. We extract textual paragraphs as described
above and then filter out probably irrelevant ones
(Section 4.3).
Preliminary supervised information extraction
experiments were carried out in our case study in
order to get an insight into the special nature of
the problem. We used a one-researcher-leave-out
evaluation setting (i.e. the train sets consisted of
the paragraphs of 88 researchers and the test sets
concerned 1 researcher), thus we avoided the situ-
ations where a training set contained possibly re-
4www.inf.u-szeged.hu/rgai/homepagecorpus
4
dundant information about the subject of the test
texts.
A two-stage information extraction system was
applied here. In the first phase, a model should
recognise each possible slot/entities of the target
information tuples (Section 4.4). Then the tuples
have to be filled, i.e. the roles have to be assigned
and irrelevant entities should be ignored (Section
4.5).
4.3 Paragraph filtering
Because just a small portion of extracted textual
paragraphs contained affiliation information, we
carried out experiments on filtering out probably
irrelevant paragraphs.
Our filtering method exploited the paragraphs
containing position (positive paragraphs).
We calculated the P (word|positive) conditional
probabilities and the best words based on this mea-
sure (e.g. university, institute and professor) then
formed the so-called positive wordset. The para-
graphs which did not contain any word from the
positive wordset were removed. Note that stan-
dard positive and negative sample-based classifi-
cation is not applicable here as the non-positive
paragraphs may contain these indicative words,
but in an irrelevant context or with a connection
to people outside of our scope of interest. Our 1-
DNF hypothesis described above uses just positive
examples and it was inspired by (Yu et al, 2002).
After performing this procedure we kept 14,686
paragraphs (from the full set of 86,735), but we
did not leave out any annotated text. Hence the in-
formation extraction module could then work with
a smaller and less noisy dataset.
4.4 Detecting possible slots
We investigated a Named Entity Recognition
(NER) tool for detecting possible actors of a
position tuple. But note that this task is not a
classical NER problem because our goal here is to
recognise just those entities which may play a role
in a position event. For example there were
many year tokens in the text ? having the same
orthographic properties ? but only a few were re-
lated to affiliation information. The contexts of the
tokens should play an important role in this kind
of an NER targeting of very narrow semantic NE
classes.
For training and evaluating the NER systems,
we used each 151 paragraphs containing at least
one manually labeled position along with 200
other manually selected paragraphs which do not
contain any labeled position. We decided to
use just this 151+200 paragraphs instead of the
full set of 86,735 paragraphs for CPU time rea-
sons. Manual selection ? instead of random sam-
pling ? was required as there were several para-
graphs which contained affiliation information un-
related to the researcher in question, thus introduc-
ing noise. In our multi-stage architecture, the NER
model trained on this reduced document set was
than predicated for the full set of paragraphs and
false positives (note that the paragraphs outside the
NER-train do not contain any gold-standard anno-
tation) has to be eliminated.
We employed the Condition Random Fields
(Lafferty et al, 2001) (implementation MALLET
(McCallum, 2002)) for our NER experiments.
The feature set employed was developed for gen-
eral NER and includes the following categories
(Szarvas et al, 2006):
orthographical features: capitalisation, word
length, bit information about the word form
(contains a digit or not, has uppercase char-
acter inside the word, and so on), character
level bi/trigrams,
dictionaries of first names, company types, de-
nominators of locations,
frequency information: frequency of the token,
the ratio of the token?s capitalised and low-
ercase occurrences, the ratio of capitalised
and sentence beginning frequencies of the to-
ken which was derived from the Gigaword
dataset5,
contextual information: sentence position, trig-
ger words (the most frequent and unambigu-
ous tokens in a window around the NEs) from
the train text, the word between quotes, and
so on.
This basic set was extended by two domain-
specific gazetteers, namely a list of university
names and position types. We should add that
a domain-specific exception list (containing e.g.
Dr., Ph.D.) for augmenting a general sentence
splitter was employed here.
Table 2 lists the phrase-level F?=1 results ob-
tained by CRF in the one-researcher-leave-out
5Linguistic Data Consortium (LDC),
catalogId: LDC2003T05
5
evaluation scheme, while Table 3 lists the results
of a baseline method which labels each member
of the university and position type gazetteers and
identifies years using regular expressions. This
comparison highlights the fact that labeling each
occurrences of this easily recognisable classes
cannot be applied. It gives an extremely low pre-
cision thus contextual information has to be lever-
aged.
Precision Recall F?=1
affiliation 66.78 53.28 59.27
position type 87.50 70.22 77.91
year 86.42 69.31 76.92
TOTAL 78.73 62.88 69.92
Table 2: The results achieved by CRF.
Precision Recall F?=1
affiliation 21.43 9.68 13.33
position type 23.27 66.77 34.51
year 65.77 98.99 79.03
TOTAL 32.16 44.08 37.19
Table 3: NER baseline results.
4.5 The assignment of roles
When we apply the NER module to unknown doc-
uments we have to decide whether the recognised
entities have any connection with the particular
person as downloaded pages often contain infor-
mation about other researchers (supervisors, stu-
dents, etc.) as well. The subject of the informa-
tion is generally expressed by a proper noun at
the beginning of the page or paragraph and then
anaphoric references are used. We assumed here
that each position tuple in a paragraph was re-
lated to exactly one person and when the subject of
the first sentence of the paragraph was a personal
pronoun I, she, he then the paragraph belonged to
the author of the page.
To automatically find the subject of the para-
graphs we tried out two procedures and evaluated
them on the predictions of the NER model intro-
duced in the previous subsection. First, we applied
a NER trained on the person names of the CoNLL-
2003 corpus (Tjong Kim Sang and De Meulder,
2003). The names predicted by this method were
then compared to the owner of the homepage us-
ing name normalisation techniques. If no name
was found by the tagger we regarded the para-
graph as belonging to the author. Its errors had two
sources; the NER trained on an out-domain corpus
made a lot of false negatives and the normalisation
method had to deal with incorrect ?names? (like
Paul Hunter Curator as a name phrase) as well.
The second method was simpler. We kept
the position tuples whose paragraph contained
any part of the researcher name or any of the ?I?,
?she?, ?he? personal pronouns. Its errors came, for
instance, from finding the ?Paul? string for ?Paul
Robertson? in the text snippet ?Paul Berger?.
We applied these two subject detection meth-
ods to the predictions of our slot detection NER
modul. Table 4 summarises the accuracies of the
systems, i.e. whether they made the correct deci-
sion on ?is this forecasted affiliation corresponds
to the researcher in question?. The columns of
this table shows how many affiliation pre-
diction was carried out by the slot detection sys-
tem, i.e. how many times has to made a de-
cision. ?name. det? and ?p. pronouns? refer
to the two methods, to the name detection-based
and to the personal pronoun-matcher ones. We
investigated their performance on the paragraphs
which contained manually labeled information,
on the paragraphs which did contained any but
the slot detection module forecasted at least one
affiliation here and on the union of these
sets of paragraphs. The figures of the table shows
that the personal pronoun detection approach per-
forms significantly better on the paragraphs which
really contains affiliation information. This is due
to the fact that this method removes less predic-
tion compared to the name based one and there are
just a few forecast which has to be removed on the
paragraphs which contain information.
#pred name det. p. pronouns
annotated 165 66.9 87.8
non-ann. 214 71.5 61.2
full set 379 69.4 73.4
Table 4: Accuracies of subject detection methods.
To find relationships among the other types of
predicated entities (affiliation, position type, start
year, end year) we used a very simple heuristic.
As the affiliation slot is the head of the tuple
we simply assigned every other detected entity to
the nearest affiliation and regarded the ear-
lier preidcated year token as the start year.
6
This method made the correct decision in the
91.3% and 71.8% of the cases applied on the gold-
standard annotation and the predicated entities, re-
spectively. We should add that using the predicted
labels during the evaluation, the false positives of
the NER counts automatically an error in relation
detection as well.
5 Discussion
The first step of the information extraction sys-
tem of this case study was the localisation of rele-
vant information. We found that Web search en-
gines are efficient tools for finding homepages.
We empirically showed that a very simple crawl-
ing (downloading everything to a depth of 1) can
be applied, because the irrelevant contents can be
removed later. The advantage of focused crawl-
ing (i.e. making a decision before download-
ing a linked page) is that it can avoid the time-
consuming analysis of pages. However making
the decision of whether the linked document might
contain relevant information is a hard task. On the
other hand we showed that the requested informa-
tion is reachable in depth 1 and that a fast string-
matching based filtering method can significantly
reduce the amount of texts which have to be anal-
ysed without losing any information. Moreover,
the positive example-based filtering approach can
be employed in a seed-driven setting as well.
For the information extraction phase we think
that a high-recall system has to be developed. We
constructed a corpus with contextual occurrences
for evaluation issues. The extraction can be re-
lationship detection-based (e.g. the state-of-the-
art seed-driven approaches seek to acquire syntac-
tic/semantic patterns which are typical of the re-
lationship itself) or entity-based (like our method,
these approaches first identify possible actors then
look for relationships among them). We expect
that the latter one is more suitable for high-recall
tasks.
The NER system of this case study achieved
significantly better results than those for the base-
line method. We experimentally showed that
it could exploit the contextual information and
that the labeled entities were those which were
affiliation-related. However, the overall system
has to be improved in the future. We manually
analysed the errors on a part of the corpus and
found a few typical errors were present. Our
annotation guide said that the geographical loca-
tion of the affiliation was a part of the affilia-
tion as it sometimes identifies the department (e.g.
?Hewlett-Packard Labs in Palo Alto?). This ex-
tension of the phrase proved to be difficult because
there were several cases with the same ortho-
graphic features (e.g. Ph.D. from MIT in Physics).
The acronyms immediately after the affiliation are
a similar case, which we regard as part of the name
and it is difficult for the NER to handle (e.g. Cen-
tre for Policy Modelling (CPM)). As there is no
partial credit; an incorrect entity boundary is pe-
nalised both as a false positive and as a false neg-
ative.
These points also explain the surprisingly low
precision of the baseline system as it labeled uni-
versity names without more detailed identifica-
tion of the unit (e.g. Department of Computer
Science, [Waterloo University]BASELINE). We
should add that these two annotation guidelines
are questionable, but we expect that information
might get lost without them. Moreover, there is
an another reason for the low recall, it is that our
human annotators found textual clues for position
types on verbs as well (e.g. I leadTY PE the Dis-
tributed Systems Group). The context of these la-
beled examples are clearly different from that of
the usual position type.
Comparing the two subject detection methods,
we see that the name detection model which learnt
on an out-domain corpus made a lot of mistakes,
thus the method based on it judged more para-
graphs as irrelevant ones. The name detection
could be improved by a domain corpus (for exam-
ple the training corpus did not contain any Prof.
NAME example) and by applying more sophisti-
cated name normalisation techniques. When we
manually analysed the errors of these procedures
we found that each false negative of the sim-
pler subject detection method was due to the er-
rors of the textual paragraph identification defini-
tion used. There were several itemisations whose
header was type of ?Previously I worked for:? and
the textual items themselves did not contain the
subject of the affiliation information. The false
positives often originated from pages which did
not belong to the researcher in question but con-
tained him name (e.g. I am a Ph.D. Student work-
ing under the supervision of Prof. NAME).
Lastly, an error analysis of the affiliation head
seeking heuristic revealed that the 44% of the
predicted position type and year entities?s
7
sentences did not contain any affiliation
prediction. With the gold-standard labeling, there
were 6 sentences without affiliation labels
and only one of them used an anaphoric refer-
ence, the others were a consequence of the erro-
neous automatic sentence splitting of the HTML
documents. The prediction of the NER sys-
tem contained many more sentences without any
affiliation label. These could be fixed
by forcing a second forecast phase to predict
affiliation in these sentences or by remov-
ing these labels in a post-processing step.
The remaining errors of the affiliation head as-
signment could be avoided just by employing a
proper syntactic analyser. The most important lin-
guistic phenomena which should be automatically
identify for this problem is enumeration. For in-
stance, we should distinguish between the enumer-
ation and clause splitting roles of ?and? (e.g. ?I?m
a senior researcher and leader of the GROUP?
and ?He got his PhD fromUNIVERSITY1 in YEAR
and has a Masters from UNIVERSITY2?). This
requires a deep syntactic analysis, i.e. the use of
a dependency parser which has to make accurate
predictions on several certain types of dependen-
cies is probably needed.
6 Conclusions
In this paper we introduced a Web Content Mining
system for gathering affiliation information from
the homepages of researchers. The affiliation in-
formation collected from this source might be of
great value for scientific social network analysis.
We discussed the special nature of this task
compared to common Web-based relation extrac-
tion approaches and identified several subtasks of
the system during our preliminary experiments.
We argued that the evaluation of this kind of sys-
tem should be carried out on a manually labeled
reference corpus. We introduced simple but ef-
fective solutions for the subproblems along with
empirical results on a corpus. We achieved rea-
sonable results with an overall phrase-level F?=1
score of 70% on the possible slot detection and
an accuracy of 61% on relation extraction (as an
aggregation of the subject detection and the affil-
iation head selection procedures). However each
subproblem requires more sophisticated solutions,
which we plan to address in the near future.
Acknowledgments
This work was supported in part by the NKTH
grant of the Jedlik A?nyos R&D Programme
(project codename TEXTREND) of the Hungar-
ian government. The authors would like to thank
the annotators of the corpus for their devoted ef-
forts.
References
Brad Adelberg. 1998. Nodose - a tool for semi-
automatically extracting structured and semistruc-
tured data from text documents. ACM SIGMOD,
27(2):283?294.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of the
web people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
A. L. Baraba?si, H. Jeong, Z. Ne?da, E. Ravasz, A. Schu-
bert, and T. Vicsek. 2002. Evolution of the so-
cial network of scientific collaborations. Physica A:
Statistical Mechanics and its Applications, 311(3-
4):590 ? 614.
Kedar Bellare, Partha Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2007. Lightly-supervised
attribute extraction for web search. In Proceedings
of NIPS 2007 Workshop on Machine Learning for
Web Search.
Mary Elaine Califf and Raymond J. Mooney. 1999.
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence, pages 328?334.
Xiwen Cheng, Peter Adolphs, Feiyu Xu, Hans Uszko-
reit, and Hong Li. 2009. Gossip galore ? a self-
learning agent for exchanging pop trivia. In Pro-
ceedings of the Demonstrations Session at EACL
2009, pages 13?16, Athens, Greece, April. Associa-
tion for Computational Linguistics.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Er Yates. 2005. Unsupervised
named-entity extraction from the web: An experi-
mental study. Artificial Intelligence, 165:91?134.
Richa?rd Farkas, Ro?bert Orma?ndi, Ma?rk Jelasity, and
Ja?nos Csirik. 2008. A manually annotated html cor-
pus for a novel scientific trend analysis. In Proc. of
The Eighth IAPR Workshop on Document Analysis
Systems.
Hong Li Feiyu Xu, Hans Uszkoreit. 2007. A seed-
driven bottom-up machine learning framework for
8
extracting relations of various complexity. In Pro-
ceedings of ACL 2007, 45th Annual Meeting of the
Association for Computational Linguistics, Prague,
Czech Republic, 6.
Dayne Freitag. 1998. Information extraction from
html: Application of a general machine learning ap-
proach. In Proceedings of the Fifteenth National
Conference on Artificial Intelligence, pages 517?
523.
A. A Goodrum, K. W McCain, S. Lawrence, and C. L
Giles. 2001. Scholarly publishing in the internet
age: a citation analysis of computer science liter-
ature. Information Processing and Management,
37:661?675, September.
Raymond Kosala and Hendrik Blockeel. 2000. Web
mining research: A survey. SIGKDD Explorations,
2:1?15.
Nicholas Kushmerick. 2000. Wrapper induction: Ef-
ficiency and expressiveness. Artificial Intelligence,
118:2000.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Bing Liu and Kevin Chen-Chuan-Chang. 2004. Edito-
rial: special issue on web content mining. SIGKDD
Explor. Newsl., 6(2):1?4.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
M. E. J. Newman. 2001. The structure of scientific
collaboration networks. In Proceedings National
Academy of Sciences USA, pages 404?418.
Marius Pas?ca. 2009. Outclassing Wikipedia in open-
domain information extraction: Weakly-supervised
acquisition of attributes over conceptual hierarchies.
In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009), Athens,
Greece, March.
Celine Robardet and Eric Fleury. 2009. Communi-
ties detection and the analysis of their dynamics in
collaborative networks. Int. J. Web Based Commu-
nities, 5(2):195?211.
Yasmin H. Said, Edward J. Wegman, Walid K. Shara-
bati, and John T. Rigsby. 2008. Social networks
of author-coauthor relationships. Computational
Statistics & Data Analysis, 52(4):2177?2184.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 731?738,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Gyo?rgy Szarvas, Richa?rd Farkas, and Andra?s Kocsor.
2006. A multilingual named entity recognition sys-
tem using boosting and c4.5 decision tree learning
algorithms. DS2006, LNAI, 4265:267?278.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pages 80?87, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Y. Yang, C. M. Au Yeung, M. J. Weal, and H. Davis.
2009. The researcher social network: A social net-
work based on metadata of scientific publications.
Hwanjo Yu, Jiawei Han, and Kevin Chen-Chuan
Chang. 2002. Pebl: positive example based learn-
ing for web page classification using svm. In KDD
?02: Proceedings of the eighth ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 239?248, New York, NY, USA.
ACM.
9
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 116?121,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Detecting noun compounds and light verb constructions: a contrastive study
Veronika Vincze1, Istva?n Nagy T.2 and Ga?bor Berend2
1Hungarian Academy of Sciences, Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
2Department of Informatics, University of Szeged
{nistvan,berendg}@inf.u-szeged.hu
Abstract
In this paper, we describe our methods to
detect noun compounds and light verb con-
structions in running texts. For noun com-
pounds, dictionary-based methods and POS-
tagging seem to contribute most to the per-
formance of the system whereas for light
verb constructions, the combination of POS-
tagging, syntactic information and restrictions
on the nominal and verbal component yield
the best result. However, focusing on deverbal
nouns proves to be beneficial for both types
of MWEs. The effect of syntax is negligible
on noun compound detection whereas it is un-
ambiguously helpful for identifying light verb
constructions.
1 Introduction
Multiword expressions are lexical items that can be
decomposed into single words and display idiosyn-
cratic features (Sag et al, 2002; Calzolari et al,
2002; Kim, 2008). They are frequent in language
use and they usually exhibit unique and idiosyn-
cratic behavior, thus, they often pose a problem to
NLP systems. A compound is a lexical unit that
consists of two or more elements that exist on their
own. Light verb constructions are verb and noun
combinations in which the verb has lost its meaning
to some degree and the noun is used in one of its
original senses (e.g. have a walk or give advice).
In this work, we aim at identifying nominal com-
pounds and light verb constructions by using rule-
based methods. Noun compounds belong to the
most frequent MWE-classes (in the Wikipedia cor-
pus we developed for evaluation (see 3.2), about
75% of the annotated multiword expressions were
noun compounds) and they are productive, i.e. new
nominal compounds are being formed in language
use all the time, which yields that they cannot be
listed exhaustively in a dictionary (as opposed to
e.g. prepositional compounds). Their inner syntactic
structure varies: they can contain nouns, adjectives
and prepositions as well.
Light verb constructions are semi-productive, that
is, new light verb constructions might enter the lan-
guage following some patterns (e.g. give a Skype
call on the basis of give a call). On the other hand,
they are less frequent in language use (only 9.5% of
multiword expressions were light verb constructions
in the Wikipedia database) and they are syntactically
flexible, that is, they can manifest in various forms:
the verb can be inflected, the noun can occur in its
plural form and the noun can be modified. The nom-
inal and the verbal component may not even be ad-
jacent in e.g. passive sentences.
Our goal being to compare how different ap-
proaches perform in the case of the different types
of multiword expressions, we have chosen these two
types of MWEs that are dissimilar in several aspects.
2 Related work
There are several applications developed for identi-
fying MWEs, which can be classified according to
the methods they make use of (Piao et al, 2003).
First, statistical models rely on word frequencies,
co-occurrence data and contextual information in
deciding whether a bigram or trigram (or even an
n-gram) of words can be labeled as a multiword ex-
pression or not. Such systems are used for several
116
languages and several types of multiword expres-
sions, see e.g. Bouma (2010). The advantage of
statistical systems is that they can be easily adapted
to other languages and other types of multiword ex-
pressions, however, they are not able to identify rare
multiword expressions (as Piao et al (2003) empha-
size, 68% of multiword expressions occur at most
twice in their corpus).
Some hybrid systems make use of both statisti-
cal and linguistic information as well, that is, rules
based on syntactic or semantic regularities are also
incorporated into the system (Evert and Kermes,
2003; Bannard, 2007; Cook et al, 2007; Al-Haj and
Wintner, 2010). This results in better coverage of
multiword expressions. On the other hand, these
methods are highly language-dependent because of
the amount of linguistic rules encoded, thus, it re-
quires much effort to adapt them to different lan-
guages or even to different types of multiword ex-
pressions. However, the combination of different
methods may improve the performance of MWE-
extracting systems (Pecina, 2010).
Several features are used in identifying multi-
word expressions, which are applicable to differ-
ent types of multiword expressions to various de-
grees. Co-occurrence statistics and POS-tags seem
to be useful for all types of multiword expressions,
for instance the tool mwetoolkit (Ramisch et al,
2010a) makes use of such features, which is illus-
trated through the example of identifying English
compound nouns (Ramisch et al, 2010b).
Caseli et al (2010) developed an alignment-based
method for extracting multiword expressions from
parallel corpora. This method is also applied to
the pediatrics domain (Caseli et al, 2009). Zarrie?
and Kuhn (2009) argue that multiword expressions
can be reliably detected in parallel corpora by using
dependency-parsed, word-aligned sentences. Sinha
(2009) detects Hindi complex predicates (i.e. a com-
bination of a light verb and a noun, a verb or an ad-
jective) in a Hindi?English parallel corpus by iden-
tifying a mismatch of the Hindi light verb meaning
in the aligned English sentence. Van de Cruys and
Moiro?n (2007) describe a semantic-based method
for identifying verb-preposition-noun combinations
in Dutch, which relies on selectional preferences for
both the noun and the verb. Cook et al (2007) dif-
ferentiate between literal and idiomatic usages of
verb and noun constructions in English. They make
use of syntactic fixedness of idioms when develop-
ing their unsupervised method. Bannard (2007) also
seeks to identify verb and noun constructions in En-
glish on the basis of syntactic fixedness. Samardz?ic?
and Merlo (2010) analyze English and German light
verb constructions in parallel corpora. They found
that linguistic features (i.e. the degree of composi-
tionality) and the frequency of the construction both
have an effect on aligning the constructions.
3 Experiments
In order to identify multiword expressions, simple
methods are worth examining, which can serve as a
basis for implementing more complex systems and
can be used as features in machine learning set-
tings. Our aim being to compare the effect of dif-
ferent methods on the identification of noun com-
pounds and light verb constructions, we considered
it important to develop methods for both MWE types
that make use of their characteristics and to adapt
those methods to the other type of MWE ? in this
way, the efficacy and the MWE-(in)dependence of
the methods can be empirically evaluated, which can
later have impact on developing statistical MWE-
detectors.
Earlier studies on the detection of light verb con-
structions generally take syntactic information as a
starting point (Cook et al, 2007; Bannard, 2007;
Tan et al, 2006), that is, their goal is to classify verb
+ object constructions selected on the basis of syn-
tactic pattern as literal or idiomatic. However, we
do not aim at classifying LVC candidates filtered by
syntactic patterns but at identifying them in running
text without assuming that syntactic information is
necessarily available. In our investigations, we will
pay distinctive attention to the added value of syn-
tactic features on the system?s performance.
3.1 Methods for MWE identification
For identifying noun compounds, we made use of a
list constructed from the English Wikipedia. Lower-
case n-grams which occurred as links were collected
from Wikipedia articles and the list was automati-
cally filtered in order to delete non-English terms,
named entities and non-nominal compounds etc. In
the case of the method ?Match?, a noun compound
117
candidate was marked if it occurred in the list. The
second method we applied for noun compounds in-
volved the merge of two possible noun compounds:
if A B and B C both occurred in the list, A B C was
also accepted as a noun compound (?Merge?). Since
the methodology of dictionary building was not ap-
plicable for collecting light verb constructions (i.e.
they do not function as links in Wikipedia), we could
not apply these two methods to them.
In the case of ?POS-rules?, a noun compound
candidate was marked if it occurred in the list and
its POS-tag sequence matched one of the previ-
ously defined patterns (e.g. JJ (NN|NNS)). For
light verb constructions, the POS-rule method meant
that each n-gram for which the pre-defined patterns
(e.g. VB.? (NN|NNS)) could be applied was ac-
cepted as light verb constructions. For POS-tagging,
we used the Stanford POS Tagger (Toutanova and
Manning, 2000). Since the methods to follow rely
on morphological information (i.e. it is required
to know which element is a noun), matching the
POS-rules is a prerequisite to apply those methods
to identify MWEs.
The ?Suffix? method exploited the fact that many
nominal components in light verb constructions are
derived from verbs. Thus, in this case only construc-
tions that contained nouns ending in certain deriva-
tional suffixes were allowed and for nominal com-
pounds the last noun had to have this ending.
The ?Most frequent? (MF) method relied on the
fact that the most common verbs function typically
as light verbs (e.g. do, make, take, have etc.) Thus,
the 15 most frequent verbs typical of light verb con-
structions were collected and constructions where
the stem of the verbal component was among those
of the most frequent ones were accepted. As for
noun compounds, the 15 most frequent nouns in En-
glish were similarly collected1 and the lemma of the
last member of the possible compound had to be
among them.
The ?Stem? method pays attention to the stem of
the noun. In the case of light verb constructions, the
nominal component is typically one that is derived
from a verbal stem (make a decision) or coincides
with a verb (have a walk). In this case, we accepted
1as listed at http://en.wikipedia.org/wiki/
Most\_common\_words\_in\_English
only candidates that had the nominal component /
the last noun whose stem was of verbal nature, i.e.
coincided with a stem of a verb.
Syntactic information can also be exploited in
identifying MWEs. Typically, the syntactic relation
between the verb and the nominal component in a
light verb construction is dobj or prep ? using
Stanford parser (Klein and Manning, 2003)). The re-
lation between the members of a typical noun com-
pound is nn or amod in attributive constructions.
The ?Syntax? method accepts candidates among
whose members these syntactic relations hold.
We also combined the above methods to identify
noun compounds and light verb constructions in our
databases (the union of candidates yielded by the
methods is denoted by ? while the intersection is
denoted by ? in the respective tables).
3.2 Results
For the evaluation of our models, we developed a
corpus of 50 Wikipedia articles, in which several
types of multiword expressions (including nomi-
nal compounds and light verb constructions) and
Named Entities were marked. The database contains
2929 occurrences of nominal compounds and 368
occurrences of light verb constructions and can be
downloaded under the Creative Commons licence at
http://www.inf.u-szeged.hu/rgai/mwe.
Table 1 shows the results of our experiments.
Methods were evaluated on the token level, i.e. each
occurrence of a light verb construction had to be
identified in text. It can be seen that the best result
for noun compound identification can be obtained
if the three dictionary-based methods are combined.
We also evaluated the method of POS-rules without
taking into account dictionary matches (POS-rules
w/o dic), which result serves as the baseline for com-
paring the effect of LVC-specific methods on noun
compound detection.
As can be seen, by adding any of the LVC-specific
features, the performance of the system declines, i.e.
none of them can beat the baseline. While the fea-
ture ?Stem? (and its combinations) improve preci-
sion, recall severely falls back: especially ?Most fre-
quent noun? (MFN) has an extremely poor effect on
it. This was expected since the lexical constraint
on the last part of the compound heavily restricts
the scope of the noun compounds available. On the
118
other hand, the 15 most frequent nouns in English
are not derived from verbs hence they do not end in
any of the pre-defined suffixes, thus, the intersection
of the features ?MFN? and ?Suffix? does not yield
any noun compound (the intersection of all the three
methods also behaves similarly). It must be men-
tioned, however, that the union of all features yields
the best recall as expected and the best F-measure
can be achieved by the union of ?Suffix? and ?Stem?.
The effect of adding syntactic rules to the system
is not unequivocal. In many cases, the improvement
is marginal (it does not exceed 1% except for the
POS-rules w/o dic method) or the performance even
degrades. The latter is most obvious in the case of
the combination of dictionary-based rules, which is
mainly caused by the decline in recall, however, pre-
cision improves. The overall decline in F-score may
thus be related to possible parsing errors.
In the case of light verb constructions, the recall
of the baseline (POS-rules) is high, however, its pre-
cision is low (i.e. not all of the candidates defined
by the POS patterns are light verb constructions).
The ?Most frequent verb? (MFV) feature proves to
be the most useful: the verbal component of the light
verb construction is lexically much more restricted
than the noun, which is exploited by this feature.
The other two features put some constraints on the
nominal component, which is typically of verbal na-
ture in light verb constructions: ?Suffix? simply re-
quires the noun to end in a given n-gram (without ex-
ploiting further grammatical information) whereas
?Stem? allows nouns derived from verbs. When
combining a verbal and a nominal feature, union re-
sults in high recall (the combinations typical verb +
non-deverbal noun or atypical verb + deverbal noun
are also found) while intersection yields high preci-
sion (typical verb + deverbal noun combinations are
found only).
We also evaluated the performance of the ?Syn-
tax? method without directly exploiting POS-rules.
Results are shown in Table 2. It is revealed that
the feature dobj is much more effective in identify-
ing light verb constructions than the feature prep,
on the other hand, dobj itself outperforms POS-
rules. If we combine the dobj feature with the
best LVC-specific feature (namely, MFV), we can
achieve an F-measure of 26.46%. The feature dobj
can achieve a recall of 59.51%, which suggests
Method P R F
Dobj 10.39 59.51 17.69
Prep 0.46 7.34 0.86
Dobj ? Prep 2.09 38.36 3.97
Dobj ? MFV 31.46 22.83 26.46
Prep ? MFV 3.24 5.12 4.06
Dobj ? Prep ? MFV 8.78 19.02 12.02
Table 2: Results of syntactic methods for light verb con-
structions in terms of precision (P), recall (R) and F-
measure (F). Dobj: verb + object pairs, Prep: verb +
prepositional complement pairs, MFV: the verb is among
the 15 most frequent light verbs.
that about 40% of the nominal components in our
database are not objects of the light verb. Thus, ap-
proaches that focus on only verb-object pairs (Cook
et al, 2007; Bannard, 2007; Tan et al, 2006) fail to
identify a considerable part of light verb construc-
tions found in texts.
The added value of syntax was also investigated
for LVC detection as well. As the results show, syn-
tax clearly helps in identifying LVCs ? its overall
effect is to add up to 4% to the F-score. The best
result, again, is yielded by the MFV method, which
is about 30% above the baseline.
4 Discussion
When contrasting results achieved for light verb
constructions and noun compounds, it is revealed
that the dictionary-based method applying POS-
rules yields the best result for noun compounds and
the MFV feature combined with syntactic informa-
tion is the most useful for LVC identification. If
no dictionary matches were taken into consideration,
the combination of the features ?Suffix? and ?Stem?
achieved the best result, however, ?Stem? alone can
also perform similarly. Since ?Stem? identifies de-
verbal nouns, that is, nouns having an argument
structure, it is not surprising that this feature is valu-
able in noun compound detection because the first
part in the compound is most probably an argument
of the deverbal noun (as in noun compound detection
the object of detection is noun compound, in other
words, we detect noun compounds). Thus, it will be
worth examining how the integration of the ?Stem?
feature can improve dictionary-based models.
Making use of only POS-rules does not seem to
119
Method
Noun compounds NC + syntax LVC LVC + syntax
P R F P R F P R F P R F
Match 37.7 54.73 44.65 49.64 48.31 48.97 - - - - - -
Merge 40.06 57.63 47.26 51.69 47.86 49.70 - - - - - -
POS-rules 55.56 49.98 52.62 59.18 46.39 52.02 - - - - - -
Combined 59.46 52.48 55.75 62.07 45.81 52.72 - - - - - -
POS-rules w/o dic 28.33 66.23 39.69 29.97 64.18 40.87 9.35 72.55 12.86 7.02 76.63 16.56
Suffix 27.02 8.91 13.4 28.58 8.84 13.5 9.62 16.3 12.1 11.52 15.22 13.11
MF 12.26 1.33 2.4 12.41 1.29 2.34 33.83 55.16 41.94 40.21 51.9 45.31
Stem 29.87 37.62 33.3 31.69 36.63 33.99 8.56 50.54 14.64 11.07 47.55 17.96
Suffix?MF 0 0 0 - - - 44.05 10.05 16.37 11.42 54.35 18.88
Suffix?MF 23.36 10.24 14.24 24.50 10.13 14.34 19.82 61.41 29.97 23.99 57.88 33.92
Suffix?Stem 28.4 6.49 10.56 30.03 6.42 10.58 10.35 11.14 11.1 12.28 11.14 11.68
Suffix?Stem 29.35 40.05 33.87 31.12 39.06 34.64 8.87 57.61 15.37 11.46 54.35 18.93
MF?Stem 9.16 0.41 0.78 9.6 0.41 0.79 39.53 36.96 38.2 46.55 34.78 39.81
MF?Stem 29.13 38.55 33.18 31.85 36.04 33.81 10.42 68.75 18.09 13.36 64.67 22.15
Suffix?MF?Stem 0 0 0 - - - 47.37 7.34 12.7 50.0 6.79 11.96
Suffix?MF?Stem 28.68 40.97 33.74 30.33 39.95 34.48 10.16 72.28 17.82 13.04 68.2 21.89
Table 1: Experimental results in terms of precision (P), recall (R) and F-measure (F). Match: dictionary match, Merge:
merge of two overlapping noun compounds, POS-rules: matching of POS-patterns, Combined: the union of Match,
Merge and POS-rules, POS-rules w/o dic: matching POS-patterns without dictionary lookup, Suffix: the (head) noun
ends in a given suffix, MF: the head noun/verb is among the 15 most frequent ones, Stem: the (head) noun is deverbal.
be satisfactory for LVC detection. However, the
most useful feature for identifying LVCs, namely,
MFV/MFN proves to perform poorly for noun com-
pounds, which can be explained by the fact that the
verbal component of LVCs usually comes from a
well-defined set of frequent verbs, thus, it is lexically
more restricted than the parts of noun compounds.
The feature ?Stem? helps improve recall and this fea-
ture can be further enhanced since in some cases,
the Porter stemmer did not render the same stem to
derivational pairs such as assumption ? assume. For
instance, derivational information encoded in word-
net relations might contribute to performance.
Concerning syntactic information, it has clearly
positive effects on LVC identification, however, this
influence is ambiguous in the case of noun com-
pounds. Since light verb constructions form a syn-
tactic phrase and noun compounds behave syntac-
tically as one unit (having an internal syntactic hi-
erarchy though), this result suggests that for noun
compound detection, POS-tagging provides enough
information while for light verb constructions, syn-
tactic information is expected to improve the system.
5 Conclusions
In this paper, we aimed at identifying noun com-
pounds and light verb constructions in running texts
with rule-based methods and compared the effect
of several features on detecting those two types
of multiword expressions. For noun compounds,
dictionary-based methods and POS-tagging seem
to contribute most to the performance of the sys-
tem whereas for light verb constructions, the com-
bination of POS-tagging, syntactic information and
restrictions on the nominal and verbal component
yield the best result. Although the effect of syntax
is negligible on noun compound detection, it is un-
ambiguously helpful for identifying light verb con-
structions. Our methods can be improved by extend-
ing the set and scope of features and refining POS-
and syntactic rules and they can be also adapted to
other languages by creating language-specific POS-
rules, lists of suffixes and light verb candidates.
For higher-level of applications, it is necessary to
know which tokens form one (syntactic or semantic)
unit, thus, we believe that our results in detecting
noun compounds and light verb constructions can be
fruitfully applied in e.g. information extraction or
machine translation as well.
Acknowledgments
This work was supported in part by the National In-
novation Office of the Hungarian government within
the framework of the project MASZEKER.
120
References
Hassan Al-Haj and Shuly Wintner. 2010. Identifying
multi-word expressions by leveraging morphological
and syntactic idiosyncrasy. In Proceedings of Coling
2010, Beijing, China, August.
Colin Bannard. 2007. A measure of syntactic flexi-
bility for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, MWE
?07, pages 1?8, Morristown, NJ, USA. ACL.
Gerlof Bouma. 2010. Collocation extraction beyond the
independence assumption. In Proceedings of the ACL
2010 Conference Short Papers, pages 109?114, Upp-
sala, Sweden, July. ACL.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod,
and Antonio Zampolli. 2002. Towards best practice
for multiword expressions in computational lexicons.
In Proceedings of LREC-2002, pages 1934?1940, Las
Palmas.
Helena de Medeiros Caseli, Aline Villavicencio, Andre?
Machado, and Maria Jose? Finatto. 2009. Statistically-
driven alignment-based multiword expression identi-
fication for technical domains. In Proceedings of
the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 1?8, Singapore, August. ACL.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1-2):59?77.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, pages
41?48, Morristown, NJ, USA. ACL.
Stefan Evert and Hannah Kermes. 2003. Experiments on
candidate data for collocation extraction. In Proceed-
ings of EACL 2003, pages 83?86.
Su Nam Kim. 2008. Statistical Modeling of Multiword
Expressions. Ph.D. thesis, University of Melbourne,
Melbourne.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?03, pages 423?430, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):137?158.
Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting multi-
word expressions with a semantic tagger. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment, pages 49?
56, Morristown, NJ, USA. ACL.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010a. Multiword Expressions in the wild?
The mwetoolkit comes in handy. In Coling 2010:
Demonstrations, Beijing, China, August.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Coling 2010: Posters, Beijing, China, August.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceedings
of CICLing-2002, pages 1?15, Mexico City, Mexico.
Tanja Samardz?ic? and Paola Merlo. 2010. Cross-lingual
variation of light verb constructions: Using paral-
lel corpora and automatic alignment for linguistic re-
search. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
52?60, Uppsala, Sweden, July. ACL.
R. Mahesh K. Sinha. 2009. Mining Complex Predicates
In Hindi Using A Parallel Hindi-English Corpus. In
Proceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambiguation
and Applications, pages 40?46, Singapore, August.
ACL.
Yee Fan Tan, Min-Yen Kan, and Hang Cui. 2006. Ex-
tending corpus-based identification of light verb con-
structions using a supervised learning framework. In
Proceedings of the EACL Workshop on Multi-Word
Expressions in a Multilingual Contexts, pages 49?56,
Trento, Italy, April. ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings of
EMNLP 2000, pages 63?70, Stroudsburg, PA, USA.
ACL.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction. In
Proceedings of the Workshop on a Broader Perspective
on Multiword Expressions, MWE ?07, pages 25?32,
Morristown, NJ, USA. ACL.
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Transla-
tional Correspondences for Pattern-Independent MWE
Identification. In Proceedings of the Workshop on
Multiword Expressions: Identification, Interpretation,
Disambiguation and Applications, pages 23?30, Sin-
gapore, August. ACL.
121
