BioNLP 2007: Biological, translational, and clinical language processing, pages 105?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
From Indexing the Biomedical Literature to Coding Clinical Text: 
Experience with MTI and Machine Learning Approaches 
Alan R. Aronson1, Olivier Bodenreider1, Dina Demner-Fushman1, Kin Wah Fung1,  
Vivian K. Lee1,2, James G. Mork1, Aur?lie N?v?ol1, Lee Peters1, Willie J. Rogers1
1Lister Hill Center 
National Library of Medicine 
Bethesda, MD 20894 
{alan, olivier, demnerd, 
kwfung, mork, neveola,  
peters, wrogers} 
@nlm.nih.gov 
 
2Vanderbilt University 
Nashville, TN 37235 
vivian.lee@vanderbilt.edu 
 
 
Abstract 
This paper describes the application of an 
ensemble of indexing and classification 
systems, which have been shown to be suc-
cessful in information retrieval and classi-
fication of medical literature, to a new task 
of assigning ICD-9-CM codes to the clini-
cal history and impression sections of radi-
ology reports. The basic methods used are: 
a modification of the NLM Medical Text 
Indexer system, SVM, k-NN and a simple 
pattern-matching method. The basic meth-
ods are combined using a variant of stack-
ing. Evaluated in the context of a Medical 
NLP Challenge, fusion produced an F-
score of 0.85 on the Challenge test set, 
which is considerably above the mean 
Challenge F-score of 0.77 for 44 participat-
ing groups. 
1 Introduction 
Researchers at the National Library of Medicine 
(NLM) have developed the Medical Text Indexer 
(MTI) for the automatic indexing of the biomedical 
literature (Aronson et al, 2004). The unsupervised 
methods within MTI were later successfully com-
bined with machine learning techniques and ap-
plied to the classification tasks in the Genomics 
Track evaluations at the Text Retrieval Conference 
(TREC) (Aronson et al, 2005 and Demner-
Fushman et al, 2006). This fusion approach con-
sists of using several basic classification methods 
with complementary strengths, combining the re-
sults using a modified ensemble method based on 
stacking (Ting and Witten, 1997). 
While these methods have shown reasonable 
performance on indexing and retrieval tasks of 
biomedical articles, it remains to be determined 
how they would perform on a different biomedical 
corpus (e.g., clinical text) and on a different task 
(e.g., coding to a different controlled vocabulary). 
However, except for competitive evaluations such 
as TREC or BioCreAtIvE, corpora and gold stan-
dards for such tasks are generally not available, 
which is a limiting factor for such studies. For a 
survey of currently available corpora and devel-
opments in biomedical language processing, see 
Hunter and Cohen, 2006. 
The Medical NLP Challenge 1  sponsored by a 
number of groups including the Computational 
Medicine Center (CMC) at the Cincinnati Chil-
dren?s Hospital Medical Center gave us the oppor-
tunity to apply our fusion approach to a clinical 
corpus. The Challenge was to assign ICD-9-CM 
codes (International Classification of Diseases, 9th 
Revision, Clinical Modification) 2  to clinical text 
consisting of anonymized clinical history and im-
pression sections of radiology reports. 
The Medical NLP Challenge organizers distrib-
uted a training corpus of almost 1,000 of the ano-
nymized, abbreviated radiology reports along with 
                                                 
1 See www.computationalmedicine.org/challenge/.
2 See www.cdc.gov/nchs/icd9.htm.
105
gold standard ICD-9-CM assignments for each 
report obtained via a consensus of three independ-
ent sets of assignments. The primary measure for 
the Challenge was defined as the balanced F-score, 
with a secondary measure being cost-sensitive ac-
curacy. These measures were computed for sub-
missions to the Challenge based on a test corpus 
similar in size to the training corpus but distributed 
without gold standard code assignments. 
The main objective of this study is to determine 
what adaptation of the original methods is required 
to code clinical text with ICD-9-CM, in contrast to 
indexing and retrieving MEDLINE?. Note that an 
earlier study (Gay et al, 2005) showed that only 
minor adaptations were required in extending the 
original model to full-text biomedical articles. A 
secondary objective is to evaluate the performance 
of our methods in this new setting. 
 
2 Methods 
In early experimentation with the training corpus 
provided by the Challenge organizers, we discov-
ered that several of the training cases involved ne-
gated assertions in the text and that deleting these 
improved the performance of all basic methods 
being tested. For example, ?no pneumonia? occurs 
many times in the impression section of a report, 
sometimes with additional context. Section 2.1 
describes the process we used to remove these ne-
gated expressions; section 2.2 consists of descrip-
tions of the four basic methods used in this study; 
and section 2.3 defines the fusion of the basic 
methods to form a final result. 
2.1 Document Preparation 
The NegEx program (Chapman et al, 2001a and 
2001b, and Goldin and Chapman, 2003), which 
discovers negated expressions in text, was used to 
find negated expressions in the training and test 
corpora using a dictionary generated from concepts 
from the 2006AD version of the UMLS? Metathe-
saurus? (excluding the AMA vocabularies). A ta-
ble containing the concept unique identifier (CUI) 
and English string (STR with LAT=?ENG?) was 
extracted from the main concept table, MRCON, 
and was used as input to NegEx to generate a dic-
tionary that was later used as the universe of ex-
pressions which NegEx could find to be negated in 
the target corpora. (See the Appendix for examples 
of the input and output to this process.) 
The XML text of the training and test corpora 
was converted to a tree representation and then 
traversed, operating on one radiology report at a 
time. The clinical history and impression sections 
of each report were tokenized to allow whitespace 
to be separated from the punctuation, numbers and 
alphabetic text. The concepts from the UMLS were 
tokenized in the same way, to allow the concepts 
found by NegEx to be aligned with the text. The 
negation phrases discovered by NegEx were also 
tokenized to find the appropriate negation phrase 
preceding or trailing the target concept. Using the 
location information obtained by matching the set 
of one or more target concepts and the associated 
negation phrase, the overlapping concept spans 
were merged and the span for the negation phrase 
and the outermost negated concept was removed. 
Any intervening concepts associated with the same 
negation phrase were removed, too. The abbrevi-
ated tree representation was then re-serialized back 
into XML. 
As an example of our use of NegEx, consider 
the report with clinical history ?13-year 2-month - 
old female evaluate for cough.? and impression 
?No focal pneumonia.? After removal of negated 
text, the clinical history becomes ?13-year 2-month 
- old female?, and the discussion is empty. 
2.2 Basic Methods 
The four basic methods used for the Medical NLP 
Challenge are MTI (a modification of NLM?s 
Medical Text Indexer system), SVM (Support 
Vector Machines), k-NN (k Nearest Neighbors) 
and Pattern Matching (a simple, pattern-based clas-
sifier). Each of these methods is described here. 
Note that the MTI method uses a ?Restrict to ICD-
9-CM? algorithm that is described in the next sec-
tion. 
 
MTI. The original Medical Text Indexer (MTI) 
system, shown in Figure 1, consists of an infra-
structure for applying alternative methods of dis-
covering MeSH? headings for citation titles and 
abstracts and then combining them into an ordered 
list of recommended indexing terms. The top por-
tion of the diagram consists of two paths, or meth-
ods, for creating a list of recommended indexing 
terms: MetaMap Indexing and PubMed? Related 
Citations. The MetaMap Indexing path actually 
106
computes UMLS Metathesaurus concepts, which 
are passed to the Restrict to MeSH process 
(Bodenreider et al, 1998). The results from each 
path are weighted and combined using Post-
Processing, which also refines the results to con-
form to NLM indexing policy. The system is 
highly parameterized not only by path weights but 
also by several parameters specific to the Restrict 
to MeSH and Post-Processing processes. 
 
 
 
Figure 1: Medical Text Indexer (MTI) System 
 
For use in the Challenge, the Medical Text In-
dexer (MTI) program itself required few adapta-
tions.  Most of the changes involved the environ-
ment from which MTI obtains the data it uses 
without changing the normal parameter settings. 
We also added a further post-processing compo-
nent to filter our results. 
For the environment, we replaced MTI?s normal 
?Restrict to MeSH? algorithm with a ?Restrict to 
ICD-9-CM? algorithm, described below, in order 
to map UMLS concepts to ICD-9-CM codes in-
stead of MeSH headings. We also trained the Pub-
Med Related Citations component, TexTool (Ta-
nabe and Wilbur, 2002), on the Medical NLP Chal-
lenge training data instead of the entire MED-
LINE/PubMed database as is the case for normal 
MTI use at NLM.  For both of these methods, we 
used the actual ICD-9-CM codes to mimic UMLS 
CUIs used internally by MTI. 
To create the new training data for the TexTool 
(Related Citations), we reformatted the Medical 
NLP Challenge training data into a pseudo-
MEDLINE format using the ?doc id? component 
as the PMID, the ?CLINICAL_HISTORY? text 
component for the Title, the ?IMPRESSION? text 
component for the Abstract, and all of the 
?CMC_MAJORITY? codes as MeSH Headings 
(see Figure 2).  This provided us with direct ICD-
9-CM codes to work with instead of MeSH Head-
ings. 
 
<doc id="97663756" type="RADIOLOGY_REPORT"> 
  <codes> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">780.6</code> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY3" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">786.2</code> 
  </codes> 
  <texts> 
    <text origin="CCHMC_RADIOLOGY" 
type="CLINICAL_HISTORY">Cough and fever.</text> 
    <text origin="CCHMC_RADIOLOGY" 
type="IMPRESSION">Normal radiographic appear-
ance of the chest, no pneumonia.</text> 
  </texts> 
</doc> 
PMID- 97663756 
TI  - Cough and fever. 
AB  - Normal radiographic appearance of the 
chest, no pneumonia. 
MH  - Fever (780.6) 
MH  - Cough (786.2) 
 
Figure 2: XML Medical NLP Training Data modi-
fied to pseudo-ASCII MEDLINE format 
 
Within MTI we also utilized an experimental 
option for MetaMap (Composite Phrases), which 
provides a longer UMLS concept match than usual. 
We did not use the following: (1) UMLS concept-
specific checking and exclusion sections; and (2) 
the MeSH Subheading generation, checking, and 
removal elements, since they were not needed for 
this Challenge. We then had MTI use the new Re-
107
strict to ICD-9-CM file and the new TexTool to 
generate its results. 
 
Restrict to ICD-9-CM. The mapping of every 
UMLS concept to ICD-9-CM developed for the 
Medical NLP Challenge is an adaptation of the 
original mapping to MeSH, later generalized to any 
target vocabulary (Fung and Bodenreider, 2005). 
Based on the UMLS Metathesaurus, the mapping 
utilizes four increasingly aggressive techniques: 
synonymy, built-in mappings, hierarchical map-
pings and associative mappings. In order to comply 
with coding rules in ICD-9-CM, mappings to non-
leaf codes are later resolved into leaf codes. 
Mappings to ICD-9-CM are identified through 
synonymy when names from ICD-9-CM are in-
cluded in the UMLS concept identified by 
MetaMap. For example, the ICD-9-CM code 592.0 
Calculus of kidney is associated with the UMLS 
concept C0392525 Nephrolithiasis through synon-
ymy. 
Built-in mappings are mapping relations be-
tween UMLS concepts implied from mappings 
provided by source vocabularies in the UMLS. For 
example, the UMLS concept C0239937 Micro-
scopic hematuria is mapped to the concept 
C0018965 (which contains the ICD-9-CM code 
599.7 Hematuria) through a mapping provided by 
SNOMED CT. 
In the absence of a mapping through synonymy 
or built-in mapping, a hierarchical mapping is 
attempted. Starting from the concept identified by 
MetaMap, a graph of ancestors is built by first us-
ing its parent concepts and broader concepts, then 
adding the parent concepts and broader concepts of 
each concept, recursively. Semantic constraints 
(based on semantic types) are applied in order to 
prevent semantic drift. Ancestor concepts closest 
to the MetaMap source concept are selected from 
the graph. Only concepts that can be resolved into 
ICD-9-CM codes (through synonymy or built-in 
mapping) are selected. For example, starting from 
C0239574 Low grade pyrexia, a mapping is found 
to ICD-9-CM code 780.6 Fever, which is con-
tained in the concept C0015967, one of the ances-
tors of C0239574. 
The last attempt to find a mapping involves not 
only hierarchical, but also associative relations. 
Instead of starting from the concept identified by 
MetaMap, associative mappings explore the con-
cepts in associative relation to this concept. For 
example, the concept C1458136 Renal stone sub-
stance is mapped to ICD-9-CM code 592.0 Calcu-
lus of kidney. 
Finally, when the identified ICD-9-CM code 
was not a leaf code (e.g., 786.5 Chest pain), we 
remapped it to one of the corresponding leaf codes 
in the training set where possible (e.g., 786.50 Un-
specified chest pain). 
Of the 2,331 UMLS concepts identified by 
MetaMap in the test set after freezing the method, 
620 (27%) were mapped to ICD-9-CM. More spe-
cifically, 101 concepts were mapped to one of the 
45 target ICD-9-CM codes present in the training 
set. Of the 101 concepts, 40 were mapped through 
synonymy, 11 through built-in mappings, 40 
through hierarchical mapping and 10 through asso-
ciative mapping. 
 
After the main MTI processing was completed, 
we applied a post-processing filter, restricting our 
results to the list of 94 valid combinations of ICD-
9-CM codes provided in the training set (hence-
forth referred to as allowed combinations) and 
slightly emphasizing MetaMap results. Examples 
of the post-processing rules are: 
? If MTI recommended 079.99 (Unspecified 
viral infection in conditions?) via either 
MetaMap or Related Citations, use 079.99, 
493.90 (Asthma, unspecified type?), and 
780.6 (Fever) for indexing. This is the only 
valid combination for this code based on the 
training corpus. 
? Similarly, if MTI recommended ?Enlarge-
ment of lymph nodes? (785.6) via the 
MetaMap path with a score greater then 
zero, use 785.6 and 786.2 (Cough) for in-
dexing. 
The best F-score (F = 0.83) for the MTI method 
was obtained on the training set using the negation-
removed text.  This was a slight improvement over 
using the original text (F = 0.82). 
 
SVM. We utilized Yet Another Learning Envi-
ronment3 (YALE), an open source application de-
veloped for machine learning and data mining, to 
determine the data classification performance of 
support vector machine (SVM) learning on the 
                                                 
3 See http://rapid-i.com. 
108
training data. To prepare the Challenge data for 
analysis, we removed all stop words and created 
feature vectors for the free text extracted from the 
?CLINICAL_HISTORY? and ?IMPRESSION? 
fields of the records.  Since both the training and 
test Challenge data had a known finite number of 
individual ICD-9-CM labels (45) and distinct com-
binations of ICD-9-CM labels (94), the data was 
prepared both as feature vectors for 45 individual 
labels as well as a model with 94 combination la-
bels.  In addition, the feature vectors were created 
using both simple term frequency as well as in-
verse document frequency (IDF) weighting, where 
the weight is (1+log(term frequency))*(total 
documents/document frequency).  There were thus 
a total of four feature vector datasets: 1) 45 indi-
vidual ICD-9-CM labels and simple term fre-
quency, 2) 45 ICD-9-CM labels and IDF weight-
ing, 3) 94 ICD-9-CM combinations and simple 
term frequency, and 4) 94 ICD-9-CM combina-
tions and IDF weighting. 
The YALE tool encompasses a number of SVM 
learners and kernel types.  For the classification 
problem at hand, we chose the C-SVM learner and 
the radial basis function (rbf) kernel.  The C-SVM 
learner attempts to minimize the error function 
?
=
+
N
i
i
T Cww
1
,
2
1 ?  
Niandbxw iii
T
i ,,1,01))(( K=???+ ????
 
where w is the vector of coefficients, b is a con-
stant, ?  is the kernel function, x are the independ-
ent variables, and ?i are parameters for handling 
the inputs.  C > 0 is the penalty parameter of the 
error function.  The rbf kernel is defined as K(x, 
x?) = exp(?? |x ? x?|2), ? > 0 where ? is a kernel 
parameter that determines the rbf width. We ran 
cross-validation experiments using YALE on all 
training datasets and varying C (10, 100, 1000, 
10000) and ? (0.01, 0.001, 0.0001, 0.00001) to de-
termine the optimal C and ? combination.  The 
cross-validation experiments generated classifica-
tion models that were then applied to the complete 
training datasets to analyze the performance of the 
learner. The 94 ICD-9-CM combination and sim-
ple term frequency dataset with C = 10000 and ? = 
0.01 had the best F-score at 0.86.  The best F-score 
for the 94 ICD-9-CM combination and IDF weight 
dataset was 0.79, where C = 0.001 and ? = 10000.   
Further preprocessing the training dataset by 
removing negated expressions was found to im-
prove the best F-score from 0.86 to 0.87.  The C = 
10000 and ? = 0.01 combination was then applied 
to the test dataset, which was preprocessed to re-
move negation and stop words and transformed to 
a feature vector using 94 ICD-9-CM combinations 
and simple term weighting.  The predicted ICD-9-
CM classifications and confidence of the predic-
tions for each clinical free text report were output 
and later combined with other methods to optimize 
the accuracy and precision of our ICD-9-CM clas-
sifications. 
 
k-NN. The Challenge training set was used to 
build a k-NN classifier. The k-NN classification 
method works by identifying, within a labelled set, 
documents similar to the document being classi-
fied, and inferring a classification for it from the 
labels of the retrieved neighbors. 
The free text in the training data set was proc-
essed to obtain a vector-space representation of the 
patient reports.  
Several methods of obtaining this representation 
were tested: after stop words were removed, simple 
term frequency and inverse document frequency 
(IDF) weighting were applied alternatively. A 
higher weight was also given to words appearing in 
the history portion of the text (vs. impression). 
Eventually, the most efficient representation was 
obtained by using controlled vocabulary terms ex-
tracted from the free text with MetaMap.4 Further 
processing on this representation of the training 
data showed that removing negated portions of the 
free text improved the results, raising the F-score 
from 0.76 to 0.79.   
Other parameters were also assessed on the 
training data, such as the number of neighbors to 
use (2 was found to be the best vs. 5, 10 or 15) and 
the restriction of the ICD-9-CM predictions to the 
set of 94 allowed combinations. When the predic-
tion for a given document was not within the set of 
allowed 94 combinations, an allowed subset of the 
ICD-9-CM codes predicted was selected based on 
the individual scores obtained for each ICD-9-CM 
code.  
The best F-score (F = 0.79) obtained on the 
training set used the MetaMap-based representa-
                                                 
4 Note that this use of MetaMap is independent of its 
inclusion as a component of MTI. 
109
tion with simple frequency counts on the text with 
negated expressions removed. ICD-9-CM predic-
tions were obtained from the nearest neighbors and 
restricted to one of the 94 allowed combinations.   
 
Pattern Matching. We developed a pattern-
matching classifier as a baseline for our more so-
phisticated classification methods. A list of all 
UMLS string representations for each of 45 codes 
(including synonyms from source vocabularies 
other than ICD-9-CM) was created as described in 
the MTI section above. The strings were then con-
verted to lower case, punctuation was removed, 
and strings containing terms unlikely to be found 
in a clinical report were pruned. For example, Ab-
domen NOS pain and Abdominal pain (finding) 
were reduced to abdominal pain. For the same rea-
sons, some of the strings were relaxed into pat-
terns. For example, it is unlikely to see PAIN 
CHEST in a chart, but very likely to find pain in 
chest. The string, therefore, was relaxed to the fol-
lowing pattern: pain.*chest. The text of the clinical 
history and the impression fields of the radiology 
reports with negated expressions removed (see 
Section 2.2) was broken up into sentences. Each 
sentence was then searched for all available pat-
terns. A corresponding code was assigned to the 
document for each matched pattern. This pattern 
matching achieved F-score = 0.79 on the training 
set. To reduce the number of codes assigned to a 
document, a check for allowed combinations was 
added as a post-processing step. The combination 
of assigned codes was looked up in the table of 
allowed codes. If not present, the codes were re-
duced to the combination of assigned codes most 
frequently occurring in the training set. This 
brought the F-score up to 0.84 on the training data. 
As the performance of this classifier was compara-
ble to other methods, we decided to include these 
results when combining the predictions of the other 
classifiers.  
2.3 Fusion of  Basic Methods: Stacking 
Experience with ad hoc retrieval tasks in the TREC 
Genomics Track has shown that combining predic-
tions of several classifiers either significantly im-
proves classification results, or at least provides 
more consistent and stable results when the train-
ing data set is small (Aronson et al, 2005). We 
therefore experimented with stacking (Ting and 
Witten, 1997), using a simple majority vote and a 
union of all assigned codes as baselines. The pre-
dictions of base classifiers described in the previ-
ous section were combined using our re-
implementation of the stacked generalization pro-
posed by Ting and Witten.  
3 Results 
Table 1 shows the results obtained for the training 
set. The best stacking results were obtained using 
predictions of all four base classifiers on the text 
with deleted negated expressions and with check-
ing for allowed combinations. We retained all final 
predictions with probability of being a valid code 
greater than 0.3. Checking for the allowed combi-
nations for the ensemble classifiers degraded the F-
score significantly. 
 
Classifier F-score 
MTI 0.83 
SVM 0.87 (x-validation) 
k-NN 0.79 (x-validation) 
Pattern Matching 0.84 
Majority 0.82 
Stacking 0.89 
 
Table 1: Training results for each classifier, the ma-
jority and stacking 
 
Since stacking produced the best F-score on the 
training corpus and is known to be more robust 
than the individual classifiers, the corresponding 
results for the test corpus were submitted to the 
Challenge submission website. The stacking results 
for the test corpus achieved an F-score of 0.85 and 
a secondary, cost-sensitive accuracy score of 0.83. 
For comparison purposes, 44 Challenge submis-
sions had a mean F-score of 0.77 with a maximum 
of 0.89. Our F-score of 0.85 falls between the 70th 
and 75th percentiles. 
4 Discussion 
It is significant that it was fairly straightforward to 
port various methods developed for ad hoc MED-
LINE citation retrieval, indexing and classification 
to the assignment of codes to clinical text. The 
modifications to MTI consisted of replacing Re-
strict to MeSH with Restrict to ICD-9-CM, training 
the Related Citations method on clinical text and 
replacing MTI?s normal post-processing with a 
much simpler version. Preprocessing the text using 
110
NegEx to remove negated expressions was a fur-
ther modification of the overall approach. 
It is noteworthy that a simple pattern-matching 
method performed as well as much more sophisti-
cated methods in the effort to fuse results from 
several methods into a final outcome. This unex-
pected success might be explained by the follow-
ing limitations of the Challenge. 
Possible limitations on the extensibility of the 
current research arise from two observations: (1) 
the Challenge cases were limited to two relatively 
narrow topics, cough/fever/pneumonia and uri-
nary/kidney problems; and (2) the clinical text was 
almost error-free, a situation that would not be ex-
pected in the majority of clinical text. It is possible 
that these conditions contributed to the success of 
the pattern-matching method but also caused 
anomalous behavior, such as the fact that simple 
frequency counts provided a better representation 
than IDF for the SVM and k-NN methods. 
Finally, as a result of low confidence in the 
ICD-9-CM code assignment, no codes were as-
signed to 29 records in the test set. It is worthwhile 
to explore the causes for such null assignments. 
One of the reasons for low confidence could be the 
aggressive pruning of the text by the negation algo-
rithm. For example, after removal of negated text 
in the sample report given in section 2.1, the only 
remaining text is ?13-year 2-month - old female? 
from the clinical history field; this provided no 
evidence for code assignment. Secondly, in some 
cases the original text was not sufficient for confi-
dent code assignment. For example, for the docu-
ment with clinical history ?Bilateral grade 3.? and 
impression ?Interval growth of normal appearing 
Kidneys?, no code was assigned by the SVM, k-
NN, or pattern-matching classifiers. Code 593.70 
corresponding to the UMLS concept Vesicouret-
eral reflux with reflux nephropathy, unspecified or 
without reflux nephropathy was assigned by MTI 
with a very low confidence, which was not suffi-
cient for the final assignment of the code. The third 
reason for assigning no code to a document was 
the wide range of assignments provided by the 
base classifiers. For example, for the following 
document: ?CLINICAL_HISTORY: 3-year - old 
male with history of left ureteropelvic and uret-
erovesical obstruction. Status post left pyeloplasty 
and left ureteral reimplantation. IMPRESSION: 1. 
Stable appearance and degree of hydronephrosis 
involving the left kidney. Stable urothelial thicken-
ing. 2. Interval growth of kidneys, left greater than 
right. 3. Normal appearance of the right kidney 
with interval resolution of right urothelial thicken-
ing.? MTI assigned codes 593.89 Other specified 
disorders of kidney and ureter and 591 Hy-
dronephrosis. Codes 593.70 Vesicoureteral reflux 
with reflux nephropathy, unspecified or without 
reflux nephropathy and 753.3 Double kidney with 
double pelvis were assigned by the k-NN classifier. 
Pattern matching resulted in assignment of code 
591 with fairly low confidence. No code was as-
signed to this document by the SVM classifier. 
Despite failing to assign codes to these 29 records, 
the conservative approach (using threshold) re-
sulted in better performance, achieving F-score 
0.85 compared to F-score 0.80 when all 1,634 
codes assigned by the base classifiers were used. 
5 Conclusion 
We are left with two conclusions. First, this re-
search confirms that combining several comple-
mentary methods for accomplishing tasks, ranging 
from ad hoc retrieval to categorization, produces 
results that are better and more stable than the re-
sults for the contributing methods. Furthermore, 
we have shown that the basic methods employing 
domain knowledge and advanced statistical algo-
rithms are applicable to clinical text without sig-
nificant modification. Second, although there are 
some limitations of the current Challenge test col-
lection of clinical text, we appreciate the efforts of 
the Challenge organizers in the creation of a test 
collection of clinical text. This collection provides 
a unique opportunity to apply existing methods to a 
new and important domain. 
Acknowledgements 
This work was supported in part by the Intramural 
Research Program of the NIH, National Library of 
Medicine and by appointments of Aur?lie N?v?ol 
and Vivian Lee to the NLM Research Participation 
Program sponsored by the National Library of 
Medicine and administered by the Oak Ridge Insti-
tute for Science and Education. 
The authors gratefully acknowledge the many 
essential contributions to MTI, especially W. John 
Wilbur for the PubMed Related Citations indexing 
method, and Natalie Xie for adapting TexTool (an 
interface to Related Citations) for this paper. 
111
References 
Aronson AR, Demner-Fushman D, Humphrey SM, Lin 
J, Liu H, Ruch P, Ruiz ME, Smith LH, Tanabe LK, 
Wilbur WJ. Fusion of knowledge-intensive and sta-
tistical approaches for retrieving and annotating tex-
tual genomics documents. Proc TREC 2005, 36-45. 
Aronson AR, Mork JG, Gay CW, Humphrey SM and 
Rogers WJ. The NLM Indexing Initiative's Medical 
Text Indexer. Medinfo. 2004: 268-72. 
Bodenreider O, Nelson SJ, Hole WT and Chang HF. 
Beyond synonymy: exploiting the UMLS semantics 
in mapping vocabularies. Proc AMIA Symp 1998: 
815-9. 
Chapman WW, Bridewell W, Hanbury P, Cooper GF, 
Buchanan B. Evaluation of negation phrases in narra-
tive clinical reports. Proc AMIA Symp. 2001a:105-9.  
Chapman WW, Bridewell W, Hanbury P, Cooper GF 
and Buchanan BG. A simple algorithm for identify-
ing negated findings and diseases in discharge sum-
maries. J Biomed Inform. 2001b;34:301-10.  
Demner-Fushman D, Humphrey SM, Ide NC, Loane RF, 
Ruch P, Ruiz ME, Smith LH, Tanabe LK, Wilbur WJ 
and Aronson AR. Finding relevant passages in scien-
tific articles: fusion of automatic approaches vs. an 
interactive team effort. Proc TREC 2006, 569-76. 
Fung KW and Bodenreider O. Utilizing the UMLS for 
semantic mapping between terminologies. AMIA 
Annu Symp Proc 2005: 266-70. 
Gay CW, Kayaalp M and Aronson AR. Semi-automatic 
indexing of full text biomedical articles. AMIA Annu 
Symp Proc. 2005:271-5. 
Goldin I and Chapman WW. Learning to detect nega-
tion with ?not? in medical texts. Proc Workshop on 
Text Analysis and Search for Bioinformatics, ACM 
SIGIR, 2003. 
Hunter L and Cohen KB. Biomedical language process-
ing: what?s beyond PubMed? Mol Cell. 2006 Mar 
3;21(5):589-94. 
Tanabe L and Wilbur WJ. (2002) Tagging gene and 
protein names in biomedical text. Bioinformatics, 
Aug 2002; 18: 1124 ?32. 
Ting WK and Witten I. 1997. Stacking bagged and dag-
ged models. 367-375. Proc. of ICML'97. Morgan 
Kaufmann, San Francisco, CA.
Appendix  
A sample of the input to NegEx for dictionary generation:  
 
C0002390 pneumonitis, allergic interstitial 
C0002390 allergic interstitial pneumonitis, nos 
C0002390 extrinsic allergic bronchiolo alveolitis 
C0002390 extrinsic allergic bronchiolo alveolitis, nos 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 eaa  extrinsic allergic alveolitis 
C0002390 allergic extrinsic alveolitis nos (disorder) 
C0002390 extrinsic allergic alveolitis (disorder) 
C0002390 hypersensitivity pneumonitis nos (disorder) 
 
A sample of the dictionary generated by NegEx for later use in detecting negated expressions:  
 
C0002098 hypersensitivity granuloma (morphologic abnormality 
C0151726 hypersensitivity injection site 
C0020517 hypersensitivity nos 
C0429891 hypersensitivity observations 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 hypersensitivity pneumonitides 
C0005592 hypersensitivity pneumonitides, avian 
C0002390 hypersensitivity pneumonitis 
C0182792 hypersensitivity pneumonitis antibody determination re-
agents 
112
BioNLP 2007: Biological, translational, and clinical language processing, pages 183?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Indexing of Specialized Documents: 
Using Generic vs. Domain-Specific Document Representations
Aur?lie N?v?ol James G. Mork
{neveola,mork,alan}@nlm.nih.gov
National Library of Medicine
8600 Rockville Pike 
Bethesda, MD 20894
USA
Alan R. Aronson
Abstract
The shift from paper to electronic docu-
ments has caused the curation of informa-
tion sources in large electronic databases
to become more generalized. In the bio-
medical domain, continuing efforts aim at 
refining indexing tools to assist with the 
update and maintenance of databases such 
as MEDLINE?. In this paper, we evaluate 
two statistical methods of producing 
MeSH? indexing recommendations for 
the genetics literature, including recom-
mendations involving subheadings, which 
is a novel application for the methods. We 
show that a generic representation of the 
documents yields both better precision 
and recall. We also find that a domain-
specific representation of the documents 
can contribute to enhancing recall. 
1 Introduction
There are two major approaches for the automatic 
indexing of text documents: statistical approaches 
that rely on various word counting techniques [su-
ch as vector space models (Salton, 1989), Latent 
Semantic Indexing (Deerwester et al, 1990) or
probabilistic models (Sparck-Jones et al, 2000)] 
and linguistic approaches that involve syntactical 
and lexical analysis [see for example term extrac-
tion and term variation recognition in systems such 
as MetaMap (Aronson, 2001), FASTR (Jacquemin 
and Tzoukermann, 1999) or IndDoc (Nazarenko 
and Ait El Mekki, 2005)]. In many cases, the com-
bination of these approaches has been shown to 
improve the performance of a single approach both 
for controlled indexing (Aronson et al, 2004) and
free text indexing (Byrne and Klein, 2003).
Recently, N?v?ol et al (2007) presented lin-
guistic approaches for the indexing of documents 
in the field of genetics. In this paper, we explore a 
statistical approach of indexing for text documents 
also in the field of genetics. This approach was 
previously used successfully to produce Medical 
Subject Headings (MeSH) main heading recom-
mendations. Our goal in this experiment is two-
fold: first, extending an existing method to the pro-
duction of recommendations involving subhead-
ings and second, assessing the possible benefit of 
using a domain-specific variant of the method. 
2 A k-Nearest-Neighbors approach for 
indexing 
2.1 Principle
The k-Nearest-Neighbors (k-NN) approach views 
indexing as a multi-class classification problem 
where a document may be assigned several
?classes? in the form of indexing terms. It requires 
a large set of labeled data composed of previously 
indexed documents. k-NN relies on the assumption 
that similar documents should be classified in a 
similar way. The algorithm consists of two steps: 
1/documents that are most ?similar? to the query 
document must be retrieved from the set of labeled 
documents. They are considered as ?neighbors? for 
the query document; 2/an indexing set must be 
produced from these and assigned to the query 
document.
Finding similar documents
All documents are represented using a vector of 
distinctive features within the representation space. 
Based on this representation, labeled documents 
183
may be ranked according to their similarity to the 
query document using usual similarity measures 
such as cosine or Dice. The challenge in this step is 
to define an appropriate representation space for 
the documents and to select optimal features for 
each document. Another issue is the number (k) of 
neighbors that should be selected to use in the next 
step.
Producing an indexing set
When applied to a single-class classification prob-
lem, the class that is the most frequent among the k 
neighbors is usually assigned to the query docu-
ment. Indexing is a multi-class problem for which 
the number of classes a document should be as-
signed is not known, as it may vary from one 
document to another. Therefore, indexing terms 
from the neighbor documents are all taken into 
account and ranked according to the number of 
neighbors that were labeled with them. The more 
neighbors labeled with a given indexing term, the 
higher the confidence that it will be a relevant in-
dexing term for the query document. This resulting 
indexing set may then be filtered to select only the 
terms that were obtained from a defined minimum 
number of neighbors.
2.2 Document representation
Generic representation 
A generic representation of documents is obtained 
from the text formed by the title and abstract. This 
text is processed so that punctuation is removed, 
stop-words from a pre-defined list (of 310 words) 
are removed, remaining words are switched to 
lower case and a minimal amount of stemming is 
applied. As described by Salton (1989) words 
should be weighted according to the number of 
times they occur in the query document and the 
number of times they occur in the whole collection
(here, MEDLINE). Moreover, words from the title 
are given an additional weight compared to words 
from the abstract. Further adjustments relative to 
document length and local weighting according to 
the Poisson distribution are detailed in (Aronson et 
al, 2000; Kim et al, 2001) where the PubMed Re-
lated Citations (PRC) algorithm is discussed. Fur-
ther experiments showed that the best results were 
obtained by using the ten nearest neighbors.
Domain-specific representation 
In specialized domains, documents from the litera-
ture may be represented with concepts or objects 
commonly used or studied in the field. For exam-
ple, (Rhodes et al, 2007) meet specific chemistry 
oriented search needs by representing US patents 
and patent applications with molecular information 
in the form of chemical terms and structures. A
similar representation is used for PubChem
(http://pubchem.ncbi.nlm.nih.gov/) records. In the 
genetics domain, genes are among the most com-
monly discussed or manipulated concepts. There-
fore, genes should provide a relevant domain-
specific description of documents from the genet-
ics literature.
The second indexing algorithm that we describe 
in this paper, know as the Gene Reference Into 
Function (GeneRIF) Related Citations (GRC) algo-
rithm, uses ?GeneRIF? links (defined in the para-
graph below) to retrieve neighbors for a query 
document.
To form a specific representation of the docu-
ment, gene names are retrieved by ABGene1 (Ta-
nabe and Wilbur, 2002) and mapped to Entrez 
Gene2 unique identifiers. The mapping was per-
formed with a version of SemRep (Rindflesch and
Fiszman, 2003) restricted to human genes. It con-
sists in normalizing the gene name (switch to lower 
case, remove spaces and hyphens) and matching 
the resulting string to one of the gene names or 
aliases listed in Entrez Gene.
For each gene, the GeneRIF links supply a sub-
set of MEDLINE citations manually selected by 
NLM indexers for describing the functions associ-
ated with the gene. These sets were used in two 
ways: 
To complete the document representation. If a 
citation was included in the GeneRIF of a 
given gene, the gene was given an additional 
weight in the document representation. 
To limit the set of possible neighbors. In the 
generic representation, all MEDLINE cita-
tions contain the representation features, 
words. Therefore, they all have to be con-
sidered as potential neighbors. However, 
                                                          
1 Software downloaded January 17, 2007, from
http://www.ncbi.nlm.nih.gov/staff/lsmith/MedPost.html 
2 Retrieved January 17, 2007, from: 
http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene
184
only a subset of citations actually contains
genes. Therefore, only those citations need 
to be considered as potential neighbors. This 
observation enables us to limit the specific 
processing to relevant citations. Possible 
neighbors for a query document consist of 
the union of the GeneRIF citations corre-
sponding to each gene in the document rep-
resentation. 
Table 1: Gene description of a sample MEDLINE 
document and its two nearest neighbors
PubMed IDs ABGene Entrez Gene IDs
15645653 abcc6 
mrp6
ldl-r
pxe
fh
368
368; 6283
3949
368; 5823
2271
10835643 mrp6
pxe
368; 6283
368; 5823
16392638 abcc6
mrp6
pxe
368
368; 6283
368; 5823
For each query document, the set of possible 
neighbors was processed and ranked according to 
gene similarity using a cosine measure. Table 1 
shows the description of a sample MEDLINE cita-
tion and its two nearest neighbors. 
Based on experiments with the PubMed Related 
Citations algorithm, ten neighbors were retained to 
form a candidate set of indexing terms.
3 Experiment
3.1 Application to MeSH indexing  
In the MEDLINE database, publications of the bio-
medical domain are indexed with Medical Subject 
Headings, or MeSH descriptors. MeSH contains 
about 24,000 main headings denoting medical con-
cepts such as foot, bone neoplasm or appendec-
tomy. MeSH also contains 83 subheadings such as 
genetics, metabolism or surgery that can be associ-
ated with the main headings in order to refer to a 
specific aspect of the concept. Moreover, each de-
scriptor (a main heading alone or associated with 
one or more subheadings) is assigned a ?minor? or 
?major? weight depending on how substantially the 
concept it denotes is discussed in the article. ?Ma-
jor? descriptors are marked with a star. 
In order to form a candidate indexing set to be 
assigned to a query document, the descriptors as-
signed to each of the neighbors were broken down 
into a set of main headings and pairs (i.e. a main 
heading associated with a single subheading). For 
this experiment, indications of major terms were 
ignored. 
For example, the MeSH descriptor 
*Myocardium/cytology/metabolism would gener-
ate the main heading Myocardium and the two 
pairs Myocardium/cytology and Myocar-
dium/metabolism. 
3.2 Test Corpus
Both methods were tested on a corpus composed of 
a selection of the 49,863 citations entered into 
MEDLINE in January 2005. The 2006 version of 
MeSH was used for the indexing in these citations. 
About one fifth of the citations (10,161) are con-
sidered to be genetics-related, as determined by 
Journal Descriptor Indexing (Humphrey, 1999). 
Our test corpus was composed of genetics-related 
citations from which Entrez Gene IDs could be 
extracted ? about 40% of the cases. The final test 
corpus size was 3,962. Appendix A shows a sam-
ple citation from the corpus.
3.3 Protocol
Figure 1 shows the setting of our experiment. 
Documents from the test corpus described above 
were processed to obtain both a generic and spe-
cific representation as described in section 2.2. The 
corresponding ten nearest neighbors were retrieved 
using the PRC and GRC algorithms. All the 
neighbors? MeSH descriptors were pooled to form 
candidate indexing sets of descriptors that were
evaluated using precision and recall measures. Pre-
cision was the number of candidate descriptors that 
were selected as indexing terms by NLM indexers 
(according to reference MEDLINE indexing) over 
the total number of candidate descriptors. Recall 
was the number of candidate descriptors that were 
selected as indexing terms by NLM indexers over 
the total number of indexing terms expected (ac-
cording to reference MEDLINE indexing). For 
better comparison between the methods, we also 
computed F-measure giving equal weight to preci-
185
sion and recall - F1=2*PR/(P+R) and giving a 
higher weight to recall - F3=10*PR/(9P+R).
Four different categories of descriptors were 
considered in the evaluation: 
MH: MeSH main headings (regardless of 
whether subheadings were attached in the 
reference indexing)
SH: stand-alone subheadings (regardless of the 
main heading(s) they were attached to in the 
reference indexing)
MH/SH: main heading/subheading pairs
DESC: MeSH descriptors, i.e. main headings 
and main heading/subheading pairs
Similarly, four different candidate indexing sets 
were considered: the indexing set resulting from 
PRC, the indexing set resulting from GRC, the in-
dexing set resulting from the pooling of PRC and 
GRC sets and finally the indexing set resulting 
from the intersection of PRC and GRC indexing 
sets (common index terms). 
Figure 1: Producing candidate indexing sets with 
generic and domain-specific representations.
4 Results
Appendix B shows the indexing sets obtained from 
the GRC and PRC algorithms for a sample citation 
from the test corpus. Table 2 presents the results of 
our experiments. For each category of descriptors, 
the best performance was bolded. It can be ob-
served that in general, the best precision and F1
scores are obtained with the common indexing set, 
the best recall is obtained with the pooling of in-
dexing sets and the best F3 score is obtained with 
PRC algorithm, the pooling of indexing sets being 
a close second. 
5 Discussion
5.1 Performance of the methods
As can be seen from the bolded figures in table 2, 
the best performance is obtained either from the 
PRC algorithm, or from a combination of PRC and 
GRC. When indexing methods are combined, it is 
usually expected that statistical methods will pro-
vide the best recall whereas linguistic methods will 
provide the best precision. Combining complemen-
tary methods is then expected to provide the best 
overall performance. In this context, it seems that 
the option of pooling the indexing sets should be 
retained for further experiments. The most signifi-
cant result of this study is that the pooling of meth-
ods achieves a recall of 92% for stand-alone 
subheading retrieval. While the precision is only 
19%, the selection of stand-alone subheadings of-
fered by our methods is nearly exhaustive and it 
reduces by 70% the size of the list of allowable 
subheadings that could potentially be used. NLM 
indexers have declared this could prove very useful 
to enhance their indexing practice.
In order to qualify the added value of the spe-
cific description, we looked at the descriptors that 
were correctly recommended by GRC and not rec-
ommended by PRC. Check Tags (descriptors used 
to denote the species, age and gender of the sub-
jects discussed in an article) seemed prominent, but 
only Human was significantly recommended cor-
rectly more often than it was recommended incor-
rectly (~2.2 times more correct than incorrect 
recommendations ? 2,712 correct vs. 1,250 incor-
rect). No other descriptor could be identified as 
being consistently recommended either correctly or 
incorrectly.
Generic 
representation 
Text Words
Specific
 representation
Genes
1- Find similar 
documents
MEDLINE document
PubMed
Related Citations
(PRC)
GeneRIFs
Related Citations
(GRC)
2- Use index terms
in similar
 documents as 
indexing candidates
PRC 
indexing set
GRC
indexing set
186
For both methods, filtering the indexing sets 
according to the number of neighbors that lead to 
include the indexing terms results in an increase of 
precision and a loss of recall. The best trade-off
(measured by F1) is obtained when indexing terms 
come from at least three neighbors (data not 
shown).
5.2 A scale of indexing performance
The problem with evaluating indexing is that, 
although inter-indexer variability is reduced when 
a controlled vocabulary is used, indexing is an 
open cognitive task for which there is no unique 
?right? solution.
Table 2: performance of the indexing methods on the four categories of descriptors
SH MH SH/MH DESC
P      R       F1     F3 P      R      F1      F3 P      R      F1      F3 P      R      F1      F3
GRC 21     72     32     58 8      49     14      32 3      23     6        14 6      38     10      25
PRC 27     88     41     72 13    61     22      45 8      56     15      36 11    59     18      41
Pool 19     92     32     67 9      82     16      44 5      62     9        29 7      74     13     38
Common 36     68     47     62 22    27     24      27 18    17     17      17 21    23     22      23
In practice, this means that there is no ideal 
unique set of descriptors to use for the indexing 
of a particular document. Therefore, when com-
paring an indexing set obtained automatically 
(e.g. here with the PRC or GRC methods) to a 
?gold standard? indexing set produced by a 
trained indexer (e.g. here, NLM indexers) the 
difference observed can be due to erroneous de-
scriptors produced by the automatic methods. 
But it is also likely that the automatic methods 
will produce terms that are semantically close to 
what the human indexer selected or even rele-
vant terms that the human indexer considered or 
forgot to select. While evaluation methods to 
assess the semantic similarity between indexing 
sets are investigated (N?v?ol et al 2006), a con-
sistency study by Funk et al (1983) can shade 
some light on inter-indexer consistency in 
MEDLINE and what range of performance may 
be expected from automatic systems. In this 
study, Hooper?s consistency (the average pro-
portion of terms in agreement between two in-
dexers) for stand-alone subheadings (SH) was 
48.7%. It was 33.8% for pairs (MH/SH) and 
48.2% for main headings (MH). In light of these 
figures, although no direct comparison with the 
results of our experiment is possible, the preci-
sion obtained from the common recommenda-
tions (especially for stand-alone subheadings, 
36%) seems reasonably useful. Further more, 
when informally presenting the indexers sample 
recommendations obtained with these methods, 
they expressed their interest in the high recall as 
reviewing a larger selection of potentially useful 
terms might help them track important descrip-
tors they may not have thought of using other-
wise.
In comparison with other research, the results 
are also encouraging: the recall resulting from 
either PRC or pooling the indexing sets is sig-
nificantly better than that obtained by N?v?ol et 
al. (2007) on a larger set of MEDLINE 2005 
citations ? 20% at best for main head-
ing/subheading pairs with a dictionary-based 
method which consisted in extracting main head-
ing and subheading separately from the citations 
(using MTI and string matching dictionary en-
tries) before forming all the allowable pairs as 
recommendations.
5.3 Limitations of the experiment
In the specific description, the mapping between 
gene names and Entrez Gene IDs only takes hu-
man genes into account, which potentially limits 
the scope of the method, since many more or-
ganisms and their genes may be discussed in the 
literature. In some cases, this limitation can lead 
to confusion with other organisms. For example, 
the gene EPO ?erythropoietin? is listed in Entrez 
Gene for 11 organisms including Homo Sapiens. 
With our current algorithm, this gene will be 
assumed to be a human gene. In the case of 
PMID 15213094 in our test corpus, the organism 
discussed in the paper was in fact Mus Musculus
(common mouse). In this particular case, the 
check tag Humans, which was erroneous, could 
be found in the candidate indexing set. However, 
187
correct indexing terms could still be retrieved 
due to the fact that both the human and mouse 
gene share common functions. 
Another limitation is the size of the test cor-
pus, which was limited to less than 4,000 docu-
ments.
5.4 Mining the biomedical literature for 
gene-concept links
Other approaches to gene-keyword mapping ex-
ploit the links between genes and diseases or 
proteins as they are described either in the re-
cords of databases such as OMIM or more for-
mally expressed as in the GeneRIF. Substantial 
work has addressed linking DNA microarray 
data to keywords in controlled vocabulary such 
as MeSH (Masys et al 2001) or characterizing 
gene clusters with text words from the literature 
(Liu et al 2004). However, no normalized ?se-
mantic fingerprinting? has been yet produced 
between controlled sets such as Entrez Gene and 
MeSH terms.
6 Conclusion and future work
In this paper, we applied a statistical method for 
indexing documents from the genetics literature. 
We presented two different document represen-
tations, one generic and one specific to the ge-
netics domain. The results bear out our 
expectations that such statistical methods can 
also be used successfully to produce recommen-
dations involving subheadings. Furthermore, 
they yield higher recall than other more linguis-
tic-based methods. In terms of recall, the best 
results are obtained when the indexing sets from 
both the specific and generic representations are 
pooled.
In future work, we plan to refine the algorithm 
based on the specific method by expending its
scope to other organisms than Homo Sapiens
and to take the gene frequency in the title and 
abstract of documents into account for the repre-
sentation. Then, we shall conduct further evalua-
tions in order to observe the impact of these 
changes, and to verify that similar results can be 
obtained on a larger corpus.
Acknowledgments
This research was supported in part by an ap-
pointment of A. N?v?ol to the Lister Hill Center 
Fellows Program sponsored by the National Li-
brary of Medicine and administered by the Oak
Ridge Institute for Science and Education. The 
authors would like to thank Halil Kilicoglu for 
his help with obtaining Entrez Gene IDs from 
the ABgene output. We also thank Susanne 
Humphrey and Sonya Shooshan for their in-
sightful comments on the preparation and edit-
ing of this manuscript.
References
Alan R. Aronson, Olivier Bodenreider, H. Florence 
Chang, Susanne M. Humphrey, James G. Mork, 
Stuart J. Nelson, Thomas C. Rindflesch and W. 
John Wilbur. 2000. The NLM Indexing Initiative. 
Proceedings of the Annual American Medical In-
formatics Association Symposium. (AMIA 2000):
17-21.
Alan R. Aronson. 2001. Effective mapping of bio-
medical text to the UMLS Metathesaurus: the 
MetaMap program. Proceedings of the Annual 
AMIA Symposium. (AMIA 2001):17-21.
Alan R. Aronson, James G. Mork, Cliff W. Gay, Su-
sanne M. Humphrey and William J. Rogers. 2004. 
The NLM Indexing Initiative's Medical Text In-
dexer. Proceedings of Medinfo 2004: 268-72.
Kate Byrne and Ewan Klein. 2003. Image Retrieval 
using Natural Language and Content-Based tech-
niques. In Arjen P. de Vries, ed. Proceedings of 
the 4th Dutch-Belgian Information Retrieval 
Workshop (DIR 2003):57-62.
Scott Deerwester, Susan Dumais, Georges Furnas, 
Thomas Landauer and Richard Harshman. 1990. 
Indexing by latent semantic analysis. Journal of 
American Society for Information Science, 6 
(41):391-407.
Mark E. Funk, Carolyn A. Reid and Leon S. 
McGoogan. 1983. Indexing consistency in 
MEDLINE. Bull. Med. Libr. Assoc. 71(2):176-
183.
Susanne M. Humphrey. 1999. Automatic indexing of 
documents from journal descriptors: a preliminary 
investigation. J Am Soc Inf Sci Technol. 50(8):661-
674
Christian Jacquemin and Evelyne Tzoukermann. 
1999. NLP for term variant extraction: Synergy of 
morphology, lexicon, and syntax. In T. 
Strzalkowski (Ed.), Natural language information 
retrieval (p. 25-74). Boston, MA: Kluwer.
188
Won Kim, Alan R. Aronson and W. John Wilbur. 
2001. Automatic MeSH term assignment and qual-
ity assessment. Proceedings of the Annual AMIA
Symposium: 319-23.
Ying Liu, Martin Brandon, Shamkant Navathe, Ray 
Dingledine and Brian J. Ciliax. 2004. Text mining 
functional keywords associated with genes. Pro-
ceedings of MEDINFO 2004: 292-296
Daniel R. Masys, John B. Welsh, J. Lynn Fink, Mi-
chael Gribskov, Igor Klacansky and Jacques Cor-
beil. 2001. Use of keyword hierarchies to interpret 
gene expression patterns. In: Bioinformatics
17(4):319-326  
Adeline Nazarenko and Touria Ait El Mekki 2005. 
Building back-of-the-book indexes. In: Terminol-
ogy 11(1):199?224
Aur?lie N?v?ol, Kelly Zeng, Olivier Bodenreider. 
2006. Besides precision & recall: Exploring alter-
native approaches to evaluating an automatic in-
dexing tool for MEDLINE. Proceedings of the 
Annual AMIA Symposium: 589-93.
Aur?lie N?v?ol, Sonya E. Shooshan, Susanne M. 
Humphrey, Thomas C. Rindflesch and Alan R 
Aronson. 2007. Multiple approaches to fine-
grained indexing of the biomedical literature. Pro-
ceedings of the 12th Pacific Symposium on Bio-
computing. 12:292-303
James Rhodes, Stephen Boyer, Jeffrey Kreulen, Ying 
Chen, Patricia Ordonez. 2007. Mining Patents Us-
ing Molecular Similarity Search. Proceedings of 
the 12th Pacific Symposium on Biocomputing. 
12:304-315
Thomas C. Rindflesch and Marcelo Fiszman. 2003. 
The interaction of domain knowledge and linguis-
tic structure in natural language processing: inter-
preting hypernymic propositions in biomedical 
text. J Biomed Inform. 36(6), 462-77
Gerald Salton. 1989. Automatic text processing : The 
transformation, analysis, and retrieval of informa-
tion by computer. Reading, MA : Addison-Wesley.
Karen Sparck-Jones, Steve Walker and Stephen E. 
Robertson. 2000. A probalistic model of informa-
tion retrieval: development and comparative ex-
periments (part 1). Information Processing and 
Management, 36(3):779-808.
Lorraine Tanabe and W. John Wilbur. 2002. Tagging 
gene and protein names in biomedical text. Bioin-
formatics. 2002 Aug;18(8):1124-32.
Appendix A: Title, abstract and reference indexing set for a sample citation
PubMed ID 15645653
Title Identification of two novel missense mutations (p.R1221C and p.R1357W) in the ABCC6 (MRP6) 
gene in a Japanese patient with pseudoxanthoma elasticum (PXE).
Abstract Pseudoxanthoma elasticum (PXE) is a rare, inherited, systemic disease of elastic tissue that in par-
ticular affects the skin, eyes, and cardiovascular system. Recently, the ABCC6 (MRP6) gene was 
found to cause PXE. A defective type of ABCC6 gene (16pl3.1) was determined in two Japanese 
patients with PXE. In order to determine whether these patients have a defect in ABCC6 gene, we 
examined each of 31 exons and flanking intron sequences by PCR methods (SSCP screening and 
direct sequencing). We found two novel missense variants in exon 26 and 29 in a compound het-
erozygous state in the first patient. One is a missense mutation (c.3661C>T; p.R1221C) in exon 26 
and the other is a missense mutation (c.4069C>T; p.R1357W) in exon 29. These mutations have 
not been detected in our control panel of 200 alleles. To our knowledge, this is the first report of 
mutation identification in the ABCC6 gene in Japanese PXE patients. The second patient was ho-
mozygous for 2542_2543delG in ABCC6 gene and heterozygous for 6 kb deletion of LDL-R gene. 
This case is the first report of a genetically confirmed case of double mutations both in PXE and 
FH loci.
MeSH 
reference 
indexing set
Adult
Aged
Female
Humans
Japan
Multidrug Resistance-Associated Proteins/*genetics
*Mutation, Missense
Pedigree
Pseudoxanthoma Elasticum/*genetics
189
Appendix B: Sample indexing sets obtained from the GRC and PRC algorithms for 
a sample citation
PubMed ID 15645653
GRC indexing 
set* (top 15 terms)
Humans (10)
Multidrug Resistance-Associated Proteins (9)
Mutation (8)
Male (7)
Female (7)
Multidrug Resistance-Associated Proteins/genetics (7)
Pseudoxanthoma Elasticum (6)
Pseudoxanthoma Elasticum/genetics (6)
Pedigree (5)
Exons (4)
DNA Mutational Analysis (4)
Mutation/genetics (4)
Adult (4)
Introns (3)
Aged (3)
PRC indexing 
set* (top 15 terms)
Multidrug Resistance-Associated Proteins (10)
Multidrug Resistance-Associated Proteins /genetics (10)
Pseudoxanthoma Elasticum (10)
Pseudoxanthoma Elasticum/genetics (10)
Mutation (7)
DNA Mutational Analysis (6)
Pedigree (5)
Genotype (4)
Polymorphism, Genetic (4)
Alleles (4)
Mutation/genetics (3)
Haplotypes (3)
Models, Genetic (3)
Gene Deletion (3)
Exons (3)
                                                          
* Terms appearing in the reference set are underlined; the number of neighbors ? out of the 10 nearest neighbors ?
labeled with each term is shown between brackets after the term.
190
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 88?89,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Automatic inference of indexing rules for MEDLINE
Aure?lie Ne?ve?ol and Sonya E. Shooshan
National Library of Medicine
8600 Rockville Pike
Bethesda, MD 20894, USA
{neveola,sonya}@nlm.nih.gov
Vincent Claveau
IRISA - CNRS
Campus de Beaulieu
35042 Rennes, France
Vincent.Claveau@irisa.fr
Abstract
This paper describes the use and customiza-
tion of Inductive Logic Programming (ILP) to
infer indexing rules from MEDLINE citations.
Preliminary results suggest this method may
enhance the subheading attachment module of
the Medical Text Indexer, a system for assist-
ing MEDLINE indexers.
1 Introduction
Indexing is a crucial step in any information retrieval
system. In MEDLINE?, a widely used database of the
biomedical literature, the indexing process involves
the selection of Medical Subject Headings (MeSH?)
in order to describe the subject matter of articles.
The need for automatic tools to assist human in-
dexers in this task is growing with the increasing
number of publications in MEDLINE. The Medical
Text Indexer (MTI) (Aronson et al, 2004) has been
available at the U.S. National Library of Medicine
(NLM) since 2002 to provide indexers with MeSH
main heading recommendations (e.g. Aphasia, Pa-
tient Care. . . ) when they create MEDLINE citations.
This paper describes a method to enhance MTI with
the capacity to attach appropriate MeSH subhead-
ings (e.g. metabolism, pharmacology) to these main
headings in order to provide MeSH pair recommen-
dations (e.g. aphasia/metabolism), which are more
specific and therefore a significant asset to NLM in-
dexers.
Subheading attachment can be accomplished us-
ing indexing rules such as:
If a main heading from the "Anatomy"
tree and a "Carboxylic Acids" term are
recommended for indexing, then the pair
"[Carboxylic Acids]/pharmacology" should
also be recommended.
Sets of manual rules developed for a few subhead-
ings show good precision but low recall. The devel-
opment of new rules is a complex, time-consuming
task. We investigate a novel approach adapting In-
ductive Logic Programming (ILP) to the context
of MEDLINE, which requires efficient processing of
large amounts of data.
2 Use of Inductive Logic Programming
ILP is a supervised machine learning technique used
to infer rules that are expressed with logical clauses
(Prolog clauses) based on a set of examples also rep-
resented using Prolog. A comprehensive descrip-
tion of ILP can be found in (Muggleton and Raedt,
1994). We selected this method because it is able to
provide simple representations for relational prob-
lems and produces rules that can be easily inter-
preted. One caveat to the use of ILP is the complex-
ity of rule inference from large sets of positive and
negative examples. Considering each of the 24,000
MeSH main headings independently would not be
computationally feasible. For this reason, based on
work by Buntine (1988) we introduce a new defini-
tion of subsumption that allows us to go through the
set of examples efficiently by exploiting hierarchical
relationships between main headings. This type of
subsumption is in fact suitable for any rule inference
problem involving structured knowledge encoded by
ontologies.
88
Subheading Method Nb. rules Precision (%) Recall(%) F-measure(%)
Overall ILP 587 47 32 38
Manual 69 59 10 18
Baseline - 32 11 16
Table 1: Performance on the test corpus using MTI main heading recommendations
3 Experiments
ILP rules were induced using a training corpus of
100,000 citations randomly chosen from MEDLINE
2006. Another corpus of 100,000 MEDLINE 2006 ci-
tations was used for testing. ILP rules were applied
on the test corpus using main headings automatically
retrieved by MTI as triggers. The performance of
ILP was compared to manual rules and a baseline
consisting of randomly formed pairs according to
their distribution in MEDLINE prior to 2006. Overall
results obtained on 4 subheadings are presented in
Table 1.
4 Discussion
Performance. As expected, the use of MTI to pro-
duce main heading recommendations used as trig-
gers for the rules results in comparable precision
but lower recall compared to the theoretical assess-
ment. In spite of this, the performance obtained by
ILP rules is superior to the baseline and shows the
best F-measure. The precision obtained by the man-
ual rules, when they exist, is higher, but they pro-
duce a recall inferior to ILP and even to the baseline
method.
ILP vs. manual rules. A detailed analysis of the
rules obtained shows that not all ILP rules are easily
understood by indexers. This is due to some unex-
pected regularities which do not seem to be relevant
but nonetheless achieved good results on the training
data used to infer rules.
Furthermore, we noticed that while most rules
typically contain a ?trigger term? (e.g. Anatomy
in our previous example) and a ?target term? (e.g.
Carboxylic Acids above), in some ILP rules the tar-
get term can also serve as the trigger term. Some
changes in the ILP inferring process are foreseen in
order to prevent the production of such rules.
Rule filtering vs. manual review. Preliminary ex-
periments with producing ILP rules suggested that
improvement could be achieved by 1/ filtering out
rules that showed a comparatively low precision on
the training corpus when applied to main headings
retrieved by MTI; and 2/ by having an indexing ex-
pert review the rules to improve their readability. On
most subheadings, filtering had little impact but gen-
erally tended to improve precision while F-measure
stayed the same, which was our goal. The manual
review of the rules seemed to degrade the perfor-
mance obtained with the original ILP.
5 Conclusion and perspectives
We have shown that ILP is an adequate method for
automatically inferring indexing rules for MEDLINE.
Further work will be necessary in order to obtain
rules for all 83 MeSH subheadings. Subsequently,
the combination of ILP rules with other subheading
attachment methods will be assessed. We anticipate
that the rule sets we have obtained will be integrated
into MTI?s subheading attachment module.
Acknowledgments
This study was supported in part by the Intramural Re-
search Programs of NIH, NLM. A. Ne?ve?ol was supported
by an appointment to the NLM Research Participation
Program administered by ORISE through an inter-agency
agreement between the U.S. Department of Energy and
NLM.
References
Alan R. Aronson, James G. Mork, Clifford W. Gay, Su-
sanne M. Humphrey, and Willie J. Rogers. 2004. The
NLM Indexing Initiative?s Medical Text Indexer. In
Proceedings of Medinfo 2004, San Francisco, Califor-
nia, USA.
Wray L. Buntine. 1988. Generalized Subsumption and
its Application to Induction and Redundancy. Artifi-
cial Intelligence, 36:375?399.
Stephen Muggleton and Luc De Raedt. 1994. Inductive
logic programming: Theory and methods. Journal of
Logic Programming, 19/20:629?679.
89
Proceedings of the Workshop on BioNLP, pages 144?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Two Biomedical Text Genres for Disease Recognition 
 
 
Aur?lie N?v?ol, Won Kim, W. John Wilbur, Zhiyong Lu* 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.gov 
 
  
 
 
Abstract 
In the framework of contextual information 
retrieval in the biomedical domain, this paper 
reports on the automatic detection of disease 
concepts in two genres of biomedical text: 
sentences from the literature and PubMed user 
queries. A statistical model and a Natural 
Language Processing algorithm for disease 
recognition were applied on both corpora. 
While both methods show good performance 
(F=77% vs. F=76%) on the sentence corpus, 
results on the query corpus indicate that the 
statistical model is more robust (F=74% vs. 
F=70%).  
1 Introduction 
Contextual Information Retrieval (IR) is making 
use of additional information or assumptions about 
the users? needs beyond the obvious intent of the 
query. IR systems need to go beyond the task of 
providing generally relevant information by assist-
ing users in finding information that is relevant to 
them and their specific needs at the time of the 
search. A practical example of a Google contextual 
IR feature is when the search engine returns a map 
showing restaurant locations to a user entering a 
query such as ?Paris restaurants.? 
The contextual aspects of a user?s search were 
defined for example by Saracevic (1997) who dis-
cussed integrating the cognitive, affective, and sit-
uational levels of human computer interaction in 
IR systems. Other research efforts studied users? 
search behavior based on their level of domain 
knowledge (Zhang et al, 2005) or aimed at  mod-
eling users? interests and search habits (Rose and 
Levinson, 2004; Teevan et al, 2005).  
Information about the search context may be 
sought explicitly from the user through profiling or 
relevance feedback (Shen et al, 2005). Recent 
work also exploited query log analysis and basic 
computer environment information (Wen et al 
2004), which involve no explicit interaction with 
the user. In adaptive information retrieval, context 
information is inferred based on query analysis and 
collection characteristics (Bai and Nie 2008).  
In the biomedical domain, a need for contextual 
information retrieval was identified in particular 
for clinical queries submitted to PubMed (Pratt and 
Wasserman, 2000). Building on the idea that a spe-
cific type of document is required for searches with 
a ?clinical? context, the PubMed Clinical Queries 
portal was developed (Haynes and Wilczynski, 
2004). A perhaps more prominent contextual fea-
ture of PubMed is the ?citation sensor?, which 
identifies queries classified by Rose and Levinson 
as reflecting a ?Navigational? or ?Obtain resource? 
goal. For example, the citation sensor will identify 
and retrieve a specific citation if the user enters the 
article title as the query. The analysis of Entrez 
logs shows that MEDLINE is the most popular 
database among the 30 or so databases maintained 
by the National Center for Biotechnology Informa-
tion (NCBI) as it receives most of Entrez traffic. 
This suggests that there is a need to complement 
the information retrieved from MEDLINE by giv-
ing contextual access to other NCBI resources re-
144
levant to users? queries, such as Entrez Gene, Clin-
ical Q&A or BookShelf. In addition, the NLM es-
timated that about 1/3 of PubMed users are not 
biomedical professionals. In this light, providing 
an access point to consumer information such as 
the Genetics Home Reference might also be useful. 
To achieve this, the sensor project was recently 
launched with the goal of recognizing a variety of 
biomedical concepts (e.g. gene, protein and drug 
names) in PubMed queries. These high-level con-
cepts will help characterize users? search context in 
order to provide them with information related to 
their need beyond PubMed. For instance, if a user 
query contains the drug name ?Lipitor?, it will be 
recognized by the drug sensor and additional in-
formation on this drug from Clinical Q&A will be 
shown in the side bar in addition to default 
PubMed results. Since disease names are common 
in PubMed queries, the goal of this work is to in-
vestigate and benchmark computational techniques 
for automatic disease name recognition as an aid to 
implementing PubMed search contexts. 
2 Related Work 
Despite a significant body of literature in biomedi-
cal named entity recognition, most work has been 
focused on gene, protein, drug and chemical names 
through challenges such as BioCreAtIvE1 or the 
TREC Genomics/Chemical tracks (Park and Kim, 
2006). Other work addressed the identification of 
?medical problems? in clinical text (Aronson et al 
2007; Meystre and Haug, 2005). This task was the 
topic of a Medical NLP challenge2, which released 
a corpus of anonymized radiography reports anno-
tated with ICD9 codes. Although there is some 
interest in the biomedical community in the identi-
fication of disease names and more specifically the 
identification of relationships between diseases and 
genes or proteins (Rindflesh and Fizman, 2003), 
there are very few resources available to train or 
evaluate automatic disease recognition systems. To 
the best of our knowledge, the only publicly avail-
able corpus for disease identification in the litera-
ture was developed by Jimeno et al (2008). The 
authors annotated 551 MEDLINE sentences with 
UMLS concepts and used this dataset to bench-
mark three different automatic methods for disease 
name recognition. A MEDLINE corpus annotated 
                                                        
1 http://biocreative.sourceforge.net/ 
2 http://www.computationalmedicine.org/challenge/index.php 
with ?malignancy? mentions and part-of-speech 
tags is also available (Jin et al 2006). This corpus 
is targeted to a very restricted type of diseases. The 
annotations are also domain specific, so that ?can-
cer of the lung? is not considered a malignancy 
mention but a mention of malignancy and a men-
tion of malignancy location. 
As in previous studies, we aim to investigate the 
complexity of automatic disease recognition using 
state-of-the-art computational techniques. This 
work is novel in at least three aspects: first, in ad-
dition to using the MEDLINE sentence corpus 
(Jimeno et al2008), we developed a new corpus 
comprising disease annotations on 500 randomly 
selected PubMed queries. This allowed us to inves-
tigate the influence of local context3 through the 
comparison of system performance between two 
different genres of biomedical text. Second, by 
using a knowledge based tool previously ben-
chmarked on the same MEDLINE corpus (Jimeno 
et al 2008), we show that significant performance 
differences can be observed when parameters are 
adjusted. Finally, a state-of-the-art statistical ap-
proach was adapted for disease name recognition 
and evaluated on both corpora.  
3 Two Biomedical Corpora with disease 
annotations 
The first issue in the development of such a corpus 
is to define the very concept of disease. Among the 
numerous terminological resources available, such 
as Medical Subject Headings (MeSH?, 4,354 dis-
ease concepts) or the International Classification of 
Diseases (ICD9, ~18,000 disease concepts), the 
UMLS Metathesaurus? is the most comprehensive: 
the 2008AB release includes 252,284 concepts in 
the disorder Semantic Group defined by McCray 
et al (2001). The UMLS Metathesaurus is part of 
the Semantic Network, which also includes a set of 
broad subject categories, or Semantic Types, that 
provide a consistent categorization of all concepts 
represented in the Metathesaurus. The Semantic 
Groups aim at providing an even broader categori-
zation for UMLS concepts. For example, the dis-
order Semantic Group comprises 12 Semantic 
Types including Disease or Syndrome, Cell or Mo-
lecular Dysfunction and Congenital Abnormalities.  
                                                        
3 Here, by context, we mean the information surrounding a 
disease mention available in the corpora. This is different from 
the ?search context? previously discussed.   
145
Furthermore, like the gene mention (Morgan et 
al. 2008) and gene normalization (Smith et al 
2008) tasks in BioCreative II, the task of disease 
name recognition can also be performed at two 
different levels: 
 
1. disease mention: the detection of a snippet 
of text that refers to a disease concept (e.g. 
?alzheimer? in the sample query shown in 
Table 2)  
2. disease concept: the recognition of a con-
trolled vocabulary disease concept (e.g. 
?C0002395-alzheimer?s disease? in our Ta-
ble 2 example) in text.  
 
In this work, we evaluate and report system per-
formance at the concept level. 
3.1 Biomedical literature corpus 
Sentence Kniest dysplasia is a moderately 
severe chondrodysplasia pheno-
type that results from mutations 
in the gene for type ii collagen 
col2a1.  
Annotations C0265279-Kniest dysplasia 
C0343284-Chondrodysplasia, 
unspecified 
Table 1: Excerpt of literature corpus (PMID: 7874117) 
 
The corpus made available by Jimeno et al con-
sists of 551 MEDLINE sentences annotated with 
UMLS concepts or concept clusters: concepts that 
were found to be linked to the same term. For ex-
ample, the concepts ?Pancreatic carcinoma? 
(C0235974) and ?Malignant neoplasm of pan-
creas? (C0346647) share the same synonym ?Pan-
creas Cancer?, thus they were clustered. The 
sentences were selected from a set of articles cu-
rated for Online Mendelian Inheritance in Man 
(OMIM) and contain an average of 27(+/- 11) to-
kens, where tokens are defined as sequences of 
characters separated by white space. A set of 
UMLS concepts (or clusters) is associated with 
each sentence in the corpus. However, no boun-
dary information linking a phrase in a sentence to 
an annotation was available. Table 1 shows a sam-
ple sentence and its annotations. 
 
 
 
3.2 Biomedical query corpus 
A total of 500 PubMed queries were randomly se-
lected and divided into two batches of 300 and 200 
queries, respectively. Queries were on average 
3.45(+/- 2.64) tokens long in the 300 query batch 
and 3.58(+/- 4.63) for the 200 query batch, which 
is consistent with the average length of PubMed 
queries (3 tokens) reported by Herskovic et al 
(2007).  
The queries in the first set were annotated using 
Knowtator (Ogren, 2006) by three annotators with 
different backgrounds (one biologist, one informa-
tion scientist, one computational linguist). Two 
annotators annotated the queries using UMLS con-
cepts from the disorder group, while the other an-
notator simply annotated diseases without 
reference to UMLS concepts. Table 2 shows a 
sample query and its annotations. A consensus set 
was obtained after a meeting between the annota-
tors where diverging annotations were discussed 
and annotators agreed on a final, unique, version of 
all annotations.  The consensus set contains 89 dis-
ease concepts (76 unique). 
 
Query alzheimer csf amyloid 
Annotations  Ann. 1: ?alzheimer?; 0-8;  
Ann. 2, 3: ?alzheimer?; 0-8; 
C0002395-alzheimer?s disease 
Table 2: Excerpt of annotated 300-query corpus. Boun-
dary information is given as the character interval of the 
annotated string in the query (here, 0-8). 
 
The queries in the second set were annotated 
with UMLS concepts from the disorder group by 
one of the annotators who also worked on the pre-
vious set. In this set, 53 disease concepts were an-
notated (51 unique). 
4 Automatic disease recognition 
With the perspective of a contextual IR applica-
tion where the disease concepts found in queries 
will be used to refer users to disease-specific in-
formation in databases other than MEDLINE, we 
are concerned with high precision performance. 
For this reason, we decided to experiment with 
methods that showed the highest precision when 
compared to others. In addition, given the size of 
the corpora available and the type of the annota-
146
tions, machine learning methods such as CRFs or 
SVM did not seem applicable.  
Table 3 shows a description of the training and 
test sets for each corpus. 
 
 Table 3: Description of the training and test sets 
4.1 Natural Language Processing 
Disease recognition was performed using the Natu-
ral Language Processing algorithm implemented in 
MetaMap (Aronson, 2001)4. The tool was re-
stricted to retrieve concepts from the disorder 
group, using the UMLS 2008AB release and 
?longest match? feature. 
In practice, MetaMap parses the input text into 
noun phrases, generates variants of these phrases 
using knowledge sources such as the SPECIALIST 
lexicon, and maps the phrases to UMLS concepts.  
4.2 Priority Model 
The priority model was first introduced in (Tanabe 
and Wilbur, 2006) and is adapted here to detect 
disease mentions in free text. Because our evalua-
tion is performed at the concept level, the mentions 
extracted by the model are then mapped to UMLS 
using MetaMap.  
The priority model approach is based on two sets 
of phrases: one names of diseases, D, and one 
names of non-diseases, N. One trains the model to 
assign two numbers, p and q, to each token t that 
appears in a phrase in either D or N. Roughly, p is 
the probability that a phrase from D or N that has 
the token t in it is actually from D and q is the rela-
tive weight that should be assigned to t for this 
purpose and represents a quality estimate. Given a 
phrase 
                                                        
4 Additional information is also available at 
http://metamap.nlm.nih.gov/ 
 
1 2 kph t t t?
    (1) 
and for each it  the corresponding numbers ip  and 
iq  we estimate the probability that ph D  by 
 
1 22 11 1k kkj i i jij j iprob p q q p q  
(2) 
 
The training procedure for the model actually 
chooses the values of all the p and q quantities to 
optimize the 
prob
 values over all of D and N.  
For this work we have extended the approach to 
include a quantity  
21 1 22 11 1k kkj i i jij j iqual q p q q p q prob
(3) 
 
which represents a weighted average of all the 
quality numbers iq . We apply this formula to ob-
tain 
qual
as long as 
0.5.prob
 If 
0.5prob
we 
replace all numbers 
ip  by 1 ip  in (2) and (3) to 
obtain 
qual
.  
For this application we obtained the sets D and 
N from the SEMCAT data (Tanabe, Thom et al 
2006) supplemented with the latest UMLS data. 
We removed any term from D and N that contained 
less than five characters in order to decrease the 
occurrence of ambiguous terms.  Also the 1,000 
most frequent terms from D were examined ma-
nually and the ambiguous ones were removed. The 
end result is a set of 332,984 phrases in D and 
4,253,758 phrases in N. We trained the priority 
model on D and N and applied the resulting train-
ing to compute for each phrase in D and N a vector 
of values 
,prob qual
. In this way D and N are 
converted to 
DV  and NV . We then constructed a 
Mahalanobis classifier (Duda, Hart and Stork, 
2001) for two dimensional vectors as the differ-
ence in the Mahalanobis distance of any such vec-
tor to Gaussian approximations to 
DV  and NV .  We 
refer to the number produced by this classifier as 
the Mahalanobis score.  By randomly dividing both 
D and N into three equal size pieces and training 
on two from each and testing on the third, in a 
three-fold cross validation we found the Mahala-
nobis classifier to perform at 98.4% average preci-
sion and 93.9% precision-recall breakeven point. 
In a final step we applied a simple regression me-
thod to estimate the probability that a given Maha-
Data Lit. Corpus Query Corpus 
Training 276 sentences 
(487 disease con-
cepts, 185 unique) 
300 queries (89 
disease concepts, 
76 unique) 
Testing 275 sentences 
(437 disease con-
cepts, 185 unique) 
200 queries (53 
disease concepts, 
51 unique) 
All 551 sentences 
(924 disease con-
cepts, 280 unique) 
500 queries (142 
disease concepts, 
120 unique) 
147
lanobis score was produced by a phrase belonging 
to D and not N. Given a phrase phr we will denote 
this final probability produced as PMA(phr).  
The second important ingredient of our statistic-
al process is how we produce phrases from a piece 
of text. Given a string of text TX we apply tokeni-
zation to TX to produce an ordered set of tokens 
1 2, , , nt t t?
. Among the tokens produced will be 
punctuation marks and stop words and we denote 
the set of all such tokens by Z . We call a token 
segment 
, ,j kt t?
 maximal if it contains no ele-
ment of Z  and if either 1j  or 
1jt Z
 and 
likewise if k n  or 
1kt Z
. Given text TX we 
will denote the set of all maximal token segments 
produced in this way by 
max ( ).S TX
 Now given a 
maximal token segment mts=
, ,j kt t?
 we define 
two different methods of finding phrases in mts. 
The first assumes we are given an arbitrary set of 
phrases PH.  We recursively define a set of phrases 
,I mts PH
 beginning with this set empty and 
with the parameter 
u j
.  Each iteration consists 
of asking for the largest v k  for which 
, ,u vt t PH?
. If there is such a v  we add 
, ,u vt t?
 to 
,I mts PH
 and set 
1u v
. 
Otherwise we set 
1u u
. We repeat this process 
as long as u k .  The second approach assumes 
we are given an arbitrary set of two token phrases 
P2.  Again we recursively define a set of phrases 
, 2J mts P
 beginning with this set empty and 
with the parameter 
u j
. Each iteration consists 
of asking for the largest v k  for which given any 
,  i u i v
,  
1, 2i it t P
. If there is such a v  
we add 
, ,u vt t?
 to 
, 2J mts P
 and set 
1u v
. Otherwise we set 
1u u
. We repeat 
this process as long as u k .   
In order to apply our phrase extraction proce-
dures we need good sets of phrases. In addition to 
D and N already defined above, we use another set 
of phrases defined as follows. Let R denote the set 
of all token strings with two or more tokens which 
do not contain tokens from Z and for which there 
are at least three MEDLINE records (title and ab-
stract text only) in which the token string is re-
peated at least twice. 
We then define
R R D N
. We make 
use of R  in addition to D and N. For the set 2P  
we take the set of all two token phrases in 
MEDLINE documents for which the two tokens 
co-occur as this phrase much more than expected, 
i.e., with a 
2 10,000
(based on the two-by-two 
contingency table).  
 
 
#Initialization: Given a text TX, set 
maxS S TX
 and .X  
#Processing: While(S ){ 
  I. select mts S  
  II. If( ,I mts D ) ,K I mts D 
       else if( ,I mts R ) ,K I mts R  
        else if( ,I mts N ) K  
        else 
if( , 2J mts P ) , 2K J mts P 
        else K  
  III. X X K  
  IV. S S mts  
     } 
#Return: All pairs , ,  phr PMA phr phr X 
 
Figure 1: Phrase finding algorithm 
 
With these preliminaries, our phrase finding al-
gorithm in pseudo-code is shown in Figure 1. 
The output of this algorithm may then be filtered 
by setting a threshold on the PMA values to accept. 
5 Results  
5.1 Assessing the difficulty of the task 
To assess the difficulty of disease recognition, we 
computed the inter-annotator agreement (IAA) on 
the 300-query corpus. Agreement was computed at 
the disease mention level for all three annotators 
and at the disease concept level for the two annota-
tors who produced UMLS annotations.  
Inter-annotator agreement measures for NLP 
applications have been recently discussed by 
Artstein and Poesio (2008) who advocate for the 
use of chance corrected measures. However, in our 
case, agreement was partly computed on a very 
large set of categories (UMLS concepts) so we 
decided to use Knowtator?s built-in feature, which 
computes IAA as the percentage of agreement and 
148
allows partial string matches. For example, in the 
query ?dog model transient ischemic attacks?, an-
notator 1 selected ?ischemic attacks? as a disorder 
while annotator 2 and 3 selected ?transient ischem-
ic attacks? as UMLS concept C0007787: Attacks, 
Transient Ischemic. In this case, at the subclass 
level (?disorder?) we have a match for this annota-
tion. But at the exact span or exact category level, 
there is no match. Table 4 shows details of IAA at 
the disease mention level when partial matches are 
taken into account. For exact span matches, the 
IAA is lower, at 64.87% on average. 
 
Disorder IAA Ann. 1 Ann. 2 Ann. 3 
Ann. 1 100% 71.77% 75.86% 
Ann. 2  100% 71.68% 
Ann. 3   100% 
Table 4: Agreement on disease mention annotations 
(partial match allowed) ? average is 73.10% 
 
At the concept level, the agreement (when par-
tial matches were allowed) varied significantly 
depending on the semantic types. It ranged be-
tween 33% for Findings and 83% for Mental or 
Behavioral Dysfunction. However, agreement on 
the most frequent category, Disease or Syndrome, 
was 72%, which is close to the annotators? overall 
agreement at the mention level. One major cause 
of disagreement was ambiguity caused by concepts 
that were clustered by Jimeno et al For example, 
in query ?osteoporosis and ?fracture pattern?, an-
notator 2 marked ?osteoporosis? with both 
?C0029456-osteoporosis?(a Disease or Syndrome 
concept) and ?C1962963-osteoporosis adverse 
event?(a Finding concept) while annotator 3 only 
used ?C0029456-osteoporosis?.    
5.2 Results on Literature corpus 
As shown in Table 3, the corpus was randomly 
split into a training set (276 sentences) and a test 
set (275 sentences). The training set was used to 
determine the optimal probability threshold for the 
Priority Model and parameter selection for Meta-
Map, respectively. 
 
Priority Model parameter adjustments: the first 
result observed from applying the Priority Model 
was that D yielded about 90% of the output of the 
algorithm. Also results coming from R  and 2P  
were not well mapped to UMLS concepts by Me-
taMap. As a result, in this work we ignored disease 
candidates retrieved based on R  and 2P . The best 
F-measure was obtained for a threshold of 0.3, 
which was consequently used on the test set.  
Since the Priority Model algorithm does not per-
form any mapping to a controlled vocabulary 
source, the mapping was performed by applying 
MetaMap to the snippets of text returned with a 
probability value above the threshold. 
 
Threshold P R F 
0 64 73 67 
.1 67 73 70 
.2 67 73 70 
.3 68 73 71 
.4 68 73 70 
.5 68 72 69 
.6 68 72 69 
.7 68 72 69 
.8 68 68 68 
.9 65 60 62 
Table 5: Precision (P), Recall (R) and F-measure of the 
Priority Model on the training set for different values of 
the probability threshold. 
 
The results presented in Table 5 were obtained 
before any MetaMap adjustments were made.  
 
MetaMap parameter adjustments: an error anal-
ysis was performed to adjust MetaMap settings. 
Errors fell into the following categories:  
 A more specific disease should have been 
recognized (e.g. ?deficiency? vs. ?C2 defi-
ciency?) 
 The definition of a cluster was lacking 
(e.g. ?G6PD deficiency? comprised 
C0237987- Glucose-6-phosphate dehydro-
genase deficiency anemia and C0017758- 
Glucosphosphate Dehydrogenase Defi-
ciency but not C0017920- Deficiency of 
glucose-6-phosphatase)  
 MetaMap mapping was erroneous (e.g. 
?hereditary breast? was mapped to 
C0729233-Dissecting aneurysm of the 
thoracic aorta instead of C0346153-
Hereditary Breast Cancer)  
 
The results of inter-annotator agreement and fur-
ther study of MetaMap mappings indicated that 
concepts with the semantic type Findings seemed 
149
to be frequently retrieved erroneously. For this rea-
son, we also experimented not taking Findings into 
account as an additional adjustment for MetaMap. 
Table 6 shows the results of applying the MetaMap 
adjustments yielded from the error analysis on the 
training corpus. 
 
Threshold Findings P R F 
.3 Yes 80 78 79 
.3 No 85 78 81 
Table 6: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments?      
 
MetaMap disorder detection was also performed 
directly on the training corpus. An error analysis 
similar to what was presented above was carried 
out to determine the best parameters. Table 7 be-
low shows the results obtained when all concepts 
from the 12 Semantic Types (STs) in the disorder 
group are taken into account with no adjustments 
(?raw?). Then, results including the adjustments 
from the error analysis are shown when all 12 STs 
are taken into account, when Findings are excluded 
(11STs) and when only the most frequent 6STs in 
the training set are taken into account.    
 
Processing P R F 
Raw (12 STs) 50 77 61 
Adjusted (12 STs) 52 75 61 
Adjusted (11 STs) 57 73 64 
Adjusted (6 STs) 77 72 74 
Table 7: Performance of MetaMap on the training set      
 
Finally, Table 8 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 80 74 77 
MetaMap 75 78 76 
Table 8: Precision (P), Recall (R) and F-measure of the 
Priority Model and MetaMap on the test set     
5.3 Results on Query Corpus 
The 300-query corpus was used as a training set 
and the 200-query corpus was used as a test set. 
For consistency with work on the literature corpus, 
we assessed the disease recognition on a gold stan-
dard set including ?clusters? of UMLS concepts 
were appropriate. As previously with the Literature 
corpus, we used the training set to determine the 
best settings for each method. The performance of 
the Priority Model at different values of the proba-
bility threshold, based on the use of D and N as the 
sets of sample phrases is similar to that obtained 
with the literature corpus; 0.3 stands out as one of 
the three values for which the best F-measure is 
obtained (tied with .5 and .8).  
Because of the brevity of queries vs. sentences, 
the MetaMap error analysis was very succinct and 
resulted in:  
 Removal of C0011860-Diabetes mellitus 
type 2  as mapping for ?diabetes? 
 Removal of all occurrences of C0600688-
Toxicity and C0424653-Weight symptom 
(finding)  
 Adjustment on the number of STs taken in-
to account 
 
The difference in performance obtained on the 
training set for the different MetaMap adjustments 
considered is shown in Table 9 when MetaMap 
was applied to Priority Model output and in Table 
10 when it was applied directly on the queries.    
 
Threshold Findings P R F 
.3 Yes 60 72 65 
.3 No 73 70 71 
Table 9: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments? 
 
Processing P R F 
Raw (12 STs) 41 82 55 
Adjusted (12 STs) 44 82 57 
Adjusted (11 STs) 58 81 68 
Adjusted (6 STs) 64 75 69 
Table 10: performance of MetaMap on the training set 
 
Finally, Table 11 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 76 72 74 
MetaMap 66 74 70 
Table 11: Precision (P), Recall (R) and F-measure of 
the Priority Model and MetaMap on the test set 
150
6 Discussion 
Comparing the Two Methods. The performance 
of both methods on the query corpus is comparable 
to inter-annotator agreement (F=70-74 vs. IAA=72 
on Disease and Syndromes). On both corpora, the 
Priority Model achieves higher precision and F-
measure, while MetaMap achieves better recall.  
Comparing the results obtained with MetaMap 
with those reported by Jimeno et al, precision is 
lower, but recall is much higher. This is likely to 
be due to the different MetaMap settings, and the 
use of different UMLS versions - Jimeno et al did 
not provide any of this information, but based on 
the publication date of their paper, it is likely that 
they used one of the 2006 UMLS releases. Meystre 
and Haug (2006) also found that significant per-
formance differences could be obtained with Me-
taMap by adjusting the content of the knowledge 
sources used.   
On both text genres, 0.3 was found to be the op-
timal probability threshold for the Priority Model. 
Based on the performance at different values of the 
threshold, it seems that the model is quite efficient 
at ruling out highly unlikely diseases. However, for 
values above .3 the performance does not vary 
greatly.  
 
Comparing Text Genres. For both methods, 
disease recognition seems more efficient on sen-
tences. This is to be expected: sentences provide 
more context (e.g. more tokens surrounding the 
disease mention are available) and allow for more 
efficient disambiguation, for example on acro-
nyms. Although acronyms are frequent both in 
queries and sentences, more undefined acronyms 
are found in queries. However, the difference in 
performance between the two methods seems 
higher on the query corpus. This indicates that the 
Priority Model could be more robust to sparse con-
text.  
It should be noted that there were diseases in all 
sentences in the literature corpus vs. about 1/3 to 
1/2 of the queries. In addition, the query corpus 
included many author names, which could create 
confusion with disease names (in particular for the 
Priority Model). This difficulty was not found in 
the sentence corpus. However, sentences some-
times contain negated mention of diseases, which 
never occurred in the query corpus where little to 
no syntax is used.  
We also noticed that while Findings seemed to 
be generally problematic concepts in both corpora, 
other concepts such as Injury and Poisoning were 
much more prevalent in the query corpus. For this 
reason, for the general task of disease recognition, 
a drastic restriction to as little as 6 STs is probably 
not advisable.  
 
Limitations of the study. One limitation of our 
study is the relatively small number of disease 
concepts in the query corpus. Although the query 
and sentence corpus contain about 500 que-
ries/sentences each, there are significantly less dis-
ease concepts found in queries compared to 
sentences. As a result, there is also less repetition 
in the disease concept found. This is partly due to 
the brevity of queries compared to sentences but 
mainly to the fact that while all the sentences in the 
literature corpus had at least one disease concept, 
this was not the case for the query corpus. We are 
currently addressing this issue with the ongoing 
development of a large scale query corpus anno-
tated for diseases and other relevant biomedical 
entities.  
7 Conclusions 
We found that of the two steps of disease recogni-
tion, disease mention gets the higher inter-
annotator agreement (vs. concept mapping). We 
have applied a statistical and an NLP method for 
the automatic recognition of disease concepts in 
two genres of biomedical text. While both methods 
show good performance (F=77% vs. F=76%) on 
the sentence corpus, results indicate that the statis-
tical model is more robust on the query corpus 
where very little disease context information is 
available (F=74% vs. F=70%). As a result, the 
priority model will be used for disease detection in 
PubMed queries in order to characterize users? 
search contexts for contextual IR. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The authors would like to thank S. 
Shooshan and T. Tao for their contribution to the 
annotation of the query corpus; colleagues in the 
NCBI engineering branch for their valuable feed-
back at every step of the project.   
151
References  
Alan R. Aronson, Olivier Bodenreider, Dina Demner-
Fushman, Kin Wah Fung, Vivan E. Lee, James G. 
Mork et al 2007. From Indexing the Biomedical Li-
terature to Coding Clinical Text: Experience with 
MTI and Machine Learning Approaches. ACL 
Workshop BioNLP.  
Alan Aronson. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proceedings of AMIA Symp:17-21. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics 34(4): 555-596 
Jing Bai, and Jian-Yun Nie. 2008. Adapting information 
retrieval to query contexts. Information Processing & 
Management. 44(6):1902-22 
Robert O. Duda, Peter. E. Hart and David G. Stork. 
2001. Pattern Classification. New York: John Wiley 
& Sons, Inc. 
R. Brian Haynes and Nancy L. Wilczynski. 2004. Op-
timal search strategies for retrieving scientifically 
strong studies of diagnosis from Medline: analytical 
survey. BMJ. 328(7447):1040. 
Jorge R. Herskovic, Len Y. Tanaka, William Hersh and 
Elmer V. Bernstam. 2007. A day in the life of 
PubMed: analysis of a typical day's query log. Jour-
nal of the American Medical Informatics Association. 
14(2):212-20. 
Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee, 
Sylvain Gaudan, Rafael Berlanga and Dietrich 
Rebholz-Schuhmann. 2008. Assessment of disease 
named entity recognition on a corpus of annotated 
sentences. BMC Bioinformatics. 11;9 Suppl 3:S3. 
Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A 
Mandel, Steven Carroll, Mark Y Liberman et al 
2006. Automated recognition of malignancy men-
tions in biomedical literature. BMC Bioinformatics.  
7:492. 
Alexa T. McCray, Anita Burgun and Olivier Bodenreid-
er. 2001. Aggregating UMLS semantic types for 
reducing conceptual complexity. Proceedings of 
Medinfo 10(Pt 1):216-20. 
St?phane Meystre and Peter J. Haug. 2006. Natural lan-
guage processing to extract medical problems from 
electronic clinical documents: performance evalua-
tion. J Biomed Inform. 39(6):589-99. 
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, 
Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al 
2008. Overview of BioCreative II gene normaliza-
tion. Genome Biol. 9 Suppl 2:S3. 
Phillip V. Ogren. 2006. Knowtator: A plug-in for creat-
ing training and evaluation data sets for Biomedical 
Natural Language systems. 9th Intl. Prot?g? Confe-
rence  
Jong C. Park and Jung-Jae Kim. 2006. Named Entity 
Recognition. In S. Ananiadou and J. McNaught 
(Eds), Text Mining for Biology and Biomedicine (pp. 
121-42). Boston|London:Artech House Inc.  
Wanda Pratt and Henry Wasserman. 2000. QueryCat: 
automatic categorization of MEDLINE queries. Pro-
ceedings of AMIA Symp:655-9.  
Tom C. Rindflesh and Marcelo Fiszman. 2003. The 
interaction of domain knowledge and linguistic struc-
ture in natural language processing: interpreting 
hypernymic propositions in biomedical text. J Bio-
med Inform. 36(6):462-77 
Daniel E. Rose and Danny Levinson. 2004. Understand-
ing user goals in web search. In Proceedings of the 
13th international Conference on World Wide 
Web:13-9  
Tefko Saracevic. 1997. The Stratified Model of Infor-
mation Retrieval Interaction: Extension and Applica-
tion. Proceedings of the 60th meeting of the. 
American Society for Information Science:313-27 
Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005 
Context-sensitive information retrieval using impli-
cit feedback, In Proceedings of the 28th annual in-
ternational conference ACM SIGIR conference on 
Research and development in information retrieval: 
43-50.  
Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-
Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al 2008. 
Overview of BioCreative II gene mention recogni-
tion. Genome Biol. 9 Suppl 2:S2. 
Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten, 
Donald C. Comeau and W. John Wilbur. 2006. 
SemCat: semantically categorized entities for ge-
nomics. Proceedings of AMIA Symp: 754-8. 
Laurraine K. Tanabe and W. John Wilbur. 2006. A 
Priority Model for Named Entities. Proceedings of 
HLT-NAACL BioNLP Workshop:33-40 
Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005. 
Personalizing search via automated analysis of in-
terests and activities. In Proceeding of ACM-
SIGIR?05:449?56. 
Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilis-
tic model for contextual retrieval. Proceedings of 
ACM-SIGIR?04:57?63 
Xiangmin Zhang, Hermina G.B. Anghelescu and Xiao-
jun Yuan. 2005. Domain knowledge, search beha-
vior, and search effectiveness of engineering and 
science students: An exploratory study, Information 
Research 10(2): 217. 
152
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 103?104,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic extraction of data deposition sentences:  
where do the research results go? 
Aur?lie N?v?ol, W. John Wilbur, Zhiyong Lu 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{Aurelie.Neveol,John.Wilbur,zhiyong.lu}@nih.gov 
  
Abstract 
Research in the biomedical domain can have a 
major impact through open sharing of data 
produced. In this study, we use machine learn-
ing for the automatic identification of data 
deposition sentences in research articles. Arti-
cles containing deposition sentences are cor-
rectly identified with 73% f-measure. These 
results show the potential impact of our meth-
od for literature curation.  
1 Background 
Research in the biomedical domain aims at further-
ing the knowledge of biological processes and im-
proving human health. Major contributions 
towards this goal can be achieved by sharing the 
results of research efforts with the community, in-
cluding datasets produced in the course of the re-
search work. While such sharing behavior is 
encouraged by funding agencies and scientific 
journals, recent work has shown that the ratio of 
data sharing is still modest compared to actual data 
production. For instance, Ochsner et al (2008) 
found the deposition rate of microarray data to be 
less than 50% for work published in 2007.  
Information about the declaration of data depo-
sition in research papers can be used both for data 
curation and for the analysis of emerging research 
trends. Our long-term research interest is in as-
sessing the value of deposition sentences for pre-
dicting future trends of data production. The initial 
step of automatically identifying deposition sen-
tences would then lead to an assessment of the 
need for storage space of incoming data in public 
repositories. 
2 Objective 
In this study, we aim at automatically perform-
ing a fine-grained identification of biological data 
deposition sentences in biomedical text. That is, 
we aim at identifying articles containing deposition 
sentences, extracting the specific sentences and 
characterizing the information contained in the 
sentences in terms of data type and deposition lo-
cation (e.g. database, accession numbers).  
3 Material and Methods 
Data deposition sentences . A collection of sen-
tences reporting the deposition of biological data 
(such as microarray data, protein structure, gene 
sequences) in public repositories was compiled 
based on previous work that we extended. We take 
these sentences as a primary method of identifying 
articles reporting on research that produced the 
kind of data deposited in public repositories. (1) 
and (2) show examples of such sentences. In con-
trast, (3) and (4) contain elements related to data 
deposition while focusing on other topics.   
(1) The sequences reported in this paper have been 
deposited in the GenBank database (acces sion 
numbers AF034483 for susceptible strain RC688s 
and AF034484 for resistant strain HD198r). 
(2) The microarray data were submitted to MIAMEx-
press at the EMBL-EBI. 
(3) Histone TAG Arrays are a repurposing of a micro-
array design originally created to represent the 
TAG sequences in the Yeast Knockout collection 
(Yuan et al2005 NCBI GEO Accession Number 
GPL1444). 
(4) The primary sequence of native Acinetobacter 
CMO is identical to the gene sequence for chnB 
deposited under accession number AB006902. 
103
Sentence classification. A Support Vector Ma-
chine (SVM) classifier was built using a corpus of 
583 positive data deposition sentences and 578 
other negative sentences. Several sets of features 
were tested, including the following: sentence to-
kens, associated part-of-speech tags obtained using 
MEDPOST1, relative position of the sentence in 
the article, identification of elements related to data 
deposition (data, deposition action, database, ac-
cession number) obtained using a CRF model2.   
Article classification. The automatic classification 
of articles relied on sentence analysis. The full text 
of articles was segmented into sentences, which 
were then scored by the sentence-level SVM clas-
sifier described above. An article is classified as 
positive if its top-scored sentence is scored higher 
than a threshold, which is predetermined as the 25th 
percentile score for positive sentences in the train-
ing set.  
Evaluation corpus . A corpus composed of 670 
PubMed Central articles was used to evaluate arti-
cle classification. 200 articles were considered as 
?positive? for data deposition based on MEDLINE 
gold standard annotations in the [si] field used to 
curate newly reported accession numbers.  
4 Results  
Table 1 shows the performance of selected SVM 
models for article classification on the test set. 
While differences were very small for cross-
validation on the training set, they are emphasized 
on the test set.   
 
Features P         R           F 
Tokens, position, part-of-
speech tags 
52%      56%     54% 
Token, position, CRF+, 
part-of-speech tags  
65%      58%     62% 
Tokens, position, CRF+/-, 
part-of-speech tags 
69%     78%     73% 
Table 1: Precision, Recall and F-measure of SVM 
models for article classification on test set. 
5 Discussion and Conclusion 
Portability of the method. Although trained 
mainly on microarray data deposition sentences, 
the method adapts well to the identification of oth-
                                                                 
1 http://www.ncbi.nlm.nih.gov/staff/lsmith/MedPost.html 
2 http://mallet.cs.umass.edu/ 
er data deposition sentences, e.g. gene sequences, 
protein coordinates.  
Comparison to other work. Our approach is not 
directly comparable to any of the previous studies. 
At the article level, we perform an automatic clas-
sification of articles containing data deposition 
sentences, in contrast with Oshner et al who per-
formed a one-time manual classification. Piwowar 
et alused machine learning and rule-based algo-
rithms for article classification. However, they re-
lied on identifying the names of five predetermined 
databases in the full text of articles. Our approach 
is generic and aiming at the automatic identifica-
tion of any biological data deposition in any public 
repository. Furthermore, our approach also re-
trieves specific data deposition sentences where 
data and deposition location are identified. At the 
sentence level, this is also different from the classi-
fication of databank accession number sentences 
performed by Kim et al (2010) in two ways: first, 
we focus on retrieving sentences containing acces-
sion numbers if they are deposition sentences (vs. 
data re-use, etc.) and second, we are also interested 
in retrieving data deposition sentences that do not 
contain accession numbers.  
Error analysis . Almost half of the articles clas-
sified as containing a deposition sentence by our 
method but not by the gold standard were found to 
indeed contain a deposition sentence.  
Conclusion. These results show the potential 
impact of our method for literature curation. In 
addition, it provides a robust tool for future work 
assessing the need for storage space of incoming 
data in public repositories. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, NLM.  
References  
Jongwoo Kim, Daniel Le, Georges R. Thoma. Na?ve 
bayes and SVM classifiers for classifying databank 
accession number sentences from online biomedical 
articles. Proc. SPIE 2010 (7534): 7534OU-OU8 
Scott A. Ochsner, Davd L Steffen, Christian J Stoeckert 
Jr, Neil J. McKenna. Much room for improvement 
in deposition rates of expression microarray da-
tasets. Nat Methods. 2008 Dec;5(12):991.  
Heather A. Piwowar, Wendy W. Chapman. Identifying 
data sharing in biomedical literature.AMIA Annu 
Symp Proc. 2008 Nov 6:596-600. 
104
LAW VIII - The 8th Linguistic Annotation Workshop, pages 54?58,
Dublin, Ireland, August 23-24 2014.
Optimizing annotation efforts to build reliable annotated corpora
for training statistical models
Cyril Grouin
1
Thomas Lavergne
1,2
Aur
?
elie N
?
ev
?
eol
1
1
LIMSI?CNRS, 91405 Orsay, France
2
Universit?e Paris Sud 11, 91400 Orsay, France
firstname.lastname@limsi.fr
Abstract
Creating high-quality manual annotations on text corpus is time-consuming and often requires the
work of experts. In order to explore methods for optimizing annotation efforts, we study three key
time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and
(iii) careful annotations. Through a series of experiments using a corpus of clinical documents
annotated for personally identifiable information written in French, we address each of these
aspects and draw conclusions on how to make the most of an annotation effort.
1 Introduction
Statistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP)
over the past decades. These methods sucessfully address NLP tasks such as part-of-speech tagging or
named entity recognition by relying on large annotated text corpora. As a result, developping high-
quality annotated corpora representing natural language phenomena that can be processed by statistical
tools has become a major challenge for the scientific community. Several aspects of the annotation task
have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA)
has been used as an indicator of annotation quality. Early work showed that the use of automatic pre-
annotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation
guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006).
Efforts have investigated methods to reduce the human workload while annotating corpora. In par-
ticular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most
benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double an-
notation and found that double annotation could be limited to carefully selected portions of a corpus.
They produced an algorithm that automatically selects portions of a corpus for double annotation. Their
approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and
maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic
pre-annotations was shown to increase annotation consistency and result in producing quality annotation
with a time gain over annotating raw data (Fort and Sagot, 2010; N?ev?eol et al., 2011; Rosset et al., 2013).
With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there
are ethic aspects to consider in addition to technical and monetary cost when using a microworking plat-
form for annotation. While selecting the adequate methods for computing IAA is important (Artstein
and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to
all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate
annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality
manual annotations is time-consuming and often requires the work of experts. The time burden is dis-
tributed between the sheer creation of the annotations, the act of producing multiple annotations for the
same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation of a
consensus. Research has addressed methods for reducing the time burden associated to these annotation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
54
activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the time
burden of annotation creation) with the final goal of producing the highest quality of annotations.
In contrast, our hypothesis in this work is that annotations are being developed for the purpose of train-
ing a machine learning model. Therefore, our experiments consist in training a named entity recognizer
on a training set comprising annotations of varying quality to study the impact of training annotation
quality on model performance. In order to explore methods for optimizing annotation efforts for the de-
velopment of training corpora, we revisit the three key time burdens of the annotation process on textual
corpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations. Through a
series of experiments using a corpus of French clinical documents annotated for personally identifiable
information (PHI), we address each of these aspects and draw conclusions on how to make the most of
an annotation effort.
2 Material and methods
2.1 Annotated corpus
Experiments were conducted with a corpus of clinical documents in French annotated for 10 categories of
PHI. The distribution of the categories over the corpus varies with some categories being more prevalent
than others. In addition, the performance of entity recognition for each type of PHI also varies (Grouin
and N?ev?eol, 2014). The datasets were split to obtain a training corpus (200 documents) and a test
corpus (100 documents). For all documents in the training corpus, three types of human annotations
are available: annotations performed independently by two human annotators and consensus annotations
obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement
on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).
The distribution of annotations over all PHI categories on both corpora (train/dev) is: address
(188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76),
last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217).
2.2 Automatic Annotation Methods
We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline
automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models
on the various sets of annotations available for the training corpus.
Features set For each CRF experiment, we used the following set of features with a l1 regularization:
? Lexical features: unigram and bigram of tokens;
? Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the
token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first
name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the
token is a trigger word for specific categories (hospital, last name) ;
? Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger
tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker
based upon the previouses POS tags;
? External features: (i) we created 320 classes of tokens using Liang?s implementation (Liang, 2005)
of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the
document (begining, middle, end).
Design of experiments The models were built to assess three annotation time-saving strategies:
1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system,
(ii) AR?H2=intersection of automatic annotations from the rule-based system with annotations
from annotator 2. This model captures a situation where the human annotator would quickly revise
the automatic annotations by removing errors: some annotations would be missing (average recall),
but the annotations present in the set would be correct (very high precision), (iii) ARH2=automatic
annotations from the rule-based system, with replacement of the three most difficult categories by
55
annotations from annotator 2. This model captures a situation where the human annotator would
focus on revising targeted categories, and (iv) ARHC=automatic annotations from the rule-based
system, with replacement of the three most difficult categories by consensus annotations;
2. Double annotation: (i) H1=annotations from annotator 1, (ii) H2=annotations from annotator 2,
(iii) H12=first half of the annotations from annotator 1, second half from annotator 2, and
(iv) H21=first half of the annotations from annotator 2, second half from annotator 1;
3. Consensus annotation: (i) H1?H2=all annotations from annotator 1 and 2 (concatenation without
adjudication), and (ii) HC=consensus annotations (after adjudication between annotator 1 and 2).
3 Results
Table 1 presents an overview of the global performance of each annotation run (H12 and H21 achieved
similar results) across all PHI categories in terms of precision, recall and F
1
-measure (Manning and
Sch?utze, 2000). Table 2 presents the detailed performance of each annotation run for individual PHI
categories in terms of F-measure.
Baseline AR AR?H2 ARH2* ARHC H1* H12 H1?H2 H2 HC
Precision .820 .868 .920 .942 .943 .959 .962 .969 .974 .974
Recall .806 .796 .763 .854 .854 .927 .934 .935 .936 .942
F-measure .813 .830 .834 .896 .896 .943 .948 .951 .955 .958
Table 1: Overall performance for all automatic PHI detection. A star indicates statistically significant
difference in F-measure over the previous model (Wilcoxon test, p<0.05)
Category Baseline AR AR?H2 ARH2 ARHC H1 H12 H1?H2 H2 HC
Address .648 .560 .000 .800 .800 .716 .744 .789 .795 .791
Zip code .950 .958 .947 .964 .958 .974 .984 .974 .984 .990
Date .958 .968 .962 .963 .967 .965 .963 .963 .959 .970
E-mail .937 .927 .927 .927 .927 1.000 1.000 1.000 1.000 1.000
Hospital .201 .248 .039 .856 .868 .789 .809 .856 .861 .867
Identifier .000 .000 .000 .762 .797 .870 .892 .823 .836 .876
Last name .816 .810 .834 .832 .828 .953 .957 .954 .961 .963
First name .849 .858 .900 .901 .902 .960 .956 .961 .965 .960
Telephone 1.000 .980 .978 .983 .980 .987 .994 .999 .999 1.000
City .869 .874 .883 .887 .887 .948 .972 .962 .965 .972
Table 2: Performance per PHI category (F-measure)
4 Discussion
4.1 Model performance
Overall, the task of automatic PHI recognition has been well studied and the rule-based tool provides a
strong baseline with .813 F-measure on the test set. Table 1 shows that there are three different types
of models, in terms of performance: the lower-performing category corresponds to models with no hu-
man input. The next category corresponds to models with some human input, and the higher-performing
models correspond to models with the most human input. This reflects the expectation that model per-
formance increases with training corpus quality. However, it also shows that, within the two categories
that include human input, there is no statistical difference in model performance with respect to the type
of human input. We observed that the model trained on annotations from the H2 human annotator per-
formed better (F=0.955) than the model trained on annotations from the H1 annotator (F=0.943). This
observation reflects the agreement of the annotators with consensus annotations, where H2 had higher
agreement than H1 (Grouin and N?ev?eol, 2014). This is also true at the category level: H2 achieved
56
higher agreement with the consensus compared to H1 on categories ?address? (F=0.985>0.767) and
?hospital? (F=0.947>0.806) but H2 had lower agreement with the consensus on the category ?identifier?
(F=0.840<0.933).
4.2 Error Analysis
The performance of CRF models depends on the size of the training corpus and the level of diversity of
the mentions. Error analysis on our test data shows that a few specific mentions are not tagged in the test
corpus, even though they occur in the training corpus. For example, some hospital names occur in the
clinical narratives either as acronyms or as full forms (e.g. ?GWH? for ?George Washington Hospital?
in transfer patient from GWH). The acronyms are overall much less prevalent than the full forms and
also happen to be difficult to identify for human annotators (depending on the context, a given acronym
could refer to either a medical procedure, a physician or a hospital). We observed that the only hospital
acronym present in the test corpus was not annotated by any of the CRF models. Nevertheless, only five
occurrences of this acronym were found in the training corpus which is not enough for the CRF to learn.
Other errors occur in recognizing sequences of doctors? names that appear without separators in sig-
natures lines at the end of documents (e.g. ?Jane BROWN John DOE Mary SMITH?). In our test set
we observed that models trained on automatic annotations correctly predicted the beginning of such se-
quences and then produced erroneous predictions for the rest of the sequence (models AR, AR?H2,
ARHC and ARH2). In contrast, models built on human annotations produced correct predictions on the
entire sequence (models H1, H12, H1?H2, H2 and HC). Similarly, for last names containing a nobiliary
particle, the models trained on automatic annotations only identified part of the last name as a PHI. We
also observed that spelling errors (e.g. ?Jhn DOE?) only resulted in correct predictions from the mod-
els trained on the human annotations. We did not find cases where the models built on the automatic
annotations performed better than the models built on the human annotations.
4.3 Annotation strategy
Table 1 indicates that for the purpose of training a machine learning entity recognizer, all types of human
input are equivalent. In practice, this means that double annotations or consensus annotations are not
necessary. The high inter-annotator agreement on our dataset may be a contributing factor for this finding.
Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towards
the annotation style of the annotator who produced the training data. Therefore, we believe that inter-
annotator should be established on a small dataset before annotators work independently. Table 2 shows
that using human annotations for selected categories results in strong improvement of the performance
over these categories (?address?, ?hospital? and ?identifier? categories in ARHC and ARH2 vs. AR) with
little impact on the performance of the model on other categories. Therefore, careful human annotations
are not necessarily needed for the entire corpus. Targeting ?hard? categories for human annotations can
be a good time-saving strategy. While the difference between the models using some human input vs.
all human input is statistically significant, the performance gain is lower than between models without
human input and some human input. Using data with partial human input for training statistical models
can cut annotation cost.
5 Conclusion and future work
Herein we have shown that full double annotation of a corpus is not necessary for the purpose of training a
competitive CRF-based model. Our results suggest that a three-step annotation strategy can optimize the
annotation effort: (i) double annotate a small subset of the corpus to ensure human annotators understand
the guidelines; (ii) have annotators work independently on different sections of the corpus to obtain wide
coverage; and (iii) train a machine-learning based model on the human annotations and apply this model
on a new dataset.
In future work, we plan to re-iterate these experiments on a different type of entity recognition task
where inter-annotator agreement may be more difficult to achieve, and may vary more between categories
in order to investigate the influence of inter-annotator-agreement on our conclusions.
57
Acknowledgements
This work was supported by the French National Agency for Research under grant CABeRneT
1
ANR-
13-JS02-0009-01 and by the French National Agency for Medicines and Health Products Safety under
grant Vigi4MED
2
ANSM-2013-S-060.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?79.
Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proc of Linguistic
Annotation Workshop (LAW), pages 65?73. Association for Computational Linguistics.
Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2013. An enhanced CRFs-based system for informa-
tion extraction from radiology reports. J Biomed Inform, 46(3):425?35, Jun.
Kar?en Fort and Beno??t Sagot. 2010. Influence of pre-annotation on POS-tagged corpus development. In Proc of
Linguistic Annotation Workshop (LAW), pages 56?63. Association for Computational Linguistics.
Kar?en Fort, Gilles Adda, and Kevin Bretonnel Cohen. 2011. Amazon Mechanical Turk: Gold Mine or Coal Mine?
Computational Linguistics, pages 413?420.
Cyril Grouin and Aur?elie N?ev?eol. 2014. De-identification of clinical notes in french: towards a protocol for
reference corpus development. J Biomed Inform.
Cyril Grouin. 2013. Anonymisation de documents cliniques : performances et limites des m?ethodes symboliques
et par apprentissage statistique. Ph.D. thesis, University Pierre et Marie Curie, Paris, France.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Chiristopher D. Manning and Hinrich Sch?utze. 2000. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, Massachusetts.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn TreeBank. Computational Linguistics, 19(2):313?330.
Aur?elie N?ev?eol, Rezarta Islamaj Do?gan, and Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed
queries: a study on quality, efficiency, satisfaction. J Biomed Inform, 44(2):310?8.
Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J?er?emy Leixa, Olivier Galibert, and Pierre
Zweigenbaum. 2013. Automatic named entity pre-annotation for out-of-domain human annotation. In Proc of
Linguistic Annotation Workshop (LAW), pages 168?177. Association for Computational Linguistics.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur. 2009. How to get the most out of your curation effort. PLoS
Comput Biol, 5(5):e1000391.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc of International Confer-
ence on New Methods in Language.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proc of the
NIPS Workshop on Cost-Sensitive Learning.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation:
definitions, guidelines and corpus construction. BMC Bioinformatics, 25(7):356.
1
CABeRneT: Compr?ehension Automatique de Textes Biom?edicaux pour la Recherche Translationnelle
2
Vigi4MED: Vigilance dans les forums sur les M?edicaments
58
