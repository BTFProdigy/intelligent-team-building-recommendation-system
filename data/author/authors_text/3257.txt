Rapid Parser Development: 
A Machine Learning Approach for Korean 
Ul f  Hermjakob  
USC In format ion  Sc iences Ins t i tu te  
4676 Admira l ty  Way #1000 ? Mar ina  del Rey,  CA  90292 ? USA 
u l f@cs .utexas .edu  
Abst ract  
This paper demonstrates that machine learning is 
a suitable approach for rapid parser development. 
From 1000 newly treebanked Korean sentences we 
generate a deterministic shift-reduce parser. The 
quality of the treebank, particularly crucial given its 
small size, is supported by a consistency checker. 
1 In t roduct ion  
Given the enormous complexity of natural anguage, 
parsing is hard enough as it is, but often unforeseen 
events like the crises in Bosnia or East-Timor create 
a sudden demand for parsers and machine transla- 
tion systems for languages that have not benefited 
from major attention of the computational linguis- 
tics community up to that point. 
Good machine translation relies strongly on the 
context of the words to be translated, a context hat 
often goes well beyond neighboring surface words. 
Often basic relationships, like that between a verb 
and its direct object, provide crucial support for 
translation. Such relationships are usually provided 
by parsers. 
The NLP resources for a language of sudden inter- 
national interest are typically quite limited. There is 
probably a dictionary, but most likely no treebank. 
Maybe basic tools for morphological analysis, but 
probably no semantic ontology. 
This paper reports on the rapid development of 
a parser based on very limited resources. We show 
that by building a small treebank of only a thousand 
sentences, we could develop a good basic parser us- 
ing machine learning within only three months. For 
the language we chose, Korean, a number of research 
groups have been working on parsing and/or ma- 
chine translation in recent years (Yoon, 1997; Seo, 
1998; Lee, 1997), but advanced resources have not 
been made publicly available, and we have not used 
any, thereby so-to-speak at least simulating a low 
density language scenario. 
2 Korean  
Like Japanese, Korean is a head-final agglutinative 
language. It is written in a phonetic alphabet called 
hangul, in which each two-byte character represents 
one syllable. While our parser operates on the orig- 
inal Korean hangul, this paper presents examples 
in a romanized transcription. In sentence (1) for 
example, the verb is preceded by a number of so- 
called eojeols (equivalent to bunsetsus in Japanese) 
like "chaeg-eul", which are typically composed of a 
content part ("chaeg" = book) and a postposition, 
which often corresponds to a preposition in English, 
but is also used as a marker of topic, subject or ob- 
ject ("eul"). 
,_ ,_ _ I-~ ?\];gl 7 
Na-neun eo-je geu chaeg-eul sass-da. 
ITOPIC yesterday this bookoBJ bought. (1) 
I bought this book yesterday. 
Our parser produces a tree describing the structure 
of a given sentence, including syntactic and semantic 
roles, as well as additional information such as tense. 
For example, the parse tree for sentence (1) is shown 
below: 
\[1\] na-netm eo-je geu chaeg-eul sass-da. \[S\] 
(SUB J) \[2\] na-neun \[NP\] 
(HEAD) \[3\] na  \[KEG-NOUN\] 
(PARTICLE) \[4\] neun \[DUPLICATE-PRT\] 
(TIME) \[5\] eo-je \[REG-ADVERB\] 
(HEAD) \[6\] eo-je \[REG-ADVERB\] 
(OBJ) \[7\] geu chaeg-eul \[NP\] 
(MOD) \[8\] geu \[DEMONSTR-ADNOMINAL\] 
(HEAD) \[9\] geu \[DEMONSTR-ADNOMINAL\] 
(HEAD) \[I0\] chaeg-eul \[NP\] 
(HEAD) \[II\] chae E \[KEG-NOUN\] 
(PARTICLE) \[12\] eul \[OBJ-CASE-PRT\] 
(HEAD) \[13\] sass-da. \[VERB; PAST-TENSE\] 
(HEAD) \[14\] sa \[VERB-STEM\] 
(SUFFIX) \[15\] eoss \[INTEEMED-SUF-VERB\] 
(SUFFIX) \[16\] da \[CONNECTIVE-SUF-VERB\] 
(DUMMY) \[17\] . \[PERIOD\] 
Figure 1: Parse tree for sentence 1 (simplified) 
For preprocessing, we use a segmenter and mor- 
phological analyzer, KMA, and a tagger, KTAG, 
both provided by the research group of Prof. Rim of 
118 
Korea University. KMA, which comes with a built- 
in Korean lexicon, segments Korean text into eojeols 
and provides a set of possible sub-segmentations and 
morphological analyses. KTAG then tries to select 
the most likely such interpretation. Our parser is 
initialized with the result of KMA, preserving all 
interpretations, but marking KTAG's  choice as the 
top alternative. 
3 T reebank ing  E f fo r t  
The additional resources used to train and test a 
parser for Korean, which we will describe in more 
detail in the next section, were (1) a 1187 sentence 
treebank, (2) a set of 133 context features, and (3) 
background knowledge in form of an 'is-a' ontology 
with about 1000 entries. These resources were built 
by a team consisting of the principal researcher and 
two graduate students, each contributing about 3 
months. 
3.1 T reebank  
The treebank sentences are taken from the Korean 
newspaper Chosun, two-thirds from 1994 and the re- 
mainder from 1999. Sentences represent continuous 
articles with no sentences kipped for length or any 
other reason. The average sentence length is 21.0 
words. 
3.2  Feature  Set  
The feature set describes the context of a partially 
parsed state, including syntactic features like the 
part of speech of the constituent at the front/top 
of the input list (as sketched in figure 2) or whether 
the second constituent on the parse stack ends in a 
comma, as well as semantic features like whether or 
not a constituent is a time expression or contains 
a location particle. The feature set can accommo- 
date any type of feature as long as it is computable, 
and can thus easily integrate different ypes of back- 
ground knowledge. 
3.3 Background Knowledge  
The features are supported by background knowl- 
edge in the form of an ontology, which for example 
has a time-particle concept with nine sub-concepts 
(accounting for 9 of the 1000 entries mentioned 
above). Most of the background knowledge groups 
concepts like particles, suffixes, units (e.g. for lengths 
or currencies), temporal adverbs - semantic lasses 
that are not covered by part of speech information 
of the lexicon, yet provide valuable clues for parsing. 
3.4 T ime Ef fort  
The first graduate student, a native Korean and 
linguistics major, hired for 11 weeks, spent about 
2 weeks getting trained, 6 weeks on building two- 
thirds of the treebank, 2 weeks providing most back- 
ground knowledge entries and 1 week helping to 
< parse stack 
-3 -2 
~ "bought" 
synt: verb 
top of 
stack 
-1 
(R 2 TO S-VP AS PRED OBJ) 
front/top of 
list 
<:input list> 
1 
i 
, "today" I 
synt: adv I 
"reduce the 2 top elements of the parse stack 
to a frame with syntax 'vp' 
and roles 'pred' and 'obj'" 
"bought" 
synt: verb 
"bought abook" 
synt: vp 
sub: (pred) (obj) 
"today" 
synt: adv 
Figure 2: A typical parse action (simplified). 
Boxes represent frames. The asterisk (*) represents he 
current parse position. Optionally, parse actions can 
have additional arguments, like target syntactic or se- 
mantic classes to overwrite any default. Elements on the 
input list are identified by positive integers, elements on 
the parse stack by negative integers. The feature 'Synt of 
-1' for example refers to the (main) syntactic ategory of 
the top stack element. Before the reduce operation, the 
feature 'Synt of-1' would evaluate to np (for "a book"), 
after the operation to vp (for "bought a book"). The in- 
put list is initialized with the morphologically analyzed 
words, possibly still ambiguous. After a sequence of shift 
(from input list to parse stack) and reduce (on the parse 
stack) operations, the parser eventually ends up with a 
single element on the parse stack, which is then returned 
as the parse tree. 
identify useful features. The other graduate student, 
a native Korean and computer science major, in- 
stalled Korean tools including a terminal for hangul 
and the above mentioned KMA and KTAG, wrote a 
number of scripts tying all tools together, made some 
tool improvements, built one-third of the treebank 
119 
and also contributed to the feature set. The prin- 
cipal researcher, who does not speak Korean, con- 
tributed about 3 person months, coordinating the 
project, training the graduate students, writing tree- 
bank consistency checking rules (see section 6), mak- 
ing extensions to the tree-to-parse-action-sequence 
module (see section 4.1) and contributing to the 
background knowledge and feature set. 
4 Learn ing  to  Parse  
We base our training on the machine learning based 
approach of (Hermjakob k: Mooney, 1997), allow- 
ing however unrestricted text and deriving the parse 
action sequences required for training from a tree- 
bank. The basic mechanism for parsing text into 
a shallow semantic representation is a shift-reduce 
type parser (Marcus, 1980) that breaks parsing into 
an ordered sequence of small and manageable parse 
actions. Figure 2 shows a typical reduce action. The 
key task of machine learning then is to learn to pre- 
dict which parse action to perform next. 
Two key advantages of this type of deterministic 
parsing are that its linear run-time complexity with 
respect to sentence length makes the parser very 
fast, and that the parser is very robust in that it 
produces a parse tree for every input sentence. 
Figure 3 shows the overall architecture of parser 
training. From the treebank, we first automatically 
generate a parse action sequence. Then, for every 
step in the parse action sequence, typically several 
dozens per sentence, we automatically compute the 
value for every feature in the feature set, add on the 
parse action as the proper classification of the parse 
action example, and then feed these examples into a 
machine learning program, for which we use an ex- 
tension of decision trees (Quinlan, 1986; Hermjakob 
& Mooney, 1997). 
We built our parser incrementally. Starting with a 
small set of syntactic features that are useful across 
all languages, early training and testing runs reveal 
machine learning conflict sets and parsing errors that 
point to additionally required features and possibly 
also additional background knowledge. A conflict 
set is a set of training examples that have identical 
values for all features, yet differ in their classification 
(= parse action). Machine learning can therefore not 
possibly learn how to handle all examples correctly. 
This is typically resolved by adding an additional 
feature that differentiates between the examples in 
a linguistically relevant way. 
Even treebanking benefits from an incremental p- 
proach. Trained on more and more sentences, and 
at the same time with also more and more features, 
parser quality improves, so that the parser as a tree- 
banking tool has to be corrected less and less fre- 
quently, thereby accelerating the treebanking pro- 
cess. 
Knowledge Base ("ontology") 
? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
temporal-concept 
~- the-year :  
i day-of-the-week ' 
Monday ... Sunday 
syntactic-element 
verb noun adverb 
count-noun mass-noun 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
I 
i 
Feature set: ', l Svnt Svnt of-2 of- 1 S~n~ 
Treebank 
computer science 
~ parse action sequence generator (automatic) 
Parse action sequence: 
Shift noun 
Shift noun 
Reduce 2 as mod head 
Done 
~ parse example generator (automatic) 
Parse action examples: 
\[Unavail Unavail Noun \[ Shift noun \[ 
\[ Unavaii Noun Noun \[ Shift noun I 
\[Noun Noun Unavail I Reduce 2 as mod head I 
\[Unavaii Noun Unavail \ [Done I 
decision structure builder (automatic) 
Parse decision structure: 
Synt of 1 N~ai l  
Shi~t noun / /~nt  of-2 
Done Reduce 2 as rood head 
Figure 3: Derivation of the parser from a treebank 
and a feature set. The resulting parser has the form 
of a decision structure, an extension of decision trees. 
Given a seen or unseen sentence in form of a list 
of words, the decision structure keeps selecting the 
next parse action until a single parse tree covering 
the entire sentence has been built. 
120 
word level constituent 
labeled precision 
+ i /SUFFIX-NOUN + I /OBJ-CASE-PRT 
+ i /NUMERAL + I /OBJ-CASE-PRT 
+ i l /UNIT-NOUN 
+ i l /REGULAR-NOUN 
86.0%- 
The analyzer divides '31i1' into groups with varying 
number of sub-components with different parts of 
speech. When shifting in an element, the parser has 
to decide which one to pick, the third one in this 
case, using context of course. 
The module generating parse action sequences 
from a tree needs special split and merge operations 
for cases where the correct segmentation is not of- 
fered as a choice at all. To make things a little ugly, 
these splits can not only occur in the middle of a leaf 
constituent, but even in the middle of a character 
that might have been contracted from two charac- 
ters, each with its own meaning. 
5 Chosun Newspaper  Exper iments  
Table 1 presents evaluation results with the number 
of training sentences varying from 32 to 1024 and 
with the remaining 163 sentences of the treebank 
used for testing. 
Precision: 
number of correct constituents in system parse 
number of constituents in system parse 
Recal l :  
number of correct constituents in system parse 
number of constituents in logged parse 
Cross ing  brackets :  number of constituents 
which violate constituent boundaries with a con- 
stituent in the logged parse. Labe led  preci- 
sion/recall measures not only structural correctness, 
but also the correctness of the syntactic label. Cor -  
rect  operat ions  measures the number of correct 
operations during a parse that is continuously cor- 
rected based on the logged sequence; it measures 
the core machine learning algorithm performance in
isolation. A sentence has a correct operat ing  se- 
quence,  if the system fully predicts the logged parse 
action sequence, and a correct s t ruc ture  and  la- 
be l ing,  if the structure and syntactic labeling of the 
final system parse of a sentence is 100% correct, re- 
gardless of the operations leading to it. 
Figures 4 and 5 plot the learning curves for two 
key metrics. While both curves are clearly heading 
z KMA actually produces 10 different alternatives in this 
case, of which only four are shown here. 
87.0%- 
85.0%- 
84.0%- 
I t I I I i 
32 64 128 256 512 1024 
number of training sentences 
2.1 
2.0 
1.9 
1.8 
1.7 
1.6 
1.5 
31/NUMERAL 
31/NUMERAL 
31/NUMERAL 
31/NUMERAL 
Figure 4: Learning curve for labeled precision corre- 
sponding to table 1 
crossings brackets per sentence 
4.1 Spec ia l  Adaptat ion  for  Korean  
The segmenter and morphological nalyzer KMA re- 
turns a list of alternatives for each eojeol. However, 
the alternatives are not atomic but rather two-level 
constituents, or mini-trees. Consider for example 
the following four  1 alternatives for the eojeol '31il' 
(the 31st day of a month): 
32 64 128 256 512 1024 
number of training sentences 
Figure 5: Learning curve for crossing brackets per 
sentence corresponding to table 1 
in the right direction, up for precision, and down 
for crossing brackets, their appearance is somewhat 
jagged. For smaller data sets like in our case, this 
can often be avoided by running an n-fold cross val- 
idation test. However, we decided not to do so, 
because many training sentences were also used for 
feature set and background knowledge development 
121 
Training sentences 32 64 128 256 512 1024 
Precision 
Recall 
Labeled precision 
Labeled recall 
Tagging accuracy 
Crossings/sentence 
0 crossings 
< 1 crossing 
< 2 crossings 
< 3 crossings 
< 4 crossings 
Correct operations 
Operation Sequence 
Structure&Label 
88.6% 
87.3% 
84.1% 
81.2% 
94.3% 
1.97 
27.6% 
56.4% 
70.6% 
81.0% 
88.3% 
63.0% 
2.5% 
5.5% 
88.1% 
87.4% 
83.9% 
81.9% 
92.9% 
2.00 
35.0% 
58.9% 
72.4% 
81.6% 
84.0% 
68.3% 
6.1% 
12.9% 
90.0% 
89.2% 
85.8% 
83.6% 
93.9% 
1.72 
38.7% 
63.2% 
73.0% 
82.2% 
91.4% 
71.5% 
8.O% 
11.7% 
89.6% 
89.1% 
85.6% 
83.6% 
93.4% 
1.79 
40.5% 
59.5% 
71.8% 
81.6% 
89.0% 
73.4% 
8.6% 
16.o% 
90.7% 
89.6% 
86.7% 
84.7% 
94.0% 
1.69 
43.6% 
64.4% 
73.0% 
82.2% 
90.8% 
75.0% 
11.0% 
19.0% 
91 .O% 
89.8% 
86.9% 
85.O% 
94.2% 
1.63 
42.9% 
62.6% 
74.2% 
83.4% 
89.6% 
76.3% 
7.4% 
16.0% 
Table 1: Evaluation results with varying number of training sentences 
as well as for intermediate inspection, and therefore 
might have unduly influenced the evaluation. 
5.1 Tagging accuracy 
A particularly striking number is the tagging accu- 
racy, 94.2%, which is dramatically below the equiv- 
alent 98% to 99% range for a good English or 
Japanese parser. In a Korean sentence, only larger 
constituents hat typically span several words are 
separated by spaces, and even then not consistently, 
so that segmentation errors are a major source for 
tagging problems (as it is to some degree however 
also for Japanese2). We found that the segmen- 
tation part of KMA sometimes still struggles with 
relatively simple issues like punctuation, proposing 
for example words that contain a parenthesis in the 
middle of standard alphabetic haracters. We have 
corrected some of these problems by pre- and post- 
processing the results of KMA, but believe that there 
is still a significant potential for further improve- 
ment. 
In order to assess the impact of the relatively low 
tagging accuracy, we conducted experiments that 
simulated a perfect agger by initializing the parser 
with the correctly segmented, morphologically ana- 
lyzed and tagged sentence according to the treebank. 
By construction, the tagging accuracy in table 2 
rises to 100%. Since the segmenter/tagger r turns 
not just atomic but rather two-level constituents, 
the precision and recall values benefit particularly 
strongly, possibly inflating the improvements for 
these metrics, but other metrics like crossing brack- 
ets per sentence show substantial gains as well. Thus 
we believe that refined pre-parsing tools, as they are 
2Whi le  Japanese does not  use spaces at all, script changes 
between kanji, hiragana, and katakana provide a lot of seg- 
mentat ion  guidance.  Modern Korean,  however, a lmost  exclu- 
sively uses only a single phonet ic  script.  
Segmentation/ Regular Simulating 
Tagging seg/tag as perfect 
( "seg/tag" ) implemented seg/tag 
Labeled precision 
Labeled recall 
Tagging accuracy 
Crossings/sentence 
0 crossings 
< 2 crossings 
Structure&Label 
86.9% 
85 .O% 
94.2% 
1.63 
42.9% 
74.2% 
16.0% 
93.4% 
92.9% 
100.0% 
1.13 
48.5% 
85.3% 
28.8% 
Table 2: Impact of segmentation/tagging errors 
in the process of becoming available for Korean, will 
greatly improve parsing accuracy. 
However, for true low density languages, uch high 
quality preprocessors are probably not available so 
that our experimental scenario might be more re- 
alistic for those conditions. On the other hand, 
some low density languages like for example Tetun, 
the principal indigenous language of East Timor, 
are based on the Latin alphabet, separate words by 
spaces and have relatively little inflection, and there- 
fore make morphological nalysis and segmentation 
relatively simple. 
6 T reebank  Cons is tency  Check ing  
It is difficult to maintain a high treebank quality. 
When training on a small treebank, this is particu- 
larly important, because there is not enough data to 
allow generous pruning. 
Treebanking is done by humans and humans err. 
Even with annotation guidelines there are often ad- 
ditional inconsistencies when there are several an- 
notators. In the Penn Treebank (Marcus, 1993) for 
example, the word ago as in 'two years ago', is tagged 
122 
414 times as an adverb and 150 times as a preposi- 
tion. 
In many treebanking efforts, basic taggers and 
parsers suggest parts of speech and tree structures 
that can be accepted or corrected, typically speed- 
ing up the treebanking effort considerably. How- 
ever, incorrect defaults can easily slip through, leav- 
ing blatant inconsistencies like the one where the 
constituent ' hat' as in 'the dog that bit her' is tree- 
banked as a noun phrase containing a conjunction 
(as opposed to a pronoun). 
From the very beginning of treebanking, we have 
therefore passed all trees to be added to the tree- 
bank through a consistency checker that looks for 
any suspicious patterns in the new tree. For every 
type of phrase, the consistency checker draws on a 
list of acceptable patterns in a BNF style notation. 
While this consistency checking certainly does not 
guarantee to find all errors, and can produce false 
alarms when encountering rare but legitimate con- 
structions, we have found it a very useful tool to 
maintain treebank quality from the very beginning, 
easily offsetting the about three man days that it 
took to adapt the consistency checker to Korean. 
For a number of typical errors, we extended the 
checker to automatically correct errors for which this 
could be done safely, or, alternatively, suggest a 
likely correction for errors and prompt for confir- 
mation/correction by the treebanker. 
7 Conc lus ions  
Comparisons with related work are unfortunately 
very problematic, because the corpora are differ- 
ent and are sometimes not even described in other 
work. In most cases Korean research groups also use 
other evaluation metrics, particularly dependency 
accuracy, which is often used in dependency struc- 
ture approaches. Training on about 40,000 sentences 
(Collins, 1997) achieves a crossing brackets rate of 
1.07, a better value than our 1.63 value for regular 
parsing or the 1.13 value assuming perfect segmen- 
tation/tagging, but even for similar text types, com- 
parisons across languages are of course problematic. 
It is clear to us that with more training sentences, 
and with more features and background knowledge 
to better leverage the increased number of train- 
ing sentences, accuracy rates can still be improved 
significantly. But we believe that the reduction of 
parser development time from two years or more 
down to three months is in many cases already very 
valuable, even if the accuracy has not 'maxed out' 
yet. And given the experience we have gained from 
this project, we hope this research to be only a first 
step to an even steeper development time reduction. 
A particularly promising research direction for this 
is to harness knowledge and training resources across 
languages. 
Acknowledgments  
I would like to thank Kyoosung Lee for installing, 
improving and conncecting Korean pre-processing 
tools like segmenter and tagger as well as starting 
the treebanking, and Mina Lee, who did most of the 
treebanking. 
Re ferences  
M. J. Collins. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. In 35th Proceedings 
of the ACL, pages 16-23. 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions From Examples 
With Rich Context. In 35th Proceedings of the 
ACL, pages 482-489. 
URL: fi le://ftp.cs.utexas.edu/pub/mooney/papers 
/contex-acl-97.ps.Z 
U. Hermjakob. 1997. Learning Parse and Transla- 
tion Decisions From Examples With Rich Context. 
Ph.D. thesis, University of Texas at Austin, Dept. 
of Computer Sciences TR 97-12. 
URL: file://ftp.cs.utexas.edu/pub/mooney/papers 
/hermjakob-dissertation-97.ps.Z 
Geunbae Lee, Jong-Hyeok Lee, and Hyuncheol Rho. 
1997. Natural Language Processing for Session- 
Based Information Retrieval Interface on the Web. 
In Proceedings of IJCAI-97 workshop on AI in dig- 
ital libraries, pages 43-48. 
M. P. Marcus. 1980. A Theory of Syntactic Recog- 
nition for Natural Language. MIT Press. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 
1993. Building a Large Annotated Corpus of En- 
glish: The Penn Treebank. Computational Lin- 
guistics 19(2), pages 313-330. 
J. R. Quinlan. 1993. C4.5 Programs for Machine 
Learning. Morgan Kaufmann Publishers, San Ma- 
teo, California. 
K. J. Seo, K. C. Nam, and K. S. Choi. 1998. A Prob- 
abilistic Model for Dependency Parsing Consider- 
ing Ascending Dependencies. Journal of Literary 
and Linguistic Computing, Vol 13(2). 
Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. 
New Parsing Method Using Global Association 
Table. In Proc. of the International Workshop on 
Parsing Technology. 
123 
 Using Knowledge to Facilitate Factoid Answer Pinpointing 
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{hovy,ulf,cyl,ravichan}@isi.edu 
 
Abstract 
In order to answer factoid questions, the 
Webclopedia QA system employs a 
range of knowledge resources.  These 
include a QA Typology with answer 
patterns, WordNet, information about 
typical numerical answer ranges, and 
semantic relations identified by a robust 
parser, to filter out likely-looking but 
wrong candidate answers.  This paper 
describes the knowledge resources and 
their impact on system performance. 
1.   Introduction 
The TREC evaluations of QA systems 
(Voorhees, 1999) require answers to be drawn 
from a given source corpus.  Early QA systems 
used a simple filtering technique, question word 
density within a fixed n-word window, to 
pinpoint answers.  Robust though this may be, 
the window method is not accurate enough.  In 
response, factoid question answering systems 
have evolved into two types:  
? Use-Knowledge: extract query words from 
the input question, perform IR against the 
source corpus, possibly segment resulting 
documents, identify a set of segments 
containing likely answers, apply a set of 
heuristics that each consults a different 
source of knowledge to score each 
candidate, rank them, and select the best 
(Harabagiu et al, 2001; Hovy et al, 2001; 
Srihari and Li, 2000; Abney et al, 2000).  
? Use-the-Web: extract query words from the 
question, perform IR against the web, 
extract likely answer-bearing sentences, 
canonicalize the results, and select the most 
frequent answer(s).  Then, for justification, 
locate examples of the answers in the source 
corpus (Brill et al, 2001; Buchholz, 2001).  
Of course, these techniques can be combined: 
the popularity ratings from Use-the-Web can 
also be applied as a filtering criterion (Clarke et 
al., 2001), or the knowledge resource heuristics 
can filter the web results.  However, simply 
going to the web without using further 
knowledge (Brill et al, 2001) may return the 
web?s majority opinions on astrology, the killers 
of JFK, the cancerous effects of microwave 
ovens, etc.?fun but not altogether trustworthy.   
In this paper we describe the range of 
filtering techniques our system Webclopedia 
applies, from simplest to most sophisticated, and 
indicate their impact on the system.   
2.   Webclopedia Architecture  
As shown in Figure 1, Webclopedia adopts the 
Use-Knowledge architecture. Its modules are 
described in more detail in (Hovy et al, 2001; 
Hovy et al, 1999):  
? Question parsing: Using BBN?s 
IdentiFinder (Bikel et al, 1999), the 
CONTEX parser (Hermjakob, 1997) 
produces a syntactic-semantic analysis of 
the question and determines the QA type.   
? Query formation: Single- and multi-word 
units (content words) are extracted from the 
analysis, and WordNet synsets (Fellbaum, 
1998) are used for query expansion.  A 
series of Boolean queries of decreasing 
specificity is formed.  
? IR: The publicly available IR engine MG 
(Witten et al, 1994) returns the top-ranked 
N documents.  
 ? Selecting and ranking sentences: For each 
document, the most promising K sentences 
are located and scored using a formula that  
 rewards word and phrase overlap with the 
question and its expanded query words.  Results 
are ranked.   
? Parsing candidates: CONTEX parses the 
top-ranked 300 sentences.   
? Pinpointing: As described in Section 3, a 
number of knowledge resources are used to 
perform filtering/pinpointing operations.   
? Ranking of answers: The candidate 
answers? scores are compared and the 
winner(s) are output. 
3. Knowledge Used for Pinpointing 
3.1   Type 1: Question Word Matching 
Unlike (Prager et al, 1999), we do not first 
annotate the source corpus, but perform IR 
directly on the source text, using MG (Witten et 
al., 1994).  To determine goodness, we assign an 
initial base score to each retrieved sentence.  We 
then compare the sentence to the question and 
adapt this score as follows:  
? exact matches of proper names double the 
base score. 
? matching an upper-cased term adds a 60% 
bonus of the base score for multi-words 
terms and 30% for single words (matching 
?United States? is better than just ?United?).  
? matching a WordNet synonym of a term 
discounts by 10% (lower case) and 50% 
(upper case).  (When ?Cage? matches 
?cage?, the former may be the last name of a 
person and the latter an object; the case 
mismatch signals less reliability.)  
? lower-case term matches after Porter 
stemming are discounted 30%; upper-case 
matches 70% (Porter stemming is more 
aggressive than WordNet stemming).  
? Porter stemmer matches of both question 
and sentence words with lower case are 
discounted 60%; with upper case, 80%.  
? if CONTEX indicates a term as being 
qsubsumed (see Section 3.9) the term is 
discouned 90% (in ?Which country 
manufactures weapons of mass 
destruction??, ?country? will be marked as 
qsubsumed).   
The top-scoring 300 sentences are passed on for 
further filtering.   
3.2  Type 2: Qtargets, the QA Typology, 
and the Semantic Ontology 
We classify desired answers by their semantic 
type, which have been taxonomized in the 
Webclopedia QA Typology (Hovy et al, 2002), 
Candidate answer parsing
? Steps: parse sentences
? Engines: CONTEX
Matching
? Steps: match general constraint patterns against parse trees
             match desired semantic type against parse tree elements
             assign score to words in sliding window
? Engine: Matcher
Ranking and answer extraction
? Steps: rank candidate answers
             extract and format them
? Engine: Answer ranker/formatter
QA typology
? QA types, categorized in taxonomy
Constraint patterns
? Identify likely answers in relation to
   other parts of the sentence
Create query
Retrieve documents
Select & rank sentences
Parse top sentences
Parse question
Input question
Perform additional inference
Rank and prepare answers
Output answers
Question parsing
? Steps: parse question
             find desired semantic type
? Engines:  IdentiFinder  (BBN)
                 CONTEX
Match sentences against answers
Query creation
?  Steps: extract, combine important words
 expand query words using WordNet
 create queries, order by specificity
?  Engines: Query creator
IR
?  Steps: retrieve top 1000 documents
?  Engines: MG (RMIT Melbourne)
Sentence selection and ranking
?  Steps: score each sentence in each document
 rank sentences and pass top 300 along
?  Engines:Ranker
Figure 1. Webclopedia architecture. 
 http://www.isi.edu/natural-language/projects/we 
bclopedia/Taxonomy/taxonomy_toplevel.html). 
The currently approx. 180 classes,  which we 
call qtargets, were developed after an analysis of 
over 17,000 questions (downloaded in 1999 
from answers.com) and later enhancements to 
Webclopedia.  They are of several types:  
? common semantic classes such as PROPER-
PERSON, EMAIL-ADDRESS, LOCATION, 
PROPER-ORGANIZATION;  
? classes particular to QA such as YES:NO, 
ABBREVIATION-EXPANSION, and WHY-
FAMOUS;  
? syntactic classes such as NP and NOUN, 
when no semnatic type can be determined 
(e.g., ?What does Peugeot manufacture??);  
? roles and slots, such as REASON and TITLE-
P respectively, to indicate a desired relation 
with an anchoring concept.    
Given a question, the CONTEX parser uses a 
set of 276 hand-built rules to identify its most 
likely qtarget(s), and records them in a backoff 
scheme (allowing more general qtarget nodes to 
apply when more specific ones fail to find a 
match).  The generalizations are captured in a 
typical concept ontology, a 10,000-node extract 
of WordNet.   
The recursive part of pattern matching is 
driven mostly by interrogative phrases.  For 
example, the rule that determines the 
applicability of the qtarget WHY-FAMOUS 
requires the question word ?who?, followed by 
the copula, followed by a proper name.  When 
there is no match at the current level, the system 
examines any interrogative constituent, or words 
in special relations to it.  For example, the 
qtarget TEMPERATURE-QUANTITY (as in 
?What is the melting point of X?? requires as 
syntactic object something that in the ontology is 
subordinate to TEMP-QUANTIFIABLE-ABS-
TRACT with, as well, the word ?how? paired 
with ?warm?, ?cold?, ?hot?, etc., or the phrase  
?how many degrees? and a TEMPERATURE-
UNIT (as defined in the ontology).   
3.3 Type 3: Surface Pattern Matching 
Often qtarget answers are expressed using rather 
stereotypical words or phrases.  For example, the 
year of birth of a person is typically expressed 
using one of these phrases:  
<name> was born in <birthyear> 
<name> (<birthyear>?<deathyear>) 
We have developed a method to learn such 
patterns automatically from text on the web 
(Ravichandran and Hovy, 2002).  We have 
added into the QA Typology the patterns for 
appropriate qtargets (qtargets with closed-list 
answers, such as PLANETS, require no patterns).  
Where some QA systems use such patterns 
exclusively (Soubbotin and Soubbotin, 2001) or 
partially (Wang et al, 2001; Lee et al, 2001), 
we employ them as an additional source of 
evidence for the answer.  Preliminary results on 
for a range of qtargets, using the TREC-10 
questions and the TREC corpus, are:  
Question type 
(qtarget) 
Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.47875 
INVENTORS 6 0.16667 
DISCOVERERS 4 0.1250 
DEFINITIONS 102 0.3445 
WHY-FAMOUS 3 0.6666 
LOCATIONS 16 0.75 
3.4  Type 4: Expected Numerical Ranges  
Quantity-targeting questions are often 
underspecified and rely on culturally shared  
cooperativeness rules and/or world knowledge: 
Q: How many people live in Chile?  
S1: ?From our correspondent comes good 
news about the nine people living in  Chile?? 
A1: nine  
While certainly nine people do live in Chile, 
we know what the questioner intends.  We have 
hand-implemented a rule that provides default 
range assumptions for POPULATION questions 
and biases quantity questions accordingly.  
3.5 Type 5: Abbreviation Expansion  
Abbreviations often follow a pattern: 
Q: What does NAFTA stand for? 
S1: ?This range of topics includes the North 
American Free Trade Agreement, NAFTA, 
and the world trade agreement GATT.?  
S2: ?The interview now changed to the subject 
of trade and pending economic issues, such as 
the issue of opening the rice market, NAFTA, 
and the issue of Russia repaying economic 
cooperation funds.?  
After Webclopedia identifies the qtarget as 
ABBREVIATION-EXPANSION, it extracts 
 possible answer candidates, including ?North 
American Free Trade Agreement? from S1 and 
?the rice market? from S2.  Rules for acronym 
matching easily prefer the former.  
3.6 Type 6: Semantic Type Matching  
Phone numbers, zip codes, email addresses, 
URLs, and different types of quantities obey 
lexicographic patterns that can be exploited for 
matching, as in  
Q: What is the zip code for Fremont, CA?  
S1: ??from Everex Systems Inc., 48431 
Milmont Drive, Fremont, CA 94538.?  
and  
Q: How hot is the core of the earth?  
S1. ?The temperature of Earth?s inner core 
may be as high as 9,000 degrees Fahrenheit 
(5,000 degrees Celsius).?  
Webclopedia identifies the qtargets respectively 
as ZIP-CODE and TEMPERATURE-QUANTITY.  
Approx. 30 heuristics (cascaded) apply to the 
input before parsing to mark up numbers and 
other orthographically recognizable units of all 
kinds, including (likely) zip codes, quotations, 
year ranges, phone numbers, dates, times, 
scores, cardinal and ordinal numbers, etc.  
Similar work is reported in (Kwok et al, 2001).  
3.7 Type 7: Definitions from WordNet  
We have found a 10% increase in accuracy in 
answering definition questions by using external 
glosses obtained from WordNet.  For  
Q: What is the Milky Way?  
Webclopedia identified two leading answer 
candidates:   
A1: outer regions  
A2: the galaxy that contains the Earth  
Comparing these with the WordNet gloss:  
WordNet: ?Milky Way?the galaxy containing 
the solar system?  
allows Webclopedia to straightforwardly match 
the candidate with the greater word overlap.   
Curiously, the system also needs to use 
WordNet to answer questions involving 
common knowledge, as in:  
Q: What is the capital of the United States?  
because authors of the TREC collection do not 
find it necessary to explain what Washington is:  
Ex: ?Later in the day, the president returned to 
Washington, the capital of the United States.?  
While WordNet?s definition  
Wordnet: ?Washington?the capital of the 
United States?  
directly provides the answer to the matcher, it 
also allows the IR module to focus its search on 
passages containing ?Washington?, ?capital?, 
and ?United States?, and the matcher to pick a 
good motivating passage in the source corpus.   
Clearly, this capability can be extended to 
include (definitional and other) information 
provided by other sources, including 
encyclopedias and the web (Lin 2002). 
3.8 Type 8: Semantic Relation Matching  
So far, we have considered individual words and 
groups of words.  But often this is insufficient to 
accurately score an answer.  As also noted in 
(Buchholz, 2001), pinpointing can be improved 
significantly by matching semantic relations 
among constituents:  
Q: Who killed Lee Harvey Oswald?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?Belli?s clients have included Jack Ruby, 
who killed John F. Kennedy assassin Lee 
Harvey Oswald, and Jim and Tammy Bakker.?  
S2: ?On Nov. 22, 1963, the building gained 
national notoriety when Lee Harvey Oswald 
allegedly shot and killed President John F. 
Kennedy from a sixth floor window as the 
presidential motorcade passed.?  
The CONTEX parser (Hermjakob, 1997; 
2001) provides the semantic relations.  The 
parser uses machine learning techniques to build 
a robust grammar that produces semantically 
annotated syntax parses of English (and Korean 
and Chinese) sentences at approx. 90% accuracy 
(Hermjakob, 1999).   
The matcher compares the parse trees of S1 
and S2 to that of the question.  Both S1 and S2 
receive credit for matching question words ?Lee 
Harvey Oswald? and ?kill? (underlined), as well 
as for finding an answer (bold) of the proper 
qtarget type (PROPER-PERSON).  However, is 
the answer ?Jack Ruby? or ?President John F. 
Kennedy??  The only way to determine this is to 
consider the semantic relationship between these 
 candidates and the verb ?kill? (parse trees 
simplified, and only portions shown here):   
 
[1] Who killed Lee Harvey Oswald?  [S-SNT] 
    (SUBJ) [2] Who  [S-INTERR-NP] 
        (PRED) [3] Who  [S-INTERR-PRON] 
    (PRED) [4] killed  [S-TR-VERB] 
    (OBJ) [5] Lee Harvey Oswald  [S-NP] 
        (PRED) [6] Lee?Oswald  [S-PROPER-NAME] 
            (MOD) [7] Lee  [S-PROPER-NAME] 
            (MOD) [8] Harvey  [S-PROPER-NAME] 
            (PRED) [9] Oswald  [S-PROPER-NAME] 
    (DUMMY) [10] ?  [D-QUESTION-MARK] 
 
[1] Jack Ruby, who killed John F. Kennedy assassin  
  Lee Harvey Oswald  [S-NP] 
   (PRED) [2] <Jack Ruby>1  [S-NP] 
   (DUMMY) [6] ,  [D-COMMA] 
   (MOD) [7] who killed John F. Kennedy assassin  
                 Lee Harvey Oswald  [S-REL-CLAUSE] 
     (SUBJ) [8] who<1>  [S-INTERR-NP] 
     (PRED) [10] killed  [S-TR-VERB] 
     (OBJ) [11] JFK assassin?Oswald  [S-NP] 
         (PRED) [12] JFK?Oswald [S-PROP-NAME] 
             (MOD) [13] JFK  [S-PROPER-NAME] 
             (MOD) [19] assassin  [S-NOUN] 
             (PRED) [20] ?Oswald [S-PROPER-NAME] 
Although the PREDs of both S1 and S2 
match that of the question ?killed?, only S1 
matches ?Lee Harvey Oswald? as the head of 
the logical OBJect.  Thus for S1, the matcher 
awards additional credit to node [2] (Jack Ruby) 
for being the logical SUBJect of the killing 
(using anaphora resolution). In S2, the parse tree 
correctly records that node [13] (?John F. 
Kennedy?) is not the object of the killing.  Thus 
despite its being closer to ?killed?, the candidate 
in S2 receives no extra credit from semantic 
relation matching.   
It is important to note that the matcher 
awards extra credit for each matching semantic 
relationship between two constituents, not only 
when everything matches.  This granularity 
improves robustness in the case of partial 
matches.   
Semantic relation matching applies not only 
to logical subjects and objects, but also to all 
other roles such as location, time, reason, etc. 
(for additional examples see http://www.isi.edu/ 
natural-language/projects/webclopedia/sem-rel-
examples.html).  It also applies at not only the 
sentential level, but at all levels, such as post-
modifying prepositional and pre-modifying 
determiner phrases  
Additionally, Webclopedia uses 10 lists of 
word variations with a total of 4029 entries for 
semantically related concepts such as ?to 
invent?, ?invention? and ?inventor?, and rules 
for handling them.  For example, via coercing 
?invention? to ?invent?, the system can give 
?Johan Vaaler? extra credit for being a likely 
logical subject of ?invention?:  
Q: Who invented the paper clip?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?The paper clip, weighing a desk-crushing 
1,320 pounds, is a faithful copy of Norwegian 
Johan Vaaler?s 1899 invention, said Per 
Langaker of the Norwegian School of 
Management.?  
while ?David? actually loses points for being 
outside of the clausal scope of the inventing:  
S2: ??Like the guy who invented the safety pin, 
or the guy who invented the paper clip,? David 
added.?  
3.9 Type 9: Word Window Scoring  
Webclopedia also includes a typical window-
based scoring module that moves a window over 
the text and assigns a score to each window 
position depending on a variety of criteria (Hovy 
et al, 1999).  Unlike (Clarke et al, 2001; Lee et 
al., 2001; Chen et al, 2001), we have not 
developed a very sophisticated scoring function, 
preferring to focus on the modules that employ 
information deeper than the word level.  
This method is applied only when no other 
method provides a sufficiently high-scoring 
answer.  The window scoring function is  
S  = (500/(500+w))*(1/r) * ?[(?I1.5*q*e*b*u)1.5] 
Factors: 
w: window width (modulated by gaps of 
various lengths: ?white house? ? ?white car and 
house?), 
r: rank of qtarget in list returned by 
CONTEX, 
I: window word information content (inverse 
log frequency score of each word), summed,  
q: # different question words matched, plus 
specific rewards (bonus q=3.0),  
e: penalty if word matches one of question 
word?s WordNet synset items (e=0.8),  
 b: bonus for matching main verb, proper 
names, certain target words (b=2.0),  
u: (value 0 or 1) indicates whether a word has 
been qsubsumed (?subsumed? by the qtarget) 
and should not contribute (again) to the score.  
For example, ?In what year did Columbus 
discover America?? the qsubsumed words are 
?what? and ?year?. 
4. Performance Evaluation  
In TREC-10?s QA track, Webclopedia received 
an overall Mean Reciprocal Rank (MRR) score 
of 0.435, which put it among the top 4 
performers of the 68 entrants (the average MRR 
score for the main QA task was about 0.234).  
The pinpointing heuristics are fairly accurate: 
when Webclopedia finds answers, it usually 
ranks them in the first place (1st place: 35.5%; 
2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%; 
not found: 41.87%).  
We determined the impact of each 
knowledge source on system performance, using 
the TREC-10 test corpus using the standard 
MRR scoring.  We applied the system to the 
questions of each knowledge type separately, 
with and without its specific knowledge 
source/algorithm.  Results are shown in Table 1, 
columns A (without) and B (with).  To indicate 
overall effect, we also show (in columns C and 
D) the percentage of questions in TREC-10 and 
-9 respecively of each knowledge type.   
5. Conclusions 
It is tempting to search for a single technique 
that will solve the whole problem (for example, 
Ittycheriah et al (2001) focus on the subset of 
factoid questions answerable by NPs, and train a 
statistical model to perform NP-oriented answer 
pinpointing).  Our experience, however, is that 
even factoid QA is varied enough to require 
various special-purpose techniques and 
knowledge.  The theoretical limits of the various 
techniques are not known, though Light et al?s 
(2001) interesting work begins to study this.   
Column A: % questions of the knowledge type  
     answered correctly without using knowlege 
Column B: % questions, now using knowledge 
Column C: % questions of type in TREC-10  
Column D: % questions of type in TREC-9  
 A B C D 
Abbreviation exp. 20.0 70.0  1.0 2.3 
Number ranges 50.0 50.0  1.2 1.8 
WordNet (def Qs) 48.3 67.5 20.9 5.1 
Semantic types     
- locator types N/A N/A  0.0 0.4 
- quantity types 22.5 48.7 10.8 5.5 
- date/year types 45.0 57.3  9.2 10.2 
Patterns      
- definitions ? 34.4 20.9 5.1 
- why-famous  ? 66.7 0.6 ? 
- locations ? 75.0 3.2 ? 
- birthyear ? 47.9 1.6 ? 
Semantic relations 39.4 46.5 72.2 85.7 
Table 1. Performance of knowledge sources. 
Semantic relation scores measured only on 
questions in which they could logically apply.   
We conclude that factoid QA performance 
can be significantly improved by the use of 
knowledge attuned to specific question types 
and specific information characteristics.  Most of 
the techniques for exploiting this knowledge 
require learning to ensure robustness.  To 
improve performance beyond this, we believe a 
combination of going to the web and turning to 
deeper world knowledge and automated 
inference (Harabagiu et al, 2001) to be the 
answer.  It remains an open question how much 
work these techniques would require, and what 
their payoff limits are.   
References  
Abney, S., M. Collins, and A. Singhal. 2000. Answer 
Extraction. Proceedings of the Applied Natural 
Language Processing Conference (ANLP-
NAACL-00), Seattle, WA, 296?301.  
Bikel, D., R. Schwartz, and R. Weischedel.  1999.  
An Algorithm that Learns What?s in a Name.  
Machine Learning?Special Issue on NL 
Learning, 34, 1?3. 
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering.  
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 183?189.  
Buchholz, S. 2001. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. Proceedings of the 
TREC-10 Conference. NIST, 496?503.  
Chen, J., A.R. Diekema, M.D. Taffet, N. McCracken, 
N. Ercan Ozgencil, O. Yilmazel, and E.D. Liddy. 
 2001. CNLP at TREC-10 QA Track. Proceedings 
of the TREC-10 Conference. NIST, 480?490. 
Clarke, C.L.A., G.V. Cormack, T.R. Lynam, C.M. Li, 
and G.L. McLearn. 2001. Web Reinforced 
Question Answering. Proceedings of the TREC-
10 Conference. NIST, 620?626.  
Clarke, C.L.A., G.V. Cormack, and T.R. Lynam. 
2001. Exploiting Redundancy in Question 
Answering. Proceedings of the SIGIR 
Conference. New Orleans, LA, 358?365.  
Fellbaum, Ch. (ed). 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Harabagiu, S., D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Buneascu, R. G?rju, V. Rus and 
P. Morarescu. 2001. FALCON: Boosting 
Knowledge for Answer Engines. Proceedings of 
the 9th Text Retrieval Conference (TREC-9), 
NIST, 479?488.  
Hermjakob, U. 1997. Learning Parse and 
Translation Decisions from Examples with Rich 
Context.  Ph.D. dissertation, University of Texas 
Austin. file://ftp.cs.utexas.edu/pub/mooney/paper 
s/hermjakob-dissertation 97.ps.gz.  
Hermjakob, U. 2001. Parsing and Question 
Classification for Question Answering. 
Proceedings of the Workshop on Question 
Answering at ACL-2001.  Toulouse, France.  
Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and 
C.-Y. Lin. 1999. Question Answering in 
Webclopedia.  Proceedings of the TREC-9 
Conference.  NIST. Gaithersburg, MD, 655?673. 
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002. A Question/Answer Typology with Surface 
Text Patterns.  Poster in Proceedings of the 
DARPA Human Language Technology 
Conference (HLT).  San Diego, CA, 234?238.   
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. The 
Use of External Knowledge in Factoid QA.   
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 166?174.  
Ittycheriah, A., M. Franz, and S. Roukos. 2001. 
IBM?s Statistical Question Answering System. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 317?323.  
Kwok, K.L., L. Grunfeld, N. Dinstl, and M. Chan. 
2001. TREC2001 Question-Answer, Web and 
Cross Language experiments using PIRCS. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 447?451.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. Lee, 
B-K. Kwak, J, Cha, D. Kim, J-H. An, H. Kim, 
and K. Kim. 2001. SiteQ: Engineering High 
Performance QA System Using Lexico=Semantic 
Pattern Matching and Shallow NLP. Proceedings 
of the TREC-10 Conference. NIST, Gaithersburg, 
MD, 437?446.  
Light, M., G.S. Mann, E. Riloff, and E. Breck. 2001. 
Analyses for Elucidating Current Question 
Answering Technology. Natural Language 
Engineering, 7:4, 325?342.  
Lin, C.-Y. 2002. The Effectiveness of Dictionary and 
Web-Based Answer Reranking.  Proceedings of 
the 19th International Conference on 
Computational Linguistics (COLING 2002), 
Taipei, Taiwan.  
Oh, JH., KS. Lee, DS. Chang, CW. Seo, and KS. 
Choi. 2001. TREC-10 Experiments at KAIST: 
Batch Filtering and Question Answering. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 354?361. 
Prager, J., E. Brown, D.R. Radev, and K. Czuba. 
1999. One Search Engine or Two for Question 
Answering. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 235?240. 
Ravichandran, D. and E.H. Hovy. 2002. Learning 
Surface Text Patterns for a Question Answering 
System. Proceedings of the ACL conference. 
Philadelphia, PA.  
Soubbotin, M.M. and S.M. Soubbotin. 2001. Patterns 
of Potential Answer Expressions as Clues to the 
Right Answer. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 175?182.   
Srihari, R. and W. Li. 2000. A Question Answering 
System Supported by Information Extraction. 
Proceedings of the 1st Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (ANLP-NAACL-00), 
Seattle, WA, 166?172. 
Voorhees, E. 1999. Overview of the Question 
Answering Track. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 71?81.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu, 
and S. Bai. 2001. TREC-10 Experiments at CAS-
ICT: Filtering, Web, and QA. Proceedings of the 
TREC-10 Conference. NIST, 229?241.  
Witten, I.H., A. Moffat, and T.C. Bell. 1994. 
Managing Gigabytes: Compressing and Indexing 
Documents and Images. New York: Van 
Nostrand Reinhold. 
 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 229?237,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Improved Word Alignment with Statistics and Linguistic Heuristics
Ulf Hermjakob
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
ulf@isi.edu
Abstract
We present a method to align words in
a bitext that combines elements of a tra-
ditional statistical approach with linguis-
tic knowledge. We demonstrate this ap-
proach for Arabic-English, using an align-
ment lexicon produced by a statistical
word aligner, as well as linguistic re-
sources ranging from an English parser
to heuristic alignment rules for function
words. These linguistic heuristics have
been generalized from a development cor-
pus of 100 parallel sentences. Our aligner,
UALIGN, outperforms both the commonly
used GIZA++ aligner and the state-of-the-
art LEAF aligner on F-measure and pro-
duces superior scores in end-to-end sta-
tistical machine translation, +1.3 BLEU
points over GIZA++, and +0.7 over LEAF.
1 Introduction
Word alignment is a critical component in training
statistical machine translation systems and has re-
ceived a significant amount of research, for exam-
ple, (Brown et al, 1993; Ittycheriah and Roukos,
2005; Fraser and Marcu, 2007), including work
leveraging syntactic parse trees, e.g., (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et
al., 2008). Word alignment is also a required
first step in other algorithms such as for learning
sub-sentential phrase pairs (Lavie et al, 2008) or
the generation of parallel treebanks (Zhechev and
Way, 2002).
Yet word alignment precision remains surpris-
ingly low, under 80% for state-of-the-art aligners
on not closely related language pairs.
Consider the following Arabic/English sen-
tence pair with alignments built by the statistical
word aligner LEAF:
Bitext Arabic: 	?A 	?A ? K


Q ?
	
?P?X@PA K
.
?Y
	
KA ? K


A

J ? @
	
P A
	
??
??J



?

J? @? , 6 - 4 ? 6 - 4 	?QJ
.
	
J

J??

J?
	
???K


Ak
.
??@Q

J?A? @ ???
6 - 4 ? 6 - 4
Q????
	
?P?K
.

?PA? ?
	
KA?? A?@ ??? ?J


	
KA
	
? ?QJ


K


Gloss: Won(1) Thai Paradorn Srichaphan(1)
on/to(2) Australian Jason(2) Stoltenberg(3) 6(4) -
4(5) and(3) 6(4) - 4(5), and Czech Jir???(7) Vane?k(7)
on/to German(6) Lars Burgsmu?ller 6(4) - 4 and(3)
6(4) - 4
Bitext English: Thailand ?s(1) Baradorn Srich-
fan(1) beat(2) Australian Gayson(1) Stultenberg(3)
6(4) - 6(4) 6(4) - 4(5) , Czech player(1) Pierre(1)
Vanic(7) beat(6) Germany(6) ?s Lars Burgsmuller 6
- 4 6 - 4
In the example above, words with the same index
in the gloss for Arabic and the English are aligned
to each other, alignment errors are underlined,
translation errors are in italics. For example, the
Arabic words for won and Srichaphan are aligned
with the English words ?s, Srichfan, Gayson,
player and Pierre.
As reflected in the example above, typical align-
ment problems include
 words that change sentence position between
languages, such as verbs, which in Arabic
are often sentence-initial (e.g. won/beat in the
example above)
 function words without a clear and explicit
equivalent in the other language (e.g. the Ara-
bic ?/and in the example above)
 lack of robustness with respect to poor trans-
lations (e.g. Gayson Stultenberg instead of
Jason Stoltenberg) or bad sentence align-
ment.
We believe we can overcome such problems
with the increased use of linguistically based
229
heuristics. We can model typical word order dif-
ferences between English and Arabic using En-
glish parse trees and a few Arabic-specific phrase
reordering heuristics. We can narrow the space of
possible alignment candidates for function words
using English parse trees and a few heuristics for
each type of function word.
These heuristics have been developed using a
development corpus of 100 parallel sentences. The
heuristics are generalizations based on patterns
of misaligned words, misaligned with respect to
a Gold Standard alignment for that development
corpus.
The following sections describe how our word
aligner works, first how relatively reliable content
words are aligned, and then how function words
and any remaining content words are aligned, with
a brief discussion of an interesting issue relating
to the Gold Standard we used. Finally we present
evaluations on word alignment accuracy as well
as the impact on end-to-end machine translation
quality.
2 Phase I: Content Words
We divide the alignment process into two phases:
first, we align relatively reliable content words,
which in phase II we then use as a skeleton to align
function words and remaining content words.
Function words such as English a, ah, all, am,
an, and, any, are, as, at, ... are common words
that often do not have an explicit equivalent word
or words in the other side of the bitext. In our
system, we use a list of 96 English and 110 Ara-
bic function words with those characteristics. For
the purposes of our algorithm, a word is a function
word if and only if it is on the function word list
for its language. A content word then is defined as
a word that is neither a function word nor punctu-
ation.
The approach for aligning content words in
phase I is as follows: First, we score each com-
bination of an Arabic content word and English
content word in an aligned sentence and align
those pairs that pass a threshold, typically gener-
ating too many alignments. Second, we compute
a more comprehensive score that also takes into
consideration matching alignments in the context
around each alignment. Third, we eliminate infe-
rior alignments that are incompatible with higher-
scoring alignments.
The score in the first step is pointwise mutual
information (PMI). The key resource to compute
this PMI is an alignment lexicon generated be-
forehand by a statistical word alignment system
from a large bitext. An alignment lexicon is a
list of triples, each consisting of an English word,
an Arabic word, and how often they have been
aligned for a given bitext. Additional counts on
how often each English and Arabic word occurs
allow us use this alignment lexicon to compute
PMI(e,f) = log p(e;f)
p(e)p(f)
. We align those Arabic
and English content words that have a PMI > 0
and a minimum alignment lexicon count ( 10
initially). Using the alignment lexicon generated
by a statistical word aligner to compute PMIs is
the principal statistical component in our system.
We explored alternative metrics such as the dice-
coefficient that was used by other researchers in
earlier alignment work, but found PMI to work
better for our system.
In a second step, we lay a window of size 5
around each aligned pair of Arabic and English
words (counting only content words) and then add
to the PMI score of the link itself the PMI scores
of other links within that window, with a distance
weight of 1
distance+1
. This yields a new score that
takes into account whether a link is supported by
context.
In the third step, we check for overgenerated
links, comparing links that share an Arabic or an
English word. If a word on one side of the bitext
is linked to multiple adjacent words on the other,
we leave them alone, as one word in one language
often corresponds to multiple words in the other.
However, if a word on one side is linked to non-
adjacent words on the other side, this flags an in-
compatibility, and we remove those links that have
inferior context-sensitive scores. This removal is
done one link at a time, with the lowest relative
scores first.
We boost the process we just described in a few
ways. In the first alignment step, we also include
as alignment candidates any content words that are
string-identical on each side, such as ASCII num-
bers and ASCII words. We finally also include
as alignment candidates those word pairs that are
transliterations of each other to cover rare proper
names (Hermjakob et al, 2008), which is impor-
tant for language pairs that don?t share the same
alphabet such as Arabic and English.
230
2.1 Reordering Using an English Parser
We use a refined notion of context window that
models word order differences between Arabic
and English. Traversing a parse tree for English,
we identify sub-trees for which the order in Ara-
bic can be substantially different. In Arabic, for
example, the verb is often sentence-initial. So for
trees or subtrees identified by the parser as sen-
tences, we generate an alternative reordering of
its subtrees where the verb has been moved to the
front. Similarly, in a noun phrase, we generate an
alternative order where adjectives are moved to the
right of the noun they modify.
For example, consider the sentence John bought
a new car . We can reorder its parse tree both at
the sentence level: (bought) (John) (a new car) (.)
as well as at its object NP level: (a) (car) (new).
If fully enumerated, this would yield these four
reordering alternatives:
1. John bought a new car .
2. John bought a car new .
3. bought John a new car .
4. bought John a car new .
We don?t actually explicitly enumerate all variants
but keep all reordering alternatives in a reorder-
ing forest, since the number of fully expanded re-
orderings grows exponentially with the number of
phrases with reordering(s). At the beginning of
Phase I, we compute from this reordering forest a
minimum distance matrix, which, for specific in-
stances of the words John and car would record
a minimum distance of 1 (based on reordering 4,
skipping the function word a).
For the example sentence at the beginning of the
paper we would get reorderings including the fol-
lowing:
Engl. orig.: thailand ?s baradorn srichfan beat ...
A reordering: beat thailand ?s baradorn srichfan ...
Arabic (gloss): won thai paradorn srichaphan ...
In the above reordered English alternative, beat
and thailand are next to each other, so their min-
imum distance is 1, which means that a link
between English thailand and Arabic thai now
strongly boosts the context-sensitive score be-
tween English beat and Arabic won.
2.2 Morphological Variation
Another challenge to content word alignment is
morphological variation which can create data
sparsity in the alignment lexicon. For example,
in a given bitext sentence, the Arabic word AlAw-
DAE might be translated as situational, for which
there might be no support in the alignment lex-
icon. However the PMI between AlAwDAE and
situation might be sufficiently high. Additionally,
there is another Arabic word, AlHAlAt, which of-
ten translates as both situation and situational.
To take advantage of such constellations, we
built morphological variation lists for both Arabic
and English, lists that for a given head word such
as situational lists variants such as situation, and
situations.
We built these lists in a one-time process by
identifying superficially similar words, i.e. those
that vary only with respect to an ending or a prefix,
and then semantically validating such candidates
using a pivot word in the other language such as
AlHAlAt that has sufficiently strong alignment lex-
icon co-alignment counts with both situation and
situational. The alignment lexicon co-alignment
count of an Arabic word w
ar
and an English word
w
en
is considered strong enough, if it is at least
2.0 and at least 0.001 times as high as the high-
est co-alignment count of w
ar
with any English
word; words shorter than four letters are excluded
from consideration. So because situation and situ-
ational are superficially similar and they are both
have a strong alignment count with AlHAlAt in the
alignment lexicon, situation is added to the En-
glish morphological variation list as a variant of
situational and vice versa.
Exploring whether we can align situational and
AlAwDAE in the bitext, we find that situational
is a morphological variant of situation (based on
our morphological variation list for English); next
we find that based on the alignment lexicon, there
is a positive PMI between situation and AlAw-
DAE, which completes the chain between situ-
ational and AlAwDAE, so we include them as
an alignment candidate after all. The PMI of
such a morphological-variation-based candidate is
weighted by a ?penalty? factor of 0.5 when com-
pared with the PMI of any competing alignment
candidate without such morphological-variation
step.
Similarly, the English pivot word situations can
be used to semantically validate the similarity
between Arabic AlAwDAE and AwDAE for our
Arabic morphological variation list. The resulting
Arabic morphological variation list has entries
for 193,263 Arabic words with an average of
4.2 variants each; our English morphological
variation list has 57,846 entries with 2.8 variants
231
each.
At the end of phase I, most content words will
be aligned with relatively high precision. Since
function words often do not have an explicit equiv-
alent word or words in the other side of a bi-
text, they can not be aligned as reliably as con-
tent words based on bilingual PMI.1 Note that
due to data sparsity, some content words will re-
mained unaligned in phase I and will subsequently
be aligned in phase II as explained in section 3.3.
3 Phase II: Function Words
In Phase II, we align function words, punctua-
tion, and some remaining content words. Func-
tion words can be classified into three categories:
monovalent, divalent and independent. Monova-
lent function words modify one head; they in-
clude articles (which modify nouns), possessive
pronouns, demonstrative adjectives and auxiliary
verbs. Divalent function words connect two words
or phrases; they include conjunctions and prepo-
sitions. Independent function words include non-
possessive pronouns and copula (e.g. is as a main
verb). Each of these types of function words is
aligned according to its own heuristics.
In this section we present three representative
examples, one for articles (monovalent), one for
prepositions (divalent), as well as a structural
heuristic.
3.1 Example: Articles
Monovalent function words have the simplest
heuristics. Recall that Arabic does not have ar-
ticles (only a definite prefix Al- added to one or
more words in a definite noun phrase), so there is
usually no explicit equivalent of the English article
on the Arabic side.
For an English article, our system identifies the
English head word that it modifies based on the
English parse tree, and then aligns it with the same
Arabic word(s) which that head word is aligned
with.
3.2 Example: Prepositions
Divalent function words are much more interest-
ing. In many cases, an English preposition corre-
sponds to an explicit Arabic preposition in basi-
1It is this lack of reliability that is the defining charac-
teristic of our function words, differentiating them from the
concept of marker words used in EBMT chunking (Way and
Gough, 2002).
cally the same position. Alignment in that case is
straightforward. However, some Arabic preposi-
tions and even more English prepositions do not
have an explicit counterpart on the other side. We
call such prepositions orphan prepositions. The
English preposition of is almost always orphaned
in this way.
The decision how to align such an orphan
preposition is not trivial. Consider the bitext is-
land of Basilan/jzyrp bAsylAn, a typical (NP1 (P
NP2)) construction on the English side. Should
we co-align the preposition of with the head of
NP1 or the head of NP2? In English syntax, the
preposition is grouped with NP2, but a preposition
is often better ?motivated? by NP1. We therefore
decided to use the English parse tree to identify
the heads of both NP1 and NP2, identify the Ara-
bic words aligned to these heads as candidates, and
then align the preposition to the Arabic candidate
word with which it has the highest bilingual PMI.
It turns out that in most cases this will be the can-
didate on the ?left?. For the example at the top of
this paragraph, of will be aligned with jzyrp (?is-
land?), which is actually desirable for MT, as it fa-
cilitates subsequent rule extraction of type ?island
of X/jzyrp X?. We refer to this orphan preposition
alignment style as MT-style.
According to the gold standard alignment
guidelines used for the LDC Gold Standard how-
ever, an orphan preposition should always be
aligned to the ?right?, to bAsylAn in the example
above. We therefore implemented an alternative
GS-style (for ?Gold Standard?) to be able to later
evaluate the impact of these alternatives alignment
styles.
The question whether GIZA or LEAF align-
ments will indeed give meaningful scores to sup-
port the MT-style attachments will be answered by
the MT experiments described in section 4.3.
Here is a more complex example with Arabic
(A), its gloss (G) and English (E):
Arabic: P@?k
.

?

??
	
J? ???

?J


?QJ


?A?@

H@Q

KA??@

HPA
	
?@ YkA?@
Gloss: sunday attacked aircraft american on/to area jiwar
Engl.: on sunday american aircraft attacked the area of jiwar
For the Arabic orphan preposition ???/ElY
(?on/to?), our system identifies two candidates
based on the English parse tree: attacked and area.
Based on a higher mutual information, our system
then aligns Arabic ElY (?on/to?) with English at-
tacked, which results in the English word attacked
now being aligned to both Arabic attacked and the
232
Arabic on/to, even though they are not adjacent.
In the Gold Standard, Arabic on/to is aligned with
English area, and LEAF aligns it with English on
(yes, the one preceding Sunday). This is appar-
ently very tempting as Arabic on/to is often trans-
lated as English on, but here it is incorrect, and our
system avoids this tempting alignment because it
is ruled out linguistically.
Note that in some cases, such as sentence-initial
prepositional phrases, there is only one candidate;
occasionally, when relevant content words remain
unaligned, no candidate can be identified, in which
case the orphan preposition remains unaligned as
well.
3.3 Example: Adjectives
It is not uncommon that content words that we
would like to be aligned are not supported by the
alignment lexicon, due to general data sparsity
or maybe a somewhat unorthodox translation. In
those cases we can use structure and word order
knowledge to make reasonable alignments any-
way.
Consider an English noun phase ADJ-E
NOUN-E and the corresponding Arabic NOUN-
A ADJ-A. If the nouns are already aligned, but the
adjectives are not yet algned, we can use the En-
glish parse tree to identify ADJ-E as a modifier
to NOUN-E, and, aware that adjectives in Arabic
post-modify their nouns, identify the correspond-
ing Arabic word based on structure and word order
alone. This can be done the other way around as
well (link nouns based on already aligned adjec-
tives) and other elements of other phrases as well.
As more and more function words and re-
maining content words get algned, heuristics that
weren?t applicable before may now apply to the
remaining unaligned words, so we perform four
passes through a sentence pair to align unaligned
words using heuristics. We found that an addi-
tional fifth pass did not yield any further improve-
ments.
4 Experiments
We evaluated our word aligner in terms of both
alignment accuracy and its impact on an end-to-
end machine translation system.
4.1 Alignment Experiments
We evaluated our word aligner against a Gold
Standard distributed by LDC. The human align-
ments of the sentences in this Gold Standard are
based on the 2006 GALE Guidelines for Arabic
Word Alignment Annotation.
Both the 100-sentence development set and the
separate 837-sentence test set are Arabic newswire
sentences from LDC2006E86. The test set in-
cludes only sentences for which our English parser
(Soricut and Marcu, 2003) could produce a parse
tree, which effectively excluded a few very long
sentences.
In the first set of experiments, we compare
two settings of our UALIGN system with other
aligners, GIZA++ (Union) (Och and Ney, 2003)
and LEAF (with 2 iterations) (Fraser and Marcu,
2007). The GIZA++ aligner is based on IBM
Model 4 (Brown et al, 1993). We chose GIZA
Union for our comparison, because it led to a
higher BLEU score for our overall MT system than
other GIZA variants such as GIZA Intersect and
Grow-Diag. The two settings of our system vary in
the style on how to align orphan prepositions. Be-
sides precision, recall and (balanced) F-measure,
we also include an F-measure variant strongly bi-
ased towards recall (=0.1), which (Fraser and
Marcu, 2007) found to be best to tune their LEAF
aligner for maximum MT accuracy. GIZA++ and
LEAF alignments are based on a parallel train-
ing corpus of 6.6 million sentence pairs, incl. the
LDC2006E86 set mentioned above.
Aligner Prec. Recall F-0.5 F-0.1
GIZA 26.9 84.3 40.8 69.5
LEAF 73.3 79.7 76.4 79.0
UALIGN MT-style 82.5 80.0 81.2 80.2
UALIGN GS-style 84.0 82.9 83.5 83.0
Table 1: Alignment precision, recall, F-measure
(=0.5), F-measure(=0.1) for different aligners;
with UALIGN using LEAF alignment lexicon.
Our aligner outperforms both GIZA and LEAF
on all metrics. Not surprisingly, the GS-style
alignments, which align ?orphan? prepositions ac-
cording to Gold Standard guidelines, yield higher
scores than MT-style alignments. And interest-
ingly by a remarkably high margin.
In a second set of experiments, we measure the
impact of using different input alignment lexicon
used by our aligner on alignment accuracy. In one
case UALIGN uses as input the alignment lexicon
produced by LEAF, in the other the alignment lex-
icon produced by GIZA. All experiments in table 2
233
are for UALIGN.
Style A-Lexicon Prec. Recall F-0.5 F-0.1
MT from LEAF 82.5 80.0 81.2 80.2
MT from GIZA 80.8 79.2 80.0 79.4
GS from LEAF 84.0 82.9 83.5 83.0
GS from GIZA 82.1 81.8 82.0 81.9
Table 2: Alignment precision, recall, F-measure
(=0.5), F-measure(=0.1), all of UALIGN, for
different alignment styles, different input align-
ment lexicons.
As LEAF clearly outperforms GIZA on F-0.1
(79.0 vs. 69.5, see table 1), the alignment lexicon
based on LEAF is better, so it is not surprising
that when we use an alignment lexicon based on
GIZA, all metrics degrade, and consistently so for
both alignment styles. However the drop in F-0.1
of about 1 point (80.2 ! 79.4 and 83.0 ! 81.9)
is much smaller than the differences between the
underlying aligners themselves. Our aligner there-
fore degrades quite gracefully for a worse align-
ment lexicon.
Aligner Arabic aligned Engl. aligned
GIZA Union 100% 100%
LEAF 99.99% 97.25%
UALIGN 92.10% 91.55%
Gold Standard 95.37% 95.86%
Table 3: Percentages of Arabic and English words
aligned
Table 3 shows how much LEAF and UALIGN
differ in the percentage of Arabic and English
words aligned (correctly or incorrectly). LEAF
is much more aggressive in making alignments,
aligning almost every Arabic word. Our aligner
still leaves some 8% of all words in a sentence un-
aligned (an opportunity for further improvements).
For comparison, in the Gold Standard, 4-5% of all
words in our test corpus are left unaligned.
4.2 Impact of Sub-Components
To better understand the impact of several align-
ment system sub-components, we ran a number of
experiments disabling individual sub-components
and then comparing the resulting alignment scores
with those of the full system. We also measured
alignment scores running Phase II with 0 to 5
passes. The test set was the same as in section
4.1.
System Prec. Recall F-0.1
Full system (FS) 84.0 82.9 83.0
FS w/o morph.variation 84.0 82.4 82.5
FS w/o Engl. tree reord. 83.8 82.7 82.8
FS w/o string identity 84.0 82.8 82.9
FS w/o name translit. 84.0 82.8 82.9
System after Phase I 90.6 44.5 46.8
+ Phase II w/ 1 pass 87.6 77.1 78.0
+ Phase II w/ 2 passes 85.8 80.3 80.8
+ Phase II w/ 3 passes 84.2 82.7 82.8
+ Phase II w/ 4 passes 84.0 82.9 83.0
+ Phase II w/ 5 passes 84.0 82.9 83.0
Table 4: Impact of sub-components on alignment
precision, recall, F-measure, with GS-style attach-
ments, based on the LEAF alignment lexicon.
Special sub-components of Phase I include
adding link candidates for ASCII-string-identical
words and transliterated names (see last paragraph
before section 2.1), reordering using an English
parser (section 2.1) and morphological variation
(section 2.2). Each of these sub-components pro-
vides a small boost to F-0.1, ranging from +0.1 to
+0.5. The second part of the table shows align-
ment scores before and after each pass of Phase II.
Our full system includes 4 passes; an additional
5th pass did not yield any further improvements.
Note that during Phase II, precision drops. This is
a reflection of (1) our strategy to first align rela-
tively reliable content words in Phase I, followed
by less reliable function words and remaining con-
tent words, and (2) the challenges of building reli-
able Gold Standard alignments for function words
and non-literal translations.
4.3 MT Experiments
The ultimate test for a word aligner is to mea-
sure its impact on an end-to-end machine trans-
lation system. For this we aligned 170,863 pairs
of Arabic/English newswire sentences from LDC,
trained a state-of-the-art syntax-based statistical
machine translation system (Galley et al, 2006)
on these sentences and alignments, and measured
BLEU scores (Papineni et al, 2002) on a sepa-
rate set of 1298 newswire test sentences. Besides
swapping in a new set of alignments for the same
set of training sentences, and automatically retun-
ing the parameters of the translation system for
each set of alignments, no other changes or ad-
justments were made to the existing MT system.
234
In the first set of experiments, we compare two
settings of our UALIGN system with other align-
ers, again GIZA++ (Union) and LEAF (with 2 it-
erations). The two settings vary in the alignment
lexicon that the UALIGN aligner uses as input.
Aligner BLEU
GIZA 47.4
LEAF 48.0
UALIGN using GIZA alignment-lexicon 48.4
UALIGN using LEAF alignment-lexicon 48.7
Table 5: BLEU scores in end-to-end statistical MT
system based on different aligners. Both UALIGN
variants use MT-style alignments.
With a BLEU score of 48.7, UALIGN using
a LEAF alignment-lexicon is significantly bet-
ter than both GIZA (+1.3) and LEAF (+0.7).
This and other significance assertions in this pa-
per are based on paired bootstrap resampling
tests with 95% confidence. UALIGN using
a GIZA alignment-lexicon significantly outper-
forms GIZA itself (+1.0).
In a second experiment, we measured the im-
pact of the two alignment styles on BLEU. Re-
call that for GS-style alignments, orphan preposi-
tions are always co-aligned to the right, following
Gold Standard annotation guidelines, whereas for
MT-style alignments, mutual information is used
to decide whether to align orphan prepositions to
the left or to the right.
Aligner BLEU
LEAF 48.0
UALIGN with GS-style alignments 48.0
UALIGN with MT-style alignments 48.7
Table 6: BLEU scores in end-to-end statistical MT
system based on different alignment styles for or-
phan prepositions. Both UALIGN variants use a
LEAF alignment lexicon.
While the GS-style alignments yielded a 2.8
point higher F-0.1 score (83.0 vs. 80.2), the MT-
style alignments result in a significantly better
BLEU score (48.7 vs. 48.0). This shows that
(1) a seemingly small difference in alignment
styles can have a remarkably high impact on both
BLEU scores and alignment accuracy as measured
against a Gold Standard, and that (2) optimiz-
ing alignment accuracy against an alignment Gold
Standard does not necessarily optimize BLEU in
end-to-end MT. The latter has been observed by
other researchers before, but these results addi-
tionally suggest that the gold-standard annotation
style might itself have to shoulder part of the
blame.
4.4 Corpus Noise Robustness
In a small random ?sanity check? sample from
the 170,863 training sentences for the MT exper-
iment, we found cases where the sentence in one
language contained much more material than the
sentence in the other language. Consider, for ex-
ample the following sentence pair (with spurious
material underlined):
Arabic:
,

?Y
	
J
	
?? @ A

?
	
JK


?? @
	
X@ ?
	
K@ ??? ?
	
JK


Q
	
k@ Y
	
JK
.
?A
	
J? A
	
?K


@
	
???
Gloss: but also there-is clause another stipulates
on/to that if not established the-hotel ,
English: but , also there is another clause that
stipulates that if the hotel is not established ,
then the government shall be compensated .
Both LEAF and UALIGN correctly align the En-
glish ?but , also ... not established ,? with the
Arabic side. LEAF further aligns all words in the
spurious English ?then the government shall be
compensated .? with seemingly random material
on the Arabic side, whereas UALIGN leaves these
spurious words completely unaligned. It would
be reasonable to speculate that this behavior, ob-
served in several cases, may be contributing to the
good BLEU scores.
5 Discussion
Building on existing statistical aligners, our new
word aligner significantly outperforms the best
word aligner to date in both alignment error rate
and BLEU score.
We have developed an approach to word align-
ment that combines a statistical component with
linguistic heuristics. It is novel in that it goes
beyond generic resources such as parsers, adding
heuristics to explicitly model word order differ-
ences and function word alignment.
The approach has numerous benefits. Our sys-
tem produces superior results both on alignment
accuracy and end-to-end machine translation qual-
ity. Alignments have a high precision. The system
is fast (about 0.7 seconds per sentence), and sen-
tences are aligned individually so that a large cor-
pus can easily be aligned on several computers in
235
parallel. All alignment links are tagged with ad-
ditional information, such as which phase and/or
heuristic created them, yielding extensive explana-
tory power to the developer for easy understanding
on how the system arrived at a given alignment.
Our approach needs and uses a parser for only one
side (English) and not for the other (Arabic).
On the other hand, some of the components
of this aligner are language-specific, such as
word order heuristics, the list of specific function
words, and morphological variation lists. While
these parts of the system need to be adapted for
new languages, the overall architecture and types
of heuristics and function words are language-
independent. Chinese for example has different
specific types of function words such as aspect
markers and measure words. But these fall into the
existing category of monovalent function words
and will be treated according the same principles
as other monovalent function words (section 3.1).
Similarly, Japanese postpositions would be treated
like other divalent function words (such as Arabic
or English prepositions). The author and devel-
oper has a basic knowledge of Arabic in general,
and an intermediate knowledge of Arabic gram-
mar, which means that no intimate knowledge
of Arabic was required to develop the language-
specific components. This same author and devel-
oper recently started to adapt UALIGN to Chinese-
English word alignment.
The alignment rate is still somewhat low. We
plan to increase it by enlarging our develop-
ment set beyond 100 sentences and adding further
heuristics, as well as generalizing the output word
alignment structure to allow alignments of words
to larger constituents in a tree, and to explicitly as-
sert that some words are not covered by the other
side of a bitext to model poor translations and poor
sentence alignments.
Acknowledgment
This research was supported under DARPA Con-
tract No. HR0011-06-C-0022. The author would
like to thank Kevin Knight and the anonymous re-
viewers for their helpful suggestions, and Steve
DeNeefe for running the end-to-end MT evalua-
tions.
References
Peter E. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra and Robert L. Mercer. 1993. The Math-
ematics of Statistical Machine Translation: Parame-
ter Estimation. In Computational Linguistics Vol.
19(2), pages 263?311.
Colin Cherry and Dekang Lin. 2006. Soft Syntactic
Constraints for Word Alignment Through Discrimi-
native Training. In Proceedings of the 44th Annual
Meeting on Association for Computational Linguis-
tics, Sydney, Australia, pages 105?112.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of the 45th Annual Meeting on Associ-
ation for Computational Linguistics, Prague, Czech
Republic, pages 17?24.
Victoria Fossum, Kevin Knight and Steven Abney.
2008. Using Syntax to Improve Word Alignment
Precision for Syntax-Based Machine Translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation, Columbus, Ohio, pages 44?52.
Alexander Fraser and Daniel Marcu. 2007. Getting
the Structure Right for Word Alignment: LEAF. In
Proceedings of Conference for Empirical Methods
in Natural Language Processing (EMNLP), Prague,
Czech Republic, pages 51?60.
Alexander Fraser and Daniel Marcu. 2007. Mea-
suring Word Alignment Quality for Statistical Ma-
chine Translation. In Computational Linguistics
Vol. 33(3), pages 293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Pro-
ceedings of the 44th Annual Meeting on Association
for Computational Linguistics, Sydney, Australia,
pages 961?968.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III
2008. Name Translation in Statistical Machine
Translation: Learning When to Transliterate. In
Proceedings of the 46th Annual Meeting on Asso-
ciation for Computational Linguistics, Columbus,
Ohio, pages 389?397.
Abraham Ittycheriah and Salim Roukos. 2005.
A Maximum Entropy Word Aligner for Arabic-
English Machine Translation. In Proceed-
ings of Joint Conference of Human Language
Technology and Empirical Methods in Natural
Language Processing (HLT/EMNLP), Vancouver,
British Columbia, Canada, pages 89?96.
Alon Lavie, Alok Parlikar and Vamshi Ambati. 2008.
Syntax-Driven Learning of Sub-Sentential Transla-
tion Equivalents and Translation Rules from Parsed
Parallel Corpora. In Proceedings of the ACL/HLT
Second Workshop on Syntax and Structure in Statis-
tical Translation (SSST-2), Columbus, Ohio, pages
87?95.
236
Dan Melamed. 2000. Models of translational equiv-
alence among words. In Computational Linguistics
Vol. 26(2), pages 221?249.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics Vol. 29(1),
pages 19?51.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. In Computational Linguistics
Vol. 30(4), pages 417?449.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, Philadelphia, PA, pages 311?318.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, Edmonton, Canada, pages 149?156.
Andy Way, Nano Gough. 2003. wEBMT: develop-
ing and validating an example-based machine trans-
lation system using the world wide web In Compu-
tational Linguistics Vol. 29(3), pages 421?457.
Ventsislav Zhechev, Andy Way. 2008. Automatic
Generation of Parallel Treebanks. In Proceed-
ings of 22nd International Conference on Compu-
tational Linguistics (COLING), Manchester, UK,
pages 1105?1112.
237
Toward Semantics-Based Answer Pinpointing
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292-6695
USA
tel: +1-310-448-8731
{hovy,gerber,ulf,cyl,ravichan}@isi.edu
ABSTRACT
We describe the treatment of questions (Question-Answer
Typology, question parsing, and results) in the Weblcopedia
question answering system.  
1. INTRODUCTION
Several research projects have recently investigated the
problem of automatically answering simple questions that have
brief phrasal answers (?factoids?), by identifying and extracting
the answer from a large collection of text.  
The systems built in these projects exhibit a fairly standard
structure: they create a query from the user?s question, perform
IR with the query to locate (segments of) documents likely to
contain an answer, and then pinpoint the most likely answer
passage within the candidate documents.  The most common
difference lies in the pinpointing. Many projects employ a
window-based word scoring method that rewards desirable
words in the window.  They move the window across the
candidate answers texts/segments and return the window at the
position giving the highest total score.  A word is desirable if
it is a content word and it is either contained in the question, or
is a variant of a word contained in the question, or if it matches
the words of the expected answer.  Many variations of this
method are possible?of the scores, of the treatment of multi-
word phrases and gaps between desirable words, of the range of
variations allowed, and of the computation of the expected
answer words.  
Although it works to some degree (giving results of up to 30%
in independent evaluations), the window-based method has
several quite serious limitations:
? it cannot pinpoint answer boundaries precisely (e.g., an
exact name or noun phrase),
? it relies solely on information at the word level, and
hence cannot recognize information of the desired type
(such as Person or Location),
? it cannot locate and compose parts of answers that are
distributed over areas wider than the window.
Window-based pinpointing is therefore not satisfactory in the
long run, even for factoid QA.  In this paper we describe work
in our Webclopedia project on semantics-based answer
pinpointing. Initially, though, recognizing the simplicity and
power of the window-based technique for getting started, we
implemented a version of it as a fallback method.  We then
implemented two more sophisticated methods: syntactic-
semantic question analysis and QA pattern matching.  This
involves classification of QA types to facilitate recognition of
desired answer types, a robust syntactic-semantic parser to
analyze the question and candidate answers, and a matcher that
combines word- and parse-tree-level information to identify
answer passages more precisely.  We expect that the two
methods will really show their power when more complex non-
factoid answers are sought.  In this paper we describe how well
the three methods did relative to each other.  Section 2 outlines
the Webclopedia system.  Sections 3, 4, and 5 describe the
semantics-based components: a QA Typology, question and
answer parsing, and matching.  Finally, we outline current
work on automatically learning QA patterns using the Noisy
Channel Model.  
2. WEBCLOPEDIA
Webclopedia?s architecture (Figure 1) follows the pattern
outlined above:
Question parsing: Using BBN?s IdentiFinder [1], our
parser CONTEX (Section 4) produces a syntactic-semantic
analysis of the question and determines the QA type (Section
3).  
Query formation : Single- and multi-word units (content
words) are extracted from the analysis, and WordNet synsets are
used for query expansion.  A Boolean query is formed. See [9].
IR: The IR engine MG [12] returns the top-ranked 1000
documents.
Segmentat ion : To decrease the amount of text to be
processed, the documents are broken into semantically
coherent segments.  Two text segmenter?TexTiling [5] and
C99 [2]?were tried; the first is used; see [9].
Ranking segments : For each segment, each sentence i s
scored using a formula that rewards word and phrase overlap
with the question and its expanded query words.  Segments are
ranked.  See [9]
Parsing segments : CONTEX parses each sentence of the
top-ranked 100 segments (Section 4).  
Pinpointing: For each sentence, three steps of matching are
performed (Section 5); two compare the analyses of the
question and the sentence; the third uses the window method to
compute a goodness score.  
Ranking of answers : The candidate answers? scores are
compared and the winner(s) are output.
3. THE QA TYPOLOGY
In order to perform pinpointing deeper than the word level, the
system has to produce a representation of what the user i s
asking.  Some previous work in automated question answering
has categorized questions by question word or by a mixture of
question word and the semantic class of the answer [11, 10].  To
ensure full coverage of all forms of simple question and answer,
and to be able to factor in deviations and special requirements,
we are developing a QA Typology.  
We motivate the Typology (a taxonomy of QA types) as
follows.  
There are many ways to ask the same thing: What is the age o f
the Queen of Holland?  How old is the Netherlands? queen?  How
long has the ruler of Holland been alive?  Likewise, there are
many ways of delivering the same answer: about 60; 63 years
old; since January 1938.  Such variations form a sort of
semantic equivalence class of both questions and answers.
Since the user may employ any version of his or her question,
and the source documents may contain any version(s) of the
answer, an efficient system should group together equivalent
question types and answer types.  Any specific question can
then be indexed into its type, from which all equivalent forms
of the answer can be ascertained.  These QA equivalence types
can help with both query expansion and answer pinpointing.
However, the equivalence is fuzzy; even slight variations
introduce exceptions: who invented the gas laser? can be
answered by both Ali Javan and a scientist at MIT, while what
is the name of the person who invented the gas laser? requires
the former only.  This inexactness suggests that the QA types
be organized in an inheritance hierarchy, allowing the answer
requirements satisfying more general questions to be
overridden by more specific ones ?lower down?.  
These considerations help structure the Webclopedia QA
Typology.  Instead of focusing on question word or semantic
type of the answer, our classes attempt to represent the user?s
intention, including for example the classes Why-Famous (for
Who was Christopher Columbus? but not Who discovered
IR
? Steps: create query from question (WordNet-expand)
             retrieve top 1000 documents
? Engines: MG (Sydney)?(Lin)
                  AT&T (TREC)?(Lin)
Segmentation
? Steps:segment each document into topical segments
? Engines: fixed-length (not used)
                 TexTiling (Hearst 94)?(Lin)
                 C99 (Choi 00)?(Lin)
                 MAXNET (Lin 00, not used)
Ranking
? Steps: score each sentence in each segment,
                              using WordNet expansion
             rank segments
? Engines: FastFinder (Junk)
Matching
? Steps: match general constraint patterns against parse trees
            match desired semantic type against parse tree elements
            match desired words against words in sentences
? Engines: matcher (Junk)
Ranking and answer extraction
? Steps: rank candidate answers
            extract and format them
? Engines: part of matcher (Junk)
Question parsing
? Steps: parse question
            find desired semantic type
? Engines: IdentiFinder (BBN)
                CONTEX (Hermjakob)
QA typology
? Categorize QA types in taxonomy (Gerber)
Constraint patterns
? Identify likely answers in relation to other
   parts of the sentence (Gerber)
Retrieve documents
Segment documents
Rank segments
Parse top segments
Parse question
Input question
Match segments against question
Rank and prepare answers
Create query
Output answers
Segment Parsing
? Steps: parse segment sentences
? Engines: CONTEX (Hermjakob)
Figure 1. Webclopedia architecture.
America?, which is the QA type Proper-Person) and
Abbreviation-Expansion (for What does HLT stand for?).  In
addition, the QA Typology becomes increasingly specific as
one moves from the root downward.
To create the QA Typology, we analyzed 17,384 questions and
their answers (downloaded from answers.com); see (Gerber, in
prep.).  The Typology (Figure 2) contains 72 nodes, whose leaf
nodes capture QA variations that can in many cases be further
differentiated.
Each Typology node has been annotated with examples and
typical patterns of expression of both Question and Answer,
using a simple template notation that expressed configurations
of words and parse tree annotations (Figure 3).  Question
pattern information (specifically, the semantic type of the
answer required, which we call a Qtarget) is produced by the
CONTEX parser (Section 4) when analyzing the question,
enabling it to output its guess(s) for the QA type.  Answer
pattern information is used by the Matcher (Section 5) to
pinpoint likely answer(s) in the parse trees of candidate answer
sentences.
Question examples and question templates
Who was Johnny Mathis' high school track coach?
Who was Lincoln's Secretary of State?
who be <entity>'s <role>
Who was President of Turkmenistan in 1994?
Who is the composer of Eugene Onegin?
Who is the chairman of GE?
who be <role> of <entity>
Answer templates and actual answers
<person>, <role> of  <entity>
Lou Vasquez, track coach of?and Johnny Mathis
<person> <role-title*> of <entity>
Signed Saparmurad Turkmenbachy [Niyazov],
president of Turkmenistan
<entity>?s <role> <person>
...Turkmenistan?s President Saparmurad Niyazov
<person>'s <entity>
...in Tchaikovsky's Eugene Onegin...
<role-title> <person> ... <entity> <role>
Mr. Jack Welch, GE chairman...
<subject>|<psv object> of related role-verb
       ...Chairman John Welch said ...GE's
Figure 3. Some QA Typology node annotations for
Proper-Person.
At the time of the TREC-9 Q&A evaluation, we had produced
approx. 500 patterns by simply cross-combining approx. 20
Question patterns with approx. 25 Answer patterns.  To our
disappointment (Section 6), these patterns were both too
specific and too few to identify answers frequently?when they
applied, they were quite accurate, but they applied too seldom.
We therefore started work on automatically learning QA
patterns in parse trees (Section 7).  On the other hand, the
semantic class of the answer (the Qtarget) is used to good effect
(Sections 4 and 6).
4. PARSING
CONTEX is a deterministic machine-learning based grammar
learner/parser that was originally built for MT [6].  For
English, parses of unseen sentences measured 87.6% labeled
precision and 88.4% labeled recall, trained on 2048 sentences
from the Penn Treebank. Over the past few years it has been
extended to Japanese and Korean [7].
4.1 Parsing Questions
Accuracy is particularly important for question parsing,
because for only one question there may be several answers in a
large document collection.  In particular, it is important to
identify as specific a Qtarget as possible.  But grammar rules
ERACITY YES:NO
TRUE:FALSE
NTIT Y A GENT NAME LAST-NAME
FIRST-NAME
ORGANIZATION
GROUP-OF-PEOPLE
A NIMAL
PERSON OCCUPATION-PERSON
GEOGRAPHICA L-PERSON
PROPER-NAMED-ENTITY PROPER-PERSON
PROPER-ORGANIZATION
PROPER-PLACE CITY
COUNTRY
STATE-DISTRICT
QUANTITY NU MERICAL-QUANTI TY
MONETARY-QUANTITY
TEMPORAL-QUANTITY
MASS-QUANTI TY
SPATIAL-QUANTITY DISTANCE-QUANTITY
AREA-QUANTITY
VOLUME-QUANTI TY
TEMP-LOC DATE
DATE-RANGE
LOCATOR ADDRESS
EMAIL-ADDRESS
PHONE-NUMBER
URL
TANGIBLE-OBJECT HU MAN-FOOD
SUBS TANCE LIQUID
BODY-PART
INSTRUMENT
GARMENT
TITLED-WORK
ABSTRACT SHAPE
ADJECTIVE COLOR
DISEASE
TEXT
NARRATIVE GENERAL-INFO DEFINITION USE
EXPRESSION-ORIGIN
HISTORY WHY-FAMOUS BIO
ANTECEDENT
INFLUENCE CONSEQUENT
CAUSE-EFFECT METHOD-MEANS
CIRCUMSTANCE-MEANS REASON
EVALUATION PRO-CON
CONTRAST
RATING
COUNSEL-ADVICE
Figure 2. Portion of Webclopedia QA Typology.
for declarative sentences do not apply well to questions, which
although typically shorter than declaratives, exhibit markedly
different word order, preposition stranding (?What university
was Woodrow Wilson President of??), etc.  
Unfortunately for CONTEX, questions to train on were not
initially easily available; the Wall Street Journal sentences
contain a few questions, often from quotes, but not enough and
not representative enough to result in an acceptable level of
question parse accuracy.  By collecting and treebanking,
however, we increased the number of questions in the training
data from 250 (for our TREC-9 evaluation version of
Webclopedia) to 400 on Oct 16 to 975 on Dec 9.  The effect i s
shown in Table 1.  In the first test run (?[trained] without
[additional questions]?), CONTEX was trained mostly on
declarative sentences (2000 Wall Street Journal sentences,
namely the enriched Penn Treebank, plus a few other non-
question sentences such as imperatives and short phrases).  In
later runs (?[trained] with [add. questions]?), the system was
trained on the same examples plus a subset of the 1153
questions we have treebanked at ISI (38 questions from the pre-
TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and
222 others).
The TREC-8 and TREC-9 questions were divided into 5 subsets,
used in a five-fold cross validation test in which the system was
trained on all but the test questions, and then evaluated on the
test questions.   
Reasons for the improvement include (1) significantly more
training data; (2) a few additional features, some more treebank
cleaning, a bit more background knowledge etc.; and (3) the
251 test questions on Oct. 16 were probably a little bit harder
on average, because a few of the TREC-9 questions initially
treebanked (and included in the October figures) were selected
for early treebanking because they represented particular
challenges, hurting subsequent Qtarget processing.
4.2 Parsing Potential Answers
The semantic type ontology in CONTEX was extended to
include 115 Qtarget types, plus some combined types; more
details in [8].  Beside the Qtargets that refer to concepts in
CONTEX?s concept ontology (see first example below),
Qtargets can also refer to part of speech labels (first example),
to constituent roles or slots of parse trees (second and third
examples), and to more abstract nodes in the QA Typology
(later examples). For questions with the Qtargets Q-WHY-
FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and
others, the parser also provides Qargs?information helpful for
matching (final examples).
Semantic ontology types (I-EN-CITY)
and part of speech labels (S-PROPER-NAME):
What is the capital of Uganda?
QTARGET: (((I-EN-CITY S-PROPER-NAME))
((EQ I-EN-PROPER-PLACE)))
Parse tree roles:
Why can't ostriches fly?
      QTARGET: (((ROLE REASON)))
Name a film in which Jude Law acted.
      QTARGET: (((SLOT TITLE-P TRUE)))
QA Typology nodes:
What are the Black Hills known for?
     Q-WHY-FAMOUS
What is Occam's Razor?
     Q-DEFINITION
What is another name for nearsightedness?
     Q-SYNONYM
Should you exercise when you're sick?
     Q-YES-NO-QUESTION
Qargs for additional information:
Who was Betsy Ross?
     QTARGET: (((Q-WHY-FAMOUS-PERSON)))  
     QARGS: (("Betsy Ross"))
How is "Pacific Bell" abbreviated?
     QTARGET: (((Q-ABBREVIATION)))
     QARGS: (("Pacific Bell"))
What are geckos?
     QTARGET: (((Q-DEFINITION)))
     QARGS: (("geckos" "gecko") ("animal"))
These Qtargets are determined during parsing using 276 hand-
written rules.  Still, for approx. 10% of the TREC-8&9
questions there is no easily determinable Qtarget (?What does
the Peugeot company manufacture??; ?What is caliente in
English??).  Strategies for dealing with this are under
investigation.  More details appear in (Hermjakob, 2001).  The
current accuracy of the parser on questions and resulting
Qtargets sentences is shown in Table 2.
5. ANSWER MATCHING
The Matcher performs three independent matches, in order:
? match QA patterns in the parse tree,
? match Qtargets and Qwords in the parse tree,
? match over the answer text using a word window.
Details appear in [9].
Table 1. Improvement in parsing of questions.
Labeled Labeled Tagging Crossing
Precision Recall Precision Recall Accuracy Brackets
Without, Oct 16 90.74% 90.72% 84.62% 83.48% 94.95% 0.6
With, Oct 16 94.19% 94.86% 91.63% 91.91% 98.00% 0.48
With, Dec 9 97.33% 97.13% 95.40% 95.13% 98.64% 0.19
Table 1.  Improvement in parsing of questions.
6. RESULTS
We entered the TREC-9 short form QA track, and received an
overall Mean Reciprocal Rank score of 0.318, which put
Webclopedia in essentially tied second place with two others.
(The best system far outperformed those in second place.)  
In order to determine the relative performance of the modules,
we counted how many correct answers their output contained,
working on our training corpus.  Table 3 shows the evolution
of the system over a sample one-month period, reflecting the
amount of work put into different modules.  The modules QA
pattern, Qtarget, Qword, and Window were all run in parallel
from the same Ranker output.  
The same pattern, albeit with lower scores, occurred in the
TREC test (Table 4).  The QA patterns made only a small
contribution, the Qtarget made by far the largest contribution,
and, interestingly, the word-level window match lay
somewhere in between.
Table 4. TREC-9 test: correct answers
attributable to each module.
IR hits QA pattern Qtarget Window Total
78.1 5.5 26.2 10.4 30.3
We are pleased with the performance of the Qtarget match.  This
shows that CONTEX is able to identify to some degree the
semantic type of the desired answer, and able to pinpoint these
types also in candidate answers.  The fact that it outperforms
the window match indicates the desirability of looking deeper
than the surface level.  As discussed in Section 4, we are
strengthening the parser?s ability to identify Qtargets.  
We are disappointed in the performance of the 500 QA patterns.
Analysis suggests that we had too few patterns, and the ones we
had were too specific.  When patterns matched, they were rather
accurate, both in finding correct answers and more precisely
pinpointing the boundaries of answers.  However, they were
too sensitive to variations in phrasing.  Furthermore, it was
difficult to construct robust and accurate question and answer
phraseology patterns manually, for several reasons.  First,
manual construction relies on the inventiveness of the pattern
builder to foresee variations of phrasing, for both question and
answer.  It is however nearly impossible to think of all
possible variations when building patterns.  
Second, it is not always clear at what level of representation to
formulate the pattern: when should one specify using words?
Parts of speech? Other parse tree nodes? Semantic classes?  The
patterns in Figure 3 include only a few of these alternatives.
Specifying the wrong elements can result in non-optimal
coverage.  Third, the work is simply tedious.  We therefore
decided to try to learn QA patterns automatically.  
7. TOWARD LEARNING QA PATTERNS
AUTOMATICALLY
To learn corresponding question and answer expressions, we
pair up the parse trees of a question and (each one of) its
answer(s).  We then apply a set of matching criteria to identify
potential corresponding portions of the trees.  We then use the
EM algorithm to learn the strengths of correspondence
combinations at various levels of representation.  This work i s
still in progress.  
In order to learn this information we observe the truism that
there are many more answers than questions. This holds for the
two QA corpora we have access to?TREC and an FAQ website
(since discontinued).  We therefore use the familiar version of
the Noisy Channel Model and Bayes? Rule.   For each basic QA
type (Location, Why-Famous, etc.):
Table 2. Question parse tree and Qtarget accuracies.
# Penn # Question Crossing Qtarget Qtarget
Treebank sentences Labeled Labele d Tagging brackets accuracy accuracy
sentences added Precision Recall Accuracy (/ sent) (strict) (lenient)
2000 0 83.47% 82.49% 94.65% 0.34 63.00% 65.50%
3000 0 84.74% 84.16% 94.51% 0.35 65.30% 67.40%
2000 38 91.20% 89.37% 97.63% 0.26 85.90% 87.20%
3000 38 91.52% 90.09% 97.29% 0.26 86.40% 87.80%
2000 975 95.71% 95.45% 98.83% 0.17 96.10% 97.30%
Date Number
Qs
IR
hits
Ranker
hits
QA
pattern
Qtgt
match
Qword
fallback
Window
fallback
Total
2-Jul 52 1.00 0.61 0.12 0.49 0.15 0.19 0.62
8-Jul 38 0.89 0.40 0.28 0.40 0.12 n/a 0.53
13-Jul 52 1.00 0.61 0.04 0.48 0.15 0.22 0.53
3-Aug 55 n/a n/a 0.04 0.32 0.15 0.19 0.41
Table 3. Relative performance of Webclopedia modules on training corpus.
P(A|Q)  =  argmax P(Q|A) . P(A)
P(A)  =   ?all trees (# nodes that may express a true A) 
/  (number of nodes in tree)
P(Q|A)  =  ?all QA tree pairs (number of covarying nodes 
in Q and A trees)
/ (number of nodes in A tree)
As usual, many variations are possible, including how to
determine likelihood of expressing a true answer; whether to
consider all nodes or just certain major syntactic ones (N, NP,
VP, etc.); which information within each node to consider
(syntactic? semantic? lexical?); how to define ?covarying
information??node identity? individual slot value equality?;
what to do about the actual answer node in the A trees; if (and
how) to represent the relationships among A nodes that have
been found to be important; etc.  Figure 4 provides an answer
parse tree that indicates likely Location nodes, determined by
appropriate syntactic class, semantic type, and syntactic role
in the sentence.  
Our initial model focuses on bags of corresponding QA parse
tree nodes, and will help to indicate for a given question what
type of node(s) will contain the answer.  We plan to extend this
model to capture structured configurations of nodes that, when
matched to a question, will help indicate where in the parse tree
of a potential answer sentence the answer actually lies.  Such
bags or structures of nodes correspond, at the surface level, to
important phrases or words.  However, by using CONTEX
output we abstract away from the surface level, and learn to
include whatever syntactic and/or semantic information is best
suited for predicting likely answers.
8. REFERENCES
[1] Bikel, D., R. Schwartz, and R. Weischedel.  1999.  An
Algorithm that Learns What s in a Name.  Machine
Learning Special Issue on NL Learning, 34, 1?3.
[2] Choi, F.Y.Y. 2000. Advances in independent linear text
segmentation. Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00), 26?33.
[3] Fellbaum, Ch. (ed). 1998. WordNet: An Electronic Lexical
Database. Cambridge: MIT Press.
[4] Gerber, L. 2001.  A QA Typology for Webclopedia. In prep.
[5] Hearst, M.A. 1994. Multi-Paragraph Segmentation of
Expository Text.  Proceedings of the Annual Conference
of the Association for Computational Linguistics (ACL-
94).
[6] Hermjakob, U. 1997. Learning Parse and Translation
Decisions from Examples with Rich Context.  Ph.D.
dissertation, University of Texas at Austin.
file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakob-
dissertation-97.ps.gz.
[7] Hermjakob, U.  2000. Rapid Parser Development: A
Machine Learning Approach for Korean. Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-2000).
http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz.
[8] Hermjakob, U. 2001. Parsing and Question Classification
for Question Answering. In prep.
[9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2000. Question Answering in Webclopedia.
Proceedings of the TREC-9 Conference.  NIST.
Gaithersburg, MD.
[10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R.
Girju, R. Goodrum, and V. Rus. 2000. The Structure and
Performance of an Open-Domain Question Answering
System. Proceedings of the Conference of the Association
for Computational Linguistics (ACL-2000), 563?570.
[11] Srihari, R. and W. Li. 2000. A Question Answering System
Supported by Information Extraction. In Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-00), 166?172.
[12] Witten, I.H., A. Moffat, and T.C. Bell. 1994. Managing
Gigabytes: Compressing and Indexing Documents and
Images. New York: Van Nostrand Reinhold.
SU
RF
  L
ux
or
 is
 fa
m
ed
 fo
r i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
.  
CA
T 
S-
SN
T 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(S
UB
J) 
SC
O
RE
 4
 
SU
RF
  i
s  
CA
T 
S-
A
U
X
 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
JP
 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(C
OM
PL
) 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 0
 
SU
RF
  f
or
 it
s V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  .
  
CA
T 
D
-P
ER
IO
D
 
LE
X
  .
  
R
O
LE
S 
(D
UM
M
Y)
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
J 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(P
RE
D)
 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 1
 
SU
RF
  f
or
  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-F
O
R 
CL
A
SS
ES
 (I
-E
P-
FO
R)
 
LE
X
  f
or
  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  a
nd
  
CA
T 
S-
CO
O
RD
-C
O
N
J 
CL
A
SS
 I-
EC
-A
N
D
 
CL
A
SS
ES
 (I
-E
C-
AN
D)
 
LE
X
  a
nd
  
R
O
LE
S 
(C
ON
J) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(C
OO
RD
) 
SC
O
RE
 2
 
SU
RF
  i
ts 
 
CA
T 
S-
PO
SS
-P
RO
N
 
CL
A
SS
 I-
EN
-P
O
SS
-P
RO
N
O
U
N
 
CL
A
SS
ES
 (I
-E
N-
PO
SS
-P
RO
NO
UN
) 
LE
X
  P
O
SS
-P
RO
N
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
RO
PE
R-
O
RG
A
N
IZ
A
TI
O
N
 
CL
A
SS
ES
 (I
-E
N-
PR
OP
ER
-O
RG
AN
IZ
AT
IO
N 
I-E
N-
OR
GA
NI
ZA
TI
ON
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  V
al
le
y 
of
 th
e 
K
in
gs
  
R
O
LE
S 
(M
OD
) 
N
A
M
ED
-E
N
TI
TY
-U
N
IT
-P
 T
RU
E 
SC
O
RE
 4
 
SU
RF
  P
ha
ra
on
ic
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
H
A
RA
O
N
IC
 
CL
A
SS
ES
 (I
-E
N-
PH
AR
AO
NI
C)
 
LE
X
  P
ha
ra
on
ic
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  o
f t
he
 K
in
gs
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  o
f  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-O
F 
CL
A
SS
ES
 (I
-E
P-
OF
) 
LE
X
  o
f  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
in
gs
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  c
om
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-K
A
RN
A
K
 
CL
A
SS
ES
 (I
-E
N-
KA
RN
AK
) 
LE
X
  k
ar
na
k 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  t
em
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
Figure 4. Candidate answer tree showing likely Location answers.
Proceedings of ACL-08: HLT, pages 389?397,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Name Translation in Statistical Machine Translation
Learning When to Transliterate
Ulf Hermjakob and Kevin Knight
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
fulf,knightg@isi.edu
Hal Daume? III
University of Utah
School of Computing
50 S Central Campus Drive
Salt Lake City, UT 84112, USA
me@hal3.name
Abstract
We present a method to transliterate names
in the framework of end-to-end statistical
machine translation. The system is trained
to learn when to transliterate. For Arabic
to English MT, we developed and trained a
transliterator on a bitext of 7 million sen-
tences and Google?s English terabyte ngrams
and achieved better name translation accuracy
than 3 out of 4 professional translators. The
paper also includes a discussion of challenges
in name translation evaluation.
1 Introduction
State-of-the-art statistical machine translation
(SMT) is bad at translating names that are not very
common, particularly across languages with differ-
ent character sets and sound systems. For example,
consider the following automatic translation:1
Arabic input 	? AK
.
?

?? P@
	
P??? pAK
.
?

J?
	
?J


J



?J


???
?J


	
?@P?
	
??
	
JJ


	
KA?kP?
	
?A??

??
	
?
	
???

JJ


K
.
?
	
?J


J


	
????QK
.
?
SMT output musicians such as Bach
Correct translation composers such as Bach,
Mozart, Chopin, Beethoven, Schumann,
Rachmaninoff, Ravel and Prokofiev
The SMT system drops most names in this ex-
ample. ?Name dropping? and mis-translation hap-
pens when the system encounters an unknown word,
mistakes a name for a common noun, or trains on
noisy parallel data. The state-of-the-art is poor for
1taken from NIST02-05 corpora
two reasons. First, although names are important to
human readers, automatic MT scoring metrics (such
as BLEU) do not encourage researchers to improve
name translation in the context of MT. Names are
vastly outnumbered by prepositions, articles, adjec-
tives, common nouns, etc. Second, name translation
is a hard problem ? even professional human trans-
lators have trouble with names. Here are four refer-
ence translations taken from the same corpus, with
mistakes underlined:
Ref1 composers such as Bach, missing name
Chopin, Beethoven, Shumann, Rakmaninov,
Ravel and Prokoviev
Ref2 musicians such as Bach, Mozart, Chopin,
Bethoven, Shuman, Rachmaninoff, Rafael and
Brokoviev
Ref3 composers including Bach, Mozart, Schopen,
Beethoven, missing name Raphael, Rahmaniev
and Brokofien
Ref4 composers such as Bach, Mozart, missing
name Beethoven, Schumann, Rachmaninov,
Raphael and Prokofiev
The task of transliterating names (independent of
end-to-end MT) has received a significant amount
of research, e.g., (Knight and Graehl, 1997; Chen et
al., 1998; Al-Onaizan, 2002). One approach is to
?sound out? words and create new, plausible target-
language spellings that preserve the sounds of the
source-language name as much as possible. Another
approach is to phonetically match source-language
names against a large list of target-language words
389
and phrases. Most of this work has been discon-
nected from end-to-end MT, a problem which we
address head-on in this paper.
The simplest way to integrate name handling into
SMT is: (1) run a named-entity identification system
on the source sentence, (2) transliterate identified
entities with a special-purpose transliteration com-
ponent, and (3) run the SMT system on the source
sentence, as usual, but when looking up phrasal
translations for the words identified in step 1, instead
use the transliterations from step 2.
Many researchers have attempted this, and it does
not work. Typically, translation quality is degraded
rather than improved, for the following reasons:
 Automatic named-entity identification makes
errors. Some words and phrases that should
not be transliterated are nonetheless sent to the
transliteration component, which returns a bad
translation.
 Not all named entities should be transliterated.
Many named entities require a mix of translit-
eration and translation. For example, in the pair
A J


	
KP?
	
? J


?A ? H
.
?
	
J k
.
/jnub kalyfurnya/Southern
California, the first Arabic word is translated,
and the second word is transliterated.
 Transliteration components make errors. The
base SMT system may translate a commonly-
occurring name just fine, due to the bitext it was
trained on, while the transliteration component
can easily supply a worse answer.
 Integration hobbles SMT?s use of longer
phrases. Even if the named-entity identifi-
cation and transliteration components operate
perfectly, adopting their translations means that
the SMT system may no longer have access to
longer phrases that include the name. For ex-
ample, our base SMT system translates ?J



KP
	
?
	
J K
.
? ? Z@P
	
P? ?@ (as a whole phrase) to ?Pre-
mier Li Peng?, based on its bitext knowledge.
However, if we force 	? 	J K
.
? ? to translate as
a separate phrase to ?Li Peng?, then the term
Z @P
	
P??@ ?J



KP becomes ambiguous (with trans-
lations including ?Prime Minister?, ?Premier?,
etc.), and we observe incorrect choices being
subsequently made.
To spur better work in name handling, an ACE
entity-translation pilot evaluation was recently de-
veloped (Day, 2007). This evaluation involves
a mixture of entity identification and translation
concerns?for example, the scoring system asks for
coreference determination, which may or may not be
of interest for improving machine translation output.
In this paper, we adopt a simpler metric. We ask:
what percentage of source-language named entities
are translated correctly? This is a precision metric.
We can readily apply it to any base SMT system, and
to human translations as well. Our goal in augment-
ing a base SMT system is to increase this percentage.
A secondary goal is to make sure that our overall
translation quality (as measured by BLEU) does not
degrade as a result of the name-handling techniques
we introduce. We make all our measurements on an
Arabic/English newswire translation task.
Our overall technical approach is summarized
here, along with references to sections of this paper:
 We build a component for transliterating be-
tween Arabic and English (Section 3).
 We automatically learn to tag those words and
phrases in Arabic text, which we believe the
transliteration component will translate cor-
rectly (Section 4).
 We integrate suggested transliterations into the
base SMT search space, with their use con-
trolled by a feature function (Section 5).
 We evaluate both the base SMT system and the
augmented system in terms of entity translation
accuracy and BLEU (Sections 2 and 6).
2 Evaluation
In this section we present the evaluation method that
we use to measure our system and also discuss chal-
lenges in name transliteration evaluation.
2.1 NEWA Evaluation Metric
General MT metrics such as BLEU, TER, METEOR
are not suitable for evaluating named entity transla-
tion and transliteration, because they are not focused
on named entities (NEs). Dropping a comma or a the
is penalized as much as dropping a name. We there-
fore use another metric, jointly developed with BBN
and LanguageWeaver.
390
The general idea of the Named Entity Weak Ac-
curacy (NEWA) metric is to
 Count number of NEs in source text: N
 Count number of correctly translated NEs: C
 Divide C/N to get an accuracy figure
In NEWA, an NE is counted as correctly translated
if the target reference NE is found in the MT out-
put. The metric has the advantage that it is easy to
compute, has no special requirements on an MT sys-
tem (such as depending on source-target word align-
ment) and is tokenization independent.
In the result section of this paper, we will use the
NEWA metric to measure and compare the accuracy
of NE translations in our end-to-end SMT transla-
tions and four human reference translations.
2.2 Annotated Corpus
BBN kindly provided us with an annotated Arabic
text corpus, in which named entities were marked
up with their type (e.g. GPE for Geopolitical Entity)
and one or more English translations. Example:
?
	
?<GPE alt=?Termoli?>????QJ



K</GPE>
<PER alt=?Abdullah II j Abdallah II?> ? ? ? @ Y J
.
?
?
	
KA

J? @</PER>
The BBN annotations exhibit a number of issues.
For the English translations of the NEs, BBN anno-
tators looked at human reference translations, which
may introduce a bias towards those human transla-
tions. Specifically, the BBN annotations are some-
times wrong, because the reference translations were
wrong. Consider for example the Arabic phrase
? ?? ?Q J



K ?
	
?
	
? @Q

KP? K
.
?
	
J ? ? (mSn? burtran
fY tyrmulY), which means Powertrain plant in Ter-
moli. The mapping from tyrmulY to Termoli is not
obvious, and even less the one from burtran to Pow-
ertrain. The human reference translations for this
phrase are
1. Portran site in Tremolo
2. Termoli plant (one name dropped)
3. Portran in Tirnoli
4. Portran assembly plant, in Tirmoli
The BBN annotators adopted the correct transla-
tion Termoli, but also the incorrect Portran. In
other cases the BBN annotators adopted both a cor-
rect (Khatami) and an incorrect translation (Kha-
timi) when referring to the former Iranian president,
which would reward a translation with such an in-
correct spelling.
 <PER alt=?KhatamijKhatimi?>??KA 	k</PER>
 <GPE alt=?the American?> ?J


?QJ


?A? @</GPE>
In other cases, all translations are correct, but ad-
ditional correct translations are missing, as for ?the
American? above, for which ?the US? is an equally
valid alternative in the specific sentence it was anno-
tated in.
All this raises the question of what is a correct
answer. For most Western names, there is normally
only one correct spelling. We follow the same con-
ventions as standard media, paying attention to how
an organization or individual spells its own name,
e.g. Senator Jon Kyl, not Senator John Kyle. For
Arabic names, variation is generally acceptable if
there is no one clearly dominant spelling in English,
e.g. GaddafijGadhafijQaddafijQadhafi, as long as a
given variant is not radically rarer than the most con-
ventional or popular form.
2.3 Re-Annotation
Based on the issues we found with the BBN annota-
tions, we re-annotated a sub-corpus of 637 sentences
of the BBN gold standard.
We based this re-annotation on detailed annota-
tion guidelines and sample annotations that had pre-
viously been developed in cooperation with Lan-
guageWeaver, building on three iterations of test an-
notations with three annotators.
We checked each NE in every sentence, using
human reference translations, automatic translitera-
tor output, performing substantial Web research for
many rare names, and checked Google ngrams and
counts for the general Web and news archives to de-
termine whether a variant form met our threshold of
occurring at least 20% as often as the most dominant
form.
3 Transliterator
This section describes how we transliterate Arabic
words or phrases. Given a word such as 	??	JJ


	
K A?kP
or a phrase such as ?J


	
?@P ?K


P??, we want to find
the English transliteration for it. This is not just a
391
romanization like rHmanynuf and murys rafyl for
the examples above, but a properly spelled English
name such as Rachmaninoff and Maurice Ravel. The
transliteration result can contain several alternatives,
e.g. RachmaninoffjRachmaninov. Unlike various
generative approaches (Knight and Graehl, 1997;
Stalls and Knight, 1998; Li et al, 2004; Matthews,
2007; Sherif and Kondrak, 2007; Kashani et al,
2007), we do not synthesize an English spelling
from scratch, but rather find a translation in very
large lists of English words (3.4 million) and phrases
(47 million).
We develop a similarity metric for Arabic and En-
glish words. Since matching against millions of can-
didates is computationally prohibitive, we store the
English words and phrases in an index, such that
given an Arabic word or phrase, we quickly retrieve
a much smaller set of likely candidates and apply
our similarity metric to that smaller list.
We divide the task of transliteration into two
steps: given an Arabic word or phrase to translit-
erate, we (1) identify a list of English translitera-
tion candidates from indexed lists of English words
and phrases with counts (section 3.1) and (2) com-
pute for each English name candidate the cost for
the Arabic/English name pair (transliteration scor-
ing model, section 3.2).
We then combine the count information with the
transliteration cost according to the formula:
score(e) = log(count(e))/20 - translit cost(e,f)
3.1 Indexing with consonant skeletons
We identify a list of English transliteration candi-
dates through what we call a consonant skeleton in-
dex. Arabic consonants are divided into 11 classes,
represented by letters b,f,g,j,k,l,m,n,r,s,t. In a one-
time pre-processing step, all 3,420,339 (unique) En-
glish words from our English unigram language
model (based on Google?s Web terabyte ngram col-
lection) that might be names or part of names
(mostly based on capitalization) are mapped to one
or more skeletons, e.g.
Rachmaninoff ! rkmnnf, rmnnf, rsmnnf, rtsmnnf
This yields 10,381,377 skeletons (average of 3.0 per
word) for which a reverse index is created (with
counts). At run time, an Arabic word to be translit-
erated is mapped to its skeleton, e.g.
	
??
	
JJ


	
K A?kP ! rmnnf
This skeleton serves as a key for the previously built
reverse index, which then yields the list of English
candidates with counts:
rmnnf ! Rachmaninov (186,216), Rachmaninoff
(179,666), Armenonville (3,445), Rachmaninow
(1,636), plus 8 others.
Shorter words tend to produce more candidates, re-
sulting in slower transliteration, but since there are
relatively few unique short words, this can be ad-
dressed by caching transliteration results.
The same consonant skeleton indexing process is
applied to name bigrams (47,700,548 unique with
167,398,054 skeletons) and trigrams (46,543,712
unique with 165,536,451 skeletons).
3.2 Transliteration scoring model
The cost of an Arabic/English name pair is com-
puted based on 732 rules that assign a cost to a pair
of Arabic and English substrings, allowing for one
or more context restrictions.
1. ?::q == ::0
2. 	??::ough == ::0
3. h::ch == :[aou],::0.1
4. ?::k == ,$:,$::0.1 ; ::0.2
5. Z:: == :,EC::0.1
The first example rule above assigns to the
straightforward pair ?/q a cost of 0. The second rule
includes 2 letters on the Arabic and 4 on the English
side. The third rule restricts application to substring
pairs where the English side is preceded by the let-
ters a, o, or u. The fourth rule specifies a cost of 0.1
if the substrings occur at the end of (both) names,
0.2 otherwise. According to the fifth rule, the Ara-
bic letter Z may match an empty string on the En-
glish side, if there is an English consonant (EC) in
the right context of the English side.
The total cost is computed by always applying the
longest applicable rule, without branching, result-
ing in a linear complexity with respect to word-pair
length. Rules may include left and/or right context
for both Arabic and English. The match fails if no
rule applies or the accumulated cost exceeds a preset
limit.
Names may have n words on the English and m on
the Arabic side. For example, New York is one word
in Arabic and Abdullah is two words in Arabic. The
392
rules handle spaces (as well as digits, apostrophes
and other non-alphabetic material) just like regular
alphabetic characters, so that our system can handle
cases like where words in English and Arabic names
do not match one to one.
The French name Beaujolais ( ?J


??k
.
?K
.
/bujulyh)
deviates from standard English spelling conventions
in several places. The accumulative cost from the
rules handling these deviations could become pro-
hibitive, with each cost element penalizing the same
underlying offense ? being French. We solve this
problem by allowing for additional context in the
form of style flags. The rule for matching eau/?
specifies, in addition to a cost, an (output) style flag
+fr (as in French), which in turn serves as an ad-
ditional context for the rule that matches ais/ ?K


at
a much reduced cost. Style flags are also used for
some Arabic dialects. Extended characters such as
e?, o?, and s? and spelling idiosyncrasies in names on
the English side of the bitext that come from various
third languages account for a significant portion of
the rule set.
Casting the transliteration model as a scoring
problem thus allows for very powerful rules with
strong contexts. The current set of rules has been
built by hand based on a bitext development corpus;
future work might include deriving such rules auto-
matically from a training set of transliterated names.
This transliteration scoring model described in
this section is used in two ways: (1) to transliter-
ate names at SMT decoding time, and (2) to identify
transliteration pairs in a bitext.
4 Learning what to transliterate
As already mentioned in the introduction, named
entity (NE) identification followed by MT is a bad
idea. We don?t want to identify NEs per se anyway
? we want to identify things that our transliterator
will be good at handling, i.e., things that should be
transliterated. This might even include loanwords
like bnk (bank) and brlman (parliament), but would
exclude names such as National Basketball Associ-
ation that are often translated rather transliterated.
Our method follows these steps:
1. Take a bitext.
2. Mark the Arabic words and phrases that have a
recognizable transliteration on the English side.
3. Remove the English side of the bitext.
4. Divide the annotated Arabic corpus into a train-
ing and test corpus.
5. Train a monolingual Arabic tagger to identify
which words and phrases (in running Arabic)
are good candidates for transliteration (section
4.2)
6. Apply the tagger to test data and evaluate its
accuracy.
4.1 Mark-up of bitext
Given a tokenized (but unaligned and mixed-case)
bitext, we mark up that bitext with links between
Arabic and English words that appear to be translit-
erations. In the following example, linked words are
underlined, with numbers indicating what is linked.
English The meeting was attended by Omani (1)
Secretary of State for Foreign Affairs Yusif (2)
bin (3) Alawi (6) bin (8) Abdallah (10) and
Special Advisor to Sultan (12) Qabus (13)
for Foreign Affairs Umar (14) bin (17)
Abdul Munim (19) al-Zawawi (21).
Arabic (translit.) uHDr allqa? uzyr aldule
al?manY (1) llsh?uun alkharjye yusf (2) bn (3)
?luY (6) bn (8) ?bd allh (10) ualmstshar alkhaS
llslTan (12) qabus (13) ll?laqat alkharjye ?mr (14)
bn (17) ?bd almn?m (19) alzuauY (21) .
For each Arabic word, the linking algorithm tries
to find a matching word on the English side, using
the transliteration scoring model described in sec-
tion 3. If the matcher reaches the end of an Arabic
or English word before reaching the end of the other,
it continues to ?consume? additional words until a
word-boundary observing match is found or the cost
threshold exceeded.
When there are several viable linking alternatives,
the algorithm considers the cost provided by the
transliteration scoring model, as well as context to
eliminate inferior alternatives, so that for example
the different occurrences of the name particle bin
in the example above are linked to the proper Ara-
bic words, based on the names next to them. The
number of links depends, of course, on the specific
corpus, but we typically identify about 3.0 links per
sentence.
The algorithm is enhanced by a number of heuris-
tics:
393
 English match candidates are restricted to cap-
italized words (with a few exceptions).
 We use a list of about 200 Arabic and English
stopwords and stopword pairs.
 We use lists of countries and their adjective
forms to bridge cross-POS translations such
as Italy?s president on the English and ?J



KP
??A?K


A? @ (?Italian president?) on the Arabic side.
 Arabic prefixes such as ?/l- (?to?) are treated
in a special way, because they are translated,
not transliterated like the rest of the word. Link
(12) above is an example.
In this bitext mark-up process, we achieve 99.5%
precision and 95% recall based on a manual
visualization-tool based evaluation. Of the 5% re-
call error, 3% are due to noisy data in the bitext such
as typos, incorrect translations, or names missing on
one side of the bitext.
4.2 Training of Arabic name tagger
The task of the Arabic name tagger (or more
precisely, ?transliterate-me? tagger) is to predict
whether or not a word in an Arabic text should be
transliterated, and if so, whether it includes a prefix.
Prefixes such as ?/u- (?and?) have to be translated
rather than transliterated, so it is important to split
off any prefix from a name before transliterating that
name. This monolingual tagging task is not trivial,
as many Arabic words can be both a name and a non-
name. For example, ?QK


	
Qj
.
? @ (aljzyre) can mean both
Al-Jazeera and the island (or peninsula).
Features include the word itself plus two words
to the left and right, along with various prefixes,
suffixes and other characteristics of all of them, to-
talling about 250 features.
Some of our features depend on large corpus
statistics. For this, we divide the tagged Arabic
side of our training corpus into a stat section and
a core training section. From the stat section we col-
lect statistics as to how often every word, bigram or
trigram occurs, and what distribution of name/non-
name patterns these ngrams have. The name distri-
bution bigram

?K


P???@

?QK


	
Qj
.
? @ 3327 00:133 01:3193 11:1
(aljzyre alkurye/?peninsula Korean?) for example
tells us that in 3193 out of 3327 occurrences in the
stat corpus bitext, the first word is a marked up as
a non-name (?0?) and the second as a name (?1?),
which strongly suggests that in such a bigram con-
text, aljzyre better be translated as island or penin-
sula, and not be transliterated as Al-Jazeera.
We train our system on a corpus of 6 million stat
sentences, and 500; 000 core training sentences. We
employ a sequential tagger trained using the SEARN
algorithm (Daume? III et al, 2006) with aggressive
updates ( = 1). Our base learning algorithm
is an averaged perceptron, as implemented in the
MEGAM package2.
Reference Precision Recall F-meas.
Raw test corpus 87.4% 95.7% 91.4%
Adjusted for GS 92.1% 95.9% 94.0%
deficiencies
Table 1: Accuracy of ?transliterate-me? tagger
Testing on 10,000 sentences, we achieve preci-
sion of 87.4% and a recall of 95.7% with respect to
the automatically marked-up Gold Standard as de-
scribed in section 4.1. A manual error analysis of
500 sentences shows that a large portion are not er-
rors after all, but have been marked as errors because
of noise in the bitext and errors in the bitext mark-
up. After adjusting for these deficiencies in the gold
standard, we achieve precision of 92.1% and recall
of 95.9% in the name tagging task.
5 Integration with SMT
We use the following method to integrate our
transliterator into the overall SMT system:
1. We tag the Arabic source text using the tagger
described in the previous section.
2. We apply the transliterator described in section
3 to the tagged items. We limit this transliter-
ation to words that occur up to 50 times in the
training corpus for single token names (or up
to 100 and 150 times for two and three-word
names). We do this because the general SMT
mechanism tends to do well on more common
names, but does poorly on rare names (and will
2Freely available at http://hal3.name/megam
394
always drop names it has never seen in the
training bitext).
3. On the fly, we add transliterations to SMT
phrase table. Instead of a phrasal probability,
the transliterations have a special binary feature
set to 1. In a tuning step, the Minimim Error
Rate Training component of our SMT system
iteratively adjusts the set of rule weights, in-
cluding the weight associated with the translit-
eration feature, such that the English transla-
tions are optimized with respect to a set of
known reference translations according to the
BLEU translation metric.
4. At run-time, the transliterations then compete
with the translations generated by the gen-
eral SMT system. This means that the MT
system will not always use the transliterator
suggestions, depending on the combination of
language model, translation model, and other
component scores.
5.1 Multi-token names
We try to transliterate names as much as possible in
context. Consider for example the Arabic name:

?J


	
?? ?K
.
@
	
???K


(?yusf abu Sfye?)
If transliterated as single words without context,
the top results would be JosephjJosefjYusufjYosefj
Youssef, AbujAbojIvojApojIbo, and SephiajSofiaj
SophiajSafiehjSafia respectively. However, when
transliterating the three words together against our
list of 47 million English trigrams (section 3), the
transliterator will select the (correct) translation
Yousef Abu Safieh. Note that Yousef was not among
the top 5 choices, and that Safieh was only choice 4.
Similarly, when transliterating 	? A K
.
?

?? P@
	
P? ??
/umuzar ushuban (?and Mozart and Chopin?) with-
out context, the top results would be MoserjMauserj
MozerjMozartjMouser and ShuppanjShoppingj
SchwabenjSchuppanjShobana (with Chopin way
down on place 22). Checking our large English lists
for a matching name, name pattern, the transliterator
identifies the correct translation ?, Mozart, Chopin?.
Note that the transliteration module provides the
overall SMT system with up to 5 alternatives,
augmented with a choice of English translations
for the Arabic prefixes like the comma and the
conjunction and in the last example.
6 End-to-End results
We applied the NEWA metric (section 2) to both
our SMT translations as well as the four human ref-
erence translations, using both the original named-
entity translation annotation and the re-annotation:
Gold Standard BBN GS Re-annotated GS
Human 1 87.0% 85.0%
Human 2 85.3% 86.9%
Human 3 90.4% 91.8%
Human 4 86.5% 88.3%
SMT System 80.4% 89.7%
Table 2: Name translation accuracy with respect to BBN
and re-annotated Gold Standard on 1730 named entities
in 637 sentences.
Almost all scores went up with re-annotations, be-
cause the re-annotations more properly reward cor-
rect answers.
Based on the original annotations, all human
name translations were much better than our SMT
system. However, based on our re-annotation, the
results are quite different: our system has a higher
NEWA score and better name translations than 3 out
of 4 human annotators.
The evaluation results confirm that the original
annotation method produced a relative bias towards
the human translation its annotations were largely
based on, compared to other translations.
Table 3 provides more detailed NEWA results.
The addition of the transliteration module improves
our overall NEWA score from 87.8% to 89.7%, a
relative gain of 16% over base SMT system. For
names of persons (PER) and facilities (FAC), our
system outperforms all human translators. Hu-
mans performed much better on Person Nominals
(PER.Nom) such as Swede, Dutchmen, Americans.
Note that name translation quality varies greatly
between human translators, with error rates ranging
from 8.2-15.0% (absolute).
To make sure our name transliterator does not de-
grade the overall translation quality, we evaluated
our base SMT system with BLEU, as well as our
transliteration-augmented SMT system. Our stan-
dard newswire training set consists of 10.5 million
words of bitext (English side) and 1491 test sen-
395
NE Type Count Baseline SMT with Human 1 Human 2 Human 3 Human 4
SMT Transliteration
PER 342 266 (77.8%) 280 (81.9%) 210 (61.4%) 265 (77.5%) 278 (81.3%) 275 (80.4%)
GPE 910 863 (94.8%) 877 (96.4%) 867 (95.3%) 849 (93.3%) 885 (97.3%) 852 (93.6%)
ORG 332 280 (84.3%) 282 (84.9%) 263 (79.2%) 265 (79.8%) 293 (88.3%) 281 (84.6%)
FAC 27 18 (66.7%) 24 (88.9%) 21 (77.8%) 20 (74.1%) 22 (81.5%) 20 (74.1%)
PER.Nom 61 49 (80.3%) 48 (78.7%) 61 (100.0%) 56 (91.8%) 60 (98.4%) 57 (93.4%)
LOC 58 43 (74.1%) 41 (70.7%) 48 (82.8%) 48 (82.8%) 51 (87.9%) 43 (74.1%)
All types 1730 1519 (87.8%) 1552 (89.7%) 1470 (85.0%) 1503 (86.9%) 1589 (91.8%) 1528 (88.3%)
Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different named
entity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi-
zation (ORG), Facility (FAC), Nominal Person, e.g. Swede (PER.Nom), other location (LOC).
tences. The BLEU scores for the two systems were
50.70 and 50.96 respectively.
Finally, here are end-to-end machine translation
results for three sentences, with and without the
transliteration module, along with a human refer-
ence translation.
Old: Al-Basha leads a broad list of musicians such
as Bach.
New: Al-Basha leads a broad list of musical acts
such as Bach, Mozart, Beethoven, Chopin, Schu-
mann, Rachmaninoff, Ravel and Prokofiev.
Ref: Al-Bacha performs a long list of works by
composers such as Bach, Chopin, Beethoven,
Shumann, Rakmaninov, Ravel and Prokoviev.
Old: Earlier Israeli military correspondent turn
introduction programme ?Entertainment Bui?
New: Earlier Israeli military correspondent turn to
introduction of the programme ?Play Boy?
Ref: Former Israeli military correspondent turns
host for ?Playboy? program
Old: The Nikkei president company De Beers said
that ...
New: The company De Beers chairman Nicky Op-
penheimer said that ...
Ref: Nicky Oppenheimer, chairman of the De Beers
company, stated that ...
7 Discussion
We have shown that a state-of-the-art statistical ma-
chine translation system can benefit from a dedi-
cated transliteration module to improve the transla-
tion of rare names. Improved named entity transla-
tion accuracy as measured by the NEWA metric in
general, and a reduction in dropped names in par-
ticular is clearly valuable to the human reader of
machine translated documents as well as for sys-
tems using machine translation for further informa-
tion processing. At the same time, there has been no
negative impact on overall quality as measured by
BLEU.
We believe that all components can be further im-
proved, e.g.
 Automatically retune the weights in the
transliteration scoring model.
 Improve robustness with respect to typos, in-
correct or missing translations, and badly
aligned sentences when marking up bitexts.
 Add more features for learning whether or not
a word should be transliterated, possibly using
source language morphology to better identify
non-name words never or rarely seen during
training.
Additionally, our transliteration method could be ap-
plied to other language pairs.
We find it encouraging that we already outper-
form some professional translators in name transla-
tion accuracy. The potential to exceed human trans-
lator performance arises from the patience required
to translate names right.
Acknowledgment
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
396
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
Transliteration of Names in Arabic Text. In Proceed-
ings of the Association for Computational Linguistics
Workshop on Computational Approaches to Semitic
Languages.
Thorsten Brants, Alex Franz. 2006. Web 1T 5-gram
Version 1. Released by Google through the Linguis-
tic Data Consortium, Philadelphia, as LDC2006T13.
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and
Shih-Chung Tsai. 1998. Proper Name Translation in
Cross-Language Information Retrieval. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics.
Hal Daume? III, John Langford, and Daniel Marcu.
2006. Search-based Structured Prediction.
Submitted to the Machine Learning Journal.
http://pub.hal3.name/#daume06searn
David Day. 2007. Entity Translation 2007 Pilot Evalua-
tion (ET07). In proceedings of the Workshop on Auto-
matic Content Extraction (ACE). College Park, Mary-
land.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
Transliteration and Back-transliteration by Decision
Tree Learning. In Conference on Language Resources
and Evaluation.
Mehdi M. Kashani, Fred Popowich, and Fatiha Sadat.
2007. Automatic Transliteration of Proper Nouns
from Arabic to English. The Challenge of Arabic For
NLP/MT, 76-84.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics.
Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward Ma-
chine Transliteration by Learning Phonetic Similar-
ity. Sixth Conference on Natural Language Learning,
Taipei, Taiwan, 2002.
David Matthews. 2007. Machine Transliteration of
Proper Names. Master?s Thesis. School of Informat-
ics. University of Edinburgh.
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001.
Using the Web as a Bilingual Dictionary. In Proceed-
ings of the Workshop on Data-driven Methods in Ma-
chine Translation.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Irina
Temnikova, Anna Widiger, Wajdi Zaghouani, and Jan
Zizka. 2006. Multilingual Person Name Recognition
and Transliteration. CORELA - COgnition, REpre-
sentation, LAnguage, Poitiers, France. Volume 3/3,
number 2, pp. 115-123.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In Proceedings of the 45th An-
nual Meeting on Association for Computational Lin-
guistics.
Richard Sproat, ChengXiang Zhai, and Tao Tao. 2006.
Named Entity Transliteration with Comparable Cor-
pora. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational Lin-
guistics.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. In
Proceedings of the COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages.
Stephen Wan and Cornelia Verspoor. 1998. Automatic
English-Chinese Name Transliteration for Develop-
ment of Multilingual Resources. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics. Montreal, Canada.
397
Parsing and Question Classification for Question Answering
Ulf Hermjakob
Information Sciences Institute
University of Southern California
ulf@isi.edu
Abstract
This paper describes machine learning based
parsing and question classification for ques-
tion answering. We demonstrate that for
this type of application, parse trees have
to be semantically richer and structurally
more oriented towards semantics than what
most treebanks offer. We empirically show
how question parsing dramatically improves
when augmenting a semantically enriched
Penn treebank training corpus with an addi-
tional question treebank.
1 Introduction
There has recently been a strong increase in the re-
search of question answering, which identifies and ex-
tracts answers from a large collection of text. Un-
like information retrieval systems, which return whole
documents or larger sections thereof, question answer-
ing systems are designed to deliver much more fo-
cused answers, e.g.
Q: Where is Ayer?s Rock?
A: in central Australia
Q: Who was Gennady Lyachin?
A: captain of the Russian nuclear submarine Kursk
The August 2000 TREC-9 short form Q&A track eval-
uations, for example, specifically limited answers to
50 bytes.
The Webclopedia project at the USC Informa-
tion Sciences Institute (Hovy 2000, 2001) pursues a
semantics-based approach to answer pinpointing that
relies heavily on parsing. Parsing covers both ques-
tions as well as numerous answer sentence candidates.
After parsing, exact answers are extracted by matching
the parse trees of answer sentence candidates against
that of the parsed question. This paper describes the
critical challenges that a parser faces in Q&A applica-
tions and reports on a number of extensions of a deter-
ministic machine-learning based shift-reduce parser,
CONTEX (Hermjakob 1997, 2000), which was previ-
ously developed for machine translation applications.
In particular, section 2 describes how additional tree-
banking vastly improved parsing accuracy for ques-
tions; section 3 describes how the parse tree is ex-
tended to include the answer type of a question, a most
critical task in question answering; section 4 presents
experimental results for question parsing and QA typ-
ing; and finally, section 5 describes how the parse trees
of potential answer sentences are enhanced semanti-
cally for better question-answer matching.
2 Question Treebank
In question answering, it is particularly important
to achieve a high accuracy in parsing the questions.
There are often several text passages that contain an
answer, so if the parser does not produce a sufficiently
good parse tree for some of the answer sentences,
there?s still a good chance that the question can be an-
swered correctly based on other sentences containing
the answer. However, when the question is analyzed
incorrectly, overall failure is much more likely.
A scenario with a question in multiple variations,
as cleverly exploited by the SMU team (Harabagiu,
2000) in TREC9 for maybe about 10% of the 500 orig-
inal questions, is probably more of an anomaly and
can?t be assumed to be typical.
Parsing accuracy of trained parsers is known to
depend significantly on stylistic similarities between
training corpus and application text. In the Penn Tree-
bank, only about half a percent of all sentences from
the Wall Street Journal are (full) questions. Many of
these are rhetorical, such as ?So what?s the catch??
or ?But what about all those non-duck ducks flapping
over Washington??. Many types of questions that are
common in question answering are however severely
underrepresented. For example, there are no ques-
tions beginning with the interrogatives When or How
much and there are no para-interrogative imperative
sentences starting with ?Name?, as in Name a Gaelic
language.
This finding is of course not really surprising, since
newspaper articles focus on reporting and are there-
fore predominantly declarative. Therefore, we have to
expect a lower accuracy for parsing questions than for
parsing declarative sentences, if the parser was trained
on the Penn treebank only. This was confirmed by
preliminary question parsing accuracy tests using a
parser trained exclusively on sentences from the Wall
Street Journal. Question parsing accuracy rates were
significantly lower than for regular newspaper sen-
tences, even though one might have expected them to
be higher, given that questions, on average, tend to be
only half as long as newspaper sentences.
To remedy this shortcoming, we treebanked addi-
tional questions as we would expect them in question
answering. At this point, we have treebanked a total of
1153 questions, including
  all 38 prep questions for TREC 8,
  all 200 questions from TREC 8,
  all 693 questions from TREC 9,
  plus 222 questions from a travel guide phrase
book and online resources, including an-
swers.com.
The online questions cover a wider cross-section of
style, including yes-no questions (of which there
was only one in the TREC questions set), true-false
questions (none in TREC), and questions with wh-
determiner phrases1 (none in TREC). The additionally
treebanked questions therefore complement the TREC
questions.
The questions were treebanked using the determin-
istic shift-reduce parser CONTEX. Stepping through
a question, the (human) treebanker just hits the return
key if the proposed parse action is correct, and types
in the correct action otherwise. Given that the parser
predicts over 90% of all individual steps correctly, this
process is quite fast, most often significantly less than
a minute per question, after the parser was trained us-
ing the first one hundred treebanked questions.
The treebanking process includes a ?sanity check?
after the treebanking proper of a sentence. The san-
ity check searches the treebanked parse tree for con-
stituents with an uncommon sub-constituent structure
and flags them for human inspection. This helps to
eliminate most human errors. Here is an example of a
(slightly simplified) question parse tree. See section 5
for a discussion of how the trees differ from the Penn
Treebank II standard.
1
?What country?s national anthem does the movie
Casablanca close to the strains of??
[1] How much does one ton of cement cost?
[SNT,PRES,Qtarget: MONETARY-QUANTITY]
(QUANT) [2] How much [INTERR-ADV]
(MOD) [3] How [INTERR-ADV]
(PRED) [4] much [ADV]
(SUBJ LOG-SUBJ) [5] one ton of cement [NP]
(QUANT) [6] one ton [NP,MASS-Q]
(PRED) [7] one ton [NP-N,MASS-Q]
(QUANT) [8] one [CARDINAL]
(PRED) [9] ton [COUNT-NOUN]
(PRED) [10] of cement [PP]
(P) [11] of [PREP]
(PRED) [12] cement [NP]
(PRED) [13] cement [NOUN]
(PRED) [14] does cost [VERB,PRES]
(AUX) [15] does [AUX]
(PRED) [16] cost [VERB]
(DUMMY) [17] ? [QUESTION-MARK]
Figure 1: a simplified sample parse tree
3 QA Typing (?Qtargets?)
Previous research on question answering, e.g.
Srihari and Li (2000), has shown that it is important to
classify questions with respect to their answer types.
For example, given the question ?How tall is Mt. Ever-
est??, it is very useful to identify the answer type as a
distance quantity, which allows us to narrow our an-
swer search space considerably. We refer to such an-
swer types as Qtargets.
To build a very detailed question taxonomy,
Gerber (2001) has categorized 18,000 online questions
with respect to their answer type. From this we de-
rived a set of currently 115 elementary Qtargets, such
as distance quantity. For some questions, like ?Who is
the owner of CNN??, the answer might be one of two
or more distinct types of elementary Qtargets, such
as proper-person or proper-organization for the owner-
ship question. Including such combinations, the num-
ber of distinct Qtargets rises to 122.
Here are some more examples:
  Q1: How long would it take to get to Mars?
Qtarget: temporal-quantity
  Q2: When did Ferraro run for vice president?
Qtarget: date, temp-loc-with-year; =temp-loc
  Q3: Who made the first airplane?
Qtarget: proper-person, proper-company;
=proper-organization
  Q4: Who was George Washington?
Qtarget: why-famous-person
  Q5: Name the second tallest peak in Europe.
Qtarget: proper-mountain
Question 1 (Q1) illustrates that it is not sufficient
to analyze the wh-group of a sentence, since ?how
long? can also be used for questions targeting a
distance-quantity. Question 2 has a complex Qtarget,
giving first preference to a date or a temporal location
with a year and second preference to a general
temporal location, such as ?six years after she was
first elected to the House of Representatives?. The
equal sign (=) indicates that sub-concepts of temp-loc
such as time should be excluded from consideration
at that preference level. Question 3 & 4 both are
who-questions, however with very different Qtargets.
Abstract Qtargets such as the why-famous-person of
question 4, can have a wide range of answer types,
for example a prominent position or occupation, or
the fact that they invented or discovered something.
Abstract Qtargets have one or more arguments that
completely describe the question: ?Who was George
Washington??, ?What was George Washington best
known for??, and ?What made George Washington
famous?? all map to Qtarget why-famous-person,
Qargs (?George Washington?). Below is a listing of
all currently used abstract Qtargets:
Abstract Qtargets
  why-famous (What is Switzerland known for?
- 3 occurrences in TREC 8&9)
? why-famous-person (Who was Lacan? - 35)
  abbreviation-expansion (What does NAFTA stand
for? - 16)
  abbreviation (How do you abbreviate limited
partnership? - 5)
  definition (What is NAFTA? - 35)
  synonym (Aspartame is also known as what? - 6)
  contrast (What?s the difference between DARPA
and NSF? - 0)
The ten most common semantic Qtargets in the
TREC8&9 evaluations were
  proper-person (98 questions)
  at-location/proper-place (68)
  proper-person/proper-organization (68)
  date/temp-loc-with-year/date-range/temp-loc
(66)
  numerical-quantity (51)
  city (39)
  (other) named entity (20)
  temporal quantity (15)
  distance quantity (14)
  monetary quantity (12)
Some of the Qtargets occurring only once were
proper-American-football-sports-team, proper-planet,
power-quantity, proper-ocean, season, color, phone-
number, proper-hotel and government-agency.
The following Qtarget examples show the hierar-
chical structure of Qtargets:
Quantity
  energy-quantity (1)
  mass-quantity (6)
  monetary-quantity (12)
  numerical-quantity (51)
  power-quantity (1)
  spatial-quantity
? distance-quantity (14)
? area-quantity (3)
? volume-quantity (0)
  speed-quantity (2)
  temperature-quantity (2)
  temporal-quantity (15)
Besides the abstract and semantic (ontology-based)
Qtargets, there are two further types.
1. Qtargets referring to semantic role
Q: Why can?t ostriches fly?
Qtarget: (ROLE REASON)
This type of Qtarget recommends constituents
that have a particular semantic role with respect
to their parent constituent.
2. Qtargets referring to marked-up constituents
Q: Name a film in which Jude Law acted.
Qtarget: (SLOT TITLE-P TRUE)
This type of Qtarget recommends constituents
with slots that the parser can mark up. For exam-
ple, the parser marks constituents that are quoted
and consist of mostly and markedly capitalized
content words as potential titles.
The 122 Qtargets are computed based on a list of
276 hand-written rules.2 One reason why there are
relatively few rules per Qtarget is that, given a seman-
tic parse tree, the rules can be formulated at a high
level of abstraction. For example, parse trees offer an
abstraction from surface word order and CONTEX?s
semantic ontology, which has super-concepts such
as monetarily-quantifiable-abstract and sub-concepts
such as income, surplus and tax, allows to keep many
tests relatively simple and general.
For 10% of the TREC 8&9 evaluation questions,
there is no proper Qtarget in our current Qtarget hi-
erarchy. Some of those questions could be covered
by further enlarging and refining the Qtarget hierar-
chy, while others are hard to capture with a semantic
super-category that would narrow the search space in
a meaningful way:
  What does the Peugeot company manufacture?
  What do you call a group of geese?
  What is the English meaning of caliente?
2These numbers for Qtargets and rules are up by a factor
of about 2 from the time of the TREC9 evaluation.
# of Penn # of add. Q. Labeled Labeled Tagging Cr. Brackets Qtarget acc. Qtarget acc.
sentences sentences Precision Recall Accuracy per sent. (strict) (lenient)
2000 0 83.47% 82.49% 94.65% 0.34 63.0% 65.5%
3000 0 84.74% 84.16% 94.51% 0.35 65.3% 67.4%
2000 38 91.20% 89.37% 97.63% 0.26 85.9% 87.2%
3000 38 91.52% 90.09% 97.29% 0.26 86.4% 87.8%
2000 238 94.16% 93.39% 98.46% 0.21 91.9% 93.1%
2000 975 95.71% 95.45% 98.83% 0.17 96.1% 97.3%
Table 1: Parse tree accuracies for varying amounts and types of training data.
Total number of test questions per experiment: 1153
4 Experiments
In the first two test runs, the system was trained on
2000 and 3000 Wall Street Journal sentences (enriched
Penn Treebank). In runs three and four, we trained the
parser with the same Wall Street Journal sentences,
augmented by the 38 treebanked pre-TREC8 ques-
tions. For the fifth run, we further added the 200
TREC8 questions as training sentences when testing
TREC9 questions, and the first 200 TREC9 questions
as training sentences when testing TREC8 questions.
For the final run, we divided the 893 TREC-8 and
TREC-9 questions into 5 test subsets of about 179 for
a five-fold cross validation experiment, in which the
system was trained on 2000 WSJ sentences plus about
975 questions (all 1153 questions minus the approx-
imately 179 test sentences held back for testing). In
each of the 5 subtests, the system was then evaluated
on the test sentences that were held back, yielding a
total of 893 test question sentences.
The Wall Street Journal sentences contain a few
questions, often from quotes, but not enough and not
representative enough to result in an acceptable level
of question parsing accuracy. While questions are typ-
ically shorter than newspaper sentences (making pars-
ing easier), the word order is often markedly different,
and constructions like preposition stranding (?What
university was Woodrow Wilson President of??) are
much more common. The results in figure 1 show how
crucial it is to include additional questions when train-
ing a parser, particularly with respect to Qtarget accu-
racy.3 With an additional 1153 treebanked questions
as training input, parsing accuracy levels improve con-
siderably for questions.
5 Answer Candidate Parsing
A thorough question analysis is however only one
part of question answering. In order to do meaning-
ful matching of questions and answer candidates, the
3At the time of the TREC9 evaluation in August 2000,
only about 200 questions had been treebanked, including
about half of the TREC8 questions (and obviously none of
the TREC9 questions).
analysis of the answer candidate must reflect the depth
of analysis of the question.
5.1 Semantic Parse Tree Enhancements
This means, for example, that when the question ana-
lyzer finds that the question ?How long does it take to
fly from Washington to Hongkong?? looks for a tem-
poral quantity as a target, the answer candidate anal-
ysis should identify any temporal quantities as such.
Similarly, when the question targets the name of an
airline, such as in ?Which airlines offer flights from
Washington to Hongkong??, it helps to have the parser
identify proper airlines as such in an answer candidate
sentence.
For this we use an in-house preprocessor to iden-
tify constituents like the 13 types of quantities in sec-
tion 3 and for the various types of temporal loca-
tions. Our named entity tagger uses BBN?s Identi-
Finder(TM) (Kubala, 1998; Bikel, 1999), augmented
by a named entity refinement module. For named
entities (NEs), IdentiFinder provides three types of
classes, location, organization and person. For better
matching to our question categories, we need a finer
granularity for location and organization in particular.
  Location   proper-city, proper-country,
proper-mountain, proper-island, proper-star-
constellation, ...
  Organization   government-agency, proper-
company, proper-airline, proper-university,
proper-sports-team, proper-american-football-
sports-team, ...
For this refinement, we use heuristics that rely both
on lexical clues, which for example works quite well
for colleges, which often use ?College? or ?Univer-
sity? as their lexical heads, and lists of proper en-
tities, which works particularly well for more lim-
ited classes of named entities like countries and gov-
ernment agencies. For many classes like mountains,
lexical clues (?Mount Whitney?, ?Humphreys Peak?,
?Sassafras Mountain?) and lists of well-known enti-
ties (?Kilimanjaro?, ?Fujiyama?, ?Matterhorn?) com-
plement each other well. When no heuristic or back-
ground knowledge applies, the entity keeps its coarse
level designation (?location?).
For other Qtargets, such as ?Which animals are the
most common pets??, we rely on the SENSUS ontol-
ogy4 (Knight and Luk, 1994), which for example in-
cludes a hierarchy of animals. The ontology allows
us to conclude that the ?dog? in an answer sentence
candidate matches the Qtarget animal (while ?pizza?
doesn?t).
5.2 Semantically Motivated Trees
The syntactic and semantic structure of a sentence of-
ten differ. When parsing sentences into parse trees
or building treebanks, we therefore have to decide
whether to represent a sentence primarily in terms of
its syntactic structure, its semantic structure, some-
thing in between, or even both.
We believe that an important criterion for this deci-
sion is what application the parse trees might be used
for. As the following example illustrates, a semantic
representation is much more suitable for question an-
swering, where questions and answer candidates have
to be matched. What counts in question answering is
that question and answer match semantically. In pre-
vious research, we found that the semantic representa-
tion is also more suitable for machine translation ap-
plications, where syntactic properties of a sentence are
often very language specific and therefore don?t map
well to another language.
Parse trees [1] and [12] are examples of our sys-
tem?s structure, whereas [18] and [30] represent the
same question/answer pair in the more syntactically
oriented structure of the Penn treebank5 (Marcus
1993).
Question and answer in CONTEX format:
[1] When was the Berlin Wall opened?
[SNT,PAST,PASSIVE,WH-QUESTION,
Qtarget: DATE-WITH-YEAR,DATE,
TEMP-LOC-WITH-YEAR,TEMP-LOC]
(TIME) [2] When [INTERR-ADV]
(SUBJ LOG-OBJ) [3] the Berlin Wall [NP]
(DET) [4] the [DEF-ART]
(PRED) [5] Berlin Wall [PROPER-NAME]
(MOD) [6] Berlin [PROPER-NAME]
(PRED) [7] Wall [COUNT-NOUN]
(PRED) [8] was opened [VERB,PAST,PASSIVE]
(AUX) [9] was [VERB]
(PRED) [10] opened [VERB]
(DUMMY) [11] ? [QUESTION-MARK]
4SENSUS was developed at ISI and is an extension and
rearrangement of WordNet.
5All trees are partially simplified; however, a little bit
more detail is given for tree [1]. UPenn is in the process of
developing a new treebank format, which is more semanti-
cally oriented than their old one, and is closer to the CONTEX
format described here.
[12] On November 11, 1989, East Germany
opened the Berlin Wall. [SNT,PAST]
(TIME) [13] On November 11, 1989,
[PP,DATE-WITH-YEAR]
(SUBJ LOG-SUBJ) [14] East Germany
[NP,PROPER-COUNTRY]
(PRED) [15] opened [VERB,PAST]
(OBJ LOG-OBJ) [16] the Berlin Wall [NP]
(DUMMY) [17] . [PERIOD]
Same question and answer in PENN TREEBANK
format:
[18] When was the Berlin Wall opened? [SBARQ]
[19] When [WHADVP-1]
[20] was the Berlin Wall opened [SQ]
[21] was [VBD]
[22] the Berlin Wall [NP-SBJ-2]
[23] opened [VP]
[24] opened [VBN]
[25] -NONE- [NP]
[26] -NONE- [*-2]
[27] -NONE- [ADVP-TMP]
[28] -NONE- [*T*-1]
[29] ? [.]
[30] On November 11, 1989, East Germany
opened the Berlin Wall. [S]
[31] On November 11, 1989, [PP-TMP]
[32] East Germany [NP-SBJ]
[33] opened the Berlin Wall [VP]
[34] opened [VBD]
[35] the Berlin Wall [NP]
[36] . [.]
The ?semantic? trees ([1] and [12]) have explicit
roles for all constituents, a flatter structure at the sen-
tence level, use traces more sparingly, separate syn-
tactic categories from information such as tense, and
group semantically related words, even if they are non-
contiguous at the surface level (e.g. verb complex [8]).
In trees [1] and [12], semantic roles match at the top
level, whereas in [18] and [30], the semantic roles are
distributed over several layers.
Another example for differences between syntac-
tic and semantic structures are the choice of the head
in a prepositional phrase (PP). For all PPs, such as
on Nov. 11, 1989, capital of Albania and [composed]
by Chopin, we always choose the noun phrase as the
head, while syntactically, it is clearly the preposition
that heads a PP.
We restructured and enriched the Penn treebank into
such a more semantically oriented representation, and
also treebanked the 1153 additional questions in this
format.
6 Conclusion
We showed that question parsing dramatically im-
proves when complementing the Penn treebank train-
ing corpus with an additional treebank of 1153 ques-
tions. We described the different answer types (?Qtar-
gets?) that questions are classified as and presented
how we semantically enriched parse trees to facilitate
question-answer matching.
Even though we started our Webclopedia project
only five months before the TREC9 evaluation, our
Q&A system received an overall Mean Reciprocal
Rank of 0.318, which put Webclopedia in essentially
tied second place with two others. (The best system
far outperformed those in second place.) During the
TREC9 evaluation, our deterministic (and therefore
time-linear) CONTEX parser robustly parsed approx-
imately 250,000 sentences, successfully producing a
full parse tree for each one of them.
Since then we scaled up question treebank from 250
to 1153; roughly doubled the number of Qtarget types
and rules; added more features to the machine-learning
based parser; did some more treebank cleaning; and
added more background knowledge to our ontology.
In the future, we plan to refine the Qtarget hierarchy
even further and hope to acquire Qtarget rules through
learning.
We plan to make the question treebank publicly
available.
References
D. Bikel, R. Schwartz and R. Weischedel. 1999. An
Algorithm that Learns What?s in a Name. In Ma-
chine Learning ? Special Issue on NL Learning, 34,
1-3.
Laurie Gerber. 2001. A QA Typology for Webclope-
dia. In prep.
Sanda Harabagiu, Marius Pasca and Steven Maiorano
2000. Experiments with Open-Domain Textual
Question Answering In Proceedings of COLING-
2000, Saarbru?cken.
Ulf Hermjakob and R. J. Mooney. 1997. Learn-
ing Parse and Translation Decisions From Examples
With Rich Context. In 35th Proceedings of the ACL,
pages 482-489.
file://ftp.cs.utexas.edu/pub/mooney/papers/con tex-
acl-97.ps.gz
Ulf Hermjakob. 2000. Rapid Parser Development: A
Machine Learning Approach for Korean. In Pro-
ceedings of the North American chapter of the As-
sociation for Computational Linguis tics (NA-ACL-
2000)
http://www.isi.edu/?ulf/papers/kor naacl00.ps.gz
Ed Hovy, L. Gerber, U. Hermjakob, M. Junk, C.-Y.
Lin 2000. Question Answering in Webclopedia
In Proceedings of the TREC-9 Conference, NIST.
Gaithersburg, MD
Ed Hovy, L. Gerber, U. Hermjakob, C.-Y. Lin, D.
Ravichandran 2001. Towards Semantics-Based
Answer Pinpointing In Proceedings of the HLT
2001 Conference, San Diego
K. Knight, S. Luc, et al 1994. Building a Large-Scale
Knowledge Base for Machine Translation. In Pro-
ceedings of the American Association of Artificial
Intelligence AAAI-94. Seattle, WA.
Francis Kubala, Richard Schwartz, Rebecca Stone,
Ralph Weischedel (BBN). 1998. Named Entity
Extraction from Speech. In 1998 DARPA Broadcast
News Transcription and Understanding Workshop
http://www.nist.gov/speech/publications/darpa
98/html/lm50/lm50.htm
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics 19(2), pages 313?330.
Ellen M. Voorhees and Dawn M. Tice. 2000. The
TREC-8 question answering track evaluation. In
E. M. Voorhees and D. K. Harman, editors, Pro-
ceedings of the Eighth Text REtrieval Conference
(TREC-8 ). http://trec.nist.gov/pubs.html
R. Srihari, C. Niu, and W. Li. 2000. A Hybrid Ap-
proach for Named Entity and Sub-Type Tagging. In
Proceedings of the conference on Applied Natural
Language Processing (ANLP 2000), Seattle.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425?429,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Aligning English Strings with Abstract Meaning Representation Graphs
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{damghani,yanggao,ulf,knight}@isi.edu
Abstract
We align pairs of English sentences and
corresponding Abstract Meaning Repre-
sentations (AMR), at the token level. Such
alignments will be useful for downstream
extraction of semantic interpretation and
generation rules. Our method involves
linearizing AMR structures and perform-
ing symmetrized EM training. We obtain
86.5% and 83.1% alignment F score on de-
velopment and test sets.
1 Introduction
Banarescu et al. (2013) describe a semantics bank
of English sentences paired with their logical
meanings, written in Abstract Meaning Represen-
tation (AMR). The designers of AMR leave open
the question of how meanings are derived from
English sentences (and vice-versa), so there are
no manually-annotated alignment links between
English words and AMR concepts. This paper
studies how to build such links automatically, us-
ing co-occurrence and other information. Auto-
matic alignments may be useful for downstream
extraction of semantic interpretation and genera-
tion rules.
AMRs are directed, acyclic graphs with labeled
edges, e.g., the sentence The boy wants to go is
represented as:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
We have hand-aligned a subset of the 13,050
available AMR/English pairs. We evaluate our
automatic alignments against this gold standard.
A sample hand-aligned AMR is here (??n? speci-
fies a link to the nth English word):
the boy wants to go
(w / want-01?3
:arg0 (b / boy?2)
:arg1 (g / go-01?5
:arg0 b))
This alignment problem resembles that of statisti-
cal machine translation (SMT). It is easier in some
ways, because AMR and English are highly cog-
nate. It is harder in other ways, as AMR is graph-
structured, and children of an AMR node are un-
ordered. There are also fewer available training
pairs than in SMT.
One approach is to define a generative model
from AMR graphs to strings. We can then use
EM to uncover hidden derivations, which align-
ments weakly reflect. This approach is used in
string/string SMT (Brown et al., 1993). How-
ever, we do not yet have such a generative graph-
to-string model, and even if we did, there might
not be an efficient EM solution. For exam-
ple, in syntax-based SMT systems (Galley et al.,
2004), the generative tree/string transduction story
is clear, but in the absence of alignment con-
straints, there are too many derivations and rules
for EM to efficiently consider.
We therefore follow syntax-based SMT custom
and use string/string alignment models in align-
ing our graph/string pairs. However, while it is
straightforward to convert syntax trees into strings
data (by taking yields), it is not obvious how to do
this for unordered AMR graph elements. The ex-
ample above also shows that gold alignment links
reach into the internal nodes of AMR.
Prior SMT work (Jones et al., 2012) describes
alignment of semantic graphs and strings, though
their experiments are limited to the GeoQuery do-
main, and their methods are not described in de-
tail. Flanigan et al (2014) describe a heuristic
AMR/English aligner. While heuristic aligners
can achieve good accuracy, they will not automat-
ically improve as more AMR/English data comes
425
online.
The contributions of this paper are:
? A set of gold, manually-aligned
AMR/English pairs.
? An algorithm for automatically aligning
AMR/English pairs.
? An empirical study establishing alignment
accuracy of 86.5% and 83.1% F score for de-
velopment and test sets respectively.
2 Method
We divide the description of our method into three
parts: preprocessing, training, and postprocessing.
In the preprocessing phase, we linearize the AMR
graphs to change them into strings, clean both the
AMR and English sides by removing stop words
and simple stemming, and add a set of correspond-
ing AMR/English token pairs to the corpus to help
the training phase. The training phase is based
on IBM models, but we modify the learning algo-
rithm to learn the parameters symmetrically. Fi-
nally, in the postprocessing stage we rebuild the
aligned AMR graph. These components are de-
scribed in more detail below.
2.1 Preprocessing
The first step of the preprocessing component is to
linearize the AMR structure into a string. In this
step we record the original structure of nodes in
the graph for later reconstruction of AMR. AMR
has a rooted graph structure. To linearize this
graph we run a depth first search from the root and
print each node as soon as it it visited. We print
but not expand the nodes that are seen previously.
For example the AMR:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
is linearized into this order: w / want-01 :arg0 b /
boy :arg1 g / go-01 :arg0 b.
Note that semantically related nodes often stay
close together after linearization.
After linearizing the AMR graph into a string,
we perform a series of preprocessing steps includ-
ing lowercasing the letters, removing stop words,
and stemming.
The AMR and English stop word lists are gen-
erated based on our knowledge of AMR design.
We know that tokens like an, the or to be verbs
will very rarely align to any AMR token; similarly,
AMR role tokens like :arg0, :quant, :opt1 etc. as
well as the instance-of token /, and tokens like
temporal-quantity or date-entity rarely align to any
English token. We remove these tokens from the
parallel corpus, but remember their position to be
able to convert the resulting string/string align-
ment back into a full AMR graph/English string
alignment. Although some stopwords participate
in gold alignments, by removing them we will buy
a large precision gain for some recall cost.
We remove the word sense indicator and quo-
tation marks for AMR concepts. For instance we
will change want-01 to want and ?ohio? to ohio.
Then we stem AMR and English tokens into their
first four letters, except for role tokens in AMR.
The purpose of stemming is to normalize English
morphological variants so that they are easier to
match to AMR tokens. For example English to-
kens wants, wanting, wanted, and want as well as
the AMR token want-01 will all convert to want
after removing the AMR word sense indicator and
stemming.
In the last step of preprocessing, we benefit
from the fact that AMR concepts and their cor-
responding English ones are frequently cognates.
Hence, after stemming, an AMR token often can
be translated to a token spelled similarly in En-
glish. This is the case for English token want and
AMR token want in the previous paragraph. To
help the training model learn from this fact, we
extend our sentence pair corpus with the set of
AMR/English token pairs that are spelled identi-
cally after preprocessing. Also, for English tokens
that can be translated into multiple AMR tokens,
like higher and high :degree more we add the cor-
responding string/string pairs to the corpus. This
set is extracted from existing lexical resources, in-
cluding lists of comparative/superlative adjectives,
negative words, etc.
After preprocessing, the AMR at the start of
this section will change into: want boy go and
the sentence The boy wants to go changes into boy
want to go, and we will also add the identity pairs
want/want, boy/boy, and go/go to the corpus.
2.2 Training
Our training method is based on IBM word align-
ment models (Brown et al., 1993). We modify
the objective functions of the IBM models to en-
426
courage agreement between learning parameters
in English-to-AMR and AMR-to-English direc-
tions of EM. The solution of this objective func-
tion can be approximated in an extremely simple
way that requires almost no extra coding effort.
Assume that we have a set of sentence pairs
{(E,A)}, where each E is an English sentence
and each A is a linearized AMR. According to
IBM models, A is generated from E through a
generative story based on some parameters.
For example, in IBM Model 2, given E we
first decide the length of A based on some prob-
ability l = p(len(A)|len(E)), then we decide
the distortions based on a distortion table: d =
p(i|j, len(A), len(E)). Finally, we translate En-
glish tokens into AMR ones based on a translation
table t = p(a|e) where a and e are AMR and En-
glish tokens respectively.
IBM models estimate these parameters to max-
imize the conditional likelihood of the data:
?
A|E
= argmaxL
?
A|E
(A|E) or ?
E|A
=
argmaxL
?
E|A
(E|A) where ? denotes the set of
parameters. The conditional likelihood is intrinsic
to the generative story of IBM models. However,
word alignment is a symmetric problem. Hence it
is more reasonable to estimate the parameters in a
more symmetric manner.
Our objective function in the training phase is:
?
A|E
, ?
E|A
= argmaxL
?
A|E
(A|E)+L
?
E|A
(E|A)
subject to ?
A|E
?
E
= ?
E|A
?
A
= ?
A,E
We approximate the solution of this objective
function with almost no change to the existing
implementation of the IBM models. We relax
the constraint to ?
A|E
= ?
E|A
, then apply the
following iterative process:
1. Optimize the first part of the objective func-
tion: ?
A|E
= argmaxL
?
A|E
(A|E) using EM
2. Satisfy the constraint: set ?
E|A
? ?
A|E
3. Optimize the second part of the objective
function: ?
E|A
= argmaxL
?
E|A
(E|A)
using EM
4. Satisfy the constraint: set ?
A|E
? ?
E|A
5. Iterate
Note that steps 1 and 3 are nothing more than
running the IBM models, and steps 2 and 4 are
just initialization of the EM parameters, using ta-
bles from the previous iteration. The initialization
steps only make sense for the parameters that in-
volve both sides of the alignment (i.e., the transla-
tion table and the distortion table). For the trans-
lation table we set t
E|A
(e|a) = t
A|E
(a|e) for En-
glish and AMR tokens e and a and then normalize
the t table. The distortion table can also be initial-
ized in a similar manner. We initialize the fertility
table with its value in the previous iteration.
Previously Liang et al. (2006) also presented a
symmetric method for training alignment parame-
ters. Similar to our work, their objective function
involves summation of conditional likelihoods in
both directions; however, their constraint is on
agreement between predicted alignments while we
directly focus on agreement between the parame-
ters themselves. Moreover their method involves a
modification of the E step of EM algorithm which
is very hard to implement for IBM Model 3 and
above.
After learning the parameters, alignments are
computed using the Viterbi algorithm in both di-
rections of the IBM models. We tried merging
the alignments of the two directions using meth-
ods like grow-diag-final heuristic or taking inter-
section of the alignments and adding some high
probability links in their union. But these methods
did not help the alignment accuracy.
2.3 Postprocessing
The main goal of the postprocessing component is
to rebuild the aligned AMR graph. We first insert
words removed as stop words into their positions,
then rebuild the graph using the recorded original
structure of the nodes in the AMR graph.
We also apply a last modification to the align-
ments in the postprocessing. Observing that pairs
like worker and person :arg0-of work-01 appear
frequently, and in all such cases, all the AMR to-
kens align to the English one, whenever we see
any of AMR tokens person, product, thing or com-
pany is followed by arg0-of, arg1-of or arg2-of
followed by an AMR concept, we align the two
former tokens to what the concept is aligned to.
3 Experiments
3.1 Data Description
Our data consists of 13,050 publicly available
AMR/English sentence pairs
1
. We have hand
1
LDC AMR release 1.0, Release date: June 16, 2014
https://catalog.ldc.upenn.edu/LDC2014T12
427
aligned 200 of these pairs to be used as develop-
ment and test sets
2
. We train the parameters on
the whole data. Table 1 presents a description of
the data. We do not count parenthesis, slash and
AMR variables as AMR tokens. Role tokens are
those AMR tokens that start with a colon. They
do not represent any concept, but provide a link
between concepts. For example in:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
the first :arg0 states that the first argument of the
concept wanting is the boy and the second argu-
ment is going.
train dev test
Sent. pairs 13050 100 100
AMR tokens 465 K 3.8 K (52%) 2.3 K (%55)
AMR role tokens 226 K 1.9 K (23%) 1.1 K (%22)
ENG tokens 248 K 2.3 K (76%) 1.7 K (%74)
Table 1: AMR/English corpus. The number in
parentheses is the percent of the tokens aligned in
gold annotation. Almost half of AMR tokens are
role tokens, and less than a quarter of role tokens
are aligned.
3.2 Experiment Results
We use MGIZA++ (Gao and Vogel, 2008) as
the implementation of the IBM models. We run
Model 1 and HMM for 5 iterations each, then run
our training algorithm on Model 4 for 4 iterations,
at which point the alignments become stable. As
alignments are usually many to one from AMR to
English, we compute the alignments from AMR to
English in the final step.
Table 2 shows the alignment accuracy for
Model 1, HMM, Model 4, and Model 4 plus the
modification described in section 2.2 (Model 4+).
The alignment accuracy on the test set is lower
than the development set mainly because it is in-
trinsically a harder set, as we only made small
modifications to the system based on the develop-
ment set. Recall error due to stop words is one
difference.
2
The development and test AMR/English pairs can be
found in /data/split/dev/amr-release-1.0-dev-consensus.txt
and /data/split/test/amr-release-1.0-test-consensus.txt, re-
spectively. The gold alignments are not included in these
files but are available separately.
model precision recall F score
Dev
Model 1 70.9 71.1 71.0
HMM 87.6 80.1 83.7
Model 4 89.7 80.4 84.8
Model 4+ 94.1 80.0 86.5
Test
Model 1 74.8 71.8 73.2
HMM 83.8 73.8 78.5
Model 4 85.8 74.9 80.0
Model 4+ 92.4 75.6 83.1
Table 2: Results on different models. Our training
method (Model 4+) increases the F score by 1.7
and 3.1 points on dev and test sets respectively.
Table 3 breaks down precision, recall, and
F score for role and non-role AMR tokens, and
also shows in parentheses the amount of recall er-
ror that was caused by removing either side of the
alignment as a stop word.
token type precision recall F score
Dev
role 77.1 48.7 59.7
non-role 97.2 88.2 92.5
all 94.1 80.0 (34%) 86.5
Test
role 71.0 37.8 49.3
non-role 95.5 84.7 89.8
all 92.4 75.6 (36%) 83.1
Table 3: Results breakdown into role and non-
role AMR tokens. The numbers in the parentheses
show the percent of recall errors caused by remov-
ing aligned tokens as stop words.
While the alignment method works very well on
non-role tokens, it works poorly on the role tokens.
Role tokens are sometimes matched with a word
or part of a word in the English sentence. For ex-
ample :polarity is matched with the un part of the
word unpopular, :manner is matched with most
adverbs, or even in the pair:
thanks
(t / thank-01
:arg0 (i / i)
:arg1 (y / you))
all AMR tokens including :arg0 and :arg1 are
matched to the only English word thanks. Incon-
sistency in aligning role tokens has made this a
hard problem even for human experts.
428
4 Conclusions and Future Work
In this paper we present the first set of manually
aligned English/AMR pairs, as well as the first
published system for learning the alignments be-
tween English sentences and AMR graphs that
provides a strong baseline for future research in
this area. As the proposed system learns the
alignments automatically using very little domain
knowledge, it can be applied in any domain and
for any language with minor adaptations.
Computing the alignments between English
sentences and AMR graphs is a first step for ex-
traction of semantic interpretation and generation
rules. Hence, a natural extension to this work
will be automatically parsing English sentences
into AMR and generating English sentences from
AMR.
Acknowledgments
This work was supported by DARPA con-
tracts HR0011-12-C-0014 and FA-8750-13-2-
0045. The authors would like to thank David Chi-
ang, Tomer Levinboim, and Ashish Vaswani (in
no particular order) for their comments and sug-
gestions.
References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Linguistic Annotation Workshop
(LAW VII-ID), ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing Workshop, ACL.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
429
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
