Automated Induction of Sense in Context
James PUSTEJOVSKY, Patrick HANKS, Anna RUMSHISKY
Brandeis University
Waltham, MA 02454, USA
{jamesp,patrick,arum}@cs.brandeis.edu
Abstract
In this paper, we introduce a model for sense as-
signment which relies on assigning senses to the
contexts within which words appear, rather than to
the words themselves. We argue that word senses
as such are not directly encoded in the lexicon
of the language. Rather, each word is associated
with one or more stereotypical syntagmatic pat-
terns, which we call selection contexts. Each selec-
tion context is associated with a meaning, which
can be expressed in any of various formal or com-
putational manifestations. We present a formalism
for encoding contexts that help to determine the
semantic contribution of a word in an utterance.
Further, we develop a methodology through which
such stereotypical contexts for words and phrases
can be identified from very large corpora, and sub-
sequently structured in a selection context dictio-
nary, encoding both stereotypical syntactic and se-
mantic information. We present some preliminary
results.
1 Introduction
This paper describes a new model for the acquisi-
tion and exploitation of selectional preferences for
predicates from natural language corpora. Our goal
is to apply this model in order to construct a dic-
tionary of normal selection contexts for natural lan-
guage; that is, a computational lexical database of
rich selectional contexts, associated with procedures
for assigning interpretations on a probabilistic basis
to less normal contexts. Such a semi-automatically
developed resource promises to have applications for
a number of NLP tasks, including word-sense disam-
biguation, selectional preference acquisition, as well
as anaphora resolution and inference in specialized
domains. We apply this methodology to a selected
set of verbs, including a subset of the verbs in the
Senseval 3 word sense discrimination task and report
our initial results.
1.1 Selectional Preference Acquisition:
Current State of the Art
Predicate subcategorization information constitutes
an essential part of the computational lexicon entry.
In recent years, a number of approaches have been
proposed for dealing computationally with selec-
tional preference acquisition (Resnik (1996); Briscoe
and Carroll (1997); McCarthy (1997); Rooth et al
(1999); Abney and Light (1999); Ciaramita and
Johnson (2000); Korhonen (2002)).
The currently available best algorithms developed
for the acquisition of selectional preferences for pred-
icates are induction algorithms modeling selectional
behavior as a distribution over words (cf. Abney and
Light (1999)). Semantic classes assigned to predi-
cate arguments in subcategorization frames are ei-
ther derived automatically through statistical clus-
tering techniques (Rooth et al (1999), Light and
Greiff (2002)) or assigned using hand-constructed
lexical taxonomies such as the WordNet hierarchy or
LDOCE semantic classes. Overwhelmingly, Word-
Net is chosen as the default resource for dealing with
the sparse data problem (Resnik (1996); Abney and
Light (1999); Ciaramita and Johnson (2000); Agirre
and Martinez (2001); Clark and Weir (2001); Carroll
and McCarthy (2000); Korhonen and Preiss (2003)).
Much of the work on inducing selectional prefer-
ences for verbs from corpora deals with predicates in-
discriminately, assuming no differentiation between
predicate senses (Resnik (1996); Abney and Light
(1999); Ciaramita and Johnson (2000); Rooth et al
(1999)). Those approaches that do distinguish be-
tween predicate senses or complementation patterns
in acquisition of selectional constraints (Korhonen
(2002); Korhonen and Preiss (2003)) do not use cor-
pus analysis for verb sense classification.
1.2 Word Sense Disambiguation: Current
State of the Art
Previous computational concerns for economy of
grammatical representation have given way to mod-
els of language that not only exploit generative
grammatical resources but also have access to large
lists of contexts of linguistic items (words), to which
new structures can be compared in new usages.
However, following the work of Yarowsky (1992),
Yarowsky (1995), many supervised WSD systems
use minimal information about syntactic structures,
for the most part restricting the notion of con-
text to topical and local features. Topical features
track open-class words that appear within a cer-
tain window around a target word, and local fea-
tures track small N-grams associated with the tar-
get word. Disambiguation therefore relies on word
co-occurrence statistics, rather than on structural
similarities. That remains the case for most systems
that participated in Senseval-2 (Preiss and Yarowsky
(2001)). Some recent work (Stetina et al (1998);
Agirre et al (2002); Yamashita et al (2003)) at-
tempts to change this situation and presents a di-
rected effort to investigate the impact of using syn-
tactic features for WSD learning algorithms. Agirre
et al(2002) and Yamashita et al (2003) report re-
sulting improvement in precision.
Stevenson and Wilks (2001) propose a somewhat
related technique to handle WSD, based on inte-
grating LDOCE classes with simulated annealing.
Although space does not permit discussion here, ini-
tial comparisons suggest that our selection contexts
could incorporate similar knowledge resources; it is
not clear what role model bias plays in associating
patterns with senses, however.
In this paper we modify the notion of word sense,
and at the same time revise the manner in which
senses are encoded. The notion of word sense that
has been generally adopted in the literature is an
artifact of several factors in the status quo, notably
the availability of lexical resources such as machine-
readable dictionaries, in which fine sense distinctions
are not supported by criteria for selecting one sense
rather than another, and WordNet, where synset
groupings are taken as defining word sense distinc-
tions. Thus, for instance, Senseval-2 WSD tasks re-
quired disambiguation using WordNet senses (see,
e.g., discussion in Palmer et al (2004)). The feature
sets used in the supervised WSD algorithms at best
use only minimal information about the typing of ar-
guments. The approach we adopt, Corpus Pattern
Analysis (CPA) (Pustejovsky and Hanks (2001)),
incorporates semantic features of the arguments of
the target word. Semantic features are expressed in
terms of a restricted set of shallow types, chosen for
their prevalence in selection context patterns. This
type system is extended with predicate-based noun
clustering, in the bootstrapping process described
below.
1.3 Related Resources: FrameNet
It is necessary to say a few words about the dif-
ferences between CPA and FrameNet. The CPA
approach has its origins in the analysis of large
corpora for lexicographic purposes (e.g. Cobuild
(Sinclair et al, 1987)) and in systemic-functional
grammar, in particular in Halliday?s notion of ?lexis
as a linguistic level? (Halliday, 1966) and Sin-
clair?s empirical approach to collocational anal-
ysis (Sinclair, 1991). FrameNet (freely avail-
able online in a beautifully designed data base at
http://www.icsi.berkeley.edu/?framenet/), is an attempt to
implement Fillmore?s 1975 proposal that, instead of
seeking to satisfy a set of necessary and sufficient
conditions, the meanings of words in text should be
analyzed by calculating resemblance to a prototype
(Fillmore, 1975).
CPA (Hanks, 2004) is concerned with establishing
prototypical norms of usage for individual words. It
is possible (and certainly desirable) that CPA norms
will be mappable onto FrameNet?s semantic frames
(for which see the whole issue of the International
Journal of Lexicography for September 2003 (in par-
ticular Atkins et al (2003a), Atkins et al (2003b),
Fillmore et al (2003a), Baker et al (2003), Fillmore
et al (2003b)). In frame semantics, the relationship
between semantics and syntactic realization is often
at a comparatively deep level, i.e. in many sentences
there are elements that are potentially present but
not actually expressed. For example, in the sentence
?he risked his life?, two semantic roles are expressed
(the risker and the valued object ?his life? that is put
at risk). But at least three other roles are sublim-
inally present although not expressed: the possible
bad outcome (?he risked his death?), the beneficiary
or goal (?he risked his life for her/for a few dollars?),
and the means (?he risked a backward glance?).
CPA, on the other hand, is shallower and more
practical: the objective is to identify, in relation to
a given target word, the overt textual clues that
activate one or more components of its meaning
potential. There is also a methodological differ-
ence: whereas FrameNet research proceeds frame by
frame, CPA proceeds word by word. This means
that when a word has been analysed in CPA the
patterns are immediately available for disambigua-
tion. FrameNet will be usable for disambiguation
only when all frames have been completely analysed.
Even then, FrameNet?s methodology, which requires
the researchers to think up all possible members of
a Frame a priori, means that important senses of
words that have been partly analysed are missing
and may continue to be missing for years to come.
There is no attempt in FrameNet to identify the
senses of each word systematically and contrastively.
In its present form, at least, FrameNet has at least
as many gaps as senses. For example, at the time
of writing toast is shown as part of the Apply Heat
frame but not the Celebrate frame. It is not clear
how or whether the gaps are to be filled systemat-
ically. We do not even know whether there is (or
is going to be) a Celebrate frame and if so what it
will be called. What is needed is a principled fix ? a
decision to proceed from evidence, not frames. This
is ruled out by FrameNet for principled reasons: the
unit of analysis for FrameNet is the frame, not the
word.
2 CPA Methodology
The Corpus Pattern Analysis (CPA) technique uses
a semi-automatic bootstrapping process to produce
a dictionary of selection contexts for predicates
in a language. Word senses for verbs are distin-
guished through corpus-derived syntagmatic pat-
terns mapped to Generative Lexicon Theory (Puste-
jovsky (1995)) as a linguistic model of interpreta-
tion, which guides and constrains the induction of
senses from word distributional information. Each
pattern is specified in terms of lexical sets for each
argument, shallow semantic typing of these sets, and
other syntagmatically relevant criteria (e.g., adver-
bials of manner, phrasal particles, genitives, nega-
tives).
The procedure consists of three subtasks: (1) the
manual discovery of selection context patterns for
specific verbs; (2) the automatic recognition of in-
stances of the identified patterns; and (3) automatic
acquisition of patterns for unanalyzed cases. Ini-
tially, a number of patterns are manually formulated
by a lexicographer through corpus pattern analysis
of about 500 occurrences of each verb lemma. Next,
for higher frequency verbs, the remaining corpus oc-
currences are scrutinized to see if any low-frequency
patterns have been missed. The patterns are then
translated into a feature matrix used for identifying
the sense of unseen instances for a particular verb.
In the remainder of this section, we describe these
subtasks in more detail. The following sections ex-
plain the current status of the implementation of
these tasks.
2.1 Lexical Discovery
Norms of usage are captured in what we call selec-
tion context patterns. For each lemma, contexts
of usage are sorted into groups, and a stereotypi-
cal CPA pattern that captures the relevant seman-
tic and syntactic features of the group is recorded.
Many patterns have alternations, recorded in satel-
lite CPA patterns. Alternations are linked to
the main CPA pattern through the same sense-
modifying mechanisms as those that allow for ex-
ploitations (coercions) of the norms of usage to be
understood. For example, here is the set of pat-
terns for the verb treat. Note that these patterns
do not capture all possible uses, and other patterns
may be added, e.g. if additional evidence is found
in domain-specific corpora.
(1) CPA Pattern set for treat:
I. [[Person 1]] treat [[Person 2]] ({at | in} [[Location]])
(for [[Event = Injury | Ailment]]); NO [Adv[Manner]]
II. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
IIIa. [[Person]] treat [[TopType 1]] {{as | like} [[TopType 2]]}
IIIb. [[Person]] treat [[TopType]] {{as if | as though | like}
[CLAUSE]}
IV. [[Person 1]] treat [[Person 2]] {to [[Event]]}
V. [[Person]] treat [[PhysObj | Stuff 1]] (with [[Stuff 2]])
There may be several patterns realizing a single
sense of a verb, as in (IIIa/IIIb) above. Also, there
may be several equivalent alternations or there may
be a stereotype. Note that alternations are different
realizations of the same norm, not exploitations (i.e.,
not coercions).
(2) Alternations for treat Pattern 1 :
[[Person 1]] treat [[Person 2]] ({at | in} [[Hospital]])
(for [[Injury | Ailment]]); NO [Adv[Manner]]
Alternation 1:
[[Person 1 <--> Medicament | Med-Procedure | Institution]]
Alternation 2:
[[Person 2 <--> Injury | Ailment | Bodypart]]
CPA Patterns
A CPA pattern extends the traditional notion of se-
lectional context to include a number of other con-
textual features, such as minor category parsing and
subphrasal cues. Accurate identification of the se-
mantically relevant aspects of a pattern is not an
obvious and straightforward procedure, as has some-
times been assumed in the literature. For example,
the presence or absence of an adverbial of manner in
the third valency slot around a verb can dramatically
alter the verb?s meaning. Simple syntactic encoding
of argument structure, for instance, is insufficient to
discriminate between the two major senses of the
verb treat, as illustrated below.
(3) a. They say their bosses treat them with respect.
b. Such patients are treated with antibiotics.
The ability to recognize the shallow semantic type
of a phrase in the context of a predicate is of course
crucial ?for example, in (3a) recognizing the PP as
(a) an adverbial, and (b) an adverbial of manner,
rather than an instrumental co-agent (as in (3b)),
is crucial for assigning the correct sense to the verb
treat above.
In the CPA model, automatic identification of
selection contexts not only captures the argument
structure of a predicate, but also more delicate fea-
tures, which may have a profound effect on the
semantic interpretation of a predicate in context.
There are four constraint sets that contribute to the
patterns for encoding selection contexts. These are:
(4) a. Shallow Syntactic Parsing: Phrase-level recogni-
tion of major categories.
b. Shallow Semantic Typing: 50-100 primitive shal-
low types, such as Person, Institution, Event, Abstract, Ar-
tifact, Location, and so forth. These are the top types se-
lected from the Brandeis Shallow Ontology (BSO), and are
similar to entities (and some relations) employed in Named
Entity Recognition tasks, such as TREC and ACE.
c. Minor Syntactic Category Parsing: e.g., loca-
tives, purpose clauses, rationale clauses, temporal adjuncts.
d. Subphrasal Syntactic Cue Recognition: e.g.,
genitives, partitives, bare plural/determiner distinctions,
infinitivals, negatives.
The notion of a selection context pattern, as pro-
duced by a human annotator, is expressed as a BNF
specification in Table 1.1 This specification relies
on word order to specify argument position, and is
easily translated to a template with slots allocated
for each argument. Within this grammar, a seman-
tic roles can be specified for each argument, but this
information currently is not used in the automated
processing.
English contains only about 8,000 verbs, of which
we estimate that about 30% have only one basic pat-
tern. The rest are evenly split between verbs hav-
ing 2-3 patterns and verbs having more than 4 or
1Round brackets indicate optional elements of the pattern,
and curly brackets indicate syntactic constituents.
CPA-Pattern ? Segment verb-lit Segment | verb-lit Segment | Segment verb-lit | CPA-Pattern ?;? Element
Segment ? Element | Segment Segment | ?? Segment ?? | ?(? Segment ?)? | Segment ?|? Segment
Element ? literal | ?[? Rstr ArgType ?]? | ?[? Rstr literal ?]? | ?[? Rstr ?]? | ?[? NO Cue ?]? | ?[? Cue ?]?
Rstr ? POS | Phrasal | Rstr ?|? Rstr | epsilon
Cue ? POS | Phrasal | AdvCue
AdvCue ? ADV ?[? AdvType ?]?
AdvType ? Manner | Dir | Location
Phrasal ? OBJ | CLAUSE | VP | QUOTE
POS ? ADJ | ADV | DET | POSDET | COREF POSDET | REFL-PRON | NEG |
MASS | PLURAL | V | INF | PREP | V-ING | CARD | QUANT | CONJ
ArgType ? ?[? SType ?]? | ?[? SType ?=? SubtypeSpec ?]? | ArgType ?|? ArgType | ?[? SType ArgIdx ?]? |
?[? SType ArgIdx ?=? SubtypeSpec ?]?
SType ? AdvType | TopType | Entity | Abstract | PhysObj | Institution | Asset | Location | Human | Animate |
Human Group | Substance | Unit of Measurement | Quality | Event | State of Affairs | Process
SubtypeSpec ? SubtypeSpec ?|? SubtypeSpec | SubtypeSpec ?&? SubtypeSpec | Role | Polarity | LSet
Role ? Role | Role ?|? Role | Benficiary | Meronym | Agent | Payer
Polarity ? Negative | Positive
LSet ? Worker | Pilot | Musician | Competitor | Hospital | Injury | Ailment | Medicament | Medical Procedure |
Hour-Measure | Bargain | Clothing | BodyPart | Text | Sewage | Part | Computer | Animal
ArgIdx ? <number> verb-lit ? <verb-word-form>
literal ? word word ? <word>
CARD ? <number> NEG ? not
POSDET ? my | your | ... INF ? to
QUANT ? CARD | a lot | longer | more | many | ...
Table 1: Pattern grammar
more patterns. About 20 light verbs have between
100 and 200 patterns each. This is less alarming
than it sounds, because the majority of light verb
patterns involve selection of just one specific nom-
inal head, e.g., take account, take plunge, take
photograph, with few if any alternations. The pat-
tern sets for verbs of different frequency groups differ
in terms of the number and type of features each pat-
tern requires, the number of patterns in a set for a
given verbs, the number of alternations for each pat-
tern, and the type of selectional preferences affecting
the verb?s arguments.
Brandeis Shallow Ontology
The Brandeis Shallow Ontology (BSO) is a shallow
hierarchy of types selected for their prevalence in
manually identified selection context patterns. At
the time of writing, there are just 65 types, in terms
of which patterns for the first one hundred verbs
have been analyzed. New types are added occasion-
ally, but only when all possibilities of using existing
types prove inadequate. Once the set of manually
extracted patterns is sufficient, the type system will
be re-populated and become pattern-driven.
The BSO type system allows multiple inheri-
tance (e.g. Document v PhysObj and Document v
Information. The types currently comprising the
ontology are listed above. The BSO contains type
assignments for 20,000 noun entries and 10,000 nom-
inal collocation entries.
Corpus-driven Type System
The acquisition strategy for selectional preferences
for predicates proceeds as follows:
(5) a. Partition the corpus occurrences of a
predicate according to the selection contexts
pattern grammar, distinguished by the four
levels of constraints mentioned in (4). These
are uninterpreted patterns for the predicate.
b. Within a given pattern, promote the statisti-
cally significant literal types from the corpus for
each argument to the predicate. This induces
an interpretation of the pattern, treating the
promoted literal type as the specific binding of
a shallow type from step (a) above.
c. Within a given pattern, coerce all lexical
heads in the same shallow type for an argu-
ment, into the promoted literal type, assigned
in (b) above. This is a coercion of a lexical
head to the interpretation of the promoted
literal type induced from step (b) above.
In a sense, (5a) can be seen as a broad multi-
level partitioning of the selectional behavior for a
predicate according to a richer set of syntactic and
semantic discriminants. Step (5b) can be seen as
capturing the norms of usage in the corpus, while
step (5c) is a way of modeling the exploitation
of these norms in the language (through coercion,
metonymy, and other generative operations). To
illustrate the way in which CPA discriminates un-
interpreted patterns from the corpus, we return to
the verb treat as it is used in the BNC. Although
there are three basic senses for this verb, the two
major senses, as illustrated in (1) above, emerge as
correlated with two distinct context patterns, us-
ing the discriminant constraints mentioned in (4)
above. For the full specification for this verb, see
www.cs.brandeis.edu/~arum/cpa/treat.html.
(6) a. [[Person 1]] treat [[Person 2]]; NO [Adv[Manner]]
b. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
Given a distinct (contextual) basis on which to an-
alyze the actual statistical distribution of the words
in each argument position, we can promote statisti-
cally relevant and significant literal types for these
positions. For example, for pattern (a) above, this
induces Doctor as Person 1, and Patient as bound
to Person 2. This produces the interpreted context
pattern for this sense as shown below.
(7) [[doctor]] treat [[patient]]
Promoted literal types are corpus-derived and
predicate-dependent, and are syntactic heads of
phrases that occur with the greatest frequency in ar-
gument positions for a given sense pattern; they are
subsequently assumed to be subtypes of the particu-
lar shallow type in the pattern. Step (5c) above then
enables us to bind the other lexical heads in these po-
sitions as coerced forms of the promoted literal type.
This can be seen below in the concordance sample,
where therapies is interpreted as Doctor, and people
and girl are interpreted as Patient.
(8) a. returned with a doctor who treated the girl till an am-
bulance arrived.
b. more than 90,000 people have been treated for cholera
since the epidemic began
c. nonsurgical therapies to treat the breast cancer, which
may involve
Model Bias
The assumption within GL is that semantic types
in the grammar map systematically to default syn-
tactic templates (cf. Pustejovsky (1995)). These
are termed canonical syntactic forms (CSFs). For
example, the CSF for the type proposition is a
tensed S. There are, however, many possible real-
izations (such as infinitival S and NP) for this type
due to the different possibilities available from gen-
erative devices in a grammar, such as coercion and
co-composition. The resulting set of syntactic forms
associated with a particular semantic type is called
a phrasal paradigm for that type. The model bias
provided by GL acts to guide the interpretation of
purely statistically based measures (cf. Pustejovsky
(2000)).
2.2 Automatic Recognition of Pattern Use
Essentially, this subtask is similar to the traditional
supervised WSD problem. Its purpose is (1) to test
the discriminatory power of CPA-derived feature-
set, (2) to extend and refine the inventory of features
captured by the CPA patterns, and (3) to allow for
predicate-based argument groupings by classifying
unseen instances. Extension and refinement of the
inventory of features should involve feature induc-
tion, but at the moment this part has not been im-
plemented. During the lexical discovery stage, lex-
ical sets that fill some of the argument slots in the
patterns are instantiated from the training exam-
ples. As more predicate-based lexical sets within
shallow types are explored, the data will permit
identification of the types of features that unite ele-
ments in lexical sets.
2.3 Automatic Pattern Acquisition
The algorithm for automatic pattern acquisition in-
volves the following steps:
(9) a. Collect all constituents in a particular argu-
ment position;
b. Identify syntactic alternations;
c. Perform clustering on all nouns that occur in
a particular argument position of a given pred-
icate;
d. For each cluster, measure its relatedness
to the known lexical sets, obtained previously
during the lexical discovery stage and extended
through WSD of unseen instances. If none of
the existing lexical sets pass the distance thresh-
old, establish the cluster as a new lexical set, to
be used in future pattern specification.
Step (9d) must include extensive filtering proce-
dures to check for shared semantic features, look-
ing for commonality between the members. That
is, there must be some threshold overlap between
subgroups of the candidate lexical set and and the
existing semantic classes. For instance, checking if,
for a certain percentage of pairs in the candidate set,
there already exists a set of which both elements are
members.
3 Current Implementation
The CPA patterns are developed using the British
National Corpus (BNC). The sorted instances are
used as a training set for the supervised disambigua-
tion. For the disambiguation task, each pattern is
translated into into a set of preprocessing-specific
features.
The BNC is preprocessed using the Robust Accu-
rate Statistical Parsing system (RASP) and seman-
tically tagged with BSO types. The RASP system
(Briscoe and Carroll (2002)) tokenizes, POS-tags,
and lemmatizes text, generating a forest of full parse
trees for each sentence and associating a probability
with each parse. For each parse, RASP produces a
set of grammatical relations, specifying the relation
type, the headword, and the dependent element. All
our computations are performed over the single top-
ranked tree for the sentences where a full parse was
successfully obtained. Some of the grammatical re-
lations identified by RASP are shown in (10).
(10) subjects: ncsubj, clausal (csubj, xsubj)
objects: dobj, iobj, clausal complement
modifiers: adverbs, modifiers of event nominals
We use endocentric semantic typing, i.e., the head-
word of each constituent is used to establish its se-
mantic type. The semantic tagging strategy is simi-
lar to the one described in Pustejovsky et al (2002).
Currently, a subset of 24 BSO types is used for se-
mantic tagging.
A CPA pattern is translated into a feature set,
which in the current implementation uses binary fea-
tures. It is further complemented with other dis-
criminant context features which, rather than dis-
tinguishing a particular pattern, are merely likely to
occur with a given subset of patterns; that is, the fea-
tures that only partially determine or co-determine
a sense. In the future, these should be learned from
the training set through feature induction from the
training sample, but at the moment, they are added
manually. The resulting feature matrix for each pat-
tern contains features such as those in (11) below.
Each pattern is translated into a template of 15-25
features.
(11) Selected context features:
a. obj institution: object belongs to the BSO type ?Insti-
tution?
b. subj human group: subject belongs to the BSO type ?Hu-
manGroup?
c. mod adv ly: target verb has an adverbial modifier, with a
-ly adverb
d. clausal like: target verb has a clausal argument intro-
duced by ?like?
e. iobj with: target verb has an indirect object by ?with?
f. obj PRP: direct object is a personal pronoun
g. stem VVG: the target verb stem is an -ing form
Each feature may be realized by a number of RASP
relations. For instance, a feature dealing with
objects would take into account RASP relations
?dobj?, ?obj2?, and ?ncsubj? (for passives). The
features such as (11a)-(11e) are typically taken di-
rectly from the pattern specification, while features
such as in (11f) and (11g) would typically be added
as co-determining the pattern.
4 Results and Discussion
The experimental trials performed to date are too
preliminary to validate the methodology outlined
above in general terms for the WSD task. Our re-
sults are encouraging however, and comparable to
the best performing systems reported from Senseval
2. For our experiments, we implemented two ma-
chine learning algorithms, instance-based k-Nearest
Neighbor, and a decision tree algorithm (a version
of ID3). For these experiments, kNN was run with
the full training set. Table 2 shows the results on a
subset of verbs that have been processed, also listing
the number of patterns in the pattern set for each of
the verbs.2
verb number of training accuracy
patterns set ID3 kNN
edit 2 100 87% 86%
treat 4 200 45% 52%
submit 4 100 59% 64%
Table 2: Accuracy of pattern identification
Further experimentation is obviously needed to
adequately gauge the effectiveness of the selection
context approach for WSD and other NLP tasks.
It is already clear, however, that the traditional
sense enumeration approach, where senses are asso-
ciated with individual lexical items, must give way
to a model where senses are assigned to the contexts
within which words appear. Furthermore, because
the variability of the stereotypical syntagmatic pat-
terns that are associated with words appears to be
relatively small, such information can be encoded as
2Test set size for each lemma is 100 instances, selected out
of several randomly chosen segments of BNC, non-overlapping
with the training set
lexically-indexed contexts. A comprehensive dictio-
nary of such contexts could prove to be a powerful
tool for a variety of NLP tasks.
References
S. Abney and M. Light. 1999. Hiding a semantic hierarchy in a
markov model.
E. Agirre and D. Martinez. 2001. Learning class-to-class se-
lectional preferences. In Walter Daelemans and Re?mi Zajac,
editors, Proceedings of CoNLL-2001, pages 15?22. Toulouse,
France.
E. Agirre, D. Martinez, and L. Marquez. 2002. Syntactic features
for high precision word sense disambiguation. COLING 2002.
S. Atkins, C. Fillmore, and C. Johnson. 2003a. Lexicographic
relevance: Selecting information from corpus evidence. Inter-
national Journal of Lexicography, 16(3):251?280, September.
S. Atkins, M. Rundell, and H. Sato. 2003b. The contribution of
Framenet to practical lexicography. International Journal of
Lexicography, 16(3):333?357, September.
C. Baker, C. Fillmore, and B. Cronin. 2003. The structure of the
Framenet database. International Journal of Lexicography,
16(3):281?296, September.
T. Briscoe and J. Carroll. 1997. Automatic extraction of sub-
categorization from corpora. Proceedings of the 5th ANLP
Conference, Washington DC, pages 356?363.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical anno-
tation of general text. Proceedings of the Third International
Conference on Language Resources and Evaluation (LREC
2002), Las Palmas, Canary Islands, May 2002, pages 1499?
1504.
J. Carroll and D. McCarthy. 2000. Word sense disambiguation
using automatically acquired verbal preferences.
M. Ciaramita and M. Johnson. 2000. Explaining away ambiguity:
Learning verb selectional preference with Bayesian networks.
COLING 2000.
S. Clark and D. Weir. 2001. Class-based probability estimation
using a semantic hierarchy. Proceedings of the 2nd Conference
of the North American Chapter of the ACL. Pittsburgh, PA.
C. Fillmore, C. Johnson, and M. Petruck. 2003a. Background to
Framenet. International Journal of Lexicography, 16(3):235?
250, September.
C. Fillmore, M. Petruck, J. Ruppenhofer, and A. Wright. 2003b.
Framenet in action: The case of attaching. International
Journal of Lexicography, 16(3):297?332, September.
C. Fillmore. 1975. Santa Cruz Lectures on Deixis. Indiana Uni-
versity Linguistics Club. Bloomington, IN.
M. A. K. Halliday. 1966. Lexis as a linguistic level. In C. E.
Bazell, J. C. Catford, M. A. K. Halliday, and R. H. Robins,
editors, In Memory of J. R. Firth. Longman.
P. Hanks. 2004. The syntagmatics of metaphor (forthcoming).
International Journal of Lexicography, 17(3), September.
forthcoming.
A. Korhonen and J. Preiss. 2003. Improving subcategorization
acquisition using word sense disambiguation. Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, Japan.
A. Korhonen. 2002. Subcategorization Acquisition. PhD thesis
published as Techical Report UCAM-CL-TR-530. Computer
Laboratory, University of Cambridge.
M. Light and W. Greiff. 2002. Statistical models for the induction
and use of selectional preferences. Cognitive Science, Volume
26(3), pp. 269- 281.
D. McCarthy. 1997. Word sense disambiguation for acquisition of
selectional preferences. In Piek Vossen, Geert Adriaens, Nico-
letta Calzolari, Antonio Sanfilippo, and Yorick Wilks, editors,
Automatic Information Extraction and Building of Lexical
Semantic Resources for NLP Applications, pages 52?60. As-
sociation for Computational Linguistics, New Brunswick, New
Jersey.
M. Palmer, H. T. Dang, and C. Fellbaum. 2004. Making fine-
grained and coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering. Preprint.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of the Sec-
ond Int. Workshop on Evaluating WSD Systems (Senseval
2). ACL2002/EACL2001.
J. Pustejovsky and P. Hanks. 2001. Very Large Lexical
Databases: A tutorial. ACL Workshop, Toulouse, France.
J. Pustejovsky, A. Rumshisky, and J. Castano. 2002. Rerendering
Semantic Ontologies: Automatic Extensions to UMLS through
Corpus Analytics. In LREC 2002 Workshop on Ontologies
and Lexical Knowledge Bases. Las Palmas, Canary Islands,
Spain.
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.):
MIT Press.
J. Pustejovsky. 2000. Lexical shadowing and argument closure.
In Y. Ravin and C. Leacock, editors, Lexical Semantics. Ox-
ford University Press.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cognition,
61:127?159.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999.
Inducing a semantically annotated lexicon via EM?based clus-
tering. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99), Mary-
land.
J. Sinclair, P. Hanks, and et al 1987. The Collins Cobuild En-
glish Language Dictionary. HarperCollins, 4th (2003) edition.
Published as Collins Cobuild Advanced Learner?s English Dic-
tionary.
J. M. Sinclair. 1991. Corpus, Concordance, Collocation. Oxford
University Press.
J. Stetina, S. Kurohashi, and M. Nagao. 1998. General word
sense disambiguation method based on A full sentential con-
text. In Sanda Harabagiu, editor, Use of WordNet in Natural
Language Processing Systems: Proceedings of the Confer-
ence, pages 1?8. Association for Computational Linguistics,
Somerset, New Jersey.
M. Stevenson and Y. Wilks. 2001. The interaction of knowledge
sources in word sense disambiguation. Computational Lin-
guistics, 27(3), September.
K. Yamashita, K. Yoshida, and Y. Itoh. 2003. Word sense dis-
ambiguation using pairwise alignment. ACL2003.
D. Yarowsky. 1992. Word-sense disambiguation using statistical
models of Roget?s categories trained on large corpora. Proc.
COLING92, Nantes, France.
D. Yarowsky. 1995. Unsupervised word sense disambiguation ri-
valing supervised methods. In Meeting of the Association for
Computational Linguistics, pages 189?196.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 95?98
Manchester, August 2008
Integrating Motion Predicate Classes with
Spatial and Temporal Annotations
James Pustejovsky
Computer Science Department
Brandeis University
jamesp@cs.brandeis.edu
Jessica L. Moszkowicz
Computer Science Department
Brandeis University
jlittman@cs.brandeis.edu
Abstract
We propose a spatio-temporal markup for
the annotation of motion predicates in text,
informed by a lexical semantic classifica-
tion of these verbs. We incorporate this
classification within a spatial event struc-
ture, based on Generative Lexicon Theory.
We discuss how the spatial event structure
suggests changes to annotation systems de-
signed solely for temporal or spatial phe-
nomena, resulting in spatio-temporal an-
notation.
1 Introduction and Motivation
The recognition of spatial entities in natural lan-
guage is an important component of understanding
a text (Mani et al, 2008). However, simply iden-
tifying fixed geospatial regions and specific ?facil-
ities? is not enough to achieve a complete repre-
sentation of all the spatial phenomena present. In
fact, this leaves out one of the most crucial aspects
of spatial information, motion. To capture motion,
we must integrate temporal and spatial information
with the lexical semantics of motion predicates and
prepositions.
The goal of this research is to further the rep-
resentational support for spatio-temporal reason-
ing from natural language text in the service of
practical applications. To create such support, we
propose to use lexical resources for motion predi-
cates to integrate two existing annotation schemes,
SpatialML and TimeML, creating a representation
that captures, in a fine-grained manner, the move-
ment of individuals through spatial and temporal
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
indexes. This work is part of a larger effort to au-
tomate such annotation and reasoning over natural
language documents using symbolic and machine
learning methods.
In this paper, we investigate different resources
and annotations for spatio-temporal information.
In section 2, we describe some of the resources we
employed for our investigation. Section 3 elabo-
rates on the classes we focus on as we work to-
wards developing a classification for the purpose
of annotating motion predicates, which we discuss
in section 4.
2 Previous Work on Motion
Classifications in Language
There has been considerable research on the lin-
guistic behavior of spatial predicates and preposi-
tions in language (e.g., (Jackendoff, 1983), (Her-
skovits, 1986), (Boas, 2001), (Cappelle and De-
clerck, 2005)). Within qualitative spatial reasoning
(QSR), work has recently started to focus on incor-
porating mereo-topological concepts into the cal-
culus of relations between regions. The most suc-
cessful of these is the Regional Connection Calcu-
lus, or RCC (Randell et al, 1992). RCC8 and other
systems like it do an adequate job of represent-
ing static information about space, but they cannot
help us deal with motion, since that task requires a
temporal component. Galton ((Galton, 1993; Gal-
ton, 1997)) began work on a commonsense theory
of motion, but this work did not focus on merging
temporal and spatial phenomena. Muller (Muller,
1998), however, proposes just such a system with
his qualitative theory of motion based on spatio-
temporal primitives. The result of Muller?s system
is a set of six motion classes: leave, hit, reach, ex-
ternal, internal, and cross.
Asher and Sablayrolles offer their own account
95
of motion verbs and spatial prepositional phrases
in French (Asher and Sablayrolles, 1995). They
propose ten groups of motion verbs as follows:
s?approcher (to approach), arriver (to arrive), en-
trer (to enter), se poser (to alight), s??eloigner (to
distance oneself from), partir (to leave), sortir (to
go out), d?ecoller (to take off), passer (par) (to go
through), d?evier (to deviate). This verb classifica-
tion is more fine-grained than Muller?s.
While Muller, Asher, Sablayrolles, and Vieu
among others have focused on the formal seman-
tics of motion, other work has been done to repre-
sent motion in the FrameNet (Baker et al, 1998)
and VerbNet (Kipper et al, 2006) projects. The
Motion frame is a high level frame in the FrameNet
hierarchy. It is defined as ?Some entity (Theme)
starts out in one place (Source) and ends up in
some other place (Goal), having covered some
space between the two (Path).?
To explore VerbNet?s take on motion predicates,
we mapped Asher and Sablayrolles? verbs to Verb-
Net classes. The mapping revealed that, while
many of the motion predicates we care about have
specific classes in VerbNet, it is not always clear
what these classes have in common unless we look
to FrameNet to find a higher level representation.
3 Classifying spatio-temporal predicates
Following (Muller, 1998), (Vieu, 1991), and
(Asher and Sablayrolles, 1995), we assume spatial
variables are incorporated into the representation
of motion predicates in language. For this paper,
we generally follow (Muller, 2002) by represent-
ing the individuals participating in spatial relations
as spatio-temporal regions (s?i). For modeling mo-
tion, however, we restrict our discussion to spatio-
temporal regions occupied by physical matter de-
noted by the type s ? i ? p.
For this work, we performed several map-
pings between Muller, Asher and Sablayrolles, and
FrameNet. The result of this mapping was a group
of classes based largely on Muller?s classifications
with some very slight modifications detailed in the
table below. The spatial event structures for each
of these classes will describe their formal seman-
tics (as in Figure 1 below).
In addition to these classes, we model the spatial
semantics of prepositions, following (Asher and
Sablayrolles, 1995), generally. Because of space
limitations, we will not discuss the contritbution
of prepositional semantics in this paper.
Move run, fly, drive
Move External drive around, pass
Move Internal walk around the room
Leave leave, desert
Reach arrive, enter, reach
Detach take off, disconnect, pull away
Hit land, hit
Follow follow, chase
Deviate flee, run from
Stay remain, stay
Table 1: Motion Classes
There is a complex interaction between a motion
verb class and the interpretation of its arguments.
For example, not all regions are occupied by the
extent of physical matter (see above), but there are
some objects which are properly both physical and
spatial, such as the concept building. Notice the
ambiguity inherent in the statement below, where
both Move Internal and Move External are pos-
sible interpretations.
(1) The man walked around the building.
This is due to the semantic nature of building as
both a physical object with extent, and also as a
volume/aperture.
To model the mapping of objects to specific ar-
gument and event structures, we adopt the frame-
work of Generative Lexicon Theory (GL). The no-
tion of ?polarity? in the (Muller, 1998) sense is
quite similar to the semantic effect brought about
by event headedness in (Pustejovsky, 1995). GL
provides an explicitly typed argument structure,
a typed subeventual structure, and a predicative
body, which we will use to express RCC8 rela-
tions. For example, a representation of the Spa-
tial Event Structure (SES) for the motion predicate
leave is illustrated below in Figure 1. Note that
the statement Polarity=initial is equivalent to say-
ing Head=left. The relation BT is shorthand for
boundary transition, which is composed of the fol-
lowing RCC8 relations: TPP, O, and EC.
Each motion class in Table 1 maps to a unique
predicative body (qualia structure) in the spatial
event structure for a verb. We demonstrate be-
low how these representations are then embed-
ded in the annotation of a text as RCC8 relations
in a modified SpatialML/TimeML format, called
Spatio-temporal Markup (STM).
The robustness of the mapping from the motion
classes in Table 1 to FrameNet is currently being
96
??
?
?
?
?
?
?
?
?
?
?
?
?
?
leave
ARGSTR =
[
ARG1 = x: s ? i ? p
ARG2 = y: s ? i
]
EVENTSTR =
?
?
E
1
= e
1
:process
E
2
= e
2
:state
RESTR = <
?
POLARITY = initial
?
?
QUALIA =
[
AGENTIVE = NTTP(e
1
,x,y)
? BT(e
1
,x.y)
FORMAL = DC(e
2
,x,y)
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Spatial Event Structure
tested and evaluated.
4 Spatio-temporal Annotation
Throughout the development of the classification
described here, we have tried to focus on how
the classification will impact the task of annotat-
ing spatio-temporal information in text. There are
currently two distinct annotation schemes for spa-
tial and temporal information. If we are to suc-
cessfully capture motion phenomena in text, these
annotations must be merged just as a topological
base and a temporal calculus need to be combined
to model motion predicates.
TimeML (Pustejovsky et al, 2003) is an anno-
tation scheme for representing temporal informa-
tion in text. The basic elements of a TimeML an-
notation are temporal expressions such as dates,
times, and durations, and events that can be an-
chored or ordered to those expressions or with re-
spect to each other.
For the annotation of spatial information, Spa-
tialML (MITRE, 2007) is being developed. The
focus of SpatialML is the markup of spatial lo-
cations that can be integrated with additional re-
sources such as databases that provide informa-
tion about a given domain (e.g. physical feature
databases, gazetteers).
While SpatialML does a reasonable job of cap-
turing locations in space, it cannot model moving
objects such as people and cars, and in fact lacks a
mechanism for capturing motion since predicates
are not annotated. As we saw above, the motion
event must be captured in addition to the locations
involved in the motion. The development of our
motion classification also reveals that the partici-
pants of the event are also needed, even if they are
not spatial locations. For example, in John flew to
Boston, John must be included in the motion anno-
tation because he is a moving object.
We can enhance the spatio-temporal informa-
tion from SpatialML and TimeML with lexical se-
mantic information from a lexicon of motion pred-
icates, resulting in annotation that is rich enough
to be able to (1) infer motion of individuals in-
vovled in specific events; and (2) to compose mo-
tions to create motion sequences (cf. (Pustejovsky
and Verhagen, 2007)).
To create a spatio-temporal markup, TimeML
and SpatialML must be enriched so that they can
adequately capture the motion predicates we are
discussing. TimeML will already annotate motion
predicates as events, but to truly reveal the motion
involved, additional attributes must be added to ac-
count for the beginning, middle, and end points of
the event. The spatio-temporal annotation will re-
quire a new kind of spatial link to capture motion
paths. Essentially, the motion path will combine
the event information from TimeML with the spa-
tial information from SpatialML. This motion path
is at the core of a spatio-temporal markup or STM.
The spatial event structure described in the previ-
ous section motivates the construction of the STM.
The concept of polarity or headedness will also
motivate some aspects of the annotation. Depend-
ing on the polarity of the motion predicate or spa-
tial prepositional phrase, the annotator will know
to look for the source or goal of the event in the
text and include that in the motion path.
The exact details of the resulting spatio-
temporal markup are still under development.
However, the following examples give an idea of
how motion class information allow us to integrate
spatial and temporal annotation.
(2) John drove from Boston to NY on Thursday.
<MOVER id=0>John</MOVER>
<EVENT id=1 tense=past start=t1 end=t2>drove</EVENT>
<SIGNAL id=2 type=spatial polarity=initial>from</SIGNAL>
<PLACE id=3>Boston</PLACE>
<SIGNAL id=4 type=spatial polarity=final>to</SIGNAL>
<PLACE id=5>New York</PLACE>
<SIGNAL id=6 type=temporal>on</SIGNAL>
<TIMEX3 id=7>Thursday</TIMEX3>
<TLINK eventID=1 timeID=7 relType=INCLUDES
signalID=6 />
<MOTION eventID=1 moverID=0 source=3
sourceTime=t1 sourceSignal=2 goal=5
goalTime=t2 goalSignal=4 class=MOVE/>
(3) John left Boston for New York.
<MOVER id=0>John</MOVER>
<EVENT id=1 tense=past start=t1 end=t2
polarity=intitial>left</EVENT>
<PLACE id=2>Boston</PLACE>
<SIGNAL id=3 type=spatial
polarity=final>for</SIGNAL>
<PLACE id=4>New York</PLACE>
<MOTION eventID=1 moverID=0 source=2 sourceTime=t1
goal=4 goalTime=t2 goalSignal=3 class=LEAVE/>
97
The Motion tag in the above examples tells us
the class of the motion predicate. This provides a
link to both the spatial event structure (as in Figure
1) and a spatio-temporal markup, which embeds
the annotaton of a text as RCC8 relations in a mod-
ified SpatialML/TimeML format. The example in
4 shows the STM for leave:
(4)
?
?
?
?
?
?
?
?
?
?
motion
TYPE = leave
EVENTID = e
MOVERID = x
SOURCE = l
1
SOURCETIME = t
1
GOAL = l
2
GOALTIME = t
2
?
?
?
?
?
?
?
?
?
?
=?
[
IN(t
1
, x, l
1
)
IN(t
2
, x, l
2
)
DC(t
2
, x, l
1
)
]
This STM indicates what additional information
is needed for the spatio-temporal annotation. In
the case of example 3, three temporally anchored
SpatialML link tags are indiated for each of the
RCC8-like relations to the second part of the STM:
(5) <LINK linkType=IN source=0 target=2 time=t1/>
<LINK linkType=DC source=0 target=2 time=t2/>
<LINK linkType=IN source=0 target=4 time=t2/>
These links that can be automatically generated
at a later stage in the annotation set up the locations
of moving objects at given times. The first link in
example 5 reveals that the moving object John was
in Boston at time t1, which is the start time of the
motion given in the annotation.
5 Conclusion
In this paper, we investigate how an expressive
classification for verbs of motion can be used to
integrate spatial and temporal annotation infor-
mation, in order to represent objects in motion,
as expressed in text. We adopt a modified ver-
sion of the classifications of verbs of motion in
(Muller, 1998) and (Asher and Sablayrolles, 1995)
and demonstrated how verb classes are mapped to
RCC8+1 relations in a temporally anchored Spa-
tialML. We are currently evaluating the reliability
of the FrameNet encoding of motion predicates,
and are developing algorithms for translating lexi-
cal structures to Spatio-temporal markup.
References
Asher, N. and P. Sablayrolles. 1995. A typology and
discourse for motion verbs and spatial pps in french.
Journal of Semantics, 12:163?209.
Baker, C., C. Fillmore, and J. Lowe. 1998. The berke-
ley framennet project. In Proceedings of the COL-
ING ACL, Montreal, Canada.
Boas, H. 2001. Frame semantics as a framework for
describing polysemy and syntactic structures of en-
glish and german motion verbs in contrastive com-
putational lexicography. Rayson, P., A. Wilson, T.
McEnery, A. Hardie, and S. Khoja, eds., Corpus lin-
guistics 2001 , vol.13, Lancaster, UK.
Cappelle, B. and R. Declerck. 2005. Spatial and tem-
poral boundedness in english motion events. Journal
of Pragmatics, 37(6):889?917, June.
Galton, A. 1993. Towards an integrated logic of space,
time, and motion. IJCAI.
Galton, A. 1997. Space, time, and movement. Stock,
O., ed., Spatial and temporal reasoning. Kluwer.
Herskovits, A. 1986. Language and Spatial Cogni-
tion: an Interdisci- plinary Study of the Prepositions
in English. Cambridge University Press.
Jackendoff, R. 1983. Semantics and Cognition. MIT.
Kipper, K., A. Korhonen, N. Ryant, and M. Palmer.
2006. Extensive classifications of english verbs. In
12th EURALEX, Turin, Italy.
Mani, I., J. Hitzeman, and C. Clark. 2008. Annotating
natural language geographic references. In Work-
shop on Methodologies and Resources for Process-
ing Spatial Language. LREC?2008.
MITRE. 2007. Spatialml: Annotation scheme for
marking spatial expressions in natural language.
http://sourceforge.net/projects/spatialml/.
Muller, P.. 1998. A qualitative theory of motion
based on spatio-temporal primitives. Cohn, A., L.
Schubert, and S. Shapiro, eds., KR?98: Principles
of Knowledge Representation and Reasoning, pages
131?141. Morgan Kaufmann.
Muller, P. 2002. Topological spatio-temporal reason-
ing and representation. Computational Intelligence,
18(3):420?450.
Pustejovsky, J. and M. Verhagen. 2007. Inferring
spatio-temporal trajectories of entities from natural
language documents. Tech. report, Brandeis U.
Pustejovsky, J., J. Casta?no, R. Ingria, R. Saur??, R.
Gaizauskas, A. Setzer, and G. Katz. 2003. Timeml:
Robust specification of event and temporal expres-
sions in text. In IWCS-5, Fifth International Work-
shop on Computational Semantics.
Pustejovsky, J. 1995. The Generative Lexicon. MIT.
Randell, D. A., Z. Cui, and A. G. Cohn. 1992. A spatial
logic based on regions and connections. Kaufmann,
M., ed., 3rd International Conference on Knowledge
Representation and Reasoning.
Vieu, L. 1991. S?emantique des relations spatiales
et inf?erences spatio-temporelles: une contribution `a
l??etude des structures formelles de l?espace en lan-
gage naturel. Ph.D. thesis, Universit?e Paul Sabatier.
98
Coling 2008: Companion volume ? Posters and Demonstrations, pages 189?192
Manchester, August 2008
Temporal Processing with the TARSQI Toolkit
Marc Verhagen
Brandeis University
Computer Science Dept.
Waltham, MA 02454-9110
marc@cs.brandeis.edu
James Pustejovsky
Brandeis University
Computer Science Dept.
Waltham, MA 02454-9110
jamesp@cs.brandeis.edu
Abstract
We present the TARSQI Toolkit (TTK),
a modular system for automatic temporal
and event annotation of natural language
texts. TTK identifies temporal expressions
and events in natural language texts, and
parses the document to order events and to
anchor them to temporal expressions.
1 Introduction
A keyword-based search is not sufficient to answer
temporally loaded questions like ?did Brazil win
the soccer world championship in 1970?? since
a boolean keyword search cannot distinguish be-
tween those documents where the event win is ac-
tually anchored to the year 1970 versus those that
are not. The TARSQI Project (Temporal Aware-
ness and Reasoning Systems for Question Inter-
pretation) focused on enhancing natural language
question answering systems so that temporally-
based questions about the events and entities in
news articles can be addressed. To explicitly mark
the needed temporal relations the project deliv-
ered a series of tools for extracting time expres-
sions, events, subordination relations and tempo-
ral relations (Verhagen et al, 2005; Mani et al,
2006; Saur?? et al, 2005; Saur?? et al, 2006a). But
although those tools performed reasonably well,
they were not integrated in a principled way.
This paper describes the TARSQI Toolkit
(TTK), which takes the TARSQI components and
integrates them into a temporal parsing framework.
The toolkit is different from the system described
in (Verhagen et al, 2005) in several major aspects:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1. the components were integrated in a toolkit
which, amongst others, split the parsing of
properties typical for a particular document
type from the temporal parsing of the text
2. a component was added that takes the re-
sults from the various components that gen-
erate temporal relations and merges them into
a consistent temporal graph
3. a new way of visualizing the results was used
In addition, some components were updated and
test suites with unit tests and regression tests were
added. In this paper, we focus on the merging of
temporal links and the visualization of temporal re-
lations.
There has been a fair amount of recent re-
search on extraction of temporal relations, includ-
ing (Chambers et al, 2007; Lapata and Lascarides,
2006; Bramsen et al, 2006; Bethard and Martin,
2007; Min et al, 2007; Pus?cas?u, 2007). However,
we are not aware of approaches that integrate tem-
poral relations from various sources in one consis-
tent whole.
All TTK components use the TimeML anno-
tation language (Pustejovsky et al, 2003; Puste-
jovsky et al, 2005). TimeML is an annotation
scheme for markup of events, times, and their
temporal relations in news articles. The TimeML
scheme flags tensed verbs, adjectives, and nomi-
nals with EVENT tags with various attributes, in-
cluding the class of event, tense, grammatical as-
pect, polarity (negative or positive), and any modal
operators which govern the event being tagged.
Time expressions are flagged with TIMEX3 tags,
an extension of the ACE 2004 TIMEX2 annotation
scheme (tern.mitre.org).
189
Subordination relations between events, as for
example between reporting events and the em-
bedded event reported on, are annotated with the
SLINK tag. For temporal relations, TimeML de-
fines a TLINK tag that links tagged events to other
events and/or times.
In section 2, we will give a short overview of the
toolkit. In section 3, we focus on the component
that merges TLINKs, and in section 4 we will dwell
on the visualization of temporal relations.
2 Overview of the toolkit
The overall architecture of TTK is illustrated in
figure 1 below. Input text is first processed by the
DocumentModel, which takes care of document-
level properties like encoding and meta tags. The
DocumentModel hands clean text to the other
components which are allowed to be more generic.
DocumentModel
PreProcessing
Text
GUTime Evita
Slinket
Temporal Processing
Temporal Parse
Figure 1: TTK Architecture
The preprocessor uses standard approaches to
tokenization, part-of-speech tagging and chunk-
ing. GUTime is a temporal expression tagger that
recognizes the extents and normalized values of
time expressions. Evita is a domain-independent
event recognition tool that performs two main
tasks: robust event identification and analysis of
grammatical features such as tense and aspect.
Slinket is an application developed to automat-
ically introduce SLINKs, which in TimeML spec-
ify subordinating relations between pairs of events,
and classify them into factive, counterfactive, evi-
dential, negative evidential, and modal, based on
the modal force of the subordinating event (Saur??
et al, 2006b). SLINKs are introduced by a well-
delimited subgroup of verbal and nominal predi-
cates (such as regret, say, promise and attempt),
and in most cases clearly signaled by a subordina-
tion context. Slinket thus relies on a combination
of lexical and syntactic knowledge.
The temporal processing stage includes three
modules that generate TLINKs: Blinker, S2T and
the TLink Classifier.
Blinker is a rule-based component that applies
to certain configurations of events and timexes. It
contains rule sets for the following cases: (i) event
and timex in the same noun phrase, (ii) events
and the document creation time, (iii) events with
their syntactically subordinated events, (iv) events
in conjunctions, (v) two main events in consecu-
tive sentences, and (vi) timexes with other timexes.
Each of these rule sets has a different flavor. For
example, the rules in (vi) simply calculate differ-
ences in the normalized ISO value of the timex
tag while the rules in (v) refer to the tense and
aspect values of the two events. Blinker is a re-
implementation and extension of GutenLink (Ver-
hagen et al, 2005).
S2T takes the output of Slinket and uses about a
dozen syntactic rules to map SLINKs onto TLINKs.
For example, one S2T rule encodes that in SLINKs
with reporting verbs where both events are in past
tense, the reporting event occurred after the event
reported on.
The TLink Classifier is a MaxEnt classifier
that identifies temporal relations between identi-
fied events in text. The classifier accepts its input
for each pair of events under consideration as a set
of features. It is trained on the TimeBank corpus
(see www.timeml.org).
Of the three TLINK generating components, S2T
derives a relatively small number of TLINKs, but
Blinker and the classifier are quite prolific. In
many cases the TLINKs derived by Blinker and the
classifier are inconsistent with each other. The sys-
tem in (Verhagen et al, 2005) used a simple voting
mechanism that favors TLINKs from components
that exhibit higher precision. In addition, if con-
fidence measures are available then these can be
used by the voting mechanism. However, this ap-
proach does not factor in consistency of temporal
relations: choosing the TLINKs with the highest
probability may result in TLINKs that are incon-
sistent. For example, say we have two TLINKs:
BEFORE(x,y) and BEFORE(y,z). And say we have
190
two competing TLINKs, derived by Blinker and
the classifier respectively: BEFORE(x,z) and BE-
FORE(z,x). If the second of these two has a higher
confidence, then we will end up with an inconsis-
tent annotation. In the following section we de-
scribe how in TTK this problem is avoided.
3 Link Merger
The link merger, together with the three TLINK-
generating components, is part of the temporal
processing module of TTK, as shown in the dia-
gram in figure 2 below.
Blinker
Classifier
S2T
Events 
and 
Times
ALinks
SLinks
Link Merging
SputLink
Figure 2: TTK Temporal Processing
The link merging component uses a greedy al-
gorithm to merge TLinks into a consistent whole.
First all links are ordered on their confidence score.
Currently these scores are either global or local.
Global confidence scores are derived from the ob-
served precision of the component that generated
the links. For example, links generated by S2T are
considered high precision and are always deemed
more reliable than links generated by the classifier.
Links generated by the classifier come with a con-
fidence score assigned by the classifier and these
scores are used to order all classifier links.
Merging proceeds by first creating a graph that
contains all events and time expressions as nodes,
but that has no constraints expressed on the edges.
Those constraints are added by the temporal links.
Links are ordered on confidence score and are
added one by one. Each time a link is added a con-
straint propagation component named Sputlink,
based on Allen?s interval algebra (Allen, 1983;
Verhagen, 2005), is applied. If a link cannot be
added because it is inconsistent with the constraint
already on the edge, then the link is skipped. The
result is a consistent annotation where high preci-
sion links are prefered over lower precision links.
4 Visualization
Providing a good visualization of a temporal graph
can be tricky. A table of temporal relations is
only useful for relations inside sentences. Full
graphs, like the ones generated by GraphViz
(http://www.graphviz.org/), do not make it that
much easier for the reader to quickly obtain a pic-
ture of the temporal structure of the document.
Timelines can be misleading because so many
events in a document cannot be ordered with re-
spect to a time stamp.
TTK uses a visualization scheme named TBox
(Verhagen, 2007). It uses left-to-right arrows,
box inclusion and stacking to encode temporal
precedence, inclusion, and simultaneity respec-
tively (see figure 3).
Figure 3: The TBox Representation
This visualization makes it easier to convey the
temporal content of a document since temporal re-
lations are strictly and unambiguously mapped to
specific ways of drawing them. And vice versa, a
particular way of positioning two events always in-
dicates the same temporal relation. Note that ver-
tical positioning does not imply any temporal rela-
tion.
5 Conclusion and Future Work
We have described TTK, a toolkit that integrates
several components that generate tags to mark
up events and time expressions, as well as non-
consuming tags that encode relations between
events and times. TTK includes a module that
combines potentially conflicting temporal rela-
tions into a consistent temporal graph of a docu-
ment, which can be succinctly displayed using the
TBox representation.
In current work, we are exploring how to split up
the task of temporal relation extraction into more
subtasks and write specialized components, both
rule-based and machine learning based, to extract
temporal relations for that task. The link merging
would then have many more input streams, each
with their own reported reliability.
The TARSQI Toolkit can be downloaded from
http://timeml.org/site/tarsqi/toolkit/.
191
Acknowledgments
The work reported in the paper was carried
out in the context of the AQUAINT program
and was funded under ARDA/DoD/IARPA grant
NBCHC040027.
References
Allen, James. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
Bethard, Steven and James H. Martin. 2007. CU-
TMP: Temporal relation classification using syntac-
tic and semantic features. In Proceedings of the
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 129?132, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Bramsen, Philip, Pawan Deshpande, Yoong Keok Lee-
and, and Regina Barzilay. 2006. Finding tempo-
ral order in discharge summaries. In Proceedings of
EMNLP.
Chambers, Nathanael, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 173?176, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Lapata, Mirella and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Mani, Inderjeet, Ben Wellner, Marc Verhagen,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 44th Annual Meeting of the Association for
Computational Linguistics, Sydney. ACL.
Min, Congmin, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to tem-
poral relation identification in news text. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 219?
222, Prague, Czech Republic, June. Association for
Computational Linguistics.
Pus?cas?u, Georgiana. 2007. WVALI: Temporal re-
lation identification by syntactico-semantic analy-
sis. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 484?487, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Pustejovsky, James, Jos?e Casta?no, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust Specification of
Event and Temporal Expressions in Text. In IWCS-5
Fifth International Workshop on Computational Se-
mantics.
Pustejovsky, James, Robert Knippen, Jessica Littman,
and Roser Saur??. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39:123?164, May.
Saur??, Roser, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005. Evita: A robust event
recognizer for qa systems. In Proceedings of the
HLT/EMNLP 2005, pages 700?707.
Saur??, Roser, Marc Verhagen, and James Pustejovsky.
2006a. Annotating and recognizing event modal-
ity in text. In Proceedings of of the 19th Inter-
national FLAIRS Conference, FLAIRS 2006, Mel-
bourne Beach, Florida, USA.
Saur??, Roser, Marc Verhagen, and James Pustejovsky.
2006b. SlinkET: A partial modal parser for events.
In Proceedings of LREC 2006, Genoa, Italy.
Verhagen, Marc, Inderjeet Mani, Roser Sauri, Jes-
sica Littman, Robert Knippen, Seok Bae Jang,
Anna Rumshisky, John Phillips, and James Puste-
jovsky. 2005. Automating temporal annotation with
TARSQI. In Proceedings of the 43th Annual Meet-
ing of the Association for Computational Linguistics,
Ann Arbor, USA. Demo session.
Verhagen, Marc. 2005. Temporal closure in an annota-
tion environment. Language Resources and Evalua-
tion, 39:211?241, May.
Verhagen, Marc. 2007. Drawing TimeML relations
with TBox. In Katz, Graham, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, volume 4795
of Lecture Notes in Computer Science, pages 7?28.
Springer.
192
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 92?101, Prague, June 2007. c?2007 Association for Computational Linguistics
 
		Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 700?707, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Evita: A Robust Event Recognizer For QA Systems
Roser Saur?? Robert Knippen Marc Verhagen James Pustejovsky
Lab for Linguistics and Computation
Computer Science Department
Brandeis University
415 South Street, Waltham, MA 02454, USA
 roser,knippen,marc,jamesp@cs.brandeis.edu
Abstract
We present Evita, an application for rec-
ognizing events in natural language texts.
Although developed as part of a suite of
tools aimed at providing question answer-
ing systems with information about both
temporal and intensional relations among
events, it can be used independently as
an event extraction tool. It is unique in
that it is not limited to any pre-established
list of relation types (events), nor is it re-
stricted to a specific domain. Evita per-
forms the identification and tagging of
event expressions based on fairly simple
strategies, informed by both linguistic-
and statistically-based data. It achieves a
performance ratio of 80.12% F-measure.1
1 Introduction
Event recognition is, after entity recognition, one of
the major tasks within Information Extraction. It is
currently being succesfully applied in different ar-
eas, like bioinformatics and text classification. Rec-
ognizing events in these fields is generally carried
out by means of pre-defined sets of relations, possi-
bly structured into an ontology, which makes such
tasks domain dependent, but feasible. Event recog-
nition is also at the core of Question Answering,
1This work was supported by a grant from the Advanced
Research and Development Activity in Information Technology
(ARDA), a U.S. Government entity which sponsors and pro-
motes research of import to the Intelligence Community which
includes but is not limited to the CIA, DIA, NSA, NIMA, and
NRO.
since input questions touch on events and situations
in the world (states, actions, properties, etc.), as they
are reported in the text. In this field as well, the use
of pre-defined sets of relation patterns has proved
fairly reliable, particularly in the case of factoid type
queries (Brill et al, 2002; Ravichandran and Hovy,
2002; Hovy et al, 2002; Soubbotin and Soubbotin,
2002).
Nonetheless, such an approach is not sensitive to
certain contextual elements that may be fundamental
for returning the appropriate answer. This is for in-
stance the case in reporting or attempting contexts.
Given the passage in (1a), a pattern-generated an-
swer to question (1b) would be (1c). Similarly, dis-
regarding the reporting context in example (2) could
erroneously lead to concluding that no one from the
White House was involved in the Watergate affair.
(1) a. Of the 14 known ways to reach the summit, only
the East Ridge route has never been successfully
climbed since George Mallory and Andrew ?Sandy?
Irvine first attempted to climb Everest in 1924.
b. When did George Mallory and Andrew Irvine first
climb Everest?
c. #In 1924.
(2) a. Nixon claimed that White House counsel John Dean
had conducted an investigation into the Watergate
matter and found that no-one from the White House
was involved.
b. What members of the White House were involved in
the Watergate matter?
c. #Nobody.
Intensional contexts like those above are gener-
ated by predicates referring to events of attempting,
intending, commanding, and reporting, among oth-
ers. When present in text, they function as modal
700
qualifiers of the truth of a given proposition, as in
example (2), or they indicate the factuality nature
of the event expressed by the proposition (whether
it happened or not), as in (1) (Saur?? and Verhagen,
2005).
The need for a more sophisticated approach that
sheds some awareness on the specificity of certain
linguistic contexts is in line with the results ob-
tained in previous TREC Question Answering com-
petitions (Voorhees, 2002, 2003). There, a system
that attempted a minimal understanding of both the
question and the answer candidates, by translating
them into their logical forms and using an infer-
ence engine, achieved a notably higher score than
any surface-based system (Moldavan et al, 2002;
Harabagiu et al, 2003).
Non-factoid questions introduce an even higher
level of difficulty. Unlike factoid questions, there
is no simple or unique answer, but more or less sat-
isfactory ones instead. In many cases, they involve
dealing with several events, or identifying and rea-
soning about certain relations among events which
are only partially stated in the source documents
(such as temporal and causal ones), all of which
makes the pattern-based approach less suitable for
the task (Small et al, 2003, Soricut and Brill, 2004).
Temporal information in particular plays a signifi-
cant role in the context of question answering sys-
tems (Pustejovsky et al, forthcoming). The ques-
tion in (3), for instance, requires identifying a set
of events related to the referred killing of peasants
in Mexico, and subsequently ordering them along a
temporal axis.
(3) What happened in Chiapas, Mexico, after the killing of
45 peasants in Acteal?
Reasoning about events in intensional contexts,
or with event-ordering relations such as temporality
and causality, is a requisite for any open-domain QA
system aiming at both factoid and non-factoid ques-
tions. As a first step, this involves the identification
of all relevant events reported in the source docu-
ments, so that later processing stages can locate in-
tensional context boundaries and temporal relations
among these events.
In this article, we present Evita, a tool for recog-
nizing events in natural language texts. It has been
developed as part of a suite of tools aimed at provid-
ing QA systems with information about both tem-
poral and intensional relations between events; we
anticipate, however, that it will be useful for other
NLP tasks as well, such as narrative understanding,
summarization, and the creation of factual databases
from textual sources.
In the next section, we provide the linguistic foun-
dations and technical details of our event recognizer
tool. Section 3 gives the results and discusses them
in the context of the task. We conclude in section 4,
with an overview of Evita?s main achievements and
a brief discussion of future directions.
2 Evita, An Event Recognition Tool
Evita (?Events In Text Analyzer?) is an event recog-
nition system developed under the ARDA-funded
TARSQI research framework. TARSQI is devoted
to two complementary lines of work: (1) estab-
lishing a specification language, TimeML, aimed
at capturing the richness of temporal and event re-
lated information in language (Pustejovsky et al,
2003a, forthcoming), and (2) the construction of a
set of tools that perform tasks of identifying, tag-
ging, and reasoning about eventive and temporal in-
formation in natural language texts (Pustejovsky and
Gaizauskas, forthcoming, Mani, 2005; Mani and
Schiffman, forthcoming; Verhagen, 2004; Verhagen
et al, 2005; Verhagen and Knippen, forthcoming).
Within TARSQI?s framework, Evita?s role is locat-
ing and tagging all event-referring expressions in the
input text that can be temporally ordered.
Evita combines linguistic- and statistically-based
techniques to better address all subtasks of event
recognition. For example, the module devoted to
recognizing temporal information that is expressed
through the morphology of certain event expressions
(such as tense and aspect) uses grammatical infor-
mation (see section 2.4), whereas disambiguating
nouns that can have both eventive and non-eventive
interpretations is carried out by a statistical module
(section 2.3).
The functionality of Evita breaks down into two
parts: event identification and analysis of the event-
based grammatical features that are relevant for tem-
poral reasoning purposes. Both tasks rely on a pre-
processing step which performs part-of-speech tag-
701
ging and chunking, and on a module for cluster-
ing together chunks that refer to the same event.
In the following subsection we provide the linguis-
tic assumptions informing Evita. Then, subsections
2.2 to 2.5 provide a detailed description of Evita?s
different subcomponents: preprocessing, clustering
of chunks, event identification, and analysis of the
grammatical features associated to events.
2.1 Linguistic settings
TimeML identifies as events those event-denoting
expressions that participate in the narrative of a
given document and which can be temporally or-
dered. This includes all dynamic situations (punc-
tual or durative) that happen or occur in the text, but
also states in which something obtains or holds true,
if they are temporally located in the text. As a result,
generics and most state-denoting expressions are fil-
tered out (see Saur?? et al (2004) for a more exhaus-
tive definition of the criteria for event candidacy in
TimeML).
Event-denoting expressions are found in a wide
range of syntactic expressions, such as finite clauses
(that no-one from the White House was involved),
nonfinite clauses (to climb Everest), noun phrases
headed by nominalizations (the young industry?s
rapid growth, several anti-war demonstrations)
or event-referring nouns (the controversial war),
and adjective phrases (fully prepared).
In addition to identifying the textual extent of
events, Evita also analyzes certain grammatical fea-
tures associated with them. These include:
  The polarity (positive or negative) of the ex-
pression tells whether the referred event has
happened or not;
  Modality (as marked by modal auxiliaries may,
can, might, could, should, etc., or adverbials
like probably, likely, etc.) qualifies the denoted
event with modal information (irrealis, neces-
sity, possibility), and therefore has implications
for the suitability of statements as answers to
questions, in a parallel way to other intensional
contexts exemplified in (1-2);
  Tense and aspect provide crucial information
for the temporal ordering of the events;
  Similarly, the non-finite morphology of certain
verbal expressions (infinitival, present partici-
ple, or past participle) has been shown as useful
in predicting temporal relations between events
(Lapata and Lascarides, 2004). We also con-
sider as possible values here the categories of
noun and adjective.
  Event class distinguishes among states (e.g., be
the director of), general occurrences (walk),
reporting (tell), intensional (attempt), and per-
ception (observe) events. This classification
is relevant for characterizing the nature of the
event as irrealis, factual, possible, reported,
etc. (recall examples (1-2) above).
Despite the fact that modality, tense, aspect, and
non-finite morphology are typically verbal features,
some nouns and adjectives can also have this sort
of information associated with them; in particular,
when they are part of the predicative complement of
a copular verb (e.g., may be ready, had been a col-
laborator). A TimeML mark-up of these cases will
tag only the complement as an event, disregarding
the copular verb. Therefore, the modality, tense, as-
pect, and non-finite morphology information associ-
ated with the verb is incorporated as part of the event
identified as the nominal or adjectival complement.
Except for event class, the characterization of all
the features above relies strictly on surface linguistic
cues. Notice that this surface-based approach does
not provide for the actual temporal interpretation of
the events in the given context. The tense of a ver-
bal phrase, for example, does not always map in a
straightforward way with the time being referred to
in the world; e.g., simple present is sometimes used
to express future time or habituality. We handle the
task of mapping event features onto their semantics
during a later processing stage, not addressed in this
paper, but see Mani and Schiffman (forthcoming).
TimeML does not identify event participants, but
the event tag and its attributes have been designed
to interface with Named Entity taggers in a straight-
forward manner. In fact, the issue of argument link-
ing to the events in TimeML is already being ad-
dressed in the effort to create a unified annotation
with PropBank and NomBank (Pustejovsky et al
2005). A complete overview of the linguistic foun-
dations of TimeML can be obtained in Pustejovsky
et al (forthcoming).
702
2.2 Preprocessing
For the task of event recognition, Evita needs ac-
cess to part of speech tags and to the result of some
form of syntactic parsing. Section 2.1 above de-
tailed some of the different syntactic structures that
are used to refer to events. However, using a shal-
low parser is enough to retrieve event referring ex-
pressions, since they are generally conveyed by three
possible part of speech categories: verbs (go, see,
say), nouns (departure, glimpse, war), and adjec-
tives (upset, pregnant, dead).
Part of speech tags and phrase chunks are also
valuable for the identification of certain grammatical
features such as tense, non-finite morphology, or po-
larity. Finally, lexical stems are necessary for those
tasks involving lexical look-up. We obtain all such
grammatical information by first preprocessing the
input file using the Alembic Workbench tagger, lem-
matizer, and chunker (Day et al, 1997). Evita?s in-
put must be XML-compliant, but need not conform
to the TimeML DTD.
2.3 Event Recognition
Event identification in Evita is based on the notion
of event as defined in the previous section. Only lex-
ical items tagged by the preprocessing stage as either
verbs, nouns, or adjectives are considered event can-
didates.
Different strategies are used for identifying events
in these three categories. Event identification in
verbal chunks is based on lexical look-up, accom-
panied by minimal contextual parsing in order to
exclude weak stative predicates, such as ?be?, and
some generics (e.g., verbs with bare plural subjects).
For every verbal chunk in the text, Evita first ap-
plies a pattern-based selection step that distinguishes
among different kinds of information: the chunk
head, which is generally the most-right element of
verbal nature in the chunk, thus disregarding par-
ticles of different sort and punctuation marks; the
modal auxiliary sequence, if any (e.g., may have to);
the sequence of do, have, or be auxiliaries, mark-
ing for aspect, tense and voice; and finally, any item
expressing the polarity of the event. The last three
pieces of information will be used later, when iden-
tifying the event grammatical features (section 2.4).
Based on basic lexical inventories, the chunk may
then be rejected if the head belongs to a certain class.
For instance, copular verbs are generally disregarded
for event tagging, although they enter into a a pro-
cess of chunk clustering, together with their predica-
tive complement (see section 2.5).
The identification of nominal and adjectival
events is also initiated by the step of information se-
lection. For each noun and adjective chunk, their
head and polarity markers, if any, are distinguished.
Identifying events expressed by nouns involves
two parts: a phase of lexical lookup, and a disam-
biguation process. The lexical lookup aims at an ini-
tial filtering of candidates to nominal events. First,
Evita checks whether the head of the noun chunk is
an event in WordNet. We identified about 25 sub-
trees from WordNet where all synsets denote nom-
inal events. One of these, the largest, is the tree
underneath the synset that contains the word event.
Other subtrees were selected by analyzing events in
SemCor and TimeBank1.22 and mapping them to
WordNet synsets. One example is the synset with
the noun phenomenon. In some cases, exceptions
are defined. For example, a noun in a subset sub-
sumed by the phenomenon synset is not an event
if it is also subsumed by the synset with the noun
cloud (in other words, many phenomena are events
but clouds are not).
If the result of lexical lookup is inconclusive (that
is, if a nominal occurs in WN as both and event and
a non-event), then a disambiguation step is applied.
This process is based on rules learned by a Bayesian
classifier trained on SemCor.
Finally, identifying events from adjectives takes
a conservative approach of tagging as events only
those adjectives that were annotated as such in Time-
Bank1.2, whenever they appear as the head of a
predicative complement. Thus, in addition to the
use of corpus-based data, the subtask relies again on
a minimal contextual parsing capable of identifying
the complements of copular predicates.
2TimeBank1.2 is our gold standard corpus of around
200 news report documents from various sources, anno-
tated with TimeML temporal and event information. A
previous version, TimeBank1.1, can be downloaded from
http://www.timeml.org/. For additional information
see Pustejovsky et al (2003b).
703
2.4 Identification of Grammatical Features
Identifying the grammatical features of events fol-
lows different procedures, depending on the part
of speech of the event-denoting expression, and
whether the feature is explicitely realized by the
morphology of such expressions.
In event-denoting expressions that contain a ver-
bal chunk, tense, aspect, and non-finite morphology
values are directly derivable from the morphology of
this constituent, which in English is quite straight-
forward. Thus, the identification of these features is
done by first extracting the verbal constituents from
the verbal chunk (disregarding adverbials, punctua-
tion marks, etc.), and then applying a set of over 140
simple linguistic rules, which define different possi-
ble verbal phrases and map them to their correspond-
ing tense, aspect, and non-finite morphology values.
Figure 1 illustrates the rule for verbal phrases of fu-
ture tense, progressive aspect, which bear the modal
form have to (as in, e.g., Participants will have to
be working on the same topics):
[form in futureForm],
[form==?have?],
[form==?to?, pos==?TO?],
[form==?be?], [pos==?VBG?],
==>
[tense=?FUTURE?,
aspect=?PROGRESSIVE?,
nf morph=?NONE?]
Figure 1: Grammatical Rule
For event-denoting expressions containing no
verbal chunk, tense and aspect is established as
null (?NONE? value), and non-finite morphology is
?noun? or ?adjective?, depending on the part-of-
speech of their head.
Modality and polarity are the two remaining
morphology-based features identified here. Evita
extracts the values of these two attributes using ba-
sic pattern-matching techniques over the approapri-
ate verbal, nominal, or adjectival chunk.
On the other hand, the identification of event class
cannot rely on linguistic cues such as the morphol-
ogy of the expression. Instead, it requires a combi-
nation of lexical resource-based look-up and word
sense disambiguation. At present, this task has been
attempted only in a very preliminary way, by tagging
events with the class that was most frequently as-
signed to them in TimeBank1.2. Despite the limita-
tions of such a treatment, the accuracy ratio is fairly
good (refer to section 3).
2.5 Clustering of Chunks
In some cases, the chunker applied at the prepro-
cessing stage identifies two independent constituents
that contribute information about the same event.
This may be due to a chunker error, but it is also sys-
tematically the case in verbal phrases containing the
have to modal form or the be going to future form
(Figure 2).
<VG>
<VX><lex pos="VBD">had</lex></VX>
</VG>
<VG-INF>
<INF><lex pos="TO">to</lex>
<lex pos="VB">say</lex>
</INF>
</VG-INF>
Figure 2: have to VP
It may be also necessary in verbal phrases with
other modal auxiliaries, or with auxiliary forms of
the have, do, or be forms, in which the auxiliary part
is split off the main verb because of the presence of
an adverbial phrase or similar (Figure 3).
<VG>
<VX><lex pos="VBZ">has</lex></VX>
</VG>
<lex pos=",">,</lex>
<lex pos="IN">of</lex>
<NG>
<HEAD><lex pos="NN">course</lex></HEAD>
</NG>
<lex pos=",">,</lex>
<VG>
<VX><lex pos="VBD">tried</lex></VX>
</VG>
Figure 3: have V en VP
Constructions with copular verbs are another kind
of context which requires clustering of chunks, in
order to group together the verbal chunk corre-
sponding to the copular predicate and the non-verbal
chunk that functions as its predicative complement.
In all these cases, additional syntactic parsing is
needed for the tasks of event recognition and gram-
matical feature identification, in order to cluster to-
gether the two independent chunks.
704
The task of clustering chunks into bigger ones is
activated by specific triggers (e.g., a chunk headed
by an auxiliary form, or a chunk headed by the cop-
ular verb be) and carried out locally in the context of
that trigger. For each trigger, there is a set of gram-
matical patterns describing the possible structures it
can be a constituent of. The form have, for instance,
may be followed by an infinitival phrase to V, con-
stituting part of the modal form have to in the big-
ger verbal group have to V, as in Figure 2 above, or
it may also be followed by a past participle-headed
chunk, with which it forms a bigger verbal phrase
have V-en expressing perfective aspect (Figure 3).
The grammatical patterns established for each
trigger are written using the standard syntax of reg-
ular expressions, allowing for a greater expressive-
ness in the description of sequences of chunks (op-
tionality of elements, inclusion of adverbial phrases
and punctuation marks, variability in length, etc.).
These patterns are then compiled into finite state au-
tomata that work with grammatical objects instead
of string characters. Such an approach is based on
well-established techniques using finite-state meth-
ods (see for instance Koskenniemi, 1992; Appelt et
al. 1993; Karttunen et al, 1996; Grefenstette, 1996,
among others).
Evita sequentially feeds each of the FSAs for the
current trigger with the right-side part of the trigger
context (up to the first sentence boundary), which is
represented as a sequence of grammatical objects. If
one of the FSAs accepts this sequence or a subpart
of it, then the clustering operation is applied on the
chunks within the accepted (sub)sequence.
3 Results
Evaluation of Evita has been carried out by com-
paring its performance against TimeBank1.2. The
current performance of Evita is at 74.03% precision,
87.31% recall, for a resulting F-measure of 80.12%
(with  =0.5). These results are comparable to the
interannotation agreement scores for the task of tag-
ging verbal and nominal events, by graduate lin-
guistics students with only basic training (Table 1).3
By basic training we understand that they had read
3These figures are also in terms of F-measure. See Hripcsak
and Rothschild (2005) for the use of such metric in order to
quantify interannotator reliability.
the guidelines, had been given some additional ad-
vice, and subsequently annotated over 10 documents
before annotating those used in the interannotation
evaluation. They did not, however, have any meet-
ings amongst themselves in order to discuss issues
or to agree on a common strategy.
Category F-measure
Nouns 64%
Verbs 80%
Table 1: Interannotation Agreement
On the other hand, the Accuracy ratio (i.e., the
percentage of values Evita marked according to the
gold standard) on the identification of event gram-
matical features is as shown:
Feature Accuracy
polarity 98.26%
aspect 97.87%
modality 97.02%
tense 92.05%
nf morph 89.95%
class 86.26%
Table 2: Accuracy of Grammatical Features
Accuracy for polarity, aspect, and modality is op-
timal: over 97% in all three cases. In fact, we were
expecting a lower accuracy for polarity, since Evita
relies only on the polarity elements present in the
chunk containg the event, but does not take into ac-
count non-local forms of expressing polarity in En-
glish, such as negative polarity on the subject of a
sentence (as in Nobody saw him or in No victims
were found).
The slightly lower ratio for tense and nf morph is
in most of the cases due to problems from the POS
tagger used in the preprocessing step, since tense
and non-finite morphology values are mainly based
on its result. Some common POS tagging mistakes
deriving on tense and nf morph errors are, for in-
stance, identifying a present form as the base form
of the verb, a simple past form as a past participle
form, or vice versa. Errors in the nf morph value are
also due to the difficulty in distinguishing sometimes
between present participle and noun (for ing-forms),
or between past participle and adjective.
705
The lowest score is for event class, which never-
theless is in the 80s%. This is the only feature that
cannot be obtained based on surface cues. Evita?s
treatment of this feature is still very basic, and we
envision that it can be easily enhanced by exploring
standard word sense disambiguation techniques.
4 Discussion and Conclusions
We have presented Evita, a tool for recognizing and
tagging events in natural language text. To our
knowledge, this is a unique tool within the commu-
nity, in that it is not based on any pre-established
list of event patterns, nor is it restricted to a specific
domain. In addition, Evita identifies the grammat-
ical information that is associated with the event-
referring expression, such as tense, aspect, polarity,
and modality. The characterization of these features
is based on explicit linguistic cues. Unlike other
work on event recognition, Evita does not attempt
to identify event participants, but relies on the use of
entity taggers for the linking of arguments to events.
Evita combines linguistic- and statistically-based
knowledge to better address each particular subtask
of the event recognition problem. Linguistic knowl-
edge has been used for the parsing of very local and
controlled contexts, such as verbal phrases, and the
extraction of morphologically explicit information.
On the other hand, statistical knowledge has con-
tributed to the process of disambiguation of nomi-
nal events, following the current trend in the Word
Sense Disambiguation field.
Our tool is grounded on simple and well-known
technologies; namely, a standard preprocessing
stage, finite state techniques, and Bayesian-based
techniques for word sense disambiguation. In ad-
dition, it is conceived from a highly modular per-
spective. Thus, an effort has been put on separating
linguistic knowledge from the processing thread. In
this way we guarantee a low-cost maintainance of
the system, and simplify the task of enriching the
grammatical knowledge (which can be carried out
even by naive programmers such as linguists) when
additional data is obtained from corpus exploitation.
Evita is a component within a larger suite of tools.
It is one of the steps within a processing sequence
which aims at providing basic semantic information
(such as temporal relations or intensional context
boundaries) to applications like Question Answer-
ing or Narrative Understanding, for which text un-
derstanding is shown to be fundamental, in addition
to shallow-based techniques. Nonetheless, Evita can
also be used independently for purposes other than
those above.
Additional tools within the TimeML research
framework are (a) GUTime, a recognizer of tempo-
ral expressions which extends Tempex for TimeML
(Mani, 2005), (b) a tool devoted to the temporal or-
dering and anchoring of events (Mani and Schiff-
man, forthcoming), and (c) Slinket, an application
in charge of identifying subordination contexts that
introduce intensional events like those exemplified
in (1-2) (Verhagen et al, 2005). Together with these,
Evita provides capabilities for a more adequate treat-
ment of temporal and intensional information in tex-
tual sources, thereby contributing towards incorpo-
rating greater inferential capabilities to applications
within QA and related fields, a requisite that has
been shown necessary in the Introduction section.
Further work on Evita will be focused on two
main areas: (1) improving the sense disambiguation
of candidates to event nominals by experimenting
with additional learning techniques, and (2) improv-
ing event classification. The accuracy ratio for this
latter task is already fairly acceptable (86.26%), but
it still needs to be enhanced in order to guarantee an
optimal detection of subordinating intensional con-
texts (recall examples 1-2). Both lines of work will
involve the exploration and use of word sense dis-
ambiguation techniques.
References
Appelt, Douglas E., Jerry R. Hobbs, John Bear, David
Israel and Mabry Tyson 1993. ?FASTUS: A Finite-
state Processor for Information Extraction from Real-
world Text?. Proceedings IJCAI-93.
Brill, Eric, Susan Dumais and Michele Banko. 2002.
?An Analysis of the AskMSR Question Answering
System?. Proceedings of EMNLP 2002.
Day, David,, John Aberdeen, Lynette Hirschman, Robyn
Kozierok, Patricia Robinson and Marc Vilain. 1997.
?Mixed-Initiative Development of Language Process-
ing Systems?. Fifth Conference on Applied Natural
Language Processing Systems: 88?95.
Grefenstette, Gregory. 1996. ?Light Parsing as Finite-
State Filtering?. Workshop on Extended Finite State
Models of Language, ECAI?96.
706
Harabagiu, S., D. Moldovan, C. Clark, M. Bowden, J.
Williams and J. Bensley. 2003. ?Answer Mining
by Combining Extraction Techniques with Abductive
Reasoning?. Proceedings of the Text Retrieval Confer-
ence, TREC 2003: 375-382.
Hovy, Eduard, Ulf Hermjakob and Deepak Ravichan-
dran. 2002. A Question/Answer Typology with Sur-
face Text Patterns. Proceedings of the Second Inter-
national Conference on Human Language Technology
Research, HLT 2002: 247-251.
Hripcsak, George and Adam S. Rothschild. 2005.
?Agreement, the F-measure, and reliability in informa-
tion retrieval?. Journal of the American Medical Infor-
matics Association, 12: 296-298.
Karttunen, L., J-P. Chanod, G. Grefenstette and A.
Schiller. 1996. ?Regular Expressions for Language
Engineering?. Natural Language Engineering, 2(4).
Koskenniemi, Kimmo, Pasi Tapanainen and Atro Vouti-
lainen. ?Compiling and Using Finite-State Syntactic
Rules?. Proceedings of COLING-92: 156-162.
Lapata, Maria and Alex Lascarides 2004. Inferring
Sentence-Internal Temporal Relations. Proceedings of
HLT-NAACL 2004.
Mani, Inderjeet. 2005. Time Expression Tagger and
Normalizer. http://complingone.georgetown.edu/ lin-
guist/GU TIME DOWNLOAD.HTML
Mani, Inderjeet and Barry Schiffman. Forthcom-
ing. ?Temporally Anchoring and Ordering Events in
News?. James Pustejovsky and Robert Gaizauskas
(eds.) Event Recognition in Natural Language. John
Benjamins.
Moldovan, D., S. Harabagiu, R. Girju, P. Morarescu, F.
Lacatusu, A. Novischi, A. Badulescu and O. Bolohan.
2002. ?LCC Tools for Question Answering?. Proceed-
ings of the Text REtrieval Conference, TREC 2002.
Pustejovsky, J., J. Castan?o, R. Ingria, R. Saur??, R.
Gaizauskas, A. Setzer, and G. Katz. 2003a. TimeML:
Robust Specification of Event and Temporal Expres-
sions in Text. IWCS-5 Fifth International Workshop
on Computational Semantics.
Pustejovsky, James and Rob Gaizauskas (editors) (forth-
coming) Reasoning about Time and Events. John
Benjamins Publishers.
Pustejovsky, J., P. Hanks, R. Saur??, A. See, R.
Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D.
Day, L. Ferro and M. Lazo. 2003b. The TIME-
BANK Corpus. Proceedings of Corpus Linguistics
2003: 647-656.
Pustejovsky, J., B. Knippen, J. Littman, R. Saur?? (forth-
coming) Temporal and Event Information in Natural
language Text. Language Resources and Evaluation.
Pustejovsky, James, Martha Palmer and Adam Meyers.
2005. Workshop on Frontiers in Corpus Annotation
II. Pie in the Sky. ACL 2005.
Pustejovsky, J., R. Saur??, J. Castan?o, D. R. Radev, R.
Gaizauskas, A. Setzer, B. Sundheim and G. Katz.
2004. Representing Temporal and Event Knowledge
for QA Systems. Mark T. Maybury (ed.) New Direc-
tions in Question Answering. MIT Press, Cambridge.
Ravichandran, Deepak and Eduard Hovy. 2002. ?Learn-
ing Surface Text Patterns for a Question Answering
System?. Proceedings of the ACL 2002.
Saur??, Roser, Jessica Littman, Robert Knippen, Rob
Gaizauskas, Andrea Setzer and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Saur??, Roser and Marc Verhagen. 2005. Temporal Infor-
mation in Intensional Contexts. Bunt, H., J. Geertzen
and E. Thijse (eds.) Proceedings of the Sixth In-
ternational Workshop on Computational Semantics.
Tilburg, Tilburg University: 404-406.
Small, Sharon, Liu Ting, Nobuyuki Shimuzu and Tomek
Strzalkowski. 2003. HITIQA, An interactive question
answering system: A preliminary report. Proceedings
of the ACL 2003 Workshop on Multilingual Summa-
rization and Question Answering.
Soricut, Radu and Eric Brill. 2004. Automatic Ques-
tion Answering: Beyond the Factoid. HLT-NAACL
2004, Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: 57-64.
Soubbotin, Martin M. and Sergei M. Soubbotin. 2002.
?Use of Patterns for Detection of Answer Strings: A
Systematic Approach?. Proceedings of TREC-11.
Verhagen, Marc. 2004. Times Between the Lines. Ph.D.
thesis. Brandeis University. Waltham, MA, USA.
Verhagen, Marc and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. James Pustejovsky and Robert
Gaizauskas (eds.) Time and Event Recognition in Nat-
ural Language. John Benjamin Publications.
Verhagen, Marc, Inderjeet Mani, Roser Saur??, Robert
Knippen, Jess Littman and James Pustejovsky. 2005.
?Automating Temporal Annotation with TARSQI?.
Demo Session. Proceedings of the ACL 2005.
Voorhees, Ellen M. 2002. ?Overview of the TREC
2002 Question Answering Track?. Proceedings of the
Eleventh Text REtrieval Conference, TREC 2002.
Voorhees, Ellen M. 2003. ?Overview of the TREC 2003
Question Answering Track?. Proceedings of 2003
Text REtrieval Conference, TREC 2003.
707
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 81?84, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automating Temporal Annotation with TARSQI
Marc Verhagen?, Inderjeet Mani?, Roser Sauri?,
Robert Knippen?, Seok Bae Jang?, Jessica Littman?,
Anna Rumshisky?, John Phillips?, James Pustejovsky?
? Department of Computer Science, Brandeis University, Waltham, MA 02254, USA
{marc,roser,knippen,jlittman,arum,jamesp}@cs.brandeis.edu
? Computational Linguistics, Georgetown University, Washington DC, USA
{im5,sbj3,jbp24}@georgetown.edu
Abstract
We present an overview of TARSQI, a
modular system for automatic temporal
annotation that adds time expressions,
events and temporal relations to news
texts.
1 Introduction
The TARSQI Project (Temporal Awareness and
Reasoning Systems for Question Interpretation)
aims to enhance natural language question an-
swering systems so that temporally-based questions
about the events and entities in news articles can be
addressed appropriately. In order to answer those
questions we need to know the temporal ordering of
events in a text. Ideally, we would have a total order-
ing of all events in a text. That is, we want an event
like marched in ethnic Albanians marched Sunday
in downtown Istanbul to be not only temporally re-
lated to the nearby time expression Sunday but also
ordered with respect to all other events in the text.
We use TimeML (Pustejovsky et al, 2003; Saur?? et
al., 2004) as an annotation language for temporal
markup. TimeML marks time expressions with the
TIMEX3 tag, events with the EVENT tag, and tempo-
ral links with the TLINK tag. In addition, syntactic
subordination of events, which often has temporal
implications, can be annotated with the SLINK tag.
A complete manual TimeML annotation is not
feasible due to the complexity of the task and the
sheer amount of news text that awaits processing.
The TARSQI system can be used stand-alone
or as a means to alleviate the tasks of human
annotators. Parts of it have been intergrated in
Tango, a graphical annotation environment for event
ordering (Verhagen and Knippen, Forthcoming).
The system is set up as a cascade of modules
that successively add more and more TimeML
annotation to a document. The input is assumed to
be part-of-speech tagged and chunked. The overall
system architecture is laid out in the diagram below.
Input Documents
GUTime
Evita
SlinketGUTenLINK
SputLink
TimeML Documents
In the following sections we describe the five
TARSQI modules that add TimeML markup to news
texts.
2 GUTime
The GUTime tagger, developed at Georgetown Uni-
versity, extends the capabilities of the TempEx tag-
ger (Mani and Wilson, 2000). TempEx, developed
81
at MITRE, is aimed at the ACE TIMEX2 standard
(timex2.mitre.org) for recognizing the extents and
normalized values of time expressions. TempEx
handles both absolute times (e.g., June 2, 2003) and
relative times (e.g., Thursday) by means of a num-
ber of tests on the local context. Lexical triggers like
today, yesterday, and tomorrow, when used in a spe-
cific sense, as well as words which indicate a posi-
tional offset, like next month, last year, this coming
Thursday are resolved based on computing direc-
tion and magnitude with respect to a reference time,
which is usually the document publication time.
GUTime extends TempEx to handle time ex-
pressions based on the TimeML TIMEX3 standard
(timeml.org), which allows a functional style of en-
coding offsets in time expressions. For example, last
week could be represented not only by the time value
but also by an expression that could be evaluated to
compute the value, namely, that it is the week pre-
ceding the week of the document date. GUTime also
handles a variety of ACE TIMEX2 expressions not
covered by TempEx, including durations, a variety
of temporal modifiers, and European date formats.
GUTime has been benchmarked on training data
from the Time Expression Recognition and Normal-
ization task (timex2.mitre.org/tern.html) at .85, .78,
and .82 F-measure for timex2, text, and val fields
respectively.
3 EVITA
Evita (Events in Text Analyzer) is an event recogni-
tion tool that performs two main tasks: robust event
identification and analysis of grammatical features,
such as tense and aspect. Event identification is
based on the notion of event as defined in TimeML.
Different strategies are used for identifying events
within the categories of verb, noun, and adjective.
Event identification of verbs is based on a lexi-
cal look-up, accompanied by a minimal contextual
parsing, in order to exclude weak stative predicates
such as be or have. Identifying events expressed by
nouns, on the other hand, involves a disambigua-
tion phase in addition to lexical lookup. Machine
learning techniques are used to determine when an
ambiguous noun is used with an event sense. Fi-
nally, identifying adjectival events takes the conser-
vative approach of tagging as events only those ad-
jectives that have been lexically pre-selected from
TimeBank1, whenever they appear as the head of a
predicative complement. For each element identi-
fied as denoting an event, a set of linguistic rules
is applied in order to obtain its temporally relevant
grammatical features, like tense and aspect. Evita
relies on preprocessed input with part-of-speech tags
and chunks. Current performance of Evita against
TimeBank is .75 precision, .87 recall, and .80 F-
measure. The low precision is mostly due to Evita?s
over-generation of generic events, which were not
annotated in TimeBank.
4 GUTenLINK
Georgetown?s GUTenLINK TLINK tagger uses
hand-developed syntactic and lexical rules. It han-
dles three different cases at present: (i) the event
is anchored without a signal to a time expression
within the same clause, (ii) the event is anchored
without a signal to the document date speech time
frame (as in the case of reporting verbs in news,
which are often at or offset slightly from the speech
time), and (iii) the event in a main clause is anchored
with a signal or tense/aspect cue to the event in the
main clause of the previous sentence. In case (iii), a
finite state transducer is used to infer the likely tem-
poral relation between the events based on TimeML
tense and aspect features of each event. For ex-
ample, a past tense non-stative verb followed by a
past perfect non-stative verb, with grammatical as-
pect maintained, suggests that the second event pre-
cedes the first.
GUTenLINK uses default rules for ordering
events; its handling of successive past tense non-
stative verbs in case (iii) will not correctly or-
der sequences like Max fell. John pushed him.
GUTenLINK is intended as one component in a
larger machine-learning based framework for order-
ing events. Another component which will be de-
veloped will leverage document-level inference, as
in the machine learning approach of (Mani et al,
2003), which required annotation of a reference time
(Reichenbach, 1947; Kamp and Reyle, 1993) for the
event in each finite clause.
1TimeBank is a 200-document news corpus manually anno-
tated with TimeML tags. It contains about 8000 events, 2100
time expressions, 5700 TLINKs and 2600 SLINKs. See (Day
et al, 2003) and www.timeml.org for more details.
82
An early version of GUTenLINK was scored at
.75 precision on 10 documents. More formal Pre-
cision and Recall scoring is underway, but it com-
pares favorably with an earlier approach developed
at Georgetown. That approach converted event-
event TLINKs from TimeBank 1.0 into feature vec-
tors where the TLINK relation type was used as the
class label (some classes were collapsed). A C5.0
decision rule learner trained on that data obtained an
accuracy of .54 F-measure, with the low score being
due mainly to data sparseness.
5 Slinket
Slinket (SLINK Events in Text) is an application
currently being developed. Its purpose is to automat-
ically introduce SLINKs, which in TimeML specify
subordinating relations between pairs of events, and
classify them into factive, counterfactive, evidential,
negative evidential, and modal, based on the modal
force of the subordinating event. Slinket requires
chunked input with events.
SLINKs are introduced by a well-delimited sub-
group of verbal and nominal predicates (such as re-
gret, say, promise and attempt), and in most cases
clearly signaled by the context of subordination.
Slinket thus relies on a combination of lexical and
syntactic knowledge. Lexical information is used to
pre-select events that may introduce SLINKs. Pred-
icate classes are taken from (Kiparsky and Kiparsky,
1970; Karttunen, 1971; Hooper, 1975) and subse-
quent elaborations of that work, as well as induced
from the TimeBank corpus. A syntactic module
is applied in order to properly identify the subor-
dinated event, if any. This module is built as a
cascade of shallow syntactic tasks such as clause
boundary recognition and subject and object tag-
ging. Such tasks are informed from both linguistic-
based knowledge (Papageorgiou, 1997; Leffa, 1998)
and corpora-induced rules (Sang and De?je?an, 2001);
they are currently being implemented as sequences
of finite-state transducers along the lines of (A??t-
Mokhtar and Chanod, 1997). Evaluation results are
not yet available.
6 SputLink
SputLink is a temporal closure component that takes
known temporal relations in a text and derives new
implied relations from them, in effect making ex-
plicit what was implicit. A temporal closure compo-
nent helps to find those global links that are not nec-
essarily derived by other means. SputLink is based
on James Allen?s interval algebra (1983) and was in-
spired by (Setzer, 2001) and (Katz and Arosio, 2001)
who both added a closure component to an annota-
tion environment.
Allen reduces all events and time expressions to
intervals and identifies 13 basic relations between
the intervals. The temporal information in a doc-
ument is represented as a graph where events and
time expressions form the nodes and temporal re-
lations label the edges. The SputLink algorithm,
like Allen?s, is basically a constraint propagation al-
gorithm that uses a transitivity table to model the
compositional behavior of all pairs of relations. For
example, if A precedes B and B precedes C, then
we can compose the two relations and infer that A
precedes C. Allen allowed unlimited disjunctions of
temporal relations on the edges and he acknowl-
edged that inconsistency detection is not tractable
in his algebra. One of SputLink?s aims is to ensure
consistency, therefore it uses a restricted version of
Allen?s algebra proposed by (Vilain et al, 1990). In-
consistency detection is tractable in this restricted al-
gebra.
A SputLink evaluation on TimeBank showed that
SputLink more than quadrupled the amount of tem-
poral links in TimeBank, from 4200 to 17500.
Moreover, closure adds non-local links that were
systematically missed by the human annotators. Ex-
perimentation also showed that temporal closure al-
lows one to structure the annotation task in such
a way that it becomes possible to create a com-
plete annotation from local temporal links only. See
(Verhagen, 2004) for more details.
7 Conclusion and Future Work
The TARSQI system generates temporal informa-
tion in news texts. The five modules presented here
are held together by the TimeML annotation lan-
guage and add time expressions (GUTime), events
(Evita), subordination relations between events
(Slinket), local temporal relations between times and
events (GUTenLINK), and global temporal relations
between times and events (SputLink).
83
In the nearby future, we will experiment with
more strategies to extract temporal relations from
texts. One avenue is to exploit temporal regularities
in SLINKs, in effect using the output of Slinket as
a means to derive even more TLINKs. We are also
compiling more annotated data in order to provide
more training data for machine learning approaches
to TLINK extraction. SputLink currently uses only
qualitative temporal infomation, it will be extended
to use quantitative information, allowing it to reason
over durations.
References
Salah A??t-Mokhtar and Jean-Pierre Chanod. 1997. Sub-
ject and Object Dependency Extraction Using Finite-
State Transducers. In Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications. ACL/EACL-97 Workshop Proceed-
ings, pages 71?77, Madrid, Spain. Association for
Computational Linguistics.
James Allen. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
David Day, Lisa Ferro, Robert Gaizauskas, Patrick
Hanks, Marcia Lazo, James Pustejovsky, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics.
Joan Hooper. 1975. On Assertive Predicates. In John
Kimball, editor, Syntax and Semantics, volume IV,
pages 91?124. Academic Press, New York.
Hans Kamp and Uwe Reyle, 1993. From Discourse to
Logic, chapter 5, Tense and Aspect, pages 483?546.
Kluwer Academic Publishers, Dordrecht, Netherlands.
Lauri Karttunen. 1971. Some Observations on Factivity.
In Papers in Linguistics, volume 4, pages 55?69.
Graham Katz and Fabrizio Arosio. 2001. The Anno-
tation of Temporal Information in Natural Language
Sentences. In Proceedings of ACL-EACL 2001, Work-
shop for Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France. Association for
Computational Linguistics.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics. A collection of Papers, pages
143?173. Mouton, Paris.
Vilson Leffa. 1998. Clause Processing in Complex Sen-
tences. In Proceedings of the First International Con-
ference on Language Resources and Evaluation, vol-
ume 1, pages 937?943, Granada, Spain. ELRA.
Inderjeet Mani and George Wilson. 2000. Processing
of News. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL2000), pages 69?76.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
Short Paper. In Proceedings of the Human Language
Technology Conference (HLT-NAACL?03).
Harris Papageorgiou. 1997. Clause Recognition in the
Framework of Allignment. In Ruslan Mitkov and
Nicolas Nicolov, editors, Recent Advances in Natural
Language Recognition. John Benjamins, Amsterdam,
The Netherlands.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In IWCS-5 Fifth
International Workshop on Computational Semantics.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Tjong Kim Sang and Erik Herve De?je?an. 2001. Introduc-
tion to the CoNLL-2001 Shared Task: Clause Identifi-
cation. In Proceedings of the Fifth Workshop on Com-
putational Language Learning (CoNLL-2001), pages
53?57, Toulouse, France. ACL.
Roser Saur??, Jessica Littman, Robert Knippen, Robert
Gaizauskas, Andrea Setzer, and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Marc Verhagen and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. In James Pustejovsky and Robert
Gaizauskas, editors, Time and Event Recognition in
Natural Language. John Benjamin Publications.
Marc Verhagen. 2004. Times Between The Lines. Ph.D.
thesis, Brandeis University, Waltham, Massachusetts,
USA.
Marc Vilain, Henry Kautz, and Peter van Beek. 1990.
Constraint propagation algorithms: A revised report.
In D. S. Weld and J. de Kleer, editors, Qualitative Rea-
soning about Physical Systems, pages 373?381. Mor-
gan Kaufman, San Mateo, California.
84
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine Learning of Temporal Relations 
Inderjeet Mani??, Marc Verhagen?, Ben Wellner?? 
Chong Min Lee? and James Pustejovsky? 
?The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
?Department of Linguistics, Georgetown University 
37th and O Streets, Washington, DC 20036, USA 
?Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.edu 
Abstract 
This paper investigates a machine learn-
ing approach for temporally ordering and 
anchoring events in natural language 
texts. To address data sparseness, we 
used temporal reasoning as an over-
sampling method to dramatically expand 
the amount of training data, resulting in 
predictive accuracy on link labeling as 
high as 93% using a Maximum Entropy 
classifier on human annotated data. This 
method compared favorably against a se-
ries of increasingly sophisticated base-
lines involving expansion of rules de-
rived from human intuitions. 
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event.  
A wealth of prior research by (Passoneau 
1988), (Webber 1988), (Hwang and Schubert 
1992), (Kamp and Reyle 1993), (Lascarides and 
Asher 1993), (Hitzeman et al 1995), (Kehler 
2000) and others, has explored the different 
knowledge sources used in inferring the temporal 
ordering of events, including temporal adver-
bials, tense, aspect, rhetorical relations, prag-
matic conventions, and background knowledge. 
For example, the narrative convention of events 
being described in the order in which they occur 
is followed in (1), but overridden by means of a 
discourse relation, Explanation in (2).  
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him.  
In addition to discourse relations, which often 
require inferences based on world knowledge, 
the ordering decisions humans carry out appear 
to involve a variety of knowledge sources, in-
cluding tense and grammatical aspect (3a), lexi-
cal aspect (3b), and temporal adverbials (3c): 
(3a) Max entered the room. He had drunk a lot 
of wine.  
(3b) Max entered the room. Mary was seated 
behind the desk.  
(3c) The company announced Tuesday that 
third-quarter sales had fallen.  
Clearly, substantial linguistic processing may 
be required for a system to make these infer-
ences, and world knowledge is hard to make 
available to a domain-independent program. An 
important strategy in this area is of course the 
development of annotated corpora than can fa-
cilitate the machine learning of such ordering 
inferences. 
This paper 1  investigates a machine learning 
approach for temporally ordering events in natu-
ral language texts. In Section 2, we describe the 
annotation scheme and annotated corpora, and 
the challenges posed by them. A basic learning 
approach is described in Section 3. To address 
data sparseness, we used temporal reasoning as 
an over-sampling method to dramatically expand 
the amount of training data.  
As we will discuss in Section 5, there are no 
standard algorithms for making these inferences 
that we can compare against. We believe 
strongly that in such situations, it?s worthwhile 
for computational linguists to devote consider-
                                                 
1Research at Georgetown and Brandeis on this prob-
lem was funded in part by a grant from the ARDA 
AQUAINT Program, Phase II.  
753
able effort to developing insightful baselines. 
Our work is, accordingly, evaluated in compari-
son against four baselines: (i) the usual majority 
class statistical baseline, shown along with each 
result, (ii) a more sophisticated baseline that uses 
hand-coded rules (Section 4.1), (iii) a hybrid 
baseline based on hand-coded rules expanded 
with Google-induced rules (Section 4.2), and (iv) 
a machine learning version that learns from im-
perfect annotation produced by (ii) (Section 4.3).  
2 Annotation Scheme and Corpora 
2.1 TimeML 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their temporal rela-
tions in news articles. The TimeML scheme flags 
tensed verbs, adjectives, and nominals with 
EVENT tags with various attributes, including 
the class of event, tense, grammatical aspect, po-
larity (negative or positive), any modal operators 
which govern the event being tagged, and cardi-
nality of the event if it?s mentioned more than 
once. Likewise, time expressions are flagged and 
their values normalized, based on TIMEX3, an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme.  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given (3a), a 
TLINK tag orders an instance of the event of 
entering to an instance of the drinking with the 
relation type AFTER. Likewise, given the sen-
tence (3c), a TLINK tag will anchor the event 
instance of announcing to the time expression 
Tuesday (whose normalized value will be in-
ferred from context), with the relation 
IS_INCLUDED. These inferences are shown (in 
slightly abbreviated form) in the annotations in 
(4) and (5). 
(4) Max <EVENT eventID=?e1? 
class=?occurrence? tense=?past? as-
pect=?none?>entered</EVENT> the room. 
He <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?>had drunk</EVENT>a 
lot of wine.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
 (5) The company <EVENT even-
tID=?e1? class=?reporting? 
tense=?past? as-
pect=?none?>announced</EVENT> 
<TIMEX3 tid=?t2? type=?DATE? tempo-
ralFunction=?false? value=?1998-01-
08?>Tuesday </TIMEX3> that third-
quarter sales <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?> had fallen</EVENT>.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
<TLINK eventID=?e1? relatedTo-
TimeID=?t2? relType=?IS_INCLUDED?/> 
 
The anchor relation is an Event-Time TLINK, 
and the order relation is an Event-Event TLINK. 
TimeML uses 14 temporal relations in the 
TLINK RelTypes, which reduce to a disjunctive 
classification of 6 temporal relations RelTypes = 
{SIMULTANEOUS, IBEFORE, BEFORE, BE-
GINS, ENDS, INCLUDES}. An event or time is 
SIMULTANEOUS with another event or time if 
they occupy the same time interval. An event or 
time INCLUDES another event or time if the 
latter occupies a proper subinterval of the former. 
These 6 relations and their inverses map one-to-
one to 12 of Allen?s 13 basic relations (Allen 
1984)2. There has been a considerable amount of 
activity related to this scheme; we focus here on 
some of the challenges posed by the TLINK an-
notation, the part that is directly relevant to the 
temporal ordering and anchoring problems. 
2.2 Challenges 
The annotation of TimeML information is on a 
par with other challenging semantic annotation 
schemes, like PropBank, RST annotation, etc., 
where high inter-annotator reliability is crucial 
but not always achievable without massive pre-
processing to reduce the user?s workload. In Ti-
meML, inter-annotator agreement for time ex-
pressions and events is 0.83 and 0.78 (average of 
Precision and Recall) respectively, but on 
TLINKs it is 0.55 (P&R average), due to the 
large number of event pairs that can be selected 
for comparison. The time complexity of the hu-
man TLINK annotation task is quadratic in the 
number of events and times in the document. 
Two corpora have been released based on Ti-
meML: the TimeBank (Pustejovsky et al 2003) 
(we use version 1.2.a) with 186 documents and 
                                                 
2Of the 14 TLINK relations, the 6 inverse relations are re-
dundant. In order to have a disjunctive classification, SI-
MULTANEOUS and IDENTITY are collapsed, since 
IDENTITY is a subtype of SIMULTANEOUS. (Specifi-
cally, X and Y are identical if they are simultaneous and 
coreferential.) DURING and IS_INCLUDED are collapsed 
since DURING is a subtype of IS_INCLUDED that anchors 
events to times that are durations. IBEFORE (immediately 
before) corresponds to Allen?s MEETS. Allen?s OVER-
LAPS relation is not represented in TimeML. More details 
can be found at timeml.org. 
754
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. The TimeBank was developed in 
the early stages of TimeML development, and 
was partitioned across five annotators with dif-
ferent levels of expertise. The Opinion Corpus 
was developed very recently, and was partitioned 
across just two highly trained annotators, and 
could therefore be expected to be less noisy. In 
our experiments, we merged the two datasets to 
produce a single corpus, called OTC. 
Table 1 shows the distribution of EVENTs and 
TIMES, and TLINK RelTypes3 in the OTC. The 
majority class percentages are shown in paren-
theses. It can be seen that BEFORE and SI-
MULTANEOUS together form a majority of 
event-ordering (Event-Event) links, whereas 
most of the event anchoring (Event-Time) links 
are INCLUDES.  
 
12750 Events, 2114 Times 
Relation Event-Event Event-Time 
IBEFORE 131 15 
BEGINS 160 112 
ENDS 208 159 
SIMULTANEOUS 1528 77 
INCLUDES 950 3001 (65.3%) 
BEFORE 3170 (51.6%) 1229 
TOTAL 6147 4593 
Table 1. TLINK Class Distributions in OTC 
Corpus 
 
The lack of TLINK coverage in human anno-
tation could be helped by preprocessing, pro-
vided it meets some threshold of accuracy. Given 
the availability of a corpus like OTC, it is natural 
to try a machine learning approach to see if it can 
be used to provide that preprocessing. However, 
the noise in the corpus and the sparseness of 
links present challenges to a learning approach. 
3 Machine Learning Approach 
3.1 Initial Learner 
There are several sub-problems related to in-
ferring event anchoring and event ordering. Once 
a tagger has tagged the events and times, the first 
task (A) is to link events and/or times, and the 
second task (B) is to label the links. Task A is 
hard to evaluate since, in the absence of massive 
preprocessing, many links are ignored by the 
human in creating the annotated corpora. In addi-
                                                 
3The number of TLINKs shown is based on the number of 
TLINK vectors extracted from the OTC. 
tion, a program, as a baseline, can trivially link 
all tagged events and times, getting 100% recall 
on Task A. We focus here on Task B, the label-
ing task. In the case of humans, in fact, when a 
TLINK is posited by both annotators between the 
same pairs of events or times, the inter-annotator 
agreement on the labels is a .77 average of P&R. 
To ensure replicability of results, we assume per-
fect (i.e., OTC-supplied) events, times, and links.  
Thus, we can consider TLINK inference as the 
following classification problem: given an or-
dered pair of elements X and Y, where X and Y 
are events or times which the human has related 
temporally via a TLINK, the classifier has to as-
sign a label in RelTypes. Using RelTypes instead 
of RelTypes ?  {NONE} also avoids the prob-
lem of heavily skewing the data towards the 
NONE class.  
To construct feature vectors for machine 
learning, we took each TLINK in the corpus and 
used the given TimeML features, with the 
TLINK class being the vector?s class feature.  
For replicability by other users of these corpora, 
and to be able to isolate the effect of components, 
we used ?perfect? features; no feature engineer-
ing was attempted. The features were, for each 
event in an event-ordering pair, the event-class, 
aspect, modality, tense and negation (all nominal 
features); event string, and signal (a preposi-
tion/adverb, e.g., reported on Tuesday), which 
are string features, and contextual features indi-
cating whether the same tense and same aspect 
are true of both elements in the event pair. For 
event-time links, we used the above event and 
signal features along with TIMEX3 time features. 
For learning, we used an off-the-shelf Maxi-
mum Entropy (ME) classifier (from Carafe, 
available at sourceforge.net/projects/carafe). As 
shown in the UNCLOSED (ME) column in Ta-
ble 24, accuracy of the unclosed ME classifier 
does not go above 77%, though it?s always better 
than the majority class (in parentheses). We also 
tried a variety of other classifiers, including the 
SMO support-vector machine and the na?ve 
Bayes tools in WEKA (www.weka.net.nz). SMO 
performance (but not na?ve Bayes) was compa-
rable with ME, with SMO trailing it in a few 
cases (to save space, we report just ME perform-
ance). It?s possible that feature engineering could 
improve performance, but since this is ?perfect? 
data, the result is not encouraging.  
                                                 
4All machine learning results, except for ME-C in Table 4, 
use 10-fold cross-validation. ?Accuracy? in tables is Predic-
tive Accuracy. 
755
 
 
 UNCLOSED (ME) CLOSED (ME-C) 
 Event-Event Event-Time Event-Event Event-Time 
Accuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3) 
Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
IBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0 
BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36 
ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62 
SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63 
INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34 
BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83 
 
Table 2. Machine learning results using unclosed and closed data
 
3.2 Expanding Training Data using Tem-
poral Reasoning 
To expand our training set, we use a temporal  
closure component SputLink (Verhagen 2004), 
that takes known temporal relations in a text and  
derives new implied relations from them, in ef-
fect making explicit what was implicit. SputLink 
was inspired by (Setzer and Gaizauskas 2000) 
and is based on Allen?s interval algebra, taking 
into account the limitations on that algebra that 
were pointed out by (Vilain et al 1990). It is ba-
sically a constraint propagation algorithm that 
uses a transitivity table to model the composi-
tional behavior of all pairs of relations in a 
document. SputLink?s transitivity table is repre-
sented by 745 axioms. An example axiom:  
 
If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE 
 
Once the TLINKs in each document in the 
corpus are closed using SputLink, the same vec-
tor generation procedure and feature representa-
tion described in Section 3.1 are used. The effect 
of closing the TLINKs on the corpus has a dra-
matic impact on learning. Table 2, in the 
CLOSED (ME-C) column shows that accura-
cies for this method (called ME-C, for Maximum 
Entropy learning with closure) are now in the 
high 80?s and low 90?s, and still outperform the 
closed majority class (shown in parentheses).  
What is the reason for the improvement?5 One 
reason is the dramatic increase in the amount of 
training data. The more connected the initial un-
                                                 
5Interestingly, performance does not improve for SIMUL-
TANEOUS.  The reason for this might be due to the rela-
tively modest increase in SIMULTANEOUS relations from 
applying closure (roughly factor of 2). 
closed graph for a document is in TLINKs, the 
greater the impact in terms of closure. When the 
OTC is closed, the number of TLINKs goes up 
by more than 11 times, from 6147 Event-Event 
and 4593 Event-Time TLINKs to 91,157 Event-
Event and 29,963 Event-Time TLINKs. The 
number of BEFORE links goes up from 3170 
(51.6%) Event-Event and 1229 Event-Time 
TLINKs (26.75%) to 68585 (75.2%) Event-
Event and 18665 (62.3%) Event-Time TLINKs, 
making BEFORE the majority class in the closed 
data for both Event-Event and Event-Time 
TLINKs. There are only an average of 0.84 
TLINKs per event before closure, but after clo-
sure it shoots up to 9.49 TLINKs per event. 
(Note that as a result, the majority class percent-
ages for the closed data have changed from the 
unclosed data.) 
Being able to bootstrap more training data is 
of course very useful. However, we need to dig 
deeper to investigate how the increase in data 
affected the machine learning. The improvement 
provided by temporal closure can be explained 
by three factors:  (1) closure effectively creates a 
new classification problem with many more in-
stances, providing more data to train on; (2) the 
class distribution is further skewed which results 
in a higher majority class baseline (3) closure 
produces additional data in such a way as to in-
crease the frequencies and statistical power of 
existing features in the unclosed data, as opposed 
to adding new features.  For example, with un-
closed data, given A BEFORE B and B BE-
FORE C, closure generates A BEFORE C which 
provides more significance for the features re-
lated to A and C appearing as first and second 
arguments, respectively, in a BEFORE relation.  
In order to help determine the effects of the 
above factors, we carried out two experiments in 
which we sampled 6145 vectors from the closed 
756
data ? i.e. approximately the number of Event-
Event vectors in the unclosed data.  This effec-
tively removed the contribution of factor (1) 
above. The first experiment (Closed Class Dis-
tribution) simply sampled 6145 instances uni-
formly from the closed instances, while the sec-
ond experiment (Unclosed Class Distribution) 
sampled instances according to the same distri-
bution as the unclosed data. Table 3 shows these 
results.  The greater class distribution skew in the 
closed data clearly contributes to improved accu-
racy. However, when using the same class distri-
bution as the unclosed data (removing factor (2) 
from above), the accuracy, 76%, is higher than 
using the full unclosed data.  This indicates that 
closure does indeed help according to factor (3). 
4 Comparison against Baselines 
4.1 Hand-Coded Rules 
Humans have strong intuitions about rules for 
temporal ordering, as we indicated in discussing 
sentences (1) to (3). Such intuitions led to the 
development of pattern matching rules incorpo-
rated in a TLINK tagger called GTag. GTag 
takes a document with TimeML tags, along with 
syntactic information from part-of-speech tag-
ging and chunking from Carafe, and then uses 
187 syntactic and lexical rules to infer and label 
TLINKs between tagged events and other tagged 
events or times. The tagger takes pairs of 
TLINKable items (event and/or time) and 
searches for the single most-confident rule to 
apply to it, if any, to produce a labeled TLINK 
between those items. Each (if-then) rule has a 
left-hand side which consists of a conjunction of 
tests based on TimeML-related feature combina-
tions (TimeML features along with part-of-
speech and chunk-related features), and a right-
hand side which is an assignment to one of the 
TimeML TLINK classes.  
The rule patterns are grouped into several dif-
ferent classes: (i) the event is anchored with or 
without a signal to a time expression within the 
same clause, e.g., (3c), (ii) the event is anchored 
without a signal to the document date (as is often 
the case for reporting verbs in news), (iii) an 
event is linked to another event in the same sen-
tence, e.g., (3c), and (iv) the event in a main 
clause of one sentence is anchored with a signal 
or tense/aspect cue to an event in the main clause 
of the previous sentence, e.g., (1-2), (3a-b). 
The performance of this baseline is shown in 
Table 4 (line GTag). The top most accurate rule 
(87% accuracy) was GTag Rule 6.6, which links 
a past-tense event verb joined by a conjunction to 
another past-tense event verb as being BEFORE 
the latter (e.g., they traveled and slept the 
night ..): 
 
If sameSentence=YES && 
 sentenceType=ANY && 
 conjBetweenEvents=YES && 
 arg1.class=EVENT && 
 arg2.class=EVENT && 
 arg1.tense=PAST && 
 arg2.tense=PAST && 
 arg1.aspect=NONE && 
 arg2.aspect=NONE && 
 arg1.pos=VB && 
 arg2.pos=VB && 
 arg1.firstVbEvent=ANY && 
 arg2.firstVbEvent=ANY  
then infer relation=BEFORE 
 
The vast majority of the intuition-bred rules 
have very low accuracy compared to ME-C, with 
intuitions failing for various feature combina-
tions and relations (for relations, for example, 
GTag lacks rules for IBEFORE, STARTS, and 
ENDS). The bottom-line here is that even when 
heuristic preferences are intuited, those prefer-
ences need to be guided by empirical data, 
whereas hand-coded rules are relatively ignorant 
of the distributions that are found in data. 
4.2 Adding Google-Induced Lexical Rules 
One might argue that the above baseline is too 
weak, since it doesn?t allow for a rich set of lexi-
cal relations. For example, pushing can result in 
falling, killing always results in death, and so 
forth. These kinds of defeasible rules have been 
investigated in the semantics literature, including 
the work of Lascarides and Asher cited in Sec-
tion 1.  
However, rather than hand-creating lexical 
rules and running into the same limitations as 
with GTag?s rules, we used an empirically-
derived resource called VerbOcean (Chklovski 
and Pantel 2004), available at 
http://semantics.isi.edu/ocean. This resource con-
sists of lexical relations mined from Google 
searches. The mining uses a set of lexical and 
syntactic patterns to test for pairs of verb 
strongly associated on the Web in an asymmetric 
?happens-before? relation. For example, the sys-
tem discovers that marriage happens-before di-
vorce, and that tie happens-before untie.  
We automatically extracted all the ?happens-
before? relations from the VerbOcean resource at 
the above web site, and then automatically con-
verted those relations to GTag format, producing 
4,199 rules. Here is one such converted rule: 
757
 
If arg1.class=EVENT && 
   arg2.class=EVENT && 
   arg1.word=learn && 
   arg2.word=forget && 
then infer relation=BEFORE 
 
Adding these lexical rules to GTag (with mor-
phological normalization being added for rule 
matching on word features) amounts to a consid-
erable augmentation of the rule-set, by a factor of 
22. GTag with this augmented rule-set might be 
a useful baseline to consider, since one would 
expect the gigantic size of the Google ?corpus? to 
yield fairly robust, broad-coverage rules.  
What if both a core GTag rule and a VerbO-
cean-derived rule could both apply? We assume 
the one with the higher confidence is chosen. 
However, we don?t have enough data to reliably 
estimate rule confidences for the original GTag 
rules; so, for the purposes of VerbOcean rule 
integration, we assigned either the original Ver-
bOcean rules as having greater confidence than 
the original GTag rules in case of a conflict (i.e., 
a preference for the more specific rule), or vice-
versa.  
 The results are shown in Table 4 (lines 
GTag+VerbOcean). The combined rule set, un-
der both voting schemes, had no statistically sig-
nificant difference in accuracy from the original 
GTag rule set. So, ME-C beat this baseline as 
well.  
The reason VerbOcean didn?t help is again 
one of data sparseness, due to most verbs occur-
ring rarely in the OTC. There were only 19 occa-
sions when a happens-before pair from VerbO-
cean correctly matched a human BEFORE 
TLINK, of which 6 involved the same rule being 
right twice (including learn happens-before for-
get, a rule which students are especially familiar 
with!), with the rest being right just once. There 
were only 5 occasions when a VerbOcean rule 
incorrectly matched a human BEFORE TLINK, 
involving just three rules. 
 
 
 Closed Class Distribution UnClosed Class Distribution 
Relation Prec Rec F Accuracy Prec Rec F Accuracy 
IBEFORE 100.0 100.0 100.0 83.33 58.82 68.96 
BEGINS 0 0 0 72.72 50.0 59.25 
ENDS 66.66 57.14 61.53 62.50 50.0 55.55 
SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34 
INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53 
BEFORE 90.68 92.60 91.63 
87.20  
(72.03) 
84.09 84.61 84.35 
76.0 
(40.95)  
Table 3. Machine Learning from subsamples of the closed data 
 
Accuracy Baseline 
Event-Event Event-Time 
GTag 63.43 72.46 
GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02 
GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37 
GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59) 
Table 4. Accuracy of ?Intuition? Derived Baselines 
 
4.3 Learning from Hand-Coded Rules 
Baseline 
The previous baseline was a hybrid confi-
dence-based combination of corpus-induced 
lexical relations with hand-created rules for tem-
poral ordering. One could consider another obvi-
ous hybrid, namely learning from annotations 
created by GTag-annotated corpora. Since the 
intuitive baseline fares badly, this may not be 
that attractive. However, the dramatic impact of 
closure could help offset the limited coverage 
provided by human intuitions.   
Table 4 (line GTag+closure+ME-C) shows the 
results of closing the TLINKs produced by 
GTag?s annotation and then training ME from 
the resulting data. The results here are evaluated 
against a held-out test set. We can see that even 
after closure, the baseline of learning from un-
closed human annotations is much poorer than 
ME-C, and is in fact substantially worse than the  
majority class on event ordering.  
This means that for preprocessing new data 
sets to produce noisily annotated data for this 
classification task, it is far better to use machine-
learning from closed human annotations rather 
758
than machine-learning from closed annotations 
produced by an intuitive baseline. 
5 Related Work 
Our approach of classifying pairs independ-
ently during learning does not take into account 
dependencies between pairs.  For example, a 
classifier may label <X, Y> as BEFORE. Given 
the pair <X, Z>,  such a classifier has no idea if 
<Y, Z> has been classified as BEFORE, in 
which case, through closure, <X, Z> should be 
classified as BEFORE. This can result in the 
classifier producing an inconsistently annotated 
text. The machine learning approach of (Cohen 
et al 1999) addresses this, but their approach is 
limited to total orderings involving BEFORE, 
whereas TLINKs introduce partial orderings in-
volving BEFORE and five other relations. Future 
research will investigate methods for tighter in-
tegration of temporal reasoning and statistical 
classification. 
The only closely comparable machine-
learning approach to the problem of TLINK ex-
traction was that of (Boguraev and Ando 2005), 
who trained a classifier on Timebank 1.1 for 
event anchoring for events and times within the 
same sentence, obtaining an F-measure (for tasks 
A and B together) of 53.1. Other work in ma-
chine-learning and hand-coded approaches, 
while interesting, is harder to compare in terms 
of accuracy since they do not use common task 
definitions, annotation standards, and evaluation 
measures. (Li et al 2004) obtained 78-88% accu-
racy on ordering within-sentence temporal rela-
tions in Chinese texts. (Mani et al 2003) ob-
tained 80.2 F-measure training a decision tree on 
2069 clauses in anchoring events to reference 
times that were inferred for each clause. (Ber-
glund et al 2006) use a document-level evalua-
tion approach pioneered by (Setzer and Gai-
zauskas 2000), which uses a distinct evaluation 
metric. Finally, (Lapata and Lascarides 2004) use 
found data to successfully learn which (possibly 
ambiguous) temporal markers connect a main 
and subordinate clause, without inferring under-
lying temporal relations. 
In terms of hand-coded approaches, (Mani and 
Wilson 2000) used a baseline method of blindly 
propagating TempEx time values to events based 
on proximity, obtaining 59.4% on a small sample 
of 8,505 words of text. (Filatova and Hovy 2001) 
obtained 82% accuracy on ?timestamping? 
clauses for a single type of event/topic on a data 
set of 172 clauses. (Schilder and Habel 2001) 
report 84% accuracy inferring temporal relations 
in German data, and (Li et al 2001) report 93% 
accuracy on extracting temporal relations in Chi-
nese. Because these accuracies are on different 
data sets and metrics, they cannot be compared 
directly with our methods. 
Recently, researchers have developed other 
tools for automatically tagging aspects of Ti-
meML, including EVENT (Sauri et al 2005) at 
0.80 F-measure and TIMEX36 tags at 0.82-0.85 
F-measure. In addition, the TERN competition 
(tern.mitre.org) has shown very high (close to .95  
F-measures) for TIMEX2 tagging, which is fairly 
similar to TIMEX3. These results suggest the 
time is ripe for exploiting ?imperfect? features in 
our machine learning approach. 
6 Conclusion 
Our research has uncovered one new finding: 
semantic reasoning (in this case, logical axioms 
for temporal closure), can be extremely valuable 
in addressing data sparseness. Without it, per-
formance on this task of learning temporal rela-
tions is poor; with it, it is excellent. We showed 
that temporal reasoning can be used as an over-
sampling method to dramatically expand the 
amount of training data for TLINK labeling, re-
sulting in labeling predictive accuracy as high as 
93% using an off-the-shelf Maximum Entropy 
classifier. Future research will investigate this 
effect further, as well as examine factors that 
enhance or mitigate this effect in different cor-
pora. 
The paper showed that ME-C performed sig-
nificantly better than a series of increasingly so-
phisticated baselines involving expansion of 
rules derived from human intuitions. Our results 
in these comparisons confirm the lessons learned 
from the corpus-based revolution, namely that 
rules based on intuition alone are prone to in-
completeness and are hard to tune without access 
to the distributions found in empirical data.  
Clearly, lexical rules have a role to play in se-
mantic and pragmatic reasoning from language, 
as in the discussion of example (2) in Section 1. 
Such rules, when mined by robust, large corpus-
based methods, as in the Google-derived VerbO-
cean, are clearly relevant, but too specific to ap-
ply more than a few times in the OTC corpus.  
It may be possible to acquire confidence 
weights for at least some of the intuitive rules in 
GTag from Google searches, so that we have a 
                                                 
6http://complingone.georgetown.edu/~linguist/GU_TIME_
DOWNLOAD.HTML 
759
level field for integrating confidence weights 
from the fairly general GTag rules and the fairly 
specific VerbOcean-like lexical rules. Further, 
the GTag and VerbOcean rules could be incorpo-
rated as features for machine learning, along with 
features from automatic preprocessing.  
We have taken pains to use freely download-
able resources like Carafe, VerbOcean, and 
WEKA to help others easily replicate and 
quickly ramp up a system. To further facilitate 
further research, our tools as well as labeled vec-
tors (unclosed as well as closed) are available for 
others to experiment with. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
Anders Berglund, Richard Johansson and Pierre 
Nugues. 2006. A Machine Learning Approach to 
Extract Temporal Information from Texts in Swed-
ish and Generate Animated 3D Scenes.  Proceed-
ings of EACL-2006. 
Branimir Boguraev and Rie Kubota Ando. 2005. Ti-
meML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270, 1999. 
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
760
  	

Temporal Discourse Models for Narrative Structure 
Inderjeet MANI 
Department of Linguistics 
Georgetown University 
ICC 452 
Washington, DC 20057 
im5@georgetown.edu 
James PUSTEJOVSKY 
Department of Computer Science 
Brandeis University 
Volen 258 
Waltham, Massachusetts 02254 
jamesp@cs.brandeis.edu 
 
Abstract 
Getting a machine to understand human 
narratives has been a classic challenge for 
NLP and AI. This paper proposes a new 
representation for the temporal structure of 
narratives. The representation is parsimonious, 
using temporal relations as surrogates for 
discourse relations. The narrative models, 
called Temporal Discourse Models, are tree-
structured, where nodes include abstract 
events interpreted as pairs of time points and 
where the dominance relation is expressed by 
temporal inclusion. Annotation examples and 
challenges are discussed, along with a report 
on progress to date in creating annotated 
corpora. 
1 Introduction 
Getting a machine to understand human narratives 
has been a classic challenge for NLP and AI. 
Central to all narratives is the notion of time and 
the unfolding of events. When we understand a 
story, in addition to understanding other aspects 
such as plot, characters, goals, etc., we are able to 
understand the order of happening of events. A 
given text may have multiple stories; when we 
understand such a text, we are able to tease apart 
these distinct stories. Thus, understanding the story 
from a text involves building a global model of the 
sequences of events in the text, as well as the 
structure of nested stories. We refer to such models 
as Temporal Discourse Models (TDMs).  
 Currently, while we have informal 
descriptions of the structure of narratives, e.g., 
(Bell 1999), we lack a precise understanding of 
this aspect of discourse. What sorts of structural 
configurations are observed? What formal 
characteristics do they have? For syntactic 
processing of natural languages, we have, 
arguably, answers to similar questions. However, 
for discourse, we have hardly begun to ask the 
questions. 
 One of the problems here is that most of 
the information about narrative structure is implicit  
in the text. Thus, while linguistic information in 
the form of tense, aspect, temporal adverbials and 
discourse markers is often present, people use 
commonsense knowledge to fill in information. 
Consider a simple discourse: Yesterday Holly was 
running a marathon when she twisted her ankle. 
David had pushed her. Here, aspectual information 
indicates that the twisting occurred during the 
running, while tense suggests that the pushing 
occurs before the twisting. Commonsense 
knowledge also suggests that the pushing caused 
the twisting.  
 We can see that even for interpreting such 
relatively simple discourses, a system might 
require a variety of sources of linguistic 
knowledge, including knowledge of tense, aspect, 
temporal adverbials, discourse relations, as well as 
background knowledge. Of course, other 
inferences are clearly possible, e.g., that the 
running stopped after the twisting, but when 
viewed as defaults, these latter inferences seem to 
be more easily violated. The need for 
commonsense inferences has motivated 
computational approaches that are domain-
specific, using hand-coded knowledge (e.g., Asher 
and Lascarides 2003, Hitzeman et al 1995).  
 A number of theories have postulated the 
existence of various discourse relations that relate 
elements in the text to produce a global model of 
discourse, e.g., (Mann and Thompson 1988), 
(Hobbs 1985), (Hovy 1990) and others. In RST 
(Mann and Thompson 1988), (Marcu 2000), these 
relations are ultimately between semantic elements 
corresponding to discourse units that can be simple 
sentences or clauses as well as entire discourses. In 
SDRT (Asher and Lascarides 2003), these relations 
are between representations of propositional 
content, called Discourse Representation 
Structures (Kamp and Reyle, 1993).  
 Despite a considerable amount of very 
productive research, annotating such discourse 
relations has proved problematic. This is due to the 
fact that discourse markers may be absent (i.e., 
implicit) or ambiguous; but more importantly, 
because in many cases the precise nature of these 
discourse relations is unclear. Although (Marcu et 
 In addition to T1, we also have the 
temporal ordering constraints C1:  {Eb < Ec, Ec < 
Ea, Ea < Ed}. These are represented separately 
from the tree. A TDM is thus a pairing of tree 
structures and temporal constraints. More 
precisely, a Temporal Discourse Model for a text is 
a pair <T, C>, where T is a rooted, unordered, 
directed tree with nodes N = ?E ? A?, where E is 
the set of events mentioned in the text and A is a 
set of abstract events, and a parent-child ordering 
relation, ? (temporal inclusion). A non-leaf node 
can be textually mentioned or abstract. Nodes also 
have a set of atomic-valued features. Note that the 
tree is temporally unordered left to right. C is a set 
of temporal ordering constraints using the ordering 
relation, < (temporal precedence) as well as (for 
states, clarified below) ?minimal restrictions? on 
the above temporal inclusion relation (expressed as 
a ?min).  
al. 1999) (Carlson et al 2001) reported relatively 
high levels of inter-annotator agreement, this was 
based on an annotation procedure where the 
annotators were allowed to iteratively revise the 
instructions based on joint discussion.  
 While we appreciate the importance of 
representing rhetorical relations in order to carry 
out temporal inferences about event ordering, we 
believe that there are substantial advantages in 
isolating the temporal aspects and modeling them 
separately as TDMs. This greatly simplifies the 
representation, which we discuss next.   
2 Temporal Discourse Models 
A TDM is a tree-structured syntactic model of 
global discourse structure, where temporal 
relations are used as surrogates for discourse 
relations, and where abstract events corresponding 
to entire discourses are introduced as nodes in the 
tree.   In (1) the embedding nodes E0 and E1 were abstract, but textually mentioned events can 
also create embeddings, as in (2) (example from 
(Spejewski 1988)): 
 We begin by illustrating the basic 
intuition. Consider discourse (1), from (Webber 
1988): (2) a. Edmond made his own Christmas 
presents this year. b. First he dried a bunch 
of tomatoes in his oven. c. Then he made a 
booklet of recipes that use dried tomatoes. d. 
He scanned in the recipes from his gourmet 
magazines. e. He gave these gifts to his 
family. 
(1) a. John went into the florist shop.  
b. He had promised Mary some flowers.  
c. She said she wouldn?t forgive him if he 
forgot. d. So he picked out three red roses. 
 The discourse structure of (1) can be 
represented by the tree, T1, shown below.  
       T2 =                 E0               E0                    Ea                Ee               Ea         E1                      Ed                  Eb    Ec                                          Ed                        Eb       Ec               Here E0 has children Ea, E1, and Ed, and 
E1 has children Eb and Ec. The nodes with 
alphabetic subscripts are events mentioned in the 
text, whereas nodes with numeric subscripts are 
abstract events, i.e., events that represent abstract 
discourse objects. A node X is a child of node Y iff 
X is temporally included in Y. In our scheme, 
events are represented as pairs of time points. So, 
E0 is an abstract node representing a top-level 
story, and E1 is an abstract node representing an 
embedded story. Note that the mentioned events 
are ordered left to right in text order for notational 
convenience, but no temporal ordering is directly 
represented in the tree. Since the nodes in this 
representation are at a semantic level, the tree 
structure is not necessarily isomorphic to a 
representation at the text level, although T1 
happens to be isomorphic.  
      C2 = {Ea < Ee, Eb < Ec} 
 Note that the partial ordering C can be 
extended using T and temporal closure axioms 
(Setzer and Gaizauskas 2001), (Verhagen 2004), so 
that in the case of <T2, C2>, we can infer, for 
example, that Eb < Ed, Ed < Ee, and so forth.  
 In representing states, we take a 
conservative approach to the problems of 
ramification and change (McCarthy and Hayes 
1969). This is the classic problem of recognizing 
when states (the effects of actions) change as a 
result of actions. Any tensed stative predicate will 
be represented as a node in the tree (progressives 
are here treated as stative). Consider an example 
like John walked home. He was feeling great. 
Here we represent the state of feeling great as 
being minimally a part of the event of walking, 
without committing to whether it extends before or 
after the event. While this is interpreted as an 
overloaded temporal inclusion in the TDM tree, a 
constraint is added to C indicating that this 
inclusion is minimal.  
 This conservative approach results in 
logical incompleteness, however. For example, 
given the discourse Max entered the room. He was 
wearing a black shirt, the system will not know 
whether the shirt was worn after he entered the 
room. States are represented as bounded intervals, 
and participate in ordering relations with events in 
the tree. It is clear that in many cases, a state 
should persist throughout the interval spanning 
subsequent events. This is not captured by the 
current tree representation. Opposition structures 
of predicates and gating operations over properties 
can be expressed as constraints introduced by 
events, however, but at this stage of development, 
we have been interested in capturing a coarser 
temporal ordering representation, very robustly. 
We believe, however, that annotation using the 
minimal inclusion relation will allow us to reason 
about persistence heuristically in the future.  
3 Prerequisites 
Prior work on temporal information extraction has 
been fairly extensive and is covered in (Mani et al 
2004). Recent research has developed the TimeML 
annotation scheme (Pustejovsky et al 2002) 
(Pustejovsky et al 2004), as well as a corpus of 
TimeML-annotated news stories (TimeBank 2004) 
and annotation tools that go along with it, such as 
the TANGO tool (Pustejovsky et al 2003). 
TimeML flags tensed verbs, adjectives, and 
nominals that correspond to events and states, 
tagging instances of them with standard TimeML 
attributes, including the class of event (perception, 
reporting, aspectual, state, etc.), tense (past, 
present, future), grammatical aspect (perfective, 
progressive, or both), whether it is negated, any 
modal operators which govern it, and its 
cardinality if the event occurs more than once. 
Likewise, time expressions are flagged, and their 
values normalized, so that Thursday in He left on 
Thursday would get a resolved ISO time value 
depending on context  (TIMEX2 2004). Finally, 
temporal relations between events and time 
expressions (e.g., that the leaving occurs during 
Thursday) are recorded by means of temporal links 
(TLINKs) that express Allen-style interval 
relations (Allen 1984).  
 Several automatic tools have been 
developed in conjunction with TimeML, including 
event taggers (Pustejovsky et al 2003), time 
expression taggers (Mani and Wilson 2000), and 
an exploratory link extractor (Mani et al 2003). 
Temporal reasoning algorithms have also been 
developed, that apply transitivity axioms to expand 
the links using temporal closure algorithms (Setzer 
and Gaizauskas 2001), (Pustejovsky et al 2003).  
 However, TimeML is inadequate as a 
temporal model of discourse: it constructs no 
global representation of the narrative structure, 
instead annotating a complex graph that links 
primitive events and times. 
4 Related Frameworks 
Since the relations in TDMs involve temporal 
inclusion and temporal ordering, the mentioned 
events can naturally be mapped to other discourse 
representations used in computational linguistics. 
A TDM tree can be converted to a first-order 
temporal logic representation (where temporal 
ordering and inclusion operators are added) by 
expanding the properties of the nodes. These 
properties include any additional predications 
made explicitly about the event, e.g., information 
from thematic arguments and adjuncts. In other 
words, a full predicate argument representation, 
e.g., as might be found in the PropBank 
(Kingsbury and Palmer 2002), can be associated 
with each node.  
 TDMs can also be mapped to Discourse 
Representation Structures (DRS) (which in turn 
can be mapped to a logical form). Since TDMs 
represent events as pairs of time points (which can 
be viewed as intervals), and DRT represents events 
as primitives, we can reintroduce time intervals 
based on the standard DRT approach (e ? t for 
events,  e O t for states, except for present tense 
states, where t ? e).  
 Consider an example from the Discourse 
Representation Theory (DRT) literature (from 
Kamp and Reyle 1993): 
(3) a. A man entered the White Hart. b. He 
was wearing a black jacket. c. Bill served 
him a beer.  
   The TDM is <T3, C3> below, with 
internal properties of the nodes as shown: 
 
T3 =      E0 
 
             Ea          Ec 
                
     
         Eb 
 
C3 = {Ea < Ec} 
node.properties(Ea): enter(Ea, x, y), 
man(x), y= theWhiteHart, Ea < n 
node.properties(Eb): PROG(wear(Eb, x1, 
y1)), black-jacket(y1), x1=x, Eb < n,  
node.properties(Ec): serve(Ec, x2, y2, z), 
beer(z), x2=Bill, y2=x, Ec < n 
From T3: Eb ? Ea 
From C3: Ea < Ec 
 The DRT representation is shown below 
 (here we have created variables for the 
 reference times): 
 
  
 
 
 
 
 
  
 
 
  
 
  
 Note that we are by no means claiming 
that DRSs and TDMs are equivalent. TDMs are 
tree-structured and DRSs are not, and the inclusion 
relations involving our abstract events, i.e., Ea ? 
E0 and Ec ? E0, are not usually represented in 
DRT. Nevertheless, there are many similarities 
between TDMs and DRT which are worth 
examining for semantic and computational 
properties. Furthermore, SDRT (Asher and 
Lascarides 2003) extends DRT to include 
discourse relations. SDRT and RST both differ 
fundamentally from TDMs, since we dispense with 
rhetorical relations.  
 It should be pointed out, nevertheless, that 
TDMs, as modeled so far, do not represent 
modality and intensional contexts in the tree 
structure. (However, information about modality 
and negation is stored in the nodes based on 
TimeML preprocessing).  One way of addressing 
this issue is to handle lexically derived modal 
subordination (such as believe and want) by 
introducing embedded events, linked to the modal 
predicate by subordinating relations. For example, 
in the sentence John believed that Mary graduated 
from Harvard, the complement event is 
represented as a subtree linked by a lexical 
relation. 
 DLTAG (Webber et al 2004) is a model 
of discourse structure where explicit or implicit 
discourse markers relating only primitive discourse 
units. Unlike TDMs, where the nodes in the tree 
can contain embedded structures, DLTAG is a 
local model of discourse structure; it thus provides 
a set of binary relations, rather than a tree Like 
TDMs, however, DLTAG models discourse 
structure without postulating the existence of 
rhetorical relations in the discourse tree. Instead, 
the rhetorical relations appear as predicates in the 
semantic forms for discourse markers. In this 
respect, they differ from TDMs, which do not 
commit to specific rhetorical relations.   
 Spejewski (1994) developed a tree-based 
model of the temporal structure of a sequence of 
sentences. Her approach is based on relations of 
temporal coordination and subordination, and is 
thus a major motivation for our own approach. 
However, her approach mixes both reference times 
and events in the same representation, so that the 
parent-child relation sometimes represents 
temporal anchoring, and at other times 
coordination. In the above example of John walked 
home. He was feeling great, her approach would 
represent the ?reference time? of the state (of 
feeling great) as being part of the event of walking 
as well as part of the state, resulting in a graph 
rather than a strict tree. Note that our approach 
uses minimality. 
Ea, x, y , Eb, x1, y1, Ec, x2, y2, z, 
t1, t2, t3 
enter(Ea, x, y), man(x), y= 
theWhiteHart 
PROG(wear(Eb, x1, y1)), black-
jacket(y1), x1=x 
serve(Ec, x2, y2, z), beer(z), 
x2=Bill, y2=x 
t1 < n, Ea ? t1, t2 < n, Eb ? t2, Eb 
? Ea, t3 < n, Ec ? t3, Ea < Ec 
 (Hitzeman et al 1995) developed a 
computational approach to distinguish various 
temporal threads in discourse. The idea here, based 
on the notion of temporal centering, is that there is 
one ?thread? that the discourse is currently 
following. Thus, in (1) above, each utterance is 
associated with exactly one of two threads: (i) 
going into the florist?s shop and (ii) interacting 
with Mary. Hitzeman et al prefer an utterance to 
continue a current thread which has the same tense 
or is semantically related to it, so that in (1) above, 
utterance d would continue the thread (i) above 
based on tense. In place of world knowledge, 
however, semantic distance between utterances is 
used, presumably based on lexical relationships. 
Whether such semantic similarity is effective is a 
matter for evaluation, which is not discussed in 
their paper. For example, it isn?t clear what would 
rule out (1c) as continuing thread (i). 
 While TDMs do not commit to rhetorical 
relations, our expectation is that they can be used 
as an intermediate representation for rhetorical 
parsing. Thus, when event A in a TDM temporally 
precedes its right sibling B, the rhetorical relation 
of Narration will typically be inferred. When B 
precedes is left sibling A, then Explanation will 
typically be inferred. When A temporally includes 
a child node B, then Elaboration is typically 
inferred, etc. TDMs are thus a useful shallow 
representation that can be a useful first step in 
deriving rhetorical relations; indeed, rhetorical 
relations may be implicit in the human annotation 
of such relations, e.g., when explicit discourse 
markers like ?because? indicate a particular 
temporal order. 
5 Annotation Scheme  
The annotation scheme involves taking each 
document that has been preprocessed with time 
expressions and event tags (complying with 
TimeML) and then representing TDM parse trees 
and temporal ordering constraints (the latter also 
compliant with TimeML TLINKS). 
 Each discourse begins with a root abstract 
node. As an annotation convention, (A1) in the 
absence of any overt or covert discourse markers 
or temporal adverbials, a tense shift will license the 
creation of an abstract node, with the event with 
the shifted tense being the leftmost daughter of the 
abstract node. The abstract node will then be 
inserted as the child of the immediately preceding 
text node. In addition, convention (A2) states that 
in the absence of temporal adverbials and overt or 
covert discourse markers, a stative event will 
always be placed as a child of the immediately 
preceding text event when the latter is non-stative. 
Further, convention (A3) states that when the 
previous event is stative, in the absence of 
temporal adverbials and explicit or implicit 
discourse markers, the stative event is a sibling of 
the previous stative (as in a scene-setting fragment 
of discourse). 
 We expect that inter-annotator reliability 
on TDM trees will be quite high, given the 
transparent nature of the tree structure along with 
clear annotation conventions. The Appendices 
provide examples of annotation, to illustrate the 
simplicity of the scheme as well as potential 
problems. 
6  Corpora  
We have begun annotating three corpora with 
Temporal Discourse Model information. The first 
is the Remedia corpus (remedia.com). There are 
115 documents in total, grouped into four reading 
levels, all of which have been tagged by a human 
for time expressions in a separate project by Lisa 
Ferro at MITRE. Each document is short, about 
237 words on average, and has a small number of 
questions after it for reading comprehension.   
 The Brandeis Reading Corpus is a 
collection of 100 K-8 Reading Comprehension 
articles, mined from the web and categorized by 
level of comprehension difficulty. Articles range 
from 50-350 words in length. Complexity of the 
reading task is defined in terms of five basic 
classes of reading difficulty. 
 The last is the Canadian Broadcasting 
Corporation (cbc4kids.ca). The materials are 
current-event stories aimed at an audience of 8-
year-old to 13-year-old students. The stories are 
short (average length around 450 words). More 
than a thousand articles are available. The 
CBC4Kids corpus is already annotated with POS 
and parse tree markup.  
7 Conclusion  
Our assumption so far has been that the temporal 
structure of narratives is tree-structured and 
context-free. Whether the context-free property is 
violated or not remains to be seen.  
 Once the annotation effort is completed, 
we plan to use the annotated corpora in statistical 
parsing algorithms to construct TDMs. This should 
allow features from the corpus to be leveraged 
together to make inferences about narrative 
structure. While such knowledge source 
combination is not by any means guaranteed to 
substitute for commonsense knowledge, it at least 
allows for the introduction of generic, machine 
learning methods for extracting narrative structure 
from stories in any domain. Earlier work in a non-
corpus based (Hitzeman et al 1995) as well as 
corpus-based setting (Mani et al 2003) attests to 
the usefulness of combining knowledge sources for 
inferring temporal relations. We expect to leverage 
similar methods in TDM parsing. 
 We believe that the temporal aspect of 
discourse provides a handle for investigating 
discourse structure, thereby simplifying the 
problem of discourse structure annotation. It is 
therefore of considerable theoretical interest. 
Further, being able to understand the structure of 
narratives will in turn allow us to summarize them 
and answer temporal questions about narrative 
structure. 
References 
J. F. Allen. 1984. Towards a General Theory of 
Action and Time.  Artificial Intelligence 23: 
123-154. 
N. Asher and A. Lascarides. 2003. Logics of 
Conversation. Cambridge University Press. 
A. Bell. 1999. News Stories as Narratives. In A. 
Jaworski and N. Coupland, The Discourse 
Reader, Routledge, London and New York, 236-
251. 
L. Carlson, D. Marcu and M. E. Okurowski. 2001. 
Building a discourse-tagged corpus in the 
framework of rhetorical structure theory. In 
Proceedings of the 2nd SIGDIAL Workshop on 
Discourse and Dialogue, Eurospeech 2001, 
Aalborg, Denmark. 
B. Grosz, A. Joshi and S. Weinstein. 1995. 
Centering: A Framework for Modeling the 
Local Coherence of Discourse. Computational 
Linguistics 2(21), pp. 203-225  
J. Hitzeman, M. Moens and C. Grover. 1995. 
Algorithms for Analyzing the Temporal 
Structure of Discourse. In Proceedings of the 
Annual Meeting of the European Chapter of the 
Association for Computational Linguistics, 
Utrecht, Netherlands, 1995, 253-260. 
J. Hobbs. 1985. On the Coherence and Structure of 
Discourse. Report No. CSLI-85-37. Stanford, 
California: Center for the Study of Language 
and Information, Stanford University. 
E. Hovy. 1990. Parsimonious and Profligate 
Approaches to the Question of Discourse 
Structure Relations. In Proceedings of the Fifth 
International Workshop on Natural Language 
Generation.  
H. Kamp and U. Reyle. 1993. Tense and Aspect. 
Part 2, Chapter 5 of From Discourse to Logic, 
483-546. 
P. Kingsbury and M. Palmer. 2002. From 
Treebank to PropBank. In Proceedings of the 
3rd International Conference on Language 
Resources and Evaluation (LREC-2002), Las 
Palmas, Spain.  
I.. Mani, B. Schiffman and J. Zhang. 2003. 
Inferring Temporal Ordering of Events in News. 
Proceedings of the Human Language 
Technology Conference, HLT?03. 
I. Mani and G. Wilson. 2000.  Processing of News. 
Proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'2000), 69-76.   
I. Mani, J. Pustejovsky and R. Gaizauskas. 2004. 
The Language of Time: A Reader. Oxford 
University Press, to appear. 
W. Mann and S. Thompson. 1988. Rhetorical 
structure theory: Toward a functional theory of 
text organization. Text, 8(3): 243-281.  
D. Marcu. 2000. The Theory and Practice of 
Discourse Parsing and Summarization. The 
MIT Press. 
D. Marcu, E. Amorrortu and M. Romera. 1999. 
Experiments in constructing a corpus of 
discourse trees. In Proceedings of the ACL 
Workshop on Standards and Tools for Discourse 
Tagging, College Park, MD, 48-57.  
J. McCarthy and P. Hayes. 1969. Some 
philosophical problems from the standpoint of 
artificial intelligence. In B.Meltzer and D. 
Michie, Eds. Machine Intelligence 4.  
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano, J. 
Littman, R. Gaizauskas, A. Setzer, G. Katz and 
I. Mani. 2004. The Specification Language 
TimeML. In I. Mani, J. Pustejovsky and R. 
Gaizauskas. The Language of Time: A Reader. 
Oxford University Press, to appear. 
A. Setzer and R. Gaizauskas.  2001. A Pilot Study 
on Annotating Temporal Relations in Text. ACL 
2001, Workshop on Temporal and Spatial 
Information Processing  
B. Spejewski. 1994. Temporal Subordination in 
Discourse. .Ph.D. Thesis, University of 
Rochester. 
J. Pustejovsky, I. Mani, L. Belanger, B. Boguraev, 
B. Knippen, J. Littman, A. Rumshisky, A. See, 
S. Symonenko, J. Van Guilder, L. Van Guilder, 
M. Verhagen, R. Ingria. 2003. TANGO Final 
Report. timeml.org. 
 J. Pustejovsky, L. Belanger, J. Castano, R. 
Gaizauskas, P. Hanks, R. Ingria, G. Katz, D. 
Radev, A. Rumshisky, A. Sanfilippo, R. Sauri, 
B. Sundheim, M. Verhagen. 2002. TERQAS 
Final Report. timeml.org. 
TIMEBANK. 2004. timeml.org.  
TIMEX2. 2004. timex2.mitre.org. 
B. Webber. 1998. Tense as Discourse Anaphor. 
Computational Linguistics 14(2): 61-73. 
B. Webber, M. Stone, A. Joshi and A. Knott. 2003. 
Computational Linguistics, 29:4, 545-588. 
Appendix A: Examples from (Hitzeman et al 
1995) 
 
1. (a) John entered the room. (b) Mary stood 
up. 
 
Ea is inserted as left daughter of root. Eb is 
attached as sister (an analogue of a Narration 
default rhetorical relation).  
  E0 
 
  
 Ea   Eb 
 C: Ea<Eb 
 
2. (a) John entered the room. (b) Mary was 
seated behind the desk. 
 
Ea is anchored as left daughter of root. Eb is a 
tensed stative, and is embedded below Ea.  
 
  E0 
 
  Ea 
 
 Ea E1 E2   Eb 
 C: Eb ?min Ea 
  
                      Eb         Ec 3. (a) John fell. (b) Mary pushed him. 
 C: Eb<Ea, Ec<Eb  
  
7. (a) John got to work late. (b) He had left 
the house at 8. (c) He had eaten a big 
breakfast. 
 
 
  E0 
  
   
 Ea  Eb 
  E0  C: Eb<Ea 
  
 4. (a) John entered the room because (b) Mary 
stood up.  Ea E1 E2 
  
   E0 
                   Eb         Ec  
  
 C: Eb<Ea, Ec<Eb  Ea  Eb 
  C: Eb<Ea 
 This is due to the ?because?-inversion rule.  
 Appendix B: Level 200 Story from the Brandeis 
Reading Corpus 5. (a) Mary was tired. (b) She was exhausted. 
 
a. David wants to buy a Christmas present for a 
very special person, his mother.  
E0 
 
b. David's father gives him $5.00 a week pocket 
money and  
 
  Ea 
c. David puts $2.00 a week into his bank 
account.  
 
 
d. After three months David takes $20.00 out of 
his bank account and  
  Eb 
C: Eb ?min Ea 
e. goes to the shopping mall.   
f. He looks and looks for a perfect gift. This case, unlike (2), would be an analogue of an 
Elaboration relation. Here, other knowledge 
sources, such as a centering (Grosz et al 1995) 
could play a role in inferring such a discourse 
relation. 
g. Suddenly he sees a beautiful brooch in the 
shape of his favorite pet.  
h. He says to himself  
i. "Mother loves jewelry, and  
j. the brooch costs only $l7.00."   
k. He buys the brooch and  6. (a) Sam rang the bell. (b) He had lost the key. 
(c) It had fallen through a hole in his pocket.  l. takes it home.  
m. He wraps the present in Christmas paper and   
n. places it under the tree.  Ea is attached as right branching event. Eb is 
attached as sister with precedence constraint 
relative to Ea coming from the past perfect 
marking. Ec is attached as sister with precedence 
constraint relative to Eb coming from past perfect. 
Losing the key is explained by (or elaborated by) 
the description of the key falling through the hole. 
Hence, it should be an embedding relation on this 
reading. Nevertheless, the current parse is arguably 
correct since the falling caused the loss of the key.  
o. He is very excited and  
p. he is looking forward to Christmas morning to 
see the joy on his mother's face. 
q. But when his mother opens the present  
r. she screams with fright because  
s. she sees a spider. 
 
Ea, Eb, and Ec are all statively interpreted due to 
the presence of modification by frequency 
adverbial TIMEX3 expressions (from TimeML), 
giving rise to habitual event interpretations. They 
are embedded inside an abstract E0 node. 
  E0 
 
 
  E3  
   E0 
  
  
 Eq Er Es  
  Ea Eb Ec 
 C: Eq<Er, Es<Er, Eq<Es  C: Ea ?min E0, Eb ?min E0, Ec ?min E0 
 E1 is created with the recognition of the time 
expression ?after three months?. Ed is attached as 
the left daughter node in E1. Ee is attached as 
sister (default Narrative). Similarly for Ef, Eg, and 
Eh.  
The TDM for the  entire article is 
represented below: 
 
 
  E1 
 
 
 
 
 Ed Ee Ef Eg Eh 
 
 C: Ed<Ee, Ee<Ef, Ef<Eg, Eg<Eh 
 
The syntactically embedded sentences in (i) and 
(j) are recognized as states and are embedded 
within Eh.  
  E1 
 
 
 
 
 Ed Ee  Ef  Eg  Eh 
 
 
         Ei                Ej 
C: Ed<Ee, Ee<Ef, Ef<Eg, Eg<Eh, Ei in Eh, Ej in 
Eh 
 
Attachment and narrative order holds for Ek, 
El, Em, and En. The states in Eo and Ep will 
be embedded under En: 
 
 E1 
 
 
 
  Ed  Ee  Ef  Eg  Eh  Ek    El   Em     En 
 
              
 
                        Ei     Ej          Eo      Ep 
 
The presence of ?when? as a TimeML signal 
creates a new abstract event, E3, and the 
subsequent ordering relation E3>E2.  
 
Finally, narration continues under E3 with Eq, 
Er, and Es, as daughters to E3, with the additional 
constraint of ?because-inversion?, Es<Er.  
Automated Induction of Sense in Context
James PUSTEJOVSKY, Patrick HANKS, Anna RUMSHISKY
Brandeis University
Waltham, MA 02454, USA
{jamesp,patrick,arum}@cs.brandeis.edu
1 Introduction
In this work, we introduce a model for sense assign-
ment which relies on assigning senses to the con-
texts within which words appear, rather than to the
words themselves. We argue that word senses as
such are not directly encoded in the lexicon of the
language. Rather, each word is associated with one
or more stereotypical syntagmatic patterns, which
we call selection contexts. Each selection context is
associated with a meaning, which can be expressed
in any of various formal or computational manifesta-
tions. We present a formalism for encoding contexts
that help to determine the semantic contribution of a
word in an utterance. Further, we develop a method-
ology through which such stereotypical contexts for
words and phrases can be identified from very large
corpora, and subsequently structured in a selection
context dictionary, encoding both stereotypical syn-
tactic and semantic information. We present some
preliminary results.
2 CPA Methodology
The Corpus Pattern Analysis (CPA) technique uses
a semi-automatic bootstrapping process to produce
a dictionary of selection contexts for predicates
in a language. Word senses for verbs are distin-
guished through corpus-derived syntagmatic pat-
terns mapped to Generative Lexicon Theory (Puste-
jovsky (1995)) as a linguistic model of interpreta-
tion, which guides and constrains the induction of
senses from word distributional information. Each
pattern is specified in terms of lexical sets for each
argument, shallow semantic typing of these sets, and
other syntagmatically relevant criteria (e.g., adver-
bials of manner, phrasal particles, genitives, nega-
tives).
The procedure consists of three subtasks: (1) the
manual discovery of selection context patterns for
specific verbs; (2) the automatic recognition of in-
stances of the identified patterns; and (3) automatic
acquisition of patterns for unanalyzed cases. Ini-
tially, a number of patterns are manually formulated
by a lexicographer through corpus pattern analysis
of about 500 occurrences of each verb lemma. Next,
for higher frequency verbs, the remaining corpus oc-
currences are scrutinized to see if any low-frequency
patterns have been missed. The patterns are then
translated into a feature matrix used for identifying
the sense of unseen instances for a particular verb.
In the remainder of this section, we describe these
subtasks in more detail. The following sections ex-
plain the current status of the implementation of
these tasks.
2.1 Lexical Discovery
Norms of usage are captured in what we call selec-
tion context patterns. For each lemma, contexts
of usage are sorted into groups, and a stereotypi-
cal CPA pattern that captures the relevant semantic
and syntactic features of the group is recorded. For
example, here is the set of common patterns for the
verb treat.
(1) CPA pattern set for treat:
I. [[Person 1]] treat [[Person 2]] ({at | in} [[Location]])
(for [[Event = Injury | Ailment]]); NO [Adv[Manner]]
II. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
IIIa. [[Person]] treat [[TopType 1]] {{as | like} [[TopType 2]]}
IIIb. [[Person]] treat [[TopType]] {{as if | as though | like}
[CLAUSE]}
IV. [[Person 1]] treat [[Person 2]] {to [[Event]]}
V. [[Person]] treat [[PhysObj | Stuff 1]] (with [[Stuff 2]])
There may be several patterns realizing a single
sense of a verb, as in (IIIa/IIIb) above. Addi-
tionally, many patterns have alternations, recorded
in satellite CPA patterns. Alternations are linked
to the main CPA pattern through the same sense-
modifying mechanisms as those that allow for coer-
cions to be understood. However, alternations are
different realizations of the same norm. For exam-
ple, the following are alternations for treat, pattern
(I):
[[Person 1 <--> Medicament | Med-Procedure | Institution]]
[[Person 2 <--> Injury | Ailment | Bodypart]]
CPA Patterns
A CPA pattern extends the traditional notion of se-
lectional context to include a number of other con-
textual features, such as minor category parsing and
subphrasal cues. Accurate identification of the se-
mantically relevant aspects of a pattern is not an
obvious and straightforward procedure, as has some-
times been assumed in the literature. For example,
the presence or absence of an adverbial of manner in
the third valency slot around a verb can dramatically
alter the verb?s meaning. Simple syntactic encoding
of argument structure, for instance, is insufficient to
discriminate between the two major senses of the
verb treat, as illustrated below.
(3) a. They say their bosses treat them with respect.
b. Such patients are treated with antibiotics.
The ability to recognize the shallow semantic type
of a phrase in the context of a predicate is of course
crucial ?for example, in (3a) recognizing the PP as
(a) an adverbial, and (b) an adverbial of manner,
rather than an instrumental co-agent (as in (3b)),
is crucial for assigning the correct sense to the verb
treat above.
There are four constraint sets that contribute to
the patterns for encoding selection contexts. These
are:
(4) a. Shallow Syntactic Parsing: Phrase-level recogni-
tion of major categories.
b. Shallow Semantic Typing: 50-100 primitive shal-
low types, such as Person, Institution, Event, Abstract, Ar-
tifact, Location, and so forth. These are the top types se-
lected from the Brandeis Shallow Ontology (BSO), and are
similar to entities (and some relations) employed in Named
Entity Recognition tasks, such as TREC and ACE.
c. Minor Syntactic Category Parsing: e.g., loca-
tives, purpose clauses, rationale clauses, temporal adjuncts.
d. Subphrasal Syntactic Cue Recognition: e.g.,
genitives, partitives, bare plural/determiner distinctions,
infinitivals, negatives.
The notion of a selection context pattern, as pro-
duced by a human annotator, is expressed as a BNF
specification in Table 1.1 This specification relies
on word order to specify argument position, and is
easily translated to a template with slots allocated
for each argument. Within this grammar, a seman-
tic roles can be specified for each argument, but this
information currently is not used in the automated
processing.
Brandeis Shallow Ontology
The Brandeis Shallow Ontology (BSO) is a shallow
hierarchy of types selected for their prevalence in
manually identified selection context patterns. At
the time of writing, there are just 65 types, in terms
of which patterns for the first one hundred verbs
have been analyzed. New types are added occasion-
ally, but only when all possibilities of using existing
types prove inadequate. Once the set of manually
extracted patterns is sufficient, the type system will
be re-populated and become pattern-driven.
The BSO type system allows multiple inheri-
tance (e.g. Document v PhysObj and Document v
Information. The types currently comprising the
ontology are listed below. The BSO contains type
assignments for 20,000 noun entries and 10,000 nom-
inal collocation entries.
1Round brackets indicate optional elements of the pattern,
and curly brackets indicate syntactic constituents.
Corpus-driven Type System
The acquisition strategy for selectional preferences
for predicates proceeds as follows:
(5) a. Partition the corpus occurrences of a predicate according
to the selection contexts pattern grammar, distinguished
by the four levels of constraints mentioned in (4). These
are uninterpreted patterns for the predicate.
b. Within a given pattern, promote the statistically
significant literal types from the corpus for each argument
to the predicate. This induces an interpretation of the
pattern, treating the promoted literal type as the specific
binding of a shallow type from step (a) above.
c. Within a given pattern, coerce all lexical heads in the
same shallow type for an argument, into the promoted
literal type, assigned in (b) above. This is a coercion of a
lexical head to the interpretation of the promoted literal
type induced from step (b) above.
In a sense, (5a) can be seen as a broad multi-level
partitioning of the selectional behavior for a pred-
icate according to a richer set of syntactic and se-
mantic discriminants. Step (5b) can be seen as cap-
turing the norms of usage in the corpus, while step
(5c) is a way of modeling the exploitation of these
norms in the language (through coercion, metonymy,
and other generative operations). To illustrate the
way in which CPA discriminates uninterpreted pat-
terns from the corpus, we return to the verb treat as
it is used in the BNC. Two of its major senses, as
listed in (1), emerge as correlated with two distinct
context patterns, using the discriminant constraints
mentioned in (4) above.
CPA-Pattern ? Segment verb-lit Segment | verb-lit Segment | Segment verb-lit | CPA-Pattern ?;? Element
Segment ? Element | Segment Segment | ?? Segment ?? | ?(? Segment ?)? | Segment ?|? Segment
Element ? literal | ?[? Rstr ArgType ?]? | ?[? Rstr literal ?]? | ?[? Rstr ?]? | ?[? NO Cue ?]? | ?[? Cue ?]?
Rstr ? POS | Phrasal | Rstr ?|? Rstr | epsilon
Cue ? POS | Phrasal | AdvCue
AdvCue ? ADV ?[? AdvType ?]?
AdvType ? Manner | Dir | Location
Phrasal ? OBJ | CLAUSE | VP | QUOTE
POS ? ADJ | ADV | DET | POSDET | COREF POSDET | REFL-PRON | NEG |
MASS | PLURAL | V | INF | PREP | V-ING | CARD | QUANT | CONJ
ArgType ? ?[? SType ?]? | ?[? SType ?=? SubtypeSpec ?]? | ArgType ?|? ArgType | ?[? SType ArgIdx ?]? |
?[? SType ArgIdx ?=? SubtypeSpec ?]?
SType ? AdvType | TopType | Entity | Abstract | PhysObj | Institution | Asset | Location | Human | Animate |
Human Group | Substance | Unit of Measurement | Quality | Event | State of Affairs | Process
SubtypeSpec ? SubtypeSpec ?|? SubtypeSpec | SubtypeSpec ?&? SubtypeSpec | Role | Polarity | LSet
Role ? Role | Role ?|? Role | Benficiary | Meronym | Agent | Payer
Polarity ? Negative | Positive
LSet ? Worker | Pilot | Musician | Competitor | Hospital | Injury | Ailment | Medicament | Medical Procedure |
Hour-Measure | Bargain | Clothing | BodyPart | Text | Sewage | Part | Computer | Animal
ArgIdx ? <number> verb-lit ? <verb-word-form>
literal ? word word ? <word>
CARD ? <number> NEG ? not
POSDET ? my | your | ... INF ? to
QUANT ? CARD | a lot | longer | more | many | ...
Table 1: Pattern grammar
(6) a. [[Person 1]] treat [[Person 2]]; NO [Adv[Manner]]
b. [[Person 1]] treat [[Person 2]] [Adv[Manner]]
Given a distinct (contextual) basis on which to an-
alyze the actual statistical distribution of the words
in each argument position, we can promote statisti-
cally relevant and significant literal types for these
positions. For example, for pattern (a) above, this
induces Doctor as Person 1, and Patient as bound
to Person 2. This produces the interpreted context
pattern for this sense as shown below.
(7) [[doctor]] treat [[patient]]
Promoted literal types are corpus-derived and
predicate-dependent, and are syntactic heads of
phrases that occur with the greatest frequency in ar-
gument positions for a given sense pattern; they are
subsequently assumed to be subtypes of the particu-
lar shallow type in the pattern. Step (5c) above then
enables us to bind the other lexical heads in these po-
sitions as coerced forms of the promoted literal type.
This can be seen below in the concordance sample,
where therapies is interpreted as Doctor, and people
and girl are interpreted as Patient.
(8) a. a doctor who treated the girl till an ambulance arrived.
b. over 90,000 people have been treated for cholera
c. nonsurgical therapies to treat the breast cancer, which
Model Bias
The assumption within GL is that semantic types
in the grammar map systematically to default syn-
tactic templates (cf. Pustejovsky (1995)). These
are termed canonical syntactic forms (CSFs). For
example, the CSF for the type proposition is a
tensed S. There are, however, many possible real-
izations (such as infinitival S and NP) for this type
due to the different possibilities available from gen-
erative devices in a grammar, such as coercion and
co-composition. The resulting set of syntactic forms
associated with a particular semantic type is called
a phrasal paradigm for that type. The model bias
provided by GL acts to guide the interpretation of
purely statistically based measures.
2.2 Automatic Recognition of Pattern Use
Essentially, this subtask is similar to the traditional
supervised WSD problem. Its purpose is (1) to test
the discriminatory power of CPA-derived feature-
set, (2) to extend and refine the inventory of features
captured by the CPA patterns, and (3) to allow for
predicate-based argument groupings by classifying
unseen instances. Extension and refinement of the
inventory of features should involve feature induc-
tion, but at the moment this part has not been im-
plemented. During the lexical discovery stage, lex-
ical sets that fill some of the argument slots in the
patterns are instantiated from the training exam-
ples. As more predicate-based lexical sets within
shallow types are explored, the data will permit
identification of the types of features that unite ele-
ments in lexical sets.
2.3 Automatic Pattern Acquisition
The algorithm for automatic pattern acquisition in-
volves the following steps:
(9) a. Collect all constituents in a particular argument position;
b. Identify syntactic alternations;
c. Perform clustering on all nouns that occur in a particular
argument position of a given predicate;
d. For each cluster, measure its relatedness to the known
lexical sets, obtained previously during the lexical discovery
stage and extended through WSD of unseen instances. If
none of the existing lexical sets pass the distance threshold,
establish the cluster as a new lexical set, to be used in future
pattern specification.
Step (9d) must include extensive filtering procedures
to check for shared semantic features, looking for
commonality between the members. That is, there
must be some threshold overlap between subgroups
of the candidate lexical set and and the existing se-
mantic classes. For instance, checking if, for a cer-
tain percentage of pairs in the candidate set, there
already exists a set of which both elements are mem-
bers.
3 Current Implementation
The CPA patterns are developed using the British
National Corpus (BNC). The sorted instances are
used as a training set for the supervised disambigua-
tion. For the disambiguation task, each pattern is
translated into into a set of preprocessing-specific
features.
The BNC is preprocessed with the RASP parser
and semantically tagged with BSO types. The
RASP system (Briscoe and Carroll (2002)) gener-
ates full parse trees for each sentence, assigning a
probability to each parse. It also produces a set of
grammatical relations for each parse, specifying the
relation type, the headword, and the dependent ele-
ment. All our computations are performed over the
single top-ranked tree for the sentences where a full
parse was successfully obtained. Some of the RASP
grammatical relations are shown in (10).
(10) subjects: ncsubj, clausal (csubj, xsubj)
objects: dobj, iobj, clausal complement
modifiers: adverbs, modifiers of event nominals
We use endocentric semantic typing, i.e., the head-
word of each constituent is used to establish its se-
mantic type. The semantic tagging strategy is simi-
lar to the one described in Pustejovsky et al (2002),
and currently uses a subset of 24 BSO types.
A CPA pattern is translated into a feature set,
currently using binary features. It is further com-
plemented with other discriminant context features
which, rather than distinguishing a particular pat-
tern, are merely likely to occur with a given subset
of patterns; that is, the features that only partially
determine or co-determine a sense. In the future,
these should be learned from the training set through
feature induction from the training sample, but at
the moment, they are added manually. The result-
ing feature matrix for each pattern contains features
such as those in (11) below. Each pattern is trans-
lated into a template of 15-25 features.
(11) Selected context features:
a. obj institution: object belongs to the BSO type ?Insti-
tution?
b. subj human group: subject belongs to the BSO type ?Hu-
manGroup?
c. mod adv ly: target verb has an adverbial modifier, with a
-ly adverb
d. clausal like: target verb has a clausal argument intro-
duced by ?like?
e. iobj with: target verb has an indirect object introduced
by ?with?
f. obj PRP: direct object is a personal pronoun
g. stem VVG: the target verb stem is an -ing form
Each feature may be realized by a number of RASP
relations. For instance, a feature dealing with
objects would take into account RASP relations
?dobj?, ?obj2?, and ?ncsubj? (for passives).
4 Results and Discussion
The experimental trials performed to date are too
preliminary to validate the methodology outlined
above in general terms for the WSD task. Our re-
sults are encouraging however, and comparable to
the best performing systems reported from Senseval
2. For our experiments, we implemented two ma-
chine learning algorithms, instance-based k-Nearest
Neighbor, and a decision tree algorithm (a version of
ID3). Table 2 shows the results on a subset of verbs
that have been processed, also listing the number of
patterns in the pattern set for each of the verbs.2
verb number of training accuracy
patterns set ID3 kNN
edit 2 100 87% 86%
treat 4 200 45% 52%
submit 4 100 59% 64%
Table 2: Accuracy of pattern identification
Further experimentation is obviously needed to
adequately gauge the effectiveness of the selection
context approach for WSD and other NLP tasks.
It is already clear, however, that the traditional
sense enumeration approach, where senses are asso-
ciated with individual lexical items, must give way
to a model where senses are assigned to the contexts
within which words appear. Furthermore, because
the variability of the stereotypical syntagmatic pat-
terns that are associated with words appears to be
relatively small, such information can be encoded as
lexically-indexed contexts. A comprehensive dictio-
nary of such contexts could prove to be a powerful
tool for a variety of NLP tasks.
References
T. Briscoe and J. Carroll. 2002. Robust accurate statistical anno-
tation of general text. Proceedings of the Third International
Conference on Language Resources and Evaluation (LREC
2002).
J. Pustejovsky, A. Rumshisky, and J. Castano. 2002. Rerendering
Semantic Ontologies: Automatic Extensions to UMLS through
Corpus Analytics. In LREC 2002 Workshop on Ontologies
and Lexical Knowledge Bases..
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.):
MIT Press.
2Test set size for each lemma is 100 instances, selected out
of several randomly chosen segments of BNC, non-overlapping
with the training set
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 9?16, Detroit, June 2005. c?2005 Association for Computational Linguistics
Adaptive String Similarity Metrics for Biomedical Reference Resolution
Ben Wellner
 

The MITRE Corporation
202 Burlington Rd
Bedford MA 01730
wellner@mitre.org
Jose? Castan?o
 
and James Pustejovsky  
 
Computer Science Department
Brandeis University
Waltham MA 02454
 jcastano,jamesp  @cs.brandeis.edu
Abstract
In this paper we present the evaluation
of a set of string similarity metrics used
to resolve the mapping from strings to
concepts in the UMLS MetaThesaurus.
String similarity is conceived as a single
component in a full Reference Resolution
System that would resolve such a map-
ping. Given this qualification, we obtain
positive results achieving 73.6 F-measure
(76.1 precision and 71.4 recall) for the
task of assigning the correct UMLS con-
cept to a given string. Our results demon-
strate that adaptive string similarity meth-
ods based on Conditional Random Fields
outperform standard metrics in this do-
main.
1 Introduction
1.1 String Similarity and Reference Resolution
String similarity/matching algorithms are used as a
component in reference resolution algorithms. We
use reference resolution in a broad sense, which in-
cludes any of the following aspects:
a. Intra-document noun phrase reference resolu-
tion.
b. Cross-document or corpus reference resolution.
c. Resolution of entities found in a corpus with
databases, dictionaries or other external knowl-
edge sources. This is also called semantic inte-
gration, e.g., (Li et al, 2005), reference ground-
ing, e.g., (Kim and Park, 2004) or normaliza-
tion, e.g., (Pustejovsky et al, 2002; Morgan et
al., 2004).
The last two aspects of reference resolution are
particularly important for information extraction,
and the interaction of reference resolution with in-
formation extraction techniques (see for example
Bagga (1998)). The extraction of a particular set of
entities from a corpus requires reference resolution
for the set of entities extracted (e.g., the EDT task in
ACE1), and it is apparent that there is more variation
in the cross-document naming conventions than in a
single document.
The importance of edit distance algorithms has
already been noticed, (Mu?ller et al, 2002) and the
importance of string similarity techniques in the
biomedical domain has also been acknowledged,
e.g., (Yang et al, 2004).
String similarity/matching algorithms have also
been used extensively in related problems such as
Name databases and similar problems in structured
data, see (Li et al, 2005) and references mentioned
therein.
The problem of determining whether two similar
strings may denotate the same entity is particularly
challenging in the biomedical literature. It has al-
ready been noticed (Cohen et al, 2002) that there
is great variation in the naming conventions, and
noun phrase constructions in the literature. It has
also been noticed that bio-databases are hardly ever
updated with the names in the literature (Blaschke
1http://www.nist.gov/speech/tests/ace/
9
et al, 2003). A further complication is that the ac-
tual mentions found in text are more complex than
just names - including descriptors, in particular. Fi-
nally, ambiguity (where multiple entities have the
same name) is very pervasive in biomedicine.
In this paper we investigate the use of several
string similarity methods to group together string
mentions that might refer to the same entity or con-
cept. Specifically, we consider the sub-problem of
assigning an unseen mention to one of a set of exist-
ing unique entities or concepts, each with an associ-
ated set of known synonyms. As our aim here is fo-
cusing on improving string matching, we have pur-
posely factored out the problem of ambiguity (to the
extent possible) by using the UMLS MetaThesaurus
as our data source, which is largly free of strings that
refer to multiple entities. Thus, our work here can be
viewed an important piece in a larger normalization
or reference resolution system that resolves ambigu-
ity (which includes filtering out mentions that don?t
refer to any entity of interest).
The experiments reported on in this paper evalu-
ate a suite of robust string similarity techniques. Our
results demonstrate considerable improvement to be
gained by using adaptive string similarity metrics
based on Conditional Random Fields customized to
the domain at hand. The resulting best metric, we
term SoftTFIDF-CRF, achieves 73.6 F-measure on
the task of assigning a given string to the correct
concept. Additionally, our experiments demonstrate
a tradeoff between efficiency and recall based on  -
gram indexing.
2 Background
2.1 Entity Extraction and Reference
Resolution in the Biomedical Domain
Most of the work related to reference resolution in
this domain has been done in the following areas: a)
Intra-document Reference resolution, e.g (Castan?o
et al, 2002; Lin and Liang, 2004) b) Intra-document
Named entity recognition (e.g Biocreative Task 1A
(Blaschke et al, 2003), and others), also called clas-
sification of biological names (Torii et al, 2004) c)
Intra-document alias extraction d) cross-document
Acronym-expansion extraction, e.g., (Pustejovsky
et al, 2001). e) Protein names resolution against
database entries in SwissProt, protein name ground-
ing, in the context of a relation extraction task
(Kim and Park, 2004). One constraint in these ap-
proaches is that they use several patterns for the
string matching problem. The results of the protein
name grounding are 59% precision and 40% recall.
The Biocreative Task 1B task challenged systems
to ground entities found in article abstracts which
contain mentions of genes in Fly, Mouse and Yeast
databases. A central component in this task was re-
solving ambiguity as many gene names refer to mul-
tiple genes.
2.2 String Similarity and Ambiguity
In this subsection consider the string similarity is-
sues that are present in the biology domain in par-
ticular. The task we consider is to associate a string
with an existing entity, represented by a set of known
strings. Although the issue of ambiguity is present
in the examples we give, it cannot be resolved by
using string similarity methods alone, but instead by
methods that take into account the context in which
those strings occur.
The protein name p21 is ambiguous at least
between two entities, mentioned as p21-ras and
p21/Waf in the literature. A biologist can look at
a set of descriptions and decide whether the strings
are ambiguous or correspond to any of these two (or
any other entity).
The following is an example of such a mapping,
where R corresponds to p21-ras, W to p21(Waf) and
G to another entity (the gene). Also it can be noticed
that some of the mappings include subcases (e.g.,
R.1).2
String Form Entity
ras-p21 protein R
p21 R/W
p21(Waf1/Cip1) W
cyclin-dependent kinase-I p21(Waf-1) W
normal ras p21 protein R
pure v-Kirsten (Ki)-ras p21 R.1
wild type p21 R/W
synthetic peptide P21 R/W.2
p21 promoter G
transforming protein v-p21 R.3
v-p21 R.3
p21CIP1/WAF1 W
protein p21 WAF1/CIP1/Sd:1 W
Table 1: A possible mapping from strings to entities.
2All the examples were taken from the MEDLINE corpus.
10
If we want to use an external knowlege source to
produce such a mapping, we can try to map it to con-
cepts in the UMLS Methatesaurus and entries in the
SwissProt database.
These two entities correspond to the concepts
C0029007 (p21-Ras) and C0288472 (p21-Waf) in
the UMLS Methathesaurus. There are 27 strings or
names in the UMLS that map to C0288472 (Table
2):
oncoprotein p21 CAP20
CDK2-associated protein 20 kDa MDA 6
Cdk2 inhibitor WAF1 CIP1
Cdk-interacting protein cdn1 protein
CDK-Interacting Protein 1 CDKN1A
CDKN1 protein Cip1 protein
Cip-1 protein mda-6 protein
Cyclin-Dependent Kinase Inhibitor 1A p21
p21 cell cycle regulator p21(cip1)
p21 cyclin kinase inhibitor p21(waf1-cip1)
Pic-1 protein (cyclin) p21-WAF1
senescent cell-derived inhibitor protein 1 protein p21
CDKN1A protein WAF1 protein
WAF-1 Protein
Table 2: UMLS strings corresponding to C0288472
There are 8 strings that map to concept C0029007
(Table 3).
Proto-Oncogene Protein p21(ras) p21(c-ras)
p21 RAS Family Protein p21 RAS Protein
Proto-Oncogene Protein ras c-ras Protein
ras Proto-Oncogene Product p21 p21(ras)
Table 3: UMLS strings corresponding to C0029007
It can be observed that there is only one exact
match: p21 in C0288472 and Table 1. It should
be noted that p21, is not present in the UMLS as a
possible string for C0029007. There are other close
matches like p21(Waf1/Cip1) (which seems very
frequent) and p21(waf1-cip1).
An expression like The inhibitor of cyclin-
dependent kinases WAF1 gene product p21 has
a high similarity with Cyclin-Dependent Kinase
Inhibitor 1 A and The cyclin-dependent kinase-I
p21(Waf-1) partially matches Cyclin-Dependent Ki-
nase
However there are other mappings which look
quite difficult unless some context is given to pro-
vide additional clues (e.g., v-p21).
The SwissProt entries CDN1A FELCA,
CDN1A HUMAN and CDN1A MOUSE are
related to p21(Waf). They have the following set of
common description names:
Cyclin-dependent kinase inhibitor 1, p21, CDK-
interacting protein 1.3
There is only one entry in SwissProt related to p21-
ras: Q9PSS8 PLAFE: with the description name
P21-ras protein and a related gene name: Ki-ras.
It should be noted that SwissProt classifies, as dif-
ferent entities, the proteins that refer to different or-
ganisms. The UMLS MetaThesaurus, on the other
hand, does not make this distinction. Neither is this
distinction always present in the literature.
3 Methods for Computing String
Similarity
A central component in the process of normaliza-
tion or reference resolution is computing string sim-
ilarity between two strings. Methods for measuring
string similarity can generally be broken down into
character-based and token-based approaches.
Character-based approaches typically consist of
the edit-distance metric and variants thereof. Edit
distance considers the number of edit operations (ad-
dition, substitution and deletion) required to trans-
form a string  into another string 
	 . The Leven-
stein distance assigns unit cost to all edit operations.
Other variations allow arbitrary costs or special costs
for starting and continuing a ?gap? (i.e., a long se-
quence of adds or deletes).
Token-based approaches include the Jaccard sim-
ilarity metric and the TF/IDF metric. The meth-
ods consider the (possibly weighted) overlap be-
tween the tokens of two strings. Hybrid token and
character-based are best represented by SoftTFIDF,
which includes not only exact token matches but
also close matches (using edit-distance, for exam-
ple). Another approach is to perform the Jaccard
similarity (or TF/IDF) between the  -grams of the
two strings instead of the tokens. See Cohen et
al. (2003) for a detailed overview and comparison
of some of these methods on different data sets.
3There are two more description names for the human and
mouse entries. The SwissProt database has also associated
Gene names to those entries which are related to some of the
possible names that we find in the literature. Those gene names
are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1,
WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are
incorporated in the UMLS as protein names.
11
Recent work has also focused on automatic meth-
ods for adapting these string similarity measures
to specific data sets using machine learning. Such
approaches include using classifiers to weight var-
ious fields for matching database records (Cohen
and Richman, 2001). (Belenko and Mooney, 2003)
presents a generative, Hidden Markov Model for
string similarity.
4 An Adaptive String Similarity Model
Conditional Random Fields (CRF) are a recent, in-
creasingly popular approach to sequence labeling
problems. Informally, a CRF bears resemblance to
a Hidden Markov Model (HMM) in which, for each
input position in a sequence, there is an observed
variable and a corresponding hidden variable. Like
HMMs, CRFs are able to model (Markov) depen-
dencies between the hidden (predicted) variables.
However, because CRFs are conditional, discrimina-
tively trained models, they can incorporate arbitrary
overlapping (non-independent) features over the en-
tire input space ? just like a discriminative classi-
fier.
CRFs are log-linear models that compute the
probability of a state sequence,    	
  ,
given an observed sequence,       	     as:
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 117?125,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Classification of Discourse Coherence Relations: An Exploratory Study
using Multiple Knowledge Sources
Ben Wellner
 
, James Pustejovsky   , Catherine Havasi   ,
Anna Rumshisky
 
and Roser Saur??
 
 
Department of Computer Science
Brandeis University
Waltham, MA USA

The MITRE Corporation
202 Burlington Road
Bedford, MA USA

wellner,jamesp,havasi,arum,roser  @cs.brandeis.edu
Abstract
In this paper we consider the problem of
identifying and classifying discourse co-
herence relations. We report initial re-
sults over the recently released Discourse
GraphBank (Wolf and Gibson, 2005). Our
approach considers, and determines the
contributions of, a variety of syntactic and
lexico-semantic features. We achieve 81%
accuracy on the task of discourse relation
type classification and 70% accuracy on
relation identification.
1 Introduction
The area of modeling discourse has arguably seen
less success than other areas in NLP. Contribut-
ing to this is the fact that no consensus has been
reached on the inventory of discourse relations
nor on the types of formal restrictions placed on
discourse structure. Furthermore, modeling dis-
course structure requires access to considerable
prior linguistic analysis including syntax, lexical
and compositional semantics, as well as the res-
olution of entity and event-level anaphora, all of
which are non-trivial problems themselves.
Discourse processing has been used in many
text processing applications, most notably text
summarization and compression, text generation,
and dialogue understanding. However, it is also
important for general text understanding, includ-
ing applications such as information extraction
and question answering.
Recently, Wolf and Gibson (2005) have pro-
posed a graph-based approach to representing in-
formational discourse relations.1 They demon-
strate that tree representations are inadequate for
1The relations they define roughly follow Hobbs (1985).
modeling coherence relations, and show that many
discourse segments have multiple parents (incom-
ing directed relations) and many of the relations
introduce crossing dependencies ? both of which
preclude tree representations. Their annotation of
135 articles has been released as the GraphBank
corpus.
In this paper, we provide initial results for the
following tasks: (1) automatically classifying the
type of discourse coherence relation; and (2) iden-
tifying whether any discourse relation exists on
two text segments. The experiments we report
are based on the annotated data in the Discourse
GraphBank, where we assume that the discourse
units have already been identified.
In contrast to a highly structured, compositional
approach to discourse parsing, we explore a sim-
ple, flat, feature-based methodology. Such an ap-
proach has the advantage of easily accommodat-
ing many knowledge sources. This type of de-
tailed feature analysis can serve to inform or aug-
ment more structured, compositional approaches
to discourse such as those based on Segmented
Discourse Representation Theory (SDRT) (Asher
and Lascarides, 2003) or the approach taken with
the D-LTAG system (Forbes et al, 2001).
Using a comprehensive set of linguistic fea-
tures as input to a Maximum Entropy classifier,
we achieve 81% accuracy on classifying the cor-
rect type of discourse coherence relation between
two segments.
2 Previous Work
In the past few years, the tasks of discourse seg-
mentation and parsing have been tackled from
different perspectives and within different frame-
works. Within Rhetorical Structure Theory (RST),
Soricut and Marcu (2003) have developed two
117
probabilistic models for identifying clausal ele-
mentary discourse units and generating discourse
trees at the sentence level. These are built using
lexical and syntactic information obtained from
mapping the discourse-annotated sentences in the
RST Corpus (Carlson et al, 2003) to their corre-
sponding syntactic trees in the Penn Treebank.
Within SDRT, Baldridge and Lascarides
(2005b) also take a data-driven approach to
the tasks of segmentation and identification of
discourse relations. They create a probabilistic
discourse parser based on dialogues from the Red-
woods Treebank, annotated with SDRT rhetorical
relations (Baldridge and Lascarides, 2005a). The
parser is grounded on headed tree representations
and dialogue-based features, such as turn-taking
and domain specific goals.
In the Penn Discourse TreeBank (PDTB) (Web-
ber et al, 2005), the identification of discourse
structure is approached independently of any lin-
guistic theory by using discourse connectives
rather than abstract rhetorical relations. PDTB
assumes that connectives are binary discourse-
level predicates conveying a semantic relationship
between two abstract object-denoting arguments.
The set of semantic relationships can be estab-
lished at different levels of granularity, depend-
ing on the application. Miltsakaki, et al (2005)
propose a first step at disambiguating the sense of
a small subset of connectives (since, while, and
when) at the paragraph level. They aim at distin-
guishing between the temporal, causal, and con-
trastive use of the connective, by means of syntac-
tic features derived from the Penn Treebank and a
MaxEnt model.
3 GraphBank
3.1 Coherence Relations
For annotating the discourse relations in text, Wolf
and Gibson (2005) assume a clause-unit-based
definition of a discourse segment. They define
four broad classes of coherence relations:
(1) 1. Resemblance: similarity (par), con-
trast (contr), example (examp), generaliza-
tion (gen), elaboration (elab);
2. Cause-effect: explanation (ce), violated
expectation (expv), condition (cond);
3. Temporal (temp): essentially narration;
4. Attribution (attr): reporting and evidential
contexts.
The textual evidence contributing to identifying
the various resemblance relations is heterogeneous
at best, where, for example, similarity and contrast
are associated with specific syntactic constructions
and devices. For each relation type, there are well-
known lexical and phrasal cues:
(2) a. similarity: and;
b. contrast: by contrast, but;
c. example: for example;
d. elaboration: also, furthermore, in addi-
tion, note that;
e. generalization: in general.
However, just as often, the relation is encoded
through lexical coherence, via semantic associa-
tion, sub/supertyping, and accommodation strate-
gies (Asher and Lascarides, 2003).
The cause-effect relations include conventional
causation and explanation relations (captured as
the label ce), such as (3) below:
(3) cause: SEG1: crash-landed in New Hope,
Ga.,
effect: SEG2: and injuring 23 others.
It also includes conditionals and violated expecta-
tions, such as (4).
(4) cause: SEG1: an Eastern Airlines Lockheed
L-1011 en route from Miami to the Bahamas
lost all three of its engines,
effect: SEG2: and land safely back in Miami.
The two last coherence relations annotated in
GraphBank are temporal (temp) and attribution
(attr) relations. The first corresponds generally to
the occasion (Hobbs, 1985) or narration (Asher
and Lascarides, 2003) relation, while the latter is
a general annotation over attribution of source.2
3.2 Discussion
The difficulty of annotating coherence relations
consistently has been previously discussed in the
literature. In GraphBank, as in any corpus, there
are inconsistencies that must be accommodated
for learning purposes. As perhaps expected, an-
notation of attribution and temporal sequence rela-
tions was consistent if not entirely complete. The
most serious concern we had from working with
2There is one non-rhetorical relation, same, which identi-
fies discontiguous segments.
118
the corpus derives from the conflation of diverse
and semantically contradictory relations among
the cause-effect annotations. For canonical cau-
sation pairs (and their violations) such as those
above, (3) and (4), the annotation was expectedly
consistent and semantically appropriate. Problems
arise, however when examining the treatment of
purpose clauses and rationale clauses. These are
annotated, according to the guidelines, as cause-
effect pairings. Consider (5) below.
(5) cause: SEG1: to upgrade lab equipment in
1987.
effect: SEG2: The university spent $ 30,000
This is both counter-intuitive and temporally false.
The rationale clause is annotated as the cause, and
the matrix sentence as the effect. Things are even
worse with purpose clause annotation. Consider
the following example discourse:3
(6) John pushed the door to open it, but it was
locked.
This would have the following annotation in
GraphBank:
(7) cause: to open it
effect: John pushed the door.
The guideline reflects the appropriate intuition
that the intention expressed in the purpose or ra-
tionale clause must precede the implementation of
the action carried out in the matrix sentence. In
effect, this would be something like
(8) [INTENTION TO SEG1] CAUSES SEG2
The problem here is that the cause-effect re-
lation conflates real event-causation with telos-
directed explanations, that is, action directed to-
wards a goal by virtue of an intention. Given that
these are semantically disjoint relations, which
are furthermore triggered by distinct grammatical
constructions, we believe this conflation should be
undone and characterized as two separate coher-
ence relations. If the relations just discussed were
annotated as telic-causation, the features encoded
for subsequent training of a machine learning al-
gorithm could benefit from distinct syntactic envi-
ronments. We would like to automatically gen-
erate temporal orderings from cause-effect rela-
tions from the events directly annotated in the text.
3This specific example was brought to our attention by
Alex Lascarides (p.c).
Splitting these classes would preserve the sound-
ness of such a procedure, while keeping them
lumped generates inconsistencies.
4 Data Preparation and Knowledge
Sources
In this section we describe the various linguistic
processing components used for classification and
identification of GraphBank discourse relations.
4.1 Pre-Processing
We performed tokenization, sentence tagging,
part-of-speech tagging, and shallow syntactic
parsing (chunking) over the 135 GraphBank docu-
ments. Part-of-speech tagging and shallow parsing
were carried out using the Carafe implementation
of Conditional Random Fields for NLP (Wellner
and Vilain, 2006) trained on various standard cor-
pora. In addition, full sentence parses were ob-
tained using the RASP parser (Briscoe and Car-
roll, 2002). Grammatical relations derived from
a single top-ranked tree for each sentence (head-
word, modifier, and relation type) were used for
feature construction.
4.2 Modal Parsing and Temporal Ordering
of Events
We performed both modal parsing and tempo-
ral parsing over events. Identification of events
was performed using EvITA (Saur?? et al, 2006),
an open-domain event tagger developed under the
TARSQI research framework (Verhagen et al,
2005). EvITA locates and tags all event-referring
expressions in the input text that can be tempo-
rally ordered. In addition, it identifies those gram-
matical features implicated in temporal and modal
information of events; namely, tense, aspect, po-
larity, modality, as well as the event class. Event
annotation follows version 1.2.1 of the TimeML
specifications.4
Modal parsing in the form of identifying sub-
ordinating verb relations and their type was per-
formed using SlinkET (Saur?? et al, 2006), an-
other component of the TARSQI framework. Slin-
kET identifies subordination constructions intro-
ducing modality information in text; essentially,
infinitival and that-clauses embedded by factive
predicates (regret), reporting predicates (say), and
predicates referring to events of attempting (try),
volition (want), command (order), among others.
4See http://www.timeml.org.
119
SlinkET annotates these subordination contexts
and classifies them according to the modality in-
formation introduced by the relation between the
embedding and embedded predicates, which can
be of any of the following types:
 factive: The embedded event is presupposed
or entailed as true (e.g., John managed to
leave the party).
 counter-factive: The embedded event is pre-
supposed as entailed as false (e.g., John was
unable to leave the party).
 evidential: The subordination is introduced
by a reporting or perception event (e.g., Mary
saw/told that John left the party).
 negative evidential: The subordination is a
reporting event conveying negative polarity
(e.g., Mary denied that John left the party).
 modal: The subordination creates an inten-
sional context (e.g., John wanted to leave the
party).
Temporal orderings between events were iden-
tified using a Maximum Entropy classifier trained
on the TimeBank 1.2 and Opinion 1.0a corpora.
These corpora provide annotated events along
with temporal links between events. The link
types included: before ( 
	 occurs before  ) , in-
cludes ( 

occurs sometime during 
	
), simultane-
ous ( 	 occurs over the same interval as  ), begins
(  	 begins at the same time as   ), ends (  	 ends at
the same time as 

).
4.3 Lexical Semantic Typing and Coherence
Lexical semantic types as well as a measure of
lexical similarity or coherence between words in
two discourse segments would appear to be use-
ful for assigning an appropriate discourse rela-
tionship. Resemblance relations, in particular, re-
quire similar entities to be involved and lexical
similarity here serves as an approximation to defi-
nite nominal coreference. Identification of lexical
relationships between words across segments ap-
pears especially useful for cause-effect relations.
In example (3) above, determining a (potential)
cause-effect relationship between crash and injury
is necessary to identify the discourse relation.
4.3.1 Corpus-based Lexical Similarity
Lexical similarity was computed using the
Word Sketch Engine (WSE) (Killgarrif et al,
2004) similarity metric applied over British Na-
tional Corpus. The WSE similarity metric imple-
ments the word similarity measure based on gram-
matical relations as defined in (Lin, 1998) with mi-
nor modifications.
4.3.2 The Brandeis Semantic Ontology
As a second source of lexical coherence, we
used the Brandeis Semantic Ontology or BSO
(Pustejovsky et al, 2006). The BSO is a lexically-
based ontology in the Generative Lexicon tradi-
tion (Pustejovsky, 2001; Pustejovsky, 1995). It fo-
cuses on contextualizing the meanings of words
and does this by a rich system of types and qualia
structures. For example, if one were to look up the
phrase RED WINE in the BSO, one would find its
type is WINE and its type?s type is ALCOHOLIC
BEVERAGE. The BSO contains ontological qualia
information (shown below). Using the BSO, one






wine
CONSTITUTIVE  Alcohol
HAS ELEMENT  Alcohol
MADE OF  Grapes
INDIRECT TELIC  drink activity
INDIRECT AGENTIVE  make alcoholic beverage






is able to find out where in the ontological type
system WINE is located, what RED WINE?s lexi-
cal neighbors are, and its full set of part of speech
and grammatical attributes. Other words have a
different configuration of annotated attributes de-
pending on the type of the word.
We used the BSO typing information to seman-
tically tag individual words in order to compute
lexical paths between word pairs. Such lexical as-
sociations are invoked when constructing cause-
effect relations and other implicatures (e.g. be-
tween crash and injure in Example 3).
The type system paths provide a measure of the
connectedness between words. For every pair of
head words in a GraphBank document, the short-
est path between the two words within the BSO
is computed. Currently, this metric only uses the
type system relations (i.e., inheritance) but prelim-
inary tests show that including qualia relations as
connections is promising. We also computed the
earliest common ancestor of the two words. These
metrics are calculated for every possible sense of
the word within the BSO.
120
The use of the BSO is advantageous compared
to other frameworks such as Wordnet because it
focuses on the connection between words and their
semantic relationship to other items. These con-
nections are captured in the qualia information and
the type system. In Wordnet, qualia-like informa-
tion is only present in the glosses, and they do
not provide a definite semantic path between any
two lexical items. Although synonymous in some
ways, synset members often behave differently in
many situations, grammatical or otherwise.
5 Classification Methodology
This section describes in detail how we con-
structed features from the various knowledge
sources described above and how they were en-
coded in a Maximum Entropy model.
5.1 Maximum Entropy Classification
For our experiments of classifying relation types,
we used a Maximum Entropy classifier5 in order
to assign labels to each pair of discourse segments
connected by some relation. For each instance (i.e.
pair of segments) the classifier makes its decision
based on a set of features. Each feature can query
some arbitrary property of the two segments, pos-
sibly taking into account external information or
knowledge sources. For example, a feature could
query whether the two segments are adjacent to
each other, whether one segment contains a dis-
course connective, whether they both share a par-
ticular word, whether a particular syntactic con-
struction or lexical association is present, etc. We
make strong use of this ability to include very
many, highly interdependent features6 in our ex-
periments. Besides binary-valued features, fea-
ture values can be real-valued and thus capture fre-
quencies, similarity values, or other scalar quanti-
ties.
5.2 Feature Classes
We grouped the features together into various
feature classes based roughly on the knowledge
source from which they were derived. Table 1
describes the various feature classes in detail and
provides some actual example features from each
class for the segment pair described in Example 5
in Section 3.2.
5We use the Maximum Entropy classifier included with
Carafe available at http://sourceforge.net/projects/carafe
6The total maximum number of features occurring in our
experiments is roughly 120,000.
6 Experiments and Results
In this section we provide the results of a set of
experiments focused on the task of discourse rela-
tion classification. We also report initial results on
relation identification with the same set of features
as used for classification.
6.1 Discourse Relation Classification
The task of discourse relation classification in-
volves assigning the correct label to a pair of dis-
course segments.7 The pair of segments to assign
a relation to is provided (from the annotated data).
In addition, we assume, for asymmetric links, that
the nucleus and satellite are provided (i.e., the di-
rection of the relation). For the elaboration rela-
tions, we ignored the annotated subtypes (person,
time, location, etc.). Experiments were carried out
on the full set of relation types as well as the sim-
pler set of coarse-grained relation categories de-
scribed in Section 3.1.
The GraphBank contains a total of 8755 an-
notated coherence relations. 8 For all the ex-
periments in this paper, we used 8-fold cross-
validation with 12.5% of the data used for test-
ing and the remainder used for training for each
fold. Accuracy numbers reported are the average
accuracies over the 8 folds. Variance was gener-
ally low with a standard deviation typically in the
range of 1.5 to 2.0. We note here also that the
inter-annotator agreement between the two Graph-
Bank annotators was 94.6% for relations when
they agreed on the presence of a relation. The
majority class baseline (i.e., the accuracy achieved
by calling all relations elaboration) is 45.7% (and
66.57% with the collapsed categories). These are
the upper and lower bounds against which these
results should be based.
To ascertain the utility of each of the various
feature classes, we considered each feature class
independently by using only features from a sin-
gle class in addition to the Proximity feature class
which serve as a baseline. Table 2 illustrates the
result of this experiment.
We performed a second set of experiments
shown in Table 3 that is essentially the converse
of the previous batch. We take the union of all the
7Each segment may in fact consist of a sequence of seg-
ments. We will, however, use the term segment loosely to
refer to segments or segment sequences.
8All documents are doubly annotated; we used the anno-
tator1 annotations.
121
Feature Description Example
Class
C Words appearing at beginning and end of the two discourse seg-
ments - these are often important discourse cue words.
first1-is-to; first2-is-The
P Proximity and direction between the two segments (in terms of
segments) - binary features such as distance less than 3, distance
greater than 10 were used in addition to the distance value itself;
the distance from beginning of the document using a similar bin-
ning approach
adjacent; dist-less-than-3; dist-less-
than-5; direction-reverse; samesentence
BSO Paths in the BSO up to length 10 between non-function words in the
two segments.
ResearchLab  EducationalActivity
 University
WSE WSE word-pair similarities between words in the two segments
were binned as (  0.05,  0.1,  0.2). We also computed sen-
tence similarity as the sum of the word similarities divided by the
sum of their sentence lengths.
WSE-greater-than-0.05; WSE-
sentence-sim = 0.005417
E Event head words and event head word pairs between segments as
identified by EvITA.
event1-is-upgrade; event2-is-spent;
event-pair-upgrade-spent
SlinkET Event attributes, subordinating links and their types between event
pairs in the two segments
seg1-class-is-occurrence; seg2-class-
is-occurrence; seg1-tense-is-infinitive;
seg2-tense-is-past; seg2-modal-seg1
C-E Cuewords of one segment paired with events in the other. first1-is-to-event2-is-spent; first2-is-
The-event1-is-upgrade
Syntax Grammatical dependency relations between two segments as iden-
tified by the RASP parser. We also conjoined the relation with one
or both of the headwords associated with the grammatical relation.
gr-ncmod; gr-ncmod-head1-equipment;
gr-ncmod-head-2-spent; etc.
Tlink Temporal links between events in the two segments. We included
both the link types and the number of occurrences of those types
between the segments
seg2-before-seg1
Table 1: Feature classes, their descriptions and example feature instances for Example 5 in Section 3.2.
Feature Class Accuracy Coarse-grained Acc.
Proximity 60.08% 69.43%
P+C 76.77% 83.50%
P+BSO 62.92% 74.40%
P+WSE 62.20% 70.10%
P+E 63.84% 78.16%
P+SlinkET 69.00% 75.91%
P+CE 67.18% 78.63%
P+Syntax 70.30% 80.84%
P+Tlink 64.19% 72.30%
Table 2: Classification accuracy over standard and
coarse-grained relation types with each feature
class added to Proximity feature class.
feature classes and perform ablation experiments
by removing one feature class at a time.
Feature Class Accuracy Coarse-grain Acc.
All Features 81.06% 87.51%
All-P 71.52% 84.88%
All-C 75.71% 84.69%
All-BSO 80.65% 87.04%
All-WSE 80.26% 87.14%
All-E 80.90% 86.92%
All-SlinkET 79.68% 86.89%
All-CE 80.41% 87.14%
All-Syntax 80.20% 86.89%
All-Tlink 80.30% 87.36%
Table 3: Classification accuracy with each fea-
ture class removed from the union of all feature
classes.
6.2 Analysis
From the ablation results, it is clear that overall
performance is most impacted by the cue-word
features (C) and proximity (P). Syntax and Slin-
kET also have high impact improving accuracy by
roughly 10 and 9 percent respectively as shown
in Table 2. From the ablation results in Table 3,
it is clear that the utility of most of the individ-
ual features classes is lessened when all the other
feature classes are taken into account. This indi-
cates that multiple feature classes are responsible
for providing evidence any given discourse rela-
tions. Removing a single feature class degrades
performance, but only slightly, as the others can
compensate.
Overall precision, recall and F-measure results
for each of the different link types using the set
of all feature classes are shown in Table 4 with the
corresponding confusion matrix in Table A.1. Per-
formance correlates roughly with the frequency of
the various relation types. We might therefore ex-
pect some improvement in performance with more
annotated data for those relations with low fre-
quency in the GraphBank.
122
Relation Precision Recall F-measure Count
elab 88.72 95.31 91.90 512
attr 91.14 95.10 93.09 184
par 71.89 83.33 77.19 132
same 87.09 75.00 80.60 72
ce 78.78 41.26 54.16 63
contr 65.51 66.67 66.08 57
examp 78.94 48.39 60.00 31
temp 50.00 20.83 29.41 24
expv 33.33 16.67 22.22 12
cond 45.45 62.50 52.63 8
gen 0.0 0.0 0.0 0
Table 4: Precision, Recall and F-measure results.
6.3 Coherence Relation Identification
The task of identifying the presence of a rela-
tion is complicated by the fact that we must con-
sider all ffProceedings of the Linguistic Annotation Workshop, pages 109?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Independent Syntactic and Semantic Annotation Schemes
Marc Verhagen, Amber Stubbs and James Pustejovsky
Computer Science Department
Brandeis University, Waltham, USA
{marc,astubbs,jamesp}@cs.brandeis.edu
Abstract
We present MAIS, a UIMA-based environ-
ment for combining information from var-
ious annotated resources. Each resource
contains one mode of linguistic annotation
and remains independent from the other re-
sources. Interactions between annotations
are defined based on use cases.
1 Introduction
MAIS is designed to allow easy access to a set of
linguistic annotations. It embodies a methodology
to define interactions between separate annotation
schemes where each interaction is based on a use
case. With MAIS, we adopt the following require-
ments for the interoperability of syntactic and se-
mantic annotations:
1. Each annotation scheme has its own philosophy
and is independent from the other annotations.
Simple and generally available interfaces pro-
vide access to the content of each annotation
scheme.
2. Interactions between annotations are not de-
fined a priori, but based on use cases.
3. Simple tree-based and one-directional merg-
ing of annotations is useful for visualization of
overlap between schemes.
The annotation schemes currently embedded in
MAIS are the Proposition Bank (Palmer et al,
2005), NomBank (Meyers et al, 2004) and Time-
Bank (Pustejovsky et al, 2003). Other linguis-
tics annotation schemes like the opinion annotation
(Wiebe et al, 2005), named entity annotation, and
discourse annotation (Miltsakaki et al, 2004) will
be added in the future.
In the next section, we elaborate on the first
two requirements mentioned above and present the
MAIS methodology to achieve interoperability of
annotations. In section 3, we present the XBank
Browser, a unified browser that allows researchers
to inspect overlap between annotation schemes.
2 Interoperability of Annotations
Our goal is not to define a static merger of all anno-
tation schemes. Rather, we avoid defining a poten-
tially complex interlingua and instead focus on how
information from different sources can be combined
pragmatically. A high-level schematic representa-
tion of the system architecture is given in figure 1.
PropBank NomBank TimeBank
PropBank NomBank TimeBank
annotation     initializers
interface interface interface
case-based 
interaction
case-based 
interaction
GUI GUI
Figure 1: Architecture of MAIS
109
The simple and extensible interoperability of
MAIS can be put in place using three components: a
unified environment that stores the annotations and
implements some common functionality, a set of an-
notation interfaces, and a set of case-based interac-
tions.
2.1 Unified Environment
All annotations are embedded as stand-off annota-
tions in a unified environment in which each annota-
tion has its own namespace. This unified environ-
ment takes care of some basic functionality. For
example, given a tag from one annotation scheme,
there is a method that returns tags from other anno-
tation schemes that have the same text extent or tags
that have an overlap in text extent. The unified envi-
ronment chosen for MAIS is UIMA, the open plat-
form for unstructured information analysis created
by IBM.1
UIMA implements a common data representation
named CAS (Common Analysis Structure) that pro-
vides read and write access to the documents being
analyzed. Existing annotations can be imported into
a CAS using CAS Initializers. UIMA also provides
a framework for Analysis Engines: modules that can
read from and write to a CAS and that can be com-
bined into a complex work flow.
2.2 Annotation Interfaces
In the unified environment, the individual annota-
tions are independent from each other and they are
considered immutable. Each annotation defines an
interface through which salient details of the anno-
tations can be retrieved. For example, annotation
schemes that encodes predicate-argument structure,
that is, PropBank and NomBank, define methods
like
args-of-relation(pred)
arg-of-relation(pred, arg)
relation-of-argument(arg)
Similarly, the interface for TimeBank includes
methods like
rel-between(eventi, eventj)
events-before(event)
event-anchorings(event)
1http://www.research.ibm.com/UIMA/
The arguments to these methods are not strings
but text positions, where each text position contains
an offset and a document identifier. Return values
are also text positions. All interfaces are required to
include a method that returns the tuples that match a
given string:
get-locations(string, type)
This method returns a set of text positions. Each
text position points to a location where the input
string occurs as being of the given type. For Time-
Bank, the type could be event or time, for Prop-
Bank and NomBank, more appropriate values are
rel or arg0.
2.3 Case-based Interactions
Most of the integration work occurs in the interac-
tion components. Specific interactions can be built
using the unified environment and the specified in-
terfaces of each annotation scheme.
Take for example, the use case of an entity chron-
icle (Pustejovsky and Verhagen, 2007). An entity
chronicle follows an entity through time, display-
ing what events an entity was engaged in, how these
events are anchored to time expressions, and how the
events are ordered relative to each other. Such an
application depends on three kinds of information:
identification of named entities, predicate-argument
structure, and temporal relations. Each of these de-
rive from a separate annotation scheme. A use case
can be built using the interfaces for each annotation:
? the named entity annotation returns the text
extents of the named entity, using the gen-
eral method get-locations(string,
type)
? the predicate-argument annotation (accessed
through the PropBank and NomBank inter-
faces) returns the predicates that go with a
named-entity argument, repeatedly using the
method relation-of-argument(arg)
? finally, the temporal annotation returns the tem-
poral relations between all those predicates,
calling rel-between(eventi, eventj)
on all pairs of predicates
110
Note that named entity annotation is not inte-
grated into the current system. As a stopgap mea-
sure we use a pre-compiled list of named entities
and feed elements of this list into the PropBank
and NomBank interfaces, asking for those text po-
sitions where the entity is expressed as an argu-
ment. This shows the utility of a general method
like get-locations(string, type).
Each case-based interaction is implemented using
one or more UIMA analysis engines. It should be
noted that the analysis engines used for the entity
chronicler do not add data to the common data repre-
sentation. This is not a principled choice: if adding
new data to the CAS is useful then it can be part of
the case-based interaction, but these added data are
not integrated into existing annotations, rather, they
are added as a separate secondary resource.2
The point of this approach is that applications can
be built pragmatically, using only those resources
that are needed. It does not depend on fully merged
syntactic and semantic representations. The entity
chronicle, for example, does not require discourse
annotation, opinion annotation or any other resource
except for the three discussed before. An a priori
requirement to have a unified representation intro-
duces complexities that go beyond what?s needed for
individual applications.
This is not to say that a unified representation is
not useful on its own, there is obvious theoretical
interest in thoroughly exploring how annotations re-
late to each other. But we feel that the unified repre-
sentation is not needed for most, if not all, practical
applications.
3 The XBank Browser
The unified browser, named the XBank Browser, is
intended as a convenience for researchers. It shows
the overlap between different annotations. Annota-
tions from different schemes are merged into one
XML representation and a set of cascading style
sheets is used to display the information.
2In fact, for the entity chronicle it would be useful to have
extra data available. The current implementation uses what?s
provided by the basic resources plus a few heuristics to super-
ficially merge data from separate documents. But a more in-
formative chronicle along the lines of (Pustejovsky and Verha-
gen, 2007) would require more temporal links than available in
TimeBank. These can be pre-compiled and added using a dedi-
cated analysis engine.
The XBank Browser does not adhere to the MAIS
philosophy that all resources are independent. In-
stead, it designates one syntactic annotation to pro-
vide the basic shape of the XML tree and requires
tags from other annotations to find landing spots in
the basic tree.
The Penn Treebank annotation (Marcus et al,
1993) was chosen to be the first among equals: it
is the starting point for the merger and data from
other annotations are attached at tree nodes. Cur-
rently, only one heuristic is used to merge in data
from other sources: go up the tree to find a Treebank
constituent that contains the entire extent of the tag
that is merged in, then select the head of this con-
stituent. A more sophisticated approach would con-
sist of two steps:
? first try to find an exact match of the imported
tag with a Treebank constituent,
? if that fails, find the constituent that contains
the entire tag that is merged in, and select this
constituent
In the latter case, there can be an option to select
the head rather than the whole constituent. In any
case, the attached node will be marked if its original
extent does not line up with the extent at the tree
node.
It should be noted that this merging is one-
directional since no attempt is made to change the
shape of the tree defined by the Treebank annota-
tion.
The unified browser currently displays markups
from the Proposition Bank, NomBank, TimeBank
and the Discourse Treebank. Tags from individual
schemes can be hidden as desired. The main prob-
lem with the XBank Browser is that there is only a
limited amount of visual clues that can be used to
distinguish individual components from each other
and cognitive overload restricts how many annota-
tion schemes can be viewed at the same time. Nev-
ertheless, the browser does show how a limited num-
ber of annotation schemes relate to each other.
All functionality of the browser can be accessed at
http://timeml.org/ula/. An idea of what
it looks like can be gleaned from the screenshot dis-
played in figure 2. In this figure, boxes represent
relations from PropBank or NomBank and shaded
111
Figure 2: A glimpse of the XBank Browser
backgrounds represent arguments. Superscripts are
indexes that identify relations, subscripts identify
what relation an argument belongs to. Red fonts
indicate events from TimeBank. Note that the real
browser is barely done justice by this picture be-
cause the browser?s use of color is not visible.
4 Conclusion
We described MAIS, an environment that imple-
ments interoperability between syntactic and seman-
tic annotation schemes. The kind of interoperabil-
ity proposed herein does not require an elaborate
representational structure that allows the interaction.
Rather, it relies on independent annotation schemes
with interfaces to the outside world that interact
given a specific use case. The more annotations
there are, the more interactions can be defined. The
complexity of the methodology is not bound by the
number of annotation schemes integrated but by the
complexity of the use cases.
5 Acknowledgments
The work reported in this paper was performed as
part of the project ?Towards a Comprehensive Lin-
guistic Annotation of Language?, and supported un-
der award CNS-0551615 of the National Science
Foundation.
References
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treeb. Computational
Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The penn discourse treebank.
In Proceedings of the Language Resources and Evalu-
ation Conference, Lisbon, Portugal.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky and Marc Verhagen. 2007. Con-
structing event-based entity chronicles. In Proceed-
ings of the IWCS-7, Tilburg, The Netherlands.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003. The timebank corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
112
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 88?93,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 7: Argument Selection and Coercion
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
jamesp@cs.brandeis.edu
Anna Rumshisky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
arum@cs.brandeis.edu
Abstract
In this paper, we describe the Argument Se-
lection and Coercion task, currently in devel-
opment for the SemEval-2 evaluation exercise
scheduled for 2010. This task involves char-
acterizing the type of compositional operation
that exists between a predicate and the argu-
ments it selects. Specifically, the goal is to
identify whether the type that a verb selects is
satisfied directly by the argument, or whether
the argument must change type to satisfy the
verb typing. We discuss the problem in detail
and describe the data preparation for the task.
1 Introduction
In recent years, a number of annotation schemes that
encode semantic information have been developed
and used to produce data sets for training machine
learning algorithms. Semantic markup schemes that
have focused on annotating entity types and, more
generally, word senses, have been extended to in-
clude semantic relationships between sentence ele-
ments, such as the semantic role (or label) assigned
to the argument by the predicate (Palmer et al, 2005;
Ruppenhofer et al, 2006; Kipper, 2005; Burchardt
et al, 2006; Ohara, 2008; Subirats, 2004).
In this task, we take this one step further, in that
this task attempts to capture the ?compositional his-
tory? of the argument selection relative to the pred-
icate. In particular, this task attempts to identify the
operations of type adjustment induced by a predicate
over its arguments when they do not match its selec-
tional properties. The task is defined as follows: for
each argument of a predicate, identify whether the
entity in that argument position satisfies the type ex-
pected by the predicate. If not, then one needs to
identify how the entity in that position satisfies the
typing expected by the predicate; that is, to identify
the source and target types in a type-shifting (or co-
ercion) operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition as in (1). Notice, however, that through a
metonymic interpretation, this constraint can be vi-
olated as demonstrated in (1).
(1) a. John reported in late from Washington.
b. Washington reported in late.
Neither the surface annotation of entity extents and
types, nor assigning semantic roles associated with
the predicate would reflect in this case a crucial
point: namely, that in order for the typing require-
ments of the predicate to be satisfied, what has been
referred to a type coercion or a metonymy (Hobbs et
al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg,
2005) has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This task
involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people,
place-for-event, place-for-product;
ii. Categories for Organizations: literal, organization-
for-members, organization-for-event, organization-for-
product, organization-for-facility.
One of the limitations of this approach, how-
ever, is that, while appropriate for these special-
ized metonymy relations, the annotation specifica-
tion and resulting corpus are not an informative
88
guide for extending the annotation of argument se-
lection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for
the verb enjoy should arguably assign similar values
to both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .
b. Mary enjoyed her beer.
The consequence of this, however, is that, under cur-
rent sense and role annotation strategies, the map-
ping to a syntactic realization for a given sense is
made more complex, and is in fact, perplexing for a
clustering or learning algorithm operating over sub-
categorization types for the verb.
2 Methodology of Annotation
Before introducing the specifics of the argument se-
lection and coercion task, let us review briefly our
assumptions regarding the role of annotation within
the development and deployment of computational
linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough to
capture the desired behavior. These linguistic de-
scriptions are typically distilled from extensive the-
oretical modeling of the phenomenon. The descrip-
tions in turn form the basis for the annotation values
of the specification language, which are themselves
the features used in a development cycle for training
and testing an identification or labeling algorithm
over text. Finally, based on an analysis and evalu-
ation of the performance of a system, the model of
the phenomenon may be revised, for retraining and
testing.
We call this particular cycle of development the
MATTER methodology:
(4) a. Model: Structural descriptions provide
theoretically-informed attributes derived from
empirical observations over the data;
b. Annotate: Annotation scheme assumes a feature
set that encodes specific structural descriptions and
properties of the input data;
c. Train: Algorithm is trained over a corpus annotated
with the target feature set;
Figure 1: The MATTER Methodology
d. Test: Algorithm is tested against held-out data;
e. Evaluate: Standardized evaluation of results;
f. Revise: Revisit the model, annotation specification,
or algorithm, in order to make the annotation more
robust and reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cycle
include:
? PropBank (Palmer et al, 2005)
? NomBank (Meyers et al, 2004)
? TimeBank (Pustejovsky et al, 2005)
? Opinion Corpus (Wiebe et al, 2005)
? Penn Discourse TreeBank (Miltsakaki et al, 2004)
3 Task Description
This task involves identifying the selectional mech-
anism used by the predicate over a particular argu-
ment.1 For the purposes of this task, the possible re-
lations between the predicate and a given argument
are restricted to selection and coercion. In selection,
the argument NP satisfies the typing requirements of
the predicate, as in (5).
(5) a. The spokesman denied the statement (PROPOSITION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn?t believe the rumor (PROPOSI-
TION).
Coercion encompasses all cases when a type-
shifting operation must be performed on the com-
plement NP in order to satisfy selectional require-
ments of the predicate, as in (6). Note that coercion
operations may apply to any argument position in a
sentence, including the subject, as seen in (6b). Co-
ercion can also be seen as an object of a proposition
as in (6c).
(6) a. The president denied the attack (EVENT ? PROPOSI-
TION).
b. The White House (LOCATION ? HUMAN) denied this
statement.
c. The Boston office called with an update (EVENT ?
INFO).
1This task is part of a larger effort to annotate text with com-
positional operations (Pustejovsky et al, 2009).
89
The definition of coercion will be extended to in-
clude instances of type-shifting due to what we term
the qua-relation.
(7) a. You can crush the pill (PHYSICAL OBJECT) between
two spoons. (Selection)
b. It is always possible to crush imagination (ABSTRACT
ENTITY qua PHYSICAL OBJECT) under the weight of
numbers. (Coercion/qua-relation)
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve the following (1) identifying the verb sense
and the associated syntactic frame, (2) identifying
selectional requirements imposed by that verb sense
on the target argument, and (3) identifying semantic
type of the target argument. Sense inventories for
the verbs and the type templates associated with dif-
ferent syntactic frames will be provided to the par-
ticipants.
3.1 Semantic Types
In the present task, we use a subset of semantic types
from the Brandeis Shallow Ontology (BSO), which
is a shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky et al,
2004; Rumshisky et al, 2006). The BSO types were
selected for their prevalence in manually identified
selection context patterns developed for several hun-
dreds English verbs. That is, they capture common
semantic distinctions associated with the selectional
properties of many verbs.
The following list of types is currently being used
for annotation:
(8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT,
ORGANIZATION, EVENT, PROPOSITION, INFORMA-
TION, SENSATION, LOCATION, TIME PERIOD, AB-
STRACT ENTITY, ATTITUDE, EMOTION, PROPERTY,
PRIVILEGE, OBLIGATION, RULE
The subset of types chosen for annotation is pur-
posefully shallow, and is not structured in a hierar-
chy. For example, we include both HUMAN and AN-
IMATE in the type system along with PHYSICAL OB-
JECT. While HUMAN is a subtype of both ANIMATE
and PHYSICAL OBJECT, the system should simply
choose the most relevant type (i.e. HUMAN) and not
be concerned with type inheritance. The present set
of types may be revised if necessary as the annota-
tion proceeds.
Figure 2: Corpus Development Architecture
4 Resources and Corpus Development
Preparing the data for this task will be done in two
phases: the data set construction phase and the an-
notation phase. The first phase consists of (1) select-
ing the target verbs to be annotated and compiling a
sense inventory for each target, and (2) data extrac-
tion and preprocessing. The prepared data is then
loaded into the annotation interface. During the an-
notation phase, the annotation judgments are entered
into the database, and the adjudicator resolves dis-
agreements. The resulting database representation is
used by the exporting module to generate the corre-
sponding XML markup or stand-off annotation. The
corpus development architecture is shown in Fig. 2.
4.1 Data Set Construction Phase
In the set of target verbs selected for the task, pref-
erence will be given to the verbs that are strongly
coercive in at least one of their senses, i.e. tend to
impose semantic typing on one of their arguments.
The verbs will be selected by examining the data
from several sources, using the Sketch Engine (Kil-
garriff et al, 2004) as described in (Rumshisky and
Batiukova, 2008).
An inventory of senses will be compiled for each
verb. Whenever possible, the senses will be mapped
to OntoNotes (Pradhan et al, 2007) and to the CPA
patterns (Hanks, 2009). For each sense, a set of type
90
templates will be compiled, associating each sense
with one or more syntactic patterns which will in-
clude type specification for all arguments. For ex-
ample, one of the senses of the verb deny is refuse
to grant. This sense is associated with the following
type templates:
(9) HUMAN deny ENTITY to HUMAN
HUMAN deny HUMAN ENTITY
The set of type templates for each verb will be built
using a modification of the CPA technique (Hanks
and Pustejovsky, 2005; Pustejovsky et al, 2004)).
A set of sentences will be randomly extracted for
each target verb from the BNC (BNC, 2000) and
the American National Corpus (Ide and Suderman,
2004). This choice of corpora should ensure a more
balanced representation of language than is available
in commonly annotated WSJ and other newswire
text. Each extracted sentence will be automatically
parsed, and the sentences organized according to the
grammatical relation involving the target verb. Sen-
tences will be excluded from the set if the target ar-
gument is expressed as anaphor, or is not present in
the sentence. Semantic head for the target grammat-
ical relation will be identified in each case.
4.2 Annotation Phase
Word sense disambiguation will need to be per-
formed as a preliminary stage for the annotation of
compositional operations. The annotation task is
thus divided into two subtasks, presented succes-
sively to the annotator:
(1) Word sense disambiguation of the target predi-
cate
(2) Identification of the compositional relationship
between target predicate and its arguments
In the first subtask, the annotator is presented with
a set of sentences containing the target verb and the
chosen grammatical relation. The annotator is asked
to select the most fitting sense of the target verb, or
to throw out the example (pick the ?N/A? option) if
no sense can be chosen either due to insufficient con-
text, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be
made in good faith. The interface is shown in Fig.
3. After this step is complete, the appropriate sense
is saved into the database, along with the associated
type template.
In the second subtask, the annotator is presented
with a list of sentences in which the target verb
is used in the same sense. The data is annotated
one grammatical relation at a time. The annotator
is asked to determine whether the argument in the
specified grammatical relation to the target belongs
to the type associated with that sense in the corre-
sponding template. The illustration of this can be
seen in Fig. 4. We will perform double annotation
and subsequent adjudication at each of the above an-
notation stages.
5 Data Format
The test and training data will be provided in XML
format. The relation between the predicate (viewed
as function) and its argument will be represented by
a composition link (CompLink) as shown below.
In case of coercion, there is a mismatch between the
source and the target types, and both types need to
be identified:
The State Department repeatedly denied the attack.
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
the
<NOUN nid="n1">attack</NOUN> .
<CompLink cid="cid1" sID="s1"
relatedToNoun="n1" gramRel="dobj"
compType="COERCION"
sourceType="EVENT"
targetType="PROPOSITION"/>
When the compositional operation is selection, the
source and the target types must match:
The State Department repeatedly denied this statement.
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
this
<NOUN nid="n1">statement</NOUN> .
<CompLink cid="cid1" sID="s1"
relatedToNoun="n1" gramRel="dobj"
compType="selection"
sourceType="PROPOSITION"
targetType="PROPOSITION"/>
6 Evaluation Methodology
Precision and recall will be used as evaluation met-
rics. A scoring program will be supplied for partic-
ipants. Two subtasks will be evaluated separately:
91
Figure 3: Predicate Sense Disambiguation for deny.
(1) identifying the compositional operation (i.e. se-
lection vs. coercion) and (2) identifying the source
and target argument type, for each relevant argu-
ment. Both subtasks require sense disambiguation
which will not be evaluated separately.
Since type-shifting is by its nature a relatively
rare event, the distribution between different types
of compositional operations in the data set will be
necessarily skewed. One of the standard sampling
methods for handling class imbalance is downsiz-
ing (Japkowicz, 2000; Monard and Batista, 2002),
where the number of instances of the major class in
the training set is artificially reduced. Another possi-
ble alternative is to assign higher error costs to mis-
classification of minor class instances (Chawla et al,
2004; Domingos, 1999).
7 Conclusion
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2, to be held
in 2010. This task involves the identifying the rela-
tion between a predicate and its argument as one that
encodes the compositional history of the selection
process. This allows us to distinguish surface forms
that directly satisfy the selectional (type) require-
ments of a predicate from those that are coerced in
context. We described some details of a specifica-
tion language for selection and the annotation task
using this specification to identify argument selec-
tion behavior. Finally, we discussed data preparation
for the task and evaluation techniques for analyzing
the results.
References
BNC. 2000. The British National Corpus.
The BNC Consortium, University of Oxford,
http://www.natcorp.ox.ac.uk/.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The salsa corpus: a german corpus resource for lexical
semantics. In Proceedings of LREC, Genoa, Italy.
N. Chawla, N. Japkowicz, and A. Kotcz. 2004. Editorial:
special issue on learning from imbalanced data sets.
ACM SIGKDD Explorations Newsletter, 6(1):1?6.
P. Domingos. 1999. Metacost: A general method for
making classifiers cost-sensitive. In Proceedings of
the fifth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 155?
164. ACM New York, NY, USA.
Marcus Egg. 2005. Flexible semantics for reinterpreta-
tion phenomena. CSLI, Stanford.
P. Hanks and J. Pustejovsky. 2005. A pattern dictionary
for natural language processing. Revue Franc?aise de
Linguistique Applique?e.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpreta-
tion as abduction. Artificial Intelligence, 63:69?142.
N. Ide and K. Suderman. 2004. The American National
Corpus first release. In Proceedings of LREC 2004,
pages 1681?1684.
92
Figure 4: Identifying Compositional Relationship for deny.
N. Japkowicz. 2000. Learning from imbalanced data
sets: a comparison of various strategies. In AAAI
workshop on learning from imbalanced data sets,
pages 00?05.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell. 2004.
The Sketch Engine. Proceedings of Euralex, Lorient,
France, pages 105?116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, University
of Pennsylvania, PA.
K. Markert and M. Nissim. 2007. Metonymy resolution
at SemEval I: Guidelines for participants. In Proceed-
ings of the ACL 2007 Conference.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?
31.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The Penn Discourse Treebank. In Proceedings of the
4th International Conference on Language Resources
and Evaluation.
M.C. Monard and G.E. Batista. 2002. Learning with
skewed class distributions. Advances in logic, artifi-
cial intelligence and robotics (LAPTEC?02).
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143?184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and mul-
tilinguality in the japanese framenet. In Proceedings
of LREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007.
Semeval-2007 task-17: English lexical sample, srl and
all words. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 87?92, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004. Au-
tomated Induction of Sense in Context. In COLING
2004, Geneva, Switzerland, pages 924?931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123?164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth International
Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy in
verbs: systematic relations between senses and their
effect on annotation. In COLING Workshop on Hu-
man Judgement in Computational Linguistics (HJCL-
2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Confer-
ence, FLAIRS 2006, Melbourne Beach, Florida, USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice.
Carlos Subirats. 2004. FrameNet Espan?ol. Una red
sema?ntica de marcos conceptuales. In VI International
Congress of Hispanic Linguistics, Leipzig.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2):165?210.
93
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 13:
Evaluating Events, Time Expressions, and Temporal Relations
(TempEval-2)
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
jamesp@cs.brandeis.edu
Marc Verhagen
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
marc@cs.brandeis.edu
Abstract
We describe the TempEval-2 task which is
currently in preparation for the SemEval-2010
evaluation exercise. This task involves iden-
tifying the temporal relations between events
and temporal expressions in text. Six distinct
subtasks are defined, ranging from identifying
temporal and event expressions, to anchoring
events to temporal expressions, and ordering
events relative to each other.
1 Introduction
Newspaper texts, narratives and other such texts de-
scribe events which occur in time and specify the
temporal location and order of these events. Text
comprehension, even at the most general level, in-
volves the capability to identify the events described
in a text and locate these in time. This capability is
crucial to a wide range of NLP applications, from
document summarization and question answering to
machine translation. As in many areas of NLP, an
open evaluation challenge in the area of temporal an-
notation will serve to drive research forward.
The automatic identification of all temporal re-
ferring expressions, events, and temporal relations
within a text is the ultimate aim of research in this
area. However, addressing this aim in a first evalua-
tion challenge was deemed too difficult and a staged
approach was suggested. The 2007 SemEval task,
TempEval (henceforth TempEval-1), was an initial
evaluation exercise based on three limited tasks that
were considered realistic both from the perspective
of assembling resources for development and test-
ing and from the perspective of developing systems
capable of addressing the tasks.
We are now preparing TempEval-2, a temporal
evaluation task based on TempEval-1. TempEval-2
is more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
2 TempEval-1
TempEval-1 consisted of three tasks:
A. determine the relation between an event and a
timex in the same sentence;
B. determine the relation between an event and the
document creation time;
C. determine the relation between the main events
of two consecutive sentences.
The data sets were based on TimeBank (Puste-
jovsky et al, 2003; Boguraev et al, 2007), a hand-
built gold standard of annotated texts using the
TimeML markup scheme.1 The data sets included
sentence boundaries, TIMEX3 tags (including the
special document creation time tag), and EVENT
tags. For tasks A and B, a restricted set of events
was used, namely those events that occur more than
5 times in TimeBank. For all three tasks, the re-
lation labels used were BEFORE, AFTER, OVER-
LAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER
and VAGUE.2 For a more elaborate description of
TempEval-1, see (Verhagen et al, 2007; Verhagen
et al, 2009).
1See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog number
LDC2006T08.
2Which is different from the set of 13 labels from TimeML.
The set of labels for TempEval-1 was simplified to aid data
preparation and to reduce the complexity of the task.
112
There were six systems competing in TempEval-
1: University of Colorado at Boulder (CU-TMP);
Language Computer Corporation (LCC-TE); Nara
Institute of Science and Technology (NAIST); Uni-
versity of Sheffield (USFD); Universities of Wolver-
hampton and Allicante (WVALI); and XEROX Re-
search Centre Europe (XRCE-T).
The difference between these systems was not
large, and details of system performance, along with
comparisons and evaluation, are presented in (Ver-
hagen et al, 2009). The scores for WVALI?s hybrid
approach were noticeably higher than those of the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the highest scoring
systems are barely ahead of the rest of the field. Sim-
ilarly, for task C using strict scoring, there is no sys-
tem that clearly separates itself from the field. Inter-
estingly, the baseline is close to the average system
performance on task A, but for other tasks the sys-
tem scores noticeably exceed the baseline. Note that
the XRCE-T system is somewhat conservative in as-
signing TLINKS for tasks A and B, producing lower
recall scores than other systems, which in turn yield
lower f-measure scores. For task A, this is mostly
due to a decision only to assign a temporal relation
between elements that can also be linked by the syn-
tactic analyzer.
3 TempEval-2
The set of tasks chosen for TempEval-1 was by no
means complete, but was a first step towards a fuller
set of tasks for temporal parsing of texts. While the
main goal of the division in subtasks was to aid eval-
uation, the larger goal of temporal annotation in or-
der to create a complete temporal characterization of
a document was not accomplished. Results from the
first competition indicate that task A was defined too
generally. As originally defined, it asks to tempo-
rally link all events in a sentence to all time expres-
sions in the same sentence. A clearer task would
have been to solicit local anchorings and to sepa-
rate these from the less well-defined temporal rela-
tions between arbitrary events and times in the same
sentence. We expect both inter-annotator agree-
ment and system performance to be higher with a
more precise subtask. Thus, the set of tasks used
in TempEval-1 is far from complete and the tasks
could have been made more restrictive. As a re-
sult, inter-annotator agreement scores lag, making
precise evaluation more challenging.
The overall goal of temporal tagging of a text is to
provide a temporal characterization of a set of events
that is as complete as possible. If the annotation
graph of a document is not completely connected
then it is impossible to determine temporal relations
between two arbitrary events because these events
could be in separate subgraphs. Hence, for the cur-
rent competition, TempEval-2, we have enriched the
task description to bring us closer to creating such
a temporal characterization for a text. We have en-
riched the TempEval-2 task definition to include six
distinct subtasks:
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL. The possible values of
TYPE are TIME, DATE, DURATION, and SET;
the value of VAL is a normalized value as de-
fined by the TIMEX2 and TIMEX3 standards.
B. Determine the extent of the events in a text as
defined by the TimeML EVENT tag. In addi-
tion, determine the value of the features TENSE,
ASPECT, POLARITY, and MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same sen-
tence. For TempEval-2, this task is further re-
stricted by requiring that either the event syn-
tactically dominates the time expression or the
event and time expression occur in the same
noun phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically dominates
the other event. This refers to examples like
?she heard an explosion? and ?he said they
postponed the meeting?.
The complete TimeML specification assumes the
temporal interval relations as defined by Allen
(Allen, 1983) in Figure 1.
113
A 
B A EQUALS B 
A 
B A is BEFORE B;  B is AFTER A 
A 
B A MEETS B;  B is MET BY A 
A 
B A OVERLAPS B;  B is OVERLAPPED BY A 
A 
B A STARTS B;  B is STARTED BY A 
A 
B A FINISHES B;  B is FINISHED BY A 
A 
B A is DURING B;  B CONTAINS A 
Figure 1: Allen Relations
For this task, however, we assume a reduced sub-
set, as introduced in TempEval-1: BEFORE, AFTER,
OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-
AFTER and VAGUE. However, we are investigat-
ing whether for some tasks the more precise set of
TimeML relations could be used.
Task participants may choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants may choose one
or more of the five languages for which we provide
data: English, Italian, Chinese, Spanish, and Ko-
rean.
3.1 Extent of Time Expression
This task involves identification of the EXTENT,
TYPE, and VAL of temporal expressions in the text.
Times can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following:
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80?s
e. Later this afternoon
f. yesterday
The TYPE of the temporal extent must be identified.
There are four temporal types that will be distin-
guished for this task;
(2) a. Time: at 2:45 p.m.
b. Date: January 27, 1920, yesterday
c. Duration two weeks
d. Set: every Monday morning
The VAL attribute will assume values according to
an extension of the ISO 8601 standard, as enhanced
by TIMEX2.
(3) November 22, 2004
<TIMEX3 tid="t1" type="DATE"
value="2004-11-22"/>
3.2 Extent of Event Expression
The EVENT tag is used to annotate those elements in
a text that describe what is conventionally referred to
as an eventuality. Syntactically, events are typically
expressed as inflected verbs, although event nomi-
nals, such as ?crash? in killed by the crash, should
also be annotated as EVENTs.
In this task, event extents must be identified and
tagged with EVENT, along with values for the fea-
tures TENSE, ASPECT, POLARITY, and MODALITY.
Examples of these features are shown below:
(4) should have bought
<EVENT id="e1" pred="BUY" pos="VERB"
tense="PAST" aspect="PERFECTIVE"
modality="SHOULD" polarity="POS"/>
(5) did not teach
<EVENT id="e2" pred="TEACH" pos="VERB"
tense="PAST" aspect="NONE"
modality="NONE" polarity="NEG"/>
The specifics on the definition of event extent
will follow the published TimeML guideline (cf.
timeml.org).
3.3 Within-sentence Event-Time Anchoring
This task involves determining the temporal relation
between an event and a time expression in the same
sentence. This was present in TempEval-1, but here,
in TempEval-2, this problem is further restricted by
requiring that the event either syntactically domi-
nates the time expression or the event and time ex-
pression occur in the same noun phrase. For exam-
ple, the following constructions will be targeted for
temporal labeling:
114
(6) Mary taughte1 on Tuesday morningt1
OVERLAP(e1,t1)
(7) They cancelled the eveningt2 classe2
OVERLAP(e2,t2)
3.4 Neighboring Sentence Event-Event
Ordering
In this task, the goal is to identify the temporal re-
lation between two main events in consecutive sen-
tences. This task was covered in the previous com-
petition, and includes pairs such as that shown be-
low:
(8) The President spokee1 to the nation on Tuesday
on the financial crisis. He had conferrede2 with
his cabinet regarding policy the day before.
AFTER(e1,e2)
3.5 Sentence Event-DCT Ordering
This task was also included in TempEval-1 and re-
quires the identification of the temporal order be-
tween the matrix event of the sentence and the Docu-
ment Creation Time (DCT) of the article or text. For
example, the text fragment below specifies a fixed
DCT, relative to which matrix events from the two
sentences are ordered:
(9) DCT: MARCH 5, 2009
a. Most troops will leavee1 Iraq by August of
2010. AFTER(e1,dct)
b. The country defaultede2 on debts for that
entire year. BEFORE(e2,dct)
3.6 Within-sentence Event-Event Ordering
The final task involves identifying the temporal re-
lation between two events, where one event syntac-
tically dominates the other event. This includes ex-
amples such as those illustrated below.
(10) The students hearde1 a fire alarme2.
OVERLAP(e1,e2)
(11) He saide1 they had postponede2 the meeting.
AFTER(e1,e2)
4 Resources and Evaluation
4.1 Data
The development corpus will contain the following
data:
1. Sentence boundaries;
2. The document creation time (DCT) for each
document;
3. All temporal expressions in accordance with
the TimeML TIMEX3 tag;
4. All events in accordance with the TimeML
EVENT tag;
5. Main event markers for each sentence;
6. All temporal relations defined by tasks C
through F.
The data for the five languages are being prepared
independently of each other. We do not provide a
parallel corpus. However, annotation specifications
and guidelines for the five languages will be devel-
oped in conjunction with one other. For some lan-
guages, we may not use all four temporal linking
tasks. Data preparation is currently underway for
English and will start soon for the other languages.
Obviously, data preparation is a large task. For En-
glish and Chinese, the data are being developed at
Brandeis University under three existing grants.
For evaluation data, we will provide two data sets,
each consisting of different documents. DataSet1 is
for tasks A and B and will contain data item 1 and 2
from the list above. DataSet2 is for tasks C though
F and will contain data items 1 through 5.
4.2 Data Preparation
For all languages, annotation guidelines are defined
for all tasks, based on version 1.2.1 of the TimeML
annotation guidelines for English3. The most no-
table changes relative to the previous TimeML
guidelines are the following:
? The guidelines are not all presented in one doc-
ument, but are split up according to the seven
TempEval-2 tasks. Full temporal annotation
has proven to be a very complex task, split-
ting it into subtasks with separate guidelines for
3See http://www.timeml.org.
115
each task has proven to make temporal annota-
tion more manageable.
? It is not required that all tasks for temporal link-
ing (tasks C through F) use the same relation
set. One of the goals during the data prepara-
tion phase is to determine what kind of relation
set makes sense for each individual task.
? The guidelines can be different depending on
the language. This is obviously required be-
cause time expressions, events, and relations
are expressed differently across languages.
Annotation proceeds in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where a
judge resolves disagreements between the annota-
tors. We are expanding the annotation tool used for
TempEval-1, making sure that we can quickly an-
notate data for all tasks while making it easy for a
language to define an annotation task in a slightly
different way from another language. The Brandeis
Annotation Tool (BAT) is a generic web-based anno-
tation tool that is centered around the notion of an-
notation tasks. With the task decomposition allowed
by BAT, it is possible to flexibly structure the com-
plex task of temporal annotation by splitting it up in
as many sub tasks as seems useful. As such, BAT is
well-suited for TempEval-2 annotation. Comparison
of annotation speed with tools that do not allow task
decomposition showed that annotation with BAT is
up to ten times faster. Annotation has started for
Italian and English.
For all tasks, precision and recall are used as eval-
uation metrics. A scoring program will be supplied
for participants.
5 Conclusion
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations between
events and temporal expressions in text. Using
a subset of TimeML temporal relations, we show
how temporal relations and anchorings can be an-
notated and identified in five different languages.
The markup language adopted presents a descrip-
tive framework with which to examine the tempo-
ral aspects of natural language information, demon-
strating in particular, how tense and temporal infor-
mation is encoded in specific sentences, and how
temporal relations are encoded between events and
temporal expressions. This work paves the way to-
wards establishing a broad and open standard meta-
data markup language for natural language texts, ex-
amining events, temporal expressions, and their or-
derings.
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. 2007. Timebank evolution as a community
resource for timeml parsing. Language Resource and
Evaluation, 41(1):91?115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics,
March.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proc. of the Fourth Int. Work-
shop on Semantic Evaluations (SemEval-2007), pages
75?80, Prague, Czech Republic, June. Association for
Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The tempeval challenge: identifying
temporal relations in text. Language Resources and
Evaluation.
116
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 178?181,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
The SILT and FlaReNet International Collaboration for Interoperability
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
James Pustejovsky
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
jamesp@cs.brandeis.edu
Nicoletta Calzolari
CNR-ILC
Pisa, Italy
glottolo@ilc.cnr.it
Claudia Soria
CNR-ILC
Pisa, Italy
claudia.soria@ilc.cnr.it
Abstract
Two major projects in the U.S. and Eu-
rope have joined in a collaboration to work
toward achieving interoperability among
language resources. In the U.S., a project
entitled ?Sustainable Interoperability for
Language Technology? (SILT) has been
funded by the National Science Founda-
tion under the INTEROP program, and in
Europe, FLaReNet Fostering Language
Resources Network has been funded
by the European Commission under the
eContentPlus framework. This interna-
tional collaborative effort involves mem-
bers of the language processing commu-
nity and others working in related ar-
eas to build consensus regarding the shar-
ing of data and technologies for language
resources and applications, to work to-
wards interoperability of existing data,
and, where possible, to promote standards
for annotation and resource building. In
addition to broad-based US and European
participation, we are seeking the partici-
pation of colleagues in Asia. This pre-
sentation describing the projects and their
goals will, we hope, serve to involve mem-
bers of the community who may not have
been aware of the effort before, in particu-
lar colleagues in Asia.
1 Overview
One of today?s greatest challenges is the develop-
ment of language processing capabilities that will
enable easy and natural access to computing facil-
ities and information. Because natural language
processing (NLP) research relies heavily on such
resources to provide training data to develop lan-
guage models and optimize statistical algorithms,
language resources?including (usually large) col-
lections of language data and linguistic descrip-
tions in machine readable form, together with
tools and systems (lemmatizers, parsers, summa-
rizers, information extractors, speech recognizers,
annotation development software, etc.)? are criti-
cal to this development.
Over the past two decades, the NLP commu-
nity has invested substantial effort in the creation
of computational lexicons and compendia of se-
mantic information (e.g., framenets, ontologies,
knowledge bases) together with language corpora
annotated for all varieties of linguistic features,
which comprise the central resource for current
NLP research. However, the lack of a thorough,
well-articulated longer-term vision for language
processing research has engendered the creation of
a disjointed set of language resources and tools,
which exist in a wide variety of (often incom-
patible) formats, are often unusable with systems
other than those for which they were developed,
and utilize linguistic categories derived from dif-
ferent theoretical frameworks. Furthermore, these
expensive investments are often produced only for
one of several relatively isolated subfields (e.g.,
NLP, information retrieval, machine translation,
speech processing), or even worse, for one appli-
cation in one subfield. In addition, the high cost of
resource development has prevented the creation
of reliable, large-scale language data and anno-
tations for many phenomena, and for languages
other than English.
Interoperability of resources, tools, and frame-
works has recently come to be recognized as per-
haps the most pressing current need for language
processing research. Interoperability is especially
178
critical at this time because of the widely recog-
nized need to create and merge annotations and
information at different linguistic levels in order
to study interactions and interleave processing at
these different levels. It has also become criti-
cal because new data and tools for emerging and
strategic languages such as Chinese and Arabic as
well as minor languages are in the early stages of
development.
Two major projects in the U.S. and Europe have
joined in a collaboration to work toward achiev-
ing interoperability among language resources. In
the U.S., a project entitled ?Sustainable Interoper-
ability for Language Technology? (SILT) has been
funded by the National Science Foundation under
the INTEROP program, and in Europe, FLaReNet
Fostering Language Resources Network has been
funded by the European Commission under the
eContentPlus framework. This international col-
laborative effort involves members of the lan-
guage processing community and others working
in related areas to build consensus regarding the
sharing of data and technologies for language re-
sources and applications, to work towards inter-
operability of existing data, and, where possible,
to promote standards for annotation and resource
building. In addition to broad-based US and Eu-
ropean participation, we are seeking the participa-
tion of colleagues in Asia.
To ensure full community involvement and con-
solidation of effort, SILT and FLaReNet are estab-
lishing ties with major ongoing projects and con-
sortia, including the International Standards Orga-
nization TC37 SC4 (Language Resource Manage-
ment)1, The World Wide Web Consortium (W3C),
the Text Encoding Initiative, the ACL Special In-
terest Group on Annotation (SIGANN)2, and oth-
ers. The ultimate goal is to create an Open Lan-
guage Infrastructure (OLI) that will provide free
and open access to resources, tools, and other in-
formation that support work in the field, in order to
facilitate collaboration, accessibility for all mem-
bers of the community, and convergence toward
interoperability.
The following sections outline the goals of SILT
and FLaReNet.
1http://www.tc37sc4.org
2http://www.cs.vassar.edu/sigann
2 SILT
The creation and use of language resources spans
several related but relatively isolated disciplines,
including NLP, information retrieval, machine
translation, speech, and the semantic web. SILT?s
goal is to turn existing, fragmented technology and
resources developed within these groups in rela-
tive isolation into accessible, stable, and interop-
erable resources that can be readily reused across
several fields.
The major activities of the effort are:
? carefully surveying the field to identify the
resources, tools, and frameworks in order to
examine what exists and what needs to be
developed, and to identify those areas for
which interoperability would have the broad-
est impact in advancing research and devel-
opment and significant applications depen-
dent on them;
? identifying the major efforts on standards de-
velopment and interoperable system design
together with existing and developing tech-
nologies, and examining ways to leverage
their results to define an interoperablity in-
frastructure for both tools and data;
? analyzing innovative methods and techniques
for the creation and maintenance of language
resources in order to reduce the high costs,
increase productivity, and enable rapid devel-
opment of resources for languages that cur-
rently lack them;
? implementing proposed annotation standards
and best practices in corpora currently under
development (e.g., American National Cor-
pus3, TimeBank4) to evaluate their viability
and feed into the process of further standards
development, testing, and use of interoper-
ability frameworks (e.g., GATE5, UIMA6)
and implementation of processing modules,
and distributing all software, data, and anno-
tations.
? ensuring the broadest possible community
engagement in the development of consensus
and agreement on strategies, priorities, and
3http://www.anc.org
4http://www.timeml.org/site/timebank/timebank.html
5http://gate.ac.uk
6http://www.oasis-open.org/committees/uima/
179
best approaches for achieving broad interop-
erability by means of sessions, open meet-
ings, and special workshops at major confer-
ences in the field, together with active main-
tenance of and involvement in open web fo-
rums and Wikis;
? providing the technical expertise necessary to
turn consensus and agreement into robust in-
teroperability frameworks along with the ap-
propriate tools and resources for their broad
use and implementation by means of tutorials
and training workshops, especially for under-
graduate and graduate students in the field.
3 FLaReNet
The multilingual Europe urgently needs language
technologies in order to bridge its language bar-
riers. In order to achieve better quality and fast
development of language technologies that seam-
lessly work on all devices, for spoken and written
language alike, the European scenario now needs a
coherent and unified effort. The demand for cross-
lingual technologies is pressing, the expectations
are high, and at the same time, the field is suf-
fering from fragmentation, lack of vision and di-
rection. The main objective of FLaReNet is to
steer the process that in the near future will de-
fine the actors, the overall direction and the prac-
tical forms of collaboration in language technolo-
gies and their ?raw material?, language resources.
Under this respect, the goals of FLaReNet lie at
a higher level than those of SILT, as they are ori-
ented towards consolidating a community around
a number of key topics that, in the end, will allow
networking of language technology professionals
and their clients, as well as easy sharing of data,
corpora, language resources and tools.
From this perspective, FLaReNet has three
main lines of action:
The creation and mobilization of a unified
and committed community in the field of Lan-
guage Resources and Technologies. To this end,
FLaReNet is bringing together leading experts of
research institutions, academies, companies, fund-
ing agencies, public and private bodies, both at
European and international level, with the spe-
cific purpose of creating consensus around short,
medium and long-term strategic objectives. The
Network is currently composed of around 200 in-
dividuals belonging to academia, research insti-
tutes, industries and government.
The identification of a set of priority themes
on which to stimulate action, under the form of a
roadmap for Language Resources and Technolo-
gies. In order to avoid scattered or conflicting ef-
forts, the major players in the field of Language
Resources and Technologies need to consensually
work together and indicate a clear direction of ac-
tion and a shared policy for the next years. This
will take the form of identification of priorities of
intervention as well as short, medium, and long-
term strategic objectives at all levels, from re-
search directions to implementation choices, from
distribution and access policies to the landscape
of languages, domain and modalities covered by
Language Resources and Technologies.
The elaboration of a blueprint of priority ar-
eas for actions in the field and a coherent set of
recommendations for the policy-makers (funding
agencies especially), the business community and
the public at large. Whatever action cannot be im-
plemented on a long term without the help of the
necessary financial and political framework to sus-
tain them. This is even most true for actions re-
garding Language Resources that typically imply
a sustained effort at national level. To this end,
the FLaReNet Network will propose the priority
themes under the form of consensual recommen-
dations and a plan of action for EC Member States,
other European-wide decision makers, companies,
as well as non-EU and International organizations.
The following Thematic Areas are currently
covered by FLaReNet:
? The Chart for the area of LRs and LT in its
different dimensions
? Methods and models for LR building, reuse,
interlinking, and maintenance
? Harmonisation of formats and standards
? Definition of evaluation and validation proto-
cols and procedures
? Methods for the automatic construction and
processing of Language Resources
FLaReNet builds upon years of research and de-
velopment in the field of standards and language
resources, as well as on the achievements (both
in terms of results and community awareness),
of past EU projects such as EAGLES7, ISLE8,
7http://www.ilc.cnr.it/EAGLES/home.html
8http://www.ilc.cnr.it/EAGLES/isle/ISLE Home Page.htm
180
INTERA9, and LIRICS10. Close collaboration is
also established with many relevant ongoing EU
projects, such as CLARIN11.
9http://www.elda.org/intera
10http://lirics.loria.fr/
11http://www.clarin.eu
181
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 6
                                                              Proceedings of HLT-NAACL
Are You Sure That This Happened?
Assessing the Factuality Degree of
Events in Text
Roser Saur???
Barcelona Media - Innovation Center
James Pustejovsky??
Brandeis University
Identifying the veracity, or factuality, of event mentions in text is fundamental for reasoning
about eventualities in discourse. Inferences derived from events judged as not having happened,
or as being only possible, are different from those derived from events evaluated as factual. Event
factuality involves two separate levels of information. On the one hand, it deals with polarity,
which distinguishes between positive and negative instantiations of events. On the other, it
has to do with degrees of certainty (e.g., possible, probable), an information level generally
subsumed under the category of epistemic modality. This article aims at contributing to a better
understanding of how event factuality is articulated in natural language. For that purpose,
we put forward a linguistic-oriented computational model which has at its core an algorithm
articulating the effect of factuality relations across levels of syntactic embedding. As a proof
of concept, this model has been implemented in De Facto, a factuality profiler for eventualities
mentioned in text, and tested against a corpus built specifically for the task, yielding an F1 of
0.70 (macro-averaging) and 0.80 (micro-averaging). These two measures mutually compensate
for an over-emphasis present in the other (either on the lesser or greater populated categories),
and can therefore be interpreted as the lower and upper bounds of the De Facto?s performance.
1. Introduction
When we talk about situations in the world, we often leave pieces of information vague
or try to complete the story with approximations, either because we do not know all
the details or we are not sure about what we know. With a lesser or greater degree, this
vagueness is pervasive in all types of accounts, regardless of the topic and the degree of
proximity of the speaker with the facts being reported: our last family gathering, what
we read about the tsunami and its aftermath in Japan, our perspective on a particular
topic, or how we feel today. Even in scientific discourse, findings tend to be expressed
with degrees of cautiousness.
? Voice and Language Group, Barcelona Media - Innovation Center, Diagonal 177, 08018 Barcelona,
Catalonia. E-mail: roser.sauri@barcelonamedia.org.
?? Computer Science Department, Brandeis University, 415 South Street, Waltham MA 02454, USA.
E-mail: jamesp@cs.brandeis.edu.
Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication:
30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
The linguistic mechanisms for coping with the vagueness and fuzziness in our
knowledge are commonly referred to as speculative language. This involves different
levels of grammatical manifestation, most significantly quantification over entities and
events, modality, and hedging devices of a varied nature. We can be vague or approx-
imate with the temporal and spatial references of situations in the world, when quan-
tifying the frequency of usual events, assessing the number of participants involved,
and describing or adscribing them into a class. We also qualify our statements with
approximative language when giving an opinion, or when we are not certain about the
degree of veracity of what we are telling.
The present article focuses on a particular kind of speculation in language, specif-
ically, that concerning the factuality status of eventualities mentioned in discourse.
Whenever we talk about situations, we express our degree of certainty about their
factual status. We can characterize them as an unquestionable fact, or qualify them with
some degree of uncertainty if we are not sure whether the situation holds, or will hold,
in the world.
Identifying the factuality status of event mentions is fundamental for reasoning
about eventualities in discourse. Inferences derived from events judged as not having
happened, or as being only possible, are different from those derived from events
evaluated as factual. Event factuality is also essential for any task involving temporal
ordering, because the plotting of event mentions into a timeline requires different
actions depending on their veracity. Karttunen and Zaenen (2005) discuss its relevance
for information extraction, and in the area of textual entailment, factuality-related infor-
mation (modality, intensional contexts, etc.) has been taken as a basic feature in some
systems participating in the PASCAL RTE challenges (e.g., Hickl and Bensley 2007).
The need for this type of information is also acknowledged in the annotation schemes
of corpora devoted to event information, such as the ACE corpus for the Event and
Relation recognition task (e.g., ACE 2008), or TimeBank, a corpus annotated with event
and temporal information (Pustejovsky et al 2006).
Significantly, in the past few years this level of information has been at the focus of
much research within the NLP area dedicated to the biomedical domain. Distinguishing
betweenwhat is reported as a fact versus a possibility in experiment reports or in patient
health records is a crucial capability for any robust information extraction tool operating
on that domain. This interest has resulted in the compilation of domain-specific corpora
devoted particularly to that level of information, such as BioScope (Vincze et al 2008),
and others that include event factivity as a further attribute in the annotation of biomed-
ical events, such as GENIA (Kim, Ohta, and Tsujii 2008). Furthermore, factuality-related
information was the main focus in the CoNLL-2010 shared task on Learning to Detect
Hedges and their Scope in Natural Language Text (Farkas et al 2010), and the topic
in a subtask of the BioNLP?09 and BioNLP?11 shared task editions on Event Extraction
(Kim et al 2009),1 dedicated to predict whether the biological event is under negation
or speculation.
The overall goal of this article is to contribute to a better understanding of this
particular aspect of speculation. We analyze all the ingredients involved in comput-
ing the factuality nature of event mentions in text, and put forward a computational
model based on that. As a proof of concept, the model is implemented into De Facto,
a factuality profiler, and its performance tested against FactBank, a corpus annotated
1 For the 2011 edition, refer to: https://sites.google.com/site/bionlpst/.
262
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
with factuality information built specifically for the task and currently available to the
community through the Linguistic Data Consortium (Saur?? and Pustejovsky 2009a).
The article begins by defining event factuality and its place in speculative language
(Section 2). The basic components for the model on event factuality are presented in Sec-
tion 3, and the algorithm integrating these is introduced in Section 4. Section 5 reports
on the experiment resulting from implementing the proposed model into De Facto, and
Section 6 relates the present work to other research in the field.
2. Event Factuality and Speculative Language
2.1 Defining Event Factuality
Event factuality (or factivity) is understood here as the level of information expressing
the factual nature of eventualities mentioned in text. That is, expressing whether they
correspond to a fact in the world (Example (1a)), a possibility (Examples 1b, 1c), or a
situation that does not hold (Example 1d), as is the case with the events denoted by the
following underlined expressions:2
(1) a. Har-Shefi regretted calling the prime minister a traitor.
b. Results indicate that Pb2+ may inhibit neurite initiation.
c. Noah?s flood may have not been as biblical in proportion as previously
thought.
d. Albert Einstein did not win a Nobel prize for his theories of Relativity.
The fact that an eventuality is depicted as holding or not does not mean that this is
the case in the world, but that this is how it is characterized by its informant. Similarly,
it does not mean that this is the real knowledge that informant has (his true cognitive
state regarding that event) but what he wants us to believe it is.
Event factuality rests upon distinctions along two different parameters: the notions
of certainty (what is certain vs. what is only possible) and polarity (positive vs. nega-
tive). In some contexts, the factual status of events is presented with absolute certainty.
Then, depending on the polarity, events are depicted as either situations that have taken
or will take place in the world (here referred to as facts; see Example (1a)), or situations
that do not hold in the world (here called counterfacts; see Example (1d)). In other
contexts, events are qualified with different shades of uncertainty. Combining that with
polarity, events are seen as possibly factual (Example (1b)) or possibly counterfactual
(Example (1c)).3
2 In this article, the terms event and eventuality will be used in a very broad sense to refer to both processes
and states, but also other abstract objects such as situations, propositions, facts, possibilities, and so on.
Furthermore, events in the examples will be identified by marking only their verb, noun, or adjective
head, together with their modal and negation particles when deemed necessary. This follows the
convention assumed in TimeML, a specification language for representing event and time information
(Pustejovsky et al 2005).
3 The term counterfactual has a long tradition in philosophy of language and linguistics, where it refers to
conditional (or if?then) statements expressing what would be the case if their antecedent was true,
although it is not. For example: If Gandhi had survived the fatal gun attack, he would have continued working
for a better world. Here, however, we extend its use to refer to negated events in general. One can argue
that negated events are facts as well. For example, it is a fact that Gandhi did not survive the fatal gun
attack. The term counterfactmust be understood here as negative fact.
263
Computational Linguistics Volume 38, Number 2
Factuality is expressed through a complex interaction of many different aspects of
the overall linguistic expression. It involves explicit polarity and modality markers,
but also lexical items, morphological elements, syntactic constructions, and discourse
relations between clauses or sentences.
Polarity particles, which convey the positive or negative factuality of events, in-
clude elements of a varied nature: adverbs (not, neither, never), determiners (no, non),
pronouns (none, nobody), and so forth. At another level, modality particles contribute
different degrees of certainty. In English, they can be realized as verbal auxiliaries
(must, may), adverbials (probably, presumably), and adjectives (likely, possible). All these
categories display an equivalent gradation of modality (Givo?n 1993).
In many cases, the factuality of events is conveyed by what we refer to as event-
selecting predicates (ESPs), that is, predicates (either verbs, nouns, or adjectives) that
select for an argument denoting an event of some sort. ESPs are of interest here because
they qualify the degree of factuality of their embedded event, which can be presented
as a fact in the world (Example (2)), a counterfact (Example (3)), or a possibility (Ex-
ample (4)). In these examples, the ESPs are in boldface and their embedded events are
underlined.
(2) a. Some of the Panamaniansmanaged [to escape with their weapons].
b. The defendant knew that [he had been in possession of narcotics].
(3) a. 1,200 voters were prevented from [casting ballots on election night].
b. The manager avoided [returning the phone calls].
(4) a. I think [they voted last weekend].
b. Hawking speculated that [most extraterrestrial life would be similar
to microbes].
Absolute factuality is conveyed by ESPs belonging to classes fairly well studied
in the literature, such as: implicative (Example (2a)) (Karttunen 1970); factive (Exam-
ple (2b)) (Kiparsky and Kiparsky 1970); perception (e.g., see a car explode); aspectual
(e.g., finish reading), and change-of-state predicates (e.g., increase its exports). Coun-
terfactuality is brought about by other implicative predicates, like avoid and prevent
(Example (3)) (Karttunen 1970), whereas predicates such as think, speculate, and suspect
qualify their complements as not totally certain (Example (4)) (Hooper 1975; Bach and
Harnish 1979; Dor 1995). The group of ESPs that leave the factuality of their event
complement underspecified is also significant. The event is mentioned in discourse,
but no information is provided concerning its factual status. Several predicate classes
create this effect, for example: volition (e.g., want, wish, hope), commitment (commit,
offer, propose), and inclination predicates (willing, ready, eager, reluctant), among others
(cf. Asher 1993).
Other information at play is evidentiality (e.g., a seen event is presented with a
factuality degree stronger than that of an event reported by someone else), and mood
(e.g., indicative vs. subjunctive). Factuality information is also introduced by certain
syntactic constructions involving subordination. In some cases, the embedded event is
presupposed as a fact, as in non-restrictive relative clauses (Example (5a)) or participial
clauses (Example (5b)). In others, like purpose clauses, the event is intensional and thus
presented as underspecified (Example (5c)).
264
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
(5) a. Obama, [who took office in January], inherited a budget deficit of $1.3 trillion.
b. [Having revolutionized linguistics], Chomsky moved to political activism.
c. Stronach resigned as CEO of Magna [to seek a seat in Canada?s Parliament].
Finally, a further means for conveying factuality information is available at the
discourse level. Some events may first have their factual status characterized in one
way, but then be presented differently in a subsequent sentence.
2.2 Notions Connected to Event Factuality
Event factuality results from the interaction between polarity and certainty. Here we
review the connections of these two notions with other ones in the study of language.
Certainty. The axis of certainty is related to epistemic modality, a category dealing with
the degree of certainty of situations in the world. Epistemic modality has been studied
from both the logical and linguistic traditions. Within linguistics, authors from differ-
ent traditions converge in analyzing modality as a subjective component of discourse
(e.g., Lyons 1977; Chafe 1986; Palmer 1986; Kiefer 1987), a view that is adopted in the
present analysis.4 Traditionally, the study of epistemic modality in linguistics has been
confined to modal auxiliaries (e.g., Palmer 1986), but more recently a wider view has
been adopted which includes other parts of speech as well, such as epistemic adverbs,
adjectives, nouns, and lexical verbs (e.g., Rizomilioti 2006).
In a more secondary way, the axis of certainty is also related to the system of
evidentiality, concerned with the way in which information about situations in the
world is acquired, such as directly experienced, witnessed, heard-about, inferred, and
so on (van Valin and LaPolla 1997; Aikhenvald 2004). Different types of evidence have
an effect on the way the factuality of an event is evaluated. For instance, something
reported as seen can more easily be assessed as a fact than something reported as
inferred.
Certainty touches as well on the notion of epistemic stance, developed from a
more cognitivist perspective and which is defined as the pragmatic relation between
speakers and their knowledge regarding the things they talk about (Biber and Finegan
1989; Mushin 2001). Similarly, within Systemic Functional Linguistics, the Appraisal
Framework develops a taxonomy of the mechanisms employed for expressing sub-
jective information such as attitude, its polarity, graduation, and so forth (Martin and
White 2005).
Within NLP, most work on uncertainty and speculative information has been ap-
proached from a hedging-based perspective. The notion of hedging is initially defined
by Lakoff (1973, page 471) as ?words whose job is making things fuzzier or less fuzzy.?
In particular, he uses this term to analyze linguistic constructions that express degrees
of the is a relationship (e.g., is a sort of, in essence/strickly speaking... is...). Due to the
fuzziness aspect of hedges, subsequent work extends the notion to include expres-
sions for qualifying the degree of commitment of the writer with respect to what is
asserted (Hyland [1996], among others). By this definition, hedging and event factuality
seem to be overlapping concepts. They differ on the extent of the phenomena they
4 This is different, however, from most of the work within truth-conditional semantics, which conceives
modality as independent from the speaker?s perspective (e.g., Kratzer 1991).
265
Computational Linguistics Volume 38, Number 2
each cover, however. First, hedging is confined only to partial degrees of uncertainty,
whereas factuality includes also the levels of absolute certainty. Second, in addition
to degrees of writer?s commitment towards the veridicity of her statements, hedging
(but not factuality) encompasses speculative expressions belonging to other scales, most
significantly, expressions of usuality (to quantify the frequency of events: often, barely,
tends to, etc.), expressions of category membership (i.e., is a downgraders, such as is a
sort of, presented by Lakoff [1973]), as well as lack of knowledge (e.g, little is known).
Polarity. The second axis configuring event factuality is the system of polarity, so called
because it articulates the polar opposition between positive and negative contexts.
Due to its recent adoption in the NLP area of sentiment analysis, the term polarity is
often taken to express only the direction of an opinion. Here, we use the term in its
original grammatical sense, that is, as conveying the distinction between affirmative and
negative contexts (e.g., Horn 1989). Beingmore abstract, this definition encompasses the
different facets of the positive/negative opposition, and not only the one that is relevant
in opinion mining.
2.3 Key Elements in the Factuality System
Identifying event factuality in text poses challenges at different levels of analysis. We
explore them in the current section.
A scale of factuality degrees. Concerning distinctions at the level of both polarity and
certainty (or modality, as is more commonly referred to within linguistics), the factuality
of events can be characterized as a double-axis scale. Figure 1 illustrates the system.
The axis of polarity defines a binary distinction (positive vs. negative), and the
axis of modality conveys certainty as a continuous scale that ranges from truly certain
to completely uncertain, passing through a whole spectrum of shades that languages
accommodate in different ways, depending on the grammatical resources they have
available. For example, assuming only a limited number of words in English, one
can create the following distinctions: improbable, slightly possible, possible, fairly possible,
probable, very probable, most probable, most certain, certain.
Figure 1
The double range of factuality.
266
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
This continuum poses a challenge in the setting of a model of factuality with
potential cross-linguistic validity. Many linguists agree, however, that speakers are able
to map areas of the modality axis into discrete values (Lyons 1977; Horn 1989; de
Haan 1997). The goal is therefore identifying the factuality distinctions that reflect our
linguistic intuitions as speakers, and that can also help define a set of sound and stable
criteria for differentiating among them. The factual value of markers such as possibly
and probably is fairly transparent. What, however, is the contribution of elements like
think, predict, suggest, or seem?
Interactions among factuality markers. The factuality status of a given event cannot be
determined from the strictly local modality and polarity operators scoping over that
event alone; rather, if present, other non-local markers must be considered as well to
obtain the adequate interpretation. Consider:
(6) a. Several EUmember states will continue to allow passengers to carry duty-free
drinks in hand luggage.
b. Several EU member states will continue to refuse to allow passengers to carry
duty-free drinks in hand luggage.
c. Several EU member states may refuse to allow passengers to carry duty-free
drinks in hand luggage.5
In all three examples above the event carry is directly embedded under the verb
allow, but receives a different interpretation depending on the elements scoping over
that. In Example (6a), where allow is embedded under the factive predicate continue,
carry is characterized as a fact in the world. Example (6b), on the other hand, depicts it as
a counterfact because of the effect of the predicate refuse scoping over allow, and finally,
Example (6c) presents it as uncertain due to the modal auxiliary may qualifying refuse.6
Any treatment aiming at adequately handling the contents of sentences like these
needs to incorporate the notion of scope in its model, but scope is not enough. As these
data show, the factuality value of an event does not depend on the element immediately
scoping over it. Neither does it rely on the meaning resulting from some sort of additive
(or concatenative) operation among all the markers. In Example (6b), for example, two
of the factuality markers that include the event carry in their scope (continue and refuse)
typically mark contradictory information. The first one presupposes the factuality of the
event it scopes over, and the second negates it. Which should be the resulting factuality
value for carry if only scope information is used?
Factuality as a property qualifying events and not the whole sentence. Factuality is a property
that qualifies the nature of events, hence operating at a level of units smaller than
sentences. Frequently sentences express more than one event (or proposition), each of
them qualified with a different degree of certainty. Consider Example (7),7 where the
main event have an easier time (e3) is depicted as a possibility in the world, event crossover
5 The original sentence in this set is (6b) (http://www.irishtimes.com/newspaper/ireland/2011/
0502/1224295867753.html). The other two have been adapted for the argument?s sake.
6 The verb allow is generally used as a two-way implicative predicate, that is, as a predicate that holds a
direct relation between its truth (or falsity) and that of its embedded event (Karttunen 1970).
7 Extracted from Rubin (2006, page 59).
267
Computational Linguistics Volume 38, Number 2
voting being barred (e2) is asserted as a fact, and event crossover voting (e1) is uncertain?
that is, the fact that it is barred does not mean that it does not take place.
(7) In future primaries, where crossover votinge1 is barrede2 , Bush may well havee3
an easier time.
Facts and their sources. Certain event components, such as the temporal reference or the
participants taking part in it, are inherent elements of any given event. For example,
the visit to the zoo with Max in April, Ivet in August, and Arlet in December are three
separate events, given the difference in participants and temporal location. By contrast,
factuality is a matter of perspective. Different sources can have divergent views about
the factuality of the very same event. Recognizing this is crucial for any task involving
text entailment. Event e in Example (8), for instance (i.e., Ruby being the niece of the
Egyptian president), will be inferred as a fact in the world if it cannot be qualified as
having been asserted by a specific source, here Berlusconi (underlined).
(8) Berlusconi said that Rubywase the niece of Egyptian President Hosni Mubarak.
By default, events mentioned in discourse always have an implicit source, namely,
the author of the text. Additional sources are introduced in discourse by means of ESPs
such as say or pretend:
(9) Nelles saide1 that Germany has been pretendinge2 for long that nuclear power
is safee3 .
In some cases, the different sources relevant for a given event may coincide with
respect to its factual status, but in others they may be in disagreement. In Example (9),
for instance, event e3 (nuclear power being safe) is assessed as a fact according to Germany
but as a counterfact according to Nelles, whereas the text author remains uncommitted.
The time variable. It is not only the case that two participants can present different
views about the same event, but also that the same (or different) participant presents
a diverging view at different points in time. Consider:
(10) a. In mid-2001, Colin Powell and Condoleezza Rice both publicly denied that Iraq
had weapons of mass destruction.
b. Secretary of State Colin Powell Thursday defended the Bush administration?s
position that Iraq had weapons of mass destruction. (CNN, 8 January 2004)
A model of event factuality needs therefore to be sensitive to the distinctions in per-
spective brought about by sources and temporal references. Only under this assumption
is it possible to account for the potential divergence of opinions on the factual status of
events, as is common in news reports.
3. Towards a Model for Event Factuality
Having identified themain aspects involved in event factuality, we explore the interplay
among these elements, and subsequently build a model that can explain these interac-
tions. Based on the structure of linguistic expressions, this model will assume an event-
268
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
centered approach in order to tackle the factuality nature of each event independently
of the others mentioned in the same sentence. Factuality distinctions are established
at a fine-grained level, and multiple perspectives on the same event are accounted for
by means of the notion of source as a participant introduced by predicates of report,
knowledge, belief, and so on. We begin by introducing the notion of a factuality profile
(Section 3.1), and then formalize the basic components that have a role in it, namely:
factuality values (Section 3.2), sources (Section 3.3), and factuality markers (Section 3.4).
The algorithm putting all these ingredients together will be presented in Section 4.
3.1 The Factuality Profile of Events
Whenever speakers talk about events, they qualify them with a degree of factuality.
Here, we refer to this act of assigning a factuality value to a given event performed
by a particular source at a specific point in time as a factuality commitment act. This
involves four components:
 The event in focus, e.
 The factuality value assigned to that event, f, which touches on both
polarity and epistemic modality distinctions as encoded in factuality
markers.
 The source assigning the factuality value to that event, s.
 The time when the factuality value assignment takes place, t.
For instance, in Example (9) Germany is presented as defending that nuclear power
is safe (event e3). This corresponds to the factuality commitment act that assesses event
e3 as a fact in the world, performed by source Germany at an underspecified point in
time t1.
Given that events in discourse can be evaluated by more than one source and at
several points in time, the factuality of each event can be characterized through more
than one factuality commitment act. We define the set of factuality commitment acts
associated to an event as its factuality profile. Formally, the factuality profile of a given
event e, pe, can be represented as follows:
pe = {?s, t, f ? | s is a relevant source for event e & t is a point in time & f is the factuality
value assigned by source s to event e at point in time t} (1)
Using example (9) again, the factuality profile of event e3 (nuclear power being safe)
contains three factuality commitment acts: one by source Germany, who commits to the
veradicity of the event, another for source Nelles, who disagrees, and finally another for
the author, who keeps an underspecified position.
pe3 = {?germany, t1, fact?, ?nelles, t2,negative fact?, ?author, t3,underspecified?} (2)
The model that will be presented here for determining the factuality profiles of
events in text will disregard the temporal component and focus only on identifying
relevant sources and factuality values.
269
Computational Linguistics Volume 38, Number 2
3.2 How Certain Are You: Factuality Values
The values for characterizing event factuality must account for distinctions along both
the polarity and the modality axes. Whereas polarity is a binary system with the val-
ues positive and negative, epistemic modality constitutes a continuum ranging from
uncertain to absolutely certain. In order to obtain consistent annotation for informing
and evaluating automatic systems, a discrete categorization of modality that effectively
reflects the main distinctions applied in natural languages is desirable.
Within modal logic two operators are typically used to express modal contexts:
necessity () and possibility (?). Most linguists, however, agree that this is inadequate
to capture the richness of cross-linguistic data. It has generally been observed that, even
though modality is a continuous system, a three-fold distinction is commonly adopted
by speakers (e.g., Lyons 1977; Palmer 1986; Halliday andMatthiessen 2004). Horn (1989)
analyzes modality and its interaction with polarity based on both linguistic tests and
the logical relations holding at the basis of the Aristotelian Square of Opposition (in
particular, the Law of Excluded Middle and the Law of Contradiction). In Horn?s work,
the system of epistemic modality is analyzed as a particular instantiation of scalar
predication, that is, as a collection of predicates Pn such as ?Pj, Pj?1, ..., P2, P1?, where Pn
outranks (i.e., is stronger than) Pn?1 on the relevant scale. The relations holding among
predicates of the same scalar predication are manifested in syntactic contexts like the
following (Horn 1972):
 Contexts with the possibility open that a higher value on the relevant scale
obtains:
? (at least) Pn?1, if not (downright) Pn.
? Pn?1, {or/ and possibly} even Pn.
 Contexts by which a higher value in the scale is known to obtain:
? Pn?1, {indeed/ in fact/ and what is more} Pn.
? not only Pn?1 but Pn.
This set of contexts allows him to conclude the existence of two independent epis-
temic scales that differ in quality (positive vs. negative polarity):8
(11) a. ?certain, likely (probable), possible?
b. ?impossible, unlikely (improbable), uncertain?
Based on Horn?s distinctions, we divide the modality axis into the values certain
(CT), probable (PR), and possible (PS), and the polarity axis into positive (+) and negative
(?). Moreover, we add an underspecified value in both axes to account for cases of non-
commitment of the source or in which the value is not known. A degree of factuality is
then characterized as a pair ?MOD, pol?, containing a modality and a polarity value (e.g.,
?CT,+?). For the sake of simplicity, these will be represented in the abbreviated form of:
MODpol (e.g., CT+). Table 1 presents the full set of factuality values.
8 The beauty of the system can be appreciated when mapped to the traditional Square of Opposition,
used to account for the interaction between negation and quantifiers or modal operators (Horn [1989],
following Aristotle). For a detailed account of that within the current framework, see Saur?? and
Pustejovsky (2009b).
270
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Table 1
Factuality values.
Positive Negative Underspecified
Certain CT+ (factual) CT? (counterfactual) CTu (certain but unknown output)
Probable PR+ (probable) PR? (not probable) [NA]
Possible PS+ (possible) PS? (not certain) [NA]
Underspecified [NA] [NA] Uu (unknown or uncommitted)
The table includes six fully committed (or specified) values (CT+, CT?, PR+, PR?,
PS+, PS?), and two underspecified ones: the partially underspecified CTu, and the fully
underspecified Uu. The use of the fully committed values should be clear from the
paraphrases in the table, but uncommitted values deserve further explanation. The
partially underspecified value CTu is for cases where the source has total certainty about
the factual nature of the event but it does not commit to its polarity. This is the case of
source John regarding event e in: John knows whetherMary camee. The fully underspecified
value Uu, on the other hand, is used when any of the following situations applies:
(i) The source does not know the factual status of the event (e.g., John does not
know whether Mary camee).
(ii) The source is not aware of the possibility of the event (e.g., John does not
know that Mary camee).
(iii) The source does not overtly commit to the event (e.g., John didn?t say that
Mary camee).
9
3.3 Who Said What: Factuality Sources
Sources are understood here as the cognitive individuals that hold a specific stance
regarding the factuality status of events in text. They correspond to one of the following
actor types:
Text author. Events mentioned in discourse always have a default source, which
corresponds to the author of the text (speaker or writer).
Other sources. Contexts of report, belief, knowledge, inference, and so forth (cre-
ated by predicates like say, think, know, see) introduce additional explicit sources,
generally expressed by the logical subject of the predicate. Similarly, impersonal
constructions (e.g., it seems, it is clear, ...) or passive constructions with no agentive
argument (e.g., it is expected) introduce an implicit source which can be rephrased
as everybody or somebody, among similar expressions. The factuality of the embed-
ded event is assessed relative to this new (explicit or implicit) source, as well as to
any source already present in the discourse, such as the text author.
In the current framework, these sources will be formally represented as: s0 (author
source), sn for n > 0 (explicit source), and GEN (for implicit, generic source).
9 The value Uu could be seen as equivalent to others, such as PS? and PR?. Note, however, that in
these two, but not in Uu, the source commits to a specific degree of uncertainty (possible or probable,
respectively), as in John said that Mary [may have not come], and John said that [Mary has probably not come].
271
Computational Linguistics Volume 38, Number 2
?Source? as a technical term. Although the term source is generally used as a synonym of
informant, in the scope of the current work it is used in a very specific, technical sense.
First, it not only refers to the typical informants, that is, those participants actively
committing to the factuality of an event by means of a speech act or a writing event of
some sort (e.g.,Mary says/claims/wrote...), but also to those that are presented as holding
(or being able to hold) a position about the factuality of that event?be it because they
hold a mental attitude about the situation (Mary knows/learned/thinks/suspects that...),
because they are the experiencers of a psychological reaction generated by the event
in question (Mary regrets/is sad that...), or because they are presented as witnesses or
perceivers of the situation (Mary saw/heard that...).
Second, the notion of source as used here includes participants that are presented
as unaware of the relevant event as well. Consider:
(12) Galbraith is claiming that President Bush was unaware that there were two
major sects of Islam just two months before the President ordered troops to
invade Iraq.
A complete analysis of the facts, causes, and consequences regarding the war in
Iraq needs to include the existence of two major sects of Islam, and what this means in
terms of the potential stability of the area. But it should also include that President Bush
did not know this piece of information beforehand, as claimed by the political actor
Galbraith. Thus, the factuality analysis of the sentence must include President Bush as
a source who at some point in time held an uncommitted factuality stance with regard
to the existence of these two Islamic sects.
Nested sources. The status of the author is, however, different from that of the additional
sources. The reader does not have direct access to the factual assessments made by these
new sources, but only according to what the author asserts. Thus, we need to appeal to
the notion of nested source as presented in Wiebe, Wilson, and Cardie (2005). That is,
Nelles in Example (13) is not a licensed source of the factuality of event e2, but Nelles
according to the author, represented here as nelles author.10 Similarly, the source referred
to as Germany corresponds to the chain: germany nelles author.
(13) Nelles saide1 that Germany has been pretendinge2 for long that nuclear
power is safee3 .
Source roles. We distinguish between two different source roles. Sources most imme-
diately committed (or uncommitted, in the case of unaware sources) to the factuality
status of an event perform the role of cognizers of that event. This is typically the
case of sources introduced in contexts of report, witnessing, belief, and so forth. On the
other hand, sources that present (or anchor) the factuality commitment of the cognizer
towards an event are referred to as the anchors. The roles of cognizer and anchor are
relative to each event. For instance, in Example (13) the cognizer of event e2 (Germany
pretending) is Nelles (according to the author, hence: nelles author) and its anchor is the
text author. On the other hand, the cognizer of event e3 (nuclear power being safe) is
10 This is equivalent to the notation ?author, nelles? in Wiebe?s work. Here, we adopt a reversed
representation of the nesting (i.e., the non-embedded source last) because it positions the most direct
source of the event at the outmost layer, thus facilitating its reading.
272
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Table 2
Polarity value given contextual polarity.
Context polarity
Marker value + ? u
+ + ? u
? ? + u
Germany (based onwhat the author claims that Nelles says, thus: germany nelles author),
and its anchor is Nelles (nelles author).11 Event e1 (Nelles saying) is directly affirmed by
the author, and so the distinction between cognizer and anchor at this level is irrelevant.
3.4 Expressing Factuality in Text: Factuality Markers
Event factuality is conveyed by means of explicit polarity and modality-denoting ex-
pressions of a wide variety. Section 2.1 gave a brief introduction to the main types
(namely, polarity and modality particles, the ESPs and syntactic constructions), and
Section 2.3 illustrated the natural interplay that takes place among them in the context
of a sentence. In the current section we organize the factuality-relevant information
present in lexical and syntactic structures so that it can be used by a model capable
of accounting for the interaction of information across levels of embedding. The focus
is on English data, but the information is easily applicable to other languages, such as
those in the Romance and Germanic families.12
Here and in the following sections, we understand the notion of context of a
factuality marker as the level of scope most immediately embedding it. For instance, the
context of the polarity particle never in Example (14) (subsequent paragraph) is set by
the main clause.
3.4.1 Polarity Particles. Polarity particles of negation (from the adverb not to pronouns
like nobody) switch the original polarity of its context (cf. Polanyi and Zaenen 2006): If
it is positive, the presence of a marker of negative polarity switches it to negative, and
vice versa. Nothing changes if the original context is underspecified. For instance, in
Example (14a) the context of the polarity particle never is positive, and so the resulting
polarity for event train is negative, as opposed to what happens in Example (14b). In
Example (14c) the contextual polarity is underspecified, and so is the factuality value
for event train.
(14) a. It is the case that [context:CT+ John never trainse]. (traine: CT?)
b. It is not the case that [context:CT? John never trainse]. (traine: CT+)
c. It is unknown whether [context:Uu John never trainse]. (traine: Uu)
Table 2 models the interaction between contextual polarity (columns) and the
polarity value contributed by a new marker (rows).
11 Therefore, a source performing the cognizer role for one event can be the anchor source of another.
12 As a matter of fact, we plan to port it to Catalan and Spanish in the near future.
273
Computational Linguistics Volume 38, Number 2
Table 3
Modality value given contextual factuality.
Contextual factuality
Polarity = + Polarity = ? Polarity = u
Marker CT PR PS U CT PR PS U CT PR PS U
CT CT PR PS Uu PS PR PS Uu CT PR PS Uu
PR PR PR PS Uu PR PR PS Uu PR PR PS Uu
PS PS PS PS Uu CT PR PS Uu PS PS PS Uu
U Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu
3.4.2 Particles of Epistemic Modality. The following are some of the most common modal-
ity particles, paired with the factuality value that they express.13
(15) Value Adverbs Adjectives Auxiliaries
CT+ : certainly, necessarily certain, necessary, sure
PR+ : apparently, probably apparent, likely, probable
PS+ : possibly, presumably, seemingly possible, presumed, hypothetical can, could; may, might
CT? : impossible
PR? : improbable, unlikely
PS? : uncertain, unsure
Uu : reportedly, supposedly reported, supposed must, should
A modality particle, however, does not necessarily color the event it scopes over
with its inherent modal value. The factuality value projected to that event depends on
the interaction between the particle on the one hand, and the modality and polarity of
its context, on the other. Consider:
(16) a. Koenig denies [context:CT? that Freidin may have lefte the country]. (lefte: CT?)
b. Koenig suspects [context:PR+ that Freidin may have lefte the country]. (lefte: PS+)
In Example (16a),may is used in a context of negative polarity and absolute certainty
(CT?) set by deny, whereas in Example (16b), it is used in a context of positive polarity
and probable modality (PR+) set by suspect. As a result, in the first example, event e
is presented as a counterfact according to Koenig (CT?), but as a possibility in the
second (PS+).
Table 3 illustrates the interaction between the polarity and modality values from
the context (columns) and the modal value contributed by the marker (rows).14 Note
that the resulting values do not specify polarity information, except for the contexts
where contextual modality or polarity is underspecified (columns 4, 8, and 12, and last
row), where the resulting polarity is u (underspecified). In all other cases, the polarity
contributed by the marker will interact with that from the context as specified in Table 2.
That is, positive contextual polarity will respect the original polarity denoted by the
marker, whereas negative polarity will switch it. For instance, the marker impossible,
which has an inherent value of CT?, in a negative context will express PS+ (e.g., it is not
13 Modal auxiliaries in English can express different types of modality (e.g., epistemic or deontic).
Disambiguating among the possible interpretations of the same auxiliary is a goal beyond the
scope of the current research.
14 It has been compiled by exploring corpus data as well as made-up examples. Combinations with mid
values (probability) are highly unusual; the resulting values are only estimated.
274
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
impossible that...). The reader can use Table 3 to verify the interactions between deny and
may in Example (16a) (corresponding to the value in column 5, row 3), and suspect and
may in Example (16b) (column 3, row 2).
3.4.3 Event-Selecting Predicates (ESPs). As presented earlier, ESPs are predicates with
an event-denoting argument (for instance, predicates of report, knowledge, belief, or
volition). As part of their meaning, they qualify the factuality nature of that event. Here,
we distinguish between two kinds of ESPs: those introducing a new source in discourse,
referred to as Source Introducing Predicates (SIPs), and those that do not, called Non-
Source Introducing Predicates (NSIPs).
Source Introducing Predicates (SIPs). The additional source they contribute tends to cor-
respond to their logical subject. They typically belong to one of the following classes:
(a) Predicates of report; for example, say, add, claim, write, publish.
(b) Predicates of knowledge: know, remember, learn, discover, forget, admit.
(c) Predicates of belief and opinion: think, consider, guess, predict, suggest.
(d) Predicates of doubt: doubt, wonder, ask.
(e) Predicates of perception: see, hear, feel.
(f) Predicates expressing proof: prove, show, support, explain.
(g) Predicates expressing some kind of inferencing process: infer, conclude,
seem (as in: it seems that).
(h) Predicates expressing some psychological reaction as a result of an event
or situation taking place: regret, be glad (that).
As part of their lexical semantics, SIPs express the factuality value that both the new
source they introduce (that is, the cognizer) as well as the anchor, assign to their event-
denoting complement. Compare the following examples built with two different SIPs:
know and say. For each sentence, the columns anchor and cognizer display the factual
values that these two sources assign to the embedded event e (underlined).
(17) anchor cognizer
a. The clients knew that his father had been killede. CT+ CT+
b. ... said ... Uu CT+
By using the SIP know (Example (17a)), the anchor (here the text author) is posi-
tioning himself as agreeing with the client (the cognizer) in considering that his father
had been killed. On the other hand, by using the SIP say (Example (17b)) the anchor
remains uncommitted. Distinctions of this kind are fundamental for any task requiring
perspective identification. SIPs can therefore be characterized and grouped according
to the configuration in the factuality assignments performed by anchor and cognizer.
Notice that none of the SIPs in the following list has the same factual configuration.
(18) anchor cognizer anchor cognizer
say: Uu CT+ know: CT+ CT+
deny: Uu CT? forget: CT+ Uu
suppose: Uu PR+ pretend: CT? CT+
275
Computational Linguistics Volume 38, Number 2
Table 4
Lexicon fragment for SIPs. Entries: know and say.
Contextual factuality
mod=CT mod<CT mod=U
+ ? u + ? u + ? u
know (a) CT+ CT+ CT+ CT+ CT+ CT+ CT+ CT+ CT+
(c) CT+ Uu Uu Uu Uu Uu Uu Uu Uu
say (a) Uu Uu Uu Uu Uu Uu Uu Uu Uu
(c) CT+ Uu Uu Uu Uu Uu Uu Uu Uu
Moreover, the factuality assessments made by anchor and cognizer will vary de-
pending on the polarity and modality in the SIP context. Compare the factuality assign-
ments for sentences a in the following examples with those for sentences b, where the
SIP is in a context of negative polarity.
(19) Context Example anchor cognizer
a. CT+ Hes knows his father diede of a heart attack. CT+ CT+
b. CT? Hes does not know his father diede... CT+ Uu
a?. CT+ Hes said his father diede of a heart attack. Uu CT+
b?. CT? Hes did not say his father diede... Uu Uu
These data can be systematized into a lexicon for SIPs, with each entry specifying
the factual value assigned to the embedded event by both the anchor and the cognizer,
relative to the polarity and modality values of the SIP context. The structure of lexical
entries is as shown in Table 4, where each predicate has the information distributed in
two different rows: one for the anchor (a), and another for the cognizer (c). For instance,
the factuality value of event die in Example (19a) can be found in the 1st column of the
rows for know, whereas the value for die in Example (19b) is in the 2nd column of the
same rows.
Non-source Introducing Predicates (NSIPs). For convenience, all ESPs that do not con-
tribute any additional source in discourse are grouped under the term of NSIPs. These
include a varied set of predicate classes, such as:
(a) Implicative and semi-implicative predicates: fail, manage, or allow.
(b) Predicates introducing a future event as their complement, like volition
(want), commissive (offer), and command (require) predicates.15
15 These predicates are considered as introducing a new source in Wiebe, Wilson, and Cardie (2005). Here,
however, they are treated as NSIPs due to semantic considerations. Whereas SIPs express the epistemic
attitude of their (logical) subject concerning the degree of certainty of the embedded event, predicates
like want or offer denote the role of their subjects as either having some degree of responsibility on the
embedded event (e.g., promise/offer to go; force somebody to go), or being in a greater or lesser favorable state
towards its accomplishment (e.g., need/want to go). In other words, they express distinctions within the
space of deontic modality. Nothing precludes us from treating them as SIPs if preferred, however.
276
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Table 5
Lexicon fragment for NSIPs. Entries: manage and fail.
Contextual factuality
CT PR PS U
+ ? u + ? u + ? u + ? u
manage (a) CT+ CT- CTu PR+ PR- PRu PS+ PS- PSu Uu Uu Uu
fail (a) CT- CT+ CTu PR- PR+ PRu PS- PS+ PSu Uu Uu Uu
(c) Change of state predicates: increase, change, or improve.
(d) Aspectual predicates: begin, continue, and terminate.
By contrast to SIPs, NSIPs express a unique factuality assignment, attributed to the
anchor source. Table 5 illustrates this with the lexical entries for NSIPs manage and fail.
We invite the reader to verify the factuality values of the embedded event as provided
by the table, given different factuality contexts of the NSIP (manage/didn?t manage/may
have managed to go, etc.).
3.4.4 Syntactic Constructions. Factuality information can be also conveyed through syn-
tactic constructions involving subordination. Here we focus only on three of these
structures: restrictive relative clauses, participial clauses, and purpose clauses.16
Purpose clauses. The main event denoted by a purpose clause is intensional in nature.
Thus, all its relevant sources will assess it as underspecified (Uu), as is the case of seek in
the following example, where the ?b? part shows the factual assessment:
(20) a. Stronach resigned as CEO of Magna [to seeke a seat in Canada?s Parliament].
b. f (e, s0)=Uu
Relative and participial clauses. Three different situations apply. We illustrate them focus-
ing on relative clauses, but assume the same treatment for participial clauses as well.
First, in generic contexts, the event denoted by the relative clause is presupposed as
corresponding to a fact in the world (CT+), regardless of the modality and polarity of
the event in the main clause. In the following sentence, for example, the main event e1
is characterized as a counterfact (CT?) but the event working in the relative clause is
presented as a fact (CT+).
(21) a. After World War II, industrial companies could not firee1 the women [relative cl.
that had beenworkinge2 in their plants during the war period].
b. f (e1, s0)=CT? f (e2, s0)=CT+
16 Our decision is motivated by practical reasons. These are the only constructions recognized by the
dependency parser on which De Facto, the implementation of our model, relies.
277
Computational Linguistics Volume 38, Number 2
Second, in quoted contexts the anchor remains uncommitted with respect to the
event in the relative clause:
(22) a. ?[quoted After World War II, industrial companies could not firee2 the women
[rel cl. that had been workinge3 in their plants during the war period]],?
arguede1 Prof. Poes1 .
b. anchor : f (e3, author)=Uu cognizer : f (e3, prof.poe author)=CT+
Third, in reported speech and attitudinal contexts, both the cognizer and the anchor
commit to the event in the relative clause as a fact (CT+).17
(23) a. Prof. Poes1 thinks/saide1 [attit./rep. that after World War II, industrial companies
could not firee2 the women [rel cl. that had beenworkinge3 in their plants during
the war period]].
b. anchor : f (e3, author)=CT+ cognizer : f (e3, prof.poe author)=CT+
The last two interpretations have for long been a matter of discussion in the litera-
ture. Here, we embrace the analyses defended by Geurts (1998) and Glanzberg (2003),
among others. As will be shown in Section 5.4, this area turned out to be a source of
both disagreement among annotators and error from our system.
4. Computing the Factuality Profiles of Events
The current section puts forward an algorithm for a factuality profiler, that is, a tool
for computing the factuality profiles of events in text. As such, it integrates all the
components presented so far: the scalar system of factuality degrees, an organized view
of factuality informants, as well as the structuring of the linguistic devices employed by
speakers to convey distinctions of factuality. The details of the system presented here
are further elaborated in Saur?? (2008).
4.1 Computational Approach
The core procedure of the factuality profiler applies top?down, traversing a dependency
tree. Two reasons motivate a top?down approach. The first one is of empirical nature.
As seen, syntactic subordination is directly involved in the factual characterization of
events (mainly through ESPs), and due to the recursive character of natural language,
the factuality of a given event may depend on non-local information located several
levels higher in the tree (cf. the set of sentences in Example (6)).
The second reason for a top?down approach is methodological. We conceive the
factuality profiler as a neutral and naive decoder; neutral in that it takes all sources as
equally reliable; and naive, because it assumes that sources are trustworthy, based on
the Griceanmaxim of quality. That is, ourmodel assumes that the information presented
in the text is true, without questioning anyone?s view or adopting a particular side.18 In
our model, the naive decoder assumption is applied by initiating the tree top of each
17 Technically speaking, the presupposition is blocked at the quoted level in Example (22), whereas it is
projected up to the embedding level in Example (23).
18 We can then consider a later postprocessing using different weights in order to favor one source as more
reliable than another.
278
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Figure 2
Computing event factuality inMia may not be aware that Joe knows (Paul is the father).
sentence with a default factuality value of CT+; that is, all sentences are assumed to
be true according to their author. This initial value will be potentially modified by the
factuality markers available at subsequent levels of the tree. Consider the sentence:
(24) Mia may not be awaree1 that Joe knowse2 Paul ise3 the father.
Figure 2 exemplifies the initial steps of the procedure computing the factuality
profiles of its events (the full-fledged algorithm will be presented in Section 4.3, after
introducing the relevant technical details).
The computation proceeds as follows. At the top level of the sentence, there is only
one source involved, namely, the author of the text (s0). She is the one uttering the
sentence, and thus the one assessing the factuality of the event placed at its top level (i.e.,
Mia not being aware of something, e1). By the naive decoder assumption, the factuality
at the top level is set to CT+ (Step 1 in Figure 2).
As the algorithm proceeds down the tree, this value is updated to PS+ by the
modal auxiliary may (Step 2) and to PS? by the polarity marker not (Step 3).19 This
is the factuality value available when the parser reaches event e1 (be aware), which
is consequently characterized as PS? according to source s0, the text author. In other
words, the factuality profile of event e1, pbe awaree1
, is the set of factuality values relative
to the relevant sources at its level: pbe awaree1
={?PS?,s0?} (Step 4). In the figure, this is
indicated by the dotted line.
The computation continues. Being a SIP, the predicate be aware contributes a new
source in the situation. In addition to the author (s0), now there is also the source Mia
(sm s0). Mia is the cognizer of event e2 (she is in an ?unaware? epistemic stance concern-
ing Joe?s knowledge), whereas the author is the source anchoring that epistemic stance.
Determining these roles is crucial, because nowwe can appeal to the lexical information
in Table 4 in order to set the perspective of each of these sources. In accordance with the
information there, the anchor of an epistemic state introduced by the SIP be aware (which
behaves like the SIP know) in a context of factuality PS? is characterized with a factuality
19 For convenience, the contribution of the marker is signaled with mod if it affects the modality value,
and pol if it impacts the polarity. Some lexical elements (e.g., the complementizer that) are left off the
representation when not relevant for the computation.
279
Computational Linguistics Volume 38, Number 2
stance of certainty (CT+), whereas the cognizer, being unaware, remains uncommitted
(Uu) (Step 5). Because there are no other factuality markers affecting these values, when
the parser reaches event e2 (Joe knowing something) these are the factuality assign-
ments constituting the factuality profile of that event: pknowe2
={?CT+,s0?,?Uu,sm s0?}
(Step 6).
Thus, the factuality of every event corresponds to the factuality information avail-
able at its context, as computed from the interaction of the different factuality markers
scoping over it. SIPs are crucial inflection points throughout this computation, given
that they reset the evaluation situation by introducing additional sources and character-
izing the factuality perspective these take. Computationally, this is modeled bymeans of
the concept of evaluation level. Every time a new source is incorporated in the discourse
bymeans of a SIP, a new evaluation level is created. The next section details the technical
specificities of this notion.
4.2 Evaluation Levels
Consider each sentence, S, as consisting of one or more evaluation levels, l. By default,
sentences have a root evaluation level, l0. Sentences with SIPs havemore, corresponding
to the levels of embedding created by these predicates. For example, a sentence with
two SIPs, in boldface in Example (25b), has three evaluation levels. We identify each
evaluation level by its embedding depth, expressed in the bracket subindices.20
(25) a. [l0 Paul is the father].
b. [l0 Mia may not be aware that [l1 Joe knows [l2 Paul is the father]].
Each evaluation level ln has:
A set Sn of relevant sources. At the root level l0, S0 contains only one relevant source,
s0, corresponding to the author of the text. At each higher level ln>0, a new source
is introduced by the SIP triggering it.
A set En of events (one or more), the factuality of which is evaluated relative to each
relevant source s ? Sn.
A set Fn of contextual factuality values. At the beginning of each new level, one or
more factuality values are set (cf. the value CT+ applying the naive decoder
assumption at the top level). These values are relative to the relevant sources in
Sn, because each source may assess the same event differently.
The task of event identification can be carried out by already existing event recog-
nizers. The next sections define the operations for identifying the set of relevant sources
Sn and the factuality values these assign to each event in any evaluation level ln.
4.2.1 Identifying Relevant Sources and Their Roles. The process for identifying the set of
relevant sources Sn at each evaluation level ln can be defined inductively.
20 Note that, because evaluation levels are only triggered by SIPs, a sentence can contain several levels of
syntactic embedding and yet only one evaluation level, corresponding to the top one, l0. The following
example contains three embedded clauses (signaled with curly brackets) but only one evaluation level.
[l0 {After four years there}, Freidin managed {to return to the country {where she was originally from}} ].
280
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Definition 1
Relevant Sources
i. The set of relevant sources at level l0 contains only a (non-nested) source, which
corresponds to the text author: S0 = {s0}.
ii. The set of relevant sources at level ln, where n > 0, is:
Sn = Sn?1 ? {sn z | sn is the new source introduced at level ln & z ? Sn?1}
Clause (i) needs no additional comment. Clause (ii) states that the set of relevant
sources Sn at level ln contains (a) the set of relevant sources at the previous level ln?1,
that is, Sn?1 (this is expressed as the first part of the union); and (b) the set of all source
chains composed of the new source sn introduced at that level by the corresponding SIP,
and a relevant source from the preceding level, z ? Sn?1 (second part of the union).
We use the sentence Mia is not aware that Joe knows Paul is the father to illustrate the
set of relevant sources Sn identified at each level ln by the previous definition:
(26)
[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2Paul ise3 the father]]]
s0 s0 s0
sm s0 sm s0
sj s0
sj sm s0
Definition 1 seems to return an excessive number of sources at level l2. In particular,
the source chains sj s0 and sj sm s0 appear to be redundant, because both of them refer
to the same person, Joe. Notwithstanding, the analysis is adequate if we want to account
for Joe?s epistemic stance relative to the other sources involved in the situation. Source
expressions sj s0 and sj sm s0 represent in fact two different perspectives. Expression
sj sm s0 includes a reference to Mia, that is, it presents Joe?s epistemic stance according
to Mia, based on what the author says. On the other hand, expression sj s0 refers to Joe?s
perspective only according to the author.
As asserted in the sentence, Mia is clueless about Joe?s knowledge concerning Paul?s
paternity, whereas according to the author, Joe knows the fact. Strictly speaking, then,
the event Paul being the father (e3) is evaluated by sj s0 as a fact in the world (CT+), but
will be presented with an uncommitted value (Uu) from the perspective expressed by
sj sm s0.
The next step now is determining the roles for each of these sources. In Section 3.4
on factuality markers, we saw that this distinction is crucial for identifying the factuality
stance of each involved source. Themechanism for finding the anchorsAn and cognizers
Cn at each evaluation level ln can be stated as follows:
Definition 2
Source Roles
i. At level l0: A0 = {s0} and C0 = {s0}.
ii. At level ln, for n > 0:
An = {s | s ? Sn?1 & f (en?1, s) = Uu} and
Cn = {sn sa | sn is the new source introduced at level ln & sa ? An}.
Clause (i) defines the sets of anchors and cognizers at the evaluation level l0, which
contains only the relevant source s0 (the text author). At this level, the distinction
281
Computational Linguistics Volume 38, Number 2
between anchor and cognizer is irrelevant, and so we arbitrarily establish s0 as per-
forming both roles.
Clause (ii) defines anchors and cognizers for higher evaluation levels, ln>0. In
particular, anchors are defined as those sources from the previous evaluation level, s ?
Sn?1, that are not uncommited (Uu) towards the factuality of en?1, which is the SIP event
embedding ln (in the definition, the notation f (e, s) expresses the factuality assessment
made by source s over event e). Returning to Example (26), this restriction prevents
selecting source Mary (sm s0) as the anchor of event e3, because she is presented as
having an uncommitted perspective (she doesn?t know) on event e2. Given that more
than one source in a level can commit to the same event, an event can have more than
one anchor, hence the notion of anchor set.
Last, clause (ii) defines cognizers as those sources composed of the new source
introduced at level ln, sn, nested relative to any anchor source at that level, sa ? An.
Computationally the notion of cognizer is therefore dependent on that of anchor, and
given that more than one anchor is possible at each level, the cognizer role can be per-
formed by several source chains as well. All other sources not satisfying the definition
of either anchor or cognizer are assigned the role of none, expressed as ( ).
We apply Definition 2 to the earlier sentence, as well as to a second one, structurally
identical but with different SIPs setting each evaluation level:
(27)
[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2Paul ise3 the father]]]
(a,c) s0 (a) s0 (a) s0
(c) sm s0 ( ) sm s0
(c) sj s0
( ) sj sm s0
(28)
[l0Miasm sayse1 that [l1 Joesj tolde2 her [l2Paul ise3 the father]]]
(a,c) s0 (a) s0 ( ) s0
(c) sm s0 (a) sm s0
( ) sj s0
(c) sj sm s0
The roles for sources at level l0 and l1 are the same in both sentences: The role
assignment at level l0 is trivial, while at level l1, Mia is the cognizer of event e2 (Joe
telling/knowing something) because she is the one cognitively aware, or unaware, of
the fact that Joe is telling/knows something. Nonetheless, source roles are different at
level l2. In Example (27) Mia cannot be the anchor of Joe?s epistemic stance because
she is presented as unaware of that (Uu). Instead, the source anchoring Joe?s epistemic
stance concerning event e3 is the author of the sentence, that is, s0 (as opposed to sm s0).
Because of this, in Example (27) the cognizer role is performed by the source chain sj s0,
whereas in Example (28) is performed by sj sm s0.
4.2.2 Identifying Contextual Factuality Values. In order to compute the factuality values
assigned by the relevant sources to the events at each level, we start by associating
a contextual factuality value f to each relevant source s ? Sn every time a new level
ln is opened. We represent this mapping as ? f, s?, and subsequently define the set of
contextual factuality values at level ln as: Fn = {? f, s?| f is a factuality value & s ? Sn }.
The set of contextual factuality values Fn can be obtained as follows.
282
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Definition 3
Contextual Factuality Values
i. At level l0: Fn={?CT+, s0?}
ii. At level ln, for n>0: Fn={ ? f, s? | s ? Sn & f=Lex(en?1, cen?1 , rs)}
Clause (i) sets the contextual factuality for evaluation level l0. By default, at level l0
the set Fn contains only the value CT+ relative to the text author: ?CT+, s0?. This applies
the naive decoder assumption.
In clause (ii), the contextual factuality value f associated to each source s is deter-
mined by function Lex, which performs a lookup into the SIPs lexical base (Table 4)
given the following parameters:
rs: The role performed by the source s ? Sn (anchor, cognizer, or none).
en?1: The SIP in the previous evaluation level ln?1 that is embedding the current level,
ln. The information in its lexical entry will provide the contextual factuality values
for the relevant sources at the current evaluation level (cf. Table 4).
cen?1 : The committed factuality value that was assigned to SIP en?1 in the previous level
ln?1. All factuality values, except for the fully underspecified Uu, are considered
committed values. For instance, in Example (29), the factuality value to be used for
setting the contextual factuality values for level l2 is CT+, the only committed value
assigned to event knows (e1) in level l1.
(29)
[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2 Paul ise3 the father ]]]
(a,c) f (e1, s0)=CT? (a) f (e2, s0)=CT+ (a) f (e3, s0)=CT+
(c) f (e2, sm s0)=Uu ( ) f (e3, sm s0)=Uu
(c) f (e3, sj s0)=CT+
( ) f (e3, sj sm s0)=Uu
We illustrate how clause (ii) works with the operation of setting the contextual fac-
tuality values when opening the evaluation level l1 in Example (29). The SIP embedding
this level (corresponding to parameter en?1 in function Lex) is be aware, which receives
the committed factuality value of CT? (parameter cen?1 ). Furthermore, at level l1 there
are two relevant sources, s0 and sm s0, the first one performing the role of anchor, and
the second the role of cognizer (parameters rs). With all that information at hand, the
contextual factuality values for level l1 will be obtained by means of a dictionary lookup
performed by function Lex(en?1, cen?1 , rs). Using the lexical information in Table 4, this
operation can establish the following contextual factuality values for the anchor and
cognizer sources at the new level l1:
a. Lex(be aware, CT?, anchors0 ) = CT+
(3)
b. Lex(be aware, CT?, cognizersm s0 ) = Uu
If the role is none, there is no need to perform the lexical look-up. The contextual
factuality value will be set to underspecified (Uu).
4.3 Algorithm
The factuality profiler algorithm is provided in Algorithm 1, which further develops
that presented in Saur?? and Pustejovsky (2007) by incorporating syntactic constructions.
283
Computational Linguistics Volume 38, Number 2
Algorithm 1 De Facto: the Factuality Profiler.
1: n ? 0
2: set level ln
3: for all i in TREE do
4: #PART 1: CHECK FOR SYNTACTIC MARKER
5: if i is head of relative, participle, or purpose clause then
6: update contextual factuality, Fn
7: end if
8: #PART 2: CHECK FOR EVENT
9: if i is an event then
10: obtain the factuality profile of i, pi
11: end if
12: #PART 3: CHECK FOR LEXICAL MARKER
13: if i is a SIP then
14: n ? n+ 1
15: set level ln
16: else if i is another type of marker then
17: update contextual factuality, Fn
18: end if
19: end for
Its core procedure (lines 3?19) consists of three main components. Part 1 implements the
effect of syntactic-based factualitymarkers (specifically, relative, participle, and purpose
clauses), Part 2 is in charge of assigning the factuality value to every found event, and
Part 3 implements the effect of lexical markers on the contextual factuality values.
Part 3 (checking whether the node found is a lexical marker of any sort and
subsequently updating the contextual factuality values) needs to be performed after
Part 2 (obtaining the factuality profile of any found event) due to the double nature of
ESPs, which are both event-denoting expressions and, at the same time, lexical markers.
As markers, they affect the contextual factuality of their embedded events. Hence,
their factuality profile (Part 2) needs to be obtained before they update the context
values (Part 3). This is illustrated in Figure 2. When the algorithm index i is at node
be aware, it must first obtain the factuality profile of that event (Step 4) before updating
the contextual factuality according to the semantics of the verb be aware (Step 5). By
contrast, Part 1 needs to be run before evaluating the factuality of the event given that it
implements the effect of syntactic constructions imposing a specific factuality value to
its main event.
The functionality of the algorithm splits into three main components, which are in
charge of: (i) setting each new evaluation level ln; (ii) updating the set of contextual
factuality values, Fn, every time a newmarker is found; and (iii) obtaining the factuality
profile of events. We discuss them in what follows.
(i) Set Level ln (lines 1?2 and 14?15). This function is called every time a new level is
opened, be it at the top of the tree (lines 1?2) or when a SIP is found (lines 14?15). It
executes the following steps:
1. Identify the set of relevant sources at the current level, Sn. This procedure
is carried out applying Definition 1.
284
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Algorithm 2 Syntactic Markers in De Facto.
1: if i is head of a participle or a relative clause then
2: if i is in a quoted area then
3: anchor=Uu ; cognizer=CT+ #APPLY PLUG
4: else
5: anchor=CT+ ; cognizer=CT+ #PROJECT PRESUPPOSITION
6: end if
7: else if i is head of a purpose clause then
8: anchor=Uu ; cognizer=Uu #APPLY UNCOMMITMENT
9: end if
2. For each s ? Sn, identify its role (anchor, cognizer, or none). Computed
applying Definition 2.
3. Set the contextual factuality values, Fn. This is performed applying
Definition 3, based on lexicon look-up.
(ii) Update the contextual factuality, Fn (lines 5?6 and 16?17). The update may be
triggered by either a syntactic or a lexical marker. Lexical markers that are appropriate
here are polarity particles, modality particles, or NSIPs.21 Any time one of them is
found in ln, the profiler updates the contextual factuality values v ? Fn according to
the information it conveys (lines 16?17). Syntactic constructions, on the other hand,
reset the contextual factuality values according to Algorithm 2, which articulates the
linguistic analysis concerning participle, relative, and purpose clauses, as presented in
Section 3.4.
(iii) Obtain the factuality profile of e, Pe (lines 9?10). Applied when an event is found.
Due to the on-the-fly updating of the contextual factuality values in Fn whenever a new
level is set (i) or a newmarker is found (ii), the event profile is in fact already computed.
The factuality profile for event en, pen , corresponds to the set of contextual factuality
values Fn available at that point.
5. Experiments and Evaluation
5.1 Implementation
The modeling of the factuality profiler put forward here has been implemented and
evaluated against a corpus annotated for that purpose. The resulting tool, called
De Facto, integrates the algorithm in the previous section, along with the linguistic
resources with lexical and syntactic information structured as presented in Section 3.4,
and articulated around the scalar definition of factuality values developed in Section 3.2.
The approach is therefore entirely symbolic, involving lexical look-up while top?
down traversing the dependency tree of each sentence. The lexical resources informing
De Facto include those listed here. They will be made available to the community in the
near future.
21 Recall that SIPs affect the contextual factuality as they set a new evaluation level.
285
Computational Linguistics Volume 38, Number 2
Table 6
Distribution of ESPs in De Facto.
Part of Speech SIPs NSIPs Total
Verbs 204 189 393
Nouns 58 107 165
Adjectives 27 61 88
Total 289 357 646
Polarity particles: A total of 11 negation particles distributed among adverbs (such as
not, neither), determiners (no, non), and pronouns (none, nobody), together with the
table on contextual polarity interactions (Table 2).
Modality particles: The set of 31 particles presented in Example (15), each accom-
panied with their default modality interpretation, as well as their interaction
table (Table 3).
ESPs: The lexical entries for a total of 646 ESPs, distributed as shown in Table 6. Lexical
entries structure their factuality information as illustrated in Tables 4 and 5 (for
SIPs and NSIPs, respectively). The information in each lexical entry was compiled
manually in a data-driven fashion by exploring its use in our corpora of reference,
TimeBank and the American National Corpus (Slate and NYTimes fragments).22
De Facto takes as input a document (or a set of them) and returns the factuality
profiles of each event. Input documents have been tokenized, POS-tagged, and parsed
into dependency trees with the Stanford Parser (version 1.6; de Marneffe, MacCartney,
and Manning 2006). In the current implementation, De Facto does not incorporate
any component for recognizing events nor identifying source mentions in text. This
information was generated frommanual annotation and fed to the tool. The chaining of
different source mentions into relevant sources is computed automatically, however, by
means of Definition 1.
As output, De Facto returns the factuality profile of each event in the input text.
Example (31) shows the factuality profiles for the events in (30).
(30) Analystss1 saide1 the governments2 knewe2 a peaceful solution wase3 in reach.
(31) e1: said fp(e1) = { ?s0, CT+? } (anchor)
e2: knew fp(e2) = { ?s0, Uu?, (anchor)
?s1 s0, CT+? } (cognizer)
e3: was fp(e3) = { ?s0, Uu?,
?s1 s0, CT+?, (anchor)
?s2 s0, Uu?,
?s2 s1 s0, CT+? } (cognizer)
5.2 Development and Evaluation Corpus
For developing and evaluating De Facto, we compiled FactBank, a corpus annotated
with information concerning the factuality of events (Saur?? and Pustejovsky 2009a).
22 Documented, respectively, at: http://www.timeml.org/site/timebank/timebank.html, and
http://americannationalcorpus.org.
286
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Table 7
Confusion matrix: Gold standard (rows) vs. De Facto output (columns).
CT+ CT? Ctu PR+ PR? PS+ PS? Uu NA Total
CT+ 1,131 0 0 0 0 2 0 84 59 1,276
CT? 13 33 0 0 0 0 0 1 4 51
CTu 1 0 0 0 0 0 0 0 0 1
PR+ 12 0 0 8 0 0 0 3 2 25
PR? 0 0 0 0 0 0 0 0 0 0
PS+ 7 0 0 0 0 22 0 2 2 33
PS? 0 0 0 0 0 0 2 0 0 2
Uu 226 4 1 2 0 17 0 532 22 804
Total 1,390 37 1 10 0 41 2 622 89 2,192
FactBank consists of 208 documents, which include all those in TimeBank (Pustejovsky
et al 2006) and a subset of those in the AQUAINT TimeML Corpus.23 The TimeBank
part was used for developing De Facto and its associated linguistic resources, and the
AQUAINT TimeML part was set as the gold standard for evaluating its performance.
TimeBank contains 183 documents (amounting to 88% of the documents in FactBank)
and 7,935 events (83.6% of the events), and the AQUAINT part has 25 documents (12%)
and 1,553 events (16.4%).
Overall, FactBank contains a total of 9,488 events. Given that each event can have
more than one relevant source, FactBank has a total of 13,506 event/source pairs
manually annotated with the set of factuality distinctions introduced in Table 1. The
annotation has applied a battery of discriminatory tests grounded on the linguistic and
logical relations at the core of Horn?s analysis (refer to Section 3.2). The inter-annotation
agreement from that exercise is ? = 0.81 (over 30% of events in the corpus). In terms
of pairwise F1-score (that is, taking one of the annotators as the gold standard), the
agreement between annotators yielded: CT+: 0.93, CT?: 0.83, PR+: 0.57, PR?: 0.46, PS+:
0.56, PS?: 0.75, and Uu: 0.88. Overall, these results are highly satisfying considering
the difficulty of the task and thus validate the approach on the annotation. See further
details in Saur?? and Pustejovsky (2009b).
5.3 Performance
The confusion matrix resulting from mapping the subset of FactBank used as gold
standard against De Facto output is shown in Table 7. The total number at the bottom-
right corner corresponds to the number of event/source pairs in the gold standard, that
is, the number of instances to be classified with a factuality value. Classes PRu and PSu
are not shown because they have no instance in the gold standard.
Instances classified in the NA column correspond to event/source pairs for which
De Facto did not return a factuality judgment. An analysis of this pointed to errors in the
dependency trees as the possible cause of this behavior. In other words, they seemed to
be pairs involving sources mentioned in subordinated clauses that had not been parsed
properly and, as a consequence, De Facto could not pair with their corresponding
events. Because subordination structures are fundamental in De Facto?s algorithm, we
23 http://www.timeml.org/site/timebank/timebank.html.
287
Computational Linguistics Volume 38, Number 2
decided to evaluate the system on two different versions of the gold standard: a first
one with the dependency trees originally returned by the parser (corresponding to the
data in Table 7), and a second one where dependency errors on subordination had been
manually corrected. In total, we corrected an estimated 2% (at the lowest bound) of the
dependencies involving subordination structures.
Table 8 shows the results from running De Facto against both versions of the gold
standard. De Facto?s performance is evaluated in terms of precision and recall (P&R)
and their harmonic mean, F1 score. We considered only those categories for which
there exist more than 10 instances classified as such in the gold standard; that is: CT+,
CT?, PR+, PS+, Uu. Furthermore, P&R for the whole corpus is obtained by applying
the measures of macro- and micro-averaging (last two columns in the table). Macro-
averaging averages the result obtained in each class, and micro-averaging applies over
the set of instances, regardless of class distribution. The first measure gives equal weight
to each class and hence over-emphasizes the performance of the less populated ones,
and the second one over-emphasizes the performance of the largest classes because
it assigns equal weight to each instance. Given the uneven class distribution in our
gold standard, we take the combination of both measures as indicative of the lower
and upper bounds of the result.
As can be seen from Table 8, the corrected version of the gold standard attains much
higher recall than the original one (especially for the classes CT?, PR+ and Uu). The
reason for that is the absence of event/source pairs tagged as NA by our system (as
opposed to what was appreciated in the confusion matrix on Table 7). In the corrected
version, De Facto was able to follow the dependency tree, appropriately pair all the
events with their sources, and return a factuality value for each pair.
The results obtained in all the categories for the corrected version of the gold
standard are equivalent to or higher than those in the original one, except for the very
particular case of PR+ precision. The fact that increasing the quality of the parsing results
in better performance of the system validates the linguistic model in De Facto.
The results for CT?, PR+, and PS+ must be interpreted cautiously, given the sparsity
of data in these classes. Nevertheless, the high precision achieved for CT? is encour-
aging, especially considering that polarity here is not only determined locally but by
means of subordinating predicates as well. Similarly, the distinction between the two
modal degrees PR and PS seems pertinent and possible to determine by the system. No
instance was misclassified between the two, as shown in the confusion matrix (Table 7).
Table 8
P&R for each relevant category and for the whole corpus (macro- and micro-average).
CT+ CT? PR+ PS+ Uu Macro-A Micro-A
Original parses
Precision 0.81 0.89 0.80 0.54 0.86 0.78 0.82
Recall 0.89 0.65 0.32 0.67 0.66 0.64 0.79
F1 0.85 0.75 0.46 0.59 0.75 0.70 0.80
Corrected parses
Precision 0.86 0.90 0.73 0.56 0.86 0.78 0.85
Recall 0.92 0.75 0.44 0.67 0.77 0.71 0.85
F-1 0.89 0.82 0.55 0.61 0.81 0.74 0.85
288
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Evaluating De Facto?s performance on both versions of the gold standard provides
a look into two different aspects of the system. Whereas the original version shows its
impact on a standard NLP pipeline, the corrected version puts the proposed algorithm
to test by exposing it to complex sentences with several levels of embedding. In order to
assess De Facto?s results regarding these two aspects, we generated a baseline from a su-
pervised learning approach, by means of support vector machines (SVM). We followed
Prabhakaran, Rambow, and Diab (2010), which is state-of-the-art on automatic tagging
of committed belief (cf. Diab et al 2009b), a notion equivalent to modality and which
distinguishes between certain vs. uncertain events. The classification that they propose
is less fine-grained than ours (certain vs. probable vs. possible), but the information
supporting the distinctions is exactly the same, and therefore we adopted the features
employed in their best classifier (listed from 1 to 12 in the following example). In
addition, we added feature 13 given that our classifier was not aiming at identifying
event mentions in the text (contrary to Prabhakaran, Rambow, and Diab?s model), and
features 14 and 15 to cope with distinctions along the axis of polarity (not addressed by
that system).
(32) 1. isNumeric Word is Alphabet or Numeric?
2. POS Word?s POS tag
3. verbType Modal, auxiliary or regular (nil if not a verb)
4. whichModalAmI If I am a modal, what am I? (nil if not a modal)
5. amVBwithDaughterTo Am I a VB (base verb) with a daughter to?
6. haveDaughterPerfect Do I have a have form daughter? (only for verbs)
7. haveDaughterShould Do I have a should daughter? (only for verbs)
8. haveDaughterWh Do I have a daughter which is: where, when, while, who, why?
9. haveReportingAncestor Am I an event with an ancestor whose lemma is: believe, accuse,
insist, seem, tell, say, find, conclude, claim, trust, think, suspect, doubt,
suppose?
10. parentPOS What is my parent POS tag?
11. whichAuxIsMyDaughter If my daughter is an auxiliary, what is it? (nil if not an auxiliary)
12. whichModalIsMyDaughter If my daughter is a modal, what is it? (nil if not a modal)
13. amEvent Am I an event?
14. whichPolarAmI If I am a polar marker, am I a conjunction (nor), a pronoun (none)
or other?
15. whichPolarIsMyDaughter If my daughter is a polar particle, what type is it?
16. amSource Am I a source?
17. whichSIPtypeAreMyAncest. If I am a source, what SIP type are my ancestors? (based on the
SIP classification in Section 3.4.3)
18. whichDepRelWithMyParent If I am a source, what is my dependency relation with my parent?
19. whichSIPtypeAmI If I am a SIP, which type am I?
Prabhakaran, Rambow, and Diab?s work assesses the committed belief of only
the author source, but in our case an event can receive several factuality values from
different sources. Hence, we decided to generate two different models: the author level
model, in which the factuality of events is assessed relative to the author of the text (i.e., at
the level of source s0), and the top source level model, in which event factuality is assigned
according to the source with a higher level of nesting in the set of relevant sources for
that event (e.g., sm sj s0). Thus, features 16?19 were added to convey information on the
top-level sources as well.
Following Prabhakaran, Rambow, and Diab?s work, we trained our SVM classifiers
using YAMCHA (Kudo andMatsumoto 2000) and used the same parameters applied to
their best classifier: context width of 2 (i.e., the feature vector of any token includes the
two tokens before and after), and the One versus allmethod for multiclass classification
on a quadratic kernel with a c value of 0.5. For evaluation, we performed a 10-fold
cross-validation.
289
Computational Linguistics Volume 38, Number 2
Table 9 shows the results (F1 measure) of the two SVM classifiers (author and top
source levels, as well as their average) running on both the original and the corrected
versions of the gold standard. For a more meaningful comparison with our system,
we also computed De Facto?s performance on these two source levels. The results are
shown in Table 10, where we also added, as a reference point, the figures obtained from
evaluating De Facto on all source levels (corresponding to the F1 rows in Table 8).
Furthermore, we assessed whether De Facto?s improvement over the baseline is
statistically significant applying a one-sample two-tailed t-test over the results for every
category at each source level. We applied the one-sample version of the t-test because De
Facto?s performance results do not conform a distribution, because they were obtained
from running the system once over the evaluation subcorpus. In the test, the sample
data corresponds to the results from the 10 runs of the SVM classifier, whereas the
De Facto?s value is taken as the expected (or null) hypothesis. For the top and author
levels, the degree of freedom is df = 9 (from 10 runs ? 1), while for their average it is
df = 19 (10 + 10 runs ? 1).
As seen in Table 9, there is no significant difference between the baseline generated
from the original and the corrected versions of the corpus, which is explained by the
fact that the SVM models are based on fairly local linguistic features and use very little
information on subordination structures. What is in fact most noticeable in the baselines
is the difference between the results on the author and the top source levels for the less
populated classes (CT?, PR+, and PS+). The top source level reachesmuch higher results,
which could be explained by the greater use of dependency-based features providing
information on the top source (features 15?18). This hypothesis underlines the role of
deep linguistic features for identifying the factuality of event mentions in text.
By contrast to the baseline, De Facto shows a significant improvement when run-
ning on the corrected version of the gold standard, which proves the adequacy of
its model to the linguistic information it targets. The downside of that is potentially
too much dependency on high quality linguistic data in order to obtain acceptable
performance degrees. Nevertheless, the results in the two tables show that De Facto
is performing equal or better than the SVM classifiers when fed with original (not
corrected) output from a standard NLP pipeline, especially in the case of less populated
classes, which happen to be the ones with marked polarity and modality values, that is,
which feature negative polarity, or probable and possible modality values.
Table 9
Baseline performance (F1 measures).
CT+ CT? PR+ PS+ Uu Macro-A Micro-A
Original parses
Author 0.88 0.53 0.07 0.29 0.75 0.53 0.83
Top sources 0.92 0.69 0.51 0.50 0.57 0.66 0.86
Average 0.90 0.61 0.29 0.39 0.66 0.59 0.84
Corrected parses
Author 0.88 0.54 0.07 0.27 0.77 0.53 0.83
Top sources 0.92 0.67 0.50 0.50 0.51 0.64 0.85
Average 0.90 0.61 0.28 0.38 0.64 0.58 0.84
290
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Table 10
De Facto performance (F1 measures).
CT+ CT? PR+ PS+ Uu Macro-A Micro-A
Original parses
All sources 0.85 0.75 0.46 0.59 0.75 0.70 0.80
Author 0.88 0.88 *** 0.67 *** 0.33 0.78 0.73 *** 0.84 *
Top sources 0.90 0.79 * 0.33 * 0.66 ** 0.58 0.67 0.84
Average 0.89 0.84 *** 0.50 ** 0.50 * 0.68 0.70 *** 0.84
Corrected parses
All sources 0.89 0.82 0.55 0.61 0.81 0.74 0.85
Author 0.90 0.91 *** 0.67 *** 0.35 0.84 ** 0.75 *** 0.88 *
Top sources 0.93 0.85 ** 0.53 0.67 ** 0.65 * 0.74 * 0.88
Average 0.92 0.88 *** 0.60 *** 0.51 * 0.75 * 0.75 *** 0.88 **
* p ? 0.05
** p ? 0.01
*** p ? 0.001
The low performance of the SVMmodels is due to the small sample of these classes
in the corpus, and so it can be expected that with more training data the classifiers
will learn to perform better, a fact that makes them dependent on the availability of
significantly larger annotated corpora. De Facto, on the other hand, is grounded on
the linguistic expression that articulates factuality distinctions in natural language, and
therefore does not depend as much on corpus size but on a good modeling of the
interaction among the relevant linguistic structures. In this sense, the results shown
here are quite promising regarding the capabilities of our system, even though it suffers
from some limitations, as will be seen next.
5.4 Error Analysis
We analyzed the errors returned by De Facto when run on the manually corrected
version of the corpus. With this choice, we wanted to avoid error from the parser and
hence obtain a more precise assessment on the adequacy of our computational model.
This version of the corpus has 320 event/source pairs wrongly classified (14.6% on the
total number of pairs), whereas the original version has 464 pairs (21.2%).
Most disagreements between De Facto?s output and the gold standard are due
to limitations in our system (84.4%), which mainly classify into insufficient coverage
of factuality markers, either lexical or syntactic, and structural and lexical ambiguity.
Other disagreements are due to some inaccuracy in the gold standard annotation
(7.5%), or to an incorrect analysis from the dependency parser which escaped our
manual correction (8.1%). Table 11 shows the error type distribution, distinguishing
between lexical and syntactic error when relevant.
Insufficient coverage. There are a number of syntactic constructions crucially involved
in determining the factuality nature of events and which, nevertheless, have not been
291
Computational Linguistics Volume 38, Number 2
accounted for here, most commonly: copulative phrases, cleft structures (e.g., But it?s not
tonight we?re worriede about), and conditional constructions (of the form if... then..., and
equivalent). This amounts to 32.5% of the total error. De Facto also suffers from gaps
at the lexical level, even though in a much lesser degree (1.9%). It lacks, for example,
ESPs such as conspiracy (as in: a conspiracy to commit murder) or easy (e.g., it is easier
to do it).
Ambiguity. De Facto does not cope with lexical polysemy of any type (18.1% of
the total error). For example, the modal auxiliary would is employed in embedded
contexts to express future (and hence CT+, which is how De Facto models this tense),
but there are certain constructions in which it expresses some degree of uncertainty. A
further interesting case involves ambiguity regarding the temporal reference of events.
De Facto assumes that aspectual predicates of termination (e.g., stop, finish) qualify
their embedded event as a fact (that is, it is a fact that they took place in the world),
whereas the gold standard treats them as counterfactual (the event does not hold
anymore).
At the syntactic level, there are cases of truly ambiguous constructions, such as
relative and participial clauses, as well as event-denoting nouns, when embedded under
contexts of report, propositional attitudes, or uncertainty (28.1% of the total error).
Some of these ambiguities have long been discussed in the linguistics literature, and
happened to be a source of remarkable disagreement among the FactBank annotators
as well (cf. Saur?? and Pustejovsky 2009b). The high error rate in this area seemed
to suggest that the approach assumed in De Facto for these constructions (following
Geurts [1998] and Glanzberg [2003]; see Section 3.4.4) was not completely adequate.
Thus, we experimented running De Facto without the part of the algorithm dealingwith
them (Algorithm 2, lines 1?9). The results, however, are inconclusive. Although there is
a slight improvement of 1 or 2 points over the F1 of categories PS+ (from 0.59/0.61
to 0.61/0.63 when running on the original/corrected parses), and Uu (from 0.75/0.81
to 0.77/0.82 on the original/corrected parses), there is a decrease in other categories,
such as PR+ (from 0.46 to 0.43, original parses) and CT? (from 0.82 to 0.80, corrected
parses).
Overall, the main limitations observed here are shared with other work also ap-
proaching tasks of sub-sentential interpretation by means of linguistically heavy and
resource-intensive models, such as Moilanen and Pulman (2007) or Neviarouskaya,
Prendinger, and Ishizuka (2009), which address sentiment analysis based on the princi-
ple of compositionality. Moilanen, Pulman, and Zhang (2010) successfully explore the
feasibility of combining this approach with a machine learning-based classifier.
Table 11
Error classification.
Error source % % Lexical % Syntactic
Insufficient coverage 34.4 1.9 32.5
De Facto Ambiguity 46.2 18.1 28.1
limitations Other 3.8 ? ?
Subtotal 84.4 20 60.6
Gold standard 7.5 ? ?Other error
Wrong dependency trees 8.1 ? ?sources
Subtotal 15.6 ? ?
292
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
6. Related Work
The last decade has seen a growing interest on speculative language and its treatment
within NLP. This has crystallized into research from a variety of perspectives, including
general but also domain-specific (mainly biomedical), and reflects not only in the build-
ing of processing systems, but also in the area of corpus creation, where most of the
conception and structuring of factuality-related information takes place, thus providing
the support for more applied investigations.
6.1 Factuality Information in Corpora
In some corpora, factuality-related information is annotated as information comple-
mentary to the main phenomenon they target. It is, for instance, contemplated in
different versions of the ACE corpus for the Event and Relation recognition task (see,
e.g., ACE 2008), in the Penn Discourse TreeBank (Prasad et al 2007), and in TimeBank
(Pustejovsky et al 2006). In other corpora, factuality information becomes the epicenter
of their annotations. For example, Rubin (2007, 2010) is concerned with the notion of
certainty, the Language Understanding Annotation Corpus (Diab et al 2009a) focuses
on the author?s committed belief towards what is reported (a notion comparable to
the modality axis in event factuality), and the small knowledge-intensive corpus by
Henriksson and Velupillai (2010) targets degrees of certainty.
In the bioNLP area, factuality and related information is lately becoming a no-
table area of research and has led to the creation of remarkable corpus resources. The
BioScope corpus (Vincze et al 2008) contains more than 20,000 sentences annotated
with speculative and negative key words and their scope. Based on this experience,
Dalianis and Skeppstedt (2010) compiled a corpus of Swedish electronic health records
with speculation and negative cues marked up, together with the values resulting from
their interaction. The corpus presented in Wilbur, Rzhetsky, and Shatkay (2006) tags
the polarity and certainty degree of clauses, along with other dimensions. The GENIA
Event corpus (Kim, Ohta, and Tsujii 2008) contains 1,000 abstracts with biological events
annotated with polarity and degrees of certainty, in addition to other information such
as the lexical cues leading to these values (Ohta, Kim, and Tsuji 2007). Such an approach
is followed by the currently on-going large scale annotation effort (Nawaz, Thompson,
and Ananiadou 2010), with an event-centered annotation that includes polarity, degrees
of certainty, and sources.
6.2 Systems for Identifying Factuality and Related Information
Systems devoted to identifying factuality-related information can be generally classified
into two groups: (a) those prioritizing the identification of linguistic structure (that is,
speculative cues and their scope); and (b) those focusing on the factuality values that
result from these cues and their interaction. The first approach mostly revolves around
the BioScope corpus, which has become a good catalyzer for research on this topic in the
biomedicine domain. Part of it was used for the CoNLL-2010 shared task on Learning To
Detect Hedges and their scope in Natural Language Text (Farkas et al 2010). Moreover,
it is at the basis of explorations on hedging and negation cues scope identification, such
as Morante and Daelemans (2009a, 2009b), which apply a supervised sequence labeling
approach, or O?zgu?r and Radev (2009) and Velldal, Ovrelid, and Oepen (2010), which
293
Computational Linguistics Volume 38, Number 2
combine supervised learning techniques with rule-based systems exploiting syntactic
patterns.
Identifying modality and polarity cues and their scope is certainly a key aspect for
determining the degree of factuality of events, but not sufficient if the values result-
ing from these cues and their interactions are not provided. Complementary to this
perspective, the second approach to factuality-related information puts the emphasis
on identifying speculative degrees (along the lines assumed in this article). Pioneering
work within this view is Light, Qiu, and Srinivasan (2004), a paper exploring the use of
speculative language in sentences from Medline abstracts. It experiments with a hand-
crafted list of hedge cues as well as a supervised SVM in order to classify sentences as
either certain, high, or low speculative. Drawing on this, Medlock and Briscoe (2007)
address the classification of sentences into speculative or non-speculative as a weakly
supervised machine learning task and perform experiments with SVMs, achieving a
precision-recall breakeven point of 0.76. This line of research is further explored by
Szarvas (2008). On the other hand, Shatkay et al (2008) use the corpus developed by
Wilbur, Rzhetsky, and Shatkay (2006) to explore machine learning classifiers for tagging
data along the five dimensions in which it is marked up, including polarity and degrees
of certainty. It is a challenging task in that it involves simultaneous multi-dimensional
classification and, in some dimensions also, multi-label tagging. They experiment with
SVMs andMaximum Entropy classifiers, and report very good results (macro-averaged
F1 of 0.71 for degrees of certainty and 0.97 for polarity).
Resourcing to rich linguistic information. As argued throughout the article, subordination
structures play a crucial role in determining the factuality values of events as well as
their relevant sources, but most of the work presented so far addresses the problem of
event factuality identification by means of classifiers fed with linguistic features that are
not fully sensitive to sentences? structural depth and the complex interactions among
their constituents. Previous work using subordination syntax to model factuality is the
tool for identifying polarity and modality using lexical information and subordinating
contexts by Saur??, Verhagen, and Pustejovsky (2006). Similarly, Kilicoglu and Bergler
(2008) use the data from Medlock and Briscoe (2007) to show the effectiveness of
lexically centered syntactic patterns for distinguishing between speculative and non-
speculative sentences.
These systems are, however, limited in that they neither account for the effect of
multiple embeddings, nor distinguish between different sources. To our knowledge, the
first system in which factuality-related information is computed applying top?down
on a dependency tree, and hence potentially overcoming these limitations, is Nairn,
Condoravdi, and Karttunen (2006), who model the percolation of the polarity feature
down the syntactic structure. A somewhat comparable perspective is adopted in the
work on sentiment analysis addressing the problem from a compositional perspective.
For example, in Moilanen and Pulman (2007) and Moilanen, Pulman, and Zhang (2010)
the well-known semantics principle of compositionality is applied for sentiment po-
larity classification at the (sub)sentence level, and in Neviarouskaya, Prendinger, and
Ishizuka (2009), for recognizing emotions such as anger, guilt, or joy. All these cases
involve the use of deep parsing and rich lexicons in a way very similar to the model
presented here for event factuality. The main difference with respect to our approach,
however, is that De Facto applies top?down, whereas these systems follow a bottom?
up processing of the data, as determined by the principle of compositionality. Such
difference is not trivial. A top?down approach allows to keep track and compute the
294
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
nesting of the different sources involved in the factuality assessment, a computation
that does not follow naturally from processing bottom?up.
Factuality information according to their sources. A common feature in all the approaches
mentioned so far is the lack of awareness of the role of information sources. The funda-
mental role of source participants is already acknowledged in previous work on opinion
and perspective (most significantly, Wiebe, Wilson, and Cardie [2005]). Concerning
factuality-related information, the work incorporating the parameter of sources in the
computation is pretty recent. It is acknowledged in Diab et al (2009b) and Prabhakaran,
Rambow, and Diab (2010), who nevertheless explore only the feasibility of identifying
the committed beliefs of the text author, as annotated in the Language Understanding
Annotation Corpus (Diab et al 2009a), by means of SVM classifiers, in the first case
with basic linguistic features whereas in the second one incorporating dependency-
based features, reaching a maximum overall F1 of 53.97 and 64.0, respectively. The
distinction of event factuality depending on sources is also present in the corpus
presented by Nawaz, Thompson, and Ananiadou (2010), who differentiate between
current (i.e., the author) or other. Nevertheless, no system has yet been built based on
these data.
Factuality distinctions in the different systems. Determining the factuality value has gen-
erally been approached as a classification problem, but there is no agreement in the
literature on what the classes should be. In assuming a three-fold distinction of values
along the certainty axis (certain, probable, possible), our model takes a middle path
between other proposals in the NLP literature that only differentiate between certain
and uncertain (e.g., Medlock and Briscoe [2007] and its subsequent work, or Diab et al
[2009b]) and approaches that distinguish among four (e.g., Henriksson and Velupillai
2010) or even five degrees (Rubin 2007, 2010). As a matter of fact, our linguistic-based
distinctions are shared with the approach in Wilbur, Rzhetsky, and Shatkay (2006),
the GENIA corpus (Kim, Ohta, and Tsujii 2008) and, in particular, that in Nawaz,
Thompson, and Ananiadou (2010).
7. Final Considerations
Knowing the factuality status of event mentions in discourse is important for any
NLP task involving some degree of text understanding, but its identification presents
challenges at different levels of analysis. First, we conceive event factuality as a con-
tinuum, but a discrete scale appears to be a better approach for its automatic iden-
tification. Second, the way language expresses the factuality of situations is complex
because it involves multiple contributing and interrelating factors. And finally, the
factuality of an event is always relative to the author but often involves other sources
as well.
In this article, we put forward a computational model of event factuality with the
aim of contributing to a better understanding of this level of speculation in language.
The model is based on the grammatical structuring of factuality in languages such
as English, and addresses the three aforementioned challenges. Specifically, it rests
upon a three-fold distinction of the factuality scale, it acknowledges the possibility of
different sources (with potentially contradictory views), and it is strongly grounded
on the information provided by linguistic operators (including polarity and modality
295
Computational Linguistics Volume 38, Number 2
particles, predicates of different types, and subordination constructions) together with
their cross-level interactions.
The model has been implemented into De Facto, a tool that takes dependency
trees as input and returns the factuality profiles of events in text. To the best of our
knowledge, it is the only system capable of identifying event factuality degrees paired
to all the relevant sources for each event. In order to better assess its results, we built a
baseline with SVMs following the state of the art in the area. We run De Facto on two
versions of the dependency parses: one with the dependency trees originally returned
by the parser, and another where dependency errors in subordination constructions had
been manually corrected. De Facto?s performance increases significantly when run on
the second one, thus proving that event factuality as modeled in our work is linguisti-
cally well-grounded. De Facto is not completely dependent on high-quality linguistic
data, however. Its performance even when run on the original dependency trees is
notably better than the baseline regarding the classes that are harder to identify, namely,
those involving negative polarity or some degree of uncertainty, therefore showing the
adequacy of De Facto as a component in a standard NLP pipeline as well.
De Facto has been implemented for English, and so the set of linguistic resources
informing it are specific to this language. Porting it to other close languages, however,
such as Romance or Germanic ones, is a feasible task. The conceptual distinctions of
certainty and polarity are shared across these languages, as well as the main linguistic
structures encoding factuality information and which are handled by De Facto, includ-
ing specific lexical types (e.g., reporting, presuppositional, or implicative predicates of
different kinds) and syntactic constructions (different structures of evidentiality such
as hearsay, perception or inference, conditional structures, etc.). Hence, the porting to
other languages would mainly involve a mapping of lexical entries.
Furthermore, given that most of these linguistic expressions are not domain-specific
but belong to the general structure of any given language, it seems plausible to believe
that the model can be applied to data from other domains, such as biomedicine, without
the burden of having to compile large amounts of annotated corpus for every new area
of knowledge. At most, it would involve enriching the set of hedging markers for each
domain. More support is needed, however, in order to confirm this claim.
On the other hand, such a highly linguistically based approach has its drawbacks as
well, because it suffers from limitations regarding its linguistic coverage (mainly syntac-
tic constructions), and its incapability to deal with ambiguity in natural language. These
are problems commonly shared with other work approaching tasks of sub-sentential
interpretation by means of linguistically heavy and resource intensive models.
All in all, De Facto can provide valuable information for different NLP tasks. For
example, it can be of great help in systems dedicated to identifying facts or tracking
rumors on news reports, detecting degrees of uncertainty in medical records, or recog-
nizing the different sources involved in reported situations. Similarly, event factuality
information can contribute, together with other semantic layers (e.g., dependency rela-
tions, semantic role labeling, or event and entity coreference), to the challenging task
of identifying textual entailment relations. In addition, any machine learning efforts
towards event factuality identification can both train over De Facto?s output, as well as
benefit from the lexical types and syntactic features it uses when considering options for
machine learning algorithm choice and feature engineering decisions. In other words,
we believe that the linguistically motivated model we propose here can, in addition
to provide actual information on natural language text, help us understand the phe-
nomenon of event factuality and complement data-driven approaches commonly used
in the field.
296
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
Acknowledgments
We are very grateful to Carlos
Rodr??guez-Penagos, Bernat Saur??,
Jordi Atserias, Guillem Masso?, Andreas
Kaltenbrunner, Toni Badia, Sabine Bergler,
and Marc Verhagen for their valuable
comments and helpful discussions.
We also want to thank our anonymous
reviewers for helping make this a much
better piece of work. All errors and mistakes
are responsibility of the authors. This work
was supported by an EU Marie Curie grant
to R. Saur??, PIRG04-GA-2008-239414.
References
ACE, 2008. Automatic Content Extraction.
English Annotation Guidelines for Relations.
Linguistic Data Consortium, version 6.0?
2008.01.07 edition. Available at http://
www.ldc.upenn.edu/Projects/ACE/.
Aikhenvald, Alexandra Y. 2004. Evidentiality.
Oxford University Press, Oxford.
Asher, Nicholas. 1993. Reference to Abstract
Objects in English. Kluwer Academic Press,
Dordrecht.
Bach, Kent and Robert M. Harnish. 1979.
Linguistic Communication and Speech Acts.
The MIT Press, Cambridge, MA.
Biber, Douglas and Edward Finegan. 1989.
Styles of stance in English: Lexical and
grammatical marking of evidentiality
and affect. Text, 9(1):93?124.
Chafe, Wallace. 1986. Evidentiality in English
conversation and academic writing. In W.
Chafe and J. Nichols, editors, Evidentiality:
The Linguistic Coding of Epistemology. Ablex
Publishing Corporation, Norwood, NJ.
Dalianis, Hercules and Maria Skeppstedt.
2010. Creating and evaluating a consensus
for negated and speculative words in a
Swedish clinical corpus. In Proceedings of
the Workshop on Negation and Speculation in
Natural Language Processing, pages 5?13,
Uppsala, Sweden.
de Haan, Ferdinand. 1997. The Interaction of
Modality and Negation: a Typological Study.
Garland, New York.
de Marneffe, Marie-Catherine, Bill
MacCartney, and Christopher D. Manning.
2006. Generating typed dependency parses
from phrase structure parses. In Proceedings
of LREC 2006, pages 449?454, Genoa, Italy.
Diab, Mona, Bonnie Dorr, Lori Levin, Teruko
Mitamura, Rebecca Passonneau, Owen
Rambow, and Lance Ramshaw. 2009a.
Language Understanding Annotation Corpus.
Linguistic Data Consortium, Philadelphia,
PA. LDC2009T10.
Diab, Mona T., Lori Levin, Teruko
Mitamura, Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009b.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, ACL-IJNLP?09,
pages 68?73, Suntec, Singapore.
Dor, Daniel. 1995. Representations,
Attitudes and Factivity Evaluations. An
Epistemically-based Analysis of Lexical
Selection. Ph.D. thesis, Stanford University.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the 14th CoNLL Conference ? Shared Task,
pages 1?12, Uppsala, Sweden.
Geurts, Bart. 1998. Presuppositions and
anaphors in attitude contexts. Linguistics
and Philosophy, 21:545?601.
Givo?n, Talmy. 1993. English Grammar.
A Function-Based Introduction.
John Benjamins, Amsterdam.
Glanzberg, Michael. 2003. Felicity and
presupposition triggers. In University
of Michigan Workshop in Philosophy and
Linguistics, Michigan.
Halliday, M. A. K. and Christian M. I. M.
Matthiessen. 2004. An Introduction to
Functional Grammar. Hodder Arnold,
London.
Henriksson, Aron and Sumithra Velupillai.
2010. Levels of certainty in knowledge-
intensive corpora: An initial annotation
study. In Proceedings of the Workshop on
Negation and Speculation in Natural
Language Processing, pages 41?45,
Uppsala, Sweden.
Hickl, Andrew and Jeremy Bensley.
2007. A discourse commitment-based
framework for recognizing textual
entailment. In Proceedings of the Workshop
on Textual Entailment and Paraphrasing,
pages 171?176, Prague.
Hooper, Joan B. 1975. On assertive
predicates. In J. Kimball, editor, Syntax and
Semantics, IV. Academic Press, New York,
pages 91?124.
Horn, Laurence R. 1972. On the Semantic
Properties of Logical Operators in English.
Ph.D. thesis, UCLA. Distributed by the
Indiana University Linguistics Club, 1976.
Horn, Laurence R. 1989. A Natural History of
Negation. University of Chicago Press,
Chicago, IL.
Hyland, Ken. 1996. Writing without
conviction? Hedging in science research
articles. Applied Linguistics, 14(4):433?454.
297
Computational Linguistics Volume 38, Number 2
Karttunen, Lauri. 1970. Implicative verbs.
Language, 47:340?358.
Karttunen, Lauri and Annie Zaenen. 2005.
Veridicity. In G. Katz, J. Pustejovsky,
and F. Schilder, editors, Dagstuhl Seminar
Proceedings, Schloss Dagstuhl, link:
http://drops.dagstuhl.de/opus/
volltexte/2005/314/pdf/05151.
KarttunenLauri.Paper.314.pdf.
Kiefer, Ferenc. 1987. On defining modality.
Folia Linguistica, XXI:67?94.
Kilicoglu, Halil and Sabine Bergler. 2008.
Recognizing speculative language
in biomedical research articles: A
linguistically motivated perspective.
BMC Bioinformatics, 9(Suppl 11):S10.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP?09
shared task on event extraction. In
Proceedings of the Workshop on Current
Trends in Biomedical Natural Language
Processing: Shared Task, pages 1?9.
Boulder, Colorado, USA.
Kim, Jin-Dong, Tomoko Ohta, and Jun?ichi
Tsujii. 2008. Corpus annotation for mining
biomedical events from literature. BMC
Bioinformatics, 9(1):10.
Kiparsky, Paul and Carol Kiparsky. 1970.
Fact. In M. Bierwisch and K. E. Heidolph,
editors, Progress in Linguistics. A Collection
of Papers. Mouton, The Hague, Paris,
pages 143?173.
Kratzer, Angelika. 1991. Modality. In
A. van Stechow and D. Wunderlich,
editors, Semantik: Ein internationales
Handbuch der zeitgenoessischen
Forschung. Walter de Gruyter, Berlin,
pages 639?650.
Kudo, Taku and Yuji Matsumoto. 2000.
Use of support vector learning for chunk
identification. In Proceedings of CoNLL-2000
and LLL-2000, pages 142?144, Lisbon,
Portugal.
Lakoff, George. 1973. Hedges: A study in
meaning criteria and the logic of fuzzy
concepts. Journal of Philosophical Logic,
2(4):458?508.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In BioLINK 2004:
Linking Biological Literature, Ontologies,
and Databases, pages 17?24, Boston,
Massachusetts, USA.
Lyons, John. 1977. Semantics. Cambridge
University Press, Cambridge.
Martin, James R. and Peter R. R. White. 2005.
Language of Evaluation: Appraisal in English.
London and New York: Palgrave
Macmillan.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature.
In Proceedings of the 45th ACL,
pages 992?999, Prague, Czech Republic.
Moilanen, Karo and Stephen Pulman. 2007.
Sentiment composition. In Proceedings of
the RANLP, pages 27?29, Borovets,
Bulgaria.
Moilanen, Karo, Stephen Pulman, and Yue
Zhang. 2010. Packed feelings and ordered
sentiments: Sentiment parsing with
quasi-compositional polarity sequencing
and compression. In Proceedings of the 1st
Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA
2010), pages 36?43, Alacant, Spain.
Morante, Roser and Walter Daelemans.
2009a. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
Workshop on Current Trends in Biomedical
Natural Language Processing, pages 28?36,
Boulder, Colorado, USA.
Morante, Roser and Walter Daelemans.
2009b. A metalearning approach to
processing the scope of negation. In
Proceedings of the Thirteenth Conference on
Computational Natural Language Learning,
pages 21?29, Boulder, Colorado, USA.
Mushin, Ilana. 2001. Evidentiality and
Epistemological Stance. John Benjamin,
Philadelphia, PA.
Nairn, Rowan, Cleo Condoravdi, and
Lauri Karttunen. 2006. Computing
relative polarity for textual inference.
In Inference in Computational Semantics,
ICoS-5, pages 67?76, Buxton, England.
Nawaz, Raheel, Paul Thompson, and
Sophia Ananiadou. 2010. Evaluating a
meta-knowledge annotation scheme for
bio-events. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 69?77,
Uppsala, Sweden.
Neviarouskaya, Alena, Helmut
Prendinger, and Mitsuru Ishizuka. 2009.
Compositionality principle in recognition
of fine-grained emotions from text.
In Proceedings of the 3rd International
ICWSM Conference, pages 278?281,
San Jose, California, USA.
Ohta, Tomoko, Jin-Dong Kim, and
Jun?ichi Tsuji. 2007. Guidelines for
event annotation. University of Tokyo,
link: http://www-tsujii.is.s.u-
tokyo.ac.jp/?genia/release/
Genia event annotation guidelines.pdf.
298
Saur?? and Pustejovsky Assessing the Factuality Degree of Events in Text
O?zgu?r, Arzucan and Dragomir Radev. 2009.
Detecting speculations and their scopes in
scientific text. In Proceedings of the 2009
EMNLP Conference, pages 1398?1407,
Suntec, Singapore.
Palmer, Frank R. 1986.Mood and Modality.
Cambridge University Press, Cambridge.
Polanyi, Livia and Annie Zaenen. 2006.
Contextual valence shifters. In W. B. Croft,
J. Shanahan, Y. Qu, and J. Wiebe, editors,
Computing Attitude and Affect in Text:
Theory and Applications, volume 20.
Springer-Verlag, New York, pages 1?10.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In CoLing
2010. Poster Volume, pages 1014?1022,
Beijing, China.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Aravind Joshi, and Bonnie Webber. 2007.
Attribution and its annotation in the
Penn Discourse Treebank. Traitement
Automatique des Langues, 47(2):43?64.
Pustejovsky, James, Bob Knippen, Jessica
Littman, and Roser Saur??. 2005. Temporal
and event information in natural language
text. Language Resources and Evaluation,
39(2):123?164.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
TimeBank 1.2. Linguistic Data Consortium,
Philadelphia, PA. LDC2006T08.
Rizomilioti, Vassiliki. 2006. Exploring
epistemic modality in academic discourse
using corpora. In E. Arno? Macia`, A. Soler
Cervera, and C. Rueda Ramos, editors,
Information Technology in Languages for
Specific Purposes, volume 7. Springer,
Berlin, pages 53?71.
Rubin, Victoria L. 2006. Identifying Certainty
in Texts. Ph.D. thesis, Syracuse University.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt: Intercoder
reliability results for manual annotation
of epistemically modalized statements.
In Proceedings of the NAACL-HLT 2007,
pages 141?144, Rochester, NY.
Rubin, Victoria L. 2010. Epistemic
modality: From uncertainty to certainty
in the context of information seeking
as interactions with texts. Information
Processing and Management, 46:533?540.
Saur??, Roser. 2008. A Factuality Profiler for
Eventualities in Text. Ph.D. thesis,
Brandeis University.
Saur??, Roser and James Pustejovsky. 2007.
Determining modality and factuality for
text entailment. In Proceedings of the First
IEEE International Conference on Semantic
Computing, pages 509?516, Irvine,
California, USA.
Saur??, Roser and James Pustejovsky. 2009a.
FactBank 1.0. Linguistic Data Consortium,
Philadelphia, PA. LDC2009T23.
Saur??, Roser and James Pustejovsky. 2009b.
FactBank. A corpus annotated with event
factuality. Language Resources and
Evaluation, 43:227?268.
Saur??, Roser, Marc Verhagen, and James
Pustejovsky. 2006. SlinkET: A partial
modal parser for events. In Proceedings
of LREC 2006, pages 1332?1337,
Genoa, Italy.
Shatkay, Hagit, Fengxia Pang, Andrey
Rzhetsky, and W. John Wilbur. 2008.
Multi-dimensional classification of
biomedical text: Toward automated,
practical provision of high-utility text
to diverse users. Bioinformatics,
24:2086?2093.
Szarvas, Gyorgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords.
In ACL 08: HLT, pages 281?289,
Columbus, Ohio, USA.
van Valin, Robert D. and Randy J. LaPolla.
1997. Syntax. Structure, Meaning and
Function. Cambridge University Press,
Cambridge.
Velldal, Erik, Lilja Ovrelid, and Stephan
Oepen. 2010. Resolving speculation:
Maxent cue classification and
dependency-based scope rules.
In Proceedings of the 14th CoNLL:
Shared Task, pages 48?55, Uppsala,
Sweden.
Vincze, Veronika, Gyo?rgy Szarvas, Richa?rd
Farkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008. The BioScope corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics,
9(Suppl 11):S9.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2):165?210.
Wilbur, W. John, Andrey Rzhetsky, and
Hagit Shatkay. 2006. New directions in
biomedical text annotation: definitions,
guidelines and corpus construction.
BMC Bioinformatics, 7(1):356?365.
299
Tutorial Abstracts of ACL 2012, page 1,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Qualitative Modeling of Spatial Prepositions and Motion Expressions
Inderjeet Mani
Children?s Organization of
Southeast Asia
Thailand
inderjeet.mani@gmail.com
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, MA USA
jamesp@cs.brandeis.edu
The ability to understand spatial prepositions and
motion in natural language will enable a variety of
new applications involving systems that can respond
to verbal directions, map travel guides, display in-
cident reports, etc., providing for enhanced infor-
mation extraction, question-answering, information
retrieval, and more principled text to scene render-
ing. Until now, however, the semantics of spatial re-
lations and motion verbs has been highly problem-
atic. This tutorial presents a new approach to the
semantics of spatial descriptions and motion expres-
sions based on linguistically interpreted qualitative
reasoning. Our approach allows for formal inference
from spatial descriptions in natural language, while
leveraging annotation schemes for time, space, and
motion, along with machine learning from annotated
corpora. We introduce a compositional semantics
for motion expressions that integrates spatial primi-
tives drawn from qualitative calculi.
No previous exposure to the semantics of spatial
prepositions or motion verbs is assumed. The tu-
torial will sharpen cross-linguistic intuitions about
the interpretation of spatial prepositions and mo-
tion constructions. The attendees will also learn
about qualitative reasoning schemes for static and
dynamic spatial information, as well as three annota-
tion schemes: TimeML, SpatialML, and ISO-Space,
for time, space, and motion, respectively.
While both cognitive and formal linguistics have
examined the meaning of motion verbs and spatial
prepositions, these earlier approaches do not yield
precise computable representations that are expres-
sive enough for natural languages. However, the
previous literature makes it clear that communica-
tion of motion relies on imprecise and highly ab-
stract geometric descriptions, rather than Euclidean
ones that specify the coordinates and shapes of ev-
ery object. This property makes these expressions
a fit target for the field of qualitative spatial reason-
ing in AI, which has developed a rich set of geomet-
ric primitives for representing time, space (including
distance, orientation, and topological relations), and
motion. The results of such research have yielded a
wide variety of spatial and temporal reasoning logics
and tools. By reviewing these calculi and resources,
this tutorial aims to systematically connect qualita-
tive reasoning to natural language.
Tutorial Schedule:
I. Introduction. i. Overview of geometric idealiza-
tions underlying spatial PPs; ii. Linguistic patterns
of motion verbs across languages; iii. A qualita-
tive model for static spatial descriptions and for path
verbs; iv. Overview of relevant annotation schemes.
II. Calculi for Qualitative Spatial Reasoning. i.
Semantics of spatial PPs mapped to qualitative spa-
tial reasoning; ii. Qualitative calculi for representing
topological and orientation relations; iii. Qualitative
calculi to represent motion.
III. Semantics of Motion Expressions. i. Introduc-
tion to Dynamic Interval Temporal Logic (DITL); ii.
DITL representations for manner-of-motion verbs
and path verbs; iii. Compositional semantics for mo-
tion expressions in DITL, with the spatial primitives
drawn from qualitative calculi.
IV. Applications and Research Topics. i. Route
navigation, mapping travel narratives, QA, scene
rendering from text, and generating event descrip-
tions; ii. Open issues and further research topics.
1
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 27?32,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 7: Argument Selection and Coercion
James Pustejovsky and Anna Rumshisky and Alex Plotnick
Dept. of Computer Science
Brandeis University
Waltham, MA, USA
Elisabetta Jezek
Dept. of Linguistics
University of Pavia
Pavia, Italy
Olga Batiukova
Dept. of Humanities
Carlos III University of Madrid
Madrid, Spain
Valeria Quochi
ILC-CNR
Pisa, Italy
Abstract
We describe the Argument Selection and
Coercion task for the SemEval-2010 eval-
uation exercise. This task involves char-
acterizing the type of compositional oper-
ation that exists between a predicate and
the arguments it selects. Specifically, the
goal is to identify whether the type that
a verb selects is satisfied directly by the
argument, or whether the argument must
change type to satisfy the verb typing. We
discuss the problem in detail, describe the
data preparation for the task, and analyze
the results of the submissions.
1 Introduction
In recent years, a number of annotation schemes
that encode semantic information have been de-
veloped and used to produce data sets for training
machine learning algorithms. Semantic markup
schemes that have focused on annotating entity
types and, more generally, word senses, have
been extended to include semantic relationships
between sentence elements, such as the seman-
tic role (or label) assigned to the argument by the
predicate (Palmer et al, 2005; Ruppenhofer et al,
2006; Kipper, 2005; Burchardt et al, 2006; Subi-
rats, 2004).
In this task, we take this one step further and
attempt to capture the ?compositional history? of
the argument selection relative to the predicate. In
particular, this task attempts to identify the oper-
ations of type adjustment induced by a predicate
over its arguments when they do not match its se-
lectional properties. The task is defined as fol-
lows: for each argument of a predicate, identify
whether the entity in that argument position satis-
fies the type expected by the predicate. If not, then
identify how the entity in that position satisfies the
typing expected by the predicate; that is, identify
the source and target types in a type-shifting or co-
ercion operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition, as in (1a). Notice, however, that through
a metonymic interpretation, this constraint can be
violated, as demonstrated in (1b).
(1) a. John reported in late from Washington.
b. Washington reported in late.
Neither the surface annotation of entity extents
and types nor assigning semantic roles associated
with the predicate would reflect in this case a cru-
cial point: namely, that in order for the typing
requirements of the predicate to be satisfied, a
type coercion or a metonymy (Hobbs et al, 1993;
Pustejovsky, 1991; Nunberg, 1979; Egg, 2005)
has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This
task involved two types with their metonymic
variants: categories-for-locations (e.g., place-
for-people) and categories-for-organizations (e.g.,
organization-for-members). One of the limitations
of this approach, however, is that while appropri-
ate for these specialized metonymy relations, the
annotation specification and resulting corpus are
not an informative guide for extending the annota-
tion of argument selection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (2) below, the sense annotation
for the verb enjoy should arguably assign similar
values to both (2a) and (2b).
27
Figure 1: The MATTER Methodology
(2) a. Mary enjoyed drinking her beer.
b. Mary enjoyed her beer.
The consequence of this is that under current sense
and role annotation strategies, the mapping to a
syntactic realization for a given sense is made
more complex, and is in fact perplexing for a clus-
tering or learning algorithm operating over subcat-
egorization types for the verb.
2 Methodology of Annotation
Before introducing the specifics of the argument
selection and coercion task, we will briefly review
our assumptions regarding the role of annotation
in computational linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough
to capture the desired behavior. These linguistic
descriptions are typically distilled from extensive
theoretical modeling of the phenomenon. The de-
scriptions in turn form the basis for the annota-
tion values of the specification language, which
are themselves the features used in a development
cycle for training and testing a labeling algorithm
over a text. Finally, based on an analysis and eval-
uation of the performance of a system, the model
of the phenomenon may be revised.
We call this cycle of development the MATTER
methodology (Fig. 1):
Model: Structural descriptions provide theoretically in-
formed attributes derived from empirical observations
over the data;
Annotate: Annotation scheme assumes a feature set that en-
codes specific structural descriptions and properties of
the input data;
Train: Algorithm is trained over a corpus annotated with the
target feature set;
Test: Algorithm is tested against held-out data;
Evaluate: Standardized evaluation of results;
Revise: Revisit the model, annotation specification, or algo-
rithm, in order to make the annotation more robust and
reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cy-
cle include PropBank (Palmer et al, 2005), Nom-
Bank (Meyers et al, 2004), and TimeBank (Puste-
jovsky et al, 2005).
3 Task Description
The argument selection and coercion (ASC) task
involves identifying the selectional mechanism
used by the predicate over a particular argument.
1
For the purposes of this task, the possible relations
between the predicate and a given argument are re-
stricted to selection and coercion. In selection, the
argument NP satisfies the typing requirements of
the predicate, as in (3):
(3) a. The spokesman denied the statement (PROPOSI-
TION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn?t believe the rumor (PROPOSI-
TION).
Coercion occurs when a type-shifting operation
must be performed on the complement NP in order
to satisfy selectional requirements of the predicate,
as in (4). Note that coercion operations may apply
to any argument position in a sentence, including
the subject, as seen in (4b). Coercion can also be
seen as an object of a proposition, as in (4c).
(4) a. The president denied the attack (EVENT? PROPO-
SITION).
b. The White House (LOCATION ? HUMAN) denied
this statement.
c. The Boston office called with an update (EVENT?
INFO).
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve (1) identifying the verb sense and the asso-
ciated syntactic frame, (2) identifying selectional
requirements imposed by that verb sense on the
target argument, and (3) identifying the semantic
type of the target argument.
4 Resources and Corpus Development
We prepared the data for this task in two phases:
the data set construction phase and the annotation
phase (see Fig. 2). The first phase consisted of
(1) selecting the target verbs to be annotated and
compiling a sense inventory for each target, and
(2) data extraction and preprocessing. The pre-
pared data was then loaded into the annotation in-
terface. During the annotation phase, the annota-
tion judgments were entered into the database, and
an adjudicator resolved disagreements. The result-
ing database was then exported in an XML format.
1
This task is part of a larger effort to annotate text with
compositional operations (Pustejovsky et al, 2009).
28
Figure 2: Corpus Development Architecture
4.1 Data Set Construction Phase: English
For the English data set, the data construction
phase was combined with the annotation phase.
The data for the task was created using the fol-
lowing steps:
1. The verbs were selected by examining the data
from the BNC, using the Sketch Engine (Kilgar-
riff et al, 2004) as described in (Rumshisky and
Batiukova, 2008). Verbs that consistently im-
pose semantic typing on one of their arguments
in at least one of their senses (strongly coercive
verbs) were included into the final data set: ar-
rive (at), cancel, deny, finish, and hear.
2. Sense inventories were compiled for each verb,
with the senses mapped to OntoNotes (Pradhan
et al, 2007) whenever possible. For each sense,
a set of type templates was compiled using a
modification of the CPA technique (Hanks and
Pustejovsky, 2005; Pustejovsky et al, 2004):
every argument in the syntactic pattern asso-
ciated with a given sense was assigned a type
specification. Although a particular sense is
often compatible with more than one semantic
type for a given argument, this was never the
case in our data set, where no disjoint types
were tested. The coercive senses of the chosen
verbs were associated with the following type
templates:
a. Arrive (at), sense reach a destination or goal : HU-
MAN arrive at LOCATION
b. Cancel, sense call off : HUMAN cancel EVENT
c. Deny, sense state or maintain that something is un-
true: HUMAN deny PROPOSITION
d. Finish, sense complete an activity: HUMAN finish
EVENT
e. Hear, sense perceive physical sound : HUMAN hear
SOUND
We used a subset of semantic types from the
Brandeis Shallow Ontology (BSO), which is a
shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky
et al, 2004; Rumshisky et al, 2006). Types
were selected for their prevalence in manually
identified selection context patterns developed
for several hundred English verbs. That is,
they capture common semantic distinctions as-
sociated with the selectional properties of many
verbs. The types used for annotation were:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
DOCUMENT,DRINK,EMOTION,ENTITY,EVENT, FOOD,
HUMAN,HUMAN GROUP, IDEA, INFORMATION, LOCA-
TION,OBLIGATION,ORGANIZATION, PATH, PHYSICAL
OBJECT, PROPERTY, PROPOSITION,RULE, SENSATION,
SOUND, SUBSTANCE, TIME PERIOD, VEHICLE
This set of types is purposefully shallow and
non-hierarchical. For example, HUMAN is a
subtype of both ANIMATE and PHYSICAL OB-
JECT, but annotators and system developers
were instructed to choose the most relevant type
(e.g., HUMAN) and to ignore inheritance.
3. A set of sentences was randomly extracted for
each target verb from the BNC (Burnard, 1995).
The extracted sentences were parsed automati-
cally, and the sentences organized according to
the grammatical relation the target verb was in-
volved in. Sentences were excluded from the set
if the target argument was expressed as anaphor,
or was not present in the sentence. The seman-
tic head for the target grammatical relation was
identified in each case.
4. Word sense disambiguation of the target predi-
cate was performed manually on each extracted
sentence, matching the target against the sense
inventory and the corresponding type templates
as described above. The appropriate senses
were then saved into the database along with the
associated type template.
5. The sentences containing coercive senses of the
target verbs were loaded into the Brandeis An-
notation Tool (Verhagen, 2010). Annotators
were presented with a list of sentences and
asked to determine whether the argument in
the specified grammatical relation to the target
belongs to the type associated with that sense
in the corresponding template. Disagreements
were resolved by adjudication.
29
Coerion Type Verb Train Test
EVENT?LOCATION arrive at 38 37
ARTIFACT?EVENT cancel 35 35
finish 91 92
EVENT?PROPOSITION deny 56 54
ARTIFACT?SOUND hear 28 30
EVENT?SOUND hear 24 26
DOCUMENT?EVENT finish 39 40
Table 1: Coercions in the English data set
6. To guarantee robustness of the data, two addi-
tional steps were taken. First, only the six most
recurrent coercion types were selected; these
are given in table 1. Preference was given to
cross-domain coercions, where the source and
the target types are not related ontologically.
Second, the distribution of selection and co-
ercion instances were skewed to increase the
number of coercions. The final English data set
contains about 30% coercions.
7. Finally, the data set was randomly split in half
into a training set and a test set. The training
data has 1032 instances, 311 of which are co-
ercions, and the test data has 1039 instances,
314 of which are coercions.
4.2 Data Set Construction Phase: Italian
In constructing the Italian data set, we adopted the
same methodology used for the English data set,
with the following differences:
1. The list of coercive verbs was selected by exam-
ining data from the ItWaC (Baroni and Kilgar-
riff, 2006) using the Sketch Engine (Kilgarriff
et al, 2004):
accusare ?accuse?, annunciare ?announce?, arrivare ?ar-
rive?, ascoltare ?listen?, avvisare ?inform?, chiamare
?call?, cominciare ?begin?, completare ?complete?, con-
cludere ?conclude?, contattare ?contact?, divorare ?de-
vour?, echeggiare ?echo?, finire ?finish?, informare ?in-
form?, interrompere ?interrupt?, leggere ?read?, raggiun-
gere ?reach?, recar(si) ?go to?, rimbombare ?resound?,
sentire ?hear?, udire ?hear?, visitare ?visit?.
2. The coercive senses of the chosen verbs were
associated with type templates, some of which
are listed listed below. Whenever possible,
senses and type templates were adapted from
the Italian Pattern Dictionary (Hanks and Jezek,
2007) and mapped to their SIMPLE equiva-
lents (Lenci et al, 2000).
a. arrivare, sense reach a location: HUMAN arriva
[prep] LOCATION
b. cominciare, sense initiate an undertaking: HUMAN
comincia EVENT
c. completare, sense finish an activity: HUMAN com-
pleta EVENT
d. udire, sense perceive a sound : HUMAN ode SOUND
e. visitare, sense visit a place: HUMAN visita LOCA-
TION
The following types were used to annotate
the Italian dataset:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY,
EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, IN-
FORMATION, LIQUID, LOCATION, ORGANIZATION,
PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND,
TIME PERIOD, VEHICLE
The annotators were provided with a set of def-
initions and examples of each type.
3. A set of sentences for each target verb was ex-
tracted and parsed from the PAROLE sottoin-
sieme corpus (Bindi et al, 2000). They were
skimmed to ensure that the final data set con-
tained a sufficient number of coercions, with
proportionally more selections than coercions.
Sentences were preselected to include instances
representing one of the chosen senses.
4. In order to exclude instances that may have been
wrongly selected, a judge performed word sense
disambiguation of the target predicate in the ex-
tracted sentences.
5. Annotators were presented with a list of sen-
tences and asked to determine the usual seman-
tic type associated with the argument in the
specified grammatical relation. Every sentence
was annotated by two annotators and one judge,
who resolved disagreements.
6. Some of the coercion types selected for Italian
were:
a. LOCATION? HUMAN (accusare, annunciare)
b. ARTIFACT? HUMAN (annunciare, avvisare)
c. EVENT? LOCATION (arrivare, raggiungere)
d. ARTIFACT? EVENT (cominciare, completare)
e. EVENT? DOCUMENT (leggere, divorare)
f. HUMAN? DOCUMENT (leggere, divorare)
g. EVENT? SOUND (ascoltare, echeggiare)
h. ARTIFACT? SOUND (ascoltare, echeggiare)
7. The Italian training data contained 1466 in-
stances, 381 of which are coercions; the test
data had 1463 instances, with 384 coercions.
5 Data Format
The test and training data were provided in XML.
The relation between the predicate (viewed as
a function) and its argument were represented
by composition link elements (CompLink), as
30
shown below. The test data differed from the train-
ing data in the omission of CompLink elements.
In case of coercion, there is a mismatch between
the source and the target types, and both types
need to be identified; e.g., The State Department
repeatedly denied the attack:
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
the <TARGET id="t1">attack</TARGET>.
<CompLink cid="cid1"
compType="COERCION"
selector_id="s1"
relatedToTarget="t1"
sourceType="EVENT"
targetType="PROPOSITION"/>
When the compositional operation is selection,
the source and target types must match; e.g., The
State Department repeatedly denied the statement:
The State Department repeatedly
<SELECTOR sid="s2">denied</SELECTOR>
the <TARGET id="t2">statement</TARGET>.
<CompLink cid="cid2"
compType="SELECTION"
selector_id="s2"
relatedToTarget="t2"
sourceType="PROPOSITION"
targetType="PROPOSITION"/>
6 Results & Analysis
We received only a single submission for the
ASC task. The UTDMet system was an SVM-
based system with features derived from two main
sources: a PageRank-style algorithm over Word-
Net hypernyms used to define semantic classes,
and statistics from a PropBank-style parse of some
8 million documents from the English Gigaword
corpus. The results, shown in Table 2, were
computed from confusion matrices constructed for
each of four classification tasks for the 1039 link
instances in the English test data: determination
of argument selection or coercion, identification of
the argument source type, identification of the ar-
gument target type, and the joint identification of
the source/target type pair.
Clearly, the UTDMet system did quite well at
this task. The one immediately noticeable outlier
is the macro-averaged precision for the joint type,
which reflects a small number of miscategoriza-
tions of rare types. For example, eliminating the
single miscategorized ARTIFACT-LOCATION link
in the submitted test data bumps this score up to
a respectable 94%. This large discrepancy can ex-
plained by the lack of any coercions with those
types in the gold-standard data.
Prec. Recall Averaging
Selection vs. 95 96 (macro)
Coercion: 96 96 (micro)
Source Type: 96 96 (macro)
96 96 (micro)
Target Type: 100 100 (both)
Joint Type: 86 95 (macro)
96 96 (micro)
Table 2: Results for the UTDMet submission.
In the absence of any other submissions, it is
difficult to provide a point of comparison for this
performance. However, we can provide a base-
line by taking each link to be a selection whose
source and target types are the most common type
(EVENT for the gold-standard English data). This
yields micro-averaged precision scores of 69% for
selection vs. coercion, 33% for source type iden-
tification, 37% for the target type identification,
and 22% for the joint type.
The performance of the UTDMet system sug-
gests that most of the type coercions were identifi-
able based largely on examination of lexical clues
associated with selection contexts. This is in fact
to be expected for the type coercions that were the
focus of the English data set. It will be interesting
to see how systems perform on the Italian data set
and an expanded corpus for English and Italian,
where more subtle and complex type exploitations
and manipulations are at play. These will hope-
fully be explored in future competitions.
7 Conclusion
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2010. This
task involves identifying the relation between a
predicate and its argument as one that encodes
the compositional history of the selection process.
This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of
a predicate from those that are coerced in context.
We described some details of a specification lan-
guage for selection, the annotation task using this
specification to identify argument selection behav-
ior, and the preparation of the data for the task.
Finally, we analyzed the results of the task sub-
missions.
31
References
M. Baroni and A. Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In Proceedings of European ACL.
R. Bindi, P. Baroni, M. Monachini, and E. Gola. 2000.
PAROLE-Sottoinsieme. ILC-CNR Internal Report.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC,
Genoa, Italy.
L. Burnard, 1995. Users? Reference Guide, British Na-
tional Corpus. British National Corpus Consortium,
Oxford, England.
Marcus Egg. 2005. Flexible semantics for reinterpre-
tation phenomena. CSLI, Stanford.
P. Hanks and E. Jezek. 2007. Building Pattern Dictio-
naries with Corpus Analysis. In International Col-
loquium on Possible Dictionaries, Rome, June, 6-7.
Oral Presentation.
P. Hanks and J. Pustejovsky. 2005. A pattern dic-
tionary for natural language processing. Revue
Franc?aise de Linguistique Appliqu?ee.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpre-
tation as abduction. Artificial Intelligence, 63:69?
142.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105?116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, Univer-
sity of Pennsylvania, PA.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowski, I. Peters, W. Peters,
N. Ruimy, et al 2000. SIMPLE: A general frame-
work for the development of multilingual lexicons.
International Journal of Lexicography, 13(4):249.
K. Markert and M. Nissim. 2007. SemEval-2007
task 8: Metonymy resolution. In Eneko Agirre,
Llu??s M`arquez, and Richard Wicentowski, editors,
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Geoffrey Nunberg. 1979. The non-uniqueness of se-
mantic solutions: Polysemy. Linguistics and Phi-
losophy, 3:143?184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, E. Hovy, MS Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In
International Conference on Semantic Computing,
2007, pages 517?526.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924?931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123?164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Compu-
tational Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and
their effect on annotation. In COLING Workshop
on Human Judgement in Computational Linguistics
(HJCL-2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Carlos Subirats. 2004. FrameNet Espa?nol. Una red
sem?antica de marcos conceptuales. In VI Interna-
tional Congress of Hispanic Linguistics, Leipzig.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
32
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57?62,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 13: TempEval-2
Marc Verhagen
?
, Roser Saur??
?
, Tommaso Caselli
?
and James Pustejovsky
?
? Computer Science Department, Brandeis University, Massachusetts, USA
?Barcelona Media, Barcelona, Spain ? ILC-CNR, Pisa, Italy
marc@cs.brandeis.edu roser.sauri@barcelonamedia.org
tommaso.caselli@ilc.cnr.it jamesp@cs.brandeis.edu
Abstract
Tempeval-2 comprises evaluation tasks for
time expressions, events and temporal re-
lations, the latter of which was split up in
four sub tasks, motivated by the notion that
smaller subtasks would make both data
preparation and temporal relation extrac-
tion easier. Manually annotated data were
provided for six languages: Chinese, En-
glish, French, Italian, Korean and Spanish.
1 Introduction
The ultimate aim of temporal processing is the au-
tomatic identification of all temporal referring ex-
pressions, events and temporal relations within a
text. However, addressing this aim is beyond the
scope of an evaluation challenge and a more mod-
est approach is appropriate.
The 2007 SemEval task, TempEval-1 (Verhagen
et al, 2007; Verhagen et al, 2009), was an initial
evaluation exercise based on three limited tempo-
ral ordering and anchoring tasks that were consid-
ered realistic both from the perspective of assem-
bling resources for development and testing and
from the perspective of developing systems capa-
ble of addressing the tasks.
1
TempEval-2 is based on TempEval-1, but is
more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
In the rest of this paper, we first introduce the
data that we are dealing with. Which gets us in
a position to present the list of task introduced by
TempEval-2, including some motivation as to why
we feel that it is a good idea to split up temporal
relation classification into sub tasks. We proceed
by shortly describing the data resources and their
creation, followed by the performance of the sys-
tems that participated in the tasks.
1
The Semeval-2007 task was actually known simply as
TempEval, but here we use Tempeval-1 to avoid confusion.
2 TempEval Annotation
The TempEval annotation language is a simplified
version of TimeML.
2
using three TimeML tags:
TIMEX3, EVENT and TLINK.
TIMEX3 tags the time expressions in the text and
is identical to the TIMEX3 tag in TimeML. Times
can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following
example.
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80?s
e. later this afternoon
The two main attributes of the TIMEX3 tag are
TYPE and VAL, both shown in the example (2).
(2) November 22, 2004
type="DATE" val="2004-11-22"
For TempEval-2, we distinguish four temporal
types: TIME (at 2:45 p.m.), DATE (January 27,
1920, yesterday), DURATION (two weeks) and SET
(every Monday morning). The VAL attribute as-
sumes values according to an extension of the ISO
8601 standard, as enhanced by TIMEX2.
Each document has one special TIMEX3 tag,
the Document Creation Time (DCT), which is in-
terpreted as an interval that spans a whole day.
The EVENT tag is used to annotate those ele-
ments in a text that describe what is conventionally
referred to as an eventuality. Syntactically, events
are typically expressed as inflected verbs, although
event nominals, such as ?crash? in killed by the
crash, should also be annotated as EVENTS. The
most salient event attributes encode tense, aspect,
modality and polarity information. Examples of
some of these features are shown below:
2
See http://www.timeml.org for language speci-
fications and annotation guidelines
57
(3) should have bought
tense="PAST" aspect="PERFECTIVE"
modality="SHOULD" polarity="POS"
(4) did not teach
tense="PAST" aspect="NONE"
modality="NONE" polarity="NEG"
The relation types for the TimeML TLINK tag
form a fine-grained set based on James Allen?s
interval logic (Allen, 1983). For TempEval, the
set of labels was simplified to aid data preparation
and to reduce the complexity of the task. We use
only six relation types including the three core re-
lations BEFORE, AFTER, and OVERLAP, the two
less specific relations BEFORE-OR-OVERLAP and
OVERLAP-OR-AFTER for ambiguous cases, and fi-
nally the relation VAGUE for those cases where no
particular relation can be established.
Temporal relations come in two broad flavours:
anchorings of events to time expressions and or-
derings of events. Events can be anchored to an
adjacent time expression as in examples 5 and 6 or
to the document creation time as in 7.
(5) Mary taught
e1
on Tuesday morning
t1
OVERLAP(e1,t1)
(6) They cancelled the evening
t2
class
e2
OVERLAP(e2,t2)
(7) Most troops will leave
e1
Iraq by August of
2010. AFTER(e1,dct)
The country defaulted
e2
on debts for that en-
tire year. BEFORE(e2,dct)
In addition, events can be ordered relative to
other events, as in the examples below.
(8) The President spoke
e1
to the nation on
Tuesday on the financial crisis. He had
conferred
e2
with his cabinet regarding pol-
icy the day before. AFTER(e1,e2)
(9) The students heard
e1
a fire alarm
e2
.
OVERLAP(e1,e2)
(10) He said
e1
they had postponed
e2
the meeting.
AFTER(e1,e2)
3 TempEval-2 Tasks
We can now define the six TempEval tasks:
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL.
B. Determine the extent of the events in a text
as defined by the TimeML EVENT tag. In
addition, determine the value of the features
CLASS, TENSE, ASPECT, POLARITY, and
MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same
sentence. This task is further restricted by
requiring that either the event syntactically
dominates the time expression or the event
and time expression occur in the same noun
phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically domi-
nates the other event.
Of these tasks, C, D and E were also defined for
TempEval-1. However, the syntactic locality re-
striction in task C was not present in TempEval-1.
Task participants could choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants could choose
one or more of the six languages for which we pro-
vided data: Chinese, English, French, Italian, Ko-
rean, and Spanish.
We feel that well-defined tasks allow us to struc-
ture the workflow, allowing us to create task-
specific guidelines and using task-specific anno-
tation tools to speed up annotation. More im-
portantly, each task can be evaluated in a fairly
straightforward way, contrary to for example the
problems that pop up when evaluating two com-
plex temporal graphs for the same document. In
addition, tasks can be ranked, allowing systems to
feed the results of one (more precise) task as a fea-
ture into another task.
Splitting the task into substask reduces the error
rate in the manual annotation, and that merging
the different sub-task into a unique layer as a post-
processing operation (see figure 1) provides better
58
Figure 1: Merging Relations
and more reliable results (annotated data) than do-
ing a complex task all at once.
4 Data Preparation
The data for the five languages were prepared in-
dependently of each other and do not comprise a
parallel corpus. However, annotation specifica-
tions and guidelines for the five languages were
developed in conjunction with one other, in many
cases based on version 1.2.1 of the TimeML an-
notation guidelines for English
3
. Not all corpora
contained data for all six tasks. Table 1 gives the
size of the training set and the relation tasks that
were included.
language tokens C D E F X
Chinese 23,000 X X X X
English 63,000 X X X X
Italian 27,000 X X X
French 19,000 X
Korean 14,000
Spanish 68,000 X X
Table 1: Corpus size and relation tasks
All corpora include event and timex annota-
tion. The French corpus contained a subcorpus
with temporal relations but these relations were
not split into the four tasks C through F.
Annotation proceeded in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where
a judge resolves disagreements between the an-
notators. Most languages used BAT, the Brandeis
Annotation Tool (Verhagen, 2010), a generic web-
based annotation tool that is centered around the
notion of annotation tasks. With the task decom-
position allowed by BAT, it is possible to structure
the complex task of temporal annotation by split-
ting it up in as many sub tasks as seems useful. As
3
See http://www.timeml.org.
such, BAT was well-suited for TempEval-2 anno-
tation.
We now give a few more details on the English
and Spanish data, skipping the other languages for
reasons that will become obvious at the beginning
of section 6.
The English data sets were based on TimeBank
(Pustejovsky et al, 2003; Boguraev et al, 2007),
a hand-built gold standard of annotated texts us-
ing the TimeML markup scheme.
4
However, all
event annotation was reviewed to make sure that
the annotation complied with the latest guidelines
and all temporal relations were added according to
the Tempeval-2 relation tasks, using the specified
relation types.
The data released for the TempEval-2 Spanish
edition is a fragment of the Spanish TimeBank,
currently under development. Its documents are
originally from the Spanish part of the AnCora
corpus (Taul?e et al, 2008). Data preparation fol-
lowed the annotation guidelines created to deal
with the specificities of event and timex expres-
sions in Spanish (Saur?? et al, 2009a; Saur?? et al,
2009b).
5 Evaluation Metrics
For the extents of events and time expres-
sions (tasks A and B), precision, recall and the
f1-measure are used as evaluation metrics, using
the following formulas:
precision = tp/(tp + fp)
recall = tp/(tp + fn)
f -measure = 2 ? (P ? R)/(P + R)
Where tp is the number of tokens that are part
of an extent in both key and response, fp is the
number of tokens that are part of an extent in the
response but not in the key, and fn is the number
of tokens that are part of an extent in the key but
not in the response.
For attributes of events and time expressions
(the second part of tasks A and B) and for relation
types (tasks C through F) we use an even simpler
metric: the number of correct answers divided by
the number of answers.
4
See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog num-
ber LDC2006T08.
59
6 System Results
Eight teams participated in TempEval-2, submit-
ting a grand total of eighteen systems. Some of
these systems only participated in one or two tasks
while others participated in all tasks. The distribu-
tion over the six languages was very uneven: six-
teen systems for English, two for Spanish and one
for English and Spanish.
The results for task A, recognition and normal-
ization of time expressions, are given in tables 2
and 3.
team p r f type val
UC3M 0.90 0.87 0.88 0.91 0.83
TIPSem 0.95 0.87 0.91 0.91 0.78
TIPSem-B 0.97 0.81 0.88 0.99 0.75
Table 2: Task A results for Spanish
team p r f type val
Edinburgh 0.85 0.82 0.84 0.84 0.63
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
JU CSE 0.55 0.17 0.26 0.00 0.00
KUL 0.78 0.82 0.80 0.91 0.55
KUL Run 2 0.73 0.88 0.80 0.91 0.55
KUL Run 3 0.85 0.84 0.84 0.91 0.55
KUL Run 4 0.76 0.83 0.80 0.91 0.51
KUL Run 5 0.75 0.85 0.80 0.91 0.51
TERSEO 0.76 0.66 0.71 0.98 0.65
TIPSem 0.92 0.80 0.85 0.92 0.65
TIPSem-B 0.88 0.60 0.71 0.88 0.59
TRIOS 0.85 0.85 0.85 0.94 0.76
TRIPS 0.85 0.85 0.85 0.94 0.76
USFD2 0.84 0.79 0.82 0.90 0.17
Table 3: Task A results for English
The results for Spanish are more uniform and
generally higher than the results for English.
For Spanish, the f-measure for TIMEX3 extents
ranges from 0.88 through 0.91 with an average of
0.89; for English the f-measure ranges from 0.26
through 0.86, for an average of 0.78. However,
due to the small sample size it is hard to make
any generalizations. In both languages, type de-
tection clearly was a simpler task than determining
the value.
The results for task B, event recognition, are given
in tables 4 and 5. Both tables contain results for
both Spanish and English, the first part of each ta-
ble contains the results for Spanish and the next
part the results for English.
team p r f
TIPSem 0.90 0.86 0.88
TIPSem-B 0.92 0.85 0.88
team p r f
Edinburgh 0.75 0.85 0.80
JU CSE 0.48 0.56 0.52
TIPSem 0.81 0.86 0.83
TIPSem-B 0.83 0.81 0.82
TRIOS 0.80 0.74 0.77
TRIPS 0.55 0.88 0.68
Table 4: Event extent results
The column headers in table 5 are abbrevia-
tions for polarity (pol), mood (moo), modality
(mod), tense (tns), aspect (asp) and class (cl). Note
that the English team chose to include modality
whereas the Spanish team used mood.
team pol moo tns asp cl
TIPSem 0.92 0.80 0.96 0.89 0.66
TIPSem-B 0.92 0.79 0.96 0.89 0.66
team pol mod tns asp cl
Edinburgh 0.99 0.99 0.92 0.98 0.76
JU CSE 0.98 0.98 0.30 0.95 0.53
TIPSem 0.98 0.97 0.86 0.97 0.79
TIPSem-B 0.98 0.98 0.85 0.97 0.79
TRIOS 0.99 0.95 0.91 0.98 0.77
TRIPS 0.99 0.96 0.67 0.97 0.67
Table 5: Event attribute results
As with the time expressions results, the sample
size for Spanish is small, but note again the higher
f-measure for event extents in Spanish.
Table 6 shows the results for all relation tasks, with
the Spanish systems in the first two rows and the
English systems in the last six rows. Recall that for
Spanish the training and test sets only contained
data for tasks C and D.
Interestingly, the version of the TIPSem sys-
tems that were applied to the Spanish data did
much better on task C compared to its English
cousins, but much worse on task D, which is rather
puzzling.
Such a difference in performance of the systems
could be due to differences in annotation accurate-
ness, or it could be due to some particularities of
how the two languages express certain temporal
60
team C D E F
TIPSem 0.81 0.59 - -
TIPSem-B 0.81 0.59 - -
JU CSE 0.63 0.80 0.56 0.56
NCSU-indi 0.63 0.68 0.48 0.66
NCSU-joint 0.62 0.21 0.51 0.25
TIPSem 0.55 0.82 0.55 0.59
TIPSem-B 0.54 0.81 0.55 0.60
TRIOS 0.65 0.79 0.56 0.60
TRIPS 0.63 0.76 0.58 0.59
USFD2 0.63 - 0.45 -
Table 6: Results for relation tasks
aspects, or perhaps the one corpus is more ho-
mogeneous than the other. Again, there are not
enough data points, but the issue deserves further
attention.
For each task, the test data provided the event
pairs or event-timex pairs with the relation type
set to NONE and participating systems would re-
place that value with one of the six allowed rela-
tion types. However, participating systems were
allowed to not replace NONE and not be penalized
for it. Those cases would not be counted when
compiling the scores in table 6. Table 7 lists those
systems that did not classify all relation and the
percentage of relations for each task that those sys-
tems did not classify.
team C D E F
TRIOS 25% 19% 36% 31%
TRIPS 20% 10% 17% 10%
Table 7: Percentage not classified
A comparison with the Tempeval-1 results from
Semeval-2007 may be of interest. Six systems
participated in the TempEval-1 tasks, compared
to seven or eight systems for TempEval-2. Table
8 lists the average scores and the standard devi-
ations for all the tasks (on the English data) that
Tempeval-1 and Tempeval-2 have in common.
C D E
tempeval-1 average 0.59 0.76 0.51
stddev 0.03 0.03 0.05
tempeval-2 average 0.61 0.70 0.53
stddev 0.04 0.22 0.05
Table 8: Comparing Tempevals
The results are very similar except for task D,
but if we take a away the one outlier (the NCSU-
joint score of 0.21) then the average becomes 0.78
with a standard deviation of 0.05. However, we
had expected that for TempEval-2 the systems
would score better on task C since we added the
restriction that the event and time expression had
to be syntactically adjacent. It is not clear why the
results on task C have not improved.
7 Conclusion
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations be-
tween events and temporal expressions in text. Us-
ing a subset of TimeML temporal relations, we
show how temporal relations and anchorings can
be annotated and identified in six different lan-
guages. The markup language adopted presents
a descriptive framework with which to examine
the temporal aspects of natural language informa-
tion, demonstrating in particular, how tense and
temporal information is encoded in specific sen-
tences, and how temporal relations are encoded
between events and temporal expressions. This
work paves the way towards establishing a broad
and open standard metadata markup language for
natural language texts, examining events, tempo-
ral expressions, and their orderings.
One thing that would need to be addressed in
a follow-up task is what the optimal number of
tasks is. Tempeval-2 had six tasks, spread out over
six languages. This brought about some logisti-
cal challenges that delayed data delivery and may
have given rise to a situation where there was sim-
ply not enough time for many systems to properly
prepare. And clearly, the shared task was not suc-
cessful in attracting systems to four of the six lan-
guages.
8 Acknowledgements
Many people were involved in TempEval-2. We
want to express our gratitude to the following key
contributors: Nianwen Xue, Estela Saquete, Lo-
tus Goldberg, Seohyun Im, Andr?e Bittar, Nicoletta
Calzolari, Jessica Moszkowicz and Hyopil Shin.
Additional thanks to Joan Banach, Judith
Domingo, Pau Gim?enez, Jimena del Solar, Teresa
Su?nol, Allyson Ettinger, Sharon Spivak, Nahed
Abul-Hassan, Ari Abelman, John Polson, Alexan-
dra Nunez, Virginia Partridge, , Amber Stubbs,
Alex Plotnick, Yuping Zhou, Philippe Muller and
61
Irina Prodanof.
The work on the Spanish corpus was supported
by a EU Marie Curie International Reintegration
Grant (PIRG04-GA-2008-239414). Work on the
English corpus was supported under the NSF-CRI
grant 0551615, ?Towards a Comprehensive Lin-
guistic Annotation of Language? and the NSF-
INT-0753069 project ?Sustainable Interoperabil-
ity for Language Technology (SILT)?, funded by
the National Science Foundation.
Finally, thanks to all the participants, for stick-
ing with a task that was not always as flawless and
timely as it could have been in a perfect world.
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and
Marc Verhagen. 2007. Timebank evolution as a
community resource for timeml parsing. Language
Resource and Evaluation, 41(1):91?115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser
Saur??, Andrew See, Andrea Setzer, and Beth Sund-
heim. 2003. The TimeBank Corpus. Corpus Lin-
guistics, March.
Roser Saur??, Olga Batiukova, and James Pustejovsky.
2009a. Annotating events in spanish. timeml an-
notation guidelines. Technical Report Version
TempEval-2010., Barcelona Media - Innovation
Center.
Roser Saur??, Estela Saquete, and James Pustejovsky.
2009b. Annotating time expressions in spanish.
timeml annotation guidelines. Technical Report
Version TempEval-2010, Barcelona Media - Inno-
vation Center.
Mariona Taul?e, Toni Mart??, and Marta Recasens. 2008.
Ancora: Multilevel annotated corpora for catalan
and spanish. In Proceedings of the LREC 2008,
Marrakesh, Morocco.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proc. of the Fourth
Int. Workshop on Semantic Evaluations (SemEval-
2007), pages 75?80, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
62
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions,
Events, and Temporal Relations
Naushad UzZaman?1, Hector Llorens?1, Leon Derczynski?,
Marc Verhagen?, James Allen? and James Pustejovsky?
?: University of Rochester, USA; ?: University of Alicante, Spain
?: Department of Computer Science, University of Sheffield, UK
?: Computer Science Department, Brandeis University, USA
1: Nuance Communications
naushad@cs.rochester.edu, hllorens@dlsi.ua.es, leon@dcs.shef.ac.uk
Abstract
Within the SemEval-2013 evaluation exercise, the
TempEval-3 shared task aims to advance research
on temporal information processing. It follows on
from TempEval-1 and -2, with: a three-part struc-
ture covering temporal expression, event, and tem-
poral relation extraction; a larger dataset; and new
single measures to rank systems ? in each task and in
general. In this paper, we describe the participants?
approaches, results, and the observations from the
results, which may guide future research in this area.
1 Introduction
The TempEval task (Verhagen et al, 2009) was added as a
new task in SemEval-2007. The ultimate aim of research
in this area is the automatic identification of temporal ex-
pressions (timexes), events, and temporal relations within
a text as specified in TimeML annotation (Pustejovsky et
al., 2005). However, since addressing this aim in a first
evaluation challenge was deemed too difficult a staged
approach was suggested.
TempEval (henceforth TempEval-1) was an initial
evaluation exercise focusing only on the categorization of
temporal relations and only in English. It included three
relation types: event-timex, event-dct,1 and relations be-
tween main events in consecutive sentences.
TempEval-2 (Verhagen et al, 2010) extended
TempEval-1, growing into a multilingual task, and con-
sisting of six subtasks rather than three. This included
event and timex extraction, as well as the three relation
tasks from TempEval-1, with the addition of a relation
task where one event subordinates another.
TempEval-3 (UzZaman et al, 2012b) is a follow-up
to TempEval 1 and 2, covering English and Spanish.
TempEval-3 is different from its predecessors in a few
respects:
1DCT stands for document creation time
Size of the corpus: the dataset used has about 600K
word silver standard data and about 100K word gold stan-
dard data for training, compared to around 50K word cor-
pus used in TempEval 1 and 2. Temporal annotation is
a time-consuming task for humans, which has limited
the size of annotated data in previous TempEval exer-
cises. Current systems, however, are performing close to
the inter-annotator reliability, which suggests that larger
corpora could be built from automatically annotated data
with minor human reviews. We want to explore whether
there is value in adding a large automatically created sil-
ver standard to a hand-crafted gold standard.
End-to-end temporal relation processing task: the
temporal relation classification tasks are performed from
raw text, i.e. participants need to extract their own events
and temporal expressions first, determine which ones to
link and then obtain the relation types. In previous Tem-
pEvals, gold timexes, events, and relations (without cate-
gory) were given to participants.
Temporal relation types: the full set of temporal re-
lations in TimeML are used, rather than the reduced set
used in earlier TempEvals.
Platinum test set: A new test dataset has been devel-
oped for this edition. It is based on manual annotations
by experts over new text (unseen in previous editions).
Evaluation: we report a temporal awareness score for
evaluating temporal relations, which helps to rank sys-
tems with a single score.
2 Data
In TempEval-3, we reviewed and corrected existing cor-
pora, and also released new corpora.
2.1 Reviewing Existing Corpora
We considered the existing TimeBank (Pustejovsky et al,
2003) and AQUAINT2 data for TempEval-3. TempEval-
2See http://timeml.org/site/timebank/timebank.html
1
Entity Agreement
Event 0.87
Event class 0.92
Timex 0.87
Timex value 0.88
Table 1: Platinum corpus entity inter-annotator agreement.
Corpus # of words Standard
TimeBank 61,418 Gold
AQUAINT 33,973 Gold
TempEval-3 Silver 666,309 Silver
TempEval-3 Eval 6,375 Platinum
TimeBank-ES Train 57,977 Gold
TimeBank-ES Eval 9,833 Gold
Table 2: Corpora used in TempEval-3.
1 and TempEval-2 had the same documents as TimeBank
but different relation types and events.
For both TimeBank and AQUAINT, we, (i) cleaned up
the formatting for all files making it easy to review and
read, (ii) made all files XML and TimeML schema com-
patible, (iii) added some missing events and temporal ex-
pressions. In TimeBank, we, (i) borrowed the events from
the TempEval-2 corpus and (ii) borrowed the temporal re-
lations from TimeBank corpus, which contains a full set
of temporal relations. In AQUAINT, we added the tem-
poral relations between event and DCT (document cre-
ation time), which was missing for many documents in
that corpus. These existing corpora comprised the high-
quality component of our training set.
2.2 New Corpora
We created two new datasets: a small, manually-
annotated set over new text (platinum); and a machine-
annotated, automatically-merged dataset based on out-
puts of multiple systems (silver).
The TempEval-3 platinum evaluation corpus was anno-
tated/reviewed by the organizers, who are experts in the
area. This process used the TimeML Annotation Guide-
lines v1.2.1 (Saur?? et al, 2006). Every file was anno-
tated independently by at least two expert annotators, and
a third was dedicated to adjudicating between annotations
and merging the final result. Some annotators based their
work on TIPSem annotation suggestions (Llorens et al,
2012b). The GATE Annotation Diff tool was used for
merging (Cunningham et al, 2013), a custom TimeML
validator ensured integrity,3 and CAVaT (Derczynski and
Gaizauskas, 2010) was used to determine various modes
of TimeML mis-annotation and inconsistency that are in-
expressable via XML schema. Post-exercise, that corpus
(TempEval-3 Platinum with around 6K tokens, on com-
pletely new text) is released for the community to review
3See https://github.com/hllorens/TimeML-validator
and improve.4 Inter-annotator agreement (measured with
F1, as per Hripcsak and Rothschild (2005)) and the num-
ber of annotation passes per document were higher than
in existing TimeML corpora, hence the name. Details are
given in Table 1. Attribute value scores are given based
on the agreed entity set. These are for exact matches.
The TempEval-3 silver evaluation corpus is a 600K
word corpus collected from Gigaword (Parker et
al., 2011). We automatically annotated this corpus
by TIPSem, TIPSem-B (Llorens et al, 2013) and
TRIOS (UzZaman and Allen, 2010). These systems were
retrained on the corrected TimeBank and AQUAINT cor-
pus to generate the original TimeML temporal relation
set. We then merged these three state-of-the-art sys-
tem outputs using our merging algorithm (Llorens et al,
2012a). In our selected merged configuration all entities
and relations suggested by the best system (TIPSem) are
added in the merged output. Suggestions from other sys-
tems (TRIOS and TIPSem-B) are added in the merged
output, only if they are also supported by another system.
The weights considered in our configuration are: TIPSem
0.36, TIPSemB 0.32, TRIOS 0.32.
For Spanish, Spanish TimeBank 1.0 corpus (Saur?? and
Badia, 2012) wads used. It is the same corpus that was
used in TempEval-2, with a major review of entity anno-
tation and an important improvement regarding temporal
relation annotation. For TempEval-3, we converted ES-
TimeBank link types to the TimeML standard types based
on Allen?s temporal relations (Allen, 1983).
Table 2 summarizes our released corpora, measured
with PTB-scheme tokens as words. All data produced
was annotated using a well-defined subset of TimeML,
designed for easy processing, and for reduced ambigu-
ity compared to standard TimeML. Participants were en-
couraged to validate their submissions using a purpose-
built tool to ensure that submitted runs were legible. We
called this standard TimeML-strict, and release it sepa-
rately (Derczynski et al, 2013).
3 Tasks
The three main tasks proposed for TempEval-3 focus on
TimeML entities and relations:
3.1 Task A (Timex extraction and normalization)
Determine the extent of the timexes in a text as defined
by the TimeML TIMEX3 tag. In addition, determine the
value of the features TYPE and VALUE. The possible
values of TYPE are time, date, duration, and set; VALUE
is a normalized value as defined by the TIMEX3 standard.
4In the ACL data and code repository, reference ADCR2013T001.
See also https://bitbucket.org/leondz/te3-platinum
2
3.2 Task B (Event extraction and classification)
Determine the extent of the events in a text as defined by
the TimeML EVENT tag and the appropriate CLASS.
3.3 Task ABC (Annotating temporal relations)
This is the ultimate task for evaluating an end-to-end sys-
tem that goes from raw text to TimeML annotation of
entities and links. It entails performing tasks A and B.
From raw text extract the temporal entities (events and
timexes), identify the pairs of temporal entities that have
a temporal link (TLINK) and classify the temporal re-
lation between them. Possible pair of entities that can
have a temporal link are: (i) main events of consecu-
tive sentences, (ii) pairs of events in the same sentence,
(iii) event and timex in the same sentence and (iv) event
and document creation time. In TempEval-3, TimeML
relation are used, i.e.: BEFORE, AFTER, INCLUDES, IS-
INCLUDED, DURING, SIMULTANEOUS, IMMEDIATELY
AFTER, IMMEDIATELY BEFORE, IDENTITY, BEGINS,
ENDS, BEGUN-BY and ENDED-BY.
In addition to this main tasks, we also include two extra
temporal relation tasks:
Task C (Annotating relations given gold entities)
Given the gold entities, identify the pairs of entities that
have a temporal link (TLINK) and classify the temporal
relations between them.
Task C relation only (Annotating relations given gold
entities and related pairs) Given the temporal entities
and the pair of entities that have a temporal link, classify
the temporal relation between them.
4 Evaluation Metrics
The metrics used to evaluate the participants are:
4.1 Temporal Entity Extraction
To evaluate temporal entities (events and temporal ex-
pressions), we need to evaluate, (i) How many entities are
correctly identified, (ii) If the extents for the entities are
correctly identified, and (iii) How many entity attributes
are correctly identified. We use classical precision and
recall for recognition.
How many entities are correctly identified: We evalu-
ate our entities using the entity-based evaluation with the
equations below.
Precision = |Sysentity?Refentity||Sysentity|
Recall = |Sysentity?Refentity||Refentity|
where, Sysentity contains the entities extracted by the
system that we want to evaluate, and Refentity contains
the entities from the reference annotation that are being
compared.
If the extents for the entities are correctly identified:
We compare our entities with both strict match and re-
laxed match. When there is a exact match between the
system entity and gold entity then we call it strict match,
e.g. ?sunday morning? vs ?sunday morning?. When there
is a overlap between the system entity and gold entity
then we call it relaxed match, e.g. ?sunday? vs ?sunday
morning?. When there is a relaxed match, we compare
the attribute values.
How many entity attributes are correctly identified: We
evaluate our entity attributes using the attribute F1-score,
which captures how well the system identified both the
entity and attribute (attr) together.
Attribute Recall =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Refentity|
Attribute Precision =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Sysentity|
Attribute F1-score = 2?p?rp+r
Attribute (Attr) accuracy, precision and recall can be
calculated as well from the above information.
Attr Accuracy = Attr F1 / Entity Extraction F1
Attr R = Attr Accuracy * Entity R
Attr P = Attr Accuracy * Entity P
4.2 Temporal Relation Processing
To evaluate relations, we use the evaluation metric pre-
sented by UzZaman and Allen (2011).5 This metric cap-
tures the temporal awareness of an annotation in terms
of precision, recall and F1 score. Temporal awareness
is defined as the performance of an annotation as identi-
fying and categorizing temporal relations, which implies
the correct recognition and classification of the tempo-
ral entities involved in the relations. Unlike TempEval-
2 relation score, where only categorization is evaluated
for relations, this metric evaluates how well pairs of enti-
ties are identified, how well the relations are categorized,
and how well the events and temporal expressions are ex-
tracted.
Precision =
|Sys?relation?Ref
+
relation|
|Sys?relation|
Recall =
|Ref?relation?Sys
+
relation|
|Ref?relation|
where, G+ is the closure of graph G and G? is the
reduced of graph G, where redundant relations are re-
moved.6
We calculate the Precision by checking the number
of reduced system relations (Sys?relation) that can be veri-
fied from the reference annotation temporal closure graph
(Ref+relation), out of number of temporal relations in the
5We used a minor variation of the formula, where we consider the
reduced graph instead of all system or reference relations. Details can
be found in Chapter 6 of UzZaman (2012).
6A relation is redundant if it can be inferred through other relations.
3
strict value
F1 P R F1 F1
HeidelTime-t 90.30 93.08 87.68 81.34 77.61
HeidelTime-bf 87.31 90.00 84.78 78.36 72.39
HeidelTime-1.2 86.99 89.31 84.78 78.07 72.12
NavyTime-1,2 90.32 89.36 91.30 79.57 70.97
ManTIME-4 89.66 95.12 84.78 74.33 68.97
ManTIME-6 87.55 98.20 78.99 73.09 68.27
ManTIME-3 87.06 94.87 80.43 69.80 67.45
SUTime 90.32 89.36 91.30 79.57 67.38
ManTIME-1 87.20 97.32 78.99 70.40 67.20
ManTIME-5 87.20 97.32 78.99 69.60 67.20
ManTIME-2 88.10 97.37 80.43 72.22 66.67
ATT-2 85.25 98.11 75.36 78.69 65.57
ATT-1 85.60 99.05 75.36 79.01 65.02
ClearTK-1,2 90.23 93.75 86.96 82.71 64.66
JU-CSE 86.38 93.28 80.43 75.49 63.81
KUL 83.67 92.92 76.09 69.32 62.95
KUL-TE3RunABC 82.87 92.04 75.36 73.31 62.15
ClearTK-3,4 87.94 94.96 81.88 77.04 61.48
ATT-3 80.85 97.94 68.84 72.34 60.43
FSS-TimEx 85.06 90.24 80.43 49.04 58.24
TIPSem (TE2) 84.90 97.20 75.36 81.63 65.31
Table 3: Task A - Temporal Expression Performance.
reduced system relations (Sys?relation). Similarly, we
calculate the Recall by checking the number of reduced
reference annotation relations (Ref?relation) that can be
verified from the system output?s temporal closure graph
(Sys+relation), out of number of temporal relations in the
reduced reference annotation (Ref?relation).
This metric evaluates Task ABC together. For Task C
and Task C - relation only, all the gold annotation entities
were provided and then evaluated using the above metric.
Our evaluation toolkit that evaluated TempEval-3 par-
ticipants is available online.7
5 Evaluation Results
The aim of this evaluation is to provide a meaningful re-
port of the performance obtained by the participants in
the tasks defined in Section 3.
Furthermore, the results include TIPSem as reference
for comparison. This was used as a pre-annotation system
in some cases. TIPSem obtained the best results in event
processing task in TempEval-2 and offered very compet-
itive results in timex and relation processing. The best
timex processing system in TempEval-2 (HeidelTime) is
participating in this edition as well, therefore we included
TIPSem as a reference in all tasks.
We only report results in main measures. Results are
divided by language and shown per task. Detailed scores
can be found on the task website.8
7See http://www.cs.rochester.edu/u/naushad/temporal
8See http://www.cs.york.ac.uk/semeval-2013/task1/
5.1 Results for English
5.1.1 Task A: Timexes
We had nine participants and 21 unique runs for tem-
poral expression extraction task, Task A. Table 3 shows
the results. Details about participants? approaches can be
found in Table 4.
We rank the participants for Task A on the F1 score
of most important timex attribute ? Value. To get the
attribute Value correct, a system needs to correctly nor-
malise the temporal expression. This score (Value F1)
captures the performance of extracting the timex and
identifying the attribute Value together (Value F1 = Timex
F1 * Value Accuracy).
Participants approached the temporal expression ex-
traction task with rule-engineered methods, machine
learning methods and also hybrid methods. For temporal
expression normalization (identifying the timex attribute
value), all participants used rule-engineered approaches.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: Competition was close for timex recogni-
tion and the best systems all performed within 1% of
each other. On our newswire corpus, statistical systems
(ClearTK) performed best at strict matching, and rule-
engineered system best at relaxed matching (NavyTime,
SUTime, HeidelTime).
Strategy: post-processing, on top of machine learning-
base temporal expression extraction, provided a statisti-
cally significant improvement in both precision and recall
(ManTIME).
Data: using the large silver dataset, alone or together
with human annotated data, did not give improvements in
performance for Task A. Human-annotated gold standard
data alone provided the best performance (ManTIME).
Data: TimeBank alone was better than TimeBank and
AQUAINT together for Task A (ClearTK).
Features: syntactic and gazetteers did not provide any
statistically significant increment of performance with re-
spect to the morphological features alone (ManTIME).
Regarding the two sub-tasks of timex annotation,
recognition and interpretation/normalisation, we noticed
a shift in the state of the art. While normalisation is
currently (and perhaps inherently) done best by rule-
engineered systems, recognition is now done well by a
variety of methods. Where formerly, rule-engineered
timex recognition always outperformed other classes of
approach, now it is clear that rule-engineering and ma-
chine learning are equally good at timex recognition.
5.1.2 Task B: Events
For event extraction (Task B) we had seven participants
and 10 unique runs. The results for this task can be found
in Table 6. We rank the participants for TaskB on the F1
score of most important event attribute ? Class. Class
4
Strategy System Training data Classifier used
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt
ClearTK-1, 2 TimeBank SVM, Logit
ClearTK-3, 4 TBAQ SVM, Logit
JU-CSE TBAQ CRF
ManTIME-1 TBAQ + TE3Silver CRF
ManTIME-3 TBAQ CRF
ManTIME-5 TE3Silver CRF
Temp : ESAfeature TBAQ MaxEnt
Temp : WordNetfeature TBAQ MaxEnt
TIPSem (TE2) TBAQ CRF
Rule-based FSS-TimEx (EN) None None
FSS-TimEx (ES) None None
HeidelTime-1.2, bf (EN) None None
HeidelTime-t (EN) TBAQ None
HeidelTime (ES) Gold None
NavyTime-1, 2 None None
SUTime None None
Hybrid KUL TBAQ + TE3Silver Logit + post-processing
KUL-TE3RunABC TBAQ +TE3Silver Logit + post-processing
ManTIME-2 TBAQ + TE3Silver CRF + post-processing
ManTIME-4 TBAQ CRF + post-processing
ManTIME-6 TE3Silver CRF + post-processing
Table 4: Automated approaches for TE3 Timex Extraction
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt ms, ss
ClearTK-1, 2 TimeBank SVM, Logit ms
ClearTK-3, 4 TBAQ SVM, Logit ms
JU-CSE TBAQ CRF
KUL TBAQ +TE3Silver Logit ms, ls
KUL-TE3RunABC TBAQ +TE3Silver Logit ms, ls
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
Temp : ESAfeature TBAQ MaxEnt ms, ls, ss
Temp : WordNetfeature TBAQ MaxEnt ms, ls
TIPSem (TE2) TBAQ CRF/SVM ms, ls, ss
Rule-based FSS-TimEx (EN) None None ls, ms
FSS-TimEx (ES) None None ls, ms
Table 5: Automated approaches for Event Extraction
5
F1 P R class F1
ATT-1 81.05 81.44 80.67 71.88
ATT-2 80.91 81.02 80.81 71.10
KUL 79.32 80.69 77.99 70.17
ATT-3 78.63 81.95 75.57 69.55
KUL-TE3RunABC 77.11 77.58 76.64 68.74
ClearTK-3,4 78.81 81.40 76.38 67.87
NavyTime-1 80.30 80.73 79.87 67.48
ClearTK-1,2 77.34 81.86 73.29 65.44
NavyTime-2 79.37 80.52 78.26 64.81
Temp:ESAfeature 68.97 78.33 61.61 54.55
JU-CSE 78.62 80.85 76.51 52.69
Temp:WordNetfeature 63.90 78.90 53.69 50.00
FSS-TimEx 65.06 63.13 67.11 42.94
TIPSem (TE2) 82.89 83.51 82.28 75.59
Table 6: Task B - Event Extraction Performance.
F1 P R
ClearTK-2 30.98 34.08 28.40
ClearTK-1 29.77 34.49 26.19
ClearTK-3 28.62 30.94 26.63
ClearTK-4 28.46 29.73 27.29
NavyTime-1 27.28 31.25 24.20
JU-CSE 24.61 19.17 34.36
NavyTime-2 21.99 26.52 18.78
KUL-TE3RunABC 19.01 17.94 20.22
TIPSem (TE2) 42.39 38.79 46.74
Table 7: Task ABC - Temporal Awareness Evaluation (Task C
evaluation from raw text).
F1 captures the performance of extracting the event and
identifying the attribute Class together (Class F1 = Event
F1 * Class Accuracy).
All the participants except one used machine learning
approaches. Details about the participants? approaches
and the linguistic knowledge9 used to solve this problem,
and training data, are in Table 5.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: All the high performing systems for event
extraction (Task B) are machine learning-based.
Data: Systems using silver data, along with the hu-
man annotated gold standard data, performed very well
(top three participants in the task ? ATT, KUL, KUL-
TE3RunABC). Additionally, TimeBank and AQUAINT
together performed better than just TimeBank alone
(NavyTime-1, ClearTK-3,4).
Linguistic Features: Semantic features (ls and ss) have
played an important role, since the best systems (TIPSem,
ATT1 and KUL) include them. However, these three are
not the only systems using semantic features.
9Abbreviations used in the table: TBAQ ? TimeBank + AQUAINT
corpus ms ? morphosyntactic information, e.g. POS, lexical informa-
tion, morphological information and syntactic parsing related features;
ls ?lexical semantic information, e.g. WordNet synsets; ss ? sentence-
level semantic information, e.g. Semantic Role labels.
F1 P R
ClearTK-2 36.26 37.32 35.25
ClearTK-4 35.86 35.17 36.57
ClearTK-1 35.19 37.64 33.04
UTTime-5 34.90 35.94 33.92
ClearTK-3 34.13 33.27 35.03
NavyTime-1 31.06 35.48 27.62
UTTime-4 28.81 37.41 23.43
JU-CSE 26.41 21.04 35.47
NavyTime-2 25.84 31.10 22.10
KUL-TE3RunABC 24.83 23.35 26.52
UTTime-1 24.65 15.18 65.64
UTTime-3 24.28 15.10 61.99
UTTime-2 24.05 14.80 64.20
TIPSem (TE2) 44.25 39.71 49.94
Table 8: Task C - TLINK Identification and Classification.
F1 P R
UTTime-1, 4 56.45 55.58 57.35
UTTime-3, 5 54.70 53.85 55.58
UTTime-2 54.26 53.20 55.36
NavyTime-1 46.83 46.59 47.07
NavyTime-2 43.92 43.65 44.20
JU-CSE 34.77 35.07 34.48
Table 9: Task C - relation only: Relation Classification.
5.1.3 Task C: Relation Evaluation
For complete temporal annotation from raw text (Task
ABC - Task C from raw text) and for temporal relation
only tasks (Task C, Task C relation only), we had five
participants in total.
For relation evaluation, we primarily evaluate on Task
ABC (Task C from raw text), which requires joint entity
extraction, link identification and relation classification.
The results for this task can be found in Table 7.
While TIPSem obtained the best results in task ABC,
especially in recall, it was used by some annotators to
pre-label data. In the interest of rigour and fairness, we
separate out this system.
For task C, for provided participants with entities and
participants identified: between which entity pairs a rela-
tion exists (link identification); and the class of that rela-
tion. Results are given in Table 8. We also evaluate the
participants on the relation by providing the entities and
the links (performance in Table 9) ? TIPSem could not be
evaluated in this setting since the system is not prepared
to do categorization only unless the relations are divided
as in TempEval-2. For these Task C related tasks, we had
only one new participant, who didn?t participate in Task
A and B: UTTime.
Identifying which pair of entities to consider for tem-
poral relations is a new task in this TempEval challenge.
The participants approached the problems in data-driven,
rule-based and also in hybrid ways (Table 1010). On
10New abbreviation in the table, e-attr ? entity attributes, e.g. event
class, tense, aspect, polarity, modality; timex type, value.
6
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit e-attr, ms
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit e-attr, ms
ClearTK-3 TBAQ SVM, Logit e-attr, ms
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit e-attr, ms
KULRunABC TBAQ SVM, Logit ms
Rule-based JU-CSE None None
UTTime-1, 2 ,3 None None
TIPSem (TE2) None None e-attr, ms, ls, ss
Hybrid NavyTime-1 TBAQ MaxEnt ms
NavyTime-2 TimeBank MaxEnt ms
UTTime-4 TBAQ Logit ms, ls, ss
UTTime-5 TBAQ + inverse relations Logit ms, ls, ss
Table 10: Automated approaches for TE3 TLINK Identification
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit ms, ls
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit ms, ls
ClearTK-3 TBAQ SVM, Logit ms, ls
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit ms, ls
JU-CSE TBAQ CRF
KULRunABC TBAQ SVM, Logit ms
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
UTTime-1,4, 2 TBAQ Logit ms, ls, ss
UTTime-3,5 TBAQ + inverse relations Logit ms, ls, ss
TIPSem (TE-2) TBAQ CRF/SVM ms, ls, ss
Table 11: Automated approaches for Relation Classification
the other hand, all the participants used data-driven ap-
proaches for temporal relations (Table 11).
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: For relation classification, all participants
used partially or fully machine learning-based systems.
Data: None of the participants implemented their sys-
tems training on the silver data. Most of the systems use
the combined TimeBank and AQUAINT (TBAQ) corpus.
Data: Adding additional high-quality relations, either
Philippe Muller?s closure-based inferences or the verb
clause relations from Bethard et al (2007), typically in-
creased recall and the overall performance (ClearTK runs
two and four).
Features: Participants mostly used the morphosyntac-
tic and lexical semantic information. The best perform-
ing systems from TempEval-2 (TIPSem and TRIOS) ad-
ditionally used sentence level semantic information. One
participant in TempEval-3 (UTTime) also did deep pars-
ing for the sentence level semantic features.
Features: Using more Linguistic knowledge is impor-
tant for the task, but it is more important to execute it
properly. Many systems performed better using less lin-
guistic knowledge. Hence a system (e.g. ClearTK) with
basic morphosyntactic features is hard to beat with more
semantic features, if not used properly.
entity extraction
strict relaxed
F1 F1 P R value
HeidelTime 85.3 90.1 96.0 84.9 87.5
TIPSemB-F 82.6 87.4 93.7 81.9 82.0
FSS-TimEx 49.5 65.2 86.6 52.3 62.7
Table 12: Task A: Temporal Expression (Spanish).
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 57.6 89.8 42.4 24.9 - -
TIPSemB-F 88.8 91.7 86.0 57.6 41.0 36.3
Table 13: Task B: Event Extraction (Spanish).
Classifier: Across the various tasks, ClearTK tried
Mallet CRF, Mallet MaxEnt, OpenNLP MaxEnt, and LI-
BLINEAR (SVMs and logistic regression). They picked
the final classifiers by running a grid search over models
and parameters on the training data, and for all tasks, a
LIBLINEAR model was at least as good as all the other
models. As an added bonus, it was way faster to train
than most of the other models.
6 Evaluation Results (Spanish)
There were two participants for Spanish. Both partici-
pated in task A and only one of them in task B. In this
7
F1 P R
TIPSemB-F 41.6 37.8 46.2
Table 14: Task ABC: Temporal Awareness (Spanish).
entity extraction attributes
strict relaxed val type
F1 F1 P R F1 F1
HeidelTime 86.4 89.8 94.0 85.9 87.5 89.8
FSS-TimEx 42.1 68.4 86.7 56.5 48.7 65.8
TIPSem 86.9 93.7 98.8 89.1 75.4 88.0
TIPSemB-F 84.3 89.9 93.0 87.0 82.0 86.5
Table 15: Task A: TempEval-2 test set (Spanish).
case, TIPSemB-Freeling is provided as a state-of-the-art
reference covering all the tasks. TIPSemB-Freeling is the
Spanish version of TIPSem with the main difference that
it does not include semantic roles. Furthermore, it uses
Freeling (Padro? and Stanilovsky, 2012) to obtain the lin-
guistic features automatically.
Table 12 shows the results obtained for task A. As it
can be observed HeidelTime obtains the best results. It
improves the previous state-of-the-art results (TIPSemB-
F), especially in normalization (value F1).
Table 13 shows the results from event extraction. In
this case, the previous state-of-the-art is not improved.
Table 14 only shows the results obtained in temporal
awareness by the state-of-the-art system since there were
not participants on this task. We observe that TIPSemB-F
approach offers competitive results, which is comparable
to results obtained in TE3 English test set.
6.1 Comparison with TempEval-2
TempEval-2 Spanish test set is included as a subset of this
TempEval-3 test set. We can therefore compare the per-
formance across editions. Furthermore, we can include
the full-featured TIPSem (Llorens et al, 2010), which
unlike TIPSemB-F used the AnCora (Taule? et al, 2008)
corpus annotations as features including semantic roles.
For timexes, as can be seen in Table 15, the origi-
nal TIPSem obtains better results for timex extraction,
which favours the hypothesis that machine learning sys-
tems are very well suited for this task (if the training data
is sufficiently representative). However, for normaliza-
tion (value F1), HeidelTime ? a rule-engineered system ?
obtains better results. This indicates that rule-based ap-
proaches have the upper hand in this task. TIPSem uses
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 59.0 90.3 43.9 24.6 - -
TIPSemB-F 90.2 92.5 88.0 58.6 39.7 38.1
TIPSem 88.2 90.6 85.8 58.7 84.9 78.7
Table 16: Task B: TempEval-2 test set (Spanish).
a partly data-driven normalization approach which, given
the small amount of training data available, seemed less
suited to the task.
Table 16 shows event extraction performance in TE2
test set. TIPSemB-F and TIPSem obtained a similar per-
formance. TIPSemB-F performed better in extraction and
TIPSem better in attribute classification.
7 Conclusion
In this paper, we described the TempEval-3 task within
the SemEval 2013 exercise. This task involves identify-
ing temporal expressions (timexes), events and their tem-
poral relations in text. In particular participating systems
were required to automatically annotate raw text using
TimeML annotation scheme
This is the first time end-to-end systems are evalu-
ated with a new single score (temporal awareness). In
TempEval-3 participants had to obtain temporal relations
from their own extracted timexes and events which is a
very challenging task and was the ultimate evaluation aim
of TempEval. It was proposed at TempEval-1 but has not
been carried out until this edition.
The newly-introduced silver data proved not so useful
for timex extraction or relation classification, but did help
with event extraction. The new single-measure helped to
rank systems easily.
Future work could investigate temporal annotation in
specific applications. Current annotations metrics evalu-
ate relations for entities in the same consecutive sentence.
For document-level understanding we need to understand
discourse and pragmatic information. Temporal question
answering-based evaluation (UzZaman et al, 2012a) can
help us to evaluate participants on document level tempo-
ral information understanding without creating any addi-
tional training data. Also, summarisation, machine trans-
lation, and information retrieval need temporal annota-
tion. Application-oriented challenges could further re-
search in these areas.
From a TimeML point of view, we still haven?t tack-
led subordinate relations (TimeML SLINKs), aspectual
relations (TimeML ALINKs), or temporal signal anno-
tation (Derczynski and Gaizauskas, 2011). The critical
questions of which links to annotate, and whether the cur-
rent set of temporal relation types are appropriate for lin-
guistic annotation, are still unanswered.
Acknowledgments
We thank the participants ? especially Steven Bethard,
Jannik Stro?tgen, Nate Chambers, Oleksandr Kolomiyets,
Michele Filannino, Philippe Muller and others ? who
helped us to improve TempEval-3 with their valuable
feedback. The third author also thanks Aarhus Univer-
sity, Denmark who kindly provided facilities.
8
References
J. F. Allen. 1983. Maintaining knowledge about temporal in-
tervals. Communications of the ACM, 26(11):832?843.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007. Timelines
from text: Identication of syntactic temporal relations. In
Proceedings of IEEE International Conference on Semantic
Computing.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva.
2013. Getting More Out of Biomedical Documents with
GATE?s Full Lifecycle Open Source Text Analytics. PLoS
computational biology, 9(2):e1002854.
L. Derczynski and R. Gaizauskas. 2010. Analysing Temporally
Annotated Corpora with CAVaT. In Proceedings of the 7th
International Conference on Language Resources and Eval-
uation, pages 398?404.
L. Derczynski and R. Gaizauskas. 2011. A Corpus-based Study
of Temporal Signals. In Proceedings of the 6th Corpus Lin-
guistics Conference.
L. Derczynski, H. Llorens, and N. UzZaman. 2013. TimeML-
strict: clarifying temporal annotation. CoRR, abs/1304.
G. Hripcsak and A. S. Rothschild. 2005. Agreement, the f-
measure, and reliability in information retrieval. Journal of
the American Medical Informatics Association, 12(3):296?
298.
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem (En-
glish and Spanish): Evaluating CRFs and Semantic Roles in
TempEval-2. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 284?291. Association
for Computational Linguistics.
H. Llorens, N. UzZaman, and J. Allen. 2012a. Merging Tem-
poral Annotations. In Proceedings of the TIME Conference.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2012b. Au-
tomatic system for identifying and categorizing temporal re-
lations in natural language. International Journal of Intelli-
gent Systems, 27(7):680?703.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2013. Ap-
plying Semantic Knowledge to the Automatic Processing of
Temporal Expressions and Events in Natural Language. In-
formation Processing & Management, 49(1):179?197.
L. Padro? and E. Stanilovsky. 2012. Freeling 3.0: Towards wider
multilinguality. In Proceedings of the Language Resources
and Evaluation Conference (LREC 2012), Istanbul, Turkey,
May. ELRA.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. LDC catalog ref.
LDC2011T07.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al
2003. The TimeBank corpus. In Corpus Linguistics.
J. Pustejovsky, B. Ingria, R. Saur??, J. Castano, J. Littman,
R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The
specification language TimeML. The Language of Time: A
reader, pages 545?557.
R. Saur?? and T. Badia. 2012. Spanish TimeBank 1.0. LDC
catalog ref. LDC2012T12.
R. Saur??, J. Littman, B. Knippen, R. Gaizauskas, A. Setzer, and
J. Pustejovsky. 2006. TimeML Annotation Guidelines Ver-
sion 1.2.1.
M. Taule?, M. A. Mart?, and M. Recasens. 2008. Ancora: Mul-
tilevel annotated corpora for catalan and spanish. In Pro-
ceedings of the 6th International Conference on Language
Resources and Evaluation (LREC-2008).
N. UzZaman and J. Allen. 2010. TRIPS and TRIOS system for
TempEval-2: Extracting temporal information from text. In
Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 276?283. Association for Computational
Linguistics.
N. UzZaman and J. Allen. 2011. Temporal Evaluation. In Pro-
ceedings of The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies.
N. UzZaman, H. Llorens, and J. Allen. 2012a. Evaluating tem-
poral information understanding with temporal question an-
swering. In Proceedings of IEEE International Conference
on Semantic Computing.
N. UzZaman, H. Llorens, J. F. Allen, L. Derczynski, M. Ver-
hagen, and J. Pustejovsky. 2012b. TempEval-3: Evaluating
Events, Time Expressions, and Temporal Relations. CoRR,
abs/1206.5333.
N. UzZaman. 2012. Interpreting the Temporal Aspects of Lan-
guage. Ph.D. thesis, University of Rochester, Rochester, NY.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
J. Moszkowicz, and J. Pustejovsky. 2009. The TempEval
challenge: identifying temporal relations in text. Language
Resources and Evaluation, 43(2):161?179.
M. Verhagen, R. Saur??, T. Caselli, and J. Pustejovsky. 2010.
SemEval-2010 task 13: TempEval-2. In Proceedings of the
5th International Workshop on Semantic Evaluation, pages
57?62. Association for Computational Linguistics.
9
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 99?109,
Dublin, Ireland, August 23-24 2014.
Generating Simulations of Motion Events from Verbal Descriptions
James Pustejovsky
Computer Science Dept.
Brandeis University
Waltham, MA USA
jamesp@cs.brandeis.edu
Nikhil Krishnaswamy
Computer Science Dept.
Brandeis University
Waltham, MA USA
nkrishna@brandeis.edu
Abstract
In this paper, we describe a computational
model for motion events in natural lan-
guage that maps from linguistic expres-
sions, through a dynamic event interpreta-
tion, into three-dimensional temporal sim-
ulations in a model. Starting with the
model from (Pustejovsky and Moszkow-
icz, 2011), we analyze motion events us-
ing temporally-traced Labelled Transition
Systems. We model the distinction be-
tween path- and manner-motion in an op-
erational semantics, and further distin-
guish different types of manner-of-motion
verbs in terms of the mereo-topological re-
lations that hold throughout the process of
movement. From these representations,
we generate minimal models, which are
realized as three-dimensional simulations
in software developed with the game en-
gine, Unity. The generated simulations
act as a conceptual ?debugger? for the se-
mantics of different motion verbs: that
is, by testing for consistency and infor-
mativeness in the model, simulations ex-
pose the presuppositions associated with
linguistic expressions and their composi-
tions. Because the model generation com-
ponent is still incomplete, this paper fo-
cuses on an implementation which maps
directly from linguistic interpretations into
the Unity code snippets that create the sim-
ulations.
1 Introduction
Semantic interpretation requires access to both
knowledge about words and how they compose.
As the linguistic phenomena associated with lexi-
cal semantics have become better understood, sev-
eral assumptions have emerged across most mod-
els of word meaning. These include the following:
(1) a. Lexical meaning involves some sort of
?componential analysis?, either through
predicative primitives or a system of
types.
b. The selectional properties of predicators
can be explained in terms of these com-
ponents;
c. An understanding of event semantics and
the different role of event participants
seems crucial for modeling linguistic ut-
terances.
As a starting point in lexical semantic analysis,
a standard methodology in both theoretical and
computational linguistics is to identify features in
a corpus that differentiate the data in meaningful
ways; meaningful in terms of prior theoretical as-
sumptions or in terms of observably differentiated
behaviors. Combining these strategies we might,
for instance, take a theoretical constraint that we
hope to justify through behavioral distinctions in
the data. An example of this is the theoretical
claim that motion verbs can be meaningfully di-
vided into two classes: manner- and path-oriented
predicates (Talmy, 1985; Jackendoff, 1983; Talmy,
2000). These constructions can be viewed as en-
coding two aspects of meaning: how the move-
ment is happening and where it is happening. The
former strategy is illustrated in (2a) and the latter
in (2b) (where m indicates a manner verb, and p
indicates a path verb).
(2) a. The ball rolled
m
.
b. The ball crossed
p
the room.
With both of the verb types, adjunction can make
reference to the missing aspect of motion, by intro-
ducing a path (as in (3a)) or the manner of move-
ment (in (3b)).
(3) a. The ball rolled
m
across the room.
b. The ball crossed
p
the room rolling.
Differences in syntactic distribution and grammat-
ical behavior in large datasets, in fact, correlate
99
fairly closely with the theoretical claims made by
linguists using small introspective datasets.
The path-manner classification is a case where
there are data-derived distinctions that corre-
late nicely with theoretically inspired predictions.
More often than not, however, lexical semantic
distinctions are formal stipulations in a linguistic
model, that often have no observable correlations
to data. For example, an examination of the man-
ner of movement class from Levin (1993) illus-
trates this point. The verbs below are all Levin-
class manner of motion verbs:
(4) MANNER OF MOTION VERBS: drive, walk,
run, crawl, fly, swim, drag, slide, hop, roll
Assuming the two-way distinction between path
and manner predication of motion mentioned
above, these verbs do, in fact, tend to pattern ac-
cording to the latter class in the corpus. Given
that they are all manner of motion verbs, however,
any data-derived distinctions that emerge within
this class will have to be made in terms of addi-
tional syntactic or semantic dimensions. While it
is most likely possible to differentiate, for exam-
ple, the verbs slide from roll, or walk from hop in
the corpus, given enough data, it is important to
realize that conceptual and theoretical modeling is
often necessary to reveal the factors that semanti-
cally distinguish such linguistic expressions, in the
first place.
We argue that this problem can be approached
with the use of minimal model generation. As
Blackburn and Bos (2008) point out, theorem
proving (essentially type satisfaction of a verb in
one class as opposed to another) provides a ?nega-
tive handle? on the problem of determining consis-
tency and informativeness for an utterance, while
model building provides a ?positive handle? on
both. For our concerns, simulation construction
provides a positive handle on whether two man-
ner of motion processes are distinguished in the
model. Further, the simulation must specify how
they are distinguished, the analogue to informa-
tiveness.
In this paper, we argue that traditional lexical
modeling can benefit greatly from examining how
semantic interpretations are contextually and con-
ceptually grounded. We explore a dynamic in-
terpretation of the lexical semantic model devel-
oped in Generative Lexicon Theory (Pustejovsky,
1995; Pustejovsky et al., 2014). Specifically, we
are interested in using model building (Blackburn
and Bos, 2008; Konrad, 2004; Gardent and Kon-
rad, 2000) and simulation generation (Coyne and
Sproat, 2001; Siskind, 2011) to reveal the concep-
tual presuppositions inherent in natural language
expressions. In this paper, we focus our attention
on motion verbs, in order to distinguish between
manner and path motion verbs, as well as to model
mereotopological distinctions within the manner
class.
2 Situating Motion in Space and Time
The interpretation of motion in language has been
one of the most researched areas in linguistics
and Artificial Intelligence (Kuipers, 2000; Freksa,
1992; Galton, 2000; Levinson, 2003; Mani and
Pustejovsky, 2012). Because of their grammatical
and semantic import, linguistic interest in identi-
fying where events happen has focused largely on
motion verbs and the role played by paths. Jack-
endoff (1983), for example, elaborates a semantics
for motion verbs incorporating explicit reference
to the path traversed by the mover, from source to
destination (goal) locations. Talmy (1983) devel-
ops a similar conceptual template, where the path
followed by the figure is integral to the conceptu-
alization of the motion against a ground. Hence,
the path can be identified as the central element in
defining the location of the event (Talmy, 2000).
Related to this idea, both Zwarts (2005) and Puste-
jovsky and Moszkowicz (2011) develop mecha-
nisms for dynamically creating the path traversed
by a mover in a manner of motion predicate, such
as run or drive. Starting with this approach, the
localization of a motion event, therefore, is at
least minimally associated with the path created
by virtue of the activity.
In addition to capturing the spatial trace of the
object in motion, several researchers have pointed
out that identifying the shape of the path dur-
ing motion is also critical for fully interpreting
the semantics of movement. Eschenbach et al.
(1999) discusses the orientation associated with
the trajectory, something they refer to as oriented
curves. Motivated more by linguistic considera-
tions, Zwarts (2006) introduces the notion of an
event shape, which is the trajectory associated
with an event in space represented by a path. He
defines a shape function, which is a partial func-
tion assigning unique paths to those events involv-
ing motion or extension in physical space. This
work suggests that the localization of an event
100
makes reference to orientational as well as config-
urational factors, a view that is pursued in Puste-
jovsky (2013b). This forces us to look at the var-
ious spatio-temporal regions associated with the
event participants, and the interactions between
them.
These issues are relevant to our present con-
cerns, because in order to construct a simulation, a
motion event must be embedded within an appro-
priate minimal embedding space. This must suf-
ficiently enclose the event localization, while op-
tionally including room enough for a frame of ref-
erence visualization of the event (the viewer?s per-
spective). We return to this issue later in the paper
when constructing our simulation from the seman-
tic interpretation associated with motion events.
3 Modeling Motion in Language
3.1 Theoretical Assumptions
The advantage of adopting a dynamic interpre-
tation of motion is that we can directly distin-
guish path predication from manner of motion
predication in an operational semantics (Miller
and Charles, 1991; Miller and Johnson-Laird,
1976) that maps nicely to a simulation environ-
ment. Models of processes using updating typi-
cally make reference to the notion of a state tran-
sition (van Benthem, 1991; Harel, 1984). This is
done by distinguishing between formulae, ?, and
programs, pi. A formula is interpreted as a clas-
sical propositional expression, with assignment of
a truth value in a specific model. We will inter-
pret specific models by reference to specific states.
A state is a set of propositions with assignments
to variables at a specific index. Atomic programs
are input/output relations ( [[pi]] ? S ? S), and
compound programs are constructed from atomic
ones following rules of dynamic logic (Harel et al.,
2000).
For the present discussion, we represent the dy-
namics of actions in terms of Labeled Transition
Systems (LTSs) (van Benthem, 1991).
1
An LTS
consists of a triple, ?S,Act,??, where: S is the
set of states; Act is a set of actions; and? is a to-
tal transition relation:?? S?Act?S. An action,
? ? Act, provides the labeling on an arrow, mak-
ing it explicit what brings about a state-to-state
1
This is consistent with the approach developed in (Fer-
nando, 2009; Fernando, 2013). This approach to a dynamic
interpretation of change in language semantics is also in-
spired by Steedman (2002).
transition. As a shorthand for (e
1
, ?, e
2
) ??, we
will also use e
1
?
?? e
2
. If reference to the state
content (rather than state name) is required for in-
terpretation purposes (van Benthem et al., 1994),
then as shorthand for ({?}
e
1
, ?, {??}
e
2
) ??, we
use, ?
e
1
?
?? ??
e
2
. Finally, when referring
to temporally-indexed states in the model, where
e
i
@i indicates the state e
i
interpreted at time i, as
shorthand for ({?}
e
1
@i
, ?, {??}
e
2
@i+1
) ??, we
will use, ?
i
e
1
?
?? ??
i+1
e
2
, as described in Puste-
jovsky (2013).
3.2 Distinguishing Path and Manner Motion
We will assume that change of location of an ob-
ject can be viewed as a special instance of a first-
order program, which we will refer to as ? (Puste-
jovsky and Moszkowicz, 2011).
2
(5) x := y (?-transition, where loc(z) is value
being updated)
?x assumes the value given to y in the next
state.?
?M, (i, i+ 1), (u, u[x/u(y)])? |= x := y
iff ?M, i, u? |= loc(z) = x ? ?M, i +
1, u[x/u(y)]? |= loc(z) = y
Given a simple transition, a process can be viewed
as simply an iteration of ? (Fernando, 2009).
However, as (Pustejovsky, 2013a) points out, since
most manner motion verbs in language are ac-
tually directed processes, simple decompositions
into change-of-location are inadequate. That is,
they are guarded transitions where the test is not
just non-coreference, but makes reference to val-
ues on a scale, C, and ensures that it continues in
an order-preserving change through the iterations.
When this test references the values on a scale, C,
we call this a directed ?-transition (~?), e.g., x 4 y,
x < y:
(6) ~? =
df
C?
x
e
i
?
?? e
i+1
.
(7) loc(z) = x
e
0
~?
?? loc(z) = y
1 e
1
~?
?? . . .
loc(z) = y
n e
n
This now provides us with our dynamic interpre-
tation of directed manner of motion verbs, such
as slide, swim, roll, where we have an iteration of
assignments of locations, undistinguished except
2
Cf. Groenendijk and Stokhof (1990) for dynamic updat-
ing, and Naumann (2001) for a related analysis.
101
that the values are order-preserving according to a
scalar constraint.
This is quite different from the dynamic inter-
pretation of path predicates. Following (Galton,
2004; Pustejovsky and Moszkowicz, 2011), path
predicates such as arrive and leave make refer-
ence to a ?distinguished location?, not an arbi-
trary location. For example, the ball enters the
room is satisified when the distinguished location,
D, (the room) is successfully tested as the loca-
tion for the moving object. That is, the location
is tested against the current location for an object
((loc(x) 6= D)?), and retested until it is satisfied
((loc(x) = D)?).
(8)
(loc(x)6=D)?
x
loc(z) = x
e
0
~?
??
(loc(x) 6=D)?
x
loc(z) = y
1 e
1
~?
?? . . .
(loc(x)=D)?
x
loc(z) = y
n e
n
While beyond the scope of the present discus-
sion, it is worth noting that the model of event
structure adopted here for motion verbs fits well
with most of the major semantic and syntactic phe-
nomena associated with event classes and Aktion-
sarten.
3
3.3 Mereotopological Distinctions in Manner
Given the formal distinction between path and
manner predicates as described above, let us ex-
amine how to differentiate meaning within the
manner class. Levin (1993) differentiates this
class in terms of argument alternation patterns,
and identifies the following verb groupings: ROLL,
RUN, EPONYMOUS VEHICLE, WALTZ, ACCOM-
PANY, and CHASE verbs. While suggestive, these
distinctions are only partially useful towards actu-
ally teasing apart the semantic dimensions along
which we identify the contributing factors of man-
ner.
Mani and Pustejovsky (2012) suggest a differ-
ent strategy involving the identification of seman-
tic parameters that clearly differentiate verb senses
from each other within this class. One parameter
exploited quite extensively within the motion class
involves the mereotopological contraints that in-
here throughout the movement of the object (Ran-
dell et al., 1992; Asher and Vieu, 1995; Gal-
ton, 2000). Using this parameter, we are able to
distinguish several of Levin?s classes of manner
3
Cf. (Pustejovsky, 2013a) and (Krifka, 1992).
as well as some novel ones, as described in (9),
where a class is defined by the constraints that hold
throughout the event (where EC is ?externally con-
nected?, and DC is ?disconnected?).
(9) For Figure (F) relative to Ground (G):
a. EC(F,G), throughout motion:
b. DC(F,G), throughout motion:
c. EC(F,G) followed by DC(F,G), through-
out motion:
d. Sub-part(F?,F), EC(F?,G) followed by
DC(F?,G), throughout motion:
e. Containment of F in a Vehicle (V).
For example, consider the semantic distinction be-
tween the verbs slide and hop or bounce. When the
latter are employed in induced (directed) motion
constructions (Levin, 1993; Jackendoff, 1996),
they take on the meaning of manner of motion
verbs. Distinguishing between a sliding and hop-
ping motion involves inspecting the next-state
content in the motion n-gram: namely, there is a
continuous satisfaction of EC(F,G) throughout the
motion for slide and a toggling effect (on-off) for
the predicates bounce and hop, as shown in (10).
(10)
?DC(x,G)?
x
loc(z) = x
e
0
~?
??
DC(x,G)?
x
loc(z) = y
1 e
1
~?
??
?DC(x,G)?
x
loc(z) = y
2 e
2
With the surface as the ground argument, these
verbs are defined in terms of two transitions.
4
B
A
s1
AA
s3s2
l1 l2 l3
Figure 1: Slide Motion
B
A
s1
A
A
s3s2
l1 l2 l3
Figure 2: Hop Motion
4
Many natural language predicates require reference to at
least three states. These include the semelfactives mentioned
above, as well as blink and iterative uses of knock and clap
(Vendler, 1967; Dowty, 1979; Rothstein, 2008).
102
Distinguishing between a sliding motion and a
rolling motion is also fairly straightforward. We
have the entailments that result from each kind of
motion, given a set of initial conditions, as in the
following short sentence describing the motion of
a ball relative to a floor (the domain for our event
simulations).
? The ball slid.: At the termination of the ac-
tion, object ball has moved relative to a sur-
face in a manner that is [+translate].
That is, the movement is a translocation
across absolute space, but other attributes
(such as the ball?s orientation) do not change.
? The ball rolled.: At the termination of the
action, object ball has moved relative to a
surface in a manner that is [+translate]
and [+rotate]. Here, the translocation
across space is preserved, with the addition
of an orientation change.
We can further decompose these features, cast-
ing the [+translate] in terms of the trans-
lation?s dimensionality. For both the ball slid
and the ball rolled, it is required that the ball re-
main in the contact with the relevant surface, thus
we can enforce a [-3-dimensional] con-
straint on the [+translate] feature. Thus,
we arrive at the following differentiating se-
mantic constraints for these verbs: (a) slide,
[+translate], [-3-dimensional]; (b)
roll, [+translate], [-3-dimensional],
[+rotate]. This is illustrated below over three
states of execution.
B
s1
A
a
b
c
B
s2
A
b
a
c
B
s3
A
c
b
a
Figure 3: Roll Motion
In our approach to conceptual modeling, we hy-
pothesize that between the members of any pair of
motion verbs, there exists at least one distinctive
feature of physical motion that distinguishes the
two predicates. While this may be too strong, it
is helpful in our use of simulations for debugging
the lexical semantics of linguistic expressions.
5
In
order to quantify the qualitative distinctions be-
tween motion predicates and identify the precise
primitive components of a motion verb, we build
a real-time simulation, within which the individ-
ual features of a single motion verb can be defined
and isolated in three-dimensional space.
The idea of constructing simulations from lin-
guistic utterances is, of course, not new. There
are two groups of researchers who have developed
related ideas quite extensively: simulation theo-
rists, working in the philosophy of mind, such as
Alvin Goldman and Robert Gordon; and cogni-
tive scientists and linguists, such as Jerry Feldman,
Ron Langacker, and Ben Bergen. According to
Goldman (1989), simulation provides a process-
driven theory of mind and mental attribution, dif-
fering from the theory-driven models proposed by
Churchland and others (Churchland, 1991). From
the cognitive linguistics tradition, simulation se-
mantics has come to denote the mental instanti-
ation of an interpretation of any linguistic utter-
ance (Feldman, 2006; Bergen et al., 2007; Bergen,
2012). While these communities do not seem to
reference each other, it is clear from our perspec-
tive, that they are both pursuing similar programs,
where distinct linguistic utterances correspond to
generated models that have differentiated struc-
tures and behaviors (Narayanan, 1999; Siskind,
2011; Goldman, 2006).
4 Simulations as Minimal Models
The approach to simulation construction intro-
duced in the previous section is inspired by work
in minimal model generation (Blackburn and Bos,
2008; Konrad, 2004). Type satisfaction in the
compositional process mirrors the theorem prov-
ing component, while construction of the specific
model helps us distinguish what is inherent in the
different manner of motion events. This latter as-
pect is the ?positive handle?, (Blackburn and Bos,
2008) which demonstrates the informativeness of
a distinction in our simulation.
Simulation software must be able to map a pred-
icate to a known behavior, its arguments to objects
in the scene, and then prompt those objects to ex-
ecute the behavior. A simple input sentence needs
5
Obviously, true synonyms in the lexicon would not be
distinguishable in a model.
103
to be tagged and parsed and transformed into pred-
icate/argument representation, and from there into
a dynamic event structure, as in (Pustejovsky and
Moszkowicz, 2011). The event structure is inter-
preted as the transformation executed over the ob-
ject or objects in each frame, and then rendered.
Ball1/NNP crossed/VBD Floor/NNP
SBJ OBJ
Ball1/NNP rolled/VBD
SBJ
Table 1: Dependency parses for Ball1 crossed
Floor (top) and Ball1 rolled (bottom).
We currently use only proper names to refer to
objects in the scene, to simplify model generation,
hence Ball1 and Floor. This facilitates easy object
identification in this prototype development stage.
Given a tagged and dependency parsed sen-
tence, we can the transform the parse into a pred-
icate formula, using the root of the parse as the
predicate, the subject as a singleton first argument,
and all objects as an optional stack of subsequent
arguments.
1. pred := cross 1. pred := roll
2. x := Ball1 2. x := Ball1
3. y.push(Floor)
cross(Ball1,[Floor]) roll(Ball1)
Table 2: Transformation to predicate formula for
Ball1 crossed Floor and Ball1 rolled.
The resulting predicates are represented in Ta-
ble 3 as expressions in Dynamic Interval Tempo-
ral Logic (DITL) (Pustejovsky and Moszkowicz,
2011), which are equivalent to the LTS expres-
sions used above.
cross(Ball1,Floor)
loc(Ball1) := y, target(Ball1) := z; b := y;
(y := w; y 6= w; d(b,y) < d(b,w),
d(b,z) > d(z,w), IN(y,Floor))
+
roll(Ball1)
loc(Ball1) := y, rot(Ball1) := z; b
loc
:= y,
b
rot
:= z; (y := w; y 6= w; d(b
loc
,y) < d(b
loc
,w),
IN(y,Floor))
+
, (z := v; z 6= v; z-b
rot
< v-b
rot
)
+
Table 3: DITL expressions for Ball1 crossed Floor
and Ball1 rolled.
The DITL expression forms the basis of the
coded behavior. The first two initialization steps
are coded into the behavior?s start function while
the the third, Kleene iterated step, is encoded in
the behavior?s update function.
5 Generating Simulations
We use the freely-available game engine, Unity,
(Goldstone, 2009) to handle all underlying graph-
ics processing, and limited our object library to
simple primitive shapes of spheroids, rectangular
prisms, and planes. For every instance of an ob-
ject, the game engine maintains a data structure for
the object?s virtual representation. Table 4 shows
the data structure for Entity, the superclass of
all movable objects.
Entity:
position: 3-vector rotation: 3-vector
scale: 3-vector transform: Matrix
collider =
center: 3-vector
min: 3-vector
max: 3-vector
radius: float
geometry: Mesh
currentBehavior: Behavior
Table 4: Data structure of motion-capable entities.
The position and scale of the object are
represented as 3-vectors of floating point numbers.
The rotation is represented as the Euler angles
of the object?s current rotation, also a 3-vector.
This 3-vector is computed as a quaternion for ren-
dering purposes. The transform matrix com-
poses the position, scale, and quaternion rotation
into the complete transformation applied to the ob-
ject at any given frame. The geometry is a mesh.
The points, edges, faces, and texture attributes that
comprise the mesh are all immutable at the mo-
ment so the mesh type is considered atomic for
our purposes. The collider contains the coor-
dinates of the center of the object, minimum and
maximum extents of the object?s boundaries, and
radius of the boundaries (for spherical objects).
Behaviors can only be executed over Entity
instances, so we also provide each one with a
currentBehavior property, referencing the
code to be executed over the object every frame
that said behavior is being run. This code performs
a transformation over the object at every step, gen-
erating a new state in a dynamic model of the
event denoted by the a given predicate. Thus, the
event
6
is decomposed into frame-by-frame trans-
formations representing the ?-transition from Sec-
tion 3.2.
We generate example simulations of behaviors
in a sample environment, shown in Figure 4, that
6
These events are linguistic events, and not the same as
?events? as used in software development or with event han-
dlers.
104
consists of a sealed four-walled room that contains
a number of primitive objects.
Figure 4: Sample environment in top-down and
perspective views.
The behaviors currently coded into our software
map directly from DITL to the simulation. The
various parts of the DITL formula that describes a
given behavior are coded into the behavior?s start
or update functions in Unity. Below is one such
C# code snippet: the per-frame transformation for
roll.
(11) transform.rotation = new Vector3(
0.0,0.0,transform.rotation.z+
(rotSpeed
*
deltaTime));
transform.position = new Vector3(
transform.position.x-radius
*
deltaTime,transform.position.y,
transform.position.z);
This ?translates? the DITL expression (y := w; y
6= w; d(b
loc
,y) < d(b
loc
,w))
+
, (z := v; z 6= v; z-b
rot
< v-b
rot
),IN(y,Floor)
+
while explicitly calculat-
ing the value of the precise differences in location
and rotation between each frame or time step. The
variables moveSpeed, rotSpeed and radius
are given explicit value. deltaTime refers to the
time elapsed between frames.
Translating a DITL formula into executable
code makes evident the differences in minimal
verb pairs, such as the ball (or box) rolled and the
ball (or box) slid. When an object rolls, one area
on the object must remain in contact with the sup-
porting surface, and that area must be adjacent to
the area contacting the surface in the previous time
step. When an object slides, the same area on the
object must contact the supporting surface. Com-
pare the per-frame transformation for slide below
to the given transformation for roll.
(12) transform.position = new Vector3(
transform.position.x-radius
*
deltaTime,
transform.position.y,
transform.position.z);
This maps the DITL expression (y := w; y 6= w;
d(b
loc
,y) < d(b
loc
,w),IN(y,Floor))
+
. Here, the ob-
ject?s location changes along a path leading away
from the start location, but does not rotate as in
roll.
DITL expressions and their coded equivalents
can also be composed into new, more specific mo-
tions. The cross formula from Section 4 can be
composed with that for roll to describe a ?roll
across? motion.
In a model, a path verb such as cross does
not necessarily need an explicit manner of mo-
tion specified. In a simulation, the manner needs
to be given a value, requiring the composition of
the path verb (e.g., cross) with one of a certain
subsets of manner verbs specifying how the ob-
ject moves relative to the supporting surface. Be-
low are DITL expressions and code implementa-
tions for two cross predicates, the first a cross mo-
tion while sliding, the second a cross motion while
rolling.
(13) loc(Ball1) := y, target(Ball1) := z; b := y;
(y := w; y 6= w; d(b,y) < d(b,w), d(b,z) >
d(z,w), IN(y,Floor))
+
offset = transform.position-
destination;
offset = Vector3.Normalize(offset);
transform.position = new Vector3(
transform.position.x-offset.x
*
radius
*
deltaTime,
transform.position.y,
transform.position.z-
offset.z
*
radius
*
deltaTime);
At each frame, the distance between the object?s
current position and its previously computed des-
tination is computed again, and the update moves
the object away from its current position (d(b,y) <
d(b,w)) toward the destination (d(b,z) > d(z,w)).
Since no other manner of motion is specified, the
object does not turn or rotate as it moves, but sim-
ply ?slides.?
105
(14) loc(Ball1) := y, target(Ball1) := z; b := y; (y
:= w; y 6= w; d(b
loc
,y) < d(b
loc
,w), d(b
loc
,z)
> d(z,w), (u := v; u 6= v; u-b
rot
< v-b
rot
),
IN(y,Floor))
+
offset = transform.position-
destination;
offset = Vector3.Normalize(offset);
transform.rotation = new Vector3(
0.0,arccos(offset.z)
*
(360/PI
*
2),
transform.rotation.z+
(rotSpeed
*
deltaTime));
transform.position = new Vector3(
transform.position.x-offset.x
*
radius
*
deltaTime,
transform.position.y,
transform.position.z-offset.z
*
radius
*
deltaTime);
Here the update is the same as above, but
with the introduction of the rolling motion. In
both code snippets, the non-changing value of
transform.position.y implicitly maps the
IN RCC condition in the DITL formulas, and
keeps the moving object attached to the floor.
If there exists a behavior corresponding to the
predicate (by name) on an entity bearing the name
of the predicate?s first (subject) argument, the
transformation encoded in that behavior is per-
formed over the entity until an end condition spe-
cific to the behavior is met. The resulting animated
motion depicts the manner of motion denoted by
the predicate. Given a predicate of arity greater
than 1, the simulator tries to prompt a behavior on
the first argument that can be run using parameters
of the subsequent arguments.
A cross behavior, for example, divides the
supporting surface into regions and attempts to
move the crossing object from one region to the
the opposite region. In figure 5, the bounds of
Floor completely surround the bounds of Ball2
(IN(Ball2,Floor) in RCC8). This configuration
makes it possible for the simulation to compute a
motion moving the Ball2 object from one side of
the Floor to the other.
The left side of figure 5 shows a ball rolling and
a box sliding, a depiction of two predicates: Box1
slid and Ball1 rolled. The right side depicts Ball2
crossed Floor (from the rear center to the front
center). The starting state of each scene is over-
laid semi-transparently while the in-progress state
is fully opaque.
6 Discussion and Conclusion
In this paper, we describe a model for mapping
natural language motion expressions into a 3D
simulation environment. Our strategy has been to
use minimal simulation generation as a conceptual
Figure 5: Roll and slide motions in progress (top),
and cross motion in progress (bottom).
debugging tool, in order to tease out the semantic
differences between linguistic expressions; specif-
ically those between verbs that are members of
conventionally homogeneous classes, according to
linguistic analysis.
It should be pointed out that our goal is different
from WordsEye (Coyne and Sproat, 2001). While
we are interested in using simulation generation
to differentiate semantic distinctions in both lex-
ical classes and compositional constructions, the
goal behind WordsEye is to provide an enhanced
interface to allow non-specialists create 3D scenes
without being familiar with special software mod-
els for everyday objects and relations. There are
obvious synergies between these two goals that
can be pursued.
The simulations we create provide an interpre-
tation of the given motion predicate over the given
entity, but not the only interpretation. Just as
Coyne et al. (2010) does for static objects in the
WordsEye system, we must apply some implicit
constraints to our motion predicates to allow them
to be visually simulated. For instance, in the roll
and slide examples given in Figure 5, both objects
are moving in the same direction?parallel to the
back wall of the room object. Had the objects been
moving perpendicular to the back wall or in any
other direction, as long as they remained in con-
106
tact with the floor at all times, the simulated mo-
tion would still be considered a ?roll? (if rotating
around an axis parallel to the floor), or a ?slide?
(if not), regardless of what the precise direction of
motion is. Minimal pairs in a model have to be
compared and contrasted in a discriminative way,
and thus in modeling a slide predicate versus a roll
predicate, knowing that the distinction is one of
rotation parallel to the surface is enough to distin-
guish the two predicates in a model.
In a simulation, the discriminative process re-
quires that the two contrasting behaviors look dif-
ferent, and as such, the simulation software must
be able to completely render a scene for each
frame from behavior start to behavior finish, and
so every variable for every object being rendered
must have an assigned value, including the posi-
tion of the object from frame to frame. If these
values are left unspecified, the software either fails
to compile or throws an exception. Thus, we are
forced to arbitrarily choose a direction of motion
(as well as direction of rotation, speed of rota-
tion, speed of motion, etc.). As long as all non-
changing variables are kept consistent between a
minimal pair of behaviors, we can evaluate the
quantitative and qualitative differences between
the values that do change. As simulations re-
quire values to be assigned to variables that can be
left unspecified in an ordinary modeling process,
simulations expose presuppositions about the se-
mantics of motion verbs and of compositions that
would not be necessary in a model alone.
In order to evaluate the appropriateness of a
given simulation, we are currently experimenting
with a strategy often used in classification and an-
notation tasks, namely pairwise similarity judg-
ments (Rumshisky et al., 2012; Pustejovsky and
Rumshisky, 2014). This involves presenting a user
with a simple discrimination task that has a re-
duced cognitive load, comparing the similarity of
the example to the target instances. In the present
context, a subject is shown a specific simulation
resulting from the translation from textual input,
through DITL, to the visualization. A set of ac-
tivity or event descriptions is given, and the sub-
ject is then asked to select which best describes
the simulation shown; e.g., ?Is this a sliding??, ?Is
this a rolling??. The results of this experiment are
presently being evaluated.
The system is currently in the prototype stage
and needs to be expanded in three main areas: ob-
ject library, parsing pipeline, and predicate han-
dling. Our object and behavior libraries are cur-
rently limited to geometric primitives and the mo-
tions that can be applied over them. While roll,
slide, and cross behaviors can be scripted for
spheres and cubes and shapes derived from them,
a predicate like walk cannot be supported on the
current infrastructure. Thus, we intend to expand
the object library to include more complex inan-
imate objects (tables, chairs, or other household
objects) as well as animate objects. Having an ob-
ject library containing forms capable of executing
greater numbers of predicates will allow us to im-
plement those predicates.
The parsing pipeline described in Section 4 is
only partially implemented, with the only com-
pleted parts being the latter stages, relating a for-
mulas to a scripted behavior and its arguments. We
intend to expand the parsing pipeline to include all
the steps described in this paper: taking input as
a simple natural language sentence, tagging and
parsing it to extract the constituent parts of a pred-
icate/argument representation, and using that out-
put to prompt a behavior in software as a dynamic
event structure. More robust parsing will afford
us the opportunity to expand the diversity of pred-
icates that the software can handle as well (Mc-
Donald and Pustejovsky, 2014). While currently
limited to unary and binary predicates, we need
to extend the capability to ternary predicates and
predicates of greater arity, including the use of ad-
junct phrases and indirect objects. We are in the
process of developing an implementation that uses
Boxer (Curran et al., 2007) so that we can create
first-order models from the dynamic expressions
used here.
Acknowledgements
We would like to thank David McDonald for com-
ments and discussion. We would also like to thank
the reviewers for several substantial suggestions
for improvements and clarifications to the paper.
All remaining errors are of course the responsi-
bilities of the authors. This work was supported
in part by the Department of the Navy, Office of
Naval Research under grant N00014-13-1-0228.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Office of Naval Research.
107
References
Nicholas Asher and Laure Vieu. 1995. Towards a ge-
ometry of common sense: a semantics and a com-
plete axiomatisation of merotopology. In Proceed-
ings of IJCAI95, Montreal, Canada.
Benjamin K. Bergen, Shane Lindsay, Teenie Matlock,
and Srini Narayanan. 2007. Spatial and linguistic
aspects of visual imagery in sentence comprehen-
sion. Cognitive Science, 31(5):733?764.
Benjamin K Bergen. 2012. Louder than words: The
new science of how the mind makes meaning. Basic
Books.
Patrick Blackburn and Johan Bos. 2008. Computa-
tional semantics. THEORIA. An International Jour-
nal for Theory, History and Foundations of Science,
18(1).
Paul M Churchland. 1991. Folk psychology and the
explanation of human behavior. The future of folk
psychology: Intentionality and cognitive science,
pages 51?69.
Bob Coyne and Richard Sproat. 2001. Wordseye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques, pages 487?496.
ACM.
Bob Coyne, Owen Rambow, Julia Hirschberg, and
Richard Sproat. 2010. Frame semantics in text-to-
scene generation. In Knowledge-Based and Intel-
ligent Information and Engineering Systems, pages
375?384. Springer.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 33?36, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David R Dowty. 1979. Word meaning and Mon-
tague grammar: The semantics of verbs and times in
generative semantics and in Montague?s PTQ, vol-
ume 7. Springer.
C. Eschenbach, C. Habel, L. Kulik, et al. 1999. Rep-
resenting simple trajectories as oriented curves. In
FLAIRS-99, Proceedings of the 12th International
Florida AI Research Society Conference, pages 431?
436.
Jerome Feldman. 2006. From molecule to metaphor:
A neural theory of language. MIT press.
Tim Fernando. 2009. Situations in ltl as strings. Infor-
mation and Computation, 207(10):980?999.
Tim Fernando. 2013. Segmenting temporal intervals
for tense and aspect. In The 13th Meeting on the
Mathematics of Language, page 30.
Christian Freksa. 1992. Using orientation information
for qualitative spatial reasoning. Springer.
Antony Galton. 2000. Qualitative Spatial Change.
Oxford University Press, Oxford.
Antony Galton. 2004. Fields and objects in space,
time, and space-time. Spatial Cognition and Com-
putation, 4(1).
Claire Gardent and Karsten Konrad. 2000. Interpreting
definites using model generation. Journal of Lan-
guage and Computation, 1(2):193?209.
Alvin I Goldman. 1989. Interpretation psycholo-
gized*. Mind & Language, 4(3):161?185.
Alvin I Goldman. 2006. Simulating minds: The phi-
losophy, psychology, and neuroscience of mindread-
ing. Oxford University Press.
Will Goldstone. 2009. Unity Game Development Es-
sentials. Packt Publishing Ltd.
Jeroen Groenendijk and Martin Stokhof. 1990. Dy-
namic predicate logic. Linguistics and Philosophy,
14:39?100.
David Harel, Dexter Kozen, and Jerzy Tiuyn. 2000.
Dynamic Logic. The MIT Press, 1st edition.
David Harel. 1984. Dynamic logic. In M. Gabbay
and F. Gunthner, editors, Handbook of Philosophi-
cal Logic, Volume II: Extensions of Classical Logic,
page 497?604. Reidel.
Ray Jackendoff. 1983. Semantics and Cognition. MIT
Press.
Ray Jackendoff. 1996. The proper treatment of mea-
suring out, telicity, and perhaps even quantification
in english. Natural Language & Linguistic Theory,
14(2):305?354.
Karsten Konrad. 2004. Model generation for natural
language interpretation and analysis, volume 2953.
Springer.
Manfred Krifka. 1992. Thematic relations as links be-
tween nominal reference and temporal constitution.
Lexical matters, 2953.
Benjamin Kuipers. 2000. The spatial semantic hierar-
chy. Artificial Intelligence, 119(1):191?233.
Beth Levin. 1993. English verb class and alternations:
a preliminary investigation. University of Chicago
Press.
S.C. Levinson. 2003. Space in Language and Cog-
nition: Explorations in Cognitive Diversity. Lan-
guage, culture, and cognition. Cambridge University
Press.
Inderjeet Mani and James Pustejovsky. 2012. Inter-
preting Motion: Grounded Representations for Spa-
tial Language. Oxford University Press.
108
David McDonald and James Pustejovsky. 2014. On
the representation of inferences and their lexicaliza-
tion. In Advances in Cognitive Systems, volume 3.
G. Miller and W. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
George A Miller and Philip N Johnson-Laird. 1976.
Language and perception. Belknap Press.
Srinivas Narayanan. 1999. Reasoning about actions in
narrative understanding. IJCAI, 99:350?357.
Ralf Naumann. 2001. Aspects of changes: a dynamic
event semantics. Journal of semantics, 18:27?81.
James Pustejovsky and Jessica Moszkowicz. 2011.
The qualitative spatial dynamics of motion. The
Journal of Spatial Cognition and Computation.
James Pustejovsky and Anna Rumshisky. 2014. Deep
semantic annotation with shallow methods. LREC
Tutorial, May.
James Pustejovsky, Anna Rumshisky, Olga Batiukova,
and Jessica Moszkowicz. 2014. Annotation of com-
positional operations with glml. In Harry Bunt, ed-
itor, Computing Meaning, pages 217?234. Springer
Netherlands.
J. Pustejovsky. 1995. The Generative Lexicon. Brad-
ford Book. Mit Press.
James Pustejovsky. 2013a. Dynamic event structure
and habitat theory. In Proceedings of the 6th Inter-
national Conference on Generative Approaches to
the Lexicon (GL2013), pages 1?10. ACL.
James Pustejovsky. 2013b. Where things happen: On
the semantics of event localization. In Proceedings
of ISA-9: International Workshop on Semantic An-
notation.
David Randell, Zhan Cui, and Anthony Cohn. 1992.
A spatial logic based on regions and connections. In
Morgan Kaufmann, editor, Proceedings of the 3rd
Internation Conference on Knowledge Representa-
tion and REasoning, pages 165?176, San Mateo.
Susan Rothstein. 2008. Two puzzles for a theory of
lexical aspect: Semelfactives and degree achieve-
ments. Event structures in linguistic form and in-
terpretation, 5:175.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley,
and James Pustejovsky. 2012. Word sense inven-
tories by non-experts. In LREC, pages 4055?4059.
Jeffrey Mark Siskind. 2011. Grounding the lexi-
cal semantics of verbs in visual perception using
force dynamics and event logic. arXiv preprint
arXiv:1106.0256.
Mark Steedman. 2002. Plans, affordances, and combi-
natory grammar. Linguistics and Philosophy, 25(5-
6):723?753.
Leonard Talmy. 1983. How language structures space.
In Herbert Pick and Linda Acredolo, editors, Spa-
tial Orientation: Theory, Research, and Application.
Plenum Press.
Leonard Talmy. 1985. Lexicalization patterns: seman-
tic structure in lexical forms. In T. Shopen, editor,
Language typology and semantic description Vol-
ume 3:, pages 36?149. Cambridge University Press.
Leonard Talmy. 2000. Towards a cognitive semantics.
MIT Press.
Johan van Benthem, Jan van Eijck, and Vera Ste-
bletsova. 1994. Modal logic, transition systems
and processes. Journal of Logic and Computation,
4(5):811?855.
Johannes Franciscus Abraham Karel van Benthem.
1991. Logic and the flow of information.
Z. Vendler. 1967. Linguistics in philosophy. Cornell
University Press Ithaca.
J. Zwarts. 2005. Prepositional aspect and the algebra
of paths. Linguistics and Philosophy, 28(6):739?
779.
J. Zwarts. 2006. Event shape: Paths in the semantics
of verbs.
109
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
  
Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference 
James Pustejovsky, Adam Meyers, Martha Palmer, Massimo Poesio 
 
Abstract 
Many recent annotation efforts for English 
have focused on pieces of the larger problem 
of semantic annotation, rather than initially 
producing a single unified representation. 
This paper discusses the issues involved in 
merging four of these efforts into a unified 
linguistic structure: PropBank, NomBank, the 
Discourse Treebank and Coreference 
Annotation undertaken at the University of 
Essex. We discuss resolving overlapping and 
conflicting annotation as well as how the 
various annotation schemes can reinforce 
each other to produce a representation that is 
greater than the sum of its parts. 
 
1. Introduction 
 
The creation of the Penn Treebank (Marcus et al 
1993) and the word sense-annotated SEMCOR 
(Fellbaum, 1997) have shown how even limited 
amounts of annotated data can result in major 
improvements in complex natural language 
understanding systems. These annotated corpora 
have led to high-level improvements for parsing 
and word sense disambiguation (WSD), on the 
same scale as previously occurred for Part of 
Speech tagging by the annotation of the Brown 
corpus and, more recently, the British National 
Corpus (BNC) (Burnard, 2000). However, the 
creation of semantically annotated corpora has 
lagged dramatically behind the creation of other 
linguistic resources: in part due to the perceived 
cost, in part due to an assumed lack of theoretical 
agreement on basic semantic judgments, in part, 
finally, due to the understandable unwillingness 
of  research groups to get involved in such an 
undertaking. As a result, the need for such 
resources has become urgent.   
 
Many recent annotation efforts for English have 
focused on pieces of the larger problem of 
semantic annotation, rather than producing a 
single unified representation like Head-driven 
Phrase Structure Grammar (Pollard and Sag 
1994) or the Prague Dependency Tecto-
gramatical Representation (Hajicova & Kucer-
ova, 2002). PropBank (Palmer et al 2005) 
annotates predicate argument structure anchored 
by verbs. NomBank (Meyers, et. al., 2004a) 
annotates predicate argument structure anchored 
by nouns.  TimeBank (Pustejovsky et al 2003) 
annotates the temporal features of propositions 
and the temporal relations between propositions. 
The Penn Discourse Treebank (Miltsakaki et al
2004a/b) treats discourse connectives as 
predicates and the sentences being joined as 
arguments. Researchers at Essex were 
responsible for the coreference markup scheme 
developed in MATE (Poesio et al 1999; Poesio, 
2004a) and have annotated corpora using this 
scheme including a subset of the Penn Treebank 
(Poesio and Vieira, 1998), and the GNOME 
corpus (Poesio, 2004a).  This paper discusses the 
issues involved in creating a Unified Linguistic 
Annotation (ULA) by merging annotation of 
examples using the schemata from these efforts. 
Crucially, all individual annotations can be kept 
separate in order to make it easy to produce 
alternative annotations of a specific type of 
semantic information without need to modify the 
annotation at the other levels. Embarking on 
separate annotation efforts has the advantage of 
allowing researchers to focus on the difficult 
issues in each area of semantic annotation and 
the disadvantage of inducing a certain amount of 
tunnel vision or task-centricity ? annotators 
working on a narrow task tend to see all 
phenomena in light of the task they are working 
on, ignoring other factors. However, merging 
these annotation efforts allows these biases to be 
dealt with. The result, we believe, could be a 
more detailed semantic account than possible if 
the ULA had been the initial annotation effort 
rather than the result of merging. 
 
There is a growing community consensus that 
general annotation, relying on linguistic cues, 
and in particular lexical cues, will produce an 
enduring resource that is useful, replicable and 
portable.  We provide the beginnings of one such 
level derived from several distinct annotation 
efforts. This level could provide the foundation 
for a major advance in our ability to 
automatically extract salient relationships from 
text. This will in turn facilitate breakthroughs in 
message understanding, machine translation, fact 
retrieval, and information retrieval. 
 
2. The Component Annotation Schemata 
 
We describe below existing independent 
annotation efforts, each one of which is focused 
on a specific aspect of the semantic 
representation task: semantic role labeling, 
5
  
coreference, discourse relations, temporal 
relations, etc.  They have reached a level of 
maturity that warrants a concerted attempt to 
merge them into a single, unified representation, 
ULA.  There are several technical and theoretical 
issues that will need to be resolved in order to 
bring these different layers together seamlessly.  
Most of these approaches have annotated the 
same type of data, Wall Street Journal text, so it 
is also important to demonstrate that the 
annotation can be extended to other genres such 
as spoken language.  The demonstration of 
success for the extensions would be the training 
of accurate statistical semantic taggers. 
 
PropBank: The Penn Proposition Bank focuses 
on the argument structure of verbs, and provides 
a corpus annotated with semantic roles, 
including participants traditionally viewed as 
arguments and adjuncts.  An important goal is to 
provide consistent semantic role labels across 
different syntactic realizations of the same verb, 
as in the window in [ARG0 John] broke [ARG1 
the window] and [ARG1 The window] broke. 
Arg0 and Arg1 are used rather than the more 
traditional Agent and Patient to keep the 
annotation as theory-neutral as possible, and to 
facilitate mapping to richer representations.  The 
1M word Penn Treebank II Wall Street Journal 
corpus has been successfully annotated with 
semantic argument structures for verbs and is 
now available via the Penn Linguistic Data 
Consortium as PropBank I (Palmer, et. al., 2005).   
Coarse-grained sense tags, based on groupings of 
WordNet senses, are being added, as well as 
links from the argument labels in the Frames 
Files to FrameNet frame elements.  There are 
close parallels to other semantic role labeling 
projects, such as FrameNet (Baker, et. al., 1998; 
Fillmore & Atkins, 1998; Fillmore & Baker, 
2001), Salsa (Ellsworth, et.al, 2004), Prague 
Tectogrammatics (Hajicova & Kucerova, 2002) 
and IAMTC, (Helmreich, et. al., 2004) 
 
NomBank: The NYU NomBank project can be 
considered part of the larger PropBank effort and 
is designed to provide argument structure for 
instances of about 5000 common nouns in the 
Penn Treebank II corpus (Meyers, et. al., 2004a).  
PropBank argument types and related verb 
Frames Files are used to provide a commonality 
of annotation.  This enables the development of 
systems that can recognize regularizations of 
lexically and syntactically related sentence 
structures, whether they occur as verb phrases or 
noun phrases. For example, given an IE system 
tuned to a hiring scenario (MUC-6, 1995), 
NomBank and PropBank annotation facilitate  
generalization over patterns. PropBank and 
NomBank would both support a single IE pattern 
stating that the object (ARG1) of appoint is John 
and the subject (ARG0) is IBM, allowing a 
system to detect that IBM hired John from each 
of the following strings: IBM appointed John, 
John was appointed by IBM, IBM's appointment 
of John, the appointment of John by IBM and 
John is the current IBM appointee.  
 
Coreference: Coreference involves the detection 
of subsequent mentions of invoked entities, as in 
George Bush,? he?.  Researchers at Essex (UK) 
were responsible for the coreference markup 
scheme developed in MATE (Poesio et al 1999; 
Poesio, 2004a), partially implemented in the 
annotation tool MMAX and now proposed as an 
ISO standard; and have been responsible for the 
creation of two small, but commonly used 
anaphorically annotated corpora ? the Vieira / 
Poesio subset of the Penn Treebank (Poesio and 
Vieira, 1998), and the GNOME corpus (Poesio, 
2004a).   Parallel coreference annotation efforts 
funded by ACE have resulted in similar 
guidelines, exemplified by BBN?s recent 
annotation of Named Entities, common nouns 
and pronouns.   These two approaches provide a 
suitable springboard for an attempt at achieving a 
community consensus on coreference. 
 
Discourse Treebank:  The Penn Discourse 
Treebank (PDTB) (Miltsakaki et al2004a/b) is 
based on the idea that discourse connectives are 
predicates with associated argument structure 
(for details see (Miltsakaki et al2004a, 
Miltsakaki et al2004b). The long-range goal is 
to develop a large scale and reliably annotated 
corpus that will encode coherence relations 
associated with discourse connectives, including 
their argument structure and anaphoric links, 
thus exposing a clearly defined level of discourse 
structure and supporting the extraction of a range 
of inferences associated with discourse 
connectives. This annotation references the Penn 
Treebank annotations as well as PropBank, and 
currently only considers Wall Street Journal text. 
 
TimeBank: The Brandeis TimeBank corpus, 
funded by ARDA, focuses on the annotation of 
all major aspects in natural language text 
associated with temporal and event information 
(Day, et al 2003, Pustejovsky, et al 2004). 
Specifically, this involves three areas of the 
annotation: temporal expressions, event-denoting 
6
  
expressions, and the links that express either an 
anchoring of an event to a time or an ordering of 
one event relative to another. Identifying events 
and their temporal anchorings is a critical aspect  
of reasoning, and without a robust ability to 
identify and extract events and their temporal 
anchoring from a text, the real aboutness of the 
article can be missed.  The core of TimeBank is a 
set of 200 news reports documents, consisting of 
WSJ, DUC, and ACE articles, each annotated to 
TimeML 1.2 specification. It is currently being 
extended to AQUAINT articles. The corpus is 
available from the timeml.org website. 
 
3. Unifying Linguistic Annotations 
  
Since September, 2004, researchers representing 
several different sites and annotation projects 
have begun collaborating to produce a detailed 
semantic annotation of two difficult sentences. 
These researchers aim to produce a single unified 
representation with some consensus from the 
NLP community. This effort has given rise to 
both a listserv email list and this workshop: 
http://nlp.cs.nyu.edu/meyers/pie-in-the-sky.html, 
http://nlp.cs.nyu.edu/meyers/frontiers/2005.html 
The merging operations discussed here would 
seem crucial to the furthering of this effort. 
 
3.1 The Initial Pie in the Sky Example 
 
The following two consecutive sentences have 
been annotated for Pie in the Sky.  
 
Two Sentences From ACE Corpus File 
NBC20001019.1830.0181 
 
? but Yemen's president says the FBI has told 
him the explosive material could only have 
come from the U.S., Israel or two Arab 
countries. 
? and to a former federal bomb investigator, 
that description suggests a powerful 
military-style plastic explosive c-4 that can 
be cut or molded into different shapes. 
 
Although the full Pie-in-the-Sky analysis 
includes information from many different 
annotation projects, the Dependency Structure in 
Figure 1 includes only those components that 
relate to PropBank, NomBank, Discourse 
annotation, coreference and TimeBank. Several 
parts of this representation require further 
explanation. Most of these are signified by the 
special arcs, arc labels, and nodes. Dashed lines 
represent transparent arcs, such as the transparent 
dependency between the argument (ARG1) of 
modal can and the or. Or is transparent in that it 
allows this dependency to pass through it to cut 
and mold. There are two small arc loops -- 
investigator is its own ARG0 and description is 
its own ARG1. Investigator is a relational noun 
in NomBank. There is assumed to be an 
underlying relation between the Investigator 
(ARG0), the beneficiary or employer (the ARG2) 
and the item investigated (ARG1). Similarly, 
description acts as its own ARG1 (the thing 
described). There are four special coreference arc 
labels: ARG0-CF, ARG-ANAPH, EVENT-
ANAPH and ARG1-SBJ-CF. At the target of 
these arcs are pointers referring to phrases from 
the previous sentence or previous discourse. The 
first three of these labels are on arcs with the 
noun description as their source. The ARG0-CF 
label indicates that the phrase Yemen's president 
(**1**) is the ARG0, the one who is doing the 
describing. The EVENT-ANAPH label points to 
a previous mention of the describing event, 
namely the clause: The FBI told him the 
explosive material? (**3**). However, as noted 
above, the NP headed by description represents 
the thing described in addition to the action. The 
ARG-ANAPH label points to the thing that the 
FBI told him the explosive material can only 
come from ? (**2**). The ARG1-SBJ-CF label 
links the NP from the discourse what the bomb 
was made from as the subject with the NP 
headed by explosive as its predicate, much the 
same as it would in a copular construction such 
as: What the bomb was made from is the 
explosive C-4. Similarly, the arc ARG1-APP 
marks C-4 as an apposite, also predicated to the 
NP headed by explosive. Finally, the thick arcs 
labeled SLINK-MOD represent TimeML SLINK 
relations between eventuality variables, i.e.,  the 
cut and molded events are modally subordinate 
to the suggests proposition. The merged 
representation aims to be compatible with the 
projects from which it derives, each of which 
analyzes a different aspect of linguistic analysis. 
Indeed most of the dependency labels are based 
on the annotation schemes of those projects. 
 
We have also provided the individual PropBank, 
NomBank and TimeBank annotations below in 
textual form, in order to highlight potential 
points of interaction. 
 
PropBank:  and [Arg2 to a former federal bomb 
investigator], [Arg0 that description]  
[Rel_suggest.01 suggests]  [Arg1 [Arg1 a powerful 
military-style plastic explosive c-4] that 
7
  
 [ArgM-MOD can] be [Rel_cut.01 cut] or  [Rel_mold.01 
molded] [ArgM-RESULT into different shapes]]. 
 
NomBank: and to a former [Arg2 federal] [Arg1 
bomb] [Rel investigator], that description 
suggests a powerful [Arg2 military] - [Rel style] 
plastic [Arg1 explosive] c-4 that can be cut 
or molded into different shapes. 
 
TimeML: and to a former federal bomb 
investigator, that description [Event = ei1 
suggests]  a powerful military-style plastic 
explosive c-4 that  can be [Event = ei2 modal=?can? cut] 
or  [Event = ei3 modal=?can? molded]  into different 
shapes. <SLINK eventInstanceID = ei1 
subordinatedEventID = ei2 relType = ?Modal?/> 
<SLINK eventInstanceID = ei1 
subordinatedEventID = ei3 relType = ?Modal?/> 
 
  
Figure 1. Dependency Analysis of Sentence 2  
 
Note that the subordinating Events indicated by 
the TimeML SLINKS refer to the predicate 
argument structures labeled by PropBank, and 
that the ArgM-MODal also labeled by PropBank 
contains modality information also crucial to the 
SLINKS. While the grammatical modal on cut 
and mold is captured as an attribute value on the 
event tag, the governing event predicate suggest 
introduces a modal subordination to its internal 
argument, along with its relative clause. While 
this markup is possible in TimeML, it is difficult 
to standardize (or automate, algorithmically) 
since arguments are not marked up unless they 
are event denoting.  
 
3.2 A More Complex Example 
 
To better illustrate the interaction between 
annotation levels, and the importance of merging 
information resident in one level but not 
necessarily in another, consider the sentence 
below which has more complex temporal 
properties than the Pie-in-the-Sky sentences and 
its dependency analysis (Figure 2). 
 
 According to reports, sea trials for a patrol boat 
developed by Kazakhstan are being conducted 
and the formal launch is planned for the 
beginning of April this year.  
 
 
Figure 2.  Dependency Analysis of a Sentence 
with Interesting Temporal Properties 
 
The graph above incorporates these distinct 
annotations into a merged representation, much 
like the previous analysis. This sentence has 
more TimeML annotation than the previous 
sentence.  Note the loops of arcs which show that 
According to plays two roles in the sentence: (1) 
it heads a constituent that is the ARGM-ADV of 
the verbs conducted and planned; (2) it indicates 
that the information in this entire sentence is 
attributed to the reports. This loop is problematic 
in some sense because the adverbial appears to 
modify a constituent that includes itself. In 
actuality, however, one would expect that the 
ARGM-ADV role modifies the sentence minus 
the adverbial, the constituent that you would get 
if you ignore the transparent arc from ARGM-
8
  
ADV to the rest of the sentence.  Alternatively, a 
merging decision may elect to delete the ARGM-
ADV arcs, once the more specific predicate 
argument structure of the sentence adverbial 
annotation is available. 
 
The PropBank annotation for this sentence 
would label arguments for develop, conduct and 
plan, as given below. 
 
 [ArgM-ADV According to reports], [Arg1sea trials for  
[Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0 
by Kazakhstan]] are being  
[Rel_conduct.01 conducted]  and [Arg1 the formal 
launch] is [Rel_plan.01 planned]  
[ArgM-TMP for the beginning of April this year].  
 
NomBank would add arguments for report, trial, 
launch and beginning as follows: 
 
 According to [Rel_report.01 reports], [Arg1 [ArgM-LOC 
sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a 
patrol boat] developed by Kazakhstan] are being 
conducted and the [ArgM-MNR formal] [Rel_launch.01 
launch] is planned for the [[REL_beginning.01 
beginning] [ARG1 of April this year]].  
 
TimeML, however, focuses on the anchoring of 
events to explicit temporal expressions (or 
document creation dates) through TLINKs, as 
well as subordinating relations, such as those 
introduced by modals, intensional predicates, 
and other event-selecting predicates, through 
SLINKs. For discussion, only part of the 
complete annotation is shown below.  
  
According to [Event = ei1  reports], sea [Event = ei3  
trials] for a boat [Event = ei4  developed]  by 
Kazakhstan are being [Event = ei5  conducted] and 
the formal [Event = ei6  launch] 
 is  [Event = ei7  planned] for the [Timex3= t1  beginning 
of April] [Timex3= t2 this year]. 
<SLINK eventID=?ei1? subordinatedEvent=?ei5, 
ei7? relType=EVIDENTIAL/> 
<TLINK eventID=?ei4? relatedToEvent =?ei3? 
relType=BEFORE/> 
<TLINK eventID=?ei6? relatedToTime=?t1? 
relType=IS_INCLUDED /> 
<SLINK eventID=?ei7? 
subordinatedEvent=?ei6? relType=?MODAL?/> 
<TLINK eventID=?ei5? relatedToEvent=?ei3? 
relType=IDENTITY/> 
 
Predicates such as plan and nominals such as 
report are lexically encoded to introduce 
SLINKs with a specific semantic relation, in this 
case, a ?MODAL? relType,. This effectively 
introduces an intensional context over the 
subordinated events. 
 
These examples illustrate the type of semantic 
representation we are trying to achieve.  It is 
clear that our various layers already capture 
many of the intended relationships, but they do 
not do so in a unified, coherent fashion.  Our 
goal is to develop both a framework and a 
process for annotation that allows the individual 
pieces to be automatically assembled into a 
coherent whole.   
 
4.0 Merging Annotations  
 
4.1 First Order Merging of Annotation 
We begin by discussing issues that arise in 
defining a single format for a merged 
representation of PropBank, NomBank and 
Coreference, the core predicate argument 
structures and  referents for the arguments.   One 
possible representation format would be to 
convert each annotation into features and values 
to be added to a larger feature structure. 1 The 
resulting feature structure would combine stand 
alone and offset annotation ? it would include 
actual words and features from the text as well as 
special features that point to the actual text 
(character offsets) and, perhaps, syntactic trees 
(offsets along the lines of PropBank/NomBank). 
Alternative global annotation schemes include 
annotation graphs (Cieri & Bird, 2001), and 
MATE (Carletta, et. al., 1999).  There are many 
areas in which the boundaries between these 
annotations have not been clearly defined, such 
as the treatment of support constructions and 
light verbs, as discussed below.  Determining the 
most suitable format for the merged 
representation should be a top priority. 
 
4.2 Resolving Annotation Overlap 
There are many possible interactions between 
different types of annotation: aspectual verbs 
have argument labels in PropBank, but are also 
important roles for temporal relations.  Support 
                                                 
 
1 The Feature Structure has many advantages as a target 
representation including: (1) it is easy to add lots of detailed 
features; and (2) the mathematical properties of Feature 
Structures are well understood, i.e., there are well-defined 
rule-writing languages, subsumption and unification 
relations, etc. defined for Feature Structures (Carpenter, 
1992) The downside is that a very informative Feature 
Structure is difficult for a human to read.  
 
9
  
constructions also have argument labels, and the 
question arises as to whether these should be 
associated with the support verb or the 
predicative nominal.  Given the sentence They 
gave the chefs a standing ovation, a PropBank 
component will assign role labels to arguments 
of give; a NomBank component will assign 
argument structure to ovation that labels the 
same participants. If the representations are 
equivalent, the question arises as to which of 
them (or both) should be included in the merged 
representation. The following graph  (Figure 3) 
is a combined PropBank and NomBank analysis 
of this sentence. "They" is the ARG0 of both 
"give" and "ovation"; "the chefs" is the ARG2 of 
"give", but the "ARG1" of ovation; "ovation" is 
the ARG1 of "give" and "give" is a support verb 
for "ovation". For this case, a reasonable choice 
might be to preserve the argument structure from 
both NomBank and PropBank, and to do the 
same for other predicative nominals that have 
give (or receive, obtain, request?) as a support 
verb, e.g., (give a kiss/hug/squeeze, give a 
lecture/speech, give a promotion, etc.).   For 
other support constructions, such as take a walk, 
have a headache and make a mistake, the noun is 
really the main predicate and it is questionable 
whether the verbal argument structure carries  
gave
chefsthe
They
a ovationstanding
NP
NP
S
ARG0
REL
ARG2
ARG1
NP
ARG1 REL
ARG0SUPPORT
 
Figure 3. Merged PropBank/NomBank representation 
of They gave the chefs a standing ovation. 
much information, e.g., there are no selection 
restrictions between light verbs and their subject 
(ARG0) -- these are inherited from the noun. 
Thus make a mistake selects a different type of 
subject than make a gain, e.g., people and 
organizations make mistakes, but stock prices 
make gains. For these constructions, the merged 
representation might not need to include the 
(ARG0) relation between the subject of the 
sentence and make, and future propbanking 
efforts might do well to ignore the shared 
arguments of such instances and leave them for 
NomBank. However, the merged representation 
would inherit PropBank?s annotation of some 
other light verb features including: negation, e.g., 
They did not take a walk; modality, e.g., They 
might take a walk; and sentence adverbials, e.g., 
They probably will take a walk. 
 
4.3 Resolving Annotation Conflicts 
Interactions between linguistic phenomena can 
aid in quality control, and conflicts found during 
the deliberate merging of different annotations 
provides an opportunity to correct and fine-tune 
the original layers. For example, predicate 
argument structure (PropBank and NomBank) 
annotation sometimes assumes different 
constituent structure than the Penn Treebank. We 
have noticed some tendencies that help resolve 
these conflicts, e.g., prenominal noun 
constituents as in Indianapolis 500, which forms 
a single argument in NomBank, is correctly 
predicted to be a constituent, even though the 
Penn Treebank II assumes a flatter structure.  
 
Similarly, idioms and multiword expressions 
often cause problems for both PropBank and 
NomBank. PropBank annotators tend to view 
argument structure in terms of verbs and 
NomBank annotators tend to view argument 
structure in terms of nouns. Thus many examples 
that, perhaps, should be viewed as idioms are 
viewed as special senses of either verbs or nouns. 
Having idioms detected and marked before 
propbanking and nombanking could greatly 
improve efficiency.   
 
Annotation accuracy is often evaluated in terms 
of inter-annotation consistency. Task definitions 
may need to err on the side of being more 
inclusive in order to simplify the annotators task. 
For example, the NomBank project assumes the 
following definition of a support verb (Meyers, 
et.al., 2004b):  ?? a verb which takes at least 
two arguments NP1 and XP2 such that XP2 is an 
argument of the head of NP1. For example, in 
John took a walk, a support verb (took) shares 
one of its arguments (John) with the head of its 
other argument (walk).? The easiest way to 
apply this definition is without exception, so it 
will include idiomatic expressions such as keep 
tabs on, take place, pull strings. Indeed, the 
dividing line between support constructions and 
idioms is difficult to draw (Meyers 2004b).   
PropBank annotators are also quite comfortable 
with associating general meanings to the main 
verbs of idiomatic expressions and labeling their 
10
  
argument roles, as in cases like bring home the 
bacon and mince words with. Since idioms often 
have interpretations that are metaphorical 
extensions of their literal meaning, this is not 
necessarily incorrect.  It may be helpful to have 
the literal dependencies and the idiomatic 
reading both represented. The fact that both 
types of meaning are available is evidenced by 
jokes, irony, and puns.  
 
With respect to idioms and light verbs, TimeML 
can be viewed as a mediator between PropBank 
and NomBank. In TimeML, light verbs and the 
nominalizations accompanying them are marked 
with two separate EVENT tags. This guarantees 
an annotation independent of textual linearity 
and therefore ensures a parallel treatment for 
different textual configurations. In (a) the light 
verb construction "make an allusion" is 
constituted of a verb and an NP headed by an 
event-denoting noun, whereas in (b) the nominal 
precedes a VP, which in addition contains a 
second N:  
(a) Max [made an allusion] to the crime.  
(b) Several anti-war [demonstrations have taken 
place] around the globe. 
Both verbal and nominal heads are tagged 
because they both contribute relevant 
information to characterizing the nature of the 
event. The nominal element plays a role in the 
more semantically based task of event 
classification. On the other hand, the information 
in the verbal component is important at two 
different levels: it provides the grammatical 
features typically associated with verbal 
morphology, such as tense and aspect, and at the 
same time it may help in disambiguating cases 
like take/give a class, make/take a phone call. 
The two tagged events are marked as identical by 
a TLINK introduced for that purpose. The 
TimeML annotation for the example in (a) is 
provided below.  
Max [Event = ei1  made] an [Event = ei2  allusion] to 
the crime.  
<TLINK eventID="ei1"relatedToEvent="ei2" 
relType=IDENTITY> 
Some cases of support in NomBank could also 
be annotated as "bridging" anaphora. Consider 
the sentence: The pieces make up the whole. 
It is unclear whether make up is a support verb 
linking whole as the ARG1 of pieces or if pieces 
is linked to whole by bridging anaphora.  
There are also clearer cases. In Nastase, a rival 
player defeated Jimmy Connors in the third 
round, the word rival and Jimmy Connors are 
clearly linked by bridging. However, a wayward 
NomBank annotator might construct a support 
chain (player + defeated) to link rival with its 
ARG1 Jimmy Connors.  In such a case, a 
merging of annotation could reveal annotation 
errors. In contrast, a NomBank annotator would 
be correct in linking John as an argument of walk 
in John took a series of walks (the support chain 
took + series consists of a support verb and a 
transparent noun), but this may not be obvious to 
the non-NomBanker. Thus the merging of 
annotation may result in the more consistent 
specifications for all.  
 
In our view, this process of annotating all layers 
of information and then merging them in a 
supervised manner, taking note of the conflicts, 
is a necessary prerequisite to defining more 
clearly the boundaries between the different 
types of annotation and determining how they 
should fit together.  Other areas of annotation 
interaction include: (1) NomBank  and 
Coreference, e.g. deriving that John teaches 
Mary from John is Mary's teacher involves: (a) 
recognizing that teacher is an argument 
nominalization such that the teacher is the ARG0 
of teach (the one who teaches); and (b) marking 
John and teacher as being linked by predication 
(in this case, an instance of type coreference); 
and (2) Time and Modality -  when a fact used to 
be true, there are two time components: one in 
which the fact is true and one in which it is false. 
Clearly more areas of interaction will emerge as 
more annotation becomes available and as the 
merging of annotation proceeds.  
 
5. Summary 
 
We proposed a way of taking advantage of the 
current practice of separating aspects of semantic 
analysis of text into small manageable pieces. 
We propose merging these pieces, initially in a 
careful, supervised way, and hypothesize that the 
result could be a more detailed semantic analysis 
than was previously available. This paper 
discusses some of the reasons that the merging 
process should be supervised. We primarily gave 
examples involving the interaction of PropBank, 
NomBank and TimeML. However, as the 
merging process continues, we anticipate other 
conflicts that will require resolution. 
 
References 
 
C. F. Baker, F. Collin, C. J. Fillmore, and J. B.  
Lowe (1998), The Berkeley FrameNet 
project. In Proc. of COLING/ACL-98,  86--90 
11
  
O. Babko-Malaya, M. Palmer, X. Nianwen, S.  
Kulick, A. Joshi (2004), Propbank II, 
Delving Deeper, In Proc.  of HLT-NAACL 
Workshop: Frontiers in Corpus Annotation. 
R. Carpenter (1992), The Logic of Typed  
Feature Structures. Cambridge Univ. Press. 
J. Carletta and A. Isard (1999), The MATE  
Annotation Workbench: User Requirements. 
In Proc. of the ACL Workshop: Towards 
Standards and Tools for Discourse Tagging. 
Univ. of Maryland, 11-17 
C. Cieri and S. Bird (2001), Annotation Graphs  
and Servers and Multi-Modal Resources: 
Infrastructure for  Interdisciplinary Education, 
Research and Development Proc. of the ACL 
Workshop on Sharing Tools and Resources 
for Research  and Education, 23-30 
D. Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.  
Lazo, J. Pustejovsky, R. Saur?, A. See, A. 
Setzer, and B. Sundheim (2003), The 
TIMEBANK Corpus. Corpus Linguistics. 
M. Ellsworth, K. Erk, P. Kingsbury and S. Pado  
(2004), PropBank, SALSA, and FrameNet: 
How Design Determines Product, in Proc. of  
LREC 2004 Workshop: Building Lexical 
Resources from Semantically Annotated 
Corpora.  
C. Fellbaum (1997), WordNet: An Electronic  
Lexical Database, MIT Press.. 
C. J. Fillmore and B. T. S. Atkins (1998), 
FrameNet and lexicographic relevance. In the 
Proc. of the First International Conference 
on Language Resources and Evaluation.  
C. J. Fillmore and C. F. Baker (2001), Frame  
semantics for text understanding. In Proc. of 
NAACL WordNet and Other Lexical 
Resources Workshop. 
E. Hajivcova and I. Kuvcerov'a (2002).  
Argument/Valency Structure in PropBank, 
LCS Database and Prague Dependency 
Treebank: A Comparative Pilot Study. In the 
Proc. of the Third International Conference 
on Language Resources and Evaluation 
(LREC 2002),  846--851. 
S. Helmreich, D. Farwell, B. Dorr, N. Habash, L. 
    Levin, T. Mitamura, F. Reeder, K. Miller, E. 
     Hovy, O. Rambow and A. Siddharthan,(2004), 
     Interlingual Annotation of Multilingual Text 
     Corpora, Proc. of the HLT-EACL Workshop 
     on Frontiers in Corpus Annotation. 
A, Meyers, R. Reeves, C. Macleod, R, Szekely,  
V. Zielinska, B. Young, and R. Grishman  
(2004a), The NomBank Project: An Interim 
Report, Proc. of HLT-EACL Workshop: 
Frontiers in Corpus Annotation. 
A. Meyers, R. Reeves, and C. Macleod (2004b),  
NP-External Arguments: A Study of 
Argument Sharing in English. In The ACL 
2004 Workshop on Multiword Expressions: 
Integrating Processing. 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber. 
 (2004a), The Penn Discourse Treebank. In 
Proc. 4th International Conference on 
Language Resources and Evaluation (LREC 
2004). 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber  
(2004b), Annotation of Discourse 
Connectives and their Arguments, in Proc. of 
HLT-NAACL Workshop: Frontiers in Corpus 
Annotation 
M.  Marcus, B. Santorini, and M. Marcinkiewicz  
(1993), Building a large annotated corpus of 
english: The penn treebank. Computational 
Linguistics, 19:313--330. 
M. Palmer, D. Gildea, P. Kingsbury (2005), The  
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics 
Journal, 31:1. 
M. Poesio (2004a), The MATE/GNOME  
Scheme for Anaphoric Annotation, Revisited, 
Proc. of SIGDIAL 
M. Poesio (2004b), Discourse Annotation and  
Semantic Annotation in the GNOME Corpus, 
Proc. of ACL Workshop on Discourse 
Annotation. 
M. Poesio and M. Alexandrov-Kabadjov (2004), 
A general-purpose, off-the-shelf system for 
anaphora resol.. Proc. of LREC. 
M. Poesio, F. Bruneseaux, and L. Romary  
(1999), The MATE meta-scheme for 
coreference in dialogues in multiple language, 
Proc. of the ACL Workshop on Standards for 
Discourse Tagging.  
M. Poesio and R. Vieira (1998), A corpus-based  
investigation of definite description use. 
Computational Linguistics, 24(2). 
C. Pollard and I. A. Sag (1994), Head-driven  
phrase structure grammar. Univ. of Chicago 
Press. 
J. Pustejovsky, R. Saur?, J. Casta?o, D. R. 
 Radev, R. Gaizauskas, A. Setzer, B. 
Sundheim and G. Katz (2004), Representing 
Temporal and Event Knowledge for QA 
Systems. In Mark T. Maybury (ed.), New 
Directions in Question Answering, MIT Press. 
J. Pustejovsky,  B. Ingria, R. Saur?, J. Casta?o, J.  
Littman, R. Gaizauskas, A. Setzer, G. Katz, 
and I. Mani (2003), The Specification 
Language TimeML. In I. Mani, J. 
Pustejovsky, and R. Gaizauskas, editors, The 
Language of Time: A Reader. Oxford Univ. 
Press. 
12
Proceedings of the 8th International Conference on Computational Semantics, pages 169?180,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
GLML: Annotating Argument
Selection and Coercion
James Pustejovsky, Anna Rumshisky,
Jessica L. Moszkowicz, Olga Batiukova
Abstract
In this paper we introduce a methodology for annotating compo-
sitional operations in natural language text, and describe a mark-up
language, GLML, based on Generative Lexicon, for identifying such
relations. While most annotation systems capture surface relation-
ships, GLML captures the ?compositional history? of the argument
selection relative to the predicate. We provide a brief overview of GL
before moving on to our proposed methodology for annotating with
GLML. There are three main tasks described in the paper: (i) Com-
positional mechanisms of argument selection; (ii) Qualia in modifica-
tion constructions; (iii) Type selection in modification of dot objects.
We explain what each task includes and provide a description of the
annotation interface. We also include the XML format for GLML in-
cluding examples of annotated sentences.
1 Introduction
1.1 Motivation
In this paper, we introduce a methodology for annotating compositional
operations in natural language text. Most annotation schemes encoding
?propositional? or predicative content have focused on the identification
of the predicate type, the argument extent, and the semantic role (or label)
assigned to that argument by the predicate (see Palmer et al, 2005, Ruppen-
hofer et al, 2006, Kipper, 2005, Burchardt et al, 2006, Ohara, 2008, Subirats,
2004).
The emphasis here will be on identifying the nature of the composi-
tional operation rather than merely annotating the surface types of the en-
tities involved in argument selection.
169
Consider the well-known example below. The distinction in semantic
types appearing as subject in (1) is captured by entity typing, but not by any
sense tagging from, e.g., FrameNet (Ruppenhofer et al, 2006) or PropBank
(Palmer et al, 2005).
(1) a. Mary called yesterday.
b. The Boston office called yesterday.
While this has been treated as type coercion or metonymy in the literature (cf.
Hobbs et al, 1993 , Pustejovsky, 1991, Nunberg, 1979, Egg, 2005), the point
here is that an annotation using frames associated with verb senses should
treat the sentences on par with one another. Yet this is not possible if the
entity typing given to the subject in (1a) is HUMAN and that given for (1b)
is ORGANIZATION.
The SemEval Metonymy task (Markert and Nissim, 2007) was a good
attempt to annotate such metonymic relations over a larger data set. This
task involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people, place-for-event,
place-for-product;
ii. Categories for Organizations: literal, organization-for-members,
organization-for-event, organization-for-product, organization-for-fa-
cility.
One of the limitations with this approach, however, is that, while appropri-
ate for these specialized metonymy relations, the annotation specification
and resulting corpus are not an informative guide for extending the anno-
tation of argument selection more broadly.
In fact, the metonymy example in (1) is an instance of a much more
pervasive phenomenon of type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for the verb enjoy should
arguably assign similar values to both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .
b. Mary enjoyed her beer.
The consequence of this, however, is that, under current sense and role an-
notation strategies, the mapping to a syntactic realization for a given sense
is made more complex, and is, in fact, perplexing for a clustering or learn-
ing algorithm operating over subcategorization types for the verb.
170
1.2 Theoretical Preliminaries
The theoretical foundations for compositional operations within the sen-
tence have long been developed in considerable detail. Furthermore, type
shifting and type coercion operations have been recognized as playing an
important role in many formal descriptions of language, in order to main-
tain compositionality (cf. Partee and Rooth, 1983; Chierchia, 1998; Groe-
nendijk and Stokhof, 1989; Egg, 2005; Pinkal, 1999; Pustejovsky, 1995, and
many others). The goal of the present work is to: (a) create a broadly appli-
cable specification of the compositional operations involved in argument
selection; (b) apply this specification over a corpus of natural language
texts, in order to encode the selection mechanisms implicated in the com-
positional structure of the language.
The creation of a corpus that explicitly identifies the ?compositional his-
tory? associated with argument selection will be useful to computational
semantics in several respects: (a) the actual contexts within which type
coercions are allowed can be more correctly identified and perhaps gen-
eralized; (b) machine learning algorithms can take advantage of the map-
ping as an additional feature in the training phase; and (c) some consensus
might emerge on the general list of type-changing operations involved in
argument selection, as the tasks are revised and enriched.
For the purpose of this annotation task, we will adopt the general ap-
proach to argument selection within Generative Lexicon, as recently out-
lined in Pustejovsky (2006) and Asher and Pustejovsky (2006). We can dis-
tinguish the following modes of composition in natural language:
(4) a. PURE SELECTION (Type Matching): the type a function requires is
directly satisfied by the argument;
b. ACCOMMODATION: the type a function requires is inherited by the
argument;
c. TYPE COERCION: the type a function requires is imposed on the
argument type. This is accomplished by either:
i. Exploitation: taking a part of the argument?s type;
ii. Introduction: wrapping the argument with the required type.
Each of these will be identified as a unique relation between the predicate
and a given argument. In this annotation effort, we restrict the possible
relations between the predicate and a given argument to selection and coer-
cion. A more fine-grained typology of relations may be applied at a later
171
point. Furthermore, qualia structure values1 are identified in both argu-
ment selection and modification contexts.
The rest of this document proceeds as follows. In Section 2, we describe
our general methodology and architecture for GL annotation. Section 3
gives an overview of each of the annotation tasks as well as some details
on the resulting GLMLmarkup. Amore thorough treatment of thematerial
we present, including the complete GLML specification and updates on the
annotation effort can be found at www.glml.org.
2 General Methodology and Architecture
In this section, we describe the set of tasks for annotating compositional
mechanisms within the GL framework. The current GL markup will in-
clude the following tasks, each of which is described below in Section 3.
(5) a. Mechanisms of Argument Selection: Verb-based Annotation
b. Qualia in Modification Constructions
c. Type Selection in Modification of Dot Objects
2.1 System Architecture
Each GLML annotation task involves two phases: the data set construction
phase and the annotation phase. The first phase consists of (1) selecting the
target words to be annotated and compiling a sense inventory for each tar-
get, and (2) data extraction and preprocessing. The prepared data is then
loaded into the annotation interface. During the annotation phase, the an-
notation judgments are entered into the database, and the adjudicator re-
solves disagreements. The resulting database representation is used by the
exporting module to generate the corresponding XML markup, stand-off
annotation, or GL logical form.
These steps will differ slightly for each of the major GLML annotation
tasks. For example, Task 1 focuses on annotating compositional processes
between the verbs and their arguments. The first step for this task involves
(1) selecting the set of target verbs, (2) compiling a sense inventory for each
1The qualia structure, inspired by Moravcsik (1975)?s interpretation of the aitia of Aris-
totle, is defined as the modes of explanation of a word or phrase, and defined below (Puste-
jovsky, 1991): (a) FORMAL: the category distinguishing the meaning of a word within a
larger domain; (b) CONSTITUTIVE: the relation between an object and its constituent parts;
(c) TELIC: the purpose or function of the object, if there is one; (d) AGENTIVE: the factors
involved in the object?s origins or ?coming into being?.
172
target, and (3) associating a type template or a set of templates with each
sense. Since the objective of the task is to annotate coercion, our choices
must include the verbs that exhibit the coercive behavior at least in some of
their senses.
At the next step, the data containing the selected target words is ex-
tracted from a corpus and preprocessed. Since the GLML annotation is
intra-sentential, each extracted instance is a sentence. Sentences are parsed
to identify the relevant arguments, adjuncts or modifiers for each target.
The data is presented to the annotatator with the target word and the head-
word of the relevant phrase highlighted.
Due to the complexity of the GLML annotation, we chose to use the
task-based annotation architecture. The annotation environment is designed
so that the annotator can focus on one facet of the annotation at a time.
Thus, in Task 1, the verbs are disambiguated by the annotator in one sub-
task, and the annotation of the actual compositional relationship is done in
another subtask. Figure 1 shows an example of the interface for the verb-
based annotation task .
Figure 1: Example of Annotation Interface for GLML Annotation
173
2.2 The Type System for Annotation
The type system we have chosen for annotation is purposefully shallow,
but we also aimed to include types that would ease the complexity of the
annotation task. The type system is not structured in a hierarchy, but rather
it is presented as a set of types. For example, we include both HUMAN and
ANIMATE in the type system along with PHYSICAL OBJECT. While HUMAN
is a subtype of both ANIMATE and PHYSICAL OBJECT, the annotator does
not need to be concerned with this. This allows the annotator to simply
choose the HUMAN type when necessary rather than having to deal with
type inheritance.
While the set of types for GLML annotation can easily be modified, the
following list is currently being used:
(6) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, IN-
FORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION,
PROPERTY, OBLIGATION, AND RULE
3 Annotation Tasks
In this section, we describe the annotation process: the steps involved in
each task and the way they are presented to the annotators. In this paper,
we focus on the task descriptions rather than an in depth review of the
annotation interface and the resulting GLML markup.
The general methodology for each task is as follows: 1) Select a target
set of words and compile a sense inventory for each one, 2) Select a set of
sentences for each target, 3) Disambiguate the sense of the target in a given
sentence, and 4) Answer questions specific to the annotation task in order
to create the appropriate GLML link.
3.1 Mechanisms of Argument Selection: Verb-based Annotation
This annotation task involves choosing which selectional mechanism is
used by the predicate over a particular argument. The possible relations
between the predicate and a given argument will, for now, be restricted
to selection and coercion. In selection, the argument NP satisfies the typ-
ing requirements of the predicate, as in The child threw the stone (PHYS-
ICAL OBJECT). Coercion encompasses all cases when a type-shifting op-
eration (exploitation or introduction) must be performed on the comple-
ment NP in order to satisfy selectional requirements of the predicate, as in
The White House (LOCATION ? HUMAN) denied this statement.
174
An initial set of verbs and sentences containing them has been selected
for annotation. For each sentence, the compositional relationship of the
verb with every argument and adjunct will be annotated. The target types
for each argument are provided in a type template that is associated with
the sense of the verb in the given sentence. For example, one of the senses
of the verb deny (glossed as ?State or maintain that something is untrue?)
would have the following type template: HUMAN deny PROPOSITION.
In the first subtask, the annotator is presented with a set of sentences
containing the target verb and the chosen grammatical relation. The anno-
tator is asked to select the most fitting sense of the target verb, or to throw
out the example (pick the ?N/A? option) if no sense can be chosen either
due to insufficient context, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be made in good faith.
Next, the annotator is presented with a list of sentences in which the
target verb is used in the same sense and is asked to determine whether the
argument in the specified grammatical relation belongs to the type speci-
fied in the corresponding template. If the argument belongs to the appro-
priate type, the ?yes? box is clicked, generating a CompLink with comp-
Type=?SELECTION?. If ?no? is selected, a type selection menu pops up
below the first question, and the annotator is asked to pick a type from a
list of shallow types which is usually associated with the argument. Con-
sequently, a CompLink with compType=?COERCION? is created with the
corresponding source and target type.
The following example of GLMLmarkup is generated from the database2:
Sir Nicholas Lyell, Attorney General, denies a cover-up.
<SELECTOR sid="s1">denies</SELECTOR>
a <NOUN nid="n1">cover-up</NOUN> .
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="dobj"
compType="COERCION" sourceType="EVENT" targetType="PROPOSITION"/>
3.2 Qualia Selection in Modification Constructions
For this task, the relevant semantic relations are defined in terms of the
qualia structure. We examine two kinds of constructions in this task: adjec-
tival modification of nouns and nominal compounds3.
2While we present these examples as an inline annotation, a LAF (Ide and Romary, 2003)
compliant offset annotation is fully compatible with GLML.
3Since target nouns have already been selected for these two tasks, it is also possible
to annotate qualia selection in verb-noun contexts such as Can you shine the lamp over here?
(TELIC). However, here we focus solely on the modification contexts mentioned here.
175
3.2.1 Adjectival Modification of Nouns
This task involves annotating how particular noun qualia values are bound
by the adjectives. Following Pustejovsky (2000), we assume that the prop-
erties grammatically realized as adjectives ?bind into the qualia structure
of nouns, to select a narrow facet of the noun?s meaning.? For example, in
the NP ?a sharp metal hunting knife?, sharp refers to the knife as a physi-
cal object, its FORMAL type, metal is associated with a material part of the
knife (CONSTITUTIVE), and hunting is associatedwith how the knife is used
(TELIC). Similarly, forged in ?a forged knife? is associated with the creation
of the knife (AGENTIVE).
The task begins with sense disambiguation of the target nouns. Ques-
tions are then used to help the annotator identify which qualia relations are
selected. For example, the TELIC question for the noun table would be ?Is
this adjective associated with the inherent purpose of table?? These ques-
tions will change according to the type associated with the noun. Thus,
for natural types such as woman, the TELIC question would be ?Is this ad-
jective associated with a specific role of woman?? Similarly, for the AGEN-
TIVE role, the question corresponding to the PHYSICAL OBJECT-denoting
nouns refers to the ?making or destroying? the object, while for the EVENT-
denoting nouns, the same question involves ?beginning or ending? of the
event. QLinks are then created based on the annotator?s answers, as in the
following example:
The walls and the wooden table had all been lustily scrubbed.
<SELECTOR sid="s1">wooden</SELECTOR>
<NOUN nid="n1">table</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="CONST"/>
3.2.2 Nominal Compounds
This task explores the semantic relationship between elements in nominal
compounds. The general relations presented in Levi (1978) are a useful
guide for beginning a classification of compound types, but the relations
between compound elements quickly prove to be too coarse-grained. War-
ren?s comprehensive work (Warren, 1978) is a valuable resource for differ-
entiating relation types between compound elements.
The class distinction in compound types in language can be broken
down into three forms (Spencer, 1991): endocentric compounds, exocen-
tric compounds, and dvandva compounds. Following Bisetto and Scalise
176
(2005), however, it is possible to distinguish three slightly differently con-
structed classes of compounds, each exhibiting endocentric and exocentric
behavior: subordinating, attributtive, and coordinate.
We will focus on the two classes of subordinating and attributive com-
pounds. Within each of these, we will distinguish between synthetic and
non-synthetic compounds. The former are deverbal nouns, and when act-
ing functionally (subordinating), take the sister noun as an argument, as
in bus driver and window cleaner. The non-synthetic counterparts of these
include pastry chef and bread knife, where the head is not deverbal in any
obvious way. While Bisetto and Scalise?s distinction is a useful one, it does
little to explain how non-relational sortal nouns such as chef and knife act
functionally over the accompanying noun in the compound, as above.
This construction has been examined within GL by Johnston and Busa
(1999). We will assume much of that analysis in our definition of the task
described here. Our basic assumption regarding the nature of the seman-
tic link between both parts of compounds is that it is generally similar to
the one present in adjectival modification. The only difference is that in
nominal compounds, for instance, the qualia of a head noun are activated
or exploited by a different kind of modifier, a noun. Given this similar-
ity, the annotation for this task is performed just as it is for the adjectival
modification task. A QLink is created as in the following example:
Our guest house stands some 100 yards away.
<SELECTOR sid="s1">guest</SELECTOR>
<NOUN nid="n1">house</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="TELIC"/>
3.3 Type Selection in Modification of Dot Objects
This task involves annotating how particular types within dot objects are
exploited in adjectival and nominal modification constructions. Dot objects
or complex types (Pustejovsky, 1995) are defined as the product of a type
constructor ? (?dot?), which creates dot objects from any two types a and
b , creating a ? b. Complex types are unique because they are made up of
seemingly incompatible types such as FOOD and EVENT.
Given a complex type c = a ? b, there are three possible options: 1) the
modifier applies to both a and b, 2) the modifier applies to a only, or 3) the
modifier applies to b only. Option 1 would be illustrated by examples such
as good book [+info, +physobj] and long test [+info, +event]. Examples such as
177
delicious lunch [+food, -event] and long lunch [-food, +event] illustrate options
2 and 3. A listing of dot objects can be found in Pustejovsky (2005).
The sense inventory for the collection of dot objects chosen for this task
will include only homonyms. That is, only contrastive senses such as the
river bank versus financial institution for bank will need to be disambiguated.
Complementary senses such as the financial institution itself versus the
building where it is located are not included.
In order to create the appropriate CompLink, the annotator will select
which type from a list of component types for a given dot object is exploited
in the sentence. The resulting GLML is:
After a while more champagne and a delicious lunch was served.
<SELECTOR sid="s1">delicious</SELECTOR>
<NOUN nid="n1">lunch</NOUN>
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="mod"
compType="SELECTION" sourceType="[PHYS_OBJ,EVENT]"
targetType="PHYS_OBJ" />
4 Conclusion
In this paper, we approach the problem of annotating the relation between
a predicate and its argument as one that encodes the compositional history
of the selection process. This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of a predicate from those
that are accommodated or coerced in context. We described a specification
language for selection, GLML, based largely on the type selective opera-
tions in GL, and three annotation tasks using this specification to identify
argument selection behavior.
There are clearly many compositional operations in language that have
not been addressed in this paper. The framework is general enough, how-
ever, to describe a broad range of type selective behavior. As the tasks be-
come more refined, the extensions will also become clearer. Furthermore,
as other languages are examined for annotation, new tasks will emerge re-
flecting perhaps language-specific constructions.
Acknowledgements
The idea for annotating a corpus according to principles of argument selec-
tion within GL arose during a discussion at GL2007 in Paris, between one
178
of the authors (J. Pustejovsky) and Nicoletta Calzolari and Pierrette Bouil-
lon. Recently, the authors met with other members of the GLML Working
Group in Pisa at the ILC (September 23-25, 2008). We would like to thank
the members of that meeting for their fruitful feedback and discussion on
an earlier version of this document. In particular, we would like to thank
Nicoletta Calzolari, Elisabetta Jezek, Alessandro Lenci, Valeria Quochi, Jan
Odijk, Tommaso Caselli, Claudia Soria, Chu-Ren Huang, Marc Verhagen,
and Kiyong Lee.
References
N. Asher and J. Pustejovsky. 2006. A type composition logic for generative
lexicon. Journal of Cognitive Science, 6:1?38.
A. Bisetto and S. Scalise. 2005. The classification of compounds. Lingue e
Linguaggio, 2:319?332.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian
Pado, and Manfred Pinkal. 2006. The salsa corpus: a german corpus
resource for lexical semantics. In Proceedings of LREC, Genoa, Italy.
Gennaro Chierchia. 1998. Reference to kinds across language. Natural Lan-
guage Semantics, 6(4).
Marcus Egg. 2005. Flexible semantics for reinterpretation phenomena. CSLI,
Stanford.
Jeroen Groenendijk and Martin Stokhof, 1989. Type-shifting rules and the
semantics of interrogatives, volume 2, pages 21?68. Kluwer, Dordrecht.
Jerry R. Hobbs, Mark Stickel, and Paul Martin. 1993. Interpretation as ab-
duction. Artificial Intelligence, 63:69?142.
Nancy Ide and L. Romary. 2003. Outline of the international standard lin-
guistic annotation framework. In Proceedings of ACL?03Workshop on Lin-
guistic Annotation: Getting the Model Right.
M. Johnston and F. Busa. 1999. The compositional interpretation of com-
pounds. In E. Viegas, editor, Breadth and Depth of Semantics Lexicons,
pages 167?167. Dordrecht: Kluwer Academic.
Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon.
Phd dissertation, University of Pennsylvania, PA.
J. N. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic
Press, New York.
K. Markert and M. Nissim. 2007. Metonymy resolution at semeval i:
Guidelines for participants. In Proceedings of the ACL 2007 Conference.
179
J. M. Moravcsik. 1975. Aitia as generative factor in aristotle?s philosophy.
Dialogue, 14:622?636.
Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Poly-
semy. Linguistics and Philosophy, 3:143?184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and multilinguality in the
japanese framenet. In Proceedings of LREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles. Computational Linguistics, 31(1):71?
106.
Barbara Partee and Mats Rooth, 1983. Generalized conjunction and type ambi-
guity, pages 361?383. de Gruyter, Berlin.
Manfred Pinkal. 1999. On semantic underspecification. In Harry Bunt and
Reinhard Muskens, editors, Proceedings of the 2nd International Workshop
on Computational Semantics (IWCS 2), January 13-15, Tilburg University,
The Netherlands.
J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics,
17(4).
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.): MIT Press.
J. Pustejovsky. 2000. Events and the semantics of opposition. In C. Tenny
and J. Pustejovsky, editors, Events as Grammatical Objects, pages 445?
482. Center for the Study of Language and Information (CSLI), Stan-
ford, CA.
J. Pustejovsky. 2005. A survey of dot objects. Technical report, Brandeis
University.
J. Pustejovsky. 2006. Type theory and lexical decomposition. Journal of Cog-
nitive Science, 6:39?76.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk.
2006. FrameNet II: Extended Theory and Practice.
A. Spencer. 1991. Morphological Theory: An Introduction to Word Structure
in Generative Grammar. Blackwell Textbooks in Linguistics, Oxford, UK
and Cambridge, USA.
Carlos Subirats. 2004. FrameNet Espan?ol. Una red sema?ntica de mar-
cos conceptuales. In VI International Congress of Hispanic Linguistics,
Leipzig.
B. Warren. 1978. Semantic Patterns of Noun-Noun Compounds. Acta Univer-
sitatis Gothoburgensis, Go?teborg.
180
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 184?185,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Medstract ? The Next Generation
Marc Verhagen
Computer Science Department
Brandeis University, Waltham, USA
marc@cs.brandeis.edu
James Pustejovsky
Computer Science Department
Brandeis University, Waltham, USA
jamesp@cs.brandeis.edu
Abstract
We present MedstractPlus, a resource for min-
ing relations from the Medline bibliographic
database. It was built on the remains of Med-
stract, a previously created resource that in-
cluded a bio-relation server and an acronym
database. MedstractPlus uses simple and scal-
able natural language processing modules to
structure text and is designed with reusability
and extendibility in mind.
1 Introduction
In the late 1990s, the Medstract project (Pustejovsky
et al, 2002) set out to use common Natural Lan-
guage Processing techniques and employ them to
access relational information in Medline abstracts.
Medstract used a set of pipelined Python scripts
where all scripts operated on in-memory objects.
The output of this pipeline was a set of relations,
indexed by the PubMed identifier of the abstract in
which they appeared. A Perl script proposed poten-
tial acronyms using a set of regular expressions on
named entities in Medline abstracts. Both relations
and acronyms were fed into an Oracle database,
where access to these datasources was enabled by
a set of Perl CGI scripts. The code, however, was
not made public and was not maintained in any se-
rious fashion after 2004. Developers of the system
dispersed over the world and the Medstract server
fatally crashed in 2007.
Here, we describe the resurrection of Medstract.
One goal was that code should be open source and
that installation should not depend on idiosyncra-
cies of the developer?s machine, which was a prob-
lem with the inherited code base. Reusability and
extendability are ensured by following the princi-
ples embodied in the Linguistic Annotation Format
(LAF) (Ide and Romary, 2006). In LAF, source data
are untouched, annotations are grouped in layers that
can refer to each other and to the source, and each
layer is required to be mappable to a graph-like pivot
format. For MedstractPlus, each component is set
up to be independent from other layers, although of
course each layer may need access to certain types
of information in order to create non-trivial output.
This allows us to swap in alternative modules, mak-
ing it easier to experiment with different versions of
the tagger and chunker for example. We now pro-
ceed to describe the system in section 2 and finish
with the current status and future work in section 3.
2 System Design and Implementation
The general design of MedstractPlus is presented in
Figure 1. The Lemmatizer creates what LAF calls
the base-segmentation, a first layer of tokenized text
that is the input to processing modules associated
with other layers. The Lemmatizer incorporates a
Python version of the Brill Tagger, extended with
entries from the UMLS Thesaurus.
The Semantic Tagger is a group of components
using (i) regular expressions for finding simple types
like URLs, (ii) dictionary lookup in the UMLS type
and concept lists as well as other typed word lists,
(iii) off-the-shelf components like the Abner gene
tagger (http://pages.cs.wisc.edu/ bsettles/abner/) and
(iv) a statistical disambiguation model for genes
trained on the GENIA corpus.
184
Medline Abstracts Lemmatizer
GENIA Corpus
Acronyms
UMLS
RelationsSemantic Types
Semantic Tagger
- regular expressions
- dictionary lookup
- Abner tagger
- disambiguation
Relation Extraction
- shallow parser
     three-level YACC parser
- argument linker
Web InterfacePubMed
Figure 1: Overview of the MedstractPlus Architecture
The Relation Extraction component now contains
a three-level 59-rule YACC parser that, starting with
simple low-level chunking of noun and verb groups,
proceeds to add more complex noun phrases and
subordinated phrases. The argument linker produces
binary relations, using a finite-state machine that
runs on the data created by the shallow parser.
An advantage of this data-driven approach is that
processing can be split up. A complete run of Med-
stractPlus on all Medline abstracts would take ap-
proximately 30 days on a entry-level desktop. But
some relatively stable components like the Lemma-
tizer and the shallow parser (the latter being the most
time-consuming component) can be run just once
and subsequent runs can be restricted to those com-
ponents that were changed.
The Web Interface gives access to the types and
relations in a fairly standard way. In its current pro-
totype form, it allows a user to type in a gene and
then view all relations that the gene participates in.
Alternatively, a pair of genes can be given.
3 Current Status and Future Work
The basic architecture depicted in Figure 1 is in
place, but some components like the type disam-
biguator are in embryonic form. The web inter-
face and the source code are or will be available at
http://medstractplus.org.
Extensive additions to the basic typing and re-
lation extraction component groups are in progress
and the Relation Extraction component can be ex-
tended with specialized rule sets for specific rela-
tions like inhibit or phosphorylate. The interaction
with the PubMed server is now limited to providing
links. But the plan is that the MedstractPlus server
will also query PubMed for relation pairs in case its
own database provides little information. This ap-
proach can be extended to other relation servers like
Chilibot (http://www.chilibot.net/), thereby moving
towards a system than presents merged relations
from the MedstractPlus database as well as relations
from other servers.
Acknowledgments
This work was supported by the National Institutes
of Health, under grant number 5R01NS057484-04.
References
Nancy Ide and Laurent Romary. 2006. Representing lin-
guistic corpora and their annotations. In Proceedings
of the Fifth Language Resources and Evaluation Con-
ference (LREC), Genoa, Italy.
James Pustejovsky, Jose? Castan?o, Roser Saur??, Anna
Rumshisky, Jason Zhang, and Wei Luo. 2002. Med-
stract: Creating large-scale information servers for
biomedical libraries. In Proceedings of ACL?02.
185
Proceedings of the Fifth Law Workshop (LAW V), pages 152?160,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Increasing Informativeness in Temporal Annotation
James Pustejovsky
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
jamesp@cs.brandeis.edu
Amber Stubbs
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
astubbs@cs.brandeis.edu
Abstract
In this paper, we discuss some of the chal-
lenges of adequately applying a specification
language to an annotation task, as embodied
in a specific guideline. In particular, we dis-
cuss some issues with TimeML motivated by
error analysis on annotated TLINKs in Time-
Bank. We introduce a document level in-
formation structure we call a narrative con-
tainer (NC), designed to increase informative-
ness and accuracy of temporal relation identi-
fication. The narrative container is the default
interval containing the events being discussed
in the text, when no explicit temporal anchor
is given. By exploiting this notion in the cre-
ation of a new temporal annotation over Time-
Bank, we were able to reduce inconsistencies
and increase informativeness when compared
to existing TLINKs in TimeBank.
1 Introduction
In linguistic annotation projects, there is often a gap
between what the annotation schema is designed to
capture and how the guidelines are interpreted by the
annotators and adjudicators given a specific corpus
and task (Ide and Bunt, 2010; Ide, 2007). The dif-
ficulty in resolving these two aspects of annotation
is compounded when tasks are looked at in a poten-
tially incomplete annotation task; namely, where the
guideline is following a specification to a point, but
in fact human annotation is not even suggested as
complete because it would be infeasible. Creating
temporal links to represent the timeline of events in
a document is an example of this: human annota-
tion of every possible temporal relationship between
events and times in a narrative would be an over-
whelming task.
In this paper, we discuss how temporal rela-
tion annotation must be sensitive to two aspects of
the task that were not mentioned in the TimeBank
guideline (Pustejovsky et al, 2005): (a) sensitivity
to the genre and style of the text; and (b) the interac-
tion with discourse relations that explicitly reference
the flow of the narrative in the text. We believe that
making reference to both these aspects in the text
during the annotation process will increase overall
informativeness and accuracy of the annotation. In
the present paper, we focus primarily on the first of
these points, and introduce a document level infor-
mation structure we call a narrative container (NC).
Because of the impossibility of humans captur-
ing every relationship, it is vital that the annotation
guidelines describe an approach that will result in
maximally informative temporal links without rely-
ing on standards that are too difficult to apply. With
this in mind, we have been examining the TimeBank
corpus (Pustejovsky et al, 2003) and the annotation
guideline that created it, and have come to these re-
alizations:
(1) ? The guideline does not specify certain types
of annotations that should be performed;
? The guideline forces some annotations to be
performed when they should not always be.
Additionally, we have discovered some inconsisten-
cies in the TimeBank corpus related to temporal
links. Furthermore, upon examination, we have be-
come aware of the importance of the text style and
152
genre, and how readers interpret temporally unah-
chored events.
This gave rise, in examining the genres that are
most frequent in TimeBank (namely news and fi-
nance), to the possibility that readers of news ar-
ticles and narratives have possible default assump-
tions about when unanchored events take place. It
seems reasonable for a reader to assume in a sen-
tence such as: Oneida Ltd. declared a 10% stock
dividend, payable Dec. 15 to stock of record Nov.
17, that the ?declared? event took place soon before
the article?s Document Creation Time (DCT).
Exactly how soon before may be related to some
proximate interval of time associated with both the
publication time and frequency. That is, it appears
that just as importantly, if not more so, than the DCT,
is a related and dependent notion of the salient in-
terval surrounding the creation time, for interpreting
the events that are being reported or written about.
We will call this the Narrative Container. There
seems to be a default value for this container affected
by many variables. For example, a print newspaper
seems to associate in the content and style a nar-
rative container of approximately 24 hours, or one
business day. A newswire article, on the other hand,
has a narrative container of 2-10 hours. Conversely,
weekly and monthly publications would likely have
a narrative container of a much longer duration (a
week or more).
Along with the narrative container, there are two
related concepts that proved useful in framing this
new approach to temporal annotation. The Narra-
tive Scope describes the timespan described in the
document, with the left marker defined by the earli-
est event mentioned in the document, and the right
by the event furthest in the future. The other impor-
tant concept is that of Narrative Time. A Narrative
Time is essentially the current temporal anchor for
events in a document, and can change as the reader
moves through the narrative.
With these as initial assumptions we did some
cursory inspection of the TimeBank data to deter-
mine if there was a correlation between Narrative
Container length and genre, and found it to be a
compelling assumption. With that in mind, we de-
termined that TLINK creation should be focused on
relationships to the narrative container, rather than
to the DCT.
Our goal is, to the extent possible, to see how
we can use a container metaphor, albeit somewhat
underspecified, to left-delineate the container within
which unanchored events might be in relation to.
2 Identifying Temporal Relations
While low-level temporal annotation tasks such as
identifying events and time expressions are rela-
tively straightforward and can be marked up with
high consistency, high-level tasks such as arrang-
ing events in a document in a temporal order have
proved to be much more challenging. The tempo-
ral ordering of events in a document, for example, is
accomplished by identifying all distinct event-event
pairings. For a document that has n events, this
requires the annotation of
(n
2
)
events pairs. Ob-
viously, for general-purpose annotation, where all
possible events are considered, the number of event
pairs grows essentially quadratically to the number
of events, and the task quickly becomes unmanage-
able.
There are, however, strategies that we can adopt
to make this labeling task more tractable. First we
need to distinguish the domains over which ordering
relations are performed. Temporal ordering relations
in text are of three kinds:
(2) a. A relation between two events;
b. A relation between two times;
c. A relation between a time and an event.
TimeML, as a formal specification of the temporal
information conveyed in language, makes no dis-
tinction between these ordering types. But a human
reader of a text does make a distinction, based on
the discourse relations established by the author of
the narrative (Miltsakaki et al, 2004; Poesio, 2004).
Temporal expressions denoting the local Narrative
Container in the text act as embedding intervals
within which events occur. Within TimeML, these
are event-time anchoring relations (TLINKs). Dis-
course relations establish how events relate to one
another in the narrative, and hence should constrain
temporal relations between two events. Thus, one
of the most significant constraints we can impose is
to take advantage of the discourse structure in the
document before event-event ordering relations are
identified.
153
Although, in principle, during an annotation a
temporal relation can be specified between any two
events in the text, it is worth asking what informa-
tiveness a given temporal relation introduces to the
annotation. The informativeness of an annotation
will be characterized as a function of the information
contained in the individual links and their closure.
We can distinguish, somewhat informally for now,
two sources of informativeness in how events are
temporally ordered relative to each other in a text:
(a) externally and (b) internally. Consider first ex-
ternal informativeness. This is information derived
from relations outside the temporal relation con-
straint set, e.g., as coming from explicit discourse re-
lations between events (and hence is associated with
the relations in (2a) above). For example, we will
assume that, for two events, e1 and e2, in a text, the
temporal relation between them is more informative
if they are also linked through a discourse relation,
e.g., a PDTB relation (Prasad et al, 2008). Mak-
ing such an assumption will allow us to focus in on
the temporal relations that are most valuable without
having to exhaustively annotate all event pairs.
Now consider internal informativeness. This is
information derived from the nature of the relation
itself, as defined largely by the algebra of relations
(Allen, 1984; Vilain et al, 1986). First, we assume
that, for two events, e1 and e2, a temporal relation
R1 is more informative than R2 if R1 entails R2.
More significantly, however, as noted above, is to
capitalize on the relations that inhere between events
and the times that anchor them (i.e., (2c) above).
Hence, we will say that, given an event, e1 and a
time t1, a temporal relation R is more informative
the more it anchors e1 to t1. That is, a containment
relation is more informative than an ordering rela-
tion, and the smaller the container, the more infor-
mative the relation.1
The Document Creation Time (DCT) as designed
in TimeML is introduced as a reference time, against
which the mentioned events and time expressions in
the document can be ordered. Consider the text frag-
ment below.
1We defer discussion of the formal definition of informative-
ness for the present paper, as we are focusing on initial results
over re-annotated data in TimeBank.
4-10-2011
Local officials reported yesterday that a
car exploded in downtown Basra.
The TimeML annotation guideline (AG) suggests
identifying relations between the DCT and textual
events. Hence standard markup as in TimeBank re-
sults in the following sort of annotation:
(3) a. DCT= t1, val=10-04-2011
b. t2 = yesterday, val=09-04-2011
b. e1 = report
c. e2 = explode
d. TLINK1 = before(e1, t1)
e. TLINK2 = before(e2, t1)
f. TLINK3 = includes(t2, e1)
This is a prototypical annotation fragment. Notice
that by focusing on the link between events and the
DCT, the annotator is forced to engage in a kind of
periodic ?back-and-forth? evaluation of the events
in the text, relative to the DCT. While there is a con-
tainer TIMEX3 that bounds e1, there is no informa-
tion given grounding the actual time of the event of
interest, namely, the explosion, e2. By following the
AG literally and through no fault of their own, the
annotators have missed an opportunity to provide a
more informative markup; namely, the identification
of the TLINK below:
(4) TLINK4 = includes(t2, e2)
That is, the explosion occurred on the date valued
for yesterday, i.e., ?09-04-2011?.
The point of this paper is to discuss the difference
encountered when applying a specification given a
particular guideline for annotating a body of text.
The example we want to discuss is the manner in
which events are linked (related) to the Document
Creation Time (DCT) in TimeML. These consider-
ations have arisen in the context of new annotation
problems in different genre and domains, hoping to
apply the principles of TimeML.
3 Narrative Scope
As previously mentioned, the Narrative Scope of a
document is the temporal span over which the events
in a document occur, as defined by the timexes in a
154
document. While not every event in a document will
necessarily occur inside the Narrative Scope (some
may still occur before or after any dates that are
specifically mentioned), the Narrative Scope pro-
vides a useful container for describing when events
discussed most likely occurred. The narrative scope
was not considered as part of the annotation task,
but it did help to ground the concepts of Narrative
Containers and Narrative Times.
4 Narrative Time
As a reader moves through a document, the intro-
duction of a new TIMEX will often shift the tem-
poral focus of the events to be anchored to this new
time point (Smith, 2003). These temporal anchors
are what we refer to as Narrative Times, and func-
tion in much the same way as newly introduced lo-
cations in spatial annotation.
However, consider how we can use Narrative
Times to increase accuracy of the TLINKS over a
document in TimeML. As mentioned above, we dis-
tinguish three types of temporal orderings in a text:
time-time, event-time, and event-event. The first
identifies orderings between two TIMEX3 expres-
sions and is performed automatically. The second
identifies what the local Narrative Time for an event
is, i.e., how an EVENT is anchored to a TIMEX3.
Event-event pairings, for the purposes of this paper,
will not be discussed, though they are a vital and
complex component of temporal annotation, largely
involving discourse relations.
To illustrate our proposed strategy, consider the
news article text shown below.
April 25, 2010 7:04 p.m. EDT -t0
S1: President Obama paid-e1 tribute Sunday -t1
to 29 workers killed-e2 in an explosion -e3 at a
West Virginia coal mine earlier this month- t2,
saying-e4 they died-e5 ?in pursuit of the Amer-
ican dream.?
S2: The blast-e6 at the Upper Big Branch Mine
was the worst U.S. mine disaster in nearly 40
years.
There are three temporal expressions in the above
text: the Document Creation time, t0; and two
TIMEXes, t1 and t2. Each of these TIMEXes func-
tions as a Narrative Time, as they are clearly provid-
ing temporal anchors to nearby events. In this case,
all the events are located within the Narrative Time
appropriate to them. Hence, the number of order-
ings is linearly determined by the number of events
in the document, since each is identified with a sin-
gle Narrative Time. Knowing the narrative time as-
sociated with each event will allow us to perform
limited temporal ordering between events that are
associated with different narrative times, which, as
mentioned above, is significantly more informative
than if events were only given partial orderings to
the DCT or to each other.
5 Narrative Containers
So far we have examined sentences that contain
specific temporal anchors for the events discussed.
Consider, however, the following sentences from ar-
ticle wsj 1031.tml in TimeBank:
10-26-1989
1 Philip Morris Cos., New York, adopted a
defense measure designed to make a hostile
takeover prohibitively expensive.
2 The giant foods, tobacco and brewing company
said it will issue common-share purchase rights to
shareholders of record Nov. 8.
Aside from the DCT, the only TIMEX in these
two sentences is Nov. 8, which is only anchoring is-
sue and record. The other events in the sentences
can only be connected to the DCT, and presum-
ably only in a ?before? or ?after? TLINK?in the ab-
sence of other information, any reader would assume
from the past tenses of adopted and said that these
events occurred before the article was published, and
that any events associated with the future (make,
takeover) are intended to happen after the DCT.
However most readers, knowing that the Wall
Street Journal is published daily, will likely assume
that any event mentioned which is not specifically
associated with a date, occurred within a certain
time frame?it would be extremely unusual for a
newspaper to use the construction presented above
if the events actually occurred, for example, a year
or even a week prior to the publication date. We call
this assumed window the Narrative Container, as it
provides left and right boundaries for when unan-
155
t2 "earlier this month"
t1 
"Sunday"
e3 
explosion
e5 "died"e1 "paid" e2 "killed"
e4 
"saying"
t0 DCT e6 "blast"
t0 DCT
t1 "Sunday"
e2 "killed"
t2 earlier 
this month
e5 "died" e6 "blast" e1 "paid"
e4 
"saying"
e3 
explosion
A
B
Figure 1: A: Times and events as appearing in the text; B: events grouped into their appropriate Narrative Times.
chored events most likely occurred, where in pre-
vious TimeML annotations these events would usu-
ally be given one-sided relationships to the DCT. In
most cases, the right boundary of the Narrative Con-
tainer is the DCT. The left boundary, however, re-
quires other factors about the article to be taken into
account before it can be given a value. The primary
factor is how frequently the source of the document
is published, but other aspects of the article may also
determine the Narrative Container size.
5.1 Style, Genre, Channel, and Anchors
In order to determine what factors might influence
the interpretation of the size of a Narrative Con-
tainer, we asked an undergraduate researcher to cat-
egorize each of the articles in TimeBank according
to the following characteristics (Lee, 2001; Biber,
2009).
(5) ? Channel: is the document written or spoken?
? Production circumstances: how was the doc-
ument distributed? broadcast, newswire, daily
publication;
? Style: what format was used to present the
information?
? Presence of a temporal anchor: Whether an
article contained a Narrative Time in the first
sentence of the document.
In general, we felt that the production circum-
stances would be the most relevant in determining
the duration of the Narrative Container. The distri-
butions of the different categories in TimeBank are
shown in Table 1. There is a 100% overlap between
the ?broadcast? and ?spoken? subcategories?all of
those articles are word-for-word transcripts of tele-
vision news reports. The ?style? category proved the
most difficult to define?the ?quiz? article is a broad-
cast transcript of a geography question asked during
the evening news, while the ?biography? articles are
overviews of people?s lives. The editorials include a
letter to the editor of the Wall Street Journal and an
editorial column from the New York Times.
Category number percent
Production Circ.
broadcast 25 13.7%
daily paper 140 76.5%
newswire 18 9.8%
Channel
spoken 25 13.7%
written 158 86.3%
Style
biography 2 1.1%
editorial 2 1.1%
finance 135 73.8%
news 43 23.5%
quiz 1 0.5%
Temporal Anchor
no 138 75.4%
yes 45 24.6%
Table 1: Distributions of categories in TimeBank
6 Preliminary Studies
In order to assess the validity of our theories on Nar-
rative Containers, Time, and Scope, we asked three
undergraduate researchers to re-annotate TimeBank
using the Narrative Container theory as a guide.
Each annotator evaluated all of the events in
TimeBank by identifying the temporal constraint
that anchored the event. If the annotators felt that
the event was not specifically anchored, they could
156
place it within the Narrative Container for the docu-
ment, or they could give the event a simple ?before?
or ?after? value related to the Narrative Container or
Document Creation Time. We also asked them to
assign start and end times to the Narrative Container
for each document.
The annotation here was not intended to be as
complete as the TimeBank annotation task, or even
the TempEval tasks?rather, the goal was to deter-
mine if the Narrative Container theory could be ap-
plied in a way that resulted in an increase in infor-
mativeness, and whether the annotators could work
with the idea of a Narrative Container. Because
these annotations are not comprehensive in their
scope, the analysis provided here is somewhat pre-
liminary, but we believe it is clear that the use of a
Narrative Container in temporal annotations is both
informative and intuitive.
6.1 Narrative container agreement
Each annotator was asked to assign a value to the
narrative container of each document. They were
given limited directions as to what the size of an NC
might be: only some suggestions regarding possible
correlations between type and frequency of publica-
tion and size of the narrative container. For example,
it was suggested that a news broadcast might have a
narrative container of only a few hours, a daily news-
paper would have one of a day (or one that extended
to the previous business day), and a newswire article
would have a narrative container that extended back
24 hours from the time of publication.
All the annotators agreed that an NC would not
extend forward beyond the document creation time
(DCT), and that in most cases the NC would end at
the DCT. Because the annotators gave their data on
the size of the NC in free text (for example, an an-
notator would say ?1 day? to indicate that the NC
for an article began the day before the article was
published) the comparison of the narrative contain-
ers was performed manually by one of the authors to
determine if the annotators agreed on the size of the
NC.
Agreement was determined using a fairly strict
matching criterion?if the narrative containers given
were clearly referring to the same interval they were
interpreted to be the same. If, however, there was
ambiguity about the date or one annotator indicated
a smaller time period than another, then they were
judged to be different. A common example of am-
biguity was related to newspaper articles that were
written on Mondays?annotators could not always
determine if the events described occurred the day
before, or on the previous business day For eval-
uation purposes, the ambiguous cases were given
?maybe? values, but were not included in analysis
that relied on the NCs being the same.
Overall, using the strict agreement metric all the
annotators agreed on the size of the narrative con-
tainer in 95 out of 183 articles?slightly over 50% of
the time. However, the annotators only completely
disagreed on 6 of the 183 articles?in all other cases
there was some level of agreement between pairs of
annotators.
6.2 NCs and Document Classifications
We compared Narrative Container agreements
against the categories outlined above: style, channel,
production circumstances, and temporal anchorings
in order to determine if any of those attributes lent
themselves to agreement about the size of the Narra-
tive Container. We disregarded the biography, quiz,
and editorial classifications as those categories were
too small to provide useful data.
For the most part, no one category stood out as
lending itself to accuracy?newswire had the high-
est levels of agreement at 72%, while daily papers
came in at 58%. Written channels had 60% agree-
ment, and the finance style had 59%. Articles with
temporal anchors in the beginning of the document
were actually slightly less likely to have agreement
on the Narrative Container than those that didn?t?
48% and 53%, respectively.
While the higher disagreement levels over Nar-
rative Container size in the presence of a temporal
anchor seems counter-intuitive, it stems from a sim-
ple cause: if the temporal anchor overlapped with
the expected narrative container but was not exactly
the same size, sometimes one annotator would use
that anchor as the Narrative Container, while the oth-
ers would not. This sometimes also happened with
a Narrative Time that was not at the start of the
document or sometimes even the Narrative Scope
would be used as the Narrative Container. While
in some articles it is the case that a Narrative Time
anchors more events than the Narrative Container,
157
ll
ll
l
0.0
0.2
0.4
0.6
0.8
1.0
Fleiss Kappa by Article Category
Fleis
s Ka
ppa
l
l
ll
0.0
0.2
0.4
0.6
0.8
1.0
l
l
l
l
0.0
0.2
0.4
0.6
0.8
1.0
l
l
ll
l
0.0
0.2
0.4
0.6
0.8
1.0
Broadcast Daily Newswire Spoken Written Finance News No YesProduction Circ Channel Style Temporal Anchorings
Figure 2: Distributions of Fleiss Kappa scores over TimeBank categories
that does not make that Narrative Time the Nar-
rative Container for the document?the Narrative
Container is always the interval during which an
unanchored event would be assumed to have taken
place. This point of confusion can easily be clarified
in the guidelines.
Spoken/broadcast articles had the lowest agree-
ment on Narrative Container size, with none of those
articles having complete agreement between anno-
tators. This was largely caused by our annotators
not agreeing on how much time those categories
would encompass by default?two felt that the narra-
tive containers for broadcast news would extend to
only a few hours before going on air, and the other
felt that, like a daily paper, the entire previous day
would be included when dealing with unanchored
times.
As for the question of how large a Narrative Con-
tainer should be for broadcast articles, the size of all
Narrative Containers will need to be studied more
in depth in order to determine how widely they can
be applied? it is possible that in general, the actual
size is less important than the simple concept of the
Narrative Container.
6.3 Agreement over event anchors
The annotators were asked to read each article in
TimeBank and ?create links from each event to the
nearest timex or to the DNC.? They were asked
specifically to not link an event to another event,
only to find the time that would be used to anchor
each event in a timeline. The annotators were also
asked to use only three relationship types: before,
after, and is included (which also stood in for ?over-
lap?). This was done in order to keep the annotation
as simple as possible: we wanted to see if the narra-
tive container was a useful tool in temporal annota-
tion, not produce a full gold standard corpus.
This differs from the TimeML annotation guide-
lines, which suggested only that ?A TLINK has to
be created each time a temporal relationship hold-
ing between events or an event and a time needs to
be annotated.? (Saur?? et al, 2006) Examples given
were for sentences such as ?John drove to Boston on
Monday??cases where an event was specifically re-
lated to a time or another event. However, because
such examples were relatively rare, and temporal re-
lationships are not always so clearly expressed, this
annotation method resulted in a corpus that was not
optimally informative. TimeML also uses a fuller
set of temporal relations.
The NC annotations, on the other hand, are much
richer in terms of informativeness. Annotators most
often linked to the NC, often with an ?is included?
relationship (as in: e1 is included NC). In fact,
roughly 50% of the events were linked to the narra-
tive container and had ?is included? as the relation-
ship type. In previous TimeML annotations, most of
those events would have been annotated as simply
occurring before or overlapping with the document
creation time, which is a significantly less informa-
tive association. Clearly the narrative container was
an intuitive concept for the annotators, and one that
was relevant to their annotations.
6.3.1 Inter-annotator agreement
We used Fleiss? kappa (Fleiss, 1971) to obtain values
for agreement between the three annotators: first,
we compared the number of times they agreed what
the temporal anchor for an event should be, then we
compared whether those links that matched had the
same relation type. Data analysis was done in R with
the irr package (R Team, 2009; Gamer et al, 2010).
158
said (e1)
enable (e10)declared (e2)purchase (e12)issue (e18)
exercised (e14)
90 days(t31)
issued (e7)issue (e32)
10/26/89 (t30) (DCT)
said (e8)has (e5)said (e17)
10/25/89 ? 10/26/89 (NC)
10/26/89 (t30) (DCT)
said (e1)declared (e2)has (e5)issued (e7)said (e8)issue (e32)said (e17)
90 days (t31)
exercised (e14)
enable  (e10)
purchase(e12)
issue (e18)
TimeBank annotation Narrative Container annotation
Wall Street Journal - wsj_1042.tml
Figure 3: Visual depictions of the TLINK annotations in TimeBank and with the Narrative Container annotations.
Solid lines indicate events and times in the box have IS INCLUDED relationships with the timex at the top, and
dotted lines indicate events that were given IDENTITY relationships
When looking at the kappa scores for the tempo-
ral anchor, it should be noted that these scores do
not always accurately reflect the level of agreement
between annotators. Because of the lack of variabil-
ity, Fleiss? Kappa will interpret any article where an
annotator only linked events to the NC received neg-
ative agreement scores. These values have been left
in the tables as data points, but it should be noted
that these annotations are entirely valid?some ar-
ticles in TimeBank contain no temporal information
other than the document creation time (and by exten-
sion, the narrative container), making it only natural
for the annotators to annotate events only in rela-
tion to the narrative container. The average Fleiss?
Kappa scores for the temporal anchors was .74, with
a maximum of 1 and a minimum of -.04.
6.4 Informativeness in NC Annotation
As we previously described, Narrative Containers
are theoretically more informative than Document
Creation Times when trying to place unanchored
events on a timeline. In practice, they are as infor-
mative as we anticipated: compare the visualizations
of TLINK annotations between TimeBank and the
NC links in Figure 3. These were created from the
file wsj 1042.tml, one that had complete agreement
between annotators about both the size of the NC
(one day before the DCT through the DCT) and all
the temporal anchors and temporal relations.
Clearly, the NC task has resulted in a more in-
formative annotation?all the events have at least one
constraint, and most have both left and right con-
straints.
7 Conclusions and Future Work
Narrative Containers, Narrative Times, and Narra-
tive Scopes are important tools for temporal annota-
tion tasks. The analysis provided here clearly shows
that annotating with an NC increases informative-
ness, and that the concept is sufficiently intuitive for
it to not add confusion to the already complicated
task of temporal annotation. However, the work in
this area is far from complete. In the future we in-
tend to study where the left boundary of the NC
should be placed for different genres and publica-
tion frequencies. Another annotation task must be
performed, requiring a more comprehensive TLINK
creation guideline, using both event-time and event-
event links. Finally, the use of all three concepts for
automated annotation tasks should be examined, as
they may prove as useful to machines as they are to
humans.
Acknowledgements
This work has been supported by NSF grant
#0753069 to Co-PI James Pustejovsky. Many thanks
to Chiara Graf, Zac Pustejovsky, and Virginia Par-
tridge for their help creating the annotations, and to
BJ Harshfield for his R expertise. We would also
like to acknowledge Aravind Joshi, Nianwen Xue,
and Marc Verhagen for useful input.
159
References
James Allen. 1984. Towards a general theory of action
and time. Arificial Intelligence, 23:123?154.
Douglas Biber. 2009. Register, Genre, and Style.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Matthias Gamer, Jim Lemon, and Ian Fellows Pus-
pendra Singh ?puspendra.pusp22@gmail.com?, 2010.
irr: Various Coefficients of Interrater Reliability and
Agreement. R package version 0.83.
Nancy Ide and Harry Bunt. 2010. Anatomy of annota-
tion schemes: Mappings to graf. In In Proceedings 4th
Linguistic Annotation Workshop (LAW IV).
Nancy Ide. 2007. Annotation science: From theory to
practice and use: Data structures for linguistics re-
sources and applications. In In Proceedings of the Bi-
enniel GLDV Conference.
David Lee. 2001. Genres, registers, text types, domains,
and styles: Clarifying the concepts and navigating a
path through the bnc jungle. Language Learning &
Technology, 5(3.3):37?72.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The penn discourse treebank.
In In Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the gnome corpus. In In Proceed-
ings of the ACL Workshop on Discourse Annotation.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
James Pustejovsky, Patrick Hanks, Roser Saur?`, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003. The timebank corpus. In
Dawn Archer, Paul Rayson, Andrew Wilson, and Tony
McEnery, editors, Proceedings of the Corpus Linguis-
tics 2003 conference, pages 647?656, Lancaster Uni-
versity. UCREL.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Saur??. 2005. Temporal and event information in
natural language text. Language Resources and Eval-
uation, 39:123?164, May.
R Team, 2009. R: A Language and Environment for Sta-
tistical Computing. R Foundation for Statistical Com-
puting, Vienna, Austria. ISBN 3-900051-07-0.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2006. TimeML Annotation Guidelines, version 1.2.1
edition, January.
Carlota Smith. 2003. Modes of Discourse. Cambridge
University Press, Cambridge, UK.
Marc Vilain, Henry Kautz, and Peter Beek. 1986. Con-
straint propagation algorithms for temporal reasoning.
In Readings in Qualitative Reasoning about Physical
Systems, pages 377?382. Morgan Kaufmann.
160
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 34?43,
Dublin, Ireland, August 23rd 2014.
The Language Application Grid Web Service Exchange Vocabulary
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
James Pustejovsky
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
jamesp@cs.brandeis.edu
Keith Suderman
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
suderman@anc.org
Marc Verhagen
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
marc@cs.brandeis.edu
Abstract
In the context of the Linguistic Applications (LAPPS) Grid project, we have undertaken the def-
inition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce lin-
guistically annotated data. The goal is not to define a new set of terms, but rather to provide a
single web location where terms relevant for exchange among NLP tools are defined and pro-
vide a ?sameAs? link to all known web-based definitions that correspond to them. The WS-EV
is intended to be used by a federation of six grids currently being formed but is usable by any
web service platform. Ultimately, the WS-EV could be used for data exchange among tools in
general, in addition to web services.
1 Introduction
There is clearly a demand within the community for some sort of standard for exchanging annotated lan-
guage data among tools.
1
This has become particularly urgent with the emergence of web services, which
has enabled the availability of language processing tools that can and should interact with one another,
in particular, by forming pipelines that can branch off in multiple directions to accomplish application-
specific processing. While some progress has been made toward enabling syntactic interoperability via
the development of standard representation formats (e.g., ISO LAF/GrAF (Ide and Suderman, 2014;
ISO-24612, 2012), NLP Interchange Format (NIF) (Hellmann et al., 2013), UIMA
2
Common Analysis
System (CAS)) which, if not identical, can be trivially mapped to one another, semantic interoperability
among NLP tools remains problematic (Ide and Pustejovsky, 2010). A few efforts to create repositories,
type systems, and ontologies of linguistic terms (e.g., ISOCat
3
, OLiA
4
, various repositories for UIMA
type systems
5
, GOLD
6
, NIF Core Ontology
7
) have been undertaken to enable (or provide) a mapping
among linguistic terms, but none has yet proven to include all requisite terms and relations or be easy
to use and reference. General repositories such as Dublin Core
8
, schema.org, and the Friend of a Friend
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
See, for example, proceedings of the recent LREC workshop on ?Language Technology Service Platforms: Synergies,
Standards, Sharing? (http://www.ilc.cnr.it/ltsp2014/).
2
https://uima.apache.org/
3
http://www.isocat.org
4
http://nachhalt.sfb632.uni-potsdam.de/owl/
5
E.g., http://www.julielab.de/Resources/Software/UIMA+type+system-p-91.html
6
http://linguistics-ontology.org
7
http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/nif-core
8
http://dublincore.org
34
project
9
include some relevant terms, but they are obviously not designed to fully cover the kinds of
information found in linguistically annotated data.
In the context of the Linguistic Applications (LAPPS) Grid project (Ide et al., 2014), we have under-
taken the definition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce linguistically
annotated data. The work is being done in collaboration with ISO TC37 SC4 WG1 in order to ensure
full community engagement and input. The goal is not to define a new set of terms, but rather to provide
a single web location where terms relevant for exchange among NLP tools are defined and provide a
?sameAs? link to all known web-based definitions that correspond to them. A second goal is to define
relations among the terms that can be used when linguistic data are exchanged. The WS-EV is intended
to be used by a federation of grids currently being formed, including the Kyoto Language Grid
10
, the
Language Grid Jakarta Operation Center
11
, the Xinjiang Language Grid, the Language Grid Bangkok
Operation Center
12
, LinguaGrid
13
, MetaNET/Panacea
14
, and LAPPS, but is usable by any web service
platform. Ultimately, the WS-EV could be used for data exchange among tools in general, in addition to
web services.
This paper describes the LAPPS WS-EV, which is currently under construction. We first describe the
LAPPS project and then overview the motivations and principles for developing the WS-EV. Because
our goal is to coordinate with as many similar projects and efforts as possible to avoid duplication, we
also describe existing collaborations and invite other interested groups to provide input.
2 The Language Application Grid Project
The Language Application (LAPPS) Grid project is in the process of establishing a framework that
enables language service discovery, composition, and reuse, in order to promote sustainability, manage-
ability, usability, and interoperability of natural language Processing (NLP) components. It is based on
the service-oriented architecture (SOA), a more recent, web- oriented version of the pipeline architecture
that has long been used in NLP for sequencing loosely-coupled linguistic analyses. The LAPPS Grid
provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA
and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide
general support for service discovery, composition, and reuse.
The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College,
Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and
is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in
the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability
and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope
and goals to ongoing projects such as The Language Grid
15
, PANACEA/MetaNET
16
, LinguaGrid
17
, and
CLARIN
18
, which also provide web service access to basic NLP processing tools and resources and
enable pipelining these tools to create custom NLP applications and composite services such as question
answering and machine translation, as well as access to language resources such as mono- and multi-
lingual corpora and lexicons that support NLP. The transformative aspect of the LAPPS Grid is therefore
not the provision of a suite of web services, but rather that it orchestrates access to and deployment of
language resources and processing functions available from servers around the globe, and enables users
to easily add their own language resources, services, and even service grids to satisfy their particular
needs.
9
http://www.foaf-project.org
10
http://langrid.nict
11
http://langrid.portal.cs.ui.ac.id/langrid/
12
http://langrid.servicegrid-bangkok.org
13
http://www.linguagrid.org/
14
http://www.panacea-lr.eu
15
http://langrid.nict
16
http://panacea-lr.eu/
17
http://www.linguagrid.org/
18
http://www.clarin.eu/
35
The most distinctive innovation in the LAPPS Grid that is not included in other projects is the provision
of an open advancement (OA) framework (Ferrucci et al., 2009a) for component- and application-based
evaluation of NLP tools and pipelines. The availability of this type of evaluation service will provide an
unprecedented tool for NLP development that could, in itself, take the field to a new level of productivity.
OA involves evaluating multiple possible solutions to a problem, consisting of different configurations
of component tools, resources, and evaluation data, to find the optimal solution among them, and en-
abling rapid identification of frequent error categories, together with an indication of which module(s)
and error type(s) have the greatest impact on overall performance. On this basis, enhancements and/or
modifications can be introduced with an eye toward achieving the largest possible reduction in error rate
(Ferrucci et al., 2009; Yang et al., 2013). OA was used in the development of IBM?s Watson to achieve
steady performance gains over the four years of its development (Ferrucci et al., 2010); more recently,
the open-source OAQA project has released software frameworks which provide general support for
open advancement (Garduno et al., 2013; Yang et al., 2013), which has been used to rapidly develop
information retrieval and question answering systems for bioinformatics (Yang et al., 2013; Patel et al.,
2013).
The fundamental system architecture of the LAPPS Grid is based on the Open Service Grid Initiative?s
Service Grid Server Software
19
developed by the National Institute of Information and Communications
Technology (NICT) in Japan and used to implement Kyoto University?s Language Grid, a service grid
that supports multilingual communication and collaboration. Like the Language Grid, the LAPPS Grid
provides three main functions: language service registration and deployment, language service search,
and language service composition and execution. As noted above, the LAPPS Grid is instrumented
to provide relevant component-level measures for standard metrics, given gold-standard test data; new
applications automatically include instrumentation for component-level and end-to-end measurement,
and intermediate (component-level) I/O is logged to support effective error analysis.
20
The LAPPS
Grid also implements a dynamic licensing system for handling license agreements on the fly
21
, provides
the option to run services locally with high-security technology to protect sensitive information where
required, and enables access to grids other than those based on the Service Grid technology.
We have adopted the JSON-based serialization for Linked Data (JSON-LD) to represent linguistically
annotated data for the purposes of web service exchange. The JavaScript Object Notation (JSON) is a
lightweight, text-based, language-independent data interchange format that defines a small set of format-
ting rules for the portable representation of structured data. Because it is based on the W3C Resource
Definition Framework (RDF), JSON-LD is trivially mappable to and from other graph-based formats
such as ISO LAF/GrAF and UIMA CAS, as well as a growing number of formats implementing the
same data model. Most importantly, JSON- LD enables services to reference categories and definitions
in web-based repositories and ontologies or any suitably defined concept at a given URI.
The LAPPS Grid currently supports SOAP services, with plans to support REST services in the
near future. We provide two APIs: org.lappsgrid.api.DataSource, which provides data
to other services, and org.lappsgrid.api.WebService, for tools that annotate, transform, or
otherwise manipulate data from a datasource or another web service. All LAPPS services exchange
org.lappsgrid.api.Data objects consisting of a discriminator (type) that indicates how to inter-
pret the payload, and a payload (typically a utf-8 string) that consists of the JSON-LD representation.
Data converters included in the LAPPS Grid Service Engines map from commonly used formats to the
JSON-LD interchange format; converters are automatically invoked as needed to meet the I/O require-
ments of pipelined services. Some LAPPS services are pre-wrapped to produce and consume JSON-LD.
Thus, JSON-LD provides syntactic interoperability among services in the LAPPS Grid; semantic inter-
19
http://servicegrid.net
20
Our current user interface provides easy (re-)configuration of single pipelines; we are currently extending the interface
to allow the user to specify an entire range of pipeline configurations using configuration descriptors (ECD; (Yang et al.,
2013) to define a space of possible pipelines, where each step might be achieved by multiple components or services and each
component or service may have configuration parameters with more than one possible value to be tested. The system will then
automatically generate metrics measurements plus variance and statistical significance calculations for each possible pipeline,
using a service-oriented version of the Configuration Space Exploration (CSE) algorithm (Yang et al., 2013).
21
See (Cieri et al., 2014) for a description of how licensing issues are handled in the LAPPS Grid.
36
operability is provided by the LAPPS Web Service Exchange Vocabulary, described in the next section.
3 LAPPS Web Service Exchange Vocabulary
3.1 Motivation
The WS-EV addresses a relatively small but critical piece of the overall LAPPS architecture: it allows
web services to communicate about the content they deliver, such that the meaning?i.e., exactly what
to do with and/or how to process the data?is understood by the receiver. As such it performs the same
function as a UIMA type system performs for tools in a UIMA pipeline that utilize that type system,
or the common annotation labels (e.g., ?Token?, ?Sentence?, etc.) required for communication among
pipelined tools in GATE: these mechanisms provide semantic interoperability among tools as long as one
remains in either the UIMA or GATE world. To pipeline a tool whose output follows GATE conventions
with a tool that expects input that complies with a given UIMA type system, some mapping of terms and
structures is likely to be required.
22
This is what the WS-EV is intended to enable; effectively, it is a
meta-type-system for mapping labels assigned to linguistically annotated data so that they are understood
and treated consistently by tools that exchange them in the course of executing a pipeline or workflow.
Since web services included in LAPPS and federated grids may use any i/o semantic conventions, the
WS-EV allows for communication among any of them?including, for example, between GATE and
UIMA services
23
The ability to pipeline components from diverse sources is critical to the implementation of the OA
development approach described in the previous section, it must be possible for the developer to ?plug
and play? individual tools, modules, and resources in order to rapidly re-configure and evaluate new
pipelines. These components may exist on any server across the globe, consist of modules developed
within frameworks such as UIMA and GATE, and or be user-defined services existing on a local machine.
3.2 WS-EV Design
The WS-EV was built around the following design principles, which were compiled based on input from
the community:
1. The WS-EV will not reinvent the wheel. Objects and features defined in the WS-EV will be linked
to definitions in existing repositories and ontologies wherever possible.
2. The WS-EV will be designed so as to allow for easy, one-to-one mapping from terms designating
linguistic objects and features commonly produced and consumed by NLP tools that are wrapped
as web services. It is not necessary for the mapping to be object-to-object or feature-to-feature.
3. The WS-EV will provide a core set of objects and features, on the principle that ?simpler is better?,
and provide for (principled) definition of additional objects and features beyond the core to represent
more specialized tool input and output.
4. The WS-EV is not LAPPS-specific; it will not be governed by the processing requirements or
preferences of particular tools, systems, or frameworks.
5. The WS-EV is intended to be used only for interchange among web services performing NLP tasks.
As such it can serve as a ?pivot? format to which user and tool-specific formats can be mapped.
6. The web service provider is responsible for providing wrappers that perform the mapping from
internally-used formats to and/or from the WS-EV.
7. The WS-EV format should be compact to facilitate the transfer of large datasets.
22
Within UIMA, the output of tools conforming to different type systems may themselves require conversion in order to be
used together.
23
Figure 5 shows a pipeline in which both GATE and UIMA services are called; GATE-to-GATE and UIMA-to-UIMA
communication does not use the WS-EV, but it is used for communication between GATE and UIMA services, as well as other
services.
37
8. The WS-EV format will be chosen to take advantage, to the extent possible, of existing technologi-
cal infrastructures and standards.
As noted in the first principle, where possible the objects and features in the WS-EV are drawn from
existing repositories such as ISOCat and the NIF Core Ontology and linked to them via the owl:sameAs
property
24
or, where appropriate, rdfs:subClassOf
25
. However, many repositories do not include some
categories and objects relevant for web service exchange (e.g., ?token? and other segment descriptors),
do include multiple (often very similar) definitions for the same concept, and/or do not specify relations
among terms. We therefore attempted to identify a set of (more or less) ?universal? concepts by surveying
existing type systems and schemas ? for example, the Julie Lab and DARPA GALE UIMA type systems
and the GATE schemas for linguistic phenomena ? together with the I/O requirements of commonly
used NLP software (e.g., the Stanford NLP tools, OpenNLP, etc.). Results of the survey for token and
sentence identification and part-of-speech labeling
26
showed that even for these basic categories, no
existing repository provides a suitable set of categories and relations.
Perhaps more problematically, sources that do specify relations among concepts, such as the various
UIMA type systems and GATE?s schemas, vary widely in their choices of what is an object and what
is a feature; for example, some treat ?token? as an object (label) and ?lemma? and ?POStag? as asso-
ciated features, while others regard ?lemma? and/or ?POStag? as objects in their own right. Decisions
concerning what is an object and what is a feature are for the most part arbitrary; no one scheme is right
or wrong, but a consistent organization is required for effective web service interchange. The WS-EV
therefore defines an organization of objects and features for the purposes of interchange only. Where
possible, the choices are principled, but they are otherwise arbitrary. The WS-EV includes sameAs and
similarTo mappings that link to like concepts in other repositories where possible, thus serving primar-
ily to group the terms and impose a structure of relations required for web service exchange in one
web-based location.
In addition to the principles above, the WS-EV is built on the principle of orthogonal design, such that
there is one and only one definition for each concept. It is also designed to be very lightweight and easy
to find and reference on the web. To that end we have established a straightforward web site (the Web
Service Exchange Vocabulary Repository
27
), similar to schema.org, in order to provide web-addressable
terms and definitions for reference from annotations exchanged among web services. Our approach is
bottom-up: we have adopted a minimalist strategy of adding objects and features to the repository only
as they are needed as services are added to the LAPPS Grid. Terms are organized in a shallow ontology,
with inheritance of properties, as shown in Figure 1.
4 WS-EV and JSON-LD
References in the JSON-LD representation used for interchange among LAPPS Grid web services point
to URIs providing definitions for specific linguistic categories in the WS-EV. They also reference doc-
umentation for processing software and rules for processes such as tokenization, entity recognition, etc.
used to produce a set of annotations, which are often left unspecified in annotated resources (see for
example (Fokkens et al., 2013)). While not required for web service exchange in the LAPPS Grid, the
inclusion of such references can contribute to the better replication and evaluation of results in the field.
Figure 3 shows the information for Token, which defines the concept, identifies application types that
produce objects of this type, cross-references a similar concept in ISOCat, and provides the URI for use
in the JSON-LD representation. It also specifies the common properties that can be specified for a set
of Token objects, and the individual properties that can be associated with a Token object. There is no
requirement to use any or all of the properties in the JSON-LD representation, and we foresee that many
web services will require definition of objects and properties not included in the WS-EVR or elsewhere.
24
http://www.w3.org/TR/2004/REC-owl-semantics-20040210/#owl sameAs
25
http://www.w3.org/TR/owl-ref/#subClassOf-def
26
Available at http://www.anc.org/LAPPS/EP/Meeting-2013-09-26-Pisa/ep-draft.pdf
27
http://vocab.lappsgrid.org
38
Figure 1: Fragment of the WS-EV ontology (associated properties in gray)
We therefore provide mechanisms for (principled) definition of objects and features beyond the WS-
EVR. Two options exist: users can provide a URI where a new term or other documentation is defined,
or users may add a definition to the WS-EVR. In the latter case, service providers use the name space
automatically assigned to them at the time of registration, thereby avoiding name clashes and providing
a distinction between general categories used across services and more idiosyncratic categories.
Figure 2 shows a fragment of the JSON-LD representation that references terms in the WS-
EV. The context statement at the top identifies the URI that is to be prefixed to any unknown
name in order to identify the location of its definition. For the purposes of the example, the
text to be processed is given inline. Our current implementation includes results from each step
in a pipeline, where applicable, together with metadata describing the service applied in each step
(here, org.anc.lapps.stanford.SATokenizer:1.4.0) and identified by an internally-defined type (stan-
ford). The annotations include references to the objects defined in the WS-EV, in this example, To-
ken (defined at http://vocab.lappsgrid.org/Token) with (inherited) features id, start, end and specific
feature string, defined at http://vocab.lappsgrid.org/Token#id, http://vocab.lappsgrid.org/Token#start,
http://vocab.lappsgrid.org/Token#end, and http://vocab.lappsgrid.orgToken/#string, respectively. The
web page defining these terms is shown in Figure 3.
"@context" : "http://vocab.lappsgrid.org/",
"metadata" : { },
"text" : {
"@value" : "Some of the strongest critics of our welfare system..." }
"steps" : [ {
"metadata" : {
"contains" : {
"Token" : {
"producer" : "org.anc.lapps.stanford.SATokenizer:1.4.0",
"type" : "stanford"
}
}
},
"annotations" : [ {
"@type" : "Token",
"id" : "tok0",
"start" : 18,
"end" : 22,
"features" : {
string" : "Some" }
},
Figure 2: JSON-LD fragment referencing the LAPPS Grid WS-EV
39
Figure 3: Token definition in the LAPPS WS-EVR
4.1 Mapping to JSON-LD
As noted above in Section 1, existing schemes and systems for organizing linguistic information ex-
changed by NLP tools vary considerably. Figure 4 shows some variants for a few commonly used NLP
tools, which differ in terminology, structure, and physical format. To be used in the LAPPS Grid, tools
such as those in the list are wrapped so that their output is in JSON-LD format, which provides syntactic
interoperability, terms are mapped to corresponding objects in the WS-EV, and the object-feature rela-
tions reflect those defined in the WS-EV. Correspondingly, wrappers transduce the JSON-LD/WS-EV
representation to the format used internally by the tool on input. This way, the tools use their internal
format as usual and map to JSON-LD/WS-EV for exchange only.
40
Name Input Form Output Form Example
Stanford tagger pt n/a word pos opl box NN1
XML n/a XML inline <word id=?0? pos=?VB?>Let</word>
NaCTeM tagger pt n/a word/pos inline box/NN1
CLAWS (1) pt n/a word pos inline box NN1
CLAWS (2) pt n/a XML inline <w id=?2? pos=?NN1?>Type</w>
CST Copenhagen pt n/a word/pos inline box/NN1
TreeTagger pt? n/a word pos lem opl The DT the
TnT token opl word pos opl der ART
word (pos pr)+ opl Falkenstein NE 8.00 NN 1.99
Twitter NLP pt opl word pos conf opl smh G 0.9406
NLTK pt s, bls [(?word?, ?pos?)] inline [(?At?, ?IN?), (?eight?, ?CD?),]
OpenNLP splitter pt n/a sentences ospl I can?t tell you if he?s here.
OpenNLP tokenizer sent ospl tokens wss, ospl I can ?t tell you if he ?s here .
OpenNLP tagger token wss, ospl word pos ospl At IN eight CD o?clock JJ on IN
pt = plain text opl = one per line wss = white space separated
ospl = one sentence per line bps = blank line separated
Figure 4: I/O variants for common splitters, tokenizers, and POS taggers
For example, the Stanford POS tagger XML output format produces output like this:
<word id="0" pos="VB">Let</word>
This maps to the following JSON-LD/WS-EV representation:
{
"@type" : "Token",
"id" : 0",
"start" : 18,
"end" : 21,
"features" : {
"string" : "Let",
"pos" : "VB"
}
}
The Stanford representation uses the term ?word? as an XML element name, gives an id and pos
as attribute-value pairs, and includes the string being annotated as element content. For conversion to
JSON-LD/WS-EV, ?word? is mapped to ?Token?, the attributes id and pos map to features of the Token
object with the same names, and the element content becomes the value of the string feature. Because
the JSON-LD representation uses standoff annotation, the attributes start and end are added in order to
provide the offset location of the string in the original data.
Services that share a format other than JSON-LD need not map into and out of JSON-LD/WS-EV
when pipelined in the LAPPS Grid. For example, two GATE services would exchange GATE XML
documents, and two UIMA services would exchange UIMA CAS, as usual. This avoids unnecessary
conversion and at the same time allows including services (consisting of individual tools or composite
workflows) from other frameworks. Figure 5 gives an example of the logical flow in the LAPPS Grid,
showing conversions into and out of JSON-LD/WS-EV where needed.
Each service in the LAPPS Grid is required to provide metadata that specifies what kind of input is
required and what kind of output is produced. For example, any service as depicted in the flow diagram
in Figure 5 can require input of a particular format (gate, uima, json-ld) with specific content (tokens,
sentences, etc.). The LAPPS Grid uses the notion of discriminators to encode these requirements, and
the pipeline composer can use these discriminators to determine if conversions are needed and/or input
requirements are met. The discriminators refer to elements of the vocabulary.
5 Collaborations
The LAPPS Grid project is collaborating with several other projects in an attempt to harmonize the
development of web service platforms, and ultimately to participate in a federation of grids and ser-
vice platforms throughout the world. Existing and potential projects across the globe are beginning to
41
Figure 5: Logical flow through the LAPPS Grid (client-server communication not represented)
converge on common data models, best practices, and standards, and the vision of a comprehensive in-
frastructure supporting discovery and deployment of web services that deliver language resources and
processing components is an increasingly achievable goal. Our vision is therefore not for a monolithic
grid, but rather a heterogeneous configuration of federated grids that implement common strategies for
managing and inter-changing linguistic information, so that services on all of these grids are mutually
accessible.
To this end, the LAPPS Grid project has established a multi-way international collaboration among the
US partners and institutions in Asia, Australia, and Europe. The basis is a formal federation among the
LAPPS Grid, the Language Grid (Kyoto University, Japan), NECTEC (Thailand), grids operated by the
University of Indonesia and Xinjiang University (China), and LinguaGrid
28
, scheduled for implementa-
tion in January 2015. The connection of these six grids into a single federated entity will enable access
to all services and resources on any of these grids by users of any one of them and, perhaps most impor-
tantly, facilitate adding additional grids and service platforms to the federation. Currently, the European
META-NET initiative is committed to joining the federation in the near future.
In addition to the projects listed above, we are also collaborating with several groups on technical
solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD
format, and a corollary development of an ontology of web service types. These collaborators include
the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project,
and the Lider project
29
. We actively seek collaboration with others in order to move closer to achieving
a ?global laboratory? for language applications.
6 Conclusion
In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WS-
EV), which provides a terminology for a core of linguistic objects and features exchanged among NLP
tools that consume and produce linguistically annotated data. The goal is to bring the field closer to
achieving semantic interoperability among NLP data, tools, and services. We are actively working to both
engage with existing projects and teams and leverage available resources to move toward convergence
of terminology in the field for the purposes of exchange, as well as promote an environment (the LAPPS
Grid) within which the WS-EV can help achieve these goals.
28
http://www.linguagrid.org/
29
http://www.lider-project.eu
42
Acknowledgements
This work was supported by National Science Foundation grants NSF-ACI 1147944 and NSF-ACI
1147912.
References
Steve Cassidy, Dominique Estival, Timothy Jones, Denis Burnham, and Jared Burghold. 2014. The Alveo Virtual
Laboratory: A Web based Repository API. In Proceedings of the Ninth International Conference on Language
Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association
(ELRA).
Christopher Cieri, Denise DiPersio, , and Jonathan Wright. 2014. Intellectual property rights management with
web services. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT,
Dublin, Ireland, August.
David Ferrucci, Eric Nyberg, James Allan, Ken Barker, Eric Brown, Jennifer Chu-Carroll, Arthur Ciccolo, Pablo
Duboue, James Fan, David Gondek, Eduard Hovy, Boris Katz, Adam Lally, Michael McCord, Paul Morarescu,
Bill Murdock, Bruce Porter, John Prager, Tomek Strzalkowski, Chris Welty, and Wlodek Zadrozny. 2009.
Towards the Open Advancement of Question Answering Systems. Technical report, IBM Research, Armonk,
New York.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010.
Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59?79.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring
from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691?1701, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Elmer Garduno, Zi Yang, Avner Maiberg, Collin McCormack, Yan Fang, and Eric Nyberg. 2013. CSE Frame-
work: A UIMA-based Distributed System for Configuration Space Exploration Unstructured Information Man-
agement Architecture. In Peter Klgl, Richard Eckart de Castilho, and Katrin Tomanek, editors, UIMA@GSCL,
CEUR Workshop Proceedings, pages 14?17. CEUR-WS.org.
Sebastian Hellmann, Jens Lehmann, S?oren Auer, and Martin Br?ummer. 2013. Integrating nlp using linked data.
In 12th International Semantic Web Conference, 21-25 October 2013, Sydney, Australia.
Nancy Ide and James Pustejovsky. 2010. What Does Interoperability Mean, Anyway? Toward an Operational
Definition of Interoperability. In Proceedings of the Second International Conference on Global Interoperability
for Language Resources. ICGL.
Nancy Ide and Keith Suderman. 2014. The Linguistic Annotation Framework: A Standard for Annotation Inter-
change and Merging. Language Resources and Evaluation.
Nancy Ide, James Pustejovsky, Nicoletta Calzolari, and Claudia Soria. 2009. The SILT and FlaReNet international
collaboration for interoperability. In Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP,
August.
Nancy Ide, James Pustejovsky, Christopher Cieri, Eric Nyberg, Di Wang, Keith Suderman, Marc Verhagen, and
Jonathan Wright. 2014. The language application grid. In Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources
Association (ELRA).
ISO-24612. 2012. Language Resource Management - Linguistic Annotation Framework. ISO 24612.
Alkesh Patel, Zi Yang, Eric Nyberg, and Teruko Mitamura. 2013. Building an optimal QA system automatically
using configuration space exploration for QA4MRE?13 tasks. In Proceedings of CLEF 2013.
Zi Yang, Elmer Garduno, Yan Fang, Avner Maiberg, Collin McCormack, and Eric Nyberg. 2013. Building optimal
information systems automatically: Configuration space exploration for biomedical information systems. In
Proceedings of the CIKM?13.
43
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 53?59,
Dublin, Ireland, August 23rd 2014.
A Conceptual Framework of Online Natural Language Processing
Pipeline Application
Chunqi Shi, Marc Verhagen, James Pustejovsky
Brandeis University
Waltham, United States
{shicq, jamesp, marc}@cs.brandeis.edu
Abstract
This paper describes a conceptual framework that enables online NLP pipelined applications to
solve various interoperability issues and data exchange problems between tools and platforms;
e.g., tokenizers and part-of-speech taggers from GATE, UIMA, or other platforms. We propose
a restful wrapping solution, which allows for universal resource identification for data manage-
ment, a unified interface for data exchange, and a light-weight serialization for data visualization.
In addition, we propose a semantic mapping-based pipeline composition, which allows experts
to interactively exchange data between heterogeneous components.
1 Introduction
The recent work on open infrastructures for human language technology (HLT) research and develop-
ment has stressed the important role that interoperability should play in developing Natural Language
Processing (NLP) pipelines. For example, GATE (Cunningham et al., 2002), UIMA (Ferrucci and Lally,
2004), and NLTK (Loper and Bird, 2002) all allow integrating components from different categories
based on common XML, or object-based (e.g., Java or Python) data presentation. The major categories
of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer,
Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution,
etc. Pipelined NLP applications can be built by composing several components; for example, a text
analysis application such as ?relationship analysis from medical records? can be composed by Sentence
Splitter, Tokenizer, POS Tagger, NER, and Coreference Resolution components.
In addition to interoperability, the very availability of a component can also play an important role in
building online application based on distributed components, especially in tasks such as online testing
and judging new NLP techniques by comparing to existing components. For example, the Language Grid
(Ishida, 2006) addresses issues relating to accessing components from different locations or providers
based on Service-Oriented Architecture (SOAs) models. In this paper, we explore structural, conceptual
interoperability, and availability issues, and provide a conceptual framework for building online pipelined
NLP applications.
The conventional view of structural interoperability is that a common set of data formats and com-
munication protocols should be specified by considering data management, data exchange, and data
visualization issues. Data management determines how to access, store and locate sources of data. For
example, GATE provides pluggable document readers or writers and XML (with meta-data configura-
tion) serialization of reusable objected-based data. UIMA provides document or database readers and
writers and XMI serialization of common object-based data structures. The Language Grid provides Java
object serialization of data collections. Data exchange strategies describe how components communi-
cate their data. For example, GATE provides CREOLE (Collection of REusable Objects for Language
Engineering) data collections for data exchange. UIMA provides CAS (Common Analysis Structure),
and NLTK provides API modules for each component type. Similarly, the Language Grid provides LSI
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
53
(Language Service Interface) for a concrete ontology for a given language infrastructure. Data visu-
alization facilitates manual reading, editing and adjudication. For example, GATE and UIMA provide
XML-based viewers for selection, searching, matching and comparison functionality.
The conventional view of conceptual interoperability is that expert knowledge should be used in bridg-
ing heterogeneous components. For example, GATE provides integration plugins for UIMA, OpenNLP,
and Stanford NLP, where experts have already engineered the specific knowledge on conversion strate-
gies among these components. This leaves open the question of how one would ensure the interoperable
pipelining of new or never-before-seen heterogeneous components, for which experts have not encoded
bridge protocols.
In order to achieve an open infrastructure of online pipelined applications, we will argue two points
regarding the conceptual design, considering both interoperability and availability:
? Universal resource identification, a SQL-like data management, and a light-weight data serialization
should be added with structural interoperability in online infrastructure of distributed components.
? By verifying and modifying inconsistent ontology mappings, experts can interactively learn con-
ceptual interoperability for online heterogeneous components pipelines.
2 Data, Tool and Knowledge Types
Interoperability in building pipelined NLP applications is intended ensure the exchange of information
between the different NLP tools. For this purpose, existing infrastructures like GATE or UIMA have
paid a lot of attention to common entity based data exchanges between the tools. When exchanging
data between heterogeneous tools (e.g., the GATE tokenizer pipelined with the NLTK POS tagger),
the knowledge of how these different entity based NLP tools can work together becomes much more
important, because there might be exchange problems between heterogeneous data or tool information,
and we may need specific knowledge to fix them. Thus, when considering interoperability, the main flow
of information should be exchanged in the open infrastructure consisting of source data information,
NLP tools information, and the knowledge that allows the tools to work together.
What are the main entity types of data and tools in designing an open infrastructure for online NLP
pipeline applications? From an abstract view of how linguistic analysis is related to human knowledge,
there are the following: Morphological, Lexical, Syntactic, Semantic, Pragmatic tool classifications; and
Utterance, Phoneme, Morpheme, Token, Syntactic Structure, Semantic Interpretation, and Pragmatic In-
terpretation data classifications. (Manaris, 1998; Pustejovsky and Stubbs, 2013). From a concrete appli-
cation perspective, where tools are available for concrete text mining for communities such as OpenNLP,
Stanford CoreNLP and NLTK, there are classification tools such as Sentence Splitter, Tokenizer, POS
Tagger, Phrase Chunker, Shallow Parser, NER, Lemmatizer, Coreference; and data classifications such
as Document, Sentence, Annotation, and Feature (Cunningham et al., 2002).
Le
xic
al
Sy
nta
ctic
Sem
ant
ic
Pra
gm
atic
Mo
rph
olo
gic
al
Figure 1: A NLP pipeline can be a (sub)-process of an abstract five-step process
POS
 Tagg
ing
Noun
-ph
rase
 
chun
king
Le
mm
atiza
tio
n
NER
Sen
ten
ce S
pli
tter
 
Token
izati
on
Core
fere
nce
Resol
utio
n
Figure 2: An example NLP pipeline of a concrete six-step process
54
The knowledge types needed for designing an open infrastructure also can be seen abstractly or con-
cretely. Abstractly, an NLP pipeline should be part of the process of morphological, lexical, syntactic,
semantic to pragmatic processing (see Figure 1). From a concrete view, each component of an NLP
pipeline should have any requisite preprocessing. For example, tokenization is required preprocessing
for POS tagging (see Figure 2). Such knowledge for building NLP pipelines can be interactively deter-
mined by the NLP expert or preset as built-in pipeline models.
Knowledge
Tool
D ata
Document Format, Structure, Style 
Morphological
(Splitter, Tokenizer , POS Tagger) 
Meta-
Information 
of 
Knowledge,
Tool, and 
Data. 
Lexical & Syntactic
(Lemmatization, Chunking, Parsing)
Semantic
(N ER, Coreference )
Pragmatic
(Indexing, Retrieval)
Knowledge of Tool Requirements, Data 
Interpretation
Figure 3: Information for NLP pipeline application description
We can put the above analyzed data, tool, and knowledge types with their meta-information together as
the information required for describing an NLP pipeline application (see Figure 3). Regarding the docu-
ment format, structure and style, for example, the Text Encoding Initiative (TEI)
1
provides one standard
for text encoding and interchange, which also enables meta-information description. Concerning the
main part (see dashdotted-line part of Figure 3), it is generally referred to as the model of annotation.
For example, GATE has its own single unified model of annotation, which is organized in annotation
graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of
features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic
Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and
Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntac-
tic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation
framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization
for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML stand-
off annotations for natively XML support, and provides XSLT-based online integration mechanism of
various annotation collections (Sch?afer, 2006). The UIMA specifies a UML-based data model of anno-
tation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold?s
XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use
GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009).
Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation
models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4)
will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion
plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will continue to be nec-
essary. In the following sections, we consider the issue of the continuing availability of such conversion
functions, and whether the current realization of those two conversion strategies is sufficient to bridge the
various annotations made available by linguistic experts, without further substantial engineering work.
3 Towards A Conceptual Design of Online Infrastructure
In this section, we discuss the conceptual design of online infrastructure, focusing on both the interop-
erability and availability of the tools. Concerning the latter, the Service-oriented architecture (SOA) is
1
http://www.tei-c.org/
55
<!-- GATE -->
<GateDocument>
<TextWithNodes>
<Node id="15"/>Sonnet<Node id="21"/>
</TextWithNodes>
<AnnotationSet>
<Annotation Id="18" Type="Token"
StartNode="15" EndNode="21">
<Feature>
<Name className="java.lang.String">length</Name>
<Value className="java.lang.String">6</Value>
</Feature>
<Feature>
<Name className="java.lang.String">category</Name>
<Value className="java.lang.String">NNP</Value>
</Feature>
<Feature>
<Name className="java.lang.String">kind</Name>
<Value className="java.lang.String">word</Value>
</Feature>
<Feature>
<Name className="java.lang.String">string</Name>
<Value className="java.lang.String">Sonnet</Value>
</Feature>
</Annotation>
</AnnotationSet>
</GateDocument>
<!-- UIMA -->
<xmi:XMI
xmlns:xmi="http://www.omg.org/XMI"
xmlns:opennlp=
"http:///org/apache/uima/examples/opennlp.ecore"
xmlns:cas="http:///uima/cas.ecore"
xmi:version="2.0">
<cas:Sofa
xmlns:cas="http:///uima/cas.ecore"
xmi:id="1" sofaNum="1" sofaID="_InitialView"
mimetype="text"
sofaString="Sonnet." />
<opennlp:Token
xmi:id="18" sofa="1"
begin="0" end="6"
posTag="NNP" />
<cas:View sofa="1"
members="18"/>
</xmi:XMI>
Figure 4: Examples of GATE XML annotation and UIMA XML annotation
a promising approach. For example, while the Language Grid infrastructure makes NLP tools highly
available (Ishida, 2006), it can still have limitations regarding interoperability issues. Generally, service
interfaces can be either operation-oriented which allows flexible operations with simple input/output
data, or resource-oriented which allows flexible input/output data with simple operations. The NLP
processing services of Language Grid are more or less operation-oriented, and lack a certain structural
flexibility for composing with each other. We present a resource-oriented view of NLP tools, which
should have universal resource identification for distributed reference, an SQL-like data management,
and a light-weight data serialization for online visualization. We propose Restful wrapping both data and
tools into Web services for this purpose.
Restful wrapping makes both data and tools easy-to-access and with a unified interface, enabling
structural interoperability between heterogeneous tools, assuming standoff annotation from various NLP
tools is applied. For example, if the NLP tools are wrapped into Restful services so that they are operated
through HTTP GET protocol, and the XML serialization of UIMA annotation is applied for input and
output, each NLP components will have the same interface and data structure.
Once an internationalized resource identifier (IRI) is given, all the input and output of tools can be
distributed and ubiquitously identified. Moreover, a PUT/GET/POST/DELETE protocol of restful data
management is equivalent to an SQL-like CRUD data management interface. For example, an IRI can
be defined by a location identifier and the URL of the data service (Wright, 2014).
In addition, a lightweight serialization of stand-off annotation can benefit the online visualization of
data, which will be easy for experts to read, judge, or edit. For example, the XML serialization of UIMA
annotation can be transferred into JSON serialization, which is preferred for online reading or editing.
NLP tool services will be available by applying restful wrapping (see Figure 5). However, structural
interoperability based on the restful wrapping is not enough for conceptual interoperability. For example,
if an OpenNLP tokenizer is wrapped using HTTP GET protocol and GATE annotation, but a Stanford
NLP POS tagger is wrapped using UIMA annotation, it will raise conceptual interoperability issues.
Based on the previously mentioned bridging strategies, a conversion service from GATE annotation to
UIMA annotation should work, or a transformation interaction with a XSLT-like service should work.
We would like to assume that the interaction and contribution of linguistic experts without online support
by engineers can solve this issue. But how can we design the interaction to take advantage of such expert
knowledge?
We present a semantic mapping-based composer for building an NLP pipeline application (see Fig-
56
NLP Pipeline Application
NLP Tool Service
Source Data
(Document, Database)
NLP Tool
( OpenNLP, Standard 
NLP,  NLTK, etc )
Restful Wrapping
1. International resource identifier (IRI) ID
2. Unified interface, GET/PUT/POST/DELETE
3. Self-description message like XML or JSON
Meta-Information
( Provider, License, 
Location)
Semantic Mapping based Composing
1. NLP tool service pipeline engine
2. Proxy service of interactive ontology mapping
Workflow Engine
( BPEL )
Stand-off Ontology
(Vocabulary) 
Meta-Information
(Process Requirements)
Figure 5: Conceptual design of online NLP pipeline application
ure 5). Conceptual interoperability requires the same vocabularies for the same concept of a standoff
annotation. Once we have the standoff ontology of annotation, we can perform automatic semantic map-
ping from NLP tool output to that ontology. The interaction from experts will be triggered once the
automatic semantic mapping has failed (see Figure 6). For example, both GATE and UIMA XML an-
notations could be transformed into JSON formation, which is easy to present as tree structure entities.
Based on these tree structure entities, automatic ontology mapping tools like UFOme, which identifies
correspondences among entities in different ontologies (Pirr?o and Talia, 2010), can be applied to build
up various mapping solutions. Knowledge from experts can also be applied interactively, and successful
mapping solutions can be stored for further reference and use.
<! -- GATE -- >
< GateDocument >
< TextWithNodes > 
<Node id="1 5 " />Sonnet<Node id="2 1 " />
</ TextWithNodes >
< AnnotationSet>
<Annotation Id=" 1 8 " Type="Token" 
StartNode ="1 5 " EndNode ="2 1 " >
<Feature>
<Name clas sName =" java.lang.String ">length</Name>
<Value clas sName =" java.lang.String ">6 </Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">category</Name>
<Value clas sName =" java.lang.String ">NNP</Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">kind</Name>
<Value clas sName =" java.lang.String ">word</Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">string</Name>
<Value clas sName =" java.lang.String ">Sonnet</Value>
</Feature>
</Annotation>
</ AnnotationSet>
</ GateDocument >
< ! - - UIMA -- >
< xmi:XM I
xmlns:xmi ="http://www.omg.org/XM I" 
xmlns:opennlp =
"http:///org/apache/uima/examples/opennlp.ecore" 
xmlns:cas ="http:/// uima/cas.ecore"
xmi:version ="2. 0">
< cas:Sofa
xmlns:cas ="http:/// uima/cas.ecore" 
xmi:id ="1" sofaNum ="1" sofaID ="_ InitialView " 
mimetype ="text" 
sofaString ="Sonnet." />
< opennlp:Token
xmi:id ="18"  sofa="1" 
begin="0" end="6" 
posTag ="NNP" />
< cas:View sofa="1" 
members="18"/ >
< / xmi:XM I >
Ontology 
Mapping
Vocabulary
Mapping 
Storage
@Id
@ StartNode
@ EndNode
Feature
Annotation
Length
Category
String
@id
@sofa
@begin
@end
Token
@ posTag
@Type
GATE Annotation
UIMA Annotation
Figure 6: Interactive ontology mapping of two different annotations of NLP tools (Tree structures are
learned from XML annotations in Figure 4 )
The semantic mapping will be interactively created by the experts, when heterogeneous components
with different data models are used in the NLP pipeline created by the end-users, who create the NLP
pipeline without consideration of components interoperability. It means that this semi-automatically
created semantic mapping separates acquiring the knowledge of tool requirements from end-users and
acquiring the knowledge of data interpretation from experts (see Figure 3). For example, the end-users
chooses two POS Taggers (OpenNLP and NLTK) and two NER tools (OpenNLP and Stanford NLP)
components in the NLP application of ?relationship analysis from medical records?. When NLTK POS
57
Tagger output are serialized in to JSON formats but cannot be directly used as the input of Stanford NLP
NER component which requires the UIMA annotation, a semantic mapping issue will be automatically
created and reported to experts. This NLTK POS Tagger JSON format output will be mapped into
the standoff ontology of annotation of POS Tagger. After that, this output will bridge with the UIMA
annotation of the Stanford NLP NER. This particular semantic mapping between JSON serialization of
a NLTK POS Tagger and the standoff ontology of annotation of POS Tagger, and between the standoff
ontology of annotation of POS Tagger and the UIMA annotation of Stanford NLP NER will be reused in
the NLP application created by other end-users.
Our conceptual framework does not exclusively rely on the above interoperability design. Our con-
ceptual framework (see Figure 5) should integrate existing knowledge of various annotation frameworks,
for example, the alignment knowledge from the Open Annotation models (Verspoor and Livingston,
2012) and the pivot bridge knowledge from the GrAF (Ide and Suderman, 2007) under the Linguistic
Annotation Framework (LAF). Thus, existing pivot conversion solutions and XSLT-based middleware
solutions can also be applied. Our interactive ontology mapping design provides a more flexible choice
for linguistic experts to build up NLP pipeline applications on top of heterogeneous components, without
online help from engineers. Below we present varying levels of online NLP applications, according to
what kind of extra support would be needed for composing different NLP components:
? Components are interoperable without extra data exchange issues. For example, tools are from the
same community (e.g., only using OpenNLP tools).
? Components are interoperable with existing solutions of data exchange issues. For example, tools
are from popular communities such as GATE plugins or UIMA modules.
? Components are interoperable with extra knowledge from experts. For example, tools are both from
popular communities and personal developments or inner group software.
? Components are interoperable with considerable effort from both experts and engineers. For exam-
ple, tools are developed under novel ontology designs.
According to these levels, our conceptual framework is targeted at the third level of interoperability
issues. Our proposal will generate a ontology mapping storage (see Figure 6), which we hope will
contribute to improving a standard annotation ontology.
4 Conclusion
In this paper, we have tried to present a conceptual framework for building online NLP pipeline applica-
tions. We have argued that restful wrapping based on the Service-Oriented Architecture and a semantic
mapping based pipeline composition benefit both the availability and interoperability of online pipeline
applications. By looking at the information surrounding the data, tools, and knowledge needed for NLP
components pipelines, we explained how experts can be limited in building online NLP pipeline applica-
tions without help from engineers, and our restful wrapping and interactive ontology mapping design can
help in such situations. Finally, we have described various levels of support needed for building online
NLP pipelines, and we believe that this study can contribute to further online implementations of NLP
applications.
Acknowledgements
This work was supported by National Science Foundation grants NSF-ACI 1147944.
References
Kalina Bontcheva, Valentin Tablan, Diana Maynard, and Hamish Cunningham. 2004. Evolving gate to meet new
challenges in language engineering. Nat. Lang. Eng., 10(3-4):349?373, September.
58
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A Framework
and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Computational Linguistics (ACL?02).
Thierry Declerck. 2006. Synaf: Towards a standard for syntactic annotation. In Proceedings of the Fifth In-
ternational Conference on Language Resources and Evaluation (LREC?06). European Language Resources
Association (ELRA).
Thierry Declerck. 2008. A framework for standardized syntactic annotation. In Bente Maegaard Joseph
Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri,
editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
David Ferrucci and Adam Lally. 2004. Uima: An architectural approach to unstructured information processing
in the corporate research environment. Nat. Lang. Eng., 10(3-4):327?348, September.
Udo Hahn, Ekaterina Buyko, Katrin Tomanek, Scott Piao, John McNaught, Yoshimasa Tsuruoka, and Sophia
Ananiadou. 2007. An annotation type system for a data-driven nlp pipeline. In Proceedings of the Linguistic
Annotation Workshop, LAW ?07, pages 33?40, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Nancy Ide and Laurent Romary. 2004. International standard for a linguistic annotation framework. Nat. Lang.
Eng., 10(3-4):211?225, September.
Nancy Ide and Keith Suderman. 2007. Graf: A graph-based format for linguistic annotations. In Proceedings of
the Linguistic Annotation Workshop, LAW ?07, pages 1?8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nancy Ide and Keith Suderman. 2009. Bridging the gaps: Interoperability for graf, gate, and uima. In Pro-
ceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP ?09, pages 27?34, Stroudsburg, PA, USA.
Association for Computational Linguistics.
T. Ishida. 2006. Language grid: an infrastructure for intercultural collaboration. In Applications and the Internet,
2006. SAINT 2006. International Symposium on, pages 5 pp.?100, Jan.
Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. In Proceedings of the ACL-02 Work-
shop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational
Linguistics - Volume 1, ETMTNLP ?02, pages 63?70, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bill Manaris. 1998. Natural language processing: A human-computer interaction perspective.
Giuseppe Pirr?o and Domenico Talia. 2010. Ufome: An ontology mapping system with strategy prediction capa-
bilities. Data Knowl. Eng., 69(5):444?471, May.
James Pustejovsky and Amber Stubbs. 2013. Natural language annotation for machine learning. O?Reilly Media,
Sebastopol, CA.
Ulrich Sch?afer. 2006. Middleware for creating and combining multi-dimensional nlp markup. In Proceedings of
the 5th Workshop on NLP and XML: Multi-Dimensional Markup in Natural Language Processing, NLPXML
?06, pages 81?84, Stroudsburg, PA, USA. Association for Computational Linguistics.
Karin Verspoor and Kevin Livingston. 2012. Towards adaptation of linguistic annotations to scholarly annotation
formalisms on the semantic web. In Proceedings of the Sixth Linguistic Annotation Workshop, LAW VI ?12,
pages 75?84, Stroudsburg, PA, USA. Association for Computational Linguistics.
Jonathan Wright. 2014. Restful annotation and efficient collaboration. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources
and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA).
59
Proceedings of SADAATL 2014, pages 31?39,
Dublin, Ireland, August 24, 2014.
Extracting Aspects and Polarity from Patents 
 Peter Anick, Marc Verhagen and James Pustejovsky 
Computer Science Department 
Brandeis University 
Waltham, MA, United States 
Peter_anick@yahoo.com, marc@cs.brandeis.edu, 
jamesp@cs.brandeis.edu 
 
Abstract 
We describe an approach to terminology extraction from patent corpora that follows from a view of pa-
tents as ?positive reviews? of inventions.  As in aspect-based sentiment analysis, we focus on identify-
ing not only the components of products but also the attributes and tasks which, in the case of patents, 
serve to justify an invention?s utility.  These semantic roles (component, task, attribute) can serve as a 
high level ontology for categorizing domain terminology, within which the positive/negative polarity of 
attributes serves to identify technical goals and obstacles.  We show that bootstrapping using a very 
small set of domain-independent lexico-syntactic features may be sufficient for constructing domain-
specific classifiers capable of assigning semantic roles and polarity to terms in domains as diverse as 
computer science and health. 
1 Introduction 
Automated data mining of patents has had a long history of research, driven by the large volume of 
patents produced each year and the many tasks to which they are put to use, including prior art inves-
tigation, competitive analysis, and trend detection and forecasting (Tseng, 2007).  Much of this work 
has concentrated on bibliographic methods such as citation analysis, but text mining has also been 
widely explored as a way to assist analysts to characterize patents, discover relationships, and facilitate 
patent searches.  One of the indicators of new technology emergence is the coinage, adoption and 
spread of new terms; hence the identification and tracking of technical terminology over time is of par-
ticular interest to researchers designing tools to support analysts engaged in technology forecasting 
(e.g., Woon, 2009; deMiranda, 2006) 
For the most part, research into terminology extraction has either (1) focused on the identification of 
keywords within individual patents or corpora without regard to the roles played by the keywords 
within the text (e.g., Sheremetyeva, 2009) or, (2) engaged in fine-grained analysis of the semantics of 
narrow domains (e.g., Yang, 2008).  In this paper we strive towards a middle ground, using a high-
level classification suitable for all domains, inspired in part by recent work on sentiment analysis (Liu, 
2012). In aspect-based sentiment analysis, natural language reviews of specific target entities, such as 
restaurants or cameras, are analyzed to extract aspects, i.e., features of the target entities, along with 
the sentiment expressed toward those features.  In the restaurant domain, for example, aspects might 
include the breadth of the menu, quality of the service, preparation of the food, and cost. Aspects thus 
tend to capture the tasks that the entity is expected to perform and various dimensions and components 
related to those tasks.  Sentiment reflects the reviewer?s assessment of these aspects on a scale from 
negative to positive.   
A patent application is required by definition to do three things: describe an invention, argue for its 
novelty, and justify its utility.  The utility of a patent is typically defined by the accomplishment of a 
new task or an improvement to some existing task along one or more dimensions.  Thus, a patent can 
be thought of as a positive review of a product with respect to specific aspects of its task(s).  Indeed, 
the most commonly occurring verbs in patents include those indicative of components (?comprise?, 
?include?), attributes (?increase?, ?reduce?), and tasks (?achieve?, ?perform?).   Organizing keywords 
along these high-level distinctions, then, would allow patent analysts to explore terminological infor-
This work is licensed under a Creative Commons Attribution 4.0 International License.  Page numbers and proceedings foot-
er are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
31
mation from several different relevant perspectives.  Furthermore, given the interpretation of a patent 
as a positive review, it should be possible to identify the default polarity of measurable aspects in the 
context of a domain.  For example, if a patent makes a reference to increasing network bandwidth, 
then this should lend support to the notion that network bandwidth is not only a relevant attribute with-
in the patent?s domain but also a positive one.  Likewise, if a patent refers to reducing power con-
sumption, then we might interpret power consumption as an aspect with negative polarity.  For ana-
lysts trying to assess trends within a technology domain, tracking the occurrences of terms signifying 
tasks and attributes, along with their polarity, could help them characterize the changing goals and ob-
stacles for inventors over time. 
The US patent office receives over half a million patent applications a year.1  These are classified by 
subject matter within several standardized hierarchical schemes, which permits dividing up the corpus 
of patents both by application date and subfield (e.g., computer science, health, chemistry).  Since our 
goal is to support analysts across all domains, it is highly desirable to extract domain-specific aspects 
through semi-supervised machine learning rather than incur the cost of domain-specific knowledge 
engineering.  To this end, we employed a bootstrapping approach in which a small number of domain 
independent features was used to generate a much larger number of domain dependent features for 
classification.  We then applied na?ve Bayes classification in a two-step classification process: first 
distinguishing attributes, components and tasks; and then classifying the extracted attribute terms by 
their polarity. 
The paper is structured as follows.  In section 2, we describe the system architecture.  Section 3 
shows results for two domains (computer science and health).  In section 4, we present an evaluation 
of results and discuss issues and shortcomings of the current implementation.  In section 5, we present 
related research and in section 6, our conclusions and directions for future work. 
2 System architecture 
2.1 Corpus processing 
Our patent collection is a set of 7,101,711 US patents in XML-markup form from Lexis-Nexis.  We 
divided the collection into subcorpora by application year and high-level domain using the patents? 
classification within the USPTO hierarchy.  The XML markup was then used to extract the relevant 
portions of patents for further analysis.  These sections included title, abstract, background, summary, 
description and claims. References, other than those embedded in the sections above, were omitted, as 
they contain many entity types (people, publications, and organizations) that are not particularly useful 
for our current task.  The text of each section was extracted and broken into sentences by the Stanford 
tagger (Toutanova, 2003) which also tokenized and tagged each token with a part of speech tag.   
We then chunked adjacent tokens into simple noun phrase chunks of the form (ADJECTIVE)? 
(NOUN)* NOUN.2  We will hereafter refer to these chunks as terms.  The majority of these patent 
terms fall into one of three major categories:  
Components: the physical constituents or processes that make up an invention, as well as the ob-
jects impacted, produced by or used in the invention.   
Tasks: the activities which inventions, their components or beneficiaries perform or undergo.   
Attributes: the measureable dimensions of tasks and components mentioned in the patent. 
 
To generate features suitable for machine learning of these semantic categories, we used a small set 
of lexico-syntactic relationships, each defined with respect to the location of the term in a sentence: 
prev_V: the closest token tagged as a verb appearing to the left of the term, along with any preposi-
tions or particles in between.  (cached_in, prioritizing, deal_with) 
prev_VNpr: a construction of the form <verb><NP><prep> appearing to the left of the term.  Only 
the head noun in the NP is retained (inform|user|of, provides|list|of, causes|increase|in) 
prev_Npr: a construction of the form <noun><prep> appearing to the left of the term. (re-
striction_on, applicability_of, time_with) 
                                                 
1 http://www.uspto.gov/web/offices/ac/ido/oeip/taf/us_stat.htm 
2 We blocked a set of 246 general adjectival modifiers (e.g., other, suitable, preferred, entire, initial,?) from participating in 
terms. 
32
prev_Jpr: a construction of form <adjective> <prep> appearing to the left of the term. (free_from, 
desirable_in, unfamiliar_with) 
prev_J: a construction of form <adjective> <prep> appearing to the left of the term. (excessive, con-
siderable, easy) 
 
These features were designed to capture specific dependency relations between the term and its pre-
modifiers and dominant verbs, nouns, and adjective phrases.  We extracted the features using localized 
rules rather than create a full dependency parse.3  One additional feature internal to the term itself was 
also included: last_word.  This simply captured the head term of the noun phrase, which often carries 
generalizable semantic information about the phrase.  Each feature instance was represented as a string 
comprising a prefix (the feature type) and its value (a token or concatenation of tokens).   
 
2.2 Classification 
 
For each term appearing in a subcorpus, the collection of co-occurring features across all documents 
was assembled into a single weighted feature vector in which the weight captured the number of doc-
uments for which the feature occurred in conjunction with the given term.  We also calculated the 
document frequency for each term, as well as its ?domain specificity score?, a metric reflecting the 
relative frequency of the term in specialized vs. randomized corpora (see section 3).   
In order to avoid the need to create manually labeled training data for each patent domain, we em-
ployed bootstrapping, a form of semi-supervised learning in which a small number of labeled features 
or seed terms are used in an iterative fashion to automaticaly identify other likely diagnostic features 
or category exemplars.  Bootstrapping approaches have previously shown considerable promise in the 
construction of semantic lexicons (Riloff, 1999; Thelen, 2002, Ziering, 2013).  By surveying common 
prev_V features in a domain-independent patent subcorpus, we selected a small set of domain-
independent diagnostic lexico-syntactic features (?seed features?) that we felt were strong indicators 
for each of the three semantic categories.  The set of seed features for each category is shown below.  
Semantically equivalent inflectional variants were also included as features.   
 
Attribute: improve, optimize, increase, decrease, reduce 
Component: comprise, contain, encompass, incorporate, use, utilize, consist_of, assembled_of, com-
posed_of 
Task: accomplish, achieve, enhance, facilitate, assisting_in, employed_in, encounter_in, perform, 
used_for, utilized_for 
 
We then utilized these manually labeled generic features to bootstrap larger feature sets F for do-
main-specific subcorpora.  For each term t in a domain-specific subcorpus, we extracted all the manu-
ally labeled features that the term co-ocurred with. Any term which co-occurred with at least two la-
beled feature instances and for which all of its labeled features were of the same class was itself la-
beled with that class for subsequent use as a seed term s for estimating the parameters of a multinomial 
na?ve Bayes classifier (Manning et al, 2008).  Each seed term so selected was represented as a bag of 
its co-occurring features.   
 
The prior probability of each class and conditional probabilities of each feature given the class were 
estimated as follows, using Laplace ?add one? smoothing to eliminate 0 probabilities: 
 
 ?(  )   
      
     
 
 
 ?(   )   
     (   )   
     ( )       
 
                                                 
3 The compute time required to produce dependency parses for the quantity of data to be analyzed led to the choice of a 
?leaner? feature extraction method. 
33
 
where    is the set of seed terms with class label j, S is the set of all seed terms, count(f,c) is the count 
of co-occurrences of feature f with seed terms in class c, count(c) is the total number of feature co-
occurrences with seed terms in class c, and F is the set of all features (used for Laplace smoothing).  
Using the na?ve Bayes conditional independence assumption, the class of each term in a subcorpus 
was then computed by maximizing the product of the prior probability for a class and the product of 
the conditional probabilities of the term?s features: 
         
    
 ( ) ? (   )
   
 
 
Terms for which no diagnostic features existed were labeled as ?unknown?.   
Once the terms in a subcorpus were categorized as attribute, component, or task, the terms identi-
fied as attributes were selected as input to a second round of classification.4  We used the same boot-
strapping process as described for the first round, choosing a small set of features highly diagnostic of 
the polarity of attributes.  For positive polarity, the seed features were: increase, raise, maximize. For 
negative polarity: avoid, lower, decrease, deal_with, eliminate, minimize, reduce, resulting_from, 
caused_by.  Based on co-occurrence with these features, a set of terms was produced from which pa-
rameters for a larger set of features could be estimated, as described above.  We then used na?ve Bayes 
classification to label the full set of attribute terms. 
3 Results 
We present results from two domains, health and computer science, using a corpus consisting of all 
US patent applications submitted in the year 2002. The health subcorpus consisted of 19,800 docu-
ments, while the computer science subcorpus contained 51,058 documents.  A ?generic? corpus com-
posed of 38,482 patents randomly selected from all domains was also constructed for the year for use 
in computing a ?domain specificity score?.  This score was designed to measure the degree to which a 
term could be considered part of a specific domain?s vocabulary and was computed as the 
log(probability of term in domain corpus / probability of term in generic corpus).  For example, in 
computer science, the term encryption technology earned a domain specificity score of 4.132, while 
speed earned .783 and color garnered .022.  Using a combination of term frequency (# of documents a 
term occurs in within a domain) thresholds and domain specificity, one can extract subsets of terms 
with varying degrees of relevance within a collection.5 
 
3.1 Attribute/Component/Task (ACT) Classification 
The bootstrapping process generated 1,644 features for use in the health domain and 3,200 in com-
puter science. Kullback-Leibler divergence is a commonly used metric for comparing the difference 
between two probability distributions (Kullback and Leibler, 1951).  By computing Kullback-Leibler 
divergence    (    ) between the distribution P of classes predicted by each feature (i.e., the proba-
bility of the class given the feature alone based on the term seed set labels) and the prior class distribu-
tion Q, we could estimate the impact of individual features in the model.  Table 1 shows some of the 
domain-specific features in the health and computer science domains, along with the category each 
tended to select for.6   
Using the features generated by bootstrapping, the classifier was able to label 61% of the 1,335,240 
terms in health and 81% of the 1,391,402 terms in computer science.  The majority of unlabeled terms 
were extremely low frequency (typically 1).  Higher frequency unlabeled terms were typically from 
categories other than those under consideration here (e.g., john wiley, j. biochem, 2nd edition).  The 
distribution of category labels for the health and computer domains is shown in Table 2. 
                                                 
4 We found relatively little evidence of explicit sentiment targeted at component and task aspects in patents and therefore 
focused our polarity analysis on attributes.  
5 Similar to Velardi?s use of ?domain relevance? and ?consensus? (Velardi, 2001). 
6 Although it is possible to use KL-Divergence for feature selection, it is applied here solely for diagnostic purposes to verify 
that feature distributions match our intuitions with respect to the classification scheme. 
34
 
 
Table 1.  Features highly associated with classes (a[ttribute], c[omponent], t[ask]) in the health and com-
puter science domains, along with an example of a term co-occurring with each feature in some patent. 
Health                                                                              Computer Science 
Feature Class Term Feature Class Term 
prev_V=performed_during                  t biopsy prev_V=automates                t retrieval 
prev_V=undergone                     t angioplasty last_word=translation            t axis translation 
prev_V=suffer                            a hypertension prev_Npr=reduction_in         a power usage 
prev_Npr=monitoring_of           a alertness Prev_Npr=degradation_in                  a audio quality 
prev_V=binds_to                        c cytokines prev_V=displayed_on           c oscillograph 
prev_Npr=salts_of                      c saccharin last_word=information          c customer infor-
mation 
 
Table 2. Number and percentage of category labels for health and computer domains (2002) 
Category Health Computer Science 
attribute 88,860   (10.8 %) 56,389   (6.5%) 
component 680,034  (83.2%) 716,688  (83.2%) 
task 48,002  (5.8 %) 88,786   (10.3%) 
 
Tables 3a and 3b show examples of machine-labeled terms for the health and computer science do-
mains.  When terms were ranked by frequency, given a relatively relaxed domain specificity threshold 
(e.g., .05 for health), the top terms tended to capture broad semantic types relevant to the domain.   As 
this threshold was increased (e.g., to 1.0 for health), the terms increased in specialization within each 
class.7 As the table entries show, while the classification is not perfect, most terms fit the definitions of 
their respective classes.  Note that in the health domain in particular, many of the ?components? reflect 
objects acted upon by the invention, not just constituents of inventions themselves.  Symptoms and 
diseases are interpreted as attributes because they are often measured according to severity and are 
targets for reduction.  
 
Table 3a. Examples  of ACT category results for health domain at two levels of domain specificity (ds). 
Component 
(ds .05) 
  
(ds 1.0) 
Attribute 
(ds .05) 
  
(ds 1.0) 
Task 
(ds .05) 
  
(ds 1.0) 
patients, 
tissue, 
blood, 
diseases, 
drugs, 
skin, 
catheter, 
brain, 
tablets, 
organs 
mitral valve, 
arterial blood, 
small incisions, 
pulmonary 
veins, 
anterior cham-
ber, 
intraocular 
lens, 
ultrasound sys-
tem, 
ultrasound en-
ergy, 
adenosine tri-
phosphate, 
bone fragments 
disease, 
infection, 
symptoms, 
pain, 
efficacy, 
side effects, 
inflammation, 
severity, 
death, 
blood flow 
cosmetic prop-
erties, 
cardiac activity, 
urination, 
tissue tempera-
ture, 
gastric empty-
ing, 
arousal 
neurotransmitter 
release, 
atrial arrhyth-
mias, 
thrombogenicity 
ventricular pac-
ing 
treatment, 
administration, 
therapy, 
surgery, 
diagnosis, 
oral admin-
istration, 
implantation, 
stimulation, 
parenteral 
administration, 
surgical pro-
cedures 
invasive proce-
dure, 
ultrasound imag-
ing, 
systole, 
anastomosis, 
spinal fusion, 
tissue ablation, 
image, recon-
struction, 
cardiac pacing, 
mass analysis, 
spinal surgery 
 
  
                                                 
7 The domain specificity thresholds chosen here differ between domains in order to compensate for the influence of the size 
of each domain?s subcorpus on the terminology mix in the ?generic? domain corpus against which domain specificity is 
measured.   In the future, we plan to compensate directly for these size disparities in the score computation.  
35
Table 3b. Examples of ACT category results for computer domain at two levels of domain specificity. 
Component 
(ds 1.5) 
 
(ds 3.0) 
Attribute 
(ds 1.5) 
  
(ds 3.0) 
Task 
(ds 1.5) 
  
(ds 3.0) 
data, 
information, 
network, 
computer, 
users, 
memory, 
internet, 
software, 
program, 
processor 
web applica-
tions, 
object access 
protocol, 
loans, 
memory sub-
system, 
function call, 
obligations, 
source file, 
file formats, 
lender 
centralized 
database 
errors, 
security, 
real time, 
traffic, 
overhead, 
delays, 
latency, 
burden, 
sales, 
copyright, 
protection 
interest rate, 
resource utiliza-
tion, 
resource con-
sumption, 
temporal locali-
ty, 
system errors, 
transport layer 
security, 
performance 
bottleneck, 
processor ca-
pacity, 
cpu utilization, 
shannon limit 
access, 
communication, 
execution, 
implementation, 
communications, 
management, 
task, 
tasks, 
stores, 
collection 
network envi-
ronments, 
business activi-
ties, 
database access, 
server process, 
search operation, 
client 's request, 
backup opera-
tion, 
project man-
agement, 
program devel-
opment, 
document man-
agement 
 
3.2 Polarity Classification 
For the polarity classification task, the system assigned positive or negative polarity to 80,870 
health and 73,289 computer science attributes. While not all the system labeled attributes merited their 
designation as attributes, the large quantity so labeled in each domain illustrates the vast number of 
conditions and dimensions for which inventions are striving to ?move the needle? one way or the oth-
er, relative to attributes in the domain.  Examples of the system?s polarity decisions are shown in Ta-
ble 4.  The system?s labels suggest that the default polarity of attributes in both domains is nearly 
evenly split. 
 
Table 4.  Examples of (pos)itive and (neg)ative polarity terms in health and computer science domains 
Domain # attributes % of total Examples 
health   
          pos 
43807 54% ambulation, hemodynamic performance, atrial rate, antico-
agulant activity, coaptation, blood oxygen saturation 
 
          neg 
37063 46% bronchospasm, thrombogenicity, ventricular pacing, with-
drawal symptoms, fibrin formation, cardiac dysfunction 
computer 
science                     
          pos 
32291 44% transport layer security, processor capacity, cpu utilization, 
routability, network speeds, microprocessor performance 
 
          neg 
40998 56% identity theft, deadlocks, system overhead, memory frag-
mentation, risk exposure, bus contention, software devel-
opment costs, network latencies, data entry errors 
 
4 Evaluation and discussion 
In order to evaluate the classification output, we first selected a subset of terms within each domain 
as candidates for evaluation based on the twin criteria of document frequency and domain specificity.  
That is, we wished to concentrate on terms with sufficient presence in the corpus as well as terms that 
were likely to express concepts of particular relevance to the domain.  Using a frequency threshold of 
10 this yielded 19,088 terms for the health corpus and 35,220 for computer science with domain speci-
ficity scores above .05 and 1.5 respectively.  For each domain, two judges annotated approximately 
150 random term instances with ACT judgments and approximately 100 machine-labeled attributes for 
polarity. The annotation tool displayed each term along with five random sentences from the corpus 
that contained the term, and asked the judge to choose the best label, given the contexts provided.  An 
36
?other? option was available if the term fit none of the target categories.  For the polarity task, the 
?other? label included cases where the attribute was neutral, could not be assigned a polarity, or was 
improperly assigned the category ?attribute?.   An adjudicated gold standard was compared to system 
labels to measure precision and recall, as shown in table 5.  
 
Table 5a. Health domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .70 .44 .54 
 component .76 1.0 .86 
 task .86 .29 .43 
Polarity  positive  .53 .85 .65 
                 negative .77 .93 .84 
 
 Table 5b. Computer domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .80 .62 .70 
 component .86 .96 .90 
 task .43 .33 .38 
Polarity  positive  .67 .88 .76 
                 negative .75 .86 .80 
 
 Although the size of the evaluation set is small, we can make some observations from this sample. 
Precision in most cases is strong, which is important for the intended use of this data to characterize 
trends along each dimension using terminology statistics over time.  The lower scores for tasks within 
the ACT classification may reflect the fact that the distinction between component and task is not al-
ways clear cut.  The term ?antivirus protection?, for example, describes a task but it is classified by the 
system as a component because it occurs with features like ?prev_V=distribute? and 
?prev_V=provided_with?, which outweigh the contribution of the feature ?last_word=protection? to 
select for the type task.  To capture such cases of role ambiguity, it may be reasonable to assign some 
terms to multiple classes when the conditional probabilities for the two most probable classes are very 
close (as they are in this case).  It may also be possible to integrate other forms of evidence, such as 
syntactic coordination patterns (Zierning, 2013) to refine system decisions. 
 One shortcoming of the current polarity classifier is that it does not attempt to identify attributes for 
which the polarity is neutral or dependent upon further context within the domain.  For example, the 
attribute ?body weight gain? is labeled as a negative.  However, in the context of premature birth or 
cancer recovery, it may be actually be a positive attribute.  Testing whether an attribute co-occurs with 
conflicting features (e.g., prev_V=increase and prev_V=decrease) could help spot such cases. 
5 Related work 
Text mining from patents has focused on identifying domain keywords and terminology for analyt-
ics (Tseng, 2007).  Velardi?s (2001) approach, using statistics to determine domain relevance and con-
sensus is very similar to that adopted here. We have also drawn inspiration from sentiment analysis, 
proposing an ontology for patents that reflects their review-like qualities (Liu, 2012).  Most relevant is 
the work on discovering aspects and opinions relating to a particular subject such as a camera or res-
taurant (Kobayashi, 2007).  There are many subtleties that have been studied in opinion mining re-
search that we have finessed in our research here, such as detecting implicit sentiment and attributes 
not expressed as noun phrases.  Wilson et al (2005, 2009) addressed the larger problem of determining 
contextual polarity for subjective expressions in general, putting considerable effort into the compila-
tion of subjectivity clues and annotations.  In contrast, our aim was to test whether we could substan-
tially reduce the annotation effort when the task is focused on polarity labeling of attributes within pa-
tents.  We hypothesized that the specialized role of patents might permit a more lightweight approach 
amenable to bootstrapping from a very small set of annotations and feature types.   
37
Bootstrapping has been successfully applied to developing semantic lexicons containing a variety of 
concept types (Riloff, 1999; Thelen, 2002).  It is often applied iteratively to learn new discriminative 
features after a set of high probability categorized terms are identified during an earlier round.  While 
this increases recall, it also runs the risk of semantic drift if some terms are erroneously labeled.  Giv-
en that the majority of unlabeled terms after a single round in our system are either extremely low fre-
quency or not relevant to our ontology, we have not felt a need to run multiple iterations.  Zierning 
(2013) used bootstrapping to identify instances of the classes substance and disease in patents, exploit-
ing the tendency of syntactic coordination to relate noun phrases of the same semantic type.  Given the 
general nature of coordination, a similar approach could be used to find corroborating evidence for the 
classifications that our system produces. 
  
6 Conclusion 
We have described an approach to text data mining from patents that strikes a middle ground be-
tween undifferentiated keywords and rich, domain specific ontologies.  Motivated by the interpretation 
of patents as ?positive reviews?, we have made use of generic lexico-syntactic features common 
across patent domains to bootstrap domain-specific classifiers capable of organizing terms according 
to their roles as components, tasks and attributes with polarity.  Although the majority of keywords in 
a domain are categorized as components, the ontology puts tasks and attributes on an equal footing 
with components, thereby shifting the emphasis from devices and processes to the goals, obstacles and 
targets of inventions, information which could be valuable for analysts attempting to detect trends and 
make forecasts. In addition to more rigorous evaluation and tuning, future research directions include 
testing the approach across a wider range of technology domains, incorporation into time series analy-
sis for forecasting, and mining relationships between terms from different categories to provide an 
even richer terminological landscape for analysts to work with. 
Acknowledgements 
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via 
Department of Interior National Business Center (DoI/NBC) contract number D11PC20154. The U.S. 
Government is authorized to reproduce and distribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein 
are those of the authors and should not be interpreted as necessarily representing the official policies 
or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. 
References 
de Miranda, G. M. Coelho, Dos, and L. F. Filho. (2006) Text mining as a valuable tool in foresight exercises: A 
study on nanotechnology.  Technological Forecasting and Social Change, 73(8):1013?1027. 
Kobayashi, N., Inui, K. and Matsumoto, Y. (2007) Extracting aspect-evaluation and aspect-of relations in opin-
ion mining, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing 
and Computational Natural Language Learning, Prague, Czech Republic, pp. 1065?1074. 
Kullback, S. and Leibler, R. (1951). "On Information and Sufficiency". Annals of Mathematical Statistics 22 (1): 
79?86. 
Liu, B. (2012): Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies, 
Morgan & Claypool Publishers. 
Manning, C., Raghavan, P. and Sch?tze, H. (2008) Introduction to Information Retrieval.  Cambridge University 
Press. 
Riloff, E. and Jones, R. (1999) Learning dictionaries for information extraction by multi-level bootstrapping. In 
Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications 
of Artificial Intelligence Conference, pp. 474?479. 
Riloff, E. and Shepherd, J. (1997) A corpus-based approach for building semantic lexicons. In Proceedings of the 
Second Conference on Empirical Methods in Natural Language Processing, pp. 117?124. 
38
Shih, M.J., Liu, D.R., and Hsu, M.L. (2008) Mining Changes in Patent Trends for Competitive Intelligence. 
PAKDD 2008: 999-1005. 
Sheremetyeva S. 2009. An Efficient Patent Keyword Extractor As Translation Resource Proceedings of the 3rd 
Workshop on Patent Translation in conjunction with MT-Summit XII Ottawa, Canada. 
Thelen, M. and Riloff, E.  (2002) A bootstrapping method for learning semantic lexicons using extraction pattern 
contexts. In Proceedings of the Conference on Empirical Methods in Natural Language. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. (2003) Feature-Rich Part-of-Speech Tagging with a Cyclic 
Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259. 
Tseng, Y.-H., Lin, C.-J., and Lin, Y.-I. (2007). Text mining techniques for patent analysis. Information Pro-
cessing & Management, 43(5):1216 ? 1247. 
Velardi, P., Fabriani, P. and Missikoff, M. (2001) FOIS '01 Proceedings of the international conference on For-
mal Ontology in Information Systems - Volume 2001, pp. 270-284. 
Wilson, T., Wiebe, J and Hoffmann, P. (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment 
Analysis. Joint Human Language Technology Conference and the Conference on Empirical Methods in Natu-
ral Language Processing (HLT-EMNLP-2005). 
 Wilson, T., Wiebe, J and Hoffmann, P. (2009). Recognizing Contextual Polarity: an exploration of features for 
phrase-level sentiment analysis. Computational Linguistics 35(3). 
Woon, W. L., Henschel, A., and Madnick, S. (2009) A Framework for Technology Forecasting and Visualiza-
tion.  Working Paper CISL# 2009-11 , Massachusetts Institute of Technology.  
Yang, S.Y., Lin, S.Y., Lin, S. N., Lee, C. F., Cheng, S. L., and Soo, V. W. (2008) Automatic extraction of se-
mantic relations from patent claims.  International Journal of Electronic Business Management, Vol. 6, No. 1, 
pp. 45-54 (2008) 45. 
Ziering, P., van der Plas, L. and Sch?tze, H. (2013) Bootstrapping Semantic Lexicons for Technical Domains. In 
Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 844?848, Nago-
ya, Japan, October 2013. Asian Federation of Natural Language Processing. 
 
 
 
39
