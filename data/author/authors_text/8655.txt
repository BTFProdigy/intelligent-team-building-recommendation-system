Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 553?561,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Using Bilingual Knowledge and Ensemble Techniques for Unsupervised 
Chinese Sentiment Analysis 
 
Xiaojun Wan 
Institute of Compute Science and Technology 
Peking University 
Beijing 100871, China 
wanxiaojun@icst.pku.edu.cn 
 
Abstract 
It is a challenging task to identify sentiment 
polarity of Chinese reviews because the re-
sources for Chinese sentiment analysis are 
limited. Instead of leveraging only monolin-
gual Chinese knowledge, this study proposes a 
novel approach to leverage reliable English 
resources to improve Chinese sentiment 
analysis. Rather than simply projecting Eng-
lish resources onto Chinese resources, our ap-
proach first translates Chinese reviews into 
English reviews by machine translation ser-
vices, and then identifies the sentiment polar-
ity of English reviews by directly leveraging 
English resources. Furthermore, our approach 
performs sentiment analysis for both Chinese 
reviews and English reviews, and then uses 
ensemble methods to combine the individual 
analysis results. Experimental results on a 
dataset of 886 Chinese product reviews dem-
onstrate the effectiveness of the proposed ap-
proach. The individual analysis of the 
translated English reviews outperforms the in-
dividual analysis of the original Chinese re-
views, and the combination of the individual 
analysis results further improves the perform-
ance.  
1 Introduction 
In recent years, sentiment analysis (including sub-
jective/objective analysis, polarity identification, 
opinion extraction, etc.) has drawn much attention 
in the NLP field. In this study, the objective of sen-
timent analysis is to annotate a given text for polar-
ity orientation (positive/negative). Polarity 
orientation identification has many useful applica-
tions, including opinion summarization (Ku et al, 
2006) and sentiment retrieval (Eguchi and 
Lavrenko, 2006).  
To date, most of the research focuses on English 
and a variety of reliable English resources for sen-
timent analysis are available, including polarity 
lexicon, contextual valence shifters, etc. However, 
the resources for other languages are limited. In 
particular, few reliable resources are available for 
Chinese sentiment analysis1 and it is not a trivial 
task to manually label reliable Chinese sentiment 
resources.  
Instead of using only the limited Chinese knowl-
edge, this study aims to improve Chinese sentiment 
analysis by making full use of bilingual knowledge 
in an unsupervised way, including both Chinese 
resources and English resources. Generally speak-
ing, there are two unsupervised scenarios for ?bor-
rowing? English resources for sentiment analysis 
in other languages: one is to generate resources in 
a new language by leveraging on the resources 
available in English via cross-lingual projections, 
and then perform sentiment analysis in the English 
language based on the generated resources, which 
has been investigated by Mihalcea et al (2007); 
the other is to translate the texts in a new language 
into English texts, and then perform sentiment 
analysis in the English language, which has not yet 
been investigated.  
In this study, we first translate Chinese reviews 
into English reviews by using machine translation 
services, and then identify the sentiment polarity of 
English reviews by directly leveraging English 
resources. Furthermore, ensemble methods are 
employed to combine the individual analysis re-
sults in each language (i.e. Chinese and English) in 
order to obtain improved results. Given machine 
translation services between the selected target 
language and English, the proposed approach can 
be applied to any other languages as well.  
Experiments have been performed on a dataset 
of 886 Chinese product reviews. Two commercial 
                                                          
1 This study focuses on Simplified Chinese. 
553
machine translation services (i.e. Google Translate 
and Yahoo Babel Fish) and a baseline dictionary-
based system are used for translating Chinese re-
views into English reviews. Experimental results 
show that the analysis of English reviews trans-
lated by the commercial translation services out-
performs the analysis of original Chinese reviews. 
Moreover, the analysis performance can be further 
improved by combining the individual analysis 
results in different languages. The results also 
demonstrate that our proposed approach is more 
effective than the approach that leverages gener-
ated Chinese resources. 
The rest of this paper is organized as follows: 
Section 2 introduces related work. The proposed 
approach is described in detail in Section 3. Sec-
tion 4 shows the experimental results. Lastly we 
conclude this paper in Section 5.  
2 Related Work 
Polarity identification can be performed on word 
level, sentence level or document level. Related 
work for word-level polarity identification includes 
(Hatzivassiloglou and McKeown, 1997; Kim and 
Hovy. 2004; Takamura et al, 2005; Yao et al 
2006; Kaji and Kitsuregawa, 2007), and related 
work for sentence-level polarity identification in-
cludes (Yu and Hatzivassiloglou, 2003; Kim and 
Hovy. 2004) Word-level or sentence-level senti-
ment analysis is not the focus of this paper.   
Generally speaking, document-level polarity 
identification methods can be categorized into un-
supervised and supervised.  
Unsupervised methods involve deriving a senti-
ment metric for text without training corpus.  Tur-
ney (2002) predicates the sentiment orientation of 
a review by the average semantic orientation of the 
phrases in the review that contain adjectives or 
adverbs, which is denoted as the semantic oriented 
method. Kim and Hovy (2004) build three models 
to assign a sentiment category to a given sentence 
by combining the individual sentiments of senti-
ment-bearing words. Hiroshi et al (2004) use the 
technique of deep language analysis for machine 
translation to extract sentiment units in text docu-
ments. Kennedy and Inkpen (2006) determine the 
sentiment of a customer review by counting posi-
tive and negative terms and taking into account 
contextual valence shifters, such as negations and 
intensifiers. Devitt and Ahmad (2007) explore a 
computable metric of positive or negative polarity 
in financial news text.  
Supervised methods consider the sentiment 
analysis task as a classification task and use la-
beled corpus to train the classifier. Since the work 
of Pang et al (2002), various classification models 
and linguistic features have been proposed to im-
prove the classification performance (Pang and Lee, 
2004; Mullen and Collier, 2004; Wilson et al, 
2005a; Read, 2005). Most recently, McDonald et al 
(2007) investigate a structured model for jointly 
classifying the sentiment of text at varying levels 
of granularity. Blitzer et al (2007) investigate do-
main adaptation for sentiment classifiers, focusing 
on online reviews for different types of products. 
Andreevskaia and Bergler (2008) present a new 
system consisting of the ensemble of a corpus-
based classifier and a lexicon-based classifier with 
precision-based vote weighting. 
Research work focusing on Chinese sentiment 
analysis includes (Tsou et al, 2005; Ye et al, 2006; 
Li and Sun, 2007; Wang et al, 2007). Such work 
represents heuristic extensions of the unsupervised 
or supervised methods for English sentiment 
analysis.  
To date, the most closely related work is Mihal-
cea et al (2007), which explores cross-lingual pro-
jections to generate subjectivity analysis resources 
in Romanian by leveraging on the tools and re-
sources available in English. They have investi-
gated two approaches: a lexicon-based approach 
based on Romanian subjectivity lexicon translated 
from English lexicon, and a corpus-based approach 
based on Romanian subjectivity-annotated corpora 
obtained via cross-lingual projections. In this study, 
we focus on unsupervised sentiment polarity iden-
tification and we only investigate the lexicon-based 
approach in the experiments. 
Other related work includes subjective/objective 
analysis (Hatzivassiloglon and Wiebe, 2000; Riloff 
and Wiebe, 2003) and opinion mining and summa-
rization (Liu et al, 2005; Popescu and Etzioni. 
2005; Choi et al, 2006; Ku et al, 2006; Titov and 
McDonald, 2008).  
3 The Proposed Approach 
3.1 Overview 
The motivation of our approach is to make full use 
of bilingual knowledge to improve sentiment 
analysis in a target language, where the resources 
554
for sentiment analysis are limited or unreliable. 
This study focuses on unsupervised polarity identi-
fication of Chinese product reviews by using both 
the rich English knowledge and the limited Chi-
nese knowledge. 
The framework of our approach is illustrated in 
Figure 1. A Chinese review is translated into the 
corresponding English review using machine trans-
lation services, and then the Chinese review and 
the English review are analyzed based on Chinese 
resources and English resources, respectively. The 
analysis results are then combined to obtain more 
accurate results under the assumption that the indi-
vidual sentiment analysis can complement each 
other. Note that in the framework, different ma-
chine translation services can be used to obtain 
different English reviews, and the analysis of Eng-
lish reviews translated by a specific machine trans-
lation service is conducted separately. For 
simplicity, we consider the English reviews trans-
lated by different machine translation services as 
reviews in different languages, despite the fact that 
in essence, they are still in English. 
 
Figure 1. Framework of our approach 
Formally, give a review rev0 in the target lan-
guage (i.e. Chinese), the corresponding review revi  
in the ith language is obtained by using a transla-
tion function:  revi =f iTrans(rev0)? where 1?i?p and  
p is the total number of machine translation ser-
vices. For each review revk in the kth language 
(0?k?p), we employ the semantic oriented ap-
proach to assign a semantic orientation value          
f kSO(revk) to the review, and the polarity orientation 
of the review can be simply predicated based on 
the value by using a threshold. Given a set of se-
mantic orientation values FSO={f kSO(revk) | 0?k?p}, 
the ensemble methods aim to derive a new seman-
tic orientation value )( 0revf EnsembleSO based on the 
values in FSO, which can be used to better classify 
the review as positive or negative.  
The steps of review translation, individual se-
mantic orientation value computation and ensem-
ble combination are described in details in the next 
sections, respectively.  
3.2 Review Translation 
Translation of a Chinese review into an English 
review is the first step of the proposed approach. 
Manual translation is time-consuming and labor-
intensive, and it is not feasible to manually trans-
late a large amount of Chinese product reviews in 
real applications. Fortunately, machine translation 
techniques have been well developed in the NLP 
field, though the translation performance is far 
from satisfactory. A few commercial machine 
translation services can be publicly accessed.  In 
this study, the following two commercial machine 
translation services and one baseline system are 
used to translate Chinese reviews into English re-
views.  
Google Translate 2  (GoogleTrans): Google 
Translate is one of the state-of-the-art commercial 
machine translation systems used today. Google 
Translate applies statistical learning techniques to 
build a translation model based on both monolin-
gual text in the target language and aligned text 
consisting of examples of human translations be-
tween the languages.  
Yahoo Babel Fish 3  (YahooTrans): Different 
from Google Translate, Yaho Babel Fish uses 
SYSTRAN?s rule-based translation engine. 
SYSTRAN was one of the earliest developers of 
machine translation software. SYSTRAN applies 
complex sets of specific rules defined by linguists 
to analyze and then transfer the grammatical struc-
ture of the source language into the target language.  
Baseline Translate (DictTrans): We simply de-
velop a translation method based only on one-to-
one term translation in a large Chinese-to-English 
                                                          
2 http://translate.google.com/translate_t 
3 http://babelfish.yahoo.com/translate_txt 
Chinese 
review 
Chinese  
Resource 
English 
review 
Machine 
translation 
Chinese 
sentiment  
analysis 
Ensemble 
English 
sentiment  
analysis 
English  
Resource 
Polarity 
Value 
Polarity 
Value 
Pos\Neg 
555
dictionary. Each term in a Chinese review is trans-
lated by the first corresponding term in the Chi-
nese-to-English dictionary, without any other 
processing steps. In this study, we use the 
LDC_CE_DIC2.04 constructed by LDC as the dic-
tionary for translation, which contains 128366 
Chinese terms and their corresponding English 
terms.  
The Chinese-to-English translation perform-
ances of the two commercial systems are deemed 
much better than the weak baseline system. Google 
Translate has achieved very good results on the 
Chinese-to-English translation tracks of NIST open 
machine translation test (MT)5 and it ranks the first 
on most tracks. In the Chinese-to-English task of 
MT2005, the BLEU-4 score of Google Translate is 
0.3531, and the BLEU-4 score of SYSTRAN is 
0.1471. We can deduce that Google Translate is 
better than Yahoo Babel Fish, without considering 
the recent improvements of the two systems.  
Here are two running example of Chinese re-
views and the translated English reviews (Human-
Trans refers to human translation): 
Positive Example: ????,?????? 
HumanTrans: Many advantages and very good shape.  
GoogleTrans: Many advantages, the shape is also very 
good. 
YahooTrans: Merit very many, the contour very is also 
good. 
DictTrans: merit very many figure also very good 
Negative example: ?????????? 
HumanTrans: The memory is too small to support IR. 
GoogleTrans: Memory is too small not to support IR. 
YahooTrans:The memory too is small does not support 
infrared. 
DictTrans: memory highest small negative not to be in 
favor of ir. 
3.3 Individual Semantic Orientation Value 
Computation 
For any specific language, we employ the semantic 
orientated approach (Kennedy and Inkpen, 2006) 
to compute the semantic orientation value of a re-
view. The unsupervised approach is quite  straight-
forward and it makes use of the following 
sentiment lexicons: positive Lexicon (Posi-
tive_Dic) including terms expressing positive po-
larity, Negative Lexicon (Negative_Dic) including 
terms expressing negative polarity, Negation 
                                                          
4 http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 
5 http://www.nist.gov/speech/tests/mt/ 
Lexicon (Negation_Dic) including terms that are 
used to reverse the semantic polarity of a particular 
term, and Intensifier Lexicon (Intensifier_Dic) 
including terms that are used to change the degree 
to which a term is positive or negative. In this 
study, we conduct our experiments within two lan-
guages, and we collect and use the following popu-
lar and available Chinese and English sentiment 
lexicons6, without any further filtering and labeling: 
1) Chinese lexicons 
Positive_Diccn: 3730 Chinese positive terms 
were collected from the Chinese Vocabulary for 
Sentiment Analysis (VSA)7 released by HOWNET.  
Negative_Diccn: 3116 Chinese negative terms 
were collected from Chinese Vocabulary for Sen-
timent Analysis (VSA) released by HOWNET. 
Negation_Diccn: 13 negation terms were col-
lected from related papers.  
Intensifier_Diccn: 148 intensifier terms were 
collected from Chinese Vocabulary for Sentiment 
Analysis (VSA) released by HOWNET. 
2) English lexicons  
Positive_Dicen: 2718 English positive terms 
were collected from the feature file subjclueslen1-
HLTEMNLP05.tff 8  containing the subjectivity 
clues used in the work (Wilson et al, 2005a; Wil-
son et al, 2005b). The clues in this file were col-
lected from a number of sources. Some were culled 
from manually developed resources, e.g. general 
inquirer9 (Stone et al, 1966). Others were identi-
fied automatically using both annotated and unan-
notated data. A majority of the clues were 
collected as part of work reported in Riloff and 
Wiebe (2003). 
Negative_Dicen: 4910 English negative terms 
were collected from the same file described above.  
Negation_Dicen: 88 negation terms were col-
lected from the feature file valenceshifters.tff used 
in the work (Wilson et al, 2005a; Wilson et al, 
2005b). 
Intensifier_Dicen: 244 intensifier terms were 
collected from the feature file intensifiers2.tff used 
in the work (Wilson et al, 2005a; Wilson et al, 
2005b). 
                                                          
6 In this study, we focus on using a few popular resources in 
both Chinese and English for comparative study, instead of 
trying to collect and use all available resources. 
7 http://www.keenage.com/html/e_index.html 
8 http://www.cs.pitt.edu/mpqa/ 
9 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
556
The semantic orientation value f kSO(revk) for revk 
is computed by summing the polarity values of all 
words in the review, making use of both the word 
polarity defined in the positive and negative lexi-
cons and the contextual valence shifters defined in 
the negation and intensifier lexicons. The algo-
rithm is illustrated in Figure 2. 
Input: a review revk in the kth language. Four lexi-
cons in the kth language: Positive_Dick, Nega-
tive_Dick, Negation_Dick, Intensifier_Dick, which are 
either Chinese or English lexicons; 
Output: Polarity Value f kSO(revk); 
Algorithm Compute_SO: 
1. Tokenize review revk into sentence set S and each 
sentence s?S  is tokenized into word set Ws;  
2. For any word w in a sentence s?S, compute its 
SO value SO(w) as follows: 
1) if w?Positive_Dick , SO(w)=PosValue; 
2) If w?Negative_Dick, SO(w)=NegValue; 
3) Otherwise, SO(w)=0; 
4) Within the window of q words previous to 
w, if there is a term w??Negation_Dick, 
SO(w)= ?SO(w); 
5) Within the window of q words previous to 
w, if there is a term w??Intensifier_Dick, 
SO(w) =??SO(w); 
3. ? ?
? ?
=
Ss Ww
kk
SO
s
wSOrevf )()( ; 
Figure 2. The algorithm for semantic orientation value 
computation 
In the above algorithm, PosValue and Neg-
Value are the polarity values for positive words 
and negative words respectively. We empirically 
set PosValue=1 and NegValue= ?2 because nega-
tive words usually contribute more to the overall 
semantic orientation of the review than positive 
words, according to our empirical analysis. ?>1 
aims to intensify the polarity value and we simply 
set ?=2. q is the parameter controlling the window 
size within which the negation terms and intensi-
fier terms have influence on the polarity words and  
here q is set to 2 words. Note that the above pa-
rameters are tuned only for Chinese sentiment 
analysis, and they are used for sentiment analysis 
in the English language without further tuning. The 
tokenization of Chinese reviews involves Chinese 
word segmentation. 
Usually, if the semantic orientation value of a 
review is less than 0, the review is labeled as nega-
tive, otherwise, the review is labeled as positive.  
3.4 Ensemble Combination 
After obtaining the set of semantic orientation val-
ues FSO={f kSO(revk) | 0?k?p} by using the semantic 
oriented approach, where p is the number of Eng-
lish translations for each Chinese review, we ex-
ploit the following ensemble methods for deriving 
a new semantic orientation value )( 0revf EnsembleSO : 
1) Average 
It is the most intuitive combination method and 
the new value is the average of the values in FSO:  
1
)(
)( 00
+
=
?
=
p
revf
revf
p
k
kk
SO
Ensemble
SO
 
Note that after the new value of a review is ob-
tained, the polarity tag of the review is assigned in 
the same way as described in Section 3.3. 
2) Weighted Average 
This combination method improves the average 
combination method by associating each individual 
value with a weight, indicating the relative confi-
dence in the value. 
?
=
=
p
k
kk
SOk
Ensemble
SO revfrevf
0
0 )()( ?  
where ?k?[0, 1] is the weight associated with         
f kSO(revk). The weights can be set in the following 
two ways: 
Weighting Scheme1: The weight of f kSO(revk) is 
set to the accuracy of the individual analysis in the 
kth language.  
Weighting Scheme2: The weight of f kSO(revk) is 
set to be the maximal correlation coefficient be-
tween the analysis results in the kth language and 
the analysis results in any other language.  The 
underlying idea is that if the analysis results in one 
language are highly consistent with the analysis 
results in another language, the results are deemed 
to be more reliable. Given two lists of semantic 
values for all reviews, we use the Pearson?s corre-
lation coefficient to measure the correlation be-
tween them. The weight associated with function f 
k
SO(revk) is then defined as the maximal Pearson?s 
correlation coefficient between the reviews? values 
in the kth language and the reviews? values in any 
other language.   
3) Max 
557
The new value is the maximum value in FSO:  { }pkrevfrevf kkSOEnsembleSO ??= 0|)(max)( 0  
4) Min 
The new value is the minimum value in FSO:  { }pkrevfrevf kkSOEnsembleSO ??= 0|)(min)( 0  
5) Average Max&Min 
The new value is the average of the maximum 
value and the minimum value in FSO:  { } { }
2
0|)(min0|)(max)( 0 pkrevfpkrevfrevf
kk
SO
kk
SOEnsemble
SO
??+??
=
 
6) Majority Voting 
This combination method relies on the final po-
larity tags, instead of the semantic orientation val-
ues. A review can obtain p+1 polarity tags based 
on the individual analysis results in the p+1 lan-
guages. The polarity tag receiving more votes is 
chosen as the final polarity tag of the review. 
4 Empirical Evaluation  
4.1 Dataset and Evaluation Metrics 
In order to assess the performance of the proposed 
approach, we collected 1000 product reviews from 
a popular Chinese IT product web site-IT16810 . 
The reviews were posted by users and they focused 
on such products as mp3 players, mobile phones, 
digital camera and laptop computers. Users usually 
selected for each review an icon indicating ?pos-
tive? or ?negative?. The reviews were first catego-
rized into positive and negative classes according 
to the associated icon. The polarity labels for the 
reviews were then checked by subjects. Finally, the 
dataset contained 886 product reviews with accu-
rate polarity labels. All the 886 reviews were used 
as test set.  
We used the standard precision, recall and F-
measure to measure the performance of positive 
and negative class, respectively, and used the Mac-
roF measure and accuracy metric to measure the 
overall performance of the system. The metrics are 
defined the same as in general text categorization. 
4.2 Individual Analysis Results 
In this section, we investigate the following indi-
vidual sentiment analysis results in each specified 
language: 
CN: This method uses only Chinese lexicons 
to analyze Chinese reviews; 
                                                          
10 http://www.it168.com 
GoogleEN: This method uses only English lex-
icons to analyze English reviews translated by 
GoogleTrans; 
YahooEN: This method uses only English lex-
icons to analyze English reviews translated by Ya-
hooTrans; 
DictEN: This method uses only English lexi-
cons to analyze English reviews translated by 
DictTrans;  
In addition to the above methods for using 
English resources, the lexicon-based method inves-
tigated in Mihalcea et al (2007) can also use Eng-
lish resources by directly projecting English 
lexicons into Chinese lexicons. We use a large 
English-to-Chinese dictionary - 
LDC_EC_DIC2.011  with 110834 entries for pro-
jecting English lexicons into Chinese lexicons via 
one-to-one translation. Based on the generated 
Chinese lexicons, two other individual methods are 
investigated in the experiments:  
CN2: This method uses only the generated 
Chinese Resources to analyze Chinese reviews. 
CN3: This method combines the original Chi-
nese lexicons and the generated Chinese lexicons 
and uses the extended lexicons to analyze Chinese 
reviews. 
Table 1 provides the performance values of all 
the above individual methods. Seen from the table, 
the performances of GoogleEN and YahooEN are 
much better than the baseline CN method, and 
even the DictEN performs as well as CN. The re-
sults demonstrate that the use of English resources 
for sentiment analysis of translated English re-
views is an effective way for Chinese sentiment 
analysis. We can also see that the English senti-
ment analysis performance relies positively on the 
translation performance, and GoogleEN performs 
the best while DictEN performs the worst, which 
is consistent with the fact the GoogleTrans is 
deemed the best of the three machine translation 
systems, while DictTrans is the weakest one.  
Furthermore, the CN method outperforms the 
CN2 and CN3 methods, and the CN2 method per-
forms the worst, which shows that the generated 
Chinese lexicons do not give any contributions to 
the performance of Chinese sentiment analysis. We 
explain the results by the fact that the term-based 
one-to-one translation is inaccurate and the gener-
ated Chinese lexicons are not reliable. Overall, the 
                                                          
11 http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 
558
approach through cross-lingual lexicon translation 
does not work well for Chinese sentiment analysis 
in our experiments. 
4.3 Ensemble Results 
In this section, we first use the simple average en-
semble method to combine different individual 
analysis results. Table 2 provides the performance 
values of the average ensemble results based on 
different individual methods. 
Seen from Tables 1 and 2, almost all of the av-
erage ensembles outperforms the baseline CN 
method and the corresponding individual methods, 
which shows that each individual methods have 
their own evidences for sentiment analysis, and 
thus fusing the evidences together can improve 
performance. For the methods of CN+GoogleEN, 
CN+YahooEN and CN+DictEN, we can see the 
ensemble performance is not positively relying on 
the translation performance: CN+YahooEN per-
forms better than CN+GoogleEN, and even 
CN+DictEN performs as well as CN+GoogleEN. 
The results show that the individual methods in the 
ensembles can complement each other, and even 
the combination of two weak individual methods 
can achieve good performance. However, the Dic-
tEN method is not effective when the ensemble 
methods have already included GoogleEN and 
YahooEN. Overall, the performances of the en-
semble methods rely on the performances of the 
most effective constituent individual methods: the 
methods including both GoogleEN and YahooEN 
perform much better than other methods, and 
CN+GoogleEN+YahooEN performs the best out 
of all the methods.  
  We further show the results of four typical av-
erage ensembles by varying the combination 
weights. The combination weights are respectively 
specified as ??CN+(1-?)?GoogleEN, ??CN+(1-
?)?YahooEN, ??CN+(1-?)?DictEN, 
?1?CN+?2?GoogleEN+(1-?1-?2)?YahooEN.  The results 
over the MacroF metric are shown in Figures 3 and 
4 respectively. We can see from the figures that 
GoogleEN and YahooEN are dominant factors in 
the ensemble methods.  
We then investigate to use other ensemble meth-
ods introduced in Section 3.4 to combine the CN, 
GoogleEN and YahooEN methods. Table 3 gives 
the comparison results. The methods of ?Weighted 
Average1? and ?Weighted Average2? are two 
weighted average ensembles using the two weigh-
ing schemes, respectively. We can see that all the 
ensemble methods outperform the constituent indi-
vidual method, while the two weighted average 
ensembles perform the best. The results further 
demonstrate the good effectiveness of the ensem-
ble combination of individual analysis results for 
Chinese sentiment analysis.  
 
Positive Negative Total Individual Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy
CN 0.681 0.929 0.786 0.882 0.549 0.677 0.732 0.743 
CN2 0.615 0.772 0.684 0.678 0.499 0.575 0.630 0.638 
CN3 0.702 0.836 0.763 0.788 0.632 0.702 0.732 0.736 
GoogleEN 0.764 0.914 0.832 0.888 0.708 0.787 0.810 0.813 
YahooEN 0.763 0.871 0.814 0.844 0.720 0.777 0.795 0.797 
DictEN 0.738 0.761 0.749 0.743 0.720 0.731 0.740 0.740 
Table 1. Individual analysis results 
Positive Negative Total Average Ensemble Precision Recall F-measure Precision Recall F-measure MacroF Accuracy
GoogleEN+YahooEN 0.820 0.900 0.858 0.885 0.795 0.838 0.848 0.848 
GoogleEN+YahooEN 
+DictEN 0.841 0.845 0.843 0.838 0.834 0.836 0.840 0.840 
CN+GoogleEN 0.754 0.949 0.840 0.928 0.678 0.784 0.812 0.816 
CN+YahooEN 0.784 0.925 0.848 0.904 0.736 0.811 0.830 0.832 
CN+DictEN 0.790 0.867 0.827 0.847 0.761 0.801 0.814 0.815 
CN+GoogleEN 
+YahooEN 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 
CN+GoogleEN+ 
YahooEN+DictEN 0.831 0.891 0.860 0.878 0.811 0.843 0.852 0.852 
Table 2. Average combination results 
559
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
M
ac
ro
F
CN+GoogleEN CN+YahooEN CN+DictEN
 
0
0.3
0.6
0.9
00.2
0.40.6
0.81
0.65
0.7
0.75
0.8
0.85
0.9
MacroF
?1
?2
0.85-0.9
0.8-0.85
0.75-0.8
0.7-0.75
0.65-0.7
 
Figure 3. Ensemble performance vs. weight ? for 
??CN+(1-?)?GoogleEN/YahooEN/DictEN 
Figure 4. Ensemble performance vs. weights ?1 and ?2 for 
?1?CN+?2?GoogleEN+(1-?1-?2) ?YahooEN 
 
Positive Negative Total Ensemble Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy
Average 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 
Weighted Average1 0.825 0.922 0.871 0.908 0.798 0.849 0.860 0.861 
Weighted Average2 0.822 0.922 0.869 0.908 0.793 0.847 0.858 0.859 
Max 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 
Min 0.901 0.787 0.840 0.805 0.910 0.854 0.847 0.848 
Average Max&Min 0.793 0.936 0.859 0.918 0.747 0.824 0.841 0.843 
Majority Voting 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 
Table 3. Ensemble results for CN & GoogleEN & YahooEN 
5 Conclusion and Future Work  
This paper proposes a novel approach to use Eng-
lish sentiment resources for Chinese sentiment 
analysis by employing machine translation and 
ensemble techniques. Chinese reviews are trans-
lated into English reviews and the analysis results 
of both Chinese reviews and English reviews are 
combined to improve the overall accuracy. Ex-
perimental results demonstrate the encouraging 
performance of the proposed approach.  
In future work, more additional English re-
sources will be used to further improve the results. 
We will also apply the idea to supervised Chinese 
sentiment analysis. 
Acknowledgments 
This work was supported by the National Science 
Foundation of China (No.60703064), the Research 
Fund for the Doctoral Program of Higher Educa-
tion of China (No.20070001059) and the National 
High Technology Research and Development Pro-
gram of China (No.2008AA01Z421). We also 
thank the anonymous reviewers for their useful 
comments.  
References  
A. Andreevskaia and S. Bergler. 2008. When specialists 
and generalists work together: overcoming domain 
dependence in sentiment tagging. In Proceedings of 
ACL-08: HLT. 
J. Blitzer, M. Dredze and F. Pereira. 2007. Biographies, 
bollywood, boom-boxes and blenders: domain adap-
tation for sentiment classification. In Proceedings of 
ACL2007. 
Y. Choi, E. Breck, and C. Cardie. 2006. Joint extraction 
of entities and relations for opinion recognition. In 
Proc. EMNLP. 
A. Devitt and K. Ahmad. 2007. Sentiment polarity iden-
tification in financial news: a cohesion-based ap-
proach. In Proceedings of ACL2007. 
K. Eguchi and V. Lavrenko. 2006. Sentiment retrieval 
using generative models. In Proceedings of EMNLP. 
V. Hatzivassiloglou and K. R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of EACL.  
V. Hatzivassiloglon and J. M. Wiebe. 2000. Effects of 
adjective orientation and gradability on sentence sub-
jectivity. In Proceedings of COLING. 
K. Hiroshi, N. Tetsuya and W. Hideo. 2004. Deeper 
sentiment analysis using machine translation tech-
nology. In Proceedings of COLING. 
560
N. Kaji and M. Kitsuregawa. 2007. Building lexicon for 
sentiment analysis from massive collection of HTML 
documents. In Proceedings of EMNLP-CONLL. 
A. Kennedy and D. Inkpen. 2006. Sentiment classifica-
tion of movie reviews using contextual valence shift-
ers. Computational Intelligence, 22(2):110-125. 
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of COLING. 
L.-W. Ku, Y.-T. Liang and H.-H. Chen. 2006. Opinion 
extraction, summarization and tracking in news and 
blog corpora. In Proceedings of AAAI. 
J. Li and M. Sun. 2007. Experimental study on senti-
ment classification of Chinese review using machine 
learning techniques. In Proceeding of IEEE-
NLPKE2007. 
B. Liu, M. Hu and J. Cheng. 2005. Opinion observer: 
Analyzing and comparing opinions on the web. In 
Proceedings of WWW. 
R. McDonald, K. Hannan, T. Neylon, M. Wells and J. 
Reynar. 2007. Structured models for fine-to-coarse 
sentiment analysis. In Proceedings of ACL2007. 
R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning 
multilingual subjective language via cross-lingual 
projections. In Proceedings of ACL. 
T. Mullen and N. Collier. 2004. Sentiment analysis us-
ing support vector machines with diverse information 
sources. In Proceedings of EMNLP. 
B. Pang, L. Lee and S. Vaithyanathan. 2002. Thumbs up? 
sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP. 
B. Pang and L. Lee. 2004. A sentimental education: 
sentiment analysis using subjectivity summarization 
based on minimum cuts. In Proceedings of ACL. 
A. ?M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of EMNLP. 
J. Read. 2005. Using emoticons to reduce dependency 
in machine learning techniques for sentiment classi-
fication. In Proceedings of ACL. 
E. Riloff and J. Wiebe 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of 
EMNLP2003. 
P. J. Stone, D. C. Dunphy, M. S. Smith, D. M. Ogilvie  
and associates. 1966. The General Inquirer: a com-
puter approach to content analysis. The MIT Press. 
H. Takamura, T. Inui and M. Okumura. 2005. Extract-
ing semantic orientation of words using spin model. 
In Proceedings of ACL. 
I. Titov and R. McDonald. 2008. A joint model of text 
and aspect ratings for sentiment summarization. In 
Proceedings of ACL-08: HLT. 
B. K. Y. Tsou, R. W. M. Yuen, O. Y. Kwong, T. B. Y. 
La and W. L. Wong. 2005. Polarity classification of 
celebrity coverage in the Chinese press. In Proceed-
ings of International Conference on Intelligence 
Analysis.  
P. Turney. 2002. Thumbs up or thumbs down? semantic 
orientation applied to unsupervised classification of 
reviews. In Proceedings of ACL. 
S. Wang, Y. Wei, D. Li, W. Zhang and W. Li. 2007. A 
hybrid method of feature selection for Chinese text 
sentiment classification. In Proceeding of IEEE-
FSKD2007. 
T. Wilson, J. Wiebe and P. Hoffmann. 2005a. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment 
Analysis. In Proceedings of HLT/EMNLP2005, Van-
couver, Canada. 
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. 
Wiebe, Y. Choi, C. Cardie, E. Riloff, S. Patwardhan. 
2005b. OpinionFinder: a system for subjectivity 
analysis. In Proceedings of HLP/EMNLP on Interac-
tive Demonstrations.  
J. Yao, G. Wu, J. Liu and Y. Zheng. 2006. Using bilin-
gual lexicon to judge sentiment orientation of Chi-
nese words. In Proceedings of IEEE CIT2006. 
Q. Ye, W. Shi and Y. Li. 2006. Sentiment classification 
for movie reviews in Chinese by improved semantic 
oriented approach. In Proceedings of 39th Hawaii In-
ternational Conference on System Sciences.  
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: separating facts from opinions 
and identifying the polarity of opinion sentences. In 
Proceedings of EMNLP2003. 
561
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 755?762,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
An Exploration of Document Impact on Graph-Based Multi-Document 
Summarization 
 
Xiaojun Wan 
Institute of Compute Science and Technology 
Peking University 
Beijing 100871, China 
wanxiaojun@icst.pku.edu.cn 
 
Abstract 
The graph-based ranking algorithm has been 
recently exploited for multi-document sum-
marization by making only use of the sen-
tence-to-sentence relationships in the 
documents, under the assumption that all the 
sentences are indistinguishable. However, 
given a document set to be summarized, dif-
ferent documents are usually not equally im-
portant, and moreover, different sentences in a 
specific document are usually differently im-
portant. This paper aims to explore document 
impact on summarization performance. We 
propose a document-based graph model to in-
corporate the document-level information and 
the sentence-to-document relationship into the 
graph-based ranking process. Various meth-
ods are employed to evaluate the two factors.  
Experimental results on the DUC2001 and 
DUC2002 datasets demonstrate that the good 
effectiveness of the proposed model. More-
over, the results show the robustness of the 
proposed model. 
1 Introduction 
Multi-document summarization aims to produce a 
summary describing the main topic in a document 
set, without any prior knowledge. Multi-document 
summary can be used to facilitate users to quickly 
understand a document cluster. For example, a 
number of news services (e.g. NewsInEssence1) 
have been developed to group news articles into 
news topics, and then produce a short summary for 
each news topic. Users can easily understand the 
topic they have interest in by taking a look at the 
short summary, without looking into each individ-
ual article within the topic cluster. 
                                                          
1http://lada.si.umich.edu:8080/clair/nie1/nie.cgi 
Automated multi-document summarization has 
drawn much attention in recent years. In the com-
munities of natural language processing and infor-
mation retrieval, a series of workshops and 
conferences on automatic text summarization (e.g. 
NTCIR, DUC), special topic sessions in ACL, 
COLING, and SIGIR have advanced the summari-
zation techniques and produced a couple of ex-
perimental online systems. 
A particular challenge for multi-document sum-
marization is that a document set might contain 
diverse information, which is either related or un-
related to the main topic, and hence we need effec-
tive summarization methods to analyze the 
information stored in different documents and ex-
tract the globally important information to reflect 
the main topic. In recent years, both unsupervised 
and supervised methods have been proposed to 
analyze the information contained in a document 
set and extract highly salient sentences into the 
summary, based on syntactic or statistical features. 
Most recently, the graph-based models have 
been successfully applied for multi-document 
summarization by making use of the ?voting? or 
?recommendations? between sentences in the 
documents (Erkan and Radev, 2004; Mihalcea and 
Tarau, 2005; Wan and Yang, 2006). The model 
first constructs a directed or undirected graph to 
reflect the relationships between the sentences and 
then applies the graph-based ranking algorithm to 
compute the rank scores for the sentences. The 
sentences with large rank scores are chosen into 
the summary.  However, the model makes uniform 
use of the sentences in different documents, i.e. all 
the sentences are ranked without considering the 
document-level information and the sentence-to-
document relationship. Actually, given a document 
set, different documents are not equally important. 
For example, the documents close to the main top-
ics of the document set are usually more important 
than the documents far away from the main topics 
755
of the document set. This document-level informa-
tion is deemed to have great impact on the sen-
tence ranking process. Moreover, the sentences in 
the same document cannot be treated uniformly, 
because some sentences in the document are more 
important than other sentences because of their 
different positions in the document or different 
distances to the document?s centroid. In brief, nei-
ther the document-level information nor the sen-
tence-to-document relationship has been taken into 
account in the previous graph-based model. 
In order to overcome the limitations of the pre-
vious graph-based model, this study proposes the 
document-based graph model to explore document 
impact on the graph-based summarization, by in-
corporating both the document-level information 
and the sentence-to-document relationship in the 
graph-based ranking process. We develop various 
methods to evaluate the document-level informa-
tion and the sentence-to-document relationship. 
Experiments on the DUC2001 and DUC2002 data-
sets have been performed and the results demon-
strate the good effectiveness of the proposed model, 
i.e., the incorporation of document impact can 
much improve the performance of the graph-based 
summarization. Moreover, the proposed model is 
robust with respect to most incorporation schemes. 
The rest of this paper is organized as follows. 
We first introduce the related work in Section 2. 
The basic graph-based summarization model and 
the proposed document-based graph model are de-
scribed in detail in Sections 3 and 4, respectively. 
We show the experiments and results in Section 5 
and finally we conclude this paper in Section 6. 
2 Related Work 
Generally speaking, summarization methods can 
be abstractive summarization or extractive summa-
rization. Extractive summarization is a simple but 
robust method for text summarization and it in-
volves assigning saliency scores to some units (e.g. 
sentences, paragraphs) of the documents and ex-
tracting those with highest scores, while abstrac-
tion summarization usually needs information 
fusion (Barzilay et al, 1999), sentence compres-
sion (Knight and  Marcu, 2002) and reformulation 
(McKeown et al, 1999). In this study, we focus on 
extractive summarization.  
The centroid-based method (Radev et al, 2004) 
is one of the most popular extractive summariza-
tion methods. MEAD2 is an implementation of the 
centroid-based method that scores sentences based 
on sentence-level and inter-sentence features, in-
cluding cluster centroids, position, TFIDF, etc. 
NeATS (Lin and Hovy, 2002) is a project on multi-
document summarization at ISI based on the sin-
gle-document summarizer-SUMMARIST. Sen-
tence position, term frequency, topic signature and 
term clustering are used to select important content. 
MMR (Goldstein et al, 1999) is used to remove 
redundancy and stigma word filters and time 
stamps are used to improve cohesion and coher-
ence. To further explore user interface issues, 
iNeATS (Leuski et al, 2003) is developed based 
on NeATS. XDoX (Hardy et al, 1998) is a cross 
document summarizer designed specifically to 
summarize large document sets. It identifies the 
most salient themes within the set by passage clus-
tering and then composes an extraction summary, 
which reflects these main themes. Much other 
work also explores to find topic themes in the 
documents for summarization, e.g. Harabagiu and 
Lacatusu (2005) investigate five different topic 
representations and introduce a novel representa-
tion of topics based on topic themes. In addition, 
Marcu (2001) selects important sentences based on 
the discourse structure of the text. TNO?s system 
(Kraaij et al, 2001) scores sentences by combining 
a unigram language model approach with a Bayes-
ian classifier based on surface features. Nenkova 
and Louis (2008) investigate how summary length 
and the characteristics of the input influence the 
summary quality in multi-document summarization.  
Graph-based models have been proposed to rank 
sentences or passages based on the PageRank algo-
rithm (Page et al, 1998) or its variants. Websumm 
(Mani and Bloedorn, 2000) uses a graph-
connectivity model and operates under the assump-
tion that nodes which are connected to many other 
nodes are likely to carry salient information. Lex-
PageRank (Erkan and Radev, 2004) is an approach 
for computing sentence importance based on the 
concept of eigenvector centrality. It constructs a 
sentence connectivity matrix and compute sentence 
importance based on an algorithm similar to Pag-
eRank. Mihalcea and Tarau (2005) also propose a 
similar algorithm based on PageRank to compute 
sentence importance for document summarization. 
Wan and Yang (2006) improve the ranking algo-
                                                          
2 http://www.summarization.com/mead/ 
756
rithm by differentiating intra-document links and 
inter-document links between sentences. All these 
methods make use of the relationships between 
sentences and select sentences according to the 
?votes? or ?recommendations? from their 
neighboring sentences, which is similar to PageR-
ank. 
Other related work includes topic-focused multi-
document summarization (Daum?. and Marcu, 
2006; Gupta et al, 2007; Wan et al, 2007), which 
aims to produce summary biased to a given topic 
or query. It is noteworthy that our proposed ap-
proach is inspired by (Liu and Ma, 2005), which 
proposes the Conditional Markov Random Walk 
Model based on two-layer web graph in the tasks 
of web page retrieval. 
3 The Basic Graph-Based Model (GM) 
The basic graph-based model is essentially a way 
of deciding the importance of a vertex within a 
graph based on global information recursively 
drawn from the entire graph. The basic idea is that 
of ?voting? or ?recommendation? between the ver-
tices. A link between two vertices is considered as 
a vote cast from one vertex to the other vertex. The 
score associated with a vertex is determined by the 
votes that are cast for it, and the score of the verti-
ces casting these votes.  
 
Figure 1. One-layer link graph 
Formally, given a document set D, let G=(V, E) be 
an undirected graph to reflect the relationships be-
tween sentences in the document set, as shown in 
Figure 1. V is the set of vertices and each vertex vi 
in V is a sentence in the document set. E is the set 
of edges. Each edge eij in E is associated with an 
affinity weight f(vi, vj) between sentences vi and vj 
(i?j). The weight is computed using the standard 
cosine measure between the two sentences.  
ji
ji
jiineji vv
vv
vvsimvvf rr
rr
?
?
== ),(),( cos  
 
(1) 
where iv
r  and jv
r are the corresponding term vec-
tors of vi and vj.  Here, we have f(vi, vj)=f(vj, vi). 
Two vertices are connected if their affinity weight 
is larger than 0 and we let f(vi, vi)=0 to avoid self 
transition. 
We use an affinity matrix M to describe G with 
each entry corresponding to the weight of an edge 
in the graph. M = (Mi,j)|V|?|V| is defined as follows: 
 
otherwise0
;  and                     
 connected is  and if),(
??
??
?
?=
,   
ji
v v,   vvf
M
jiji
i,j
 
(2) 
Then M is normalized to M~ as follows to make 
the sum of each row equal to 1: 
??
??
?
?
=
??
==
otherwise0
0if~
|V|
1
|V|
1
   ,             
M ,   MM
M j
i,j
j
i,ji,j
i,j
 
 
(3) 
Based on matrix M~ , the saliency score Sen-
Score(vi) for sentence vi can be deduced from those 
of all other sentences linked with it and it can be 
formulated in a recursive form as in the PageRank 
algorithm: 
?
?
?
+??=
iall j
j,iji V
MvSenScorevSenScore
||
)1(~)()( ??
 
(4) 
And the matrix form is: 
e
V
?M? T r
rr
||
)1(~ ?? ?+=   (5) 
where 1||)]([ ?= VivSenScore?
r
is the vector of sen-
tence saliency scores. er  is a vector with all ele-
ments equaling to 1. ? is the damping factor 
usually set to 0.85, as in the PageRank algorithm. 
The above process can be considered as a 
Markov chain by taking the sentences as the states 
and the corresponding transition matrix is given 
by TT ee
|V|
MA rr)1(~ ?? ?+= . The stationary prob-
ability distribution of each state is obtained by the 
principal eigenvector of the transition matrix.  
For implementation, the initial scores of all sen-
tences are set to 1 and the iteration algorithm in 
Equation (4) is adopted to compute the new scores 
of the sentences. Usually the convergence of the 
iteration algorithm is achieved when the difference 
between the scores computed at two successive 
iterations for any sentences falls below a given 
threshold (0.0001 in this study).  
We can see that the basic graph-based model is 
built on the single-layer sentence graph and the 
transition probability between two sentences in the 
Markov chain depends only on the sentences them-
selves, not taking into account the document-level 
information and the sentence-to-document rela-
tionship.  
E 
Sentences 
757
4 The Document-Based Graph Model 
(DGM) 
4.1 Overview 
As we mentioned in previous section, there may be 
many factors that can have impact on the impor-
tance analysis of the sentences. This study aims to 
examine the document impact by incorporating the 
document importance and the sentence-to-
document correlation into the sentence ranking 
process. Our assumption is that the sentences, whi-
ch belong to an important document and are highly 
correlated with the document, will be more likely 
to be chosen into the summary.  
In order to incorporate the document-level in-
formation and the sentence-to-document relation-
ship, the document-based graph model is proposed 
based on the two-layer link graph including both 
sentences and documents. The novel representation 
is shown in Figure 2. As can be seen, the lower 
layer is just the traditional link graph between sen-
tences that has been well studied in previous work. 
And the upper layer represents the documents. The 
dashed lines between these two layers indicate the 
conditional influence between the sentences and 
the documents.  
 
Figure 2. Two-layer link graph 
Formally, the new representation for the two-
layer graph is denoted as G*=<Vs, Vd, Ess, Esd>, 
where Vs=V={vi} is the set of sentences and 
Vd=D={dj} is the set of documents; Ess=E={eij|vi, 
vj?Vs} includes all possible links between sen-
tences and Esd={eij|vi?Vs, dj?Vd and dj=doc(vi)} 
includes the correlation link between any sentence 
and its belonging document.  Here, we use doc(vi) 
to denote the document containing sentence vi. For 
further discussions, we let ?(doc(vi)) ?[0,1] de-
note the importance of document doc(vi) in the 
document set, and let ?(vi, doc(vi)) ?[0,1] denote 
the strength of the correlation between sentence vi 
and its document doc(vi).   
The two factors are incorporated into the affinity 
weight between sentences and the new sentence-to-
sentence affinity weight is denoted as f(vi, vj|doc(vi), 
doc(vj)), which is conditioned on the two docu-
ments containing the two sentences. The new con-
ditional affinity weight is computed by linearly 
combining the affinity weight conditioned on the 
first document (i.e. f(vi,vj|doc(vi))) and the affinity 
weight conditioned on the second document (i.e. 
f(vi,vj|doc(vj))). 
Formally, the conditional affinity weight is 
computed as follows to incorporate the two factors: 
)))(,())(()1(         
))(,())(((),(      
)))(,())(()1(         
))(,())(((),(      
))(,())((),()1(         
))(,())((),(      
))(|,()1())(|,(      
))(),(|,(
cos
jjj
iiijiine
jjj
iiiji
jjjji
iiiji
jjiiji
jiji
vdocvvdoc
vdocvvdocvvsim
vdocvvdoc
vdocvvdocvvf
vdocvvdocvvf
vdocvvdocvvf
vdocvvfvdocvvf
vdocvdocvvf
???
???
???
???
???
???
??
???+
???=
???+
???=
????+
???=
??+?=
 
 
 
 
 
(6) 
where ??[0,1] is the combination weight control-
ling the relative contributions from the first docu-
ment and the second document. Note that usually 
f(vi, vj|doc(vi), doc(vj)) is not equal to f(vj, vi|doc(vj), 
doc(vi)), but the two scores are equal when ? is set 
to 0.5. Various methods can be used to evaluate the 
document importance and the sentence-document 
correlation, which will be described in next sec-
tions. 
The new affinity matrix M* is then constructed 
based on the above conditional sentence-to-
sentence affinity weight.  
 
otherwise0
  and                                         
connected is   and ifif))(),(|,(
*
??
??
?
?=
,   
ji
v v ,   vdocvdocvvf
M
jijiji
i,j
 
(7) 
Likewise, M* is normalized to *~M  and the itera-
tive computation as in Equation (4) is then based 
on *~M .  The transition matrix in the Markov chain 
is then denoted by TT ee
|V|
MA rr)1(~ ** ?? ?+=  and 
the sentence scores is obtained by the principle 
eigenvector of the new transition matrix A*. 
4.2 Evaluating Document Importance (?) 
The function ?(doc(vi)) aims to evaluate the impor-
tance of document doc(vi) in the document set D. 
The following three methods are developed to 
evaluate the document importance.  
Sentences 
Documents 
Esd 
Ess 
758
?1: It uses the cosine similarity value between 
the document and the whole document set as the 
importance score of the document3: 
)),(())(( cos1 Dvdocsimvdoc iinei =?  (8) 
?2: It uses the average similarity value between 
the document and any other document in the 
document set as the importance score of the docu-
ment: 
1||
)'),((
))(( )('  and  '
cos
2
?
=
?
??
D
dvdocsim
vdoc ivdocdDd
iine
i?  
 
(9) 
?3: It constructs a weighted graph between docu-
ments and uses the PageRank algorithm to com-
pute the rank scores of the documents as the 
importance scores of the documents. The link 
weight between two documents is computed using 
the cosine measure. The equation for iterative 
computation is the same with Equation (4). 
4.3 Evaluating Sentence-Document Cor-
relation (?) 
The function ?(vi, doc(vi)) aims to evaluate the 
correlation between sentence vi and its document 
doc(vi). The following four methods are developed 
to compute the strength of the correlation. The first 
three methods are based on sentence position in the 
document, under the assumption that the first sen-
tences in a document are usually more important 
than other sentences. The last method is based on 
the content similarity between the sentence and the 
document. 
?1: The correlation strength between sentence vi 
and its document doc(vi) is based on the position of 
the sentence as follows: 
??
? ?
=
Otherwise  5.0
3)( if  1
))(,(1
i
ii
vpos
vdocv?  
 
(10) 
where pos(vi) returns the position number of sen-
tence vi in its document. For example, if vi is the 
first sentence in its document, pos(vi) is 1.  
?2: The correlation strength between sentence vi 
and its document doc(vi) is based on the position of 
the sentence as follows: 
))((_
1)(
1))(,(2
i
i
ii vdoccountsen
vpos
vdocv
?
?=?  
 
(11) 
where sen_count(doc(vi)) returns the total number 
of sentences in document doc(vi). 
                                                          
3 A document set is treated as a single text by concatenating 
all the document texts in the set. 
?3: The correlation strength between sentence vi 
and its document doc(vi) is based on the position of 
the sentence as follows: 
1)(
15.0))(,(3 +
+=
i
ii vpos
vdocv?  
 
(12) 
?4: The correlation strength between sentence vi 
and its document doc(vi) is based on the cosine 
similarity between the sentence and the document: 
))(,())(,( cos4 iiineii vdocvsimvdocv =?  (13) 
5 Empirical Evaluation 
5.1 Dataset and Evaluation Metric 
Generic multi-document summarization has been 
one of the fundamental tasks in DUC 20014 and 
DUC 20025 (i.e. task 2 in DUC 2001 and task 2 in 
DUC 2002), and we used the two tasks for evalua-
tion. DUC2001 provided 30 document sets and 
DUC 2002 provided 59 document sets (D088 is 
excluded from the original 60 document sets by 
NIST) and generic abstracts of each document set 
with lengths of approximately 100 words or less 
were required to be created. The documents were 
news articles collected from TREC-9. The sen-
tences in each article have been separated and the 
sentence information has been stored into files.  
The summary of the two datasets are shown in Ta-
ble 1.  
 DUC 2001 DUC 2002
Task Task 2 Task 2 
Number of documents 309 567 
Number of clusters 30 59 
Data source TREC-9 TREC-9 
Summary length 100 words 100 words 
  Table 1. Summary of datasets  
We used the ROUGE (Lin and Hovy, 2003) 
toolkit (i.e. ROUGEeval-1.4.2 in this study) for 
evaluation, which has been widely adopted by 
DUC for automatic summarization evaluation. It 
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and 
word pairs between the candidate summary and the 
reference summary. ROUGE-N was an n-gram 
recall measure computed as follows: 
? ?
? ?
? ?
? ?
?
?
=?
Sum} {Ref
Sum} {Ref
)(
S Sn-gram
S Sn-gram
match
gram)Count(n
gramnCount
NROUGE
 
 
(14) 
                                                          
4 http://www-nlpir.nist.gov/projects/duc/guidelines/2001.html 
5 http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html 
759
where n stood for the length of the n-gram, and 
Countmatch(n-gram) was the maximum number of 
n-grams co-occurring in a candidate summary and 
a set of reference summaries. Count(n-gram) was 
the number of n-grams in the reference summaries. 
ROUGE toolkit reported separate scores for 1, 2, 
3 and 4-gram, and also for longest common subse-
quence co-occurrences. Among these different 
scores, unigram-based ROUGE score (ROUGE-1) 
has been shown to agree with human judgment 
most (Lin and Hovy. 2003). We showed three of 
the ROUGE metrics in the experimental results: 
ROUGE-1 (unigram-based), ROUGE-2 (bigram-
based), and ROUGE-W (based on weighted long-
est common subsequence, weight=1.2). In order to 
truncate summaries longer than length limit, we 
used the ?-l? option in ROUGE toolkit. We also 
used the ?-m? option for word stemming. 
5.2 Evaluation Results 
In the experiments, the combination weight ? for 
the proposed summarization model is typically set 
to 0.5 without tuning, i.e. the two documents for 
two sentences have equal influence on the summa-
rization process. Note that after the saliency scores 
of sentences have been obtained, a greedy algo-
rithm (Wan and Yang, 2006) is applied to remove 
redundancy and finally choose both informative 
and novel sentences into the summary. The algo-
rithm is actually a variant version of the MMR al-
gorithm (Goldstein et al, 1999). 
The proposed document-based graph model (de-
noted as DGM) with different settings is compared 
with the basic graph-based Model (denoted as GM),  
the top three performing systems and two baseline 
systems on DUC2001 and DUC2002, respectively. 
The top three systems are the systems with highest 
ROUGE scores, chosen from the performing sys-
tems on each task respectively. The lead baseline 
and coverage baseline are two baselines employed 
in the generic multi-document summarization tasks 
of DUC2001 and DUC2002. The lead baseline 
takes the first sentences one by one in the last 
document in the collection, where documents are 
assumed to be ordered chronologically. And the 
coverage baseline takes the first sentence one by 
one from the first document to the last document. 
Tables 2 and 3 show the comparison results on 
DUC2001 and DUC2002, respectively. In Table 1, 
SystemN, SystemP and System T are the top three 
performing systems for DUC2001. In Table 2, Sys-
tem19, System26, System28 are the top three per-
forming systems for DUC2002. The document-
based graph model is configured with different 
settings (i.e. ?1-?3, ?1-?4).  For example, 
DGM(?1+?1) refers to the DGM model with ?1 to 
evaluate the document importance and ?1 to evalu-
ate the correlation between a sentence and its docu-
ment. 
 System ROUGE-1 ROUGE-2 ROUGE-W
DGM(?1+?1) 0.35658 0.05926 0.10712 
DGM(?1+?2) 0.35945 0.06304* 0.10820 
DGM(?1+?3) 0.36349* 0.06472* 0.10952 
DGM(?1+?4) 0.35421 0.05934 0.10695 
DGM(?2+?1) 0.35555 0.06554* 0.10924 
DGM(?2+?2) 0.37228* 0.06787* 0.11295* 
DGM(?2+?3) 0.37347* 0.06612* 0.11352* 
DGM(?2+?4) 0.36340 0.06397* 0.11006 
DGM(?3+?1) 0.35333 0.06353* 0.10834 
DGM(?3+?2) 0.37082* 0.06708* 0.11235 
DGM(?3+?3) 0.37056* 0.06503* 0.11227* 
DGM(?3+?4) 0.36667* 0.06585* 0.11114 
GM 0.35527 0.05608 0.10641 
SystemN 0.33910 0.06853 0.10240 
SystemP 0.33332 0.06651 0.10068 
SystemT 0.33029 0.07862 0.10215 
Coverage 0.33130 0.06898 0.10182 
Lead 0.29419 0.04033 0.08880 
Table 2. Comparison results on DUC2001 
System ROUGE-1 ROUGE-2 ROUGE-W
DGM(?1+?1) 0.37891 0.08398 0.12390 
DGM(?1+?2) 0.39013* 0.08770* 0.12726* 
DGM(?1+?3) 0.38490* 0.08355 0.12570 
DGM(?1+?4) 0.38464 0.08371 0.12443 
DGM(?2+?1) 0.38296 0.08369 0.12499 
DGM(?2+?2) 0.38143 0.08792* 0.12506 
DGM(?2+?3) 0.38177 0.08624* 0.12511 
DGM(?2+?4) 0.38576* 0.08167 0.12611 
DGM(?3+?1) 0.38079 0.08391 0.12392 
DGM(?3+?2) 0.38103 0.08608* 0.12446 
DGM(?3+?3) 0.38236 0.08675* 0.12478 
DGM(?3+?4) 0.38719* 0.08150 0.12633* 
GM 0.37595 0.08304 0.12173 
System26 0.35151 0.07642 0.11448 
System19 0.34504 0.07936 0.11332 
System28 0.34355 0.07521 0.10956 
Coverage 0.32894 0.07148 0.10847 
Lead 0.28684 0.05283 0.09525 
Table 3. Comparison results on DUC2002 
(* indicates that the improvement over the baseline GM 
model is statistically significant at 95% confidence level) 
Seen from the tables, the proposed document-
based graph model with different settings can out-
perform the basic graph-based model and other 
baselines over almost all three metrics on both 
760
DUC2001 and DUC2002 datasets. The results 
demonstrate the good effectiveness of the proposed 
model, i.e. the incorporation of document impact 
does benefit the graph-based summarization model. 
It is interesting that the three methods for comput-
ing document importance and the four methods for 
computing the sentence-document correlation are 
almost as effective as each other on the DUC2002 
dataset. However, ?1 does not perform as well as ?2 
and ?3, and ?1 and ?4 does not perform as well as 
?2 and ?3 on the DUC2001 dataset.  
In order to investigate the relative contributions 
from the two documents for two sentences to the 
summarization performance, we varies the combi-
nation weight ? from 0 to 1 and Figures 3-6 show 
the ROUGE-1 and ROUGE-W curves on 
DUC2001 and DUC2002 respectively. The similar 
ROUGE-2 curves are omitted here. 
 
0.35
0.355
0.36
0.365
0.37
0.375
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E-
1
DGM(?1+?1)
DGM(?1+?2)
DGM(?1+?3)
DGM(?1+?4)
DGM(?2+?1)
DGM(?2+?2)
DGM(?2+?3)
DGM(?2+?4)
DGM(?3+?1)
DGM(?3+?2)
DGM(?3+?3)
DGM(?3+?4)
GM
 
Figure 3. ROUGE-1 vs. ? on DUC2001 
0.104
0.106
0.108
0.11
0.112
0.114
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E-
W
DGM(?1+?1)
DGM(?1+?2)
DGM(?1+?3)
DGM(?1+?4)
DGM(?2+?1)
DGM(?2+?2)
DGM(?2+?3)
DGM(?2+?4)
DGM(?3+?1)
DGM(?3+?2)
DGM(?3+?3)
DGM(?3+?4)
GM
 
Figure 4. ROUGE-W vs. ? on DUC2001 
0.37
0.375
0.38
0.385
0.39
0.395
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E-
1
DGM(?1+?1)
DGM(?1+?2)
DGM(?1+?3)
DGM(?1+?4)
DGM(?2+?1)
DGM(?2+?2)
DGM(?2+?3)
DGM(?2+?4)
DGM(?3+?1)
DGM(?3+?2)
DGM(?3+?3)
DGM(?3+?4)
GM
 
Figure 5. ROUGE-1 vs. ? on DUC2002 
0.12
0.121
0.122
0.123
0.124
0.125
0.126
0.127
0.128
0.129
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E-
W
DGM(?1+?1)
DGM(?1+?2)
DGM(?1+?3)
DGM(?1+?4)
DGM(?2+?1)
DGM(?2+?2)
DGM(?2+?3)
DGM(?2+?4)
DGM(?3+?1)
DGM(?3+?2)
DGM(?3+?3)
DGM(?3+?4)
GM
 
Figure 6. ROUGE-W vs. ? on DUC2002 
We can see from the figures that the proposed 
document-based graph model with different set-
tings can almost always outperform the basic 
graph-based model, with respect to different values 
of ?. The results show the robustness of the pro-
posed model. We can also see that for most set-
tings of the propose model, very large values or 
very small values of ? can deteriorate the summari-
zation performance, i.e. both the first document 
and the second document in the computation of the 
conditional affinity weight between sentences have 
great impact on the summarization performance.  
6 Conclusion and Future Work 
This paper examines the document impact on the 
graph-based model for multi-document summari-
zation. The document-level information and the 
sentence-to-document relationship are incorporated 
into the graph-based ranking algorithm. The ex-
perimental results on DUC2001 and DUC2002 
demonstrate the good effectiveness of the proposed 
model.  
761
In this study, we directly make use of the coarse-
grained document-level information.  Actually, a 
document can be segmented into a few subtopic 
passages by using the TextTiling algorithm (Hearst, 
1997), and we believe the subtopic passage is more 
fine-grained than the original document. In future 
work, we will exploit this kind of subtopic-level 
information to further improve the summarization 
performance.  
Acknowledgments 
This work was supported by the National Science 
Foundation of China (No.60703064), the Research 
Fund for the Doctoral Program of Higher Educa-
tion of China (No.20070001059) and the National 
High Technology Research and Development Pro-
gram of China (No.2008AA01Z421). We also 
thank the anonymous reviewers for their useful 
comments.  
References  
R. Barzilay,  K. R. McKeown and M. Elhadad. 1999. 
Information fusion in the context of multi-document 
summarization. In Proceedings of ACL1999. 
H. Daum? and D. Marcu. 2006. Bayesian query-focused 
summarization. 2006. In Proceedings of COLING-
ACL2006. 
G. Erkan and D. Radev. 2004. LexPageRank: prestige in 
multi-document text summarization. In Proceedings 
of EMNLP?04. 
J. Goldstein, M. Kantrowitz, V. Mittal and J. Carbonell. 
1999. Summarizing text documents: sentence selec-
tion and evaluation metrics. In Proceedings of SIGIR-
99. 
S. Gupta, A. Nenkova and D. Jurafsky. 2007. Measuring 
importance and query relevance in topic-focused 
multi-document summarization. In Proceedings of 
ACL-07. 
S. Harabagiu and F. Lacatusu. 2005. Topic themes for 
multi-document summarization. In Proceedings of 
SIGIR?05. 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting,  G. B. 
Wise. and X. Zhang. 2002. Cross-document summa-
rization by concept classification. In Proceedings of 
SIGIR?02. 
M. Hearst. 1997. TextTiling: segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 23(1): 33-64. 
K. Knight. and  D. Marcu. 2002. Summarization beyond 
sentence extraction: a probabilistic approach to sen-
tence compression, Artificial Intelligence, 139(1). 
W. Kraaij, M. Spitters and M. van der Heijden. 2001. 
Combining a mixture language model and Na?ve 
Bayes for multi-document summarization. In SIGIR 
2001 Workshop on Text Summarization. 
A. Leuski, C.-Y. Lin and E. Hovy. 2003.  iNeATS: in-
teractive multi-document summarization. In Proceed-
ings of ACL2003. 
C.-Y. Lin and E. H. Hovy. 2002. From single to multi-
document summarization: a prototype system and its 
evaluation. In Proceedings of ACL-2002. 
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation 
of summaries using n-gram co-occurrence statistics. 
In Proceedings of HLT-NAACL2003. 
T.-Y. Liu and W.-Y. Ma. 2005. Webpage importance 
analysis using Conditional Markov Random Walk. In 
Proceedings of WI2005.  
I. Mani and E. Bloedorn. 2000. Summarizing similari-
ties and differences among related documents. Infor-
mation Retrieval, 1(1). 
D. Marcu. Discourse-based summarization in DUC?
2001. 2001. In SIGIR 2001 Workshop on Text Sum-
marization. 
K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzi-
lay and E. Eskin. 1999. Towards multidocument 
summarization by reformulation: progress and pros-
pects, in Proceedings of AAAI1999. 
R. Mihalcea and P. Tarau. 2005. A language independ-
ent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP?2005. 
A. Nenkova and A. Louis. 2008. Can you summarize 
this? Identifying correlates of input difficulty for ge-
neric multi-document summarization. In Proceedings 
of ACL-08: HLT. 
L. Page, S. Brin, R. Motwani and T. Winograd. 1998. 
The pagerank citation ranking: Bringing order to the 
web. Technical report, Stanford Digital Libraries. 
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. 
Centroid-based summarization of multiple documents. 
Information Processing and Management, 40: 919-
938. 
X. Wan and J. Yang. 2006. Improved affinity graph 
based multi-document summarization. In Proceedings 
of HLT-NAACL2006. 
X. Wan, J. Yang and J. Xiao. 2007. Manifold-ranking 
based topic-focused multi-document summarization. 
In Proceedings of IJCAI2007. 
762
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 181?184,
New York, June 2006. c?2006 Association for Computational Linguistics
Improved Affinity Graph Based Multi-Document Summarization 
Xiaojun Wan, Jianwu Yang 
Institute of Computer Science and Technology, Peking University 
Beijing 100871, China 
{wanxiaojun, yangjianwu}@icst.pku.edu.cn 
Abstract
This paper describes an affinity graph 
based approach to multi-document sum-
marization. We incorporate a diffusion 
process to acquire semantic relationships 
between sentences, and then compute in-
formation richness of sentences by a 
graph rank algorithm on differentiated in-
tra-document links and inter-document 
links between sentences. A greedy algo-
rithm is employed to impose diversity 
penalty on sentences and the sentences 
with both high information richness and 
high information novelty are chosen into 
the summary. Experimental results on 
task 2 of DUC 2002 and task 2 of DUC 
2004 demonstrate that the proposed ap-
proach outperforms existing state-of-the-
art systems. 
1 Introduction 
Automated multi-document summarization has 
drawn much attention in recent years. Multi-
document summary is usually used to provide con-
cise topic description about a cluster of documents 
and facilitate the users to browse the document 
cluster. A particular challenge for multi-document 
summarization is that the information stored in 
different documents inevitably overlaps with each 
other, and hence we need effective summarization 
methods to merge information stored in different 
documents, and if possible, contrast their differ-
ences.
A variety of multi-document summarization 
methods have been developed recently. In this 
study, we focus on extractive summarization, 
which involves assigning saliency scores to some 
units (e.g. sentences, paragraphs) of the documents 
and extracting tKe sentences with highest scores. 
MEAD is an implementation of the centroid-based 
method (Radev et al, 2004) that scores sentences 
based on sentence-level and inter-sentence features, 
including cluster centroids, position, TF*IDF, etc. 
NeATS (Lin and Hovy, 2002) selects important 
content using Ventence position, term frequency, 
topic signature and term clustering, and then uses 
MMR (Goldstein et al, 1999) to remove redun-
dancy. XDoX (Hardy et al, 1998) identifies the 
most salient themes within the set by passage clus-
tering and then composes an extraction summary, 
which reflects these main themes. Harabagiu and 
Lacatusu (2005) investigate different topic repre-
sentations and extraction methods.
Graph-based methods have been proposed to 
rank sentences or passages. Websumm (Mani and 
Bloedorn, 2000) uses a graph-connectivity model 
and operates under the assumption that nodes 
which are connected to many other nodes are likely 
to carry salient information. LexPageRank (Erkan 
and Radev, 2004) is an approach for computing 
sentence importance based on the concept of ei-
genvector centrality. Mihalcea and Tarau (2005) 
also propose similar algorithms based on PageR-
ank and HITS to compute sentence importance for 
document summarization.  
In this study, we extend the above graph-based 
works by proposing an integrated framework for 
considering both information richness and infor-
mation novelty of a sentence based on sentence 
affinity graph. First, a diffusion process is imposed 
on sentence affinity graph in order to make the af-
finity graph reflect true semantic relationships be-
tween sentences. Second, intra-document links and 
inter-document links between sentences are differ-
entiated to attach more importance to inter-
document links for sentence information richness 
computation. Lastly, a diversity penalty process is 
imposed on sentences to penalize redundant sen-
tences. Experiments on DUC 2002 and DUC 2004 
data are performed and we obtain encouraging re-
sults and conclusions. 
181
2 The Affinity Graph Based Approach 
The proposed affinity graph based summarization
method consists of three steps: (1) an affinity graph
is built to reflect the semantic relationship between
sentences in the document set; (2) information
richness of each sentence is computed based on the
affinity graph; (3) based on the affinity graph and 
the information richness scores, diversity penalty is 
imposed to sentences and the affinity rank score 
for each sentence is obtained to reflect both infor-
mation richness and information novelty of the 
sentence. The sentences with high affinity rank 
scores are chosen to produce the summary.
2.1 Affinity Graph Building
Given a sentence collection S={si | 1?i?n}, the af-
finity weight aff(si, sj) between a sentence pair of si
and sj  is calculated using the cosine measure. The 
weight associated with term t is calculated with the
tft*isft formula, where tft is the frequency of term t
in the corresponding sentence and isft is the inverse 
sentence frequency of term t, i.e. 1+log(N/nt),
where N is the total number of sentences and nt is 
the number of sentences containing term t.  If sen-
tences are considered as nodes, the sentence collec-
tion can be modeled as an undirected graph by
generating the link between two sentences if their
affinity weight exceeds 0, i.e. an undirected link
between si and sj (i?j) with affinity weight aff(si,sj)
is constructed if aff(si,sj)>0; otherwise no link is 
constructed. Thus, we construct an undirected
graph G reflecting the semantic relationship be-
tween sentences by their content similarity. The
graph is called as Affinity Graph. We use an adja-
cency (affinity) matrix M to describe the affinity
graph with each entry corresponding to the weight 
of a link in the graph. M = (Mi,j)n?nis defined as 
follows:
)s,s(affM jij,i  (1)
Then M is normalized to make the sum of each
row equal to 1. Note that we use the same notation 
to denote a matrix and its normalized matrix.
However, the affinity weight between two sen-
tences in the affinity graph is currently computed
simply based on their own content similarity and 
ignore the affinity diffusion process on the graph.
Other than the direct link between two sentences,
the possible paths with more than two steps be-
tween the sentences in the graph also convey more
or less semantic relationship. In order to acquire 
the implicit semantic relationship between sen-
tences, we apply a diffusion process Kandola et 
al., 2002 on the graph to obtain a more appropri-
ate affinity matrix. Though the number of possible 
paths between any two given nodes can grow ex-
ponentially, recent spectral graph theory (Kondor
and Lafferty, 2002) shows that it is possible to
compute the affinity between any two given nodes
efficiently without examining all possible paths. 
The diffusion process on the graph is as follows: 
t1t
1t
~ MM
-?f  J
(2)
where ?(0<?<1) is the decay factor set to 0.9. 
is the t-th power of the initial affinity matrix
and the entry in it is given by
tM
M
? ?
  
?

 

 
ju,iu
n}{1,...,u
1t
1
u,u
t
ji
t1
t
1
MM
"
"",
(3)
that is the sum of the products of the weights over 
all paths of length t that start at node i and finish at 
node j in the graph on the examples. If the entries 
satisfy that they are all positive and for each node 
the sum of the connections is 1, we can view the 
entry as the probability that a random walk begin-
ning at node i reaches node j after t steps.  The ma-
trix M is normalized to make the sum of each row
equal to 1. t is limited to 5 in this study.
~
2.2 Information Richness Computation 
The computation of information richness of sen-
tences is based on the following three intuitions: 1) 
the more neighbors a sentence has, the more in-
formative it is; 2) the more informative a sen-
tence?s neighbors are, the more informative it is; 3) 
the more heavily a sentence is linked with other
informative sentences, the more informative it is.
Based on the above intuitions, the information
richness score InfoRich(si) for a sentence si can be
deduced from those of all other sentences linked
with it and it can be formulated in a recursive form
as follows: 
?
z
?? 
ijall
i,jji n
)d1(M~)s(InfoRichd)s(InfoRich (4)
And the matrix form is: 
e
n
)d1(~d T &
&&  OO M (5)
182
where 1ni )]s(InfoRich[ u O
&
is the eigenvector of 
. is a unit vector with all elements equaling 
to 1. d is the damping factor set to 0.85.
T~M e&
Note that given a link between a sentence pair of 
si and sj, if si and sj comes from the same document,
the link is an intra-document link; and if si and sj
comes from different documents, the link is an in-
ter-document link. We believe that inter-document
links are more important than intra-document links 
for information richness computationDifferent
weights are assigned to intra-document links and 
inter-document links respectively, and the new af-
finity matrix is:
interintra
~~? MMM ED  (6)
where intra
~M is the affinity matrix containing only
the intra-document links (the entries of inter-
document links are set to 0) and inter
~M is the affin-
ity matrix containing only the inter-document links 
(the entries of intra-document links are set to 0). ?,
? are weighting parameters and we let 0??, ??1.
7he matrix is normalized and now the matrix  is 
replaced by  in Equations (4) and (5). 
M~
M?
2.3 Diversity Penalty Imposition 
Based on the affinity graph and obtained informa-
tion richness scores, a greedy algorithm is applied
to impose the diversity penalty and compute the
final affinity rank scores of sentences as follows: 
1. Initialize two sets A=?, B={si | i=1,2,?,n}, and
each sentence?s affinity rank score is initialized to 
its information richness score, i.e. ARScore(si) = 
InfoRich(si), i=1,2,?n.
2. Sort the sentences in B by their current affinity rank
scores in descending order.
3. Suppose si is the highest ranked sentence, i.e. the
first sentence in the ranked list. Move sentence si
from B to A, and then a diversity penalty is im-
posed to the affinity rank score of each sentence
linked with si as follows:
For each sentence sj  in B, we have
)InfoRich(sM~?)ARScore(s)ARScore(s iij,jj ?? (7)
where ?>0 is the penalty degree factor. The larger
? is, the greater penalty is imposed to the affinity
rank score. If ?=0, no diversity penalty is imposed
at all. 
4. Go to step 2 and iterate until B= ? or the iteration
count reaches a predefined maximum number.
After the affinity rank scores are obtained for all
sentences, the sentences with highest affinity rank 
scores are chosen to produce the summary accord-
ing to the summary length limit.
3 Experiments and Results
We compare our system with top 3 performing
systems and two baseline systems on task 2 of 
DUC 2002 and task 4 of DUC 2004 respectively.
ROUGE (Lin and Hovy, 2003) metrics is used for
evaluation1 and we mainly concern about ROUGE-
1. The parameters of our system are tuned on DUC 
2001 as follows: ?=7, ?=0.3 and ?=1.
We can see from the tables that our system out-
performs the top performing systems and baseline 
systems on both DUC 2002 and DUC 2004 tasks 
over all three metrics. The performance improve-
ment achieved by our system results from three
factors: diversity penalty imposition, intra-
document and inter-document link differentiation
and diffusion process incorporation. The ROUGE-
1 contributions of the above three factors are 
0.02200, 0.00268 and 0.00043 respectively.
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.38125 0.08196 0.12390
S26 0.35151 0.07642 0.11448
S19 0.34504 0.07936 0.11332
S28 0.34355 0.07521 0.10956
Coverage Baseline 0.32894 0.07148 0.10847
Lead Baseline 0.28684 0.05283 0.09525
Table 1. System comparison on task 2 of DUC 2002
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.41102 0.09738 0.12560
S65 0.38232 0.09219 0.11528
S104 0.37436 0.08544 0.11305
S35 0.37427 0.08364 0.11561
Coverage Baseline 0.34882 0.07189 0.10622
Lead Baseline 0.32420 0.06409 0.09905
Table 2. System comparison on task 2 of DUC 2004
Figures 1-4 show the influence of the parameters 
in our system. Note that ?: ? denotes the real val-
ues ? and ? are set to. ?w/ diffusion? is the system
with the diffusion process (our system) and  ?w/o
diffusion? is the system without the diffusion proc-
1 We use ROUGEeval-1.4.2 with ?-l? or ?-b? option for trun-
cating longer summaries, and ?-m? option for word stemming. 
183
ess. The observations demonstrate that ?w/ diffu-
sion? performs better than ?w/o diffusion? for most
parameter settings. Meanwhile, ?w/ diffusion? is
more robust than ?w/o diffusion? because the 
ROUGE-1 value of ?w/ diffusion? changes less
when the parameter values vary. Note that in Fig-
ures 3 and 4 the performance decreases sharply 
with the decrease of the weight ? of inter-
document links and it is the worst case when inter-
document links are not taken into account (i.e. ?:
?=1:0), while if intra-document links are not taken 
into account (i.e. ?:?=0:1), the performance is still
good, which demonstrates the great importance of 
inter-document links. 



Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552?559,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Towards an Iterative Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction 
Xiaojun Wan                     Jianwu Yang                     Jianguo Xiao 
Institute of Computer Science and Technology  
Peking University, Beijing 100871, China 
{wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cn
Abstract 
Though both document summarization and 
keyword extraction aim to extract concise 
representations from documents, these two 
tasks have usually been investigated inde-
pendently. This paper proposes a novel it-
erative reinforcement approach to simulta-
neously extracting summary and keywords 
from single document under the assump-
tion that the summary and keywords of a 
document can be mutually boosted. The 
approach can naturally make full use of the 
reinforcement between sentences and key-
words by fusing three kinds of relation-
ships between sentences and words, either 
homogeneous or heterogeneous. Experi-
mental results show the effectiveness of the 
proposed approach for both tasks. The cor-
pus-based approach is validated to work 
almost as well as the knowledge-based ap-
proach for computing word semantics.  
1 Introduction 
Text summarization is the process of creating a 
compressed version of a given document that de-
livers the main topic of the document. Keyword 
extraction is the process of extracting a few salient 
words (or phrases) from a given text and using the 
words to represent the text. The two tasks are simi-
lar in essence because they both aim to extract 
concise representations for documents. Automatic 
text summarization and keyword extraction have 
drawn much attention for a long time because they 
both are very important for many text applications, 
including document retrieval, document clustering, 
etc.  For example, keywords of a document can be 
used for document indexing and thus benefit to 
improve the performance of document retrieval, 
and document summary can help to facilitate users 
to browse the search results and improve users? 
search experience.  
Text summaries and keywords can be either 
query-relevant or generic. Generic summary and 
keyword should reflect the main topics of the 
document without any additional clues and prior 
knowledge. In this paper, we focus on generic 
document summarization and keyword extraction 
for single documents. 
Document summarization and keyword extrac-
tion have been widely explored in the natural lan-
guage processing and information retrieval com-
munities. A series of workshops and conferences 
on automatic text summarization (e.g. SUMMAC, 
DUC and NTCIR) have advanced the technology 
and produced a couple of experimental online sys-
tems. In recent years, graph-based ranking algo-
rithms have been successfully used for document 
summarization (Mihalcea and Tarau, 2004, 2005; 
ErKan and Radev, 2004) and keyword extraction 
(Mihalcea and Tarau, 2004). Such algorithms make 
use of ?voting? or ?recommendations? between 
sentences (or words) to extract sentences (or key-
words). Though the two tasks essentially share 
much in common, most algorithms have been de-
veloped particularly for either document summari-
zation or keyword extraction.  
Zha (2002) proposes a method for simultaneous 
keyphrase extraction and text summarization by 
using only the heterogeneous sentence-to-word 
relationships. Inspired by this, we aim to take into 
account all the three kinds of relationships among 
sentences and words (i.e. the homogeneous rela-
tionships between words, the homogeneous rela-
tionships between sentences, and the heterogene-
ous relationships between words and sentences) in 
552
a unified framework for both document summari-
zation and keyword extraction. The importance of 
a sentence (word) is determined by both the impor-
tance of related sentences (words) and the impor-
tance of related words (sentences). The proposed 
approach can be considered as a generalized form 
of previous graph-based ranking algorithms and 
Zha?s work (Zha, 2002).  
In this study, we propose an iterative reinforce-
ment approach to realize the above idea. The pro-
posed approach is evaluated on the DUC2002 
dataset and the results demonstrate its effectiveness 
for both document summarization and keyword 
extraction. Both knowledge-based approach and 
corpus-based approach have been investigated to 
compute word semantics and they both perform 
very well.  
The rest of this paper is organized as follows: 
Section 2 introduces related works. The details of 
the proposed approach are described in Section 3. 
Section 4 presents and discusses the evaluation 
results. Lastly we conclude our paper in Section 5. 
2 Related Works 
2.1 Document Summarization 
Generally speaking, single document summariza-
tion methods can be either extraction-based or ab-
straction-based and we focus on extraction-based 
methods in this study. 
Extraction-based methods usually assign a sali-
ency score to each sentence and then rank the sen-
tences in the document. The scores are usually 
computed based on a combination of statistical and 
linguistic features, including term frequency, sen-
tence position, cue words, stigma words, topic sig-
nature (Hovy and Lin, 1997; Lin and Hovy, 2000), 
etc. Machine learning methods have also been em-
ployed to extract sentences, including unsupervised 
methods (Nomoto and Matsumoto, 2001) and su-
pervised methods (Kupiec et al, 1995; Conroy and 
O?Leary, 2001; Amini and Gallinari, 2002; Shen et 
al., 2007). Other methods include maximal mar-
ginal relevance (MMR) (Carbonell and Goldstein, 
1998), latent semantic analysis (LSA) (Gong and 
Liu, 2001). In Zha (2002), the mutual reinforce-
ment principle is employed to iteratively extract 
key phrases and sentences from a document.   
Most recently, graph-based ranking methods, in-
cluding TextRank ((Mihalcea and Tarau, 2004, 
2005) and LexPageRank (ErKan and Radev, 2004) 
have been proposed for document summarization. 
Similar to Kleinberg?s HITS algorithm (Kleinberg, 
1999) or Google?s PageRank (Brin and Page, 
1998), these methods first build a graph based on 
the similarity between sentences in a document and 
then the importance of a sentence is determined by 
taking into account global information on the 
graph recursively, rather than relying only on local 
sentence-specific information. 
2.2 Keyword Extraction 
Keyword (or keyphrase) extraction usually in-
volves assigning a saliency score to each candidate 
keyword by considering various features. Krulwich 
and Burkey (1996) use heuristics to extract key-
phrases from a document. The heuristics are based 
on syntactic clues, such as the use of italics, the 
presence of phrases in section headers, and the use 
of acronyms. Mu?oz (1996) uses an unsupervised 
learning algorithm to discover two-word key-
phrases. The algorithm is based on Adaptive Reso-
nance Theory (ART) neural networks. Steier and 
Belew (1993) use the mutual information statistics 
to discover two-word keyphrases. 
Supervised machine learning algorithms have 
been proposed to classify a candidate phrase into 
either keyphrase or not. GenEx (Turney, 2000) and 
Kea (Frank et al, 1999; Witten et al, 1999) are 
two typical systems, and the most important fea-
tures for classifying a candidate phrase are the fre-
quency and location of the phrase in the document. 
More linguistic knowledge (such as syntactic fea-
tures) has been explored by Hulth (2003). More 
recently, Mihalcea and Tarau (2004) propose the 
TextRank model to rank keywords based on the 
co-occurrence links between words. 
3 Iterative Reinforcement Approach 
3.1 Overview 
The proposed approach is intuitively based on the 
following assumptions: 
Assumption 1: A sentence should be salient if it 
is heavily linked with other salient sentences, and a 
word should be salient if it is heavily linked with 
other salient words. 
Assumption 2: A sentence should be salient if it 
contains many salient words, and a word should be 
salient if it appears in many salient sentences. 
The first assumption is similar to PageRank 
which makes use of mutual ?recommendations? 
553
between homogeneous objects to rank objects. The 
second assumption is similar to HITS if words and 
sentences are considered as authorities and hubs 
respectively. In other words, the proposed ap-
proach aims to fuse the ideas of PageRank and 
HITS in a unified framework.  
In more detail, given the heterogeneous data 
points of sentences and words, the following three 
kinds of relationships are fused in the proposed 
approach: 
SS-Relationship: It reflects the homogeneous 
relationships between sentences, usually computed 
by their content similarity. 
WW-Relationship: It reflects the homogeneous 
relationships between words, usually computed by 
knowledge-based approach or corpus-based ap-
proach. 
SW-Relationship: It reflects the heterogeneous 
relationships between sentences and words, usually 
computed as the relative importance of a word in a 
sentence. 
Figure 1 gives an illustration of the relationships.  
 
Figure 1. Illustration of the Relationships 
 
The proposed approach first builds three graphs 
to reflect the above relationships respectively, and 
then iteratively computes the saliency scores of the 
sentences and words based on the graphs. Finally, 
the algorithm converges and each sentence or word 
gets its saliency score. The sentences with high 
saliency scores are chosen into the summary, and 
the words with high saliency scores are combined 
to produce the keywords. 
3.2 Graph Building 
3.2.1  Sentence-to-Sentence Graph ( SS-Graph)  
Given the sentence collection S={si | 1IiIm} of a 
document,  if each sentence is considered as a node, 
the sentence collection can be modeled as an undi-
rected graph by generating an edge between two 
sentences if their content similarity exceeds 0, i.e. 
an undirected link between si and sj (iKj) is con-
structed and the associated weight is their content 
similarity. Thus, we construct an undirected graph 
GSS to reflect the homogeneous relationship be-
tween sentences. The content similarity between 
two sentences is computed with the cosine measure. 
We use an adjacency matrix U to describe GSS with 
each entry corresponding to the weight of a link in 
the graph. U= [Uij]m?m is defined as follows: 



?

?
otherwise,
j, if iss
ss
U ji
ji
ij
0
rr
rr
(1) 
where is and js
r are the corresponding term vec-
tors of sentences si and sj respectively. The weight 
associated with term t is calculated with tft.isft,
where tft is the frequency of term t in the sentence 
and isft is the inverse sentence frequency of term t,
i.e. 1+log(N/nt), where N is the total number of 
sentences and nt is the number of sentences con-
taining term t in a background corpus. Note that 
other measures (e.g. Jaccard, Dice, Overlap, etc.) 
can also be explored to compute the content simi-
larity between sentences, and we simply choose the 
cosine measure in this study. 
Then U is normalized to U~ as follows to make 
the sum of each row equal to 1: 


 ?
erwise , oth 
U, if UUU
m
j
ij
m
j
ijij
ij
0
0~
11 (2) 
3.2.2  Word-to-Word Graph ( WW-Graph)  
Given the word collection T={tj|1IjIn } of a docu-
ment1 , the semantic similarity between any two 
words ti and tj can be computed using approaches 
that are either knowledge-based or corpus-based 
(Mihalcea et al, 2006).   
Knowledge-based measures of word semantic 
similarity try to quantify the degree to which two 
words are semantically related using information 
drawn from semantic networks. WordNet (Fell-
baum, 1998) is a lexical database where each 
 
1 The stopwords defined in the Smart system have been re-
moved from the collection. 
sentence
word
SS
WW
SW
554
unique meaning of a word is represented by a 
synonym set or synset. Each synset has a gloss that 
defines the concept that it represents. Synsets are 
connected to each other through explicit semantic 
relations that are defined in WordNet. Many ap-
proaches have been proposed to measure semantic 
relatedness based on WordNet. The measures vary 
from simple edge-counting to attempt to factor in 
peculiarities of the network structure by consider-
ing link direction, relative path, and density, such 
as  vector, lesk, hso, lch, wup, path, res, lin and jcn 
(Pedersen et al, 2004). For example, ?cat? and 
?dog? has higher semantic similarity than ?cat? 
and ?computer?. In this study, we implement the 
vector measure to efficiently evaluate the similari-
ties of a large number of word pairs. The vector 
measure (Patwardhan, 2003) creates a co?
occurrence matrix from a corpus made up of the 
WordNet glosses. Each content word used in a 
WordNet gloss has an associated context vector. 
Each gloss is represented by a gloss vector that is 
the average of all the context vectors of the words 
found in the gloss. Relatedness between concepts 
is measured by finding the cosine between a pair of 
gloss vectors. 
 Corpus-based measures of word semantic simi-
larity try to identify the degree of similarity be-
tween words using information exclusively derived 
from large corpora. Such measures as mutual in-
formation (Turney 2001), latent semantic analysis 
(Landauer et al, 1998), log-likelihood ratio (Dun-
ning, 1993) have been proposed to evaluate word 
semantic similarity based on the co-occurrence 
information on a large corpus. In this study, we 
simply choose the mutual information to compute 
the semantic similarity between word ti and tj as 
follows: 
)()(
)(log)(
ji
ji
ji tptp
,ttpN,ttsim
 (3) 
which indicates the degree of statistical depend-
ence between ti and tj. Here, N is the total number 
of words in the corpus and p(ti) and p(tj) are re-
spectively the probabilities of the occurrences of ti
and tj, i.e. count(ti)/N and count(tj)/N, where 
count(ti) and count(tj) are the frequencies of ti and tj.
p(ti, tj) is the probability of the co-occurrence of ti
and tj within a window with a predefined size k, i.e. 
count(ti, tj)/N, where count(ti, tj) is the number of 
the times ti and tj co-occur within the window.  
Similar to the SS-Graph, we can build an undi-
rected graph GWW to reflect the homogeneous rela-
tionship between words, in which each node corre-
sponds to a word and the weight associated with 
the edge between any different word ti and tj is 
computed by either the WordNet-based vector 
measure or the corpus-based mutual information 
measure. We use an adjacency matrix V to de-
scribe GWW with each entry corresponding to the 
weight of a link in the graph. V= [Vij]n?n, where Vij 
=sim(ti, tj) if iKj and Vij=0 if i=j.
Then V is similarly normalized to V~ to make 
the sum of each row equal to 1. 
3.2.3  Sentence-to-Word Graph ( SW-Graph)  
Given the sentence collection S={si | 1IiIm} and 
the word collection T={tj|1IjIn } of a document, 
we can build a weighted bipartite graph GSW from S
and T in the following way: if word tj appears in 
sentence si, we then create an edge between si and 
tj. A nonnegative weight aff(si,tj) is specified on the 
edge, which is proportional to the importance of 
word tj in sentence si, computed as follows: 

i
jj
st
tt
tt
ji isftf
isftf
,tsaff )(  (4)
where t represents a unique term in si and tft, isft
are respectively the term frequency in the sentence 
and the inverse sentence frequency.  
We use an adjacency (affinity) matrix 
W=[Wij]m?n to describe GSW  with each entry Wij 
corresponding to aff(si,tj). Similarly, W is normal-
ized to W~ to make the sum of each row equal to 1. 
In addition, we normalize the transpose of W, i.e. 
WT, to W? to make the sum of each row in WT
equal to 1. 
3.3 Reinforcement Algorithm 
We use two column vectors u=[u(si)]m?1 and v
=[v(tj)]n?1 to denote the saliency scores of the sen-
tences and words in the specified document. The 
assumptions introduced in Section 3.1 can be ren-
dered as follows: 
 j jjii suUsu )(~)( (5) 
 i iijj tvVtv )(~)( (6) 
 j jjii tvWsu )(?)( (7) 
555
 i iijj suWtv )(~)( (8) 
After fusing the above equations, we can obtain 
the following iterative forms: 
 n
j
jji
m
j
jjii tvW)suU*su
11
)(?)(~)( (9)
 m
i
iij
n
i
iijj suW)tvV*tv
11
)(~)(~)( (10)
And the matrix form is: 
vWuUu TT )* ?~ (11)
uWvVv TT )* ~~ (12) 
where * and ) specify the relative contributions to 
the final saliency scores from the homogeneous 
nodes and the heterogeneous nodes and we have 
*+)=1. In order to guarantee the convergence of 
the iterative form, u and v are normalized after 
each iteration. 
For numerical computation of the saliency 
scores, the initial scores of all sentences and words 
are set to 1 and the following two steps are alter-
nated until convergence, 
1. Compute and normalize the scores of sen-
tences: 
)(n-T)(n-T(n) )* 11 ?~ vWuUu ,
1
(n)(n)(n) / uuu
2. Compute and normalize the scores of words: 
)(n-T)(n-T(n) )* 11 ~~ uWvVv ,
1
(n)(n)(n) / vvv
where u(n) and v(n) denote the vectors computed at 
the n-th iteration.   
Usually the convergence of the iteration algo-
rithm is achieved when the difference between the 
scores computed at two successive iterations for 
any sentences and words falls below a given 
threshold (0.0001 in this study).  
4 Empirical Evaluation 
4.1 Summarization Evaluation 
4.1.1 Evaluation Setup 
We used task 1 of DUC2002 (DUC, 2002) for 
evaluation. The task aimed to evaluate generic 
summaries with a length of approximately 100 
words or less. DUC2002 provided 567 English 
news articles collected from TREC-9 for single-
document summarization task. The sentences in 
each article have been separated and the sentence 
information was stored into files.  
In the experiments, the background corpus for 
using the mutual information measure to compute 
word semantics simply consisted of all the docu-
ments from DUC2001 to DUC2005, which could 
be easily expanded by adding more documents. 
The stopwords were removed and the remaining 
words were converted to the basic forms based on 
WordNet. Then the semantic similarity values be-
tween the words were computed.   
We used the ROUGE (Lin and Hovy, 2003) 
toolkit (i.e.ROUGEeval-1.4.2 in this study) for 
evaluation, which has been widely adopted by 
DUC for automatic summarization evaluation. It 
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and 
word pairs between the candidate summary and the 
reference summary. ROUGE toolkit reported sepa-
rate scores for 1, 2, 3 and 4-gram, and also for 
longest common subsequence co-occurrences. 
Among these different scores, unigram-based 
ROUGE score (ROUGE-1) has been shown to 
agree with human judgment most (Lin and Hovy, 
2003). We showed three of the ROUGE metrics in 
the experimental results: ROUGE-1 (unigram-
based), ROUGE-2 (bigram-based), and ROUGE-
W (based on weighted longest common subse-
quence, weight=1.2).  
In order to truncate summaries longer than the 
length limit, we used the ?-l? option 2 in the 
ROUGE toolkit. 
4.1.2 Evaluation Results 
For simplicity, the parameters in the proposed ap-
proach are simply set to *=)=0.5, which means 
that the contributions from sentences and words 
are equally important. We adopt the WordNet-
based vector measure (WN) and the corpus-based 
mutual information measure (MI) for computing 
the semantic similarity between words.  When us-
ing the mutual information measure, we heuristi-
cally set the window size k to 2, 5 and 10, respec-
tively.  
The proposed approaches with different word 
similarity measures (WN and MI) are compared 
 
2 The ?-l? option is very important for fair comparison. Some 
previous works not adopting this option are likely to overes-
timate the ROUGE scores.  
556
with two solid baselines: SentenceRank and Mutu-
alRank. SentenceRank is proposed in Mihalcea and 
Tarau (2004) to make use of only the sentence-to-
sentence relationships to rank sentences, which 
outperforms most popular summarization methods. 
MutualRank is proposed in Zha (2002) to make use 
of only the sentence-to-word relationships to rank 
sentences and words. For all the summarization 
methods, after the sentences are ranked by their 
saliency scores, we can apply a variant form of the 
MMR algorithm to remove redundancy and choose 
both the salient and novel sentences to the sum-
mary. Table 1 gives the comparison results of the 
methods before removing redundancy and Table 2 
gives the comparison results of the methods after 
removing redundancy. 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47100
*# 0.20424*# 0.16336#
Our Approach
(MI:k=2) 0.46711
# 0.20195# 0.16257#
Our Approach
(MI:k=5) 0.46803
# 0.20259# 0.16310#
Our Approach
(MI:k=10) 0.46823
# 0.20301# 0.16294#
SentenceRank 0.45591 0.19201 0.15789 
MutualRank 0.43743 0.17986 0.15333 
Table 1. Summarization Performance before Re-
moving Redundancy (w/o MMR) 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47329
*# 0.20249# 0.16352#
Our Approach
(MI:k=2) 0.47281
# 0.20281# 0.16373#
Our Approach
(MI:k=5) 0.47282
# 0.20249# 0.16343#
Our Approach
(MI:k=10) 0.47223
# 0.20225# 0.16308#
SentenceRank 0.46261 0.19457 0.16018 
MutualRank 0.43805 0.17253 0.15221 
Table 2. Summarization Performance after Remov-
ing Redundancy (w/ MMR) 
 (* indicates that the improvement over SentenceRank is sig-
nificant and # indicates that the improvement over Mutual-
Rank is significant, both by comparing the 95% confidence 
intervals provided by the ROUGE package.)
Seen from Tables 1 and 2, the proposed ap-
proaches always outperform the two baselines over 
all three metrics with different word semantic 
measures. Moreover, no matter whether the MMR 
algorithm is applied or not, almost all performance 
improvements over MutualRank are significant 
and the ROUGE-1 performance improvements 
over SentenceRank are significant when using 
WordNet-based measure (WN). Word semantics 
can be naturally incorporated into the computation 
process, which addresses the problem that Sen-
tenceRank cannot take into account word seman-
tics, and thus improves the summarization per-
formance. We also observe that the corpus-based 
measure (MI) works almost as well as the knowl-
edge-based measure (WN) for computing word 
semantic similarity. 
In order to better understand the relative contri-
butions from the sentence nodes and the word 
nodes, the parameter * is varied from 0 to 1. The 
larger * is, the more contribution is given from the 
sentences through the SS-Graph, while the less 
contribution is given from the words through the 
SW-Graph. Figures 2-4 show the curves over three 
ROUGE scores with respect to *. Without loss of 
generality, we use the case of k=5 for the MI 
measure as an illustration. The curves are similar 
to Figures 2-4 when k=2 and k=10.   
 
0.435
0.44
0.445
0.45
0.455
0.46
0.465
0.47
0.475
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*
RO
UG
E-
1
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 2. ROUGE-1 vs. *
0.17
0.175
0.18
0.185
0.19
0.195
0.2
0.205
0.21
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
2
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 3. ROUGE-2 vs. *
557
0.151
0.153
0.155
0.157
0.159
0.161
0.163
0.165
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
W
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 4. ROUGE-W vs. *
Seen from Figures 2-4, no matter whether the 
MMR algorithm is applied or not (i.e. w/o MMR 
or w/ MMR), the ROUGE scores based on either 
word semantic measure (MI or WN) achieves the 
peak when * is set between 0.4 and 0.6. The per-
formance values decrease sharply when * is very 
large (near to 1) or very small (near to 0). The 
curves demonstrate that both the contribution from 
the sentences and the contribution from the words 
are important for ranking sentences; moreover, the 
contributions are almost equally important. Loss of 
either contribution will much deteriorate the final 
performance.  
Similar results and observations have been ob-
tained on task 1 of DUC2001 in our study and the 
details are omitted due to page limit. 
4.2 Keyword Evaluation 
4.1.1   Evaluation Setup 
In this study we performed a preliminary evalua-
tion of keyword extraction. The evaluation was 
conducted on the single word level instead of the 
multi-word phrase (n-gram) level, in other words, 
we compared the automatically extracted unigrams 
(words) and the manually labeled unigrams 
(words). The reasons were that: 1) there existed 
partial matching between phrases and it was not 
trivial to define an accurate measure to evaluate 
phrase quality; 2) each phrase was in fact com-
posed of a few words, so the keyphrases could be 
obtained by combining the consecutive keywords.  
We used 34 documents in the first five docu-
ment clusters in DUC2002 dataset (i.e. d061-d065).  
At most 10 salient words were manually labeled 
for each document to represent the document and 
the average number of manually assigned key-
words was 6.8. Each approach returned 10 words 
with highest saliency scores as the keywords. The 
extracted 10 words were compared with the manu-
ally labeled keywords. The words were converted 
to their corresponding basic forms based on 
WordNet before comparison. The precision p, re-
call r, F-measure (F=2pr/(p+r)) were obtained for 
each document and then the values were averaged 
over all documents for evaluation purpose. 
4.1.2 Evaluation Results 
Table 3 gives the comparison results. The proposed 
approaches are compared with two baselines: 
WordRank and MutualRank. WordRank is pro-
posed in Mihalcea and Tarau (2004) to make use 
of only the co-occurrence relationships between 
words to rank words, which outperforms tradi-
tional keyword extraction methods. The window 
size k for WordRank is also set to 2, 5 and 10, re-
spectively. 
 
System Precision Recall F-measure
Our Approach
(WN) 0.413 0.504 0.454 
Our Approach
(MI:k=2) 0.428 0.485 0.455 
Our Approach
(MI:k=5) 0.425 0.491 0.456 
Our Approach
(MI:k=10) 0.393 0.455 0.422 
WordRank 
(k=2) 0.373 0.412 0.392 
WordRank 
(k=5) 0.368 0.422 0.393 
WordRank 
(k=10) 0.379 0.407 0.393 
MutualRank 0.355 0.397 0.375 
Table 3. The Performance of Keyword Extraction  
Seen from the table, the proposed approaches 
significantly outperform the baseline approaches. 
Both the corpus-based measure (MI) and the 
knowledge-based measure (WN) perform well on 
the task of keyword extraction. 
A running example is given below to demon-
strate the results: 
Document ID: D062/AP891018-0301 
Labeled keywords:
insurance earthquake insurer damage california Francisco 
pay 
Extracted keywords:
WN: insurance earthquake insurer quake california 
spokesman cost million wednesday damage 
MI(k=5): insurance insurer earthquake percent benefit 
california property damage estimate rate 
558
5 Conclusion and Future Work 
In this paper we propose a novel approach to si-
multaneously document summarization and key-
word extraction for single documents by fusing the 
sentence-to-sentence, word-to-word, sentence-to-
word relationships in a unified framework. The 
semantics between words computed by either cor-
pus-based approach or knowledge-based approach 
can be incorporated into the framework in a natural 
way. Evaluation results demonstrate the perform-
ance improvement of the proposed approach over 
the baselines for both tasks. 
In this study, only the mutual information meas-
ure and the vector measure are employed to com-
pute word semantics, and in future work many 
other measures mentioned earlier will be investi-
gated in the framework in order to show the ro-
bustness of the framework. The evaluation of key-
word extraction is preliminary in this study, and 
we will conduct more thorough experiments to 
make the results more convincing. Furthermore, 
the proposed approach will be applied to multi-
document summarization and keyword extraction, 
which are considered more difficult than single 
document summarization and keyword extraction. 
Acknowledgements 
This work was supported by the National Science 
Foundation of China (60642001). 
References 
M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to 
improve supervised learning for text summarization. In Pro-
ceedings of SIGIR2002, 105-112. 
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertex-
tual Web search engine. Computer Networks and ISDN Sys-
tems, 30(1?7). 
J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-
based reranking for reordering documents and producing 
summaries. In Proceedings of SIGIR-1998, 335-336. 
J. M. Conroy and D. P. O?Leary. 2001. Text summarization via 
Hidden Markov Models. In Proceedings of SIGIR2001, 406-
407. 
DUC. 2002. The Document Understanding Workshop 2002. 
http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html 
T. Dunning. 1993. Accurate methods for the statistics of surprise 
and coincidence. Computational Linguistics 19, 61?74. 
G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in 
multi-document text summarization. In Proceedings of 
EMNLP2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. 
The MIT Press.  
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. 
Nevill-Manning. 1999. Domain-specific keyphrase extraction. 
Proceedings of IJCAI-99, pp. 668-673.  
Y. H. Gong and X. Liu. 2001. Generic text summarization using 
Relevance Measure and Latent Semantic Analysis. In Proceed-
ings of SIGIR2001, 19-25. 
E. Hovy and C. Y. Lin. 1997. Automated text summarization in 
SUMMARIST. In Proceeding of ACL?1997/EACL?1997 Wor-
shop on Intelligent Scalable Text Summarization.
A. Hulth. 2003. Improved automatic keyword extraction given 
more linguistic knowledge. In Proceedings of EMNLP2003,
Japan, August. 
J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked 
environment. Journal of the ACM, 46(5):604?632. 
B. Krulwich and C. Burkey. 1996. Learning user information 
interests through the extraction of semantically significant 
phrases. In AAAI 1996 Spring Symposium on Machine Learn-
ing in Information Access.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document 
summarizer. In Proceedings of SIGIR1995, 68-73. 
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to 
latent semantic analysis. Discourse Processes 25. 
C. Y. Lin and  E. Hovy. 2000. The automated acquisition of topic 
signatures for text Summarization. In Proceedings of ACL-
2000, 495-501. 
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summa-
ries using n-gram co-occurrence statistics. In Proceedings of 
HLT-NAACL2003, Edmonton, Canada, May. 
R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based 
and knowledge-based measures of text semantic similarity. In 
Proceedings of AAAI-06.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into 
texts. In Proceedings of EMNLP2004.
R. Mihalcea and P.Tarau. 2005. A language independent algo-
rithm for single and multiple document summarization. In 
Proceedings of IJCNLP2005.
A. Mu?oz. 1996. Compound key word generation from document 
databases using a hierarchical clustering ART model. Intelli-
gent Data Analysis, 1(1). 
T. Nomoto and Y. Matsumoto. 2001. A new approach to unsuper-
vised text summarization. In Proceedings of SIGIR2001, 26-34. 
S. Patwardhan. 2003. Incorporating dictionary and corpus infor-
mation into a context vector measure of semantic relatedness. 
Master?s thesis, Univ. of Minnesota, Duluth. 
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-
Net::Similarity ? Measuring the relatedness of concepts. In 
Proceedings of AAAI-04.
D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document 
Summarization using Conditional Random Fields. In Proceed-
ings of IJCAI 07.
A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statisti-
cal analysis of topical language.  In Proceedings of Second 
Symposium on Document Analysis and Information Retrieval,
pp. 179-190. 
P. D. Turney. 2000. Learning algorithms for keyphrase extraction. 
Information Retrieval, 2:303-336. 
P. Turney. 2001. Mining the web for synonyms: PMI-IR versus 
LSA on TOEFL. In Proceedings of ECML-2001.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. 
Nevill-Manning. 1999. KEA: Practical automatic keyphrase 
extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 
254-256. 
H. Y. Zha. 2002. Generic summarization and keyphrase extraction 
using mutual reinforcement principle and sentence clustering. 
In Proceedings of SIGIR2002, pp. 113-120. 
559
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235?243,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Co-Training for Cross-Lingual Sentiment Classification 
 
Xiaojun Wan 
Institute of Compute Science and Technology & Key Laboratory of Computational Lin-
guistics, MOE 
Peking University, Beijing 100871, China 
wanxiaojun@icst.pku.edu.cn 
 
 
Abstract 
The lack of Chinese sentiment corpora limits 
the research progress on Chinese sentiment 
classification. However, there are many freely 
available English sentiment corpora on the 
Web.  This paper focuses on the problem of 
cross-lingual sentiment classification, which 
leverages an available English corpus for Chi-
nese sentiment classification by using the Eng-
lish corpus as training data. Machine transla-
tion services are used for eliminating the lan-
guage gap between the training set and test set, 
and English features and Chinese features are 
considered as two independent views of the 
classification problem. We propose a co-
training approach to making use of unlabeled 
Chinese data.  Experimental results show the 
effectiveness of the proposed approach, which 
can outperform the standard inductive classifi-
ers and the transductive classifiers.  
1 Introduction 
Sentiment classification is the task of identifying 
the sentiment polarity of a given text. The senti-
ment polarity is usually positive or negative and 
the text genre is usually product review. In recent 
years, sentiment classification has drawn much 
attention in the NLP field and it has many useful 
applications, such as opinion mining and summa-
rization (Liu et al, 2005; Ku et al, 2006; Titov 
and McDonald, 2008). 
To date, a variety of corpus-based methods 
have been developed for sentiment classification. 
The methods usually rely heavily on an anno-
tated corpus for training the sentiment classifier. 
The sentiment corpora are considered as the most 
valuable resources for the sentiment classifica-
tion task. However, such resources in different 
languages are very imbalanced. Because most 
previous work focuses on English sentiment 
classification, many annotated corpora for Eng-
lish sentiment classification are freely available 
on the Web. However, the annotated corpora for 
Chinese sentiment classification are scarce and it 
is not a trivial task to manually label reliable 
Chinese sentiment corpora. The challenge before 
us is how to leverage rich English corpora for 
Chinese sentiment classification. In this study, 
we focus on the problem of cross-lingual senti-
ment classification, which leverages only English 
training data for supervised sentiment classifica-
tion of Chinese product reviews, without using 
any Chinese resources. Note that the above prob-
lem is not only defined for Chinese sentiment 
classification, but also for various sentiment 
analysis tasks in other different languages.  
Though pilot studies have been performed to 
make use of English corpora for subjectivity 
classification in other languages (Mihalcea et al, 
2007; Banea et al, 2008), the methods are very 
straightforward by directly employing an induc-
tive classifier (e.g. SVM, NB), and the classifica-
tion performance is far from satisfactory because 
of the language gap between the original lan-
guage and the translated language.  
In this study, we propose a co-training ap-
proach to improving the classification accuracy 
of polarity identification of Chinese product re-
views. Unlabeled Chinese reviews can be fully 
leveraged in the proposed approach. First, ma-
chine translation services are used to translate 
English training reviews into Chinese reviews 
and also translate Chinese test reviews and addi-
tional unlabeled reviews into English reviews. 
Then, we can view the classification problem in 
two independent views: Chinese view with only 
Chinese features and English view with only 
English features. We then use the co-training 
approach to making full use of the two redundant 
views of features. The SVM classifier is adopted 
as the basic classifier in the proposed approach. 
Experimental results show that the proposed ap-
proach can outperform the baseline inductive 
classifiers and the more advanced transductive 
classifiers.  
The rest of this paper is organized as follows: 
Section 2 introduces related work. The proposed 
235
co-training approach is described in detail in 
Section 3. Section 4 shows the experimental re-
sults. Lastly we conclude this paper in Section 5. 
2 Related Work 
2.1 Sentiment Classification 
Sentiment classification can be performed on 
words, sentences or documents. In this paper we 
focus on document sentiment classification. The 
methods for document sentiment classification 
can be generally categorized into lexicon-based 
and corpus-based.  
Lexicon-based methods usually involve deriv-
ing a sentiment measure for text based on senti-
ment lexicons.  Turney (2002) predicates the sen-
timent orientation of a review by the average se-
mantic orientation of the phrases in the review 
that contain adjectives or adverbs, which is de-
noted as the semantic oriented method. Kim and 
Hovy (2004) build three models to assign a sen-
timent category to a given sentence by combin-
ing the individual sentiments of sentiment-
bearing words. Hiroshi et al (2004) use the tech-
nique of deep language analysis for machine 
translation to extract sentiment units in text 
documents. Kennedy and Inkpen (2006) deter-
mine the sentiment of a customer review by 
counting positive and negative terms and taking 
into account contextual valence shifters, such as 
negations and intensifiers. Devitt and Ahmad 
(2007) explore a computable metric of positive 
or negative polarity in financial news text.  
Corpus-based methods usually consider the 
sentiment analysis task as a classification task 
and they use a labeled corpus to train a sentiment 
classifier. Since the work of Pang et al (2002), 
various classification models and linguistic fea-
tures have been proposed to improve the classifi-
cation performance (Pang and Lee, 2004; Mullen 
and Collier, 2004; Wilson et al, 2005; Read, 
2005). Most recently, McDonald et al (2007) 
investigate a structured model for jointly classi-
fying the sentiment of text at varying levels of 
granularity. Blitzer et al (2007) investigate do-
main adaptation for sentiment classifiers, focus-
ing on online reviews for different types of prod-
ucts. Andreevskaia and Bergler (2008) present a 
new system consisting of the ensemble of a cor-
pus-based classifier and a lexicon-based classi-
fier with precision-based vote weighting. 
Chinese sentiment analysis has also been stud-
ied (Tsou et al, 2005; Ye et al, 2006; Li and Sun, 
2007) and most such work uses similar lexicon-
based or corpus-based methods for Chinese sen-
timent classification.  
To date, several pilot studies have been per-
formed to leverage rich English resources for 
sentiment analysis in other languages. Standard 
Na?ve Bayes and SVM classifiers have been ap-
plied for subjectivity classification in Romanian 
(Mihalcea et al, 2007; Banea et al, 2008), and 
the results show that automatic translation is a 
viable alternative for the construction of re-
sources and tools for subjectivity analysis in a 
new target language. Wan (2008) focuses on lev-
eraging both Chinese and English lexicons to 
improve Chinese sentiment analysis by using 
lexicon-based methods. In this study, we focus 
on improving the corpus-based method for cross-
lingual sentiment classification of Chinese prod-
uct reviews by developing novel approaches.  
2.2 Cross-Domain Text Classification 
Cross-domain text classification can be consid-
ered as a more general task than cross-lingual 
sentiment classification. In the problem of cross-
domain text classification, the labeled and unla-
beled data come from different domains, and 
their underlying distributions are often different 
from each other, which violates the basic as-
sumption of traditional classification learning.  
To date, many semi-supervised learning algo-
rithms have been developed for addressing the 
cross-domain text classification problem by 
transferring knowledge across domains, includ-
ing Transductive SVM (Joachims, 1999), 
EM(Nigam et al, 2000), EM-based Na?ve Bayes 
classifier (Dai et al, 2007a), Topic-bridged 
PLSA (Xue et al, 2008), Co-Clustering based 
classification (Dai et al, 2007b), two-stage ap-
proach (Jiang and Zhai, 2007). Daum?III and 
Marcu (2006) introduce a statistical formulation 
of this problem in terms of a simple mixture 
model.  
In particular, several previous studies focus on 
the problem of cross-lingual text classification, 
which can be considered as a special case of 
general cross-domain text classification. Bel et al 
(2003) present practical and cost-effective solu-
tions. A few novel models have been proposed to 
address the problem, e.g. the EM-based algo-
rithm (Rigutini et al, 2005), the information bot-
tleneck approach (Ling et al, 2008), the multi-
lingual domain models (Gliozzo and Strapparava, 
2005), etc. To the best of our knowledge, co-
training has not yet been investigated for cross-
domain or cross-lingual text classification. 
236
3 The Co-Training Approach  
3.1 Overview 
The purpose of our approach is to make use of 
the annotated English corpus for sentiment polar-
ity identification of Chinese reviews in a super-
vised framework, without using any Chinese re-
sources. Given the labeled English reviews and 
unlabeled Chinese reviews, two straightforward 
methods for addressing the problem are as fol-
lows:  
1) We first learn a classifier based on the la-
beled English reviews, and then translate Chi-
nese reviews into English reviews. Lastly, we 
use the classifier to classify the translated Eng-
lish reviews.  
2) We first translate the labeled English re-
views into Chinese reviews, and then learn a 
classifier based on the translated Chinese reviews 
with labels. Lastly, we use the classifier to clas-
sify the unlabeled Chinese reviews.  
The above two methods have been used in 
(Banea et al, 2008) for Romanian subjectivity 
analysis, but the experimental results are not very 
promising. As shown in our experiments, the 
above two methods do not perform well for Chi-
nese sentiment classification, either, because the 
underlying distribution between the original lan-
guage and the translated language are different.  
In order to address the above problem, we 
propose to use the co-training approach to make 
use of some amounts of unlabeled Chinese re-
views to improve the classification accuracy. The 
co-training approach can make full use of both 
the English features and the Chinese features in a 
unified framework. The framework of the pro-
posed approach is illustrated in Figure 1. 
 The framework consists of a training phase 
and a classification phase. In the training phase, 
the input is the labeled English reviews and some 
amounts of unlabeled Chinese reviews1. The la-
beled English reviews are translated into labeled 
Chinese reviews, and the unlabeled Chinese re-
views are translated into unlabeled English re-
views, by using machine translation services. 
Therefore, each review is associated with an 
English version and a Chinese version. The Eng-
lish features and the Chinese features for each 
review are considered two independent and re-
dundant views of the review. The co-training 
algorithm is then applied to learn two classifiers 
                                                 
1 The unlabeled Chinese reviews used for co-training do not 
include the unlabeled Chinese reviews for testing, i.e., the 
Chinese reviews for testing are blind to the training phase.  
and finally the two classifiers are combined into 
a single sentiment classifier. In the classification 
phase, each unlabeled Chinese review for testing 
is first translated into English review, and then 
the learned classifier is applied to classify the 
review into either positive or negative.  
The steps of review translation and the co-
training algorithm are described in details in the 
next sections, respectively.  
 
 
Figure 1. Framework of the proposed approach 
3.2 Review Translation 
In order to overcome the language gap, we must 
translate one language into another language. 
Fortunately, machine translation techniques have 
been well developed in the NLP field, though the 
translation performance is far from satisfactory. 
A few commercial machine translation services 
can be publicly accessed, e.g. Google Translate2, 
Yahoo Babel Fish3 and Windows Live Translate4. 
                                                 
2 http://translate.google.com/translate_t 
3 http://babelfish.yahoo.com/translate_txt 
4 http://www.windowslivetranslator.com/ 
Unlabeled 
Chinese 
Reviews
Labeled 
English 
Reviews
Machine 
Translation 
(CN-EN) 
Co-Training 
Machine 
Translation 
(EN-CN) 
Labeled 
Chinese 
Reviews
Unlabeled 
English 
Reviews
Pos\Neg 
Chinese View English View
Test     
Chinese 
Review
Sentiment 
Classifier 
Machine 
Translation 
(CN-EN) 
Test     
English 
Review
Training Phase
Classification Phase
237
In this study, we adopt Google Translate for both 
English-to-Chinese Translation and Chinese-to-
English Translation, because it is one of the 
state-of-the-art commercial machine translation 
systems used today. Google Translate applies 
statistical learning techniques to build a transla-
tion model based on both monolingual text in the 
target language and aligned text consisting of 
examples of human translations between the lan-
guages.  
3.3 The Co-Training Algorithm 
The co-training algorithm (Blum and Mitchell, 
1998) is a typical bootstrapping method, which 
starts with a set of labeled data, and increase the 
amount of annotated data using some amounts of 
unlabeled data in an incremental way. One im-
portant aspect of co-training is that two condi-
tional independent views are required for co-
training to work, but the independence assump-
tion can be relaxed. Till now, co-training has 
been successfully applied to statistical parsing 
(Sarkar, 2001), reference resolution (Ng and 
Cardie, 2003), part of speech tagging (Clark et 
al., 2003), word sense disambiguation (Mihalcea, 
2004) and email classification (Kiritchenko and 
Matwin, 2001). 
In the context of cross-lingual sentiment clas-
sification, each labeled English review or unla-
beled Chinese review has two views of features: 
English features and Chinese features. Here, a 
review is used to indicate both its Chinese ver-
sion and its English version, until stated other-
wise. The co-training algorithm is illustrated in 
Figure 2. In the algorithm, the class distribution 
in the labeled data is maintained by balancing the 
parameter values of p and n at each iteration. 
The intuition of the co-training algorithm is 
that if one classifier can confidently predict the 
class of an example, which is very similar to 
some of labeled ones, it can provide one more 
training example for the other classifier. But, of 
course, if this example happens to be easy to be 
classified by the first classifier, it does not mean 
that this example will be easy to be classified by 
the second classifier, so the second classifier will 
get useful information to improve itself and vice 
versa (Kiritchenko and Matwin, 2001). 
In the co-training algorithm, a basic classifica-
tion algorithm is required to construct Cen and 
Ccn. Typical text classifiers include Support Vec-
tor Machine (SVM), Na?ve Bayes (NB), Maxi-
mum Entropy (ME), K-Nearest Neighbor (KNN), 
etc. In this study, we adopt the widely-used SVM 
classifier (Joachims, 2002). Viewing input data 
as two sets of vectors in a feature space, SVM 
constructs a separating hyperplane in the space 
by maximizing the margin between the two data 
sets. The English or Chinese features used in this 
study include both unigrams and bigrams5 and 
the feature weight is simply set to term fre-
quency6. Feature selection methods (e.g. Docu-
ment Frequency (DF), Information Gain (IG), 
and Mutual Information (MI)) can be used for 
dimension reduction. But we use all the features 
in the experiments for comparative analysis, be-
cause there is no significant performance im-
provement after applying the feature selection 
techniques in our empirical study. The output 
value of the SVM classifier for a review indi-
cates the confidence level of the review?s classi-
fication. Usually, the sentiment polarity of a re-
view is indicated by the sign of the prediction 
value.  
Given: 
- Fen and Fcn are redundantly sufficient 
sets of features, where Fen represents 
the English features, Fcn represents the 
Chinese features; 
- L is a set of labeled training reviews; 
- U is a set of unlabeled reviews; 
Loop for I iterations: 
1. Learn the first classifier Cen from L 
based on Fen; 
2. Use Cen to label reviews from U 
based on Fen; 
3. Choose p positive and n negative the 
most confidently predicted reviews 
Een from U; 
4. Learn the second classifier Ccn from L 
based on Fcn; 
5. Use Ccn to label reviews from U 
based on Fcn; 
6. Choose p positive and n negative the 
most confidently predicted reviews 
Ecn from U; 
7. Removes reviews Een?Ecn from U7; 
8. Add reviews Een?Ecn with the corre-
sponding labels to L; 
Figure 2. The co-training algorithm 
In the training phase, the co-training algorithm 
learns two separate classifiers: Cen and Ccn. 
                                                 
5 For Chinese text, a unigram refers to a Chinese word and a 
bigram refers to two adjacent Chinese words.  
6 Term frequency performs better than TFIDF by our em-
pirical analysis.  
7 Note that the examples with conflicting labels are not in-
cluded in Een?Ecn In other words, if an example is in both 
Een and Ecn, but the labels for the example is conflicting, the 
example will be excluded from Een?Ecn. 
238
Therefore, in the classification phase, we can 
obtain two prediction values for a test review.  
We normalize the prediction values into [-1, 1] 
by dividing the maximum absolute value. Finally, 
the average of the normalized values is used as 
the overall prediction value of the review.  
4 Empirical Evaluation 
4.1 Evaluation Setup 
4.1.1 Data set 
The following three datasets were collected and 
used in the experiments: 
Test Set (Labeled Chinese Reviews): In or-
der to assess the performance of the proposed 
approach, we collected and labeled 886 product 
reviews (451 positive reviews + 435 negative 
reviews) from a popular Chinese IT product web 
site-IT1688. The reviews focused on such prod-
ucts as mp3 players, mobile phones, digital cam-
era and laptop computers.  
Training Set (Labeled English Reviews): 
There are many labeled English corpora avail-
able on the Web and we used the corpus con-
structed for multi-domain sentiment classifica-
tion (Blitzer et al, 2007)9, because the corpus 
was large-scale and it was within similar do-
mains as the test set. The dataset consisted of 
8000 Amazon product reviews (4000 positive 
reviews + 4000 negative reviews) for four differ-
ent product types: books, DVDs, electronics and 
kitchen appliances.  
Unlabeled Set (Unlabeled Chinese Reviews): 
We downloaded additional 1000 Chinese product 
reviews from IT168 and used the reviews as the 
unlabeled set. Therefore, the unlabeled set and 
the test set were in the same domain and had 
similar underlying feature distributions.  
Each Chinese review was translated into Eng-
lish review, and each English review was trans-
lated into Chinese review. Therefore, each re-
view has two independent views: English view 
and Chinese view. A review is represented by 
both its English view and its Chinese view.  
Note that the training set and the unlabeled set 
are used in the training phase, while the test set is 
blind to the training phase. 
4.1.2 Evaluation Metric 
We used the standard precision, recall and F-
measure to measure the performance of positive 
and negative class, respectively, and used the 
                                                 
8 http://www.it168.com 
9 http://www.cis.upenn.edu/~mdredze/datasets/sentiment/ 
accuracy metric to measure the overall perform-
ance of the system. The metrics are defined the 
same as in general text categorization. 
4.1.3 Baseline Methods 
In the experiments, the proposed co-training ap-
proach (CoTrain) is compared with the following 
baseline methods: 
SVM(CN): This method applies the inductive 
SVM with only Chinese features for sentiment 
classification in the Chinese view. Only English-
to-Chinese translation is needed. And the unla-
beled set is not used.  
SVM(EN): This method applies the inductive 
SVM with only English features for sentiment 
classification in the English view. Only Chinese-
to-English translation is needed. And the unla-
beled set is not used. 
SVM(ENCN1): This method applies the in-
ductive SVM with both English and Chinese fea-
tures for sentiment classification in the two 
views. Both English-to-Chinese and Chinese-to-
English translations are required. And the unla-
beled set is not used. 
SVM(ENCN2): This method combines the re-
sults of SVM(EN) and SVM(CN) by averaging 
the prediction values in the same way with the 
co-training approach.  
TSVM(CN): This method applies the trans-
ductive SVM with only Chinese features for sen-
timent classification in the Chinese view. Only 
English-to-Chinese translation is needed. And 
the unlabeled set is used.  
TSVM(EN): This method applies the trans-
ductive SVM with only English features for sen-
timent classification in the English view. Only 
Chinese-to-English translation is needed. And 
the unlabeled set is used. 
TSVM(ENCN1): This method applies the 
transductive SVM with both English and Chinese 
features for sentiment classification in the two 
views. Both English-to-Chinese and Chinese-to-
English translations are required. And the unla-
beled set is used.  
TSVM(ENCN2): This method combines the 
results of TSVM(EN) and TSVM(CN) by aver-
aging the prediction values. 
Note that the first four methods are straight-
forward methods used in previous work, while 
the latter four methods are strong baselines be-
cause the transductive SVM has been widely 
used for improving the classification accuracy by 
leveraging additional unlabeled examples.  
239
4.2 Evaluation Results 
4.2.1 Method Comparison 
In the experiments, we first compare the pro-
posed co-training approach (I=40 and p=n=5) 
with the eight baseline methods. The three pa-
rameters in the co-training approach are empiri-
cally set by considering the total number (i.e. 
1000) of the unlabeled Chinese reviews. In our 
empirical study, the proposed approach can per-
form well with a wide range of parameter values, 
which will be shown later. Table 1 shows the 
comparison results.  
Seen from the table, the proposed co-training 
approach outperforms all eight baseline methods 
over all metrics. Among the eight baselines, the 
best one is TSVM(ENCN2), which combines the 
results of two transductive SVM classifiers. Ac-
tually, TSVM(ENCN2) is similar to CoTrain 
because CoTrain also combines the results of 
two classifiers in the same way. However, the 
co-training approach can train two more effective 
classifiers, and the accuracy values of the com-
ponent English and Chinese classifiers are 0.775 
and 0.790, respectively, which are higher than 
the corresponding TSVM classifiers. Overall, the 
use of transductive learning and the combination 
of English and Chinese views are beneficial to 
the final classification accuracy, and the co-
training approach is more suitable for making 
use of the unlabeled Chinese reviews than the 
transductive SVM.  
4.2.2 Influences of Iteration Number (I) 
Figure 3 shows the accuracy curve of the co-
training approach (Combined Classifier) with 
different numbers of iterations. The iteration 
number I is varied from 1 to 80. When I is set to 
1, the co-training approach is degenerated into 
SVM(ENCN2). The accuracy curves of the com-
ponent English and Chinese classifiers learned in 
the co-training approach are also shown in the 
figure. We can see that the proposed co-training 
approach can outperform the best baseline-
TSVM(ENCN2) after 20 iterations. After a large 
number of iterations, the performance of the co-
training approach decreases because noisy train-
ing examples may be selected from the remain-
ing unlabeled set. Finally, the performance of the 
approach does not change any more, because the 
algorithm runs out of all possible examples in the 
unlabeled set. Fortunately, the proposed ap-
proach performs well with a wide range of itera-
tion numbers. We can also see that the two com-
ponent classifier has similar trends with the co-
training approach. It is encouraging that the com-
ponent Chinese classifier alone can perform bet-
ter than the best baseline when the iteration 
number is set between 40 and 70. 
4.2.3 Influences of Growth Size (p, n) 
Figure 4 shows how the growth size at each it-
eration (p positive and n negative confident ex-
amples) influences the accuracy of the proposed 
co-training approach. In the above experiments, 
we set p=n, which is considered as a balanced 
growth. When p differs very much from n, the 
growth is considered as an imbalanced growth. 
Balanced growth of (2, 2), (5, 5), (10, 10) and 
(15, 15) examples and imbalanced growth of (1, 
5), (5, 1) examples are compared in the figure. 
We can see that the performance of the co-
training approach with the balanced growth can 
be improved after a few iterations. And the per-
formance of the co-training approach with large 
p and n will more quickly become unchanged, 
because the approach runs out of the limited ex-
amples in the unlabeled set more quickly. How-
ever, the performance of the co-training ap-
proaches with the two imbalanced growths is 
always going down quite rapidly, because the 
labeled unbalanced examples hurt the perform-
ance badly at each iteration.  
 
Positive Negative Total Method Precision Recall F-measure Precision Recall F-measure Accuracy
SVM(CN) 0.733 0.865 0.793 0.828 0.674 0.743 0.771 
SVM(EN) 0.717 0.803 0.757 0.766 0.671 0.716 0.738 
SVM(ENCN1) 0.744 0.820 0.781 0.792 0.708 0.748 0.765 
SVM(ENCN2) 0.746 0.847 0.793 0.816 0.701 0.754 0.775 
TSVM(CN) 0.724 0.878 0.794 0.838 0.653 0.734 0.767 
TSVM(EN) 0.732 0.860 0.791 0.823 0.674 0.741 0.769 
TSVM(ENCN1) 0.743 0.878 0.805 0.844 0.685 0.756 0.783 
TSVM(ENCN2) 0.744 0.896 0.813 0.863 0.680 0.761 0.790 
CoTrain          
(I=40; p=n=5) 0.768 0.905 0.831 0.879 0.717 0.790 0.813 
Table 1. Comparison results  
240
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.8
0.81
0.82
1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Iteration Number (I )
A
cc
ur
ac
y
English Classifier(CoTrain) Chinese Classifier(CoTrain)
Combined Classifier(CoTrain) TSVM(ENCN2)
 
Figure 3. Accuracy vs. number of iterations for co-training (p=n=5) 
0.5
0.55
0.6
0.65
0.7
0.75
0.8
1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Iteration Number (I )
A
cc
ur
ac
y
(p=2,n=2) (p=5,n=5) (p=10,n=10)
(p=15,n=15) (p=1,n=5) (p=5,n=1)
 
Figure 4. Accuracy vs. different (p, n) for co-training 
0.76
0.77
0.78
0.79
0.8
0.81
0.82
25% 50% 75% 100%
Feature size
A
cc
ur
ac
y
TSVM(ENCN1) TSVM(ENCN2) CoTrain (I=40; p=n=5)
 
Figure 5. Influences of feature size 
 
241
4.2.4 Influences of Feature Selection 
In the above experiments, all features (unigram + 
bigram) are used. As mentioned earlier, feature 
selection techniques are widely used for dimen-
sion reduction. In this section, we further con-
duct experiments to investigate the influences of 
feature selection techniques on the classification 
results. We use the simple but effective docu-
ment frequency (DF) for feature selection.  Fig-
ures 6 show the comparison results of different 
feature sizes for the co-training approach and 
two strong baselines. The feature size is meas-
ured as the proportion of the selected features 
against the total features (i.e. 100%).   
We can see from the figure that the feature se-
lection technique has very slight influences on 
the classification accuracy of the methods. It can 
be seen that the co-training approach can always 
outperform the two baselines with different fea-
ture sizes. The results further demonstrate the 
effectiveness and robustness of the proposed co-
training approach.  
5 Conclusion and Future Work  
In this paper, we propose to use the co-training 
approach to address the problem of cross-lingual 
sentiment classification. The experimental results 
show the effectiveness of the proposed approach. 
In future work, we will improve the sentiment 
classification accuracy in the following two ways: 
1) The smoothed co-training approach used in 
(Mihalcea, 2004) will be adopted for sentiment 
classification. The approach has the effect of 
?smoothing? the learning curves. During the 
bootstrapping process of smoothed co-training, 
the classifier at each iteration is replaced with a 
majority voting scheme applied to all classifiers 
constructed at previous iterations.  
2) The feature distributions of the translated 
text and the natural text in the same language are 
still different due to the inaccuracy of the ma-
chine translation service. We will employ the 
structural correspondence learning (SCL) domain 
adaption algorithm used in (Blitzer et al, 2007) 
for linking the translated text and the natural text.  
Acknowledgments 
This work was supported by NSFC (60873155), 
RFDP (20070001059), Beijing Nova Program 
(2008B03), National High-tech R&D Program 
(2008AA01Z421) and NCET (NCET-08-0006). 
We also thank the anonymous reviewers for their 
useful comments. 
References  
A. Andreevskaia and S. Bergler. 2008. When special-
ists and generalists work together: overcoming 
domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT. 
C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008. 
Multilingual subjectivity analysis using machine 
translation. In Proceedings of EMNLP-2008.  
N. Bel, C. H. A. Koster, and M. Villegas. 2003. 
Cross-lingual text categorization. In Proceedings of 
ECDL-03. 
J. Blitzer, M. Dredze and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: do-
main adaptation for sentiment classification. In 
Proceedings of ACL-07. 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with cotraining. In Proceedings 
of COLT-98. 
S. Brody, R. Navigli and M. Lapata. 2006. Ensemble 
methods for unsupervised WSD. In Proceedings of 
COLING-ACL-2006. 
S. Clark, J. R. Curran, and M. Osborne. 2003. Boot-
strapping POS taggers using unlabelled data. In 
Proceedings of CoNLL-2003. 
W. Dai, G.-R. Xue, Q. Yang, Y. Yu. 2007a. Transfer-
ring Na?ve Bayes Classifiers for text classification. 
In Proceedings of AAAI-07. 
W. Dai, G.-R. Xue, Q. Yang, Y. Yu. 2007b. Co-
clustering based classification for out-of-domain 
documents. In Proceedings of KDD-07.  
H. Daum?III and D. Marcu. 2006. Domain adaptation 
for statistical classifiers. Journal of Artificial Intel-
ligence Research, 26:101?126. 
A. Devitt and K. Ahmad. 2007. Sentiment polarity 
identification in financial news: a cohesion-based 
approach. In Proceedings of ACL2007. 
T. G. Dietterich. 1997. Machine learning research: 
four current directions. AI Magazine, 18(4), 1997. 
A. Gliozzo and C. Strapparava. 2005. Cross language 
text categorization by acquiring multilingual do-
main models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and 
Using Parallel Texts.  
K. Hiroshi, N. Tetsuya and W. Hideo. 2004. Deeper 
sentiment analysis using machine translation tech-
nology. In Proceedings of COLING-04. 
J. Jiang and C. Zhai. 2007. A two-stage approach to 
domain adaptation for statistical classifiers. In Pro-
ceedings of CIKM-07.  
T. Joachims. 1999. Transductive inference for text 
classification using support vector machines. In 
Proceedings of ICML-99. 
242
T. Joachims. 2002. Learning to classify text using 
support vector machines. Dissertation, Kluwer, 
2002.   
A. Kennedy and D. Inkpen. 2006. Sentiment classifi-
cation of movie reviews using contextual valence 
shifters. Computational Intelligence, 22(2):110-
125. 
S.-M. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of COLING-04. 
S. Kiritchenko and S. Matwin. 2001. Email classifica-
tion with co-training. In Proceedings of the 2001 
Conference of the Centre for Advanced Studies on 
Collaborative Research.  
L.-W. Ku, Y.-T. Liang and H.-H. Chen. 2006. Opin-
ion extraction, summarization and tracking in news 
and blog corpora. In Proceedings of AAAI-2006. 
J. Li and M. Sun. 2007. Experimental study on senti-
ment classification of Chinese review using ma-
chine learning techniques. In Proceeding of IEEE-
NLPKE-07. 
X. Ling, W. Dai, Y. Jiang, G.-R. Xue, Q. Yang, and Y. 
Yu. 2008. Can Chinese Web pages be classified 
with English data source? In Proceedings of 
WWW-08. 
B. Liu, M. Hu and J. Cheng. 2005. Opinion observer: 
Analyzing and comparing opinions on the web. In 
Proceedings of WWW-2005. 
R. McDonald, K. Hannan, T. Neylon, M. Wells and J. 
Reynar. 2007. Structured models for fine-to-coarse 
sentiment analysis. In Proceedings of ACL-07. 
R. Mihalcea. 2004. Co-training and self-training for 
word sense disambiguation. In Proceedings of 
CONLL-04.  
R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning 
multilingual subjective language via cross-lingual 
projections. In Proceedings of ACL-2007. 
T. Mullen and N. Collier. 2004. Sentiment analysis 
using support vector machines with diverse infor-
mation sources. In Proceedings of EMNLP-04. 
V. Ng and C. Cardie. 2003. Weakly supervised natu-
ral language learning without redundant views. In 
Proceedings of HLT-NAACL-03. 
K. Nigam, A. K. McCallum, S. Thrun, and T. 
Mitchell. 2000. Text Classification from Labeled 
and Unlabeled Documents using EM. Machine 
Learning, 39(2-3):103?134. 
B. Pang, L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? sentiment classification using machine learn-
ing techniques. In Proceedings of EMNLP-02. 
B. Pang and L. Lee. 2004. A sentimental education: 
sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of 
ACL-04. 
J. Read. 2005. Using emoticons to reduce dependency 
in machine learning techniques for sentiment clas-
sification. In Proceedings of ACL-05. 
L. Rigutini, M. Maggini and B. Liu. 2005. An EM 
based training algorithm for cross-language text 
categorization. In Proceedings of WI-05.  
A. Sarkar. 2001. Applying cotraining methods to sta-
tistical parsing. In Proceedings of NAACL-2001. 
I. Titov and R. McDonald. 2008. A joint model of text 
and aspect ratings for sentiment summarization. In 
Proceedings of ACL-08:HLT. 
B. K. Y. Tsou, R. W. M. Yuen, O. Y. Kwong, T. B. Y. 
La and W. L. Wong. 2005. Polarity classification 
of celebrity coverage in the Chinese press. In Pro-
ceedings of International Conference on Intelli-
gence Analysis. 
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of ACL-2002. 
X. Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of EMNLP-2008. 
T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proceedings of HLT/EMNLP-05. 
G.-R. Xue, W. Dai, Q. Yang, Y. Yu. 2008. Topic-
bridged PLSA for cross-domain text classification. 
In Proceedings of SIGIR-08. 
Q. Ye, W. Shi and Y. Li. 2006. Sentiment classifica-
tion for movie reviews in Chinese by improved 
semantic oriented approach. In Proceedings of 39th 
Hawaii International Conference on System Sci-
ences, 2006. 
243
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 969?976
Manchester, August 2008
CollabRank: Towards a Collaborative Approach to Single-Document 
Keyphrase Extraction 
Xiaojun Wan and Jianguo Xiao 
Institute of Computer Science and Technology 
Peking University, Beijing 100871, China 
{wanxiaojun,xiaojianguo}@icst.pku.edu.cn 
 
Abstract 
Previous methods usually conduct the 
keyphrase extraction task for single docu-
ments separately without interactions for 
each document, under the assumption 
that the documents are considered inde-
pendent of each other. This paper pro-
poses a novel approach named Col-
labRank to collaborative single-document 
keyphrase extraction by making use of 
mutual influences of multiple documents 
within a cluster context. CollabRank is 
implemented by first employing the clus-
tering algorithm to obtain appropriate 
document clusters, and then using the 
graph-based ranking algorithm for col-
laborative single-document keyphrase ex-
traction within each cluster. Experimental 
results demonstrate the encouraging per-
formance of the proposed approach. Dif-
ferent clustering algorithms have been 
investigated and we find that the system 
performance relies positively on the qual-
ity of document clusters. 
1 Introduction 
A keyphrase is defined as a meaningful and sig-
nificant expression consisting of one or more 
words in a document. Appropriate keyphrases 
can be considered as a highly condensed sum-
mary for a document, and they can be used as a 
label for the document to supplement or replace 
the title or summary, thus facilitating users? fast 
browsing and reading. Moreover, document key-
phrases have been successfully used in the fol-
lowing IR and NLP tasks: document indexing 
(Gutwin et al, 1999), document classification 
(Krulwich and Burkey, 1996), document cluster-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
ing (Zhang et al, 2004; Hammouda et al, 2005) 
and document summarization (Berger and Mittal, 
2000; Buyukkokten et al, 2001). 
Keyphrases are usually manually assigned by 
authors, especially for journal or conference arti-
cles. However, the vast majority of documents 
(e.g. news articles, magazine articles) do not 
have keyphrases, therefore it is beneficial to 
automatically extract a few keyphrases from a 
given document to deliver the main content of 
the document. Here, keyphrases are selected 
from within the body of the input document, 
without a predefined list (i.e. controlled vocabu-
lary). Most previous work focuses on keyphrase 
extraction for journal or conference articles, 
while this paper focus on keyphrase extraction 
for news articles because news article is one of 
the most popular document genres on the web 
and most news articles have no author-assigned 
keyphrases. 
Very often, keyphrases of all single documents 
in a document set are required to be extracted. 
However, all previous methods extract key-
phrases for a specified document based only on 
the information contained in that document, such 
as the phrase?s TFIDF, position and other syntac-
tic information in the document. One common 
assumption of existing methods is that the docu-
ments are independent of each other. Hence the 
keyphrase extraction task is conducted separately 
without interactions for each document. However, 
the multiple documents within an appropriate 
cluster context usually have mutual influences 
and contain useful clues which can help to ex-
tract keyphrases from each other. For example, 
two documents about the same topic ?earth-
quake? would share a few common phrases, e.g. 
?earthquake?, ?victim?, and they can provide 
additional knowledge for each other to better 
evaluate and extract salient keyphrases from each 
other. The idea is borrowed from human?s per-
ception that a user would better understand a 
topic expressed in a document if the user reads 
more documents about the same topic.  
969
Based on the above assumption, we propose a 
novel framework for collaborative single-
document keyphrase extraction by making use of 
the additional information from multiple docu-
ments within an appropriate cluster context. The 
collaborative framework for keyphrase extraction 
consists of the step of obtaining the cluster con-
text and the step of collaborative keyphrase ex-
traction in each cluster. In this study, the cluster 
context is obtained by applying the clustering 
algorithm on the document set, and we have in-
vestigated how the cluster context influences the 
keyphrase extraction performance by employing 
different clustering algorithms. The graph-based 
ranking algorithm is employed for collaborative 
keyphrase extraction for each document in a 
specified cluster. Instead of making only use of 
the word relationships in a single document, the 
algorithm can incorporate the ?voting? or ?rec-
ommendations? between words in all the docu-
ments of the cluster, thus making use of the 
global information existing in the cluster context. 
The above implementation of the collaborative 
framework is denoted as CollabRank in this pa-
per. 
Experiments have been performed on a dataset 
consisting of 308 news articles with human-
annotated keyphrases, and the results demon-
strate the good effectiveness of the CollabRank 
approach. We also find that the extraction per-
formance is positively correlated with the quality 
of cluster context, and existing clustering algo-
rithms can yield appropriate cluster context for 
collaborative keyphrase extraction. 
The rest of this paper is organized as follows: 
Section 2 introduces the related work. The pro-
posed CollabRank is described in detail in Sec-
tion 3. Empirical evaluation is demonstrated in 
Section 4 and lastly we conclude this paper in 
Section 5. 
2 Related Work 
The methods for keyphrase (or keyword) extrac-
tion can be roughly categorized into either unsu-
pervised or supervised.  
Unsupervised methods usually involve assign-
ing a saliency score to each candidate phrases by 
considering various features. Krulwich and Bur-
key (1996) use heuristics based on syntactic 
clues to extract keyphrases from a document. 
Barker and Cornacchia (2000) propose a simple 
system for choosing noun phrases from a docu-
ment as keyphrases. Mu?oz (1996) uses an unsu-
pervised learning algorithm to discover two-word 
keyphrases. The algorithm is based on Adaptive 
Resonance Theory (ART) neural networks. 
Steier and Belew (1993) use the mutual informa-
tion statistics to discover two-word keyphrases. 
Tomokiyo and Hurst (2003) use pointwise KL-
divergence between multiple language models 
for scoring both phraseness and informativeness 
of phrases. More recently, Mihalcea and Tarau 
(2004) propose the TextRank model to rank key-
words based on the co-occurrence links between 
words. Such algorithms make use of ?voting? or 
?recommendations? between words to extract 
keyphrases. 
Supervised machine learning algorithms have 
been proposed to classify a candidate phrase into 
either keyphrase or not. GenEx (Turney, 2000) 
and Kea (Frank et al, 1999; Witten et al, 1999) 
are two typical systems, and the most important 
features for classifying a candidate phrase are the 
frequency and location of the phrase in the 
document. More linguistic knowledge has been 
explored by Hulth (2003). Statistical associations 
between keyphrases have been used to enhance 
the coherence of the extracted keyphrases (Tur-
ney, 2003). Song et al (2003) present an infor-
mation gain-based keyphrase extraction system 
called KPSpotter. Medelyan and Witten (2006) 
propose KEA++ that enhances automatic key-
phrase extraction by using semantic information 
on terms and phrases gleaned from a domain-
specific thesaurus. Nguyen and Kan (2007) focus 
on keyphrase extraction in scientific publications 
by using new features that capture salient mor-
phological phenomena found in scientific key-
phrases.  
The tasks of keyphrase extraction and docu-
ment summarization are similar and thus they 
have been conducted in a uniform framework. 
Zha (2002) proposes a method for simultaneous 
keyphrase extraction and text summarization by 
using the heterogeneous sentence-to-word rela-
tionships. Wan et al (2007a) propose an iterative 
reinforcement approach to simultaneous key-
phrase extraction and text summarization. Other 
related works include web page keyword extrac-
tion (Kelleher and Luz, 2005; Zhang et al, 2005; 
Chen et al, 2005), advertising keywords finding 
(Yih et al, 2006). 
To the best of our knowledge, all previous 
work conducts the task of keyphrase extraction 
for each single document independently, without 
making use of the collaborative knowledge in 
multiple documents. We focus on unsupervised 
methods in this study. 
970
3 The Proposed CollabRank Approach 
3.1 Framework Description 
Given a document set for keyphrase extraction of 
each single document, CollabRank first employs 
the clustering algorithm to group the documents 
into a few clusters. The documents within each 
cluster are expected to be topic-related and each 
cluster can be considered as a context for any 
document in the cluster. Given a document clus-
ter, CollabRank makes use of the global word 
relationships in the cluster to evaluate and rank 
candidate phrases for each single document in 
the cluster based on the graph-based ranking al-
gorithm. Figure 1 gives the framework of the 
proposed approach.  
1. Document Clustering: Group the documents in the 
document set D into a few clusters using the cluster-
ing algorithm;  
2. Collaborative Keyphrase Extraction: For each 
cluster C, perform the following steps respectively 
to extract keyphrases for single documents in the 
cluster in a batch mode: 
1) Cluster-level Word Evaluation: Build a 
global affinity graph G based on all candidate 
words restricted by syntactic filters in the documents 
of the given cluster C, and employ the graph-ranking 
based algorithm to compute the cluster-level sali-
ency score for each word. 
2) Document-level Keyphase Extraction: For 
any single document d in the cluster, evaluate the 
candidate phrases in the document based on the 
scores of the words contained in the phrases, and fi-
nally choose a few phrases with highest scores as 
the keyphrases of the document. 
Figure 1. The Framework of CollabRank 
In the first step of the above framework, dif-
ferent clustering algorithms will yield different 
clusters. The documents in a high-quality cluster 
are usually deemed to be highly topic-related (i.e. 
appropriate cluster context), while the documents 
in a low-quality cluster are usually not topic-
related (i.e. inappropriate cluster context). The 
quality of a cluster will influence the reliability 
of the contextual information for evaluating the 
words in the cluster. A number of clustering al-
gorithms will be investigated in the experiments, 
including the agglomerative algorithm (both av-
erage-link and complete-link), the divisive algo-
rithm, and the kmeans algorithm (Jain et al, 
1999), whose details will be described in the 
evalution section. 
In the second step of the above framework, 
substep 1) aims to evaluate all candidate words 
in the cluster based on the graph-based ranking 
algorithm. The global affinity graph aims to re-
flect the cluster-level co-occurrence relationships 
between all candidate words in the documents of 
the given cluster. The saliency scores of the 
words are computed based on the global affinity 
graph to indicate how much information about 
the main topic the words reflect. Substep 2) aims 
to evaluate candidate phrases of each single 
document based on the cluster-level word scores, 
and then choose a few salient phrases as key-
phrases of the document. Substep 1) is performed 
on all documents in the cluster in order to evalu-
ate the words from a global perspective, while 
substep 2) is performed on each single document 
in order to extract keyphrases from a local per-
spective. A keyphrase of a document is expected 
to include highly salient words. We can see that 
the keyphrase extraction tasks are conducted in a 
batch mode for each cluster. The substeps of 1) 
and 2) will be described in next sections respec-
tively. If substep 1) is performed on each single 
document without considering the cluster context, 
the approach is degenerated into the simple Tex-
tRank model (Mihalcea and Tarau, 2004), which 
is denoted as SingleRank in this paper.  
It is noteworthy that in addition to the graph-
based ranking algorithm, other keyphrase extrac-
tion methods can also be integrated in the pro-
posed collaborative framework to exploit the col-
laborative knowledge in the cluster context.  
3.2 Cluster-Level Word Evaluation 
Like the PageRank algorithm (Page et al, 1998), 
the graph-based ranking algorithm employed in 
this study is essentially a way of deciding the 
importance of a vertex within a graph based on 
global information recursively drawn from the 
entire graph. The basic idea is that of ?voting? or 
?recommendation? between the vertices. A link 
between two vertices is considered as a vote cast 
from one vertex to the other vertex. The score 
associated with a vertex is determined by the 
votes that are cast for it, and the score of the ver-
tices casting these votes.  
Formally, given a specified cluster C, let G=(V, 
E) be an undirected graph to reflect the relation-
ships between words in the cluster. V is the set of 
vertices and each vertex is a candidate word2 in 
the cluster. Because not all words in the docu-
ments are good indicators of keyphrases, the 
words added to the graph are restricted with syn-
tactic filters, i.e., only the words with a certain 
part of speech are added. As in Mihalcea and 
Tarau (2004), the documents are tagged by a 
                                                 
2 The original words are used without stemming. 
971
POS tagger, and only the nouns and adjectives 
are added into the vertex set3. E is the set of 
edges, which is a subset of V?V. Each edge eij in 
E is associated with an affinity weight aff(vi,vj) 
between words vi and vj. The weight is computed 
based on the co-occurrence relation between the 
two words, controlled by the distance between 
word occurrences. The co-occurrence relation 
can express cohesion relationships between 
words. Two vertices are connected if the corre-
sponding words co-occur at least once within a 
window of maximum k words, where k can be set 
anywhere from 2 to 20 words. The affinity 
weight aff(vi,vj) is simply set to be the count of 
the controlled co-occurrences between the words 
vi and vj in the whole cluster as follows: 
)()( ji
Cd
dji ,vvcount,vvaff ?
?
=  (1) 
where countd(vi,vj) is the count of the controlled 
co-occurrences between words vi and vj  in docu-
ment d.  
The graph is built based on the whole cluster 
and it is called Global Affinity Graph.  The big-
gest difference between CollabRank and 
SingleRank is that SingleRank builds a local 
graph based on each single document.  
We use an affinity matrix M to describe G 
with each entry corresponding to the weight of 
an edge in the graph. M = (Mi,j)|V|?|V| is defined as 
follows: 
 
otherwise0
;  and   with  links if)(
??
? ?
=
,   
jiv v,   ,vvaff
M jijii,j
 
(2) 
Then M is normalized to M~ as follows to make 
the sum of each row equal to 1: 
??
??
?
?
=
??
==
otherwise0
0if~
|V|
1
|V|
1
   ,             
M ,   MM
M j
i,j
j
i,ji,j
i,j
 
 
(3) 
Based on the global affinity graph G, the clus-
ter-level saliency score WordScoreclus(vi) for 
word vi can be deduced from those of all other 
words linked with it and it can be formulated in a 
recursive form as in the PageRank algorithm: 
?
?
?
+??=
iall j
j,ijclusiclus V
MvWordScorevWordScore
||
)1(~)()( ??  
(4) 
And the matrix form is: 
e
V
?M? T r
rr
||
)1(~ ?? ?+=   (5) 
                                                 
3 The corresponding POS tags of the candidate words 
include ?JJ?, ?NN?, ?NNS?, ?NNP?, ?NNPS?. We 
used the Stanford log-linear POS tagger (Toutanova 
and Manning, 2000) in this study.  
where 1||)]([ ?= Viclus vWordScore?
r
is the vector of 
word saliency scores. er  is a vector with all ele-
ments equaling to 1. ? is the damping factor usu-
ally set to 0.85, as in the PageRank algorithm. 
The above process can be considered as a 
Markov chain by taking the words as the states 
and the corresponding transition matrix is given 
by TT ee
|V|
M rr)1(~ ?? ?+ . The stationary probabil-
ity distribution of each state is obtained by the 
principal eigenvector of the transition matrix.  
For implementation, the initial scores of the 
words are set to 1 and the iteration algorithm in 
Equation (4) is adopted to compute the new 
scores of the words. Usually the convergence of 
the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any words falls below a 
given threshold (0.0001 in this study).  
For SingleRank, the saliency score Word-
Scoredoc(vi) for word vi is computed in the same 
iterative way based on the local graph for the 
single document.  
3.3 Document-Level Keyphrase Extraction 
After the scores of all candidate words in the 
cluster have been computed, candidate phrases 
are selected and evaluated for each single docu-
ment in the cluster. The candidate words (i.e. 
nouns and adjectives) of a specified document d 
in the cluster, which is a subset of V, are marked 
in the document text, and sequences of adjacent 
candidate words are collapsed into a multi-word 
phrase. The phrases ending with an adjective are 
not allowed, and only the phrases ending with a 
noun are collected as the candidate phrases for 
the document. For instance, in the following sen-
tence: ?Mad/JJ cow/NN disease/NN has/VBZ 
killed/VBN 10,000/CD cattle/NNS?, the candi-
date phrases are ?Mad cow disease? and ?cattle?. 
The score of a candidate phrase pi is computed 
by summing the cluster-level saliency scores of 
the words contained in the phrase. 
 ?
?
=
ij pv
jclusi vWordScorepePhraseScor )()(  (6) 
All the candidate phrases in the document are 
ranked in decreasing order of the phrase scores 
and the top n phrases are selected as the key-
phrases of the document. n ranges from 1 to 20 in 
this study. Similarly for SingleRank, the phrase 
score is computed based on the document-level 
saliency scores of the words.  
972
4 Empirical Evaluation 
4.1 Data Set 
To our knowledge, there is no gold standard 
news dataset with assigned keyphrases for 
evaluation. So we manually annotated the 
DUC2001 dataset   (Over, 2001) and used the 
annotated dataset for evaluation in this study. 
The dataset was originally used for document 
summarization. It consisted of 309 news articles 
collected from TREC-9, in which two articles 
were duplicate (i.e. d05a\FBIS-41815 and 
d05a\FBIS-41815~). The average length of the 
documents was 740 words. Two graduate stu-
dents were employed to manually label the key-
phrases for each document. At most 10 key-
phrases could be assigned to each document. The 
annotation process lasted two weeks. The Kappa 
statistic for measuring inter-agreement among 
annotators was 0.70. And the annotation conflicts 
between the two subjects were solved by discus-
sion. Finally, 2488 keyphrases were labeled for 
the dataset. The average keyphrase number per 
document was 8.08 and the average word num-
ber per keyphrase was 2.09.  
The articles have been grouped into 30 clusters 
manually by NIST annotators for multi-
document summarization, and the documents 
within each cluster were topic-related or relevant. 
The manually labeled clusters were considered as 
the ground truth clusters or gold clusters. In order 
to investigate existing clustering algorithms, the 
documents in the clusters were mixed together to 
form the whole document set for automatic clus-
tering. 
4.2 Document Clustering Algorithm 
In the experiments, several popular clustering 
algorithms and random clustering algorithms are 
explored to produce cluster contexts. Note that 
we have already known the number (i.e. 30) of 
the clusters for the dataset beforehand and thus 
we simply use it as input for the following clus-
tering algorithms4.  
Gold Standard Clustering: It is a pseudo 
clustering algorithm by manually grouping the 
documents. We use the ground truth clusters as 
the upperbound of the following automatic clus-
tering algorithms.  
Kmeans Clustering: It is a partition based 
clustering algorithm. The algorithm randomly 
                                                 
4 How to obtain the number of desired clusters is not 
the focus of this study. 
selects 30 documents as the initial centroids of 
the 30 clusters and then iteratively assigns all 
documents to the closest cluster, and recomputes 
the centroid of each cluster, until the centroids do 
not change. The similarity between a document 
and a cluster centroid is computed using the 
standard Cosine measure.   
Agglomerative (AverageLink) Clustering: It 
is a bottom-up hierarchical clustering algorithm 
and starts with the points as individual clusters 
and, at each step, merges the most similar or 
closest pair of clusters, until the number of the 
clusters reduces to the desired number 30. The 
similarity between two clusters is computed us-
ing the AverageLink method, which computes 
the average of the Cosine similarity values be-
tween any pair of documents belonging to the 
two clusters respectively as follows:   
21
1 1
21
)(
)(
cc
,ddsim
,ccsim
m
i
n
j
ji
?
=
??
= =  
 
(7) 
where di, dj are two documents in cluster c1 and 
cluster c2 respectively, and |c1| and |c2| are respec-
tively the numbers of documents in clusters c1 
and c2. 
Agglomerative (CompleteLink) Clustering: 
It differs from the above agglomerative (Aver-
ageLink) clustering algorithm only in that the 
similarity between two clusters is computed us-
ing the CompleteLink method, which computes 
the minimum of the Cosine similarity values be-
tween any pair of documents belonging to the 
two clusters respectively as follows:   
)}({min)(
2121 jic,dcd
,ddsim,ccsim
ji ??
=  (8) 
Divisive Clustering: It is a top-down hierar-
chical clustering algorithm and starts with one, 
all-inclusive cluster and, at each step, splits the 
largest cluster (i.e. the cluster with most docu-
ments) into two small clusters using the Kmeans 
algorithm until the number of clusters increases 
to the desired number 30.  
Random Clustering: It produces 30 clusters 
by randomly assigning each document into one 
of the k clusters.  Three different randomization 
processes are performed and we denote them as 
Random1, Random2 and Random3, respectively. 
CollabRank relies on the clustering algorithm 
for document clustering, and the combination of 
CollabRank and any clustering algorithm will be 
investigated.  
4.3 Evaluation Metric 
For evaluation of document clustering results, we 
adopt the widely used F-Measure to measure the 
973
performance of the clustering algorithm (i.e. the 
quality of the clusters) by comparing the pro-
duced clusters with the gold clusters (classes) 
(Jain et al, 1999).  
For evaluation of keyphrase extraction results, 
the automatic extracted keyphrases are compared 
with the manually labeled keyphrases. The words 
are converted to their corresponding basic forms 
using word stemming before comparison. The 
precision p=countcorrect/countsystem, recall 
r=countcorrect/counthuman, F-measure (F=2pr/(p+r)) 
are used as evaluation metrics, where countcorrect 
is the total number of correct keyphrases ex-
tracted by the system, and countsystem is the total 
number of automatic extracted keyphrases, and 
counthuman is the total number of human-labeled 
keyphrases. 
4.4 Evaluation Results 
First of all, we show the document clustering 
results in Table 1. The gold standard clustering 
result is the upperbound of all automatic cluster-
ing results. Seen from the table, all the four 
popular clustering algorithms (i.e. CompleteLink, 
AverageLink, KMeans and Divisive) perform 
much better than the three random clustering al-
gorithms (i.e. Random1, Random2 and Ran-
dom3). Different clustering results lead to differ-
ent document relationships and a high-quality 
cluster produced by popular algorithms is 
deemed to build an appropriate cluster context 
for collaborative keyphrase extraction. 
Clustering Algorithm F-Measure 
Gold 1.000 
CompleteLink 0.907 
AverageLink 0.877 
Divisive 0.924 
Kmeans 0.866 
Random1 0.187 
Random2 0.189 
Random3 0.183 
Table 1. Clustering Results  
Now we show the results for keyphrase extrac-
tion. In the experiments, the keyphrase number is 
typically set to 10 and the co-occurrence window 
size is also simply set to 10. Table 2 gives the 
comparison results of baseline methods and the 
proposed CollabRank methods with different 
clustering algorithms. The TFIDF baseline com-
putes the word scores for each single document 
based on the word?s TFIDF value. The SingleR-
ank baseline computes the word scores for each 
single document based on the graph-based rank-
ing algorithm. The two baselines do not make 
use of the cluster context.  
Seen from Table 2, the CollabRank methods 
with the gold standard clustering algorithm or 
popular clustering algorithms (i.e. Kmeans, 
CompleteLink, AverageLink and Divisive) per-
form much better than the baseline methods over 
all three metrics. The results demonstrate the 
good effectiveness of the proposed collaborative 
framework. We can also see that the performance 
is positively correlated with the clustering results. 
The CollabRank method with the best perform-
ing gold standard clustering results achieves the 
best performance. While the methods with low-
quality clustering results (i.e. the three random 
clustering results) do not perform well, even 
much worse than the baseline SingleRank 
method. This is because that the documents in a 
low-quality cluster are not truly topic-related, 
and the mutual influences between the docu-
ments are not reliable for evaluating words from 
a global perspective. 
System Precision Recall F-measure
TFIDF 0.232 0.281 0.254 
SingleRank 0.247 0.303 0.272 
CollabRank 
(Gold) 0.283 0.348 0.312 
CollabRank 
(Kmeans) 0.276 0.339 0.304 
CollabRank 
(CompleteLink) 0.281 0.345 0.310 
CollabRank 
(AverageLink) 0.277 0.340 0.306 
CollabRank 
(Divisive) 0.274 0.337 0.302 
CollabRank 
(Random1) 0.210 0.258 0.232 
CollabRank 
(Random2) 0.216 0.265 0.238 
CollabRank 
(Random3) 0.209 0.257 0.231 
Table 2. Keyphrase Extraction Results 
In order to investigate how the co-occurrence 
window size k and the keyphrase number n influ-
ence the performance, we first vary k from 2 to 
20 when n is fixed as 10 and the results are 
shown in Figures 2-4 over three metrics respec-
tively. The results demonstrate that all the meth-
ods are not significantly affected by the window 
size. We then vary n from 1 to 20 when k is fixed 
as 10 and the results are shown in Figures 5-7.  
The results demonstrate that the precision values 
decrease with the increase of n, and the recall 
values increases with the increase of n, while the 
F-measure values first increase and then tend to 
decrease with the increase of n.  
We can also see from Figures 2-7 that the Col-
labRank methods with high-quality clustering 
results always perform better than the baseline 
974
SingleRank method under different window sizes 
and different keyphrase numbers, and they al-
ways  lead to poor performance with low-quality 
clustering results. This further proves that an ap-
propriate cluster context is very important for the 
CollabRank method. Fortunately, existing clus-
tering algorithms can obtain the desired cluster 
context.  
 
0 . 1 7
1 8 1 6
K e y p h r a s e  n u m b e r  n
P
r
e
c
i
s
i
o
n
SingleRank CollabRank(Gold) CollabRank(Kmeans)
CollabRank(CompleteLink) CollabRank(AverageLink) CollabRank(Divisive)
CollabRank(Random1) CollabRank(Random2) CollabRank(Random3)  
0.19
0.21
0.23
0.25
0.27
0.29
2 4 6 8 10 12 14 16 18 20
Window size k
Pr
ec
is
io
n
 
Figure 2. Precision vs. Window Size k 
0.23
0.25
0.27
0.29
0.31
0.33
0.35
2 4 6 8 10 12 14 16 18 20
Window size k
R
ec
al
l
Figure 3. Recall vs. Window Size k 
0.2
0.22
0.24
0.26
0.28
0.3
0.32
2 4 6 8 10 12 14 16 18 20
Window size k
F-
m
ea
su
re
Figure 4. F-measure vs. Window Size k 
0.17
0.22
0.27
0.32
0.37
0.42
0.47
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
Pr
ec
is
io
n
 
Figure 5. Precision vs. Keyphrase Number 
n 
0.02
0.07
0.12
0.17
0.22
0.27
0.32
0.37
0.42
0.47
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
R
ec
al
l
Figure 6. Recall vs. Keyphrase Number n
0.05
0.1
0.15
0.2
0.25
0.3
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
F-
m
ea
su
re
Figure 7. F-measure vs. Keyphrase Num-
ber n 
 
The proposed CollabRank method makes only 
use of the global information based on the global 
graph for the cluster. In order to investigate the 
relative contributions from the whole cluster and 
the single document to the final performance, we 
experiment with the method named RankFusion 
which makes both of the cluster-level global in-
formation and the document-level local informa-
tion. The overall word score WordScorefusion(vi) 
for word vi in a document in RankFusion is a lin-
ear combination of the global word score and the 
local word score as follows: 
where ??[0,1] is the fusion weight. Then the 
phrase score is computed based on the fusion 
scores of the words. The RankFusion method is 
the same with CollabRank if ?=1 and it is the 
same with SingleRank if ?=0.  
Figure 8 shows the F-measure curves for the 
RankFusion methods with different high-quality 
clustering algorithms under different fusion 
weights. We can see that when ??(0.5,1), the 
RankFusion methods with high-quality clusters 
can outperform both the corresponding SingleR-
ank and the corresponding CollabRank. However, 
the performance improvements of RankFusion 
over CollabRank are not significant. We can 
conclude that the cluster-level global information 
plays the key role for evaluating the true saliency 
of the words.  
0.27
0.28
0.29
0.3
0.31
0.32
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fusion weight ?
F-
m
ea
su
re
Gold Kmeans CompleteLink
AverageLink Divisive
 
Figure 8. RankFusion Results (F-measure) vs. Fusion 
Weight ? 
5 Conclusion and Future Work 
In this paper, we propose a novel approach 
named CollabRank for collaborative single-
document keyphrase extraction, which makes use 
of the mutual influences between documents in 
appropriate cluster context to better evaluate the 
saliency of words and phrases. Experimental re-
)()1()()( idociclusifusion vWordScorevWordScorevWordScore ??+?= ?? (9) 
975
sults demonstrate the good effectiveness of Col-
labRank. We also find that the clustering algo-
rithm is important for obtaining the appropriate 
cluster context and the low-quality clustering 
results will deteriorate the extraction perform-
ance. It is encouraging that most existing popular 
clustering algorithms can meet the demands of 
the proposed approach.    
The proposed collaborative framework has 
more implementations than the implementation 
based on the graph-based ranking algorithm in 
this study. In future work, we will explore other 
keyphrase extraction methods in the proposed 
collaborative framework to validate the robust-
ness of the framework.  
Acknowledgements 
This work was supported by the National Science 
Foundation of China (No.60703064), the Re-
search Fund for the Doctoral Program of Higher 
Education of China (No.20070001059) and the 
National High Technology Research and Devel-
opment Program of China (No.2008AA01Z421).  
References 
A. Berger and V. Mittal. 2000. OCELOT: A system for 
summarizing Web Pages. In Proceedings of SIGIR2000. 
K. Barker and N. Cornacchia. 2000. Using nounphrase 
heads to extract document keyphrases. In Canadian Confer-
ence on AI. 
O. Buyukkokten, H. Garcia-Molina, and A. Paepcke. 2001. 
Seeing the whole in parts: text summarization for web 
browsing on handheld devices. In Proceedings of 
WWW2001. 
M. Chen, J.-T. Sun, H.-J. Zeng and K.-Y. Lam. 2005. A 
practical system for keyphrase extraction for web pages. In 
Proceedings of CIKM2005. 
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. 
Nevill-Manning. 1999. Domain-specific keyphrase extrac-
tion. Proceedings of IJCAI-99, pp. 668-673.  
C. Gutwin, G. W. Paynter, I. H. Witten, C. G. Nevill-
Manning and E. Frank. 1999. Improving browsing in digital 
libraries with keyphrase indexes. Journal of Decision Sup-
port Systems, 27, 81-104. 
K. M. Hammouda, D. N. Matute and M. S. Kamel. 2005. 
CorePhrase: keyphrase extraction for document clustering. 
In Proceedings of MLDM2005. 
A. Hulth. 2003. Improved automatic keyword extraction 
given more linguistic knowledge. In Proceedings of 
EMNLP2003, Japan, August. 
A. K. Jain, M. N. Murty and P. J. Flynn. 1999. Data cluster-
ing: a review. ACM Computing Surveys, 31(3):264-323. 
D. Kelleher and S. Luz. 2005. Automatic hypertext key-
phrase detection. In Proceedings of IJCAI2005. 
B. Krulwich and C. Burkey. 1996. Learning user informa-
tion interests through the extraction of semantically signifi-
cant phrases. In AAAI 1996 Spring Symposium on Machine 
Learning in Information Access.  
O. Medelyan and I. H. Witten. 2006. Thesaurus based auto-
matic keyphrase indexing. In Proceedings of JCDL2006. 
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order 
into texts. In Proceedings of EMNLP2004. 
A. Mu?oz. 1996. Compound key word generation from 
document databases using a hierarchical clustering ART 
model. Intelligent Data Analysis, 1(1). 
T. D. Nguyen and M.-Y. Kan. 2007. Keyphrase extraction 
in scientific publications. In Proceedings of ICADL2007. 
P. Over. 2001. Introduction to DUC-2001: an intrinsic 
evaluation of generic news text summarization systems. In 
Proceedings of DUC2001. 
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The 
pagerank citation ranking: Bringing order to the web. Tech-
nical report, Stanford Digital Libraries. 
M. Song, I.-Y. Song and X. Hu. 2003. KPSpotter: a flexible 
information gain-based keyphrase extraction system. In 
Proceedings of WIDM2003. 
A. M. Steier and R. K. Belew. 1993. Exporting 
phrases: A statistical analysis of topical language.  In 
Proceedings of Second Symposium on Document Analysis 
and Information Retrieval, pp. 179-190. 
T. Tomokiyo and M. Hurst. 2003. A language model ap-
proach to keyphrase extraction. In: Proceedings of ACL 
Workshop on Multiword Expressions. 
K. Toutanova and C. D. Manning. 2000. Enriching the 
knowledge sources used in a maximum entropy Part-of-
Speech tagger. In Proceedings of EMNLP/VLC-2000. 
P. D. Turney. 2000. Learning algorithms for keyphrase ex-
traction. Information Retrieval, 2:303-336. 
P. D. Turney. 2003. Coherent keyphrase extraction via web 
mining. In Proc. of IJCAI-03, pages 434?439. 
X. Wan, J. Yang and J. Xiao. 2007a. Towards an iterative 
reinforcement approach for simultaneous document summa-
rization and keyword extraction. In Proceedings of 
ACL2007. 
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. 
Nevill-Manning. 1999. KEA: Practical automatic keyphrase 
extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 
254-256. 
W.-T. Yih, J. Goodman and V. R. Carvalho. 2006. Finding 
advertising keywords on web pages. In Proceedings of 
WWW2006.  
H. Y. Zha. 2002. Generic summarization and keyphrase 
extraction using mutual reinforcement principle and sen-
tence clustering. In Proceedings of SIGIR2002, pp. 113-120. 
Y. Zhang, N. Zincir-Heywood, and E. Milios. 2004. Term-
Based Clustering and Summarization of Web Page Collec-
tions. In Proceedings of the Seventeenth Conference of the 
Canadian Society for Computational Studies of Intelligence. 
Y. Zhang, N. Zincir-Heywood and E. Milios. 2005. Narra-
tive text classification for automatic key phrase extraction in 
web document corpora. In Proceedings of WIDM2005. 
976
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1137?1145,
Beijing, August 2010
Towards a Unified Approach to Simultaneous Single-Document and 
Multi-Document Summarizations 
 
Xiaojun Wan 
Institute of Compute Science and Technology 
The MOE Key Laboratory of Computational Linguistics 
Peking University 
wanxiaojun@icst.pku.edu.cn 
 
Abstract 
Single-document summarization and multi-
document summarization are very closely re-
lated tasks and they have been widely investi-
gated independently.  This paper examines 
the mutual influences between the two tasks 
and proposes a novel unified approach to si-
multaneous single-document and multi-
document summarizations. The mutual influ-
ences between the two tasks are incorporated 
into a graph model and the ranking scores of a 
sentence for the two tasks can be obtained in 
a unified ranking process. Experimental re-
sults on the benchmark DUC datasets demon-
strate the effectiveness of the proposed ap-
proach for both single-document and multi-
document summarizations.  
1 Introduction 
Single-document summarization aims to pro-
duce a concise and fluent summary for a single 
document, and multi-document summarization 
aims to produce a concise and fluent summary 
for a document set consisting of multiple related 
documents. The two tasks are very closely re-
lated in both task definition and solution method. 
Moreover, both of them are very important in 
many information systems and applications. For 
example, given a cluster of news articles, a 
multi-document summary can be used to help 
users to understand the whole cluster, and a sin-
gle summary for each article can be used to help 
users to know the content of the specified article.  
To date, single-document and multi-document 
summarizations have been investigated exten-
sively and independently in the NLP and IR 
fields. A series of special conferences or work-
shops on automatic text summarization (e.g. 
SUMMAC, DUC, NTCIR and TAC) have ad-
vanced the technology and produced a couple of 
experimental online systems. However, the two 
summarization tasks have not yet been simulta-
neously investigated in a unified framework.  
Inspired by the fact that the two tasks are very 
closely related and they can be used simultane-
ously in many applications, we believe that the 
two tasks may have mutual influences on each 
other. In this study, we propose a unified ap-
proach to simultaneous single-document and 
multi-document summarizations. The mutual 
influences between the two tasks are incorpo-
rated into a graph-based model. The ranking 
scores of sentences for single-document summa-
rization and the ranking scores of sentences for 
multi-document summarization can boost each 
other, and they can be obtained simultaneously 
in a unified graph-based ranking process. To the 
best of our knowledge, this study is the first at-
tempt for simultaneously addressing the two 
summarization tasks in a unified graph-based 
framework. Moreover, the proposed approach 
can be easily adapted for topic-focused summa-
rizations.  
Experiments have been performed on both the 
single-document and multi-document summari-
zation tasks of DUC2001 and DUC2002. The 
results demonstrate that the proposed approach 
can outperform baseline independent methods 
for both the two summarization tasks. The two 
tasks are validated to have mutual influences on 
each other.  
The rest of this paper is organized as follows: 
Section 2 introduces related work. The details of 
the proposed approach are described in Section 
3. Section 4 presents and discusses the evalua-
tion results. Lastly we conclude our paper in 
Section 5. 
1137
2 Related Work 
Document summarization methods can be either 
extraction-based or abstraction-based. In this 
section, we focus on extraction-based methods.  
Extraction-based methods for single-
document summarization usually assign a sali-
ency score to each sentence in a document and 
then rank and select the sentences. The score is 
usually computed based on a combination of 
statistical and linguistic features, such as term 
frequency, sentence position, cue words and 
stigma words (Luhn, 1969; Edmundson, 1969; 
Hovy and Lin, 1997). Machine learning tech-
niques have also been used for sentence extrac-
tion (Kupiec et al, 1995; Conroy and O?Leary, 
2001; Shen et al, 2007; Li et al, 2009). The 
mutual reinforcement principle has been ex-
ploited to iteratively extract key phrases and 
sentences from a document (Zha, 2002; Wan et 
al, 2007a). Wan et al (2007b) propose the Col-
labSum algorithm to use additional knowledge 
in a cluster of documents to improve single 
document summarization in the cluster.   
In recent years, graph-based ranking methods 
have been investigated for document summari-
zation, such as TextRank (Mihalcea and Tarau, 
2004; Mihalcea and Tarau, 2005) and LexPag-
eRank (ErKan and Radev, 2004). Similar to 
PageRank (Page et al, 1998), these methods 
first build a graph based on the similarity rela-
tionships between the sentences in a document 
and then the saliency of a sentence is determined 
by making use of the global information on the 
graph recursively. The basic idea underlying the 
graph-based ranking algorithm is that of ?vot-
ing? or ?recommendation? between sentences.  
Similar methods have been used for generic 
multi-document summarization. A typical 
method is the centroid-based method (Radev et 
al., 2004). For each sentence, the method com-
putes a score based on each single feature (e.g. 
cluster centroids, position and TFIDF) and then 
linearly combines all the scores into an overall 
sentence score. Topic signature is used as a 
novel feature for selecting important content in 
NeATS (Lin and Hovy, 2002). Various sentence 
features have been combined by using machine 
learning techniques (Wong et al, 2008). A 
popular way for removing redundancy between 
summary sentences is the MMR algorithm (Car-
bonell and Goldstein, 1998). Themes (or topics, 
clusters) in documents have been discovered and 
used for sentence selection (Harabagiu and La-
catusu, 2005). Hachey (2009) investigates the 
effect of various source document representa-
tions on the accuracy of the sentence extraction 
phase of a multi-document summarization task. 
Graph-based methods have also been used to 
rank sentences in a document set. The methods 
first construct a graph to reflect sentence rela-
tionships at different granularities, and then 
compute sentence scores based on graph-based 
learning algorithms. For example, Wan (2008) 
proposes to use only cross-document relation-
ships for graph building and sentence ranking. 
Cluster-level information has been incorporated 
in the graph model to better evaluate sentences 
(Wan and Yang, 2008).  
For topic-focused multi-document summari-
zation, many methods are extensions of generic 
summarization methods by incorporating the 
information of the given topic or query into ge-
neric summarizers. In recent years, a few novel 
methods have been proposed for topic-focused 
summarization (Daum? and Marcu, 2006; Wan 
et al, 2007c; Nastase 2008; Li et al, 2008; 
Schilder and Kondadadi, 2008; Wei et al, 2008).   
The above previous graph-based summariza-
tion methods aim to address either single-
document summarization or multi-document 
summarization, and the two summarization tasks 
have not yet been addressed in a unified graph-
based framework.  
3 The Unified Summarization Ap-
proach 
3.1 Overview 
Given a document set, in which the whole docu-
ment set and each single document in the set are 
required to be summarized, we use local sali-
ency to indicate the importance of a sentence in 
a particular document, and use global saliency 
to indicate the importance of a sentence in the 
whole document set. 
In previous work, the following two assump-
tions are widely made for graph-based summari-
zation models: 
Assumption 1: A sentence is locally impor-
tant in a particular document if it is heavily 
linked with many locally important sentences in 
the same document.  
1138
Assumption 2: A sentence is globally impor-
tant in the document set if it is heavily linked 
with many globally important sentences in the 
document set.  
   The above assumptions are the basis for Pag-
eRank-like algorithms for single document 
summarization and multi-document summariza-
tion, respectively.  In addition to the above two 
assumptions, we make the following two as-
sumptions to consider the mutual influences be-
tween the two summarization tasks: 
Assumption 3: A sentence is locally impor-
tant in a particular document, if it is heavily 
linked with many globally important sentences 
in the document set.   
The above assumption is reasonable because 
the documents in the set are relevant and the 
globally important information in the document 
set will be expressed in many single documents. 
Therefore, if a sentence is salient in the whole 
document set, the sentence may be salient in a 
particular document in the set. 
Assumption 4: A sentence is globally impor-
tant in the document set, if it is heavily linked 
with many locally important sentences.  
The above assumption is reasonable because 
the documents in the set are relevant and the 
globally important information in the whole set 
is the aggregation of the locally important in-
formation in each single document. Therefore, if 
a sentence is salient in a particular document, 
the sentence has the potential to be salient in the 
whole document set. 
In brief, the local saliency and global saliency 
of a sentence can mutually influence and boost 
each other: high local saliency will lead to high 
global saliency, and high global saliency will 
lead to high local saliency.  
Based on the above assumptions, our pro-
posed approach first builds affinity graphs (each 
graph is represented by an affinity matrix) to 
reflect the different kinds of relationships be-
tween sentences, respectively, and then itera-
tively computes the local saliency scores and the 
global saliency scores of the sentences based on 
the graphs. Finally, the algorithm converges and 
the local saliency score and global saliency 
score of each sentence are obtained. The sen-
tences with high local saliency scores in a par-
ticular document are chosen into the summary of 
the single document, and the sentences with 
high global saliency scores in the set are chosen 
into the summary of the document set.  
Note that for both summarization tasks, after 
the saliency scores of sentences have been ob-
tained, the greedy algorithm used in (Wan et al, 
2007c) is applied to remove redundancy and 
finally choose both informative and novel sen-
tences into the summary. 
3.2 Algorithm Details 
Formally, the given document set is denoted as 
D={di|1?i?m}, and the whole sentence set is 
denoted as S={si|1?i?n}.  We let Infosingle(si)  
denote the local saliency score of sentence si in a 
particular document d(si)?D, and it is used to 
select summary sentences for the single docu-
ment d(si).   And we let Infomulti(si) denote the 
global saliency score of sentence si in the whole 
document set D,  and it is used to select sum-
mary sentences for the document set D.  
The four assumptions in Section 3.1 can be 
rendered as follows: 
?? j jglejiAigle sInfoWsInfo )()()( sinsin  (1) 
?? j jmultijiBimulti sInfoWsInfo )()()(  (2) 
?? j jmultijiCigle sInfoWsInfo )()()(sin  (3) 
?? j jglejiDimulti sInfoWsInfo )()()( sin  (4) 
where WA, WB, WC, WD are n?n affinity matrices 
reflecting the different kinds of relationships 
between sentences in the document set, where n 
is the number of all sentences in the document 
set. The detailed derivation of the matrices will 
be presented later. 
After fusing the above equations, we can ob-
tain the following unified forms: 
?
?
?+
=
j jmultijiC
j jglejiAigle
sInfoW
sInfoWsInfo
)()()1(                
)()()( sinsin
?
?   (5) 
?
?
?+
=
j jglejiD
j jmultijiBimulti
sInfoW
sInfoWsInfo
)()()1(             
)()()(
sin?
?
 
 
  (6) 
However, the above summarization method 
ignores the feature of sentence position, which 
has been validated to be very important for 
document summarizations. In order to incorpo-
rate this important feature, we add one prior 
score to each computation as follows: 
)()()(
)()()(
sin
sinsin
iglej jmultijiC
j jglejiAigle
spriorsInfoW
sInfoWsInfo
?++
=
?
?
??
?   
(7) 
1139
)()()(
)()()(
sin imultij jglejiD
j jmultijiBimulti
spriorsInfoW
sInfoWsInfo
?++
=
?
?
??
?  
 
  (8) 
where ?, ?, ??[0,1] specify the relative contri-
butions to the final saliency scores from the dif-
ferent factors, and we have ?+?+?=1. pri-
orsingle(si) is the prior score for the local saliency 
of sentence si, and here priorsingle(si)  is com-
puted based on sentence position of si in the par-
ticular document d(si). priormulti(si) is the prior 
score for the global saliency of sentence si, and 
we also compute priormulti(si) based on sentence 
position of si. 
We use two column vectors 
ur =[Infosingle(si)]n?1 and vr =[Infomulti(si)]n?1 to 
denote the local and global saliency scores of all 
the sentences in the set, respectively. And the 
matrix forms of the above equations are as fol-
lows: 
gle
TT ???
CA sin
pvWuWu rrrr ++=   (9) 
multi
TT ???
DB
puWvWv rrrr ++=    (10) 
where 
1sinsin )]([ ?= niglegle spriorp
r and 
1)]([ ?= nimultimulti spriorp
r
 are the prior column vec-
tors. 
The above matrices and prior vectors are con-
structed as follows, respectively: 
WA: This affinity matrix aims to reflect the 
local relationships between sentences in each 
single document, which is defined as follows: 
   
Otherwise   0,
ji  and                                   
 )d( )d( if  ),,(
)(
cos
??
??
?
?
=
=
jijiine
ijA
sssssim
W  
  
(11) 
where d(si) refers to the document containing 
sentence si. simcosine(si,sj) is the cosine similarity 
between sentences si and sj.  
ji
ji
jiine ss
ss
sssim rr
rr
?
?
=),(cos  
 
(12) 
where is
r  and js
r are the corresponding term vec-
tors of si and sj. Note that we have (WA)ij = (WA)ji, 
and we have (WA)ii =0 to avoid self loops.  
   We can see that the matrix contains only the 
within-document relationships between sen-
tences.  
WB: This affinity matrix aims to reflect the 
global relationships between sentences in the 
document set, which is defined as follows: 
    
Otherwise   0,
 )d( )d( if  ),,(
)( cos
??
? ?
=
jijiine
ijB
sssssim
W
  
(13) 
    We can see that the matrix contains only the 
cross-document relationships between sentences. 
We do not include the within-document sen-
tence relationships in the matrix because it has 
been shown that the cross-document relation-
ships are more appropriate to reflect the global 
mutual influences between sentences than the 
within-document relationships in (Wan, 2008). 
WC: This affinity matrix aims to reflect the 
cross-document relationships between sentences 
in the document set. However, the relationships 
in this matrix are used for carrying the influ-
ences of the sentences in other documents on the 
local saliency of the sentences in a particular 
document. If we directly use Equation (13) to 
compute the matrix, the mutual influences 
would be overly used. Because other documents 
might not be sampled from the same generative 
model as the specified document, we probably 
do not want to trust them so much as the speci-
fied document. Thus a confidence value is used 
to reflect out belief that the document is sampled 
from the same underlying model as the specified 
document. Heuristically, we use the cosine simi-
larity between documents as the confidence 
value. And we use the confidence value as the 
decay factor in the matrix computation as fol-
lows: 
   
Otherwise   0,
    )d( )d( if                                     
)),(),((),(
)(
coscos
??
??
?
?
?
= ji
jiinejiine
ijc ss
sdsdsimsssim
W
  
(14) 
WD: This affinity matrix aims to reflect the 
within-document relationships between sen-
tences. Thus we have WD=WA, which means that 
the global saliency score of a sentence is influ-
enced only by the local saliency scores of the 
sentences in the same document, without con-
sidering the sentences in other documents.  
Note that the above four matrices are symmet-
ric and we can replace TAW , TBW , TCW and TDW  
by WA, WB, WC and WD in Equations (9) and 
(10), respectively. 
priorsingle(si): It is computed under the as-
sumption that the first sentences in a document 
are usually more important than other sentences.  
1)(
15.0)(sin
+
+=
i
igle sposition
sprior   
(15) 
where position(si) returns the position number of 
sentence si in its document d(si). For example, if 
1140
si is the first sentence in its document, position(si) 
is 1.  
The  prior weight is then normalized by: 
?= i igle
igle
igle sprior
sprior
sprior
)(
)(
)(
sin
sin
sin
  
(16) 
priormulti(si): We also let the prior weight re-
flect the influence of sentence position. 
)()( sin igleimulti spriorsprior =  (17) 
And then the prior weight is normalized in the 
same way. 
The above definitions are for generic docu-
ment summarizations and the above algorithm 
can be easily adapted for topic-focused summa-
rizations. Given a topic q, the only change for 
the above computation is priormulti(si). The topic 
relevance is incorporated into the prior weight as 
follows: 
),()( cos qssimsprior iineimulti =  (18) 
?= i imulti
imulti
imulti sprior
spriorsprior
)(
)()(   (19) 
In order to solve the iterative problem defined 
in Equations (9) and (10), we let TT ]  [ Tvur rrr = , 
T]  [ Tmulti
T
single ppp
rrr ??= , 
???
?
???
?
=
T
B
T
D
T
C
T
A
WW
WWW
??
??
   
   , and 
then the iterative equations correspond to the 
following linear system: 
prWr rrr +=  (20) 
prWI rr =? )(  (21) 
To guarantee the solution of the above linear 
system, W is normalized by columns. If all the 
elements of a column are zero, we replace the 
elements with 1/(2n), where 2n equals to the 
element number of the column. We then multi-
ply W by a decay factor ? (0<?<1) to scale down 
each element in W, but remain the meaning of 
W. Here, ? is empirically set to 0.61. Finally, 
Equation (21) is rewritten as follows: 
prWI rr =?? )( ?  (22) 
Thus, the matrix (I-?W) is a strictly diago-
nally dominant matrix and the solution of the 
linear system exists and we can apply the Gauss-
Seidel method used in (Li et al, 2008) to solve 
the linear system. The GS method is a well-
know method for numeric computation in 
                                                 
1  In our pilot study, we can observe good performance 
when ? is in a wide range of [0.4, 0.8]. 
mathematics and the details of the method is 
omitted here.  
4 Empirical Evaluation 
4.1 Dataset and Evaluation Metric 
Generic single-document and multi-document 
summarizations have been the fundamental tasks 
in DUC 2001 and DUC 2002 (i.e. tasks 1 and 2 
in DUC 2001 and tasks 1 and 2 in DUC 2002), 
and we used the two datasets for evaluation. 
DUC2001 provided 309 articles, which were 
grouped into 30 document sets. Generic sum-
mary of each article was required to be created 
for task 1, and generic summary of each docu-
ment set was required to be created for task 2. 
The summary length was 100 words or less. 
DUC 2002 provided 59 document sets consist-
ing of 567 articles (D088 is excluded from the 
original 60 document sets by NIST) and generic 
summaries for each article and each document 
set with a length of approximately 100 words 
were required to be created. The sentences in 
each article have been separated and the sen-
tence information has been stored into files.  The 
summary of the two datasets are shown in Table 
1.  
 DUC 2001 DUC 2002
Task Tasks 1, 2 Tasks 1, 2 
Number of documents 309 567 
Number of clusters 30 59 
Data source TREC-9 TREC-9 
summary length 100 words 100 words 
  Table 1. Summary of datasets  
We used the ROUGE toolkit2  (Lin and Hovy, 
2003) for evaluation, which has been widely 
adopted by DUC for automatic summarization 
evaluation. It measured summary quality by 
counting overlapping units such as the n-gram, 
word sequences and word pairs between the 
candidate summary and the reference summary.  
The ROUGE toolkit reported separate recall-
oriented scores for 1, 2, 3 and 4-gram, and also 
for longest common subsequence co-
occurrences. We showed three of the ROUGE 
metrics in the experimental results: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on weighted longest 
common subsequence, weight=1.2). In order to 
truncate summaries longer than the length limit, 
                                                 
2 We used ROUGEeval-1.4.2 in this study. 
1141
we used the ?-l 100? option in ROUGE toolkit. 
We also used the ?-m? option for word stem-
ming. 
4.2 Evaluation Results 
4.2.1 System Comparison 
In the experiments, the combination weight ? for 
the prior score is fixed at 0.15, as in the PageR-
ank algorithm. Therefore, we have ?+?=0.85. 
Here, we use ?/(?+?) to indicate the relative 
contributions of the first two parts in Equations 
(9) and (10). We empirically set ?/(?+?)=0.4 in 
the experiments.  The proposed unified approach 
(i.e. UnifiedRank) is compared with a few base-
line approaches and the top three participating 
systems.  
The graph-based baselines for single-
document summarization are described as fol-
lows: 
BasicRank: This baseline approach adopts 
the basic PageRank algorithm to rank sentences 
based on all sentence relationships in a single 
document, similar to previous work (Mihalcea 
and Tarau, 2004).  
PositionRank: This baseline approach im-
proves the basic PageRank algorithm by using 
the position weight of a sentence as the prior 
score for the sentence. The position weight of a 
sentence is computed by using Equation (15). 
CollabRank1: This baseline approach is the 
?UniformLink(Gold)? approach proposed in 
(Wan et al 2007b).  It uses a cluster of multiple 
documents to improve single document summa-
rization by constructing a global affinity graph.   
CollabRank2: This baseline approach is the  
?UnionLink(Gold)? approach proposed in (Wan 
et al 2007b).  
The graph-based baselines for multi-
document summarization are described as fol-
lows: 
BasicRank: This baseline approach adopts 
the basic PageRank algorithm to rank sentences 
based on all sentence relationships in document 
set. Both within-document and cross-document 
sentence relationships are used for constructing 
the affinity graph. 
PositionRank: Similarly, this baseline ap-
proach improves the basic PageRank algorithm 
by using the position weight of a sentence as the 
prior score for the sentence.  
TwoStageRank: This baseline approach lev-
erages the results of single document summari-
zation for multi-document summarization. It 
first computes the score of each sentence within 
each single document by using the PositionRank 
method, and then computes the final score of 
each sentence within the document set by con-
sidering the document-level sentence score as 
the prior score in the improved PageRank algo-
rithm.  
The top three systems are the systems with 
highest ROUGE scores, chosen from the partici-
pating systems on each task, respectively. Ta-
bles 2 and 3 show the comparison results for 
single-document summarization on DUC2001 
and DUC2002, respectively. Tables 4 and 5 
show the comparison results for multi-document 
summarization on DUC2001 and DUC2002, 
respectively. In the tables, SystemX (e.g. Sys-
tem28, SystemN) represents one of the top per-
forming systems. The systems are sorted by de-
creasing order of the ROUGE-1 scores.  
For single-document summarization, the pro-
posed UnifiedRank approach always outper-
forms the four graph-based baselines over all 
three metrics on both two datasets. The per-
formance differences are all statistically signifi-
cant by using t-test (p-value<0.05). The 
ROUGE-1 score of UnifiedRank is higher than 
that of the best participating systems and the 
ROUGE-2 and ROUGE-W scores of Unifie-
dRank are comparable to that of the best partici-
pating systems.  
For multi-document summarization, the pro-
posed UnifiedRank approach outperforms all the 
three graph-based baselines over all three met-
rics on the DUC2001 dataset, and it outperforms 
the three baselines over ROUGE-1 and 
ROUGE-W on the DUC2002 dataset. In particu-
lar, UnifiedRank can significantly outperform 
BasicRank and TwoStageRank over all three 
metrics on the DUC2001 dataset (t-test, p-
value<0.05). Moreover, the ROUGE-1 and 
ROUGE-W scores of UnifiedRank are higher 
than that of the best participating systems and 
the ROUGE-2 score of UnifiedRank is compa-
rable to that of the best participating systems. 
The results demonstrate that the single-
document and multi-document summarizations 
can benefit each other by making use of the mu-
tual influences between the local saliency and 
1142
global saliency of the sentences. Overall, the 
proposed unified graph-based approach is effec-
tive for both single document summarization 
and multi-document summarization. However, 
the performance improvement for single-
document summarization is more significant 
than that for multi-document summarization, 
which shows that the global information in a 
document set is very beneficial to summariza-
tion of each single document in the document 
set.  
 
System ROUGE-1 ROUGE-2 ROUGE-W
UnifiedRank 0.45377 0.17649 0.14328 
CollabRank2 0.44038 0.16229 0.13678 
CollabRank1 0.43890 0.16213 0.13676 
PositionRank 0.43596 0.15936 0.13684 
BasicRank 0.43407 0.15696 0.13629 
Table 2. Comparison results for single-document 
summarization on DUC20013 
System ROUGE-1 ROUGE-2 ROUGE-W
UnifiedRank 0.48478 0.21462 0.16877 
System28 0.48049 0.22832 0.17073 
System21 0.47754 0.22273 0.16814 
CollabRank1 0.47187 0.20102 0.16318 
CollabRank2 0.47028 0.20046 0.16260 
PositionRank 0.46618 0.19853 0.16180 
System31 0.46506 0.20392 0.16162 
BasicRank 0.46261 0.19457 0.16018 
Table 3. Comparison results for single-document 
summarization on DUC2002 
System ROUGE-1 ROUGE-2 ROUGE-W
UnifiedRank 0.36360 0.06496 0.10950 
PositionRank 0.35733 0.06092 0.10798 
BasicRank 0.35527 0.05608 0.10641 
TwoStageRank 0.35221 0.05500 0.10515 
SystemN 0.33910 0.06853 0.10240 
SystemP 0.33332 0.06651 0.10068 
SystemT 0.33029 0.07862 0.10215 
Table 4. Comparison results for multi-document 
summarization on DUC2001 
System ROUGE-1 ROUGE-2 ROUGE-W
UnifiedRank 0.38343 0.07855 0.12341 
PositionRank 0.38056 0.08238 0.12292 
TwoStageRank 0.37972 0.08166 0.12261 
BasicRank 0.37595 0.08304 0.12173 
System26 0.35151 0.07642 0.11448 
System19 0.34504 0.07936 0.11332 
System28 0.34355 0.07521 0.10956 
Table 5. Comparison results for multi-document 
summarization on DUC2002 
                                                 
3 The summarization results for participating systems on 
DUC2001 are incomplete. 
4.2.2 Influences of Combination Weight 
In the above experiments, the relative contribu-
tions from the first two parts in Equations (9) 
and (10) are empirically set as ?/(?+?)=0.4. In 
this section, we investigate how the relative con-
tributions influence the summarization perform-
ance by varying ?/(?+?) from 0 to 1. A small 
value of ?/(?+?) indicates that the contribution 
from the same kind of saliency scores of the sen-
tences is less important than the contribution 
from the different kind of saliency scores of the 
sentences, and vice versa. Figures 1-8 show the 
ROUGE-1 and ROUGE-W curves for single-
document summarization and multi-document 
summarization on DUC2001 and DUC2002, 
respectively.  
For single document summarization, very 
small value or very large value for ?/(?+?) will 
lower the summarization performance values on 
the two datasets. The results demonstrate that 
both the two kinds of contributions are impor-
tant to the final performance of single document 
summarization. 
For multi-document summarization, a rela-
tively large value (?0.4) for ?/(?+?) will lead to 
relatively high performance values on the 
DUC2001 dataset, but a very large value for 
?/(?+?) will decrease the performance values. 
On the DUC2002 dataset, a relatively small 
value (?0.4) will lead to relatively high per-
formance values, but a very small value for 
?/(?+?) will decrease the performance values. 
Though the trends of the curves on the 
DUC2001 and DUC2002 datasets are not very 
consistent with each other, the results show that 
both the two kinds of contributions are benefi-
cial to the final performance of multi-document 
summarization. 
5 Conclusion and Future Work 
In this study, we propose a novel unified ap-
proach to simultaneous single-document and 
multi-document summarization by making using 
of the mutual influences between the two tasks. 
Experimental results on the benchmark DUC 
datasets show the effectiveness of the proposed 
approach.  
In future work, we will perform comprehen-
sive experiments for topic-focused document 
1143
summarizations to show the robustness of the 
proposed approach.  
DUC2001
0.444
0.446
0.448
0.45
0.452
0.454
0.456
0.458
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
1
 
Figure 1. ROUGE-1 vs. combination weight for sin-
gle-document summarization on DUC2001 
DUC2001
0.14
0.1405
0.141
0.1415
0.142
0.1425
0.143
0.1435
0.144
0.1445
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
W
 
Figure 2. ROUGE-W vs. combination weight for 
single-document summarization on DUC2001 
DUC2002
0.474
0.476
0.478
0.48
0.482
0.484
0.486
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
1
 
Figure 3. ROUGE-1 vs. combination weight for sin-
gle-document summarization on DUC2002 
DUC2002
0.164
0.165
0.166
0.167
0.168
0.169
0.17
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
R
O
U
G
E-
W
 
Figure 4. ROUGE-W vs. combination weight for 
single-document summarization on DUC2002 
DUC2001
0.34
0.345
0.35
0.355
0.36
0.365
0.37
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
1
 
Figure 5. ROUGE-1 vs. combination weight for 
multi-document summarization on DUC2001 
DUC2001
0.102
0.104
0.106
0.108
0.11
0.112
0.114
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
W
 
Figure 6. ROUGE-W vs. combination weight for 
multi-document summarization on DUC2001 
DUC2002
0.374
0.376
0.378
0.38
0.382
0.384
0.386
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
R
O
U
G
E-
1
 
Figure 7. ROUGE-1 vs. combination weight for 
multi-document summarization on DUC2002 
DUC2002
0.12
0.121
0.122
0.123
0.124
0.125
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?/(?+?)
RO
U
G
E-
W
 
Figure 8. ROUGE-W vs. combination weight for 
multi-document summarization on DUC2002 
Acknowledgments 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03) and NCET 
(NCET-08-0006).  
1144
References  
J. Carbonell, J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering Docu-
ments and Producing Summaries. In Proceedings 
of SIGIR1998, 335-336. 
J. M. Conroy, D. P. O?Leary. 2001. Text Summariza-
tion via Hidden Markov Models. In Proceedings 
of SIGIR2001, 406-407. 
H. Daum? and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL-
06. 
H. P. Edmundson. 1969. New Methods in Automatic 
Abstracting. Journal of the Association for com-
puting Machinery, 16(2): 264-285. 
G. ErKan, D. R. Radev. 2004. LexPageRank: Pres-
tige in Multi-Document Text Summarization. In 
Proceedings of EMNLP2004. 
B. Hachey. 2009. Multi-document summarisation 
using generic relation extraction. In Proceedings 
of EMNLP2009. 
S. Harabagiu and F. Lacatusu. 2005. Topic themes 
for multi-document summarization. In Proceed-
ings of SIGIR-05. 
E. Hovy, C. Y. Lin. 1997. Automated Text Summari-
zation in SUMMARIST. In Proceeding of 
ACL?1997/EACL?1997 Worshop on Intelligent 
Scalable Text Summarization. 
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable 
Document Summarizer. In Proceedings of 
SIGIR1995, 68-73. 
W. Li, F. Wei, Q. Lu and Y. He. 2008. PNR2: rank-
ing sentences with positive and negative rein-
forcement for query-oriented update summariza-
tion. In Proceedings of COLING-08. 
L. Li, K. Zhou, G.-R. Xue, H. Zha, Y. Yu. 2009. 
Enhancing diversity, coverage and balance for 
summarization through structure learning. In Pro-
ceedings of WWW-09. 
C..-Y. Lin and E.. H. Hovy. 2002. From Single to 
Multi-document Summarization: A Prototype 
System and its Evaluation. In Proceedings of 
ACL-02. 
C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence 
Statistics. In Proceedings of HLT-NAACL -03. 
H. P. Luhn. 1969. The Automatic Creation of litera-
ture Abstracts. IBM Journal of Research and De-
velopment, 2(2). 
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of EMNLP2004. 
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of IJCNLP-
05. 
V. Nastase. 2008. Topic-driven multi-document 
summarization with encyclopedic knowledge and 
spreading activation. In Proceedings of EMNLP-
08.  
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. 
The pagerank citation ranking: Bringing order to 
the web. Technical report, Stanford Digital Librar-
ies. 
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. 
Centroid-based summarization of multiple docu-
ments. Information Processing and Management, 
40: 919-938. 
F. Schilder and R. Kondadadi. 2008. FastSum: fast 
and accurate query-based multi-document sum-
marization. In Proceedings of ACL-08: HLT. 
D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 
2007. Document Summarization using Condi-
tional Random Fields. In Proceedings of 
IJCAI2007. 
X. Wan. 2008. Using Only Cross-Document Rela-
tionships for Both Generic and Topic-Focused 
Multi-Document Summarizations. Information 
Retrieval, 11(1): 25-49. 
X. Wan and J. Yang. 2008. Multi-document summa-
rization using cluster-based link analysis. In Pro-
ceedings of SIGIR-08. 
X. Wan, J. Yang and J. Xiao. 2007a. Towards an 
Iterative Reinforcement Approach for Simultane-
ous Document Summarization and Keyword Ex-
traction. In Proceedings of ACL2007.  
X. Wan, J. Yang and J. Xiao. 2007b. CollabSum: 
Exploiting Multiple Document Clustering for Col-
laborative Single Document Summarizations. In 
Proceedings of SIGIR2007.  
X. Wan, J. Yang and J. Xiao. 2007c. Manifold-
ranking based topic-focused multi-document 
summarization. In Proceedings of IJCAI-07.  
F. Wei, W. Li, Q. Lu and Y. He. 2008. Query-
sensitive mutual reinforcement chain and its ap-
plication in query-oriented multi-document sum-
marization. In Proceedings of SIGIR-08.  
K.-F. Wong, M. Wu and W. Li. 2008. Extractive 
summarization using supervised and semi-
supervised learning. In Proceedings of COLING-
08. 
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of SIGIR2002, 113-120. 
 
1145
Coling 2010: Poster Volume, pages 782?790,
Beijing, August 2010
Opinion Target Extraction in Chinese News Comments 
Tengfei Ma 
 
Xiaojun Wan* 
 
 
Abstract 
News Comments on the web express 
readers? attitudes or opinions about an 
event or object in the corresponding 
news article. And opinion target extrac-
tion from news comments is very impor-
tant for many useful Web applications. 
However, many sentences in the com-
ments are irregular and informal, and 
sometimes the opinion targets are impli-
cit. Thus the task is very challenging and 
it has not been investigated yet. In this 
paper, we propose a new approach to un-
iformly extracting explicit and implicit 
opinion targets from news comments by 
using Centering Theory. The approach 
uses global information in news articles 
as well as contextual information in ad-
jacent sentences of comments. Our expe-
rimental results verify the effectiveness 
of the proposed approach.  
1 Introduction 
With the dramatic development of web 2.0, there 
are more and more news web sites allowing 
users to comment on news events. These 
comments have become valuable resources for 
researchers to make advanced opinion analysis, 
such as tracking the attitudes to a focused event, 
person or corporation. In these advanced opinion 
analysis tasks, opinion target extraction is a 
necessary step. Unfortunately, former works did 
not focus on the domain of news comments. 
Though some researchers and workshops have 
investigated the task of opinion target extraction 
in product reviews and news articles, the 
                                                 
* Contact author 
methods cannot perform well on news comments. 
Actually, target extraction in news comments 
significantly differs from that in product reviews 
and news articles in the following ways. 
1) Products usually have a set of definite 
attributes (e.g. size) and related opinion words 
(e.g. large), and thus researchers can use a small 
fixed set of keywords to recognize frequent fea-
ture words (Zhuang et al, 2006), or leverage the 
associated rules between feature words and opi-
nion words to improve the performance (Hu and 
Liu, 2004; Su et al, 2008; Jin and Ho, 2009; Du 
and Tan, 2009). But news comments are more 
complicated. There are much more potential 
opinion targets in news comments. In other 
words, the candidate targets are in a much more 
open domain.  On the other hand, the opinion 
targets in news comments are not strongly asso-
ciated with the opinion words. We cannot judge 
a target by a special opinion word as easily as in 
product reviews. 
2) The opinionated sentences in news articles 
mostly contain opinion operators (e.g. believe, 
realize), which can be used to find the positions 
of opinion expressions. However, news com-
ments have already been considered to be de-
clared by readers and they do not have many 
operators to indicate the positions of opinion 
targets.  
3) Furthermore, many comment sentences are 
of free style. In many cases, there are even no 
manifest targets in the comment sentences. For 
example, a news article and its relational com-
ment are as follows: 
News: ?????????????????? 
(Dubai will build the highest skyscraper in the 
world)  
Comment:  
??????????????? 
(Really high, but what (is it) used for?) 
Institute of Compute Science and Technology 
The MOE Key Laboratory of Computational Linguistics 
Peking University 
{matengfei, wanxiaojun}@icst.pku.edu.cn 
782
The comment sentence obviously comments 
on ?skyscraper? by human understanding, but in 
the sentence we cannot find the word or an alter-
native. Instead, the real target is included in the 
news article. Now we give two definitions of the 
phenomenon. 
Implicit targets: The implicit targets are 
those opinion targets which do not occur in the 
current sentence. The sentence is called implicit 
sentence. 
Explicit targets: The explicit targets are those 
opinion targets which occur in the current right 
sentence, and the sentence is called explicit sen-
tence. 
In Chinese comments, the phenomena of im-
plicit targets are fairly common. In our dataset, 
the sentences with implicit targets make up near-
ly 30 percents of the total. 
In this paper, we focus on opinion target ex-
traction from news comments and propose a 
novel framework uniformly extracting explicit 
and implicit opinion targets. The method uses 
both information in news articles and informa-
tion in comment contexts to improve the result. 
We extract focused concepts in news articles as 
candidate implicit targets, and exploit a new ap-
proach based on Centering Theory to taking ad-
vantage of comment contexts.  
We evaluate our system on a test corpus con-
taining different topics. The results show that it 
improves the baseline by 8.8%, and the accuracy 
is also 8.1% higher over the popular SVM-based 
method.  
The rest of this paper is organized as follows: 
The next section gives an overview of the related 
work in opinion analysis. Section 3 introduces 
the background of Centering Theory and Section 
4 describes our framework based on Centering 
Theory. In Section 5 we test the results and give 
a discussion on the errors. Finally Section 6 
draws a conclusion. 
2 Related Work 
The early research of opinion mining only fo-
cused on the sentiment classification (Turney et 
al., 2002; Pang et al, 2002). However, for many 
applications only judging the sentiment orienta-
tion is not sufficient (eg. Hu and Liu, 2004). 
Fine-grained opinion analysis has attracted more 
and more attention these years. It mainly in-
cludes these types: opinion holder extraction 
(Kim and Hovy, 2005; Choi et al, 2005), opi-
nion target extraction (Kim and Hovy, 2006; 
Ruppenhofer et al, 2008), and the identification 
of opinion proposals (Bethard et al, 2004) and 
some special opinion expressions (Bloom et al, 
2007). Also, there are some other related tasks, 
such as detecting users? needs and wants (Ka-
nayama and Nasukawa, 2008). However, these 
general systems are different from ours because 
they do not have or use any contextual informa-
tion, and implicit opinion targets are not recog-
nized and handled there. 
A more special domain of feature extraction is 
product and movie reviews. Hu and Liu (2004) 
design a system to mine product features and 
generate opinion summaries of customer reviews. 
Frequent features are extracted by a statistical 
approach, and infrequent features are generated 
by the associated opinion words.  The product 
features are limited in amount and they are 
strongly associated with specific opinion words, 
so researchers can use a fixed set of keywords or 
templates to extract frequent features (Zhuang et 
al., 2006; Popescu and Etzioni, 2005) or try var-
ious methods to augment the database of product 
features and improve the extraction accuracy by 
using the relations between attributes and opi-
nions (Ghani et al, 2006; Su et al, 2008; Jin and 
Ho, 2009; Du and Tan, 2009). However, in news 
comments, the opinion targets are not strongly 
associated with specific opinion words and these 
techniques cannot be used. 
There are also some works focusing on the 
target extraction in news articles, such as 
NTCIR7-MOAT (Seki et al, 2008). Different 
from the news comments, there are opinion indi-
cators in the subjective sentences. However, in 
our task of this paper, the opinion holders are 
pre-assigned as the reviewers, so few opinion 
indicators and holders can be found. 
To our best knowledge, this paper is the first 
work of extracting opinion targets in news com-
ments. We analyze the complex phenomena in 
news comments and propose a framework to 
solve the problems of implicit targets. Our me-
thod synthesizes the information from related 
articles and contexts of comments, and it can 
effectively improve the extracting results. 
783
3 Background of Centering Theory 
Centering Theory (Grosz, Joshi and Weinstein, 
1995) was developed for an original purpose of 
indicating the coherence of a discourse and 
choosing a referring expression. In the theory, 
the term ?centers? of an utterance is used to 
refer to the entities serving to link this utterance 
to another utterance in a discourse. But this is 
not the only function of centers, and there are 
some other useful characteristics of centers to be 
recognized. Our observation shows that a center 
always represents the focus of attention, and the 
salience of a center indicates the significance of 
the component as a commented target. In news 
comments, we consider a comment as a 
discourse and a sentence as an utterance. If an 
utterance has a ?center?, then the center can be 
regarded as the target of the sentence. 
Before introducing the common process of 
choosing the centers in utterances, several defi-
nitions are elaborated as follows: 
Forward-looking center: Given an utter-
ance U, there is a set of forward-looking cen-
ters Cf(U) assigned. The set is a collection of 
all potential centers that may be realized by 
the next utterance. 
Backward-looking center: Each utterance 
is assigned exactly one (in fact at most one) 
backward-looking center Cb. The backward-
looking center of utterance Un+1 connects with 
one of the forward-looking centers of Un. The 
Cb is the real focus of the utterance. 
Rank: The rank is the salience of an ele-
ment of Cf. Ranking of elements in Cf(Un) 
guides determination of Cb(Un+1). The more 
highly ranked an element of Cf(Un ), the more 
likely it is to be Cb(Un+1). The most highly 
ranked element of Cf(Un) that is realized in 
Un+1 is the Cb(Un+1). The rank is affected by 
several factors, the most important of which 
depends on the grammatical role, with SUB-
JECT > OBJECT(S) > OTHER.  
Preferred center: In the set of Cf(Un), the 
element with the highest rank is a preferred 
center Cp(Un). This means that it has the high-
est probability to be Cb(Un+1). 
 
Table 1 is an example of the centers. In the 
example, the target of the first sentence is ?Jack?, 
which is exactly the preferred center; while in 
the second sentence, it is easy to see that ?him? 
gets more attention than ?the company? in this 
environment and thus the backward-looking cen-
ter is more likely to be the target. So we assume 
that if Cb(Un) exists, it can be regarded as the 
opinion target of Un, otherwise the Cp(Un) is the 
target. 
 
Utterance Center 
U1:????????
???????? 
(Jack regards the com-
pany as his life.) 
Cf: ??(Jack)/ 
?? (the company)/ 
??(life) 
Cb: null Cp: ??(Jack) U2: ????????
??????? 
(It attributes to him that 
the company can obtain 
today?s achievement.)
Cf: ??(the company)/ 
??(achievement)/  
?(??) (him(Jack)) 
Cb:?(??) (him(Jack)) Cp:??(the company)
Table 1 Example of different centers. 
4 Proposed Approach 
Due to the problems we introduced in Section 1, 
the techniques of target extraction in other do-
mains are not appropriate in news comments, 
and general approaches encounter the problems 
of free style sentences and implicit targets. For-
tunately, news comments have their own charac-
teristics, which can be used to improve the target 
extraction performance. 
One important characteristic is that though po-
tential opinion targets may be in large quantities, 
most comments focus on several central con-
cepts in the corresponding news article, especial-
ly in the title. So we can extract the focused con-
cepts in the news and use them as potential im-
plicit targets for the comments. 
784
The other useful information comes from the 
fact that the sentences in one comment are usual-
ly coherent. As the comments may be long and 
each comment contains several sentences, the 
sentences within one comment are relevant and 
coherent. So the opinion targets in previous sen-
tences have some influence on that in subsequent 
sentences. Using this kind of contextual informa-
tion, we can eliminate noisy candidates and relax 
the dependence on an unreliable syntactic parser. 
Considering the above characteristics, we 
propose a framework of target extraction based 
on focused concepts recognition and Centering 
Theory, as shown in Figure 1. 
Given a news article and its relevant com-
ments, we first adopt some syntactic rules to 
classify the comment sentences into implicit or 
explicit type. Whether a sentence includes an 
explicit target is mainly decided by whether it 
owns a subject. A few heuristic rules, such as the 
appearance of the subject, the combination of the 
POS, and the position of the predicate, are used 
based on the parse result by using a Chinese 
NLP toolkit1, and the rule-based classification 
can attain an accuracy of 77.33%.  
Then we exploit two different approaches for 
dealing with the two types of sentences, respec-
tively. For the implicit type, we extract the fo-
                                                 
1 LTP, http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm 
LTP is an integrated NLP toolkit which contains segmenta-
tion, parsing, semantic role labeling, and etc. 
cused concepts in the news article as candidate 
implicit targets, and rank them by calculating the 
semantic relatedness between the targets and the 
sentence. For the explicit type, all nouns and 
pronouns in the sentence are extracted as candi-
date targets and ranked mainly by their gram-
matical roles. At last, Centering Theory is used 
to choose the best candidate using the ranks and 
contextual information.  
The details of the main parts are explained in 
the following sections. 
4.1 Focused Concepts (FC) Recognition 
As the comments usually point to the news 
article, it is highly probable that the implicit 
targets appear in the news article. Generally, the 
focused concepts of the news article are more 
likely to be the commented targets. Thus, if we 
extract the focused concepts of the news article, 
we will get the candidate implicit targets. 
In general, the focused concepts are named 
entities (Zhang et al 2004) or specific noun 
phrases. Taking the news 
?????????????????(D
ubai will build the highest skyscraper in the 
world)?    ----NEWS1 
as an example, ???(Dubai)? and 
?????(skyscraper)? are the potential opi-
nion targets. ?Dubai? is a named entity, and 
?skyscraper? is a specific noun phrase. In addi-
tion, the focused concepts may also appear in the 
content of the news article, if they attract enough 
attention or have strong relations with the fo-
cused named entities in the title.   
As the number of noun phrases is usually 
large, if we extract the two types of concepts 
together, there must be much noise to impact the 
final result. To be simple and accurate, we first 
extract focused named entities (FNE), and then 
expand them with other focused noun phrases, 
for the reason that the focused noun phrases 
usually have a strong relation with the focused 
named entities.   
 
Entity Type Person, Location, Organization, 
Time 
Title In title or not 
Frequency The number of occurrence 
Relative 
Frequency 
Frequency/the number of total 
words 
News Article News Comments
Sentences 
of 
Implicit 
Type
Sentences 
of 
Explicit 
Type
Focused named 
entity classifier
Focused 
Concepts
Implicit 
candidate 
targets
Explicit 
candidate 
targets
Choosing a best target via 
Centering Theory
RankingWikipedia-based
ESA
Grammar Role 
Analysis
Opinion 
targets
Rule Based 
Classifier
Figure 1: Framework of opinion target ex-
traction in news comments 
785
Distribution 
Entropy 
(Here we take 
N=5 according 
to the length of 
articles)
 
1
log
N
i
Entropy p pi i=
=? , where 
th
i
Occurrence in the i  Sectionp= Occurrence in Total  
Table 2 Features of FNE classification 
Extracting FNEs can be seen as a classifica-
tion problem. In this work, we choose the fea-
tures in Table 2. 
Given a news document, we first recognize all 
named entities with our own named entity re-
cognizer (NER).Then all named entities are clas-
sified based on the above mentioned features. 
The noun phrases in the title are also extracted 
and filtered by their frequency in the news ar-
ticle and co-occurrences with FNEs. The filter-
ing threshold is set to a relatively high value to 
guarantee that not much noise is brought in. 
Thus we can get a small set of focused concepts 
in the news article. 
4.2 Ranking Implicit Targets 
We use the semantic relatedness to decide which 
potential target is most likely to be the right im-
plicit target. There are many methods to calcu-
late the semantic relatedness. We choose the 
Wikipedia-base explicit semantic analysis (ESA) 
(Gabrilovich and Markovitch, 2007), for its 
adaptability and effectiveness for Chinese lan-
guage. The method converts a word or a sen-
tence to a series of wiki concepts, and then cal-
culates the similarity between words or sen-
tences. 
Input:  a Focused Concept t0 in the news Output: a vector C with a length of N.  C= 
<(cj,wj)>, where cj is a Wikipedia concept, and wj is the weight of cj 1. Find all nouns, adjectives and verbs co-occurring
with t0 in the same sentence, and put them into the set S= {ti}. 2. Compute MI (Mutual information) of each ti with t0. 3. Choose 10 words in S with the highest MI (ac-
cording to the total number of words, 10 is a 
proper value). Combine them with t0 into a word vector and assign each word ti a weight of its frequency vi in the news article. The vec-tor V= <(ti,vi)>, |V|?11. 4. Let <kij> be an inverted index entry for ti, where kij quantifies the strength of association of ti with Wikipedia concept cj. Then the vector V can be interpreted as a vector constructed by 
All Wikipedia concepts. Each concept cj has a 
weight wj= i i ijVt v k?? .5. Select N concepts with the highest weights.  
Table 3: Algorithm that converts a focused 
concept to a vector of Wikipedia concepts 
Chinese Wikipedia is not as large as English 
Wikipedia. When some words are not included 
in the database, the original ESA algorithm will 
fail. To solve the problem, we first expand the 
input FC with a few words extracted from the 
news article. The words represent the semantic 
information related to the article, so they are 
more informative than a single concept while 
easily recognized by the Wikipedia database. 
The details of the algorithm are shown in Table 
3. 
On the other hand, when given a comment 
sentence, we segment it to words and remove the 
stop words (e.g. ?? (of)?). Then the serial of 
words are also converted by ESA into a vector of 
Wikipedia concepts. 
After getting the vectors of wiki concepts for 
focused concepts and the comment sentence, we 
use the cosine metric to obtain their relatedness 
scores. In this way, the focused concepts are 
ranked by their relatedness scores with the sen-
tence. 
4.3 Ranking Explicit Targets 
A comment sentence with explicit targets usual-
ly has a complete syntactic structure. According 
to Centering Theory, the ranks of explicit targets 
are decided mainly by their grammatical roles. 
Generally, a subject is most likely to be the opi-
nion target, and the rank can be heuristically as-
signed by SUBJECT > OBJECT(S) > OTHER. 
4.4 Choosing Best Candidate target via 
Centering Theory (CT) 
After getting the candidate targets and their 
ranks, we start the matching step to make use of 
contextual information. The algorithm originates 
from the process of choosing preferred centers 
and backward-looking centers. A subtle adaption 
is that we add some global information in the 
news article as the context when dealing with the 
first sentence in a comment. The details of the 
algorithm are represented in Table 4. 
Now we give an example to show the whole 
process of the framework. The following com-
ment is associated with NEWS1 in Section 4.1. 
U1:???????????????? 
786
(Dubai is developing travel and trades.) 
U2:??????????? 
((It) is an active city.) 
U3:?????????????? 
(In Dubai you can encounter many miracles.) 
First, U1, U2 and U3 are classified as explicit, 
implicit and explicit, respectively. Then for U1 
and U3 we choose noun phrases and pronouns in 
the sentence as candidate targets and rank them 
according to their grammatical roles. U2 chooses 
FC as candidates, and ?Dubai? is more related 
than ?skyscraper?. At last, the final target is cho-
sen by the algorithm in Table 4 and the whole 
process is illustrated in Table5. 
 
Input: A comment with M sentences S={si}, each sentence has a candidate target set Cf(si)={ci}; The Focused Concepts set FC in the news article. 
Output: A target set {ti}, where each ti is the opinion target of sentence si. 1. For each si in S 2.         If i=1 (si is the first sentence) 3.              For each  ci in Cf(si) 4.                      If ci is contained in FC 5.                            Add ci into the set Cb(si) 6.              If Cb(si) is not void  7.                    Choose the highest ranked ele-ment in Cb(si) as ti  8.              Else 
9.                     Choose the highest ranked ele-ment in Cf(si) as ti 10.       Else 
11.             For each  ci in Cf(si)  12.                  If ci realizes (equals or refers to) an element c?i in Cf(si-1) 13.                            Add c?i into the set Cb(si) 14.             If Cb(si) is not void  15.                   Choose the highest ranked ele-
ment in Cb(si) as ti  16.             Else 17.                 Choose the highest ranked element 
in Cf(si) as ti 
Table 4 Algorithm of choosing the best candi-
date target via CT 
 type ranks of candidates target
U1 Explicit ??>??>????
(Dubai >travel >trade) 
??
(Dubai)
U2 Implicit ??>????
(Dubai>skyscraper) 
?? 
(Dubai)
U3 Explicit ?>??>??
(you>miracles>Dubai) 
?? 
(Dubai)
Table 5 Example of the extraction process 
5 Experiments 
5.1 Evaluation Setup 
To evaluate the whole system, we evaluate not 
only the result of the final target extraction but 
also some key steps. This makes the analysis of 
the bottleneck possible. 
We first build a FNE dataset to evaluate the 
FNE classification result. As our target extrac-
tion task focuses on news comments, we collect 
1000 news articles and the associated user com-
ments from http://comment.news.sohu.com, 
which is a famous website offering a platform 
for users to comment on the news. Every news 
articles are annotated with its focused named 
entities, which are also the most possible com-
mented targets.  
Then we build the target dataset to evaluate 
the final target extraction. 9 articles and asso-
ciated comments are randomly chosen from the 
FNE dataset, and each of their comment sen-
tences is annotated with the opinion target. The 
target dataset focuses on 3 different topics: eco-
nomics, technology and sports. Each document 
contains a news article and about 100 relevant 
comments, and there are 1597 comment sen-
tences in total. 
We assume that each comment sentence has 
one opinion target, but 108 sentences have more 
than one focused objects.  In that case, we anno-
tate all targets for evaluation and the result is 
regarded as true if we extract only one of the 
annotated targets. 
In the target dataset, there are 444 sentences 
with implicit targets. This demonstrates that the 
implicit target extraction problem is prevalent 
and worth solving.  
For the final target extraction, we use the ac-
curacy metric to evaluate the result. It is defined 
as follows: 
We do not use the precision and recall metric 
because every comment sentence in our dataset 
must have a target after extracting. The precision 
and the recall are both equal to the accuracy. 
5.2 Evaluation Results 
5.2.1 FNE Results 
Number of sentences with right extractionAccuracy= Number of total sentences
787
We perform a 4:1 cross-validation on the FNE 
dataset using a commonly used classifier SVM-
light2 and gain a mean f-measure of 80.43%. 
Then, to assess the improvement by the FNE 
step and the classification of implicit and explicit 
sentences, we estimate the theoretic upper limit 
of the following three target extractions on the 
target dataset. Test 1 assumes every noun phras-
es or nouns in the sentence can be possible to be 
extracted as the target. So if there is one candi-
date matching the target, we can recognize the 
sentence as extractable. Test 2 adopts the anno-
tation results of the classification of explicit and 
implicit sentences. For the manually annotated 
implicit targets, we adapt the candidate to be FC. 
Then, as same as Test 1, all candidates are de-
termined whether to be the target. In Test 3, we 
follow the ruled-based classification of implicit 
and explicit sentences in our system and then 
judge the sentences whether extractable or not.  
 
 Proportion of extractable sentences 
Test 1 55.0% 
Test 2 69.6% 
Test 3 61.7% 
Table 6 Improvement of the proportion of ex-
tractable sentences by FNE classification and 
explicit/implicit sentence classification 
 
Table 6 shows the proportions of extractable 
sentences in the three tests. It is easy to see that 
the proportion of extractable sentences means 
the theoretic optimization of target extraction. So, 
by Test 2 we can see the extracted FC set is an 
effective complement of the candidate targets, 
while Test 3 demonstrates that the system still 
has much potential to improve the baseline after 
the rule-based classification of explicit and im-
plicit sentences.  
 
5.2.2 Target Extraction Results 
To demonstrate the effectiveness of our ap-
proach, we design two baselines.  
Baseline 1 treats all sentences as explicit type. 
In the method, we extract all noun phrases and 
pronouns in a sentence as candidates and obtain 
their ranks according to their grammatical roles.  
Baseline 2, a SVM-based approach, is offered 
to compare with the popular target extraction 
methods. In this method we regard the target 
                                                 
2 http://svmlight.joachims.org/ 
extraction as a classification problem. We ex-
tract the candidate noun phrases in a sentence 
first, and then use the semantic features to classi-
fy them as targets or not. The features mainly 
include: POS, whether or not a Named Entity, 
the positions in the sentence, the syntactic rela-
tions with the verb, and etc. As it is a supervised 
approach, the result is tested by a 2:1 cross vali-
dation. 
Then we use a method called FC-only (using 
only Focused Concepts) to improve Baseline 1 
by using the global information in news articles. 
For sentences of explicit type, we use the me-
thod in Baseline1. For sentences of implicit type, 
we take focused concepts in news articles as po-
tential targets, and choose the highest ranked 
element as the final target. 
Finally, our proposed approach CT (using 
Centering Theory) uses both Focused Concepts 
and Centering Theory. When the size of Wiki-
pedia concept vector is set to be 800, the com-
parison results of the four approaches are shown 
in Table 7: 
 
Accuracy
Baseline1 34.38% 
Baseline2(SVM-based) 35.13% 
FC-only 37.25% 
CT 43.20% 
Table 7 Comparison results 
FC-only is better than Baseline1, which de-
monstrates that the focused concepts are useful 
to provide information to implicit targets extrac-
tion. 444 implicit sentences are a large propor-
tion of the total corpus. And the focused con-
cepts do represent the global information and 
have influence on the target extraction. 
Centering Theory is naturally another im-
provement. It mainly takes advantage of the in-
formation of contexts within a comment, using a 
rule of coherence to decide the center of atten-
tion. And the result indicates that it is very help-
ful.  
Compared with the SVM-based approach, our 
approach is also much better. The SVM-based 
approach is only a little higher than Baseline 1. 
It seems that the manually annotated information 
is not very useful in target extraction in news 
comments. The reason may be that the target 
rules are complicated and exist not only in the 
current sentence. Using global and contextual 
788
information is a more economic and effective 
way to improve the result.  
In the Wikipedia-based ESA algorithm,there 
is a parameter of N, which is the vector size of 
the expanded vector. It is important to choose a 
proper parameter value to achieve a high accura-
cy and meanwhile keep a low computational 
complexity. The accuracy curves for FC and CT 
with different values of N are represented in 
Figure 2. Apparently, when N exceeds 600, the 
extraction performance almost does not change 
any more. So we finally take 800 as the value of 
N . 
5.3 Error Analysis  
Generally there are two major types of errors 
in the extraction results. One common error is 
that the target is not in our extracted candidate 
nouns or noun phrases. For example: 
??????????????????.? (It 
is a disaster of Chinese beverage that Coca Cola 
buys HuiYuan.) 
The sentence comments on the event of ?Coca 
Cola buys HuiYuan? but not a single concept 
?Coca Cola? or ?HuiYuan?. But our system can-
not recognize this type of targets properly. Also 
there are some cases that the noun phrases 
missed to be extracted by the LTP toolkit. It 
causes that the target is not matched by the can-
didates.  
Another error originates from the wrong clas-
sification of explicit and implicit sentences. For 
example, 
?????????????????.? (Re-
turning profits to civilians can get through the 
crisis of little companies.) 
In this sentence, ?????(Returning profits 
to civilians)? is the opinion target and the sen-
tence has a explicit target. But the rules based on 
the Chinese parser failed to recognize the phrase 
as a subject and thus the sentence is considered 
as implicit type by our approach. And lastly the 
target is extracted incorrectly. 
In 5.2.1, we test the theoretic upper limit of 
the target extraction and prove the potential ef-
fectiveness of two steps. The tests also can be 
used to estimate the proportion of the types of 
errors and analyze the bottleneck. In Test 2, 
there are 298 un-extractable sentences among 
the annotated explicit sentences. It shows that 
there is at least 18.6% loss in accuracy caused by 
the candidate recognition, which accounts for the 
first error type. As for the second error type, its 
proportion can be computed by the reduction 
from Test 2 to Test 3, which is 7.9%. 
6 Conclusion and Future Work  
In this paper, we propose a novel approach to 
extracting opinion targets in Chinese news 
comments. In order to solve the problem of im-
plicit target extraction, we extract focused con-
cepts and rank their importance by computing 
the semantic relatedness with sentences via Wi-
kipedia. In addition, we apply Centering Theory 
to the target extraction system, for utilizing con-
textual information. The experiment results 
demonstrate that our approach is effective.  
Currently, the result does not reach an abso-
lutely high accuracy. One bottleneck is that Chi-
nese parsing results are far from satisfactory. 
Actually this bottleneck has impacted the gener-
al target extraction long, such as the low perfor-
mances of all participants in the target extraction 
task of NTCIR7-MOAT-CS. We hope to im-
prove our results by avoid this disadvantage. 
Moreover, the phenomenon of implicit opinion 
targets exists not only in Chinese but also in 
English and other languages, while sometimes it 
is similar to zero anaphora. So the approach in 
this paper can be extended to news comments in 
other languages.  
Acknowledgement 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03), NCET 
(NCET-08-0006) and National High-tech R&D 
Program (2008AA01Z421). We thank the ano-
nymous reviewers for their useful comments. 
Figure 2: Accuracy vs. vector size N
789
References 
Bethard, Steven, Hong Yu, Ashley Thornton, Vasi-
leios Hatzivassiloglou, and Dan Jurafsky. 2004. 
Automatic Extraction of Opinion Propositions and 
their Holders. In Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text: Theories and Applications. 
Choi, Yejin, Claire Cardie, Ellen Riloff, and Sidd-
harth Patwardhan. 2005. Identifying Sources of 
Opinions with Conditional Random Fields and Ex-
traction Patterns. In Proceeding of HLT/EMNLP? 
05. 
Ding Xiaowen, Bing Liu, Philip S. Yu. 2008. A Ho-
listic Lexicon based Approach to Opinion Mining.  
Proceeding of the international conference on Web 
Search and Web Data Mining (WSDM?08), 231-
239. 
Du, Weifu. and Songbo Tan. 2009. An Iterative Rein-
forcement Approach for Fine-Grained Opinion 
Mining. The 2009 Annual Conference of the North 
American Chapter of the ACL 
Gabrilovich, Evgeniy. and Shaul Markovitch. 2007. 
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings 
of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI). 
Ghani, Rayid, Katharina Probst, Yan Liu, Marko 
Krema, and Andrew Fano. 2006. Text Mining for 
Product Attribute Extraction. The Twelfth ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining. 
Grosz, Barbara J., Scott Winstein, and Aravind K. 
Joshi. (1995). Centering: A Framework for Model-
ing the Local Coherence of Discourse. In Compu-
tational Linguistics, 21(2). 
Hu, Minqing and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
Nineteenth National Conference on Artificial Intel-
ligence (AAAI-2004) 
Jin, Wei and Hung Hay Ho. 2009. A Novel Lexica-
lized HMM-based Learning Framework for Web 
Opinion Mining. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML 
2009). 
Jin, Wei and Hung Hay Ho, Rohini K. Srihari. 2009. 
OpinionMiner: A Novel Machine Learning System 
for Web Opinion Mining and Extraction. In The 
15th ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed 
in Online News Media Text. In ACL Workshop on 
Sentiment and Subjectivity in Text. 
Kim, Soo-Min. and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opi-
nion Texts. In Proceedings of AAAI-05 Workshop 
on Question Answering in Restricted Domains. 
Pang, Bo and Lillian Lee, and Vaithyanathan, S. 2002. 
Thumbs up? Sentiment classification using ma-
chine learning techniques. In EMNLP 2002. 
Popescu, Ana-Maria. and Oren Etzioni. 2005. Ex-
tracting Product Features and Opinions from Re-
views. In Proceeding of 2005 Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP?05), 339-346. 
Riloff, Ellen and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Pro-
ceedings of the 2003 Conference on EMNLP.  
Ruppenhofer, Josef, Swapna Somasundaran, and Ja-
nyce Wiebe. 2008. Finding the Sources and Tar-
gets of Subjective Expressions. In LREC08. 
Seki, Yohei, David K. Evans, Lun-Wei Ku, Le Sun, 
Hsin-Hsi Chen, and Noriko Kando. 2008. Over-
view of Multilingual Opinion Analysis Task at 
NTCIR-7. The 7th NTCIR workshop (2007/2008). 
Su Qi, Xinying Xu, Honglei Guo, Zhili Guo, XianWu, 
Xiaoxun Zhang, Bin Swen and Zhong Su. 2008. 
Hidden Sentiment Association in Chinese WebO-
pinion Mining. In The 17th International World 
Wide Web Conference (WWW). 
Turney, Peter D. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). 
Zhang, Li, Yue Pan, and Tong Zhang. 2004. Focused 
Named Entity Recognition using Machine Learn-
ing. The 27th Annual International ACM SIGIR 
Conference. 
Zhuang, Li, Feng Jing. and Xiao-yan Zhu. 2006. 
Movie Review Mining and Summarization. In Pro-
ceedings of the 15th ACM International Confe-
rence on Information and Knowledge Management 
(CIKM?06), 43-50. 
790
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 433?443,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Timeline Generation through Evolutionary Trans-Temporal Summarization
Rui Yan?, Liang Kong? , Congrui Huang?, Xiaojun Wan?, Xiaoming Li\, Yan Zhang??
?School of Electronics Engineering and Computer Science, Peking University, China
?Institute of Computer Science and Technology, Peking University, China
\State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China
{r.yan,kongliang,hcr,lxm}@pku.edu.cn,
wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn
Abstract
We investigate an important and challeng-
ing problem in summary generation, i.e.,
Evolutionary Trans-Temporal Summarization
(ETTS), which generates news timelines from
massive data on the Internet. ETTS greatly
facilitates fast news browsing and knowl-
edge comprehension, and hence is a neces-
sity. Given the collection of time-stamped web
documents related to the evolving news, ETTS
aims to return news evolution along the time-
line, consisting of individual but correlated
summaries on each date. Existing summariza-
tion algorithms fail to utilize trans-temporal
characteristics among these component sum-
maries. We propose to model trans-temporal
correlations among component summaries for
timelines, using inter-date and intra-date sen-
tence dependencies, and present a novel com-
bination. We develop experimental systems to
compare 5 rival algorithms on 6 instinctively
different datasets which amount to 10251 doc-
uments. Evaluation results in ROUGE metrics
indicate the effectiveness of the proposed ap-
proach based on trans-temporal information.
1 Introduction
Along with the rapid growth of the World Wide
Web, document floods spread throughout the Inter-
net. Given a large document collection related to
a news subject (for example, BP Oil Spill), readers
get lost in the sea of articles, feeling confused and
powerless. General search engines can rank these
?Corresponding author.
news webpages by relevance to a user specified as-
pect, i.e., a query such as ?first relief effort for BP
Oil Spill?, but search engines are not quite capable
of ranking documents given the whole news subject
without particular aspects. Faced with thousands of
news documents, people usually have a myriad of in-
terest aspects about the beginning, the development
or the latest situation. However, traditional infor-
mation retrieval techniques can only rank webpages
according to their understanding of relevance, which
is obviously insufficient (Jin et al, 2010).
Even if the ranked documents could be in a satis-
fying order to help users understand news evolution,
readers prefer to monitor the evolutionary trajecto-
ries by simply browsing rather than navigate every
document in the overwhelming collection. Summa-
rization is an ideal solution to provide an abbrevi-
ated, informative reorganization for faster and bet-
ter representation of news documents. Particularly,
a timeline (see Table 1) can summarize evolutionary
news as a series of individual but correlated com-
ponent summaries (items in Table 1) and offer an
option to understand the big picture of evolution.
With unique characteristics, summarizing time-
lines is significantly different from traditional sum-
marization methods which are awkward in such sce-
narios. We first study a manual timeline of BP Oil
Spill in Mexico Gulf in Table 1 from Reuters News1
to understand why timelines generation is observ-
ably different from traditional summarization. No
traditional method has considered to partition corpus
into subsets by timestamps for trans-temporal cor-
relations. However, we discover two unique trans-
1http://www.reuters.com
433
Table 1: Part of human generated timeline about BP Oil
Spill in 2010 from Reuters News website.
April 22, 2010
The Deepwater Horizon rig, valued at more than $560 million,
sinks and a five mile long (8 km) oil slick is seen.
April 25, 2010
The Coast Guard approves a plan to have remote underwater vehi-
cles activate a blowout preventer and stop leak. Efforts to activate
the blowout preventer fail.
April 28, 2010
The Coast Guard says the flow of oil is 5,000 barrels per day (bpd)
(210,000 gallons/795,000 litres) ? five times greater than first esti-
mated. A controlled burn is held on the giant oil slick.
April 29, 2010
U.S. President Barack Obama pledges ?every single available re-
source,? including the U.S. military, to contain the spreading spill.
Obama also says BP is responsible for the cleanup. Louisiana de-
clares state of emergency due to the threat to the state?s natural
resources.
April 30, 2010
An Obama aide says no drilling will be allowed in new areas, as the
president had recently proposed, until the cause of the Deepwater
Horizon accident is known.
temporal characteristics of component summaries
from the handcrafted timeline. Individuality. The
component summaries are summarized locally: the
component item on date t is constituted by sentences
with timestamp t. Correlativeness. The compo-
nent summaries are correlative across dates, based
on the global collection. To the best of our knowl-
edge, no traditional method has examined the rela-
tionships among these timeline items.
Although it is profitable, summarizing timeline
faces with new challenges:
? The first challenge for timeline generation is
to deliver important contents and avoid information
overlaps among component summaries under the
trans-temporal scenario based on global/local source
collection. Component items are individual but not
completely isolated due to the dynamic evolution.
? As we have individuality and correlativeness
to evaluate the qualities of component summaries,
both locally and globally, the second challenge is to
formulate the combination task into a balanced op-
timization problem to generate the timelines which
satisfy both standards with maximum utilities.
We introduce a novel approach for the web min-
ing problem Evolutionary Trans-Temporal Summa-
rization (ETTS). Taking a collection relevant to a
news subject as input, the system automatically out-
puts a timeline with items of component summaries
which represent evolutionary trajectories on specific
dates. We classify sentence relationships as inter-
date and intra-date dependencies. Particularly, the
inter-date dependency calculation includes temporal
decays to project sentences from all dates onto the
same time horizon (Figure 1 (a)). Based on intra-
/inter-date sentence dependencies, we then model
affinity and diversity to compute the saliency score
of each sentence and merge local and global rank-
ings into one unified ranking framework. Finally we
select top ranked sentences. We build an experimen-
tal system on 6 real datasets to verify the effective-
ness of our methods compared with other 4 rivals.
2 Related Work
Multi-document summarization (MDS) aims to pro-
duce a summary delivering the majority of informa-
tion content from a set of documents and has drawn
much attention in recent years. Conferences such as
ACL, SIGIR, EMNLP, etc., have advanced the tech-
nology and produced several experimental systems.
Generally speaking, MDS methods can be either
extractive or abstractive summarization. Abstractive
summarization (e.g. NewsBlaster2) usually needs
information fusion, sentence compression and refor-
mulation. We focus on extraction-based methods,
which usually involve assigning saliency scores to
some units (e.g. sentences, paragraphs) of the docu-
ments and extracting the units with highest scores.
To date, various extraction-based methods have
been proposed for generic multi-document summa-
rization. The centroid-based method MEAD (Radev
et al, 2004) is an implementation of the centroid-
based method that scores sentences based on fea-
tures such as cluster centroids, position, and TF.IDF,
etc. NeATS (Lin and Hovy, 2002) adds new features
such as topic signature and term clustering to select
important content, and use MMR (Goldstein et al,
1999) to remove redundancy.
Graph-based ranking methods have been pro-
posed to rank sentences/passages based on ?votes?
or ?recommendations? between each other. Tex-
tRank (Mihalcea and Tarau, 2005) and LexPageR-
ank (Erkan and Radev, 2004) use algorithms similar
to PageRank and HITS to compute sentence impor-
tance. Wan et al have improved the graph-ranking
2http://www1.cs.columbia.edu/nlp/newsblaster/
434
algorithm by differentiating intra-document and
inter-document links between sentences (2007b),
and have proposed a manifold-ranking method to
utilize sentence-to-sentence and sentence-to-topic
relationships (Wan et al, 2007a).
ETTS seems to be related to a very recent task of
?update summarization? started in DUC 2007 and
continuing with TAC. However, update summariza-
tion only dealt with a single update and we make a
novel contribution with multi-step evolutionary up-
dates. Further related work includes similar timeline
systems proposed by (Swan and Allan, 2000) us-
ing named entities, by (Allan et al, 2001) measured
in usefulness and novelty, and by (Chieu and Lee,
2004) measured in interest and burstiness. We have
proposed a timeline algorithm named ?Evolution-
ary Timeline Summarization (ETS)? in (Yan et al,
2011b) but the refining process based on generated
component summaries is time consuming. We aim
to seek for more efficient summarizing approach.
To the best of our knowledge, neither update sum-
marization nor traditional systems have considered
the relationship among ?component summaries?, or
have utilized trans-temporal properties. ETTS ap-
proach can also naturally and simultaneously take
into account global/local summarization with biased
information richness and information novelty, and
combine both summarization in optimization.
3 Trans-temporal Summarization
We conduct trans-temporal summarization based on
the global biased graph using inter-date dependency
and local biased graph using intra-date dependency.
Each graph is the complementary graph to the other.
3.1 Global Biased Summarization
The intuition for global biased summarization is that
the selected summary should be correlative with sen-
tences from neighboring dates, especially with those
informative ones. To generate the component sum-
mary on date t, we project all sentences in the collec-
tion onto the time horizon of t to construct a global
affinity graph, using temporal decaying kernels.
3.1.1 Temporal Proximity Based Projection
Clearly, a major technical challenge in ETTS is
how to define the temporal biased projection func-
tion ?(?t), where ?t is the distance between the
Figure 1: Construct global/local biased graphs. Solid cir-
cles denote intra-date sentences on the pending date t and
dash ones represent inter-date sentences from other dates.
Figure 2: Proximity-based kernel functions, where ?=10.
pending date t and neighboring date t?, i.e., ?t =
|t? ? t|. As in (Lv and Zhai, 2009), we present 5
representative kernel functions: Gaussian, Triangle,
Cosine, Circle, and Window, shown in Figure 2. Dif-
ferent kernels lead to different projections.
1. Gaussian kernel
?(?t) = exp[??t
2
2?2 ]
2. Triangle kernel
?(?t) =
{
1? ?t? if ?t ? ?
0 otherwise
3. Cosine (Hamming) kernel
?(?t) =
{
1
2 [1 + cos(?t?pi? )] if ?t ? ?
0 otherwise
4. Circle kernel
?(?t) =
{?
1? (?t? )2 if ?t ? ?
0 otherwise
435
5. Window kernel
?(?t) =
{
1 if ?t ? ?
0 otherwise
All kernels have one parameter ? to tune, which
controls the spread of kernel curves, i.e., it restricts
the projection scope of each sentence. In general,
the optimal setting of ? may vary according to the
news set because sentences presumably would have
wider semantic scope in certain news subjects, thus
requiring a higher value of ? and vice versa.
3.1.2 Modeling Global Affinity
Given the sentence collectionC partitioned by the
timestamp set T , C = {C1, C2, . . . , C |T |}, we ob-
tain Ct = {sti|1 ? i ? |Ct|} where si is a sentence
with the timestamp t = tsi . When we generate com-
ponent summary on t, we project all sentences onto
time horizon t. After projection, all sentences are
weighted by their influence on t. We use an affinity
matrix M t with the entry of the inter-date transition
probability on date t. The sum of each row equals to
1. Note that for the global biased matrix, we mea-
sure the affinity between local sentences from t and
global sentences from other dates. Therefore, intra-
date transition probability between sentences with
the timestamp t is set to 0 for local summarization.
M ti,j is the transition probability of si to sj based
on the perspective of date t, i.e., p(si ? sj |t):
p(si ? sj |t) =
{ f(si?sj |t)?
|C| f(si?sk|t)
if ? f 6= 0
0 if tsi = tsj = t
(1)
f(si ? sj |t) is defined as the temporal weighted
cosine similarity between two sentences:
f(si ? sj |t) =
?
w?si?sj
pi(w, si|t) ? pi(w, sj |t) (2)
where the weight pi associated with term w is calcu-
lated with the temporal weighted tf.isf formula:
pi(w, s|t) =
?|t? ts| ? tf(w, s)(1 + log( |C|Nw ))??
|s|(tf(w, s)(1 + log(
|C|
Nw )))2
.
(3)
where ts is the timestamp of sentence s, and
tf(w, s) is the term frequency of w in s. ts can be
any date from T . |C| is the sentences set size and
Nw is the number of sentences containing term w.
We let p(si ? si|t)=0 to avoid self transition.
Note that although f(.) is a symmetric function,
p(si ? sj |t) is usually not equal to p(sj ? si|t),
depending on the degrees of nodes si and sj .
Now we establish the affinity matrix M ti,j and by
using the general form of PageRank, we obtain:
~? = ?M?1~?+ 1? ?|C| ~e (4)
where ~? is the selective probability of all sentence
nodes and ~e is a column vector with all elements
equaling to 1. ? is the damping factor set as 0.85.
Usually the convergence of the iteration algorithm is
achieved when difference between the scores com-
puted at two successive iterations for any sentences
falls below a given threshold (0.0001 in this study).
3.1.3 Modeling Diversity
Diversity is to reflect both biased information
richness and sentence novelty, which aims to reduce
information redundancy. However, using standard
PageRank of Equation (4) will not result in diver-
sity. The aggregational effect of PageRank assigns
high salient scores to closely connected node com-
munities (Figure 3 (b)). A greedy vertex selection
algorithm may achieve diversity by iteratively se-
lecting the most prestigious vertex and then penal-
izing the vertices ?covered? by the already selected
ones, such as Maximum Marginal Relevance and its
applications in Wan et al (2007b; 2007a). Most re-
cently diversity rank DivRank is another solution
to diversity penalization in (Mei et al, 2010).
We incorporate DivRank in our general ranking
framework, which creates a dynamicM during each
iteration, rather than a static one. After z times of
iteration, the matrix M becomes:
M (z) = ?M (z?1) ? ~?(z?1) + 1? ?|C| ~e (5)
Equation (5) raises the probability for nodes with
higher centrality and nodes already having high
weights are likely to ?absorb? the weights of its
neighbors directly, and the weights of neighbors?
neighbors indirectly. The process is to iteratively ad-
just matrix M according to ~? and then to update ~?
according to the changed M . As iteration increases
436
there emerges a rich-gets-richer phenomenon (Fig-
ure 3 (c) and (d)). By incorporating DivRank, we
obtain rank r?i and the global biased ranking score
Gi for sentence si from date t to summarize Ct.
3.2 Local Biased Summarization
Naturally, the component summary for date t should
be informative within Ct. Given the sentence col-
lection Ct = {sti|1 ? i ? |Ct|}, we build an affin-
ity matrix for Figure 1 (b), with the entry of intra-
date transition probability calculated from standard
cosine similarity. We incorporate DivRank within
local summarization and we obtain the local biased
rank and ranking score for si, denoted as r?i and Li.
3.3 Optimization of Global/Local Combination
We do not directly add the global biased ranking
score and local biased ranking score, as many previ-
ous works did (Wan et al, 2007b; Wan et al, 2007a),
because even the same ranking score gap may indi-
cate different rank gaps in two ranking lists.
Given subset Ct, let R = {ri}(i = 1,. . . ,|Ct|), ri
is the final ranking of si to estimate, optimize the
following objective cost function O(R),
O(R) =?
|Ct|?
i=1
Gi?
ri
?i
? r
?
i
Gi
?2
+ ?
|Ct|?
i=1
Li?
ri
?i
? r
?
i
Li
?2
(6)
where Gi is the global biased ranking score while Li
is the local biased ranking score. ?i is expected to
be the merged ranking score, namely sentence im-
portance, which will be defined later. Among the
two components in the objective function, the first
component means that the refined rank should not
deviate too much from the global biased rank. We
use ? ri?i ?
r?i
Gi ?
2 instead of ?ri? r?i ?2 in order to dis-
tinguish the differences between sentences from the
same rank gap. The second component is similar by
refining rank from local biased summarization.
Our goal is to find R = R? to minimize the cost
function, i.e.,R? = argmin{O(R)}. R? is the final
rank merged by our algorithm. To minimize O(R),
we compute its first-order partial derivatives.
?O(R)
?ri
= 2??i
( Gi?i
ri ? r?i ) +
2?
?i
(Li?i
ri ? r?i ) (7)
Let ?O(R)?ri = 0, we get
r?i =
??ir?i + ??ir
?
i
?Gi + ?Li
(8)
Two special cases are that if (1) ? = 0, ? 6= 0:
we obtain ri = ?ir?i /Li, indicating we only use the
local ranking score. (2) ? 6= 0, ? = 0, indicating we
ignore local ranking score and only consider global
biased summarization using inter-date dependency.
There can be many ways to calculate the sen-
tence importance ?i. Here we define ?i as the
weighted combination of itself with ranking scores
from global biased and local biased summarization:
?(z)i =
?Gi + ?Li + ??(z?1)i
?+ ? + ? . (9)
To save one parameter we let ?+?+? = 1. In the z-
th iteration, r(z)i is dependent on ?(z?1)i and ?(z)i is
indirectly dependent on r(z)i via ?(z?1)i . ?(0)i = 0.
We iteratively approximate final ?i for the ultimate
rank listR?. The expectation of stable ?i is obtained
when ?(z)i = ?(z?1)i . Final ?i is expected to satisfy
?i = ?Gi + ?Li + ??i:
?i =
?Gi + ?Li
1? ? =
?Gi + ?Li
?+ ? (10)
Final ?i is dependent only on original global/local
biased ranking scores. Equation (8) becomes more
concise with no ? or ?: r? is a weighted combina-
tion of global and local ranks by ?? (? 6= 0, ? 6= 0):
r?i =
?
?+ ? r
?
i +
?
?+ ? r
?
i
= 11 + ?/?r
?
i +
1
1 + ?/? r
?
i
(11)
4 Experiments and Evaluation
4.1 Datasets
There is no existing standard test set for ETTS meth-
ods. We randomly choose 6 news subjects with
special coverage and handcrafted timelines by ed-
itors from 10 selected news websites: these 6 test
sets consist of news datasets and golden standards to
evaluate our proposed framework empirically, which
amount to 10251 news articles. As shown in Ta-
ble 2, three of the sources are in UK, one of them
437
(a) An illustrative network. (a) PageRank on t. (b) DivRank on t (c) DivRank on t?
Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from
DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect
temporal biased processes on t {1,9} in (c) and t? {2,12} in (d).
is in China and the rest are in the US. We choose
these sites because many of them provide timelines
edited by professional editors, which serve as refer-
ence summaries. The news belongs to different cate-
gories of Rule of Interpretation (ROI) (Kumaran and
Allan, 2004). More detailed statistics are in Table 3.
Table 2: News sources of 6 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 6 datasets.
News Subjects #size #docs #stamps #RT AL
1.Influenza A 115026 2557 331 5 83
2.Financial Crisis 176435 2894 427 2 118
3.BP Oil Spill 63021 1468 135 6 76
4.Haiti Earthquake 12073 247 83 2 32
5.Jackson Death 37819 925 168 3 64
6.Obama Presidency 79761 2160 349 5 92
size: the whole sentence counts; #stamps: the number of timestamps;
Note average size of subsets is calculated as: avg.size=#size/#stamps;
RT: reference timelines; AL: avg. length of RT measured in sentences.
4.2 Experimental System Setups
? Preprocessing. As ETTS faces with much larger
corpus compared with traditional MDS, we apply
further data preprocessing besides stemming and
stop-word removal. We extract text snippets repre-
senting atomic ?events? from all documents with a
toolkit provided by Yan et al (2010; 2011a), by
which we attempt to assign more fine-grained and
accurate timestamps for every sentence within the
text snippets. After the snippet extraction procedure,
we filter the corpora by discarding non-event texts.
? Compression Rate and Date Selection. After
preprocessing, we obtain numerous snippets with
fine-grained timestamps, and then decompose them
into temporally tagged sentences as the global col-
lection C. We partition C according to timestamps
of sentences, i.e., C = C1 ? C2 ? ? ? ? ? C |T |.
Each component summary is generated from its cor-
responding sub-collection. The sizes of component
summaries are not necessarily equal, and moreover,
not all dates may be represented, so date selection
is also important. We apply a simple mechanism
that users specify the overall compression rate ?, and
we extract more sentences for important dates while
fewer sentences for others. The importance of dates
is measured by the burstiness, which indicates prob-
able significant occurrences (Chieu and Lee, 2004).
The compression rate on ti is set as ?i = |Ci||C| .
4.3 Evaluation Metrics
The ROUGE measure is widely used for evaluation
(Lin and Hovy, 2003): the DUC contests usually of-
ficially employ ROUGE for automatic summariza-
tion evaluation. In ROUGE evaluation, the summa-
rization quality is measured by counting the num-
ber of overlapping units, such as N-gram, word se-
quences, and word pairs between the candidate time-
lines CT and the reference timelines RT . There are
several kinds of ROUGE metrics, of which the most
important one is ROUGE-N with 3 sub-metrics:
1 ROUGE-N-R is an N-gram recall metric:
ROUGE-N-R =
?
I?RT
?
N-gram?I
Countmatch(N-gram)
?
I?RT
?
N-gram?I
Count (N-gram)
438
2 ROUGE-N-P is an N-gram precision metric:
ROUGE-N-P =
?
I?CT
?
N-gram?I
Countmatch(N-gram)
?
I?CT
?
N-gram?I
Count (N-gram)
3 ROUGE-N-F is an N-gram F1 metric:
ROUGE-N-F = 2? ROUGE-N-P? ROUGE-N-RROUGE-N-P + ROUGE-N-R
I denotes a timeline. N in these metrics stands for
the length of N-gram and N-gram?RT denotes the
N-grams in reference timelines while N-gram?CT
denotes the N-grams in the candidate timeline.
Countmatch(N-gram) is the maximum number of N-
gram in the candidate timeline and in the set of ref-
erence timelines. Count(N-gram) is the number of N-
grams in reference timelines or candidate timelines.
According to (Lin and Hovy, 2003), among all
sub-metrics, unigram-based ROUGE (ROUGE-1)
has been shown to agree with human judgment most
and bigram-based ROUGE (ROUGE-2) fits summa-
rization well. We report three ROUGE F-measure
scores: ROUGE-1, ROUGE-2, and ROUGE-W,
where ROUGE-W is based on the weighted longest
common subsequence. The weight W is set to be
1.2 in our experiments by ROUGE package (version
1.55). Intuitively, the higher the ROUGE scores, the
similar the two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used sum-
marization algorithms as baseline systems. They
are designed for traditional summarization without
trans-temporal dimension. The first intuitive way to
generate timelines by these methods is via a global
summarization on collection C and then distribu-
tion of selected sentences to their source dates. The
other one is via an equal summarization on all local
sub-collections. For baselines, we average both in-
tuitions as their performance scores. For fairness we
conduct the same preprocessing for all baselines.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according
to the following three parameters: centroid value,
positional value, and first-sentence overlap.
GMDS: The graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
Chieu: (Chieu and Lee, 2004) present a simi-
lar timeline system with different goals and frame-
works, utilizing interest and burstiness ranking but
neglecting trans-temporal news evolution.
ETTS: ETTS is an algorithm with optimized
combination of global/local biased summarization.
RefTL: As we have used multiple human time-
lines as references, we not only provide ROUGE
evaluations of the competing systems but also of the
human timelines against each other, which provides
a good indicator as to the upper bound ROUGE
score that any system could achieve.
4.5 Overall Performance Comparison
We use a cross validation manner among 6 datasets,
i.e., train parameters on one subject set and exam-
ine the performance on the others. After 6 training-
testing processes, we take the average F-score per-
formance in terms of ROUGE-1, ROUGE-2, and
ROUGE-W on all sets. The overall results are shown
in Figure 4 and details are listed in Tables 4?6.
Figure 4: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected.
? The results of Centroid are better than those of
Random, mainly because the Centroid method takes
439
Table 4: Overall performance comparison on Influenza
A (ROI? category: Science) and Financial Crisis (ROI
category: Finance). ?=0.4, kernel=Gaussian, ?=60.
1. Influenza A 2. Financial Crisis
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.491 0.114 0.161 0.458 0.112 0.159
Random 0.257 0.039 0.081 0.230 0.030 0.071
Centroid 0.331 0.050 0.114 0.305 0.041 0.108
GMDS 0.364 0.062 0.130 0.327 0.054 0.110
Chieu 0.350 0.059 0.128 0.325 0.052 0.109
ETTS 0.375 0.071 0.132 0.339 0.058 0.112
Table 5: Overall performance comparison on BP Oil
(ROI category: Accidents) and Haiti Quake (ROI cate-
gory: Disasters). ?=0.4, kernel=Gaussian, ?=30.
3. BP Oil 4. Haiti Quake
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.517 0.135 0.183 0.528 0.139 0.187
Random 0.262 0.041 0.096 0.266 0.043 0.093
Centroid 0.369 0.062 0.128 0.362 0.060 0.129
GMDS 0.389 0.084 0.139 0.380 0.106 0.137
Chieu 0.384 0.083 0.139 0.383 0.110 0.138
ETTS 0.441 0.107 0.158 0.436 0.111 0.145
Table 6: Overall performance comparison on Jackson
Death (ROI category: Legal Cases) and Obama Presi-
dency (ROI category: Politics). ?=0.4, kernel=Gaussian,
?=30.
5. Jackson Death 6. Obama Presidency
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.482 0.113 0.161 0.495 0.115 0.163
Random 0.232 0.033 0.080 0.254 0.039 0.084
Centroid 0.320 0.051 0.109 0.325 0.053 0.111
GMDS 0.341 0.059 0.127 0.359 0.061 0.129
Chieu 0.344 0.059 0.128 0.346 0.060 0.125
ETTS 0.358 0.061 0.130 0.369 0.074 0.133
?ROI: news categorization defined by Linguistic Data Consortium.
into account positional value and first-sentence over-
lap, which facilitate main aspects summarization.
? The GMDS system outperforms centroid-based
summarization methods. This is due to the fact that
PageRank-based framework ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
Traditional MDS only consider sentence selection
from either the global or the local scope, and hence
bias occurs. Mis-selected sentences result in a low
recall. Generally the performance of global priority
intuition (i.e. only global summarization and then
distribution to temporal subsets) is better than local
priority methods (only local summarization). Proba-
ble bias is enlarged by searching for worthy sentence
in single dates. However, precision drops due to ex-
cessive choice of global timeline-worthy sentences.
Figure 5: ?/?: global/local combination.
Figure 6: ? on long topics (?1 year).
Figure 7: ? on short topics (<1 year).
? In general, the result of Chieu is better than
Centroid but unexpectedly, worse than GMDS. The
reason may be that Chieu does not capture suffi-
cient timeline attributes. The ?interest? modeled
440
in the algorithms actually performs flat clustering-
based summarization which is proved to be less use-
ful (Wang and Li, 2010). GMDS utilizes sentence
linkage, and partly captures ?correlativeness?.
? ETTS under our proposed framework outper-
forms baselines, indicating that the properties we
use for timeline generation are beneficial. We also
add a direct comparison between ETTS and ETS
(Yan et al, 2011b). We notice that both balanced
algorithms achieve comparable performance (0.386
v.s. 0.412: a gap of 0.026 in terms of ROUGE-
1), but ETTS is much faster than ETS. It is under-
standable that ETS refines timelines based on neigh-
boring component summaries iteratively while for
ETTS neighboring information is incorporated in
temporal projection and hence there is no such pro-
cedure. Furthermore, ETS has 8 free parameters to
tune while ETTS has only 2 parameters. In other
words, ETTS is more simple to control.
? The performance on intensive focused news
within short time range (|last timestamp?first times-
tamp |<1 year) is better than on long lasting news.
Having proved the effectiveness of our proposed
methods, we carry the next move to identity how
global?local combination ratio ?/? and projection
kernels take effects to enhance the quality of a sum-
mary in parameter tuning.
4.6 Parameter Tuning
Each time we tune one parameter while others are
fixed. To identify how global and local biased sum-
marization combine, we provide experiments on the
performance of varying ?/? in Figure 5. Results in-
dicate that a balance between global and local biased
summarization is essential for timeline generation
because the performance is best when ?? ? [10, 100]and outperforms global and local summarization in
isolation, i.e., when ?=0 or ? = 0 in Figure 5. Inter-
estingly, we conclude an opposite observation com-
pared with ETS. Different approaches might lead to
different optimum of global/local combination.
Another key parameter ? measures the temporal
projection influence from global collection to local
collection and hence the size of neighboring sen-
tence set. 6 datasets are classified into two groups.
Subject 1, 2, 6 are grouped as long news with a time
span of more than one year and the others are short
news. The effect of ? varies on long news sets and
short news sets. In Figure 6 ? is best around 60 and
in Figure 7 it is best at about 20?40, indicating long
news has relatively wider semantic scope.
We then examine the effect of different projection
kernels. Generally, Gaussian kernel outperforms
others and window kernel is the worst, probably be-
cause Gaussian kernel provides the best smoothing
effect with no arbitrary cutoffs. Window kernel fails
to distinguish different weights of neighboring sets
by temporal proximity, so its performance is as ex-
pected. Other 3 kernels are comparable.
4.7 Sample Output and Case Study
Sample output is presented in Table 7 and it shares
major information similarity with the human time-
line in Table 1. Besides, we notice that a dynamic
?i is reasonable. Important burstiness is worthy of
more attention. Fewer sentences are selected on the
dates when nothing new occurs.
Interesting Findings. We notice that humans have
biases to generate timelines for they have (1) pref-
erence on local occurrences and (2) different writ-
ing styles. For instance, news outlets from United
States tend to summarize reactions by US govern-
ment while UK websites tend to summarize British
affairs. Some editors favor statistical reports while
others prefer narrative style, and some timelines
have detailed explanations while others are ex-
tremely concise with no more than two sentences for
each entry. Our system-generated timelines have a
large variance among all golden standards. Proba-
bly a new evaluation metric should be introduced to
measure the quality of human generated timelines
to mitigate the corresponding biases. A third in-
teresting observation is that subjects have different
volume patterns, e.g., H1N1 has a slow start and a
bursty evolution and BP Oil has a bursty start and a
quick decay. Obama is different in nature because
the report volume is temporally stable and scattered.
5 Conclusion
We present a novel solution for the important
web mining problem, Evolutionary Trans-Temporal
Summarization (ETTS), which generates trajectory
timelines for news subjects from massive data. We
formally formulate ETTS as a combination of global
and local summarization, incorporating affinity and
441
Table 7: Selected part of timeline generated by ETTS for BP Oil.
April 20, 2010
s1: An explosion on the Deepwater Horizon offshore oil drilling rig in
the Gulf of Mexico, around 40 miles south east of Louisiana, causing
several kills and injuries.
s2: The rig was drilling in about 5,000ft (1,525m) of water, pushing
the boundaries of deepwater drilling technology.
s3: The rig is owned and operated by Transocean, a company hired by
BP to carry out the drilling work.
s4: Deepwater Horizon oil rig fire leaves 11 missing.
April 22, 2010
s1: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s2: The Deepwater Horizon sinks to the bottom of the Gulf after burn-
ing for 36 hours, raising concerns of a catastrophic oil spill.
s3: Deepwater Horizon rig sinks in 5,000ft of water.
April 23, 2010
s1: The US coast guard suspends the search for missing workers, who
are all presumed dead.
s2: The Coast Guard says it had no indication that oil was leaking from
the well 5,000ft below the surface of the Gulf.
s3: Underwater robots try to shut valves on the blowout preventer to
stop the leak, but BP abandons that failed effort two weeks later.
s4: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s5: Deepwater Horizon clean-up workers fight to prevent disaster.
April 24, 2010
s1: Oil is found to be leaking from the well.
April 26, 2010
s1: BP?s shares fall 2% amid fears that the cost of cleanup and legal
claims will hit the London-based company hard.
s2: Roughly 15,000 gallons of dispersants and 21,000ft of containment
boom are placed at the spill site.
April 27, 2010
s1: BP reports a rise in profits, due in large part to oil price increases,
as shares rise again.
s2: The US departments of interior and homeland security announce
plans for a joint investigation of the explosion and fire.
s3: Minerals Management Service (MMS) approves a plan for two re-
lief wells.
s4: BP chairman Tony Hayward says the company will take full re-
sponsibility for the spill, paying for legitimate claims and cleanup cost.
April 28, 2010
s1: The coast guard says the flow of oil is 5,000bpd, five times greater
than first estimated, after a third leak is discovered.
s2: BP?s attempts to repair a hydraulic leak on the blowout preventer
valve are unsuccessful.
s3: BP reports that its first-quarter profits more than double to ?3.65
billion following a rise in oil prices.
s4: Controlled burns begin on the giant oil slick.
diversity into a unified ranking framework. We im-
plement a system under such framework for ex-
periments on real web datasets to compare all ap-
proaches. Through our experiment we notice that
the combination plays an important role in timeline
generation, and global optimization weights slightly
higher (?/? ? [10, 100]), but auxiliary local infor-
mation does help to enhance performance in ETTS.
Acknowledgments
This work was partially supported by NSFC with
Grant No.61073082, 60933004, 70903008 and
61073081, and Xiaojun Wan was supported by
NSFC with Grant No.60873155 and Beijing Nova
Program (2008B03).
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 121?128.
Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010.
Topic initiator detection on the world wide web. In
Proceedings of the 19th international conference on
WWW?10, pages 481?490.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL?03, pages 71?78.
442
Yuanhua Lv and ChengXiang Zhai. 2009. Positional lan-
guage models for information retrieval. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09, pages 299?306.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD?10, pages 1009?1018.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Russell Swan and James Allan. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR?00, pages 49?56.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Dingding Wang and Tao Li. 2010. Document update
summarization using incremental hierarchical cluster-
ing. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ?10, pages 279?288.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In Information Retrieval Tech-
nology - 6th Asia Information Retrieval Societies Con-
ference, AIRS 2010, pages 490?501.
Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming
Li. 2011a. A fine-grained digestion of news webpages
through event snippet extraction. In Proceedings of
the 20th international conference companion on world
wide web, WWW ?11, pages 157?158.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings of
the 34th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?11.
443
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1840?1850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Opinion Target Extraction in Chinese Microblogs 
 
 
 
Xinjie Zhou, Xiaojun Wan* and Jianguo Xiao 
Institute of Computer Science and Technology   
The MOE Key Laboratory of Computational Linguistics   
Peking University  
No. 5, Yiheyuan Road, Beijing, China 
{zhouxinjie,wanxiaojun,xiaojianguo}@pku.edu.cn 
 
 
Abstract 
Microblog messages pose severe challenges 
for current sentiment analysis techniques due 
to some inherent characteristics such as the 
length limit and informal writing style. In this 
paper, we study the problem of extracting 
opinion targets of Chinese microblog messag-
es. Such fine-grained word-level task has not 
been well investigated in microblogs yet. We 
propose an unsupervised label propagation al-
gorithm to address the problem. The opinion 
targets of all messages in a topic are collec-
tively extracted based on the assumption that 
similar messages may focus on similar opinion 
targets. Topics in microblogs are identified by 
hashtags or using clustering algorithms. Ex-
perimental results on Chinese microblogs 
show the effectiveness of our framework and 
algorithms. 
1 Introduction 
Microblogging services such as Twitter 1 , Sina 
Weibo2 and Tencent Weibo3 have swept across the 
globe in recent years. Users of microblogs range 
from celebrities to ordinary people, who usually 
express their emotions or attitudes towards a broad 
range of topics. It is reported that there are more 
than 340 million tweets per day on Twitter and 
more than 200 million on Sina Weibo. A tweet 
means a post on Twitter. Since we mainly focus 
on Chinese microblogs instead of Twitter in this 
paper, we will refer to a post as a message. Each 
message is limited to 140 Chinese characters and 
usually contains several sentences. 
                                                          
* Xiaojun Wan is the corresponding author. 
1 https://twitter.com 
2 http://weibo.com/ 
3 http://t.qq.com/ 
Currently, researches on microblog sentiment 
analysis have been conducted on polarity classifi-
cation (Barbosa and Feng, 2010; Jiang el al., 2011; 
Speriosu et al, 2011) and have been proved to be 
useful in many applications, such as opinion poll-
ing (Tang et al, 2012), election prediction 
(Tumasjan et al, 2010) and even stock market 
prediction (Bollen et al, 2011). However, classify-
ing microblog texts at the sentence level is often 
insufficient for applications because it does not 
identify the opinion targets. In this paper, we will 
study the task of opinion target extraction for Chi-
nese microblog messages.  
Opinion target extraction aims to find the object 
to which the opinion is expressed. For example, in 
the sentence ?The sound quality is good!?, ?sound 
quality? is the opinion target. This task is mostly 
studied in customer review texts in which opinion 
targets are often referred as features or aspects 
(Liu, 2012). Most of the opinion target extraction 
approaches rely on dependency parsing (Zhuang et 
al., 2006; Jakob and Gurevych, 2010; Qiu et al, 
2011) and are regarded as a domain-dependent 
task (Li et al, 2012a). However, such approaches 
are not suitable for microblogs because the natural 
language processing tools perform poorly on mi-
croblog texts due to their inherent characteristics. 
Studies show that one of the state-of-the-art part-
of-speech taggers - OpenNLP only achieves the 
accuracy of 74% on tweets (Liu et al 2011). The 
syntactic analysis tool that generates dependency 
relation may perform even worse. Besides, mi-
croblog messages may express opinion in different 
ways and do not always contain opinion words, 
which lowers the performance of methods utiliz-
ing opinion words to find opinion targets.  
In this study, we propose an unsupervised 
method to collectively extract the opinion targets 
from opinionated sentences in the same topic. 
1840
Topics are directly identified by hashtags. We first 
present a dynamic programming based segmenta-
tion algorithm for Chinese hashtag segmentation. 
By leveraging the contents in a topic, our segmen-
tation algorithm can successfully identify out-of-
vocabulary words and achieve promising results. 
Afterwards, all the noun phrases in each sentence 
and the hashtag segments are extracted as opinion 
target candidates. We propose an unsupervised 
label propagation algorithm to collectively rank 
the candidates of all sentences based on the as-
sumption that similar sentences in a topic may 
share the same opinion targets. Finally, for each 
sentence, the candidate which gets the highest 
score after unsupervised label propagation is se-
lected as the opinion target. 
Our contributions in this study are summarized 
as follows: 1) our method considers not only the 
explicit opinion targets within the sentence but 
also the implicit opinion targets in the hashtag or 
mentioned in the previous sentence. 2) We devel-
op an efficient algorithm to segment Chinese 
hashtags. It can successfully identify out-of-
vocabulary words by leveraging contextual infor-
mation and help to improve the segmentation per-
formance of the messages in the topic. 3) We 
develop an unsupervised label propagation algo-
rithm for collective opinion target extraction. La-
bel propagation (Zhu and Ghahramani, 2002) aims 
to spread label distributions from a small training 
set throughout the graph.   However, our unsuper-
vised algorithm leverages the connection between 
two adjacent unlabeled nodes to find the correct 
labels for both of them. The proposed unsuper-
vised method does not need any training corpus 
which will cost much human labor especially for 
fine-grained annotation. 4) To the best of our 
knowledge, the task of opinion target extraction in 
microblogs has not been well studied yet. It is 
more challenging than microblog sentiment classi-
fication and opinion target extraction in review 
texts.  
2 Characteristics of Chinese Microblogs 
Most of previous microblog sentiment analysis 
researches focus on Twitter and especially in Eng-
lish. However, the analysis of Chinese microblogs 
has some differences with that of Twitter: 1) Chi-
nese word segmentation is a necessary step for 
Chinese sentiment analysis, but the existing seg-
mentation tool performs poorly on microblogs 
because the microblog texts are much different 
from regular texts. 2) Wang et al (2011) find that 
hashtags in English tweets are used to highlight 
the sentiment information such as ? #love?, 
?#sucks? or serve as user-annotated coarse topics 
such as ?#news?, ?#sports?. But in Chinese mi-
croblogs, most of the hashtags are used to indicate 
fine-grained topics such as #NBA??????# 
(#NBAFinalG7#). Besides, hashtags in Twitter 
always appear within a sentence such as ?I love 
#BarackObama!? while hashtags in Chinese mi-
croblogs are always isolated and are surrounded 
by two # symbols such as ?#??????# ??
?!? (?#BarackObama# I love him??). 
It is noteworthy that topics aggregated by the 
same hashtag play an important role in Chinese 
microblog websites. These websites often provide 
an individual webpage4 to list hot topics and invite 
people to participate in the discussion, where each 
topic consists of tens of thousands of messages 
with the same hashtag. The hot topics have a wide 
coverage of timely events and entities. Analyzing 
the opinion targets of these topics can help to get a 
deeper overview of the public attitudes towards 
the entities involved in the hot topics. 
3 Motivation 
As described above, #hashtags# in Chinese mi-
croblogs often indicate fine-grained topics. In this 
study, we aim to collectively extract the opinion 
targets of messages with the same hashtag, i.e. in 
the same topic. Opinion target of a sentence can be 
divided into two types, one of which called explic-
it target appears in the sentence such as ?I love 
Obama?, and the other one called implicit target 
                                                          
4 http://huati.weibo.com/ 
Topic Sentence 
#??????# 
#Property publicity 
of government offic
-ials# 
1. ????? 
(Just for show?) 
2. ???????????? 
(Property publicity is just a show 
in China.) 
#???????# 
#Philippine navy 
vessel hits Chinese 
fishing boat# 
1. ????????? 
(The government is not tough 
enough.) 
2. ??????????? 
(Why cannot the government take 
a tougher line?) 
Table 1. Motivation Examples 
1841
may appear out of the sentence, for example, the 
sentence ?Just for show!?  in Table 1 directly 
comments on the target in the hashtag ?#Property 
publicity of government officials#? . Such implicit 
opinion targets are not considered in previous 
works and are more difficult to extract than explic-
it targets. However, we believe that the contextual 
information will help to locate both of the two 
kinds of opinion targets because similar sentences 
in a topic may share the same opinion target, 
which provides the possibility for collective ex-
traction. 
Table 1 shows the motivation examples of two 
topics and four sentences. The two sentences in 
each topic are considered to be similar because 
they share several Chinese words. In the topic #?
?????# (#Property publicity of government 
officials#), the first sentence omits the opinion 
target. However, the second one contains an ex-
plicit target ?????? (?property publicity?) in 
the sentence. If we find the correct opinion target 
for sentence 2, we can infer that sentence 1 may 
have an implicit opinion target similar to the opin-
ion target in sentence 2. In the second topic, both 
sentences contain a noun word ???? (?govern-
ment?). The similarity between these two sentenc-
es may indicate that both of the two sentences are 
expressing opinion on ????. 
Based on the above observation, we can assume 
that similar sentences in a topic may have the 
same opinion targets. Such assumption can help to 
locate both explicit and implicit opinion targets. 
Following this idea, we firstly extract all the noun 
phrases in each sentence as opinion target candi-
dates after applying Chinese word segmentation 
and part-of-speech tagging. Afterwards, an unsu-
pervised label propagation algorithm is proposed 
to rank these candidates for all sentences in the 
topic. 
In our methods, hashtags are used to find gold-
standard topics. For messages without hashtags, an 
alternative way is to generate pseudo topics by 
clustering microblogs messages and then apply the 
proposed algorithm to each pseudo topic. The de-
tailed discussion of such general circumstance is 
shown in Section 5.7. 
4 Methodology 
4.1 Context-Aware Hashtag Segmentation 
In our approach, the Chinese word segmentations 
of hashtags and topic contents are treated separate-
ly. Existing Chinese word segmentation tools 
work poorly on microblog texts. The segmentation 
errors especially on opinion target words will di-
rectly influence the results of part-of-speech tag-
ging and candidate extraction. However, some of 
the opinion target words in a topic are often in-
cluded in the hashtag. By finding the correct seg-
ments of a hashtag and adding them to the user 
dictionary of the Chinese word segmentation tool, 
we can remarkably improve the overall segmenta-
tion performance.  
The following example can help to understand 
the idea better. In the topic #90????# (means 
?A young man hits an old man?), ?90?? (literally 
?90 later? and means a young man born in the 90s) 
is an important word because it is the opinion tar-
get of many sentences. However, existing Chinese 
word segmentation tools will regard it as two sep-
arate words ?90? and ??? (?later?). Then in the 
part-of-speech tagging stage, ?90? will be tagged 
as number and ??? will be tagged as localizer. As 
we only extract noun phrases as opinion target 
candidates, the wrong segmentation on ?90 ?? 
makes it impossible to find the right opinion target. 
Such error may occur many times in sentences that 
mention the word ?90?? and express opinion on 
it. In our method, the message texts of the topic 
are utilized to identify such out-of-vocabulary 
words based on its frequency in the topic. For ex-
ample, the high frequency of ?90?? is a strong 
indication that it should be regard as a single word. 
After segmenting the hashtag correctly into ?90?
/?/???, we can add the hashtag segments to the 
user dictionary of the segmentation tool to further 
segment the message texts of the topic. 
The basic idea for our hashtag segmentation al-
gorithm is to regard strings that appear frequently 
in a topic as words. Formally, given a hashtag h 
that contains n Chinese characters c1c2...cn. We 
want to segment into several words w1w2...wm, 
where each word is formed by one of more charac-
ters. 
Firstly, we define the stickiness score for a Chi-
nese string c1c2...cn based on the Symmetrical 
Conditional Probability (SCP) (Silva and Lopes, 
1999): 
1842
2
1 2
1 2
1 1
1
Pr( ... )( ... ) 1 Pr( ... )Pr( ... )1
n
n n
i i n
i
c c cSCP c c c
c c c cn ??
?
? ?
 (1) 
and SCP(c1) = Pr(c1)
2 for string with only one 
character. Pr(c1c2...cn) is the occurrence frequency 
of the string in the topic.  
Following (Li et al, 2012b), we smooth the 
SCP value by taking logarithm calculation. Be-
sides, the length of the string is taken into consid-
eration, 
 
1 2 1 2( ... ) log ( ... )n nSCP c c c n SCP c c c? ? ? (2) 
where n is the number of characters in the string. 
Then the stickiness score is defined by the sig-
moid function as follows: 
 
1 21 2 ( ... )
2( ... ) 1 nn SCP c c cStickiness c c c e ??? ?
 (3) 
For the hashtag h = c1c2...cn, we want to seg-
ment it into m words w1w2...wm which maximize 
the following equation, 
 
1
max ( )m i
i
Stickness w
?
?
  (4) 
The optimization of Equation (4) can be solved 
efficiently by dynamic programming which itera-
tively segments a string into two substrings. Dif-
ferent from (Li et al, 2012b) which calculates the 
SCP value of each string based on Microsoft Web 
N-Gram, our hashtag segmentation algorithm only 
uses the topic content and do not need any addi-
tional corpus. 
4.2 Candidate Extraction 
After segmenting the hashtag, all the hashtag seg-
ments with length greater than one are added to 
the user dictionary of the Chinese word segmenta-
tion tool ICTCLAS5 to further segment the mes-
sage texts of the topic. It also assigns the part-of-
speech tag for each word after segmentation. The 
noun phrases in each sentence is extracted by the 
following regular expression:
( | )( ) .noun adj noun adj noun??That means a 
noun phrase can only include nouns, adjectives 
and the Chinese word ??? (?of?). It should begin 
with a noun or adjective and end with a noun. For 
                                                          
5 http://www.ictclas.org/ 
example, in the following sentence, ???/n ?/u 
??/n ??/n ?/v ??/n ?/w? (?Chinese edu-
cation system has problems.?), ????????? 
(?Chinese education system?) and ???? (?prob-
lem?) are extracted as noun phrases.  
The character number of a noun phrase is lim-
ited between two and seven Chinese characters. 
For each sentence, all phrases that match the regu-
lar expression and meet the length restriction are 
extracted as explicit opinion target candidates. The 
hashtag segments are regarded as implicit candi-
dates for all sentences. Besides, some opinionated 
sentences in microblogs do not contain any noun 
phase, such as ?????? ? (?So boring!?). 
These sentences may express opinion on object 
that has been mentioned before. Therefore, the 
explicit candidates of the previous sentence in the 
same message are also taken as the implicit candi-
dates for such sentences.  
We do not use any syntactic parsing tool to ex-
tract noun phrases because the parsing results on 
microblogs are not reliable. A performance com-
parison of our rule based method and the state-of-
the-art syntactic parser will be shown in Section 5. 
4.3 Unsupervised Label Propagation for 
Candidate Ranking 
We simply assume that each opinionated sentence 
has one opinion target, which is consistent with 
Algorithm 1 Unsupervised Label Propagation 
Input: 
Graph:                              , ,G V E W?? ?  
Candidate Similarity:     M MS R ???  
Prior labeling:                 1 MvY R ???  for v?V  
Filtering Matrix:             M MvF R ??? for v?V 
Probability:                       pinj and pcont 
Output: 
 Label vector:                   1? MvY R ???  
1: for all v?V do 
2:      
v? vY Y?  
3: end for 
4: repeat 
5:       for all v?V do 
6:         
? ?
,
?v uv u vu V u vD W Y S F? ?? ? ??
 
7:        ? inj contv v vY p Y p D? ?  
8:       end for 
9: until convergence 
 
1843
the statistical result of our dataset that over 93% 
sentences have only one opinion target and each 
sentence has an average of 1.09 targets. Therefore, 
the most confident candidate of each sentence will 
be selected as the opinion target. In this section, 
we introduce an unsupervised graph-based label 
propagation algorithm to collectively rank the 
candidates of all sentences in a topic.  
Label propagation (Zhu and Ghahramani, 2002; 
Talukdar and Crammer, 2009) is a semi-
supervised algorithm which spreads label distribu-
tions from a small set of nodes seeded with some 
initial label information throughout the graph. The 
basic idea is to use information from the labeled 
nodes to label the adjacent nodes in the graph. 
However, our idea is to use the connection be-
tween different nodes to find the correct labels for 
all of them. Our unsupervised label propagation 
algorithm is summarized in Algorithm 1. Sentenc-
es are regarded as nodes and candidates of each 
sentence are regarded as labels. The label vector 
for each node is initialized based on the results of 
the candidate extraction step, which means no 
manually-labeled instances are needed in our 
model. In each iteration, the label vector of one 
node is propagated to the adjacent nodes. Both the 
sentence (node) similarity and the candidate (label) 
similarity are considered during propagation. Fi-
nally, we select the candidate with the highest 
score in the label vector as the opinion target for 
each sentence. The details of Algorithm 1 are pre-
sented as follows. 
Formally, an undirected graph , ,G V E W?? ?  
is built for each topic. A node v?V represents a 
sentence in the topic and an edge e = (a, b) ?E 
indicates that the labels of the two vertices should 
be similar.  W  is the normalized weight matrix to 
reflect the strength of this similarity. The similari-
ty between two nodes Wab is simply calculated by 
using the cosine measure (Salton et al, 1975) of 
the two sentences. 
 
( , ) a bab a b
a b
T TW cos T T T T
?? ? ?
 (5) 
where Ta and Tb are the term vectors of sentences a 
and b represented by the standard vector space 
model and weighted by term frequency. After cal-
culating the similarity matrix W, we get the weight 
matrix W  by normalizing each row of W such that 
1abb W ??
. 
For each sentence (node) v, a candidate set Cv is 
extracted in the previous step. The candidate set 
CT for the whole topic is the union of all Cv, 
 
vCT C?
 (6) 
The total number of candidates in the topic is 
denoted by M = |CT|. We calculate the candidate 
similarity matrix M MS R ???  based on Jaccard In-
dex: 
 ( ) ( ) 1( ) ( )
i j
ij
i j
A CT A CTS i j MA CT A CT? ? ? ?
 (7) 
where A(CTi) and A(CTj) are the Chinese character 
sets of the i-th and j-th candidates in CT respec-
tively. 
Candidates are regarded as labels in our model 
and without loss of generality we assume that the 
possible labels for the whole topic are L = {1?M} 
and each label in L corresponds to a unique can-
didate in CT. For each node v?V, a label vector 
1 MvY R ???  is initialized as 
  
? ? 10 k vv k k v
w L CY k ML C
??? ? ?? ??
 (8) 
where w is the initial weight of the candidate. We 
set w = we if Lk is an explicit candidate (extracted 
noun phrase) of v and w = wi if Lk is an implicit 
candidate (hashtag segment or inherited from pre-
vious sentence) of v. If Lk is not a candidate of the 
current sentence, then the corresponding value in 
the label vector is 0. These values which are ini-
tialized as zero should always remain zero during 
the propagation algorithm because the correspond-
ing label does not belong to the candidate set Cv of 
node v. To reset the values on these positions, a 
diagonal matrix M MvF R ???  is created for all nodes 
v, 
 
? ? ? ?? ?
1 0 10 0
v k
v kk
v k
YF k MY
? ??? ? ?? ???
 (9) 
where the subscript kk denotes the k-th position in 
the diagonal of matrix Fv. We can right-multiply 
Yv by Fv to clear the values of the invalid candi-
1844
dates. Figure 1 shows an example of creating the 
filtering matrix for a label vector. 
The propagation process is formalized via two 
possible actions: inject and continue, with pre-
defined probabilities pinj and pcont. Their sum is 
unit: pinj + pcont = 1. In each iteration, every node is 
influenced by its adjacent nodes. The propagation 
influence for each node v is 
 ? ?
,
?v uv u vu V u vD W Y S F? ?? ? ??
 (10) 
where 
u?Y  is the label vector of node u at the previ-
ous iteration. By multiplying the candidate simi-
larity matrix S, we aim to propagate the score of 
the i-th candidate of node u not only to the i-th 
candidate of node v, but also to all the other can-
didates. Wuv measures the strength of such propa-
gation. The filtering matrix Fv is used to clear the 
values of the invalid candidates as described 
above. 
Then the label vector of node v is updated as 
follow, 
  ? inj contv v vY p Y p D? ?  (11) 
When the positions of the largest values in all 
label vectors keep unchanged in ten iterations, it is 
regarded that the algorithm has already converged.  
5 Experiments 
5.1 Dataset 
We use the dataset from the 2012 Chinese Mi-
croblog Sentiment Analysis Evaluation (CMSAE)6 
held by China Computer Federation (CCF). There 
are three tasks in the evaluation: subjectivity clas-
sification, polarity classification and opinion target 
extraction. The dataset contains 20 topics collect-
ed from Tencent Weibo, a popular Chinese mi-
croblogging website. All the messages in a topic 
contain the same hashtag. The dataset has a total 
                                                          
6 http://tcci.ccf.org.cn/conference/2012/pages/page04_eva.
html. The dataset can also be publicly accessed on the website. 
of 17518 messages and 31675 sentences. In each 
topic, 100 messages are manually annotated with 
subjectivity, polarity and opinion targets. A total 
of 2361opinion targets are annotated for 2152 
opinionated sentences.  
5.2 Evaluation Metric  
Precision, recall and F-measure are used in the 
evaluation. Since expression boundaries are hard 
to define exactly in annotation guidelines (Wiebe 
et al, 2005), both the strict evaluation metric and 
the soft evaluation metric are used in CMSAE. 
Strict Evaluation: For a proposed opinion tar-
get, it is regarded as correct only if it covers the 
same span with the annotation result. Note that, in 
CMSAE, an opinion target should be proposed 
along with its polarity. The correctness of the po-
larity is also necessary. 
Soft Evaluation: The soft evaluation metric 
presented in (Johansson and Moschitti, 2010) is 
adopted by CMSAE. The span coverage c be-
tween each pair of the proposed target span s and 
the gold standard span s? is calculated as follows, 
 
? ?, s sc s s s
??? ? ?
 (12) 
In Equation 12, the operator |?| counts Chinese 
characters, and the intersection ? gives the set of 
characters that two spans have in common. 
Using the span coverage, the span set coverage 
C of a set of spans S with respect to another set S? 
is 
? ?, ( , )
s S s S
C S S c s s
? ?? ?
?? ???
 (13) 
The soft precision P and recall R of a proposed 
set of spans S?  with respect to a gold standard set 
S is defined as follows: 
 ? ?( , ) ( , )Precision Recall? | || |
C S S C S S
SS? ?
 (14) 
Note that the operator |?| counts spans in Equation 
14. The soft F-measure is the harmonic mean of 
soft precision and recall. 
5.3 Comparison Methods 
Our proposed approach is first compared with the 
CMSAE teams. 
CMSAE Teams: Sixteen teams participated in 
the opinion target extraction task of CMSAE. The 
methods of the top 3 teams are used as baselines 
? ?
1 0 0 0
0 1 0 0
1 1 0.5 0
0 0 1 0
0 0 0 0
v vY F
? ?
? ?
? ?? ? ?
? ?
? ?
? ?
 
Figure 1. Example of filtering matrix 
1845
here. They are denoted as Team-1, Team-2 and 
Team-3 respectively. The average result of all the 
sixteen teams is also included and is denoted as 
Team-Avg. We will briefly introduce the best 
team?s method. The most important component of 
their model is a topic-dependent opinion target 
lexicon which is called object sheet. If a word or 
phrase in the object sheet appears in a sentence or 
a hashtag, it is extracted as opinion target. The 
object sheet is manually built for each topic, 
which means their method cannot be applied to 
new topics. 
The following models are also used for compar-
ison. 
AssocMi: We implement the unsupervised 
method for opinion target extraction based on (Hu 
and Liu, 2004), which relies on association mining 
and a sentiment lexicon to extract frequent and 
infrequent product features. 
CRF: The CRF-based method used in (Jakob 
and Gurevych, 2010) is also used for comparison. 
We implement both the single-domain and cross-
domain models. Both models are evaluated using 
5-fold cross-validation. More specifically, the sin-
gle-domain model, denoted as CRF-S, trains dif-
ferent models for different topics. In each cross-
validation round, 80 percent of each topic is used 
for training and the other 20 percent is used for 
test. The cross-domain model, denoted as CRF-C, 
uses 16 topics for training and the rest 4 topics for 
test in each round.  
5.4 Comparison Results  
CMSAE requires all the teams to perform the sub-
jectivity and polarity classification task in advance. 
The opinion targets are extracted only for opinion-
ated sentences and should be proposed along with 
their polarity. To make a fair comparison, we di-
rectly use the subjectivity and polarity classifica-
tion results of Team-1. Then our unsupervised 
label propagation (ULP) method is used to extract 
the opinion targets for the proposed opinionated 
sentences. The parameters of our method are 
simply set as pinj = pcont = 0.5, we = 1 and wi = 0.5. 
Table 2 lists the comparison results with 
CMSAE teams. The average F-measure of all 
teams is 0.12 and 0.20 in strict and soft evaluation, 
respectively. It shows that opinion target extrac-
tion is a quite hard problem in Chinese microblogs. 
Our method performs better than all the teams. It 
increases by 10% and 13% in the two kinds of F-
measure compared to the best team. Besides, we 
do not need any prior information of the topics 
while Team-1 has to manually build an opinion 
target lexicon for each topic. 
To compare with the other opinion target ex-
traction methods, we only use gold-standard opin-
ionated sentences for evaluation and do not 
classify the polarity of the opinion targets. Table 3 
shows the experimental results of the four models. 
Our approach achieves the best result among them. 
AssocMi performs worst in strict evaluation but 
gets better results than the two CRF-based models 
in soft evaluation. The two CRF-based models 
achieve high precision but low recall. We can also 
observe that CRF-S is much more effective than 
CRF-C. It achieves high results because it has al-
ready seen the opinion targets in the training set. 
However, it is impossible to build such single-
domain model in practical applications because 
Method 
Strict Soft 
Precision Recall F-Measure Precision Recall F-Measure 
AssocMi 0.22 0.20 0.21 0.47 0.43 0.45 
CRF-C 0.59 0.15 0.24 0.70 0.18 0.28 
CRF-S 0.61 0.27 0.35 0.73 0.31 0.41 
ULP 0.43 0.39 0.41 0.61 0.55 0.58 
Table 3. Comparison results with baseline methods (only gold-standard opinionated sentences are used) 
Method. 
 
Strict Soft 
Precision Recall F-measure Precision Recall F-measure 
Team-Avg 0.17 0.09 0.12 0.29 0.15 0.20 
Team-3 0.26 0.16 0.20 0.40 0.25 0.31 
Team-2 0.31 0.18 0.23 0.40 0.22 0.29 
Team-1 0.30 0.27 0.29 0.39  0.36 0.37 
ULP 0.37 0.27 0.32 0.48 0.37 0.42 
Table 2. Comparison results with CMSAE teams (with subjectivity and polarity classification in advance) 
1846
labeled instances are not available for new topics. 
Our proposed method does not require any train-
ing data and gets an increase of 17% over CRF-S 
and 70% over CRF-C in strict evaluation. In terms 
of soft evaluation, we achieve an increase of 41% 
and 107% over the two CRF models.  
5.5 Parameter Sensitivity Study 
In this section, we study the parameter sensitivity. 
There are two major parameters in our algorithm: 
the initial weight w for both explicit and implicit 
candidates in Equation 8 and the injection proba-
bility pinj in Equation 11.  
The initial weights of explicit and implicit can-
didates are set differently because the explicit can-
didates are more likely to be the opinion targets. 
These two kinds of initial weights are denoted as 
we and wi for explicit and implicit candidate, re-
spectively. To study the impact of the initial 
weights, we fix we at 1 and tune wi because we 
only care about the relative contribution of them. 
The injection probability is fixed at 0.5. Figure 2(a) 
displays the opinion target extraction performance 
when wi varies from 0 to 1.5. Due to limited space, 
we only list the strict F-measure of opinion target 
extraction evaluated on opinioned sentences (same 
experimental setup as Table 3).  
In particular, when wi is equal to 0, only explicit 
candidates are considered. When wi becomes larg-
er than 1, the implicit candidates become more 
important than explicit candidates. From the curve 
in Figure 2(a), we can observe that the implicit 
candidates help to improve the performance sig-
nificantly when wi varies from 0 to 0.1. The per-
formance reaches the peak when wi = 0.7 and 
declines rapidly when wi gets larger than 1.  
To study the impact of injection probability pinj, 
we fix the initial weights for explicit and implicit 
candidates as 1 and 0.5, respectively. Figure 2(b) 
shows the results of opinion target extraction with 
respect to different values of the injection proba-
bility. We can observe that the performance keeps 
steady except for the two extreme values 0 and 1. 
From the above two figures, we can conclude that 
our proposed method performs well and robustly 
with a wide range of parameter values. 
5.6 Analysis of Candidate Extraction  
Candidate extraction is an important step in our 
proposed method. If the correct opinion target is 
not extracted as a candidate, the ranking step will 
be in vain. As described in Section 3, we develop 
a hashtag segmentation algorithm and use a rule 
based method to extract noun phrases from each 
sentence. We do not use any parsing tool because 
we believe the performance of these tools is not 
good enough when applied on microblogs. A 
quantitative comparison is shown in this section.  
We use one of the state-of-the-art syntactic 
analysis tools - Berkeley Parser (Petrov et al, 
2006) for comparison here. Noun phrases are di-
rectly extracted from the parsing results. Our 
method HS+Rule leverages the hashtag segments 
to enhance the segmentation result and extracts 
explicit candidate using a regular expression. To 
demonstrate the effectiveness of our hashtag seg-
mentation algorithm, the second comparison base-
line Rule directly uses ICTCLAS to segment the 
whole topic content and labels each word with its 
part-of-speech tag. The explicit candidates are ex-
tracted by using the same regular expression. 
The performance on candidate extraction is 
compared in Table 4. The second column shows 
the number of all extracted candidates for all the 
opinionated sentences by different methods. The 
third column shows the number of correct opinion 
targets among them. We can find that the two rule-
based models both outperform Berkeley Parser 
and our HS+Rule method finds 14% more correct 
opinion targets than Rule. It proves the effective-
ness of our hashtag segmentation algorithm. The 
Method Total Correct 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
Berkley 
Parser 
4554 877 0.36 0.56 
Rule 4105 918 0.37 0.56 
HS + Rule 4094 1042 0.41 0.58 
Table 4. Performance of candidate extraction and 
opinion target extraction 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1
Precision
Recall
F-Measure
pinj 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1 1.2 1.4
Precision
Recall
F-Measure
wi 
(a) Initial Candidate Weight        (b) Injection Probability 
Figure 2. Influence of the parameters 
 
1847
total number of candidates extracted by HS+Rule 
is also less than the other two methods. Therefore, 
the performance of label propagation will be im-
proved when there are fewer candidates to rank. It 
can be demonstrated by the F-measure of opinion 
target extraction in the fourth and fifth columns. 
The experiments are conducted on opinionated 
sentence only as above. By using HS+Rule to ex-
tract candidates, our label propagation algorithm 
gets the highest F-measure in both evaluation met-
rics.  
5.7 Performance on Pseudo Topics by Mes-
sage Clustering 
In our collective extraction algorithm, topics are 
directly identified by hashtags. For messages 
without hashtags, we can first employ clustering 
algorithms to obtain pseudo topics (clusters) and 
then exploiting the topic-oriented algorithm for 
collective opinion target extraction. To test the 
performance of the proposed method in such cir-
cumstance, we use the popular clustering algo-
rithm - Affinity Propagation (Frey and Dueck, 
2007) to generate topics. The experimental results 
are shown in Table 5. APCluster means that the 
messages are clustered after removing all the 
hashtags. APCluster+HS means that all the 
hashtags are retained as normal texts for calculat-
ing message similarity. Therefore, the clustering 
performance can be largely improved. The stand-
ard cosine similarity is used to measure the dis-
tance between microblog messages for Affinity 
Propagation in the above two methods. The last 
method denoted as GoldCluster directly uses 
hashtags to identify the gold-standard topics which 
shows the upper bound of the performance. After 
clustering microblogs, the opinion targets of mes-
sages in each cluster are collectively extracted by 
the proposed unsupervised label propagation algo-
rithm. The experiments are conducted on opinion-
ated sentences only. 
From the results, we can see that clustering mi-
croblogs without hashtags is a quite difficult job 
which only gets an F-Measure of 0.27. However, 
the corresponding opinion target extraction per-
formance is still promising, which outperforms the 
AssocMi and CRF-C methods in Table 3. With the 
help of hashtags, the clustering performance of 
APCluster+HS is largely improved and the opin-
ion target extraction performance is also increased. 
It outperforms all the baseline methods in Table 3. 
The above results reveal that our proposed unsu-
pervised label propagation algorithm works well 
in pseudo topics and the performance can be in-
creased with better clustering results. Therefore, 
we can try to incorporate other social network in-
formation to improve the message clustering per-
formance, which will be studied in our future 
work. 
6 Related Work 
Sentiment analysis, a.k.a. opinion mining, is the 
field of studying and analyzing people?s opinions, 
sentiments, evaluations, appraisals, attitudes, and 
emotions (Liu, 2012). Most of the previous senti-
ment analysis researches focus on customer re-
views (Pang et al, 2002; Hu and Liu, 2004) and 
some of them focus on news (Kim and Hovy, 
2006) and blogs (Draya et al, 2009). However, 
sentiment analysis on microblogs has recently at-
tracted much attention and has been proved to be 
very useful in many applications. 
Classification of opinion polarity is the most 
common task studied in microblogs. Go et.al 
(2009) follow the supervised machine learning 
approach of Pang et al (2002) to classify the po-
larity of each tweet by distant supervision. The 
training dataset of their method is not manually 
labeled but automatically collected using the 
emoticons. Barbosa and Feng (2010) use the simi-
lar pseudo training data collected from three 
online websites which provide Twitter sentiment 
analysis services. Speriosu et al (2009) explore 
the possibility of exploiting the Twitter follower 
graph to improve polarity classification.  
Opinion target extraction is a fine-grained 
word-level task of sentiment analysis. Currently, 
this task has not been well studied in microblogs 
yet. It is mostly performed on product reviews 
where opinion targets are always described as 
product features or aspects. The pioneering re-
search on this task is conducted by Hu and Liu 
Clustering Method 
F-Measure 
of Clustering 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
APCluster 0.27 0.35 0.50 
APCluster+HS 0.71 0.37 0.55 
GoldCluster 1.00 0.41 0.58 
Table 5.  Performance of clustering and opinion 
target extraction 
1848
(2004) who propose a method which extracts fre-
quent nouns and noun phrases as the opinion tar-
gets. Jakob and Gurevych (2010) model the 
problem as a sequence labeling task based on 
Conditional Random Fields (CRF). Qiu et al 
(2011) propose a double propagation method to 
extract opinion word and opinion target simulta-
neously. Liu et al (2012) use the word translation 
model in a monolingual scenario to mine the asso-
ciations between opinion targets and opinion 
words.  
7 Conclusion and Future Work  
In this paper, we study the problem of opinion 
target extraction in Chinese microblogs which has 
not been well investigated yet. We propose an un-
supervised label propagation algorithm to collec-
tively rank the opinion target candidates of all 
sentences in a topic. We also propose a dynamic 
programming based algorithm for segmenting 
Chinese hashtags. Experimental results show the 
effectiveness of our method. 
In future work, we will try to collect and anno-
tate data for microblogs in other languages to test 
the robustness of our method. The repost and reply 
messages can also be integrated into our graph 
model to help improve the results.  
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101).  
References  
Barbosa Luciano and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data. 
In Proceedings of the 23rd International Conference 
on Computational Linguistics: Posters. Association 
for Computational Linguistics, 2010. 
Johan Bollen, Huina Mao and Xiaojun Zeng. 2011. 
Twitter mood predicts the stock market. Journal of 
Computational Science 2.1 (2011): 1-8. 
G?rard Dray, Michel Planti?, Ali Harb, Pascal Poncelet, 
Mathieu Roche and Fran?ois Trousset. 2009. Opin-
ion Mining from Blogs. In International Journal of 
Computer Informa-tion Systems and Industrial Man-
agement Applications. 
Brendan J. Frey and Delbert Dueck. 2007. "Clustering 
by passing messages between data points." Science 
315.5814 (2007): 972-976. 
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter 
sentiment classification using distant supervision. 
CS224N Project Report, Stanford (2009): 1-12. 
Minqing Hu and Bing Liu. Mining and summarizing 
customer reviews. 2004. In Proceedings of the tenth 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 168-177. 
ACM. 
Long Jiang , Mo Yu, Ming Zhou, Xiaohua Liu and 
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational 
Linguistics: Human Language Technologies, vol. 1, 
pp. 151-160. 
Niklas Jakob and Iryna Gurevych. Extracting opinion 
targets in a single-and cross-domain setting with 
conditional random fields. 2010. In Proceedings of 
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics. 
Richard Johansson and Alessandro Moschitti. 2010. 
Syntactic and semantic structure for opinion expres-
sion detection. Proceedings of the Fourteenth Con-
ference on Computational Natural Language 
Learning. Association for Computational Linguistics. 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders and Topics Expressed in 
Online News Media Text. In Proceedings of the 
ACL Workshop on Sentiment and Subjectivity in 
Text, 2006, pp. 1?8.  
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics, pages 410?419, Jeju, 
Republic of Korea, 8-14 July 2012. 
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, 
Anwitaman Datta, Aixin Sun and Bu-Sung Lee. 
2012b. Twiner: Named entity recognition in targeted 
twitter stream. In Proceedings of the 35th interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp. 721-730. 
ACM. 
Xiaohua Liu, Kuan Li, Ming Zhou and Zhongyang 
Xiong. 2011. Collective semantic role labeling for 
tweets with clustering. In Proceedings of the Twen-
ty-Second international joint conference on Artificial 
1849
Intelligence-Volume Volume Three, pp. 1832-1837. 
AAAI Press. 
Bing Liu. 2012. Sentiment analysis and opinion mining. 
Synthesis Lectures on Human Language Technolo-
gies 5.1 (2012): 1-167. 
Kang Liu, Liheng Xu and Jun Zhao. 2012. Opinion 
Target Extraction Using Word-Based Translation 
Model. In Proceedings of the 2012 Joint Conference 
on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings of the 
ACL-02 conference on Empirical methods in natural 
language processing-Volume 10, pp. 79-86. Associa-
tion for Computational Linguistics. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. Learning accurate, compact, and interpretable 
tree annotation. 2006. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics, pp. 433-440. 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion word expansion and target extraction 
through double propagation. Computational linguis-
tics 37, no. 1 (2011): 9-27. 
G. Salton, A. Wong and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing, Communica-
tions of the ACM, vol. 18, nr. 11, pages 613?620. 
J. F. da Silva and G. P. Lopes. 1999. A local maxima 
method and a fair dispersion normalization for ex-
tracting multi-word units from corpora. In Proc. of 
the 6th Meeting on Mathematics of Language . 
Michael Speriosu, Nikita Sudan, Sid Upadhyay and 
Jason Baldridge. 2011. Twitter polarity classification 
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First Workshop 
on Unsupervised Learning in NLP, pp. 53-63. Asso-
ciation for Computational Linguistics, 2011. 
Partha Talukdar and Koby Crammer. New regularized 
algorithms for transductive learning. 2009. Machine 
Learning and Knowledge Discovery in Databases 
(2009): 442-457. 
Jie Tang, Yuan Zhang, Jimeng Sun, Jinhai Rao, Wen-
jing Yu, Yiran Chen and A. C. M. Fong. 2012. 
Quantitative study of individual emotional states in 
social networks. Affective Computing, IEEE Trans-
actions on 3, no. 2 (2012): 132-144. 
Andranik Tumasjan, Timm O. Sprenger, Philipp G. 
Sandner and Isabell M. Welpe. 2010. Predicting 
elections with twitter: What 140 characters reveal 
about political sentiment. In Proceedings of the 
fourth international aaai conference on weblogs and 
social media, pp. 178-185. 
X. Zhu and Z. Ghahramani. 2002. Learning from la-
beled and unlabeled data with label propagation. 
Technical report, CMU CALD tech report. 
Li Zhuang, Feng Jing and Xiaoyan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the ACM 15th Conference on Information and 
Knowledge Management, pages 43?50, Arlington, 
Virginia, USA, November. 
 
1850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624?1633,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Automatic Generation of Related Work Sections in Scientific Papers: 
An Optimization Approach 
 
 
 Yue Hu and Xiaojun Wan 
Institute of Computer Science and Technology 
The MOE Key Laboratory of Computational Linguistics 
Peking University, Beijing, China 
{ayue.hu,wanxiaojun}@pku.edu.cn 
 
 
  
 
Abstract 
In this paper, we investigate a challeng-
ing task of automatic related work gener-
ation. Given multiple reference papers as 
input, the task aims to generate a related 
work section for a target paper. The gen-
erated related work section can be used 
as a draft for the author to complete his 
or her final related work section. We 
propose our Automatic Related Work 
Generation system called ARWG to ad-
dress this task. It first exploits a PLSA 
model to split the sentence set of the giv-
en papers into different topic-biased parts, 
and then applies regression models to 
learn the importance of the sentences. At 
last it employs an optimization frame-
work to generate the related work section. 
Our evaluation results on a test set of 150 
target papers along with their reference 
papers show that our proposed ARWG 
system can generate related work sec-
tions with better quality. A user study is 
also performed to show ARWG can 
achieve an improvement over generic 
multi-document summarization baselines. 
1 Introduction 
The related work section is an important part of a 
paper. An author often needs to help readers to 
understand the context of his or her research 
problem and compare his or her current work 
with previous works. A related work section is 
often used for this purpose to show the differ-
ences and advantages of his or her work, com-
pared with related research works. In this study, 
we attempt to automatically generate a related 
work section for a target academic paper with its 
reference papers. This kind of related work sec-
tions can be used as a basis to reduce the author?s 
time and effort when he or she wants to complete 
his or her final related work section. 
Automatic related work section generation is a 
very challenging task. It can be considered a top-
ic-biased, multiple-document summarization 
problem. The input is a target academic paper, 
which has no related work section, along with its 
reference papers. The goal is to create a related 
work section that describes the related works and 
addresses the relationship between the target pa-
per and the reference papers. Here we assume 
that the set of reference papers has been given as 
part of the input. Existing works in the NLP and 
recommendation systems communities have al-
ready focused on the task of finding reference 
papers. For example, citation prediction (Nal-
lapati et al., 2008) aims at finding individual pa-
per citation patterns. 
Generally speaking, automatic related work 
section generation is a strikingly different prob-
lem and it is much more difficult in comparison 
with general multi-document summarization 
tasks. For example, multi-document summariza-
tion of news articles aims at synthesizing con-
tents of similar news and removing the redundant 
information contained by the different news arti-
cles. However, each scientific paper has much 
specific content to state its own work and contri-
bution. Even for the papers that investigate the 
same research topic, their contributions and con-
tents can be totally different. The related work 
section generation task needs to find the specific 
contributions of individual papers and arrange 
them into one or several paragraphs. 
In this study, we focus on the problem of au-
tomatic related work section generation and pro-
pose a novel system called ARWG to address the 
1624
problem. For the target paper, we assume that the 
abstract and introduction sections have already 
been written by the author and they can be used 
to help generate the related work section. For the 
reference papers, we only consider and extract 
the abstract, introduction, related work and con-
clusion sections, because other sections like the 
method and evaluation sections always describe 
the extreme details of the specific work and they 
are not suitable for this task. Then we generate 
the related work section using both sentence sets 
which are extracted from the target paper and 
reference papers, respectively. 
Firstly, we use a PLSA model to group both 
sentence sets of the target paper and its reference 
papers into different topic-biased clusters. Sec-
ondly, the importance of each sentence in the 
target paper and the reference papers is learned 
by using two different Support Vector Regres-
sion (SVR) models. At last, a global optimization 
framework is proposed to generate the related 
work section by selecting sentences from both 
the target paper and the reference papers. Mean-
while, the framework selects sentences from dif-
ferent topic-biased clusters globally. 
Experimental results on a test set of 150 target 
papers show our method can generate related 
work sections with better quality than those of 
several baseline methods. With the ROUGE 
toolkit, the results indicate the related work sec-
tions generated by our system can get higher 
ROUGE scores. Moreover, our related work sec-
tions can get higher rating scores based on a user 
study. Therefore, our related work sections can 
be much more suitable for the authors to prepare 
their final related work sections. 
2 Related Work 
There are few studies to directly address auto-
matic related work generation. Hoang and Kan 
(2010) proposed a related work summarization 
system given the set of keywords arranged in a 
hierarchical fashion that describes the paper?s 
topic. They used two different rule-based strate-
gies to extract sentences for general topics as 
well as detailed ones. 
A few studies focus on multi-document scien-
tific article summarization. Agarwal et al., (2011) 
introduced an unsupervised approach to the prob-
lem of multi-document summarization. The input 
is a list of papers cited together within the same 
source article. The key point of this approach is a 
topic based clustering of fragments extracted 
from each co-cited article. They rank all the clus-
ters using a query generated from the context 
surrounding the co-cited list of papers. Yeloglu 
et al., (2011) compared four different approaches 
for multi-document scientific articles summariza-
tion: MEAD, MEAD with corpus specific vo-
cabulary, LexRank and W3SS. 
Other studies investigate mainly on the single-
document scientific article summarization. Early 
works including (Luhn 1958; Baxendale 1958; 
Edumundson 1969) tried to use various features 
specific to scientific text (e.g., sentence position, 
or rhetorical clues features). They have proved 
that these features are effective for the scientific 
article summarization. Citation information has 
been already shown effective in summarize the 
scientific articles. Works including (Mei and 
Zhai 2008; Qazvinian and Radev 2008; Schwartz 
and Hearst 2006; Mohammad et al., 2009) em-
ployed citation information for the single scien-
tific article summarization. Earlier work (Nakov 
et al., 2004) indicated that citation sentences may 
contain important concepts that can give useful 
descriptions of a paper. 
Various methods have been proposed for news 
document summarization, including rule-based 
methods (Barzilay and Elhadad 1997; Marcu and 
Daniel 1997), graph-based methods (Mani and 
Bloedorn 2000; Erkan and Radev 2004; Michal-
cea and Tarau 2005), learning-based methods 
(Conroy et al., 2001; Shen et al., 2007; Ouyang 
et al., 2007; Galanis et al., 2008), optimization-
based methods (McDonald 2007; Gillick et al., 
2009; Xie et al., 2009; Berg-Kirkpatrick et al., 
2011; Lei Huang et al., 2011; Woodsend et al., 
2012; Galanis 2012), etc. 
The most relevant work is (Hoang and Kan, 
2010) as mentioned above. They also assumed 
the set of reference papers was given as part of 
the input. They also adopt the hierarchical topic 
tree that describes the topic structure in the target 
paper as an essential input for their system. 
However, it is non-trivial to build the hierar-
chical topic tree. Moreover, they do not consider 
the content of the target paper to construct the 
related work section, which is actually crucial in   
the related work section. To the best of our 
knowledge, no previous works have used super-
vised learning and optimization framework to 
deal with the multiple scientific article summari-
zation tasks. 
3 Problem Analysis and Corpus 
3.1 Problem Analysis 
1625
We firstly analyze the structure of related work 
sections briefly. By using examples for illustra-
tion, we can gain insight on how to generate re-
lated work sections. A specific related work ex-
ample is shown in Figure 1. 
This related work section introduces previous 
related works for a paper on Automatic Taxono-
my Induction. From Figure 1, we can have a 
glance at the structure of related work sections. 
Related work sections usually discuss several 
different topics, such as ?pattern-based? and 
?cluster-based? approaches shown in the Figure 
1. Besides the knowledge of previous works, the 
author often compares his own work with the 
previous works. The differences and advantages 
are generally mentioned. The example in Figure 
1 also indicates this phenomenon. 
Therefore, we design our system to generate 
related work sections according to the related 
work section structure mentioned above. Our 
system takes the target paper for which a related 
work section needs to be drafted besides its ref-
erence papers as input. The goal of our system is 
to generate a related work section with the above 
structure. The generated related work section 
should have several topic-biased parts. The au-
thor's own work is also needed to be described 
and its difference with other works is needed to 
be emphasized on. 
3.2 Corpus and Preprocessing 
We build a corpus that contains academic papers 
and their corresponding reference papers. The 
academic papers are selected from the ACL An-
thology 1 . The ACL Anthology currently hosts 
                                                 
1 http://aclweb.org/anthology/ 
over 24,500 papers from major conferences such 
as ACL, EMNLP, COLING in the fields of com-
putational linguistics and natural language pro-
cessing. We remove the papers that contain relat-
ed work sections with very short length, and ran-
domly select 1050 target papers to construct our 
whole corpus. 
The papers are all in PDF format. We extract 
their texts by using PDFlib 2  and detect their 
physical structures of paragraphs, subsections 
and sections by using ParsCit3 . For the target 
papers, the related work sections are directly ex-
tracted as the gold summaries. The references are 
also extracted. For the references that can be 
found in the ACL Anthology, we download them 
from the ACL Anthology. The other reference 
papers are searched and downloaded by using 
Google Scholar. References to books and PhD 
theses are discarded, for their verbosity may 
change the problem drastically (Mihalcea and 
Ceylan, 2007). 
The input of our system includes the abstract 
and introduction sections of the target paper, and 
the abstract, introduction, related work and con-
clusion sections of the reference papers. As men-
tioned above, the method and evaluation sections 
in the reference papers are not used as input be-
cause these sections usually describe extreme 
details of the methods and evaluation results and 
they are not suitable for related work generation. 
Note that it is reasonable to make use of the ab-
stract and introduction sections of the target pa-
per to help generate the related work section, 
because an author usually has already written the 
abstract and introduction sections before he or 
she wants to write the related work section for 
the target paper.  Otherwise, we cannot get any 
information about the author?s own work. All 
other sections in the target paper are not used.  
4 Our Proposed System 
4.1 Overview 
In this paper, we propose a system called ARWG 
to automatically generate a related work section 
for a given target paper. The architecture of our 
system is shown in Figure 2. We take both the 
target paper and its reference papers as input and 
they are represented by several sections men-
tioned in Section 3.2. After preprocessing, we 
extract the feature vectors for sentences in the 
target paper and the reference papers, respective-
                                                 
2 http://www.pdflib.com/ 
3 http://aye.comp.nus.edu.sg/parsCit/ 
 
 
Figure 1: A sample related work section (Yang and 
Callan 2009) 
There has been a substantial amount of research on automatic 
taxonomy induction. As we mentioned earlier, two main 
approaches are pattern-based and clustering-based.
Pattern-based approaches are the main trend for automatic 
taxonomy induction. ?
Pattern-based approaches started from and still pay a great deal 
of attention to the most common is-a relations. ?
Clustering-based approaches usually represent word contexts as 
vectors and cluster words based on similarities of the vectors 
(Brown et al., 1992; Lin, 1998). ?
Many clustering-based approaches face the challenge of 
appropriately labeling non-leaf clusters. ? In this paper, we take 
an incremental clustering approach,... The advantage of the 
incremental approach is that it eliminates the trouble of 
inventing cluster labels and concentrates on placing terms in the 
correct positions in a taxonomy hierarchy.
The o k by S ow et al. (2006) is the most similar to ours ? 
Moreover, our approach employs heterogeneous features from a 
wide rang ; hile heir approach only used syntactic dependency.
Two different 
topics
Comparison 
with the 
author?s work
1626
ly. The importance scores for sentences in the 
target paper and the reference papers are as-
signed by using two SVR based sentence scoring 
models. The two SVR models are trained for 
sentences in the target paper and the reference 
papers, respectively. Meanwhile, a topic model is 
applied to the whole set of sentences in both the 
target paper and reference papers. The sentences 
are grouped into several different topic-biased 
clusters. The sentences with importance scores 
and topic cluster information are taken as the 
input for the global optimization framework. The 
optimization framework extracts sentences to 
describe both the author?s own work and back-
ground knowledge. More details of each part will 
be discussed in the following sections. 
4.2 Topic Model Learning 
As mentioned in the previous section, the related 
work section usually addresses several different 
topics. The topics may be different research 
themes or different aspects of a broad research 
theme. The related work section should describe 
the specific details for each topic, respectively. 
Therefore, we aim to discover the hidden top-
ics of the input papers, and we use the Probabil-
istic latent semantic analysis (PLSA) (Hofmann, 
1999) to solve this problem.  
The PLSA approach models each word in a 
document as a sample from a mixture model. 
The mixture components are multinomial ran-
dom variables that can be viewed as representa-
tions of ?topics?. Different words in a document 
may be generated from different topics. Each 
document is represented a list of mixing propor-
tions for these mixture components and can be 
reduced to a probability distribution on a fixed 
set of topics. 
Considering that the sentences in one paper 
may relate to different topics, we treat each sen-
tence as a ?document? d. We treat the noun 
phases in the sentences as the ?words? w. In or-
der to extract the noun phrases, chunking imple-
mented by the OpenNLP toolkit 4 is applied to 
the sentences. Noun phrases that contain words 
such as ?paper? and ?data? are discarded.  
Then the sentences with their corresponding 
noun phrases are taken as input into the PLSA 
model. Here both the sentences in the target pa-
per and the sentences in the reference papers are 
treated the same in the model. Finally, we can 
get the sentence set with topic information and 
use it in the subsequent steps. Each sentence has 
a topic weight t in each topic. 
4.3 Sentence Important Assessment 
In our proposed system, sentence importance 
assessment aims to assign an importance score to 
each sentence in the target paper and reference 
papers. The score of each sentence will be used 
in the subsequent optimization framework. We 
propose to use the support vector regression 
model to achieve this goal. In the above topic 
model learning process, we do not distinguish the 
sentences in the target paper and reference pa-
pers. In contrast, we train two different support 
vector regression models separately for the sen-
tences in the target paper and the sentences in the 
reference papers. In the related work section, the 
sentences that describe the author?s own work 
usually address the differences from the related 
works, while the sentences that describe the re-
lated works often focus on the specific details. 
We think the two kinds of sentences should be 
treated differently. 
Scoring Method 
To construct training data based on the papers 
collected, we apply a similarity scoring method 
to assign the importance scores to the sentences 
in the papers. The main hypothesis is that the 
sentences in the gold related work sections 
should summarize the target paper and reference 
papers as well. Thus the sentences in the papers 
which are more similar to the sentences in the 
gold related work sections should be considered 
more important and suitable to be selected. Our 
scoring method should assign higher scores to 
them. 
                                                 
4 http://opennlp.apache.org/ 
 
 
Figure 2: System Architecture 
Target paper Reference papers
Preprocessing
Topic Model
Sentence Score 
Assessment(target)
Sentence Score 
Assessment(reference)
Optimization 
Framework
Postprocessing
Related Work 
Section
1627
We define the importance score of a sentence 
in the papers as below: 
?????(?) =  ???
??
????
(???(?, ??
?))      (1) 
where s is a sentence in the papers,  ?? is the set 
of the sentences in the corresponding gold relat-
ed work section. The standard cosine measure is 
employed as the similarity function. 
Considering the difference between the sen-
tences that describe the author?s work and the 
sentences that describe the related works, we 
split the set of sentences in the gold related work 
section into two parts: one discusses the author?s 
own work and the other introduces the related 
works. We observe that sentences related to the 
author?s own work often feature specific words 
or phrases (such as ?we?, ?our work?, ?in this 
paper? etc.) in the related work section.  So we 
check the sentences about whether they contain 
clue words or phrases (i.e., ?in this paper?, ?our 
work? and 18 other phrases). If the clue phrase 
check fails, the sentence belongs to the related 
work part. If not, it belongs the own work part. 
Thus for the sentences in the target paper,  ?? 
is the set of sentences in the own work part of the 
gold related work section, while for the sentences 
in the reference papers,  ?? is the set of sentences 
in the related work part of the gold related work 
section. Then we can use the scoring method to 
compute the target scores of the sentences in the 
training set. It is noteworthy that two SVR mod-
els can be trained on the two parts of the training 
data, respectively.  
Feature 
Each sentence is represented by a set of features. 
The common features used for the sentences of 
the target paper and reference papers are shown 
in Table 1. The additional features applied to the 
sentences of the target paper are introduced in 
Table 2. 
Here, s is a sentence that needs to extract fea-
tures. th is paper title, section headings and sub-
section headings set of the reference papers or 
target paper for the two SVR models, respective-
ly. Each feature with ?*? represent a feature set 
that contains similar features. 
All the features are scaled into [0, 1]. Thus we 
can learn SVR models based on the features and 
importance scores of the sentences, and then use 
the models to predict an importance score for 
each sentence in the test set. The SVR models 
are trained and applied for the target paper and 
reference papers, respectively. 
Table 1: Common features employed in the SVR 
models 
Feature Description 
???(?, ??)? The similarity between s and each 
title in th; Stop words are removed 
and stemming is employed. 
WS(s,th) Number of words shared by s and 
th. 
??(?)? The position of s in its section or 
subsection 
???(?)? The parse tree information of s, 
including the number of noun 
phrase and verb phrases, the depth 
of the parse tree, etc. 
??????(?)? Indicates whether s is the first sen-
tence of the section or subsection 
?????(?)? Indicates whether s is the last sen-
tence of the section or subsection 
SWP(s) The percentage of the stop words  
Length(s) The length of sentence s 
Length_rw(s) The length of s after removing stop 
words 
SI(s) The section index of s that indi-
cates which section s is from. 
??????????(?)? Indicates whether a clue phrase 
appears in s. the clue phrases in-
clude ?our work?, ?propose? and 
other 20 words. Each clue phrase 
corresponds to one feature. 
 
Table 2: Additional features for sentences in the 
target paper 
Feature Description 
HasCitation(s) Indicates whether s contains a 
citation 
?h??????????(?)? Indicates whether s contains 
words or phrases used for com-
parison such as ?in contrast?, 
?instead? and other 26 words. 
Each word or phrase corre-
sponds to one feature. 
 
4.4 A Global Optimization Framework 
In the above steps, we can get the predicted im-
portance score and topic information for each 
sentence in the target paper and reference papers. 
Here, we introduce a global optimization frame-
work to generate the related work section. 
According to the structure of the related work 
section mentioned above, the related work sec-
tion usually discusses several topics. In each top-
ic, the related works and their details are intro-
duced. Besides, the author often compares his 
own work with these previous works. 
Therefore, we propose to formulate the genera-
tion as an optimization problem. Basically, we 
will be searching for a set of sentences to opti-
mize the objective function. 
1628
Table 3: Notations used in this section 
Symbol Description 
???/??? the sentence in the reference/target paper 
???/??? the length of sentence ???/ ??? 
???/??? the importance score of ???/??? 
????/???? indicates whether ???/??? is selected into 
the part of topic j in the generated related 
work section 
nr/nt the number of sentences in the refer-
ence/target papers 
m the topic count 
??? the topic weight of  ???/???  in topic j from 
the PLSA model 
B the set of unique bigrams 
?? indicates whether bigram ??  is included 
in the result 
??? the count of the occurrences of bigram ?? 
in the both target paper and reference 
papers 
???? the maximum word count of the related 
work section 
??  the maximum word count of the part of 
topic j which depends on the percentage 
of sentences belong to topic j 
?? the total set of bigrams in the whole pa-
per set 
?? the set of bigrams that sentence  ???/??? 
contains 
???/??? the set of sentences that include bigram 
?? in the reference/target papers 
 ?1,  ?2, ?3 parameters for tuning 
 
To design the objective function, three aspects 
should be considered: 
1) First, the related work section we generate 
should introduce the previous works well. In 
our assumption, sentences with higher im-
portance scores are better to be selected. In 
addition, very short sentences should be pe-
nalized. So we introduce the first part of our 
objective function below: 
? (?????? ? ???????)
?
?=1
??
?=1                    (2) 
We add the sentence length as a multipli-
cation factor in order to penalize the very 
short sentences, or the objective function 
tends to select more and shorter sentences. 
At the same time, the objective function does 
not tend to select the very long sentences. 
The total length of the sentences selected is 
fixed. So if the objective function tends to 
select the longer sentences, the fewer sen-
tences can be selected. A tradeoff needs to be 
made between the number and the average 
length of the sentences selected. 
 The constraints introduced below ensure 
that the sentence can only be selected into 
one topic and the topic weight is used to 
measure the degree that the sentence is rele-
vant to the specific topic. 
2) Second, similar to the first part, we should 
consider the own work part of the related 
work section. Thus the second part of our ob-
jective function is shown as follows: 
? (?????? ? ???????)
?
?=1
??
?=1                    (3) 
3) At last, redundancy reduction should be con-
sidered in the objective function. The last 
part of the objective function is shown below: 
? ?????
|?|
?=1                               (4) 
The intuition is that the more unique bi-
grams the related work section contains, the 
less redundancy the related work section has. 
We add  ??? as the weight of the bigram in or-
der to include more important bigrams. 
By combing all the parts defined above, we 
have the following full objective function: 
max
??,??
?1? (
???
?????
??? ? ???????)
?
?=1 +
??
?=1
?2? (
???
(1??)????
??? ? ???????)
?
?=1
??
?=1 +
?3?
?????
|??|
|?|
?=1                                                       (5) 
Subject to: 
? ???????
??
?=1 + ? ???????
??
?=1 < ??, ??? ? = 1,? ,
?                        (6) 
? ? ???????
?
?=1
??
?=1 < ?????         (7) 
? ? ???????
?
?=1
??
?=1 < (1 ? ?)????         (8) 
 ? ????
?
?=1 ? 1, ??? ? = 1,? , ??           (9) 
? ????
?
?=1 ? 1, ??? ? = 1,? , ??          (10) 
? ???????  ? |??| ? ????
?
?=1 , ??? ? = 1,? , ?? (11) 
? ???????  ? |??| ? ????
?
?=1 , ??? ? = 1,? , ?? (12) 
? ? ????
?
?=1 + ? ? ????
?
?=1?????????????? ? ?? ,
? = 1,? |?|                                                    (13) 
????, ????, ?? ? {0,1}                  (14) 
All the three parts in the objective function are 
normalized to [0, 1] by using the maximum 
length ???? and the total number of bigrams |?
?|. 
 ?1, ?2 and ?3 are parameters for tuning the three 
parts and we set  ?1+?2+?3 = 1. 
We explain the constraints as follows: 
Constraint (6): It ensures that the total word 
count of the part of topic j does not exceed  ??. 
Constraints (7), (8): The two constraints try to 
balance the lengths of the previous works part 
and the own work part, respectively. ? is set to 
2/3. 
Constraints (9), (10): These two constraints 
guarantee that the sentence can only be included 
into one topic. 
1629
Constraints (11), (12): When these two con-
straints hold, all bigrams that ?? has are selected 
if ?? is selected.  
Constraint (13): This constraint makes sure 
that at least one sentence in ??? or ??? is select-
ed if bigram ?? is selected. 
Therefore, we transform our optimization 
problem into a linear programing problem. We 
solve this linear programming problem by using 
the IBM CPLEX optimizer5. It generally takes 
tens of seconds to solve the problem and it is 
very efficient. 
Finally, ARWG post-processes sentences to 
improve readability, including replacing agentive 
forms with a citation to the specific article (e.g., 
?our work? ? ?(Hoang and Kan, 2010)?) for the 
sentences extracted from reference papers. The 
sentences belonging to different topics are placed 
separately.  
5 Evaluation 
5.1 Evaluation Setup 
To set up our experiments, we divide our dataset 
which contains 1050 target papers and their ref-
erence papers into two parts: 700 target papers 
for training, 150 papers for test and the other 200 
papers for validation. The PLSA topic model is 
applied to the whole dataset. We train two SVR 
regression models based on the own work part 
and the previous work part of the training data 
and apply the models to the test data. The global 
optimization framework is used to generate the 
related work sections. We set the maximum word 
count of the generated related work section to be 
equal to that of the gold related work section. 
The parameter values of  ?1, ?2 and ?3 are set to 
0.3, 0.1 and 0.6, respectively. The parameter val-
ues are tuned on the validation data.  
We compare our system with five baseline sys-
tems: MEAD-WT, LexRank-WT, ARWG-WT, 
MEAD and LexRank. MEAD 6  (Radev et al., 
2004) is an open-source extractive multi-
document summarizer. LexRank 7  (Eran and 
Radev, 2004) is a multi-document summarization 
system which is based on a random walk on the 
similarity graph of sentences. We also implement 
the MEAD, LexRank baselines and our method 
                                                 
5 www-01.ibm.com/software/integration/optimization/cplex-
optimizer/ 
6 http://www.summarization.com/mead/ 
7 In our experiments, LexRank performs much better than 
the more complex variant - C-LexRank (Qazvinian and 
Radev, 2008), and thus we choose LexRank, rather than C-
LexRank, to represent graph-based summarization methods 
for comparison in this paper.  
with only the reference papers (i.e. the target pa-
per?s content is not considered). Those methods 
are signed by ?-WT?.  
To evaluate the effectiveness of the SVR mod-
els we employ, we implement a baseline system 
RWGOF that uses the random walk scores as the 
important scores of the sentences and take the 
scores as inputs for the same global optimization 
framework as our system to generate the related 
work section. The random walk scores are com-
puted for the sentences in the reference papers 
and the target paper, respectively. 
We use the ROUGE toolkit to evaluate the 
content quality of the generated related work sec-
tions. ROUGE (Lin, 2004) is a widely used au-
tomatic summarization evaluation method based 
on n-gram comparison. Here, we use the F-
Measure scores of ROUGE-1, ROUGE-2 and 
ROUGE-SU4. The model texts are set as the 
gold related work sections extracted from the 
target papers, and word stemming is utilized. 
ROUGE-N is an n-gram based measure between 
a candidate text and a reference text. The recall 
oriented score, the precision oriented score and 
the F-measure score for ROUGE-N are comput-
ed as follows: 
     ????? ? ???????  
= ? ? ??????????(?????)???????{????????? ????}  / 
   ? ? ?????(?????)???????{????????? ????}        (15) 
     ????? ? ??????????  
= ? ? ??????????(?????)???????{????????? ????}  / 
   ? ? ?????(?????)???????{????????? ????}       (16) 
     ????? ? ?????????? 
= 2 ? ????? ? ??????? ? ????? ? ?????????? / 
   ????? ? ??????? + ????? ? ??????????       (17) 
where n stands for the length of the n-gram 
????? , and ??????????(?????) is the maxi-
mum number of n-grams co-occurring in a can-
didate text and a reference text. 
In addition, we conducted a user study to sub-
jectively evaluate the related work sections to get 
more evidences. We selected the related work 
sections generated by different methods for 15 
random target papers in the test set. We asked 
three human judges to follow an evaluation 
guideline we design and evaluate these related 
work sections. The human judges are graduate 
students in the computer science field and they 
did not know the identities of the evaluated relat-
ed work sections. They were asked to give a rat-
ing on a scale of 1 (very poor) to 5 (very good) 
for the correctness, readability and usefulness of 
the related work sections, respectively: 
1630
1) Correctness: Is the related work section ac-
tually related to the target paper? 
2) Readability: Is the related work section 
easy for the readers to read and grasp the 
key content? 
3) Usefulness: Is the related work section 
useful for the author to prepare their final 
related work section? 
Paired T-Tests are applied to both the ROUGE 
scores and rating scores for comparing ARWG 
and baselines and comparing the systems with 
WT and without WT. 
5.2 Results and Discussion 
Table 4: ROUGE F-measure comparison results 
Method ROUGE-1 ROUGE-2 ROUGE-
SU4 
Mead-
WT 
0.39720 0.08785 0.14694 
LexRank-
WT 
0.43267 0.09228 0.16312 
ARWG-
WT 
0.45077?{1,2} 0.09987?{1,2} 0.16731?{1}#{2} 
Mead 0.41012?{1} 0.09642?{1} 0.15441?{1} 
LexRank 0.44235?{2} 0.10090?{2} 0.17067?{2} 
ARWG ?. ??????{???} ?. ??????{???} ?. ??????{???} 
(* represents pairwise t-test value p < 0.01; # rep-
resents p < 0.05; the numbers in the brackets rep-
resent the indices of the methods compared, e.g. 
1 for MEAD-WT, 2 for LexRank-WT, etc.) 
 
Table 5: Average rating scores of judges 
Method Correctness Readability Usefulness 
Mead 2.971 2.664 2.716 
LexRank 2.958 2.847 2.784 
ARWG 3.433?# 3.420?# 3.382?# 
(*# represents pairwise t-test value p < 0.01, 
compared with Mead and LexRank, respectively.)  
 
Table 6: ROUGE F-measure comparison of dif-
ferent sentence importance scores 
Method ROUGE-1 ROUGE-2 ROUGE-SU4 
RWGOF 0.46932 0.11791 0.18426 
ARWG 0.47940 0.12176 0.18618 
 
The evaluation results over ROUGE metrics are 
presented in Table 4. It shows that our proposed 
system can get higher ROUGE scores, i.e., better 
content quality. In our system, we split the sen-
tence set into different topic-biased parts, and the 
importance scores of sentences in the target pa-
per and reference papers are learned differently. 
So the obtained importance scores of the sen-
tences are more reliable.  
The global optimization framework considers 
the extraction of both the previous work part and 
the own work part. We can see the importance of 
the own work part by comparing the results of 
the methods with or without considering the own 
work part. MEAD, LexRank and our method all 
get a significant improvement after considering 
the own work part by extracting sentences from 
the target paper. The results also prove our as-
sumption about the related work section structure. 
Figure 3 presents the fluctuation of ROUGE 
scores when tuning the parameters ?1, ?2 and ?3. 
We can see our method generally performs better 
than the baselines. All the three parts in the ob-
jective function are useful to generate related 
work sections with good quality. 
The average scores rated by human judges for 
each method are showed in Table 5. We can see 
that the related work sections generated by our 
system are more related to the target papers. 
Moreover, because of the good structure of our 
generated related work sections, our generated 
related work sections are considered more reada-
ble and more useful for the author to prepare the 
final related work sections. 
T-test results show that the performance im-
provements of our method over baselines are 
statistically significant on both automatic and 
manual evaluations. Most of p-values for t-test 
are far smaller than 0.01. 
Overall, the results indicate that our method 
can generate much better related work sections 
0
0.6
0
0.2
0.4
0.6
0 0.3 0.6 0.9 ?
1
R
O
U
G
E-
1
?2
ROUGE-1
0.4-
0.6
0.2-
0.4
0
0.6
0
0.05
0.1
0.15
0 0.3 0.6 0.9
?
1
R
O
U
G
E-
2
?2
ROUGE-2
0.1-
0.15
0.05-
0.1
0
0.6
0
0.05
0.1
0.15
0.2
0 0.3 0.6 0.9 ?
1R
O
U
G
E-
SU
4
?2
ROUGE-SU4
0.15
-0.2
0.1-
0.15
Figure 3: Parameter influences (horizontal, vertical axis are ?1, ?2 , respectively, ?3 = 1 ? ?1 ? ?2 ) 
1631
than the baselines on both automatic and human 
evaluations. 
Table 6 shows the comparison results between 
ARWG and RWGOF. We can see ARWG per-
forms better than RWGOF. It proves that the 
SVR models can better estimate the importance 
scores of the sentences. For the SVR models are 
trained from the large dataset, the sentence 
scores predicted by the SVR models can be more 
reliable to be used in the global optimization 
framework. 
6 Conclusion and Future Work 
This paper proposes a novel system called 
ARWG to generate related work sections for ac-
ademic papers. It first exploits a PLSA model to 
split the sentence set of the given papers into dif-
ferent topic-biased parts, and then applies regres-
sion models to learn the importance scores of the 
sentences. At last an optimization framework is 
proposed to generate the related work section. 
Evaluation results show that our system can gen-
erate much better related work sections than the 
baseline methods. 
In future work, we will make use of citation 
sentences to improve our system. Citation sen-
tences are the sentences that contains an explicit 
reference to another paper and they usually high-
light the most important aspects of the cited pa-
pers. So citation sentences are likely to contain 
important and rich information for generating 
related work sections. 
Acknowledgments 
The work was supported by National Natural 
Science Foundation of China (61170166, 
61331011), Beijing Nova Program (2008B03) 
and National Hi-Tech Research and Develop-
ment Program (863 Program) of China 
(2012AA011101). We also thank the anonymous 
reviewers for very helpful comments. The corre-
sponding author of this paper, according to the 
meaning given to this role by Peking University, 
is Xiaojun Wan. 
Reference 
Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and 
Carolyn Penstein Ros?. 2011. Towards multi-
document summarization of scientific articles: 
making interesting comparisons with SciSumm. 
In Proceedings of the Workshop on Automatic 
Summarization for Different Genres, Media, and 
Languages, pp. 8-15. Association for Computa-
tional Linguistics. 
Phyllis B. Baxendale. 1958. Machine-made index for 
technical literature: an experiment. IBM Journal of 
Research and Development 2, no. 4: 354-361. 
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 
2011. Jointly learning to extract and compress. 
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics: Human 
Language Technologies-Volume 1, pp. 481-490. 
Association for Computational Linguistics. 
Chih-Chung Chang, and Chih-Jen Lin. 2011. 
LIBSVM: a library for support vector ma-
chines. ACM Transactions on Intelligent Systems 
and Technology (TIST) 2, no. 3: 27. 
John M. Conroy, and Dianne P. O'leary. 2001. Text 
summarization via hidden markov models. 
In Proceedings of the 24th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval, pp. 406-407. ACM. 
Harold P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM (JACM) 16, 
no. 2: 264-285. 
G?nes Erkan, and Dragomir R. Radev. 2004. LexPag-
eRank: Prestige in Multi-Document Text Summa-
rization. In EMNLP, vol. 4, pp. 365-371. 
G?nes Erkan, and Dragomir R. Radev. 2004. 
LexRank: Graph-based lexical centrality as sali-
ence in text summarization. J. Artif. Intell. 
Res.(JAIR) 22, no. 1: 457-479. 
Dimitrios Galanis, Gerasimos Lampouras, and Ion 
Androutsopoulos. 2012. Extractive Multi-
Document Summarization with Integer Linear Pro-
gramming and Support Vector Regression. 
In COLING, pp. 911-926. 
Dimitrios Galanis, and Prodromos Malakasiotis. 2008. 
Aueb at tac 2008. InProceedings of the TAC 2008 
Workshop. 
Dan Gillick, and Benoit Favre. 2009. A scalable glob-
al model for summarization. InProceedings of the 
Workshop on Integer Linear Programming for 
Natural Langauge Processing, pp. 10-18. Associa-
tion for Computational Linguistics. 
Cong Duy Vu Hoang, and Min-Yen Kan. 2010. To-
wards automated related work summarization. 
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics: Posters, pp. 
427-435. Association for Computational Linguis-
tics. 
Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li. 
2010. Modeling document summarization as multi-
objective optimization. In Intelligent Information 
Technology and Security Informatics (IITSI), 2010 
Third International Symposium on, pp. 382-386. 
IEEE. 
1632
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pp. 50-57. 
ACM. 
Chin-Yew Lin. 2004. Rouge: A package for automatic 
evaluation of summaries. InText Summarization 
Branches Out: Proceedings of the ACL-04 Work-
shop, pp. 74-81. 
Hans Peter Luhn. 1958. The automatic creation of 
literature abstracts. IBM Journal of research and 
development 2, no. 2: 159-165. 
Inderjeet Mani, and Eric Bloedorn. 1999. Summariz-
ing similarities and differences among related doc-
uments. Information Retrieval 1, no. 1-2: 35-67. 
Ryan McDonald. 2007. A study of global inference 
algorithms in multi-document summarization. 
Springer Berlin Heidelberg. 
Qiaozhu Mei, and ChengXiang Zhai. 2008. Generat-
ing Impact-Based Summaries for Scientific Litera-
ture. In ACL, vol. 8, pp. 816-824. 
Rada Mihalcea, and Paul Tarau. 2005. A language 
independent algorithm for single and multiple doc-
ument summarization. 
Rada Mihalcea, and Hakan Ceylan. 2007. Explora-
tions in Automatic Book Summarization. 
In EMNLP-CoNLL, pp. 380-389. 
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed 
Hassan, Pradeep Muthukrishan, Vahed Qazvinian, 
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. 
In Proceedings of Human Language Technologies: 
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational 
Linguistics, pp. 584-592. Association for Computa-
tional Linguistics. 
Preslav Nakov, Ariel Schwartz, and M. Hearst. 2004. 
Citation sentences for semantic analysis of biosci-
ence text. In Proceedings of the SIGIR'04 work-
shop on Search and Discovery in Bioinformatics. 
Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and 
William W. Cohen. 2008. Joint latent topic models 
for text and citations. In Proceedings of the 14th 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 542-
550. ACM. 
Vahed Qazvinian, and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pp. 689-696. Association for Computa-
tional Linguistics. 
You Ouyang, Sujian Li, and Wenjie Li. 2007. Devel-
oping learning strategies for topic-based summari-
zation. In Proceedings of the sixteenth ACM con-
ference on Conference on information and 
knowledge management, pp. 79-86. ACM. 
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko 
Dimitrov, Elliott Drabek et al. 2004. MEAD-a plat-
form for multidocument multilingual text summa-
rization. Proceedings of the 4th International Con-
ference on Language Resources and Evaluation 
(LREC 2004). 
Ariel S. Schwartz, and Marti Hearst. 2006. Summariz-
ing key concepts using citation sentences. 
In Proceedings of the Workshop on Linking Natu-
ral Language Processing and Biology: Towards 
Deeper Biological Literature Analysis, pp. 134-135. 
Association for Computational Linguistics. 
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and 
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, vol. 7, 
pp. 2862-2867. 
Andreas  Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, 
Paul Taylor, Rachel Martin, Carol Van Ess-
Dykema, and Marie Meteer. 2000. Dialogue act 
modeling for automatic tagging and recognition of 
conversational speech. Computational linguis-
tics 26, no. 3: 339-373. 
Kristian Woodsend, and Mirella Lapata. 2012. Multi-
ple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning, pp. 233-243. Association for 
Computational Linguistics. 
Shasha Xie, Benoit Favre, Dilek Hakkani-T?r, and 
Yang Liu. 2009. Leveraging sentence weights in a 
concept-based optimization framework for extrac-
tive meeting summarization. In INTERSPEECH, 
pp. 1503-1506. 
Hui Yang, and Jamie Callan. 2009. A metric-based 
framework for automatic taxonomy induction. 
In Proceedings of the Joint Conference of the 47th 
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pp. 
271-279. Association for Computational Linguis-
tics. 
Ozge Yeloglu, Evangelos Milios, and Nur Zincir-
Heywood. 2011. Multi-document summarization of 
scientific corpora. In Proceedings of the 2011 ACM 
Symposium on Applied Computing, pp. 252-258. 
ACM. 
1633
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828?1833,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Decoding of Tree Transduction Models for Sentence Compression
Jin-ge Yao Xiaojun Wan Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{yaojinge, wanxiaojun, xiaojianguo}@pku.edu.cn
Abstract
In this paper, we provide a new method for
decoding tree transduction based sentence
compression models augmented with lan-
guage model scores, by jointly decoding
two components. In our proposed so-
lution, rich local discriminative features
can be easily integrated without increasing
computational complexity. Utilizing an
unobvious fact that the resulted two com-
ponents can be independently decoded, we
conduct efficient joint decoding based on
dual decomposition. Experimental results
show that our method outperforms tradi-
tional beam search decoding and achieves
the state-of-the-art performance.
1 Introduction
Sentence compression is the task of generating a
grammatical and shorter summary for a long sen-
tence while preserving its most important informa-
tion. One specific instantiation is deletion-based
compression, namely generating a compression by
dropping words. Various approaches have been
proposed to challenge the task of deletion-based
compression. Earlier pioneering works (Knight
and Marcu, 2000) considered several insightful
approaches, including noisy-channel based gen-
erative models and discriminative decision tree
models. Structured discriminative compression
models (McDonald, 2006) are capable of inte-
grating rich features and have been proved effec-
tive for this task. Another powerful paradigm for
sentence compression should be mentioned here
is constraints-based compression,including inte-
ger linear programming solutions (Clarke and La-
pata, 2008) and first-order Markov logic networks
(Huang et al., 2012; Yoshikawa et al., 2012).
A notable class of methods that explicitly deal
with syntactic structures are tree transduction
models (Cohn and Lapata, 2007; Cohn and Lap-
ata, 2009). In such models a synchronous gram-
mar is extracted from a corpus of parallel syn-
tax trees with leaves aligned. Compressions are
generated from the grammar with learned weights.
Previous works have noticed that local coherence
is usually needed by introducing ngram language
model scores, which will make accurate decoding
intractable. Traditional approaches conduct beam
search to find approximate solutions (Cohn and
Lapata, 2009).
In this paper we propose a joint decoding strat-
egy to challenge this decoding task. We ad-
dress the problem as jointly decoding a simple
tree transduction model that only considers rule
weights and an ngram compression model. Al-
though either part can be independently solved by
dynamic programming, the naive way to integrate
two groups of partial scores into a huge dynamic
programming chart table is computationally im-
practical. We provide an effective dual decompo-
sition solution that utilizes the efficient decoding
of both parts. By integrating rich structured fea-
tures that cannot be efficiently involved in normal
formulation, results get significantly improved.
2 Motivation
Under the tree transduction models, the sentence
compression task is formulated as learning a map-
ping from an input source syntax tree to a target
tree with reduced number of leaves. This map-
ping is known as a synchronous grammar. The
synchronous grammar discussed through out this
paper will be synchronous tree substitution gram-
mar (STSG), as in previous studies.
In such formulations, sentence compression is
finding the best derivation from a syntax tree that
produces a simpler target tree, under the current
definition of grammar and learned parameters.
Each derivation is attached with a score. For the
sake of efficient decoding, the score often decom-
1828
poses with rules involved in the derivation. A typ-
ical score definition for a derivation y of source
tree x is in such form (Cohn and Lapata, 2008;
Cohn and Lapata, 2009):
S(x,y)=
?
r?y
w
T
?
r
(x)+logP (ngram(y)) (1)
The first term is a weighted sum of features ?
r
(x)
defined on each rule r. It is plausible to introduce
local scores from ngram models. The second term
in the above score definition is added with such
purpose.
Cohn and Lapata (2009) explained that ex-
act decoding of Equation 1 is intractable. They
proposed a beam search decoding strategy cou-
pled with cube-pruning heuristic (Chiang, 2007),
which can further improve decoding efficiency at
the cost of largely losing exactness in log probabil-
ity calculations. For efficiency reasons, rich local
ngram features have not been introduced as well.
3 Components of Joint Decoding
The score in Equation 1 consists of two parts: sum
of weighted rule features and local ngram scores
retrieved from a language model. There is an im-
plicit fact that either part can be used alone with
slight modifications to generate a coarse candidate
compression. Therefore, we can build a joint de-
coding system that consists of these two indepen-
dently decodable components.
In this section we will refer to these two in-
dependent models as the pure tree transduction
model and the pure ngram compression model,
described in Section 3.1 and Section 3.2 respec-
tively. There is a direct generalization of the
ngram model by introducing rich local features,
which results in the structured discriminative mod-
els (Section 3.3).
3.1 Pure Tree Transduction model
By merely considering scores from tree transduc-
tion rules, i.e. the first part of Equation 1, we can
have our scores factorized with rules. Then finding
the best derivation from a STSG grammar can be
easily solved by a dynamic programming process
described by Cohn and Lapata (2007).
This simplified pure tree transduction model
can still produce decent compressions if the rule
weights are properly learned during training.
3.2 Pure Ngram based Compression
The pure ngram based model will try to find
the most locally smooth compression, reflected
by having the maximum log probability score of
ngrams.
To avoid the trivial solution of deleting all
words, we find the target compression with speci-
fied length by dynamic programming.
Furthermore, we can integrate features other
than log probabilities. This is equivalent to using a
structured discriminative model with rich features
on ngrams of candidate compressions.
3.3 Structured Discriminative Model
The structured discriminative model proposed by
McDonald (2006) defines rich features on bigrams
of possible compressions. The score is defined as
weighted linear combination of those features:
f(x, z) =
|z|
?
j=2
w ? f(x, L(z
j?1
), L(z
j
)) (2)
where the functionL(z
k
) maps a token z
k
in com-
pression z back to the index of the original sen-
tence x. Decoding can still be efficiently done by
dynamic programming.
With rich local structural information, the struc-
tured discriminative model can play a complemen-
tary role to the tree transduction model that focus
more on global syntactic structures.
4 Joint Decoding
From now on the remaining issue is jointly de-
coding the components. Either part factorizes
over local structures: rules for the tree transduc-
tion model and ngrams for the language model or
structured discriminative model. We may build a
large dynamic programming table to utilize this
kind of locality. Unfortunately this is computa-
tionally impractical. It is mathematically equiva-
lent to perform exact dynamic programming de-
coding of Equation 1, which would consume
asymptotically O(SRL
2(n?1)V
)
1
time for build-
ing the chart (Cohn and Lapata, 2009). Cohn and
Lapata (2009) proposed a beam search approxima-
tion along with cube-pruning heuristics to reduce
the time complexity down to O(SRBV )
2
.
1
S, R, L and V denote respectively for the number of
source tree nodes, the number of rules, size of target lexicon
and number of variables involved in each rule.
2
B denotes the beam width.
1829
In this work we utilize the efficiency of indepen-
dent decoding from the two components respec-
tively and then combine their solutions according
to certain standards. This naturally results in a
dual decomposition (Rush et al., 2010) solution.
Dual decomposition has been applied in sev-
eral natural language processing tasks, including
dependency parsing (Koo et al., 2010), machine
translation (Chang and Collins, 2011; Rush and
Collins, 2011) and information extraction (Re-
ichart and Barzilay, 2012). However, the strength
of this inference strategy has seldom been noticed
in researches on language generation tasks.
We briefly describe the formulation here.
4.1 Description
We denote the pure tree transduction part and the
pure ngram part as g(y) and f(z) respectively.
Then joint decoding is equivalent to solving:
max
y?Y,z?Z
g(y) + f(z) (3)
s.t. z
kt
= y
kt
, ?k ? {1, ..., n}, ?t ? {0, 1},
where y denotes a derivation which yields a final
compression {y
1
, ...,y
m
}. This derivation comes
from a pure tree transduction model. z denotes the
compression composed of {z
1
, ..., z
m
} from an
ngram compression model. Without loss of gener-
ality, we consider y
k
and z
k
as indicators that take
value 1 if the k?s token of original sentence has
been preserved in the compression and 0 if it has
been deleted. In the constraints of problem 3, y
kt
or z
kt
denote indicator variables that take value 1
if y
k
or z
k
= t and 0 otherwise.
Let L(u,y, z) be the Lagrangian of (3). Then
the dual objective naturally factorizes into two
parts that can be evaluated independently:
L(u) = max
y?Y,z?Z
L(u,y, z)
= max
y?Y,z?Z
g(y) + f(z) +
?
k,t
u
kt
(z
kt
? y
kt
)
= max
y?Y
(g(y)?
?
k,t
u
kt
y
kt
) +
max
z?Z
(f(z) +
?
k,t
u
kt
z
kt
)
With this factorization, Algorithm 1 tries to
solve the dual problem min
u
L(u) by alternatively
decoding each component.
This framework is feasible and plausible in that
the two subproblems (line 3 and line 4 in Algo-
rithm 1) can be easily solved with slight modifica-
Algorithm 1 Dual Decomposition Joint Decoding
1: Initialization: u
(0)
k
= 0, ?k ? {1, ..., n}
2: for i = 1 to MAX ITER do
3: y
(i)
? argmax
y?Y
(g(y)?
?
k,t
u
(i?1)
kt
y
kt
)
4: z
(i)
? argmax
z?Z
(f(z) +
?
k,t
u
(i?1)
kt
z
kt
)
5: if y
(i)
kt
= z
(i)
kt
?k ?t then
6: return (y
(i)
, z
(i)
)
7: else
8: u
(i)
kt
? u
(i?1)
kt
? ?
i
(z
(i)
kt
? y
(i)
kt
)
9: end if
10: end for
tions on the values of the original dynamic pro-
gramming chart. Joint decoding of a pure tree
transduction model and a structured discriminative
model is almost the same.
The asymptotic time complexity of Algorithm 1
is O(k(SRV + L
2(n?1)
)), where k denotes the
number of iterations. This is a significant re-
duction of O(SRL
2(n?1)V
) by directly solving
the original problem and is also comparable to
O(SRBV ) of conducting beam search decoding.
We apply a similar heuristic with Rush and
Collins (2012) to set the step size ?
i
=
1
t+1
, where
t < i is the number of past iterations that increase
the dual value. This setting decreases the step
size only when the dual value moves towards the
wrong direction. We limit the maximum iteration
number to 50 and return the best primal solution
y
(i)
among all previous iterations for cases that do
not converge in reasonable time.
5 Experiments
5.1 Baselines
The pure tree transduction model and the discrim-
inative model naturally become part of our base-
lines for comparison
3
. Besides comparing our
methods against the tree-transduction model with
ngram scores by beam search decoding, we also
compare them against the available previous work
from Galanis and Androutsopoulos (2010). This
state-of-the-art work adopts a two-stage method to
rerank results generated by a discriminative maxi-
mum entropy model.
5.2 Data Preparation
We evaluated our methods on two standard cor-
pora
4
, refer to as Written and Spoken respectively.
3
The pure ngram language model should not be consid-
ered here as it requires additional length constraints and in
general does not produce competitive results at all merely by
itself.
4
Available at http://jamesclarke.net/research/resources
1830
We split the datasets according to Table 1.
Table 1: Dataset partition (number of sentences)
Corpus Training Development Testing
Written 1,014 324 294
Spoken 931 83 254
All tree transduction models require parallel
parse trees with aligned leaves. We parsed all sen-
tences with the Stanford Parser
5
and aligned sen-
tence pairs with minimum edit distance heuristic
6
. Syntactic features of the discriminative model
were also taken from these parse trees.
For systems involving ngram scores, we trained
a trigram language model on the Reuters Corpus
(Volume 1)
7
with modified Kneser-Ney smooth-
ing, using the widely used tool SRILM
8
.
5.3 Model Training
The training process of a tree transduction model
followed similarly to Cohn and Lapata (2007) us-
ing structured SVMs (Tsochantaridis et al., 2005).
The structured discriminative models were trained
according to McDonald (2006).
5.4 Evaluation Metrics
We assessed the compression results by the F1-
score of grammatical relations (provided by a
dependency parser) of generated compressions
against the gold-standard compression (Clarke and
Lapata, 2006). All systems were controlled to pro-
duce similar compression ratios (CR) for fair com-
parison. We also reported manual evaluation on a
sampled subset of 30 sentences from each dataset.
Three unpaid volunteers with self-reported fluency
in English were asked to rate every candidate. Rat-
ings are in the form of 1-5 scores for each com-
pression.
6 Results
We report test set performance of the struc-
tured discriminative model, the pure tree transduc-
tion (T3), Galanis and Androutsopoulos (2010)?s
method (G&A2010), tree transduction with lan-
guage model scores by beam search and the pro-
posed joint decoding solutions.
5
http://nlp.stanford.edu/software/lex-parser.shtml
6
Ties were broken by always aligning a token in compres-
sion to its last appearance in the original sentence. This may
better preserve the alignments of full constituents.
7
http://trec.nist.gov/data/reuters/reuters.html
8
http://www-speech.sri.com/projects/srilm/
Table 2 shows the compression ratios and F-
measure of grammatical relations in average for
each dataset. Table 3 presents averaged human rat-
ing results for each dataset. We carried out pair-
wise t-test to examine the statistical significance
of the differences
9
. In both datasets joint decod-
ing with dual decomposition solution outperforms
other systems, especially when structured models
involved. We can also find certain improvements
of joint modeling with dual decomposition on the
original beam search decoding of Equation 1, un-
der very close compression ratios.
Joint decoding of pure tree transduction and dis-
criminative model gives better performance than
the joint model of tree transduction and language
model. From Table 3 we can see that integrat-
ing discriminative model will mostly improve the
preservation of important information rather than
grammaticality. This is reasonable under the fact
that the language model is trained on large scale
data and will often preserve local grammatical co-
herence, while the discriminative model is trained
on small but more compression specific corpora.
Table 2: Results of automatic evaluation. (?:
sig. diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD) for p < 0.01)
Written CR(%) GR-F1(%)
Discriminative 70.3 52.4
??
G&A2010 71.6 60.2
?
Pure Tree-Transduction 72.6 52.3
??
T3+LM (Beam Search) 70.4 58.8
?
T3+LM (Dual Decomp.) 70.7 60.5
T3+Discr. (Dual Decomp.) 71.0 62.3
Gold-Standard 71.4 100.0
Spoken CR(%) GR-F1(%)
Discriminative 69.5 50.6
??
G&A2010 71.7 59.2
?
Pure Tree-Transduction 73.6 53.8
??
T3+LM (Beam Search) 75.5 59.5
?
T3+LM (Dual Decomp.) 75.3 61.5
T3+Discr. (Dual Decomp.) 74.9 63.3
Gold-Standard 72.4 100.0
Table 4 shows some examples of compressed
sentences produced by all the systems in compar-
ison. The two groups of outputs are compressions
of one sentence from the Written corpora and
the Spoken corpora respectively. Ungrammatical
compressions can be found very often by several
baselines for different reasons, such as the outputs
from pure tree transduction and the discriminative
model in the first group. The reason behind the
9
For all multiple comparisons in this paper, significance
level was adjusted by the Holm-Bonferroni method.
1831
Table 3: Results of human rating. (?: sig.
diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD), for p < 0.01)
Written GR. Imp. CR(%)
Discriminative 3.92
??
3.46
??
70.6
G&A2010 4.11
??
3.50
??
72.4
Pure Tree-Transduction 3.85
??
3.42
??
70.1
T3+LM (Beam Search) 4.22
??
3.69
?
73.0
T3+LM (Dual Decomp.) 4.63 3.98 73.2
T3+Discr. (Dual Decomp.) 4.62 4.25 73.5
Gold-Standard 4.89 4.76 72.9
Spoken GR. Imp. CR(%)
Discriminative 3.95
??
3.62
??
71.2
G&A2010 4.09
??
3.96
?
72.5
Pure Tree-Transduction 3.92
??
3.55
??
71.4
T3+LM (Beam Search) 4.20
?
3.78
?
75.0
T3+LM (Dual Decomp.) 4.35 4.18 74.5
T3+Discr. (Dual Decomp.) 4.47 4.26 74.7
Gold-Standard 4.83 4.80 73.1
under generation of pure tree transduction is that it
mainly deals with global syntactic integrity merely
in terms of the application of synchronous rules.
Introducing language model scores will smooth
the candidate compressions and avoid many ag-
gressive decisions of tree transduction. Discrim-
inative models are good at local decisions with
poor consideration of grammaticality. We can see
that the joint models have collected their predic-
tive power together. Unfortunately we can still
observe some redundancy from our outputs in the
examples. The size of training corpus is not large
enough to provide enough lexicalized information.
On the other hand, the time consumption of
the joint model with dual decomposition decoding
in our experiments matched the aforementioned
asymptotic analysis. The training process based
on new decoding method consumes similar time
as beam search with cube-pruning heuristic.
7 Conclusion and Future Work
In this paper we propose a joint decoding scheme
for tree transduction based sentence compression.
Experimental results suggest that the proposed
framework works well. The overall performance
gets further improved under our framework by in-
troducing the structured discriminative model.
As several recent efforts have focused on ex-
tracting large-scale parallel corpus for sentence
compression (Filippova and Altun, 2013), we
would like to study how larger corpora can af-
fect tree transduction and our joint decoding so-
Table 4: Example outputs
Original: It was very high for people who took their
full-time education beyond the age of 18 , and higher
among women than men for all art forms except jazz
and art galleries .
Discr.: It was high for people took education higher
among women .
(Galanis and Androutsopoulos, 2010): It was high for
people who took their education beyond the age of 18 ,
and higher among women .
Pure T3: It was very high for people who took .
T3+LM-BeamSearch: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+LM-DualDecomp: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+Discr.: It was high for people who took education
beyond the age of 18 , and higher among women than
men .
Gold-Standard: It was very high for people who took
full-time education beyond 18 , and higher among
women for all except jazz and galleries .
Original: But they are still continuing to search the
area to try and see if there were , in fact , any further
shooting incidents .
Discr.: they are continuing to search the area to try and
see if there were , further shooting incidents .
(Galanis and Androutsopoulos, 2010): But they are still
continuing to search the area to try and see if there
were , in fact , any further shooting incidents .
Pure T3: they are continuing to search the area to try
and see if there were any further shooting incidents .
T3+LM-BeamSearch: But they are continuing to
search the area to try and see if there were , in fact ,
any further shooting incidents .
T3+LM-DualDecomp: But they are continuing to
search the area to try and see if there were any further
shooting incidents .
T3+Discr.: they are continuing to search the area to try
and see if there were further shooting incidents .
Gold-Standard: they are continuing to search the area
to see if there were any further incidents .
lution. Meanwhile, We would like to explore on
how other text-rewriting problems can be formu-
lated as a joint model and be applicable to similar
strategies described in this work.
Acknowledgements
This work was supported by National Hi-Tech Re-
search and Development Program (863 Program)
of China (2014AA015102, 2012AA011101) and
National Natural Science Foundation of China
(61170166, 61331011). We also thank the anony-
mous reviewers for very helpful comments.
The contact author of this paper, according to
the meaning given to this role by Peking Univer-
sity, is Xiaojun Wan.
1832
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 26?37, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377?384. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
73?82, Prague, Czech Republic, June. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 137?144. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1481?1491, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 885?893, Los Angeles, California,
June. Association for Computational Linguistics.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1288?1298, Cambridge, MA, October.
Association for Computational Linguistics.
Ryan T McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 70?
79, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72?82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305?362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1?11, Cambridge, MA, October.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning Re-
search, pages 1453?1484.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers-Volume 2, pages
349?353. Association for Computational Linguis-
tics.
1833
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917?926,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Language Document Summarization Based on Machine 
Translation Quality Prediction 
 
Xiaojun Wan, Huiying Li and Jianguo Xiao 
Institute of Compute Science and Technology, Peking University, Beijing 100871, China 
Key Laboratory of Computational Linguistics (Peking University), MOE, China 
{wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cn 
 
 
Abstract 
 
Cross-language document summarization is a 
task of producing a summary in one language 
for a document set in a different language. Ex-
isting methods simply use machine translation 
for document translation or summary transla-
tion. However, current machine translation 
services are far from satisfactory, which re-
sults in that the quality of the cross-language 
summary is usually very poor, both in read-
ability and content.  In this paper, we propose 
to consider the translation quality of each sen-
tence in the English-to-Chinese cross-language 
summarization process. First, the translation 
quality of each English sentence in the docu-
ment set is predicted with the SVM regression 
method, and then the quality score of each sen-
tence is incorporated into the summarization 
process. Finally, the English sentences with 
high translation quality and high informative-
ness are selected and translated to form the 
Chinese summary. Experimental results dem-
onstrate the effectiveness and usefulness of the 
proposed approach.  
 
1 Introduction 
Given a document or document set in one source 
language, cross-language document summariza-
tion aims to produce a summary in a different 
target language. In this study, we focus on Eng-
lish-to-Chinese document summarization for the 
purpose of helping Chinese readers to quickly 
understand the major content of an English docu-
ment or document set. This task is very impor-
tant in the field of multilingual information ac-
cess.  
Till now, most previous work focuses on 
monolingual document summarization, but 
cross-language document summarization has re-
ceived little attention in the past years. A 
straightforward way for cross-language docu-
ment summarization is to translate the summary 
from the source language to the target language 
by using machine translation services. However, 
though machine translation techniques have been 
advanced a lot, the machine translation quality is 
far from satisfactory, and in many cases, the 
translated texts are hard to understand. Therefore, 
the translated summary is likely to be hard to 
understand by readers, i.e., the summary quality 
is likely to be very poor. For example, the trans-
lated Chinese sentence for an ordinary English 
sentence (?It is also Mr Baker who is making the 
most of presidential powers to dispense lar-
gesse.?) by using Google Translate is ?????
????????????????????. 
The translated sentence is hard to understand 
because it contains incorrect translations and it is 
very disfluent. If such sentences are selected into 
the summary, the quality of the summary would 
be very poor.  
In order to address the above problem, we 
propose to consider the translation quality of the 
English sentences in the summarization process. 
In particular, the translation quality of each Eng-
lish sentence is predicted by using the SVM re-
gression method, and then the predicted MT 
quality score of each sentence is incorporated 
into the sentence evaluation process, and finally 
both informative and easy-to-translate sentences 
are selected and translated to form the Chinese 
summary.  
An empirical evaluation is conducted to evalu-
ate the performance of machine translation qual-
ity prediction, and a user study is performed to 
evaluate the cross-language summary quality. 
The results demonstrate the effectiveness of the 
proposed approach.  
The rest of this paper is organized as follows: 
Section 2 introduces related work. The system is 
overviewed in Section 3. In Sections 4 and 5, we 
present the detailed algorithms and evaluation 
917
results of machine translation quality prediction 
and cross-language summarization, respectively. 
We discuss in Section 6 and conclude this paper 
in Section 7.  
2 Related Work 
2.1 Machine Translation Quality Prediction 
Machine translation evaluation aims to assess the 
correctness and quality of the translation. Usu-
ally, the human reference translation is provided, 
and various methods and metrics have been de-
veloped for comparing the system-translated text 
and the human reference text. For example, the 
BLEU metric, the NIST metric and their relatives 
are all based on the idea that the more shared 
substrings the system-translated text has with the 
human reference translation, the better the trans-
lation is. Blatz et al (2003) investigate training 
sentence-level confidence measures using a vari-
ety of fuzzy match scores. Albrecht and Hwa 
(2007) rely on regression algorithms and refer-
ence-based features to measure the quality of 
sentences.  
Transition evaluation without using reference 
translations has also been investigated. Quirk 
(2004) presents a supervised method for training 
a sentence level confidence measure on transla-
tion output using a human-annotated corpus. 
Features derived from the source sentence and 
the target sentence (e.g. sentence length, perplex-
ity, etc.) and features about the translation proc-
ess are leveraged. Gamon et al (2005) investi-
gate the possibility of evaluating MT quality and 
fluency at the sentence level in the absence of 
reference translations, and they can improve on 
the correlation between language model perplex-
ity scores and human judgment by combing these 
perplexity scores with class probabilities from a 
machine-learned classifier. Specia et al (2009) 
use the ICM theory to identify the threshold to 
map a continuous predicted score into ?good? or 
?bad? categories. Chae and Nenkova (2009) use 
surface syntactic features to assess the fluency of 
machine translation results.   
In this study, we further predict the translation 
quality of an English sentence before the ma-
chine translation process, i.e., we do not leverage 
reference translation and the target sentence.  
2.2 Document Summarization  
Document summarization methods can be gener-
ally categorized into extraction-based methods 
and abstraction-based methods. In this paper, we 
focus on extraction-based methods. Extraction-
based summarization methods usually assign 
each sentence a saliency score and then rank the 
sentences in a document or document set.  
  For single document summarization, the sen-
tence score is usually computed by empirical 
combination of a number of statistical and lin-
guistic feature values, such as term frequency, 
sentence position, cue words, stigma words, 
topic signature (Luhn 1969; Lin and Hovy, 2000). 
The summary sentences can also be selected by 
using machine learning methods (Kupiec et al, 
1995; Amini and Gallinari, 2002) or graph-based 
methods (ErKan and Radev, 2004; Mihalcea and 
Tarau, 2004). Other methods include mutual re-
inforcement principle (Zha 2002; Wan et al, 
2007). 
  For multi-document summarization, the cen-
troid-based method (Radev et al, 2004) is a typi-
cal method, and it scores sentences based on 
cluster centroids, position and TFIDF features. 
NeATS (Lin and Hovy, 2002) makes use of new 
features such as topic signature to select impor-
tant sentences. Machine Learning based ap-
proaches have also been proposed for combining 
various sentence features (Wong et al, 2008).  
The influences of input difficulty on summariza-
tion performance have been investigated in 
(Nenkova and Louis, 2008). Graph-based meth-
ods have also been used to rank sentences in a 
document set. For example, Mihalcea and Tarau 
(2005) extend the TextRank algorithm to com-
pute sentence importance in a document set. 
Cluster-level information has been incorporated 
in the graph model to better evaluate sentences 
(Wan and Yang, 2008). Topic-focused or query 
biased multi-document summarization has also 
been investigated (Wan et al, 2006). Wan et al 
(2010) propose the EUSUM system for extract-
ing easy-to-understand English summaries for 
non-native readers.  
Several pilot studies have been performed for 
the cross-language summarization task by simply 
using document translation or summary transla-
tion. Leuski et al (2003) use machine translation 
for English headline generation for Hindi docu-
ments. Lim et al (2004) propose to generate a 
Japanese summary without using a Japanese 
summarization system, by first translating Japa-
nese documents into Korean documents, and 
then extracting summary sentences by using Ko-
rean summarizer, and finally mapping Korean 
summary sentences to Japanese summary sen-
tences. Chalendar et al (2005) focuses on se-
mantic analysis and sentence generation tech-
niques for cross-language summarization. Orasan 
918
and Chiorean (2008) propose to produce summa-
ries with the MMR method from Romanian news 
articles and then automatically translate the 
summaries into English. Cross language query 
based summarization has been investigated in 
(Pingali et al, 2007), where the query and the 
documents are in different languages. Other re-
lated work includes multilingual summarization 
(Lin et al, 2005), which aims to create summa-
ries from multiple sources in multiple languages. 
Siddharthan and McKeown (2005) use the in-
formation redundancy in multilingual input to 
correct errors in machine translation and thus 
improve the quality of multilingual summaries.  
3 The Proposed Approach 
Previous methods for cross-language summariza-
tion usually consist of two steps: one step for 
summarization and one step for translation. Dif-
ferent order of the two steps can lead to the fol-
lowing two basic English-to-Chinese summariza-
tion methods:   
Late Translation (LateTrans): Firstly, an 
English summary is produced for the English 
document set by using existing summarization 
methods. Then, the English summary is auto-
matically translated into the corresponding Chi-
nese summary by using machine translation ser-
vices.  
Early Translation (EarlyTrans): Firstly, the 
English documents are translated into Chinese 
documents by using machine translation services. 
Then, a Chinese summary is produced for the 
translated Chinese documents.  
Generally speaking, the LateTrans method has 
a few advantages over the EarlyTrans method: 
1) The LateTrans method is much more effi-
cient than the EarlyTrans method, because only a 
very few summary sentences are required to be 
translated in the LateTrans method, whereas all 
the sentences in the documents are required to be 
translated in the EarlyTrans method.  
2)  The LateTrans method is deemed to be 
more effective than the EarlyTrans method, be-
cause the translation errors of the sentences have 
great influences on the summary sentence extrac-
tion in the EarlyTrans method. 
Thus in this study, we adopt the LateTrans 
method as our baseline method. We also adopt 
the late translation strategy for our proposed ap-
proach. 
In the baseline method, a translated Chinese 
sentence is selected into the summary because 
the original English sentence is informative. 
However, an informative and fluent English sen-
tence is likely to be translated into an uninforma-
tive and disfluent Chinese sentence, and there-
fore, this sentence cannot be selected into the 
summary.  
In order to address the above problem of exist-
ing methods, our proposed approach takes into 
account a novel factor of each sentence for cross-
language summary extraction. Each English sen-
tence is associated with a score indicating its 
translation quality. An English sentence with 
high translation quality score is more likely to be 
selected into the original English summary, and 
such English summary can be translated into a 
better Chinese summary.   Figure 1 gives the ar-
chitecture of our proposed approach.  
 
 
Figure 1: Architecture of our proposed ap-
proach 
Seen from the figure, our proposed approach 
consists of four main steps: 1) The machine 
translation quality score of each English sentence 
is predicted by using regression methods; 2) The 
informativeness score of each English sentence is 
computed by using existing methods; 3) The 
English summary is produced by making use of 
both the machine translation quality score and 
the informativeness score; 4) The extracted Eng-
lish summary is translated into Chinese summary 
by using machine translation services.  
In this study, we adopt Google Translate1 for 
English-to-Chinese translation. Google Translate 
is one of the state-of-the-art commercial machine 
translation systems used today. It applies statisti-
cal learning techniques to build a translation 
                                                 
1 http://translate.google.com/translate_t 
English 
Sentences 
Sentence    
MT Quality 
Prediction
Sentence      
Informativeness 
Evaluation 
English 
Summary 
Extraction 
EN-to-CN 
Machine 
Translation 
Chinese Summary 
Informativeness score 
English summary 
MT quality score
919
model based on both monolingual text in the tar-
get language and aligned text consisting of ex-
amples of human translations between the lan-
guages. 
The first step and the evaluation results will be 
described in Section 4, and the other steps and 
the evaluation results will be described together 
in Section 5.  
4 Machine Translation Quality Predic-
tion  
4.1 Methodology 
In this study, machine translation (MT) quality 
reflects both the translation accuracy and the flu-
ency of the translated sentence. An English sen-
tence with high MT quality score is likely to be 
translated into an accurate and fluent Chinese 
sentence, which can be easily read and under-
stand by Chinese readers.  The MT quality pre-
diction is a task of mapping an English sentence 
to a numerical value corresponding to a quality 
level. The larger the value is, the more accurately 
and fluently the sentence can be translated into 
Chinese sentence.  
As introduced in Section 2.1, several related 
work has used regression and classification 
methods for MT quality prediction without refer-
ence translations. In our approach, the MT qual-
ity of each sentence in the documents is also pre-
dicted without reference translations. The differ-
ence between our task and previous work is that 
previous work can make use of both features in 
source sentence and features in target sentence, 
while our task only leverages features in source 
sentence, because in the late translation strategy, 
the English sentences in the documents have not 
been translated yet at this step.  
In this study, we adopt the ?-support vector re-
gression (?-SVR) method (Vapnik 1995) for the 
sentence-level MT quality prediction task.  The 
SVR algorithm is firmly grounded in the frame-
work of statistical learning theory (VC theory). 
The goal of a regression algorithm is to fit a flat 
function to the given training data points. 
Formally, given a set of training data points 
D={(xi,yi)| i=1,2,?,n} ? Rd?R,  where xi is input 
feature vector and yi is associated score, the goal 
is to fit a function f which approximates the rela-
tion inherited between the data set points. The 
standard form is:  
??
==
++
n
i
i
n
i
i
T
bw
CCww
1
*
1,,, 2
1  min
*
??
??
 
Subject to 
iii
T ybxfw ?? +??+)(  
*)( ii
T
i bxfwy ?? +???  
.,...,1  ,0,, * niii =????  
  The constant C>0 is a parameter for determin-
ing the trade-off between the flatness of f and the 
amount up to which deviations larger than ? are 
tolerated.   
  In the experiments, we use the LIBSVM tool 
(Chang and Lin, 2001) with the RBF kernel for 
the task, and we use the parameter selection tool 
of 10-fold cross validation via grid search to find 
the best parameters on the training set with re-
spect to mean squared error (MSE), and then use 
the best parameters to train on the whole training 
set.   
  We use the following two groups of features 
for each sentence: the first group includes several 
basic features, and the second group includes 
several parse based features2. They are all de-
rived based on the source English sentence.  
  The basic features are as follows: 
1) Sentence length:  It refers to the number of 
words in the sentence.   
2) Sub-sentence number: It refers to the num-
ber of sub-sentences in the sentence. We 
simply use the punctuation marks as indica-
tors of sub-sentences. 
3) Average sub-sentence length: It refers to 
the average number of words in the sub-
sentences within the sentence.   
4) Percentage of nouns and adjectives: It re-
fers to the percentage of noun words or ad-
jective words in the in the sentence. 
5) Number of question words: It refers to the 
number of question words (who, whom, 
whose, when, where, which, how, why, what) 
in the sentence. 
  We use the Stanford Lexicalized Parser (Klein 
and Manning, 2002) with the provided English 
PCFG model to parse a sentence into a parse tree. 
The output tree is a context-free phrase structure 
grammar representation of the sentence. The 
parse features are then selected as follows: 
1) Depth of the parse tree:  It refers to the 
depth of the generated parse tree.  
2) Number of SBARs in the parse tree:  
SBAR is defined as a clause introduced by a 
(possibly empty) subordinating conjunction. 
It is an indictor of sentence complexity.  
                                                 
2  Other features, including n-gram frequency, perplexity 
features, etc., are not useful in our study. MT features are 
not used because Google Translate is used as a black box.  
920
3) Number of NPs in the parse tree:  It refers 
to the number of noun phrases in the parse 
tree.   
4) Number of VPs in the parse tree:  It refers 
to the number of verb phrases in the parse 
tree.    
  All the above feature values are scaled by us-
ing the provided svm-scale program.   
At this step, each English sentence si can be 
associated with a MT quality score TransScore(si) 
predicted by the ?-SVR method. The score is fi-
nally normalized by dividing by the maximum 
score. 
4.2 Evaluation  
4.2.1 Evaluation Setup 
In the experiments, we first constructed the gold-
standard dataset in the following way:  
DUC2001 provided 309 English news articles 
for document summarization tasks, and the arti-
cles were grouped into 30 document sets. The 
news articles were selected from TREC-9. We 
chose five document sets (d04, d05, d06, d08, 
d11) with 54 news articles out of the DUC2001 
document sets. The documents were then split 
into sentences and we used 1736 sentences for 
evaluation. All the sentences were automatically 
translated into Chinese sentences by using the 
Google Translate service. 
Two Chinese college students were employed 
for data annotation. They read the original Eng-
lish sentence and the translated Chinese sentence, 
and then manually labeled the overall translation 
quality score for each sentence, separately. The 
translation quality is an overall measure for both 
the translation accuracy and the readability of the 
translated sentence.  The score ranges between 1 
and 5, and 1 means ?very bad?, and 5 means 
?very good?, and 3 means ?normal?. The correla-
tion between the two sets of labeled scores is 
0.646. The final translation quality score was the 
average of the scores provided by the two anno-
tators.  
After annotation, we randomly separated the 
labeled sentence set into a training set of 1428 
sentences and a test set of 308 sentences. We 
then used the LIBSVM tool for training and test-
ing. 
Two metrics were used for evaluating the pre-
diction results. The two metrics are as follows: 
Mean Square Error (MSE): This metric is a 
measure of how correct each of the prediction 
values is on average, penalizing more severe er-
rors more heavily. Given the set of prediction 
scores for the test sentences: },...1|?{? niyY i == , and 
the manually assigned scores for the sentences: 
},...1|{ niyY i == , the MSE of the prediction result 
is defined as  
?
=
?=
n
i
ii yyn
YMSE
1
2)?(1)?(  
Pearson?s Correlation Coefficient (?):  This 
metric is a measure of whether the trends of pre-
diction values matched the trends for human-
labeled data. The coefficient between Y and Y?  is 
defined as  
yy
n
i
ii
sns
yyyy
?
1
)??)((?
=
??
=?  
where y and y?  are the sample means of Y and 
Y? , ys and ys ? are the sample standard deviations 
of Y and Y? . 
4.2.2 Evaluation Results 
Table 1 shows the prediction results. We can see 
that the overall results are promising. And the 
correlation is moderately high. The results are 
acceptable because we only make use of the fea-
tures derived from the source sentence. The re-
sults guarantee that the use of MT quality scores 
in the summarization process is feasible.  
We can also see that both the basic features 
and the parse features are beneficial to the over-
all prediction results.   
  
Feature Set MSE ? 
Basic features 0.709 0.399 
Parse features 0.702 0.395 
All features 0.683 0.433 
Table 1: Prediction results 
5 Cross-Language Document Summari-
zation  
5.1 Methodology 
In this section, we first compute the informative-
ness score for each sentence. The score reflect 
how the sentence expresses the major topic in the 
documents. Various existing methods can be 
used for computing the score. In this study, we 
adopt the centroid-based method. 
The centroid-based method is the algorithm 
used in the MEAD system. The method uses a 
heuristic and simple way to sum the sentence 
scores computed based on different features. The 
score for each sentence is a linear combination of 
921
the weights computed based on the following 
three features: 
  Centroid-based Weight. The sentences close 
to the centroid of the document set are usually 
more important than the sentences farther away. 
And the centroid weight C(si) of a sentence si is 
calculated as the cosine similarity  between the 
sentence text and the concatenated text for the 
whole document set D. The weight is then nor-
malized by dividing the maximal weight. 
  Sentence Position. The leading several sen-
tences of a document are usually important. So 
we calculate for each sentence a weight to reflect 
its position priority as P(si)=1-(i-1)/n, where i is 
the sequence of the sentence si and n is the total 
number of sentences in the document. Obviously, 
i ranges from 1 to n.  
  First Sentence Similarity. Because the first 
sentence of a document is very important, a sen-
tence similar to the first sentence is also impor-
tant. Thus we use the cosine similarity value be-
tween a sentence and the corresponding first sen-
tence in the same document as the weight F(si) 
for sentence si. 
  After all the above weights are calculated for 
each sentence, we sum all the weights and get the 
overall score for the sentence as follows: 
)()()()( iiii sFsPsCsInfoScore ?+?+?= ???  
where ?, ? and ? are parameters reflecting the 
importance of different features. We empirically 
set ?=?=?=1.  
  After the informativeness scores for all sen-
tences are computed, the score of each sentence 
is normalized by dividing by the maximum score.  
After we obtain the MT quality score and the 
informativeness score of each sentence in the 
document set, we linearly combine the two 
scores to get the overall score of each sentence.  
Formally, let TransScore(si)?[0,1] and Info-
Score(si)?[0,1] denote the MT quality score and 
the informativeness score of sentence si, the 
overall score of the sentence is: 
where ??[0,1] is a parameter controlling the 
influences of the two factors. If ? is set to 0, the 
summary is extracted without considering the 
MT quality factor. In the experiments, we em-
pirically set the parameter to 0.3 in order to bal-
ance the two factors of content informativeness 
and translation quality.   
For multi-document summarization, some sen-
tences are highly overlapping with each other, 
and thus we apply the same greedy algorithm in 
(Wan et al, 2006) to penalize the sentences 
highly overlapping with other highly scored sen-
tences, and finally the informative, novel, and 
easy-to-translate sentences are chosen into the 
English summary. 
  Finally, the sentences in the English summary 
are translated into the corresponding Chinese 
sentences by using Google Translate, and the 
Chinese summary is formed.   
5.2 Evaluation 
5.2.1 Evaluation Setup 
In this experiment, we used the document sets 
provided by DUC2001 for evaluation. As men-
tioned in Section 4.2.1, DUC2001 provided 30 
English document sets for generic multi-
document summarization. The average document 
number per document set was 10. The sentences 
in each article have been separated and the sen-
tence information has been stored into files. Ge-
neric reference English summaries were pro-
vided by NIST annotators for evaluation. In our 
study, we aimed to produce Chinese summaries 
for the English document sets. The summary 
length was limited to five sentences, i.e. each 
summary consisted of five sentences. 
The DUC2001 dataset was divided into the 
following two datasets:  
Ideal Dataset: We have manually labeled the 
MT quality scores for the sentences in five 
document sets (d04-d11), and we directly used 
the manually labeled scores in the summarization 
process. The ideal dataset contained these five 
document sets. 
Real Dataset: The MT quality scores for the 
sentences in the remaining 25 document sets 
were automatically predicted by using the 
learned SVM regression model. And we used the 
automatically predicted scores in the summariza-
tion process. The real dataset contained these 25 
document sets. 
  We performed two evaluation procedures: one 
based on the ideal dataset to validate the 
feasibility of the proposed approach, and 
the other based on the real dataset to 
demonstrate the effectiveness of the proposed 
approach in real applications.  
To date, various methods and metrics have 
been developed for English summary evaluation 
by comparing system summary with reference 
summary, such as the pyramid method (Nenkova 
et al, 2007) and the ROUGE metrics (Lin and 
Hovy, 2003). However, such methods or metrics 
cannot be directly used for evaluating Chinese 
summary without reference Chinese summary.  
)()()1()( iii sTransScoresInfoScoresreOverallSco ?+??= ??
922
Instead, we developed an evaluation protocol as 
follows: 
The evaluation was based on human scoring. 
Four Chinese college students participated in the 
evaluation as subjects. We have developed a 
friendly tool for helping the subjects to evaluate 
each Chinese summary from the following three 
aspects: 
Content: This aspect indicates how much a 
summary reflects the major content of the docu-
ment set. After reading a summary, each user can 
select a score between 1 and 5 for the summary. 
1 means ?very uninformative? and 5 means 
?very informative?. 
Readability:  This aspect indicates the read-
ability level of the whole summary. After reading 
a summary, each user can select a score between 
1 and 5 for the summary. 1 means ?hard to read?, 
and 5 means ?easy to read?. 
Overall:  This aspect indicates the overall 
quality of a summary. After reading a summary, 
each user can select a score between 1 and 5 for 
the summary. 1 means ?very bad?, and 5 means 
?very good?. 
We performed the evaluation procedures on 
the ideal dataset and the read dataset, separately. 
During each evaluation procedure, we compared 
our proposed approach (?=0.3) with the baseline 
approach without considering the MT quality 
factor (?=0). And the two summaries produced 
by the two systems for the same document set 
were presented in the same interface, and then 
the four subjects assigned scores to each sum-
mary after they read and compared the two 
summaries.  And the assigned scores were finally 
averaged across the documents sets and across 
the subjects.  
5.2.2 Evaluation Results 
Table 2 shows the evaluation results on the ideal 
dataset with 5 document sets. We can see that 
based on the manually labeled MT quality scores, 
the Chinese summaries produced by our pro-
posed approach are significantly better than that 
produced by the baseline approach over all three 
aspects. All subjects agree that our proposed ap-
proach can produce more informative and easy-
to-read Chinese summaries than the baseline ap-
proach.   
Table 3 shows the evaluation results on the 
real dataset with 25 document sets. We can see 
that based on the automatically predicted MT 
quality scores, the Chinese summaries produced 
by our proposed approach are significantly better 
than that produced by the baseline approach over 
the readability aspect and the overall aspect. Al-
most all subjects agree that our proposed ap-
proach can produce more easy-to-read and high-
quality Chinese summaries than the baseline ap-
proach.   
Comparing the evaluation results in the two 
tables, we can find that the performance differ-
ence between the two approaches on the ideal 
dataset is bigger than that on the real dataset, es-
pecially on the content aspect. The results dem-
onstrate that the more accurate the MT quality 
scores are, the more significant the performance 
improvement is.  
   Overall, the proposed approach is effective to 
produce good-quality Chinese summaries for 
English document sets. 
 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 3.2 2.6 2.8 3.4 3.0 3.4 
Subject2 3.0 3.2 3.2 3.4 3.6 3.4 
Subject3 3.4 2.8 3.2 3.6 3.8 3.8 
Subject4 3.2 3.0 3.2 3.8 3.8 3.8 
Average 3.2 2.9 3.1 3.55* 3.55* 3.6* 
Table 2: Evaluation results on the ideal dataset (5 document sets) 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 2.64 2.56 2.60 2.80 3.24 2.96 
Subject2 3.60 2.76 3.36 3.52 3.28 3.64 
Subject3 3.52 3.72 3.44 3.56 3.80 3.48 
Subject4 3.16 2.96 3.12 3.16 3.44 3.52 
Average 3.23 3.00 3.13 3.26 3.44* 3.40* 
Table 3: Evaluation results on the real dataset (25 document sets) 
(* indicates the difference between the average score of the proposed approach and that of the baseline approach 
is statistically significant by using t-test.) 
923
 5.2.3 Example Analysis 
In this section, we give two running examples to 
better show the effectiveness of our proposed 
approach. The Chinese sentences and the original 
English sentences in the summary are presented 
together. The normalized MT quality score for 
each sentence is also given at the end of the Chi-
nese sentence.  
 
Document set 1: D04 from the ideal dataset 
Summary by baseline approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced. ) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ?????????????????????????
???????????????4000???&#39;? (0.44) 
(GENERAL ACCIDENT said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as much as Dollars 
40m'.) 
s4: ???????????????4??????????
??? (0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s5: ??????1.6??????????????????
???????????????????????????
???(0.44) 
(New Orleans, with a population of 1.6m, is particularly vulnerable 
because the city lies below sea level, has the Mississippi River 
running through its centre and a large lake immediately to the north.) 
 
Summary by proposed approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced.) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ???????????????4??????????
???(0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s4: ?????????????????????????
??????? (0.89) 
(The brunt of the losses are likely to be concentrated among US 
insurers, industry analysts said yesterday.) 
s5: ?????????????(1.0) 
(In north Miami, damage is minimal.) 
 
Document set 2: D54 from the real dataset 
Summary by baseline approach: 
s1: ????11?6???????????????????
???????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
??????????(0.20) 
(Proposals to limit the terms of members of Congress and of state 
legislators are popular and getting more so, according to the pundits 
and the polls.) 
s4: ?????????????????????????
?????????????????(0.24) 
(State statutes that bar first-time candidates from running for Con-
gress have been held to add to the qualifications set forth in the 
Constitution and have been invalidated.) 
s5: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
Summary by proposed approach: 
s1: ???? 11? 6??????????????????
????????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
s4: ????????????????????????
?????????????(0.39) 
(There are two solid reasons for congressional term limitation that 
economists, at least those of the public-choice persuasion, should 
fully appreciate.) 
s5: ?????????????????????????
?????(0.47) 
(The root of the problems with Congress is that, barring major 
scandal, it is almost impossible to defeat an incumbent.) 
6 Discussion  
In this study, we adopt the late translation strat-
egy for cross-document summarization. As men-
tioned earlier, the late translation strategy has 
some advantages over the early translation strat-
egy. However, in the early translation strategy, 
we can use the features derived from both the 
source English sentence and the target Chinese 
sentence to improve the MT quality prediction 
results.  
Overall, the framework of our proposed ap-
proach can be easily adapted for cross-document 
summarization with the early translation strategy. 
924
And an empirical comparison between the two 
strategies is left as our future work. 
Though this study focuses on English-to-
Chinese document summarization, cross-
language summarization tasks for other lan-
guages can also be solved by using our proposed 
approach.  
7 Conclusion and Future Work  
In this study we propose a novel approach to ad-
dress the cross-language document summariza-
tion task. Our proposed approach predicts the 
MT quality score of each English sentence and 
then incorporates the score into the summariza-
tion process. The user study results verify the 
effectiveness of the approach. 
In future work, we will manually translate 
English reference summaries into Chinese refer-
ence summaries, and then adopt the ROUGE 
metrics to perform automatic evaluation of the 
extracted Chinese summaries by comparing them 
with the Chinese reference summaries. Moreover, 
we will further improve the sentence?s MT qual-
ity by using sentence compression or sentence 
reduction techniques.  
Acknowledgments 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03), NCET 
(NCET-08-0006), RFDP (20070001059) and 
National High-tech R&D Program 
(2008AA01Z421). We thank the students for 
participating in the user study. We also thank the 
anonymous reviewers for their useful comments. 
References  
J. Albrecht and R. Hwa. 2007. A re-examination of 
machine learning approaches for sentence-level mt 
evaluation. In Proceedings of ACL2007. 
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
beled Data to Improve Supervised Learning for 
Text Summarization. In Proceedings of SIGIR2002. 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 
2003. Confidence estimation for statistical machine 
translation. Johns Hopkins Summer Workshop Fi-
nal Report. 
J. Chae and A. Nenkova. 2009. Predicting the fluency 
of text with shallow structural features: case studies 
of machine translation and human-written text. In 
Proceedings of EACL2009. 
G. de Chalendar, R. Besan?on, O. Ferret, G. Grefen-
stette, and O. Mesnard. 2005. Crosslingual summa-
rization with thematic extraction, syntactic sen-
tence simplification, and bilingual generation. In 
Workshop on Crossing Barriers in Text Summari-
zation Research, 5th International Conference on 
Recent Advances in Natural Language Processing  
(RANLP2005). 
C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige 
in Multi-Document Text Summarization. In Pro-
ceedings of EMNLP2004. 
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
level MT evaluation without reference translations: 
beyond language modeling. In Proceedings of 
EAMT2005. 
D. Klein and C. D. Manning. 2002. Fast Exact Infer-
ence with a Factored Model for Natural Language 
Parsing. In Proceedings of NIPS2002. 
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable 
Document Summarizer. In Proceedings of 
SIGIR1995. 
A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och, 
E. Hovy. 2003. Cross-lingual C*ST*RD: English 
access to Hindi information. ACM Transactions on 
Asian Language Information Processing, 2(3): 
245-269. 
J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
document summarization using cross-language 
texts. In Proceedings of NTCIR-4.  
C. Y. Lin, E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In 
Proceedings of the 17th Conference on Computa-
tional Linguistics. 
C..-Y. Lin and E.. H. Hovy. 2002. From Single to 
Multi-document Summarization: A Prototype Sys-
tem and its Evaluation. In Proceedings of ACL-02. 
C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence 
Statistics. In Proceedings of HLT-NAACL -03. 
C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual 
summarization evaluation 2005: automatic evalua-
tion report. In Proceedings of MSE (ACL-2005 
Workshop). 
H. P. Luhn. 1969. The Automatic Creation of litera-
ture Abstracts. IBM Journal of Research and De-
velopment, 2(2). 
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of EMNLP2004. 
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of IJCNLP-05. 
A. Nenkova and A. Louis. 2008. Can you summarize 
this? Identifying correlates of input difficulty for 
generic multi-document summarization. In Pro-
ceedings of ACL-08:HLT. 
A. Nenkova, R. Passonneau, and K. McKeown. 2007. 
The Pyramid method: incorporating human content 
selection variation in summarization evaluation. 
925
ACM Transactions on Speech and Language Proc-
essing (TSLP), 4(2). 
C. Orasan, and O. A. Chiorean. 2008. Evaluation of a 
Crosslingual Romanian-English Multi-document 
Summariser. In Proceedings of 6th Language Re-
sources and Evaluation Conference (LREC2008). 
P. Pingali, J. Jagarlamudi and V. Varma. 2007. Ex-
periments in cross language query focused multi-
document summarization. In Workshop on Cross 
Lingual Information Access Addressing the Infor-
mation Need of Multilingual Societies in 
IJCAI2007. 
C. Quirk. 2004. Training a sentence-level machine 
translation confidence measure. In Proceedings of 
LREC2004. 
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. 
Centroid-based summarization of multiple docu-
ments. Information Processing and Management, 
40: 919-938. 
A. Siddharthan and K. McKeown. 2005. Improving 
multilingual summarization: using redundancy in 
the input to correct MT errors. In Proceedings of 
HLT/EMNLP-2005. 
L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C. 
Saunders. 2009. Improving the Confidence of Ma-
chine Translation Quality Estimates. In MT Summit 
2009 (Machine Translation Summit XII). 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer. 
X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting 
easy-to-understand English summaries for non-
native readers. In Proceedings of  SIGIR2010. 
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
document random walks for topic-focused multi-
documetn summarization. In Proceedings of 
WI2006. 
X. Wan and J. Yang. 2008. Multi-document summari-
zation using cluster-based link analysis. In Pro-
ceedings of SIGIR-08. 
X. Wan, J. Yang and J. Xiao. 2007. Towards an Itera-
tive Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction. 
In Proceedings of ACL2007.  
K.-F. Wong, M. Wu and W. Li. 2008. Extractive sum-
marization using supervised and semi-supervised 
learning. In Proceedings of COLING-08. 
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of SIGIR2002. 
 
926
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1546?1555,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Bilingual Information for Cross-Language Document              
Summarization 
 
 
Xiaojun Wan 
Institute of Compute Science and Technology, Peking University, Beijing 100871, China 
Key Laboratory of Computational Linguistics (Peking University), MOE, China 
wanxiaojun@icst.pku.edu.cn 
 
 
 
 
Abstract 
Cross-language document summarization is de-
fined as the task of producing a summary in a 
target language (e.g. Chinese) for a set of 
documents in a source language (e.g. English). 
Existing methods for addressing this task make 
use of either the information from the original 
documents in the source language or the infor-
mation from the translated documents in the 
target language. In this study, we propose to use 
the bilingual information from both the source 
and translated documents for this task. Two 
summarization methods (SimFusion and 
CoRank) are proposed to leverage the bilingual 
information in the graph-based ranking frame-
work for cross-language summary extraction. 
Experimental results on the DUC2001 dataset 
with manually translated reference Chinese 
summaries show the effectiveness of the pro-
posed methods.  
 
1 Introduction 
Cross-language document summarization is de-
fined as the task of producing a summary in a dif-
ferent target language for a set of documents in a 
source language (Wan et al, 2010). In this study, 
we focus on English-to-Chinese cross-language 
summarization, which aims to produce Chinese 
summaries for English document sets. The task is 
very useful in the field of multilingual information 
access. For example, it is beneficial for most Chi-
nese readers to quickly browse and understand 
English news documents or document sets by read-
ing the corresponding Chinese summaries.  
A few pilot studies have investigated the task in 
recent years and exiting methods make use of ei-
ther the information in the source language or the 
information in the target language after using ma-
chine translation. In particular, for the task of Eng-
lish-to-Chinese cross-language summarization, one 
method is to directly extract English summary sen-
tences based on English features extracted from the 
English documents, and then automatically trans-
late the English summary sentences into Chinese 
summary sentences. The other method is to auto-
matically translate the English sentences into Chi-
nese sentences, and then directly extract Chinese 
summary sentences based on Chinese features. The 
two methods make use of the information from 
only one language side.   
However, it is not very reliable to use only the 
information in one language, because the machine 
translation quality is far from satisfactory, and thus 
the translated Chinese sentences usually contain 
some errors and noises. For example, the English 
sentence ?Many destroyed power lines are thought 
to be uninsured, as are trees and shrubs uprooted 
across a wide area.? is automatically translated 
into the Chinese sentence ??????????
????????????????????
???????? by using Google Translate1 , 
but the Chinese sentence contains a few translation 
errors. Therefore, on the one side, if we rely only 
on the English-side information to extract Chinese 
                                                          
1 http://translate.google.com/.  Note that the translation service 
is updated frequently and the current translation results may be 
different from that presented in this paper.  
1546
summary sentences, we cannot guarantee that the 
automatically translated Chinese sentences for sa-
lient English sentences are really salient when 
these sentences may contain many translation er-
rors and other noises. On the other side, if we rely 
only on the Chinese-side information to extract 
Chinese summary sentences, we cannot guarantee 
that the selected sentences are really salient be-
cause the features for sentence ranking based on 
the incorrectly translated sentences are not very 
reliable, either.  
In this study, we propose to leverage both the in-
formation in the source language and the informa-
tion in the target language for cross-language 
document summarization. In particular, we pro-
pose two graph-based summarization methods 
(SimFusion and CoRank) for using both English-
side and Chinese-side information in the task of 
English-to-Chinese cross-document summarization. 
The SimFusion method linearly fuses the English-
side similarity and the Chinese-side similarity for 
measuring Chinese sentence similarity. The 
CoRank method adopts a co-ranking algorithm to 
simultaneously rank both English sentences and 
Chinese sentences by incorporating mutual influ-
ences between them.  
We use the DUC2001 dataset with manually 
translated reference Chinese summaries for evalua-
tion. Experimental results based on the ROUGE 
metrics show the effectiveness of the proposed 
methods. Three important conclusions for this task 
are summarized below:  
1) The Chinese-side information is more benefi-
cial than the English-side information.  
2) The Chinese-side information and the Eng-
lish-side information can complement each 
other.  
3) The proposed CoRank method is more reli-
able and robust than the proposed SimFusion 
method. 
The rest of this paper is organized as follows: 
Section 2 introduces related work. In Section 3, we 
present our proposed methods. Evaluation results 
are shown in Section 4. Lastly, we conclude this 
paper in Section 5.  
2 Related Work 
2.1 General Document Summarization  
Document summarization methods can be extrac-
tion-based, abstraction-based or hybrid methods. 
We focus on extraction-based methods in this 
study, and the methods directly extract summary 
sentences from a document or document set by 
ranking the sentences in the document or document 
set.  
In the task of single document summarization, 
various features have been investigated for ranking 
sentences in a document, including term frequency, 
sentence position, cue words, stigma words, and 
topic signature (Luhn 1969; Lin and Hovy, 2000). 
Machine learning techniques have been used for 
sentence ranking (Kupiec et al, 1995; Amini and 
Gallinari, 2002). Litvak et al (2010) present a lan-
guage-independent approach for extractive summa-
rization based on the linear optimization of several 
sentence ranking measures using a genetic algo-
rithm. In recent years, graph-based methods have 
been proposed for sentence ranking (Erkan and 
Radev, 2004; Mihalcea and Tarau, 2004). Other 
methods include mutual reinforcement principle 
(Zha 2002; Wan et al, 2007).  
In the task of multi-document summarization, 
the centroid-based method (Radev et al, 2004) 
ranks the sentences in a document set based on 
such features as cluster centroids, position and 
TFIDF. Machine Learning techniques have also 
been used for feature combining (Wong et al, 
2008).  Nenkova and Louis (2008) investigate the 
influences of input difficulty on summarization 
performance. Pitler et al (2010) present a system-
atic assessment of several diverse classes of met-
rics designed for automatic evaluation of linguistic 
quality of multi-document summaries. Celikyilmaz 
and Hakkani-Tur (2010) formulate extractive 
summarization as a two-step learning problem by 
building a generative model for pattern discovery 
and a regression model for inference. Aker et al 
(2010) propose an A* search algorithm to find the 
best extractive summary up to a given length, and 
they propose a discriminative training algorithm 
for directly maximizing the quality of the best 
summary. Graph-based methods have also been 
used to rank sentences for multi-document summa-
rization (Mihalcea and Tarau, 2005; Wan and 
Yang, 2008).  
1547
2.2 Cross-Lingual Document Summariza-
tion  
Several pilot studies have investigated the task of 
cross-language document summarization.  The ex-
isting methods use only the information in either 
language side. Two typical translation schemes are 
document translation or summary translation. The 
document translation scheme first translates the 
source documents into the corresponding docu-
ments in the target language, and then extracts 
summary sentences based only on the information 
on the target side. The summary translation scheme 
first extracts summary sentences from the source 
documents based only on the information on the 
source side, and then translates the summary sen-
tences into the corresponding summary sentences 
in the target language.  
For example Leuski et al (2003) use machine 
translation for English headline generation for 
Hindi documents. Lim et al (2004) propose to 
generate a Japanese summary by using Korean 
summarizer. Chalendar et al (2005) focus on se-
mantic analysis and sentence generation techniques 
for cross-language summarization. Orasan and 
Chiorean (2008) propose to produce summaries 
with the MMR method from Romanian news arti-
cles and then automatically translate the summaries 
into English. Cross language query based summa-
rization has been investigated in (Pingali et al, 
2007), where the query and the documents are in 
different languages. Wan et al (2010) adopt the 
summary translation scheme for the task of Eng-
lish-to-Chinese cross-language summarization. 
They first extract English summary sentences by 
using English-side features and the machine trans-
lation quality factor, and then automatically trans-
late the English summary into Chinese summary. 
Other related work includes multilingual summari-
zation (Lin et al, 2005; Siddharthan and McKe-
own, 2005), which aims to create summaries from 
multiple sources in multiple languages. 
3 Our Proposed Methods 
As mentioned in Section 1, existing methods rely 
only on one-side information for sentence ranking, 
which is not very reliable. In order to leveraging 
both-side information for sentence ranking, we 
propose the following two methods to incorporate 
the bilingual information in different ways.   
3.1 SimFusion 
This method uses the English-side information for 
Chinese sentence ranking in the graph-based 
framework. The sentence similarities in the two 
languages are fused in the method. In other words, 
when we compute the similarity value between two 
Chinese sentences, the similarity value between the 
corresponding two English sentences is used by 
linear fusion. Since sentence similarity evaluation 
plays a very important role in the graph-based 
ranking algorithm, this method can leverage both-
side information through similarity fusion.   
Formally, given the Chinese document set Dcn 
translated from an English document set, let 
Gcn=(Vcn, Ecn) be an undirected graph to reflect the 
relationships between the sentences in the Chinese 
document set. Vcn is the set of vertices and each 
vertex scni in Vcn represents a Chinese sentence. Ecn 
is the set of edges. Each edge ecnij in Ecn is associ-
ated with an affinity weight f(scni, scnj) between sen-
tences scni and scnj (i?j). The weight is computed by 
linearly combining the similarity value simcosine(scni, 
scnj) between the Chinese sentences and the simi-
larity value simcosine(seni, senj) between the corre-
sponding English sentences. 
 
),()1(),(),( coscos
en
j
en
iine
cn
j
cn
iine
cn
j
cn
i sssimsssimssf ??+?= ??
 
where senj and seni are the source English sentences 
for scnj and scni. ??[0, 1] is a parameter to control 
the relative contributions of the two similarity val-
ues. The similarity values simcosine(scni, scnj) and 
simcosine(seni, senj) are computed by using the stan-
dard cosine measure. The weight for each term is 
computed based on the TFIDF formula. For Chi-
nese similarity computation, Chinese word seg-
mentation is performed. Here, we have f(scni, 
scnj)=f(scnj, scni) and let f(scni, scni)=0 to avoid self 
transition. We use an affinity matrix Mcn to de-
scribe Gcn with each entry corresponding to the 
weight of an edge in the graph. Mcn=(Mcnij)|Vcn|?|Vcn| 
is defined as Mcnij=f(scni,scnj). Then Mcn is normal-
ized to cnM~  to make the sum of each row equal to 1. 
Based on matrix cnM~ , the saliency score Info-
Score(scni) for sentence scni can be deduced from 
those of all other sentences linked with it and it can 
be formulated in a recursive form as in the PageR-
ank algorithm: 
1548
?
?
?
+??=
iall j
cn
ji
cn
j
cn
i n
MsInfoScoresInfoScore
)1(~
)()(
??  
where n is the sentence number, i.e. n= |Vcn|. ? is 
the damping factor usually set to 0.85, as in the 
PageRank algorithm.  
For numerical computation of the saliency 
scores, we can iteratively run the above equation 
until convergence. 
For multi-document summarization, some sen-
tences are highly overlapping with each other, and 
thus we apply the same greedy algorithm in Wan et 
al. (2006) to penalize the sentences highly overlap-
ping with other highly scored sentences, and fi-
nally the salient and novel Chinese sentences are 
directly selected as summary sentences.  
3.2 CoRank 
This method leverages both the English-side in-
formation and the Chinese-side information in a 
co-ranking way. The source English sentences and 
the translated Chinese sentences are simultane-
ously ranked in a unified graph-based algorithm. 
The saliency of each English sentence relies not 
only on the English sentences linked with it, but 
also on the Chinese sentences linked with it. Simi-
larly, the saliency of each Chinese sentence relies 
not only on the Chinese sentences linked with it, 
but also on the English sentences linked with it. 
More specifically, the proposed method is based on 
the following assumptions: 
Assumption 1: A Chinese sentence would be 
salient if it is heavily linked with other salient Chi-
nese sentences; and an English sentence would be 
salient if it is heavily linked with other salient Eng-
lish sentences. 
Assumption 2: A Chinese sentence would be 
salient if it is heavily linked with salient English 
sentences; and an English sentence would be sali-
ent if it is heavily linked with salient Chinese sen-
tences. 
The first assumption is similar to PageRank 
which makes use of mutual ?recommendations? 
between the sentences in the same language to rank 
sentences. The second assumption is similar to 
HITS if the English sentences and the Chinese sen-
tences are considered as authorities and hubs, re-
spectively. In other words, the proposed method 
aims to fuse the ideas of PageRank and HITS in a 
unified framework. The mutual influences between 
the Chinese sentences and the English sentences 
are incorporated in the method. 
   Figure 1 gives the graph representation for the 
method. Three kinds of relationships are exploited: 
the CN-CN relationships between Chinese sen-
tences, the EN-EN relationships between English 
sentences, and the EN-CN relationships between 
English sentences and Chinese sentences.  
Formally, given an English document set Den and 
the translated Chinese document set Dcn, let G=(Ven, 
Vcn, Een, Ecn, Eencn) be an undirected graph to reflect 
all the three kinds of relationships between the sen-
tences in the two document sets. Ven ={seni | 1?i?n} 
is the set of English sentences. Vcn={scni | 1?i?n} is 
the set of Chinese sentences. scni is the correspond-
ing Chinese sentence translated from seni.  n is the 
number of the sentences.  Een is the edge set to re-
flect the relationships between the English sen-
tences. Ecn is the edge set to reflect the 
relationships between the Chinese sentences. Eencn 
is the edge set to reflect the relationships between 
the English sentences and the Chinese sentences. 
Based on the graph representation, we compute the 
following three affinity matrices to reflect the three 
kinds of sentence relationships: 
 
 
Figure 1. The three kinds of sentence relationships 
 
1) Mcn=(Mcnij)n?n:  This affinity matrix aims to 
reflect the relationships between the Chinese sen-
tences. Each entry in the matrix corresponds to the 
cosine similarity between the two Chinese sen-
tences.  
??
??? ?=
 otherwise,         
j,  if isssim
M
cn
j
cn
iinecn
ij
0
),(cos  
English Sentences 
CN-CN
EN-EN
EN-CN
Chinese sentences 
1549
 Then Mcn is normalized to cnM~  to make the 
sum of each row equal to 1. 
2) Men=(Meni,j)n?n: This affinity matrix aims to 
reflect the relationships between the English sen-
tences. Each entry in the matrix corresponds to the 
cosine similarity between the two English sen-
tences.  
??
??? ?=
 otherwise,         
j,  if isssim
M
en
j
en
iineen
ij
0
),(cos  
Then Men is normalized to enM~  to make the 
sum of each row equal to 1. 
3) Mencn=(Mencnij)n?n: This affinity matrix aims to 
reflect the relationships between the English sen-
tences and the Chinese sentences. Each entry   
Mencnij in the matrix corresponds to the similarity 
between the English sentence seni and the Chinese 
sentence scnj. It is hard to directly compute the 
similarity between the sentences in different lan-
guages. In this study, the similarity value is com-
puted by fusing the following two similarity values: 
the cosine similarity between the sentence seni and 
the corresponding source English sentence senj for 
scnj, and the cosine similarity between the corre-
sponding translated Chinese sentence scni for seni 
and the sentence scnj. We use the geometric mean 
of the two values as the affinity weight.  
 
),(),( coscos
cn
j
cn
iine
en
j
en
iine
encn
ij sssimsssimM ?=
 
Note that we have Mencnij=Mencnji and 
Mencn=(Mencn)T. Then Mencn is normalized to encnM~  
to make the sum of each row equal to 1.  
We use two column vectors u=[u(scni)]n?1 and v 
=[v(senj)]n?1 to denote the saliency scores of the 
Chinese sentences and the English sentences, re-
spectively. Based on the three kinds of relation-
ships, we can get the following four assumptions: 
?? j cnjcnjicni suMsu )(~)(  
?? i enienijenj svMsv )(~)(  
?? j enjencnjicni svMsu )(~)(  
?? i cniencnijenj suMsv )(~)(  
After fusing the above equations, we can obtain 
the following iterative forms: 
?? += j enjencnjij cnjcnjicni svM?suM?su )(~)(~)(  
?? += i cniencniji enienijenj suM?svM?sv )(~)(~)(  
And the matrix form is: 
vMuMu cn TencnT ?? )~()~( +=  
uMvMv en TencnT ?? )~()~( +=  
where ? and ? specify the relative contributions to 
the final saliency scores from the information in 
the same language and the information in the other 
language and we have ?+?=1.  
For numerical computation of the saliency 
scores, we can iteratively run the two equations 
until convergence. Usually the convergence of the 
iteration algorithm is achieved when the difference 
between the scores computed at two successive 
iterations for any sentences and words falls below 
a given threshold. In order to guarantee the con-
vergence of the iterative form, u and v are normal-
ized after each iteration. 
After we get the saliency scores u for the Chi-
nese sentences, we apply the same greedy algo-
rithm for redundancy removing. Finally, a few 
highly ranked sentences are selected as summary 
sentences.  
4 Experimental Evaluation 
4.1 Evaluation Setup 
There is no benchmark dataset for English-to-
Chinese cross-language document summarization, 
so we built our evaluation dataset based on the 
DUC2001 dataset by manually translating the ref-
erence summaries. 
DUC2001 provided 30 English document sets 
for generic multi-document summarization. The 
average document number per document set was 
10. The sentences in each article have been sepa-
rated and the sentence information has been stored 
into files. Three or two generic reference English 
summaries were provided by NIST annotators for 
each document set. Three graduate students were 
employed to manually translate the reference Eng-
lish summaries into reference Chinese summaries. 
Each student manually translated one third of the 
reference summaries. It was much easier and more 
reliable to provide the reference Chinese summa-
ries by manual translation than by manual summa-
rization. 
1550
 ROUGE-2
Average_F
ROUGE-W 
Average_F
ROUGE-L 
Average_F
ROUGE-SU4 
Average_F 
Baseline(EN) 0.03723 0.05566 0.13259 0.07177 
Baseline(CN) 0.03805 0.05886 0.13871 0.07474 
SimFusion  0.04017 0.06117 0.14362 0.07645 
CoRank  0.04282 0.06158 0.14521 0.07805 
Table 1: Comparison Results 
 
 All the English sentences in the document set 
were automatically translated into Chinese sen-
tences by using Google Translate, and the Stanford 
Chinese Word Segmenter2 was used for segment-
ing the Chinese documents and summaries into 
words. For comparative study, the summary length 
was limited to five sentences, i.e. each Chinese 
summary consisted of five sentences. 
We used the ROUGE-1.5.5 (Lin and Hovy, 
2003) toolkit for evaluation, which has been 
widely adopted by DUC and TAC for automatic 
summarization evaluation. It measured summary 
quality by counting overlapping units such as the 
n-gram, word sequences and word pairs between 
the candidate summary and the reference summary. 
We showed three of the ROUGE F-measure scores 
in the experimental results: ROUGE-2 (bigram-
based), ROUGE-W (based on weighted longest 
common subsequence, weight=1.2), ROUGE-L 
(based on longest common subsequences), and 
ROUGE-SU4 (based on skip bigram with a maxi-
mum skip distance of 4). Note that the ROUGE 
toolkit was performed for Chinese summaries after 
using word segmentation.   
Two graph-based baselines were used for com-
parison.   
Baseline(EN): This baseline adopts the sum-
mary translation scheme, and it relies on the Eng-
lish-side information for English sentence ranking. 
The extracted English summary is finally auto-
matically translated into the corresponding Chinese 
summary. The same sentence ranking algorithm 
with the SimFusion method is adopted, and the 
affinity weight is computed based only on the co-
sine similarity between English sentences.   
Baseline(CN): This baseline adopts the docu-
ment translation scheme, and it relies on the Chi-
nese-side information for Chinese sentence ranking. 
The Chinese summary sentences are directly ex-
tracted from the translated Chinese documents. 
The same sentence ranking algorithm with the 
SimFusion method is adopted, and the affinity 
                                                          
2 http://nlp.stanford.edu/software/segmenter.shtml 
weight is computed based only on the cosine simi-
larity between Chinese sentences.   
For our proposed methods, the parameter val-
ues are empirically set as ?=0.8 and ?=0.5. 
4.2 Results and Discussion 
Table 1 shows the comparison results for our pro-
posed methods and the baseline methods. Seen 
from the tables, Baseline(CN) performs better than 
Baseline(EN) over all the metrics. The results dem-
onstrate that the Chinese-side information is more 
beneficial than the English-side information for 
cross-document summarization, because the sum-
mary sentences are finally selected from the Chi-
nese side. Moreover, our proposed two methods 
can outperform the two baselines over all the met-
rics. The results demonstrate the effectiveness of 
using bilingual information for cross-language 
document summarization. It is noteworthy that the 
ROUGE scores in the table are not high due to the 
following two reasons: 1) The use of machine 
translation may introduce many errors and noises 
in the peer Chinese summaries; 2) The use of Chi-
nese word segmentation may introduce more 
noises and mismatches in the ROUGE evaluation 
based on Chinese words.  
    We can also see that the CoRank method can 
outperform the SimFusion method over all metrics. 
The results show that the CoRank method is more 
suitable for the task by incorporating the bilingual 
information into a unified ranking framework.  
In order to show the influence of the value of the 
combination parameter ? on the performance of the 
SimFusion method, we present the performance 
curves over the four metrics in Figures 2 through 5, 
respectively. In the figures, ? ranges from 0 to 1, 
and ?=1 means that SimFusion is the same with 
Baseline(CN), and ?=0 means that only English-
side information is used for Chinese sentence rank-
ing. We can see that when ? is set to a value larger 
than 0.5, SimFusion can outperform the two base-
lines over most metrics. The results show that ? 
can be set in a relatively wide range. Note that 
1551
?>0.5 means that SimFusion relies more on the 
Chinese-side information than on the English-side 
information. Therefore, the Chinese-side informa-
tion is more beneficial than the English-side in-
formation.  
In order to show the influence of the value of the 
combination parameter ? on the performance of the 
CoRank method, we present the performance 
curves over the four metrics in Figures 6 through 9, 
respectively. In the figures, ? ranges from 0.1 to 
0.9, and a larger value means that the information 
from the same language side is more relied on, and 
a smaller value means that the information from 
the other language side is more relied on. We can 
see that CoRank can always outperform the two 
baselines over all metrics with different value of ?. 
The results show that ? can be set in a very wide 
range. We also note that a very large value or a 
very small value of ? can lower the performance 
values. The results demonstrate that CoRank relies 
on both the information from the same language 
side and the information from the other language 
side for sentence ranking. Therefore, both the Chi-
nese-side information and the English-side infor-
mation can complement each other, and they are 
beneficial to the final summarization performance.  
Comparing Figures 2 through 5 with Figures 6 
through 9, we can further see that the CoRank 
method is more stable and robust than the Sim-
Fusion method. The CoRank method can outper-
form the SimFusion method with most parameter 
settings. The bilingual information can be better 
incorporated in the unified ranking framework of 
the CoRank method.  
Finally, we show one running example for the 
document set D59 in the DUC2001 dataset. The 
four summaries produced by the four methods are 
listed below: 
 
Baseline(EN): ?????? 24???????????
????????????????????????????
????????????????????????????
????????????????????????????
??????????????? JT8D ????-200 ????
???????????????????????1988 ? 7
???????? DC-10?????????????????
???????? 112?? 
 
Baseline(CN): ???????????????? 1987
? 8 ? 16 ??????? 156 ??????????????
????????????????????????????
????? MD-82??? 1985?? 1986??????????
????????????????? 24 ?????????
????????????? 4 ??????????????
????????????????????????????
? 200 ?? JT8D ????????????????????
????????????????????????? 
SimFusion: ???????????????? 1987? 8
? 16 ??????? 156 ????????????????
?????????????????? 24 ????????
????????????????????????????
????????????????????????????
????????????????????????????
??????????? MD-82??? 1985?? 1986????
????????????????? 
CoRank : ?????? 24 ?????????????
??????????????????????? 1987 ? 8
? 16 ??????? 156 ????????????????
????????????????????????????
????????????????????????????
??????????????????????? 1979 ??
???????????????????????????
?? 
5 Conclusion and Future Work 
In this paper, we propose two methods (SimFusion 
and CoRank) to address the cross-language docu-
ment summarization task by leveraging the bilin-
gual information in both the source and target 
language sides. Evaluation results demonstrate the 
effectiveness of the proposed methods. The Chi-
nese-side information is validated to be more bene-
ficial than the English-side information, and the 
CoRank method is more robust than the SimFusion 
method.  
    In future work, we will investigate to use the 
machine translation quality factor to further im-
prove the fluency of the Chinese summary, as in 
Wan et al (2010). Though our attempt to use 
GIZA++ for evaluating the similarity between 
Chinese sentences and English sentences failed, we 
will exploit more advanced measures based on sta-
tistical alignment model for cross-language simi-
larity computation. 
Acknowledgments 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03) and NCET 
(NCET-08-0006). We thank the three students for 
translating the reference summaries. We also thank 
the anonymous reviewers for their useful com-
ments. 
 
1552
0.03
0.032
0.034
0.036
0.038
0.04
0.042
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E
-2
(F
)
SimFusion Baseline(EN) Baseline(CN)
 
Figure 2. ROUGE-2(F) vs. ? for SimFusion 
0.052
0.053
0.054
0.055
0.056
0.057
0.058
0.059
0.06
0.061
0.062
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E
-W
(F
)
SimFusion Baseline(EN) Baseline(CN)
 
Figure 3. ROUGE-W(F) vs. ? for SimFusion 
0.125
0.127
0.129
0.131
0.133
0.135
0.137
0.139
0.141
0.143
0.145
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E
-L
(F
)
SimFusion Baseline(EN) Baseline(CN)
 
Figure 4. ROUGE-L(F) vs. ? for SimFusion 
0.064
0.066
0.068
0.07
0.072
0.074
0.076
0.078
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
R
O
U
G
E
-S
U
4(
F)
SimFusion Baseline(EN) Baseline(CN)
 
Figure 5. ROUGE-SU4(F) vs. ? for SimFusion 
 
0.036
0.037
0.038
0.039
0.04
0.041
0.042
0.043
0.044
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
?
R
O
U
G
E
-2
(F
)
CoRank Baseline(EN) Baseline(CN)
 
Figure 6. ROUGE-2(F) vs. ? for CoRank 
0.055
0.056
0.057
0.058
0.059
0.06
0.061
0.062
0.063
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
?
R
O
U
G
E
-W
(F
)
CoRank Baseline(EN) Baseline(CN)
 
Figure 7. ROUGE-W(F) vs. ? for CoRank 
0.13
0.132
0.134
0.136
0.138
0.14
0.142
0.144
0.146
0.148
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
?
R
O
U
G
E
-L
(F
)
CoRank Baseline(EN) Baseline(CN)
 
Figure 8. ROUGE-L(F) vs. ? for CoRank 
0.07
0.071
0.072
0.073
0.074
0.075
0.076
0.077
0.078
0.079
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
?
R
O
U
G
E
-S
U
4(
F)
CoRank Baseline(EN) Baseline(CN)
 
Figure 9. ROUGE-SU4(F) vs. ? for CoRank 
1553
References  
A. Aker, T. Cohn, and R. Gaizauskas. 2010. Multi-
document summarization using A* search and 
discriminative training. In Proceedings of 
EMNLP2010. 
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
beled Data to Improve Supervised Learning for 
Text Summarization. In Proceedings of 
SIGIR2002. 
G. de Chalendar, R. Besan?on, O. Ferret, G. Gre-
fenstette, and O. Mesnard. 2005. Crosslingual 
summarization with thematic extraction, syntac-
tic sentence simplification, and bilingual genera-
tion. In Workshop on Crossing Barriers in Text 
Summarization Research, 5th International Con-
ference on Recent Advances in Natural Lan-
guage Processing  (RANLP2005). 
A. Celikyilmaz and D. Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document 
summarization. In Proceedings of ACL2010.  
G. ErKan, D. R. Radev. LexPageRank. 2004. Pres-
tige in Multi-Document Text Summarization. In 
Proceedings of EMNLP2004. 
D. Klein and C. D. Manning. 2002. Fast Exact In-
ference with a Factored Model for Natural Lan-
guage Parsing. In Proceedings of NIPS2002. 
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable 
Document Summarizer. In Proceedings of 
SIGIR1995. 
A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. 
Och, E. Hovy. 2003. Cross-lingual C*ST*RD: 
English access to Hindi information. ACM 
Transactions on Asian Language Information 
Processing, 2(3): 245-269. 
J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
document summarization using cross-language 
texts. In Proceedings of NTCIR-4.  
C. Y. Lin, E. Hovy. 2000. The Automated Acquisi-
tion of Topic Signatures for Text Summarization. 
In Proceedings of the 17th Conference on Com-
putational Linguistics. 
C.-Y. Lin and E.H. Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL -03. 
C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilin-
gual summarization evaluation 2005: automatic 
evaluation report. In Proceedings of MSE (ACL-
2005 Workshop). 
M. Litvak, M. Last, and M. Friedman. 2010. A 
new approach to improving multilingual sum-
marization using a genetic algorithm. In Pro-
ceedings of ACL2010. 
H. P. Luhn. 1969. The Automatic Creation of lit-
erature Abstracts. IBM Journal of Research and 
Development, 2(2). 
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of 
EMNLP2004. 
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of 
IJCNLP-05. 
A. Nenkova and A. Louis. 2008. Can you summa-
rize this? Identifying correlates of input diffi-
culty for generic multi-document summarization. 
In Proceedings of ACL-08:HLT. 
A. Nenkova, R. Passonneau, and K. McKeown. 
2007. The Pyramid method: incorporating hu-
man content selection variation in summariza-
tion evaluation. ACM Transactions on Speech 
and Language Processing (TSLP), 4(2). 
C. Orasan, and O. A. Chiorean. 2008. Evaluation 
of a Crosslingual Romanian-English Multi-
document Summariser. In Proceedings of 6th 
Language Resources and Evaluation Confer-
ence (LREC2008). 
P. Pingali, J. Jagarlamudi and V. Varma. 2007. 
Experiments in cross language query focused 
multi-document summarization. In Workshop on 
Cross Lingual Information Access Addressing 
the Information Need of Multilingual Societies 
in IJCAI2007. 
E. Pitler, A. Louis, and A. Nenkova. 2010. Auto-
matic evaluation of linguistic quality in multi-
document summarization. In Proceedings of 
ACL2010.  
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 
2004. Centroid-based summarization of multiple 
documents. Information Processing and Man-
agement, 40: 919-938. 
1554
A. Siddharthan and K. McKeown. 2005. Improv-
ing multilingual summarization: using redun-
dancy in the input to correct MT errors. In 
Proceedings of HLT/EMNLP-2005. 
X. Wan, H. Li and J. Xiao. 2010. Cross-language 
document summarization based on machine 
translation quality prediction. In Proceedings of 
ACL2010.  
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
document random walks for topic-focused 
multi-documetn summarization. In Proceedings 
of WI2006. 
X. Wan and J. Yang. 2008. Multi-document sum-
marization using cluster-based link analysis. In 
Proceedings of SIGIR-08. 
X. Wan, J. Yang and J. Xiao. 2007. Towards an 
Iterative Reinforcement Approach for Simulta-
neous Document Summarization and Keyword 
Extraction. In Proceedings of ACL2007.  
K.-F. Wong, M. Wu and W. Li. 2008. Extractive 
summarization using supervised and semi-
supervised learning. In Proceedings of 
COLING-08. 
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceed-
ings of SIGIR2002. 
 
1555
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 648?653,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Comparative News Summarization Using Linear Programming
Xiaojiang Huang Xiaojun Wan? Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{huangxiaojiang, wanxiaojun, xiaojianguo}@icst.pku.edu.cn
Abstract
Comparative News Summarization aims to
highlight the commonalities and differences
between two comparable news topics. In
this study, we propose a novel approach to
generating comparative news summaries. We
formulate the task as an optimization problem
of selecting proper sentences to maximize the
comparativeness within the summary and the
representativeness to both news topics. We
consider semantic-related cross-topic concept
pairs as comparative evidences, and con-
sider topic-related concepts as representative
evidences. The optimization problem is
addressed by using a linear programming
model. The experimental results demonstrate
the effectiveness of our proposed model.
1 Introduction
Comparative News Summarization aims to highlight
the commonalities and differences between two
comparable news topics. It can help users to analyze
trends, draw lessons from the past, and gain insights
about similar situations. For example, by comparing
the information about mining accidents in Chile and
China, we can discover what leads to the different
endings and how to avoid those tragedies.
Comparative text mining has drawn much atten-
tion in recent years. The proposed works differ
in the domain of corpus, the source of comparison
and the representing form of results. So far, most
researches focus on comparing review opinions of
products (Liu et al, 2005; Jindal and Liu, 2006a;
?Corresponding author
Jindal and Liu, 2006b; Lerman and McDonald,
2009; Kim and Zhai, 2009). A reason is that the
aspects in reviews are easy to be extracted and the
comparisons have simple patterns, e.g. positive
vs. negative. A few other works have also
tried to compare facts and views in news article
(Zhai et al, 2004) and Blogs (Wang et al, 2009).
The comparative information can be extracted from
explicit comparative sentences (Jindal and Liu,
2006a; Jindal and Liu, 2006b; Huang et al, 2008),
or mined implicitly by matching up features of
objects in the same aspects (Zhai et al, 2004; Liu
et al, 2005; Kim and Zhai, 2009; Sun et al,
2006). The comparisons can be represented by
charts (Liu et al, 2005), word clusters (Zhai et al,
2004), key phrases(Sun et al, 2006), and summaries
which consist of pairs of sentences or text sections
(Kim and Zhai, 2009; Lerman and McDonald,
2009; Wang et al, 2009). Among these forms,
the comparative summary conveys rich information
with good readability, so it keeps attracting interest
in the research community. In general, document
summarization can be performed by extraction or
abstraction (Mani, 2001). Due to the difficulty
of natural sentence generation, most automatic
summarization systems are extraction-based. They
select salient sentences to maximize the objective
functions of generated summaries (Carbonell and
Goldstein, 1998; McDonald, 2007; Lerman and
McDonald, 2009; Kim and Zhai, 2009; Gillick et al,
2009). The major difference between the traditional
summarization task and the comparative summa-
rization task is that traditional summarization task
places equal emphasis on all kinds of information in
648
the source, while comparative summarization task
only focuses on the comparisons between objects.
News is one of the most important channels for
acquiring information. However, it is more difficult
to extract comparisons in news articles than in
reviews. The aspects are much diverse in news.
They can be the time of the events, the person
involved, the attitudes of participants, etc. These
aspects can be expressed explicitly or implicitly in
many ways. For example, ?storm? and ?rain? both
talk about ?weather?, and thus they can form a
potential comparison. All these issues raise great
challenges to comparative summarization in the
news domain.
In this study, we propose a novel approach for
comparative news summarization. We consider
comparativeness and representativeness as well as
redundancy in an objective function, and solve the
optimization problem by using linear programming
to extract proper comparable sentences. More
specifically, we consider a pair of sentences
comparative if they share comparative concepts;
we also consider a sentence representative if it
contains important concepts about the topic. Thus
a good comparative summary contains important
comparative pairs, as well as important concepts
about individual topics. Experimental results
demonstrate the effectiveness of our model, which
outperforms the baseline systems in quality of
comparison identification and summarization.
2 Problem Definition
2.1 Comparison
A comparison identifies the commonalities or
differences among objects. It basically consists
of four components: the comparee (i.e. what is
compared), the standard (i.e. to what the compare
is compared), the aspect (i.e. the scale on which
the comparee and standard are measured), and the
result (i.e. the predicate that describes the positions
of the comparee and standard). For example, ?Chile
is richer than Haiti.? is a typical comparison, where
the comparee is ?Chile?; the standard is ?Haiti?; the
comparative aspect is wealth, which is implied by
?richer?; and the result is that Chile is superior to
Haiti.
A comparison can be expressed explicitly in a
comparative sentence, or be described implicitly
in a section of text which describes the individual
characteristics of each object point-by-point. For
example, the following text
Haiti is an extremely poor country.
Chile is a rich country.
also suggests that Chile is richer than Haiti.
2.2 Comparative News Summarization
The task of comparative news summarization is to
briefly sum up the commonalities and differences
between two comparable news topics by using
human readable sentences. The summarization
system is given two collections of news articles,
each of which is related to a topic. The system
should find latent comparative aspects, and generate
descriptions of those aspects in a pairwise way, i.e.
including descriptions of two topics simultaneously
in each aspect. For example, when comparing
the earthquake in Haiti with the one in Chile,
the summary should contain the intensity of each
temblor, the damages in each disaster area, the
reactions of each government, etc.
Formally, let t1 and t2 be two comparable news
topics, and D1 and D2 be two collections of
articles about each topic respectively. The task of
comparative summarization is to generate a short
abstract which conveys the important comparisons
{< t1, t2, r1i, r2i >}, where r1i and r2i are
descriptions about topic t1 and t2 in the same
latent aspect ai respectively. The summary can be
considered as a combination of two components,
each of which is related to a news topic. It can also
be subdivided into several sections, each of which
focuses on a major aspect. The comparisons should
have good quality, i.e., be clear and representative to
both topics. The coverage of comparisons should be
as wide as possible, which means the aspects should
not be redundant because of the length limit.
3 Proposed Approach
It is natural to select the explicit comparative
sentences as comparative summary, because they
express comparison explicitly in good qualities.
However, they do not appear frequently in regular
news articles so that the coverage is limited. Instead,
649
it is more feasible to extract individual descriptions
of each topic over the same aspects and then
generate comparisons.
To discover latent comparative aspects, we
consider a sentence as a bag of concepts, each of
which has an atom meaning. If two sentences have
same concepts in common, they are likely to discuss
the same aspect and thus they may be comparable
with each other. For example,
Lionel Messi named FIFA Word Player of
the Year 2010.
Cristiano Ronalo Crowned FIFA Word
Player of the Year 2009.
The two sentences compare on the ?FIFA Word
Player of the Year?, which is contained in both
sentences. Furthermore, semantic related concepts
can also represent comparisons. For example,
?snow? and ?sunny? can indicate a comparison
on ?weather?; ?alive? and ?death? can imply a
comparison on ?rescue result?. Thus the pairs
of semantic related concepts can be considered as
evidences of comparisons.
A comparative summary should contain as many
comparative evidences as possible. Besides, it
should convey important information in the original
documents. Since we model the text with a
collection of concept units, the summary should
contain as many important concepts as possible.
An important concept is likely to be mentioned
frequently in the documents, and thus we use the
frequency as a measure of a concept?s importance.
Obviously, the more accurate the extracted
concepts are, the better we can represent the
meaning of a text. However, it is not easy to extract
semantic concepts accurately. In this study, we
use words, named entities and bigrams to simply
represent concepts, and leave the more complex
concept extraction for future work.
Based on the above ideas, we can formulate
the summarization task as an optimization problem.
Formally, letCi = {cij} be the set of concepts in the
document set Di, (i = 1, 2). Each concept cij has a
weight wij ? R. ocij ? {0, 1} is a binary variable
indicating whether the concept cij is presented in the
summary. A cross-topic concept pair < c1j , c2k >
has a weight ujk ? R that indicates whether it
implies a important comparison. opjk is a binary
variable indicating whether the pair is presented in
the summary. Then the objective function score of a
comparative summary can be estimated as follows:
?
|C1|
?
j=1
|C2|
?
k=1
ujk ?opjk +(1??)
2
?
i=1
|Ci|
?
j=1
wij ?ocij (1)
The first component of the function estimates the
comparativeness within the summary and the second
component estimates the representativeness to both
topics. ? ? [0, 1] is a factor that balances these two
factors. In this study, we set ? = 0.55.
The weights of concepts are calculated as follows:
wij = tfij ? idfij (2)
where tfij is the term frequency of the concept cij
in the document set Di, and idfij is the inverse
document frequency calculated over a background
corpus.
The weights of concept pairs are calculated as
follows:
ujk =
{
(w1j + w2k)/2, if rel(c1j , c2k) > ?
0, otherwise
(3)
where rel(c1j , c2k) is the semantic relevance be-
tween two concepts, and it is calculated using the
algorithms basing on WordNet (Pedersen et al,
2004). If the relevance is higher than the threshold
? (0.2 in this study), then the concept pair is
considered as an evidence of comparison.
Note that a concept pair will not be presented in
the summary unless both the concepts are presented,
i.e.
opjk ? oc1j (4)
opjk ? oc2k (5)
In order to avoid bias towards the concepts which
have more related concepts, we only count the most
important relation of each concept, i.e.
?
k
opjk ? 1, ?j (6)
?
j
opjk ? 1, ?k (7)
The algorithm selects proper sentences to max-
imize the objective function. Formally, let Si =
650
{sik} be the set of sentences in Di, ocsijk be
a binary variable indicating whether concept cij
occurs in sentence sik, and osik be a binary variable
indicating whether sik is presented in the summary.
If sik is selected in the summary, then all the
concepts in it are presented in the summary, i.e.
ocij ? ocsijk ? osik, ?1 ? j ? |Ci| (8)
Meanwhile, a concept will not be present in the
summary unless it is contained in some selected
sentences, i.e.
ocij ?
|Si|
?
k=1
ocsijk ? osik (9)
Finally, the summary should satisfy a length
constraint:
2
?
i=1
|Si|
?
k=1
lik ? osik ? L (10)
where lik is the length of sentence sik, and L is the
maximal summary length.
The optimization of the defined objective function
under above constraints is an integer linear program-
ming (ILP) problem. Though the ILP problems
are generally NP-hard, considerable works have
been done and several software solutions have been
released to solve them efficiently.1
4 Experiment
4.1 Dataset
Because of the novelty of the comparative news
summarization task, there is no existing data set
for evaluating. We thus create our own. We first
choose five pairs of comparable topics, then retrieve
ten related news articles for each topic using the
Google News2 search engine. Finally we write the
comparative summary for each topic pair manually.
The topics are showed in table 1.
4.2 Evaluation Metrics
We evaluate the models with following measures:
Comparison Precision / Recall / F-measure:
let aa and am be the numbers of all aspects
1We use IBM ILOG CPLEX optimizer to solve the problem.
2http://news.google.com
ID Topic 1 Topic 2
1 Haiti Earth quake Chile Earthquake
2 Chile Mining Acci-
dent
New Zealand Mining
Accident
3 Iraq Withdrawal Afghanistan
Withdrawal
4 Apple iPad 2 BlackBerry Playbook
5 2006 FIFAWorld Cup 2010 FIFAWorld Cup
Table 1: Comparable topic pairs in the dataset.
involved in the automatically generated summary
and manually written summary respectively; ca
be the number of human agreed comparative
aspects in the automatically generated summary.
The comparison precision (CP ), comparison recall
(CR) and comparison F-measure (CF ) are defined
as follows:
CP = ca
aa
; CR = ca
am
; CF = 2 ? CP ? CR
CP + CR
ROUGE: the ROUGE is a widely used metric
in summarization evaluation. It measures summary
quality by counting overlapping units between the
candidate summary and the reference summary (Lin
and Hovy, 2003). In the experiment, we report
the f-measure values of ROUGE-1, ROUGE-2 and
ROUGE-SU4, which count overlapping unigrams,
bigrams and skip-4-grams respectively. To evaluate
whether the summary is related to both topics,
we also split each comparative summary into two
topic-related parts, evaluate them respectively, and
report the mean of the two ROUGE values (denoted
as MROUGE).
4.3 Baseline Systems
Non-Comparative Model (NCM): The
non-comparative model treats the task as a
traditional summarization problem and selects the
important sentences from each document collection.
The model is adapted from our approach by setting
? = 0 in the objection function 1.
Co-Ranking Model (CRM): The co-ranking
model makes use of the relations within each
topic and relations across the topics to reinforce
scores of the comparison related sentences. The
model is adapted from (Wan et al, 2007). The
651
SS, WW and SW relationships are replaced by
relationships between two sentences within each
topic and relationships between two sentences from
different topics.
4.4 Experiment Results
We apply all the systems to generate comparative
summaries with a length limit of 200 words. The
evaluation results are shown in table 2. Compared
with baseline models, our linear programming based
comparative model (denoted as LPCM) achieves
best scores over all metrics. It is expected to find
that the NCM model does not perform well in this
task because it does not focus on the comparisons.
The CRM model utilizes the similarity between
two topics to enhance the score of comparison
related sentences. However, it does not guarantee
to choose pairwise sentences to form comparisons.
The LPCM model focus on both comparativeness
and representativeness at the same time, and thus
it achieves good performance on both comparison
extraction and summarization. Figure 1 shows
an example of comparative summary generated by
using the CLPM model. The summary describes
several comparisons between two FIFA World Cups
in 2006 and 2010. Most of the comparisons are clear
and representative.
5 Conclusion
In this study, we propose a novel approach to
summing up the commonalities and differences
between two news topics. We formulate the
task as an optimization problem of selecting
sentences to maximize the score of comparative and
representative evidences. The experiment results
show that our model is effective in comparison
extraction and summarization.
In future work, we will utilize more semantic
information such as localized latent topics to help
capture comparative aspects, and use machine
learning technologies to tune weights of concepts.
Acknowledgments
This work was supported by NSFC (60873155),
Beijing Nova Program (2008B03) and NCET
(NCET-08-0006).
Model CP CR CF ROUGE-1 ROUGE-2 ROUGE-su4 MROUGE-1 MROUGE-2 MROUGE-su4
NCM 0.238 0.262 0.247 0.398 0.146 0.174 0.350 0.122 0.148
CRM 0.313 0.285 0.289 0.426 0.194 0.226 0.355 0.146 0.175
LPCM 0.359 0.419 0.386 0.427 0.205 0.234 0.380 0.171 0.192
Table 2: Evaluation results of systems
World Cup 2006 World Cup 2010
The 2006 Fifa World Cup drew to a close on Sunday
with Italy claiming their fourth crown after beating
France in a penalty shoot-out.
Spain have won the 2010 FIFA World Cup South Africa
final, defeating Netherlands 1-0 with a wonderful goal
from Andres Iniesta deep into extra-time.
Zidane won the Golden Ball over Italians Fabio
Cannavaro and Andrea Pirlo.
Uruguay star striker Diego Forlan won the Golden
Ball Award as he was named the best player of the
tournament at the FIFA World Cup 2010 in South
Africa.
Lukas Podolski was named the inaugural Gillette Best
Young Player.
German youngster Thomas Mueller got double delight
after his side finished third in the tournament as he was
named Young Player of the World Cup
Germany striker Miroslav Klose was the Golden Shoe
winner for the tournament?s leading scorer.
Among the winners were goalkeeper and captain Iker
Casillas who won the Golden Glove Award.
England?s fans brought more colour than their team. Only four of the 212 matches played drew more that
40,000 fans.
Figure 1: A sample comparative summary generated by using the LPCM model
652
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 335?336. ACM.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP
?09, pages 4769?4772, Washington, DC, USA. IEEE
Computer Society.
Xiaojiang. Huang, Xiaojun. Wan, Jianwu. Yang, and
Jianguo. Xiao. 2008. Learning to Identify
Comparative Sentences in Chinese Text. PRICAI
2008: Trends in Artificial Intelligence, pages 187?198.
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 244?251. ACM.
Nitin Jindal and Bing Liu. 2006b. Mining comparative
sentences and relations. In proceedings of the 21st
national conference on Artificial intelligence - Volume
2, pages 1331?1336. AAAI Press.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating
comparative summaries of contradictory opinions in
text. In Proceeding of the 18th ACM conference
on Information and knowledge management, pages
385?394. ACM.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer reviews.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
113?116. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 71?78,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the Web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351. ACM.
Inderjeet Mani. 2001. Automatic summarization. Natu-
ral Language Processing. John Benjamins Publishing
Company.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR re-
search, ECIR?07, pages 557?564, Berlin, Heidelberg.
Springer-Verlag.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers at
HLT-NAACL 2004 on XX, pages 38?41. Association
for Computational Linguistics.
Jian-Tao Sun, Xuanhui Wang, Dou Shen, Hua-Jun Zeng,
and Zheng Chen. 2006. CWS: a comparative
web search system. In Proceedings of the 15th
international conference on World Wide Web, pages
467?476. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for
simultaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
552?559, Prague, Czech Republic, June. Association
for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Comparative document summarization
via discriminative sentence selection. In Proceeding
of the 18th ACM conference on Information and
knowledge management, pages 1963?1966. ACM.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
mining. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 743?748. ACM.
653
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232?241,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Reducing Approximation and Estimation Errors for Chinese Lexical
Processing with Heterogeneous Annotations
Weiwei Sun? and Xiaojun Wan? ?
??Institute of Computer Science and Technology, Peking University
?Saarbru?cken Graduate School of Computer Science
?Department of Computational Linguistics, Saarland University
?Language Technology Lab, DFKI GmbH
{ws,wanxiaojun}@pku.edu.cn
Abstract
We address the issue of consuming heteroge-
neous annotation data for Chinese word seg-
mentation and part-of-speech tagging. We em-
pirically analyze the diversity between two
representative corpora, i.e. Penn Chinese
Treebank (CTB) and PKU?s People?s Daily
(PPD), on manually mapped data, and show
that their linguistic annotations are systemat-
ically different and highly compatible. The
analysis is further exploited to improve pro-
cessing accuracy by (1) integrating systems
that are respectively trained on heterogeneous
annotations to reduce the approximation error,
and (2) re-training models with high quality
automatically converted data to reduce the es-
timation error. Evaluation on the CTB and
PPD data shows that our novel model achieves
a relative error reduction of 11% over the best
reported result in the literature.
1 Introduction
A majority of data-driven NLP systems rely on
large-scale, manually annotated corpora that are im-
portant to train statistical models but very expensive
to build. Nowadays, for many tasks, multiple het-
erogeneous annotated corpora have been built and
publicly available. For example, the Penn Treebank
is popular to train PCFG-based parsers, while the
Redwoods Treebank is well known for HPSG re-
search; the Propbank is favored to build general se-
mantic role labeling systems, while the FrameNet is
attractive for predicate-specific labeling. The anno-
?This work is mainly finished when the first author was
in Saarland University and DFKI. Both authors are the corre-
sponding authors.
tation schemes in different projects are usually dif-
ferent, since the underlying linguistic theories vary
and have different ways to explain the same lan-
guage phenomena. Though statistical NLP systems
usually are not bound to specific annotation stan-
dards, almost all of them assume homogeneous an-
notation in the training corpus. The co-existence of
heterogeneous annotation data therefore presents a
new challenge to the consumers of such resources.
There are two essential characteristics of hetero-
geneous annotations that can be utilized to reduce
two main types of errors in statistical NLP, i.e. the
approximation error that is due to the intrinsic sub-
optimality of a model and the estimation error that is
due to having only finite training data. First, hetero-
geneous annotations are (similar but) different as a
result of different annotation schemata. Systems re-
spectively trained on heterogeneous annotation data
can produce different but relevant linguistic analy-
sis. This suggests that complementary features from
heterogeneous analysis can be derived for disam-
biguation, and therefore the approximation error can
be reduced. Second, heterogeneous annotations are
(different but) similar because their linguistic analy-
sis is highly correlated. This implies that appropriate
conversions between heterogeneous corpora could
be reasonably accurate, and therefore the estimation
error can be reduced by reason of the increase of re-
liable training data.
This paper explores heterogeneous annotations
to reduce both approximation and estimation errors
for Chinese word segmentation and part-of-speech
(POS) tagging, which are fundamental steps for
more advanced Chinese language processing tasks.
We empirically analyze the diversity between two
representative popular heterogeneous corpora, i.e.
232
Penn Chinese Treebank (CTB) and PKU?s People?s
Daily (PPD). To that end, we manually label 200
sentences from CTB with PPD-style annotations.1
Our analysis confirms the aforementioned two prop-
erties of heterogeneous annotations. Inspired by
the sub-word tagging method introduced in (Sun,
2011), we propose a structure-based stacking model
to fully utilize heterogeneous word structures to re-
duce the approximation error. In particular, joint
word segmentation and POS tagging is addressed
as a two step process. First, character-based tag-
gers are respectively trained on heterogeneous an-
notations to produce multiple analysis. The outputs
of these taggers are then merged into sub-word se-
quences, which are further re-segmented and tagged
by a sub-word tagger. The sub-word tagger is de-
signed to refine the tagging result with the help of
heterogeneous annotations. To reduce the estima-
tion error, we employ a learning-based approach to
convert complementary heterogeneous data to in-
crease labeled training data for the target task. Both
the character-based tagger and the sub-word tagger
can be refined by re-training with automatically con-
verted data.
We conduct experiments on the CTB and PPD
data, and compare our system with state-of-the-
art systems. Our structure-based stacking model
achieves an f-score of 94.36, which is superior to
a feature-based stacking model introduced in (Jiang
et al, 2009). The converted data can also enhance
the baseline model. A simple character-based model
can be improved from 93.41 to 94.11. Since the
two treatments are concerned with reducing differ-
ent types of errors and thus not fully overlapping, the
combination of them gives a further improvement.
Our final system achieves an f-score of 94.68, which
yields a relative error reduction of 11% over the best
published result (94.02).
2 Joint Chinese Word Segmentation and
POS Tagging
Different from English and other Western languages,
Chinese is written without explicit word delimiters
such as space characters. To find and classify the
1The first 200 sentences of the development data for experi-
ments are selected. This data set is submitted as a supplemental
material for research purposes.
basic language units, i.e. words, word segmentation
and POS tagging are important initial steps for Chi-
nese language processing. Supervised learning with
specifically defined training data has become a dom-
inant paradigm. Joint approaches that resolve the
two tasks simultaneously have received much atten-
tion in recent research. Previous work has shown
that joint solutions led to accuracy improvements
over pipelined systems by avoiding segmentation er-
ror propagation and exploiting POS information to
help segmentation (Ng and Low, 2004; Jiang et al,
2008a; Zhang and Clark, 2008; Sun, 2011).
Two kinds of approaches are popular for joint
word segmentation and POS tagging. The first is the
?character-based? approach, where basic processing
units are characters which compose words (Jiang et
al., 2008a). In this kind of approach, the task is for-
mulated as the classification of characters into POS
tags with boundary information. For example, the
label B-NN indicates that a character is located at the
begging of a noun. Using this method, POS infor-
mation is allowed to interact with segmentation. The
second kind of solution is the ?word-based? method,
also known as semi-Markov tagging (Zhang and
Clark, 2008; Zhang and Clark, 2010), where the ba-
sic predicting units are words themselves. This kind
of solver sequentially decides whether the local se-
quence of characters makes up a word as well as its
possible POS tag. Solvers may use previously pre-
dicted words and their POS information as clues to
process a new word.
In addition, we proposed an effective and efficient
stacked sub-word tagging model, which combines
strengths of both character-based and word-based
approaches (Sun, 2011). First, different character-
based and word-based models are trained to produce
multiple segmentation and tagging results. Sec-
ond, the outputs of these coarse-grained models are
merged into sub-word sequences, which are fur-
ther bracketed and labeled with POS tags by a fine-
grained sub-word tagger. Their solution can be
viewed as utilizing stacked learning to integrate het-
erogeneous models.
Supervised segmentation and tagging can be im-
proved by exploiting rich linguistic resources. Jiang
et al (2009) presented a preliminary study for an-
notation ensemble, which motivates our research as
well as similar investigations for other NLP tasks,
233
e.g. parsing (Niu et al, 2009; Sun et al, 2010). In
their solution, heterogeneous data is used to train an
auxiliary segmentation and tagging system to pro-
duce informative features for target prediction. Our
previous work (Sun and Xu, 2011) and Wang et al
(2011) explored unlabeled data to enhance strong
supervised segmenters and taggers. Both of their
work fall into the category of feature induction based
semi-supervised learning. In brief, their methods
harvest useful string knowledge from unlabeled or
automatically analyzed data, and apply the knowl-
edge to design new features for discriminative learn-
ing.
3 About Heterogeneous Annotations
For Chinese word segmentation and POS tag-
ging, supervised learning has become a dominant
paradigm. Much of the progress is due to the devel-
opment of both corpora and machine learning tech-
niques. Although several institutions to date have
released their segmented and POS tagged data, ac-
quiring sufficient quantities of high quality training
examples is still a major bottleneck. The annotation
schemes of existing lexical resources are different,
since the underlying linguistic theories vary. Despite
the existence of multiple resources, such data cannot
be simply put together for training systems, because
almost all of statistical NLP systems assume homo-
geneous annotation. Therefore, it is not only inter-
esting but also important to study how to fully utilize
heterogeneous resources to improve Chinese lexical
processing.
There are two main types of errors in statistical
NLP: (1) the approximation error that is due to the
intrinsic suboptimality of a model and (2) the esti-
mation error that is due to having only finite train-
ing data. Take Chinese word segmentation for ex-
ample. Our previous analysis (Sun, 2010) shows
that one main intrinsic disadvantage of character-
based model is the difficulty in incorporating the
whole word information, while one main disadvan-
tage of word-based model is the weak ability to ex-
press word formation. In both models, the signifi-
cant decrease of the prediction accuracy of out-of-
vocabulary (OOV) words indicates the impact of the
estimation error. The two essential characteristics
about systematic diversity of heterogeneous annota-
tions can be utilized to reduce both approximation
and estimation errors.
3.1 Analysis of the CTB and PPD Standards
This paper focuses on two representative popular
corpora for Chinese lexical processing: (1) the Penn
Chinese Treebank (CTB) and (2) the PKU?s Peo-
ple?s Daily data (PPD). To analyze the diversity be-
tween their annotation standards, we pick up 200
sentences from CTB and manually label them ac-
cording to the PPD standard. Specially, we employ a
PPD-style segmentation and tagging system to auto-
matically label these 200 sentences. A linguistic ex-
pert who deeply understands the PPD standard then
manually checks the automatic analysis and corrects
its errors.
These 200 sentences are segmented as 3886 and
3882 words respectively according to the CTB and
PPD standards. The average lengths of word tokens
are almost the same. However, the word bound-
aries or the definitions of words are different. 3561
word tokens are consistently segmented by both
standards. In other words, 91.7% CTB word tokens
share the same word boundaries with 91.6% PPD
word tokens. Among these 3561 words, there are
552 punctuations that are simply consistently seg-
mented. If punctuations are filtered out to avoid
overestimation of consistency, 90.4% CTB words
have same boundaries with 90.3% PPD words. The
boundaries of words that are differently segmented
are compatible. Among all annotations, only one
cross-bracketing occurs. The statistics indicates that
the two heterogenous segmented corpora are sys-
tematically different, and confirms the aforemen-
tioned two properties of heterogeneous annotations.
Table 1 is the mapping between CTB-style tags
and PPD-style tags. For the definition and illus-
tration of these tags, please refers to the annotation
guidelines2. The statistics after colons are how many
times this POS tag pair appears among the 3561
words that are consistently segmented. From this ta-
ble, we can see that (1) there is no one-to-one map-
ping between their heterogeneous word classifica-
tion but (2) the mapping between heterogeneous tags
is not very uncertain. This simple analysis indicates
2Available at http://www.cis.upenn.edu/
?chinese/posguide.3rd.ch.pdf and http://www.
icl.pku.edu.cn/icl_groups/corpus/spec.htm.
234
that the two POS tagged corpora also hold the two
properties of heterogeneous annotations. The dif-
ferences between the POS annotation standards are
systematic. The annotations in CTB are treebank-
driven, and thus consider more functional (dynamic)
information of basic lexical categories. The annota-
tions in PPD are lexicon-driven, and thus focus on
more static properties of words. Limited to the doc-
ument length, we only illustrate the annotation of
verbs and nouns for better understanding of the dif-
ferences.
? The CTB tag VV indicates common verbs that
are mainly labeled as verbs (v) too according
to the PPD standard. However, these words can
be also tagged as nominal categories (a, vn, n).
The main reason is that there are a large num-
ber of Chinese adjectives and nouns that can be
realized as predicates without linking verbs.
? The tag NN indicates common nouns in CTB.
Some of them are labeled as verbal categories
(vn, v). The main reason is that a majority of
Chinese verbs could be realized as subjects and
objects without form changes.
4 Structure-based Stacking
4.1 Reducing the Approximation Error via
Stacking
Each annotation data set alne can yield a predictor
that can be taken as a mechanism to produce struc-
tured texts. With different training data, we can con-
struct multiple heterogeneous systems. These sys-
tems produce similar linguistic analysis which holds
the same high level linguistic principles but differ in
details. A very simple idea to take advantage of het-
erogeneous structures is to design a predictor which
can predict a more accurate target structure based
on the input, the less accurate target structure and
complementary structures. This idea is very close
to stacked learning (Wolpert, 1992), which is well
developed for ensemble learning, and successfully
applied to some NLP tasks, e.g. dependency parsing
(Nivre and McDonald, 2008; Torres Martins et al,
2008).
Formally speaking, our idea is to include two
?levels? of processing. The first level includes one
AS? u:44; CD? m:134;
DEC? u:83; DEV? u:7;
DEG? u:123; ETC? u:9;
LB? p:1; NT? t:98;
OD? m:41; PU? w:552;
SP? u:1; VC? v:32;
VE? v:13; BA? p:2; d:1;
CS? c:3; d:1; DT? r:15; b:1;
MSP? c:2; u:1; PN? r:53; n:2;
CC? c:73; p:5; v:2; M? q:101; n:11; v:1;
LC? f:51; Ng:3; v:1; u:1; P? p:133; v:4; c:2; Vg:1;
VA ? a:57; i:4; z:2; ad:1;
b:1;
NR ? ns:170; nr:65; j:23;
nt:21; nz:7; n:2; s:1;
VV ? v:382; i:5; a:3; Vg:2;
vn:2; n:2; p:2; w:1;
JJ ? a:43; b:13; n:3; vn:3;
d:2; j:2; f:2; t:2; z:1;
AD? d:149; c:11; ad:6; z:4;
a:3; v:2; n:1; r:1; m:1; f:1;
t:1;
NN ? n:738; vn:135; v:26;
j:19; Ng:5; an:5; a:3; r:3; s:3;
Ag:2; nt:2; f:2; q:2; i:1; t:1;
nz:1; b:1;
Table 1: Mapping between CTB and PPD POS Tags.
or more base predictors f1, ..., fK that are indepen-
dently built on different training data. The second
level processing consists of an inference function h
that takes as input ?x, f1(x), ..., fK(x)?3 and out-
puts a final prediction h(x, f1(x), ..., fK(x)). The
only difference between model ensemble and anno-
tation ensemble is that the output spaces of model
ensemble are the same while the output spaces of an-
notation ensemble are different. This framework is
general and flexible, in the sense that it assumes al-
most nothing about the individual systems and take
them as black boxes.
4.2 A Character-based Tagger
With IOB2 representation (Ramshaw and Marcus,
1995), the problem of joint segmentation and tag-
ging can be regarded as a character classification
task. Previous work shows that the character-based
approach is an effective method for Chinese lexical
processing. Both of our feature- and structure-based
stacking models employ base character-based tag-
gers to generate multiple segmentation and tagging
results. Our base tagger use a discriminative sequen-
tial classifier to predict the POS tag with positional
information for each character. Each character can
be assigned one of two possible boundary tags: ?B?
for a character that begins a word and ?I? for a char-
acter that occurs in the middle of a word. We denote
3x is a given Chinese sentence.
235
a candidate character token ci with a fixed window
ci?2ci?1cici+1ci+2. The following features are used
for classification:
? Character unigrams: ck (i? l ? k ? i+ l)
? Character bigrams: ckck+1 (i? l ? k < i+ l)
4.3 Feature-based Stacking
Jiang et al (2009) introduced a feature-based stack-
ing solution for annotation ensemble. In their so-
lution, an auxiliary tagger CTagppd is trained on a
complementary corpus, i.e. PPD, to assist the tar-
get CTB-style tagging. To refine the character-based
tagger CTagctb, PPD-style character labels are di-
rectly incorporated as new features. The stacking
model relies on the ability of discriminative learning
method to explore informative features, which play
central role to boost the tagging performance. To
compare their feature-based stacking model and our
structure-based model, we implement a similar sys-
tem CTagppd?ctb. Apart from character uni/bigram
features, the PPD-style character labels are used to
derive the following features to enhance our CTB-
style tagger:
? Character label unigrams: cppdk (i?l
ppd ? k ?
i+ lppd)
? Character label bigrams: cppdk c
ppd
k+1 (i? l
ppd ?
k < i+ lppd)
In the above descriptions, l and lppd are the win-
dow sizes of features, which can be tuned on devel-
opment data.
4.4 Structure-based Stacking
We propose a novel structured-based stacking model
for the task, in which heterogeneous word struc-
tures are used not only to generate features but also
to derive a sub-word structure. Our work is in-
spired by the stacked sub-word tagging model in-
troduced in (Sun, 2011). Their work is motivated
by the diversity of heterogeneous models, while
our work is motivated by the diversity of heteroge-
neous annotations. The workflow of our new sys-
tem is shown in Figure 1. In the first phase, one
character-based CTB-style tagger (CTagctb) and
one character-based PPD-style tagger (CTagppd)
are respectively trained to produce heterogenous
Raw sentences
CTB-style character
tagger CTagctb
PPD-style character
tagger CTagppd
Segmented and
tagged sentences
Segmented and
tagged sentences
Merging
Sub-word
sequences
CTB-style
sub-word tag-
ger STagctb
Figure 1: Sub-word tagging based on heterogeneous tag-
gers.
word boundaries. In the second phase, this system
first combines the two segmentation and tagging re-
sults to get sub-words which maximize the agree-
ment about word boundaries. Finally, a fine-grained
sub-word tagger (STagctb) is applied to bracket sub-
words into words and also to label their POS tags.
We can also apply a PPD-style sub-word tagger. To
compare with previous work, we specially concen-
trate on the PPD-to-CTB adaptation.
Following (Sun, 2011), the intermediate sub-word
structures is defined to maximize the agreement of
CTagctb and CTagppd. In other words, the goal is
to make merged sub-words as large as possible but
not overlap with any predicted word produced by
the two taggers. If the position between two con-
tinuous characters is predicted as a word boundary
by any segmenter, this position is taken as a separa-
tion position of the sub-word sequence. This strat-
egy makes sure that it is still possible to correctly
re-segment the strings of which the boundaries are
disagreed with by the heterogeneous segmenters in
the sub-word tagging stage.
To train the sub-word tagger STagctb, features
are formed making use of both CTB-style and PPD-
style POS tags provided by the character-based tag-
gers. In the following description, ?C? refers to the
content of a sub-word; ?Tctb? and ?Tppd? refers to
the positional POS tags generated from CTagctb and
CTagppd; lC , lctbT and l
ppd
T are the window sizes.
For convenience, we denote a sub-word with its con-
236
text ...si?1sisi+1..., where si is the current token.
The following features are applied:
? Unigram features: C(sk) (i ? lC ? k ? +lC),
Tctb(sk) (i ? lctbT ? k ? i + l
ctb
T ), Tppd(sk)
(i? lppdT ? k ? i+ l
ppd
T )
? Bigram features: C(sk)C(sk+1) (i ? lC ? k <
i + lC), Tctb(sk)Tctb(sk+1) (i ? lctbT ? k <
i+ lctbT ), Tppd(sk)Tppd(sk+1) (i? l
ppd
T ? k <
i+ lppdT )
? C(si?1)C(si+1) (if lC ? 1),
Tctb(si?1)Tctb(si+1) (if lctbT ? 1),
Tppd(si?1)Tppd(si+1) (if l
ppd
T ? 1)
? Word formation features: character n-gram
prefixes and suffixes for n up to 3.
Cross-validation CTagctb and CTagppd are di-
rectly trained on the original training data, i.e. the
CTB and PPD data. Cross-validation technique has
been proved necessary to generate the training data
for sub-word tagging, since it deals with the train-
ing/test mismatch problem (Sun, 2011). To con-
struct training data for the new heterogeneous sub-
word tagger, a 10-fold cross-validation on the origi-
nal CTB data is performed too.
5 Data-driven Annotation Conversion
It is possible to acquire high quality labeled data
for a specific annotation standard by exploring ex-
isting heterogeneous corpora, since the annotations
are normally highly compatible. Moreover, the ex-
ploitation of additional (pseudo) labeled data aims to
reduce the estimation error and enhances a NLP sys-
tem in a different way from stacking. We therefore
expect the improvements are not much overlapping
and the combination of them can give a further im-
provement.
The stacking models can be viewed as annota-
tion converters: They take as input complementary
structures and produce as output target structures.
In other words, the stacking models actually learn
statistical models to transform the lexical represen-
tations. We can acquire informative extra samples
by processing the PPD data with our stacking mod-
els. Though the converted annotations are imperfect,
they are still helpful to reduce the estimation error.
Character-based Conversion The feature-based
stacking model CTagppd?ctb maps the input char-
acter sequence c and its PPD-style character label
sequence to the corresponding CTB-style character
label sequence. This model by itself can be taken as
a corpus conversion model to transform a PPD-style
analysis to a CTB-style analysis. By processing the
auxiliary corpus Dppd with CTagppd?ctb, we ac-
quire a new labeled data set D?ctb = D
CTagppd?ctb
ppd?ctb .
We can re-train the CTagctb model with both origi-
nal and converted data Dctb ?D?ctb.
Sub-word-based Conversion Similarly, the
structure-based stacking model can be also taken
as a corpus conversion model. By processing the
auxiliary corpus Dppd with STagctb, we acquire
a new labeled data set D??ctb = D
STagctb
ppd?ctb. We can
re-train the STagctb model with Dctb ? D??ctb. If
we use the gold PPD-style labels of D??ctb to extract
sub-words, the new model will overfit to the gold
PPD-style labels, which are unavailable at test time.
To avoid this training/test mismatch problem, we
also employ a 10-fold cross validation procedure to
add noise.
It is not a new topic to convert corpus from one
formalism to another. A well known work is trans-
forming Penn Treebank into resources for various
deep linguistic processing, including LTAG (Xia,
1999), CCG (Hockenmaier and Steedman, 2007),
HPSG (Miyao et al, 2004) and LFG (Cahill et al,
2002). Such work for corpus conversion mainly
leverages rich sets of hand-crafted rules to convert
corpora. The construction of linguistic rules is usu-
ally time-consuming and the rules are not full cover-
age. Compared to rule-based conversion, our statis-
tical converters are much easier to built and empiri-
cally perform well.
6 Experiments
6.1 Setting
Previous studies on joint Chinese word segmenta-
tion and POS tagging have used the CTB in experi-
ments. We follow this setting in this paper. We use
CTB 5.0 as our main corpus and define the train-
ing, development and test sets according to (Jiang
et al, 2008a; Jiang et al, 2008b; Kruengkrai et al,
2009; Zhang and Clark, 2010; Sun, 2011). Jiang et
237
al. (2009) present a preliminary study for the annota-
tion adaptation topic, and conduct experiments with
the extra PPD data4. In other words, the CTB-sytle
annotation is the target analysis while the PPD-style
annotation is the complementary/auxiliary analysis.
Our experiments for annotation ensemble follows
their setting to lead to a fair comparison of our sys-
tem and theirs. A CRF learning toolkit, wapiti5
(Lavergne et al, 2010), is used to resolve sequence
labeling problems. Among several parameter esti-
mation methods provided by wapiti, our auxiliary
experiments indicate that the ?rprop-? method works
best. Three metrics are used for evaluation: preci-
sion (P), recall (R) and balanced f-score (F) defined
by 2PR/(P+R). Precision is the relative amount of
correct words in the system output. Recall is the rel-
ative amount of correct words compared to the gold
standard annotations. A token is considered to be
correct if its boundaries match the boundaries of a
word in the gold standard and their POS tags are
identical.
6.2 Results of Stacking
Table 2 summarizes the segmentation and tagging
performance of the baseline and different stacking
models. The baseline of the character-based joint
solver (CTagctb) is competitive, and achieves an
f-score of 92.93. By using the character labels
from a heterogeneous solver (CTagppd), which is
trained on the PPD data set, the performance of this
character-based system (CTagppd?ctb) is improved
to 93.67. This result confirms the importance of a
heterogeneous structure. Our structure-based stack-
ing solution is effective and outperforms the feature-
based stacking. By better exploiting the heteroge-
neous word boundary structures, our sub-word tag-
ging model achieves an f-score of 94.03 (lctbT and
lppdT are tuned on the development data and both set
to 1).
The contribution of the auxiliary tagger is two-
fold. On one hand, the heterogeneous solver pro-
vides structural information, which is the basis to
construct the sub-word sequence. On the other
hand, this tagger provides additional POS informa-
tion, which is helpful for disambiguation. To eval-
4http://icl.pku.edu.cn/icl_res/
5http://wapiti.limsi.fr/
Devel. P R F
CTagctb 93.28% 92.58% 92.93
CTagppd?ctb 93.89% 93.46% 93.67
STagctb 94.07% 93.99% 94.03
Table 2: Performance of different stacking models on the
development data.
uate these two contributions, we do another experi-
ment by just using the heterogeneous word boundary
structures without the POS information. The f-score
of this type of sub-word tagging is 93.73. This re-
sult indicates that both the word boundary and POS
information are helpful.
6.3 Learning Curves
We do additional experiments to evaluate the effect
of heterogeneous features as the amount of PPD data
is varied. Table 3 summarizes the f-score change.
The feature-based model works well only when a
considerable amount of heterogeneous data is avail-
able. When a small set is added, the performance is
even lower than the baseline (92.93). The structure-
based stacking model is more robust and obtains
consistent gains regardless of the size of the com-
plementary data.
PPD? CTB
#CTB #PPD CTag STag
18104 7381 92.21 93.26
18104 14545 93.22 93.82
18104 21745 93.58 93.96
18104 28767 93.55 93.87
18104 35996 93.67 94.03
9052 9052 92.10 92.40
Table 3: F-scores relative to sizes of training data. Sizes
(shown in column #CTB and #PPD) are numbers of sen-
tences in each training corpus.
6.4 Results of Annotation Conversion
The stacking models can be viewed as data-driven
annotation converting models. However they are not
trained on ?real? labeled samples. Although the tar-
get representation (CTB-style analysis in our case)
is gold standard, the input representation (PPD-style
analysis in our case) is labeled by a automatic tag-
ger CTagppd. To make clear whether these stacking
238
models trained with noisy inputs can tolerate per-
fect inputs, we evaluate the two stacking models on
our manually converted data. The accuracies pre-
sented in Table 4 indicate that though the conver-
sion models are learned by applying noisy data, they
can refine target tagging with gold auxiliary tagging.
Another interesting thing is that the gold PPD-style
analysis does not help the sub-word tagging model
as much as the character tagging model.
Auto PPD Gold PPD
CTagppd?ctb 93.69 95.19
STagctb 94.14 94.70
Table 4: F-scores with gold PPD-style tagging on the
manually converted data.
6.5 Results of Re-training
Table 5 shows accuracies of re-trained models. Note
that a sub-word tagger is built on character taggers,
so when we re-train a sub-word system, we should
consider whether or not re-training base character
taggers. The error rates decrease as automatically
converted data is added to the training pool, espe-
cially for the character-based tagger CTagctb. When
the base CTB-style tagging is improved, the final
tagging is improved in the end. The re-training does
not help the sub-word tagging much; the improve-
ment is very modest.
CTagctb STagctb P(%) R(%) F
Dctb ?D?ctb - - 94.46 94.06 94.26
Dctb ?D?ctb Dctb 94.61 94.43 94.52
Dctb Dctb ?D??ctb 94.05 94.08 94.06
Dctb ?D?ctb Dctb ?D
??
ctb 94.71 94.53 94.62
Table 5: Performance of re-trained models on the devel-
opment data.
6.6 Comparison to the State-of-the-Art
Table 6 summarizes the tagging performance of
different systems. The baseline of the character-
based tagger is competitive, and achieve an f-score
of 93.41. By better using the heterogeneous word
boundary structures, our sub-word tagging model
achieves an f-score of 94.36. Both character and
sub-word tagging model can be enhanced with auto-
matically converted corpus. With the pseudo labeled
data, the performance goes up to 94.11 and 94.68.
These results are also better than the best published
result on the same data set that is reported in (Jiang
et al, 2009).
Test P R F
(Sun, 2011) - - - - 94.02
(Jiang et al, 2009) - - - - 94.02
(Wang et al, 2011) - - - - 94.186
Character model 93.31% 93.51% 93.41
+Re-training 93.93% 94.29% 94.11
Sub-word model 94.10% 94.62% 94.36
+Re-training 94.42% 94.93% 94.68
Table 6: Performance of different systems on the test
data.
7 Conclusion
Our theoretical and empirical analysis of two rep-
resentative popular corpora highlights two essential
characteristics of heterogeneous annotations which
are explored to reduce approximation and estima-
tion errors for Chinese word segmentation and POS
tagging. We employ stacking models to incorporate
features derived from heterogeneous analysis and
apply them to convert heterogeneous labeled data for
re-training. The appropriate application of hetero-
geneous annotations leads to a significant improve-
ment (a relative error reduction of 11%) over the best
performance for this task. Although our discussion
is for a specific task, the key idea to leverage het-
erogeneous annotations to reduce the approximation
error with stacking models and the estimation error
with automatically converted corpora is very general
and applicable to other NLP tasks.
Acknowledgement
This work is mainly finished when the first author
was in Saarland University and DFKI. At that time,
this author was funded by DFKI and German Aca-
demic Exchange Service (DAAD). While working
in Peking University, both author are supported by
NSFC (61170166) and National High-Tech R&D
Program (2012AA011101).
6This result is achieved with much unlabeled data, which is
different from our setting.
239
References
Aoife Cahill, Mairead Mccarthy, Josef Van Genabith, and
Andy Way. 2002. Automatic annotation of the penn
treebank with lfg f-structure information. In Proceed-
ings of the LREC Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping Anno-
tated Language Data, Las Palmas, Canary Islands,
pages 8?15.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
A corpus of ccg derivations and dependency structures
extracted from the penn treebank. Computational Lin-
guistics, 33(3):355?396.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT, pages 897?904, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 385?392, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522?530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. pages 504?
513, July.
Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In IJCNLP, pages 684?693.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277?
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarowsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word
segmentation using unlabeled data. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 970?979, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Weiwei Sun, Rui Wang, and Yi Zhang. 2010. Dis-
criminative parse reranking for Chinese with homoge-
neous and heterogeneous annotations. In Proceedings
of Joint Conference on Chinese Language Processing
(CIPS-SIGHAN), Beijing, China, August.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1211?1219, Beijing, China, August. Coling
2010 Organizing Committee.
Weiwei Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech tag-
ging. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1385?1394, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking dependency
parsers. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 157?166, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving chinese word segmentation and
pos tagging with semi-supervised methods using large
240
auto-analyzed data. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 309?317, Chiang Mai, Thailand, Novem-
ber. Asian Federation of Natural Language Processing.
David H. Wolpert. 1992. Original contribution: Stacked
generalization. Neural Netw., 5:241?259, February.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of Natural Lan-
guage Processing Pacific Rim Symposium, pages 398?
403.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings of ACL-08: HLT, pages 888?896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 843?852, Cambridge, MA,
October. Association for Computational Linguistics.
241
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 87?91,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Order Natural Language Texts 
Jiwei Tana, b, Xiaojun Wana* and Jianguo Xiaoa 
aInstitute of Computer Science and Technology, The MOE Key Laboratory of Computa-
tional Linguistics, Peking University, China 
bSchool of Information Science and Technology, Beijing Normal University, China 
tanjiwei8@gmail.com, {wanxiaojun,jgxiao}@pku.edu.cn 
 
Abstract 
Ordering texts is an important task for many 
NLP applications. Most previous works on 
summary sentence ordering rely on the contex-
tual information (e.g. adjacent sentences) of 
each sentence in the source document. In this 
paper, we investigate a more challenging task 
of ordering a set of unordered sentences with-
out any contextual information. We introduce 
a set of features to characterize the order and 
coherence of natural language texts, and use 
the learning to rank technique to determine the 
order of any two sentences. We also propose 
to use the genetic algorithm to determine the 
total order of all sentences. Evaluation results 
on a news corpus show the effectiveness of 
our proposed method. 
1 Introduction 
Ordering texts is an important task in many natu-
ral language processing (NLP) applications. It is 
typically applicable in the text generation field, 
both for concept-to-text generation and text-to-
text generation (Lapata, 2003), such as multiple 
document summarization (MDS), question an-
swering and so on. However, ordering a set of 
sentences into a coherent text is still a hard and 
challenging problem for computers. 
Previous works on sentence ordering mainly 
focus on the MDS task (Barzilay et al, 2002; 
Okazaki et al, 2004; Nie et al, 2006; Ji and 
Pulman, 2006; Madnani et al, 2007; Zhang et al, 
2010; He et al, 2006; Bollegala et al, 2005; Bol-
legala et al, 2010). In this task, each summary 
sentence is extracted from a source document. 
The timestamp of the source documents and the 
adjacent sentences in the source documents can 
be used as important clues for ordering summary 
sentences. 
In this study, we investigate a more challeng-
ing and more general task of ordering a set of 
unordered sentences (e.g. randomly shuffle the 
                                                 
* Xiaojun Wan is the corresponding author. 
sentences in a text paragraph) without any con-
textual information. This task can be applied to 
almost all text generation applications without 
restriction. 
In order to address this challenging task, we 
first introduce a few useful features to character-
ize the order and coherence of natural language 
texts, and then propose to use the learning to 
rank algorithm to determine the order of two sen-
tences. Moreover, we propose to use the genetic 
algorithm to decide the overall text order. Evalu-
ations are conducted on a news corpus, and the 
results show the prominence of our method. Each 
component technique or feature in our method 
has also been validated.  
2 Related Work 
For works taking no use of source document, 
Lapata (2003) proposed a probabilistic model 
which learns constraints on sentence ordering 
from a corpus of texts. Experimental evaluation 
indicated the importance of several learned lexi-
cal and syntactic features. However, the model 
only works well when using single feature, but 
unfortunately, it becomes worse when multiple 
features are combined. Barzilay and Lee (2004) 
investigated the utility of domain-specific con-
tent model for representing topic and topic shifts 
and the model performed well on the five se-
lected domains. Nahnsen (2009) employed fea-
tures which were based on discourse entities, 
shallow syntactic analysis, and temporal prece-
dence relations retrieved from VerbOcean. How-
ever, the model does not perform well on data-
sets describing the consequences of events. 
3 Our Proposed Method  
3.1 Overview 
The task of text ordering can be modeled like 
(Cohen et al, 1998), as measuring the coherence 
of a text by summing the association strength of 
any sentence pairs. Then the objective of a text 
ordering model is to find a permutation which 
can maximize the summation. 
87
Formally, we define an association strength 
function PREF( , ) Ru v ?  to measure how strong 
it is that sentence u  should be arranged before 
sentence v  (denoted as u v; ). We then define 
function AGREE( ,PREF)?  as: 
, : ( ) ( )
AGREE( ,PREF) = PREF( , )
u v u v
u v
? ?
?
>
? (1)
where ?  denotes a sentence permutation and 
( ) ( )u v? ?>  means u v;  in the permutation ? . 
Then the objective of finding an overall order of 
the sentences becomes finding a permutation ?  
to maximize AGREE( ,PREF)? . 
The main framework is made up of two parts: 
defining a pairwise order relation and determin-
ing an overall order. Our study focuses on both 
the two parts by learning a better pairwise rela-
tion and proposing a better search strategy, as 
described respectively in next sections. 
3.2 Pairwise Relation Learning 
The goal for pairwise relation learning is defin-
ing the strength function PREF for any sentence 
pair. In our method we define the function PREF 
by combining multiple features. 
Method: Traditionally, there are two main 
methods for defining a strength function: inte-
grating features by a linear combination (He et 
al., 2006; Bollegala et al, 2005) or by a binary 
classifier (Bollegala et al, 2010). However, the 
binary classification method is very coarse-
grained since it considers any pair of sentences 
either ?positive? or ?negative?. Instead we pro-
pose to use a better model of learning to rank to 
integrate multiple features.  
In this study, we use Ranking SVM imple-
mented in the svmrank toolkit (Joachims, 2002; 
Joachims, 2006) as the ranking model. The ex-
amples to be ranked in our ranking model are 
sequential sentence pairs like u v; . The feature 
values for a training example are generated by a 
few feature functions ( , )if u v , and we will intro-
duce the features later. We build the training ex-
amples for svmrank  as follows:  
For a training query, which is a paragraph with 
n  sequential sentences as 1 2 ... ns s s; ; ; , we 
can get 2 ( 1)nA n n= ?  training examples. For 
pairs like ( 0)a a ks s k+ >;  the target rank values 
are set to n k? , which means that the longer the 
distance between the two sentences is, the small-
er the target value is. Other pairs like a k as s+ ;  
are all set to 0. In order to better capture the or-
der information of each feature, for every sen-
tence pair u v; , we derive four feature values 
from each function ( , )if u v , which are listed as 
follows: 
,1 ( , )iiV f u v=  (2)
,2
1 / 2, if ( , ) ( , ) 0
( , )
, otherwise
( , ) ( , )
i i
i i
i i
f u v f v u
V f u v
f u v f v u
+ =??
= ?? +?
(3)
,3
1 / if ( , ) 0
( , ) / ( , ), otherwise
i
y S y u
i
i i
y S y u
S f u y
V
f u v f u y
? ? ?
? ? ?
? =?
= ???
?
?
?
(4)
,4
1 / if ( , ) 0
( , ) / ( , ), otherwise
i
x S x v
i
i i
x S x v
S f x v
V
f u v f x v
? ? ?
? ? ?
? =?
= ???
?
?
?
(5)
where S  is the set of all sentences in a paragraph 
and S  is the number of sentences in S . The 
three additional feature values of (3) (4) (5) are 
defined to measure the priority of u v;  to v u; ,   
u v;  to { , }u y S u v? ? ?;  and u v;  to 
{ , }x S u v v? ? ? ;  respectively, by calculating 
the proportion of ( , )if u v  in respective summa-
tions. 
The learned model can be used to predict tar-
get values for new examples. A paragraph of un-
ordered sentences is viewed as a test query, and 
the predicted target value for u v;  is set as 
PREF( , )u v . 
Features: We select four types of features to 
characterize text coherence. Every type of fea-
tures is quantified with several functions distin-
guished by i  in the formulation of ( , )if u v  and 
normalized to [0,1] . The features and definitions 
of ( , )if u v  are introduced in Table 1. 
Type Description 
sim( , )u v  
Similarity 
sim(latter( ),former( ))u v  
overlap ( , ) / min(| |,| |)j u v u v  
Overlap overlap (latter( ),former( ))
overlap ( , )
j
j
u v
u v
Number of  
coreference chains Coreference
Number of 
coreference words 
Noun 
Verb 
Verb & noun dependency 
Probability
Model 
Adjective & adverb 
Table 1: Features used in our model. 
88
As in Table 1, function sim( , )u v  denotes the 
cosine similarity of sentence u  and v ; latter( )u  
and former( )v  denotes the latter half part of u  
and  the former part of v  respectively, which are 
separated by the most centered comma (if exists) 
or word (if no comma exits); overlap ( , )j u v  de-
notes the number of mutual words of u  and v , 
for 1,2,3j =  representing lemmatized noun, 
verb and adjective or adverb respectively; | |u  is 
the number of words of sentence u . The value 
will be set to 0 if the denominator is 0.  
For the coreference features we use the ARK-
ref 1  tool. It can output the coreference chains 
containing words which represent the same entity 
for two sequential sentences u v; .  
The probability model originates from (Lapata, 
2003), and we implement the model with four 
features of lemmatized noun, verb, adjective or 
adverb, and verb and noun related dependency.  
3.3 Overall Order Determination 
Cohen et al (1998) proved finding a permutation 
?  to maximize AGREE( ,PREF)?  is NP-
complete. To solve this, they proposed a greedy 
algorithm for finding an approximately optimal 
order. Most later works adopted the greedy 
search strategy to determine the overall order.  
However, a greedy algorithm does not always 
lead to satisfactory results, as our experiment 
shows in Section 4.2. Therefore, we propose to 
use the genetic algorithm (Holland, 1992) as the 
search strategy, which can lead to better results. 
Genetic Algorithm: The genetic algorithm 
(GA) is an artificial intelligence algorithm for 
optimization and search problems. The key point 
of using GA is modeling the individual, fitness 
function and three operators of crossover, muta-
tion and selection. Once a problem is modeled, 
the algorithm can be constructed conventionally. 
In our method we set a permutation ?  as an 
individual encoded by a numerical path, for ex-
ample a permutation 2 1 3s s s; ;  is encoded as (2 
1 3). Then the function AGREE( ,PREF)?  is just 
the fitness function. We adopt the order-based 
crossover operator which is described in (Davis, 
1985). The mutation operator is a random inver-
sion of two sentences. For selection operator we 
take a tournament selection operator which ran-
domly selects two individuals to choose the one 
with the greater fitness value AGREE( ,PREF)? . 
                                                 
1 http://www.ark.cs.cmu.edu/ARKref/ 
After several generations of evolution, the indi-
vidual with the greatest fitness value will be a 
close solution to the optimal result. 
4 Experiments 
4.1 Experiment Setup 
Data Set and Evaluation Metric: We con-
ducted the experiments on the North American 
News Text Corpus2. We trained the model on 80 
thousand paragraphs and tested with 200 shuffled 
paragraphs. We use Kendall?s ?  as the evalua-
tion metric, which is based on the number of in-
versions in the rankings.  
Comparisons: It is incomparable with other 
methods for summary sentence ordering based 
on special summarization corpus, so we imple-
mented Lapata?s probability model for compari-
son, which is considered the state of the art for 
this task. In addition, we implemented a random 
ordering as a baseline. We also tried to use a 
classification model in place of the ranking mod-
el. In the classification model, sentence pairs like 
1a as s +;  were viewed as positive examples and 
all other pairs were viewed as negative examples. 
When deciding the overall order for either rank-
ing or classification model we used three search 
strategies: greedy, genetic and exhaustive (or 
brutal) algorithms. In addition, we conducted a 
series of experiments to evaluate the effect of 
each feature. For each feature, we tested in two 
experiments, one of which only contained the 
single feature and the other one contained all the 
other features. For comparative analysis of fea-
tures, we tested with an exhaustive search algo-
rithm to determine the overall order.  
4.2 Experiment Results 
The comparison results in Table 2 show that our 
Ranking SVM based method improves the per-
formance over the baselines and the classifica-
tion based method with any of the search algo-
rithms. We can also see the greedy search strat-
egy does not perform well and the genetic algo-
rithm can provide a good approximate solution to 
obtain optimal results. 
Method Greedy Exhaustive Genetic
Baseline -0.0127 
Probability 0.1859 
Classification 0.5006 0.5360 0.5264
Ranking 0.5191 0.5768 0.5747
Table 2: Average ?  of different methods. 
                                                 
2 The corpus is available from 
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalog
Id=LDC98T30 
89
Ranking vs. Classification: It is not surpris-
ing that the ranking model is better, because 
when using a classification model, an example 
should be labeled either positive or negative. It is 
not very reasonable to label a sentence pair like 
( 1)a a ks s k+ >;  as a negative example, nor a pos-
itive one, because in some cases, it is easy to 
conclude one sentence should be arranged after 
another but hard to decide whether they should 
be adjacent. As we see in the function AGREE , 
the value of PREF( , )a a ks s +  also contributes to 
the summation. In a ranking model, this informa-
tion can be quantified by the different priorities 
of sentence pairs with different distances. 
Single Feature Effect: The effects of differ-
ent types of features are shown in Table 3. Prob 
denotes Lapata?s probability model with differ-
ent features.  
Feature Only Removed
Similarity 0.0721 0.4614 
Overlap 0.1284 0.4631 
Coreference 0.0734 0.4704 
Probnoun 0.3679 0.3932 
Probverb 0.0615 0.4544 
Probadjective&adverb 0.2650 0.4258 
Probdependency 0.2687 0.4892 
All 0.5768 
Table 3: Effects of different features. 
It can be seen in Table 3 that all these features 
contribute to the final result. The two features of 
noun probability and dependency probability 
play an important role as demonstrated in (La-
pata, 2003). Other features also improve the final 
performance. A paragraph which is ordered en-
tirely right by our method is shown in Figure 1. 
 
Sentences which should be arranged together 
tend to have a higher similarity and overlap. Like 
sentence (3) and (4) in Figure 1, they have a 
highest cosine similarity of 0.2240 and most 
overlap words of ?Israel? and ?nuclear?. How-
ever, the similarity or overlap of the two sen-
tences does not help to decide which sentence 
should be arranged before another. In this case 
the overlap and similarity of half part of the sen-
tences may help. For example latter((3)) and 
former((4)) share an overlap of ?Israel? while 
there is no overlap for latter((4)) and former((3)). 
Coreference is also an important clue for or-
dering natural language texts. When we use a 
pronoun to represent an entity, it always has oc-
curred before. For example when conducting 
coreference resolution for (1) (2); , it will be 
found that ?He? refers to ?Vanunu?. Otherwise 
for (2) (1); , no coreference chain will be found.  
4.3 Genetic Algorithm 
There are three main parameters for GA includ-
ing the crossover probability (PC), the mutation 
probability (PM) and the population size (PS). 
There is no definite selection for these parame-
ters. In our study we experimented with a wide 
range of parameter values to see the effect of 
each parameter. It is hard to traverse all possible 
combinations so when testing a parameter we 
fixed the other two parameters. The results are 
shown in Table 4. 
  Value
Para Avg Max Min Stddev
PS 0.5731 0.5859 0.5606 0.0046
PC 0.5733 0.5806 0.5605 0.0038
PM 0.5741 0.5803 0.5337 0.0045
Table 4: Results of GA with different parameters. 
As we can see in Table 4, when adjusting the 
three parameters the average ?  values are all 
close to the exhaustive result of 0.5768 and their 
standard deviations are low. Table 4 shows that 
in our case the genetic algorithm is not very sen-
sible to the parameters. In the experiments, we 
set PS to 30, PC to 0.5 and PM to 0.05, and 
reached a value of 0.5747, which is very close to 
the theoretical upper bound of 0.5768. 
5 Conclusion and Discussion  
In this paper we propose a method for ordering 
sentences which have no contextual information 
by making use of Ranking SVM and the genetic 
algorithm. Evaluation results demonstrate the 
good effectiveness of our method. 
In future work, we will explore more features 
such as semantic features to further improve the 
performance. 
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101). 
(1) Vanunu, 43, is serving an 18-year sentence for 
treason.  
(2) He was kidnapped by Israel's Mossad spy 
agency in Rome in 1986 after giving The Sun-
day Times of London photographs of the in-
side of the Dimona reactor.  
(3) From the photographs, experts determined 
that Israel had the world's sixth largest stock-
pile of nuclear weapons.  
(4) Israel has never confirmed or denied that it 
has a nuclear capability. 
Figure 1: A right ordered paragraph. 
90
References  
Danushka Bollegala, Naoaki Okazaki, Mitsuru Ishi-
zuka. 2005. A machine learning approach to sen-
tence ordering for multi-document summarization 
and its evaluation. In Proceedings of the Second in-
ternational joint conference on Natural Language 
Processing (IJCNLP '05), 624-635. 
Danushka Bollegala, Naoaki Okazaki, and Mitsuru 
Ishizuka. 2010. A bottom-up approach to sentence 
ordering for multi-document summarization. Inf. 
Process. Manage. 46, 1 (January 2010), 89-109. 
John H. Holland. 1992. Adaptation in Natural and 
Artificial Systems: An Introductory Analysis with 
Applications to Biology, Control and Artificial In-
telligence. MIT Press, Cambridge, MA, USA. 
Lawrence Davis. 1985. Applying adaptive algorithms 
to epistatic domains. In Proceedings of the 9th in-
ternational joint conference on Artificial intelli-
gence - Volume 1 (IJCAI'85), Aravind Joshi (Ed.), 
Vol. 1. Morgan Kaufmann Publishers Inc., San 
Francisco, CA, USA, 162-164. 
Mirella Lapata. 2003. Probabilistic text structuring: 
experiments with sentence ordering. InProceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics - Volume 1(ACL '03), 
Vol. 1. Association for Computational Linguistics, 
Stroudsburg, PA, USA, 545-552.  
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishi-
zuka. 2004. Improving chronological sentence or-
dering by precedence relation. In Proceedings of 
the 20th international conference on Computa-
tional Linguistics (COLING '04). Association for 
Computational Linguistics, Stroudsburg, PA, 
USA, , Article 750 . 
Nitin Madnani, Rebecca Passonneau, Necip Fazil 
Ayan, John M. Conroy, Bonnie J. Dorr, Judith L. 
Klavans, Dianne P. O'Leary, and Judith D. Schle-
singer. 2007. Measuring variability in sentence or-
dering for news summarization. In Proceedings of 
the Eleventh European Workshop on Natural Lan-
guage Generation (ENLG '07), Stephan Busemann 
(Ed.). Association for Computational Linguistics, 
Stroudsburg, PA, USA, 81-88. 
Paul D. Ji and Stephen Pulman. 2006. Sentence order-
ing with manifold-based classification in multi-
document summarization. In Proceedings of the 
2006 Conference on Empirical Methods in Natural 
Language Processing (EMNLP '06). Association 
for Computational Linguistics, Stroudsburg, PA, 
USA, 526-533. 
Regina Barzilay, Noemie Elhadad, and Kathleen 
McKeown. 2002. Inferring strategies for sentence 
ordering in multidocument news summarization. 
Journal of Artificial Intelligence Research, 17:35?
55. 
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic content models, with applica-
tions to generation and summarization. In HLT-
NAACL2004: Proceedings of the Main Conference, 
pages 113?120. 
Renxian Zhang, Wenjie Li, and Qin Lu. 2010. Sen-
tence ordering with event-enriched semantics and 
two-layered clustering for multi-document news 
summarization. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics: 
Posters (COLING '10). Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, 1489-
1497. 
Thade Nahnsen. 2009. Domain-independent shallow 
sentence ordering. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference 
of the North American Chapter of the Association 
for Computational Linguistics, Companion Volume: 
Student Research Workshop and Doctoral Consor-
tium (SRWS '09). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 78-83. 
Thorsten Joachims. 2002. Optimizing search engines 
using click through data. In Proceedings of the 
eighth ACM SIGKDD international conference on 
Knowledge discovery and data mining (KDD '02). 
ACM, New York, NY, USA, 133-142. 
Thorsten Joachims. 2006. Training linear SVMs in 
linear time. In Proceedings of the 12th ACM 
SIGKDD international conference on Knowledge 
discovery and data mining (KDD '06). ACM, New 
York, NY, USA, 217-226. 
William W. Cohen, Robert E. Schapire, and Yoram 
Singer. 1998. Learning to order things. InProceed-
ings of the 1997 conference on Advances in neural 
information processing systems 10(NIPS '97), Mi-
chael I. Jordan, Michael J. Kearns, and Sara A. 
Solla (Eds.). MIT Press, Cambridge, MA, USA, 
451-457. 
Yanxiang He, Dexi Liu, Hua Yang, Donghong Ji, 
Chong Teng, and Wenqing Qi. 2006. A hybrid sen-
tence ordering strategy in multi-document summa-
rization. In Proceedings of the 7th international 
conference on Web Information Systems (WISE'06), 
Karl Aberer, Zhiyong Peng, Elke A. Rundensteiner, 
Yanchun Zhang, and Xuhui Li (Eds.). Springer-
Verlag, Berlin, Heidelberg, 339-349. 
Yu Nie, Donghong Ji, and Lingpeng Yang. 2006. An 
adjacency model for sentence ordering in multi-
document summarization. In Proceedings of the 
Third Asia conference on Information Retrieval 
Technology (AIRS'06), 313-322. 
91
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 526?531,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Co-Regression for Cross-Language Review Rating Prediction 
 
Xiaojun Wan 
Institute of Computer Science and Technology, The MOE Key Laboratory of 
Computational Linguistics, Peking University, Beijing 100871, China 
wanxiaojun@pku.edu.cn 
 
Abstract 
The task of review rating prediction can be 
well addressed by using regression algorithms 
if there is a reliable training set of reviews 
with human ratings. In this paper, we aim to 
investigate  a more challenging task of cross-
language review rating prediction, which 
makes use of only rated reviews in a source 
language (e.g. English) to predict the rating 
scores of unrated reviews in a target language 
(e.g. German). We propose a new co-
regression algorithm to address this task by 
leveraging unlabeled reviews.  Evaluation re-
sults on several datasets show that our pro-
posed co-regression algorithm can consistently 
improve the prediction results. 
1 Introduction 
With the development of e-commerce, more and 
more people like to buy products on the web and 
express their opinions about the products by 
writing reviews. These reviews usually contain 
valuable information for other people?s reference 
when they buy the same or similar products. In 
some applications, it is useful to categorize a re-
view into either positive or negative, but in many 
real-world scenarios, it is important to provide 
numerical ratings rather than binary decisions.  
The task of review rating prediction aims to 
automatically predict the rating scores of unrated 
product reviews. It is considered as a finer-
grained task than the binary sentiment classifica-
tion task. Review rating prediction has been 
modeled as a multi-class classification or regres-
sion task, and the regression based methods have 
shown better performance than the multi-class 
classification based methods in recent studies (Li 
et al 2011). Therefore, we focus on investigating 
regression-based methods in this study.  
Traditionally, the review rating prediction task 
has been investigated in a monolingual setting, 
which means that the training reviews with hu-
man ratings and the test reviews are in the same 
language. However, a more challenging task is to 
predict the rating scores of the reviews in a target 
language (e.g. German) by making use of the 
rated reviews in a different source language (e.g. 
English), which is called Cross-Language Re-
view Rating Prediction. Considering that the re-
sources (i.e. the rated reviews) for review rating 
prediction in different languages are imbalanced, 
it would be very useful to make use of the re-
sources in resource-rich languages to help ad-
dress the review rating prediction task in re-
source-poor languages.  
The task of cross-language review rating pre-
diction can be typically addressed by using ma-
chine translation services for review translation, 
and then applying regression methods based on 
the monolingual training and test sets. However, 
due to the poor quality of machine translation, 
the reviews translated from one language A to 
another language B are usually very different 
from the original reviews in language B, because 
the words or syntax of the translated reviews 
may be erroneous or non-native.  This phenome-
non brings great challenges for existing regres-
sion algorithms.  
In this study, we propose a new co-regression 
algorithm to address the above problem by lever-
aging unlabeled reviews in the target language.  
Our algorithm can leverage both views of the 
reviews in the source language and the target 
language to collaboratively determine the confi-
dently predicted ones out of the unlabeled re-
views, and then use the selected examples to 
enlarge the training set. Evaluation results on 
several datasets show that our proposed co-
regression algorithm can consistently improve 
the prediction results. 
2 Related Work 
Most previous works on review rating prediction 
model this problem as a multi-class classification 
task or a regression task. Various features have 
been exploited from the review text, including 
words, patterns, syntactic structure, and semantic 
topic (Qu et al 2010; Pang and Lee, 2005; Leung 
et al 2006; Ganu et al 2009). Traditional learn-
526
ing models, such as SVM, are adopted for rating 
prediction. Most recently, Li et al (2011) pro-
pose a novel tensor-based learning framework to 
incorporate reviewer and product information 
into the text based learner for rating prediction.  
Saggion et al (2012) study the use of automatic 
text summaries instead of the full reviews for 
movie review rating prediction. In addition to 
predicting the overall rating of a full review, 
multi-aspect rating prediction has also been in-
vestigated (Lu et al 2011b; Snyder and Barzilay, 
2007; Zhu et al 2009; Wang et al 2010; Lu et al 
2009; Titov and McDonald, 2008). All the above 
previous works are working under a monolingual 
setting, and to the best of our knowledge, there 
exists no previous work on cross-language re-
view rating prediction.  
    It is noteworthy that a few studies have been 
conducted for the task of cross-lingual sentiment 
classification or text classification, which aims to 
make use of labeled data in a language for the 
binary classification task in a different language 
(Mihalcea et al, 2007; Banea et al, 2008; Wan 
2009; Lu et al 2011a; Meng et al 2012; Shi et 
al., 2010; Prettenhofer and Stein 2010). However, 
the binary classification task is very different 
from the regression task studied in this paper, 
and the proposed methods in the above previous 
works cannot be directly applied.  
3 Problem Definition and Baseline Ap-
proaches 
Let L={(x1, y1), ?, (xi, yi), ?, (xn, yn)} denote the 
labeled training set of reviews in a source lan-
guage (e.g. English), where xi is the i-th review 
and yi is its real-valued label, and n is the number 
of labeled examples; Let T denote the test review 
set in a different target language (e.g. German); 
Then the task of cross-language review rating 
prediction aims at automatically predicting the 
rating scores of the reviews in T by leveraging 
the labeled reviews in L. No labeled reviews in 
the target language are allowed to be used. 
The task is a regression problem and it is chal-
lenging due to the language gap between the la-
beled training dataset and the test dataset. Fortu-
nately, due to the development of machine trans-
lation techniques, a few online machine transla-
tion services can be used for review translation. 
We adopt Google Translate1 for review transla-
tion. After review translation, the training re-
views and the test reviews are now in the same 
                                                 
1 http://translate.google.com 
language, and any regression algorithm (e.g. lo-
gistic regression, least squares regression, KNN 
regressor) can be applied for learning and predic-
tion.  In this study, without loss of generality, we 
adopt the widely used regression SVM (Vapnik 
1995; Joachims 1999) implemented in the 
SVMLight toolkit 2  as the basic regressor. For 
comparative analysis, we simply use the default 
parameter values in SVMLight with linear kernel. 
The features include all unigrams and bigrams in 
the review texts, and the value of each feature is 
simply set to its frequency (TF) in a review.  
Using features in different languages, we have 
the following baseline approaches for addressing 
the cross-language regression problem.  
REG_S:  It conducts regression learning and 
prediction in the source language.   
REG_T: It conducts regression learning and 
prediction in the target language.   
REG_ST: It conducts regression learning and 
prediction with all the features in both languages.   
REG_STC: It combines REG_S and REG_T   
by averaging their prediction values.  
However, the above regression methods do not 
perform very well due to the unsatisfactory ma-
chine translation quality and the various lan-
guage expressions. Therefore, we need to find 
new approaches to improve the above methods.  
4 Our Proposed Approach 
4.1 Overview 
Our basic idea is to make use of some amounts 
of unlabeled reviews in the target language to 
improve the regression performance. Consider-
ing that the reviews have two views in two lan-
guages and inspired by the co-training style algo-
rithms (Blum and Mitchell, 1998; Zhou and Li, 
2005), we propose a new co-training style algo-
rithm called co-regression to leverage the unla-
beled data in a collaborative way.  The proposed 
co-regression algorithm can make full use of 
both the features in the source language and the 
features in the target language in a unified 
framework similar to (Wan 2009). Each review 
has two versions in the two languages. The 
source-language features and the target-language 
features for each review are considered two re-
dundant views of the review. In the training 
phase, the co-regression algorithm is applied to 
learn two regressors in the two languages. In the 
prediction phase, the two regressors are applied 
to predict two rating scores of the review. The 
                                                 
2 http://svmlight.joachims.org 
527
final rating score of the review is the average of 
the two rating scores.  
4.2 Our Proposed Co-Regression Algorithm 
In co-training for classification, some confidently 
classified examples by one classifier are pro-
vided for the other classifier, and vice versa. 
Each of the two classifiers can improve by learn-
ing from the newly labeled examples provided 
by the other classifier. The intuition is the same 
for co-regression. However, in the classification 
scenario, the confidence value of each prediction 
can be easily obtained through consulting the 
classifier. For example, the SVM classifier pro-
vides a confidence value or probability for each 
prediction. However, in the regression scenario, 
the confidence value of each prediction is not 
provided by the regressor. So the key question is 
how to get the confidence value of each labeled 
example. In (Zhou and Li, 2005), the assumption 
is that the most confidently labeled example of a 
regressor should be with such a property, i.e. the 
error of the regressor on the labeled example set 
(i.e. the training set) should decrease the most if 
the most confidently labeled example is utilized. 
In other words, the confidence value of each la-
beled example is measured by the decrease of the 
error (e.g. mean square error) on the labeled set 
of the regressor utilizing the information pro-
vided by the example. Thus, each example in the 
unlabeled set is required to be checked by train-
ing a new regression model utilizing the example. 
However, the model training process is usually 
very time-consuming for many regression algo-
rithms, which significantly limits the use of the 
work in (Zhou and Li, 2005). Actually, in (Zhou 
and Li, 2005), only the lazy learning based KNN 
regressor is adopted. Moreover, the confidence 
of the labeled examples is assessed based only on 
the labeled example set (i.e. the training set), 
which makes the generalization ability of the 
regressor not good.  
In order to address the above problem, we 
propose a new confidence evaluation strategy 
based on the consensus of the two regressors. 
Our intuition is that if the two regressors agree 
on the prediction scores of an example very well, 
then the example is very confidently labeled. On 
the contrary, if the prediction scores of an exam-
ple by the two regressors are very different, we 
can hardly make a decision whether the example 
is confidently labeled or not. Therefore, we use 
the absolute difference value between the predic-
tion scores of the two regressors as the confi-
dence value of a labeled example, and if the ex-
ample is chosen, its final prediction score is the 
average of the two prediction scores. Based on 
this strategy, the confidently labeled examples 
can be easily and efficiently chosen from the 
unlabeled set as in the co-training algorithm, and 
these examples are then added into the labeled 
set for re-training the two regressors. 
 
Given: 
- Fsource and Ftarget are redundantly sufficient 
sets of features, where Fsource represents 
the source language features, Ftarget repre-
sents the target language features; 
- L is a set of labeled training reviews; 
- U is a set of unlabeled reviews; 
Loop for I iterations: 
1. Learn the first regressor Rsource from L 
based on Fsource; 
2. Use Rsource to label reviews from U based 
on Fsource; Let 
source
iy? denote the predic-
tion score of review xi;  
3. Learn the second classifier Rtarget from L 
based on Ftarget; 
4. Use Rtarget to label reviews from U based 
on Ftarget; Let 
ett
iy
arg? denote the predic-
tion score of review xi;  
5. Choose m most confidently predicted re-
views E={ top m reviews with the small-
est value of sourcei
ett
i yy ??
arg
? } from U, 
where the final prediction score of each 
review in E is 2?? arg sourcei
ett
i yy + ; 
6. Removes reviews E from U and add re-
views E with the corresponding predic-
tion scores to L; 
Figure 1. Our proposed co-regression algorithm 
 
Our proposed co-regression algorithm is illus-
trated in Figure 1. In the proposed co-regression 
algorithm, any regression algorithm can be used 
as the basic regressor to construct Rsource and Rtar-
get, and in this study, we adopt the same regres-
sion SVM implemented in the SVMLight toolkit 
with default parameter values. Similarly, the fea-
tures include both unigrams and bigrams and the 
feature weight is simply set to term frequency.  
There are two parameters in the algorithm: I is 
the iteration number and m is the growth size in 
each iteration. I and m can be empirically set ac-
cording to the total size of the unlabeled set U, 
and we have I?m? |U|. 
Our proposed co-regression algorithm is much 
more efficient than the COREG algorithm (Zhou 
and Li, 2005). If we consider the time-
consuming regression learning process as one  
528
(a) Target language=German & Category=books
1.1
1.12
1.14
1.16
1.18
1.2
1.22
1 10 20 30 40 50 60 70 80 90 100110120130140150
Iteration Number (I)
M
SE
 (b) Target language=German & Category=dvd
1.1
1.12
1.14
1.16
1.18
1.2
1.22
1.24
1.26
1 10 20 30 40 50 60 70 80 90 100110120130140150
Iteration Number (I)
M
SE
(c) Target language=German & Category=music
1.19
1.21
1.23
1.25
1.27
1.29
1.31
1.33
1.35
1 10 20 30 40 50 60 70 80 90 100110120130140150
Iteration Number (I)
M
SE
Rsource
Rtarget
co-regression
REG_S
REG_T
REG_ST
REG_STC
COREG
Figure 2. Comparison results vs. Iteration Number (I) (Rsource and Rtarget are the two component regressors)  
basic operation and make use of all unlabeled 
examples in U, the computational complexity of 
COREG is O(|U|+I). By contrast, the computa-
tional complexity of our proposed co-regression 
algorithm is just O(I). Since |U| is much larger 
than I, our proposed co-regression algorithm is 
much more efficient than COREG, and thus our 
proposed co-regression algorithm is more suit-
able to be used in applications with a variety of 
regression algorithms.  
Moreover, in our proposed co-regression algo-
rithm, the confidence of each prediction is de-
termined collaboratively by two regressors. The 
selection is not restricted by the training set, and 
it is very likely that a portion of good examples 
can be chosen for generalize the regressor to-
wards the test set.  
5 Empirical Evaluation 
We used the WEBIS-CLS-10 corpus3 provided 
by (Prettenhofer and Stein, 2010) for evaluation.  
It consists of Amazon product reviews for three 
product categories (i.e. books, dvds and music) 
written in different languages including English, 
German, etc. For each language-category pair 
there exist three sets of training documents, test 
documents, and unlabeled documents. The train-
ing and test sets comprise 2000 documents each, 
whereas the number of unlabeled documents var-
ies from 9000 ? 170000. The dataset is provided 
with the rating score between 1 to 5 assigned by 
users, which can be used for the review rating 
prediction task. We extracted texts from both the 
summary field and the text field to represent a 
review text. We then extracted the rating score as 
a review?s corresponding real-valued label. In 
the cross-language scenario, we regarded English 
as the source language, and regarded German as 
the target language. The experiments were con-
ducted on each product category separately. 
Without loss of generality, we sampled and used 
                                                 
3 http://www.uni-weimar.de/medien/webis/research/corpora/ 
corpus-webis-cls-10.html 
only 8000 unlabeled documents for each product 
category. We use Mean Square Error (MSE) as 
the evaluation metric, which penalizes more se-
vere errors more heavily.  
In the experiments, our proposed co-regression 
algorithm (i.e. ?co-regression?) is compared with 
the COREG algorithm in (Zhou and Li, 2005) 
and a few other baselines. For our proposed co-
regression algorithm, the growth size m is simply 
set to 50. We implemented the COREG algo-
rithm by replacing the KNN regressor with the 
regression SVM and the pool size is also set to 
50. The iteration number I varies from 1 to 150. 
The comparison results are shown in Figure 2.  
We can see that on all product categories, the 
MSE values of our co-regression algorithm and 
the two component regressors tend to decline 
over a wide range of I, which means that the se-
lected confidently labeled examples at each itera-
tion are indeed helpful to improve the regressors.  
Our proposed co-regression algorithm outper-
forms all the baselines (including COREG) over 
different iteration members, which verifies the 
effectiveness of our proposed algorithm. We can 
also see that the COREG algorithm does not per-
form well for this cross-language regression task. 
Overall, our proposed co-regression algorithm 
can consistently improve the prediction results. 
6 Conclusion and Future Work 
In this paper, we study a new task of cross-
language review rating prediction and propose a 
new co-regression algorithm to address this task. 
In future work, we will apply the proposed co-
regression algorithm to other cross-language or 
cross-domain regression problems in order to 
verify its robustness.  
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101). 
529
References 
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing, pp. 127-135. 
John Blitzer, Mark Dredze, and Fernando Pereira. 
2007. Biographies, bollywood, boom-boxes and 
blenders: Domain adaptation for sentiment classifi-
cation. In Annual Meeting-Association For Com-
putational Linguistics. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on 
Computational learning theory, pp. 92-100. 
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. 
Comparative experiments on sentiment classifica-
tion for online product reviews. In Proceedings of 
the National Conference on Artificial Intelligence. 
Gayatree Ganu, Noemie Elhadad, and Am?lie Marian. 
2009. Beyond the stars: Improving rating predic-
tions using review text content. In WebDB. 
Thorsten Joachims, 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, MIT-Press. 
CaneWing Leung, Stephen Chi Chan, and Fu Chung. 
2006. Integrating collaborative filtering and senti-
ment analysis: A rating inference approach. In 
ECAI Workshop, pages 300?307. 
Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao, 
Qiang Yang and Xiaoyan Zhu. 2011. Incorporating 
reviewer and product information for review rating 
prediction. In Proceedings of the Twenty-Second 
International Joint Conference on Artificial Intelli-
gence (IJCAI2011).  
Yue Lu, ChengXiang Zhai, Neel Sundaresan. 2009. 
Rated Aspect Summarization of Short Comments. 
Proceedings of the World Wide Conference 2009 
( WWW'09), pages 131-140. 
Bin Lu, Chenhao Tan, Claire Cardie, Ka Yin Benja-
min TSOU. 2011a. Joint bilingual sentiment classi-
fication with unlabeled parallel corpora. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human 
Language Technologies, pp. 320-330. 
Bin Lu, Myle Ott, Claire Cardie and Benjamin K. 
Tsou. 2011b. Multi-aspect sentiment analysis with 
topic models. In Proceedings of Data Minig 
Workshps (ICDMW), 2011 IEEE 11th Interna-
tional Conference on, pp. 81-88, IEEE.  
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, 
Ge Xu, and Houfeng Wang. 2012. Cross-Lingual 
Mixture Model for Sentiment Classification. In 
Proceedings of ACL-2012.  
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 
2007. Learning multilingual subjective language 
via cross-lingual projections. In Proceedings of 
ACL-2007. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? sentiment classification using 
machine learning techniques. In Proceedings of the 
ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10, pp. 79-86, 
2002. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization 
with respect to rating scales. In Proceedings of the 
ACL, pages 115?124.  
Peter Prettenhofer and Benno Stein. 2010. Cross-
Language Text Classification using Structural Cor-
respondence Learning. In 48th Annual Meeting of 
the Association of Computational Linguistics 
(ACL 10), 1118-1127. 
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In COL-
ING, pages 913?921, Stroudsburg, PA, USA, 2010. 
ACL.  
Horacio Saggion, Elena Lloret, and Manuel Palomar. 
2012. Can text summaries help predict ratings? a 
case study of movie reviews. Natural Language 
Processing and Information Systems (2012): 271-
276. 
Lei Shi, Rada Mihalcea, and Mingjun Tian. 2010. 
Cross language text classification by model transla-
tion and semi-supervised learning. In Proceedings 
of the 2010 Conference on Empirical Methods in 
Natural Language Processing, pp. 1057-1067, 2010. 
Benjamin Snyder and Regina Barzilay. 2007. Multi-
ple aspect ranking using the good grief algorithm. 
Proceedings of the Joint Human Language Tech-
nology/North American Chapter of the ACL Con-
ference (HLT-NAACL). 
Ivan Titov and Ryan McDonald. 2008. A joint model 
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL-08:HLT, pages 308-
316.  
Peter D. Turney. 2002. Thumbs up or thumbs down?: 
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational 
Linguistics, pp. 417-424. 
Vladimir N. Vapnik, 1995. The Nature of Statistical 
Learning Theory. Springer. 
Xiaojun Wan. 2009. Co-training for cross-lingual 
sentiment classification. In Proceedings of the Joint 
Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Conference on 
530
Natural Language Processing of the AFNLP, pp. 
235-243. 
Hongning Wang, Yue Lu, ChengXiang Zhai. 2010. 
Latent Aspect Rating Analysis on Review Text 
Data: A Rating Regression Approach. Proceedings 
of the 17th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining 
(KDD'10), pages 115-124. 
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and 
Muhua Zhu. 2009. Multi-aspect opinion polling 
from textual reviews. In Proceedings of the 18th 
ACM conference on Information and knowledge 
management, pp. 1799-1802. ACM. 
Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised 
regression with co-training. In Proceedings of the 
19th international joint conference on Artificial in-
telligence, pp. 908-913. Morgan Kaufmann Pub-
lishers Inc. 
531
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446?456,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Grammatical Relations in Chinese:
GB-Ground Extraction and Data-Driven Parsing
Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, Xiaojun Wan
?
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,duyantao,kouxin,wanxiaojun}@pku.edu.cn, dsy100@gmail.com
Abstract
This paper is concerned with building linguistic re-
sources and statistical parsers for deep grammatical
relation (GR) analysis of Chinese texts. A set of
linguistic rules is defined to explore implicit phrase
structural information and thus build high-quality
GR annotations that are represented as general di-
rected dependency graphs. The reliability of this
linguistically-motivated GR extraction procedure is
highlighted by manual evaluation. Based on the
converted corpus, we study transition-based, data-
driven models for GR parsing. We present a novel
transition system which suits GR graphs better than
existing systems. The key idea is to introduce a new
type of transition that reorders top k elements in the
memory module. Evaluation gauges how successful
GR parsing for Chinese can be by applying data-
driven models.
1 Introduction
Grammatical relations (GRs) represent functional
relationships between language units in a sen-
tence. They are exemplified in traditional gram-
mars by the notions of subject, direct/indirect
object, etc. GRs have assumed an important
role in linguistic theorizing, within a variety of
approaches ranging from generative grammar to
functional theories. For example, several com-
putational grammar formalisms, such as Lexi-
cal Function Grammar (LFG; Bresnan and Ka-
plan, 1982; Dalrymple, 2001) and Head-driven
Phrase Structure Grammar (HPSG; Pollard and
Sag, 1994) encode grammatical functions directly.
In particular, GRs can be viewed as the depen-
dency backbone of an LFG analysis that provide
general linguistic insights, and have great potential
advantages for NLP applications, (Kaplan et al,
2004; Briscoe and Carroll, 2006; Clark and Cur-
ran, 2007a; Miyao et al, 2007).
?
Email correspondence.
In this paper, we address the question of an-
alyzing Chinese sentences with deep GRs. To
acquire high-quality GR corpus, we propose a
linguistically-motivated algorithm to translate a
Government and Binding (GB; Chomsky, 1981;
Carnie, 2007) grounded phrase structure treebank,
i.e. Chinese Treebank (CTB; Xue et al, 2005)
to a deep dependency bank where GRs are ex-
plicitly represented. Different from popular shal-
low dependency parsing that focus on tree-shaped
structures, our GR annotations are represented as
general directed graphs that express not only lo-
cal but also various long-distance dependencies,
such as coordinations, control/raising construc-
tions, topicalization, relative clauses and many
other complicated linguistic phenomena that goes
beyond shallow syntax (see Fig. 1 for example.).
Manual evaluation highlights the reliability of our
linguistically-motivated GR extraction algorithm:
The overall dependency-based precision and recall
are 99.17 and 98.87. The automatically-converted
corpus would be of use for a wide variety of NLP
tasks.
Recent years have seen the introduction of a
number of treebank-guided statistical parsers ca-
pable of generating considerably accurate parses
for Chinese. With the high-quality GR resource
at hand, we study data-driven GR parsing. Previ-
ous work on dependency parsing mainly focused
on structures that can be represented in terms of
directed trees. We notice two exceptions. Sagae
and Tsujii (2008) and Titov et al (2009) individ-
ually studied two transition systems that can gen-
erate more general graphs rather than trees. In-
spired by their work, we study transition-based
models for building deep dependency structures.
The existence of a large number of crossing arcs in
GR graphs makes left-to-right, incremental graph
spanning computationally hard. Applied to our
data, the two existing systems cover only 51.0%
and 76.5% GR graphs respectively. To better suit
446
?? ?? ? ?? ?? ? ?? ?? ?? ? ??? ??
Pudong recently issue practice involve economic field regulatory document
root root
comp
temp
temp
subj
subj
prt
prt
obj
obj
comp
subj*ldd
obj
nmod
relative
nmod
Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.
The symbol ?*ldd? indicates long-distance dependencies; ?subj*ldd? between the word ???/involve?
and the word ???/documents? represents a long-range subject-predicate relation. The arguments and
adjuncts of the coordinated verbs, namely ???/issue? and ???/practice,? are separately yet distribu-
tively linked the two heads.
our problem, we extend Titov et al?s work and
study what we call K-permutation transition sys-
tem. The key idea is to introduce a new type of
transition that reorders top k (2 ? k ? K) el-
ements in the memory module of a stack-based
transition system. With the increase of K, the ex-
pressiveness of the corresponding system strictly
increases. We propose an oracle deriving method
which is guaranteed to find a sound transition se-
quence if one exits. Moreover, we introduce an
effective approximation of that oracle, which de-
creases decoding ambiguity but practically covers
almost exactly the same graphs for our data.
Based on the stronger transition system, we
build a GR parser with a discriminative model for
disambiguation and a beam decoder for inference.
We conduct experiments on CTB 6.0 to profile this
parser. With the increase of the K, the parser is
able to utilize more GR graphs for training and
the numeric performance is improved. Evaluation
gauges how successful GR parsing for Chinese
can be by applying data-driven models. Detailed
analysis reveal some important factors that may
possibly boost the performance. To our knowl-
edge, this work provides the first result of exten-
sive experiments of parsing Chinese with GRs.
We release our GR processing kit and gold-
standard annotations for research purposes. These
resources can be downloaded at http://www.
icst.pku.edu.cn/lcwm/omg.
2 GB-grounded GR Extraction
In this section, we discuss the construction of the
GR annotations. Basically, the annotations are au-
tomatically converted from a GB-grounded phrase-
structure treebank, namely CTB. Conceptually,
this conversion is similar to the conversions from
CTB structures to representations in deep gram-
mar formalisms (Tse and Curran, 2010; Yu et al,
2010; Guo et al, 2007; Xia, 2001). However, our
work is grounded in GB, which is the linguistic ba-
sis of the construction of CTB. We argue that this
theoretical choice makes the conversion process
more compatible with the original annotations and
therefore more accurate. We use directed graphs to
explicitly encode bi-lexical dependencies involved
in coordination, raising/control constructions, ex-
traction, topicalization, and many other compli-
cated phenomena. Fig. 1 shows an example of
such a GR graph and its original CTB annotation.
2.1 Linguistic Basis
GRs are encoded in different ways in different lan-
guages. In some languages, e.g. Turkish, gram-
matical function is encoded by means of morpho-
logical marking, while in highly configurational
languages, e.g. Chinese, the grammatical function
of a phrase is heavily determined by its constituent
structure position. Dominant Chomskyan theo-
ries, including GB, have defined GRs as configu-
rations at phrase structures. Following this princi-
ple, CTB groups words into constituents through
the use of a limited set of fundamental grammat-
ical functions. Transformational grammar utilizes
empty categories (ECs) to represent long-distance
dependencies. In CTB, traces are provided by
relating displaced linguistic material to where it
should be interpreted semantically. By exploiting
configurational information, traces and functional
tag annotations, GR information can be hopefully
447
IP
VP
?=?
VP
?=?
NP
?=(? OBJ)
NP
?=?
NN
?=?
??
NN
??(? NMOD)
???
CP
??(? REL)
DEC
?=?
?
IP
?=(? COMP)
VP
?=?
NP
?=(? OBJ)
NN
?=?
??
NP
??(? NMOD)
NN
?=?
??
VV
?=?
??
NP
?=(? SBJ)
-NONE-
*T*
AS
?=(? PRT)
?
VCD
?=?
VV
???
??
VV
???
??
LCP
??(? TMP)
LC
?=?
?
NP
?=(? COMP)
NT
?=?
??
NP
?=(? SBJ)
NR
??
Figure 2: The original CTB annotation augmented with LFG-like f-structure annotations of the running
example.
derived from CTB trees with high accuracy.
2.2 The Extraction Algorithm
Our treebank conversion algorithm borrows
key insights from Lexical Functional Grammar
(LFG; Bresnan and Kaplan, 1982; Dalrymple,
2001). LFG posits two levels of representation:
c(onstituent)-structure and f(unctional)-structure
minimally. C-structure is represented by phrase-
structure trees, and captures surface syntactic con-
figurations such as word order, while f-structure
encodes grammatical functions. It is easy to ex-
tract a dependency backbone which approximates
basic predicate-argument-adjunct structures from
f-structures. The construction of the widely used
PARC DepBank (King et al, 2003) is a good ex-
ample.
LFG relates c-structure and f-structure through
f-structure annotations, which compositionally
map every constituent to a corresponding f-
structure. Borrowing this key idea, we translate
CTB trees to dependency graphs by first augment-
ing each constituency with f-structure annotations,
then propagating the head words of the head or
conjunct daughter(s) upwards to their parents, and
finally creating a dependency graph. The follow-
ing presents details step-by-step.
Tapping implicit information. Xue (2007) in-
troduced a systematic study to tap the implicit
functional information of CTB. This gives us a
very good start to extract GRs. We slightly modify
their method to enrich a CTB tree with f-structure
annotations: Each node in a resulting tree is anno-
tated with one and only one corresponding equa-
tion. See Fig. 2 for example. Comparing the orig-
inal annotation and enriched one, we can see that
the functionality of this step is to explicitly repre-
sent and regulate grammatical functions.
Beyond CTB annotations: tracing more. Nat-
ural languages do not always interpret linguistic
material locally. In order to obtain accurate and
complete GR, predicate-argument, or logical form
representations, a hallmark of deep grammars is
that they usually involve a non-local dependency
resolution mechanism. CTB trees utilize ECs and
coindexed materials to represent long-distance de-
pendencies. An EC is a nominal element that does
not have any phonological content and is therefore
unpronounced. Two kinds of anaphoric ECs, i.e.
big PRO and trace, are annotated in CTB. Theo-
retically speaking, only trace is generated as the
result of movement and therefore annotated with
antecedents in CTB. We carefully check the anno-
tation and find that a considerable amount of an-
tecedents are not labeled, and hence a lot of impor-
448
VP{??,??}
VP{??,??}
NP{??}
NP{??}CP{?}
DEC{?}IP{??}
VP{??}NP{??*ldd}
AS{?}VP{??,??}
LCP{?}
Figure 3: An example of lexicalized tree after
head word upward passing. Only partial result is
shown. The long-distance dependency between
???/involve? and ???/document? is created
through copying the dependent to a coindexed
anaphoric EC position.
tant non-local information is missing. In addition,
since the big PRO is also anaphoric, it is possible
to find coindexed components sometimes. Such
non-local information is also very valuable.
Beyond CTB annotations, we introduce a num-
ber of phrase-structure patterns to extract more
non-local dependencies. The method heavily
leverages linguistic rules to exploit structural in-
formation. We take into account both theoreti-
cal assumptions and analyzing practices to enrich
coindexation information according to phrase-
structure patterns. In particular, we try to link
an anaphoric EC e with its c-commonders if no
non-empty antecedent has already been coindexed
with e. Because the CTB is influenced deeply by
the X-bar syntax, which regulates constituent anal-
ysis much, the number of our linguistic rules is
quite modest. For the development of conversion
rules, we used the first 9 files of CTB, which con-
tains about 100 sentences. Readers can refer to
the well-documented Perl script for details. See
Fig. 2 for example. The noun phrase ????
??/regulatory documents? is related to the trace
?*T*.? This coindexation is not labeled by the
original annotation.
Passing head words and linking ECs. Based
on an enriched tree, our algorithm propagates the
head word of the head daughter upwards to their
parents, linking coindexed units, and finally creat-
ing a GR graph. The partial result after head word
passing of the running example is shown in Fig. 3.
There are two differences of the head word passing
between our GR extraction and a ?normal? depen-
dency tree extraction. First, the GR extraction pro-
cedure may pass multiple head words to its parent,
especially in a coordination construction. Second,
Precision Recall F-score
Unlabeled 99.48 99.17 99.32
Labeled 99.17 98.87 99.02
Table 1: Manual evaluation of 209 sentences.
long-distance dependencies are created by linking
ECs and their coindexed phrases.
2.3 Manual Evaluation
To have a precise understanding of whether our ex-
traction algorithm works well, we have selected 20
files that contains 209 sentences in total for man-
ual evaluation. Linguistic experts carefully exam-
ine the corresponding GR graphs derived by our
extraction algorithm and correct all errors. In other
words, a gold standard GR annotation set is cre-
ated. The measure for comparing two dependency
graphs is precision/recall of GR tokens which are
defined as ?w
h
, w
d
, l? tuples, wherew
h
is the head,
w
d
is the dependent and l is the relation. Labeled
precision/recall (LP/LR) is the ratio of tuples cor-
rectly identified by the automatic generator, while
unlabeled precision/recall (UP/UR) is the ratio re-
gardless of l. F-score is a harmonic mean of pre-
cision and recall. These measures correspond to
attachment scores (LAS/UAS) in dependency tree
parsing. To evaluate our GR parsing models that
will be introduced later, we also report these met-
rics.
The overall performance is summarized in Tab.
1. We can see that the automatical GR extraction
achieves relatively high performance. There are
two sources of errors in treebank conversion: (1)
inadequate conversion rules and (2) wrong or in-
consistent original annotations. During the cre-
ation of the gold standard corpus, we find that
the former is mainly caused by complicated un-
bounded dependencies and the lack of internal
structure for some kinds of phrases. Such prob-
lems are very hard to solve through rules only, if
not possible, since original annotations do not pro-
vide sufficient information. The latter problem is
more scattered and unpredictable.
2.4 Statistics
Allowing non-projective dependencies generally
makes parsing either by graph-based or transition-
based dependency parsing harder. Substantial re-
search effort has been devoted in recent years to
the design of elegant solutions for this problem.
There are much more crossing arcs in the GR
449
graphs than syntactic dependency trees. In the
training data (defined in Section 4.1), there are
558132 arcs and 86534 crossing pairs, About half
of the sentences have crossing arcs (10930 out of
22277). The wide existence of crossing arcs poses
an essential challenge for GR parsing, namely, to
find methods for handling crossing arcs without a
significant loss in accuracy and efficiency.
3 Transition-based GR Parsing
The availability of large-scale treebanks has con-
tributed to the blossoming of statistical approaches
to build accurate shallow constituency and depen-
dency parsers. With high-quality GR resources at
hand, it is possible to study statistical approaches
to automatically parse GR graphs. In this section,
we investigate the feasibility of applying a data-
driven, grammar-free approach to build GRs di-
rectly. In particular, transition-based dependency
parsing method is studied.
3.1 Data-Driven Dependency Parsing
Data-driven, grammar-free dependency parsing
has received an increasing amount of attention in
the past decade. Such approaches, e.g. transition-
based (Yamada and Matsumoto, 2003; Nivre,
2008) and graph-based (McDonald, 2006; Tor-
res Martins et al, 2009) models have attracted
the most attention of dependency parsing in re-
cent years. Transition-based parsers utilize tran-
sition systems to derive dependency trees together
with treebank-induced statistical models for pre-
dicting transitions. This approach was pioneered
by (Yamada and Matsumoto, 2003) and (Nivre
et al, 2004). Most research concentrated on sur-
face syntactic structures, and the majority of ex-
isting approaches are limited to producing only
trees. We notice two exceptions. Sagae and Tsu-
jii (2008) and Titov et al (2009) individually in-
troduced two transition systems that can generate
specific graphs rather than trees. Inspired by their
work, we study transition-based approach to build
GR graphs.
3.2 Transition Systems
Following (Nivre, 2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, c
s
, C
t
), where
1. C is a set of configurations, each of which
contains a buffer ? of (remaining) words and
a set A of dependency arcs,
Transitions
SHIFT (?, j|?,A)? (?|j, ?,A)
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
POP (?|i, ?, A)? (?, ?,A)
ROTATE
k
(?|i
k
| . . . |i
2
|i
1
, ?, A)? (?|i
1
|i
k
| . . . |i
2
, ?, A)
Table 2: K-permutation System.
2. T is a set of transitions, each of which is a
(partial) function t : C 7? C,
3. c
s
is an initialization function, mapping a
sentence x to a configuration, with ? =
[1, . . . , n],
4. C
t
? C is a set of terminal configurations.
Given a sentence x = w
1
, . . . , w
n
and a graph
G = (V,A) on it, if there is a sequence of tran-
sitions t
1
, . . . , t
m
and a sequence of configura-
tions c
0
, . . . , c
m
such that c
0
= c
s
(x), t
i
(c
i?1
) =
c
i
(i = 1, . . . ,m), c
m
? C
t
, and A
c
m
= A, we say
the sequence of transitions is an oracle sequence.
And we define
?
A
c
i
= A ? A
c
i
for the arcs to be
built in c
i
. In a typical transition-based parsing
process, the input words are put into a queue and
partially built structures are organized by a stack.
A set of SHIFT/REDUCE actions are performed se-
quentially to consume words from the queue and
update the partial parsing results.
3.3 Online Reordering
Among existing systems, Sagae and Tsujii?s is de-
signed for projective graphs (denoted by G
1
in
Definition 1), and Titov et al?s handles only a
specific subset of non-projective graphs as well
as projective graphs (G
2
). Applied to our data,
only 51.0% and 76.5% of the extracted graphs are
parsable with their systems. Obviously, it is nec-
essary to investigate new transition systems for the
parsing task in our study. To deal with crossing
arcs, Titov et al (2009) and Nivre (2009) designed
a SWAP transition that switches the position of the
two topmost nodes on the stack. Inspired by their
work, we extend this approach to parse more gen-
eral graphs. The basic idea is to provide our new
system with an ability to reorder more nodes dur-
ing decoding in an online fashion, which we refer
to as online reordering.
3.4 K-Permutation System
We define a K-permutation transition system
S
K
= (C, T, c
s
, C
t
), where a configuration c =
450
(?, ?,A) ? C contains a stack ? of nodes be-
sides ? and A. We set the initial configuration
for a sentence x = w
1
, . . . , w
n
to be c
s
(x) =
([], [1, . . . , n], {}), and take C
t
to be the set of all
configurations of the form c
t
= (?, [], A) (for any
arc set A). The set of transitions T contains five
types of actions, as shown in Tab. 2:
1. SHIFT removes the front element from ? and
pushes it onto ?.
2. LEFT-ARC
l
/RIGHT-ARC
l
updates a configu-
ration by adding (i, l, j)/(j, l, i) to A where i
is the top of ?, and j is the front of ?.
3. POP deletes the top element of ?.
4. ROTATE
k
updates a configuration with stack
?|i
k
| . . . |i
2
|i
1
by rotating the top k nodes
in stack left by one index, obtaining
?|i
1
|i
k
| . . . |i
2
, with constraint 2 ? k ? K.
We refer to this system as K-permutation because
by rotating the top k (2 ? k ? K) nodes in the
stack, we can obtain all the permutations of the
top K nodes. Note that S
2
is identical to Titov
et al?s; S
?
is complete with respect to the class of
all directed graphs without self-loop, since we can
arbitrarily permute the nodes in the stack. The K-
permutation system exhibits a nice property: The
sets of corresponding graphs are strictly mono-
tonic with respect to the ? operation.
Definition 1. If a graphG can be parsed with tran-
sition system S
K
, we say G is a K-perm graph.
We use G
K
to denote the set of all k-perm graphs.
Specially, G
0
= ?, G
1
is the set of all projective
graphs, and G
?
=
?
?
k=0
G
k
.
Theorem 1. G
i
( G
i+1
,?i ? 0.
Proof. It is obvious that G
i
? G
i+1
and G
0
( G
1
.
Fig. 4 gives an example which is in G
i+1
but not
in G
i
for all i > 0, indicating G
i
6= G
i+1
.
Theorem 2. G
?
is the set of all graphs without
self-loop.
Proof. It follows immediately from the fact that
G ? G
|V |
, ?G = ?V,E?.
The transition systems introduced in (Sagae and
Tsujii, 2008) and (Titov et al, 2009) can be viewed
as S
1
1
and S
2
.
1
Though Sagae and Tsujii (2008) introduced additional
constraints to exclude cyclic path, the fundamental transition
mechanism of their system is the same to S
1
.
w
1
? ? ? w
i
w
i+1
? ? ? w
2i
w
2i+1
w
2i+2
Figure 4: A graph which is in G
i+1
, but not in G
i
.
3.5 Normal Form Oracle
The K-permutation transition system may allow
multiple oracle transition sequences on one graph,
but trying to sum all the possible oracles is usu-
ally computational expensive. Here we give a con-
struction procedure which is guaranteed to find an
oracle sequence if one exits. We refer it as normal
form oracle (NFO).
Let L(j) be the ordered list of nodes connected
to j in
?
A
c
i?1
for j ? ?
c
i?1
, and let L
K
(?
c
i?1
) =
[L(j
1
), . . . , L(j
max{l,K})
]. If ?
c
i?1
is empty, then
we set t
i
to SHIFT; if there is no arc linked to
j
1
in
?
A
c
i?1
, then we set t
i
to POP; if there exits
a ?
?
A
c
i?1
linking j
1
and b, then we set t
i
to LEFT-
ARC or RIGHT-ARC correspondingly. When there
are only SHIFT and ROTATE left, we first apply
a sequence of ROTATE?s to make L
K
(?) com-
plete ordered by lexicographical order, then apply
a SHIFT. Let c
i
= t
i
(c
i?1
), we continue to com-
pute t
i+1
, until ?
c
i
is empty.
Theorem 3. If a graph is parsable with the transi-
tion system S
K
then the construction procedure is
guaranteed to find an oracle transition sequence.
Proof. During the construction, all the arcs are
built by LEFT-ARC or RIGHT-ARC, which links
the top of the stack and the front of the buffer.
Therefore, we prefer L(?) to be as orderly as pos-
sible, to make the words to be linked sooner on the
top of the stack. the construction procedure above
does best within the power of the system S
K
.
3.6 An Approximation for NFO
In the construction of NFO transitions, we ex-
haustively use the ROTATE?s to make L(?) com-
plete ordered. We also observed that the tran-
sition LEFT-ARC, RIGHT-ARC and SHIFT only
change the relative order between the first element
of L(?) and the rest elements. Therefore we ex-
plored an approximate procedure to determine the
ROTATE?s, based on the observation. We call it ap-
proximate NFO (ANFO). Using notation defined
in Section 3.5, the approximate procedure goes as
follows. When it comes to the determination of
451
w1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
Figure 5: A graph that can be parsed
with S
3
with a transition sequence
SSSSR
3
SR
3
APAPR
2
R
3
SR
3
SR
3
APAPAPAPAP,
where S stands for SHIFT, R for ROTATE, A for
LEFT-ARC, and P for POP. But the approximate
procedure fails to find the oracle, since R
2
R
3
in
bold in the sequence are not to be applied.
the ROTATE sequence, let k be the largest m such
that 0 ? m ? min{K, l} and L(j
m
) strictly pre-
cedes L(j
1
) by the lexicographical order (here we
assume L(j
0
) strictly precedes any L(j), j ? ?).
If k > 0, we set t
i
to ROTATE
k
; else we set t
i
to
SHIFT. The approximation assumes L(?) is com-
pletely ordered except the first element, and insert
the first element to its proper place each time.
Definition 2. We define
?
G
K
as the graphs the ora-
cle of which can be extracted by S
K
with the ap-
proximation procedure.
It can be inferred similarly that Theorem 1 and
Theorem 2 also hold for
?
G?s. However, the
?
G
K
is
not equal to G
K
in non-trivial cases.
Theorem 4.
?
G
i
( G
i
,?i ? 3.
Proof. It is trivial that
?
G
i
? G
i
. An example graph
that is in G
3
but not in
?
G
3
is shown in Figure 5,
examples for arbitrary i > 3 can be constructed
similarly.
The above theorem indicates the inadequacy of
the ANFO deriving procedure. Nevertheless, em-
pirical evaluation (Section 4.2) shows that the cov-
erage of AFO and ANFO deriving procedures are
almost identical when applying to linguistic data.
3.7 Statistical Parsing
When we parse a sentence w
1
w
2
? ? ?w
n
, we start
with the initial configuration c
0
= c
s
(x), and
choose next transition t
i
= C(c
i?1
) iteratively ac-
cording to a discriminative classifier trained on or-
acle sequences. To build a parser, we use a struc-
tured classifier to approximate the oracle, and ap-
ply the Passive-Aggressive (PA) algorithm (Cram-
mer et al, 2006) for parameter estimation. The
PA algorithm is similar to the Perceptron algo-
rithm, the difference from which is the update of
weight vector. We also use parameter averaging
and early update to achieve better training. Devel-
oping features has been shown crucial to advanc-
ing the state-of-the-art in dependency tree parsing
(Koo and Collins, 2010; Zhang and Nivre, 2011).
To build accurate deep dependency parsers, we
utilize a large set of features for disambiguation.
See the notes included in the supplymentary ma-
terial for details. To improve the performance, we
also apply the technique of beam search, which
keep a beam of transition sequences with highest
scores when parsing.
4 Experiments
4.1 Experimental setup
CTB is a segmented, part-of-speech (POS) tagged,
and fully bracketed corpus in the constituency for-
malism, and very popular to evaluate fundamen-
tal NLP tasks, including word segmentation (Sun
and Xu, 2011), POS tagging (Sun and Uszkoreit,
2012), and syntactic parsing (Zhang and Clark,
2009; Sun and Wan, 2013). We use CTB 6.0 and
define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. We use
gold-standard word segmentation and POS tag-
ing results as inputs. All transition-based parsing
models are trained with beam 16 and iteration 30.
Overall precision/recall/f-score with respect to de-
pendency tokens is reported. To evaluate the abil-
ity to recover non-local dependencies, the recall of
such dependencies are reported too.
4.2 Coverage and Accuracy
There is a dual effect of the increase of the param-
eter k to our transition-based dependency parser.
On one hand, the higher k is, the more expres-
sivity the corresponding transition system has. A
system with higher k covers more structures and
allows to use more data for training. On the other
hand, higher k brings more ambiguities to the cor-
responding parser, and the parsing performance
may thus suffer. Note that the ambiguity exists not
only in each step for transition decision, but also
in selecting the training oracle.
The left-most columns of Tab. 3 shows the cov-
erage of K-permutation transition system with re-
spect to different K and different oracle deriving
algorithms. Readers may be surprised that the
coverage of NFO and ANFO deriving procedures
is the same. Actually, all the covered graphs by
the two oracle deriving procedures are exactly the
452
System NFO ANFO UP UR UF LP LR LF UR
L
LR
L
UR
NL
LR
NL
S
2
76.5 76.5 85.88 81.00 83.37 83.98 79.21 81.53 81.93 80.34 58.88 52.17
S
3
89.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28
S
4
95.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77
S
5
98.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30
Table 3: Coverage and accuracy of the GR parser on the development data.
same, except for S
3
. Only 1 from 22277 sen-
tences can find a NFO but not an ANFO. This
number demonstrates the effectiveness of ANFO.
In the following experiments, we use the ANFO?s
to train our parser.
Applied to our data, S
2
, i.e. the exact system in-
troduced by Titov et al (2009), only covers 76.5%
GR graphs. This is very different from the re-
sult obtained on the CoNLL shared task data for
English semantic role labeling (SRL). According
to (Titov et al, 2009), 99% semantic-role-labelled
graphs can be generated by S
2
. We think there are
two main reasons accounting for the differences,
and highlight the importance of the expressiveness
of transition systems to solve deep dependency
parsing problems. First, the SRL task only focuses
on finding arguments and adjuncts of verbal (and
nominal) predicates, while dependencies headed
by other words are not contained in its graph rep-
resentation. On contrast, a deep dependency struc-
ture, like GR graph, approximates deep syntactic
or semantic information of a sentence as a whole,
and therefore is more dense. As a result, permuta-
tion system with a very low k is incapable to han-
dle more cases. Another reason is about the Chi-
nese language. Some language-specific properties
result in complex crossing arcs. For example, se-
rial verb constructions are widely used in Chinese
to describe several separate events without con-
junctions. The verbal heads in such constructions
share subjects and adjuncts, both of which are be-
fore the heads. The distributive dependencies be-
tween verbal heads and subjects/adjuncts usually
produce crossing arcs (see Fig. 6). To test our as-
sumption, we evaluate the coverage of S
2
over the
functor-argument dependency graphs provided by
the English and Chinese CCGBank (Hockenmaier
and Steedman, 2007; Tse and Curran, 2010). The
result is 96.9% vs. 89.0%, which confirms our
linguistic intuition under another grammar formal-
ism.
Tab. 3 summarizes the performance of the
transition-based parser with different configura-
tions to reveal how well data-driven parsing can
subject adjunct verb
1
verb
2
Figure 6: A simplified example to illustrate cross-
ing arcs in serial verbal constructions.
be performed in realistic situations. We can see
that with the increase of K, the overall parsing ac-
curacy incrementally goes up. The high complex-
ity of Chinese deep dependency structures demon-
strates the importance of the expressiveness of a
transition system, while the improved numeric ac-
curacies practically certify the benefits. The two
points merit further exploration to more expressive
transition systems for deep dependency parsing, at
least for Chinese. The labeled evaluation scores
on the final test data are presented in Tab. 4.
Test UP UR UF LR
L
LR
NL
S
5
83.93 79.82 81.82 80.94 54.38
Table 4: Performance on the test data.
4.3 Precision vs. Recall
A noteworthy thing about the overall performance
is that the precision is promising but the recall is
too low behind. This difference is consistent with
the result obtained by a shift-reduce CCG parser
(Zhang and Clark, 2011). The functor-argument
dependencies generated by that parser also has a
relatively high precision but considerably low re-
call. There are two similarities between our parser
and theirs: 1) both parsers produce dependency
graphs rather trees; 2) both parser employ a beam
decoder that does not guarantee global optimality.
To build NLP application, e.g. information extrac-
tion, systems upon GR parsing, such property mer-
its attention. A good trade-off between the preci-
sion and the recall may have a great impact on final
results.
453
4.4 Local vs. Non-local
Although the micro accuracy of all dependencies
are considerably good, the ability of current state-
of-the-art statistical parsers to find difficult non-
local materials is far from satisfactory, even for
English (Rimell et al, 2009; Bender et al, 2011).
We report the accuracy in terms of local and non-
local dependencies respectively to show the diffi-
culty of the recovery of non-local dependencies.
The last four columns of Tab. 3 demonstrates the
labeled/unlabeled recall of local (UR
L
/LR
L
) and
non-local dependencies (UR
NL
/LR
NL
). We can
clearly see that non-local dependency recovery is
extremely difficult for Chinese parsing.
4.5 Deep vs. Deep
CCG and HPSG parsers also favor the dependency-
based metrics for evaluation (Clark and Curran,
2007b; Miyao and Tsujii, 2008). Previous work
on Chinese CCG and HPSG parsing unanimously
agrees that obtaining the deep analysis of Chinese
is more challenging (Yu et al, 2011; Tse and Cur-
ran, 2012). The successful C&C and Enju parsers
provide very inaccurate results for Chinese texts.
Though the numbers profiling the qualities of deep
dependency structures under different formalisms
are not directly comparable, all empirical eval-
uation indicates that the state-of-the-art of deep
linguistic processing for Chinese lag behind very
much.
5 Related Work
Wide-coverage in-depth and accurate linguistic
processing is desirable for many practical NLP ap-
plications, such as machine translation (Wu et al,
2010) and information extraction (Miyao et al,
2008). Parsing in deep formalisms, e.g. CCG,
HPSG, LFG and TAG, provides valuable, richer
linguistic information, and researchers thus draw
more and more attention to it. Very recently, study
on deep linguistic processing for Chinese has been
initialized. Our work is one of them.
To quickly construct deep annotations, corpus-
driven grammar engineering has been studied.
Phrase structure trees in CTB have been semi-
automatically converted to deep derivations in the
CCG (Tse and Curran, 2010), LFG (Guo et al,
2007), TAG (Xia, 2001) and HPSG (Yu et al,
2010) formalisms. Our GR extraction work is sim-
ilar, but grounded in GB, which is more consistent
with the construction of the original annotations.
Based on converted fine-grained linguistic an-
notations, successful English deep parsers, such as
C&C (Clark and Curran, 2007b) and Enju (Miyao
and Tsujii, 2008), have been evaluated (Yu et al,
2011; Tse and Curran, 2012). We also borrow
many ideas from recent advances in deep syntac-
tic or semantic parsing for English. In particular,
Sagae and Tsujii (2008)?s and Titov et al (2009)?s
studies on transition-based deep dependency pars-
ing motivated our work very much. However, sim-
ple adoption of their systems does not resolve Chi-
nese GR parsing well because the GR graphs are
much more complicated. Our investigation on the
K-permutation transition system advances the ca-
pacity of existing methods.
6 Conclusion
Recent years witnessed rapid progress made on
deep linguistic processing for English, and ini-
tial attempts for Chinese. Our work stands in
between traditional dependency tree parsing and
deep linguistic processing. We introduced a sys-
tem for automatically extracting grammatical rela-
tions of Chinese sentences from GB phrase struc-
ture trees. The present work remedies the re-
source gap by facilitating the accurate extraction
of GR annotations from GB trees. Manual evalua-
tion demonstrate the effectiveness of our method.
With the availability of high-quality GR resources,
transition-based methods for GR parsing was stud-
ied. A new formal system, namely K-permutation
system, is well theoretically discussed and prac-
tically implemented as the core module of a deep
dependency parser. Empirical evaluation and anal-
ysis were presented to give better understanding of
the Chinese GR parsing problem. Detailed anal-
ysis reveals some important directions for future
investigation.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
References
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 397?408.
Association for Computational Linguistics, Edinburgh,
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1037.
454
J. Bresnan and R. M. Kaplan. 1982. Introduction: Grammars
as mental representations of language. In J. Bresnan, edi-
tor, The Mental Representation of Grammatical Relations,
pages xvii?lii. MIT Press, Cambridge, MA.
Ted Briscoe and John Carroll. 2006. Evaluating the ac-
curacy of an unlexicalized statistical parser on the parc
depbank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41?48. Associ-
ation for Computational Linguistics, Sydney, Australia.
URL http://www.aclweb.org/anthology/P/
P06/P06-2006.
Andrew Carnie. 2007. Syntax: A Generative Introduction.
Blackwell Publishing, Blackwell Publishing 350 Main
Street, Malden, MA 02148-5020, USA, second edition.
Noam Chomsky. 1981. Lectures on Government and Binding.
Foris Publications, Dordecht.
Stephen Clark and James Curran. 2007a. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248?255. Associa-
tion for Computational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/anthology/
P07-1032.
Stephen Clark and James R. Curran. 2007b. Wide-coverage
efficient statistical parsing with CCG and log-linear mod-
els. Comput. Linguist., 33(4):493?552. URL http://
dx.doi.org/10.1162/coli.2007.33.4.493.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551?585.
M. Dalrymple. 2001. Lexical-Functional Grammar, vol-
ume 34 of Syntax and Semantics. Academic Press, New
York.
Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007.
Treebank-based acquisition of lfg resources for Chinese.
In Proceedings of the LFG07 Conference. CSLI Publica-
tions, California, USA.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
corpus of CCG derivations and dependency structures ex-
tracted from the penn treebank. Computational Linguis-
tics, 33(3):355?396.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages 97?
104. Association for Computational Linguistics, Boston,
Massachusetts, USA.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700
dependency bank. In In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1?8.
Terry Koo and Michael Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 1?11. Association for Computational Linguistics,
Uppsala, Sweden. URL http://www.aclweb.org/
anthology/P10-1001.
Ryan McDonald. 2006. Discriminative learning and span-
ning tree algorithms for dependency parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented evalu-
ation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54. Associ-
ation for Computational Linguistics, Columbus, Ohio.
URL http://www.aclweb.org/anthology/P/
P08/P08-1006.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Ann Copestake, editor, Proceedings of
the GEAF 2007 Workshop, CSLI Studies in Computa-
tional Linguistics Online, page 21 pages. CSLI Publica-
tions. URL http://www.cs.cmu.edu/
?
sagae/
docs/geaf07miyaoetal.pdf.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Comput. Lin-
guist., 34(1):35?80. URL http://dx.doi.org/10.
1162/coli.2008.34.1.35.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Comput. Linguist., 34:513?
553. URL http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 351?
359. Association for Computational Linguistics, Sun-
tec, Singapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1040.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-
based dependency parsing. In Hwee Tou Ng and Ellen
Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Con-
ference on Computational Natural Language Learning
(CoNLL-2004), pages 49?56. Association for Computa-
tional Linguistics, Boston, Massachusetts, USA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press,
Chicago.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Un-
bounded dependency recovery for parser evaluation. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 813?821.
Association for Computational Linguistics, Singapore.
URL http://www.aclweb.org/anthology/D/
D09/D09-1085.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguistics,
pages 753?760. Coling 2008 Organizing Committee,
Manchester, UK. URL http://www.aclweb.org/
anthology/C08-1095.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradig-
matic and syntagmatic lexical relations: Towards accu-
rate Chinese part-of-speech tagging. In Proceedings of the
50th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics.
Weiwei Sun and Xiaojun Wan. 2013. Data-driven, pcfg-based
and pseudo-pcfg-based models for Chinese dependency
parsing. Transactions of the Association for Computa-
tional Linguistics (TACL).
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?979.
Association for Computational Linguistics, Edinburgh,
455
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont con-
ference on Artifical intelligence, pages 1562?1567. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, USA.
URL http://dl.acm.org/citation.cfm?id=
1661445.1661696.
Andre Torres Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350. Asso-
ciation for Computational Linguistics, Suntec, Singapore.
URL http://www.aclweb.org/anthology/P/
P09/P09-1039.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the penn Chi-
nese treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling 2010),
pages 1083?1091. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-1122.
Daniel Tse and James R. Curran. 2012. The challenges
of parsing Chinese with combinatory categorial gram-
mar. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
pages 295?304. Association for Computational Linguis-
tics, Montr?eal, Canada. URL http://www.aclweb.
org/anthology/N12-1030.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
325?334. Association for Computational Linguistics, Up-
psala, Sweden. URL http://www.aclweb.org/
anthology/P10-1034.
Fei Xia. 2001. Automatic grammar generation from two dif-
ferent perspectives. Ph.D. thesis, University of Pennsylva-
nia.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn Chinese treebank: Phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11:207?238. URL http://portal.acm.org/
citation.cfm?id=1064781.1064785.
Nianwen Xue. 2007. Tapping the implicit information for the
PS to DS conversion of the Chinese treebank. In Proceed-
ings of the Sixth International Workshop on Treebanks and
Linguistics Theories.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
The 8th International Workshop of Parsing Technologies
(IWPT2003), pages 195?206.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang,
and Junichi Tsujii. 2011. Analysis of the difficul-
ties in Chinese deep parsing. In Proceedings of the
12th International Conference on Parsing Technologies,
pages 48?57. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.org/
anthology/W11-2907.
Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki,
and Junichi Tsujii. 2010. Semi-automatically devel-
oping Chinese hpsg grammar from the penn Chinese
treebank for deep parsing. In Coling 2010: Posters,
pages 1417?1425. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-2162.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global discrim-
inative model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT?09),
pages 162?171. Association for Computational Linguis-
tics, Paris, France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 683?692.
Association for Computational Linguistics, Portland,
Oregon, USA. URL http://www.aclweb.org/
anthology/P11-1069.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies, pages 188?193. Association for Computational Lin-
guistics, Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
456
Data-driven, PCFG-based and Pseudo-PCFG-based Models for Chinese
Dependency Parsing
Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,wanxiaojun}@pku.edu.cn
Abstract
We present a comparative study of transition-,
graph- and PCFG-based models aimed at il-
luminating more precisely the likely contri-
bution of CFGs in improving Chinese depen-
dency parsing accuracy, especially by com-
bining heterogeneous models. Inspired by
the impact of a constituency grammar on de-
pendency parsing, we propose several strate-
gies to acquire pseudo CFGs only from de-
pendency annotations. Compared to linguistic
grammars learned from rich phrase-structure
treebanks, well designed pseudo grammars
achieve similar parsing accuracy and have
equivalent contributions to parser ensemble.
Moreover, pseudo grammars increase the di-
versity of base models; therefore, together
with all other models, further improve sys-
tem combination. Based on automatic POS
tagging, our final model achieves a UAS of
87.23%, resulting in a significant improve-
ment of the state of the art.
1 Introduction
Popular approaches to dependency parsing can
be divided into two classes: grammar-free and
grammar-based. Data-driven, grammar-free ap-
proaches make essential use of machine learning
from linguistic annotations in order to parse new
sentences. Such approaches, e.g. transition-based
(Nivre, 2008) and graph-based (McDonald, 2006;
Torres Martins et al, 2009) have attracted the most
attention in recent years. In contrast, grammar-
based approaches rely on linguistic grammars (in
either dependency or constituency formalisms) to
shape the search space for possible syntactic anal-
ysis. In particular, CFG-based dependency parsing
exploits a mapping between dependency and con-
stituency representations and reuses parsing algo-
rithms developed for CFG to produce dependency
structures. In previous work, data-driven, discrim-
inative approaches have been widely discussed for
Chinese dependency parsing. On the other hand,
various PCFG-based constituent parsing methods
have been applied to obtain phrase-structures as
well. With rich linguistic rules, phrase-structures of
Chinese sentences can be well transformed to their
corresponding dependency structures (Xue, 2007).
Therefore, PCFG parsers with such conversion rules
can be taken as another type of dependency parser.
We call them PCFG-based parsers, in this paper.
Explicitly defining linguistic rules to express
precisely generic grammatical regularities, a con-
stituency grammar can be applied to arrange sen-
tences into a hierarchy of nested phrases, which de-
termines constructions between larger phrases and
their smaller component phrases. This type of infor-
mation is different from, but highly related to, the
information captured by a dependency representa-
tion. A constituency grammar, thus, has great possi-
ble contributions to dependency parsing. In order
to pave the way for new and better methods, we
study the impact of CFGs on Chinese dependency
parsing. A series of empirical analysis of state-of-
the-art graph-, transition-and PCFG-based parsers is
presented to illuminate more precisely the properties
of heterogeneous models. We show that CFGs have
a great impact on dependency parsing and PCFG-
based models have complementary predictive pow-
ers to data-driven models.
System ensemble is an effective and important
technique to build more accurate parsers based on
multiple, diverse, weaker models. Exploiting differ-
301
Transactions of the Association for Computational Linguistics, 1 (2013) 301?314. Action Editor: Jason Eisner.
Submitted 6/2012; Revised 10/2012; Published 7/2013. c?2013 Association for Computational Linguistics.
ent data-driven models, e.g. transition- and graph-
based models, has received the most attention in
dependency parser ensemble (Nivre and McDon-
ald, 2008; Torres Martins et al, 2008; Sagae and
Lavie, 2006). Only a few works investigate inte-
grating data-driven and PCFG-based models (Mc-
Donald, 2006). We argue that grammars can signif-
icantly increase the diversity of base models, which
plays a central role in parser ensemble, and therefore
lead to better and more promising hybrid systems.
We introduce a general classifier enhancing tech-
nique, i.e. bootstrap aggregating (Bagging), to im-
prove dependency parsing accuracy. This technique
can be applied to enhance a single-view parser, or
to combine multiple heterogeneous parsers. Exper-
iments on the CoNLL 09 shared task data demon-
strate its effectiveness: (1) Bagging can improve in-
dividual single-view parsers, especially the PCFG-
based one; (2) Bagging is more effective than pre-
viously introduced ensemble methods to combine
multi-view parsers; (3) Integrating data-driven and
PCFG-based models is more useful than combining
different data-driven models.
Although PCFG-based models have a big con-
tribution to data-driven dependency parsing, they
have a serious limitation: There are no corre-
sponding constituency annotations for some depen-
dency treebanks, e.g. Chinese Dependency Tree-
bank (LDC2012T05). To overcome this limita-
tion, we propose several strategies to acquire pseudo
grammars only from dependency annotations. In
particular, dependency trees are converted to pseudo
constituency trees and PCFGs can be extracted from
such trees. Another motivation of this study is to in-
crease the diversity of candidate models for parser
ensemble. Experiments show that pseudo-PCFG-
based models are very competitive: (1) Pseudo
grammars achieve similar or even better parsing re-
sults than linguistic grammars learned from rich
constituency annotations; (2) Compared to linguistic
grammars, well designed, single-view pseudo gram-
mars have an equivalent contribution to parser en-
semble; (3) Combining different pseudo grammars
even work better for ensemble than linguistic gram-
mars; (4) Pseudo-PCFG-based models increase the
diversity of base models, and therefore lead to fur-
ther improvements for ensemble.
Based on automatic POS tagging, our final model
achieves a UAS of 87.23% on the CoNLL data and
84.65% on CTB5, which yield relative error reduc-
tions of 18-24% over the best published results in
the literature.
2 Background and related work
2.1 Data-driven dependency parsing
The mainstream work on recent dependency pars-
ing focuses on data-driven approaches that automat-
ically learn to produce dependency graphs for sen-
tences solely from a hand-crafted dependency tree-
bank. The advantage of such models is that they
are easily ported to any language in which labeled
linguistic resources exist. Practically all statisti-
cal models that have been proposed in recent years
can be mainly described as either graph-based or
transition-based (McDonald and Nivre, 2007). Both
models have been adopted to learn Chinese depen-
dency structures (Zhang and Clark, 2011; Zhang
and Nivre, 2011; Huang and Sagae, 2010; Hatori
et al, 2011; Li et al, 2011, 2012). According to
published results, graph-based and transition-based
parsers achieve similar accuracy.
In the graph-based framework, informative evalu-
ation results have been presented in (Li et al, 2011).
First, second and third order projective parsing mod-
els are well evaluated. In the transition-based frame-
work, two advanced techniques have been stud-
ied. First, developing features has been shown
crucial to advancing parsing accuracy and a very
rich feature set is carefully evaluated by Zhang and
Nivre (2011). Second, beyond deterministic greedy
search, principled dynamic programming strategies
can be employed to explore more possible hypothe-
ses (Huang and Sagae, 2010). Both techniques have
been examined and shown helpful for Chinese de-
pendency parsing. Furthermore, Hatori et al (2011)
combined both and obtained a state-of-the-art super-
vised parsing result.
2.2 PCFG-based dependency parsing
PCFG-based dependency parsing approaches are
based on the finding that projective dependency trees
can be transformed from constituency trees by ap-
plying rich linguistic rules. In such approaches, de-
pendency parsing can be resolved by a two-step pro-
cess: constituent parsing and rule-based extraction
302
of dependencies from phrase structures. The ad-
vantage of constituency-grammar-based approach is
that all the well-studied parsing methods for such
grammars can be used for dependency parsing as
well. Two language-specific properties essentially
make PCFG-based approaches easy to be applied
to Chinese dependency parsing: (1) Chinese gram-
maticians favor using projective structures;1 (2) Chi-
nese phrase-structure annotations normally contain
richer information and thus are reliable for tree con-
version.
2.2.1 Constituency parsing
Compared to many other languages, statistical
constituent parsing for Chinese has reached early
success, due to the fact that the language has rela-
tively fixed word order and extremely poor inflec-
tional morphology. Both facts allow PCFG-based
statistical modeling to perform well. For the con-
stituent parsing, the majority of the state-of-the-
art parsers are based on generative PCFG learn-
ing. For example, the well-known and success-
ful Collins and Charniak&Johnson parsers (Collins,
2003; Charniak, 2000; Charniak and Johnson, 2005)
implement generative lexicalized statistical models.
Apart from lexicalized PCFG parsing, unlex-
icalized parsing with latent variable grammars
(PCFGLA) can also produce comparable accuracy
(Matsuzaki et al, 2005; Petrov et al, 2006). Latent
variable grammars model an observed treebank of
coarse parse trees with a model over more refined,
but unobserved, derivation trees that represent much
more complex syntactic processes. Rather than
attempting to manually specify fine-grained cate-
gories, previous work shows that automatically in-
ducing the sub-categories from data can work quite
well. A PCFGLA parser leverages on an automatic
procedure to learn refined grammars and are there-
fore more robust to parse non-English languages that
are not well studied. For Chinese, such a parser
achieves the state-of-the-art performance and de-
feats many other types of parsers, including Collins
as well as Charniak parser (Che et al, 2012) and
1For example, as two popular dependency treebanks, the
CoNLL 2009 data and the Chinese Dependency Treebank both
excluede non-projective annotations. It is worth noting that the
former one is converted from a constituency treebank while the
latter one is directly annotated by lingusitics.
discriminative transition-based models (Zhang and
Clark, 2009).
2.2.2 CS to DS conversion
In the absence of dependency and constituency
structures for a particular treebank, treebank-guided
parser developers normally apply rich linguistic
rules to convert one representation formalism to an-
other to get necessary data to train parsers. Xue
(2007) examines the linguistic adequacy of depen-
dency structure annotation automatically converted
from phrase structure treebanks with rule-based ap-
proaches. A structural approach is introduced for
the constituency structure (CS) to dependency struc-
ture (DS) conversion for the Chinese Treebank data,
which is the basis of the CoNLL 2009 shared task
data. By applying this conversion procedure on the
outputs of an automatic phrase structure parser, we
can build a PCFG-based dependency parser.
2.3 Parser ensemble
NLP systems built on particular single views nor-
mally capture different properties of an original
problem, and therefore differ in predictive powers.
As a result, NLP systems can take advantage of com-
plementary strengths of multiple views. Combining
the outputs of several systems has been shown in the
past to improve parsing performance significantly,
including integrating phrase-structure parsers (Hen-
derson and Brill, 1999), dependency parsers (Nivre
and McDonald, 2008), or both (McDonald, 2006).
Several ensemble models have been proposed for
the parsing of syntactic constituents and dependen-
cies, including learning-based stacking (Nivre and
McDonald, 2008; Torres Martins et al, 2008) and
learning-free post-inference (Henderson and Brill,
1999; Sagae and Lavie, 2006). Surdeanu and Man-
ning (2010) present a systematic analysis of these
ensemble methods and find several non-obvious
facts:
? the diversity of base parsers is more important
than complex models for learning, and
? simplest scoring model for voting and repars-
ing performs essentially as well as other more
complex models.
303
3 A comparative analysis of heterogeneous
parsers
The information encoded in a dependency repre-
sentation is different from the information captured
in a constituency representation. While the depen-
dency structure represents head-dependent relations
between words, the constituency structure repre-
sents the grouping of words into phrases, classified
by structural categories. These differences concern
what is explicitly encoded in the respective represen-
tations, and affects data-driven and PCFG-based de-
pendency parsing models substantially. In this sec-
tion, we give a comparative analysis of transition-,
graph- and PCFG-based models aimed at illuminat-
ing more precisely the likely contribution of CFGs
in dependency parsing.
3.1 Experimental setup
Penn Chinese TreeBank (CTB) is a segmented,
POS tagged, and fully bracketed corpus in the con-
stituency formalism, and very popular to evaluate
fundamental NLP tasks, including word segmenta-
tion, POS tagging, constituent parsing as well as de-
pendency parsing. We use CTB 6 as our main corpus
and define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. To evaluate
and analyze dependency parsers, we directly use the
CoNLL data. CTB?s syntactic annotations also in-
cludes functional information and empty categories.
Modern parsers, e.g. Collins and Berkeley parsers,
ignore these types of linguistic knowledge. To train
a constituent parser, we perform a heuristic proce-
dure on the treebank data to delete function tags and
empty categories as well as its associated redundant
ancestors. Many papers reported parsing results of
an older version CTB (namely CTB 5). To compare
with systems introduced in these papers, we evaluate
our final ensemble model on CTB5 in Section 5.4.
For dependency parsing, we choose a second
order graph-based parser2 (Bohnet, 2010) and a
transition-based parser (Hatori et al, 2011), for
experiments. For constituent parsing, we choose
Berkeley parser,3 a well known implementation of
the unlexicalized PCFGLA model and Bikel parser,4
2code.google.com/p/mate-tools/
3code.google.com/p/berkeleyparser/
4cis.upenn.edu/?dbikel/software.html
a well known implementation of Collins? lexical-
ized model, for experiments. In data-driven pars-
ing, features consisting of POS tags are very effec-
tive, so typically POS tagging is performed as a pre-
processing. We use the baseline sequential tagger
described in (Sun and Uszkoreit, 2012) to provide
such lexical information to the graph-based parser.
Note that the transition-based parser performs a joint
inference to acquire POS and dependency informa-
tion simultaneously, so there is no need to offer extra
tagging results to it.
3.2 Overall performance
Table 1 (Column 2-6) summarizes the overall accu-
racy of different parsers. Two transition-based pars-
ing results are presented: The first one employ a
simple feature set (Zhang and Clark, 2008) and a
small beam (16); the second one employ rich fea-
tures (Zhang and Nivre, 2011) and a larger beam
(32). Two graph-based parsing results are reported;
the difference between them is whether integrate re-
lation labels into the parsing procedure. Roughly
speaking, currently state-of-the-art data-driven mod-
els achieves slightly better precision than unlexical-
ized PCFG-based models with regard to unlabeled
dependency prediction.
There is a big gap between lexicalized and unlexi-
calized parsing. The same phenomenon has been ob-
served by (Che et al, 2012) and (Zhuang and Zong,
2010). In addition to dependency parsing, Zhuang
and Zong (2010) found that Berkeley parser pro-
duce much more accurate syntactic analyses to assist
a Chinese semantic role labeler than Bikel parser.
Charniak and Stanford parsers are two other well-
known and frequently used tools that can provide
lexicalized parsing results. According to (Che et al,
2012), they perform even worse than Bikel parser,
at least for Stanford dependencies. Due to the poor
parsing performance, we only concentrate on the un-
lexicalized model in the remainder of this paper.
The performance of labeled dependency predic-
tion of the unlexicalized PCFG-based parser is much
lower. We can learn that the CS to DS conversion is
not robust to assign functional categories to depen-
dencies and simple linguistic rules are not capable
to do fine-grained classification. Previous research
on English indicates that the main difficulty in de-
pendency parsing is the prediction of dependency
304
Devel. UAS LAS Compl. Fsib Fgrd
Tran[b=16,Z08] 82.80 N/A 29.00 66.55 79.74
Tran[b=32,Z11] 83.80 N/A 31.61 68.58 80.87
Graph[-lab] 83.66 N/A 29.28 67.96 80.82
Graph[+lab] 84.24 80.55 30.99 69.11 81.38
Unlex 82.86 67.44 27.98 69.07 81.22
Lex 70.38 58.10 - - - - - -
Bagging(15)
Tran[b=16,Z08] 83.25 N/A 28.66 67.17 78.89
Tran[b=32,Z11] 84.25 N/A 31.21 69.14 81.49
Graph[-lab] 83.81 N/A 29.68 68.00 80.62
Graph[+lab] 84.50 N/A 31.44 69.48 81.10
Unlex 84.92 N/A 32.35 71.08 83.66
Bagging(8)
Unlex 84.35 N/A 31.16 70.49 83.57
Table 1: Accuracy of different parsers. The first block
presents baseline parsers; the last two blocks present
Bagging-enhanced parsers, where m is respectively set to
15 and 8. Z08 and Z11 distinguish different feature sets;
b=16 and b=32 are beam sizes. +/-lab means whether to
incorporate relation labels to a model.
structures, and an extra statistical classifier can be
employed to label automatically recognized depen-
dencies with a high accuracy. Although this issue
is not well studied for Chinese dependency parsing,
previous research on function tag labeling (Sun and
Sui, 2009) and semantic role labeling (Sun, 2010a)
gives us some clues. Their research shows that both
functional and predicate-argument structural infor-
mation is relatively easy to predict if high-quality
syntactic parses are available. We mainly focus on
the UAS metric in the following experiments.
3.3 Constraints
A grammar-based model utilizes an explicitly de-
fined formal grammar to shape the search space for
possible syntactic hypotheses. Parameters of a sta-
tistical grammar-based model are related to a gram-
mar rule, and as a result specific language construc-
tions are constrained by each other. For example,
parameters are assigned to rewrite rules for a CFG-
based model. Since the PCFG-based model lever-
ages rewrite rules to locally constrain several possi-
ble dependents for one head word, it does relatively
better for locally connected dependencies. The tra-
ditional evaluation metrics, i.e. UAS and LAS, only
consider bi-lexical (first order) dependencies, which
are smallest pieces of a dependency structure. Be-
sides bi-lexical dependencies, we report the predic-
tion accuracy of grandparent and sibling dependen-
cies, i.e. second order dependencies. The metrics
are defined as follows.
? For every word d whose parent is not the root,
we consider the word triple ?d, p, g? among d
and its parent p and grandparent g. A word
triple ?d, p, g? from a predicted tree is consid-
ered as correct if it also apprears in the corre-
sponding gold tree. Based on this definition,
precison, recall and f-score of grandparent de-
pendency can be defined in a normal sense. All
punctuations are excluded for calculation.
? For every word h that governs at least two chil-
dren (d1, ..., dn), we consider every word triple
?h, di, di+1?, among h and its sibling depen-
dents di as well as di+1 (0 ? i < n). Similar
to the grandparent dependencies, we can define
evaluation metrics for sibling dependencies.
From Table 1, we can see that the grammar-based
model parses relatively better for slightly larger frag-
ments. For example, the UAS of the graph-based
model is significantly higher than the grammar-
based one, but their sibling and grandparent scores
are similar. In the next section, we will introduce
a general parser enhancement technique and present
more discussions based on enhanced parsing results
(Column 7-14).
3.4 Endocentric and exocentric constructions
<-NN<- <-NR<- <-NT<- <-PN<- <-VA <- <-VC<- <-VE<- <-VV<-
Unlex 2 7.61 19.3 17.2 5 14.0 9 39.72 45.51 49.83 41.44
G raph[+lab] 2 4.82 17.45 12 .2 12 .1 38.12 49.9 51.18 42 .14
Tran[b=32 ,Z0 8] 2 5.2 5 17.82 15.16 13.48 41.32 47.7 49.83 42 .34
10
2 0
30
40
50
Er
ro
r 
ra
te
 
Figure 1: Nominal vs. verbal constructions.
Arguments in exocentric constructions help com-
plete the meaning of a predicate and are taken to be
obligatory and selected by their heads; adjuncts in
305
endocentric constructions are structurally dispens-
able parts that provide auxiliary information and
taken to be optional and not selected by their heads.
An important annotation policy of the CTB is ?one
grammatical relation per bracket?, which means
each constituent falls into one of the three primitive
grammatical relations: (1) head-complementation,
(2) head-adjunction and (3) coordination. Addi-
tionally, the argument is attached at a level that is
?closer? to the head than the adjuncts. Due to the
linguistic properties of different dependents and the
annotation strategies, a grammar-based model can
capture more syntactic preference properties of ar-
guments via hard constraints, i.e. grammar rules,
and are therefore more suitable to analyze exocen-
tric constructions.
Figure 1 is the error rate of unlabeled dependen-
cies considering different construction. A construc-
tion ?? X ?? is considered as correctly predicted
if and only if all dependent words and head word of
X are completely correctly found. The error rate
in terms of this metric seems rather high because
the units we consider are normally much larger than
word pairs. From this figure, we can clearly see that
the data-driven parser does better for the prediction
of nominal constructions (NN/NR/NT/PN5), which
relate more on optional adjuncts or modifiers; the
grammar-based parser performs better for the pre-
diction of verbal constructions (VC/VE/VV), which
relate more on obligatory arguments. The evalua-
tion of the nominal and verbal constructions roughly
confirms the strength of grammar-based model to
predict verbal constructions.
4 Bagging parsers
The comparative analysis highlights the fundamen-
tal diversity between data-driven and PCFG-based
models. In order to exploit the diversity gain, we ad-
dress the issue of parser combination. We employ
a general ensemble learning technique, i.e. Bag-
ging, to enhance a single-view parser and to com-
bine multi-view parsers.
5For the definition and illustration of these tags, please refer
to the annotation guidelines (http://www.cis.upenn.
edu/?chinese/posguide.3rd.ch.pdf).
4.1 Applying Bagging to dependency parsing
Bagging is a machine learning ensemble meta-
algorithm to improve classification and regression
models in terms of stability and classification accu-
racy (Breiman, 1996). It also reduces variance and
helps to avoid overfitting. Given a training set D of
size n, Bagging generates m new training sets Di
of size n? ? n, by sampling examples from D. m
models are separately learned on the m new train-
ing sets and combined by voting (for classification)
or averaging the output (for regression). Hender-
son and Brill (2000) successfully applied Bagging
to enhance a constituent parser. Moreover, Bagging
has been applied to combine multiple solutions for
Chinese lexical processing (Sun, 2010b; Sun and
Uszkoreit, 2012). In this paper, we apply Bagging
to dependency parsing. Since training even one sin-
gle parser takes hours (if not days), experiments on
Bagging is time-consuming. To save time, we con-
duct data-driven parsing experiments based on sim-
ple configuration. More specifically, the beam size
of the transition-based parser is set to 16, and the
simple feature set is utilized; dependency relations
are not incorporated for the graph-based parser.
Bootrapping step. In the training phase, given a
training set D of size n, our model generates m new
training sets Di of size ?n by sampling uniformly
without replacement. Each Di can be used to train
a single-view parser or multiple parsers according
to different views. Using this strategy, we can get m
weak parsers or km parsers if multiple views are im-
plemented. In the parsing phase, for each sentence,
the (k)m models output (k)m candidate analyses
that are combined in a post-inference procedure.
Aggregating step. Different from classification
problems, simple voting scheme is not suitable for
parsing, which is a typical structured prediction
problem. To aggregate outputs of (k)m sub-models,
a structured inference procedure is needed. Sagae
and Lavie (2006) present a framework for combin-
ing the output of several different parsers to produce
results that are superior to each of the individual
parsers. We implement their method to aggregate
models. Once we have obtained multiple depen-
dency trees respectively from base parsers, we can
build a graph where each word in the sentence is a
306
node. We then create weighted directed edges be-
tween the nodes corresponding to words for which
dependencies are obtained from each of the initial
structures. The weights are the word-by-word voting
results of sub-models. Based on this graph, the sen-
tence can be reparsed by a graph-based algorithm.
Taking Chinese as a projective language, we use Eis-
ner?s algorithm (Eisner, 1996) to combine multiple
dependency parses. Surdeanu and Manning (2010)
indicates that reparsing performs essentially as well
as other simpler or more complex models.
4.2 Parameter tuning
We evaluate our combination model on the same
data set used in the last section. The two hyper-
parameters (? and m) of our Bagging model are
tuned on the development (validation) set. On one
hand, with the increase of the size of sub-samples,
i.e. ?, the performance of sub-models is improved.
However, since the sub-models overlap more, the di-
versity of base models for ensemble will decrease
and the final prediction accuracy may go down. To
evaluate the effect of ?, we separately sample 50%,
60%, 70% and 80% sentences from the original
training data 5 times, train 5 sub-models for each
parser, and combine them together. The beam size
of the transition-based parser is set to 16. Table 2
shows the influence of the choice of ??s. For all fol-
lowing experiments, we set ? = 0.7.
? 50% 60% 70% 80%
Tran+Graph[-lab]+Unlex 83.50 85.96 86.15 85.60
Table 2: UAS of Bagging(5) models with different ?.
The second parameter for Bagging is the number
of sub-models to be used for combination. Figure 2
summarizes the Bagging performance when differ-
ent models are employed and different number (i.e.
m) of subsamples are used. From this figure, we can
learn the influence of the number of sub-models.
4.3 Bagging single-view parsers
4.3.1 Results
Table 1 indicates that Bagging can improve in-
dividual single-view parsers, especially Berkeley
parser. If we take Bagging as a general parser en-
hancement technique and still consider a Bagging-
enhanced parser as a single view, we conclude
81.5
82 .5
83.5
84.5
85.5
86.5
3 4 5 6 7 8 9
G raph[-lab]
Tran
Unlex
G raph[-lab]+Unlex
Tran+Unlex
G raph[-lab]+Tran
Figure 2: Averaged UAS of different Bagging models
with different numbers of sampling data sets.
that Bagging-enhanced PCFG-based method works
best among state-of-the-art approaches. For the
transition-based parser, though the score over single
words goes up, the score over sentences goes down.
The main reason is that the reparsing algorithm is a
graph-based one, which performs worse with regard
to the prediction of a whole sentence. The improve-
ment for the graph-based parser is very modest.
We train a Bagging(8)-enhanced Berkeley parser,
which achieves equivalent overall UAS to data-
driven parsers, and compare their parsing abilities
of second order dependencies. Now we can more
clearly see that the Bagging-enhanced PCFG-based
model performs better in the prediction of second
order dependencies.
4.3.2 Related experiments on sequence models
Bagging has been applied to enhance discrimina-
tive sequence models for Chinese word segmenta-
tion (Sun, 2010b) and POS tagging (Sun and Uszko-
reit, 2012). For word segmentation, experiments
on discriminative Markov and semi-Markov tagging
models are reported. Their experiments showed that
Bagging can consistently enhance a semi-Markov
model but not the Markov one. Experiments on POS
tagging indicated that BaggingMarkov models hurts
tagging performance. It seems that the relationships
among basic processing units affect Bagging.
PCFGLA parsers are built upon generative mod-
els with latent annotations. The use of automati-
cally induced latent variables may also affect Bag-
ging. Generative sequence models with latent anno-
307
tations can also achieve good performance for Chi-
nese POS tagging. Huang et al (2009) described
and evaluated a bi-gram HMM tagger that utilizes
latent annotations. Different from negative results of
Bagging discriminative models, our auxiliary exper-
iment shows that Bagging Huang et al?s tagger can
help Chinese POS tagging. In other words, Bagging
substantially improves both HMMLA and PCFGLA
models, at least for Chinese POS tagging and con-
stituency parsing. It seems that Bagging favors the
use of latent variables.
4.4 Bagging multi-view parsers
4.4.1 Results
Figure 2 clearly shows that the Bagging model
taking both data-driven and PCFG-based models as
basic systems outperform the Bagging model taking
either model in isolation as basic systems. The com-
bination of a PCFG-based model and a data-driven
model (either graph-based or transition-based) is
more effective than the combination of two data-
driven models, which has received the most atten-
tion in dependency parser ensemble. Table 3 is
the performance of reparsing on the development
data. From this table, we can see by utilizing more
parsers, Bagging can enhance reparsing. According
to Surdeanu and Manning (2010)?s findings, repars-
ing performs as well as other combination mod-
els. Our auxiliary experiments confirm this finding:
Learning-based stacking cannot achieve better per-
formance. Limited to the document length, we do
not give descriptions of these experiments.
Devel. UAS
Reparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82
+Bagging(15) 86.37
bagging(reparse(g, t, c)) 86.09
reparse(bagging(g, t, c)) 85.86
Table 3: UAS of reparsing and Bagging.
4.4.2 Analysis
In our proposed model, Bagging has a two-fold
effect: One is as a system combination technique
and the other as a general parser enhancing tech-
nique. Two additional experiments are performed
to evaluate these two effects. To illustrate the differ-
ences between these two experiments, respectively
denote graph-based, transition-based and PCFG-
based parsers as g, t and c; denote the reparsing
procedure as reparse and the Bagging procedure as
bagging. The two experiments are as follows.
? Bagging a hybrid parser. In this experiment,
for each sub-sample Di, we first train three
parsers: gi, ti and ci. Then we combine these
three parsers by reparsing and construct a hy-
brid parser reparse(gi, ti, ci). Finally, all hy-
brid parsers are collected to build the final
parser: bagging(reparse(g, t, c)).
? Combining Bagging-enhanced parsers. In
this experiment, for each model, we first train
three Bagging-enhanced parsers: bagging(g),
bagging(t) and bagging(c). Then these
three Bagging-enhanced parsers are com-
bined by reparsing to build the final parser:
reparse(bagging(g, t, c)).
Evaluation results are presented in Table 3.
5 Pseudo-grammar-based models
Although the combination of data-driven and
grammar-based models is very effective, it has a
serious limitation: It is only applicable when con-
stituency annotations are available to learning a
grammar. However, many treebanks, e.g. Chinese
Dependency Treebank (LDC2012T05), do not have
such linguistically rich structures. Our experiments
also suggest that a constituency grammar can sig-
nificantly increase the diversity of base models for
parser ensemble, which plays a major role in boost-
ing prediction accuracy.
In order to reduce the need for phrase-structure
annotations, and to increase the diversity of candi-
date parsers, we study learning pseudo grammars
for dependency parsing. The key idea is very sim-
ple: By converting a dependency structure to a
constituency one, we can reuse the PCFGLA ap-
proach to learn pseudo grammars for dependency
parsing. Figure 3 is an example. The first tree is
an original dependency parse, while the second tree
is the corresponding CTB annotation. The next two
trees are two automatically converted pseudo con-
stituency trees. By applying DS to CS rules, we
can acquire pseudo constituency treebanks and then
learn pseudo grammars from them.
308
(1) Dependency tree (2) Linguistic constituency tree
(3) Flat constituency tree (4) Binarized constituency tree
Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure.
The basic idea of our method is to use parsing
models in one formalism for parsing in another for-
malism. In previous work, PCFGs are used to solve
parsing problems in many other formalisms, includ-
ing dependency (Collins et al, 1999), CCG (Fowler
and Penn, 2010), LFG (Cahill et al, 2004) and HPSG
(Zhang and Krieger, 2011) parsing.
5.1 Strategies for DS to CS conversion
The conversion from DS to CS is a non-trivial prob-
lem. One main issue in the conversion is the indeter-
minancy in the choice of a phrasal category given a
dependency relation, the level and position of attach-
ment of a dependent in the constituency structure, as
dependency relations typically do not encode such
information. To convert a DS to a CS, especially
for dependency parsing, we should consider (1) how
to transform between the topological structures, (2)
how to induce a syntactic category, and (3) how to
easily recover dependency trees from pseudo con-
stituency trees. From these three aspects, we present
the following strategies.
5.1.1 Topological structure
The topological structures represent the boundary
information of constituents in a given sentence. De-
pendency structures do not directly represent such
boundary information. Nevertheless, a complete
subtree in a projective dependency tree should be
considered as a constituent. We can construct a very
flat constituent tree, of which nodes are associated
with complete subtrees of a dependency parse. The
third tree in Figure 3 is an example of such conver-
sion.
Right-to-left binarization According to the study
in (Sun, 2010a), head words of most phrases in
Chinese are located at the first or the last position.
That means for binarizing most phrases, we only
need sequentially combine the right or left parts to-
gether with their head phrases. Main exceptions are
clauses, of which the head predicate locates inside,
since Chinese is an SVO language. To deal with
these exceptions, we split each phrase whose head
child is inside itself into three parts: left child(ren),
head and right child(ren). We first sequentially com-
bine the head and its right child(ren) that are usu-
ally objects as intermediate phrases, then sequen-
tially combine the left child(ren) until reach the orig-
inal parent node. For example, the first rewrite rule
in follows should be transferred into the second and
third types of rules.
1. Xp ? X1, ..., Xi, ..., Xm
309
2. X?p ? Xi, Xi+1; Xp?? ? Xp?, Xi+2; ...
3. X?p ? Xi?1, Xp?...?; X??p ? Xi?2, X?p; ...
This right-to-left binarization strategy is consistent
with most Chinese treebank annotation schemes.
The fourth tree in Figure 3 is an example of bina-
rized pseudo tree.
5.1.2 Phrasal category
Projection principle is introduced by Chomsky
to link together the levels of syntactic description.
It connects syntactic structures with lexical entries:
Lexical structure must be represented categorically
at every syntactic level, and representations at each
level of syntax are projected from the lexicon in that
they observe the subcategorisation properties of lex-
ical items. According to this principle, it is reason-
able to use the lexical category (POS) of the head
word as the phrasal category of a phrase.
5.1.3 Auxiliary symbol
We can use auxiliary symbols to denote the head
phrase position in a CFG rule. In other words, some
categories may be splitted into subcategories accord-
ing to if they are head phrases of their parent nodes
or which children are their head phrases. Auxiliary
symbols could be either assigned to one of the right
hand side or the left hand side. The first choice is to
conveniently use a H symbol to indicate that current
phrase is the head of its parent node. The second
choice is to practically use an L or R symbol to indi-
cate the head of current node is its left or right child,
in a binarized tree. The following table gives an ex-
ample of different rules with auxiliary symbols.
With head symbol With left/right symbol
Xl ? Xl#H, Xr Xl#L? Xl, Xr
Xr ? Xl, Xr#H Xr#R? Xl, Xr
5.2 Three conversions
Taking into account the above strategies, we propose
three concrete DS to CS conversions:
Flat conversion with H auxiliary symbol (FlatH).
Just as shown as the third tree in Figure 3, we can
learn a grammar from very flat constituency trees
where the auxiliary symbol H is used for extracting
dependencies.
Right-to-left binarizing with H auxiliary symbol
(BinH). Different from the flat conversion, we bi-
narize a tree according to the right-to-left principle.
Auxiliary symbol H is chosen.
Right-to-left binarizing with LR auxiliary sym-
bol (BinLR). Different from the second type of
conversion, we use auxiliary L/R symbols to denote
head phrases. See the fourth tree in Figure 3 for in-
stance.
Practically, every constituency parse that is pro-
duced by parsers trained with binarized trees exactly
maps to one dependency tree. However, the parser
trained with flat trees may produce very bad con-
stituency results. Sometimes, one parent node may
have zero child that is assigned with H or more than
one children that are are assigned H. In the first case,
we select the right most child as the head of such
parent, while in the second case, we select the right
most one from the children that are assigned H.
5.3 Evaluation
5.3.1 Equivalent parsing accuracy
Devel. Base Bagging(15)
CTB 83.49% 84.92%
FlatH 80.15% 83.53%
BinH 81.80% 84.64%
BinLR 82.46% 84.90%
Table 4: UAS of pseudo-grammar-based models.
Table 4 summarizes the performance of differ-
ent pseudo-grammar-based models. Compared to
the linguistic grammar learned from CTB, we can
see that pseudo grammars are very competitive.
Not that, the FlatH/BinH/BinLR trees are derived
from the CoNLL data, rather than the original CTB.
Among different DS to CS conversion strategies, the
BinLR conversion works best. More interestingly,
when we enhance the PCFGLA method by using
Bagging, the BinLR model performs as well as the
real-grammar-based model.
5.3.2 Better contribution to ensemble
The experiments above indicate that we can eas-
ily build good grammar-based dependency parser
without any constituency annotations. The fol-
lowing experiments on parser combination show
that compared to the linguistic grammar, binH and
310
Devel. UAS
Tran+Graph+CTB 86.37%
Tran+Graph+FlatH 86.14%
Tran+Graph+BinH 86.29%
Tran+Graph+BinLR 86.28%
Tran+Graph+flat+BinH+BinLR 87.03%
Tran+Graph+CTB+FlatH 86.96%
Tran+Graph+CTB+BinH 87.10%
Tran+Graph+CTB+BinLR 87.15%
Tran+Graph+CTB+BinH+BinLR 87.38%
Tran+Graph+CTB+FlatH+BinH+BinLR 87.35%
Table 5: UAS of different Bagging(15) models.
binLR grammars have equivalent contributions to
parser ensemble. Table 5 presents the ensem-
ble performance on the development data. By
Bagging, the data-driven models together with ei-
ther real grammar-based or pseudo-grammar-based
model reach a similar UAS.
5.3.3 Increased parser diversity
Since pseudo grammars are very different from
real grammars that are induced from large-scale lin-
guistic annotations. Pseudo-grammar-based parsing
models behave very differently with grammar-based
models. In other words, they increase the diver-
sity of model candidates for parser ensemble. As
a result, pseudo-grammar-based models lead to fur-
ther improvements for parser combination. Table 5
shows that the combination of data-driven, PCFG-
based and binarized pseudo-grammar-based models
is significantly better than the combination of data-
driven and PCFG-based models.
5.4 Comparison to the state-of-the-art
Table 6 summarizes the parsing performance on the
test data set, as well as the best published result re-
ported in Li et al (2012). To fairly compare the per-
formance of our parser and other systems which are
built without linguistic constituency trees, we only
use pseudo-PCFGs in this experiment. Based on
automatic POS tagging, our final model achieves a
UAS of 87.23%, which yields a relative error reduc-
tion of 24% over the best published result. Table
6 also presents the results evaluated on the CTB5
data that is more widely used for previous research.
Li et al (2011) and Hatori et al (2011) respec-
tively evaluated their graph-based and transition-
based parsers; Zhang and Clark (2011) evaluated
CoNLL-test UAS
(Li et al, 2012) 83.23%
Graph+Tran+FlatH+BinH+BinLR 87.23%
CTB5-test UAS
(Li et al, 2011) 80.79%
(Hatori et al, 2011) 81.33%
(Zhang and Clark, 2011) 81.21%
Graph+Tran+FlatH+BinH+BinLR 84.65%
Table 6: UAS of different models on the test data.
a hybrid data-driven parser. Our model is signifi-
cantly better than these systems: It achieves a UAS
of 84.65%, which obtains an error reduction of 18%
over the best system in the literature.
6 Conclusion and Future Work
There have been several attempts to develop high
accuracy parsers in both constituency and depen-
dency formalisms for Chinese, and many successful
parsing algorithms designed for English have been
applied. However, the state-of-the-art still falls far
short when compared to English. This paper stud-
ies data-driven and PCFG-based models for Chinese
dependency parsing. We present a comparative anal-
ysis of transition-, graph-, and PCFG-based parsers,
which highlights the systematic differences between
data-driven and PCFG-based models. Our analysis
may benefit parser ensemble, parser co-training, ac-
tive learning for treebank construction, and so on.
In order to exploit the diversity gain, we address
the issue of parser combination. To overcome the
limitation of the lack of constituency treebanks, we
study pseudo-grammar-based models. Experimental
results show that combining various data-driven and
PCFG-based models significantly advance the state-
of-the-art, and by converting parse trees, we can still
take advantages of the constituency representation
even without constituency annotations.
Acknowledgement
We would like to thank thank all anonymous review-
ers whose valuable comments led to signilicant re-
visions. The first author would like to thank Prof.
Hans Uszkoreit for discussion and feedback of an
early version of this work.
The work was supported by NSFC (61170166),
Beijing Nova Program (2008B03) and National
High-Tech R&D Program (2012AA011101).
311
References
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
89?97. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1011.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef Van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically
acquired wide-coverage pcfg-based lfg approx-
imations. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 319?
326. Barcelona, Spain. URL http://www.
aclweb.org/anthology/P04-1041.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 173?180. Associa-
tion for Computational Linguistics, Ann Arbor,
Michigan.
Wanxiang Che, Valentin Spitkovsky, and Ting
Liu. 2012. A comparison of chinese parsers
for stanford dependencies. In Proceedings
of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume
2: Short Papers), pages 11?16. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-2003.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 505?512. Association for Com-
putational Linguistics, College Park, Maryland,
USA. URL http://www.aclweb.org/
anthology/P99-1065.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: an ex-
ploration. In Proceedings of the 16th con-
ference on Computational linguistics - Vol-
ume 1, COLING ?96, pages 340?345. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/992628.992688.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cat-
egorial grammar. In Proceedings of the 48th
Annual Meeting of the Association for Com-
putational Linguistics, pages 335?344. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-1035.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Asian Federation of Natural Language Process-
ing, Chiang Mai, Thailand. URL http://www.
aclweb.org/anthology/I11-1136.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combin-
ing parsers. In In Proceedings of the Fourth Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 187?194.
John C. Henderson and Eric Brill. 2000. Bag-
ging and boosting a treebank parser. In Pro-
ceedings of the 1st North American chapter of
the Association for Computational Linguistics
conference, NAACL 2000, pages 34?41. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA. URL http://dl.acm.org/
citation.cfm?id=974305.974310.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
312
1077?1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213?216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhenghua Li, Ting Liu, and Wanxiang Che.
2012. Exploiting multiple treebanks for pars-
ing with quasi-synchronous grammars. In Pro-
ceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 675?684. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-1071.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180?1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05, pages 75?82. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
RyanMcDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency pars-
ing. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA. AAI3225503.
Ryan McDonald and Joakim Nivre. 2007. Char-
acterizing the errors of data-driven dependency
parsing models. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-
CoNLL), pages 122?131. Association for Com-
putational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/
anthology/D/D07/D07-1013.
Joakim Nivre. 2008. Algorithms for de-
terministic incremental dependency pars-
ing. Comput. Linguist., 34:513?553. URL
http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based
dependency parsers. In Proceedings of ACL-08:
HLT, pages 950?958. Association for Compu-
tational Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1108.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440. Association for Computational Linguis-
tics, Sydney, Australia.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the
Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers,
NAACL-Short ?06, pages 129?132. Association
for Computational Linguistics, Stroudsburg, PA,
USA. URL http://portal.acm.org/
citation.cfm?id=1614049.1614082.
Weiwei Sun. 2010a. Improving Chinese se-
mantic role labeling with rich syntactic fea-
tures. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 168?172. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-2031.
Weiwei Sun. 2010b. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
313
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211?1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation. Hong Kong.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations:
Towards accurate Chinese part-of-speech tagging.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, pages 649?652. Association
for Computational Linguistics, Los Angeles, Cal-
ifornia. URL http://www.aclweb.org/
anthology/N10-1091.
Andre Torres Martins, Noah Smith, and Eric Xing.
2009. Concise integer linear programming for-
mulations for dependency parsing. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Process-
ing of the AFNLP, pages 342?350. Associa-
tion for Computational Linguistics, Suntec, Sin-
gapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1039.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 157?166. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii. URL http://www.aclweb.org/
anthology/D08-1017.
Nianwen Xue. 2007. Tapping the implicit infor-
mation for the PS to DS conversion of the Chi-
nese treebank. In Proceedings of the Sixth Inter-
national Workshop on Treebanks and Linguistics
Theories.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-
scale corpus-driven pcfg approximation of an
hpsg. In Proceedings of the 12th International
Conference on Parsing Technologies, pages 198?
208. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.
org/anthology/W11-2923.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562?571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 162?171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Comput. Linguist., 37(1):105?
151. URL http://dx.doi.org/10.1162/
coli_a_00037.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 188?
193. Association for Computational Linguistics,
Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
Tao Zhuang and Chengqing Zong. 2010. A min-
imum error weighting combination strategy for
Chinese semantic role labeling. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 1362?
1370. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1153.
314
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459?464,
Dublin, Ireland, August 23-24, 2014.
Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph
Parsing
Yantao Du, Fan Zhang, Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{duyantao,ws,wanxiaojun}@pku.edu.cn, zhangf717@gmail.com
Abstract
Using the SemEval-2014 Task 8 data, we
profile the syntactic tree parsing tech-
niques for semantic graph parsing. In par-
ticular, we implement different transition-
based and graph-based models, as well as
a parser ensembler, and evaluate their ef-
fectiveness for semantic dependency pars-
ing. Evaluation gauges how successful
data-driven dependency graph parsing can
be by applying existing techniques.
1 Introduction
Bi-lexical dependency representation is quite pow-
erful and popular to encode syntactic or semantic
information, and parsing techniques under the de-
pendency formalism have been well studied and
advanced in the last decade. The major focus is
limited to tree structures, which fortunately corre-
spond to many computationally good properties.
On the other hand, some leading linguistic theo-
ries argue that more general graphs are needed to
encode a wide variety of deep syntactic and se-
mantic phenomena, e.g. topicalization, relative
clauses, etc. However, algorithms for statistical
graph spanning have not been well explored be-
fore, and therefore it is not very clear how good
data-driven parsing techniques developed for tree
parsing can be for graph generating.
Following several well-established syntactic
theories, SemEval-2014 task 8 (Oepen et al.,
2014) proposes using graphs to represent seman-
tics. Considering that semantic dependency pars-
ing is a quite new topic and there is little previ-
ous work, we think it worth appropriately profil-
ing successful tree parsing techniques for graph
parsing. To this end, we build a hybrid system
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
that combines several important data-driven pars-
ing techniques and evaluate their impact with the
given data. In particular, we implement different
transition-based and graph-based models, as well
as a parser ensembler.
Our experiments highlight the following facts:
? Graph-based models are more effective than
transition-based models.
? Parser ensemble is very useful to boost the
parsing accuracy.
2 Architecture
We explore two kinds of basic models: One is
transition-based, and the other is tree approxima-
tion. Transition-based models are widely used for
dependency tree parsing, and they can be adapted
to graph parsing (Sagae and Tsujii, 2008; Titov
et al., 2009). Here we implement 5 transition-
based models for dependency graph parsing, each
of which is based on different transition system.
The motivation of developing tree approxima-
tion models is to apply existing graph-based tree
parsers to generate graphs. At the training time,
we convert the dependency graphs from the train-
ing data into dependency trees, and train second-
order arc-factored models
1
. At the test phase, we
parse sentences using this tree parser, and convert
the output trees back into semantic graphs. We
think tree approximation can appropriately evalu-
ate the possible effectiveness of graph-based mod-
els for graph spanning.
Finally, we integrate the outputs of different
models with a simple voter to boost the perfor-
mance. The motivation of using system combi-
nation and the choice of voting is mainly due to
the experiments presented by (Surdeanu and Man-
ning, 2010). When we obtain all the outputs of
1
The mate parser (code.google.com/p/
mate-tools/) is used.
459
these models, we combine them into a final result,
which is better than any of them. For combination,
we explore various systems for this task, since em-
pirically we know that variety leads to better per-
formance.
3 Transition-Based Models
Transition-based models are usually used for de-
pendency tree parsing. For this task, we exploit it
for dependency graph parsing.
A transition system S contains a set C of con-
figurations and a set T of transitions. A configu-
ration c ? C generally contains a stack ? of nodes,
a buffer ? of nodes, and a set A of arcs. The ele-
ments in A is in the form (x, l, y), which denotes
a arc from x to y labeled l. A transition t ? T can
be applied to a configuration and turn it into a new
one by adding new arcs or manipulating elements
of the stack or the buffer. A statistical transition-
based parser leverages a classifier to approximate
an oracle that is able to generate target graphs by
transforming the initial configuration c
s
(x) into a
terminal configuration c
t
? C
t
.
An oracle of a given graph on sentence x is a
sequence of transitions which transform the initial
configuration to the terminal configuration the arc
set A
c
t
of which is the set of the arcs of the graph.
3.1 Our Transition Systems
We implemented 5 different transition systems for
graph parsing. Here we describe two of them
in detail, one is the Titov system proposed in
(Titov et al., 2009), and the other is our Naive
system. The configurations of the two systems
each contain a stack ?, a buffer ?, and a set A of
arcs, denoted by ??, ?,A?. The initial configura-
tion of a sentence x = w
1
w
2
? ? ?w
n
is c
s
(x) =
?[0], [1, 2, ? ? ? , n], {}?, and the terminal configu-
ration set C
t
is the set of all configurations with
empty buffer. These two transition systems are
shown in 1.
The transitions of the Titov system are:
? LEFT-ARC
l
adds an arc from the front of the
buffer to the top of the stack, labeled l, into
A.
? RIGHT-ARC
l
adds an arc from the top of the
stack to the front of the buffer, labeled l, into
A.
? SHIFT removes the front of the buffer and
push it onto the stack;
? POP pops the top of the stack.
? SWAP swaps the top two elements of the
stack.
This system uses a transition SWAP to change the
node order in the stack, thus allowing some cross-
ing arcs to be built.
The transitions of the Naive system are similar
to the Titov system?s, except that we can directly
manipulate all the nodes in the stack instead of just
the top two. In this case, the transition SWAP is not
needed.
The Titov system can cover a great proportion,
though not all, of graphs in this task. For more
discussion, see (Titov et al., 2009). The Naive
system, by comparison, covers all graphs. That
is to say, with this system, we can find an oracle
for any dependency graph on a sentence x. Other
transition systems we build are also designed for
dependency graph parsing, and they can cover de-
pendency graphs without self loop as well.
3.2 Statistical Disambiguation
First of all, we derive oracle transition sequences
for every sentence, and train Passive-Aggressive
models (Crammer et al., 2006) to predict next tran-
sition given a configuration. When it comes to
parsing, we start with the initial configuration, pre-
dicting next transition and updating the configura-
tion with the transition iteratively. And finally we
will get a terminal configuration, we then stop and
output the arcs of the graph contained in the final
configuration.
We extracted rich feature for we utilize a set
of rich features for disambiguation, referencing to
Zhang and Nivre (2011). We examine the several
tops of the stack and the one or more fronts of the
buffer, and combine the lemmas and POS tags of
them in many ways as the features. Additionally,
we also derive features from partial parses such as
heads and dependents of these nodes.
3.3 Sentence Reversal
Reversing the order the words of a given sentence
is a simple way to yield heterogeneous parsing
models, thus improving parsing accuracy of the
model ensemble (Sagae, 2007). In our experi-
ments, one transition system produces two mod-
els, one trained on the normal corpus, and the other
on the corpus of reversed sentences. Therefore we
can get 10 parse of a sentence based on 5 transition
systems.
460
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP (?|i, ?, A)? (?, ?,A)
SWAP (?|i|j, ?,A)? (?|j|i, ?, A)
Titov System
LEFT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(j, l, i
k
)})
RIGHT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(i
k
, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP
k
(?|i
k
|i
k?1
| . . . |i
2
|i
1
, ?, A)? (?|i
k?1
| . . . |i
2
|i
1
, ?, A)
Naive System
Figure 1: Two of our transition systems.
4 Tree Approximation Models
Parsing based on graph spanning is quite challeng-
ing since computational properties of the seman-
tic graphs given by the shared task are less ex-
plored and thus still unknown. On the other hand,
finding the best higher-order spanning for general
graph is NP complete, and therefore it is not easy,
if not impossible, to implement arc-factored mod-
els with exact inference. In our work, we use a
practical idea to indirectly profile the graph-based
parsing techniques for dependency graph parsing.
Inspired by the PCFG approximation idea (Fowler
and Penn, 2010; Zhang and Krieger, 2011) for
deep parsing, we study tree approximation ap-
proaches for graph spanning.
This tree approximation technique can be ap-
plied to both transition-based and graph-based
parsers. However, since transition systems that
can directly handle build graphs have been devel-
oped, we only use this technique to evaluate the
possible effectiveness of graph-based models for
semantic parsing.
4.1 Graph-to-Tree Transformation
In particular, we develop different methods to con-
vert a semantic graph into a tree, and use edge
labels to encode dependency relations as well as
structural information which helps to transform a
converted tree back to its original graph. By the
graph-to-tree transformation, we can train a tree
parser with a graph-annotated corpus, and utilize
the corresponding tree-to-graph transformation to
generate target graphs from the outputs of the tree
parser. Given that the tree-to-graph transformation
is quite trivial, we only describe the graph-to-tree
transformation approach.
We use graph traversal algorithms to convert a
directed graph to a directed tree. The transforma-
tion implies that we may lose, add or modify some
dependency relations in order to make the graph a
tree.
4.2 Auxiliary Labels
In the transformed trees, we use auxiliary labels to
carry out information of the original graphs. To
encode multiple edges to one, we keep the origi-
nal label on the directed edge but may add other
edges? information. On the other hand, through-
out most transformations, some edges must be re-
versed to make a tree, so we need a symbol to in-
dicate a edge on the tree is reversed during trans-
formation. The auxiliary labels are listed below:
? Label with following ?R: The symbol ?R
means this directed edge is reversed from the
original directed graph.
? Separator: Semicolon separates two encoded
original edges.
? [N ] followed by label: The symbol [N ] (N
is an integer) represents the head of the edge.
The dependent is the current one, but the head
is the dependent?s N -th ancestor where 1st
ancestor is its father and 2nd ancestor is its
father?s father.
See Figure 2 for example.
4.3 Traversal Strategies
Given directed graph (V,E), the task is to traverse
all edges on the graph and decide how to change
the labels or not contain the edge on the output.
We use 3 strategies for traversal. Here we use
x ?
g
y to denote the edge on graph, and x ?
t
y
the edge on tree.
461
Mrs Ward was relieved
noun ARG1 verb ARG1 verb ARG2
adj ARG1
root
Mrs Ward was relieved
noun ARG1?R verb ARG1 verb ARG2
root
Mrs Ward was relieved
noun ARG1?R verb ARG2
adj ARG1;[2]verb ARG1
root
Figure 2: One dependency graph and two possible
dependency trees after converting.
Depth-first-search We try graph traversal by
depth-first-search starting from the root on the di-
rected graph ignoring the direction of edges. Dur-
ing the traversal, we add edges to the directed tree
with (perhaps new) labels. We traverse the graph
recursively. Suppose the depth-first-search is run-
ning at the node x and the nodes set A which have
been searched. And suppose we find node y is
linked to x on the graph (x ?
g
y or y ?
g
x).
If y /? A, we add the directed edge x ?
t
y to the
tree immediately. In the case of y ?
g
x, we add
?R to the edge label. If y ? A, then y must be one
of the ancestors of x. In this case, we add this in-
formation to the label of the existing edge z ?
t
x.
Since the distance between two nodes x and y is
sufficient to indicate the node y, we use the dis-
tance to represent the head or dependent of this
directed edge and add the label and the distance to
the label of z ?
t
x. It is clear that the auxiliary
label [N ] can be used for multiple edge encoding.
Under this strategy, all edges can be encoded on
the tree.
Breadth-first-search An alternative traversal
strategy is based on breadth-first-search starting
from the root. This search ignores the direction
of edge too. We regard the search tree as the de-
pendency tree. During the breadth-first-search, if
(x, l, y) exists but node y has been searched, we
just ignore the edge. Under this strategy, we may
lose some edges.
Iterative expanding This strategy is based on
depth-first-search but slightly different. The strat-
egy only searches through the forward edges on
the directed graph at first. When there is no for-
ward edge to expend, a traversed node linked to
some nodes that are not traversed must be the de-
pendent of them. Then we choose an edge and add
it (reversed) to the tree and continue to expand the
tree. Also, we ignore the edges that does not sat-
isfy the tree constraint. We call this strategy iter-
ative expanding. When we need to expand output
tree, we need to design a strategy to decide which
edge to be add. The measure to decide which node
should be expanded first is its possible location on
the tree and the number of nodes it can search dur-
ing depth-first-search. Intuitively, we want the re-
versed edges to be as few as possible. For this
purpose, this strategy is practical but not necessar-
ily the best. Like the Breadth-first-search strategy,
this strategy may also cause edge loss.
4.4 Forest-to-Tree
After a primary searching process, if there is still
edge x ?
g
y that has not been searched yet, we
start a new search procedure from x or y. Even-
tually, we obtain a forest rather than a tree. To
combine disconnected trees in this forest to the fi-
nal dependency tree, we use edges with label None
to link them. Let the node setW be the set of roots
of the trees in the forest, which are not connected
to original graph root. The mission is to assign a
node v /? W for each w ? W . If we assign v
i
for
w
i
, we add the edge v
i
? w
i
labeled by None to
the final dependency tree. We try 3 strategies in
this step:
? For each w ? W we look for the first node
v /?W on the left of w.
? For each w ? W we look for the first node
v /?W on the right of w.
? By defining the distance between two nodes
as how many words are there between the two
words, we can select the nearest node. If the
distances of more than one node are equal,
we choose v randomly.
We also tried to link all of the nodes in W di-
rectly to the root, but it does not work well.
5 Model Ensemble
We have 19 heterogeneous basic models (10
transition-based models, 9 tree approximation
models), and use a simple voter to combine their
outputs.
462
Algorithm DM PAS PCEDT
DFS 0 0 0
BFS 0.0117 0.0320 0.0328
FEF 0.0127 0.0380 0.0328
Table 1: Edge loss of transformation algorithms.
For each pair of words of a sentence, we count
the number of the models that give positive pre-
dictions. If the number is greater than a threshold,
we put this arc to the final graph, and label the arc
with the most common label of what the models
give.
Furthermore, we find that the performance of
the tree approximation models is better than the
transition based models, and therefore we take
weights of individual models too. Instead of just
counting, we sum the weights of the models that
give positive predictions. The tree approximation
models are assigned higher weights.
6 Experiments
There are 3 subtasks in the task, namely DM, PAS,
and PCEDT. For subtask DM, we finally obtained
19 models, just as stated in previous sections.
For subtask PAS and PCEDT, only 17 models are
trained due to the tight schedule.
The tree approximation algorithms may cause
some edge loss, and the statistics are shown in Ta-
ble 1. We can see that DFS does not cause edge
loss, but edge losses of other two algorithm are
not negligible. This may result in a lower recall
and higher precision, but we can tune the final re-
sults during model ensemble. Edge loss in subtask
DM is less than those in subtask PAS and PCEDT.
We present the performance of several repre-
sentative models in Table 2. We can see that the
tree approximation models performs better than
the transition-based models, which highlights the
effective of arc-factored models for semantic de-
pendency parsing. For model ensemble, besides
the accuracy of each single model, it is also im-
portant that the models to be ensembled are very
different. As shown in Table 2, the evaluation be-
tween some of our models indicates that our mod-
els do vary a lot.
Following the suggestion of the task organizers,
we use section 20 of the train data as the devel-
opment set. With the help of development set,
we tune the parameters of the models and ensem-
Models DM PAS PCEDT
Titov 0.8468 0.8754 0.6978
Titov
r
0.8535 0.8928 0.7063
Naive 0.8481 - -
DFS
n
0.8692 0.9034 0.7370
DFS
l
0.8692 0.9015 0.7246
BFS
n
0.8686 0.8818 0.7247
Titov vs. Titov
r
0.8607 0.8831 0.7613
Titov vs. Naive 0.9245 - -
Titov vs. DFS
n
0.8590 0.8865 0.7650
DFS
n
vs. DFS
l
0.9273 0.9579 0.8688
DFS
n
vs. BFS
n
0.9226 0.9169 0.8367
Table 2: Evaluation between some of our models.
Labeled f-score on test set is shown. Titov
r
stands
for reversed Titov, DFS
n
for DFS+nearest, DFS
l
for DFS+left, and BFS
n
for BFS+nearest. The up-
per part gives the performance, and the lower part
gives the agreement between systems.
Format LP LR LF LM
DM 0.9027 0.8854 0.8940 0.2982
PAS 0.9344 0.9069 0.9204 0.3872
PCEDT 0.7875 0.7396 0.7628 0.1120
Table 3: Final results of the ensembled model.
bling. We set the weight of each transition-based
model 1, and tree approximation model 2 in run
1, 3 in run 2. The threshold is set to a half of the
total weight. The final results given by the orga-
nizers are shown in Table 3. Compared to Table 2
demonstrates the effectiveness of parser ensemble.
7 Conclusion
Data-driven dependency parsing techniques have
been greatly advanced during the parst decade.
Two dominant approaches, i.e. transition-based
and graph-based methods, have been well stud-
ied. In addition, parser ensemble has been shown
very effective to take advantages to combine the
strengthes of heterogeneous base parsers. In this
work, we propose different models to profile the
three techniques for semantic dependency pars-
ing. The experimental results suggest several di-
rections for future study.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
463
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551?585.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cate-
gorial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, Uppsala, Sweden, July.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 753?760, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Kenji Sagae. 2007. Dependency parsing and domain
adaptation with lr models and parser ensembles. In
In Proceedings of the Eleventh Conference on Com-
putational Natural Language Learning.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Los Angeles, California, June.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisa-
tion for synchronous parsing of semantic and syn-
tactic dependencies. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1562?1567, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale
corpus-driven PCFG approximation of an hpsg. In
Proceedings of the 12th International Conference on
Parsing Technologies, Dublin, Ireland, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
464
 
 
 
 
Xiao Qin, Liang Zong, Yuqian Wu, Xiaojun Wan and Jianwu Yang 
Institute of Computer Science and Technology 
Peking University, China, 100871 
{qinxiao,zongliang,wuyuqian,wanxiaojun,yangjianwu} 
@cist.pku.edu.cn 
 
 
 
Abstract 
This paper describes our experiments on 
the cross-domain Chinese word segmen-
tation task at the first CIPS-SIGHAN 
Joint Conference on Chinese Language 
Processing. Our system is based on the 
Conditional Random Fields (CRFs) 
model. Considering the particular prop-
erties of the out-of-domain data, we pro-
pose some novel steps to get some im-
provements for the special task.  
1 Introduction 
Chinese word segmentation is one of the most   
important tasks in the field of Chinese informa-
tion processing and it is meaningful to intelligent 
information processing technologies. After a lot 
of researches, Chinese word segmentation has 
achieved a high accuracy. Many methods have 
been presented, among which the CRFs model 
has attracted more and more attention. Zhao?s 
group used the CRFs model in the task of Chi-
nese word segmentation in Bakeoff-4 and they 
ranked at the top in all closed tests of word seg-
mentation (Zhao and Kit, 2008). The CRFs 
model has been widely used because of its excel-
lent performance. However, finding a better 
segmentation algorithm for the out-of-domain 
text is the focus of CIP-SIGHAN-2010 bakeoff. 
We still consider word segmentation as a se-
quence labeling problem. What we concern is 
how to use the unlabeled corpora to enrich the 
supervised CRFs learning. So we take some 
strategies to make use of the information of the 
texts in the unlabeled corpora.  
2 System Description 
In this section, we will describe our system in 
details. The system is based on the CRFs model 
and we propose some novel steps for some im-
provements. It mainly consists of three steps: 
preprocessing, CRF-based labeling, and re-
labeling.  
2.1 Preprocessing 
This step mainly includes two operations. First, 
we should cut the whole text into a series of sen-
tences. We regard ???, ???, ??? and ??? as the 
symbols of the boundary between sentences. 
Then we do atomic segmentation to all the sen-
tences. Here Atomic segmentation represents 
that we should regard the continuous non-
Chinese characters as a whole. Take the word 
?computer? as an example, we should regard 
?computer? as a whole, but not treat it as 8 sepa-
rate letters of ?c?, ?o?, ?m?, ?p?, ?u?, ?t?, ?e?, and 
?r?.    
2.2 CRF-based Labeling 
Conditional random field (CRF) is an extension 
of both Maximum Entropy Model (MEMs) and 
Hidden Markov Models (HMMs), which was 
firstly introduced by Lafferty (Lafferty et al, 
2001). It is an undirected graphical model 
trained to maximize the conditional probability 
of the desired outputs given the corresponding 
inputs. This model has achieved great successes 
in word segmentation. 
In the CRFs model, the conditional distribu-
tion P(y|x) of the labels Y givens observations X 
directly is defined: 
CRF-based Experiments for Cross-Domain Chinese 
Word Segmentation at CIPS-SIGHAN-2010 
1
1
1( / ) exp{ ( , , , )}
T
k k t t
t kx
P y x f y y x tZ ? ??? ??
y is the label sequence, x is observation sequence, 
Zx is a normalization term that makes the proba-
bility of all state sequences sum to one; fk(yt-1, yt, 
t) is often a binary-valued feature function and 
? k is the weight of fk. 
In our system, we choose six types of tags ac-
cording to character position in a word. Accord-
ing to Zhao?s work (Zhao et al, 2006a), the 6-
tag set enables our system to generate a better 
CRF model than the 4-tag set. In our experi-
ments, we test both the 6-tag set and the 4-tag 
set, and the 6-tag set truly has a better result. The 
6-tag set is defined as below: 
T = {B, B2, B3, M, E, S} 
Here B, B2, B3, M, E represent the first, 
second, third, continuing and end character posi-
tions in a multi-character word, and S is the sin-
gle-character word tag. 
We adopt 6 n-gram feature templates as fea-
tures. Some researches have proved that the 
combination of 6-tag set and 6 n-gram feature 
template can achieve a better performance (Zhao 
et al, 2006a; Zhao et al, 2006b; Zhao and Kit, 
2007). 
The 6 n-gram feature templates used in our 
system are C-1, C0, C1, C-1C0, C0C1, C-1C1. Here 
C stands for a character and the subscripts -1, 0 
and 1 stand for the previous, current and next 
character, respectively. 
Furthermore, we try to take advantage of the 
types for the characters. For example, in our sys-
tem D stands for the date, N stands for the num-
ber, L stands for the letter, P stands for the punc-
tuation and C stands for the other characters. 
Introducing these features is beneficial to the 
CRFs learning.  
2.3 Re-labeling step 
Since the unlabeled corpora belong to different 
domains, traditional methods have some limita-
tions. In this section, we propose an additional 
step to make good use of the unlabelled data for 
this special task. This step is based on the out-
puts of the CRFs model in the previous step. 
After CRFs learning, we get a training mod-
el. With this model, we can label the literature, 
computer, medicine and finance corpora. Ac-
cording to the outputs of the CRFs model, we 
choose some labeled sentences with high confi-
dence and add them to the training corpus. Here 
the selection of high confidence must guarantee 
that the probability of the sentences selected be-
ing correct segmentations is rather high and the 
number of the sentences selected is not too little 
or they will make no difference to the generation 
of the new CRF model. Since the existing train-
ing model does not contain the information in 
the out-of-domain data, we treat the labeled sen-
tences with high confidence as additional train-
ing corpus. Then we re-train the CRFs model 
with the new training data. With the training da-
ta extracted from different domains, the training 
model incorporates more cross-domain informa-
tion and it can work better in the corresponding 
cross-domain prediction task. 
3 Experiments 
3.1 Experiment Setup 
There are two sources for the corpora: the train-
ing corpora and the test corpus. And in the train-
ing corpora, there exist two types of corpus in 
this task. The labeled corpus is Chinese text 
which has been segmented into words while the 
unlabelled corpus covers two domains: literature 
and computer science. The test corpus contains 4 
domains, which are literature, computer science, 
medicine and finance. 
There are four evaluation metrics used in 
this bake-off task: Precision, Recall, F1 measure 
(F1 = 2RP/(R+P)) and OOV measure, where R 
and P are the recall and precision of the segmen-
tation and OOV (Out-Of-Vocabulary Word) is a 
word which occurs in the reference corpus but 
does not occur in the labeled training corpus. 
Our system uses the CRF++ package Ver-
sion 0.49 implemented by Taku Kudo 1  from 
sourceforge. 
3.2 Results and Discussions 
We test the techniques described in section 2 
with the given data. Now we will show the re-
sults of each operation. 
3.2.1  Preprocessing 
As we have mentioned in section 2.1, the first 
step is to cut the text into a series of sentences. 
                                               
1 http://crfpp.sourceforge.net/ 
Then we should give each character in one sen-
tence a label. Before this step, it is necessary to 
do atomic segmentation. And we will regard the 
continuous non-Chinese characters as a whole 
and give the whole part a single label. This is 
meaningful to those corpora containing a lot of 
English words. Due to the diversity of the Eng-
lish words, segmenting the sentences with a lot 
non-Chinese characters correctly is rather diffi-
cult only through CRF learning. We should do 
atomic segmentation to all training and test cor-
pora. This may achieve a higher accuracy in a 
certain degree. 
The results of word segmentation are re-
ported in Table 1. ?Clouse+/-? indicates whether 
text clause has been done. 
 
Table 1: Results with clause and without clause 
 corpus Precision Recall F 
Literature 
Clause+ 0.922 0.916 0.919 
Clause- 0.921 0.915 0.918 
Computer 
Clause+ 0.934 0.939 0.937 
Clause- 0.934 0.939 0.936 
Medicine 
Clause+ 0.911 0.917 0.914 
Clause- 0.509 0.511 0.510 
Finance 
Clause+ 0.940 0.943 0.941 
Clause- 0.933 0.940 0.937 
 
From Table 1, we can see there is some im-
provement in different degree and the effect in 
the medicine corpus is the most obvious. So we 
can conclude that our preprocessing is useful to 
the word segmentation. 
3.2.2 CRF-based labeling 
After preprocessing, we can use CRF++ package 
to learn and test.  
The selection of feature template is also an 
important factor. For the purpose of comparison, 
we test two kinds of feature templates in our sys-
tem. The one is showed in Table 2 and the other 
one is showed in Table 3. 
Table 2: Template 1 
# Unigram 
U00:%x[-1,0] 
U01:%x[0,0] 
U02:%x[1,0] 
U03:%x[-1,0]/%x[0,0] 
U04:%x[0,0]/%x[1,0] 
U05:%x[-1,0]/%x[1,0] 
# Bigram 
B 
 
Table 3: Template 2 
# Unigram 
U00:%x[-1,0] 
U01:%x[0,0] 
U02:%x[1,0] 
U03:%x[-1,0]/%x[0,0] 
U04:%x[0,0]/%x[1,0] 
U05:%x[-1,0]/%x[1,0] 
U10:%x[-1,1] 
U11:%x[0,1] 
U12:%x[1,1] 
U13:%x[-1,1]/%x[0,1] 
U14:%x[0,1]/%x[1,1] 
U15:%x[-1,1]/%x[1,1] 
# Bigram 
B 
 
Now we will explain the meanings of the 
templates. Here is an example. In table 4, we 
show the format of the input file. The first col-
umn represents the word itself and the second 
represents the feature of the word, where there 
are five kinds of features: date (D), number (N), 
letter (L), punctuation (P) and others (C). The 
meanings of the templates are showed in table 5. 
 
Table 4: the format of the input file for CRF 
? C 
? D 
? C 
? C 
? P 
? C 
? C 
? C 
1 N 
? C 
? P 
 
Table 5: the example of the templates 
template Expanded feature 
%x[0,0] ? 
%x[0,1] C 
%x[1,0] ? 
%x[-1,0] ? 
%x[-1,0]/ %x[0,0] ?/? 
%x[0,0]/ %x[0,1] ?/C 
With two different feature templates, we con-
tinue our experiments in the four different do-
mains. The segmentation performances of our 
system on test corpora using different feature 
templates are presented in Table 6.  
 
Table 6: Results with different feature templates 
 corpus Precision Recall F 
Literature 
T1 0.917 0.909 0.913 
T2 0.922 0.916 0.919 
Computer 
T1 0.914 0.902 0.908 
T2 0.934 0.939 0.937 
Medicine 
T1 0.906 0.905 0.905 
T2 0.911 0.917 0.914 
Finance 
T1 0.937 0.925 0.931 
T2 0.940 0.943 0.941 
 
Here T1 stands for Template 1 while T2 
stands for Template 2. 
From the Table 4 we can see the second fea-
ture templates make the results of the segmenta-
tion improved more significantly. 
At the same time we need get the outputs with 
confidence measure by setting some parameters 
in CRF test. 
3.2.3 Re-labeling 
As for the outputs with confidence measure 
generated by previous step, we should do some 
special processes. Here we set a particular value 
as our standard and choose the sentences with 
confidence above the value. As we know, the 
test corpora are limited, the higher confidence 
may cause the corpora meeting our requirements 
are less. The lower confidence may not guaran-
tee the reliability. So the setting of the confi-
dence value is very significant. In our experi-
ments, we set the parameter at 0.8. 
Then we add the sentences whose confidence 
is above 0.8 to the training corpus. We should 
re-learn with new corpora, generate the new 
model and re-test the corpora related with 4 do-
mains. The segmentation performances after re-
labeling are represented in Table 7. 
 
Table 7: Results with re-labeling and without re-
labeling 
 corpus Precision Recall F 
Literature 
Re + 0.922 0.916 0.919 
Re - 0.921 0.916 0.918 
Computer 
Re + 0.934 0.939 0.937 
Re - 0.932 0.934 0.933 
Medicine 
Re + 0.911 0.917 0.914 
Re - 0.912 0.918 0.915 
Finance 
Re + 0.940 0.943 0.941 
Re - 0.937 0.941 0.939 
 
Here Re+/- indicates whether the re-labeling 
step is to be done. 
From the results we know, even though the re-
labeling step makes the results in the medicine 
corpus a little worse, it has much better effect in 
the other corpora. Overall, the operation of re-
labeling is necessary. 
3.3 Our results in this bakeoff 
In this task, our results are showed in Table 8. 
 
Table 8: our results in this bakeoff 
 Precision Recall F 
Literature 0.922 0.916 0.919 
Computer 0.934 0.939 0.937 
Medicine 0.911 0.917 0.914 
Finance 0.940 0.943 0.941 
 
From Table 6, we can see our system can 
achieve a high precision, especially in the do-
mains of computer and finance.  This proves our 
methods are fairly effective. 
4 Discussion 
4.1 Segmentation Features 
In our system, we only take advantage of the 
features of the words. We try to add other fea-
tures to our experiments such as AV feature 
(Feng et al, 2004a; Feng et al, 2004b; Hai Zhao 
et al, 2007) with the expectation of improving 
the results. But the results are not satisfying. We 
believe that the feature of words frequency may 
be an important factor, but how to use it is worth 
studying. So finding some meaningful and effec-
tive features is the crucial point. 
4.2 OOV 
In our system, we do not process the words 
out of vocabulary in the special way. The recog-
nition of OOV is still a problem. In a word, there 
is still much to be done to improve our system. 
In the present work, we make use of some sur-
face features, and further study should be con-
tinued to find more effective features. 
5 Conclusion 
In this paper, we have briefly described the 
Chinese word segmentation for out-of-domain 
texts. The CRFs model is implemented. In order 
to make the best use of the test corpora, some 
special strategies are introduced. Further im-
provement is made with these strategies. How-
ever, there is still much to do to achieve more 
improvement. From the results, we got good ex-
perience and knew the weaknesses of our system. 
These all help to improve the performance of our 
system in the future. 
Acknowledgements 
The research described in this paper was sup-
ported by NSFC (Grant No. 60875033). 
References 
Hai Zhao, Changning Huang, and Mu Li. 2006. An 
improved Chinese Word Segmentation System 
with Conditional Random Field. Proceedings of 
the Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia. 
Hai Zhao and Chunyu Kit. 2007. Incorporating global 
information into supervised for Chinese word 
segmentation. In PACALING-2007, Melbourne, 
Australia. 
Hai Zhao, Changning Huang, Mu Li and Bao-Liang 
Lu. 2006b. Effective tag set selection in Chinese 
word segmentation via conditional random field 
modeling. In PACLIC-20, Wuhan, China. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of ICML 2001, Mor-
gan Kaufmann, San Francisco, CA 
Hai Zhao and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character 
Tagging for Word Segmentation and Named Enti-
ty Recognition. Processing of the Sixth SIGHAN 
Workshop on Chinese Language Processing, Hy-
derabad, India. 
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin 
Zheng. 2004a. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics. 
Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie 
Deng. 2004b. Unsupervised segmentation of Chi-
nese corpus using accessor variety. In First Inter-
national Joint Conference on Natural Language 
Processing. Sanya, Hainan Island, China. 
