Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 9?12,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
XLike Project Language Analysis Services
Xavier Carreras
?
, Llu??s Padr
?
o
?
, Lei Zhang
?
, Achim Rettinger
?
, Zhixing Li
1
,
Esteban Garc??a-Cuesta

,
?
Zeljko Agi
?
c
?
, Bo?zo Bekavac
/
, Blaz Fortuna
?
, Tadej
?
Stajner
?
? Universitat Polit`ecnica de Catalunya, Barcelona, Spain.  iSOCO S.A. Madrid, Spain.
/ University of Zagreb, Zagreb, Croatia. ? University of Potsdam, Germany.
? Jo?zef Stefan Institute, Ljubljana, Slovenia. 1 Tsinghua University, Beijing, China.
? Karlsruhe Institute of Technology, Karlsruhe, Germany.
Abstract
This paper presents the linguistic analysis
infrastructure developed within the XLike
project. The main goal of the imple-
mented tools is to provide a set of func-
tionalities supporting the XLike main ob-
jectives: Enabling cross-lingual services
for publishers, media monitoring or de-
veloping new business intelligence appli-
cations. The services cover seven major
and minor languages: English, German,
Spanish, Chinese, Catalan, Slovenian, and
Croatian. These analyzers are provided
as web services following a lightweigth
SOA architecture approach, and they are
publically accessible and shared through
META-SHARE.
1
1 Introduction
Project XLike
2
goal is to develop technology able
to gather documents in a variety of languages and
genres (news, blogs, tweets, etc.) and to extract
language-independent knowledge from them, in
order to provide new and better services to pub-
lishers, media monitoring, and business intelli-
gence. Thus, project use cases are provided by
STA (Slovenian Press Agency) and Bloomberg, as
well as New York Times as an associated partner.
Research partners in the project are Jo?zef Ste-
fan Institute (JSI), Karlsruhe Institute of Technol-
ogy (KIT), Universitat Polit`ecnica de Catalunya
(UPC), University of Zagreb (UZG), and Tsinghua
University (THU). The Spanish company iSOCO
is in charge of integration of all components de-
veloped in the project.
This paper deals with the language technology
developed within the project XLike to convert in-
1
accessible and shared here means that the services are
publicly callable, not that the code is open-source.
http://www.meta-share.eu
2
http://www.xlike.org
put documents into a language-independent rep-
resentation that afterwards enables knowledge ag-
gregation.
To achieve this goal, a bench of linguistic pro-
cessing pipelines is devised as the first step in the
document processing flow. Then, a cross-lingual
semantic annotation method, based on Wikipedia
and Linked Open Data (LOD), is applied. The
semantic annotation stage enriches the linguistic
anaylsis with links to knowledge bases for differ-
ent languages, or links to language independent
representations.
2 Linguistic Analyzers
Apart from basic state-of-the-art tokenizers, lem-
matizers, PoS/MSD taggers, and NE recogniz-
ers, each pipeline requires deeper processors able
to build the target language-independent seman-
tic representantion. For that, we rely on three
steps: dependency parsing, semantic role label-
ing and word sense disambiguation. These three
processes, combined with multilingual ontologi-
cal resouces such as different WordNets and Pred-
icateMatrix (L?opez de la Calle et al., 2014), a
lexical semantics resource combining WordNet,
FrameNet, and VerbNet, are the key to the con-
struction of our semantic representation.
2.1 Dependency Parsing
We use graph-based methods for dependency
parsing, namely, MSTParser
3
(McDonald et al.,
2005) is used for Chinese and Croatian, and
Treeler
4
is used for the other languages. Treeler is
a library developed by the UPC team that imple-
ments several statistical methods for tagging and
parsing.
We use these tools in order to train dependency
parsers for all XLike languages using standard
available treebanks.
3
http://sourceforge.net/projects/mstparser
4
http://treeler.lsi.upc.edu
9
2.2 Semantic Role Labeling
As with syntactic parsing, we are developing SRL
methods with the Treeler library. In order to train
models, we will use the treebanks made available
by the CoNLL-2009 shared task, which provided
data annotated with predicate-argument relations
for English, Spanish, Catalan, German and Chi-
nese. No treebank annotated with semantic roles
exists for Slovene or Croatian. A prototype of
SRL has been integrated in all pipelines (except
the Slovene and Croatian pipelines). The method
implemented follows a pipeline architecture de-
scribed in (Llu??s et al., 2013).
2.3 Word Sense Disambiguation
Word sense disambiguation is performed for all
languages with a publicly available WordNet. This
includes all languages in the project except Chi-
nese. The goal of WSD is to map specific lan-
guages to a common semantic space, in this case,
WN synsets. Thanks to existing connections be-
tween WN and other resources, SUMO and Open-
CYC sense codes are also output when available.
Thanks to PredicateMatrix, the obtained con-
cepts can be projected to FrameNet, achieving a
normalization of the semantic roles produced by
the SRL (which are treebank-dependent, and thus,
not the same for all languages). The used WSD
engine is the UKB (Agirre and Soroa, 2009) im-
plementation provided by FreeLing (Padr?o and
Stanilovsky, 2012).
2.4 Frame Extraction
The final step is to convert all the gathered linguis-
tic information into a semantic representation. Our
method is based on the notion of frame: a seman-
tic frame is a schematic representation of a situ-
ation involving various participants. In a frame,
each participant plays a role. There is a direct cor-
respondence between roles in a frame and seman-
tic roles; namely, frames correspond to predicates,
and participants correspond to the arguments of
the predicate. We distinguish three types of par-
ticipants: entities, words, and frames.
Entities are nodes in the graph connected to
real-world entities as described in Section 3.
Words are common words or concepts, linked to
general ontologies such as WordNet. Frames cor-
respond to events or predicates described in the
document. Figure 1 shows an example sentence,
the extracted frames and their arguments.
It is important to note that frames are a more
general representation than SVO-triples. While
SVO-triples represent a binary relation between
two participants, frames can represent n-ary rela-
tions (e.g. predicates with more than two argu-
ments, or with adjuncts). Frames also allow repre-
senting the sentences where one of the arguments
is in turn a frame (as is the case with plan to make
in the example).
Finally, although frames are extracted at sen-
tence level, the resulting graphs are aggregated
in a single semantic graph representing the whole
document via a very simple coreference resolution
based on detecting named entity aliases and repe-
titions of common nouns. Future improvements
include using an state-of-the-art coreference reso-
lution module for languages where it is available.
3 Cross-lingual Semantic Annotation
This step adds further semantic annotations on top
of the results obtained by linguistic processing.
All XLike languages are covered. The goal is
to map word phrases in different languages into
the same semantic interlingua, which consists of
resources specified in knowledge bases such as
Wikipedia and Linked Open Data (LOD) sources.
Cross-lingual semantic annotation is performed in
two stages: (1) first, candidate concepts in the
knowledge base are linked to the linguistic re-
sources based on a newly developed cross-lingual
linked data lexica, called xLiD-Lexica, (2) next
the candidate concepts get disambiguated based
on the personalized PageRank algorithm by utiliz-
ing the structure of information contained in the
knowledge base.
The xLiD-Lexica is stored in RDF format and
contains about 300 million triples of cross-lingual
groundings. It is extracted from Wikipedia dumps
of July 2013 in English, German, Spanish, Cata-
lan, Slovenian and Chinese, and based on the
canonicalized datasets of DBpedia 3.8 contain-
ing triples extracted from the respective Wikipedia
whose subject and object resource have an equiv-
alent English article.
4 Web Service Architecture Approach
The different language functionalities are imple-
mented following the service oriented architec-
ture (SOA) approach defined in the project XLike.
Therefore all the pipelines (one for each language)
have been implemented as web services and may
10
Figure 1: Graphical representation of frames in the sentence Acme, based in New York, now plans to
make computer and electronic products.
be requested to produce different levels of analy-
sis (e.g. tokenization, lemmatization, NERC, pars-
ing, relation extraction). This approach is very ap-
pealing due to the fact that it allows to treat ev-
ery language independently and execute the whole
language analysis process at different threads or
computers allowing an easier parallelization (e.g.
using external high perfomance platforms such as
Amazon Elastic Compute Cloud EC2
5
) as needed.
Furthermore it also provides independent develop-
ment lifecycles for each language which is crucial
in this type of research projects. Recall that these
web services can be deployed locally or remotely,
maintaining the option of using them in a stand-
alone configuration.
The main structure for each one of the pipelines
is described below:
? Spanish, English, and Catalan: all mod-
ules are based on FreeLing (Padr?o and
Stanilovsky, 2012) and Treeler.
? German: German shallow processing is
based on OpenNLP
6
, Stanford POS tagger
and NE extractor (Toutanova et al., 2003;
Finkel et al., 2005). Dependency parsing,
semantic role labeling, word sense disam-
biguation, and SRL-based frame extraction
are based on FreeLing and Treeler.
? Slovene: Slovene shallow processing is pro-
vided by JSI Enrycher
7
(
?
Stajner et al., 2010),
which consists of the Obeliks morphosyntac-
tic analysis library (Gr?car et al., 2012), the
LemmaGen lemmatizer (Jur?si?c et al., 2010)
and a CRF-based entity extractor (
?
Stajner et
al., 2012). Dependency parsing, word sense
5
http://aws.amazon.com/ec2/
6
http://opennlp.apache.org
7
http://enrycher.ijs.si
disambiguation are based on FreeLing and
Treeler. Frame extraction is rule-based since
no SRL corpus is available for Slovene.
? Croatian: Croatian shallow processing is
based on proprietary tokenizer, POS/MSD-
tagging and lemmatisaton system (Agi?c et
al., 2008), NERC system (Bekavac and
Tadi?c, 2007) and dependency parser (Agi?c,
2012). Word sense disambiguation is based
on FreeLing. Frame extraction is rule-based
since no SRL corpus is available for Croatian.
? Chinese: Chinese shallow and deep process-
ing is based on a word segmentation compo-
nent ICTCLAS
8
and a semantic dependency
parser trained on CSDN corpus. Then, rule-
based frame extraction is performed (no SRL
corpus nor WordNet are available for Chi-
nese).
Each language analysis service is able to pro-
cess thousands of words per second when per-
forming shallow analysis (up to NE recognition),
and hundreds of words per second when produc-
ing the semantic representation based on full anal-
ysis. Moreover, the web service architecture en-
ables the same server to run a different thread for
each client, thus taking advantage of multiproces-
sor capabilities.
The components of the cross-lingual semantic
annotation stage are:
? xLiD-Lexica: The cross-lingual groundings
in xLiD-Lexica are translated into RDF data
and are accessible through a SPARQL end-
point, based on OpenLink Virtuoso
9
as the
back-end database engine.
8
http://ictclas.org/
9
http://virtuoso.openlinksw.com/
11
? Semantic Annotation: The cross-lingual se-
mantic annotation service is based on the
xLiD-Lexica for entity mention recognition
and the JUNG Framework
10
for graph-based
disambiguation.
5 Conclusion
We presented the web service based architecture
used in XLike FP7 project to linguistically ana-
lyze large amounts of documents in seven differ-
ent languages. The analysis pipelines perform ba-
sic processing as tokenization, PoS-tagging, and
named entity extraction, as well as deeper analy-
sis such as dependency parsing, word sense disam-
biguation, and semantic role labelling. The result
of these linguistic analyzers is a semantic graph
capturing the main events described in the docu-
ment and their core participants.
On top of that, the cross-lingual semantic an-
notation component links the resulting linguistic
resources in one language to resources in a knowl-
edge bases in any other language or to language
independent representations. This semantic repre-
sentation is later used in XLike for document min-
ing purposes such as enabling cross-lingual ser-
vices for publishers, media monitoring or devel-
oping new business intelligence applications.
The described analysis services are currently
available via META-SHARE as callable RESTful
services.
Acknowledgments
This work was funded by the European Union
through project XLike (FP7-ICT-2011-288342).
References
?
Zeljko Agi?c, Marko Tadi?c, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
?
Zeljko Agi?c. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1?12,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th conference of the European
chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
10
Java Universal Network/Graph Framework
http://jung.sourceforge.net/
Bo?zo Bekavac and Marko Tadi?c. 2007. Implementa-
tion of Croatian NERC system. In Proceedings of
the Workshop on Balto-Slavonic Natural Language
Processing (BSNLP2007), Special Theme: Informa-
tion Extraction and Enabling Technologies, pages
11?18. Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL?05), pages 363?370.
Miha Gr?car, Simon Krek, and Kaja Dobrovoljc. 2012.
Obeliks: statisti?cni oblikoskladenjski ozna?cevalnik
in lematizator za slovenski jezik. In Zbornik Osme
konference Jezikovne tehnologije, Ljubljana, Slove-
nia.
Matjaz Jur?si?c, Igor Mozeti?c, Tomaz Erjavec, and Nada
Lavra?c. 2010. Lemmagen: Multilingual lemmati-
sation with induced ripple-down rules. Journal of
Universal Computer Science, 16(9):1190?1214.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics, 1:219?230.
Maddalen L?opez de la Calle, Egoitz Laparra, and Ger-
man Rigau. 2014. First steps towards a predicate
matrix. In Proceedings of the Global WordNet Con-
ference (GWC 2014), Tartu, Estonia, January. GWA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 91?98, Ann Ar-
bor, Michigan, June.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Tadej
?
Stajner, Delia Rusu, Lorand Dali, Bla?z Fortuna,
Dunja Mladeni?c, and Marko Grobelnik. 2010. A
service oriented framework for natural language text
enrichment. Informatica, 34(3):307?313.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Lin- guistics on Human Language Technology
(NAACL?03).
Tadej
?
Stajner, Toma?z Erjavec, and Simon Krek.
2012. Razpoznavanje imenskih entitet v slovenskem
besedilu. In In Proceedings of 15th Internation
Multiconference on Information Society - Jezikovne
Tehnologije, Ljubljana, Slovenia.
12
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 465?470,
Dublin, Ireland, August 23-24, 2014.
Potsdam: Semantic Dependency Parsing by Bidirectional Graph-Tree
Transformations and Syntactic Parsing
?
Zeljko Agi
?
c
University of Potsdam
zagic@uni-potsdam.de
Alexander Koller
University of Potsdam
koller@ling.uni-potsdam.de
Abstract
We present the Potsdam systems that par-
ticipated in the semantic dependency pars-
ing shared task of SemEval 2014. They
are based on linguistically motivated bidi-
rectional transformations between graphs
and trees and on utilization of syntactic de-
pendency parsing. They were entered in
both the closed track and the open track
of the challenge, recording a peak average
labeled F
1
score of 78.60.
1 Introduction
In the semantic dependency parsing (SDP) task of
SemEval 2014, the meaning of a sentence is repre-
sented in terms of binary head-argument relations
between the lexical units ? bi-lexical dependencies
(Oepen et al., 2014). Since words can be seman-
tic dependents of multiple other words, this frame-
work results in graph representations of sentence
meaning. For the SDP task, three such annotation
layers are provided on top of the WSJ text of the
Penn Treebank (PTB) (Marcus et al., 1993):
? DM: the reduction of DeepBank HPSG anno-
tation (Flickinger et al., 2012) into bi-lexical
dependencies following (Oepen and L?nning,
2006; Ivanova et al., 2012),
? PAS: the predicate-argument structures derived
from the training set of the Enju HPSG parser
(Miyao et al., 2004) and
? PCEDT: a subset of the tectogrammatical anno-
tation layer from the English side of the Prague
Czech-English Dependency Treebank (Cinkov?a
et al., 2009).
The three annotation schemes provide three di-
rected graph representations for each PTB sen-
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
tence, with word forms as nodes and labeled de-
pendency relations as edges pointing from func-
tors to arguments. The SDP-annotated PTB text is
split into training (sections 00?19), development
(sec. 20) and testing sets (sec. 21). This in turn
makes the SDP parsing task a problem of data-
driven graph parsing, in which systems are to be
trained for producing dependency graph represen-
tations of sentences respecting the three underly-
ing schemes.
While a number of theoretical and preliminary
contributions to data-driven graph parsing exist
(Sagae and Tsujii, 2008; Das et al., 2010; Jones
et al., 2013; Chiang et al., 2013; Henderson et
al., 2013), our goal here is to investigate the sim-
plest approach that can achieve competitive per-
formance. Our starting point is the observation
that the SDP graphs are relatively tree-like. On it,
we build a system for data-driven graph parsing by
(1) transforming dependency graphs into depen-
dency trees in preprocessing, (2) training and us-
ing syntactic dependency parsers over these trees
and (3) transforming their output back into graphs
in postprocessing. This way, we inherit the accu-
racy and speed of syntactic dependency parsers.
The secondary benefit is insight into the struc-
ture of the semantic representations, as graph-tree
transformations can make the phenomena that re-
quire non-tree-like structures more explicit.
2 Data and Systems
We present the basic statistics for the SDP train-
ing sets in Table 1. The graphs contain no cycles,
i.e., all SDP meaning representations are directed
acyclic graphs (DAGs). DM and PAS are auto-
matically derived from HPSG annotations, while
PCEDT is based on manual tectogrammatical an-
notation. This is reflected in more than half of the
PCEDT graphs being disjoint sets of dependency
trees, i.e., forests. The number of forests in DM
and PAS is negligible, on the other hand. The edge
465
Feature DM PAS PCEDT
Sentences 32,389 32,389 32,389
Tokens 742,736 742,736 742,736
Edge labels 52 43 71
Cyclic graphs 0 0 0
Forests 810 418 18,527
Treewidth (undirected) 1.30 1.71 1.45
Tree labels
LOCAL 79 77 124
DFS 79 81 133
Table 1: Basic statistics for the training sets.
label set of PCEDT is also substantially larger than
the label sets of DM and PAS.
2.1 Baseline
A directed acyclic graph is a dependency tree in
the sense of (Nivre, 2006) if any two nodes are
connected by exactly one simple path. In other
words, a DAG is a dependency tree if there are
no disconnected (singleton) nodes and if there are
no node reentrancies, i.e., all nodes have an in-
degree of 1. We calculate the average treewidth
of SDP graphs by converting them to undirected
graphs and applying the algorithm of (Gogate and
Dechter, 2004). As we show in Table 1, the
treewidth is low for all three representations. The
low treewidth indicates that, even if the SDP se-
mantic representations are graphs and not trees,
these graphs are very tree-like and, as such, easily
transformed into trees as there are not many edges
that would require deletion. Thus, one could per-
form a lossy graph-to-tree conversion by (a) de-
tecting singleton nodes and attaching them triv-
ially and (b) detecting reentrant nodes and deleting
all but one incoming edge.
The official SDP baseline system
1
(Oepen et al.,
2014) is based precisely on this principle: single-
tons are attached to their right neighbors, only the
edges to the closest predicates are kept for reen-
trant nodes, with a preference for leftward predi-
cates in ties, and all remaining nodes with an in-
degree of 0 are attached to the root. Two dummy
labels are introduced in the process: root for at-
tachments to root and null for the remaining new
attachments. The baseline is thus limited by the
lossy approach to graph-to-tree reductions and the
lack of linguistic motivation for these particular re-
duction operations. Here, we aim at introducing
1
http://alt.qcri.org/semeval2014/
task8/index.php?id=evaluation
Figure 1: Distributions of node indegrees for (a)
all nodes and (b) source nodes of edges participat-
ing in reentrancies.
Figure 2: Distributions of parts of speech for reen-
trancy source nodes with zero indegree. Ten most
frequent parts of speech are displayed.
less lossy and more linguistically motivated reduc-
tions.
2.2 Local Edge Flipping
Furthermore, inspecting the distribution of node
indegrees in the SDP data in Figure 1, we make
two important observations: (1) from its left his-
togram, that most of the nodes in all three annota-
tions have an indegree of 0 or 1, and (2) from its
right histogram, that most source nodes of edges
causing reentrancies themselves have an indegree
of 0. Figure 2 deepens this observation by provid-
ing a part-of-speech distribution of source nodes
in reentrancies. It shows that the edges in DM
and PAS are systematically pointed from modi-
466
System DM PAS PCEDT
BASELINE 66.19 57.66 90.70
LOCAL 89.93 88.73 91.86
DFS 95.52 93.98 92.85
Table 2: Upper bound LF scores on the develop-
ment set for LOCAL and DFS conversion compared
to the baseline. This score indicates the quality of
graph-tree transformation as no parsing is done.
Dataset P R F
1
DM 73.30 62.99 67.76
PAS 76.03 72.12 74.02
PCEDT 79.40 78.52 78.96
Table 3: Top node detection accuracy with CRFs
on the development set for the three annotations.
Precision (P), recall (R) and the F
1
scores relate to
marking tokens with the binary top node flag.
fiers to modifiees, while coordinating conjunctions
in PCEDT introduce the coordinated nodes. We
conclude that edges in reentrancies, for which the
source nodes have zero indegree, could be flipped
by changing places of their source and target nodes
and encoding the switch in the edge labels by ap-
pending the suffix flipped to the existing labels.
This is the basis for our first system: LOCAL.
In it, we locally flip all edges in reentrancies for
which the source node has zero indegree and run
the BASELINE conversion on the resulting graphs.
We apply this conversion on the training data, use
the converted training sets to train syntactic de-
pendency parsers (Bohnet, 2010) and utilize the
parsing models on the development and test data.
The parsing outputs are converted back to graphs
by simply re-flipping all the edges denoted as
flipped.
2.3 Depth-first Edge Flipping
Our second system, DFS, is based on depth-first
search graph traversal and edge flipping. In it, we
create a undirected copy of the input graph and
connect all nodes with zero indegree to the root us-
ing dummy edges. We do a depth-first traversal of
this graph, starting from the root, while perform-
ing edge lookup in the original DAG. For each DFS
edge traversal in the undirected copy, we check if
the direction of this edge in the original DAG is
identical or reversed to the traversal direction. If
it is identical, we keep the existing edge. If we
traverse the edge against its original direction, we
DM PAS PCEDT
closed LAS UAS LAS UAS LAS UAS
LOCAL 79.09 81.35 81.93 83.79 81.16 89.60
DFS 82.02 83.74 87.06 87.93 79.94 88.04
open
LOCAL 80.86 82.73 85.16 86.18 82.04 90.79
DFS 84.23 85.77 88.42 89.26 80.82 89.02
Table 4: Syntactic dependency parsing accuracy
of our systems before the tree-to-graph transfor-
mations, given as a set of labeled (LAS) and un-
labeled (UAS) attachment scores. The scores are
given for the development set.
reverse it. Finally, we delete the dummy edges and
convert the resulting graph to a dependency tree by
running the baseline, to connect the singletons to
their neighbors, and to attach predicates with zero
indegree and sentence-final nodes to the root.
We illustrate our graph-to-tree transformations
LOCAL and DFS on a gold standard graph from the
training data in Figure 3. It shows how DFS man-
ages to preserve more edges than LOCAL by per-
forming traversal flipping, while LOCAL flips only
the edges that have source nodes with zero inde-
gree. On the other hand, DFS performs more flip-
ping operations than LOCAL, but as Table 1 shows,
this does not result in substantial increase of the
label sets.
2.4 Parsing and Top Node Detection
The same syntactic parser and top node detector
are used in both LOCAL and DFS. Both systems
ran in the closed SDP track, with no additional
features for learning, and in the open track, where
they used the SDP companion data, i.e., the out-
puts of a syntactic dependency parser (Bohnet and
Nivre, 2012) and phrase-based parser (Petrov et
al., 2006) as additional features. Our choice of
parser was based on the high non-projectivity of
the resulting trees, while parsers of (Bohnet and
Nivre, 2012; Bohnet et al., 2013) could also be
used, among others. We use the parser out of
the box, i.e., without any parameter tuning or ad-
ditional features other than what was previously
listed for the open track.
Top node detection is implemented separately,
by training a sequence labeling model (Lafferty
et al., 2001; Kudo, 2005) on tokens and part-of-
speech tags from the training sets. Its accuracy
is given in Table 3. We use only the tokens and
parts of speech as features for these models, and
467
Figure 3: Illustration of graph-to-tree transformations of a gold standard graph for LOCAL and DFS. Edge
labels are omitted. The sentence (PAS, #20415005): Who that winner will be is highly uncertain.
we design our feature set by adapting the chunking
template from the CRF++ toolkit documentation.
2
We note that this model can be improved by, e.g.,
adding the open track companion features to the
feature set, but they were not used in the experi-
ments we present here.
3
Our graph-to-tree conversions expand the label
sets by appending the edge flip flag. The sizes of
the new label sets are given in Table 1 in compar-
ison to the original ones. The increase in size is
expected to affect the parsing accuracy. The pars-
ing accuracies on the development sets are given
in Table 4. The scores correlate with the label
set sizes, with a notable difference between the la-
beled (LAS) and unlabeled (UAS) attachment score
for PCEDT. The LOCAL approach tends to out-
perform DFS for PCEDT, while DFS parsers also
significantly outperform LOCAL for DM and PAS.
The open track parsers tend to perform a little bet-
ter as they make use of the additional features.
In Table 2, we measure the theoretical maxi-
mum accuracy for parsers based on our two con-
versions in comparison with the baseline. There,
we run BASELINE, LOCAL and DFS on the devel-
opment set and convert the trees back to graphs
right away, i.e., without the parsing step, so as
to observe the dissipation of the conversion. The
scores show that LOCAL and DFS outperform
BASELINE by a large margin, while the maximum
accuracy for DFS is larger than the one for LOCAL,
1 point for PCEDT and around 5 points for DM
and PAS. This is due to DFS performing non-local
edge flipping, thus preserving more edges. The
parsing scores from Table 4 and the maximum ac-
curacy from Table 2 show that our systems are not
2
http://crfpp.googlecode.com/svn/
trunk/doc/index.html
3
The recall would increase by 15 points, amounting to a
10 point increase in F
1
for top node detection in DM.
closed open
dev LF UF LF UF
LOCAL 76.70 82.01 77.87 83.19
DFS 78.49 83.78 80.03 85.31
test
LOCAL 75.94 81.58 76.79 82.52
DFS 77.34 82.99 78.60 84.32
Table 5: Overall accuracy for our LOCAL and DFS
systems, i.e., averaged labeled and unlabeled F
1
scores over the three annotations.
as lossy in graph-tree conversions as the baseline,
while they pay the price in the number of new la-
bels in actual parsing and, subsequently, in the ac-
curacy of the dependency parsers. Thus, LAS and
UAS for the baseline are 1-2 points higher than the
scores in Table 4 for DM and PCEDT, while our
scores are 3-4 points higher for PAS.
3 Results and Discussion
As in the official SDP scoring, we express the
results in terms of labeled and unlabeled preci-
sion (LP, UP) and recall (LR, UR), their harmonic
means, the F
1
scores (LF, UF), and sentence-level
exact matches (LM, UM). The official SDP scorer
reports on two variants of these scores: the one
taking into account the virtual edges to top nodes
and the one excluding those edges. The former is
less relaxed as it requires the top nodes to be pre-
dicted, and this is the only one we use in this re-
port. We note that for our systems, the scores with-
out the virtual edges are approximately 2 points
higher for all the metrics.
The overall scores are given in Table 5. There,
we provide the labeled and unlabeled F
1
scores on
the development and test data in the closed and
open track, averaged for all three annotations. The
open track systems consistently score approxi-
468
closed track DM PAS PCEDT
LP LR LF LM LP LR LF LM LP LR LF LM
LOCAL 83.39 72.88 77.78 4.53 88.18 74.00 80.47 2.00 72.25 67.10 69.58 6.38
DFS 79.36 79.34 79.35 9.05 88.15 81.60 84.75 7.72 69.68 66.25 67.92 5.86
?4.03 +6.46 +1.57 +4.52 ?0.03 +7.60 +4.28 +5.72 ?2.57 ?0.85 ?1.66 ?0.52
UP UR UF UM UP UR UF UM UP UR UF UM
LOCAL 85.47 74.70 79.72 5.04 89.70 75.28 81.86 2.23 86.36 80.21 83.17 19.44
DFS 81.56 81.54 81.55 10.31 89.62 82.96 86.16 7.86 83.37 79.27 81.27 17.51
?3.91 +6.84 +1.83 +5.27 ?0.08 +7.69 +4.30 +5.63 ?3.00 ?0.94 ?1.91 ?1.93
open track DM PAS PCEDT
LP LR LF LM LP LR LF LM LP LR LF LM
LOCAL 84.54 73.80 78.80 4.53 89.72 75.08 81.75 2.00 72.52 67.33 69.83 6.08
DFS 81.32 80.91 81.11 10.46 89.41 82.61 85.88 8.46 70.35 67.33 68.80 5.79
?3.22 +7.11 +2.31 +5.93 ?0.31 +7.53 +4.13 +6.46 ?2.17 +0.00 ?1.03 ?0.29
UP UR UF UM UP UR UF UM UP UR UF UM
LOCAL 86.43 75.45 80.57 5.49 90.99 76.14 82.91 2.30 87.32 81.07 84.08 19.73
DFS 83.37 82.95 83.16 11.94 90.78 83.87 87.19 8.75 84.46 80.83 82.60 18.47
?3.06 +7.50 +2.59 +6.45 ?0.22 +7.73 +4.28 +6.45 ?2.86 ?0.24 ?1.48 ?1.26
Table 6: Breakdown of the scores for our LOCAL and DFS systems on the test sets. We provide labeled
and unlabeled precision (LP, UP), recall (LR, UR), F
1
scores (LF, UF) and exact matches (LM, UM) for
all three annotations in both the closed and the open evaluation track.
mately 1 point higher than their closed track coun-
terparts, apparently taking advantage of the ad-
ditional features available in training and testing.
The DFS system is 2 points better than LOCAL in
all scenarios, owing to the higher maximum cover-
age of the original graphs in the conversions. The
large label sets amount to a difference of approxi-
mately 6 points between the labeled and unlabeled
accuracies in favor of the latter attachment.
Table 6 is a breakdown of the scores in Table 5
across the three annotations and the two tracks.
Here, we pair the F
1
scores with the correspond-
ing precision and recall scores. We also explicitly
denote the differences in scores between LOCAL
and DFS. For DM and PAS, the score patterns
are very similar: due to the larger label set and
less regular edge flipping, DFS has a 3-4 points
lower precision than LOCAL, while its recall is 6-8
points higher, amounting to the overall improve-
ment of approximately 4 points F
1
. In contrast, on
the PCEDT data, LOCAL outperforms DFS by ap-
proximately 1.5 points. We note that the label sets
for PCEDT are much larger than for DM and PAS
and that the favorable reentrancies in PCEDT are
much less frequent to begin with (see Table 1, Ta-
ble 2 and Figure 2). At 14 points F
1
, the discrep-
ancy between the labeled and unlabeled scores is
much higher for PCEDT than for DM and PAS,
for which we observe a 1-2 point difference.
The exact match scores (LM, UM) favor DFS
over LOCAL by approximately 5 points for DM
and PAS, while LOCAL is better than DFS for
PCEDT by 1-2 points. In absolute terms, the PAS
scores are higher than those for DM and PAS in
both our systems. This difference between the
token-level and the sentence-level scores stems
from the properties of our graph-tree transforma-
tions as, e.g., certain edges in undirected cycles
could not be addressed by our edge inversions.
At approximately 81, 86 and 70 points F
1
for
DM, PAS and PCEDT, in this contribution we
have shown that focusing on graph-tree transfor-
mations for the utilization of a syntactic depen-
dency parser lets us achieve good overall perfor-
mance in the semantic dependency parsing task. In
the future, we will further investigate what trans-
formations are appropriate for different styles of
graph-based semantic representations, and what
we can learn from this both for improving SDP
parser accuracy and for making linguistically mo-
tivated design choices for graph-based seman-
tic representations. Furthermore, we will extend
our system to cover inherently non-tree-like struc-
tures, such as those induced by control verbs.
Acknowledgements We are grateful to Stephan
Oepen for all the discussions on the properties of
the SDP datasets, and for providing the infrastruc-
ture for running the systems. We also thank the
anonymous reviewers for their valuable insight.
469
References
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In
Proc. EMNLP-CoNLL, pages 1455?1465.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?ard Farkas, Filip Ginter, and Jan Haji?c. 2013.
Joint Morphological and Syntactic Analysis for
Richly Inflected Languages. TACL, 1:415?428.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proc. COL-
ING, pages 89?97.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing Graphs with Hyperedge
Replacement Grammars. In Proc. ACL, pages
924?932.
Silvie Cinkov?a, Josef Toman, Jan Haji?c, Krist?yna
?
Cerm?akov?a, V?aclav Klime?s, Lucie Mladov?a,
Jana
?
Sindlerov?a, Krist?yna Tom?s?u, and Zden?ek
?
Zabokrtsk?y. 2009. Tectogrammatical Annotation
of the Wall Street Journal. The Prague Bulletin of
Mathematical Linguistics, 92:85?104.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In Proc. NAACL, pages 948?956.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: A Dynamically Annotated Treebank of
the Wall Street Journal. In Proc. TLT, pages 85?96.
Vibhav Gogate and Rina Dechter. 2004. A Complete
Anytime Algorithm for Treewidth. In Proc. UAI,
pages 201?208.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual Joint Pars-
ing of Syntactic and Semantic Dependencies with a
Latent Variable Model. Computational Linguistics,
39(4):949?998.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who Did What to Whom?
A Contrastive Study of Syntacto-Semantic Depen-
dencies. In Proc. Linguistic Annotation Workshop,
pages 2?11.
Bevan Keeley Jones, Sharon Goldwater, and Mark
Johnson. 2013. Modeling Graph Languages with
Grammars Extracted via Tree Decompositions. In
Proc. FSMNLP, pages 54?62.
Taku Kudo. 2005. CRF++: Yet another CRF
toolkit. Software available at http://crfpp.
sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proc. ICML, pages 282?289.
Mitchell Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented Grammar Development for
Acquiring a Head-Driven Phrase Structure Grammar
from the Penn Treebank. In Proc. IJCNLP, pages
684?693.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Stephan Oepen and Jan Tore L?nning. 2006.
Discriminant-Based MRS Banking. In Proc. LREC,
pages 1250?1255.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-Coverage Semantic Dependency Parsing.
In Proceedings of the 8th International Workshop on
Semantic Evaluation.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proc. COLING-
ACL, pages 433?440.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-Reduce
Dependency DAG Parsing. In Proc. COLING, pages
753?760.
470
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 130?140,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Treebank Translation for Cross-Lingual Parser Induction
J
?
org Tiedemann
Dep. of Linguistics and Philology
Uppsala University
jorg.tiedemann@lingfil.uu.se
?
Zeljko Agi
?
c
Linguistics Department
University of Potsdam
zagic@uni-potsdam.de
Joakim Nivre
Dep. of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Cross-lingual learning has become a popu-
lar approach to facilitate the development
of resources and tools for low-density lan-
guages. Its underlying idea is to make
use of existing tools and annotations in
resource-rich languages to create similar
tools and resources for resource-poor lan-
guages. Typically, this is achieved by either
projecting annotations across parallel cor-
pora, or by transferring models from one or
more source languages to a target language.
In this paper, we explore a third strategy
by using machine translation to create syn-
thetic training data from the original source-
side annotations. Specifically, we apply
this technique to dependency parsing, us-
ing a cross-lingually unified treebank for
adequate evaluation. Our approach draws
on annotation projection but avoids the use
of noisy source-side annotation of an unre-
lated parallel corpus and instead relies on
manual treebank annotation in combination
with statistical machine translation, which
makes it possible to train fully lexicalized
parsers. We show that this approach signif-
icantly outperforms delexicalized transfer
parsing.
1 Introduction
The lack of resources and tools is a serious problem
for the majority of the world?s languages (Bender,
2013). Many applications require robust tools and
the development of language-specific resources is
expensive and time consuming. Furthermore, many
tasks such as data-driven syntactic parsing require
strong supervision to achieve reasonable results
for real-world applications, since the performance
of fully unsupervised methods lags behind by a
large margin in comparison with the state of the
art. Cross-lingual learning has been proposed as
one possible solution to quickly create initial tools
for languages that lack the appropriate resources
(Ganchev and Das, 2013). By and large, there
are two main strategies that have been proposed
in the literature: annotation projection and model
transfer.
1.1 Previous Cross-Lingual Approaches
Annotation projection relies on the mapping of lin-
guistic annotation across languages using paral-
lel corpora and automatic alignment as basic re-
sources (Yarowsky et al., 2001; Hwa et al., 2005;
T?ackstr?om et al., 2013a). Tools that exist for the
source language are used to annotate the source
side of the corpus and projection heuristics are then
applied to map the annotation through word align-
ment onto the corresponding target language text.
Target language tools can then be trained on the
projected annotation assuming that the mapping is
sufficiently correct. Less frequent, but also possi-
ble, is the scenario where the source side of the cor-
pus contains manual annotation (Agi?c et al., 2012).
This addresses the problem created by projecting
noisy annotations, but it presupposes parallel cor-
pora with manual annotation, which are rarely avail-
able, and expensive and time-consuming to pro-
duce.
Model transfer instead relies on universal fea-
tures and model parameters that can be transferred
from one language to another. Abstracting away
from all language-specific parameters makes it pos-
sible to train, e.g., delexicalized parsers that ignore
lexical information. This approach has been used
with success for a variety of languages, drawing
from a harmonized POS tagset (Petrov et al., 2012)
that is used as the main source of information. One
advantage compared to annotation projection is
that no parallel data is required. In addition, train-
ing can be performed on gold standard annotation.
However, model transfer assumes a common fea-
130
ture representation across languages (McDonald et
al., 2013), which can be a strong bottleneck. Sev-
eral extensions have been proposed to make the
approach more robust. First of all, multiple source
languages can be involved to increase the statistical
basis for learning (McDonald et al., 2011; Naseem
et al., 2012), a strategy that can also be used in
the case of annotation projection. Cross-lingual
word clusters can be created to obtain additional
universal features (T?ackstr?om et al., 2012). Tech-
niques for target language adaptation can be used
to improve model transfer with multiple sources
(T?ackstr?om et al., 2013b).
1.2 The Translation Approach
In this paper, we propose a third strategy, based
on automatically translating training data to a new
language in order to create annotated resources di-
rectly from the original source. Recent advances
in statistical machine translation (SMT) combined
with the ever-growing availability of parallel cor-
pora are now making this a realistic alternative. The
relation to annotation projection is obvious as both
involve parallel data with one side being annotated.
However, the use of direct translation brings two
important advantages. First of all, using SMT, we
do not accumulate errors from two sources: the tool
? e.g., tagger or parser ? used to annotate the source
language of a bilingual corpus and the noise com-
ing from alignment and projection. Instead, we use
the gold standard annotation of the source language
which can safely be assumed to be of much higher
quality than any automatic annotation obtained by
using a tool trained on that data. Moreover, using
SMT may help in bypassing domain shift problems,
which are common when applying tools trained
(and evaluated) on one resource to text from an-
other domain. Secondly, we can assume that SMT
will produce output that is much closer to the input
than manual translations in parallel texts usually
are. Even if this may seem like a short-coming
in general, in the case of annotation projection it
should rather be an advantage, because it makes it
more straightforward and less error-prone to trans-
fer annotation from source to target. Furthermore,
the alignment between words and phrases is inher-
ently provided as an output of all common SMT
models. Hence, no additional procedures have to be
performed on top of the translated corpus. Recent
research (Zhao et al., 2009; Durrett et al., 2012)
has attempted to address synthetic data creation
for syntactic parsing via bilingual lexica. We seek
to build on this work by utilizing more advanced
translation techniques.
Further in the paper, we first describe the tools
and resources used in our experiments (?2). We
elaborate on our approach to translating treebanks
(?3) and projecting syntactic annotations (?4) for a
new language. Finally, we provide empirical evalu-
ation of the suggested approach (?5) and observe
a substantial increase in parsing accuracy over the
delexicalized parsing baselines.
2 Resources and Tools
In our experiments, we rely on standard resources
and tools for both dependency parsing and ma-
chine translation without any special enhancements.
Since we are primarily trying to provide a proof
of concept for the use of SMT-derived synthetic
training data in dependency parsing, we believe it
is more important to facilitate reproducibility than
to tweak system components to obtain maximum
accuracy.
We use the Universal Dependency Treebank v1
(McDonald et al., 2013) for annotation projection,
parser training and evaluation. It is a collection
of data sets with consistent syntactic annotation
for six languages: English, French, German, Ko-
rean, Spanish, and Swedish.
1
The annotation is
based on Stanford Typed Dependencies for English
(De Marneffe et al., 2006) but has been adapted
and harmonized to allow adequate annotation of
typologically different languages. This is the first
collection of data sets that allows reliable evalua-
tion of labeled dependency parsing accuracy across
multiple languages (McDonald et al., 2013). We
use the dedicated training and test sets from the
treebank distribution in all our experiments. As ar-
gued in (McDonald et al., 2013), most cross-lingual
dependency parsing experiments up to theirs relied
on heterogeneous treebanks such as the CoNLL
datasets for syntactic dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al., 2007a), mak-
ing it difficult to address challenges like consistent
cross-lingual analysis for downstream applications
and reliable cross-lingual evaluation of syntactic
parsers. More specifically, none of the previous
research could report full labeled parsing accura-
cies, but rather just unlabeled structural accuracies
across different attachment schemes. Following
the line of McDonald et al. (2013) regarding the
1
https://code.google.com/p/uni-dep-tb/
131
emphasized importance of homogenous data and
the assignment of labels, we only report labeled
attachment scores (LAS) in all our experiments.
As it is likely the first reliable cross-lingual pars-
ing evaluation, we also choose their results as the
baseline reference point for comparison with our
experiments.
For dependency parsing, we use MaltParser
(Nivre et al., 2006a)
2
due to its efficiency in both
training and parsing, and we facilitate MaltOpti-
mizer (Ballesteros and Nivre, 2012)
3
to bypass the
tedious task of manual feature selection. Malt-
Parser is a transition-based dependency parser
that has been evaluated on a number of different
languages with competitive results (Nivre et al.,
2006b; Nivre et al., 2007b; Hall et al., 2007) and it
is widely used for benchmarking and application
development. Although more accurate dependency
parsers exist for the task of monolingual supervised
parsing, it is not clear that these differences carry
over to the cross-lingual scenario, where baselines
are lower and more complex models are more likely
to overfit. The use of a transition-based parser also
facilitates comparison with delexicalized transfer
parsing, where transition-based parsers are domi-
nant so far (McDonald et al., 2011; McDonald et
al., 2013). We leave the exploration of additional
parsing approaches for future research.
For machine translation, we select the popular
Moses toolbox (Koehn et al., 2007) and the phrase-
based translation paradigm as our basic frame-
work. Phrase-based SMT has the advantage of
being straightforward and efficient in training and
decoding, while maintaining robustness and relia-
bility for many language pairs. More details about
the setup and the translation procedures are given
in Section 3 below. The most essential ingredient
for translation performance is the parallel corpus
used for training the translation models. For our
experiments we use the freely available and widely
used Europarl corpus v7 (Koehn, 2005).
4
It is com-
monly used for training SMT models and includes
parallel data for all languages represented in the
Universal Treebank except Korean, which we will,
therefore, leave out in our experiments. For tuning
we apply the newstest 2012 data provided by the an-
nual workshop on statistical machine translation.
5
For language modeling, we use a combination of
2
http://www.maltparser.org/
3
http://nil.fdi.ucm.es/maltoptimizer/
4
http://www.statmt.org/europarl/
5
http://www.statmt.org/wmt14
DE EN ES FR SV
DE 94 M 94 M 96 M 81 M
EN 2.0 M 103 M 105 M 89 M
ES 1.9 M 2.0 M 104 M 89 M
FR 1.9 M 2.0 M 2.0 M 91 M
SV 1.8 M 1.9 M 1.8 M 1.9 M
mono 22.9 M 17.1 M 6.3 M 6.3 M 2.3 M
Table 1: Parallel data and monolingual data used
for training the SMT models. Lower-left triangle
= number of sentence pairs; upper-right triangle
= number of tokens (source and target language
together); bottom row = number of sentences in
monolingual corpora.
Europarl and News data provided from the same
source. The statistics of the corpora are given in
Table 1.
3 Translating Treebanks
The main contribution of this paper is the empirical
study of automatic treebank translation for parser
transfer. We compare three different translation
approaches in order to investigate the influence of
several parameters. All of them are based on auto-
matic word alignment and subsequent extraction of
translation equivalents as common in phrase-based
SMT. In particular, word alignment is performed us-
ing GIZA++ (Och and Ney, 2003) and IBM model
4 as the final model for creating the Viterbi word
alignments for all parallel corpora used in our ex-
periments. For the extraction of translation tables,
we use the Moses toolkit with its standard settings
to extract phrase tables with a maximum of seven
tokens per phrase from a symmetrized word align-
ment. Symmetrization is done using the grow-diag-
final-and heuristics (Koehn et al., 2003). We tune
phrase-based SMT models using minimum error
rate training (Och, 2003) and the development data
for each language pair. The language model is a
standard 5-gram model estimated from the mono-
lingual data using modified Kneser-Ney smoothing
without pruning (applying KenLM tools (Heafield
et al., 2013)).
Our first translation approach is based on a very
simple word-by-word translation model. For this,
we select the most reliable translations of single
words from the phrase translation tables extracted
from the parallel corpora as described above. We
restrict the model to tokens with alphabetic char-
acters only using pre-defined Unicode character
132
sets. The selection of translation alternatives is
based on the Dice coefficient, which combines the
two essential conditional translation probabilities
given in the phrase table. The Dice coefficient is in
fact the harmonic mean of these two probabilities
and has successfully been used for the extraction of
translation equivalents before (Smadja et al., 1996):
Dice(s, t) =
2 p(s, t)
p(s) + p(t)
= 2
(
1
p(s|t)
+
1
p(t|s)
)
?1
Other association measures would be possible as
well but Smadja et al. (1996) argue that the Dice
coefficient is more robust with respect to low fre-
quency events than other common metrics such as
pointwise mutual information, which can be a seri-
ous issue with the unsmoothed probability estima-
tions in standard phrase tables. Our first translation
model then applies the final one-to-one correspon-
dences to monotonically translate treebanks word
by word. We refer to it as the LOOKUP approach.
Note that any bilingual dictionary could have been
used to perform the same procedure.
The second translation approach (WORD-BASED
MT) is slightly more elaborate but still restricts
the translation model to one-to-one word mappings.
For this, we extract all single word translation pairs
from the phrase tables and apply the standard beam-
search decoder implemented in Moses to translate
the original treebanks to all target languages. The
motivation for this model is to investigate the im-
pact of reordering and language models while still
keeping the projection of annotated data as simple
as possible. Note that the language model may
influence not only the word order but also the lex-
ical choice as we now allow multiple translation
options in our phrase table.
The final model implements translation based
on the entire phrase table using the standard ap-
proach to PHRASE-BASED SMT. We basically run
the Moses decoder with default settings and the pa-
rameters and models trained on our parallel corpora.
Note that it is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase segmentation
and word alignment (if the latter is coded into the
phrase tables). This will be important as we will
see in the annotation projection discussed below.
ORIGINAL
DE EN ES FR SV
14.0 0.00 7.90 13.3 4.20
WORD-BASED MT
DE EN ES FR SV
DE ? 49.1 62.6 52.8 60.4
EN 43.3 ? 27.6 34.8 0.00
ES 54.9 25.1 ? 12.3 18.3
FR 68.2 39.6 32.8 ? 57.8
SV 34.1 5.20 21.6 33.7 ?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 51.5 57.3 58.8 46.8
EN 49.3 ? 50.3 61.7 14.6
ES 65.9 66.7 ? 62.8 49.0
FR 58.0 53.7 44.7 ? 38.2
SV 43.9 43.6 49.6 57.1 ?
Table 2: Non-projectivity in synthetic treebanks.
4 Transferring Annotation
The next step in preparing synthetic training data is
to project the annotation from the original treebank
to the target language. Given the properties of a
dependency tree, where every word has exactly one
syntactic head and dependency label, the annota-
tion transfer is trivial for the two initial translation
models. All annotation can simply be copied us-
ing the dictionary LOOKUP in which we enforce
a monotonic one-to-one word mapping between
source and target language.
In the second approach, we only have to keep
track of reordering, which is reported by the de-
coder when translating with our model. Note that
the mapping is strictly one-to-one (bijective) as
phrase-based SMT does not allow deletions or in-
sertions at any point. This also ensures that we
will always maintain a tree structure even though
reordering may have a strong impact on projectiv-
ity (see Table 2). An illustration of this type of
annotation transfer is shown in the left image of
Figure 1.
The third model, full PHRASE-BASED SMT, re-
quires the most attention when transferring anno-
tation across languages. Here we have to rely on
the alignment information and projection heuris-
tics similar to the ones presented in related work
(Hwa et al., 2005). In their work, Hwa et al. (2005)
define a direct projection algorithm that transfers
automatic annotation to a target language via word
alignment. The algorithm defines a number of
133
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Th
at
Go
d
hel
p
him
com
e
in
!
CO
NJ
NO
UN
NO
UN
PR
ON
VE
RB
AD
P
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
exp
lns
ub
j
adp
ob
j
iob
j
roo
tad
pm
od p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
DU
MM
Y
hel
p
DU
MM
Y
DU
MM
Y
him
!
NO
UN
CO
NJ
VE
RB
AD
P
NO
UN
PR
ON
.
exp
l n
sub
j io
bj
roo
t
adp
mo
d
adp
ob
j
p
nsu
bj
exp
l
roo
ta
dpm
od
adp
ob
j
iob
j p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
PN
OU
N
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
hel
p
him
!
NO
UN
VE
RB
PR
ON
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
nsu
bj
roo
t
iob
j
p
Figure 1: Transferring annotation from French to an English translation with a WORD-BASED translation
model (left) and with a PHRASE-BASED translation model (middle and right). Annotation projection using
the Direct Projection Algorithm by Hwa et al. (2005) (middle) and our approach (right).
heuristics to handle unaligned, one-to-many, many-
to-one and many-to-many alignments. As a side ef-
fect, this approach produces several dummy-nodes
in the target language to ensure a complete pro-
jection of the source language tree (see Hwa et al.
(2005) for more details).
In our approach, we try to make use of the addi-
tional information provided by the SMT decoder to
avoid dummy-nodes and relations that may nega-
tively influence the induced target language parser.
Compared to the annotation projection approach
of Hwa et al. (2005), the situation in our PHRASE-
BASED SMT setting is slightly different. Here, we
have two types of alignments that can be considered
when relating source and target language items: (i)
the alignment between phrases (pairs of consec-
utive n-grams) and (ii) the phrase-internal word
alignment on which phrase extraction is based. The
primary information used for annotation transfer
is still the latter which has the same properties as
described by Hwa et al. (2005) (except that we have
truly many-to-many alignments in our data which
were not available in their experiments).
Note that words may be unaligned in phrase-
based SMT as the phrase extraction algorithm used
in Moses includes unaligned adjacent tokens. How-
ever, for these unaligned words, we know to which
phrase they belong and can also identify the corre-
sponding phrase in the other language using phrase
alignment information. This makes it possible to
avoid the creation of dummy-nodes altogether and
instead to link unaligned words to existing nodes
based on the given phrase segmentation.
Similarly, we define heuristics for handling one-
to-many, many-to-one and many-to-many align-
ments that avoid the creation of dummy-nodes. The
main procedure is illustrated in Figure 2.
The key feature of this projection algorithm is
that ambiguous alignments are handled by attach-
ing words to the nodes that are highest up in the
dependency tree (the procedure find highest() re-
turns the node with minimum distance to the root
of the tree). This ensures that we avoid cycles
and isolated cliques in the graph. Furthermore,
unaligned words are attached to the head of the
target phrase they belong to, which seems to be the
most appropriate place without further knowledge.
The procedures in trg phrase() and in src phrase()
make use of the phrase segmentation used in the
translation process.
One complication is the search for the corre-
sponding target head word in cases where the
source language head is not aligned or aligned to
multiple target language words. Figure 3 shows
the head alignment procedure that we define in our
projection algorithm. Procedure find aligned() re-
turns the rightmost word of all words aligned to the
given source language word s. Other heuristics or
linguistically motivated rules based on POS tags
and general language properties would be possible
here as well. If s is not aligned, we move up in
the dependency tree until we hit ROOT or find an
aligned word. If we are at the root position we
return ROOT as this does not require further map-
pings. The effect of this algorithm is illustrated by
the right-hand side image in Figure 1.
5 Parsing Across Languages
In this section, we present the results of two ex-
perimental batches. First, we establish the base-
134
Input: source tree S, target sentence T ,
word alignment A, phrase segmentation P
Output: syntactic heads head[],
word attributes attr[]
1 treeSize = max distance to root(S) ;
2 attr = [] ;
3 head = [] ;
4 for t ? T do
5 if is unaligned trg(t,A) then
6 for t? ? in trg phrase(t,P) do
7 [s
x
,..,s
y
] = aligned to(t?) ;
8 ?s = find highest([s
x
,..,s
y
],S) ;
9
?
t = find aligned(?s,S,T,A) ;
10 attr[t] = DUMMY ;
11 head[t] =
?
t ;
12 end
13 else
14 [s
x
,..,s
y
] = aligned to(t) ;
15 s = find highest([s
x
,..,s
y
],S) ;
16 attr[t] = attr(s) ;
17 ?s = head of(s,S) ;
18
?
t = find aligned(?s,S,T,A) ;
19 if
?
t == t then
20 [s
x
,..,s
y
] = in src phrase(s,P) ;
21 s* = find highest([s
x
,..,s
y
],S) ;
22 ?s = head of(s*,S) ;
23
?
t = find aligned(?s,S,T,A) ;
24 head[t] =
?
t ;
25 end
26 end
27 end
Figure 2: Annotation projection algorithm.
lines by comparing monolingual supervised pars-
ing to delexicalized transfer parsing following the
approach of McDonald et al. (2013). Second, we
present the results obtained with parsers trained
on target language treebanks produced using ma-
chine translation and annotation projection. Here,
we also look at delexicalized models trained on
translated treebanks to show the effect of machine
translation without additional lexical features.
5.1 Baseline Results
First we present the baseline parsing scores. The
baselines we explore are: (i) the monolingual base-
line, i.e., training and testing using the same lan-
guage data from the Universal Dependency Tree-
bank and (ii) the delexicalized baseline, i.e., apply-
ing delexicalized parsers across languages.
For the monolingual baseline, MaltParser mod-
els are trained on the original treebanks with uni-
versal POS labels and lexical features but leaving
out other language-specific features if they exist in
the original treebanks. The delexicalized parsers
are trained on universal POS labels only for each
language and are then applied to all other languages
Input: node s, source tree S with root ROOT,
target sentence T , word alignment A
Output: node t*
1 if s == ROOT then
2 return ROOT ;
3 end
4 while is unaligned src(s,A) do
5 s = head of(s,S) ;
6 if s == ROOT then
7 return ROOT ;
8 end
9 end
10 p = 0 ;
11 t* = undef ;
12 for t? ? aligned(s,A) do
13 if position(t?,T) > p then
14 t* = t? ;
15 p = position(t?,T) ;
16 end
17 end
18 return t* ;
Figure 3: Procedure find aligned().
without modification. For all models, features and
options are optimized using MaltOptimizer. The
accuracy is given in Table 3 as a set of labeled at-
tachment scores (LAS). We include punctuation
in our evaluation. Ignoring punctuation generally
leads to slightly higher scores as we have noted in
our experiments but we do not report those num-
bers here. Note also that the columns represent the
target languages (used for testing), while the rows
denote the source languages (used in training), as
in McDonald et al. (2013).
From the table, we can see that the baseline
scores are compatible with the ones in the orig-
inal experiments presented by (McDonald et al.,
2013), included in Table 3 for reference. The dif-
ferences are due to parser selection, as they use a
transition-based parser with beam search and per-
ceptron learning along the lines of Zhang and Nivre
(2011) whereas we rely on greedy transition-based
parsing with linear support vector machines. In the
following, we will compare results to our baseline
as we have a comparable setup in those experi-
ments. However, most improvements shown below
also apply in comparison with (McDonald et al.,
2013).
5.2 Translated Treebanks
Now we turn to the experiments on translated tree-
banks. We consider two setups. First, we look at
the effect of translation when training delexical-
ized parsers. In this way, we can perform a direct
comparison to the baseline performance presented
135
MONOLINGUAL
DE EN ES FR SV
72.13 87.50 78.54 77.51 81.28
DELEXICALIZED
DE EN ES FR SV
DE 62.71 43.20 46.09 46.09 50.64
EN 46.62 77.66 55.65 56.46 57.68
ES 44.03 46.73 68.21 57.91 53.82
FR 43.91 46.75 59.65 67.51 52.01
SV 50.69 49.13 53.62 51.97 70.22
MCDONALD ET AL. (2013)
DE EN ES FR SV
DE 64.84 47.09 48.14 49.59 53.57
EN 48.11 78.54 56.86 58.20 57.04
ES 45.52 47.87 70.29 63.65 53.09
FR 45.96 47.41 62.56 73.37 52.25
SV 52.19 49.71 54.72 54.96 70.90
Table 3: Baselines ? labeled attachment score
(LAS) for monolingual and delexicalized transfer
parsing. Delexicalized transfer parsing results of
McDonald et al. (2013) included for reference.
above. The second setup then considers fully lexi-
calized models trained on translated treebanks. The
main advantage of the translation approach is the
availability of lexical information and this final
setup represents the real power of this approach.
In it, we compare lexicalized parsers trained on
translated treebanks with their delexicalized coun-
terparts and avoid a direct comparison with the
delexicalized baselines as they involve different
types of features.
5.3 Delexicalized Parsers
Table 4 presents the scores obtained by training
delexicalized parsing models on synthetic data cre-
ated by our translation approaches presented earlier.
Feature models and training options are the same
as for the delexicalized source language models
when training and testing on the target language
data. Note that we exclude the simple dictionary
LOOKUP approach here, because this approach
leads to identical models as the basic delexicalized
models. This is because words are translated one-
to-one without any reordering which leads to ex-
actly the same annotation sequences as the source
language treebank after projecting POS labels and
dependency relations.
From the table, we can see that all but one model
improve the scores obtained by delexicalized base-
line models. The improvements are quite substan-
tial up to +6.38 LAS. The boost in performance
WORD-BASED MT
DE EN ES FR SV
DE ? 48.12
(4.92)
50.84
(4.75)
52.92
(6.83)
55.52
(4.88)
EN 49.53
(2.91)
? 57.41
(1.76)
58.53
(2.07)
57.82
(0.14)
ES 45.48
(1.45)
48.46
(1.73)
? 58.29
(0.38)
55.25
(1.43)
FR 46.59
(2.68)
47.88
(1.13)
59.72
(0.07)
? 52.31
(0.30)
SV 52.16
(1.47)
49.14
(0.01)
56.50
(2.88)
56.71
(4.74)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 45.43
(2.23)
47.26
(1.17)
49.14
(3.05)
53.37
(2.73)
EN 49.16
(2.54)
? 57.12
(1.47)
58.23
(1.77)
58.23
(0.55)
ES 46.75
(2.72)
46.82
(0.09)
? 58.22
(0.31)
54.14
(0.32)
FR 48.02
(4.11)
49.06
(2.31)
60.23
(0.58)
? 55.24
(3.23)
SV 50.96
(0.27)
46.12
?3.01
55.95
(2.33)
54.71
(2.74)
?
Table 4: Translated treebanks: labeled attachment
score (LAS) for delexicalized parsers trained on
synthetic data created by translation. Numbers in
superscript show the absolute improvement over
our delexicalized baselines.
is especially striking for the simpleWORD-BASED
translation model considering that the only differ-
ence to the baseline model is word order. The
impact of the more complex PHRASE-BASED trans-
lation model is, however, difficult to judge. In
14 out of 20 models it actually leads to a drop in
LAS when applying phrase-based translation in-
stead of single-word translation. This is somewhat
surprising but is probably related to the additional
ambiguity in annotation projection introduced by
many-to-many alignments. The largest drop can be
seen for Swedish translated to English, which even
falls behind the baseline performance when using
the PHRASE-BASED translation model.
5.4 Lexicalized Parsers
The final experiment is concerned with lexical
parsers trained on translated treebanks. The main
objective here is to test the robustness of fully lexi-
calized models trained on noisy synthetic data cre-
ated by simple automatic translation engines. Ta-
ble 5 lists the scores obtained by our models when
trained on treebanks translated with our three ap-
proaches (dictionary LOOKUP, WORD-BASED MT
and full PHRASE-BASED translation). Again, we
use the same feature model and training options as
for the source language model when training mod-
els for the target languages. This time, of course,
this refers to the features used by the lexicalized
baseline models.
The capacity of the parsing models increases due
to the lexical information which is now included.
In order to see the effect of lexicalization, we com-
136
DE
T
DE
T
NO
UN
VE
RB
AD
P
NO
UN
CO
NJ
AD
P
DE
T
NO
UN
AD
J
.
To
us
ses
pr
od
uit
s
so
nt
de
qu
ali
te?
et
d?
un
e
fra
ich
eu
re
xe
mp
lai
res
.
Al
l
his
pr
od
uc
ts
ar
e
hig
h-
qu
ali
ty
an
d
a
co
ld
mu
lle
t
co
pie
s
.
DE
T
DE
T
NO
UN
VE
RB
NO
UN
AD
P
CO
NJ
DE
T
NO
UN
NO
UN
AD
J
.
de
t p
os
s
ns
ub
j
ro
ot
ad
pm
od
ad
po
bj
cc
co
nj
de
t
ad
po
bj
am
od
p
de
tp
os
s
ns
ub
j
ro
ot
ad
po
bj
ad
pm
od
cc
de
t
ad
po
bj a
dp
ob
j
am
od
p
Figure 4: Problematic annotation projection with ambiguous word alignment.
pare the performance now with the corresponding
delexicalized models. Note that the LOOKUP ap-
proach relates to the delexicalized baseline models
without any translation.
As we can see, all models outperform their cor-
responding delexicalized version (with one excep-
tion), which demonstrates the ability of the training
procedure to pick up valuable lexical information
from the noisy translations. Again, we can see
substantial absolute improvements of up to +7.31
LAS showing the effectiveness of the translation
approach. Note that this also means that we outper-
form the delexicalized baselines in all cases by a
large margin, even if we should not directly com-
pare these models as they draw on different fea-
ture sets. Once again, we can also see that the
very simple methods are quite successful. Even the
very basic LOOKUP approach leads to significant
improvements with one minor exception. Surpris-
ingly, no gain can be seen with the PHRASE-BASED
translation approach. The translation quality is cer-
tainly better when manually inspecting the data.
However, the increased complexity of annotation
projection seems to pull down the parsers induced
on that kind of data. A question for future work
is whether the performance of those models can
be improved by better projection algorithms and
heuristics that lead to cleaner annotations of other-
wise better translations of the original treebanks.
One possible reason for this disappointing re-
sult could be the unreliable mapping of POS labels
across many-to-many alignments. Figure 4 illus-
trates a typical case of link ambiguity that leads to
erroneous projections. For example, the mapping
of the label ADP onto the English word quality is
due to the left-to-right procedure applied in our pro-
jection algorithm and the mapping of the NOUN
label to the English adjective cold is due to the
link to fraicheur. How much these errors effect our
parsing models trained on the projected treebanks
is difficult to estimate and further investigations are
required to pinpoint these issues and to find ways
of addressing problems that may occur in various
contexts.
Nevertheless, the overall results are very positive.
The experiments clearly show the potentials of the
translation approach. Note that this paper presents
the first attempt to study the effect of translation on
cross-lingual parser induction. Further optimiza-
tion of the translation process and the connected
annotation projection procedures should lead to
further improvements over our basic models.
6 Conclusions and Future Work
In this paper, we have addressed the problem of
cross-lingual parser induction by using statistical
machine translation to create synthetic training data.
Our SMT approach avoids the noisy source-side
137
LOOKUP
DE EN ES FR SV
DE ? 48.63
(5.43)
52.66
(6.57)
52.06
(5.97)
58.78
(8.14)
EN 48.59
(1.97)
? 57.79
(2.14)
57.80
(1.34)
62.21
(4.53)
ES 47.36
(3.33)
49.13
(2.40)
? 62.24
(4.33)
57.50
(3.68)
FR 47.57
(3.66)
54.06
(7.31)
66.31
(6.66)
? 57.73
(5.72)
SV 51.88
(1.19)
48.84
(0.29)
54.74
(1.12)
52.95
(0.98)
?
WORD-BASED MT
DE EN ES FR SV
DE ? 51.86
(3.74)
55.90
(5.06)
57.77
(4.85)
61.65
(6.13)
EN 53.80
(4.27)
? 60.76
(3.35)
63.32
(4.79)
62.93
(5.11)
ES 49.94
(4.46)
49.93
(1.47)
? 65.60
(7.31)
59.22
(3.97)
FR 52.07
(5.48)
54.44
(6.56)
65.63
(5.91)
? 57.67
(5.36)
SV 53.18
(1.02)
50.91
(1.77)
60.82
(4.32)
59.14
(2.43)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 50.89
(5.46)
52.54
(5.28)
54.99
(5.85)
59.46
(6.09)
EN 53.71
(4.55)
? 60.70
(3.58)
62.89
(4.66)
64.01
(5.78)
ES 49.59
(2.84)
48.35
(1.53)
? 64.88
(6.66)
58.99
(4.85)
FR 51.83
(3.81)
53.81
(4.75)
65.55
(5.32)
? 59.01
(3.77)
SV 53.22
(2.26)
49.06
(2.94)
58.41
(2.46)
58.04
(3.33)
?
Table 5: Translated treebanks: labeled attachment score (LAS) for lexicalized parsers trained on synthetic
data. Numbers in superscript show the absolute improvements over the delexicalized models based on the
same translation strategy.
annotations of traditional annotation projection and
makes it possible to train fully lexicalized target lan-
guage models that significantly outperform delexi-
calized transfer parsers. We have also demonstrated
that translation leads to better delexicalized models
that can directly be compared with each other as
they are based on the same feature space.
We have compared three SMT methods for syn-
thesizing training data: LOOKUP-based translation,
WORD-BASED translation and full PHRASE-BASED
translation. Our experiments show that even noisy
data sets and simple translation strategies can be
used to achieve positive results. For all three ap-
proaches, we have recorded substantial improve-
ments over the state of the art in labeled cross-
lingual parsing (McDonald et al., 2013). According
to our results, simple word-by-word translations
are often sufficient to create reasonable translations
to train lexicalized parsers on. More elaborated
phrase-based models together with advanced anno-
tation projection strategies do not necessarily lead
to any improvements.
As future work, we want to improve our model
by (i) studying the impact of other SMT properties
and improve the quality of treebank translation,
(ii) implementing more sophisticated methods for
annotation projection and (iii) using n-best lists
provided by SMT models to introduce additional
synthetic data using a single resource. We also aim
at (iv) applying our approach to transfer parsing
for closely related languages (see Agi?c et al. (2012)
and Zeman and Resnik (2008) for related work),
(v) testing it in a multi-source transfer scenario
(McDonald et al., 2011) and, finally, (vi) comparing
different dependency parsing paradigms within our
experimental framework.
Multi-source approaches are especially appeal-
ing using the translation approach. However, initial
experiments (which we omit in this presentation)
revealed that simple concatenation is not sufficient
to obtain results that improve upon the single-best
translated treebanks. A careful selection of appro-
priate training examples and their weights given
to the training procedure seems to be essential to
benefit from different sources.
7 Acknowledgements
This work was supported by the Swedish Research
Council (Vetenskapsr?adet) through the project on
Discourse-Oriented Machine Translation (2012-
916).
138
References
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proceedings of IS-LTC 2012, pages 5?
9.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-
timizer: An Optimization Tool for MaltParser. In
Proceedings of EACL 2012, pages 58?62.
Emily M. Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In Proceedings of CoNLL 2006, pages 149?164.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC 2006, pages 449?454.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Pro-
ceedings of EMNLP-CoNLL 2012, pages 1?11.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
Lingual Discriminative Learning of Sequence Mod-
els with Posterior Regularization. In Proceedings of
EMNLP 2013, pages 1996?2006.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsen Eryi?git,
Be?ata Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 933?939.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL 2013, pages 690?696.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase Based Translation. In Pro-
ceedings of NAACL-HLT 2003, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ond?rej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL 2007, pages
177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005, pages 79?86.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP 2011, pages 62?
72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proceedings of ACL 2013, pages 92?97.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of ACL 2012, pages
629?637.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A Data-Driven Parser-Generator for De-
pendency Parsing. In Proceedings of LREC 2006,
pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git,
and Svetoslav Marinov. 2006b. Labeled Pseudo-
Projective Dependency Parsing with Support Vector
Machines. In Proceedings of CoNLL 2006, pages
221?225.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser: A
Language-Independent System for Data-Driven De-
pendency Parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003, pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of LREC 2012, pages 2089?2096.
Frank Smadja, Vasileios Hatzivassiloglou, and Kath-
leen R. McKeown. 1996. Translating Colloca-
tions for Bilingual Lexicons: A Statistical Approach.
Computational Linguistics, 22(1):1?38.
139
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings of
NAACL 2012, pages 477?487.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013a. Token and
Type Constraints for Cross-lingual Part-of-speech
Tagging. Transactions of the Association for Com-
putational Linguistics, 1:1?12.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013b. Target Language Adaptation of Discrimi-
native Transfer Parsers. In Proceedings of NAACL
2013, pages 1061?1071.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analysis
Tools via Robust Projection Across Aligned Corpora.
In Proceedings of HLT 2011, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proceedings of IJCNLP 2008, pages 35?
42.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of ACL 2011, pages 188?193.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross Language Dependency Parsing Using a
Bilingual Lexicon. In Proceedings of ACL-IJCNLP
2009, pages 55?63.
140
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 13?24,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-lingual Dependency Parsing of Related Languages with Rich
Morphosyntactic Tagsets
?
Zeljko Agi
?
c J
?
org Tiedemann Kaja Dobrovoljc
zagic@uni-potsdam.de jorg.tiedemann@lingfil.uu.se kaja.dobrovoljc@trojina.si
Simon Krek Danijela Merkler Sara Mo?ze
simon.krek@ijs.si dmerkler@ffzg.hr s.moze@wlv.ac.uk
Abstract
This paper addresses cross-lingual depen-
dency parsing using rich morphosyntac-
tic tagsets. In our case study, we experi-
ment with three related Slavic languages:
Croatian, Serbian and Slovene. Four dif-
ferent dependency treebanks are used for
monolingual parsing, direct cross-lingual
parsing, and a recently introduced cross-
lingual parsing approach that utilizes sta-
tistical machine translation and annota-
tion projection. We argue for the benefits
of using rich morphosyntactic tagsets in
cross-lingual parsing and empirically sup-
port the claim by showing large improve-
ments over an impoverished common fea-
ture representation in form of a reduced
part-of-speech tagset. In the process, we
improve over the previous state-of-the-art
scores in dependency parsing for all three
languages.
1 Introduction
A large majority of human languages are under-
resourced in terms of text corpora and tools avail-
able for applications in natural language process-
ing (NLP). According to recent surveys (Bender,
2011; Uszkoreit and Rehm, 2012; Bender, 2013),
this is especially apparent with syntactically anno-
tated corpora, i.e., treebanks ? both dependency-
based ones and others. In this paper, we fo-
cus on dependency parsing (K?ubler et al., 2009),
but the claims should hold in general. The lack
of dependency treebanks is due to the fact that
they are expensive and time-consuming to con-
struct (Abeill?e, 2003). Since dependency parsing
of under-resourced languages nonetheless draws
substantial interest in the NLP research commu-
nity, over time, we have seen a number of research
efforts directed towards their processing despite
the absence of training data for supervised learn-
ing of parsing models. We give a brief overview of
the major research directions in the following sub-
section. Here, we focus on supervised learning of
dependency parsers, as the performance of unsu-
pervised approaches still falls far behind the state
of the art in supervised parser induction.
1.1 Related Work
There are two basic strategies for data-driven pars-
ing of languages with no dependency treebanks:
annotation projection and model transfer. Both
fall into the general category of cross-lingual de-
pendency parsing as they attempt to utilize ex-
isting dependency treebanks or parsers from a
resource-rich language (source) for parsing the
under-resourced (target) language.
Annotation projection: In this approach, de-
pendency trees are projected from a source lan-
guage to a target language using word alignments
in parallel corpora. It is based on a presumption
that source-target parallel corpora are more read-
ily available than dependency treebanks. The ap-
proach comes in two varieties. In the first one, par-
allel corpora are exploited by applying the avail-
able state-of-the-art parsers on the source side
and subsequent projection to the target side us-
ing word alignments and heuristics for resolving
possible link ambiguities (Yarowsky et al., 2001;
Hwa et al., 2005). Since dependency parsers typ-
ically make heavy use of various morphological
and other features, the apparent benefit of this ap-
proach is the possibility of straightforward pro-
jection of these features, resulting in a feature-
rich representation for the target language. On the
downside, the annotation projection noise adds up
to dependency parsing noise and errors in word
alignment, influencing the quality of the resulting
target language parser.
The other variety is rare, since it relies on paral-
lel corpora in which the source side is a depen-
13
dency treebank, i.e., it is already manually an-
notated for syntactic dependencies (Agi?c et al.,
2012). This removes the automatic parsing noise,
while the issues with word alignment and annota-
tion heuristics still remain.
Model transfer: In its simplest form, transfer-
ring a model amounts to training a source lan-
guage parser and running it directly on the target
language. It is usually coupled with delexicaliza-
tion, i.e., removing all lexical features from the
source treebank for training the parser (Zeman and
Resnik, 2008; McDonald et al., 2013). This in turn
relies on the same underlying feature model, typi-
cally drawing from a shared part-of-speech (POS)
representation such as the Universal POS Tagset of
Petrov et al. (2012). Negative effects of using such
an impoverished shared representation are typi-
cally addressed by adapting the model to better fit
the target language. This includes selecting source
language data points appropriate for the target lan-
guage (S?gaard, 2011; T?ackstr?om et al., 2013),
transferring from multiple sources (McDonald et
al., 2011) and using cross-lingual word clusters
(T?ackstr?om et al., 2012). These approaches need
no projection and enable the usage of source-side
gold standard annotations, but they all rely on
a shared feature representation across languages,
which can be seen as a strong bottleneck. Also,
while most of the earlier research made use of
heterogenous treebanks and thus yielded linguisti-
cally implausible observations, research stemming
from an uniform dependency scheme across lan-
guages (De Marneffe and Manning, 2008; Mc-
Donald et al., 2013) made it possible to perform
more consistent experiments and to assess the ac-
curacy of dependency labels.
Other approaches: More recently, Durrett et
al. (2012) suggested a hybrid approach that in-
volves bilingual lexica in cross-lingual phrase-
based parsing. In their approach, a source-side
treebank is adapted to a target language by ?trans-
lating? the source words to target words through
a bilingual lexicon. This approach is advanced
by Tiedemann et al. (2014), who utilize full-
scale statistical machine translation (SMT) sys-
tems for generating synthetic target language tree-
banks. This approach relates to annotation pro-
jection, while bypassing the issue of dependency
parsing noise as gold standard annotations are pro-
jected. The SMT noise is in turn mitigated by
better word alignment quality for synthetic data.
The influence of various projection algorithms in
this approach is further investigated by Tiedemann
(2014). This line of cross-lingual parsing research
substantially improves over previous work.
1.2 Paper Overview
All lines of previous cross-lingual parsing research
left the topics of related languages and shared rich
feature representations largely unaddressed, with
the exception of Zeman and Resnik (2008), who
deal with phrase-based parsing test-cased on Dan-
ish and Swedish treebanks, utilizing a mapping
over relatively small POS tagsets.
In our contribution, the goal is to observe the
properties of cross-lingual parsing in an envi-
ronment of relatively free-word-order languages,
which are related and characterized by rich mor-
phology and very large morphosyntactic tagsets.
We experiment with four different small- and
medium-size dependency treebanks of Croatian
and Slovene, and cross-lingually parse into Croa-
tian, Serbian and Slovene. Along with monolin-
gual and direct transfer parsing, we make use of
the SMT framework of Tiedemann et al. (2014).
We are motivated by:
? observing the performance of various ap-
proaches to cross-lingual dependency parsing
for closely related languages, including the very
recent treebank translation approach by Tiede-
mann et al. (2014);
? doing so by using rich morphosyntactic tagsets,
in contrast to virtually all other recent cross-
lingual dependency parsing experiments, which
mainly utilize the Universal POS tagset of
Petrov et al. (2012);
? reliably testing for labeled parsing accuracy in
an environment with heterogenous dependency
annotation schemes; and
? improving the state of the art for Croatian,
Slovene and Serbian dependency parsing across
these heterogenous schemes.
In Section 2, we describe the language resources
used: treebanks, tagsets and test sets. Section 3
describes the experimental setup, which includes
a description of parsing, machine translation and
annotation projection. In Section 4, we discuss the
results of the experiments, and we conclude the
discussion by sketching the possible directions for
future research in Section 5.
14
Figure 1: Histogram of edge distances in the tree-
banks. Edge distance is measured in tokens be-
tween heads and dependents. Distance of 1 de-
notes adjacent tokens.
Figure 2: Histogram of average tree depths.
2 Resources
We make use of the publicly available language re-
sources for Croatian, Serbian and Slovene. These
include dependency treebanks, test sets annotated
for morphology and dependency syntax, and a
morphosyntactic feature representation drawing
from the Multext East project (Erjavec, 2012).
A detailed assessment of the current state of de-
velopment for morphosyntactic and syntactic pro-
cessing of these languages is given by Agi?c et al.
(2013) and Uszkoreit and Rehm (2012). Here, we
provide only a short description.
2.1 Treebanks
We use two Croatian and two Slovene dependency
treebanks.
1
One for each language is based on the
Prague Dependency Treebank (PDT) (B?ohmov?a
et al., 2003) annotation scheme, while the other
two introduced novel and more simplified syntac-
tic tagsets. All four treebanks use adaptations of
1
No treebanks of Serbian were publicly available at the
time of conducting this experiment.
Feature hr PDT hr SET sl PDT sl SSJ
Sentences 4,626 8,655 1,534 11,217
Tokens 117,369 192,924 28,750 232,241
Types 25,038 37,749 7,128 48,234
Parts of speech 13 13 12 13
MSDs 821 685 725 1,142
Syntactic tags 26 15 26 10
Table 1: Basic treebank statistics.
the Multext East version 4 tagset (Erjavec, 2012)
for the underlying morphological annotation layer,
which we shortly describe further down. Basic
statistics for the treebanks are given in Table 1.
hr PDT: This treebank is natively referred to
as the Croatian Dependency Treebank (HOBS)
(Tadi?c, 2007; Berovi?c et al., 2012). Its most recent
instance, HOBS 2.0 (Agi?c et al., 2014) slightly de-
parts from the PDT scheme. Thus, in this exper-
iment, we use the older version, HOBS 1.0, and
henceforth refer to it as hr PDT for consistency and
more clear reference to its annotation.
2
hr SET: The SETIMES.HR dependency treebank
of Croatian has a 15-tag scheme. It is targeted
towards high parsing accuracy, while maintaining
a clear distinction between all basic grammatical
categories of Croatian. Its publicly available 1.0
release consists of approximately 2,500 sentences
(Agi?c and Merkler, 2013), while release 2.0 has
just under 4,000 sentences (Agi?c and Ljube?si?c,
2014) of newspaper text. Here, we use an even
newer, recently developed version with more than
8,500 sentences from multiple domains.
3
sl PDT: The PDT-based Slovene Dependency
Treebank (D?zeroski et al., 2006) is built on top of
a rather small portion of Orwell?s novel 1984 from
the Multext East project (Erjavec, 2012). Even if
the project was discontinued, it is still heavily used
as part of the venerable CoNLL 2006 and 2007
shared task datasets (Buchholz and Marsi, 2006;
Nivre et al., 2007).
4
sl SSJ: The Slovene take on simplifying syntac-
tic annotations resulted in the 10-tag strong JOS
Corpus of Slovene (Erjavec et al., 2010). Similar
to hr SET, this new annotation scheme is loosely
2
HOBS is available through META-SHARE (Tadi?c and
V?aradi, 2012).
3
http://nlp.ffzg.hr/resources/corpora/
setimes-hr/
4
http://nl.ijs.si/sdt/
15
PDT-based, but considerably reduced to facilitate
manual annotation. The initial 100,000 token cor-
pus has recently doubled in size, as described by
Dobrovoljc et al. (2012). We use the latter version
in our experiment.
5
The statistics in Table 1 show a variety of tree-
bank sizes and annotations. Figure 1 illustrates the
structural complexity of the treebanks by provid-
ing a histogram of egdes by token distance. While
adjacent edges expectedly dominate the distribu-
tions, it is interesting to see that almost 30% of
all edges in sl SSJ attach to root, resulting in an
easily parsable flattened tree structure. Knowing
that relations denoting attributes account for more
than one third of all non-root dependents in the re-
mainder, one can expect dependency parsing per-
formance comparable to CoNLL-style chunking
(Tjong Kim Sang and Buchholz, 2000). This is
further supported by the distributions of sentences
in the four treebanks by average tree depth in Fig-
ure 2. We can see that virtually all sl SSJ trees have
average depths of 1 to 3, while the other treebanks
exhibit the more common structural properties of
dependency trees.
In these terms of complexity, the Croatian tree-
banks are richer than their Slovene counterparts.
In sl SSJ, attributes and edges to root account for
more than 60% of all dependencies. Even in the
other three treebanks, 20-30% of the edges are la-
beled as attributes, while the rest is spread more
evenly between the basic syntactic categories such
as predicates, subject and objects. More detailed
and more linguistically motivated comparisons of
the three annotation guidelines fall outside the
scope of our paper. Instead, we refer to the pre-
viously noted publications on the respective tree-
banks, and to (Agi?c and Merkler, 2013; Agi?c et
al., 2013) for comparisons between PDT and SET
in parsing Croatian and Serbian.
2.2 Morphosyntactic Tagset
All four treebanks were manually created: they
are sentence- and token-split, lemmatized, mor-
phosyntactically tagged and syntactically anno-
tated. In morphosyntactic annotation, they all
make use of the Multext East version 4 (MTE
4) guidelines (Erjavec, 2012).
6
MTE 4 is a po-
sitional tagset in which morphosyntactic descrip-
tors of word forms are captured by a morphosyn-
5
http://eng.slovenscina.eu/
tehnologije/ucni-korpus
6
http://nl.ijs.si/ME/V4/
tactic tag (MSD) created by merging atomic at-
tributes in the predefined positions. This is illus-
trated in Table 2 through an example verb tag. The
first character of the tag denotes the part of speech
(POS), while each of the following characters en-
codes a specific attribute in a specific position.
Both the positions and the attributes are language-
dependent in MTE 4, but the attributes are still
largely shared between these three languages due
to their relatedness.
The Slovene treebanks closely adhere to the
specification, while each of the Croatian treebanks
implements slight adaptations of the tagset to-
wards Croatian specifics. In hr PDT, the adaptation
is governed by and documented in the Croatian
Morphological Lexicon (Tadi?c and Fulgosi, 2003),
and the modifications in hr SET were targeted to
more closely match the ones for Slovene.
7
2.3 Test Sets
Recent research by McDonald et al. (2013) has
uncovered the downsides of experimenting with
parsing using heterogenous dependency annota-
tions, while at the same time providing possi-
bly the first reliable results in cross-lingual pars-
ing. They did so by creating the uniformly anno-
tated Universal Dependency Treebanks collection
based on Stanford Typed Dependencies (De Marn-
effe and Manning, 2008), which in turn also en-
abled measuring both labeled (LAS) and unla-
beled (UAS) parsing accuracy.
Having four treebanks with three different an-
notation schemes, we seek to enable reliable ex-
perimentation through our test sets. Along with
Croatian and Slovene, which are represented in the
training sets, we introduce Serbian as a target-only
language in the test data. Following the CoNLL
shared tasks setup (Buchholz and Marsi, 2006;
Nivre et al., 2007), our test sets have 200 sentences
(approx. 5,000 tokens) per language, split 50:50
between newswire and Wikipedia text. Each test
set is manually annotated for morphosyntax, fol-
lowing the MTE 4 guidelines for the respective
languages, and checked by native speakers for va-
lidity. On top of that, all test sets are annotated
with all three dependency schemes: PDT, SET and
SSJ. This enables observing LAS in a heteroge-
nous experimental environment, as we test each
monolingual and cross-lingual parser on an anno-
7
http://nlp.ffzg.hr/data/tagging/
msd-hr.html
16
Language MSD tag Attribute-value pairs
hr Vmn Category = Verb, Type = main, Vform = infinitive
sl Vmen Category = Verb, Type = main, Aspect = perfective, VForm = infinitive
sr Vmn----an-n---e Category = Verb, Type = main, VForm = infinitive, Voice = active,
Negative = no, Clitic = no, Aspect = perfective
Table 2: Illustration of the Multext East version 4 tagset for Croatian, Serbian and Slovene. The attributes
are language-dependent, as well as their positions in the tag, which are also dependent on the part of
speech, denoted by position zero in the tag.
tation layer matching its training set. In contrast,
the MTE 4 tagsets are not adjusted, i.e., each test
set only has a single language-specific MTE 4 an-
notation. We rely on their underlying similarities
in feature representations to suffice for improved
cross-lingual parsing performance.
3 Experiment Setup
This section describes the experiment settings. We
list the general workflow of the experiment and
then provide the details on the parser setup and
the more advanced approaches used for target lan-
guage adaptation of the models.
3.1 Workflow
The experiment consists of three work packages:
(1) monolingual parsing, (2) direct cross-lingual
parsing, and (3) cross-lingual parsing using syn-
thetic training data from SMT. In the first one, we
train dependency parsers on the four treebanks and
test them on the corresponding languages, thus
assessing the monolingual parsing performance.
The second stage observes the effects of directly
applying the parsers from the first stage across the
languages. Finaly, in the third work package, we
use four different approaches to automatic transla-
tion to create synthetic training data. We translate
the Croatian treebanks to Slovene and vice versa,
project the annotations using two different projec-
tion algorithms, and train and apply the adapted
parsers across the languages. The details are in-
cluded in the two following subsections.
Two general remarks apply to our experiment.
First, we perform cross-lingual parsing, and not
cross-annotation-scheme parsing. Thus, we do not
compare the dependency parsing scores between
the annotation schemes, but rather just between
the in-scheme parsers. Second, we use Serbian as
a test-set-only language. As there are no treebanks
of Serbian, we cannot use it as a source language,
and we leave SMT and annotation projection into
Serbian for future work.
3.2 Dependency Parsing
In all experiments, we use the graph-based de-
pendency parser by Bohnet (2010) with default
settings. We base our parser choice on its state-
of-the-art performance across various morpholog-
ically rich languages in the SPMLR 2013 shared
task (Seddah et al., 2013). While newer contribu-
tions targeted at joint morphological and syntactic
analysis (Bohnet and Kuhn, 2012; Bohnet et al.,
2013) report slightly higher scores, we chose the
former one for speed and robustness, and because
we use gold standard POS/MSD annotations. The
choice of gold standard preprocessing is motivated
by previous research in parsing Croatian and Ser-
bian (Agi?c et al., 2013), and by insight of Sed-
dah et al. (2013), who report a predictable linear
decrease in accuracy for automatic preprocessing.
This decrease amounts to approximately 3 points
LAS for Croatian and Serbian across various test
cases in (Agi?c et al., 2013).
We observe effects of (de)lexicalization and of
using full MSD tagset as opposed to only POS tags
in all experiments. Namely, in all work packages,
we compare parsers trained with {lexicalized,
delexicalized} ? {MSD, POS} features. In lexi-
calized parsers, we use word forms and features,
while we exclude lemmas from all experiments ?
both previous research using MSTParser (McDon-
ald et al., 2005) and our own test runs show no
use for lemmas as features in dependency parsing.
Delexicalized parsers are stripped of all lexical
features, i.e., word forms are omitted from training
and testing data. Full MSD parsers use both the
POS information and the sub-POS features in the
form of atomic attribute-value pairs, while POS-
only parsers are stripped of the MSD features ?
they use just the POS information. The delexi-
calized POS scenario is thus very similar to the
17
direct transfer by McDonald et al. (2013), since
MTE 4 POS is virtually identical to Universal POS
(Petrov et al., 2012).
8
3.3 Treebank Translation and Annotation
Projection
For machine translation, we closely adhere to the
setup implemented by Tiedemann et al. (2014) in
their treebank translation experiments. Namely,
our translations are based on automatic word
alignment and subsequent extraction of translation
equivalents as common in phrase-based SMT. We
perform word alignment by using GIZA++ (Och
and Ney, 2003), while utilizing IBM model 4 for
creating the Viterbi word alignments for parallel
corpora. For the extraction of translation tables,
we use the de facto standard SMT toolbox Moses
(Koehn et al., 2007) with default settings. Phrase-
based SMT models are tuned using minimum er-
ror rate training (Och, 2003). Our monolingual
language modeling using KenLM tools
9
(Heafield,
2011) produces standard 5-gram language mod-
els using modified Kneser-Ney smoothing without
pruning.
For building the translation models, we use
the OpenSubtitles parallel resources from OPUS
10
(Tiedemann, 2009) for the Croatian-Slovene pair.
Even if we expect this to be a rather noisy paral-
lel resource, we justify the choice by (1) the fact
that no other parallel corpora
11
of Croatian and
Slovene exist, other than Orwell?s 1984 from the
Multext East project, which is too small for SMT
training and falls into a very narrow domain, and
(2) evidence from (Tiedemann et al., 2014) that the
SMT-supported cross-lingual parsing approach is
very robust to translation noise.
For translating Croatian treebanks into Slovene
and vice versa, we implement and test four dif-
ferent methods of translation. They are coupled
with approaches to annotation projection from the
source side gold dependency trees to the target
translations via the word alignment information
available from SMT.
8
A mapping from Slovene MTE 4 to Universal
POS is available at https://code.google.com/p/
universal-pos-tags/ as an example.
9
https://kheafield.com/code/kenlm/
10
http://opus.lingfil.uu.se/
11
We note the Croatian-Slovene parallel corpus project de-
scribed by Po?zgaj Had?zi and Tadi?c (2000), but it appears that
the project was not completed and the corpus itself is not pub-
licly available.
LOOKUP: The first approach to translation in
our experiment is the dictionary lookup approach.
We simply select the most reliable translations of
single words in the source language into the tar-
get language by looking up the phrase translation
tables extracted from the parallel corpus. This is
very similar to what Agi?c et al. (2012) did for the
Croatian-Slovene pair. However, their approach
involved both translating and testing on the same
small corpus (Orwell?s novel), while here we ex-
tract the translations from full-blown SMT phrase
tables on a much larger scale. The trees projec-
tion from source to target is trivial since the num-
ber and the ordering of words between them does
not change. Thus, the dependencies are simply
copied.
CHAR: By this acronym, we refer to an ap-
proach known as character-based statistical ma-
chine translation. It is shown to perform very
well for closely related languages (Vilar et al.,
2007; Tiedemann, 2012; Tiedemann and Nakov,
2013). The motivation for character-level transla-
tion is the ability of such models to better gener-
alize the mapping between similar languages es-
pecially in cases of rich productive morphology
and limited amounts of training data. With this,
character-level models largely reduce the num-
ber of out-of-vocabulary words. In a nutshell,
our character-based model performs word-to-word
translation using character-level modeling. Simi-
lar to LOOKUP, this is also a word-to-word trans-
lation model, which also requires no adaptation of
the source dependency trees ? they are once again
simply copied to target sentences.
WORD: Our third take on SMT is slightly more
elaborate but still restricts the translation model
to one-to-one word mappings. In particular, we
extract all single word translation pairs from the
phrase tables and apply the standard beam-search
decoder implemented in Moses to translate the
original treebanks to all target languages. Thus,
we allow word reordering and use a language
model while still keeping the projection of anno-
tated data as simple as possible. The language
model may influence not only the word order but
also the lexical choice as we now allow multiple
translation options in our phrase table. Also note
that this approach may introduce additional non-
projectivity in the projected trees. This system
is the overall top-performer in (Tiedemann et al.,
18
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
ei
nfo
rm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
e
pis
arn
e
inf
orm
ati
vn
e
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
e
,
da
bo
od
prl
a
DU
MM
Y
inf
orm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
--
du
mm
y
du
mm
y
du
mm
y
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Pre
d
Atv
Obj
Sb
Pre
d
Atv
dum
mydu
mmyd
umm
y
Obj
Atr
Figure 3: An illustration of the projections. Left side = CHAR, middle = WORD, right side = PHRASE. As
illustrated, WORD might introduce reorderings, while PHRASE can enter dummy nodes and edges to the
dependency trees. The sentence: The government plans to open information offices. See (Tiedemann et
al., 2014; Tiedemann, 2014) for detailed insight into projection algorithms.
2014), where reordering played an important role
in adapting the models to the target languages. We
test whether it holds for related languages as well.
PHRASE: This model implements translation
based on the entire phrase table using the standard
approach to phrase-based SMT. We basically run
the Moses decoder with default settings and the
parameters and models trained on our parallel cor-
pus. Here, we can have many-to-many word align-
ments, which require a more elaborate approach to
the projection of the source side dependency an-
notatio s. It is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase seg enta-
tion and word alignment. We use the annotation
projection algorithm of Hwa et al. (2005). As
illustrated in Figure 3, it resolves many-to-many
alignments by introducing dummy nodes to the
dependency trees. We use the implementation by
Tiedemann (2014), which addresses certain issues
with algorithm choices for ambiguous alignments
which were left unaccounted for in the original
work. Since this paper does not focus on the intri-
cacies of annotation projection, but rather on ap-
plying it in an environment of related languages
and rich MSD tagsets, we refer the reader to re-
lated work regarding the details.
We translate from Croatian to Slovene and vice
versa using four different treebanks and these
four different methods of translation and annota-
tion projection. As we stated in the experiment
overview, for each of these, we also experiment
with (de)lexicalization and MSD vs. POS, and we
test on all three languages. The three experimental
batches ? monolingual, direct and SMT-supported
transfer ? produce a large number of observations,
all of which we assess in the following section.
4 Results and Discussion
We split our discussion of the parsing results into
the following three subsections. We first observe
the performance of monolingual parsers. Sec-
ondly, we measure the quality of these when ap-
plied directly on the other two languages. Finally,
we look into the accuracy of parsers trained on
SMT-generated artificial treebank data when ap-
plied across the test languages.
4.1 Monolingual Parsing
Accuracies of parsers trained and applied on train-
ing and testing data belonging to the same lan-
guage ? i.e., our monolingual parsers ? are pro-
vided in the g ayed out sections of Table 3.
Parsing Croatian using hr PDT yields a high
score of 69.45 LAS, better than the former state
of the art on this test set (Agi?c et al., 2013) simply
due to applying a newer generation parser. This
score is provided by a lexicalized model with the
full MSD feature set. Replacing MSD with POS or
delexicalizing this model results in a 3-point drop
in LAS, while applying both replacements sub-
stantially decreases the score ? by more than 11
points LAS. We observe virtually the same pattern
for the other Croatian treebank, hr SET, where this
latter drop is even more significant, at 14 points.
Incidentally, 76.36 points LAS is also the new
state of the art for hr SET parsing, owing to the
recent enlargement of the treebank.
The Slovene parsers exhibit effectively the same
behavior as the Croatian ones. The lexicalized
MSD models of sl PDT and sl SSJ both record new
state-of-the-art scores, although the latter one on a
different test set than in previous research (Dobro-
voljc et al., 2012). At over 92 points LAS, sl SSJ
19
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
hr PDT 69.45 66.95 60.09 50.19 69.42 66.96 66.03 57.79 57.98 42.66 66.79 57.41
SET 76.36 73.02 68.65 59.52 76.08 73.37 72.52 62.31 68.16 55.17 72.71 62.04
sl PDT 51.19 47.99 76.46 73.33 52.46 49.64 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.50 74.18 92.38 88.93 78.94 75.96 75.23 66.23 87.19 77.92 75.25 67.47
Table 3: Monolingual and direct cross-lingual parsing accuracy, expressed by the labeled accuracy metric
(LAS). Scores are split for lexicalized and delexicalized, full MSD and POS only parsers. Monolingual
scores are in grey. Row indices represent source languages and treebanks.
expectedly shows to be the easiest to parse, most
likely due to the relatively flat tree structure and its
small label set.
We note the following general pattern of fea-
ture importance. Dropping MSD features seems
to carry the most weight in all models, followed
by lexicalization. Dropping MSD is compensated
in part by lexical features paired with POS, while
dropping both MSD and word forms severely de-
grades all models. At this point, it is very impor-
tant to note that at 60-70 points LAS, these de-
creased scores closely resemble those of McDon-
ald et al. (2013) for the six languages in the Uni-
versal Treebanks. This observation is taken further
in the next subsection.
4.2 Direct Cross-lingual Parsing
The models used for monolingual parsing are here
directly applied on all languages but the treebank
source language, thus constituting a direct cross-
lingual parsing scenario. Its scores are also given
in Table 3, but now in the non-grey parts.
Croatian models are applied to Slovene and Ser-
bian test sets. For hr PDT, the highest score is
60.09 LAS on Slovene and 69.42 LAS on Serbian,
the latter noted as the state of the art for Serbian
PDT parsing. Comparing the cross-lingual score to
monolingual Slovene, the difference is substantial
as expected and comparable to the drops observed
by McDonald et al. (2013) in their experiments.
Our ranking of feature significance established in
the monolingual experiments holds here as well,
or rather, the absolute differences are even more
pronounced. Most notably, the difference between
the lexicalized MSD model and the delexicalized
POS model is 17 points LAS in favor of the for-
mer one on Slovene. hr SET appears to be more
resilient to delexicalization and tagset reduction
when applied on Slovene and Serbian, most likely
due to the treebank?s size, well-balanced depen-
dency label set and closer conformance with the
official MTE 4 guidelines. That said, the feature
patterns still hold. Also, 76.08 LAS for Serbian is
the new state of the art for SET parsing.
Slovene PDT is an outlier due to its small size,
as its training set is just over 1,500 sentences. Still,
the scores maintain the level of those in related
research, and the feature rankings hold. Perfor-
mance of parsing Croatian and Serbian using sl
SSJ is high, arguably up to the level of usability
in down-stream applications. These are the first
recorded scores in parsing the two languages us-
ing SSJ, and they reach above 78 points LAS for
both. Even if the scores are not comparable across
the annotation schemes due to their differences, it
still holds that the SSJ scores are the highest ab-
solute parsing scores recorded in the experiment.
This might hold significance in applications that
require robust parsing for shallow syntax.
Generally, the best transfer scores are quite
high in comparison with those on Universal Tree-
banks (McDonald et al., 2013; Tiedemann et al.,
2014). This is surely due to the relatedness of
the three languages. However, even for these ar-
guably closely related languages, the performance
of delexicalized models that rely only on POS fea-
tures ? averaging at around 55 points LAS ? is vir-
tually identical to that on more distant languages
test-cased in related work. We see this as a very
strong indicator of fundamental limitations of us-
ing linguistically impoverished shared feature rep-
resentations in cross-lingual parsing.
4.3 Cross-lingual Parsing with Treebank
Translation
Finally, we discuss what happens to parsing per-
formance when we replace direct cross-lingual ap-
plication of parsers with training models on trans-
lated treebanks. We take a treebank, Croatian or
Slovene, and translate it into the other language.
20
Target Approach PDT SET SSJ
hr monolingual 69.45 76.36 ?
direct 51.19 ? 78.50
translated 67.55 ? 74.68 ? 79.51 ?
sl monolingual 76.46 ? 92.38
direct 60.09 68.65 ?
translated 72.35 ? 70.52 ? 88.71 ?
sr monolingual ? ? ?
direct 69.42 76.08 78.94
translated 68.11 ? 74.31 ? 79.81 ??
Legend: ? CHAR ? LOOKUP ? PHRASE ? WORD
Table 4: Parsing score (LAS) summary for the top-
performing systems with respect to language and
approach to parser induction. All models are MSD
+ lexicalized.
We then train a parser on the translation and ap-
ply it on all three target test sets. We do this for all
the treebanks, and in all variations regarding trans-
lation and projection methods, morphological fea-
tures and lexicalization.
All scores for this evaluation stage are given in
Table 5 for completeness. The table contains 192
different LAS scores, possibly constituting a te-
dious read. Thus, in Table 4 we provide a sum-
mary of information on the top-performing parsers
from all three experimental stages, which includes
treebank translation.
We can see that the best models based on
translating the treebanks predominantly stem from
word-to-word SMT, i.e., from WORD transla-
tion models that basically enrich the lexical fea-
ture space and perform word reordering, enabling
straightforward copying of syntactic structures
from translation sources to translation targets. Fol-
lowing them are the CHAR and LOOKUP models,
expectedly leaving ? although not too far behind
? PHRASE behind given the similarities of the lan-
guage pair. Since Croatian and Slovene are related
languages, the differences between the models are
not as substantial as in (Tiedemann et al., 2014),
but WORD models still turn out to be the most ro-
bust ones, even if word reordering might not be so
frequent in this language pair as in the data from
(McDonald et al., 2013). Further, when compar-
ing the best SMT-supported models to monolin-
gual parsers, we see that the models with trans-
lation come really close to monolingual perfor-
mance. In comparison with direct transfer, models
trained on translated treebanks manage to outper-
form them in most cases, especially for the more
distant language pairs. For example, the sl ? hr
SSJ WORD model is 1 point LAS better on Croat-
ian than the directly applied Slovene model, and
the same holds for testing on Serbian with the
same dataset. On the other side, directly applied
models from Croatian SET outperform the trans-
lated ones for Serbian. For PDT, the translated
models are substantially better between Croatian
and Slovene since sl PDT is an outlier in terms
of size and dataset selection, while direct trans-
fer from Croatian seems to work better for Serbian
than the translated models.
Reflecting on the summary in Table 4 more
generally, by and large, we see high parsing ac-
curacies. Averages across the formalisms reach
well beyond 70 points LAS. We attribute this to
the relatedness of the languages selected for this
case study, as well as to the quality of the un-
derlying language resources. From another view-
point, the table clearly shows the prominence of
lexical and especially rich morphosyntactic tagset
features throughout the experiment. Across our
monolingual, direct and SMT-supported parsing
experiments, these features are represented in the
best systems, and dropping them incurs significant
decreases in accuracy.
5 Conclusions and Future Work
In this contribution, we addressed the topic of
cross-lingual dependency parsing, i.e., applying
dependency parsers from typically resource-rich
source languages to under-resourced target lan-
guages. We used three Slavic languages ? Croat-
ian, Slovene and Serbian ? as a test case for related
languages in different stages of language resource
development. As these are relatively free-word-
order languages with rich morphology, we were
able to test the cross-lingual parsers for perfor-
mance when using training features drawing from
large morphosyntactic tagsets ? typically consist-
ing of over 1,000 different tags ? in contrast to
impoverished common part-of-speech representa-
tions. We tested monolingual parsing, direct cross-
lingual parsing and a very recent promising ap-
proach with artificial creation of training data via
machine translation. In the experiments, we ob-
served state-of-the-art results in dependency pars-
ing for all three languages. We strongly argued
and supported the case for using common rich rep-
resentations of morphology in dependency parsing
21
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
CHAR hr ? sl PDT 66.92 60.25 61.49 55.57 67.83 62.04 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.65 64.64 70.52 66.11 72.95 64.44 72.98 62.98 69.03 54.81 72.74 62.73
sl ? hr PDT 51.96 48.14 72.35 63.71 53.11 49.47 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.69 75.45 88.21 78.88 79.25 77.09 75.23 66.23 87.19 77.92 75.25 67.47
LOOKUP hr ? sl PDT 67.55 59.96 60.81 56.54 67.78 61.41 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.58 64.98 69.93 68.09 73.70 64.25 72.52 62.72 68.47 55.27 72.71 62.73
sl ? hr PDT 51.74 49.15 72.02 63.08 53.49 51.33 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 79.25 77.06 88.10 78.53 79.81 77.23 75.23 66.23 87.19 77.92 75.25 67.47
WORD hr ? sl PDT 67.33 59.24 61.80 57.14 68.11 61.13 65.84 57.12 58.17 42.99 67.12 57.70
SET 73.26 65.87 69.98 68.98 73.63 65.85 72.71 62.29 68.50 55.06 73.14 62.40
sl ? hr PDT 51.67 49.58 71.47 63.51 54.62 51.82 50.25 43.17 71.27 62.79 50.79 44.07
SSJ 79.51 76.89 88.71 79.69 79.81 78.03 75.95 67.19 86.92 77.28 75.89 68.18
PHRASE hr ? sl PDT 67.28 58.90 60.53 56.79 67.92 61.36 65.77 55.06 58.18 45.41 66.16 55.79
SET 74.68 65.29 69.42 68.55 74.31 65.17 73.36 60.77 68.16 58.42 72.15 61.55
sl ? hr PDT 49.92 46.82 68.18 58.18 52.15 49.42 47.73 41.08 68.51 55.29 48.93 42.59
SSJ 79.29 78.09 88.24 78.75 79.32 78.85 75.33 68.10 86.59 75.66 75.91 68.67
Table 5: Parsing scores (LAS) for cross-lingual parsers trained on translated treebanks. Scores are
split for lexicalized and delexicalized, full MSD and POS only parsers, and with respect to the trans-
lation/projection approaches. Row indices represent source languages and treebanks, and indicate the
direction of applying SMT (e.g., hr ? sl denotes a Croatian treebank translated to Slovene).
for morphologically rich languages. Through our
multilayered test set annotation, we also facilitated
a reliable cross-lingual evaluation in a heteroge-
nous testing environment. We list our most impor-
tant observations:
? Even for closely related languages, using only
the basic POS features ? which are virtually
identical to the widely-used Universal POS of
Petrov et al. (2012) ? substantially decreases
parsing accuracy up to the level comparable with
results of McDonald et al. (2013) across the Uni-
versal Treebanks language groups.
? Adding MSD features heavily influences all the
scores in a positive way. This has obvious im-
plications for improving over McDonald et al.
(2013) on the Universal Treebanks dataset.
? Other than that, we show that it is possible
to cross-lingually parse Croatian, Serbian and
Slovene using all three syntactic annotation
schemes, and with high accuracy. A treebank for
Serbian does not exist, but we accurately parse
Serbian by using PDT, SET and SSJ-style annota-
tions. We parse Croatian using SSJ (transferred
from Slovene) and Slovene using SSJ (trans-
ferred from Croatian). This clearly indicates the
possibilities of uniform downstream pipelining
for any of the schemes.
? We show clear benefits of using the SMT ap-
proach for transferring SSJ parsers to Croatian
and SET parsers to Slovene. We observe these
benefits regardless of the low-quality, out-of-
domain SMT training data (OpenSubs).
Given the current interest for cross-lingual depen-
dency parsing in the natural language processing
community, we will seek to further test our obser-
vations on shared morphological features by us-
ing other pairs of languages of varying relatedness,
drawing from datasets such as Google Universal
Treebanks (McDonald et al., 2013) or HamleDT
(Zeman et al., 2012; Rosa et al., 2014). The goal
of cross-lingual processing in general is to enable
improved general access to under-resourced lan-
guages. With this in mind, seeing how we intro-
duced a test case of Serbian as a language cur-
rently without a treebank, we hope to explore other
options for performing cross-lingual experiments
on actual under-resourced languages, rather than
in an exclusive group of resource-rich placehold-
ers, possibly by means of down-stream evaluation.
Acknowledgments The second author was sup-
ported by the Swedish Research Council (Veten-
skapsr?adet), project 2012-916. The fifth author is
funded by the EU FP7 STREP project XLike.
22
References
Anne Abeill?e. 2003. Treebanks: Building and Using
Parsed Corpora. Springer.
?
Zeljko Agi?c and Nikola Ljube?si?c. 2014. The SE-
Times.HR Linguistically Annotated Corpus of Croa-
tian. In Proc. LREC, pages 1724?1727.
?
Zeljko Agi?c and Danijela Merkler. 2013. Three
Syntactic Formalisms for Data-Driven Dependency
Parsing of Croatian. LNCS, 8082:560?567.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proc. IS-LTC, pages 5?9.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2013. Parsing Croatian and Serbian by Using Croat-
ian Dependency Treebanks. In Proc. SPMRL, pages
22?33.
?
Zeljko Agi?c, Da?sa Berovi?c, Danijela Merkler, and
Marko Tadi?c. 2014. Croatian Dependency Tree-
bank 2.0: New Annotation Guidelines for Improved
Parsing. In Proc. LREC, pages 2313?2319.
Emily Bender. 2011. On achieving and evaluating
language-independence in nlp. Linguistic Issues in
Language Technology, 6(3):1?26.
Emily Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Da?sa Berovi?c,
?
Zeljko Agi?c, and Marko Tadi?c. 2012.
Croatian Dependency Treebank: Recent Develop-
ment and Initial Experiments. In Proc. LREC, pages
1902?1906.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank.
In Treebanks, pages 103?127.
Bernd Bohnet and Jonas Kuhn. 2012. The Best of
Both Worlds ? A Graph-based Completion Model
for Transition-based Parsers. In Proc. EACL, pages
77?87.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?ard Farkas, Filip Ginter, and Jan Hajic. 2013.
Joint Morphological and Syntactic Analysis for
Richly Inflected Languages. TACL, 1:415?428.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proc. COL-
ING, pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proc. CoNLL, pages 149?164.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The Stanford Typed Dependencies Rep-
resentation. In Proc. COLING, pages 1?8.
Kaja Dobrovoljc, Simon Krek, and Jan Rupnik. 2012.
Skladenjski raz?clenjevalnik za sloven?s?cino. In Proc.
IS-LTC, pages 42?47.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Proc.
EMNLP-CoNLL, pages 1?11.
Sa?so D?zeroski, Toma?z Erjavec, Nina Ledinek, Petr Pa-
jas, Zdenek
?
Zabokrtsky, and Andreja
?
Zele. 2006.
Towards a Slovene Dependency Treebank. In Proc.
LREC, pages 1388?1391.
Toma?z Erjavec, Darja Fi?ser, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged Cor-
pus of Slovene. In Proc. LREC, pages 1806?1809.
Toma?z Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. WSMT, pages
187?197.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
ACL, pages 177?180.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective Dependency Pars-
ing Using Spanning Tree Algorithms. In Proc. HLT-
EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proc. EMNLP, pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proc. ACL, pages 92?97.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proc. CoNLL, pages 915?932.
23
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. ACL,
pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proc. LREC,
pages 2089?2096.
Vesna Po?zgaj Had?zi and Marko Tadi?c. 2000. Croatian-
Slovene Parallel Corpus. In Proc. IS-LTC.
Rudolf Rosa, Jan Ma?sek, David Mare?cek, Martin
Popel, Daniel Zeman, and Zden?ek
?
Zabokrtsk?y.
2014. HamleDT 2.0: Thirty Dependency Treebanks
Stanfordized. In Proc. LREC, pages 2334?2341.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli?nski, Alina Wr?oblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 Shared Task: Cross-framework Evaluation of
Parsing Morphologically Rich Languages. In Proc.
SPMRL, pages 146?182.
Anders S?gaard. 2011. Data Point Selection for Cross-
language Adaptation of Dependency Parsers. In
Proc. ACL, pages 682?686.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proc. NAACL,
pages 477?487.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013. Target Language Adaptation of Discrimina-
tive Transfer Parsers. In Proc. NAACL, pages 1061?
1071.
Marko Tadi?c and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proc. BSNLP,
pages 41?46.
Marko Tadi?c and Tam?as V?aradi. 2012. Central and
South-East European Resources in META-SHARE.
Proc. COLING, pages 431?438.
Marko Tadi?c. 2007. Building the Croatian Depen-
dency Treebank: The Initial Stages. Suvremena
lingvistika, 63:85?92.
J?org Tiedemann and Preslav Nakov. 2013. Analyzing
the Use of Character-Level Translation with Sparse
and Noisy Datasets. In Proc. RANLP, pages 676?
684.
J?org Tiedemann,
?
Zeljko Agi?c, and Joakim Nivre. 2014.
Treebank Translation for Cross-Lingual Parser In-
duction. In Proc. CoNLL, pages 130?140.
J?org Tiedemann. 2009. News from OPUS: A Collec-
tion of Multilingual Parallel Corpora with Tools and
Interfaces. In Proc. RANLP, volume 5, pages 237?
248.
J?org Tiedemann. 2012. Character-Based Pivot Trans-
lations for Under-Resourced Languages and Do-
mains. In Proc. EACL, pages 141?151.
J?org Tiedemann. 2014. Rediscovering Annotation
Projection for Cross-Lingual Parser Induction. In
Proc. COLING.
Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 Shared Task:
Chunking. In Proc. CoNLL, pages 127?132.
Hans Uszkoreit and Georg Rehm. 2012. Language
White Paper Series. Springer.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can We Translate Letters? In Proc. WMT,
pages 33?39.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analy-
sis Tools via Robust Projection Across Aligned Cor-
pora. In Proc. HLT, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proc. IJCNLP, pages 35?42.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan Step?anek, Zdenek
Zabokrtsk`y, and Jan Hajic. 2012. HamleDT: To
Parse or Not to Parse? In Proc. LREC, pages 2735?
2741.
24
