Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 125?130,
Prague, June 2007. c?2007 Association for Computational Linguistics
Hypothesis Transformation and Semantic Variability Rules Used in 
Recognizing Textual Entailment 
Adrian Iftene 
?Al. I. Cuza? University, Faculty of 
Computer Science, Iasi, Romania 
 adiftene@info.uaic.ro 
Alexandra Balahur-Dobrescu 
?Al. I. Cuza? University, Faculty of 
Computer Science, Iasi, Romania 
 abalahur@info.uaic.ro 
 
Abstract 
Based on the core approach of the tree edit 
distance algorithm, the system central mod-
ule is designed to target the scope of TE ? 
semantic variability. The main idea is to 
transform the hypothesis making use of ex-
tensive semantic knowledge from sources 
like DIRT, WordNet, Wikipedia, acronyms 
database. Additionally, we built a system to 
acquire the extra background knowledge 
needed and applied complex grammar rules 
for rephrasing in English. 
1 Introduction 
Many NLP applications need to recognize when 
the meaning of one text can be expressed by, or 
inferred from, another text. Information Retrieval 
(IR), Question Answering (QA), Information Ex-
traction (IE), Text Summarization (SUM) are ex-
amples of applications that need to assess such a 
semantic relationship between text segments. Tex-
tual Entailment Recognition (RTE) (Dagan et al, 
2006) has recently been proposed as an application 
independent task to capture such inferences. 
This year our textual entailment system partici-
pated for the first time in the RTE1 competition. 
Next chapters present its main parts, the detailed 
results obtained and some possible future im-
provements. 
2 System description 
The process requires an initial pre-processing, fol-
lowed by the execution of a core module which 
uses the output of the first phase and obtains in the 
end the answers for all pairs. Figure 1 shows how 
                                                          
1
 http://www.pascal-network.org/Challenges/RTE3/ 
the pre-processing is realized with the MINIPAR 
(Lin, 1998) and LingPipe2 modules which provide 
the input for the core module. This one uses four 
databases: DIRT, Acronyms, Background knowl-
edge and WordNet. 
 
Figure 1: System architecture 
The system architecture is based on a peer-to-
peer networks design, in which neighboring com-
puters collaborate in order to obtain the global fit-
ness for every text-hypothesis pair. Eventually, 
based on the computed score, we decide for which 
pairs we have entailment. This type of architecture 
was used in order to increase the computation 
speed. 
3 Initial pre-processing    
The first step splits the initial file into pairs of files 
for text and hypothesis. All these files are then sent 
to the LingPipe module in order to find the Named 
entities.  
                                                          
2
 http://www.alias-i.com/lingpipe/ 
Initial 
data 
DIRT 
Minipar 
module 
Dependency 
trees for   
(T, H) pairs 
LingPipe 
module 
Named 
entities for     
(T, H) pairs 
Final 
result 
Core 
Module3 
Core 
Module2 
Core 
Module1 
Acronyms 
Background 
knowledge 
Wordnet 
P2P  
Computers 
Wikipedia 
125
In parallel, we transform with MINIPAR both 
the text and the hypothesis into dependency trees. 
Figure 2 shows the output associated with the sen-
tence: ?Le Beau Serge was directed by Chabrol.?. 
 
Figure 2: MINIPAR output ? dependency tree 
For every node from the MINIPAR output, we 
consider a stamp called entity with three main fea-
tures: the node lemma, the father lemma and the 
edge label (which represents the relation between 
words) (like in Figure 3). 
 
Figure 3: Entity components 
Using this stamp, we can easily distinguish be-
tween nodes of the trees, even if these have the 
same lemma and the same father. In the example 
from Figure 1, for the ?son? nodes we have two 
entities (Le_Beau_Serge, direct, s) and 
(Le_Beau_Serge, direct, obj). 
4 The hypothesis tree transformation 
Presently, the core of our approach is based on a 
tree edit distance algorithm applied on the depend-
ency trees of both the text and the hypothesis 
(Kouylekov, Magnini 2005). If the distance (i.e. the 
cost of the editing operations) among the two trees 
is below a certain threshold, empirically estimated 
on the training data, then we assign an entailment 
relation between the two texts. 
The main goal is to map every entity in the de-
pendency tree associated with the hypothesis 
(called from now on hypothesis tree) to an entity in 
the dependency tree associated with the text (called 
from now on text tree).  
For every mapping we calculate a local fitness 
value which indicates the appropriateness between 
entities. Subsequently, the global fitness is calcu-
lated from these partial values. 
For every node (refers to the word contained in 
the node) which can be mapped directly to a node 
from the text tree, we consider the local fitness 
value to be 1. When we cannot map one word of 
the hypothesis to one node from the text, we have 
the following possibilities: 
? If the word is a verb in the hypothesis tree, we 
use the DIRT resource (Lin and Pantel, 2001) 
in order to transform the hypothesis tree into an 
equivalent one, with the same nodes except the 
verb. Our aim in performing this transforma-
tion is to find a new value for the verb which 
can be better mapped in the text tree.  
? If the word is marked as named entity by Ling-
Pipe, we try to use an acronyms? database3 or if 
the word is a number we try to obtain informa-
tion related to it from the background knowl-
edge. In the event that even after these 
operations we cannot map the word from the 
hypothesis tree to one node from the text tree, 
no fitness values are computed for this case 
and we decide the final result: No entailment.  
? Else, we use WordNet (Fellbaum, 1998) to 
look up synonyms for this word and try to map 
them to nodes from the text tree.  
Following this procedure, for every transforma-
tion with DIRT or WordNet, we consider for local 
fitness the similarity value indicated by these re-
sources. If after all checks, one node from the hy-
pothesis tree cannot be mapped, some penalty is 
inserted in the value of the node local fitness.  
4.1 The DIRT resource 
For the verbs in the MINIPAR output, we extract 
templates with DIRT- like format. For the sample 
output in Figure 2, where we have a single verb 
?direct?, we obtain the following list of ?full? tem-
plates:N:s:V<direct>V:by:N and N:obj:V<direct> 
V:by:N. To this list we add a list of ?partial? tem-
plates: N:s:V<direct>V:, :V<direct>V:by:N, 
:V<direct>V:by:N, and N:obj:V<direct>V:. 
In the same way, we build a list with templates 
for the verbs in the text tree. With these two lists 
we perform a search in the DIRT database and ex-
tract the ?best? trimming, considering the template 
type (full or partial) and the DIRT score. 
According to the search results, we have the fol-
lowing situations: 
                                                          
3
 http://www.acronym-guide.com 
direct (V) 
Le_Beau_Serge (N) be (be) Chabrol 
Le_Beau_Serge (N) 
Le (U) Beau (U) 
s 
be by 
obj 
lex-mod 
node lemma 
edge label 
father lemma 
lex-mod
126
a) left ? left relations similarity 
This case is described by the following two tem-
plates for the hypothesis and the text: 
relation1 HypothesisVerb relation2 
relation1 TextVerb relation3  
This is the most frequent case, in which a verb is 
replaced by one of its synonyms or equivalent ex-
pressions  
The transformation of the hypothesis tree is done 
in two steps:  
1. Replace the relation2 with relation3, 
2. Replace the verb from the hypothesis with 
the corresponding verb from the text. (see 
Figure 4). 
 
Figure 4: Left-left relation similarity 
b) right ? right relations similarity: the same 
idea from the previous case. 
c) left ? right relations similarity 
This case can be described by the following two 
templates for the hypothesis and the text: 
relation1 HypothesisVerb relation2 
relation3 TextVerb relation1  
The transformation of the hypothesis tree is:  
1. Replace the relation2 with relation3, 
2. Replace the verb from the hypothesis with 
the corresponding verb from the text. 
3. Rotate the subtrees accordingly: left sub-
tree will be right subtree and vice-versa 
right subtree will become left-subtree (as it 
can be observed in Figure 5). 
 
Figure 5: Left-right relation similarity 
This case appears for pair 161 with the verb ?at-
tack?: 
T: ?The demonstrators, convoked by the solidarity 
with Latin America committee, verbally attacked 
Salvadoran President Alfredo Cristiani.? 
H: ?President Alfredo Cristiani was attacked by 
demonstrators.? 
In this case, for the text we have the template 
N:subj:V<attack>V:obj:N, and for the hypothesis 
the template N:obj:V<attack>V:by:N. Using DIRT, 
hypothesis H is transformed into:  
H?: Demonstrators attacked President Alfredo 
Cristiani.  
Under this new form, H is easier comparable to T. 
d) right ? left relations similarity: the same 
idea from the previous case 
For every node transformed with DIRT, we con-
sider its local fitness as being the similarity value 
indicated by DIRT. 
4.2 Extended WordNet 
For non-verbs nodes from the hypothesis tree, if in 
the text tree we do not have nodes with the same 
lemma, we search for their synonyms in the ex-
tended WordNet4. For every synonym, we check to 
see if it appears in the text tree, and select the map-
ping with the best value according to the values 
from Extended WordNet. Subsequently, we change 
the word from the hypothesis tree with the word 
from WordNet and also its fitness with its indicated 
similarity value. For example, the relation between 
?relative? and ?niece? is accomplished with a score 
of 0.078652. 
                                                          
4
 http://xwn.hlt.utdallas.edu/downloads.html  
HypothesisVerb 
 
relation1 relation2 
TextVerb 
 
relation3 relation1 
Left 
Subtree 
Right 
Subtree 
Right 
Subtree 
Left 
Subtree 
HypothesisVerb 
 
relation1 
relation2 
TextVerb 
 relation1 relation3 
Left 
Subtree 
Right 
Subtree 
Right 
Subtree 
Left 
Subtree 
127
4.3 Acronyms 
The acronyms? database helps our program find 
relations between the acronym and its meaning: 
?US - United States?, and ?EU - European Union?. 
We change the word with the corresponding ex-
pression from this database. Since the meaning is 
the same, the local fitness is considered maximum, 
i.e. 1. 
4.4 Background Knowledge 
Some information cannot be deduced from the al-
ready used databases and thus we require addi-
tional means of gathering extra information of the 
form: 
Argentine [is] Argentina 
Netherlands [is] Holland 
2 [is] two 
Los Angeles [in] California 
Chinese [in] China 
Table 1: Background knowledge 
Background knowledge was built semi-
automatically, for the named entities (NEs) and for 
numbers from the hypothesis without correspon-
dence in the text. For these NEs, we used a module 
to extract from Wikipedia5 snippets with informa-
tion related to them. Subsequently, we use this file 
with snippets and some previously set patterns of 
relations between NEs, with the goal to identify a 
known relation between the NE for which we have 
a problem and another NE.  
If such a relation is found, we save it to an out-
put file. Usually, not all relations are correct, but 
those that are will help us at the next run.  
Our patterns identify two kinds of relations be-
tween words: 
? ?is?, when the module extracts information of 
the form: ?Argentine Republic? (Spanish: 'Re-
publica Argentina', IPA)? or when explanations 
about the word are given in brackets, or when 
the extracted information contains one verb 
used to define something, like ?is?, ?define?, 
?represent?: '2' ('two') is a number. 
? ?in? when information is of the form: 'Chinese' 
refers to anything pertaining to China or in the 
form Los Angeles County, California, etc. 
                                                          
5
 http://en.wikipedia.org/wiki/Main_Page  
In this case, the local fitness for the node is set to 
the maximum value for the [is]-type relations, and 
it receives some penalties for the [in]-type relation. 
5 Determination of entailment  
After transforming the hypothesis tree, we calcu-
late a global fitness score using the extended local 
fitness value for every node from the hypothesis - 
which is calculated as sum of the following values: 
1. local fitness obtained after the tree trans-
formation and node mapping, 
2. parent fitness after parent mapping, 
3. mapping of the node edge label from the 
hypothesis tree onto the text tree, 
4. node position (left, right) towards its father 
in the hypothesis and position of the map-
ping nodes from the text. 
After calculating this extended local fitness score, 
the system computes a total fitness for all the nodes 
in the hypothesis tree and a negation value associ-
ated to the hypothesis tree. Tests have shown that 
out of these parameters, some are more important 
(the parameter at 1.) and some less (the parameter 
at 3.). Below you can observe an example of how 
the calculations for 3 and 4 are performed and what 
the negation rules are. 
5.1 Edge label mapping 
After the process of mapping between nodes, we 
check how edge labels from the hypothesis tree are 
mapped onto the text tree. Thus, having two adja-
cent nodes in the hypothesis, which are linked by 
an edge with a certain label, we search on the path 
between the nodes? mappings in the text tree this 
label. (see Figure 6) 
 
Figure 6: Entity mapping 
Text tree 
node 
mapping 
father 
mapping 
edge label 
mapping  
Hypothesis tree 
128
It is possible that more nodes until the label of the 
edge linking the nodes in the hypothesis exist, or it 
is possible that this label is not even found on this 
path. According to the distance or to the case in 
which the label is missing, we insert some penalties 
in the extended local fitness. 
5.2 Node position 
After mapping the nodes, one of the two following 
possible situations may be encountered: 
? The position of the node towards its father and 
the position of the mapping node towards its 
father?s mapping are the same (left-left or 
right-right). In this case, the extended local fit-
ness is incremented. 
? The positions are different (left-right or right-
left) and in this case a penalty is applied ac-
cordingly. 
5.3 Negation rules 
For every verb from the hypothesis we consider a 
Boolean value which indicates whether the verb 
has a negation or not, or, equivalently, if it is re-
lated to a verb or adverb ?diminishing? its sense or 
not. Consequently, we check in its tree on its de-
scending branches to see whether one or more of 
the following words are to be found (pure form of 
negation or modal verb in indicative or conditional 
form): ?not, may, might, cannot, should, could, 
etc.?. For each of these words we successively ne-
gate the initial truth value of the verb, which by 
default is ?false?. The final value depends on the 
number of such words. 
Since the mapping is done for all verbs in the 
text and hypothesis, regardless of their original 
form in the snippet, we also focused on studying 
the impact of the original form of the verb on its 
overall meaning within the text. Infinitives can be 
identified when preceded by the particle ?to?. Ob-
serving this behavior, one complex rule for nega-
tion was built for the particle ?to? when it precedes 
a verb. In this case, the sense of the infinitive is 
strongly influenced by the active verb, adverb or 
noun before the particle ?to?, as follows: if it is 
being preceded by a verb like ?allow, impose, gal-
vanize? or their synonyms, or adjective like ?nec-
essary, compulsory, free? or their synonyms or 
noun like ?attempt?, ?trial? and their synonyms, the 
meaning of the verb in infinitive form is stressed 
upon and becomes ?certain?. For all other cases, 
the particle ?to? diminish the certainty of the action 
expressed in the infinitive-form verb. Based on the 
synonyms database with the English thesaurus6, we 
built two separate lists ? one of ?certainty stressing 
(preserving)? ? ?positive? and one of ?certainty 
diminishing? ? ?negative? words. Some examples 
of these words are ?probably?, ?likely? ? from the 
list of ?negative? words and ?certainly?, ?abso-
lutely? ? from the list of ?positive? words. 
5.4 Global fitness calculation 
We calculate for every node from the hypothesis 
tree the value of the extended local fitness, and af-
terwards consider the normalized value relative to 
the number of nodes from the hypothesis tree. We 
denote this result by TF (total fitness): 
rNodesNumbeHypothesis
calFitnessExtendedLo
TF Hnode
node
?
=  
After calculating this value, we compute a value 
NV (the negation value) indicating the number of 
verbs with the same value of negation, using the 
following formula: 
rOfVerbsTotalNumbe
rVerbsNumbePositiveNV _=  
where the Positive_VerbsNumber is the number of 
non-negated  verbs from the hypothesis using the 
negation rules, and TotalNumberOfVerbs is the 
total number of verbs from the hypothesis. 
Because the maximum value for the extended 
fitness is 4, the complementary value of the TF is 
4-TF and the formula for the global fitness used is: 
)4(*)1(* TFNVTFNVessGlobalFitn ??+=  
For pair 518 we have the following: 
Initial entity Node 
Fitness 
Extended 
local fitness 
(the, company, det) 1 3.125 
(French, company, nn) 1 3.125 
(railway, company, nn) 1 3.125 
(company, call, s) 1 2.5 
(be, call, be) 1 4 
(call, -, -) 0.096 3.048 
(company, call, obj) 1 1.125 
(SNCF, call, desc) 1 2.625 
Table 2: Entities extended fitness 
                                                          
6
 http://thesaurus.reference.com/ 
129
TF = (3.125 + 3.125 + 3.125 + 2.5 + 4 + 3.048 + 
1.125 + 2.625)/8 = 22.673/8 = 2.834 
NV = 1/1 = 1 
GlobalFitness = 1*2.834+(1?1)*(4-2.834) = 2.834 
Using the development data, we establish a 
threshold value of 2.06. Thus, pair 518 will have 
the answer ?yes?. 
6 Results 
Our system has a different behavior on different 
existing tasks, with higher results on Question An-
swering (0.87) and lower results on Information 
Extraction (0.57). We submitted two runs for our 
system, with different parameters used in calculat-
ing the extended local fitness. However, the results 
are almost the same (see Table 3).  
 IE IR QA SUM Global 
Run01 0.57 0.69 0.87 0.635 0.6913 
Run02 0.57 0.685 0.865 0.645 0.6913 
Table 3: Test results 
To be able to see each component?s relevance, the 
system was run in turn with each component re-
moved. The results in the table below show that the 
system part verifying the NEs is the most impor-
tant.    
System Description Precision Relevance 
Without DIRT 0.6876 0.54 % 
Without WordNet 0.6800 1.63 % 
Without Acronyms 0.6838  1.08 % 
Without BK 0.6775 2.00 % 
Without Negations 0.6763 2.17 % 
Without NEs 0.5758 16.71 % 
Table 4: Components relevance 
7 Conclusions 
The system?s core algorithm is based on the tree 
edit distance approach, however, focused on trans-
forming the hypothesis. It presently uses wide-
spread syntactic analysis tools like Minipar, lexical 
resources like WordNet and LingPipe for Named 
Entities recognition and semantic resources like 
DIRT. The system?s originality resides firstly in 
creating a part-of and equivalence ontology using 
an extraction module for Wikipedia data on NEs 
(the background knowledge), secondly in using a 
distinct database of acronyms from different do-
mains, thirdly acquiring a set of important context 
influencing terms and creating a semantic equiva-
lence set of rules based on English rephrasing con-
cepts and last, but not least, on the technical side, 
using a distributed architecture for time perform-
ance enhancement.   
The approach unveiled some issues related to the 
dependency to parsing tools, for example separat-
ing the verb and the preposition in the case of 
phrasal verbs, resulting in the change of meaning.  
Another issue was identifying expressions that 
change context nuances, which we denoted by 
?positive? or ?negative? words. Although we ap-
plied rules for them, we still require analysis to 
determine their accurate quantification. 
For the future, our first concern is to search for a 
method to establish more precise values for penal-
ties, in order to obtain lower values for pairs with 
No entailment. Furthermore, we will develop a new 
method to determine the multiplication coefficients 
for the parameters in the extended local fitness and 
the global threshold.  
8 Acknowledgements 
The authors thank the members of the NLP group 
in Iasi for their help and support at different stages 
of the system development. Special thanks go to 
Daniel Matei which was responsible for preparing 
all the input data. 
The work on this project is partially financed by 
Siemens VDO Iai and by the CEEX Rotel project 
number 29. 
References 
Dagan, I., Glickman, O., and Magnini, B. 2006. The 
PASCAL Recognising Textual Entailment Challenge. 
In Qui?onero-Candela et al, editors, MLCW 2005, 
LNAI Volume 3944, pages 177-190. Springer-Verlag. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, Mass. 
Kouylekov, M. and Magnini, B. 2005. Recognizing Tex-
tual Entailment with Tree Edit Distance Algorithms. 
In Proceedings of the First Challenge Workshop Rec-
ognising Textual Entailment, Pages 17-20, 25?28 
April, 2005, Southampton, U.K. 
Lin, D. 1998. Dependency-based Evaluation of 
MINIPAR. In Workshop on the Evaluation of Parsing 
Systems, Granada, Spain, May, 1998. 
Lin, D., and Pantel, P. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM Con-
ference on Knowledge Discovery and Data Mining 
(KDD-01). pp. 323-328. San Francisco, CA. 
130
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Building a Competitive Opinion Summarization System: 
Challenges and Keys 
 
 
Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andr?s Montoyo 
Department of Software and Computing Systems 
University of Alicante 
Apartado de Correos 99, E-03080, Alicante, Spain 
{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es 
 
 
 
Abstract 
This paper presents an overview of our participation in 
the TAC 2008 Opinion Pilot Summarization task, as 
well as the proposed and evaluated post-competition 
improvements. We first describe our opinion 
summarization system and the results obtained. Further 
on, we identify the system?s weak points and suggest 
several improvements, focused both on information 
content, as well as linguistic and readability aspects. We 
obtain encouraging results, especially as far as F-
measure is concerned, outperforming the competition 
results by approximately 80%. 
1 Introduction 
The Opinion Summarization Pilot (OSP) task 
within the TAC 2008 competition consisted in 
generating summaries from answers to opinion 
questions retrieved from blogs (the Blog061 
collection). The questions were organized around 
25 targets ? persons, events, organizations etc.  
Additionally, a set of text snippets that contained 
the answers to the questions were provided by the 
organizers, their use being optional. An example of 
target, question and provided snippet is given in 
Figure 1. 
 
 
 
 
 
Figure 1. Examples of target, question and snippet 
 
                                                           
*Elena Lloret is funded by the FPI program (BES-2007-
16268) from the Spanish Ministry of Science and Innovation, 
under the project TEXT-MESS (TIN-2006-15265)  
1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html 
The techniques employed by the participants were 
mainly based on the already existing 
summarization systems. While most participants 
added new features (sentiment, pos/neg sentiment, 
pos/neg opinion) to account for the presence of 
positive opinions or negative ones - CLASSY 
(Conroy and Schlessinger, 2008); CCNU (He et 
al.,2008);  LIPN (Bossard et al, 2008);  IIITSum08 
(Varma et al, 2008) -, efficient methods were 
proposed focusing on the retrieval and filtering 
stage, based on polarity ? DLSIUAES (Balahur et 
al., 2008) - or on separating information rich 
clauses - italica (Cruz et al, 2008). In general, 
previous work in opinion mining includes 
document level sentiment classification using 
supervised (Chaovalit and Zhou, 2005) and 
unsupervised methods (Turney, 2002), machine 
learning techniques and sentiment classification 
considering rating scales (Pang, Lee and 
Vaithyanathan, 2002), and scoring of features 
(Dave, Lawrence and Pennock, 2003). Other 
research has been conducted in analysing 
sentiment at a sentence level using bootstrapping 
techniques (Riloff and Wiebe, 2003), finding 
strength of opinions (Wilson, Wiebe and Hwa, 
2004), summing up orientations of opinion words 
in a sentence (Kim and Hovy, 2004), and 
identifying opinion holders (Stoyanov and Cardie, 
2006). Finally, fine grained, feature-based opinion 
summarization is defined in (Hu and Liu, 2004).  
2 Opinion Summarization System 
In order to tackle the OSP task, we considered the 
use of two different methods for opinion mining 
and summarization, differing mainly with respect 
to the use of the optional text snippets provided. 
Our first approach (the Snippet-driven Approach) 
Target : George Clooney 
Question: Why do people like George Clooney? 
Snippet 1: 1050 BLOG06-20060125-015-
0025581509 he is a great actor 
72
used these snippets, whereas the second one (Blog-
driven Approach) found the answers directly in the 
corresponding blogs. A general overview of the 
system?s architecture is shown in Figure 2, where 
three main parts can be distinguished: the question 
processing stage, the snippets processing stage 
(only carried out for the first approach), and the 
final summary generation module. Next, the main 
steps involved in each process will be explained in 
more detail.  
 
Figure 2. System architecture 
 
The first step was to determine the polarity of each 
question, extract the keywords from each of them 
and finally, build some patterns of reformulation. 
The latter were defined in order to give the final 
summary an abstract nature, rather than a simple 
joining of sentences. The polarity of the question 
was determined using a set of created patterns, 
whose goal was to extract for further classification 
the nouns, verbs, adverbs or adjectives indicating 
some kind of polarity (positive or negative). These 
extracted words, together with their determiners, 
were classified using the emotions lists in 
WordNet Affect (Strapparava and Valitutti, 2005), 
jointly with the emotions lists of attitudes, triggers 
resource (Balahur and Montoyo, 2008 [1]), four 
created lists of attitudes, expressing criticism, 
support, admiration and rejection and two 
categories for value (good and bad), taking for the 
opinion mining systems in (Balahur and Montoyo, 
2008 [2]). Moreover, the focus of each question 
was automatically extracted using the Freeling2 
Named Entity Recognizer module. This 
information was used to determine whether or not 
all the questions within the same topic had the 
same focus, as well as be able to decide later on 
which text snippet belonged to which question.  
Regarding the given text snippets, we also 
computed their polarity and their focus. The 
                                                           
2
 http://garraf.epsevg.upc.es/freeling/ 
polarity was calculated as a vector similarity 
between the snippets and vectors constructed from 
the list of sentences contained in the ISEAR corpus 
(Scherer and Wallbot, 1997), WordNet Affect 
emotion lists of anger, sadness, disgust and joy and 
the emotion triggers resource, using Pedersen's 
Text Similarity Package.3  
Concerning the blogs, our opinion mining and 
summarization system is focused only on plain 
text; therefore, as pre processing stage, we 
removed all unnecessary tags and irrelevant 
information, such as links, images etc. Further on, 
we split the remaining text into individual 
sentences. A matching between blogs' sentences 
and text snippets was performed so that a 
preliminary set of potential meaningful sentences 
was recorded for further processing. To achieve 
this, snippets not literally contained in the blogs 
were tokenized and stemmed using Porter's 
Stemmer,4 and stop words were removed in order 
to find the most similar possible sentence 
associated with it. Subsequently, by means of the 
same Pedersen Text Similarity Package as for 
computing the snippets' polarity, we computed the 
similarity between the given snippets and this 
created set of potential sentences. We extracted the 
complete blog sentences to which each snippet was 
related. Further on, we extracted the focus for each 
blog phrase sentence as well. Then, we filtered 
redundant sentences using a na?ve similarity based 
approach. Once we obtained the possible answers, 
we used Minipar5 to filter out incomplete 
sentences.  
Having computed the polarity for the questions and 
snippets, and set out the final set of sentences to 
produce the summary, we bound each sentence to 
its corresponding question, and we grouped all 
sentences which were related to the same question 
together, so that we could generate the language 
for this group, according to the patterns of 
reformulation previously mentioned. Finally, the 
speech style was changed to an impersonal one, in 
order to avoid directly expressed opinion 
sentences. A POS-tagger tool (TreeTagger6) was 
used to identify third person verbs and change 
them to a neutral style. A set of rules to identify 
                                                           
3http://www.d.umn.edu/~tpederse/text-similarity.html 
4http://tartarus.org/~martin/PorterStemmer/ 
5http://www.cs.ualberta.ca/~lindek/minipar.htm 
6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/ 
73
pronouns was created, and they were also changed 
to the more general pronoun ?they? and its 
corresponding forms, to avoid personal opinions.  
3 Evaluation 
Table 1 shows the final results obtained by our 
approaches in the TAC 2008 Opinion Pilot (the 
rank among the 36 participating systems is shown 
in brackets for each evaluation measure). Both of 
our approaches were totally automatic, and the 
only difference between them was the use of the 
given snippets in the first one (A1) and not in the 
second (A2). The column numbers stand for the 
following average scores: summarizerID (1); 
pyramid F-score (Beta=1) (2), grammaticality (3); 
non-redundancy (4); structure/coherence 
(including focus and referential clarity) (5); overall 
fluency/readability (6); overall responsiveness (7). 
 
1 2 3 4 5 6 7 
A1 0.357 
(7) 
4.727 
(8) 
5.364 
(28) 
3.409 
(4) 
3.636 
(16) 
5.045 
(5) 
A2 0.155 
(23) 
3.545 
(36) 
4.364 
(36) 
3.091 
(13) 
2.636 
(36) 
2.227 
(28) 
Table 1. Evaluation results 
 
As it can be noticed from Table 1, our system 
performed well regarding F-measure, the first run 
being classified 7th among the 36 evaluated. As far 
as the structure and coherence are concerned, the 
results were also good, placing the first approach 
in the fourth. Also worth mentioning is the good 
performance obtained regarding the overall 
responsiveness, where A1 ranked 5th. Generally 
speaking, the results for A1 showed well-balanced 
among all the criteria evaluated, except for non 
redundancy and grammaticality.  For the second 
approach, results were not as good, due to the 
difficulty in selecting the appropriate opinion blog 
sentence by only taking into account the keywords 
of the question.  
4 Post-competition tests, experiments 
and improvements 
When an exhaustive examination of the nuggets 
used for evaluating the summaries was done, we 
found some problems that are worth mentioning. 
 
a) Some nuggets with high score did not exist in 
the snippet list (e.g. ?When buying from 
CARMAX, got a better than blue book trade-in 
on old car? (0.9)).  
b) Some nuggets for the same target express the 
same idea, despite their not being identical 
(e.g. ?NAFTA needs to be renegotiated to 
protect Canadian sovereignty? and ?Green 
Party: Renegotiate NAFTA to protect 
Canadian Sovereignty?). 
c) The meaning of one nugget can be deduced 
from another's (e.g. ?reasonably healthy food? 
and ?sandwiches are healthy?). 
d) Some nuggets are not very clear in meaning 
(e.g. ?hot?, ?fun?). 
e) A snippet can be covered by several nuggets 
(e.g. both nuggets ?it is an honest book? and 
?it is a great book? correspond to the same 
snippet ?It was such a great book- honest and 
hard to read (content not language 
difficulty)?). 
 
On the other hand, regarding the use of the 
optional snippets, the main problem to address is to 
remove redundancy, because many of them are 
repeated for the same target, and we have to 
determine which snippet represents better the idea 
for the final summary, in order to avoid noisy 
irrelevant information. 
4.1 Measuring the Performance of a 
Generic Summarization System 
Several participants in the TAC 2008 edition 
performed the OSP task by using generic 
summarization systems. Most were adjusted by 
integrating an opinion classifier module so that the 
task could be fulfilled, but some were not (Bossard 
et al, 2008), (Hendrickx and Bosma, 2008). This 
fact made us realize that a generic summarizer 
could be used to achieve this task. We wanted to 
analyze the effects of such a kind of summarizer to 
produce opinion summaries. We followed the 
approach described in (Lloret et al, 2008). The 
main idea employed is to score sentences of a 
document with regard to the word frequency count 
(WF), which can be combined with a Textual 
Entailment (TE) module.  
Although the first approach suggested for opinion 
summarization obtained much better results in the 
evaluation than the second one (see Section 3.1), 
we decided to run the generic system over both 
approaches, with and without applying TE, to 
74
provide a more extent analysis and conclusions. 
After preprocessing the blogs and having all the 
possible candidate sentences grouped together, we 
considered these as the input for the generic 
summarizer. The goal of these experiments was to 
determine whether the techniques used for a 
generic summarizer would have a positive 
influence in selecting the main relevant 
information to become part of the final summary.  
4.2 Results and Discussion 
We re-evaluated the summaries generated by the 
generic system following the nuggets? list provided 
by the TAC 2008 organization, and counting 
manually the number of nuggets that were covered 
in the summaries. This was a tedious task, but it 
could not be automatically performed because of 
the fact that many of the provided nuggets were 
not found in the original blog collection. After the 
manual matching of nuggets and sentences, we 
computed the average Recall, Precision and F-
measure (Beta =1) in the same way as in the TAC 
2008 was done, according to the number and 
weight of the nuggets that were also covered in the 
summary. Each nugget had a weight ranging from 
0 to 1 reflecting its importance, and it was counted 
only once, even though the information was 
repeated within the summary.  
The average for each value was calculated taking 
into account the results for all the summaries in 
each approach. Unfortunately, we could not 
measure criteria such as readability or coherence as 
they were manually evaluated by human experts.  
Table 2 points out the results for all the approaches 
reported. We have also considered the results 
derived from our participation in the TAC 2008 
conference (OpSum-1 and OpSum-2), in order to 
analyze whether they have been improved or not. 
From these results it can be stated that the TE 
module in conjunction with the WF counts, have 
been very appropriate in selecting the most 
important information of a document. Although it 
can be thought that applying TE can remove some 
meaningful sentences which contained important 
information, results show the opposite. It benefits 
the Precision value, because a shorter summary 
contains greater ratio of relevant information. On 
the other hand, taking into consideration the F-
measure value only, it can be seen that the 
approach combining TE and WF, for the sentences 
in the first approach, has beaten significantly the 
best F-measure result among the participants of 
TAC 2008 (please see Table 3), increasing its 
performance by 20% (with respect to WF only), 
and improving by approximately 80% with respect 
to our first approach submitted to TAC 2008.    
However, a simple generic summarization system 
like the one we have used here is not enough to 
produce opinion oriented summaries, since 
semantic coherence given by the grouping of 
positive and negative opinions is not taken into 
account. Therefore, the opinion classification stage 
must be added in the same manner as used in the 
competition. 
 
SYSTEM RECALL PRECISION F-MEASURE 
OpSum-1 0.592 0.272 0.357 
OpSum-2 0.251 0.141 0.155 
WF-1 0.705 0.392 0.486 
TE+WF -1  0.684 0.630  0.639 
WF -2 0.322 0.234  0.241 
TE+WF-2 0.292 0.282 0.262 
Table 2. Comparison of the results 
4.3 Improving the quality of summaries 
In the evaluation performed by the TAC 
organization, a manual quality evaluation was also 
carried out. In this evaluation the important aspects 
were grammaticality, non-redundancy, structure 
and coherence, readability, and overall 
responsiveness. Although our participating systems 
obtained good F-measure values, in other scores, 
especially in grammaticality and non-redundancy, 
the results achieved were very low. Focusing all 
our efforts in improving the first approach, 
OpSum-1, non-redundancy and grammaticality 
verification had to be performed. In this approach, 
we wanted to test how much of the redundant 
information would be possible to remove by using 
a Textual Entailment system similar to (Iftene and 
Balahur-Dobrescu, 2007), without it affecting the 
quality of the remaining data. As input for the TE 
system, we considered the snippets retrieved from 
the original blog posts. We applied the entailment 
verification on each of the possible pairs, taking in 
turn all snippets as Text and Hypothesis with all 
other snippets as Hypothesis and Text, 
respectively. Thus, as output, we obtained the list 
of snippets from which we eliminated those that 
75
are entailed by any of the other snippets. We 
further eliminated those snippets which had a high 
entailment score with any of the remaining 
snippets. 
 
SYSTEM F-MEASURE 
Best system  0.534 
Second best system 0.490 
OpSum-1 + TE  0.530 
OpSum-1 0.357 
Table 3. F-measure results after improving the system 
 
Table 3 shows that applying TE before generating 
the final summary leads to very good results 
increasing the F-measure by 48.50% with respect 
to the original first approach. Moreover, it can be 
seen form Table 3 that our improved approach 
would have ranked in the second place among all 
the participants, regarding F-measure. The main 
problem with this approach is the long processing 
time. We can apply Textual Entailment in the 
manner described within the generic 
summarization system presented, successively 
testing the relation as Snippet1 entails Snippet2?, 
Snippet1+Snippet2 entails Snippet3? and so on. 
The problem then becomes the fact that this 
approach is random, since different snippets come 
from different sources, so there is no order among 
them. Further on, we have seen that many 
problems arise from the fact that extracting 
information from blogs introduces a lot of noise. In 
many cases, we had examples such as: 
At 4:00 PM John said Starbucks coffee tastes great 
John said Starbucks coffee tastes great, always get one 
when reading New York Times. 
To the final summary, the important information 
that should be added is ?Starbucks coffee tastes 
great?. Our TE system contains a rule specifying 
that the existence or not of a Named Entity in the 
hypothesis and its not being mentioned in the text 
leads to the decision of ?NO? entailment. For the 
example given, both snippets are maintained, 
although they contain the same data.  
Another issue to be addressed is the extra 
information contained in final summaries that is 
not scored as nugget. As we have seen from our 
data, much of this information is also valid and 
correctly answers the questions. Therefore, what 
methods can be employed to give more weight to 
some and penalize others automatically?  
Regarding the grammaticality criteria, once we had 
a summary generated we used the module 
Language Tool7 as a post-processing step. The 
errors that we needed correcting included the 
number matching between nouns and determiners 
as well as among subject and predicate, upper case 
for sentence start, repeated words or punctuation 
marks and lack of punctuation marks. The rules 
present in the module and that we ?switched off?, 
due to the fact that they produced more errors, 
were those concerning the limit in the number of 
consecutive nouns and the need for an article 
before a noun (since it always seemed to want to 
correct ?Vista? for ?the Vista? a.o.). We evaluated 
by observing the mistakes that the texts contained, 
and counting the number of remaining or 
introduced errors in the output. The results 
obtained can be seen in Table 4. 
 
Problem Rightly corrected 
 
Wrongly 
corrected 
Match S-P 90% 10% 
Noun-det 75% 25% 
Upper case 80% 20% 
Repeated words 100% 0% 
Repeated ?.? 80% 20% 
Spelling mistakes 60% 40% 
Unpaired ??/() 100% 0% 
Table 4. Grammaticality analysis 
 
The greatest problem encountered was the fact that 
bigrams are not detected and agreement is not 
made in cases in which the noun does not appear 
exactly after the determiner. All in all, using this 
module, the grammaticality of our texts was 
greatly improved. 
5 Conclusions and future work 
The Opinion Pilot in the TAC 2008 competition 
was a difficult task, involving the development of 
systems including components for QA, IR, polarity 
classification and summarization. Our contribution 
presented in this paper resides in proposing an 
opinion mining and summarization method using 
different approaches and resources, evaluating 
each of them in turn. We have shown that using a 
generic summarization system, we obtain 80% 
improvement over the results obtained in the 
competition, with coherence being maintained by 
using the same polarity classification mechanisms. 
                                                           
7http://community.languagetool.org/ 
76
Using redundancy removal with TE, as opposed to 
our initial polarity strength based sentence filtering 
improved the system performance by almost 50%.    
Finally, we showed that grammaticality can be 
checked and improved using an independent 
solution given by Language Tool.  
Further work includes the improvement of the 
polarity classification component by using 
machine learning over annotated corpora and other 
techniques, such as anaphora resolution. As we 
could see, the well functioning of this component 
ensures logic, structure and coherence to the 
produced summaries. Moreover, we plan to study 
the manner in which opinion sentences of 
blogs/bloggers can be coherently combined. 
References  
Balahur, A., Lloret, E., Ferr?ndez, ?., Montoyo, A., 
Palomar, M., Mu?oz, R., The DLSIUAES Team?s 
Participation in the TAC 2008 Tracks. In 
Proceedings of the Text Analysis Conference (TAC), 
2008. 
Balahur, A. and Montoyo, A. [1]. An Incremental 
Multilingual Approach to Forming a Culture 
Dependent Emotion Triggers Database. In 
Proceedings of the 8th International Conference on 
Terminology and Knowledge Engineering, 2008. 
Balahur, A. and Montoyo, A. [2]. Multilingual Feature--
driven Opinion Mining and Summarization from 
Customer Reviews. In Lecture Notes in Computer 
Science 5039, pg. 345-346. 
Bossard, A., G?n?reux, M. and  Poibeau, T.. Description 
of the LIPN systems at TAC 2008: Summarizing 
information and opinions. In Proceedings of the Text 
Analysis Conference (TAC), 2008. 
Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a 
Comparison between Supervised and Unsupervised 
Classification Approaches. In Proceedings of HICSS-
05, the 38th Hawaii International Conference on 
System Sciences. 
Cruz, F., Troyani, J.A., Ortega, J., Enr?quez, F. The 
Italica System at TAC 2008 Opinion Summarization 
Task. In Proceedings of the Text Analysis 
Conference (TAC), 2008. 
Cui, H., Mittal, V., Datar, M. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of the 21st National 
Conference on Artificial Intelligence AAAI 2006.  
Dave, K., Lawrence, S., Pennock, D. 2003. Mining the 
Peanut Gallery: Opinion Extraction and Semantic 
Classification of Product Reviews. In Proceedings of 
WWW-03.  
Lloret, E., Ferr??ndez, O., Mu?oz, R. and Palomar, M. A 
Text Summarization Approach under the Influence of 
Textual Entailment. In Proceedings of the 5th 
International Workshop on Natural Language 
Processing and Cognitive Science (NLPCS 2008), 
pages 22?31, 2008. 
Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E. 
2005. Mining Customer Opinions from Free Text. 
Lecture Notes in Computer Science. 
He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008: 
Proceeding on Using Semantic Method for 
Automated Summarization Yield. In Proceedings of 
the Text Analysis Conference (TAC), 2008. 
Hendrickx, I. and Bosma, W.. Using coreference links 
and sentence compression in graph-based 
summarization. In Proceedings of the Text Analysis 
Conference (TAC), 2008.     
Hu, M., Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of 19th National 
Conference on Artificial Intelligence AAAI. 
Iftene, A., Balahur-Dobrescu, A. Hypothesis 
Transformation and Semantic Variability Rules for 
Recognizing Textual Entailment. In Proceedings of 
the ACL 2007 Workshop on Textual Entailment and 
Paraphrasis, 2007. 
Kim, S.M., Hovy, E. 2004. Determining the Sentiment 
of Opinions. In Proceedings of COLING 2004. 
Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP-02, the 
Conference on Empirical Methods in Natural 
Language Processing. 
Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns 
for Subjective Expressions. In Proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing.  
Scherer, K. and Wallbott, H.G. The ISEAR 
Questionnaire and Codebook, 1997.  
Stoyanov, V., Cardie, C. 2006. Toward Opinion 
Summarization: Linking the Sources. In: COLING-
ACL 2006 Workshop on Sentiment and Subjectivity 
in Text. 
Strapparava, C. and Valitutti, A. "WordNet-Affect: an 
affective extension of WordNet". In Proceedings 
ofthe 4th International Conference on Language 
Resources and Evaluation, 2004, pp. 1083-1086.  
Turney, P., 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceedings of the 40th 
Annual Meeting of the ACL 
Varma, V., Pingali, P., Katragadda, R., Krisha, S., 
Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty, 
H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT 
Hyderabad at TAC 2008. In Proceedings of the Text 
Analysis Conference (TAC), 2008.  
Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are 
you? Finding strong and weak opinion clauses. In: 
Proceedings of AAAI 2004. 
77
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157?160,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Opinion and Generic Question Answering Systems: a Performance 
Analysis 
 
 
Alexandra Balahur 1,2 
1DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
2IPSC, EC Joint Research Centre 
Via E. Fermi, 21027, Ispra 
abalahur@dlsi.ua.es 
Ester Boldrini 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
patricio@dlsi.ua.es 
 
Abstract 
The importance of the new textual genres such 
as blogs or forum entries is growing in parallel 
with the evolution of the Social Web. This pa-
per presents two corpora of blog posts in Eng-
lish and in Spanish, annotated according to the 
EmotiBlog annotation scheme. Furthermore, 
we created 20 factual and opinionated ques-
tions for each language and also the Gold 
Standard for their answers in the corpus. The 
purpose of our work is to study the challenges 
involved in a mixed fact and opinion question 
answering setting by comparing the perform-
ance of two Question Answering (QA) sys-
tems as far as mixed opinion and factual set-
ting is concerned. The first one is open do-
main, while the second one is opinion-
oriented. We evaluate separately the two sys-
tems in both languages and propose possible 
solutions to improve QA systems that have to 
process mixed questions. 
Introduction and motivation 
In the last few years, the number of blogs has 
grown exponentially. Thus, the Web contains 
more and more subjective texts. A research from 
the Pew Institute shows that 75.000 blogs are 
created daily (Pang and Lee, 2008). They ap-
proach a great variety of topics (computer sci-
ence, sociology, political science or economics) 
and are written by different types of people, thus 
are a relevant resource for large community be-
havior analysis. Due to the high volume of data 
contained in blogs, new Natural Language Proc-
essing (NLP) resources, tools and methods are 
needed in order to manage their language under-
standing. Our fist contribution consists in carry-
ing out a multilingual research, for English and 
Spanish. Secondly, many sources are present in 
blogs, as people introduce quotes from newspa-
per articles or other information to support their 
arguments and make references to previous posts 
in the discussion thread. Thus, when performing 
a task such as Question Answering (QA), many 
new aspects have to be taken into consideration. 
Previous studies in the field (Stoyanov, Cardie 
and Wiebe, 2005) showed that certain types of 
queries, which are factual in nature, require the 
use of Opinion Mining (OM) resources and tech-
niques to retrieve the correct answers. A further 
contribution this paper brings is the analysis and 
definition of the criteria for the discrimination 
among types of factual versus opinionated ques-
tions. Previous researchers mainly concentrated 
on newspaper collections. We formulated and 
annotated of a set of questions and answers over 
a multilingual blog collection. A further contri-
bution is the evaluation and comparison of two 
different approaches to QA a fact-oriented one 
and another designed for opinion QA scenarios.  
Related work 
Research in building factoid QA systems has a 
long history. However, it is only recently that 
studies have started to focus also on the creation 
and development of QA systems for opinions. 
Recent years have seen the growth of interest in 
this field, both by the research performed and the 
publishing of various studies on the requirements 
157
and peculiarities of opinion QA systems (Stoy-
anov, Cardie and Wiebe, 2005), (Pustejovsky 
and Wiebe, 2006), as well as the organization of 
international conferences that promote the crea-
tion of effective QA systems both for general and 
subjective texts, as, for example, the Text Analy-
sis Conference (TAC)1. Last year?s TAC 2008 
Opinion QA track proposed a mixed setting of 
factoid (?rigid list?) and opinion questions 
(?squishy list?), to which the traditional systems 
had to be adapted. The Alyssa system (Shen et 
al., 2007), classified the polarity of the question 
and of the extracted answer snippet, using a Sup-
port Vector Machines classifier trained on the 
MPQA corpus (Wiebe, Wilson and Cardie, 
2005), English NTCIR2 data and rules based on 
the subjectivity lexicon (Wilson, Wiebe and 
Hoffman, 2005). The PolyU (Wenjie et al, 
2008) system determines the sentiment orienta-
tion with two estimated language models for the 
positive versus negative categories. The 
QUANTA (Li, 2008) system detects the opinion 
holder, the object and the polarity of the opinion 
using a semantic labeler based on PropBank3 and 
some manually defined patterns.  
Evaluation 
In order to carry out our evaluation, we em-
ployed a corpus of blog posts presented in 
(Boldrini et al, 2009). It is a collection of blog 
entries in English, Spanish and Italian. However, 
for this research we used the first two languages. 
We annotated it using EmotiBlog (Balahur et al, 
2009) and we also created a list of 20 questions 
for each language. Finally, we produced the Gold 
Standard, by labeling the corpus with the correct 
answers corresponding to the questions. 
1.1 Questions 
No TYPE QUESTION 
 
1 
 
F 
 
F 
What international organization do people criticize for 
its policy on carbon emissions? 
?Cu?l fue uno de los primeros pa?ses que se preocup? 
por el problema medioambiental? 
 
 
2 
 
 
O 
 
 
F 
What motivates people?s negative opinions on the 
Kyoto Protocol? 
?Cu?l es el pa?s con mayor responsabilidad de la 
contaminaci?n mundial seg?n la opini?n p?blica? 
 
 
3 
 
 
F 
 
 
F 
What country do people praise for not signing the 
Kyoto Protocol? 
?Qui?n piensa que la reducci?n de la contaminaci?n se 
deber?a apoyar en los consejos de los cient?ficos? 
 
 
4 
 
 
F 
 
 
F 
What is the nation that brings most criticism to the 
Kyoto Protocol? 
?Qu? administraci?n act?a totalmente en contra de la 
lucha contra el cambio clim?tico? 
                                                 
1 http://www.nist.gov/tac/ 
2 http://research.nii.ac.jp/ntcir/ 
3 http://verbs.colorado.edu/~mpalmer/projects/ace.html 
 
 
5 
 
 
O 
 
 
F 
What are the reasons for the success of the Kyoto 
Protocol? 
?Qu? personaje importante est? a favor de la 
colaboraci?n del estado en la lucha contra el 
calentamiento global? 
 
 
6 
 
 
O 
 
 
F 
What arguments do people bring for their criticism of 
media as far as the Kyoto Protocol is concerned? 
?A qu? pol?ticos americanos culpa la gente por la 
grave situaci?n en la que se encuentra el planeta? 
 
7 
 
O 
 
F 
Why do people criticize Richard Branson? 
?A qui?n reprocha la gente el fracaso del Protocolo de 
Kyoto? 
 
8 
 
F 
 
F 
What president is criticized worldwide for his reaction 
to the Kyoto Protocol? 
?Qui?n acusa a China por provocar el mayor da?o al 
medio ambiente? 
 
9 
 
F 
 
O 
What American politician is thought to have developed 
bad environmental policies? 
?C?mo ven los expertos el futuro? 
 
10 
 
F 
 
O 
What American politician has a positive opinion on the 
Kyoto protocol? 
C?mo se considera el atentado del 11 de septiembre? 
 
11 
 
O 
 
O 
What negative opinions do people have on Hilary 
Benn? 
?Cu?l es la opini?n sobre EEUU? 
 
12 
 
O 
 
O 
Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol and other environmental issues? 
?De d?nde viene la riqueza de EEUU? 
 
13 
 
F 
 
O 
What country disregards the importance of the Kyoto 
Protocol? 
?Por qu? la guerra es negativa? 
 
14 
 
F 
 
O 
What country is thought to have rejected the Kyoto 
Protocol due to corruption? 
?Por qu? Bush se retir? del Protocolo de Kyoto? 
 
15 
 
F/
O 
 
O 
What alternative environmental friendly resources do 
people suggest to use instead of gas en the future? 
?Cu?l fue la posici?n de EEUU sobre el Protocolo de 
Kyoto? 
 
16 
 
F/
O 
 
O 
 Is Arnold Schwarzenegger pro or against the reduction 
of CO2 emissions? 
?Qu? piensa Bush sobre el cambio clim?tico? 
 
17 
 
F 
 
O 
What American politician supports the reduction of 
CO2 emissions? 
?Qu? impresi?n da Bush? 
 
18 
 
F/
O 
 
O 
What improvements are proposed to the Kyoto Proto-
col? 
?Qu? piensa China del calentamiento global? 
 
19 
 
F/
O 
 
O 
What is Bush accused of as far as political measures 
are concerned? 
?Cu?l es la opini?n de Rusia sobre el Protocolo de 
Kyoto? 
 
20 
 
F/
O 
 
O 
What initiative of an international body is thought to be 
a good continuation for the Kyoto Protocol? 
?Qu? cree que es necesario hacer Yvo Boer? 
 
Table 1: List of question in English and Spanish 
 
As it can be seen in the table above, we created 
factoid (F) and opinion (O) queries for English 
and for Spanish; however, there are some that 
could be defined between factoid and opinion 
(F/O) and the system can retrieve multiple an-
swers after having selected, for example, the po-
larity of the sentences in the corpus. 
1.2 Performance of the two systems 
We evaluated and compared the generic QA sys-
tem of the University of Alicante (Moreda et al, 
2008) and the opinion QA system presented in 
(Balahur et al, 2008), in which Named Entity 
Recognition with LingPipe4 and FreeLing5 was 
                                                 
4 http://alias-i.com/lingpipe/ 
5 http://garraf.epsevg.upc.es/freeling/ 
158
added, in order to boost the scores of answers 
containing NEs of the question Expected Answer 
Type (EAT). Table 2 presents the results ob-
tained for English and Table 3 for Spanish. We 
indicate the id of the question (Q), the question 
type (T) and the number of answer of the Gold 
Standard (A). We present the number of the re-
trieved questions by the traditional system 
(TQA) and by the opinion one (OQA). We take 
into account the first 1, 5, 10 and 50 answers. 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
   TQA OQA TQA OQA TQA OQA TQA OQA 
1 F 5 0 0 0 2 0 3 4 4 
2 O 5 0 0 0 1 0 1 0 3 
3 F 2 1 1 2 1 2 1 2 1 
4 F 10 1 1 2 1 6 2 10 4  
5 O 11 0 0 0 0 0 0 0 0 
6 O 2 0 0 0 0 0 1 0 2 
7 O 5 0 0 0 0 0 1 0 3 
8 F 5 1 0 3 1 3 1 5 1 
9 F 5 0 1 0 2 0 2 1 3 
10 F 2 1 0 1 0 1 1 2 1 
11 O 2 0 1 0 1 0 1 0 1 
12 O 3 0 0 0 1 0 1 0 1 
13 F 1 0 0 0 0 0 0 0 1 
14 F 7 1 0 1 1 1 2 1 2 
15 F/O 1 0 0 0 0 0 1 0 1 
16 F/O 6 0 1 0 4 0 4 0 4 
17 F 10 0 1 0 1 4 1 0 2 
18 F/O 1 0 0 0 0 0 0 0 0 
19 F/O 27 0 1 0 5 0 6 0 18 
20 F/O 4 0 0 0 0 0 0 0 0 
 
Table 2: Results for English 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
    TQA  OQA  TQA  OQA  TQA  OQA  TQA  OQA 
1 F 9 1 0 0 1 1 1 1 3 
2 F 13 0 1 2 3 0 6 11 7 
3 F 2 0 1 0 2 0 2 2 2 
4 F 1 0 0 0 0 0 0 1 0 
5 F 3 0 0 0 0 0 0 1 0 
6 F 2 0 0 0 1 0 1 2 1 
7 F 4 0 0 0 0 1 0 4 0 
8 F 1 0 0 0 0 0 0 1 0 
9 O 5 0 1 0 2 0 2 0 4 
10 O 2 0 0 0 0 0 0 0 0 
11 O 5 0 0 0 1 0 2 0 3 
12 O 2 0 0 0 1 0 1 0 1 
13 O 8 0 1 0 2 0 2 0 4 
14 O 25 0 1 0 2 0 4 0 8 
15 O 36 0 1 0 2 0 6 0 15 
16 O 23 0 0 0 0 0 0 0 0 
17 O 50 0 1 0 5 0 6 0 10 
18 O 10 0 1 0 1 0 2 0 2 
19 O 4 0 1 0 1 0 1 0 1 
20 O 4 0 1 0 1 0 1 0 1 
 
Table 3: Results for Spanish 
1.3 Results and discussion 
There are many problems involved when trying 
to perform mixed fact and opinion QA. The first 
can be the ambiguity of the questions e.g. ?De 
d?nde viene la riqueza de EEUU?. The answer 
can be explicitly stated in one of the blog sen-
tences, or a system might have to infer them 
from assumptions made by the bloggers and their 
comments. Moreover, most of the opinion ques-
tions have longer answers, not just a phrase snip-
pet, but up to 2 or 3 sentences. As we can ob-
serve in Table 2, the questions for which the 
TQA system performed better were the pure fac-
tual ones (1, 3, 4, 8, 10 and 14), although in some 
cases (question number 14) the OQA system re-
trieved more correct answers.  At the same time, 
opinion queries, although revolving around NEs, 
were not answered by the traditional QA system, 
but were satisfactorily answered by the opinion 
QA system (2, 5, 6, 7, 11, 12). Questions 18 and 
20 were not correctly answered by any of the two 
systems. We believe the reason is that question 
18 was ambiguous as far as polarity of the opin-
ions expressed in the answer snippets (?im-
provement? does not translate to either ?positive? 
or ?negative?) and question 20 referred to the 
title of a project proposal that was not annotated 
by any of the tools used. Thus, as part of the fu-
ture work in our OQA system, we must add a 
component for the identification of quotes and 
titles, as well as explore a wider range of polar-
ity/opinion scales. Furthermore, questions 15, 16, 
18, 19 and 20 contain both factual as well as 
opinion aspects and the OQA system performed 
better than the TQA, although in some cases, 
answers were lost due to the artificial boosting of 
the queries containing NEs of the EAT (Ex-
pected Answer Type). Therefore, it is obvious 
that an extra method for answer ranking should 
be used, as Answer Validation techniques using 
Textual Entailment. In Table 3, the OQA missed 
some of the answers due to erroneous sentence 
splitting, either separating text into two sentences 
where it was not the case or concatenating two 
consecutive sentences; thus missing out on one 
of two consecutively annotated answers. Exam-
ples are questions number 16 and 17, where 
many blog entries enumerated the different ar-
guments in consecutive sentences. Another 
source of problems was the fact that we gave a 
high weight to the presence of the NE of the 
sought type within the retrieved snippet and in 
some cases the name was misspelled in the blog 
entries, whereas in other NER performed by 
159
FreeLing either attributed the wrong category to 
an entity, failed to annotate it or wrongfully an-
notated words as being NEs.  Not of less impor-
tance is the question duality aspect in question 
17. Bush is commented in more than 600 sen-
tences; therefore, when polarity is not specified, 
it is difficult to correctly rank the answers. Fi-
nally, also the problems of temporal expressions 
and the coreference need to be taken into ac-
count.  
Conclusions and future work 
In this article, we created a collection of both 
factual and opinion queries in Spanish and Eng-
lish. We labeled the Gold Standard of the an-
swers in the corpora and subsequently we em-
ployed two QA systems, one open domain, one 
for opinion questions. Our main objective was to 
compare the performances of these two systems 
and analyze their errors, proposing solutions to 
creating an effective QA system for both factoid 
an opinionated queries. We saw that, even using 
specialized resources, the task of QA is still chal-
lenging. Opinion QA can benefit from a snippet 
retrieval at a paragraph level, since in many 
cases the answers were not simple parts of sen-
tences, but consisted in two or more consecutive 
sentences. On the other hand, we have seen cases 
in which each of three different consecutive sen-
tences was a separate answer to a question. Our 
future work contemplates the study of the impact 
anaphora resolution and temporality on opinion 
QA, as well as the possibility to use Answer 
Validation techniques for answer re-ranking. 
 
Acknowledgments 
 
The authors would like to thank Paloma Moreda, 
Hector Llorens, Estela Saquete and Manuel 
Palomar for evaluating the questions on their QA 
system. This research has been partially funded 
by the Spanish Government under the project 
TEXT-MESS (TIN 2006-15265-C06-01), by the 
European project QALL-ME (FP6 IST 033860) 
and by the University of Alicante, through its 
doctoral scholarship. 
References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo, 
and Patricio Mart?nez-Barco, 2009. Cross-topic 
Opinion Mining for Real-time Human-Computer 
Interaction. In Proceedings of the 6th Workshop in 
Natural Language Processing and Cognitive Sci-
ence, ICEIS 2009 Conference, Milan, Italy. 
Alexandra Balahur, Elena Lloret, Oscar Ferrandez, 
Andr?s Montoyo, Manuel Palomar, Rafael Mu?oz. 
2008. The DLSIUAES Team?s Participation in the 
TAC 2008 Tracks. In Proceedings of the Text 
Analysis Conference (TAC 2008). 
Ester Boldrini, Alexandra Balahur, Patricio Mart?nez-
Barco, and Andr?s Montoyo. 2009. EmotiBlog: An 
Annotation Scheme for Emotion Detection and 
Analysis in Non-Traditional Textual Genres. To 
appear in Proceedings of the 5th Conference on 
data Mining. Las Vegas, Nevada, USA. 
W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC 
2008. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu, 
Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie 
Huang. THU QUANTA at TAC 2008 QA and RTE 
track. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Bo Pang, and Lilian. Lee, Opinion mining and senti-
ment analysis. Foundations and Trends R. In In-
formation Retrieval Vol. 2, Nos. 1?2 (2008) 1?135, 
2008. 
James Pustejovsky and Janyce. Wiebe. Introduction 
to Special Issue on Advances in Question Answer-
ing. In Language Resources and Evaluation (2005) 
39: 119?122. Springer, 2006. 
Dan Shen, Jochen L. Leidner, Andreas Merkel, Diet-
rich Klakow. The Alyssa system at TREC QA 2007: 
Do we need Blog06? In Proceedings of The Six-
teenth Text Retrieval Conference (TREC 2007), 
Gaithersburg, MD, USA, 2007 
Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe. 
Multi-Perspective Question Answering Using the 
OpQA Corpus. In Proceedings of HLT/EMNLP. 
2005. 
Paloma Moreda, Hector Llorens, Estela Saquete, 
Manuel Palomar. 2008. Automatic Generalization 
of a QA Answer Extraction Module Based on Se-
mantic Roles. In: AAI - IBERAMIA, Lisbon, Portu-
gal, pages 233-242, Springer. 
Janyce. Wiebe, Theresa Wilson, and Claire Cardie 
Annotating expressions of opinions and emotions 
in language. Language Resources and Evaluation, 
volume 39, issue 2-3, pp. 165-210, 2005. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
Recognising Contextual Polarity in Phrase-level 
sentiment Analysis. In Proceedings of Human lan-
guage Technologies Conference/Conference on 
Empirical methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2005. 
160
Coling 2010: Poster Volume, pages 27?35,
Beijing, August 2010
Going Beyond Traditional QA Systems: Challenges and Keys 
in Opinion Question Answering 
Alexandra Balahur 
Dept. of Software and Computing Systems  
University of Alicante 
abalahur@dlsi.ua.es 
Ester Boldrini  
Dept. of Software and Computing Systems  
University of Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
Dept. of Software and Computing Systems  
University of Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
Dept. of Software and Computing Systems  
University of Alicante 
patricio@dlsi.ua.es 
Abstract  
The treatment of factual data has been 
widely studied in different areas of Nat-
ural Language Processing (NLP). How-
ever, processing subjective information 
still poses important challenges. This 
paper presents research aimed at assess-
ing techniques that have been suggested 
as appropriate in the context of subjec-
tive - Opinion Question Answering 
(OQA). We evaluate the performance of 
an OQA with these new components 
and propose methods to optimally tackle 
the issues encountered. We assess the 
impact of including additional resources 
and processes with the purpose of im-
proving the system performance on two 
distinct blog datasets. The improve-
ments obtained for the different combi-
nation of tools are statistically signifi-
cant. We thus conclude that the pro-
posed approach is adequate for the OQA 
task, offering a good strategy to deal 
with opinionated questions. 
1 Introduction 
The State of the Blogosphere 2009 survey pub-
lished by Technorati 1 concludes that in the past 
years the blogosphere has gained a high influ-
ence on a high variety of topics, ranging from 
cooking and gardening, to economics, politics 
and scientific achievements. The development 
                                                 
1 http://technorati.com/ 
of the Social Web and the new communication 
frameworks also influenced the way informa-
tion is transmitted through communities. Blogs 
are part of the so-called new textual genres. 
They have distinctive features when compared 
to the traditional ones, such as newspaper ar-
ticles. Blog language contains formal and in-
formal expressions, and other elements, as re-
peated punctuation or emoticons (used to stress 
upon different text elements). With the growth 
in the content of the blogosphere, the quantity 
of subjective data of the Web is increasing ex-
ponentially (Cui et al, 2006). As it is being up-
dated in real-time, this data becomes a source of 
timely information on many topics, exploitable 
by different applications. In order to properly 
manage the content of this subjective informa-
tion, its processing must be automated. The 
NLP task, which deals with the classification of 
opinionated content is called Sentiment Analy-
sis (SA). Research in this field aims at discover-
ing appropriate mechanisms to properly re-
trieve, extract and classify opinions expressed in 
text. While techniques to retrieve objective in-
formation have been widely studied, imple-
mented and evaluated, opinion-related tasks still 
represent an important challenge. As a conse-
quence, the aim of our research is to study, im-
plement and evaluate appropriate methods for 
the task of Question Answering (QA) in the 
opinion treatment framework.  
2 Motivation and Contribution 
Research in opinion-related tasks gained impor-
tance in the past years. However, there are still 
many aspects that require analysis and im-
27
provement, especially for approaches that com-
bine SA with other NLP tasks such as QA or 
automatic summarization. The TAC 2008 Opi-
nion Pilot task and the subsequent research per-
formed on the competition data have demon-
strated that answering opinionated questions 
and summarizing subjective information are 
significantly different from the equivalent tasks 
in the same context, but dealing with factual 
data.  This finding was confirmed by the recent 
work by (Kabadjov et al, 2009). The first moti-
vation of our work is the need to detect and ex-
plore the challenges raised by opinion QA 
(OQA), as compared to factual QA. To this aim, 
we analyze the improvements that can be 
brought at the different steps of the OQA 
process: question treatment (identification of 
expected polarity ? EPT, expected source ? ES 
and expected target ?ET-), opinion retrieval (at 
the level of one and three-sentences long snip-
pets, using topic-related words or using paraph-
rases), opinion analysis (using topic detection 
and anaphora resolution). This preliminary re-
search is motivated by the conclusions drawn by 
previous studies (Balahur et al, 2009). Our pur-
pose is to verify if the inclusion of new ele-
ments and methods - source and target detection 
(using semantic role labeling (SRL)), topic de-
tection (using Latent Semantic Analysis), pa-
raphrasing and joint topic-sentiment analysis 
(classification of the opinion expressed only in 
sentences related to the topic), followed by ana-
phora resolution (using a system whose perfor-
mance is not optimal), affects the results of the 
system and how. Our contribution to this respect 
is the identification of the challenges related to 
OQA compared to traditional QA. A further 
contribution consists in adding the appropriate 
methods, tools and resources to resolve the 
identified challenges. With the purpose of test-
ing the effect of each tool, resource and tech-
nique, we carry out a separate and a global 
evaluation. An additional motivation of our 
work is the fact that although previous ap-
proaches showed that opinion questions have 
longer answers than factual ones, the research 
done in OQA so far has only considered a sen-
tence-level approach. Another contribution this 
paper brings is the retrieval at 1 and 3-sentence 
level and the retrieval based on similarity to 
query paraphrases enriched with topic-related 
words). We believe retrieving longer text could 
cause additional problems such as redundancy, 
coreference and temporal expressions or the 
need to apply contextual information. Paraph-
rasing, on the other hand, had account for lan-
guage variability in a more robust manner; 
however, the paraphrase collections that are 
available at the moment are known to be noisy. 
The following sections are structured as fol-
lows: Section 3 presents the related work in the 
field and the competitions organized for systems 
tackling the OQA task. In Section 4 we describe 
the corpora used for the experiments we carried 
out and the set of questions asked over each of 
them. Section 5 presents the experimental set-
tings and the different system configurations we 
assessed. Section 6 shows the results of the 
evaluations, discusses the improvements and 
drops in performance using different configura-
tions. We finally conclude on our approaches in 
Section 7, proposing the lines for future work. 
3 Related Work 
QA can be defined as the task in which given a 
set of questions and a collection of documents, 
an automatic NLP system is employed to re-
trieve the answer to the queries in Natural Lan-
guage (NL). Research focused on building fac-
toid QA systems has a long tradition; however, 
it is only recently that researchers have started 
to focus on the development of OQA systems. 
(Stoyanov et al, 2005) and (Pustejovsky and 
Wiebe, 2006) studied the peculiarities of opi-
nion questions. (Cardie et al, 2003) employed 
opinion summarization to support a Multi-
Perspective QA system, aiming at identifying 
the opinion-oriented answers for a given set of 
questions. (Yu and Hatzivassiloglou, 2003) se-
parated opinions from facts and summarized 
them as answer to opinion questions. (Kim and 
Hovy, 2005) identified opinion holders, which 
are a key component in retrieving the correct 
answers to opinion questions. Due to the rea-
lized importance of blog data, recent years have 
also marked the beginning of NLP research fo-
cused on the development of opinion QA sys-
tems and the organization of international con-
ferences encouraging the creation of effective 
QA systems both for fact and subjective texts. 
The TAC 20082 QA track proposed a collection 
                                                 
2 http://www.nist.gov/tac/ 
28
of factoid and opinion queries called ?rigid list? 
(factoid) and ?squishy list? (opinion) respective-
ly, to which the traditional QA systems had to 
be adapted. Some participating systems treated 
opinionated questions as ?other? and thus they 
did not employ opinion specific methods. How-
ever, systems that performed better in the 
?squishy list? questions than in the ?rigid list? 
implemented additional components to classify 
the polarity of the question and of the extracted 
answer snippet. The Alyssa system (Shen et al 
2007) uses a Support Vector Machines (SVM) 
classifier trained on the MPQA corpus (Wiebe 
et al, 2005), English NTCIR3 data and rules 
based on the subjectivity lexicon (Wilson et al, 
2005). (Varma et al, 2008) performed query 
analysis to detect the polarity of the question 
using defined rules. Furthermore, they filter 
opinion from fact retrieved snippets using a 
classifier based on Na?ve Bayes with unigram 
features, assigning for each sentence a score that 
is a linear combination between the opinion and 
the polarity scores. The PolyU (Venjie et al, 
2008) system determines the sentiment orienta-
tion of the sentence using the Kullback-Leibler 
divergence measure with the two estimated lan-
guage models for the positive versus negative 
categories. The QUANTA (Li et al, 2008) sys-
tem performs opinion question sentiment analy-
sis by detecting the opinion holder, the object 
and the polarity of the opinion. It uses a seman-
tic labeler based on PropBank 4  and manually 
defined patterns. Regarding the sentiment clas-
sification, they extract and classify the opinion 
words. Finally, for the answer retrieval, they 
score the retrieved snippets depending on the 
presence of topic and opinion words and only 
choose as answer the top ranking results. Other 
related work concerns opinion holder and target 
detection. NTCIR 7 and 8 organized MOAT 
(the Multilingual Opinion Analysis Task), in 
which most participants employed machine 
learning approaches using syntactic patterns 
learned on the MPQA corpus (Wiebe et al, 
2005). Starting from the abovementioned re-
search, our aim is to take a step forward to 
present approaches and employ opinion specific 
methods focused on improving the performance 
of our OQA. We perform the retrieval at 1 sen-
                                                 
3 http://research.nii.ac.jp/ntcir/ 
4http://verbs.colorado.edu/~mpalmer/projects/ace.html 
tence and 3 sentence-level and also determine 
the expected source (ES) and the expected tar-
get (ET) of the questions, which are fundamen-
tal to properly retrieve the correct answer. These 
two elements are selected employing semantic 
roles (SR). The expected answer type (EAT) is 
determined using Machine Learning (ML) using 
Support Vector Machine (SVM), by taking into 
account the interrogation formula, the subjectiv-
ity of the verb and the presence of polarity 
words in the target SR. In the case of expected 
opinionated answers, we also compute the ex-
pected polarity type (EPT) ? by applying opi-
nion mining (OM) on the affirmative version of 
the question (e.g. for the question ?Why do 
people prefer Starbucks to Dunkin Donuts??, 
the affirmative version is ?People prefer Star-
bucks to Dunkin Donuts because X?). These 
experiments are presented in more detail in  
Section 5.  
4 Corpora 
In order to carry out the present research for 
detecting and solving the complexities of opi-
nion QA, we employed two corpora of blog 
posts: EmotiBlog (Boldrini et al, 2009a) and 
the TAC 2008 Opinion Pilot test collection (part 
of the Blog06 corpus). 
The TAC 2008 Opinion Pilot test collection is 
composed by documents with the answers to the 
opinion questions given on 25 targets. EmotiB-
log is a collection of blog posts in English ex-
tracted form the Web. As a consequence, it 
represents a genuine example of this textual ge-
nre. It consists in a monothematic corpus about 
the Kyoto Protocol, annotated with the im-
proved version of EmotiBlog (Boldrini et al, 
2009b). It is well know that Opinion Mining 
(OM) is a very complex task due to the high 
variability of the language employed. Thus, our 
objective is to build an annotation model that is 
able to capture the whole range of phenomena 
specific to subjectivity expression. Additional 
criteria employed when choosing the elements 
to be annotated were effectiveness and noise 
minimization. Thus, from the first version of the 
model, the elements which did not prove to be 
statistically relevant have been eliminated. The 
elements that compose the improved version of 
the annotation model are presented in Table 1.   
 
29
Elements Description 
Obj. speech Confidence, comment, source, target. 
Subj. speech Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Adjec-
tives/Adverbs 
Confidence, comment, level, emotion, 
phenomenon, modifier/not, polarity, 
source and target. 
Verbs/ Names Confidence, comment, level, emotion, 
phenomenon, polarity, mode, source 
and target. 
Anaphora Confidence, comment, type, source and 
target. 
Capital letter/ 
Punctuation 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Phenomenon Confidence, comment, type, colloca-
tion, saying, slang, title, and rhetoric. 
Reader/Author 
Interpr. (obj.) 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Emotions Confidence, comment, accept, anger, 
anticipation, anxiety, appreciation, bad, 
bewilderment, comfort, compassion? 
Table 1: EmotiBlog improved structure 
 
The first distinction consists in separating objec-
tive and subjective speech. Subsequently, a fin-
er-grained annotation is employed for each of 
the two types of data. Objective sentences are 
annotated with source and target (when neces-
sary, also the level of confidence of the annota-
tor and a comment). Subjective elements can be 
annotated at a sentence level, but they also have 
to be labeled at a word and/or phrase level. 
EmotiBlog also contains annotations of anapho-
ra at a cross-document level (to interpret the 
storyline of the posts) and the sentence type 
(simple sentence or title, but also saying or col-
location). Finally, the Reader and the Writer 
interpretation have to be marked in objective 
sentences. These elements are employed to 
mark and interpret correctly an apparent objec-
tive discourse, whose aim is to implicitly ex-
press an opinion (e.g. ?The camera broke in two 
days?). The first is useful to extract what is the 
interpretation of the reader (for example if the 
writer says The result of their governing was an 
increase of 3.4% in the unemployment rate in-
stead of The result of their governing was a dis-
aster for the unemployment rate) and the second 
to understand the background of the reader (i.e.. 
These criminals are not able to govern instead 
of saying the x party is not able to govern). 
From this sentence, for example, the reader can 
deduce the political ideas of the writer. The 
questions whose answers are annotated with 
EmotiBlog are the subset of opinion questions in 
English presented in (Balahur et al, 2009). The 
complete list of questions is shown in Table 2.  
 
N Question 
2 What motivates people?s negative opinions on the 
Kyoto Protocol? 
5 What are the reasons for the success of the Kyoto 
Protocol? 
6 What arguments do people bring for their criticism 
of media as far as the Kyoto Protocol is concerned? 
7 Why do people criticize Richard Branson? 
11 What negative opinions do people have on Hilary 
Benn? 
12 Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol? 
15 What alternative environmental friendly resources 
do people suggest to use instead of gas en the future? 
16 Is Arnold Schwarzenegger pro or against the reduc-
tion of CO2 emissions? 
18 What improvements are proposed to the Kyoto Pro-
tocol? 
19 What is Bush accused of as far as political measures 
are concerned? 
20 What initiative of an international body is thought to 
be a good continuation for the Kyoto Protocol? 
Table 2: Questions over the EmotiBlog  
corpus 
 
The main difference between the two corpora 
employed is that Emotiblog is monothematic, 
containing only posts about the Kyoto Protocol, 
while the TAC 2008 corpus contains documents 
on a multitude of subjects. Therefore, different 
techniques must be adjusted in order to treat 
each of them.  
5 Experiments 
5.1 Question Analysis 
In order to be able to extract the correct answer 
to opinion questions, different elements must be 
considered. As stated in (Balahur et al, 2009) 
we need to determine both the expected answer 
type (EAT) of the question ? as in the case of 
factoid ones - as well as new elements ? such as 
expected polarity type (EPT). However, opi-
nions are directional ? i.e., they suppose the ex-
istence of a source and a target to which they 
are addressed. Thus, we introduce two new 
elements in the question analysis ? expected 
source (ES) and expected target (ET). These 
two elements are selected by applying SR and 
choosing the source as the agent in the sentence 
and the direct object (patient) as the target of the 
opinion. Of course, the source and target of the 
30
opinions expressed can also be found in other 
roles, but at this stage we only consider these 
cases. The expected answer type (EAT) (e.g. 
opinion or other) is determined using Machine 
Learning (ML) using Support Vector Machine 
(SVM), by taking into account the interrogation 
formula, the subjectivity of the verb and the 
presence of polarity words in the target SR. In 
the case of expected opinionated answers, we 
also compute the expected polarity type (EPT) ? 
by applying OM on the affirmative version of 
the question. An example of such a transforma-
tion is: given the question ?What are the rea-
sons for the success of the Kyoto Protocol??, 
the affirmative version of the question is ?The 
reasons for the success of the Kyoto Protocol 
are X?.  
5.2 Candidate Snippet Retrieval 
In the answer retrieval stage, we employ four 
strategies:  
1. Using the JIRS (JAVA Information Re-
trieval System) IR engine (G?mez et al, 
2007) to find relevant snippets. JIRS re-
trieves passages (of the desired length), 
based on searching the question struc-
tures (n-grams) instead of the keywords, 
and comparing them.  
2. Using the ?Yahoo? search engine to re-
trieve the first 20 documents that are 
most related to the query. Subsequently, 
we apply LSA on the retrieved docu-
ments and extract the words that are 
most related to the topic. Finally, we 
expand the query using words that are 
very similar to the topic and retrieve 
snippets that contain at least one of 
them and the ET. 
3. Generating equivalent expressions for 
the query, using the DIRT paraphrase 
collection (Lin and Pantel, 2001) and 
retrieving candidate snippets of length 1 
and 3 (length refers to the number of 
sentences retrieved) that are similar to 
each of the new generated queries and 
contain the ET. Similarity is computed 
using the cosine measure. Examples of 
alternative queries for ?People like 
George Clooney? are ?People adore 
George Clooney?, ?People enjoy 
George Clooney?, ?People prefer 
George Clooney?. 
4. Enriching the equivalent expressions for 
the query in 3. with the topic-related 
words discovered in 2. using LSA. 
5.3 Polarity and topic-polarity classifica-
tion of snippets 
In order to determine the correct answers from 
the collection of retrieved snippets, we must 
filter for the next processing stage only the can-
didates that have the same polarity as the ques-
tion EPT. For polarity detection, we use a com-
bined system employing SVM ML on unigram 
and bigram features trained on the NTCIR 
MOAT 7 data and an unsupervised lexicon-
based system. In order to compute the features 
for each of the unigrams and bigrams, we com-
pute the tf-idf scores. 
The unsupervised system uses the Opinion 
Finder lexicon to filter out subjective sentences 
? that contain more than two subjective words 
or a subjective word and a valence shifter (ob-
tained from the General Inquirer resource). Sub-
sequently, it accounts for the presence of opi-
nionated words from four different lexicons ? 
MicroWordNet (Cerini et al, 2007), WordNet 
Affect (Strapparava and Valitutti, 2004) Emo-
tion Triggers (Balahur and Montoyo, 2008) and 
General Inquirer (Stone et al, 1966). For the 
joint topic-polarity analysis, we first employ 
LSA to determine the words that are strongly 
associated to the topic, as described in Section 
5.2 (second list item). Consequently, we com-
pute the polarity of the sentences that contain at 
least one topic word and the question target. 
5.4 Filtering using SR 
Finally, answers are filtered using the Semrol 
system for SR labeling described in (Moreda, 
2008). Subsequently, we filter all snippets with 
the required target and source as agent or pa-
tient. Semrol receives as input plain text with 
information about grammar, syntax, word 
senses, Named Entities and constituents of each 
verb. The system output is the given text, in 
which the semantic roles information of each 
constituent is marked. Ambiguity is resolved 
31
depending on the machine algorithm employed, 
which in this case is TIMBL5. 
6 Evaluation and Discussion 
We evaluate our approaches on both the Emo-
tiBlog question collection, as well as on the 
TAC 2008 Opinion Pilot test set. We compare 
them against the performance of the system eva-
luated in (Balahur et al, 2009) and the best 
(Copeck et al, 2008) and worst (Varma et al, 
2008) scoring systems (as far as F-measure is 
concerned) in the TAC 2008 task.  For both the 
TAC 2008 and EmotiBlog sets of questions, we 
employ the SR system in SA and determine the 
ES, ET and EPT. Subsequently, for each of the 
two corpora, we retrieve 1-phrase and 3-phrase 
snippets. The retrieval of the of the EmotiBlog 
candidate snippets is done using query expan-
sion with LSA and filtering according to the ET. 
Further on, we apply sentiment analysis (SA) 
using the approach described in Section 5.3 and 
select only the snippets whose polarity is the 
same as the determined question EPT. The re-
sults are presented in Table 3.  
 
Q 
N
o. 
N
o.  
A 
Baseline 
(Balahur et al, 
2009) 
1 phrase + 
ET+SA 
3 phrases 
+ET+SA 
  @ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@
2
0 
2 5 0 2 3 4 1 2 3 4 1 2 3 4 
5 1
1 
0 0 0 0 0 2 2 2 1 2 3 4 
6 2 0 0 1 2 1 1 2 2 0 1 2 2 
7 5 0 0 1 3 1 1 1 3 0 2 2 4 
1
1 
2 1 1 1 1 0 0 0 0 0 0 0 1 
1
2 
3 0 1 1 1 0 1 2 3 0 0 1 2 
1
5 
1 0 0 1 1 0 0 1 1 1 1 1 1 
1
6 
6 1 4 4 4 0 1 1 2 1 2 2 6 
1
8 
1 0 0 0 0 0 0 0 0 0 0 0 0 
1
9 
2
7 
1 5 6 1
8 
0 1 1 2 0 1 1 1 
2
0 
4 0 0 0 0 0 0 1 1 0 0 1 2 
Table 3: Results for questions over  
EmotiBlog 
 
                                                 
5
http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.2_Manual
.pdf and http://ilk.uvt.nl/timbl/ 
The retrieval of the TAC 2008 1-phrase and 3-
phrase candidate snippets was done using JIRS 
and, in a second approach, using the cosine si-
milarity measure between alternative queries 
generated using paraphrases and candidate 
snippets. Subsequently, we performed different 
evaluations, in order to assess the impact of us-
ing different resources and tools. Since the TAC 
2008 had a limit of the output of 7000 charac-
ters, in order to compute a comparable F-
measure, at the end of each processing chain, 
we only considered the snippets for the 1-phrase 
retrieval and for the 3-phases one until this limit 
was reached. 
1. In the first evaluation, we only apply the 
sentiment analysis tool and select the snip-
pets that have the same polarity as the ques-
tion EPT and the ET is found in the snippet.  
(i.e. What motivates peoples negative opi-
nions on the Kyoto Protocol? The Kyoto 
Protocol becomes deterrence to economic 
development and international cooperation/ 
Secondly, in terms of administrative aspect, 
the Kyoto Protocol is difficult to implement.  
- same EPT and ET) 
We also detected cases of same polarity but 
no ET, e.g. These attempts mean annual ex-
penditures of $700 million in tax credits in 
order to endorse technologies, $3 billion in 
developing research and $200 million in 
settling technology into developing coun-
tries ? EPT negative but not same ET. 
2. In the second evaluation, we add the result 
of the LSA process to filter out the snippets 
from 1., containing the words related to the 
topic starting from the retrieval performed 
by Yahoo, which extracts the first 20 docu-
ments about the topic. 
3. In the third evaluation, we filter the results 
in 2 by applying the Semrol system and set-
ting the condition that the ET and ES are the 
agent or the patient of the snippet. 
4. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases (as explained in 
the third method in section 5.2.). We subse-
quently filtered these results based on their 
polarity  (so that it corresponds to the EPT) 
and on the condition that the source and tar-
get of the opinion (identified through SRL 
using Semrol) correspond to the ES and ET.  
32
5. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases, enriched with the 
topic words determined using LSA. We 
subsequently filtered these results based on 
their polarity (so that it corresponds to the 
EPT) and on the condition that the source 
and target of the opinion (identified through 
SRL using Semrol) correspond to the ES 
and ET.  
 
System F-measure 
Best TAC 0.534 
Worst TAC 0.101 
JIRS + SA+ET (1 phrase)  0.377 
JIRS + SA+ET (3 phrases)  0.431 
JIRS + SA+ET+LSA (1 phrase)  0.489 
JIRS + SA+ET+LSA (3 phrases)  0.505 
JIRS + SA+ET+LSA+SR (1 
phrase)  
0. 533 
JIRS + SA+ET+LSA+SR (3 
phrases) 
0.571 
PAR+SA+ET+SR(1 phrase) 0.345 
PAR+SA+ET+SR(2 phrase) 0.386 
PAR_LSA+SA+ET+SR (1 phra-
se) 
0.453 
PAR_LSA+SA+ET+SR (3 phra-
ses) 
0.434 
Table 4: Results for the TAC 2008 test set 
 
From the results obtained (Table 3 and Table 4), 
we can draw the following conclusions. Firstly, 
the hypothesis that OQA requires the retrieval 
of longer snippets was confirmed by the im-
proved results, both in the case of EmotiBlog, as 
well as the TAC 2008 corpus. Secondly, opi-
nion questions require the use of joint topic-
sentiment analysis. As we can see from the re-
sults, the use of topic-related words when com-
puting of the affect influences the results in a 
positive manner and joint topic-sentiment anal-
ysis is especially useful for the cases of ques-
tions asked on a monothematic corpus. Thirdly, 
another conclusion that we can draw is that tar-
get and source detection are highly relevant 
steps at the time of answer filtering, not only 
helping the more accurate retrieval of answers, 
but also at placing at the top of the retrieval the 
relevant results (as more relevant information is 
contained within these 7000 characters). The 
use of paraphrases at the retrieval stage was 
shown to produce a significant drop in results, 
which we explain by the noise introduced and 
the fact that more non-relevant answer candi-
dates were introduced among the results. None-
theless, as we can see from the overall relatively 
low improvement in the results, much remains 
to be done in order to appropriately tackle 
OQA. As seen in the results, there are still ques-
tions for which no answer is found (e.g. 18). 
This is due to the fact that the treatment of such 
questions requires the use of inference tech-
niques that are presently unavailable (i.e. define 
terms such as ?improvement?, possibly as ?X 
better than Y?, in which case opinion extraction 
from comparative sentences should be intro-
duced in the model).  
The results obtained when using all the compo-
nents for the 3-sentence long snippets signifi-
cantly improve the results obtained by the best 
system participating in the TAC 2008 Opinion 
Pilot competition (determined using a paired t-
test for statistical significance, with confidence 
level 5%). Finally, from the analysis of the er-
rors, we could see that even though some tools 
are in theory useful and should produce higher 
improvements ? such as SR ? their performance 
in reality does not produce drastically higher 
results. The idea to use paraphrases for query 
expansion also proved to decrease the system 
performance. From preliminary results obtained 
using JavaRap6  for coreference resolution, we 
also noticed that the performance of the OQA 
lowered, although theoretically it should have 
improved. 
7 Conclusions ad Future Work 
In this paper, we presented and evaluated differ-
ent methods and techniques with the objective 
of improving the task of QA in the context of 
opinion data. From the evaluations performed 
using different NLP resources and tools, we 
concluded that joint topic-sentiment analysis, as 
well as the target and source identification, are 
crucial for the correct performance of this task. 
We have also demonstrated that by retrieving 
longer answers, the results have improved. We 
tested, within a simple setting, the impact of 
using paraphrases in the context of opinion 
questions and saw that their use lowered the 
system results. Although such paraphrase col-
                                                 
6http://wing.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.ht
m 
33
lections include a lot of noise and have been 
shown to decrease system performance even in 
the case of factual questions, we believe that 
other types of paraphrasing methods should be 
investigated in the context of OQA. We thus 
showed that opinion QA requires the develop-
ment of appropriate strategies at the different 
stages of the task (recognition of subjective 
questions, detection of subjective content of the 
questions, source and target identification, re-
trieval and classification of the candidate an-
swer data). Due to the high level of complexity 
of subjective language, our future work will be 
focused on testing higher-performing tools for 
coreference resolution, other (opinion) paraph-
rases collections and paraphrasing methods and 
the employment of external knowledge sources 
that refine the semantics of queries. We also 
plan to include other SA methods and extend 
the semantic roles considered for ET and ES, 
with the purpose of checking if they improve or 
not the performance of the QA system. 
 
Acknowledgements 
This paper has been partially supported by Mi-
nisterio de Ciencia e Innovaci?n - Spanish Gov-
ernment (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat Va-
lenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/286). 
References 
Balahur, A. and Montoyo, A. 2008. Applying a 
Culture Dependent Emotion Triggers Data-
base for Text Valence and Emotion 
Classification. In Proceedings of the AISB 
2008 Symposium on Affective Language in 
Human and Machine, Aberdeen, Scotland. 
Balahur, A., Lloret, E., Ferr?ndez, O., Montoyo, 
A., Palomar, M., and Mu?oz, R. 2008. The 
DLSIUAES Team?s Participation in the TAC 
2008 Tracks. In Proceedings of the Text 
Analysis Conference 2008 Workshop. 
Balahur, A., Boldrini, E., Montoyo A. and 
Mart?nez-Barco P. 2009. Opinion and Generic 
Question Answering Systems: a Performance 
Analysis. In Proceedings of ACL. Singapur.  
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and  Montoyo. A. 2009a. EmotiBlog: an An-
notation Scheme for Emotion Detection and 
Analysis in Non-traditional Textual Genre. In 
Proceedings of DMIN 2009, Las Vegas. Ne-
vada. 
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and Montoyo. A. 2009b. EmotiBlog: a fine-
grained model for emotion detection in non-
traditional textual genre. In Proceedings of 
WOMSA 2009. Seville. 
Cardie, C., Wiebe, J., Wilson, T. and Litman, D. 
2003. Combining Low-Level and Summary 
Representations of Opinions for Multi-
Perspective Question Answering. AAAI 
Spring Symposium on New Directions in 
Question Answering. 
Cerini, S., Compagnoni, V., Demontis, A., 
Formentelli, M. and Gandini, C. 2007. Mi-
cro-WNOp: A gold standard for the evalua-
tion of automatically compiledlexical re-
sources for opinion mining. In: A.Sanso 
(ed.): Language resources and linguistic 
theory: Typology, Second Language Acqui-
sition, English Linguistics. Milano. IT. 
Copeck, T.,  Kazantseva, A., Kennedy, A., 
Kunadze, A., Inkpen, D. and Szpakowicz, 
S. 2008. Update Summary Update. In Pro-
ceedings of the Text Analysis Conference 
(TAC) 2008. 
Cui, H., Mittal, V. and Datar, M. 2006. Com-
parative Experiments on Sentiment Classifi-
cation for Online Product Review. Proceed-
ings, The Twenty-First National Conference 
on Artificial Intelligence and the Eighteenth 
Innovative Applications of Artificial Intelli-
gence Conference. Boston, Massachusetts, 
USA. 
G?mez, J.M., Rosso, P. and Sanchis, E. 2007. 
JIRS Language-Independent Passage Re-
trieval System: A Comparative Study. 5th 
International Conference on Natural 
Language Proceeding (ICON 2007). 
Kabadjov, M., Balahur, A. And Boldrini, E. 
2009. Sentiment Intensity: Is It a Good 
Summary Indicator?. Proceedings of the 4th 
Language Technology Conference LTC, pp. 
380-384. Poznan, Poland, 6-8.11.2009. 
Kim, S. M. and Hovy, E. 2005. Identifying 
Opinion Holders for Question Answering in 
Opinion Texts. Proceedings of the 
Workshop on Question Answering in 
Restricted Domain at the Conference of the 
American Association of Artificial 
Intelligence (AAAI-05).  Pittsburgh, PA. 
34
Li, F., Zheng, Z.,Yang T., Bu, F., Ge, R., Zhu, 
X., Zhang, X., and Huang, M. 2008. THU 
QUANTA at TAC 2008. QA and RTE track. 
In Proceedings of the Text Analysis 
Conference (TAC). 
Lin, D. and Pantel, P. 2001. Discovery of 
Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-
360. 
Moreda. P. 2008. Los Roles Sem?nticos en la 
Tecnolog?a del Lengauje Humano: Anota-
ci?n y Aplicaci?n. Doctoral Thesis. Univer-
sity of Alicante. 
Pustejovsky, J. and Wiebe, J. 2006. Introduction 
to Special Issue on Advances in Question 
Answering. Language Resources and Eval-
uation (2005), (39). 
Shen, D., Wiegand, M., Merkel, A., Kazalski, 
S., Hunsicker, S., Leidner, J. L. and 
Klakow, D. 2007. The Alyssa System at 
TREC QA 2007: Do We Need Blog06? In 
Proceedings of the Sixteenth Text Retrieval 
Conference (TREC 2007), Gaithersburg, 
MD, USA. 
Strapparava, C. and Valitutti, A. 2004. Word-
Net-Affect: an affective extension of Word-
Net. In Proceedings of 4th International Con-
ference on Language Resources and Evalua-
tion (LREC 2004), pages 1083 ? 1086, Lis-
bon. 
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. 
Multiperspective question answering using 
the opqa corpus. In Proceedings of the 
Human Language Technology Conference 
and the Conference on Empirical Methods 
in Natural Language Processing 
(HLT/EMNLP 2005). 
Varma, V., Pingali, P., Katragadda, S., Krishna, 
R., Ganesh, S., Sarvabhotla, K. Garapati, 
H., Gopisetty, H., Reddy, K. and 
Bharadwaj, R. 2008. IIIT Hyderabad at 
TAC 2008. In Proceedings of Text Analysis 
Conference (TAC).  
Wenjie, L., Ouyang, Y., Hu, Y. and Wei, F. 
2008. PolyU at TAC 2008. In Proceedings 
of the Text Analysis Conference (TAC). 
Wiebe, J., Wilson, T., and Cardie, C. 2005. 
Annotating expressions of opinions and 
emotions in language. Language Resources 
and Evaluation, volume 39, issue 2-3, pp. 
165-210. 
Wilson, T., J. Wiebe, and Hoffmann, P. 2005. 
Recognizing Contextual Polarity in Phrase-
level sentiment Analysis. In Proceedings of 
the Human Language Technologies 
Conference/Conference on Empirical 
Methods in Natural Language Processing 
(HLT/ EMNLP). 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions. In Proceedings of 
EMNLP-03. 
Wiebe, J., Wilson, T., and Cardie, C. (2005). 
Annotating expressions of opinions and 
emotions in language. In Language 
Resources and Evaluation. Vol. 39. 
35
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 25?30,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Detecting Event-Related Links and Sentiments from Social Media Texts
Alexandra Balahur and Hristo Tanev
European Commission Joint Research Centre
Via E. Fermi 2749, T.P. 267
21027 Ispra (VA), Italy
{alexandra.balahur, hristo.tanev}@jrc.ec.europa.eu
Abstract
Nowadays, the importance of Social Me-
dia is constantly growing, as people often
use such platforms to share mainstream
media news and comment on the events
that they relate to. As such, people no
loger remain mere spectators to the events
that happen in the world, but become part
of them, commenting on their develop-
ments and the entities involved, sharing
their opinions and distributing related con-
tent. This paper describes a system that
links the main events detected from clus-
ters of newspaper articles to tweets related
to them, detects complementary informa-
tion sources from the links they contain
and subsequently applies sentiment analy-
sis to classify them into positive, negative
and neutral. In this manner, readers can
follow the main events happening in the
world, both from the perspective of main-
stream as well as social media and the pub-
lic?s perception on them.
This system will be part of the EMM me-
dia monitoring framework working live
and it will be demonstrated using Google
Earth.
1 Introduction
In the context of the Web 2.0, the importance
of Social Media has been constantly growing in
the past years. People use Twitter, Facebook,
LinkedIn, Pinterest, blogs and Web forums to give
and get advice, share information on products,
opinions and real-time information about ongoing
and future events. In particular Twitter, with its
half a billion active members, was used during dis-
asters, protests, elections, and other events to share
updates, opinions, comments and post links to on-
line resources (e.g. news, videos, pictures, blog
posts, etc.). As such, Twitter can be used as a com-
plementary source of information, from which we
can retrieve additional facts, but also learn about
the attitude of the people towards certain events.
On the one hand, news from the traditional me-
dia focus on the factual side of events, important
for the society or at least large groups of people.
On the other hand, social media reflects subjec-
tive interpretations of facts, with different levels of
relevance (societal or only individual). Therefore,
the events reported in online news can be consid-
ered a point of intersection for both types of me-
dia, which are able to offer complementary views
on these events.
In this context, we describe a system that we
developed as an additonal component to the EMM
(Europe Media Monitor)1 news monitoring frame-
work, linking mainstream news to related texts
from social media and detecting the opinion (sen-
timent) users express on these topics.
In the EMM news monitoring system, the dif-
ferent news sites are monitored and new articles
are scraped from them, with a refresh rate of 10
minutes. Subsequently, news items are clustered
and the most important ones are displayed (top
10). These are called ?stories?. Our system subse-
quently links these stories to messages from Twit-
ter (tweets) and extracts the related URLs they
contain. Finally, it analyzes the sentiments ex-
pressed in the tweets by using a hybrid knowledge-
based and statistical sentiment detection module.
The overview of the system is depicted in Figure
1http://emm.jrc.it/NewsBrief/clusteredition/en/latest.html
25
1.
Figure 1: Overview of the news clusters-Twitter
linking and sentiment analysis system.
The system will be demonstrated using the
Google Earth interface (Figure 2), presenting the
characteristics of the event described in the story
(type, date, location, the first words in the arti-
cle that is the centroid of the news cluster for that
story). In addition, we present new information
that we extract from Twitter - links (URLs) that
we find from the tweets we retrieved linked to the
story and positive, negative and neutral sentiment,
respectively, as a proportion of the total number of
tweets retrieved.
Figure 2: Demo interface for the event-Twitter
linking and sentiment analysis.
2 Related Work and Contribution
The work presented herein is mostly related to the
linking of events with social media texts and sen-
timent analysis from Twitter.
Although Twitter was used as an information
source in the context of different crisis events, rel-
atively little work focused on linking and extract-
ing content about events which are known a priori,
e.g., Becker et al [2011].
In this context, the main challenge is to deter-
mine relevant keywords to search for event-related
tweets and rank them according to their relevance.
Related approaches (e.g., Verma et al [2011]) re-
port on the use of semantic features (e.g., objec-
tivity, impersonality, formality, etc.) for detecting
tweets with content relevant to situational aware-
ness during mass emergencies. Other approaches
elaborate on machine learning-based techniques
for Named Entity Recognition (NER) from tweets,
which are subsequently employed as search query
terms ( Ritter et al [2011], Liu et al [2011]).
Related research on sentiment analysis from
Twitter was done by Alec Go and Huang [2009],
Pak and Paroubek [2010] and Agarwal et al
[2011]. Alec Go and Huang [2009] and Pak and
Paroubek [2010] exploit the presence of emoticons
that represent positive or negative feelings to build
a training set of tweets with sentiment labels, using
which they build models based on n-gram features
and part-of-speech tags. Agarwal et al [2011] em-
ploy emoticons dictionaries and replace certain el-
ements such as URLs and topics with predefinded
labels. They employ syntactic features and spe-
cialized tree kernels and obtain around 75% to
80% accuracy for the sentiment classification.
The main contributions of our system reside in
the linking of mainstream news to the complemen-
tary content found in social media (tweets and,
through them, to the links to additional informa-
tion sources like blogs, flickr, youtube, etc.) and
the analysis of sentiment on these important news.
For events such as ?The Arab Spring?, protests, fi-
nancial news (e.g. the fluctuations of the Euro, the
bailout of different European countries, the rise in
unemployment rate, etc.), it was seen that the sen-
timent expressed in social media has a high impact
on the subsequent development of the story2 (Saif
et al [2012], Bollen et al [2011]). The impact of
sentiment expressed in social media is also visi-
ble for topics which apparently have an apriori va-
lence (e.g. disasters, crisis, etc.). Nevertheless, in
these cases, people communicate using the social
media platforms not only to express their negative
feelings, but also their will to help, their situation,
their messages of encouragement, their grateful-
ness for the help and so on.
2http://cs229.stanford.edu/proj2011/ChenLazer-
SentimentAnalysisOfTwitterFeedsForThePrediction
OfStockMarketMovement.pdf
26
Secondly, the methods employed in our system
are simple, work fast and efficient and can be eas-
ily adapted to other languages.
Finally, the methods presented take into account
the specificity of social media languages, applying
methods to normalize the language and adapting
the features considered for the supervised learning
process.
3 Linking News Clusters to Twitter
The first step in our system involves linking the
news stories detected by EMM to related tweets.
The linking system employs the Twitter Search
API3. For each news story, our application detects
relevant URLs by finding tweets that are lexically
similar to the news story, represented by a cluster
of news, and are mentioned frequently in Twitter.
In Figure 3, we provide an example of the top six
stories on the afternoon of April 2nd, 2013.
Figure 3: Top six clusters of news in the afternoon
of April 2nd, 2013.
In order to detect lexically similar tweets, we
use vector similarity: We build a term vector for
both the news story and the tweet and then we
consider as a similarity measure the projection
of the tweet vector on the story vector. We do
not calculate cosine similarity, since this would
give an advantage to short tweets. We experi-
mentally set a similarity threshold above which
the tweets with URL are accepted. To define
the similarity threshold and the coefficients in the
URL ranking formula, we used a development set
of about 100 randomly selected English-language
news clusters, downloaded during a week. The
3https://dev.twitter.com/docs/api/1/get/search
threshold and the coefficients were derived empir-
ically. We consider experimenting with SVM and
other machine-learning approaches to define these
parameters in a more consistent way.
Once the tweets that relate to the news story are
retrieved, we evaluate each URL taking into ac-
count the following parameters:
? Number of mentions, which we will desig-
nate as Mentions.
? Number of retweets, designated Retweet.
? Number of mentions in conversations, desig-
nated InConv.
? Number of times the URL was favortited,
designated Favorited.
? Number of tweets which replied to tweets,
mentioning the URL, designated ReplyTo.
The score of the URL is calculated using the
following empirically derived formula. The coef-
ficients were defined based on the empirical anal-
ysis described above.
score(URL) = ((Mentions?1)+Retweets.1, 3
+Favorited ? 4).(InConv + 2 ?ReplyTo + 1)
In this formula we give slight preference to the
retweets with respect to the mentions. We made
this choice, since retweets happen inside Twitter
and reflect the dynamics of the information spread
inside this social media. On the other hand, multi-
ple mentions of news-related tweets (which are not
retweeted) are due to clicking the ?Share in Twit-
ter? button, which nowadays is present on most
of the news sites. In this way, news from visited
web sites appear more often in Twitter. This phe-
nomena is to be further explored. It should also be
noted that our formula boosts significantly URLs,
which are mentioned inside a conversation thread
and even more the ones, to which there were ?re-
ply to? tweets. Conversations tend to be cen-
tered around topics which are of interest to Twit-
ter users and in this way they are a good indica-
tor of how interesting an URL is. Replying to a
tweet requires more time and attention than just
pressing the ?Retweet? button, therefore conversa-
tions show more interest to an URL, with respect
to retweeting. Examples of tweets extracted that
complement information from mainstream media
are presented in Figure 4.
27
Figure 4: Examples of tweets extracted on the
North Korea crisis (anonimized).
4 Sentiment Analysis on Tweets Related
to Events Reported in News
After extracting the tweets related to the main
news clusters detected by the media monitoring
system, we pass them onto the sentiment analy-
sis system, where they are classified according to
their polarity (into positive, negative and neutral).
In order to classify the tweet?s sentiment, we
employ a hybrid approach based on supervised
learning with a Support Vector Machines Sequen-
tial Minimal Optimization (SVM SMO - Platt
[1998]) linear kernel, on unigram and bigram fea-
tures, but exploiting as features sentiment dictio-
naries, emoticon lists, slang lists and other social
media-specific features. We do not employ any
specific language analysis software. The aim is to
be able to apply, in a straightforward manner, the
same approach to as many languages as possible.
The approach can be extended to other languages
by using similar dictionaries that have been cre-
ated in our team.
The sentiment analysis process contains two
stages: preprocessing and sentiment classification.
4.1 Tweet Preprocessing
The language employed in Social Media sites is
different from the one found in mainstream me-
dia and the form of the words employed is some-
times not the one we may find in a dictionary. Fur-
ther on, users of Social Media platforms employ a
special ?slang? (i.e. informal language, with spe-
cial expressions, such as ?lol?, ?omg?), emoticons,
and often emphasize words by repeating some of
their letters. Additionally, the language employed
in Twitter has specific characteristics, such as the
markup of tweets that were reposted by other users
with ?RT?, the markup of topics using the ?#?
(hash sign) and of the users using the ?@? sign.
All these aspects must be considered at the time
of processing tweets. As such, before applying su-
pervised learning to classify the sentiment of the
tweets, we preprocess them, to normalize the lan-
guage they contain. The preprocessing stage con-
tains the following steps:
? Repeated punctuation sign normalization.
In the first step of the preprocessing, we de-
tect repetitions of punctuation signs (?.?, ?!?
and ???). Multiple consecutive punctuation
signs are replaced with the labels ?multi-
stop?, for the fullstops, ?multiexclamation?
in the case of exclamation sign and ?multi-
question? for the question mark and spaces
before and after.
? Emoticon replacement. In the second step
of the preprocessing, we employ the anno-
tated list of emoticons from SentiStrength4
and match the content of the tweets against
this list. The emoticons found are replaced
with their polarity (?positive? or ?negative?)
and the ?neutral? ones are deleted.
? Lower casing and tokenization. Subse-
quently, the tweets are lower cased and split
into tokens, based on spaces and punctuation
signs.
? Slang replacement. The next step involves
the normalization of the language employed.
In order to be able to include the semantics
of the expressions frequently used in Social
Media, we employed the list of slang from a
specialized site 5.
? Word normalization. At this stage, the to-
kens are compared to entries in Roget?s The-
saurus. If no match is found, repeated
letters are sequentially reduced to two or
one until a match is found in the dictio-
nary (e.g. ?perrrrrrrrrrrrrrrrrrfeeect? becomes
?perrfeect?, ?perfeect?, ?perrfect? and subse-
quently ?perfect?). The words used in this
form are maked as ?stressed?.
? Affect word matching. Further on, the tokens
in the tweet are matched against three dif-
ferent sentiment lexicons: General Inquirer,
LIWC and MicroWNOp, which were pre-
viously split into four different categories
4http://sentistrength.wlv.ac.uk/
5http://www.chatslang.com/terms/social media
28
(?positive?, ?high positive?, ?negative? and
?high negative?). Matched words are re-
placed with their sentiment label - i.e. ?pos-
itive?, ?negative?, ?hpositive? and ?hnega-
tive?.
? Modifier word matching. Similar to the
previous step, we employ a list of expres-
sions that negate, intensify or diminish the
intensity of the sentiment expressed to detect
such words in the tweets. If such a word is
matched, it is replaced with ?negator?, ?in-
tensifier? or ?diminisher?, respectively.
? User and topic labeling. Finally, the users
mentioned in the tweet, which are marked
with ?@?, are replaced with ?PERSON? and
the topics which the tweet refers to (marked
with ?#?) are replaced with ?TOPIC?.
4.2 Sentiment Classification of Tweets
Once the tweets are preprocessed, they are passed
on to the sentiment classification module. We em-
ployed supervised learning using SVM SMO with
a linear kernel, employing boolean features - the
presence or absence of unigrams and bigrams de-
termined from the training data (tweets that were
previousely preprocessed as described above) that
appeared at least twice. Bigrams are used espe-
cially to spot the influence of modifiers (nega-
tions, intensifiers, diminishers) on the polarity of
the sentiment-bearing words. We tested the ap-
proach on different datasets and dataset splits, us-
ing the Weka data mining software 6. The training
models are built on a cluster of computers (4 cores,
5000MB of memory each).
5 Evaluation and Discussion
5.1 Evaluation of the News-Twitter Linking
Component
The algorithm employed to retrieve tweets simi-
lar to news clusters was evaluated by Tanev et al
[2012]. The precision attained was 75%. Recall
cannot be computed, as the use of the Twitter API
allows only the retrieval of a subset of tweets.
In order to evaluate the link extraction compo-
nent, we randomly chose 68 URLs, extracted from
10 different news stories. For each URL, we eval-
uated its relevance to the news story in the follow-
ing way: A URL is considered relevant only if it
6http://www.cs.waikato.ac.nz/ml/weka/
reports about the same news story or talks about
facts, like effects, post developments and motiva-
tions, directly related to this news story. It turned
out that 66 out of the 68 were relevant, which gives
accuracy of 97%.
5.2 Evaluation of the Sentiment Analysis
System
In order to evaluate the sentiment analysis sys-
tem on external resources, we employed the data
provided for training in the SemEval 2013 Task
2 ?Sentiment Analysis from Twitter? 7. The ini-
tial training data has been provided in two stages:
1) sample datasets for the first task and the sec-
ond task and 2) additional training data for the two
tasks. We employ the joint sample datasets as test
data (denoted as t?) and the data released subse-
quently as training data (denoted as T?). We em-
ploy the union of these two datasets to perform
cross-validation experiments (the joint dataset is
denoted as T ? +t?. The characteristics of the
dataset are described in Table 1. On the last col-
umn, we also include the baseline in terms of ac-
curacy, which is computed as the number of ex-
amples of the majoritary class over the total num-
ber of examples. The results of the experiments
Data #Tweet #Pos #Neg #Neu B%
T* 19241 4779 2343 12119 62
t* 2597 700 393 1504 57
T*+t* 21838 5479 2736 13623 62
Table 1: Characteristics of the training (T*), test-
ing (t*) and joint training and testing datasets.
are presented in Table 2. Given the difficulty of
Measure Train(T*) & test(t*) 10-fold CV
Acc. 0.74 0.93
Ppos 0.66 0.91
Rpos 0.88 0.69
Pneg 0.94 0.62
Rneg 0.81 0.49
Pneu 0.93 0.80
Rneg 0.97 0.82
Table 2: Results in terms of accuracy and preci-
sion and recall per polarity class on training and
test sets evaluation and 10-fold cross-validation.
language in social media, the results are good and
7http://www.cs.york.ac.uk/semeval-2013/task2/
29
useful in the context of our application (Figure 2).
6 Conclusions and Future Work
In this demo paper, we presented a system that
links mainstream media stories to tweets that com-
ment on the events covered. The system retrieves
relevant tweets, extracts the links they contain and
subsequently performs sentiment analysis. The
system works at a good level, giving an accurate
picture of the social media reaction to the main-
stream media stories.
As future work, we would like to extend the sys-
tem to more languages and analyze and include
new features that are particular to social media to
improve the performance of both the retrieval and
sentiment analysis components.
Acknowledgements
We would like to thank the EMM team of the OP-
TIMA action at the European Commission Joint
Research Centre for the technical support.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen
Rambow, and Rebecca Passonneau. Sentiment
analysis of twitter data. In Proceedings of LSM
2011, LSM ?11, pages 30?38, 2011.
Richa Bhayani Alec Go and Lei Huang. Twit-
ter sentiment classication using distant supervi-
sion. Technical report, Technical report, Stan-
ford University, 2009.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naa-
man, and Luis Gravano. Automatic identifi-
cation and presentation of twitter content for
planned events. In Proceedings of ICWSM
2011, 2011.
J. Bollen, H. Mao, and X. Zeng. Twitter mood
predicts the stock market. Journal of Computa-
tional Science, 2011.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and
Ming Zhou. Recognizing Named Entities in
Tweets. In Proceedings of ACL 2011, pages
359?367, Stroudsburg, PA, USA, 2011.
Alexander Pak and Patrick Paroubek. Twitter
based system: Using twitter for disambiguat-
ing sentiment ambiguous adjectives. In Pro-
ceedings of SemEval 2010, SemEval ?10, pages
436?439, 2010.
John C. Platt. Sequential minimal optimization:
A fast algorithm for training support vector ma-
chines. Technical report, Advances in Kernel
Methods - Support Vector Learning, 1998.
Alan Ritter, Sam Clark, Mausam, and Oren Et-
zioni. Named Entity Recognition in Tweets: An
Experimental Study. In Proceedings of EMNLP
2011, pages 1524?1534, Edinburgh, Scotland,
UK., 2011.
Hassan Saif, Yulan He, and Harith Alani. Alleviat-
ing data sparsity for twitter sentiment analysis.
In Making Sense of Microposts (#MSM2012),
pages 2?9, 2012.
Hristo Tanev, Maud Ehrmann, Jakub Piskorski,
and Vanni Zavarella. Enhancing event descrip-
tions through twitter mining. In John G. Bres-
lin, Nicole B. Ellison, James G. Shanahan, and
Zeynep Tufekci, editors, ICWSM. The AAAI
Press, 2012.
Sudha Verma, Sarah Vieweg, William Corvey,
Leysia Palen, James Martin, Martha Palmer,
Aaron Schram, and Kenneth Anderson. Natural
Language Processing to the Rescue? Extracting
?Situational Awareness?? Tweets During Mass
Emergency. In Proceedings of ICWSM 2011,
pages 385?392. AAAI, 2011.
30
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 444?447,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
OpAL: Applying Opinion Mining Techniques for the Disambiguation of 
Sentiment Ambiguous Adjectives in SemEval-2 Task 18  
 
Alexandra Balahur 
University of Alicante 
Department of Software and  
Computing Systems 
abalahur@dlsi.ua.es 
Andr?s Montoyo 
University of Alicante 
Department of Software and  
Computing Systems 
montoyo@dlsi.ua.es 
  
 
  
 
Abstract 
 
The task of extracting the opinion expressed in 
text is challenging due to different reasons. 
One of them is that the same word (in particu-
lar, adjectives) can have different polarities 
depending on the context. This paper presents 
the experiments carried out by the OpAL team 
for the participation in the SemEval 2010 Task 
18 ? Disambiguation of Sentiment Ambiguous 
Adjectives. Our approach is based on three dif-
ferent strategies: a) the evaluation of the polar-
ity of the whole context using an opinion min-
ing system; b) the assessment of the polarity of 
the local context, given by the combinations 
between the closest nouns and the adjective to 
be classified; c) rules aiming at refining the lo-
cal semantics through the spotting of modifi-
ers. The final decision for classification is tak-
en according to the output of the majority of 
these three approaches.  The method used 
yielded good results, the OpAL system run 
ranking fifth among 16 in micro accuracy and 
sixth in macro accuracy.   
1 Credits  
This research has been supported by Ministerio 
de Ciencia e Innovaci?n - Spanish Government 
(grant no. TIN2009-13391-C04-01), and Consel-
leria d'Educaci?n-Generalitat Valenciana (grant 
no. PROMETEO/2009/119 and ACOMP/2010/ 
288).  
2 Introduction 
Recent years have marked the beginning and ex-
pansion of the Social Web, in which people free-
ly express and respond to opinion on a whole 
variety of topics. Moreover, at the time of taking 
a decision, more and more people search for in-
formation and opinions expressed on the Web on 
their matter of interest and base their final deci-
sion on the information found (Pang and Lee, 
2008). Nevertheless, the high quantity of data 
that has to be analysed imposed the development 
of specialized Natural Language Processing 
(NLP) systems that automatically extract, classi-
fy and summarize the opinions available on the 
web on different topics. Research in this field, of 
opinion mining (sentiment analysis), has ad-
dressed the problem of extracting and classifying 
opinions from different perspectives and at dif-
ferent levels, depending on various factors. 
While determining the overall opinion on a mov-
ie is sufficient for taking the decision to watch it 
or not, when buying a product, people are inter-
ested in the individual opinions on the different 
product characteristics.  Especially in this con-
text, opinion mining systems are confronted with 
a difficult problem: the fact that the adjectives 
used to express opinion have different polarities 
depending on the characteristic they are men-
tioned with. For example, ?high price? is nega-
tive, while ?high resolution? is positive. There-
fore, specialized methods have to be employed to 
correctly determine the contextual polarity of 
such words and thus accurately assign polarity to 
the opinion.    
This is the aim of the SemEval 2010 Task 18 ? 
Disambiguation of Sentiment Ambiguous Adjec-
tives (Wu and Jin, 2010). In the following sec-
tions, we first present state-of-the art approaches 
towards polarity classification of opinions, sub-
sequently describing our approach in the SemEv-
al task. Finally, we present the results we ob-
tained in the evaluation and our plans for future 
work.    
444
3 State of the Art  
Subjectivity analysis is defined by (Wiebe, 1994) 
as the ?linguistic expression of somebody?s opi-
nions, sentiments, emotions, evaluations, beliefs 
and speculations?. Sentiment analysis, on the 
other hand, is defined as the task of extracting, 
from a text, the opinion expressed on an object 
(product, person, topic etc.) and classifying it as 
positive, negative or neutral. The task of senti-
ment analysis, considered a step further to sub-
jectivity analysis, is more complex than the lat-
ter, because it involves an extra step: the classifi-
cation of the retrieved opinion words according 
to their polarity. There are a series of techniques 
that were used to obtain lexicons of subjective 
words ? e.g. the Opinion Finder lexicon (Wilson 
et al, 2005) and opinion words with associated 
polarity. (Hu and Liu, 2004) start with a set of 
seed adjectives (?good? and ?bad?) and apply 
synonymy and antonymy relations in WordNet. 
A similar approach was used in building Word-
Net Affect (Strapparava and Valitutti, 2004), 
starting from a larger set of seed affective words, 
classified according to the six basic categories of 
emotion (joy, sadness, fear, surprise, anger and 
disgust) and expanding the lexicon using paths in 
WordNet. Another related method was used in 
the creation of SentiWordNet (Esuli and Sebas-
tiani, 2005), using a set of seed words whose po-
larity was known and expanded using gloss simi-
larity. The collection of appraisal terms in (Whi-
telaw et al, 2005), the terms also have polarity 
assigned. MicroWNOp (Cerini et al, 2007), 
another lexicon containing opinion words with 
their associated polarity, was built on the basis of 
a set of terms extracted from the General Inquirer 
lexicon and subsequently adding all the synsets 
in WordNet where these words appear. Other 
methods built sentiment lexicons using the local 
context of words. (Pang et al, 2002) built a lex-
icon of sentiment words with associated polarity 
value, starting with a set of classified seed adjec-
tives and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. (Turney, 2002) classifies words ac-
cording to their polarity on the basis of the idea 
that terms with similar orientation tend to co-
occur in documents. Thus, the author computes 
the Pointwise Mutual Information score between 
seed words and new words on the basis of the 
number of AltaVista hits returned when querying 
the seed word and the word to be classified with 
the ?NEAR? operator. In our work in (Balahur 
and Montoyo, 2008a), we compute the polarity 
of new words using ?polarity anchors? (words 
whose polarity is known beforehand) and Nor-
malized Google Distance (Cilibrasi and Vitanyi, 
2006) scores. Another approach that uses the po-
larity of the local context for computing word 
polarity is (Popescu and Etzioni, 2005), who use 
a weighting function of the words around the 
context to be classified.   
4 The OpAL system at SemEval 2010 
Task 18 
In the SemEval 2010 Task 18, the participants 
were given a set of contexts in Chinese, in which  
14 dynamic sentiment ambiguous adjectives are 
selected. They are: ?|big, ?|small, ?|many, ?
|few, ?|high, ?|low, ?|thick, ?|thin, ?|deep, 
?|shallow, ?|heavy, ? |light, ??|huge, ??
|grave. The task was to automatically classify the 
polarity of these adjectives, i.e. to detect whether 
their sense in the context is positive or negative. 
The contexts were given in two forms: as plain 
text, in which the adjective to be classified was 
marked; in the second for, the text was tokenized 
and the tokens were tagged with part of speech 
(POS). There was no training set provided.  
  Our approach uses a set of opinion mining re-
sources and an opinion mining system that is 
implemented to work for English. This is why, 
the first step we took in our approach was to 
translate the given contexts into English using 
the Google Translator1. In order to perform this 
task, we first split the initial file into 10 smaller 
files, using a specialized program ? GSplit32.  
The OpAL adjective polarity disambiguation 
system combines supervised methods with unsu-
pervised ones.  In order to judge the polarity of 
the adjectives, it uses three types of judgments. 
The first one is the general polarity of the con-
text, determined by our in-house opinion mining 
system - based on SVM machine learning on the 
NTCIR data and the EmotiBlog (Boldrini et al, 
2009) annotations and different subjectivity, opi-
nion and emotion lexica (Opinion Finder, Mi-
croWordNet Opinion, General Inquirer, Word-
Net Affect, emotion triggers (Balahur and Mon-
toyo, 2008b). The second one is the local polari-
ty, given by the highest number of results ob-
tained when issuing queries containing the clos-
est noun with the adjective to be disambiguated 
followed by the conjunction ?AND? and a prede-
fined set of 6 adjectives whose polarity is non-
                                                 
1 http://translate.google.com/ 
2 www.gdgsoft.com/gsplit/ 
445
ambiguous ? 3 positive - ?positive?, ?beautiful?, 
?good? and 3 negative ? ?negative?, ?ugly?, 
?bad?. An example of such queries is ?price high 
and good?. The third component is made up of 
rules, depending on the presence of specific 
modifiers in a window of 4 words before the ad-
jective.  The final verdict is given based on the 
vote given by the majority of the three compo-
nents, explained in detail in the next sections: 
4.1 The OpAL opinion mining component 
First, we process each context using Minipar3. 
We compute, for each word in a sentence, a se-
ries of features, computed from the NTCIR 7 
data and the EmotiBlog annotations. These 
words are used to compute vectors of features for 
each of the individual contexts: 
 the part of speech (POS)  
 opinionatedness/intensity - if the word is 
annotated as opinion word, its polarity, i.e. 1 
and -1 if the word is positive or negative, re-
spectively and 0 if it is not an opinion word, 
its intensity (1, 2 or 3) and 0 if it is not a 
subjective word 
 syntactic relatedness with other opinion 
word ? if it is directly dependent of an opi-
nion word or modifier (0 or 1), plus the po-
larity/intensity and emotion of this word (0 
for all the components otherwise) 
  role in 2-word, 3-word, 4-word and sen-
tence annotations: opinionatedness, intensity 
and emotion of the other words contained in 
the annotation, direct dependency relations 
with them if they exist and 0 otherwise.  
We add to the opinion words annotated in 
EmotiBlog the list of opinion words found in the 
Opinion Finder, Opinion Finder, MicroWordNet 
Opinion, General Inquirer, WordNet Affect, 
emotion triggers lexical resources. We train the 
model using the SVM SMO implementation in 
Weka4. 
4.2 Assessing local polarity using Google 
queries 
This approach aimed at determining the polarity 
of the context immediately surrounding the ad-
jective to be classified. To that aim, we con-
structed queries using the noun found before the 
adjective in the context given, and issued six dif-
ferent queries on Google, together with six pre-
defined adjectives whose polarity is known (3 
                                                 
3 http://webdocs.cs.ualberta.ca/~lindek/minipar.htm 
4 http://www.cs.waikato.ac.nz/ml/weka/ 
positive - ?positive?, ?beautiful?, ?good? and 3 
negative ? ?negative?, ?ugly?, ?bad?). The form 
of the queries was ?noun+adjective+AND+pre-
defined adjective?.  The local polarity was consi-
dered as the one for which the query issued the 
highest number of total results (total number of 
results for the 3 queries corresponding to the pos-
itive adjectives or to the negative adjectives, re-
spectively).  
4.3 Modifier rules for contextual polarity  
This rule accounts for the original, most fre-
quently used polarity of the given adjectives (e.g. 
high is positive, low is negative). For each of 
them, we define its default polarity. Subsequent-
ly, we determine whether in the window of 4 
words around the adjective there are any modifi-
ers (valence shifters). If this is the case, and they 
have an opposite value of polarity, the adjective 
is assigned a polarity value opposite from its de-
fault one (e.g. too high is negative).  We employ 
a list of 82 positive and 87 negative valence shif-
ters.  
5  Evaluation  
Table 1 and Table 2 present the results obtained 
by the OpAL system in the SemEval 2010 Task 
18 competition. The system ranked fifth, with a 
Micro accuracy of 0.76037 and sixth, with a Ma-
cro accuracy of 0.7037.  
 
System name Micro accura-
cy 
98-35_result 0.942064 
437-381_HITSZ_CITYU_ 
Task18_Run1.key 
0.936236 
437-380_HITSZ_CITYU_ 
Task18_Run2.key 
0.93315 
53-211_dsaa 0.880699 
186-325_OpAL_results.txt 0.76037 
291-389_submission4.txt 0.724717 
291-388_submission3.txt 0.715461 
437-382_HITSZ_CITYU_ 
Task18_Run3 
0.665752 
 Table 1: Results - top 8 runs (micro accuracy) 
 
System name Macro  accu-
racy 
437-380_HITSZ_CITYU_ 
Task18_Run2.key 0.957881 
437-381_HITSZ_CITYU_ 
Task18_Run1.key 0.953238 
98-35_result 0.929308 
53-211_dsaa 0.861964 
446
291-388_submission3.txt 0.755387 
186-325_OpAL_results.txt 0.703777 
291-389_submission4.txt 0.698037 
460383_New_Task18_ 
Chinese_test_pos_QiuLikun_R.rar 0.695448 
Table 2: Results ? top 8 runs (macro accuracy) 
 
Since the gold standard was not provided, we 
were not able to perform an exhaustive analysis 
of the errors. However, from a random inspec-
tion of the system results, we could see that a 
large number of errors was due to the translation 
? through which modifiers are placed far from 
the word they determine or the words are not 
translated with their best equivalent.  
6 Conclusions and future work 
In this article we presented our approach towards 
the disambiguation of polarity ambiguous adjec-
tives depending on the context in which they ap-
pear. The OpAL system?s run was based on three 
subcomponents working in English ? one assess-
ing the overall polarity of the context using an 
opinion mining system, the second assessing the 
local polarity using Google queries formed by 
expressions containing the noun present in the 
context before the adjective to be classified and 
the third one evaluating contextual polarity based 
on the adjective?s default value and the modifiers 
around it. The final output is based on the vote 
given by the majority of the three components. 
The approach had a good performance, the 
OpAL system run ranking fifth among 16 runs. 
Future work includes the separate evaluation of 
the three components and their combination in a 
unique approach, using machine learning, as well 
as a thorough assessment of errors that are due to 
translation.   
References  
Balahur, A. and Montoyo, A. 2008a. A feature-driven 
approach to opinion mining and classification. In 
Proceedings of the NLPKE 2008. 
Balahur, A. and Montoyo, A. 2008b. Applying a cul-
ture dependent emotion triggers database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 40(40). 
Boldrini, E., Balahur, A., Mart?nez-Barco, P., and 
Montoyo, A. 2009. EmotiBlog: an annotation 
scheme for emotion detection and analysis in non-
traditional textual genres. In Proceedings of the 
5th International Conference on Data Mining 
(DMIN 2009). 
Cerini, S., Compagnoni, V., Demontis, A., Formentel-
li, M., and Gandini, G. 2007. Micro-WNOp: A gold 
standard for the evaluation of automatically com-
piled lexical resources for opinion mining. 
Cilibrasi, D. and Vitanyi, P. 2006. Automatic Mean-
ing Discovery Using Google. IEEE Journal of 
Transactions on Knowledge and Data Engineering. 
Esuli, A. and Sebastiani, F. 2006. SentiWordNet: a 
publicly available resource for opinion mining. In 
Proceedings of the 6th International Conference on 
Language Resources and Evaluation. 
Hu, M. and Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of Nineteenth 
National Conference on Artificial Intellgience 
AAAI-2004. 
Pang, B. and Lee, L. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2), pp. 1?135, 2008 
Pang, B., Lee, L., and Vaithyanathan, S. 2002. 
Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of 
EMNLP-02, the Conference on Empirical Methods 
in Natural Language Processing. 
Popescu, A. M. and Etzioni, O. 2005. Extracting 
product features and opinions from reviews. In In 
Proceedings of HLTEMNLP 2005. 
Stone, P., Dumphy, D. C., Smith, M. S., and Ogilvie, 
D. M. 1966. The General Inquirer: A Computer 
Approach to Content Analysis. The MIT Press. 
Strapparava, C. and Valitutti, A. 2004.WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC 2004). 
Turney, P. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguis-
tics. 
Whitelaw, C., Garg, N., and Argamon, S. 2005. Using 
appraisal groups for sentiment analysis. In Pro-
ceedings of the CIKM 2005. 
Wiebe, J. (1994). Tracking point of view in narrative. 
Computational Linguistics, 20. 
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Re-
cognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of HLT-EMNLP 
2005. 
Wu, Y., Jin, P. 2010. SemEval-2010 Task 18: Disam-
biguating Sentiment Ambiguous Adjectives. In Pro-
ceedings of the SemEval 2010 Workshop, ACL 
2010.  
447
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 460?465, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
OPTWIMA: Comparing Knowledge-rich and Knowledge-poor Approaches
for Sentiment Analysis in Short Informal Texts
Alexandra Balahur
European Commission Joint Research Centre
Via E. Fermi 2749
21027 Ispra (VA), Italy
{alexandra.balahur}@jrc.ec.europa.eu
Abstract
The fast development of Social Media made it
possible for people to no loger remain mere
spectators to the events that happen in the
world, but become part of them, comment-
ing on their developments and the entities in-
volved, sharing their opinions and distribut-
ing related content. This phenomenon is of
high importance to news monitoring systems,
whose aim is to obtain an informative snap-
shot of media events and related comments.
This paper presents the strategies employed in
the OPTWIMA participation to SemEval 2013
Task 2-Sentiment Analysis in Twitter. The
main goal was to evaluate the best settings for
a sentiment analysis component to be added to
the online news monitoring system.
We describe the approaches used in the com-
petition and the additional experiments per-
formed combining different datasets for train-
ing, using or not slang replacement and gener-
alizing sentiment-bearing terms by replacing
them with unique labels.
The results regarding tweet classification are
promising and show that sentiment generaliza-
tion can be an effective approach for tweets
and that SMS language is difficult to tackle,
even when specific normalization resources
are employed.
1 Introduction
Sentiment analysis is the Natural Language Process-
ing (NLP) task dealing with the detection and clas-
sification of sentiments in texts. Usually, the classes
considered are ?positive?, ?negative? and ?neutral?,
although in some cases finer-grained categories are
added (e.g. ?very positive? and ?very negative?) or
only the ?positive? and ?negative? classes are taken
into account.
This task has received a lot of interest from the re-
search community in the past years. The work done
regarded the manner in which sentiment can be clas-
sified from texts pertaining to different genres and
distinct languages, in the context of various applica-
tions, using knowledge-based, semi-supervised and
supervised methods [Pang and Lee, 2008]. The re-
sult of the analyses performed have shown that the
different types of text require specialized methods
for sentiment analysis, as, for example, sentiments
are not conveyed in the same manner in newspaper
articles and in blogs, reviews, forums or other types
of user-generated contents [Balahur et al, 2010].
In the light of these findings, dealing with senti-
ment analysis in tweets and SMS (that we can gener-
ally call ?short informal texts?) requires an analysis
of the characteristics of such texts and the design of
adapted methods.
Our participation in the SemEval 2013 Task 2
[Wilson et al, 2013] had as objective to test how
well our proposed methods for sentiment analysis
for short informal texts (especially tweets) would
perform. The two subtasks proposed in this com-
petition were: a) the classification of sentiment from
snippets from tweets and SMS marked as start and
end position and b) the classification of sentiment
from entire tweets and SMS. Each team could sub-
mit 2 runs for each dataset and task, one employ-
ing as training data only the data provided within
the competition (?constrained?) and the second em-
460
ploying any additional data (?unconstrained?). We
submitted 2 of such runs for each of the subtasks
and datasets.
The main requirements for the system we imple-
mented were: a) not to use language-specific NLP
processing tools (since our final goal is to make the
present system work for many more languages); and
b) to work fast, so that it can be integrated in a near
real time media monitoring system.
2 Related Work and Contribution
One of the first studies on the classification of polar-
ity in tweets was Go et al [2009]. The authors con-
ducted a supervised classification study on tweets
in English, using the emoticons (e.g. ?:)?, ?:(?,
etc.) as markers of positive and negative tweets.
Read [2005] employed this method to generate a
corpus of positive tweets, with positive emoticons
?:)?, and negative tweets with negative emoticons
?:(?. Subsequently, they employ different supervised
approaches (SVM, Na??ve Bayes and Maximum En-
tropy) and various sets of features and conclude that
the simple use of unigrams leads to good results, but
it can be slightly improved by the combination of
unigrams and bigrams.
In the same line of thinking, Pak and Paroubek
[2010] also generated a corpus of tweets for sen-
timent analysis, by selecting positive and negative
tweets based on the presence of specific emoticons.
Subsequently, they compare different supervised ap-
proaches with n-gram features and obtain the best
results using Na??ve Bayes with unigrams and part-
of-speech tags.
Another approach on sentiment analysis in tweet
is that of Zhang et al [2011]. Here, the authors em-
ploy a hybrid approach, combining supervised learn-
ing with the knowledge on sentiment-bearing words,
which they extract from the DAL sentiment dictio-
nary [Whissell, 1989]. Their pre-processing stage
includes the removal of retweets, translation of ab-
breviations into original terms and deleting of links,
a tokenization process, and part-of-speech tagging.
They employ various supervised learning algorithms
to classify tweets into positive and negative, using n-
gram features with SVM and syntactic features with
Partial Tree Kernels, combined with the knowledge
on the polarity of the words appearing in the tweets.
The authors conclude that the most important fea-
tures are those corresponding to sentiment-bearing
words. Finally, Jiang et al [2011] classify sentiment
expressed on previously-given ?targets? in tweets.
They add information on the context of the tweet to
its text (e.g. the event that it is related to). Subse-
quently, they employ SVM and General Inquirer and
perform a three-way classification (positive, nega-
tive, neutral).
The main contributions of the approaches con-
sidered for the competition reside in the evaluation
of different strategies to adapt sentiment analysis
methods to the language employed in short informal
texts.
The methods employed in our system are simple,
work fast and efficient and can be easily adapted
to other languages. The main adaptations we con-
sider are part of a pre-processing step, in which the
language in these short informal texts is normalized
(brought to a dictionary form).
Finally, the methods presented are compared on
different configurations and training sets, so that the
conclusions drawn are relevant to the phenomena
found in this type of informal texts.
3 Methods Employed by OPTWIMA in
SemEval 2013 Task 2
We employ two different approaches: a) one
based on supervised learning using Support Vector
Machines Sequential Minimal Optimization (SVM
SMO) using unigram and bigram features; and b) a
hybrid approach, based on supervised learning with
a SVM SMO linear kernel, on unigram and bigram
features, but exploiting as features sentiment dictio-
naries, emoticon lists, slang lists and other social
media-specific features. SVM SMO was preferred
due to the computation speed. We do not employ
any specific language analysis software. The aim
is to be able to apply, in a straightforward manner,
the same approach to as many languages as possible.
The approach can be extended to other languages by
using similar dictionaries that have been created in
our team Steinberger et al [2011].
The sentiment analysis process contains two
stages: preprocessing and sentiment classification.
461
3.1 Preprocessing of Short Informal Texts
The language employed in short informal texts such
as tweets and SMS is different from the one found
in other types of texts, such as newspaper articles
and the form of the words employed is sometimes
not the one we may find in a dictionary. Further
on, users writing on Twitter or SMS-ing on their
cell phone employ a special ?slang? (i.e. informal
language, with special expressions, such as ?lol?,
?omg?), emoticons, and often emphasize words by
repeating some of their letters. Additionally, the lan-
guage employed in Twitter has specific characteris-
tics, such as the markup of tweets that were reposted
by other users with ?RT?, the markup of topics us-
ing the ?#? (hash sign) and of the users using the
?@? sign.
All these aspects must be considered at the time
of processing tweets and, to some extent, SMS.
As such, before applying supervised learning to
classify the sentiment of the short informal texts
considered, we preprocess them, to normalize the
language they contain and try to abstract on the con-
cepts that are sentiment-bearing, by replacing them
with labels, according to their polarity1. In case of
SMS messages, the slang employed, the short forms
of words and the acronyms make these texts non pro-
cessable without prior replacement and normaliza-
tion of the slang. The preprocessing stage contains
the following steps:
? Repeated punctuation sign normalization
(RPSN).
In the first step of the preprocessing, we detect
repetitions of punctuation signs (?.?, ?!? and
???). Multiple consecutive punctuation signs
are replaced with the labels ?multistop?, for
the fullstops, ?multiexclamation? in the case of
exclamation sign and ?multiquestion? for the
question mark and spaces before and after.
? Emoticon replacement (ER).
In the second step of the preprocessing, we em-
ploy the annotated list of emoticons from Sen-
tiStrength2 and match the content of the tweets
1The preprocessing steps involving the use of affect dictio-
naries and modifier replacement are used only in one of the two
methods considered
2http://sentistrength.wlv.ac.uk/
against this list. The emoticons found are re-
placed with their polarity (?positive? or ?nega-
tive?) and the ?neutral? ones are deleted.
? Lower casing and tokenization (LCN).
Subsequently, the tweets are lower cased and
split into tokens, based on spaces and punctua-
tion signs.
? Slang replacement (SR).
The next step involves the normalization of the
language employed. In order to be able to
include the semantics of the expressions fre-
quently used in Social Media, we employed the
list of slang expressions from dedicated sites 3.
This step is especially relevant to SMS texts,
whose language in their original form has little
to do with language employed in ordinary texts.
? Word normalization (WN).
At this stage, the tokens are compared to entries
in Roget?s Thesaurus. If no match is found, re-
peated letters are sequentially reduced to two or
one until a match is found in the dictionary (e.g.
?perrrrrrrrrrrrrrrrrrfeeect? becomes ?perrfeect?,
?perfeect?, ?perrfect? and subsequently ?per-
fect?). The words used in this form are maked
as ?stressed?.
? Affect word matching (AWM).
Further on, the tokens in the tweet are matched
against three different sentiment lexicons: Gen-
eral Inquirer, LIWC and MicroWNOp, which
were previously split into four different cate-
gories (?positive?, ?high positive?, ?negative?
and ?high negative?). Matched words are re-
placed with their sentiment label - i.e. ?posi-
tive?, ?negative?, ?hpositive? and ?hnegative?.
? Modifier word matching (MWM).
Similar to the previous step, we employ a list
of expressions that negate, intensify or dimin-
ish the intensity of the sentiment expressed to
detect such words in the tweets. If such a word
is matched, it is replaced with ?negator?, ?in-
tensifier? or ?diminisher?, respectively.
3www.noslang.com/dictionary, www.smsslang.com
462
? User and topic labeling (UTL).
Finally, the users mentioned in the tweet, which
are marked with ?@?, are replaced with ?PER-
SON? and the topics which the tweet refers to
(marked with ?#?) are replaced with ?TOPIC?.
3.2 Sentiment Classification of Short Informal
Texts
Once the texts are preprocessed, they are passed on
to the sentiment classification module.
We employed supervised learning using Support
Vector Machines Sequential Minimal Optimization
(SVM SMO) [Platt, 1998] with a linear kernel, em-
ploying boolean features - the presence or absence
of unigrams and bigrams determined from the train-
ing data (tweets that were previousely preprocessed
as described above) that appeared at least twice. Bi-
grams are used especially to spot the influence of
modifiers (negations, intensifiers, diminishers) on
the polarity of the sentiment-bearing words. We
tested different parameters for the kernel and modi-
fied only the C constant to the best value determined
on the training data (5.0)/
We tested the approach on different datasets and
dataset splits, using the Weka data mining software
4. The training models are built on a cluster of com-
puters (4 cores, 5000MB of memory each).
4 Evaluation and Discussion
We participated in SemEval 2013 in Task 2 with
two versions of the system, for each of the two sub-
tasks (A and B). The main difference among them is
the use of dictionaries for affect and modifier word
matching and replacement. As such, in the first
method (denoted as ?Dict?), we perform all the pre-
processing steps mentioned above, while the second
method is applied on the data on which the AWM
and MWM are not performed (i.e. words that are
associated with a sentiment in a lexicon are not re-
placed with labels). This second method will be de-
noted ?NoDict?.
Another difference between the different evalu-
ations we performed are the datasets employed for
training. We created different models, employing:
1) For both the ?Constrained? and ?Uncon-
strained? submissions, the development and train-
4http://www.cs.waikato.ac.nz/ml/weka/
ing data from the corresponding subtask (i.e. using
as training the data in subtask A - the sets given as
training and development together - to train a classi-
fier for the test data in task A; the same for subtask
B). In this case, the training data is marked with the
corresponding subtask (i.e. training data ?A?, train-
ing data ?B?);
2) For both the ?Constrained? and ?Uncon-
strained? submissions, the development and training
data from both subtasks - both training and develop-
ment sets - to train one classifier which is used for
both subtasks. This training set is denoted as ?A+B?;
3) For the ?Unconstrained? submissions, we
added to the joint training and development data
from both subtasks the set of MySpace comments
provided by [Thelwall et al, 2010]. This small set
contains 1300 short texts from the MySpace social
network5. The motivation behind this choice is that
texts from this source are very similar in language
and structure to tweets and (after slang replacement)
SMS.
Finally, we trained different classifiers on the
training sets described, with and without replacing
the affective and modifier words and with and with-
out employing the slang replacement pre-processing
step.
The results are presented in Tables 1, 2, 3, 4, in
terms of average F-measure of the positive and neg-
ative classes (as used by the organizers). The runs
submitted in the competition are marked with an as-
terisk (?*?). We did not perform all the experiments
for the sets of SMS without slang replacement, as
the first results were very low.
As we can see from the results, our approach per-
formed better in classifying the overall sentiment of
texts than small snippets. The results were signifi-
cantly better for the classification of tweets in com-
parison to SMS, whose language (even with slang
replacement) made them difficult to tackle. We can
also see that the joint use of slang replacement and
dictionaries for tweets leads to significantly lower
results, meaning that this step (at least with the re-
sources we employed for slang treatment), is not
necessary for the treatment of tweets. Instead, for
these texts, the use of affect dictionaries and mod-
ifier lists and their generalizaton lead to better re-
5http://www.myspace.com/
463
Trained on A+B with slang replacement (Constrained)
Test set Dict NoDict
Task A Tweets 0.35 0.37
Task A SMS 0.35 0.37*
Task B Tweets 0.45* 0.54
Task B SMS 0.40* 0.47
Table 1: Results obtained using A+B (train and developement data) as training set and replacing the slang.
Trained on A+B+MySpace with slang replacement (Unconstrained)
Test Set Dict NoDict
Task A Tweets 0.36 0.39*
Task A SMS 0.37* 0.37
Task B Tweets 0.46 0.54*
Task B SMS 0.40 0.37*
Table 2: Results obtained using A+B+MySpace (train and developement data) as training set and replacing the slang.
sults. This proves that such a generalization, in the
context of ?legible? texts, is a useful tool for senti-
ment analysis. Further on, the results showed that
adding a small quantity of training data led to no
significant growth in performance (for the data in
which slang was replaced). Additional evaluations
could be made to quantify the effect of this data
when other methods to generalize are not applied.
As an observation, our results were balanced for all
three classes, with even higher scores for the neutral
class. We believe this class should have been con-
sidered as well, since in real-world settings systems
for sentiment analysis must also be able to classify
texts pertaining to this category.
Finally, we can see that in the case of SMS, the
difference between the use of slang with or without
affect label generalizations is insignificant. We be-
lieve this is due to the fact that the expressions with
which the slang is replaced are very infrequent in
traditional sentiment dictionaries (such as the ones
we employed). Even by replacing the short forms
and slang with their equivalents, the texts obtained
contain words that are infrequent in other types of
texts, even tweets. However, we will perform addi-
tional experiments with other lists of slang and add,
as much as it is possible, the informal sentiment-
bearing expressions to create new affect resources
for this types of texts.
5 Conclusions and Future Work
In this article, we presented and evaluated the ap-
proaches considered for our participation in the Se-
mEval 2013 Task 2. We evaluated different com-
binations of features, resources and training sets
and applied different methods to tackle the issues
brought by the informal language used in tweets and
SMS.
As future work, we would like to extend the sys-
tem to more languages, using the dictionaries cre-
ated by Steinberger et al [2011] and analyze and in-
clude new features that are particular to social media
- especially tweets - to improve the performance of
the sentiment analysis component. Further on, we
would like to quantify the influence of using linguis-
tic processing tools to perform lemmatizing, POS-
tagging and the inclusion of corresponding features
on the final performance of the system. Finally, we
would like to explore additional resources to deal
with the issue of language informality in tweets and
further explore the problems posed by the peculiar
language employed in SMS.
References
Alexandra Balahur, Ralf Steinberger, Mijail Kabad-
jov, Vanni Zavarella, Erik van der Goot, Matina
Halkia, Bruno Pouliquen, and Jenya Belyaeva.
Sentiment analysis in the news. In Proceedings
464
Trained on data of subtask (A or B) with slang replacement
Test Set Dict NoDict
Task A Tweets 0.36 0.37
Task A SMS 0.36 0.37
Task B Tweets 0.5 0.55
Task B SMS 0.49 0.53
Table 3: Results obtained using A (train and developement data) or B (train and developement data) as training set and
replacing the slang.
Trained on data of subtask (A or B), no slang replacement Trained on A+B, no slang replacement
Test Set Dict NoDict Dict NoDict
Task A Tweets 0.69* 0.59 0.6 0.69
Task B Tweets 0.59 0.51 0.62 0.44
Table 4: Results obtained for tweet classification using A+B or A or B as training set and not replacing the slang.
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta, may 2010.
Alec Go, Richa Bhayani, and Lei Huang. Twitter
sentiment classification using distant supervision.
Processing, pages 1?6, 2009.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. Target-dependent twitter sentiment
classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, HLT ?11, pages 151?160. ACL, 2011. ISBN
978-1-932432-87-9.
Alexander Pak and Patrick Paroubek. Twitter as a
corpus for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference
on International Language Resources and Eval-
uation (LREC?10), Valletta, Malta; ELRA, may
2010. ELRA. ISBN 2-9517408-6-7. 19-21.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Found. Trends Inf. Retr., 2(1-2):
1?135, January 2008. ISSN 1554-0669.
John C. Platt. Sequential minimal optimization:
A fast algorithm for training support vector ma-
chines. Technical report, Advances in Kernel
Methods - Support Vector Learning, 1998.
Jonathon Read. Using emoticons to reduce depen-
dency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL
Student Research Workshop, ACLstudent ?05,
pages 43?48, Stroudsburg, PA, USA, 2005.
J. Steinberger, P. Lenkova, M. Ebrahim,
M. Ehrmann, A. Hurriyetoglu, M. Kabad-
jov, R. Steinberger, H. Tanev, V. Zavarella, and
S. Va?zquez. Creating sentiment dictionaries via
triangulation. In Proceedings of WASSA 2011,
WASSA ?11, pages 28?36. ACL, 2011.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. Sentiment in short
strength detection informal text. Journal of the
American Society for Information Science and
Technology, 61(12):2544?2558, December 2010.
Cynthia Whissell. The Dictionary of Affect in Lan-
guage. In Robert Plutchik and Henry Kellerman,
editors, Emotion: theory, research and experi-
ence, volume 4, The measurement of emotions.
Academic Press, London, 1989.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Rit-
ter. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June
2013.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil,
Meichun Hsu, and Bing Liu. Combining lexicon-
based and learning-based methods for twitter sen-
timent analysis. Technical Report HPL-2011-89,
HP, 21/06/2011 2011.
465
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
EmotiBlog: a finer-grained and more precise learning of subjectivity expression models 
Ester Boldrini University of Alicante, Department of Software and Computing Systems eboldrini@dlsi.ua.es 
Alexandra Balahur University of Alicante, Department of Software and Computing Systems abalahur@dlsi.ua.es Patricio Mart?nez-Barco University of Alicante, Department of Software and Computing Systems patricio@dlsi.ua.es 
Andr?s Montoyo University of Alicante, Department of Software and Computing Systems montoyo@dlsi.ua.es  Abstract  
The exponential growth of the subjective in-formation in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog ? a fine-grained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the sys-tems using it for training, through the experi-ments we carried out on opinion mining and emotion detection. We employ corpora of dif-ferent textual genres ?a set of annotated re-ported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life self-expressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detec-tion. 1 Credits This paper has been supported by Ministe-rio de Ciencia e Innovaci?n- Spanish Gov-ernment (grant no. TIN2009-13391-C04-01), and Conselleria d'Educaci?n-Generalitat Valenciana (grant no. PRO-METEO/2009/119 and A-COMP/2010/288).  
2 Introduction The exponential growth of the subjective infor-mation with Web 2.0 created the need to develop new Natural Language Processing (NLP) tools to automatically process and manage the content available on the Internet. Apart from the tradi-tional textual genres, at present we have new ones such as blogs, forums and reviews. The main difference between them is that the latter are predominantly subjective, containing per-sonal judgments. At the moment, NLP tools and methods for analyzing objective information have a better performance than the new ones the research community is creating for managing the subjective content. The survey called ?The State of the Blogosphere 2009?, published by Tech-norati 1 , demonstrates that users are blogging more than ever. Furthermore, in contrast to the general idea about bloggers, each day it is more and more the number of professionals who de-cide to use this means of communication, contra-dicting the common belief about the predomi-nance of an informal editing (Balahur et al, 2009). Due to the growing interest in this text type, the subjective data of the Web is increasing on a daily basis, becoming a reflection of peo-ple?s opinion about a wide range of topics. (Cui, Mittal and Datar, 2006). Blogs represent an im-portant source of real-time, unbiased informa-tion, useful for the development of many applica-tions for concrete purposes. Given the proved importance of automatically processing this data, a new task has appeared in NLP task, dealing with the treatment of subjective data: Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al, 2009), a fine-grained annotation scheme for labeling sub-jectivity in the new textual genres. Subjectivity                                                 1 http://technorati.com/ 
1
can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2  and opinions, generally denomi-nated ?private states? (Uspensky, 1973), not open to verification (Wiebe, 1994). We per-formed a series of experiments focused on dem-onstrating that EmotiBlog represents a step for-ward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, mov-ing a step forward the mere polarity recognition. In particular, the experiments concern expres-sions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification. To that aim, we employ corpora of different textual genres? a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Bala-hur et al, 2010) and the set of news titles anno-tated with polarity and emotion from the SemE-val 2007 Task No. 14 (Strapparava and Mihal-cea, 2007), as well as a corpus of real-life self-expressed emotion entitled ISEAR (Scherer and Walbott, 1999). We subsequently show, through the quality of the results obtained, that Emoti-Blog, through its structure and annotation para-digm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection.  3 Motivation and Contribution The main motivation of this research is the dem-onstrated necessity to work towards the harmoni-zation and interoperability of the increasingly large number of tools and frameworks that sup-port the creation, instantiation, manipulation, querying, and exploitation of annotated resource. This necessity is stressed by the new tools and resources, which have been recently created for processing the subjectivity in the new-textual genres born with the Web 2.0. Such predomi-nantly subjective data is increasing at an expo-nential rate (about 75000 new blogs are reported to be created every day) and contains opinions on the most diverse set of topics. Given its world-wide availability, the subjective data on the Web has become a primary source of information (Balahur et al, 2009). As a consequence, new mechanisms have to be implemented so that this                                                 2 http://dictionary.cambridge.org/ 
data is effectively analyzed and processed. The main challenge of the opinionated content is that, unlike the objective one, which presents facts, the subjective information is most of the times difficult and complex to extract and classify us-ing in grammatically static and fixed rules. Ex-pression of subjectivity is more spontaneous and even if the majority is quite formal, new means of expressivity can be encountered, such as the use of colloquialisms, sayings, collocations or anomalies in the use of punctuation; this is moti-vated by the fact that subjectivity expression is part of our daily life. For example, at the time of taking a decision, people search for information and opinions expressed on the Web on their mat-ter of interest and base their final decision on the information found. At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the perform-ance of that product before purchasing it. There-fore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users. On the other hand, the amount of data to be analyzed requires the development of special-ized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics. (Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Compu-tational Linguistics, which is concerned not with the topic a document is about, but with the opin-ion it expresses. Research in this field has proven the task to be very difficult, due to the high se-mantic variability of affective language. Differ-ent authors have addressed the problem of ex-tracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (?Nokia E65?/?Why do people buy Nokia E65??), type of text (review on forum/blog/dialogue/press arti-cle), and manner of expression of opinion - di-rectly (using opinion statements, e.g. ?I think this product is wonderful!?/?This is a bright initia-tive?), indirectly (using affect vocabulary, e.g. ?I love the pictures this camera takes!?/?Personally, I am shocked one can pro-pose such a law!?) or implicitly (using adjectives and evaluative expressions, e.g. ?It?s light as a feather and fits right into my pocket!?). While determining the overall opinion on a movie is sufficient for taking the decision to watch it or not, when buying a product, people are interested in the individual opinions on the different prod-
2
uct characteristics. When discussing a person, one can judge and give opinion on the person?s actions. Moreover, the approaches taken can vary depending on the manner in which a user asks for the data (general formula such as ?opinions on X? or a specific question ?Why do people like X?? and the text source that needs to be queried). Retrieving opinion information in newspaper articles or blogs posts is more complex, because it involves the detection of different discussion topics, the subjective phrases present and subse-quently their classification according to polarity. Especially in the blog area, determining points of view expressed in dialogues together with the mixture of quotes and pastes from newspapers on a topic can, additionally, involve determining the persons present and whether or not the opinion expressed is on the required topic or on a point previously made by another speaker. This diffi-cult NLP problem requires the use of specialized data for system training and tuning, gathered, annotated and tested within the different text spheres. At the present moment, these specialized resources are scarce and when they exist, they are rather simplistically annotated or highly domain-dependent. Moreover, most of these resources created are for the English. The contribution we describe in this paper intends to propose solutions to the above-mentioned problems, and consists of the following points: first of all, we overcome the problem of corpora scarcity in other languages except English and also improve the English ones; we present the manner in which we compiled a multilingual corpus of blog posts on different topics of interest in three languages-Spanish, Italian and English. The second issue we tried to solve was the coarse-grained annotation schemas employed in other annotation schema. Thus, we describe the new annotation model, EmotiBlog built up in order to capture the different subjectivity/objectivity, emotion/opinion/attitude aspects we are interested in at a finer-grained level. We justify the need for a more detailed annotation model, the sources and the reasons taken into consideration when constructing the corpus and its annotation. Thirdly, we address an aspect strongly related to blogs annotation: due the presence of ?copy and pastes? from news articles or other blogs, the frequent quotes, we include the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research. 
this research. Finally, we conclude on our ap-proach and propose the lines for future work. 4 Related Work In recent years, different researchers have ad-dressed the needs and possible methodologies from the linguistic, theoretical and practical points of view. Thus, the first step involved re-sided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or ?emotion triggers? (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeni-able influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giv-ing a method to annotate a corpus depending on these two aspects ? MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the senti-ment task, as part of Opinion Information Re-trieval  (last three editions of the TREC Blog tracks 3  competitions, the TAC 2008 competi-tion4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al, 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial dis-crimination, but we also add a deeper level of emotion annotation. Since expressions of emo-tion are also highly related to opinions, related work also includes customer review classifica-tion at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations                                                 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 
3
and other attributes with SVM (Mullena and Col-lier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sen-timent classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in ana-lysing sentiment at a sentence level using boot-strapping techniques (Riloff, Wiebe, 2003), con-sidering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then ap-plying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opin-ion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orienta-tion of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, fine-grained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into ac-count modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They are extremely useful and a poll for discussing about any topic with the world. For this reason, the first corpus object of our study is a collection of blog posts extracted from the Web. The texts we selected have distinctive features, extremely different from traditional tex-tual ones. In fact people writing a post can use an informal language colloquialism, emoticons, etc. to express their feelings and it is not rare to find a mix of sources in the same post; people usually mention some facts or discourses and then they give their opinion about them. As we can deduce, 
the source detection represents one of the most complex tasks. As we mentioned above, we car-ried out a multilingual research, collecting texts in three languages: Spanish, Italian, and English about three subjects of interest. The first one contains blog posts commenting upon the signing of the Kyoto Protocol against global warming, the second collection consists of blog entries about the Mugabe government in Zimbabwe, and finally we selected a series of blog posts discuss-ing the issues related to the 2008 USA presiden-tial elections. For each of the abovementioned topics, we have gathered 100 texts, summing up a total of 30.000 words approximately for each language. However in this research we start with English but consider as future work labeling the other languages we have. The second corpus we employed for this research is a collection of 1592 quotes extracted from the news in April 2008. As a consequence they are about many different top-ics and in English (Balahur and Steinberg, 2009). Both of these corpora have been annotated with EmotiBlog that is presented in the next section. 6 EmotiBlog Annotation Model Our annotation schema can be defined as a fine-grained model for labelling subjectivity of the new-textual genres born with the Web 2.0. As mentioned above, it represents a step forward to previous research and it is focused on detecting the linguistic elements, which give subjectivity to the text. The EmotiBlog annotation is divided into different levels (Figure 1).  
 Figure 1: General structure of EmotiBlog.  As we can observe in Figure 1, the first distinc-tion to be made is between objective and subjec-tive speech. If we are labelling an objective sen-tence, we insert the source element, while if we are annotating a subjective discourse, a list of elements with the corresponding attributes have to be added. We select among the list of subjec-tive elements and specify the element?s attrib-
4
utes. Table 1 presents the annotation model in detail.  Elem. Description Obj. speech Confidence, comment, source, target. Subj. speech Confidence, comment, level, emotion, phenomenon, polarity, source and target. Adjectives Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Adverbs Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Verbs Confidence, comment, level, emotion, phenomenon, polarity, mode, source and target. Anaphora Confidence, comment, type, source and target. Capital letter Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Punctuation Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Names Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, and source. Phenomenon Confidence, comment, type: collocation, saying, slang, title, and rhetoric. Reader Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Author Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Emotions Confidence, comment, accept, anger, anticipation, anxiety, appreciation, bad, bewilderment, comfort, ? Table 1: EmotiBlog structure  Each element of the discourse has its own attrib-utes with a series of features, which have to be annotated. Due to space reasons it is impossible to detail each one of them, however we would like to underline the most innovative and rel-evant. For each element we are labelling the an-notator has to insert his level of confidence. In this way we will assign each label a weight that will be computed for future evaluations. More-over, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a com-plete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation pro-cess. In fact emotions of the same subgroup will have less impact when calculating the inter-annotation agreement. In order to make this sub-division proper and effective division, we were inspired by (Scherer, 2005) who created an alter-native dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sen-timents and after that between active and passive. Furthermore emotions are grouped between ob-structive and conductive, and finally between high power and low power control. We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive. Further on, we distributed the sentiments within our list into the Scherer slots creating other smaller categories included in the abovementioned general ones. The result of this division is shown in Table 2: 
Table 2: Alternative dimensional structures of the semantic space for emotions  Following with the description of the model, we said that the first distinction to be made is be-tween objective and subjective speech. Analys-ing the texts we collected, we realised that even if the writer uses an objective speech, sometimes it is just apparently objective and for this reason we added two elements: reader and author inter-pretation. The first one is the impres-sion/feeling/reaction the reader has reading the intervention and what s/he can deduce from the piece of text and the author interpretation is what we can understand from the author (politic orien-tation, preferences). All this information can be deduced form some linguistic elements that ap-parently are not so objective as they may appear. Another innovative element we inserted in the model is the coreference but just at a cross-post level. It is necessary because blogs are composed by posts linked between them and thus cross-
Group Emotions Criticism Sarcasm, irony, incorrect, criticism, objection, opposition, scepticism. Happiness Joy, joke. Support Accept, correct, good, hope, support, trust, rapture, respect, patience, appreciation, excuse. Importance Important, interesting, will, justice, longing, anticipation, revenge. Gratitude Thank. Guilt Guilt, vexation. Fear Fear, fright, troubledness, anxiety. Surprise Surprise, bewilderment, disappoint-ment, consternation. Anger Rage, hatred, enmity, wrath, force, anger, revendication. Envy Envy, rivalry, jealousy. Indifference Unimportant, yield, sluggishness. Pity Compassion, shame, grief. Pain Sadness, lament, remorse, mourning, depression, despondency. Shyness Timidity. Bad Bad, malice, disgust, greed. 
5
document coreference can help the reader to fol-low the conversations. We also label the unusual usage of capital letters and repeated punctuation. In fact, it is very common in blogs to find words written in capital letter or with no conventional usage of punctuation; these features usually mean shouts or a particular mood of the writer. Using EmotiBlog, we annotate the single ele-ments, but we also mark sayings or collocations, representative of each language. A saying is a well-known and wise statement, which often has a meaning, different from the simple meanings of the words it contains5; while a collocation is a word or phrase, which is frequently used with another word or phrase, in a way that sounds cor-rect to native speakers, but might not be expected from the individual words? meanings6. Finally we insert for each element the source and topic. An example of annotation can be:  <phenomenon target="Kyoto Protocol" category="phrase" degree="medium" source="w" polarity="positive" emotion="good">The Onion has a <adjective target="Kyoto Protocol" phenomenon="phrase" de-gree="medium" polarity="positive" emotion="good" source="w" ismodifier="yes">great</adjective> story today titled ?Bush Told to Sign Birthday Treaty for Someone Named Kyoto." </phenomenon> 7 Experiments and Evaluation In order to evaluate the appropriateness of the EmotiBlog annotation scheme and to prove that the fine-grained level it aims at has a positive impact on the performance of the systems em-ploying it as training, we performed several ex-periments. Given that a) EmotiBlog contains an-notations for individual words, as well as for multi-word expressions and at a sentence level, and b) they are labeled with polarity, but also emotion, our experiments show how the anno-tated elements can be used as training for the opinion mining and polarity classification task, as well as for emotion detection. Moreover, tak-ing into consideration the fact that EmotiBlog labels the intensity level of the annotated ele-ments, we performed a brief experiment on de-termining the sentiment intensity, measured on a three-level scale: low, medium and high. In order to perform these three different evaluations, we chose three different corpora. The first one is a collection of quotes (reported speech) from newspaper articles presented in (Balahur et al, 2010), enriched with the manual fine-grained 
                                                5  Definition according to the Cambridge Advanced Learner?s Dictionary 6   Definition according to the Cambridge Advanced Learner?s Dictionary 
annotation of EmotiBlog7; the second one is the collection of newspaper titles in the test set of the SemEval 2007 task number 14 ? Affective Text. Finally, the third one is a corpus of self-reported emotional response ? ISEAR (Scherer and Wal-bott, 1999). The intensity classification task is evaluated only on the second corpus, given that it is the only one in which scores between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles.  6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a ?_? all the tokens pertaining to the NE. All the annotations of punctuation signs that had a specific meaning together were also united un-der a single punctuation sign. Subsequently, we processed the annotated data, using Minipar. We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al, 2005): ? the part of speech (POS)  ? capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) ? opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polar-ity, i.e. 1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) ? syntactic relatedness with other opinion word ? if it is directly dependent of an opin-ion word or modifier (0 or 1), plus the polar-ity/intensity and emotion of this word (0 for all the components otherwise) ?  role in 2-word, 3-word and 4-word annota-tions: opinionatedness, intensity and emo-tion of the other words contained in the an-notation, direct dependency relations with them if they exist and 0 otherwise.  We compute the length of the longest sentence in EmotiBlog. The feature vector for each of the sentences contains the feature vectors of each of its words and 0s for the corresponding feature vectors of the words, which the current sentence has less than the longest annotated sentence. Fi-nally, we add for each sentence as feature binary features for subjectivity and polarity, the value corresponding to the intensity of opinion and the                                                 7 Freely available on request to the authors. 
6
general emotion. These feature vectors are fed into the Weka8 SVM SMO ML algorithm and a model is created (EmotiBlog I). A second model (EmotiBlog II) is created by adding to the collec-tion of single opinion and emotion words anno-tated in EmotiBlog, the Opinion Finder lexicon and the opinion words found in MicroWordNet, the General Inquirer resource and WordNet Af-fect.   6.2 Evaluation of models on test sets  In order to evaluate the performance of the mod-els extracted from the features of the annotations in EmotiBlog, we performed different tests. The first one regarded the evaluation of the polarity and intensity classification task using the Emoit-blog I and II constructed models on two test sets ? the JRC quotes collection and the SemEval 2007 Task Number 14 test set. Since the quotes often contain more than a sentence, we consider the polarity and intensity of the entire quote as the most frequent result in each class, corre-sponding to its constituent sentences. Also, given the fact that the SemEval Affective Text head-lines were given intensity values between -100 and 100, we mapped the values contained in the Gold Standard of the task into three categories: [-100, -67] is high (value 3 in intensity) and nega-tive (value -1 in polarity), [-66, 34] medium negative and [33, 1] is low negative. The values between [1 and 100] are mapped in the same manner to the positive category. 0 was consid-ered objective, so containing the value 0 for in-tensity. The results are presented in Table 3 (the values I and II correspond to the models Emoti-Blog I and EmotiBlog II):   Test  Corpus Evaluation type Precision Recall Polarity 32.13 54.09 JRC quotes I Intensity 36.00 53.2 Polarity 36.4 51.00 JRC quotes II Intensity 38.7 57.81 Polarity 38.57 51.3 SemEval I Intensity 37.39 50.9 Polarity 35.8 58.68 SemEval II Intensity 32.3 50.4 Table 3. Results for polarity and intensity classifi-cation using the models built from the EmotiBlog annotations The results shown in Table 2 show a signifi-cantly high improvement over the results ob-tained in the SemEval task in 2007. This is ex-plainable, on the one hand, by the fact that sys-                                                8 http://www.cs.waikato.ac.nz/ml/weka/ 
tems performing the opinion task did not have at their disposal the lexical resources for opinion employed in the EmotiBlog II model, but also because of the fact that they did not use machine learning on a corpus comparable to EmotiBlog (as seen from the results obtained when using solely the EmotiBlog I corpus). Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in preci-sion, with a recall that is comparable to most of the participating systems. In the second experi-ment, we tested the performance of emotion clas-sification using the two models built using Emo-tiBlog on the three corpora ? JRC quotes, SemE-val 2007 Task No.14 test set and the ISEAR cor-pus. The JRC quotes are labeled using Emoti-Blog; however, the other two are labeled with a small set of emotions ? 6 in the case of the Se-mEval data (joy, surprise, anger, fear, sadness, disgust) and 7 in ISEAR (joy, sadness, anger, fear, guilt, shame, disgust). Moreover, the Se-mEval data contains more than one emotion per title in the Gold Standard, therefore we consider as correct any of the classifications containing one of them. In order to unify the results and ob-tain comparable evaluations, we assessed the performance of the system using the alternative dimensional structures defined in Table 1. The ones not overlapping with the category of any of the 8 different emotions in SemEval and ISEAR are considered as ?Other? and are not included either in the training, nor test set. The results of the evaluation are presented in Table 4. Again, the values I and II correspond to the models EmotiBlog I and II. The ?Emotions? category contains the following emotions: joy, sadness, anger, fear, guilt, shame, disgust, surprise.  Test  corpus Evaluation  type Precision Recall JRC quotes I Emotions   24.7 15.08 
JRC quotes II Emotions  33.65 18.98 
SemEval I Emotions 29.03 18.89 SemEval II Emotions 32.98 18.45 ISEAR I Emotions 22.31 15.01 ISEAR II Emotions 25.62 17.83 Table 4. Results for emotion classification using the models built from the EmotiBlog annotations. The best results for emotion detection were ob-tained for the ?anger? category, where the preci-sion was around 35 percent, for a recall of 19 percent. The worst results obtained were for the ISEAR category of ?shame?, where precision was around 12 percent, with a recall of 15 per-
7
cent. We believe this is due to the fact that the latter emotion is a combination of more complex affective states and it can be easily misclassified to other categories of emotion. Moreover, from the analysis performed on the errors, we realized that many of the affective phenomena presented were more explicit in the case of texts expressing strong emotions such as ?joy? and ?anger?, and were mostly related to common-sense interpreta-tion of the facts presented in the weaker ones. As it can be seen in Table 3, results for the texts per-taining to the news category obtain better results, most of all news titles. This is due to the fact that such texts, although they contain a few words, have a more direct and stronger emotional charge than direct speech (which may be biased by the need to be diplomatic, find the best suited words etc.). Finally, the error analysis showed that emo-tion that is directly reported by the persons expe-riencing is more ?hidden?, in the use of words carrying special signification or related to gen-eral human experience. This fact makes emotion detection in such texts a harder task. Neverthe-less, the results in all corpora are comparable, showing that the approach is robust enough to handle different text types. All in all, the results obtained using the fine and coarse-grained anno-tations in EmotiBlog increased the performance of emotion detection as compared to the systems in the SemEval competition.   6.3 Discussion on the overall results  From the results obtained, we can see that this approach combining the features extracted from the EmotiBlog fine and coarse-grained annota-tions helps to balance between the results ob-tained for precision and recall. The impact of using additional resources that contain opinion words is that of increasing the recall of the sys-tem, at the cost of a slight drop in precision, which proves that the approach is robust enough so that additional knowledge sources can be added. Although the corpus is small, the results obtained show that the phenomena it captures is relevant in the OM task, not only for the blog sphere, but also for other types of text (newspa-per articles, self-reported affect). 8 Conclusions and future work Due to the exponential increase of the subjective information result of the high-level usage of the Internet and the Web 2.0, NLP able to process this data are required. In this paper we presented 
the procedure by which we compiled a multilin-gual corpus of blog posts on different topics of interest in three languages: Spanish, Italian and English. Further on, we explained the need to create a finer-grained annotation schema that can be used to improve the performance of subjectiv-ity mining systems. Thus, we presented the new annotation model, EmotiBlog and justified the benefits of this detailed annotation schema, pre-senting the sources and the reasons taken into consideration when building up the corpus and its labeling. Furthermore, we addressed the pres-ence of ?copy and pastes? from news articles or other blogs, the frequent quotes. For solving this possible ambiguity we included the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We performed several experiments on three dif-ferent corpora, aimed at finding and classifying both the opinion, as well as the expressions of emotion they contained; we showed that the fine and coarse-grained levels of annotation that EmotiBlog contains offers important information on the structure of affective texts, leading to an improvement of the performance of systems trained on it. Although the EmotiBlog corpus is small, the results obtained are promising and show that the phenomena it captures are relevant in the OM task, not only for the blog sphere, but also for other textual-genres. It is well known that OM is an extremely challenging task and a young discipline, thus there is room for im-provement above all to solve linguistic phenom-ena such as the correference resolution at a cross document level, temporal expression recognition. In addition to this, more experiments would need to be done in order to verify the complete ro-bustness of EmotiBlog. Last but not least, our idea is to include the existing tools for a more effective semi-supervised annotation. After the training of the ML system we obtain automati-cally some markables which have to be validated or not by the annotator and the ideal option would be to connect these terms the system de-tects automatically with tools, such as the map-ping with an opinion lexicon based on WordNet (SentiWordNet, WordNet Affect, MicroWord-Net), in order to automatically annotate all the synonyms and antonyms with the same or the opposite polarity respectively and assigning them some other elements contemplated into the Emo-tiBlog annotation schema. This would mean an important step forward for saving time during the annotation process and it will also assure a high quality annotation due to the human supervision. 
8
References Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J. 2010. Sentiment Analysis in the News.  In Proceedings of LREC 2010. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. A Comparative Study of Open Domain and Opinion Question Answering Systems for Fac-tual and Opinionated Queries. In Proceedings of the Recent Advances in Natural Language Proc-essing. Balahur A., Montoyo A. 2008. Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification. In Proceed-ings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scot-land. Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back. In Proceeding of WOMSA 2009. Seville. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. Summarizing Threads in Blogs Using Opinion Polarity. In Proceedings of ETTS work-shop. RANLP. 2009. Boldrini E., Balahur A., Mart?nez-Barco P., Montoyo A. 2009. EmotiBlog: a fine-grained model for emotion detection in non-traditional textual gen-res. In Proceedings of WOMSA. Seville, Spain. Boldrini E., Fern?ndez J., G?mez J.M., Mart?nez-Barco P. 2009. Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres. In Proceedings of WOMSA 2009. Seville, Spain. Chaovalit P, Zhou L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS-05. Carletta J. 1996. Assessing agreement on classification task: the kappa statistic. Computa-tional Linguistics, 22(2): 249?254. Cui H., Mittal V., Datar M. 2006. Comparative Ex-periments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st Na-tional Conference on Artificial Intelligence AAAI. Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007. Language resources and linguistic theory: Typology, second language ac-quisition. English linguistics (Forthcoming), chap-ter Micro-WNOp: A gold standard for the evalua-tion of automatically compiled lexical resources for opinion mining. Franco Angeli Editore, Milano, IT. Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005. Identifying Sources of Opinions with Conditional Random  Fields and Extraction Patterns.  In Pro-ceedings of the HLT/EMNLP.  Dave K., Lawrence S., Pennock, D. ?Mining the Pea-nut Gallery: Opinion Extraction and Semantic Classification of Product Reviews?. In Proceedings of WWW-03. 2003. 
Esuli A., Sebastiani F. 2006. SentiWordNet: A Pub-licly Available Resource for Opinion Mining. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.  Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science. Goldberg A.B., Zhu J. 2006. Seeing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Process-ing. Hu M., Liu B. 2004. Mining Opinion Features in Cus-tomer Reviews. In Proceedings of Nineteenth Na-tional Conference on Artificial Intelligence AAAI. Hatzivassiloglou V., Wiebe J. 2000. Effects of adjec-tive orientation and gradability on sentence subjec-tivity. In Proceedings of COLING.  Kim S.M., Hovy E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING. Mullen T., Collier N. 2006. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proceedings of EMNLP. 2004. Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A. ?Which Side are You On? Identifying Perspectives at the Document and Sentence Levels?. In Proceedings of the Tenth Conference on Natural Language Learn-ing CoNLL.2006.  Ng V., Dasgupta S. and Arifin S. M. 2006. Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Re-views. In the proceedings of the ACL, Sydney. Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Lan-guage Processing. Riloff E., Wiebe J. 2003. Learning Extraction Pat-terns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.  Strapparava C. Valitutti A. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings ofthe 4th International  Conference on Language Resources and Evaluation, LREC. Russell J.A. 1983. Pancultural aspects of the human conceptual organization of emotions. Journal of Personality and Social Psychology 45: 1281?8. Scherer K. R. 2005. What are emotions? And how can they be measured? Social Science Information, 44(4), 693?727. Stoyanov V. and Cardie C. 2006. Toward Opinion Summarization: Linking the Sources. COLING-ACL. Workshop on Sentiment and Subjectivity in Text. Stoyanov V., Cardie C., Litman D., and Wiebe J. 2004. Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and An-
9
swer Corpus. AAAI Spring Symposium on Explor-ing Attitude and Affect in Text.  Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text. In  Proceedings of the ACL.   Turney P. 2002. Thumbs Up or Thumbs Down? Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews. ACL 2002: 417-424. Turney P., Littman M. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems 21. Uspensky B. 1973. A Poetics of Composition. Univer-sity of California Press, Berkeley, California. Wiebe J. M. 1994. Tracking point of view in narra-tive. Computational Linguistics, vol. 20, pp. 233?287. Wiebe J., Wilson T. and Cardie C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation. Wilson T., Wiebe J., Hwa R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI. Wiebe J., Wilson T. and Cardie C. 2005. ?Annotation Expressions of Opinions and Emotions in Lan-guage. Language Resources and Evaluation.  Wiebe J., Riloff E. 2005. Creating Subjective and Objective Sentence Classifiers from Unannotated Texts. In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing).      
10
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,
Uppsala, July 2010.
A Survey on the Role of Negation in Sentiment Analysis
Michael Wiegand
Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Alexandra Balahur
University of Alicante
Alicante, Spain
abalahur@dlsi.ua.es
Benjamin Roth and Dietrich Klakow
Saarland University
Saarbru?cken, Germany
benjamin.roth@lsv.uni-saarland.de
dietrich.klakow@lsv.uni-saarland.de
Andre?s Montoyo
University of Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
This paper presents a survey on the role of
negation in sentiment analysis. Negation
is a very common linguistic construction
that affects polarity and, therefore, needs
to be taken into consideration in sentiment
analysis.
We will present various computational ap-
proaches modeling negation in sentiment
analysis. We will, in particular, focus
on aspects, such as level of representation
used for sentiment analysis, negation word
detection and scope of negation. We will
also discuss limits and challenges of nega-
tion modeling on that task.
1 Introduction
Sentiment analysis is the task dealing with the
automatic detection and classification of opinions
expressed in text written in natural language.
Subjectivity is defined as the linguistic expression
of somebody?s opinions, sentiments, emotions,
evaluations, beliefs and speculations (Wiebe,
1994). Subjectivity is opposed to objectivity,
which is the expression of facts. It is important to
make the distinction between subjectivity detec-
tion and sentiment analysis, as they are two sep-
arate tasks in natural language processing. Sen-
timent analysis can be dependently or indepen-
dently done from subjectivity detection, although
Pang and Lee (2004) state that subjectivity de-
tection performed prior to the sentiment analysis
leads to better results in the latter.
Although research in this area has started only re-
cently, the substantial growth in subjective infor-
mation on the world wide web in the past years
has made sentiment analysis a task on which con-
stantly growing efforts have been concentrated.
The body of research published on sentiment anal-
ysis has shown that the task is difficult, not only
due to the syntactic and semantic variability of
language, but also because it involves the extrac-
tion of indirect or implicit assessments of objects,
by means of emotions or attitudes. Being a part
of subjective language, the expression of opinions
involves the use of nuances and intricate surface
realizations. That is why the automatic study of
opinions requires fine-grained linguistic analysis
techniques and substantial efforts to extract fea-
tures for machine learning or rule-based systems,
in which subtle phenomena as negation can be ap-
propriately incorporated.
Sentiment analysis is considered as a subsequent
task to subjectivity detection, which should ideally
be performed to extract content that is not factual
in nature. Subsequently, sentiment analysis aims
at classifying the sentiment of the opinions into
polarity types (the common types are positive and
negative). This text classification task is also re-
ferred to as polarity classification.
This paper presents a survey on the role of nega-
tion in sentiment analysis. Negation is a very com-
mon linguistic construction that affects polarity
and, therefore, needs to be taken into considera-
tion in sentiment analysis. Before we describe the
computational approaches that have been devised
to account for this phenomenon in sentiment anal-
ysis, we will motivate the problem.
2 Motivation
Since subjectivity and sentiment are related to ex-
pressions of personal attitudes, the way in which
this is realized at the surface level influences the
manner in which an opinion is extracted and its
polarity is computed. As we have seen, sentiment
analysis goes a step beyond subjectivity detection,
60
including polarity classification. So, in this task,
correctly determining the valence of a text span
(whether it conveys a positive or negative opinion)
is equivalent to the success or failure of the auto-
matic processing.
It is easy to see that Sentence 1 expresses a posi-
tive opinion.
1. I like+ this new Nokia model.
The polarity is conveyed by like which is a polar
expression. Polar expressions, such as like or hor-
rible, are words containing a prior polarity. The
negation of Sentence 1, i.e. Sentence 2, using the
negation word not, expresses a negative opinion.
2. I do [not like+]? this new Nokia model.
In this example, it is straightforward to notice the
impact of negation on the polarity of the opinion
expressed. However, it is not always that easy
to spot positive and negative opinions in text. A
negation word can also be used in other expres-
sions without constituting a negation of the propo-
sition expressed as exemplified in Sentence 3.
3. Not only is this phone expensive but it is also heavy and
difficult to use.
In this context, not does not invert the polarity of
the opinion expressed which remains negative.
Moreover, the presence of an actual negation word
in a sentence does not mean that all its polar opin-
ions are inverted. In Sentence 4, for example, the
negation does not modify the second polar expres-
sion intriguing since the negation and intriguing
are in separate clauses.
4. [I do [not like+]? the design of new Nokia model] but
[it contains some intriguing+ new functions].
Therefore, when treating negation, one must be
able to correctly determine the scope that it has
(i.e. determine what part of the meaning expressed
is modified by the presence of the negation).
Finally, the surface realization of a negation is
highly variable, depending on various factors,
such as the impact the author wants to make on
the general text meaning, the context, the textual
genre etc. Most of the times, its expression is far
from being simple (as in the first two examples),
and does not only contain obvious negation words,
such as not, neither or nor. Research in the field
has shown that there are many other words that in-
vert the polarity of an opinion expressed, such as
diminishers/valence shifters (Sentence 5), connec-
tives (Sentence 6), or even modals (Sentence 7).
5. I find the functionality of the new phone less practical.
6. Perhaps it is a great phone, but I fail to see why.
7. In theory, the phone should have worked even under
water.
As can be seen from these examples, modeling
negation is a difficult yet important aspect of sen-
timent analysis.
3 The Survey
In this survey, we focus on work that has presented
novel aspects for negation modeling in sentiment
analysis and we describe them chronologically.
3.1 Negation and Bag of Words in Supervised
Machine Learning
Several research efforts in polarity classification
employ supervised machine-learning algorithms,
like Support Vector Machines, Na??ve Bayes Clas-
sifiers or Maximum Entropy Classifiers. For these
algorithms, already a low-level representation us-
ing bag of words is fairly effective (Pang et al,
2002). Using a bag-of-words representation, the
supervised classifier has to figure out by itself
which words in the dataset, or more precisely fea-
ture set, are polar and which are not. One either
considers all words occurring in a dataset or, as
in the case of Pang et al (2002), one carries out
a simple feature selection, such as removing infre-
quent words. Thus, the standard bag-of-words rep-
resentation does not contain any explicit knowl-
edge of polar expressions. As a consequence of
this simple level of representation, the reversal
of the polarity type of polar expressions as it is
caused by a negation cannot be explicitly modeled.
The usual way to incorporate negation modeling
into this representation is to add artificial words:
i.e. if a word x is preceded by a negation word,
then rather than considering this as an occurrence
of the feature x, a new feature NOT x is created.
The scope of negation cannot be properly modeled
with this representation either. Pang et al (2002),
for example, consider every word until the next
punctuation mark. Sentence 2 would, therefore,
result in the following representation:
8. I do not NOT like NOT this NOT new NOT Nokia
NOT model.
The advantage of this feature design is that a plain
occurrence and a negated occurrence of a word are
61
reflected by two separate features. The disadvan-
tage, however, is that these two contexts treat the
same word as two completely different entities.
Since the words to be considered are unrestricted,
any word ? no matter whether it is an actual po-
lar expression or not ? is subjected to this nega-
tion modification. This is not only linguistically
inaccurate but also increases the feature space with
more sparse features (since the majority of words
will only be negated once or twice in a corpus).
Considering these shortcomings, it comes to no
surprise that the impact of negation modeling on
this level of representation is limited. Pang et al
(2002) report only a negligible improvement by
adding the artificial features compared to plain bag
of words in which negation is not considered.
Despite the lack of linguistic plausibility, super-
vised polarity classifiers using bag of words (in
particular, if training and testing are done on the
same domain) offer fairly good performance. This
is, in particular, the case on coarse-grained clas-
sification, such as on document level. The suc-
cess of these methods can be explained by the
fact that larger texts contain redundant informa-
tion, e.g. it does not matter whether a classifier
cannot model a negation if the text to be classi-
fied contains twenty polar opinions and only one
or two contain a negation. Another advantage
of these machine learning approaches on coarse-
grained classification is their usage of higher order
n-grams. Imagine a labeled training set of docu-
ments contains frequent bigrams, such as not ap-
pealing or less entertaining. Then a feature set us-
ing higher order n-grams implicitly contains nega-
tion modeling. This also partially explains the ef-
fectiveness of bigrams and trigrams for this task as
stated in (Ng et al, 2006).
The dataset used for the experiments in (Pang et
al., 2002; Ng et al, 2006) has been established as
a popular benchmark dataset for sentiment analy-
sis and is publicly available1.
3.2 Incorporating Negation in Models that
Include Knowledge of Polar Expressions
- Early Works
The previous subsection suggested that appropri-
ate negation modeling for sentiment analysis re-
quires the awareness of polar expressions. One
way of obtaining such expressions is by using a
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
polarity lexicon which contains a list of polar ex-
pressions and for each expression the correspond-
ing polarity type. A simple rule-based polarity
classifier derived from this knowledge typically
counts the number of positive and negative polar
expressions in a text and assigns it the polarity
type with the majority of polar expressions. The
counts of polar expressions can also be used as
features in a supervised classifier. Negation is typ-
ically incorporated in those features, e.g. by con-
sidering negated polar expressions as unnegated
polar expressions with the opposite polarity type.
3.2.1 Contextual Valence Shifters
The first computational model that accounts for
negation in a model that includes knowledge of
polar expressions is (Polanyi and Zaenen, 2004).
The different types of negations are modeled via
contextual valence shifting. The model assigns
scores to polar expressions, i.e. positive scores to
positive polar expressions and negative scores to
negative polar expressions, respectively. If a polar
expression is negated, its polarity score is simply
inverted (see Example 1).
clever (+2) ? not clever (?2) (1)
In a similar fashion, diminishers are taken into
consideration. The difference is, however, that
the score is only reduced rather than shifted to the
other polarity type (see Example 2).
efficient (+2)? rather efficient (+1) (2)
Beyond that the model also accounts for modals,
presuppositional items and even discourse-based
valence shifting. Unfortunately, this model is
not implemented and, therefore, one can only
speculate about its real effectiveness.
Kennedy and Inkpen (2005) evaluate a nega-
tion model which is fairly identical to the one pro-
posed by Polanyi and Zaenen (2004) (as far as
simple negation words and diminishers are con-
cerned) in document-level polarity classification.
A simple scope for negation is chosen. A polar
expression is thought to be negated if the negation
word immediately precedes it. In an extension of
this work (Kennedy and Inkpen, 2006) a parser is
considered for scope computation. Unfortunately,
no precise description of how the parse is used
for scope modeling is given in that work. Neither
is there a comparison of these two scope models
measuring their respective impacts.
62
Final results show that modeling negation is im-
portant and relevant, even in the case of such sim-
ple methods. The consideration of negation words
is more important than that of diminishers.
3.2.2 Features for Negation Modeling
Wilson et al (2005) carry out more advanced
negation modeling on expression-level polarity
classification. The work uses supervised machine
learning where negation modeling is mostly en-
coded as features using polar expressions. The
features for negation modeling are organized in
three groups:
? negation features
? shifter features
? polarity modification features
Negation features directly relate to negation ex-
pressions negating a polar expression. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar
expression. The other feature accounts for a polar
predicate having a negated subject. This frequent
long-range relationship is illustrated in Sentence 9.
9. [No politically prudent Israeli]
subject
could
support
polar pred
either of them.
All negation expressions are additionally disam-
biguated as some negation words do not function
as a negation word in certain contexts, e.g. not to
mention or not just.
Shifter features are binary features checking the
presence of different types of polarity shifters. Po-
larity shifters, such as little, are weaker than ordi-
nary negation expressions. They can be grouped
into three categories, general polarity shifters,
positive polarity shifters, and negative polarity
shifters. General polarity shifters reverse polarity
like negations. The latter two types only reverse
a particular polarity type, e.g. the positive shifter
abate only modifies negative polar expressions as
in abate the damage. Thus, the presence of a pos-
itive shifter may indicate positive polarity. The set
of words that are denoted by these three features
can be approximately equated with diminishers.
Finally, polarity modification features describe
polar expressions of a particular type modify-
ing or being modified by other polar expressions.
Though these features do not explicitly contain
negations, language constructions which are sim-
ilar to negation may be captured. In the phrase
[disappointed? hope+]?, for instance, a negative
polar expression modifies a positive polar expres-
sion which results in an overall negative phrase.
Adding these three feature groups to a feature
set comprising bag of words and features count-
ing polar expressions results in a significant im-
provement. In (Wilson et al, 2009), the experi-
ments of Wilson et al (2005) are extended by a
detailed analysis on the individual effectiveness of
the three feature groups mentioned above. The re-
sults averaged over four different supervised learn-
ing algorithms suggest that the actual negation fea-
tures are most effective whereas the binary polar-
ity shifters have the smallest impact. This is con-
sistent with Kennedy and Inkpen (2005) given the
similarity of polarity shifters and diminishers.
Considering the amount of improvement that is
achieved by negation modeling, the improvement
seems to be larger in (Wilson et al, 2005). There
might be two explanations for this. Firstly, the
negation modeling in (Wilson et al, 2005) is con-
siderably more complex and, secondly, Wilson et
al. (2005) evaluate on a more fine-grained level
(i.e. expression level) than Kennedy and Inkpen
(2005) (they evaluate on document level). As al-
ready pointed out in ?3.1, document-level polar-
ity classification contains more redundant infor-
mation than sentence-level or expression-level po-
larity classification, therefore complex negation
modeling on these levels might be more effective
since the correct contextual interpretation of an in-
dividual polar expression is far more important2.
The fine-grained opinion corpus used in (Wilson
et al, 2005; Wilson et al, 2009) and all the re-
sources necessary to replicate the features used in
these experiments are also publicly available3.
3.3 Other Approaches
The approaches presented in the previous sec-
tion (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2005; Wilson et al, 2005) can be consid-
ered as the works pioneering negation modeling
in sentiment analysis. We now present some more
recent work on that topic. All these approaches,
however, are heavily related to these early works.
2This should also explain why most subsequent works
(see ?3.3) have been evaluated on fine-grained levels.
3The corpus is available under:
http://www.cs.pitt.edu/mpqa/
databaserelease and the resources
for the features are part of OpinionFinder:
http://www.cs.pitt.edu/mpqa/
opinionfinderrelease
63
3.3.1 Semantic Composition
In (Moilanen and Pulman, 2007), a method to
compute the polarity of headlines and complex
noun phrases using compositional semantics is
presented. The paper argues that the principles of
this linguistic modeling paradigm can be success-
fully applied to determine the subsentential polar-
ity of the sentiment expressed, demonstrating it
through its application to contexts involving senti-
ment propagation, polarity reversal (e.g. through
the use of negation following Polanyi and Zae-
nen (2004) and Kennedy and Inkpen (2005)) or
polarity conflict resolution. The goal is achieved
through the use of syntactic representations of sen-
tences, on which rules for composition are de-
fined, accounting for negation (incrementally ap-
plied to constituents depending on the scope) us-
ing negation words, shifters and negative polar ex-
pressions. The latter are subdivided into differ-
ent categories, such that special words are defined,
whose negative intensity is strong enough that they
have the power to change the polarity of the entire
text spans or constituents they are part of.
A similar approach is presented by Shaikh et al
(2007). The main difference to Moilanen and
Pulman (2007) lies in the representation format
on which the compositional model is applied.
While Moilanen and Pulman (2007) use syntac-
tic phrase structure trees, Shaikh et al (2007) con-
sider a more abstract level of representation be-
ing verb frames. The advantage of a more abstract
level of representation is that it more accurately
represents the meaning of the text it describes.
Apart from that, Shaikh et al (2007) design a
model for sentence-level classification rather than
for headlines or complex noun phrases.
The approach by Moilanen and Pulman (2007) is
not compared against another established classifi-
cation method whereas the approach by Shaikh et
al. (2007) is evaluated against a non-compositional
rule-based system which it outperforms.
3.3.2 Shallow Semantic Composition
Choi and Cardie (2008) present a more lightweight
approach using compositional semantics towards
classifying the polarity of expressions. Their
working assumption is that the polarity of a phrase
can be computed in two steps:
? the assessment of polarity of the constituents
? the subsequent application of a set of previously-
defined inference rules
An example rule, such as:
Polarity([NP1]? [IN] [NP2]?) = + (3)
may be applied to expressions, such as
[lack]?NP1 [of]IN [crime]?NP2 in rural areas.
The advantage of these rules is that they restrict
the scope of negation to specific constituents
rather than using the scope of the entire target
expression.
Such inference rules are very reminiscent of
polarity modification features (Wilson et al,
2005), as a negative polar expression is modified
by positive polar expression. The rules presented
by Choi and Cardie (2008) are, however, much
more specific, as they define syntactic contexts of
the polar expressions. Moreover, from each con-
text a direct polarity for the entire expression can
be derived. In (Wilson et al, 2005), this decision
is left to the classifier. The rules are also similar
to the syntactic rules from Moilanen and Pulman
(2007). However, they involve less linguistic
processing and are easier to comprehend4 . The
effectiveness of these rules are both evaluated in
rule-based methods and a machine learning based
method where they are anchored as constraints
in the objective function. The results of their
evaluation show that the compositional methods
outperform methods using simpler scopes for
negation, such as considering the scope of the
entire target expression. The learning method
incorporating the rules also slightly outperforms
the (plain) rule-based method.
3.3.3 Scope Modeling
In sentiment analysis, the most prominent work
examining the impact of different scope models
for negation is (Jia et al, 2009). The scope de-
tection method that is proposed considers:
? static delimiters
? dynamic delimiters
? heuristic rules focused on polar expressions
Static delimiters are unambiguous words, such as
because or unless marking the beginning of an-
other clause. Dynamic delimiters are, however,
4It is probably due to the latter, that these rules have
been successfully re-used in subsequent works, most promi-
nently Klenner et al (2009).
64
ambiguous, e.g. like and for, and require disam-
biguation rules, using contextual information such
as their pertaining part-of-speech tag. These de-
limiters suitably account for various complex sen-
tence types so that only the clause containing the
negation is considered.
The heuristic rules focus on cases in which po-
lar expressions in specific syntactic configurations
are directly preceded by negation words which re-
sults in the polar expression becoming a delimiter
itself. Unlike Choi and Cardie (2008), these rules
require a proper parse and reflect grammatical re-
lationships between different constituents.
The complexity of the scope model proposed
by Jia et al (2009) is similar to the ones of
the compositional models (Moilanen and Pulman,
2007; Shaikh et al, 2007; Choi and Cardie, 2008)
where scope modeling is exclusively incorporated
in the compositional rules.
Apart from scope modeling, Jia et al (2009) also
employ a complex negation term disambiguation
considering not only phrases in which potential
negation expressions do not have an actual negat-
ing function (as already used in (Wilson et al,
2005)), but also negative rhetorical questions and
restricted comparative sentences.
On sentence-level polarity classification, their
scope model is compared with
? a simple negation scope using a fixed window size
(similar to the negation feature in (Wilson et al, 2005))
? the text span until the first occurrence of a polar expres-
sion following the negation word
? the entire sentence
The proposed method consistently outperforms
the simpler methods proving that the incorpora-
tion of linguistic insights into negation modeling
is meaningful. Even on polarity document re-
trieval, i.e. a more coarse-grained classification
task where contextual disambiguation usually
results in a less significant improvement, the
proposed method also outperforms the other
scopes examined.
There have only been few research efforts in
sentiment analysis examining the impact of scope
modeling for negation in contrast to other research
areas, such as the biomedical domain (Huang and
Lowe, 2007; Morante et al, 2008; Morante and
Daelemans, 2009). This is presumably due to the
fact that only for the biomedical domain, publicly
available corpora containing annotation for the
scope of negation exist (Szarvas et al, 2008). The
usability of those corpora for sentiment analysis
has not been tested.
3.4 Negation within Words
So far, negation has only be considered as a phe-
nomenon that affects entire words or phrases.
The word expressing a negation and the words
or phrases being negated are disjoint. There are,
however, cases in which both negation and the
negated content which can also be opinionated
are part of the same word. In case, these words
are lexicalized, such as flaw-less, and are conse-
quently to be found a polarity lexicon, this phe-
nomenon does not need to be accounted for in sen-
timent analysis. However, since this process is (at
least theoretically) productive, fairly uncommon
words, such as not-so-nice, anti-war or offensive-
less which are not necessarily contained in lexical
resources, may emerge as a result of this process.
Therefore, a polarity classifier should also be able
to decompose words and carry out negation mod-
eling within words.
There are only few works addressing this particu-
lar aspect (Moilanen and Pulman, 2008; Ku et al,
2009) so it is not clear how much impact this type
of negation has on an overall polarity classification
and what complexity of morphological analysis is
really necessary. We argue, however, that in syn-
thetic languages where negation may regularly be
realized as an affix rather than an individual word,
such an analysis is much more important.
3.5 Negation in Various Languages
Current research in sentiment analysis mainly fo-
cuses on English texts. Since there are signifi-
cant structural differences among the different lan-
guages, some particular methods may only cap-
ture the idiosyncratic properties of the English lan-
guage. This may also affect negation modeling.
The previous section already stated that the need
for morphological analyses may differ across the
different languages.
Moreover, the complexity of scope modeling may
also be language dependent. In English, for ex-
ample, modeling the scope of a negation as a
fixed window size of words following the oc-
currence of a negation expression already yields
a reasonable performance (Kennedy and Inkpen,
2005). However, in other languages, for example
German, more complex processing is required as
the negated expression may either precede (Sen-
65
tence 10) or follow (Sentence 11) the negation ex-
pression. Syntactic properties of the negated noun
phrase (i.e. the fact whether the negated polar ex-
pression is a verb or an adjective) determine the
particular negation construction.
10. Peter mag den Kuchen nicht.
Peter likes the cake not.
?Peter does not like the cake.?
11. Der Kuchen ist nicht ko?stlich.
The cake is not delicious.
?The cake is not delicious.?
These items show that, clearly, some more ex-
tensive cross-lingual examination is required in or-
der to be able to make statements of the general
applicability of specific negation models.
3.6 Bad and Not Good are Not the Same
The standard approach of negation modeling sug-
gests to consider a negated polar expression, such
as not bad, as an unnegated polar expression with
the opposite polarity, such as good. Liu and Seneff
(2009) claim, however, that this is an oversimpli-
fication of language. Not bad and good may have
the same polarity but they differ in their respec-
tive polar strength, i.e. not bad is less positive
than good. That is why, Liu and Seneff (2009)
suggest a compositional model in which for indi-
vidual adjectives and adverbs (the latter include
negations) a prior rating score encoding their in-
tensity and polarity is estimated from pros and
cons of on-line reviews. Moreover, compositional
rules for polar phrases, such as adverb-adjective or
negation-adverb-adjective are defined exclusively
using the scores of the individual words. Thus,
adverbs function like universal quantifiers scaling
either up or down the polar strength of the specific
polar adjectives they modify. The model indepen-
dently learns what negations are, i.e. a subset of
adverbs having stronger negative scores than other
adverbs. In short, the proposed model provides
a unifying account for intensifiers (e.g. very), di-
minishers, polarity shifters and negation words. Its
advantage is that polarity is treated composition-
ally and is interpreted as a continuum rather than
a binary classification. This approach reflects its
meaning in a more suitable manner.
3.7 Using Negations in Lexicon Induction
Many classification approaches illustrated above
depend on the knowledge of which natural lan-
guage expressions are polar. The process of ac-
quiring such lexical resources is called lexicon in-
duction. The observation that negations co-occur
with polar expressions has been used for inducing
polarity lexicons on Chinese in an unsupervised
manner (Zagibalov and Carroll, 2008). One ad-
vantage of negation is that though the induction
starts with just positive polar seeds, the method
also accomplishes to extract negative polar expres-
sions since negated mentions of the positive po-
lar seeds co-occur with negative polar expressions.
Moreover, and more importantly, the distribution
of the co-occurrence between polar expressions
and negations can be exploited for the selection of
those seed lexical items. The model presented by
Zagibalov and Carroll (2008) relies on the obser-
vation that a polar expression can be negated but it
occurs more frequently without the negation. The
distributional behaviour of an expression, i.e. sig-
nificantly often co-occurring with a negation word
but significantly more often occurring without a
negation word makes up a property of a polar ex-
pression. The data used for these experiments are
publicly available5 .
3.8 Irony ? The Big Challenge
Irony is a rhetorical process of intentionally using
words or expressions for uttering meaning that is
different from the one they have when used liter-
ally (Carvalho et al, 2009). Thus, we consider
that the use of irony can reflect an implicit nega-
tion of what is conveyed through the literal use of
the words. Moreover, due to its nature irony is
mostly used to express a polar opinion.
Carvalho et al (2009) confirm the relevance of
(verbal) irony for sentiment analysis by an error
analysis of their present classifier stating that a
large proportion of misclassifications derive from
their system?s inability to account for irony.
They present predictive features for detecting
irony in positive sentences (which are actually
meant to have a negative meaning). Their find-
ings are that the use of emoticons or expressions
of gestures and the use of quotation marks within
a context in which no reported speech is included
are a good signal of irony in written text. Although
the use of these clues in the defined patterns helps
to detect some situations in which irony is present,
they do not fully represent the phenomenon.
5http://www.informatics.sussex.ac.uk/
users/tz21/coling08.zip
66
A data-driven approach for irony detection on
product-reviews is presented in (Tsur et al, 2010).
In the first stage, a considerably large list of simple
surface patterns of ironic expressions are induced
from a small set of labeled seed sentences. A pat-
tern is a generalized word sequence in which con-
tent words are replaced by a generic CW symbol.
In the second stage, the seed sentences are used to
collect more examples from the web, relying on
the assumption that sentences next to ironic ones
are also ironic. In addition to these patterns, some
punctuation-based features are derived from the
labeled sentences. The acquired patterns are used
as features along the punctuation-based features
within a k nearest neighbour classifier. On an in-
domain test set the classifier achieves a reasonable
performance. Unfortunately, these experiments
only elicit few additional insights into the general
nature of irony. As there is no cross-domain eval-
uation of the system, it is unclear in how far this
approach generalizes to other domains.
4 Limits of Negation Modeling in
Sentiment Analysis
So far, this paper has not only outlined the impor-
tance of negation modeling in sentiment analysis
but it has also shown different ways to account for
this linguistic phenomenon. In this section, we
present the limits of negation modeling in senti-
ment analysis.
Earlier in this paper, we stated that negation mod-
eling depends on the knowledge of polar expres-
sions. However, the recognition of genuine polar
expressions is still fairly brittle. Many polar ex-
pressions, such as disease are ambiguous, i.e. they
have a polar meaning in one context (Sentence 12)
but do not have one in another (Sentence 13).
12. He is a disease to every team he has gone to.
13. Early symptoms of the disease are headaches, fevers,
cold chills and body pain.
In a pilot study (Akkaya et al, 2009), it has al-
ready been shown that applying subjectivity word
sense disambiguation in addition to the feature-
based negation modeling approach of Wilson et al
(2005) results in an improvement of performance
in polarity classification.
Another problem is that some polar opinions are
not lexicalized. Sentence 14 is a negative prag-
matic opinion (Somasundaran and Wiebe, 2009)
which can only be detected with the help of exter-
nal world knowledge.
14. The next time I hear this song on the radio, I?ll throw
my radio out of the window.
Moreover, the effectiveness of specific negation
models can only be proven with the help of cor-
pora containing those constructions or the type of
language behaviour that is reflected in the mod-
els to be evaluated. This presumably explains why
rare constructions, such as negations using con-
nectives (Sentence 6 in ?2), modals (Sentence 7
in ?2) or other phenomena presented in the con-
ceptual model of Polanyi and Zaenen (2004), have
not yet been dealt with.
5 Conclusion
In this paper, we have presented a survey on
the role of negation in sentiment analysis. The
plethora of work presented on the topic proves that
this common linguistic construction is highly rel-
evant for sentiment analysis.
An effective negation model for sentiment analy-
sis usually requires the knowledge of polar expres-
sions. Negation is not only conveyed by common
negation words but also other lexical units, such as
diminishers. Negation expressions are ambiguous,
i.e. in some contexts do not function as a nega-
tion and, therefore, need to be disambiguated. A
negation does not negate every word in a sentence,
therefore, using syntactic knowledge to model the
scope of negation expressions is useful.
Despite the existence of several approaches to
negation modeling for sentiment analysis, in or-
der to make general statements about the effective-
ness of specific methods systematic comparative
analyses examining the impact of different nega-
tion models (varying in complexity) with regard to
classification type, text granularity, target domain,
language etc. still need to be carried out.
Finally, negation modeling is only one aspect that
needs to be taken into consideration in sentiment
analysis. In order to fully master this task, other
aspects, such as a more reliable identification of
genuine polar expressions in specific contexts, are
at least as important as negation modeling.
Acknowledgements
Michael Wiegand was funded by the BMBF project NL-
Search under contract number 01IS08020B. Alexandra Bal-
ahur was funded by Ministerio de Ciencia e Innovacio?n -
Spanish Government (grant no. TIN2009-13391-C04-01),
and Conselleria d?Educacio?n-Generalitat Valenciana (grant
no. PROMETEO/2009/119 and ACOMP/2010/286).
67
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjec-
tivity Word Sense Disambiguation. In Proceedings
of EMNLP.
P. Carvalho, L. Sarmento, M. J. Silva, and
E. de Oliveira. 2009. Clues for Detecting
Irony in User-Generated Contents: Oh...!! It?s ?so
easy? ;-). In Proceedings of CIKM-Workshop TSA.
Y. Choi and C. Cardie. 2008. Learning with Compo-
sitional Semantics as Structural Inference for Sub-
sentential Sentiment Analysis. In Proceedings of
EMNLP.
Y. Huang and H. J. Lowe. 2007. A Novel Hybrid Ap-
proach to Automated Negation Detection in Clinical
Radiology Reports. JAMIA, 14.
L. Jia, C. Yu, and W. Meng. 2009. The Effect of Nega-
tion on Sentiment Analysis and Retrieval Effective-
ness. In Proceedings of CIKM.
A. Kennedy and D. Inkpen. 2005. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. In Proceedings of FINEXIN.
A. Kennedy and D. Inkpen. 2006. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. Computational Intelligence, 22.
M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust
Compositional Polarity Classification. In Proceed-
ings of RANLP.
L. Ku, T. Huang, and H. Chen. 2009. Using Morpho-
logical and Syntactic Structures for Chinese Opinion
Analysis. In Proceedings ACL/IJCNLP.
J. Liu and S. Seneff. 2009. Review Sentiment Scoring
via a Parse-and-Paraphrase Paradigm. In Proceed-
ings of EMNLP.
K. Moilanen and S. Pulman. 2007. Sentiment Con-
struction. In Proceedings of RANLP.
K. Moilanen and S. Pulman. 2008. The Good, the Bad,
and the Unknown. In Proceedings of ACL/HLT.
R. Morante and W. Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the Scope of Negation in Biomedical
Texts. In Proceedings of EMNLP.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of COLING/ACL.
B. Pang and L. Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Context Valence
Shifters. In Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
M. A. M. Shaikh, H. Prendinger, and M. Ishizuka.
2007. Assessing Sentiment of Text by Semantic De-
pendency and Contextual Valence Analysis. In Pro-
ceedings of ACII.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing Stances in Online Debates. In Proceedings of
ACL/IJCNLP.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope Corpus: Annotation for Negation,
Uncertainty and Their Scope in Biomedical Texts.
In Proceedings of BioNLP.
O. Tsur, D. Davidov, and A. Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceeding of ICWSM.
J. Wiebe. 1994. Tracking Point of View in Narrative.
Computational Linguistics, 20.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of HLT/EMNLP.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing Contextual Polarity: An Exploration for
Phrase-level Analysis. Computational Linguistics,
35:3.
T. Zagibalov and J. Carroll. 2008. Automatic Seed
Word Selection for Unsupervised Sentiment Classi-
fication of Chinese Text. In Proceedings of COL-
ING.
68
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 53?60,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Detecting Implicit Expressions of Sentiment in Text  
Based on Commonsense Knowledge 
 
 
Alexandra Balahur, Jes?s M. Hermida, Andr?s Montoyo 
Department of Software and Computing Systems  
University of Alicante 
Apartado de correos 99, E-03080 Alicante, Spain 
{abalahur, jhermida, montoyo}@dlsi.ua.es 
 
 
 
 
 
 
Abstract 
Sentiment analysis is one of the recent, 
highly dynamic fields in Natural Language 
Processing. Most existing approaches are 
based on word-level analysis of texts and 
are able to detect only explicit expressions 
of sentiment.  In this paper, we present an 
approach towards automatically detecting 
emotions (as underlying components of 
sentiment) from contexts in which no clues 
of sentiment appear, based on 
commonsense knowledge. The resource we 
built towards this aim ? EmotiNet - is a 
knowledge base of concepts with 
associated affective value. Preliminary 
evaluations show that this approach is 
appropriate for the task of implicit emotion 
detection, thus improving the performance 
of sentiment detection and classification in 
text. 
1 Introduction 
Research in affect has a long established tradition 
in many sciences - linguistics, psychology, socio-
psychology, cognitive science, pragmatics, 
marketing or communication science. Recently, 
many closely related subtasks were developed also 
in the field of Natural Language Proceesing (NLP), 
such as emotion detection, subjectivity analysis, 
opinion mining to sentiment analysis, attitude and 
appraisal analysis or review mining (Pang and Lee, 
2008). 
Among these tasks, sentiment analysis aims at 
detecting the expressions of sentiment in text and 
subsequently classify them, according to their 
polarity (semantic orientation) among different 
categories (usually, among positive and negative). 
The problem is defined by Pang and Lee (2008) as 
?the binary classification task of labeling an 
opinionated document as expressing either an 
overall positive or an overall negative.? (Pang and 
Lee, 2008) 
According to the Webster dictionary 
(http://www.merriam-webster.com/), sentiment suggests 
a settled opinion reflective of one?s feelings, where 
the term feeling is defined as the conscious 
subjective experience of emotion. (Van den Bos, 
2006), ?a single component of emotion, denoting 
the subjective experience process? (Scherer, 2005).  
Most of the research performed in the field of 
sentiment analysis has aimed at detecting explicit 
expressions of sentiment (i.e. situations where 
specific words or word combinations are found in 
texts). Nevertheless, the expression of emotion is 
most of the times not achieved through the use of 
emotion-bearing words (Pennebaker et al, 2003), 
but indirectly, by presenting situations that based 
on commonsense knowledge can be interpreted in 
an affective manner (Balahur and Montoyo, 2008; 
Balahur and Steinberger, 2009).  
In this paper, we present a method to build a 
commonsense knowledge base (EmotiNet) 
representing situations that trigger emotions. We 
demonstrate that by using this resource, we are 
53
able to detect emotion from textual contexts in 
which no explicit mention of affect is present.   
2 State of the Art 
In Artificial Intelligence (AI), the term affective 
computing was first introduced by Picard (1995). 
Previous approaches to spot affect in text include 
the use of models simulating human reactions 
according to their needs and desires (Dyer, 1987), 
fuzzy logic (Subasic and Huettner, 2000), lexical 
affinity based on similarity of contexts ? the basis 
for the construction of WordNet Affect  
(Strapparava and Valitutti, 2004) or SentiWord-
Net (Esuli and Sebastiani, 2005), detection of 
affective keywords (Riloff et al, 2003) and 
machine learning using term frequency (Pang et 
al., 2002; Riloff and Wiebe, 2003), or term 
discrimination (Danisman and Alpkocak, 2008). 
Other proposed methods include the creation of 
syntactic patterns and rules for cause-effect 
modeling (Mei Lee et al, 2009). Significantly 
different proposals for emotion detection in text 
are given in the work by (Liu et al 2003) and the 
recently proposed framework of sentic computing 
(Cambria et al, 2009), whose scope is to model 
affective reaction based on commonsense 
knowledge. For a survey on the affect models and 
their affective computing applications, see (Calvo 
and D?Mello, 2010).  
3 Motivation and Contribution 
The tasks of emotion detection and sentiment 
analysis have been approached by a large volume 
of research in NLP . Nevertheless, most of this 
research has concentrated on developing methods 
for detecting only explicit mentions of sentiment in 
text. Therefore, sentences such as ?I?m going to a 
party?, which express an underlying emotion, 
cannot be classified by most of the existing 
approaches. A method to overcome this issue is 
proposed in by sentic computing (Cambria et al, 
2009) and by (Liu et al 2003), whose main idea is 
acquiring knowledge on the emotional effect of 
different concepts. In this manner, the system 
would know that ?going to a party? is something 
that produces ?joy?. However, more complex 
contexts, such as ?I?m going to a party, although I 
should study for my exam.?, where the emotion 
expressed is most probably ?guilt?, cannot be 
correctly detected and classified by present 
systems. 
In the light of these considerations, our 
contribution relies in proposing and implementing 
a framework for modeling affect based on the 
appraisal theories, which can support the automatic 
processing of texts to extract: 
? The components of the situation presented 
(which we denote by ?action chains?) and 
their relation (temporal, causal etc.) 
? The elements on which the appraisal is 
done in each action of the chain (agent, 
action, object); 
? The appraisal criteria that can 
automatically be determined from the text 
(modifiers of the action, actor, object in 
each action chain); 
4 Modeling Affective Reaction Using 
Commonsense Knowledge  
Our main idea is that emotion can be expressed in 
text by presenting a sequence of actions (situations 
in which different concepts appear), which, based 
on commonsense knowledge and previous 
experiences, trigger an emotional reaction. This 
idea is linked to the Appraisal Theories, which 
claim that emotions are elicited and differentiated 
on the basis of the subjective evaluation of the 
personal significance of a situation, object or event 
(De Rivera, 1977; Frijda, 1986; Johnson-Laird and 
Oatley, 1989 ? among others). Viewed in a simpler 
manner, a situation is presented as a chain of 
actions, each with an actor and an object; the 
appraisal depends on the temporal and causal 
relationship between them, on the characteristics of 
the actors involved in the action and on the object 
of the action.  
Given this insight, the general idea behind our 
approach is to model situations as chains of actions 
and their corresponding emotional effect using an 
ontological representation. According to the 
definition provided by Studer et al (1998), an 
ontology captures knowledge shared by a 
community that can be easily sharable with other 
communities. These two characteristics are 
especially relevant if we want the recall of our 
approach to be increased. Knowledge managed in 
our approach has to be shared by a large 
community and it also needs to be fed by 
heterogeneous sources of common knowledge to 
54
avoid uncertainties. However, specific assertions 
can be introduced to account for the specificities of 
individuals or contexts. In this manner, we can 
model the interaction of different events in the 
context in which they take place. 
5 Building a Knowledge Base for 
Detecting Implicit Expressions of 
Emotion 
In order to build a resource that is capable of 
capturing emotional reaction to real-world 
situations in which commonsense knowledge plays 
a significant role in the affective interpretation, we 
aim at representing chains of actions and their 
corresponding emotional labels from several 
situations in such a way that we will be able to 
extract general patterns of appraisal. Our approach 
defines an action chain as a sequence of action 
links, or simply actions that trigger an emotion on 
an actor. Each specific action link can be described 
with a tuple (actor, action type, patient, emotional 
reaction). 
In order to manage and store action chains, the 
approach we propose defines a new knowledge 
base, called EmotiNet, which aims to be a resource 
for detecting emotions in text, and a 
(semi)automatic, iterative process to build it, which 
is based on existing knowledge from different 
sources. This process extracts the action chains 
from a set of documents and adds them to the KB. 
Specifically, EmotiNet was built by following the 
next steps: 
1. The design of an ontology, which contains 
the definitions of the main concepts of the 
domain.  
2. The extension and population of this 
ontology using the situations stored in the 
ISEAR International Survey of Emotional 
Antecedents and Reactions (ISEAR, 
http://www.unige.ch/fapse/emotion/databanks/isear.
html) ? (Scherer and Wallbott, 1997) 
database. 
3.  The expansion of the ontology using 
existing commonsense knowledge bases ? 
ConceptNet (Liu and Singh, 2004) and 
other resources ? VerbOcean (Chklovski 
and Pantel, 2004). 
5.1 Design of the Ontology 
As mentioned before, the process of building the 
core of the EmotiNet knowledge base (KB) of 
action chains started with the design of the core 
ontology, whose design process was specifically 
divided in three stages:  
1. Establishing the scope and purpose of the 
ontology. The EmotiNet ontology needs to capture 
and manage knowledge from three domains: 
kinship membership, emotions (and their relations) 
and actions (characteristics and relations between 
them).   
2. Reusing knowledge from existing ontologies. 
In a second stage, we searched for other ontologies 
on the Web containing concepts related to the 
knowledge cores we specified. At the end of the 
process, we located two ontologies that are reused 
in our ontological representation: the ReiAction 
ontology (www.cs.umbc.edu/~lkagal1/rei 
/ontologies/ReiAction.owl), which represents actions 
between entities in a general manner, and the 
family ontology (www.dlsi.ua.es/~jesusmhc/emotinet 
/family.owl), which contains knowledge about 
family members and the relations between them.  
anger
fea
r
surprise
joy
sadness
shame
guilt
basicEmotion
basicEmotion
oppositeEmotion
anticipation
disgust
trust
oppositeEmotion
oppositeEmotion
op
po
sit
eE
m
ot
io
n
optimism
hasEmotion
vigilance
hasHigherIntensity
Emotion
CompositeEmotion
hasEmotion
oppositeEmotion
hasHigherIntensity
basicEmotion
rdfs:subClassOf
rdf:type
 
 
Figure 1. Partial RDF graph of the Emotion Ontology. 
 
3. Creating the final knowledge core from the 
ontologies imported. This third stage involved the 
design of the last remaining core, i.e. emotion, and 
the combination of the different knowledge sources 
into a single ontology: EmotiNet. In order to 
describe the emotions and the way they relate and 
compose, we employ Robert Plutchik?s wheel of 
emotion (Plutchik, 2001) and Parrot?s tree-
55
structured list of emotions (Parrot, 2001). These 
models contain an explicit modeling of the 
relations between the different emotions. At the 
end of the design process, the knowledge core 
included different types of relations between 
emotions and a collection of specific instances of 
emotion (e.g. anger, joy). In the last step, these 
three cores were combined using new classes and 
relations between the existing members of these 
ontologies (Fig. 2). 
Emotion
Person
Action
SimpleAction
DomainAction
Feel
Forget
ArgueCrash
emotionFelt
?
Agent
Object
rdfs:subClassOf
rdfs:subClassOf
actor
target
rdfs:subClassOf
rdfs:subClassOf
Modifier
isAffectedBy
rdfs:subClassOf
implyEmotion
 
Figure 2. Main concepts of EmotiNet. 
5.2 Extension and Population of the Ontology 
In order to have a homogenous starting base, we 
selected from the 7667 examples in the ISEAR 
database only the 1081 cases that contained 
descriptions of situations between family members. 
Subsequently, the examples were POS-tagged 
using TreeTagger. Within each emotion class, we 
then computed the similarity of the examples with 
one another, using the implementation of the Lesk 
distance in Ted Pedersen?s Similarity Package. 
This score was used to split the examples in each 
emotion class into six clusters using the Simple K-
Means implementation in Weka. The idea behind 
this approach, confirmed by the output of the 
clusters, was to group examples that are similar, in 
vocabulary and structure. From this collection, we 
manually selected a subset of 175 documents with 
25 expressions related to each of the emotions: 
anger, disgust, guilt, fear, sadness, joy and shame. 
The criteria for choosing this subset were the 
simplicity of the sentences and the variety of 
actions described. 
The next step was to extract the actions chains 
described in each of the examples. For this, we 
employed Semrol, the semantic role labeling (SRL) 
system introduced by Moreda et al (2007). For the 
core of knowledge in the EmotiNet KB, we need 
100% accurate information. Therefore, we 
manually extract the agent, the verb and the patient 
(the surface object of the verb) from the output of 
Semrol. For example, if we use the input sentence 
?I?m going to a family party because my mother 
obliges me to?, the system extracts two triples with 
the main actors of the sentences: (I, go, family 
party) and (mother, oblige, me), related by the 
causal adverb ?because?.  
Further on, we resolve the anaphoric expressions 
automatically, using a heuristic selection of the 
family member mentioned in the text that is closest 
to the anaphoric reference and whose properties 
(gender, number) are compatible with the ones of 
the reference. The replacement of the references to 
the speaker, e.g. ?I?, ?me?, ?myself?, is resolved by 
taking into consideration the entities mentioned in 
the sentence. In case of ambiguity, we choose the 
youngest, female member. Following the last 
example, the subject of the action would be 
assigned to the daughter of the family and the 
triples would be updated: (daughter, go, 
family_party) and (mother, oblige, daughter). 
Finally, the action links (triplets) are grouped and 
sorted in action chains. This process of sorting is 
determined by the adverbial expressions that 
appear within the sentence, which actually specify 
the position of each action on a temporal line (e.g. 
?although? ?because?, ?when?). We defined 
pattern rules according to which the actions 
introduced by these modifiers happen prior to or 
after the current context.   
Using our combined emotion model as a reference, 
we manually assigned one of the seven most basic 
emotions, i.e. anger, fear, disgust, shame, sadness, 
joy or guilt, or the neutral value to all the action 
links obtained, thus generating 4-tuples (subject, 
action, object, emotion), e.g. (daughter, go, family 
party, neutral) or (mother, oblige, daughter, 
disgust).  
Once we carried out these processes on the chosen 
documents, we obtained 175 action chains (ordered 
lists of tuples). In order to be included in the 
EmotiNet knowledge base, all their action links 
needed to be mapped to existing concepts or 
instances within the KB. When these did not exist, 
they were added to it. We would like to highlight 
that in EmotiNet, each tuple (actor, action, patient, 
emotion) extracted has its own representation as an 
instance of the subclasses of Action. Each in-stance 
56
of Action is related to an instance of the class Feel, 
which represents the emotion felt in this action. 
Subsequently, these instances (action links) were 
grouped in sequences of actions (class Sequence) 
ended by an instance of the class Feel, which 
determine the final emotion felt by the main 
actor(s) of the chain.  
In our example, we created two new classes Go 
and Oblige (subclasses of DomainAction) and two 
new instances of them: instance act1 (?Go?, 
?daughter?, ?family_party?, ?Neutral?); and 
instance act2 (?Oblige?, ?mother?, ?daughter?, 
?Angry?). The last action link already existed 
within EmotiNet from another chain so we reused 
it: instance act3 (?Feel?, ?daughter?, ?anger?). The 
next step consisted in grouping these instances into 
sequences by means of instances of the class 
Sequence, which is a subclass of Action that can 
establish the temporal order between two actions 
(which one occurred first). Fig. 3 shows an 
example of a RDF graph with the action chain of 
our example. We used Jena 
(http://jena.sourceforge.net/) and MySQL for the 
management and storage of EmotiNet on a 
database.  
hasChild
feel_anger_1
go_1
oblige_1
sequence_1
sequence_2
emotionFelt
actor actor
actor
Action Chain
target
target
anger
second
second
first
first
mother_f1
daughter_f1
disgust
implies
party_1
 
Figure 3. RDF graph of an action chain. 
5.3 Ontology Expansion 
In order to extend the coverage of the resource, we 
expanded the ontology with the actions and 
relations from VerbOcean. This process is essential 
for EmotiNet, since it adds new types of action and 
relations between actions, which might not have 
been analyzed before, thus reducing the degree of 
dependency between the resource and the initial set 
of examples. In particular, 299 new actions were 
automatically included as subclasses of 
DomainAction, which were directly related to any 
of the actions of our ontology through three new 
relations: can-result-in, happens-before and 
similar. 
6 Experiments and Evaluation 
The evaluation of our approach consists in testing 
if by employing the model we built and the 
knowledge contained in the core of EmotiNet 
(which we denote by ?knowledge sets?), we are 
able to detect the emotion expressed in new 
examples pertaining to the categories in ISEAR. 
Therefore, we use a test set (marked with B) that 
contains 895 examples (ISEAR phrases 
corresponding to the seven emotions modeled, 
from which core examples were removed).  
In order to assess the system performance on the 
two test sets, we followed the same process we 
used for building the core of EmotiNet, with the 
exception that the manual modeling of examples 
into tuples was replaced with the automatic 
extraction of (actor, verb, patient) triples from the 
output given by Semrol. Subsequently, we 
eliminated the stopwords in the phrases contained 
in these three roles and performed a simple corefe-
rence resolution. Next, we ordered the actions 
presented in the phrase, using the adverbs that 
connect the sentences, through the use of patterns 
(temporal, causal etc.). The resulted action chains 
for each of the examples in the two test sets will be 
used in carrying different experiments:  
 (1). In the first approach, for each of the situations 
in the test sets (represented now as action chains), 
we search the EmotiNet KB to encounter the 
sequences in which these actions in the chains are 
involved and their corresponding subjects. As a 
result of the search process, we obtain the emotion 
label corresponding to the new situation and the 
subject of the emotion based on a weighting 
function. This function takes into consideration the 
number of actions and the position in which they 
appear in the sequence contained in EmotiNet. The 
issue in this first approach is that many of the 
examples cannot be classified, as the knowledge 
they contain is not present in the ontology.  
(2). A subsequent approach aimed at surpassing the 
issues raised by the missing knowledge in 
EmotiNet. In a first approximation, we aimed at 
introducing extra knowledge from VerbOcean, by 
adding the verbs that were similar to the ones in 
57
the core examples (represented in VerbOcean 
through the ?similar? relation). Subsequently, each 
of the actions in the examples to be classified that 
was not already contained in EmotiNet, was sought 
in VerbOcean. In case one of the similar actions 
was already contained in the KB, the actions were 
considered equivalent. Further on, each action was 
associated with an emotion, using the ConceptNet 
relations and concepts (HasSubevent, Causes, 
ConceptuallyRelatedTo, HasPrerequisite). Finally, 
new examples were matched against chains of 
actions containing the same emotions, in the same 
order.  While more complete than the first 
approximation, this approach was also affected by 
lack of knowledge about the emotional content of 
actions. To overcome this issue, we proposed two 
heuristics: 
(2a) In the first one, actions on which no affect 
information was available, were sought in within 
the examples already introduced in the EmotiNet 
and were assigned the most frequent class of 
emotion labeling them. The corresponding results 
are marked with A2a and B2a, respectively. 
 (2b) In the second approximation, we used the 
most frequent emotion associated to the known 
links of a chain, whose individual emotions were 
obtained from ConceptNet.  In this case, the core 
of action chains is not involved in the process. The 
corresponding results are marked with A2b and 
B2b. 
We performed the steps described on test set B. 
The results are shown in Table 1 (results on 
classified examples) and Table 2 (results on all 
examples). 
 
Emotio
n 
Correct Total Accuracy 
B1 B2
a 
B2
b 
B1 B 
2a 
B2
b 
B1 B2a B2b 
disgust 16 16 21 44 42 40 
36.3
6 
38.0
9 
52.5
0 
shame 25 25 26 70 78 73 
35.7
1 
32.0
5 
35.6
2 
anger 31 47 57 
10
5 
11
5 121 
29.5
2 
40.8
6 
47.1
1 
fear 35 34 37 58 65 60 
60.3
4 
52.3
0 
61.6
7 
sadness 46 45 41 
11
1 
12
3 125 
41.4
4 
36.5
8 
32.8
0 
joy 13 16 18 25 29 35 52 
55.1
7 
51.4
3 
guilt 59 68 64 
15
8 
16
5 171 
37.3
4 
41.2
1 
37.4
3 
Total 22
5 251 264 
57
1 
61
7 625 
39.4
0 
40.6
8 
42.2
4 
Table 1. Results of the emotion detection using 
EmotiNet on classified examples in test set B 
 
Emotion Correct Total Recall 
B1 B2a B2b B1 B1 B2a B2b 
Disgust 16 16 21 59 27.11 27.11 35.59 
Shame 25 25 26 91 27.47 27.47 28.57 
Anger 31 47 57 145 21.37 32.41 39.31 
Fear 35 34 37 85 60.34 52.30 61.67 
Sadness 46 45 41 267 17.22 16.85 15.36 
Joy 13 16 18 50 26 32 36.00 
Guilt 59 68 64 198 29.79 34.34 32.32 
Total 225 251 264 895 25.13 28.04 29.50 
Baseline 126 126 126 895 14.0.7 14.07 14.07 
Table 2. Results of the emotion detection using 
EmotiNet on all test examples in test set B 
7 Discussion and conclusions 
From the results in Table 1 and 2, we can conclude 
that the approach is valid and represents a method 
that is appropriate for the detection of emotions 
from contexts where no affect-related words are 
present. Nonetheless, much remains to be done to 
fully exploit the capabilities of EmotiNet. We 
showed that the approach has a high degree of 
flexibility, i.e. new information can be easily 
introduced from existing common-sense 
knowledge bases, such as ConceptNet, mainly due 
to its internal structure and degree of granularity.  
The error analysis we performed shed some light 
on the causes of error of the system. The first 
finding is that extracting only the action, verb and 
patient semantic roles is not sufficient. There are 
other roles, such as the modifiers, which change 
the overall emotion in the text. Therefore, such 
modifiers should be included as attributes of the 
concepts identified in the roles. A further source of 
errors was that lack of knowledge on specific 
actions. Thus, the results of our approach can be 
practically limited by the structure, expressivity 
and degree of granularity of the imported 
resources. Therefore, to obtain the final, extended 
version of EmotiNet we should analyze the 
interactions between the core and the imported 
resources and among these re-sources as well. 
Finally, other errors were produced by NLP 
processes and propagated at various steps of the 
processing chain (e.g. SRL, coreference 
resolution). Some of these errors cannot be 
eliminated; however, others can be partially solved 
by using alternative NLP tools.  
Future work aims at extending the model by 
adding affective properties to the concepts 
58
included, so that more of the appraisal criteria can 
be introduced in the model, testing new methods to 
assign affective value to the concepts and adding 
new knowledge from sources such as CYC.  
Acknowledgments 
This paper has been supported by the Spanish 
Ministry of Science and Innovation (grant no. 
TIN2009-13391-C04-01), by the Spanish Ministry 
of Education under the FPU Program (AP2007-
03076), and by the Valencian Ministry of 
Education (grant no. PROMETEO/2009/119 and 
ACOMP/ 2010/288). 
References  
A. Balahur and A. Montoyo. 2008. Applying a Culture 
Dependent Emotion Triggers Database for Text 
Valence and Emotion Classification, proceedings of 
the AISB 2008 Convention ?Communication, 
Interaction and Social Intelligence?. 
A. Balahur and R. Steinberger. 2009. Rethinking 
Opinion Mining in Newspaper Articles: from Theory 
to Practice and Back, proceedings of the first work-
shop on Opinion Mining and Sentiment Analysis 
(WOMSA 2009). 
A. Esuli and F. Sebastiani. 2005. Determining the 
semantic orientation of terms through gloss analysis?, 
proceedings of CIKM 2005. 
B. Pang and L. Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, Vol 2, Nr. 1-2, 2008. 
B. Pang, L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learning 
techniques, proceedings of EMNLP-02.  
C. Strapparava and R. Mihalcea. 2007. Semeval 2007 
task 14: Affective text, proceedings of ACL 2007. 
E. Cambria, A. Hussain, C. Havasi and C. Eckl. 2009. 
Affective Space: Blending Common Sense and 
Affective Knowledge to Perform Emotive 
Reasoning, proceedings of the 1st Workshop on 
Opinion Mining and Sentiment Analysis (WOMSA). 
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions, proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing. 
E. Riloff, J. Wiebe and T. Wilson. 2003. Learning 
subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Conference on 
Natural Language Learning (CoNLL) 2003, pp.25-
32, Edmonton, Canada. 
G. Van den Bos. 2006. APA Dictionary of Psychology. 
Washington, DC: American Psychological 
Association. 
H. Liu and P. Singh. 2004. ConceptNet: A Practical 
Commonsense Reasoning Toolkit, BT Technology 
Journal, Volume 22, Kluwer Academic Publishers. 
H. Liu, H. Lieberman and T. Selker. 2003. A Model of 
Textual Affect Sensing Using Real-World Know-
ledge, proceedings of IUI 2003.  
J. De Rivera. 1977. A structural theory of the emotions, 
Psychological Issues, 10 (4), Monograph 40. 
J. W. Pennebaker, M. R. Mehl and K. Niederhoffer. 
2003. Psychological aspects of natural language use: 
Our words, our selves, Annual Review of Psychology 
54, 547-577. 
K. Scherer and H. Wallbott. 1997. The ISEAR 
Questionnaire and Codebook, Geneva Emotion Re-
search Group. 
K. Scherer, K. 2005. What are emotions? and how can 
they be measured? Social Science Information, 3(44), 
695-729. 
M. Dyer. 1987. Emotions and their computations: three 
computer models, Cognition and Emotion, 1, 323-
347. 
N. Frijda. 1986. The emotions, Cambridge University 
Press. 
P. Moreda, B. Navarro and M. Palomar. 2007. Corpus-
based semantic role approach in information 
retrieval, Data Knowl. Eng. (DKE) 61(3):467-483. 
P. N. Johnson-Laird and K. Oatley. 1989. The language 
of emotions: An analysis of a semantic field, 
Cognition and Emotion, 3, 81-123. 
P. Subasic and A. Huettner. 2000. Affect Analysis of 
text using fuzzy semantic typing, IEEE Trasactions 
on Fuzzy System, 9, 483-496.   
R. A. Calvo and S. D?Mello. 2010. Affect Detection: An 
Interdisciplinary Review of Models, Methods and 
Their Applications, IEEE Transactions on Affective 
Computing, Vol. 1, No. 1, Jan.-Jun.  
R. Picard. 1995. Affective computing, Technical re-
port, MIT Media Laboratory. 
R. Plutchik. 2001. The Nature of Emotions. American 
Scientist. 89, 344. 
R. Studer, R. V. Benjamins and D. Fensel. 1998. 
Knowledge engineering: Principles and methods, 
Data & Knowledge Engineering, 25(1-2):161?197. 
59
S. Y. Mei Lee, Y. Chen and C.-R. Huang. 2009. Cause 
Event Representations of Happiness and Surprise, 
proceedings of PACLIC 2009. 
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining 
the Web for Fine-Grained Semantic Verb Relations?, 
proceedings of EMNLP-04. 
T. Danisman and A. Alpkocak. 2008. Feeler: Emotion 
Classification of Text Using Vector Space Model, 
proceedings of the AISB 2008 Convention, ?Com-
munication, Interaction and Social Intelligence?.  
W. Parrott. 2001. Emotions in Social Psychology, 
Psychology Press, Philadelphia. 
 
60
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 168?174,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Towards a Unified Approach for Opinion Question Answering and
Summarization
Elena Lloret and Alexandra Balahur and Manuel Palomar and Andre?s Montoyo
Department of Software and Computing Systems
University of Alicante
Alicante 03690, Spain
{elloret,abalahur, mpalomar, montoyo}@dlsi.ua.es
Abstract
The aim of this paper is to present an ap-
proach to tackle the task of opinion question
answering and text summarization. Follow-
ing the guidelines TAC 2008 Opinion Sum-
marization Pilot task, we propose new meth-
ods for each of the major components of the
process. In particular, for the information
retrieval, opinion mining and summarization
stages. The performance obtained improves
with respect to the state of the art by approxi-
mately 12.50%, thus concluding that the sug-
gested approaches for these three components
are adequate.
1 Introduction
Since the birth of the Social Web, users play a cru-
cial role in the content appearing on the Internet.
With this type of content increasing at an exponen-
tial rate, the field of Opinion Mining (OM) becomes
essential for analyzing and classifying the sentiment
found in texts.
Nevertheless, real-world applications of OM of-
ten require more than an opinion mining component.
On the one hand, an application should allow a user
to query about opinions in natural language. There-
fore, Question Answering (QA) techniques must be
applied in order to determine the information re-
quired by the user and subsequently retrieve and
analyze it. On the other hand, opinion mining of-
fers mechanisms to automatically detect and classify
sentiments in texts, overcoming the issue given by
the high volume of such information present on the
Internet. However, in many cases, even the result of
the opinion processing by an automatic system still
contains large quantities of information, which are
still difficult to deal with manually. For example,
for questions such as ?Why do people like George
Clooney?? we can find thousands of answers on the
Web. Therefore, finding the relevant opinions ex-
pressed on George Clooney, classifying them and
filtering only the positive opinions is not helpful
enough for the user. He/she will still have to sift
through thousands of texts snippets, containing rele-
vant, but also much redundant information. For that,
we need to use Text Summarization (TS) techniques.
TS provides a condensed version of one or several
documents (i.e., a summary) which can be used as a
substitute of the original ones (Spa?rck Jones, 2007).
In this paper, we will concentrate on proposing ad-
equate solutions to tackle the issue of opinion ques-
tion answering and summarization. Specifically, we
will propose methods to improve the task of ques-
tion answering and summarization over opinionated
data, as defined in the TAC 2008 ?Opinion Sum-
marization pilot?1. Given the performance improve-
ments obtained, we conclude that the approaches we
proposed for these three components are adequate.
2 Related Work
Research focused on building factoid QA systems
has a long tradition, however, it is only recently that
studies have started to focus on the creation and de-
velopment of opinion QA systems. Example of this
can be (Stoyanov et al, 2004) who took advantage of
opinion summarization to support Multi-Perspective
QA system, aiming at extracting opinion-oriented
information of a question. (Yu and Hatzivassiloglou,
2003) separated opinions from facts and summa-
rized them as answer to opinion questions. Apart
from these studies, specialized competitions for sys-
tems dealing with opinion retrieval and QA have
been organized in the past few years. The TAC
2008 Opinion Summarization Pilot track proposed
a mixed setting of factoid and opinion questions.
1http://www.nist.gov/tac/2008/summarization/
168
It is interesting to note that most of the participat-
ing systems only adapted their factual QA systems
to overcome the newly introduced difficulties re-
lated to opinion mining and polarity classification.
Other relevant competition focused on the treatment
of subjective data is the NTCIR MOAT (Multilin-
gual Opinion Analysis Test Collection). The ap-
proaches taken by the participants in this task are rel-
evant to the process of opinion retrieval, which is the
first step performed by an opinion mining question
answering system. For example, (Taras Zabibalov,
2008) used an almost unsupervised approach ap-
plied to two of the sub-tasks: opinionated sentence
and topic relevance detection.(Qu et al, 2008) ap-
plied a sequential tagging approach at the token level
and used the learned token labels in the sentence
level classification task and their formal run submis-
sion was is trained on MPQA (Wiebe et al, 2005).
3 Text Analysis Conferences
In 2008, the Opinion Summarization Pilot task at
the Text Analysis Conferences2 (TAC) consisted in
generating summaries from blogs, according to spe-
cific opinion questions provided by the TAC orga-
nizers. Given a set of blogs from the Blog06 col-
lection3 and a list of questions, participants had to
produce a summary that answered these questions.
The questions generally required determining opin-
ion expressed on a target, each of which dealt with a
single topic (e.g. George Clooney). Additionally, a
set of text snippets were also provided, which con-
tained the answers to the questions. Table 1 depicts
an example of target, question, and optional snippet.
Target: George Clooney
Questions: Why do people like George Clooney?Why do people dislike George Clooney?
Snippets: 1050 BLOG06-20060209-006-0013539097
he?s a great actor.
Table 1: Example of target, question, and snippet.
Following the results obtained in the evaluation
at TAC 2008 (Balahur et al, 2008), we propose
an opinion question answering and summarization
(OQA&S) approach, which is described in detail in
the following sections.
2www.nist.gov/tac/
3http://ir.dcs.gla.ac.uk/test collections/access to data.html
4 An Opinion Question Answering and
Summarization Approach
In order to improve the results of the OQA&S sys-
tem presented at TAC, we propose new methods for
each of the major components of the system: infor-
mation retrieval, opinion mining and text summa-
rization.
4.1 Opinion Question Answering and
Summarization Components
? Information Retrieval
JAVA Information Retrieval system (JIRS) is
a IR system especially suited for QA tasks
(Go?mez, 2007). Its purpose is to find frag-
ments of text (passages) with more probabil-
ity of containing the answer to a user question
made in natural language instead of finding rel-
evant documents for a query. To that end, JIRS
uses the own question structure and tries to
find an equal or similar expression in the docu-
ments. The more similar the structure between
the question and the passage is, the higher the
passage relevance.
JIRS is able to find question structures in a
large document collection quickly and effi-
ciently using different n-gram models. Subse-
quently, each passage is assessed depending on
the extracted n-grams, the weight of these n-
grams, and the relative distance between them.
Finally, it is worth noting that the number of
passages in JIRS is configurable, and in this
research we are going to experiment with pas-
sages of length 1 and 3.
? Opinion Mining
The first step we took in our approach was
to determine the opinionated sentences, as-
sign each of them a polarity (positive or neg-
ative) and a numerical value corresponding to
the polarity strength (the higher the negative
score, the more negative the sentence and vice
versa). In our first approximation (OMaprox1),
we employed a simple, yet efficient method,
presented in Balahur et al (Balahur et al,
2009). As lexicons for affect detection, we
used WordNet Affect (Strapparava and Vali-
tutti, 2004), SentiWordNet (Esuli and Sebas-
169
tiani, 2006), and MicroWNOp (Cerini et al,
2007). Each of the resources we employed
were mapped to four categories, which were
given different scores: positive (1), negative
(-1), high positive (4) and high negative (-4).
First, the score of each of the blog posts was
computed as the sum of the values of the words
that were identified. Subsequently, we per-
formed sentence splitting4 and classified the
sentences we thus obtained according to their
polarity, by adding the individual scores of the
affective words identified.
In the second approach (OMaprox2), we first
filter out the sentences that are associated to
the topic discussed, using LSA. Further on, we
score the sentences identified as relating to the
topic of the blog post, in the same manner as
in the previous approach. The aim of this ap-
proach is to select for further processing only
the sentences which contain opinions on the
post topic. In order to filter these sentences
in, we first create a small corpus of blog posts
on each of the topics included in our collec-
tion5. For each of the corpora obtained, we
apply LSA, using the Infomap NLP Software6.
Subsequently, we compute the 100 most asso-
ciated words with two of the terms that are most
associated with each of the topics and the 100
most associated words with the topic word. The
approach was proven to be successful in (Bal-
ahur et al, 2010).
? Text Summarization
The text summarization approach used in this
paper was presented in (Lloret and Palomar,
2009). In order to generate a summary, the
suggested approach first carries out a basic pre-
processing stage comprising HTML parsing,
sentence segmentation, tokenization, and stem-
ming. Once the input document or documents
have been pre-processed, a relevance detection
stage, which is the core part of the approach, is
applied. The objective of this step is to identify
4http://alias-i.com/lingpipe/
5These small corpora (30 posts for each of the top-
ics) are gathered using the search on topic words on
http://www.blogniscient.com/ and crawling the resulting pages.
6http://infomap-nlp.sourceforge.net/
potential relevant sentences in the document by
means of three techniques: textual entailment,
term frequency and the code quantity principle
(Givo?n, 1990). Then, each potential relevant
sentence is given a score which is computed
on the basis of the aforementioned techniques.
Finally, all sentences are ordered according
to their scores, and the highest ranked ones
(which mean those sentences contain more im-
portant information) are selected and extracted
up to the desired length, thus building the fi-
nal summary. It is worth stressing upon the fact
that in an attempt to maintain the coherence of
the original documents, sentences are shown in
the same order they appear in the original doc-
uments.
4.2 Experimental Framework
The objective of this section is to describe the corpus
used and the experiments performed with the data
provided in TAC 2008 Opinion Summarization Pi-
lot7 task. The approaches analyzed comprise:
? OQA&S: The three components explained
in the previous section (information retrieval,
opinion mining and summarization) were
bound together in order to produce summaries
that include the answer to opinionated ques-
tions. First, the most relevant passages of
length 1 and 3 are retrieved by the IR module,
as in the aforementioned approach, and then
the subjective information is found and classi-
fied within them using the OM approaches de-
scribed in the previous section. Further on, we
incorporate the TS module, to select and ex-
tract the most relevant opinionated facts from
the pool of subjective information identified
by the OM module. We generate opinion-
oriented summaries of compression rates rang-
ing from 10% to 50%. In the end, four dif-
ferent approaches result from the integration
of the three components: IRp1-OMaprox1-
TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-
TS; and IRp3-OMaprox2-TS.
Moreover, apart from these approaches, two base-
lines were also defined. On the one hand, we sug-
7http://www.nist.gov/tac/data/past-
blog06/2008/OpSummQA08.html#OpSumm
170
gest a baseline using the list of snippets provided by
the TAC organization (QA-snippets). This baseline
produces a summary by joining all the answers in the
snippets that related to the same topic On the other
hand, we took as a second baseline the approach
from our participation in TAC 2008 (DLSIUAES),
without not taking into account any information re-
trieval or question answering system to retrieve the
fragments of information which may be relevant to
the query. In contrast, this was performed by com-
puting the cosine similarity8 between each sentence
in the blog and the query. After all the potential rel-
evant sentences for the query were identified, they
were classified in terms of subjectivity and polarity,
and the most relevant ones were selected for the final
summary.
4.3 Evaluation Methodology
Since we used the corpus provided at the Opinion
Summarization Pilot task, and we followed simi-
lar guidelines, we should evaluate our OQA&S ap-
proach in the same way as participant systems were
assessed. However, the evaluation methodology
proposed differs slightly from the one carried out
in the competition. The reason why we took such
decision was due to the fact that the evaluation car-
ried out in TAC had some limitations, and therefore
was not suitable for our purposes. In this manner,
our evaluation is also based on the gold-standard
nuggets provided by TAC, but in addition we pro-
posed an extended version of them, by adding other
pieces of information that are also relevant to the
topics.
In this section, all the issues concerning the eval-
uation are explained. These comprise the original
evaluation method used in the Opinion Summariza-
tion Pilot task at TAC (Section 4.3.1) , its draw-
backs (Section 4.3.2), and the extended version for
the evaluation method we propose (Section 4.3.3).
Further on, the results obtained together with a wide
discussion, as well as its comparison with the base-
lines and the TAC participants is provided in Section
4.4.
4.3.1 Nugget-based Evaluation at TAC
Within the Opinion Summarization Pilot task,
each summary was evaluated according to its con-
8http://www.d.umn.edu/ tpederse/text-similarity.html
tent using the Pyramid method (Nenkova et al,
2007). A list of nuggets was provided and the asses-
sors used such list of nuggets to count the number
of nuggets a summary contained. Depending on the
number of nuggets the summary included and the
importance of each one given by their weight, the
values for recall, precision and F-measure were ob-
tained. An example of several nuggets correspond-
ing to different topics can be seen in Table 2, where
the weight for each one is also shown in brackets.
Topic Nugget (weight)
Carmax CARMAX prices are firm, the price is
the price (0.9)
Jiffy Lube They should have torque wrenches (0.2)
Talk show hosts Funny (0.78)
Table 2: Example of evaluation nuggets and associated
weights.
4.3.2 Limitations of the Nugget Evaluation
The evaluation method suggested at TAC requires
a lot of human effort when it comes to identify
the relevant fragments of information (nuggets) and
compute how many of them a summary contains, re-
sulting in a very costly and time-consuming task.
This is a general problem associated to the evalua-
tion of summaries, which makes the task of summa-
rization evaluation especially hard and difficult.
But, apart from this, when an exhaustive exam-
ination of the nuggets used in TAC is done, some
other problems arised which are worth mentioning.
The average number of nuggets for each topic is
27, and this would mean, that longer summaries
will be highly penalized, because it will contain
more useless information according to the nuggets.
After analyzing in detail all the provided nuggets,
we mainly classified the possible problems into six
groups, which are:
1. Some of the nuggets were expressed differently
from how they appeared in the original blogs.
Since most of the summarization systems are ex-
tractive, this fact forced that humans had to evaluate
the summaries, otherwise it would be very difficult
to account for the presence of such nugget in the
summary, if they are not using the same vocabulary
as the original blogs.
2. Some nuggets for the same topic express the
171
same idea, despite not being identical. In these
cases, we are counting a single piece of informa-
tion in the summary twice, if the idea that nuggets
expressed is included.
3. Moreover, the meaning of one nugget can be de-
duced from another?s, which is also related to the
problem stated before.
4. Some of the nuggets are not very clear in mean-
ing (e.g. ?hot?, ?fun?). This would mean that a
summary might include such terms in a different
context, thus, obtaining incorrectly that it is reve-
lant when might be out of context.
5. A sentence in the original blog can be covered by
several nuggets. For instance, both nuggets ?it is
an honest book? and ?it is a great book? correspond
to the same sentence ?It was such a great book-
honest and hard to read (content not language dif-
ficulty)?. In this case, it is not clear how to proceed
with the evaluation; whether to count both nuggets
or just one of them.
6. Some information which is also relevant for the
topic is not present in any nugget. For instance:
?I go to Starbucks because they generally provide
me better service?. Although it is relevant with re-
spect to the topic and it appears in a number of sum-
maries, it would be not counted because it has not
been chosen as a nugget.
4.3.3 Extended Nugget-based Evaluation
Since we are interested in testing a wide range of
approaches involving IR, OM and TS, sticking to the
rules to the original TAC evaluation would mean that
a lot of time as well as human effort will be required,
as well as not accounting for important information
that summaries may contain in addition to the one
expressed by the nuggets. Therefore, taking as a ba-
sis the nuggets provided at TAC, we set out a modi-
fied version of them.
The underlying idea behind this is to create an ex-
tended set of nuggets that serve as a reference for
assessing the content of the summaries. In this man-
ner, we will map each original nugget with the set of
sentences in the original blogs that are most similar
to it, thus generating a gold-standard summary for
each topic. For creating this extended gold-standard
nuggets we compute the cosine similarity9 between
9The cosine similarity was computed using Pedersen?s
every nugget and all the sentences in the blog related
to the same topic. We empirically established a sim-
ilarity threshold of 0.5, meaning that if a sentence
was equal or above such similarity value, it will be
considered also relevant. One main disadvantage of
such a lower threshold value is that we can consider
relevant sentences that share the same vocabulary
but in fact they are not relevant to the summary. In
order to avoid this, once we had identified all the
most similar sentences to each nugget, we carried
out a manual analysis to discard cases like this. Hav-
ing created the extended set of nuggets, we grouped
all of them pertaining to the same topic, and consid-
ered it a gold-standard summary. Now, the average
number of nuggets per topic is 53, which we have
increased by twice the number of original nuggets
provided at TAC.
Further on, our summaries are compared against
this new gold-standard using ROUGE (Lin, 2004).
This tool computes the number of different kinds
of overlap n-grams between an automatic summary
and a human-made summary. For our evaluation,
we compute ROUGE-1 (unigrams), ROUGE-2 (bi-
grams), ROUGE-SU4 (it measures the overlap of
skip-bigrams between a candidate summary and a
set of reference summaries with a maximum skip
distance of 4), and ROUGE-L (Longest Common
Subsequence between two texts). The results and
discussion are next provided.
4.4 Results and Discussion
This section contains the results obtained for our
OQA&S approach and all the sub-approaches tested.
IRpN refers to the length of the passage employed
in the information retrieval approach, whereas
OMaproxN indicates the approach used for the opin-
ion mining component. Firstly, we show and ana-
lyze the results of our different approaches, and then
we compared the best performing one with the base-
lines and the average Opinion Summarization Pilot
task participants results in TAC.
Table 3 shows the precision (Pre), recall (Rec) and
F-measure results of ROUGE-1 (R-1) for all the ap-
proaches we experimented with.
Generally speaking, the results obtained show
better figures for precision than for recall, and there-
Text Similarity Package: http://www.d.umn.edu/ tpederse/text-
similarity.html
172
Approach Summary length
Name R-1 10% 20% 30% 40% 50%
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 14.45 18.58 22.32 23.63 26.32
-OMaprox1-TS F?=1 16.53 20.65 24.58 25.75 28.12
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 16.90 20.02 23.36 24.15 26.77
-OMaprox2-TS F?=1 19.45 22.13 25.36 25.94 28.40
Pre 27.27 30.18 30.91 30.05 30.19
IRp3 Rec 20.56 24.76 28.25 31.67 34.47
-OMaprox1-TS F?=1 22.65 26.23 27.98 29.18 29.74
Pre 30.16 32.11 32.35 32.41 32.11
IRp3 Rec 20.64 24.03 27.25 29.78 32.68
-OMaprox2-TS F?=1 23.28 25.64 27.42 28.44 29.21
Table 3: Results of our OQA&S approaches
Approach Performance (ROUGE)
Name % R-1 R-2 R-L R-SU4
Pre 32.11 7.34 29.00 11.37
IRp3-OMaprox2 Rec 32.68 8.31 33.24 12.76
-TS (50%) F?=1 29.21 7.22 28.60 11.13
Pre 17.97 8.76 17.65 9.98
QA-snippets Rec 71.24 31.30 70.10 37.44
F?=1 24.73 11.58 24.29 13.45
Pre 20.54 7.00 19.46 9.29
DLSIUAES Rec 57.66 18.98 54.61 25.77
F?=1 27.04 9.10 25.59 12.22
Pre 23.74 8.35 22.72 10.81
Average TAC Rec 56.65 19.37 54.56 25.40
participants F?=1 27.45 9.64 26.33 12.46
Pre 20.42 6.06 19.55 8.62
Average TAC Rec 56.45 17.3 54.40 24.11
participants? F?=1 24.31 7.25 23.31 10.29
Table 4: Comparison with other systems
fore the F-measure value, which combines both val-
ues, will be affected. Good precision values means
that the information our approaches select is the cor-
rect one, despite not including all the relevant infor-
mation.
Our best performing approach in general is the
one which uses a length passage of 3 and, as far
as OM is concerned, when topic-sentiment analy-
sis is carried out (IRp3-OMaprox2-TS). This shows
that the approach dealing with topic-sentiment anal-
ysis in opinion mining is more suitable than the one
which does not consider topic relevance. Taking a
look at some individual results, we next try to eluci-
date the reasons why our approach performs better
at some approaches and not so good at others. Con-
cerning the IR module, it is important to mention
that a passage length of 1 always obtains poorer re-
sults that when it is increased to 3, meaning that the
longer the passage, the better.
Regarding the best summary length, we observed
that in general terms, the more content we allow
for the summary, the better. In other words, com-
pression rates of 50% get higher results than 20%
or 10%. However, there are cases in which shorter
summaries (10% and 20%) obtains better results
than longer ones (e.g. IRp3-OMaprox2-TS vs. IRp3-
OMaprox1-TS).
Although the results theirselves are not very high
(around 30%), they are in line with the state-of-the-
art, as can be seen in Table 4, where our best per-
forming approach is compared with respect to other
approaches.
Although the compression rate which obtains best
results is not very high (50%), indeed the final sum-
maries have an average length of 2,333 non-white
space characters. This is really low compared to the
length that TAC organization allowed for the Opin-
ion Summarization Pilot task, which was 7,000 non-
white space characters per question, and most of
the times there were two questions for each topic.
Whereas the results of TAC participants are much
better for the recall value than ours, if we take a look
at the precision, our approach outperforms them ac-
cording to this value in all of the cases. The longer
a summary is, the more chances it has to contain in-
formation related to the topic. However, not all this
information may be relevant, as it is shown in the
results for the precision values, which decrease con-
siderably compared to the recall ones. In contrast,
due to the fact that our approach is missing some
relevant information because we use a rather short
passage length (3 sentences), we do not obtain such
high values for the recall, but we obtain good preci-
sion results, which indicate that the information that
we keep is important.
Moreover, comparing those results with the ones
obtained by our approach, it is worth mentioning
that IRp3-OMaprox2-TS outperforms the F-measure
value for all the ROUGE metrics with respect to Av-
erage TAC participants?. More in detail, when the
ROUGE scores are averaged, IRp3-OMaprox2-TS
improves by 12.50% the Average TAC participants?
for the F-measure value.
173
5 Conclusion and Future Work
In this paper, we tackled the process of OQA&S.
In particular, we analyzed specific methods within
each component of this process, i.e., information
retrieval, opinion mining and text summarization.
These components are crucial in this task, since our
final goal was to provide users with the correct infor-
mation containing the answer of a question. How-
ever, contrary to most research work in question an-
swering, we focus on opinionated questions rather
than factual, increasing the difficulty of the task.
Our analysis comprises different configurations
and approaches: i) varying the length for retrieving
the passages of the documents in the retrieval infor-
mation stage; ii) studying a method that take into
consideration topic-sentiment analysis for detecting
and classifying opinions in the retrieved passages
and comparing it to another that does not; and iii)
generating summaries of different compression rates
(10% to 50%). The results obtained showed that
the proposed methods are appropriate to tackle the
OQA&S task, improving state of the art approaches
by 12.50% approximately.
In the future, we plan to continue investigating
suitable approaches for each of the proposed com-
ponents. Our final goal is to build an integrated and
complete approach.
Acknowledgments
This research work has been funded by the Spanish Gov-
ernment through the research program FPI (BES-2007-
16268) associated to the project TEXT-MESS (TIN2006-
1526-C06-01). Moreover, it has been also partially
funded by projects TEXT-MESS 2.0 (TIN2009-13391-
C04), and PROMETEO (PROMETEO/2009/199) from
the Spanish and the Valencian Government, respectively.
References
A. Balahur, E. Lloret, O. Ferra?ndez, A. Montoyo,
M. Palomar, and R. Mun?oz. 2008. The DLSIUAES
team?s participation in the tac 2008 tracks. In Pro-
ceedings of the Text Analysis Conference.
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
Bruno Pouliquen, and Mijai Kabadjov. 2009. Opinion
mining from newspaper quotations. In Proceedings of
the Workshop on Intelligent Analysis and Processing
of Web News Content.
A. Balahur, M. Kabadjov, and J. Steinberger. 2010.
Exploiting higher-level semantic information for the
opinion-oriented summarization of blogs. In Proceed-
ings of CICLing?2010.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Language re-
sources and linguistic theory: Typology, second lan-
guage acquisition, English linguistics.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceedings of LREC.
Talmy Givo?n, 1990. Syntax: A functional-typological in-
troduction, II. John Benjamins.
Jose? M. Go?mez. 2007. Recuperacio?n de Pasajes Multil-
ingu?e para la Bu?squeda de Respuestas. Ph.D. thesis.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. In Proceedings of ACL Text
Summarization Workshop, pages 74?81.
Elena Lloret and Manuel Palomar. 2009. A gradual com-
bination of features for building automatic summarisa-
tion systems. In Proceedings of TSD, pages 16?23.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2):4.
Lizhen Qu, Cigdem Toprak, Niklas jakob, and iryna
Gurevych. 2008. Sentence level subjectivity and sen-
timent analysis experiments in ntcir-7 moat challenge.
In Proceedings of NTCIR-7 Workshop meeting.
Karen Spa?rck Jones. 2007. Automatic summarising: The
State of the Art. Information Processing & Manage-
ment, 43(6):1449?1481.
V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe. 2004.
Evaluating an opinion annotation scheme using a new
multi-perspective question and answer corpus. In
AAAI Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1083?1086.
John Carroll Taras Zabibalov. 2008. Almost-
unsupervised cross-language opinion analysis at ntcis-
7. In Proceedings of NTCIR-7 Workshop meeting.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Language Resources and Evaluation, volume 39.
D. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
174
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 52?60,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Multilingual Sentiment Analysis using Machine Translation?
Alexandra Balahur and Marco Turchi
European Commission Joint Research Centre
Institute for the Protection and Security of the Citizen
Via E. Fermi 2749, Ispra, Italy
alexandra.balahur, marco.turchi@jrc.ec.europa.eu
Abstract
The past years have shown a steady growth
in interest in the Natural Language Process-
ing task of sentiment analysis. The research
community in this field has actively proposed
and improved methods to detect and classify
the opinions and sentiments expressed in dif-
ferent types of text - from traditional press ar-
ticles, to blogs, reviews, fora or tweets. A less
explored aspect has remained, however, the
issue of dealing with sentiment expressed in
texts in languages other than English. To this
aim, the present article deals with the prob-
lem of sentiment detection in three different
languages - French, German and Spanish - us-
ing three distinct Machine Translation (MT)
systems - Bing, Google and Moses. Our ex-
tensive evaluation scenarios show that SMT
systems are mature enough to be reliably em-
ployed to obtain training data for languages
other than English and that sentiment analysis
systems can obtain comparable performances
to the one obtained for English.
1 Introduction
Together with the increase in the access to tech-
nology and the Internet, the past years have shown
a steady growth of the volume of user-generated
contents on the Web. The diversity of topics cov-
ered by this data (mostly containing subjective and
opinionated content) in the new textual types such
as blogs, fora, microblogs, has been proven to be
of tremendous value to a whole range of applica-
tions, in Economics, Social Science, Political Sci-
ence, Marketing, to mention just a few. Notwith-
standing these proven advantages, the high quan-
tity of user-generated contents makes this informa-
tion hard to access and employ without the use of
automatic mechanisms. This issue motivated the
rapid and steady growth in interest from the Natural
Language Processing (NLP) community to develop
computational methods to analyze subjectivity and
sentiment in text. Different methods have been pro-
posed to deal with these phenomena for the distinct
types of text and domains, reaching satisfactory lev-
els of performance for English. Nevertheless, for
certain applications, such as news monitoring, the
information in languages other than English is also
highly relevant and cannot be disregarded. Addi-
tionally, systems dealing with sentiment analysis in
the context of monitoring must be reliable and per-
form at similar levels as the ones implemented for
English.
Although the most obvious solution to these is-
sues of multilingual sentiment analysis would be to
use machine translation systems, researchers in sen-
timent analysis have been reluctant to using such
technologies due to the low performance they used
to have. However, in the past years, the performance
of Machine Translation systems has steadily im-
proved. Open access solutions (e.g. Google Trans-
late1, Bing Translator2) offer more and more accu-
rate translations for frequently used languages.
Bearing these thoughts in mind, in this article
we study the manner in which sentiment analysis
can be done for languages other than English, using
Machine Translation. In particular, we will study
1http://translate.google.it/
2http://www.microsofttranslator.com/
52
this issue in three languages - French, German and
Spanish - using three different Machine Translation
systems - Google Translate, Bing Translator and
Moses (Koehn et al, 2007).
We employ these systems to obtain training and
test data for these three languages and subsequently
extract features that we employ to build machine
learning models using Support Vector Machines Se-
quential Minimal Optimization. We additionally
employ meta-classifiers to test the possibility to min-
imize the impact of noise (incorrect translations) in
the obtained data.
Our experiments show that machine translation
systems are mature enough to be employed for mul-
tilingual sentiment analysis and that for some lan-
guages (for which the translation quality is high
enough) the performance that can be attained is sim-
ilar to that of systems implemented for English.
2 Related Work
Most of the research in subjectivity and sentiment
analysis was done for English. However, there were
some authors who developed methods for the map-
ping of subjectivity lexicons to other languages. To
this aim, (Kim and Hovy, 2006) use a machine trans-
lation system and subsequently use a subjectivity
analysis system that was developed for English to
create subjectivity analysis resources in other lan-
guages. (Mihalcea et al, 2009) propose a method
to learn multilingual subjective language via cross-
language projections. They use the Opinion Finder
lexicon (Wilson et al, 2005) and use two bilin-
gual English-Romanian dictionaries to translate the
words in the lexicon. Since word ambiguity can ap-
pear (Opinion Finder does not mark word senses),
they filter as correct translations only the most fre-
quent words. The problem of translating multi-word
expressions is solved by translating word-by-word
and filtering those translations that occur at least
three times on the Web. Another approach in obtain-
ing subjectivity lexicons for other languages than
English was explored by Banea et al (Banea et al,
2008b). To this aim, the authors perform three dif-
ferent experiments, obtaining promising results. In
the first one, they automatically translate the anno-
tations of the MPQA corpus and thus obtain subjec-
tivity annotated sentences in Romanian. In the sec-
ond approach, they use the automatically translated
entries in the Opinion Finder lexicon to annotate a
set of sentences in Romanian. In the last experi-
ment, they reverse the direction of translation and
verify the assumption that subjective language can
be translated and thus new subjectivity lexicons can
be obtained for languages with no such resources.
Further on, another approach to building lexicons
for languages with scarce resources is presented by
Banea et al (Banea et al, 2008a). In this research,
the authors apply bootstrapping to build a subjectiv-
ity lexicon for Romanian, starting with a set of seed
subjective entries, using electronic bilingual dictio-
naries and a training set of words. They start with
a set of 60 words pertaining to the categories of
noun, verb, adjective and adverb from the transla-
tions of words in the Opinion Finder lexicon. Trans-
lations are filtered using a measure of similarity to
the original words, based on Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) scores. Yet
another approach to mapping subjectivity lexica to
other languages is proposed by Wan (2009), who
uses co-training to classify un-annotated Chinese re-
views using a corpus of annotated English reviews.
He first translates the English reviews into Chinese
and subsequently back to English. He then performs
co-training using all generated corpora. (Kim et al,
2010) create a number of systems consisting of dif-
ferent subsystems, each classifying the subjectivity
of texts in a different language. They translate a cor-
pus annotated for subjectivity analysis (MPQA), the
subjectivity clues (Opinion finder) lexicon and re-
train a Nave Bayes classifier that is implemented in
the Opinion Finder system using the newly gener-
ated resources for all the languages considered. Fi-
nally, (Banea et al, 2010) translate the MPQA cor-
pus into five other languages (some with a similar
ethimology, others with a very different structure).
Subsequently, they expand the feature space used in
a Nave Bayes classifier using the same data trans-
lated to 2 or 3 other languages. Their conclusion is
that by expanding the feature space with data from
other languages performs almost as well as training
a classifier for just one language on a large set of
training data.
Attempts of using machine translation in differ-
ent natural language processing tasks have not been
widely used due to poor quality of translated texts,
53
but recent advances in Machine Translation have
motivated such attempts. In Information Retrieval,
(Savoy and Dolamic, 2009) proposed a comparison
between Web searches using monolingual and trans-
lated queries. On average, the results show a drop
in performance when translated queries are used,
but it is quite limited, around 15%. For some lan-
guage pairs, the average result obtained is around
10% lower than that of a monolingual search while
for other pairs, the retrieval performance is clearly
lower. In cross-language document summarization,
(Wan et al, 2010; Boudin et al, 2010) combined
the MT quality score with the informativeness score
of each sentence in a set of documents to automat-
ically produce summary in a target language using
a source language texts. In (Wan et al, 2010), each
sentence of the source document is ranked accord-
ing both the scores, the summary is extracted and
then the selected sentences translated to the target
language. Differently, in (Boudin et al, 2010), sen-
tences are first translated, then ranked and selected.
Both approaches enhance the readability of the gen-
erated summaries without degrading their content.
3 Motivation and Contribution
The main motivation for the experiments we present
in this article is the known lack of resources and ap-
proaches for sentiment analysos in languages other
than English. Although, as we have seen in the
Related Work section, a few attempts were made
to build systems that deal with sentiment analysis
in other languages, they mostly employed bilingual
dictionaries and used unsupervised approaches. The
very few that employed supervised learning using
translated data have, in change, concentrated only
on the issue of sentiment classification and have dis-
regarded the impact of the translation quality and
the difference that the use of distinct translation sys-
tems can make in this settings. Moreover, such ap-
proaches have usually employed only simple ma-
chine learning algorithms. No attempt has been
made to study the use of meta-classifiers to enhance
the performance of the classification through the re-
moval of noise in the data.
Our main contribution in this article is the com-
parative study of multilingual sentiment analysis
performance using distinct machine translation sys-
tems, with varying levels of translation quality. In
this sense, we employ three different systems - Bing
Translator, Google Translate and Moses to translate
data from English to three languages - French, Ger-
man and Spanish. We subsequently study the perfor-
mance of classifying sentiment from the translated
data and different methods to minimize the effect of
noise in the data.
Our comparative results show, on the one hand,
that machine translation can be reliably used for
multilingual sentiment analysis and, on the other
hand, which are the main characteristics of the data
for such approaches to be successfully employed.
4 Dataset Presentation and Analysis
For our experiments, we employed the data provided
for English in the NTCIR 8 Multilingual Opinion
Analysis Task (MOAT)3. In this task, the organiz-
ers provided the participants with a set of 20 top-
ics (questions) and a set of documents in which sen-
tences relevant to these questions could be found,
taken from the New York Times Text (2002-2005)
corpus. The documents were given in two differ-
ent forms, which had to be used correspondingly,
depending on the task to which they participated.
The first variant contained the documents split into
sentences (6165 in total) and had to be used for
the task of opinionatedness, relevance and answer-
ness. In the second form, the sentences were also
split into opinion units (6223 in total) for the opin-
ion polarity and the opinion holder and target tasks.
For each of the sentences, the participants had to
provide judgments on the opinionatedness (whether
they contained opinions), relevance (whether they
are relevant to the topic). For the task of polar-
ity classification, the participants had to employ the
dataset containing the sentences that were also split
into opinion units (i.e. one sentences could contain
two/more opinions, on two/more different targets or
from two/more different opinion holders).
For our experiments, we employed the latter rep-
resentation. From this set, we randomly chose 600
opinion units, to serve as test set. The rest of opin-
ion units will be employed as training set. Subse-
quently, we employed the Google Translate, Bing
3http://research.nii.ac.jp/ntcir/ntcir-
ws8/permission/ntcir8xinhua-nyt-moat.html
54
Translator and Moses systems to translate, on the
one hand, the training set and on the other hand
the test set, to French, German and Spanish. Ad-
ditionally, we employed the Yahoo system to trans-
late only the test set into these three languages. Fur-
ther on, this translation of the test set by the Yahoo
service has been corrected by a person for all the
languages. This corrected data serves as Gold Stan-
dard4. Most of these sentences, however, contained
no opinion (were neutral). Due to the fact that the
neutral examples are majoritary and can produce a
large bias when classifying, we decided to eliminate
these examples and employ only the positive and
negative sentences in both the training, as well as
the test sets. After this elimination, the training set
contains 943 examples (333 positive and 610 nega-
tive) and the test set and Gold Standard contain 357
examples (107 positive and 250 negative).
5 Machine Translation
During the 1990?s the research community on Ma-
chine Translation proposed a new approach that
made use of statistical tools based on a noisy chan-
nel model originally developed for speech recogni-
tion (Brown et al, 1994). In the simplest form, Sta-
tistical Machine Translation (SMT) can be formu-
lated as follows. Given a source sentence written
in a foreign language f , the Bayes rule is applied
to reformulate the probability of translating f into a
sentence e written in a target language:
ebest = argmax
e
p(e|f) = argmax
e
p(f |e)pLM (e)
where p(f |e) is the probability of translating e to f
and pLM (e) is the probability of producing a fluent
sentence e. For a full description of the model see
(Koehn, 2010).
The noisy channel model was extended in differ-
ent directions. In this work, we analyse the most
popular class of SMT systems: PBSMT. It is an ex-
tension of the noisy channel model using phrases
rather than words. A source sentence f is segmented
4Please note that each sentence may contain more than one
opinion unit. In order to ensure a contextual translation, we
translated the whole sentences, not the opinion units separately.
In the end, we eliminate duplicates of sentences (due to the fact
that they contained multiple opinion units), resulting in around
400 sentences in the test and Gold Standard sets and 5700 sen-
tences in the training set
into a sequence of I phrases f I = {f1, f2, . . . fI}
and the same is done for the target sentence e, where
the notion of phrase is not related to any grammat-
ical assumption; a phrase is an n-gram. The best
translation ebest of f is obtained by:
ebest = argmax
e
p(e|f) = argmax
e
p(f |e)pLM (e)
= argmax
e
I?
i=1
?(fi|ei)
??d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating a
phrase ei into a phrase fi. d(ai ? bi?1) is the
distance-based reordering model that drives the sys-
tem to penalise significant reorderings of words dur-
ing translation, while allowing some flexibility. In
the reordering model, ai denotes the start position
of the source phrase that is translated into the ith
target phrase, and bi?1 denotes the end position of
the source phrase translated into the (i ? 1)th target
phrase. pLM (ei|e1 . . . ei?1) is the language model
probability that is based on the Markov?s chain as-
sumption. It assigns a higher probability to flu-
ent/grammatical sentences. ??, ?LM and ?d are
used to give a different weight to each element. For
more details see (Koehn et al, 2003).
Three different SMT systems were used to trans-
late the human annotated sentences: two existing
online services such as Google Translate and Bing
Translator5 and an instance of the open source
phrase-based statistical machine translation toolkit
Moses (Koehn et al, 2007).
To train our models based on Moses we used the
freely available corpora: Europarl (Koehn, 2005),
JRC-Acquis (Steinberger et al, 2006), Opus (Tiede-
mann, 2009), News Corpus (Callison-Burch et al,
2009). This results in 2.7 million sentence pairs for
English-French, 3.8 for German and 4.1 for Span-
ish. All the modes are optimized running the MERT
algorithm (Och, 2003) on the development part of
the News Corpus. The translated sentences are re-
cased and detokonized (for more details on the sys-
tem, please see (Turchi et al, 2012).
5http://translate.google.com/ and http://
www.microsofttranslator.com/
55
Performances of a SMT system are automati-
cally evaluated comparing the output of the system
against human produced translations. Bleu score
(Papineni et al, 2001) is the most used metric and it
is based on averaging n-gram precisions, combined
with a length penalty which penalizes short transla-
tions containing only sure words. It ranges between
0 and 1, and larger value identifies better translation.
6 Sentiment Analysis
In the field of sentiment analysis, most work has
concentrated on creating and evaluating methods,
tools and resources to discover whether a specific
?target?or ?object? (person, product, organization,
event, etc.) is ?regarded? in a positive or negative
manner by a specific ?holder? or ?source? (i.e. a per-
son, an organization, a community, people in gen-
eral, etc.). This task has been given many names,
from opinion mining, to sentiment analysis, review
mining, attitude analysis, appraisal extraction and
many others.
The issue of extracting and classifying sentiment
in text has been approached using different methods,
depending on the type of text, the domain and the
language considered. Broadly speaking, the meth-
ods employed can be classified into unsupervised
(knowledge-based), supervised and semi-supervised
methods. The first usually employ lexica or dictio-
naries of words with associated polarities (and val-
ues - e.g. 1, -1) and a set of rules to compute the
final result. The second category of approaches em-
ploy statistical methods to learn classification mod-
els from training data, based on which the test data
is then classified. Finally, semi-supervised methods
employ knowledge-based approaches to classify an
initial set of examples, after which they use different
machine learning methods to bootstrap new training
examples, which they subsequently use with super-
vised methods.
The main issue with the first approach is that ob-
taining large-enough lexica to deal with the vari-
ability of language is very expensive (if it is done
manually) and generally not reliable (if it is done
automatically). Additionally, the main problem of
such approaches is that words outside contexts are
highly ambiguous. Semi-supervised approaches, on
the other hand, highly depend on the performance of
the initial set of examples that is classified. If we are
to employ machine translation, the errors in translat-
ing this small initial set would have a high negative
impact on the subsequently learned examples. The
challenge of using statistical methods is that they re-
quire training data (e.g. annotated corpora) and that
this data must be reliable (i.e. not contain mistakes
or ?noise?). However, the larger this dataset is, the
less influence the translation errors have.
Since we want to study whether machine transla-
tion can be employed to perform sentiment analy-
sis for different languages, we employed statistical
methods in our experiments. More specifically, we
used Support Vector Machines Sequential Minimal
Optimization (SVM SMO) since the literature in the
field has confirmed it as the most appropriate ma-
chine learning algorithm for this task.
In the case of statistical methods, the most impor-
tant aspect to take into consideration is the manner
in which texts are represented - i.e. the features that
are extracted from it. For our experiments, we repre-
sented the sentences based on the unigrams and the
bigrams that were found in the training data. Al-
though there is an ongoing debate on whether bi-
grams are useful in the context of sentiment classi-
fication, we considered that the quality of the trans-
lation can also be best quantified in the process by
using these features (because they give us a measure
of the translation correctness, both regarding words,
as well as word order). Higher level n-grams, on the
other hand, would only produce more sparse feature
vectors, due to the high language variability and the
mistakes in the traslation.
7 Experiments
In order to test the performance of sentiment classi-
fication when using translated data, we performed a
series of experiments:
? In the first set of experiments, we trained an
SVM SMO classifier on the training data ob-
tained for each language, with each of the three
machine translations, separately (i.e. we gen-
erated a model for each of the languages con-
sidered, for each of the machine translation
systems employed). Subsequently, we tested
the models thus obtained on the correspond-
ing test set (e.g. training on the Spanish train-
56
ing set obtained using Google Translate and
testing on the Spanish test set obtained using
Google Translate) and on the Gold Standard for
the corresponding language (e.g. training on
the Spanish training set obtained using Google
Translate and testing on the Spanish Gold Stan-
dard). Additionally, in order to study the man-
ner in which the noise in the training data can
be removed, we employed two meta-classifiers
- AdaBoost and Bagging (with varying sizes of
the bag).
? In the second set of experiments, we combined
the translated data from all three machine trans-
lation systems for the same language and cre-
ated a model based on the unigram and bigram
features extracted from this data (e.g. we cre-
ated a Spanish training model using the uni-
grams and bigrams present in the training sets
generated by the translation of the training set
to Spanish by Google Translate, Bing Trans-
lator and Moses). We subsequently tested the
performance of the sentiment classification us-
ing the Gold Standard for the corresponding
language, represented using the features of this
model.
Table 1 presents the number of unigram and bi-
gram features employed in each of the cases.
In the following subsections, we present the re-
sults of these experiments.
7.1 Individual Training with Translated Data
In the first experiment, we translated the training
and test data from English to all the three other
languages considered, using each of the three ma-
chine translation systems. Subsequently, we rep-
resented, for each of the languages and translation
systems, the sentences as vectors, whose features
marked the presence/absence (1 or 0) of the uni-
grams and bigrams contained in the corresponding
trainig set (e.g. we obtained the unigrams and bi-
grams in all the sentences in the training set ob-
tained by translating the English training data to
Spanish using Google and subsequently represented
each sentence in this training set, as well as the test
set obtained by translating the test data in English to
Spanish using Google marking the presence of the
unigram and bigram features). In order to test the
approach on the Gold Standard (for each language),
we represented this set using the corresponding un-
igram and bigram features extracted from the cor-
responding training set (for the example given, we
represented each sentence in the Gold Standard by
marking the presence/absence of the unigrams and
bigrams from the training data for Spanish using
Google Translate).
The results of these experiments are presented in
Table 2, in terms of weighted F1 measure.
7.2 Joint Training with Translated Data
In the second set of experiments, we added together
all the translations of the training data obtained for
the same language, with the three different MT sys-
tems. Subsequently, we represented, for each lan-
guage in part, each of the sentences in the joint train-
ing corpus as vectors, whose features represented
the presence/absence of the unigrams and bigrams
contained in this corpus. In order to test the perfor-
mance of the sentiment classification, we employed
the Gold Standard for the corresponding language,
representing each sentence it contains according to
the presence or absence of the unigrams and bigrams
in the corresponding joint training corpus for that
language. Finally, we applied SVM SMO to classify
the sentences according to the polarity of the senti-
ment they contained. Additionally, we applied the
AdaBoost and Bagging meta-classifiers to test the
possibilities to minimize the impact of noise in the
data. The results are presented in Tables 3 and 4,
again, in terms of weighter F1 measure.
Language SMO AdaBoost M1 Bagging
To German 0.565? 0.563? 0.565?
To Spanish 0.419 0.494 0.511
To French 0.25 0.255 0.23
Table 3: For each language, each classifier has been
trained merging the translated data coming form differ-
ent SMT systems, and tested using the Gold Standard.
?Classifier is not able to discriminate between positive
and negative classes, and assigns most of the test points
to one class, and zero to the other.
8 Results and Discussion
Generally speaking, from our experiments using
SVM, we could see that incorrect translations imply
57
Bing Google T. Moses
To German 0.57? 0.572? 0.562?
To Spanish 0.392 0.511 0.448
To French 0.612? 0.571? 0.575?
Table 4: For each language, the SMO classifiers have
been trained merging the translated data coming form dif-
ferent SMT systems, and tested using independently the
translated test sets. ?Classifier is not able to discriminate
between positive and negative classes, and assigns most
of the test points to one class, and zero to the other.
an increment of the features, sparseness and more
difficulties in identifying a hyperplane which sepa-
rates the positive and negative examples in the train-
ing phase. Therefore, a low quality of the translation
leads to a drop in performance, as the features ex-
tracted are not informative enough to allow for the
classifier to learn.
From Table 2, we can see that:
a) There is a small difference between performances
of the sentiment analysis system using the English
and translated data, respectively. In the worst case,
there is a maximum drop of 8 percentages.
b) Adaboost is sensitive to noisy data, and it is
evident in our experiments where in general it does
not modify the SMO performances or there is a
drop. Vice versa, Bagging, reducing the variance
in the estimated models, produces a positive effect
on the performances increasing the F-score. These
improvements are larger using the German data,
this is due to the poor quality of the translated data,
which increases the variance in the data.
Looking at the results in Tables 3 and 4, we can
see that:
a) Adding all the translated training data together
drastically increases the noise level in the training
data, creating harmful effects in terms of clas-
sification performance: each classifier loses its
discriminative capability.
b) At language level, clearly the results depend
on the translation performance. Only for Spanish
(for which we have the highest Bleu score), each
classifies is able to properly learn from the training
data and try to properly assign the test samples. For
the other languages, translated data are so noisy
that the classifier is not able to properly learn the
correct information for the positive and the negative
classes, this results in the assignment of most of
the test points to one class and zero to the other. In
Table 3, for the French language we have significant
drop in performance, but the classifier is still able
to learn something from the training and assign the
test points to both the classes.
c) The results for Spanish presented in Table 3
confirm the capability of Bagging to reduce the
model variance and increase the performance in
classification.
d) At system level in Table 4, there is no evidence
that better translated test set alows better classifica-
tion performance.
9 Conclusions and Future Work
In this work we propose an extensive evaluation of
the use of translated data in the context of sentiment
analysis. Our findings show that SMT systems are
mature enough to produce reliably training data for
languages other than English. The gap in classifi-
cation performance between systems trained on En-
glish and translated data is minimal, with a maxi-
mum of 8
Working with translated data implies an incre-
ment number of features, sparseness and noise in the
data points in the classification task. To limit these
problems, we test three different classification ap-
proaches showing that bagging has a positive impact
in the results.
In future work, we plan to investigate different
document representations, in particular we believe
that the projection of our documents in space where
the features belong to a sentiment lexical and in-
clude syntax information can reduce the impact of
the translation errors. As well we are interested to
evaluate different term weights such as tf-idf.
Acknowledgments
The authors would like to thank Ivano Azzini, from
the BriLeMa Artificial Intelligence Studies, for the
advice and support on using meta-classifiers. We
would also like to thank the reviewers for their use-
ful comments and suggestions on the paper.
58
References
Turchi, M. and Atkinson, M. and Wilcox, A. and Craw-
ley, B. and Bucci, S. and Steinberger, R. and Van der
Goot, E. 2012. ONTS: ?Optima? News Translation
System.. Proceedings of EACL 2012.
Banea, C., Mihalcea, R., and Wiebe, J. 2008. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources.. Proceedings of the
Conference on Language Resources and Evaluations
(LREC 2008), Maraakesh, Marocco.
Banea, C., Mihalcea, R., Wiebe, J., and Hassan, S.
2008. Multilingual subjectivity analysis using ma-
chine translation. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), 127-135, Honolulu, Hawaii.
Banea, C., Mihalcea, R. and Wiebe, J. 2010. Multilin-
gual subjectivity: are more languages better?. Pro-
ceedings of the International Conference on Computa-
tional Linguistics (COLING 2010), p. 28-36, Beijing,
China.
Boudin, F. and Huet, S. and Torres-Moreno, J.M. and
Torres-Moreno, J.M. 2010. A Graph-based Ap-
proach to Cross-language Multi-document Summa-
rization. Research journal on Computer science
and computer engineering with applications (Polibits),
43:113?118.
P. F. Brown, S. Della Pietra, V. J. Della Pietra and R. L.
Mercer. 1994. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation, Computa-
tional Linguistics 19:263?311.
C. Callison-Burch, and P. Koehn and C. Monz and J.
Schroeder. 2009. Findings of the 2009 Workshop on
Statistical Machine Translation. Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 1?28. Athens, Greece.
Deerwester, S., Dumais, S., Furnas, G. W., Landauer, T.
K., and Harshman, R. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 3(41).
Kim, S.-M. and Hovy, E. 2006. Automatic identification
of pro and con reasons in online reviews. Proceedings
of the COLING/ACL Main Conference Poster Ses-
sions, pages 483490.
Kim, J., Li, J.-J. and Lee, J.-H. 2006. Evaluating
Multilanguage-Comparability of Subjectivity Analysis
Systems. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
595603, Uppsala, Sweden, 11-16 July 2010.
P. Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
Machine Translation Summit X, pages 79-86. Phuket,
Thailand.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn and F. J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation, Proceedings of the North
America Meeting on Association for Computational
Linguistics, 48?54.
P. Koehn and H. Hoang and A. Birch and C. Callison-
Burch and M. Federico and N. Bertoldi and B. Cowan
and W. Shen and C. Moran and R. Zens and C. Dyer
and O. Bojar and A. Constantin and E. Herbst 2007.
Moses: Open source toolkit for statistical machine
translation. Proceedings of the Annual Meeting of the
Association for Computational Linguistics, demon-
stration session, pages 177?180. Columbus, Oh, USA.
Mihalcea, R., Banea, C., and Wiebe, J. 2009. Learn-
ing multilingual subjective language via cross-lingual
projections. Proceedings of the Conference of the An-
nual Meeting of the Association for Computational
Linguistics 2007, pp.976-983, Prague, Czech Repub-
lic.
F. J. Och 2003. Minimum error rate training in statisti-
cal machine translation. Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 160?167. Sapporo, Japan.
K. Papineni and S. Roukos and T. Ward and W. J. Zhu
2001. BLEU: a method for automatic evaluation of
machine translation. Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318. Philadelphia, Pennsylvania.
J. Savoy, and L. Dolamic. 2009. How effective is
Google?s translation service in search?. Communi-
cations of the ACM, 52(10):139?143.
R. Steinberger and B. Pouliquen and A. Widiger and C.
Ignat and T. Erjavec and D. Tufis? and D. Varga. 2006.
The JRC-Acquis: A multilingual aligned parallel cor-
pus with 20+ languages. Proceedings of the 5th Inter-
national Conference on Language Resources and Eval-
uation, pages 2142?2147. Genova, Italy.
J. Tiedemann. 2009. News from OPUS-A Collection of
Multilingual Parallel Corpora with Tools and Inter-
faces. Recent advances in natural language processing
V: selected papers from RANLP 2007, pages 309:237.
Wan, X. and Li, H. and Xiao, J. 2010. Cross-language
document summarization based on machine transla-
tion quality prediction. Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 917?926.
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. Proceedings of HLT-EMNLP 2005, pp.347-354,
Vancouver, Canada.
59
Language SMT system Nr. of unigrams Nr. of bigrams
French
Bing 7441 17870
Google 7540 18448
Moses 6938 18814
Bing+Google+Moses 9082 40977
German
Bing 7817 16216
Google 7900 16078
Moses 7429 16078
Bing+Google+Moses 9371 36556
Spanish
Bing 7388 17579
Google 7803 18895
Moses 7528 18354
Bing+Google+Moses 8993 39034
Table 1: Features employed.
Language SMT Test Set SMO AdaBoost M1 Bagging Bleu Score
English GS 0.685 0.685 0.686
To German
Bing
GS 0.641 0.631 0.648
Tr 0.658 0.636 0.662 0.227
To German
Google T.
GS 0.646 0.623 0.674
Tr 0.687 0.645 0.661 0.209
To German
Moses
GS 0.644 0.644 0.676
Tr 0.667 0.667 0.674 0.17
To Spanish
Bing
GS 0.656 0.658 0.646
Tr 0.633 0.633 0.633 0.316
To Spanish
Google T.
GS 0.653 0.653 0.665
Tr 0.636 0.667 0.636 0.341
To Spanish
Moses
GS 0.664 0.664 0.671
Tr 0.649 0.649 0.663 0.298
To French
Bing
GS 0.644 0.645 0.664
Tr 0.644 0.649 0.652 0.243
To French
Google T.
GS 0.64 0.64 0.659
Tr 0.652 0.652 0.678 0.274
To French
Moses
GS 0.633 0.633 0.645
Tr 0.666 0.666 0.674 0.227
Table 2: Results obtained using the individual training sets obtained by translating with each of the three considered
MT systems, to each of the three languages considered.
60
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 120?128,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis in Social Media Texts
Alexandra Balahur
European Commission Joint Research Centre
Vie E. Fermi 2749
21027 Ispra (VA), Italy
alexandra.balahur@jrc.ec.europa.eu
Abstract
This paper presents a method for sentiment
analysis specifically designed to work with
Twitter data (tweets), taking into account their
structure, length and specific language. The
approach employed makes it easily extendible
to other languages and makes it able to pro-
cess tweets in near real time. The main contri-
butions of this work are: a) the pre-processing
of tweets to normalize the language and gener-
alize the vocabulary employed to express sen-
timent; b) the use minimal linguistic process-
ing, which makes the approach easily portable
to other languages; c) the inclusion of higher
order n-grams to spot modifications in the po-
larity of the sentiment expressed; d) the use of
simple heuristics to select features to be em-
ployed; e) the application of supervised learn-
ing using a simple Support Vector Machines
linear classifier on a set of realistic data. We
show that using the training models generated
with the method described we can improve
the sentiment classification performance, irre-
spective of the domain and distribution of the
test sets.
1 Introduction
Sentiment analysis is the Natural Language Process-
ing (NLP) task dealing with the detection and clas-
sification of sentiments in texts. Usually, the classes
considered are ?positive?, ?negative? and ?neutral?,
although in some cases finer-grained categories are
added (e.g. ?very positive? and ?very negative?) or
only the ?positive? and ?negative? classes are taken
into account. Another related task - emotion detec-
tion - concerns the classification of text into several
classes of emotion, usually the basic ones, as de-
scribed by Paul Ekman (Ekman, 1992). Although
different in some ways, some of the research in the
field has considered these tasks together, under the
umbrella of sentiment analysis.
This task has received a lot of interest from the re-
search community in the past years. The work done
regarded the manner in which sentiment can be clas-
sified from texts pertaining to different genres and
distinct languages, in the context of various applica-
tions, using knowledge-based, semi-supervised and
supervised methods (Pang and Lee, 2008). The re-
sult of the analyses performed have shown that the
different types of text require specialized methods
for sentiment analysis, as, for example, sentiments
are not conveyed in the same manner in newspaper
articles and in blogs, reviews, forums or other types
of user-generated contents (Balahur et al, 2010).
In the light of these findings, dealing with sen-
timent analysis in Twitter requires an analysis of
the characteristics of such texts and the design of
adapted methods.
Additionally, the sentiment analysis method em-
ployed has to consider the requirements of the fi-
nal application in which it will be used. There is
an important difference between deploying a system
working for languages such as English, for which
numerous linguistic resources and analysis tools ex-
ist and a system deployed for languages with few
such tools or one that is aimed at processing data
from a large set of languages. Finally, a sentiment
analysis system working with large sets of data (such
as the one found in Twitter) must be able to process
texts fast. Therefore, using highly complex methods
may delay producing useful results.
In the light of these considerations, this paper
120
presents a method for sentiment analysis that takes
into account the special structure and linguistic con-
tent of tweets. The texts are pre-processed in or-
der to normalize the language employed and re-
move noisy elements. Special usage of language
(e.g. repeated punctuation signs, repeated letters)
are marked as special features, as they contribute to
the expressivity of the text in terms of sentiment.
Further on, sentiment-bearing words, as they are
found in three highly-accurate sentiment lexicons -
General Inquirer (GI) (Stone et al, 1966), Linguis-
tic Inquiry and Word Count (LIWC) (Tausczik and
Pennebaker, 2010) and MicroWNOp (Cerini et al,
2007) - are replaced with unique labels, correspod-
ing to their polarity. In the same manner, modifiers
(negations, intensifiers and diminishers) are also re-
placed with unique labels representing their seman-
tic class. Finally, we employ supervised learning
with Support Vector Machines Sequential Minimal
Optimization (SVM SMO) (Platt, 1998) using a
simple, linear kernel (to avoid overfitting of data)
and the unigrams and bigrams from the training set
as features. We obtain the best results by using
unique labels for the affective words and the mod-
ifiers, unigrams and bigrams as features and posing
the condition that each feature considered in the su-
pervised learning process be present in the training
corpora at least twice.
The remainder of this article is structured as fol-
lows: Section 2 gives an overview of the related
work. In Section 3, we present the motivations and
describe the contributions of this work. In the fol-
lowing section, we describe in detail the process fol-
lowed to pre-process the tweets and build the classi-
fication models. In Section 5, we present the results
obtained using different datasets and combinations
of features and discuss their causes and implications.
Finally, Section 6 summarizes the main findings of
this work and sketches the lines for future work.
2 Related Work
One of the first studies on the classification of po-
larity in tweets was (Go et al, 2009). The au-
thors conducted a supervised classification study on
tweets in English, using the emoticons (e.g. ?:)?,
?:(?, etc.) as markers of positive and negative tweets.
(Read, 2005) employed this method to generate a
corpus of positive tweets, with positive emoticons
?:)?, and negative tweets with negative emoticons
?:(?. Subsequently, they employ different supervised
approaches (SVM, Na??ve Bayes and Maximum En-
tropy) and various sets of features and conclude that
the simple use of unigrams leads to good results, but
it can be slightly improved by the combination of
unigrams and bigrams.
In the same line of thinking, (Pak and Paroubek,
2010) also generated a corpus of tweets for sen-
timent analysis, by selecting positive and negative
tweets based on the presence of specific emoticons.
Subsequently, they compare different supervised ap-
proaches with n-gram features and obtain the best
results using Na??ve Bayes with unigrams and part-
of-speech tags.
Another approach on sentiment analysis in tweet
is that of (Zhang et al, 2011). Here, the authors em-
ploy a hybrid approach, combining supervised learn-
ing with the knowledge on sentiment-bearing words,
which they extract from the DAL sentiment dictio-
nary (Whissell, 1989). Their pre-processing stage
includes the removal of retweets, translation of ab-
breviations into original terms and deleting of links,
a tokenization process, and part-of-speech tagging.
They employ various supervised learning algorithms
to classify tweets into positive and negative, using
n-gram features with SVM and syntactic features
with Partial Tree Kernels, combined with the knowl-
edge on the polarity of the words appearing in the
tweets. The authors conclude that the most impor-
tant features are those corresponding to sentiment-
bearing words. Finally, (Jiang et al, 2011) classify
sentiment expressed on previously-given ?targets?
in tweets. They add information on the context of
the tweet to its text (e.g. the event that it is related
to). Subsequently, they employ SVM and General
Inquirer and perform a three-way classification (pos-
itive, negative, neutral).
3 Motivation and Contribution
As we have seen in the previous section, several im-
portant steps have already been taken into analyzing
the manner in which sentiment can be automatically
detected and classified from Twitter data. The re-
search we described in previous section has already
dealt with some of the issues that are posed by short,
121
informal texts, such as the tweets. However, these
small snippets of text have several liguistic peculiar-
ities that can be employed to improve the sentiment
classification performance. We describe these pecu-
liarities below:
? Tweets are short, user-generated text that may
contain no more than 140 characters (strongly
related to the standard 160-character length of
SMS 1). Users are marked with the ?@? sign
and topics with the ?#? (hashtag) sign.
? In general, the need to include a large quantity
of information in small limit of characters leads
to the fact that tweets sometimes have no gram-
matical structure, contain misspellings and ab-
breviations.
? Some of the tweets are simply posted from
the websites of news providers (news agencies,
newspapers) and therefore they contain only ti-
tles of news. However, subjective tweets, in
which users comment on an event, are highly
marked by sentiment-bearing expressions, ei-
ther in the form of affective words, or by em-
ployins specific modalities - e.g. the use of
capital letters or repeated punctuation signs to
stress upon specific words. Most of the times,
these words are sentiment-bearing ones.
? The language employed in subjective tweets in-
cludes a specific slang (also called ?urban ex-
pressions? 2) and emoticons (graphical expres-
sions of emotions through the use of punctua-
tion signs).
? Most of the times, the topic that is discusses
in the tweets is clearly marked using hashtags.
Thus, there is no need to employ very complex
linguistic tools to determine it.
? In major events, the rate of tweets per minute
commenting or retweeting information sur-
passes the rate of thousands per minute.
? Twitter is available in more than 30 languages.
However, users tweet in more than 80 lan-
guages. The information it contains can be use-
ful to obtain information and updates about, for
1http://en.wikipedia.org/wiki/Twitter
2http://www.urbandictionary.com/
example, crisis events 3, in real time. In order to
benefit from this, however, a system processing
these texts has to be easily adaptable to other
languages and it has to work in near real time.
Bearing this in mind, the main contributions we
bring in this paper are:
1. The pre-processing of tweets to normalize the
language and generalize the vocabulary em-
ployed to express sentiment. At this stage, we
take into account the linguistic peculiarities of
tweets, regarding spelling, use of slang, punc-
tuation, etc., and also replace the sentiment-
bearing words from the training data with a
unique label. In this way, the sentence ?I love
roses.? will be equivalent to the sentence ?I like
roses.?, because ?like? and ?love? are both pos-
itive words according to the GI dictionary. If
example 1 is contained in the training data and
example 2 is contained in the test data, replac-
ing the sentiment-bearing word with a general
label increases the chance to have example 2
classified correctly. In the same line of thought,
we also replaced modifiers with unique corre-
sponding labels.
2. The use of minimal linguistic processing,
which makes the approach easily portable to
other languages. We employ only tokenization
and do not process texts any further. The reason
behind this choice is that we would like the fi-
nal system to work in a similar fashion for as
many languages as possible and for some of
them, little or no tools are available.
3. The inclusion of bigrams to spot modifications
in the polarity of the sentiment expressed. As
such, we can learn general patterns of senti-
ment expression (e.g. ?negation positive?, ?in-
tensifier negative?, etc.).
4. The use of simple heuristics to select features
to be employed. Although feature selection al-
gorithms are easy to apply when employing a
data mining environment, the final choice is in-
fluenced by the data at hand and it is difficult to
3http://blog.twitter.com/2012/10/hurricane-sandy-
resources-on-twitter.html
122
employ on new sets of data. After performing
various tests, we chose to select the features to
be employed in the classification model based
on the condition that they should occur at least
once in the training set.
5. The application of supervised learning using a
simple Support Vector Machines linear classi-
fier on a set of realistic data.
We show that using the training models generated
with the method described we can improve the sen-
timent classification performance, irrespective of the
domain and distribution of the test sets.
4 Sentiment Analysis in Tweets
Our sentiment analysis system is based on a hybrid
approach, which employs supervised learning with a
Support Vector Machines Sequential Minimal Opti-
mization (Platt, 1998) linear kernel, on unigram and
bigram features, but exploiting as features sentiment
dictionaries, emoticon lists, slang lists and other so-
cial media-specific features. We do not employ any
specific language analysis software. The aim is to
be able to apply, in a straightforward manner, the
same approach to as many languages as possible.
The approach can be extended to other languages by
using similar dictionaries that have been created in
our team. They were built using the same dictio-
naries we employ in this work and their corrected
translation to Spanish. The new sentiment dictionar-
ies were created by simultaneously translating from
these two languages to a third one and considering
the intersection of the trainslations as correct terms.
Currently, new such dictionaries have been created
for 15 other languages.
The sentiment analysis process contains two
stages: pre-processing and sentiment classification.
4.1 Tweet Pre-processing
The language employed in Social Media sites is dif-
ferent from the one found in mainstream media and
the form of the words employed is sometimes not
the one we may find in a dictionary. Further on,
users of Social Media platforms employ a special
?slang? (i.e. informal language, with special expres-
sions, such as ?lol?, ?omg?), emoticons, and often
emphasize words by repeating some of their letters.
Additionally, the language employed in Twitter has
specific characteristics, such as the markup of tweets
that were reposted by other users with ?RT?, the
markup of topics using the ?#? (hash sign) and of
the users using the ?@? sign.
All these aspects must be considered at the time of
processing tweets. As such, before applying super-
vised learning to classify the sentiment of the tweets,
we preprocess them, to normalize the language they
contain. The pre-processing stage contains the fol-
lowing steps:
? Repeated punctuation sign normalization
In the first step of the pre-processing, we detect
repetitions of punctuation signs (?.?, ?!? and
???). Multiple consecutive punctuation signs
are replaced with the labels ?multistop?, for
the fullstops, ?multiexclamation? in the case of
exclamation sign and ?multiquestion? for the
question mark and spaces before and after.
? Emoticon replacement
In the second step of the pre-processing, we
employ the annotated list of emoticons from
SentiStrength4 and match the content of the
tweets against this list. The emoticons found
are replaced with their polarity (?positive? or
?negative?) and the ?neutral? ones are deleted.
? Lower casing and tokenization.
Subsequently, the tweets are lower cased and
split into tokens, based on spaces and punctua-
tion signs.
? Slang replacement
The next step involves the normalization of the
language employed. In order to be able to
include the semantics of the expressions fre-
quently used in Social Media, we employed the
list of slang from a specialized site 5.
? Word normalization
At this stage, the tokens are compared to entries
in Rogets Thesaurus. If no match is found, re-
peated letters are sequentially reduced to two or
one until a match is found in the dictionary (e.g.
4http://sentistrength.wlv.ac.uk/
5http://www.chatslang.com/terms/social media
123
?perrrrrrrrrrrrrrrrrrfeeect? becomes ?perrfeect?,
?perfeect?, ?perrfect? and subsequently ?per-
fect?). The words used in this form are maked
as ?stressed?.
? Affect word matching
Further on, the tokens in the tweet are matched
against three different sentiment lexicons: GI,
LIWC and MicroWNOp, which were previ-
ously split into four different categories (?pos-
itive?, ?high positive?, ?negative? and ?high
negative?). Matched words are replaced with
their sentiment label - i.e. ?positive?, ?nega-
tive?, ?hpositive? and ?hnegative?. A version
of the data without these replacements is also
maintained, for comparison purposes.
? Modifier word matching
Similar to the previous step, we employ a list
of expressions that negate, intensify or dimin-
ish the intensity of the sentiment expressed to
detect such words in the tweets. If such a word
is matched, it is replaced with ?negator?, ?in-
tensifier? or ?diminisher?, respectively. As in
the case of affective words, a version of the data
without these replacements is also maintained,
for comparison purposes.
? User and topic labeling
Finally, the users mentioned in the tweet, which
are marked with ?@?, are replaced with ?PER-
SON? and the topics which the tweet refers to
(marked with ?#?) are replaced with ?TOPIC?.
4.2 Sentiment Classification of Tweets
Once the tweets are pre-processed, they are passed
on to the sentiment classification module. We em-
ployed supervised learning using SVM SMO with a
linear kernel, based on boolean features - the pres-
ence or absence of n-grams (unigrams, bigrams and
unigrams plus bigrams) determined from the train-
ing data (tweets that were previousely pre-processed
as described above). Bigrams are used specifically
to spot the influence of modifiers (negations, inten-
sifiers, diminishers) on the polarity of the sentiment-
bearing words. We tested the approach on differ-
ent datasets and dataset splits, using the Weka data
mining software 6. The training models are built on
a cluster of computers (4 cores, 5000MB of mem-
ory each). However, the need for such extensive re-
sources is only present at the training stage. Once
the feature set is determined and the models are built
using Weka, new examples must only be represented
based on the features extracted from the training set
and the classification is a matter of miliseconds.
The different evaluations scenarios and results are
presented in the following section.
5 Evaluation and Discussion
Although the different steps included to eliminate
the noise in the data and the choice of features have
been refined using our in-house gathered Twitter
data, in order to evaluate our approach and make it
comparable to other methods, we employ three dif-
ferent data sets, which are described in detail in the
following subsections.
5.1 Data Sets
? SemEval 2013 Data
The first one is the data provided for training
for the upcoming SemEval 2013 Task 2 ?Sen-
timent Analysis from Twitter? 7. The initial
training data has been provided in two stages:
1) sample datasets for the first task and the sec-
ond task and 2) additional training data for the
two tasks. We employ the joint sample datasets
as test data (denoted as t?) and the data released
subsequently as training data (denoted as T?).
We employ the union of these two datasets to
perform cross-validation experiments (the joint
dataset is denoted as T ? +t?. The character-
istics of the dataset are described in Table 1.
On the last column, we also include the base-
line in terms of accuracy, which is computed as
the number of examples of the majoritary class
over the total number of examples:
? Set of tweets labeled with basic emotions.
The set of emotion-annotated tweets by (Mo-
hammad, 2012), which we will denote as
TweetEm. It contains 21051 tweets anno-
tated according to the Ekman categories of ba-
6http://www.cs.waikato.ac.nz/ml/weka/
7http://www.cs.york.ac.uk/semeval-2013/task2/
124
sic emotion - anger, disgust, fear, joy, sadness,
surprise. We employ this dataset to test the re-
sults of our best-performing configurations on
the test set. This set contains a total of 21051
tweets (anger - 1555, disgust - 761, fear - 2816,
joy - 8240, sadness - 3830, surprise - 3849). As
mentioned in the paper by (Mohammad, 2012),
a system that would guess the classes, would
perfom at aroung 49.9% accuracy.
? Set of short blog sentences labeled with basic
emotions.
The set of blog sentences employed by (Aman
and Szpakowicz, 2007), which are annotated
according to the same basic emotions identi-
fied by Paul Ekman, with the difference that the
?joy? category is labeled as ?happy?. This test
set contains also examples which contain no
emotions. These sentences were removed. We
will denote this dataset as BlogEm. This set
contains 1290 sentences annotated with emo-
tion (anger - 179, disgust - 172, fear - 115, joy -
536, sadness - 173, surprise - 115). We can con-
sider as baseline the case in which all the ex-
amples are assigned to the majority class (joy),
which would lead to an accuracy of 41.5%.
Data #Tweet #Pos. #Neg. #Neu. Bl%
T* 19241 4779 2343 12119 62
t* 2597 700 393 1504 57
T*+t* 21838 5479 2736 13623 62
Table 1: Characteristics of the training (T*), testing (t*)
and joint training and testing datasets.
5.2 Evaluation and Results
In order to test our sentiment analysis approach, we
employed the datasets described above. In the case
of the SemEval data, we performed an exhaustive
evaluation of the possible combination of features
to be employed. We tested the entire dataset of
tweets (T*+t*) using 10-fold cross-validation. The
first set of evaluations concerned the use of the pre-
processed tweets in which the affective words and
modifiers were have not been replaced. The com-
bination of features tested were: unigrams (U ), bi-
grams (B), unigrams and bigrams together (U +B)
and unigrams and bigrams together, selecting only
the features that appear at least twice in the data
(U +B+FS). The second set of evaluations aimed
at quantifying the difference in performance when
the affective words and the modifiers were replaced
with generic labels. We tested the best performing
approaches from the first set of evaluations (U + B
and U +B+FS), by replacing the words that were
found in the affect dictionaries and the modifiers
with their generic labels. These evaluations are de-
noted as U + B + D and U + B + D + FS. The
results of these evaluations are shown in Table 2.
Features 10-f-CV T*+t*
U 71.82
B 66.30
U +B 82.01
U +B +D 81.15
U +B + FS 74.00
U +B +D + FS 85.07
Table 2: Results in terms of accuracy for 10-fold cross-
validation using different combinations of features for the
sentiment classification of tweets on the entire set of Se-
mEval 2013 training data.
The same experiments are repeated by employing
T* as training data and t* as test data. The aim of
these experiments is to test how well the method can
perform on new data. The results of these evalu-
ations are shown in Table 3. In order to test if in-
Features Train(T*) & test(t*)
U 74.90
B 63.27
U +B 77.00
U +B +D 76.45
U +B + FS 75.69
U +B +D + FS 79.97
Table 3: Results in terms of accuracy for the different
combination of features for the sentiment classification
of tweets, using T* as training and t* as test set.
deed the use of sentiment dictionaries, modifiers and
the simple feature selection method improves on the
best performing approach that does not employ these
additional features, we tested both the approaches on
the TweetEm and BlogEm datasets. In this case,
125
however, the classification is done among 6 differ-
ent classes of emotions. Although the results are
lower(as it can be seen in Table 4, they are compara-
ble to those obtained by (Mohammad, 2012) (when
using U+B) and show an improvement when using
the affect dictionaries and simple feature selection.
They also confirm the fact that the best performance
on the data is obtained replacing the modifiers and
the words found in affect dictionaries with generic
labels, using unigrams and bigrams as and eliminat-
ing those n-grams that appear only once.
Features Tweet Em Blog Em
U +B 49.00 51.08
U +B +D + FS 51.08 53.70
Table 4: Results in terms of accuracy for the different
combination of features for the emotion classification of
tweets and short blog sentences.
The results obtained confirm that the use of uni-
gram and bigram features (appearing at least twice)
with generalized affective words and modifiers ob-
tains the best results. Although there is a signifi-
cant improvement in the accuracy of the classifica-
tion, the most important difference in the classifica-
tion performance is given by the fact that using this
combination, the classifier is no longer biased by the
class with the highest number of examples. We can
notice this for the case of tweets, for which the con-
fusion matrices are presented in Table 5 and Table
6. In the table header, the correspondence is: a =
joy, b = fear, c = surprise, d = anger, e = disgust, f
= sadness. In the first case, the use of unigrams and
bigrams leads to the erroneous classification of ex-
amples to the majoritary class. When employing the
features in which affective words and modifiers have
been replaced with generic labels, the results are not
only improved, but they classifier is less biased to-
wards the majoritary class. In this case, the incorrect
assignments are made to classes that are more sim-
ilar in vocabulary (e.g. anger - disgust, anger - sad-
ness). In the case of surprise, examples relate both
to positive, as well as negative surprises. Therefore,
there is a similarity in the vocabulary employed to
both these classes.
a b c d e f
a 5879 178 865 246 349 723
b 657 1327 339 67 59 367
c 1243 248 1744 123 129 362
d 549 189 79 419 48 271
e 167 55 45 89 160 245
f 570 405 611 625 233 1386
Table 5: Confusion matrix for the emotion classification
of the TweetEm dataset employing the sentiment dictio-
naries.
a b c d e f
a 6895 252 395 57 20 622
b 1384 861 207 49 11 302
c 1970 147 1258 39 13 421
d 884 133 88 101 18 332
e 433 54 60 32 40 142
f 2097 192 287 72 23 1160
Table 6: Confusion matrix for the emotion classification
of the TweetEm dataset without employing the senti-
ment dictionaries.
5.3 Discussion
From the results obtained, we can conclude that, on
the one hand, the best features to be employed in
sentiment analysis in tweets are unigrams and bi-
grams together. Secondly, we can see that the use of
generalizations, by employing unique labels to de-
note sentiment-bearing words and modifiers highly
improves the performance of the sentiment classi-
fication. The usefulness of pre-processing steps is
visible from the fact that among the bigrams that
were extracted from the training data we can find
the unique labels employed to mark the use of re-
peated punctuation signs, stressed words, affective
words and modifiers and combinations among them.
Interesting bigrams that were discovered using these
generalizations are, e.g. ?negative multiexclama-
tion?, ?positive multiexclamation?, ?positive multi-
stop? - which is more often found in negative tweets
-,?negator positive?, ?diminisher positive?, ?mostly
diminisher?, ?hnegative feeling?, ?hnegative day?,
?eat negative?,?intensifier hnegative?. All these ex-
tracted features are very useful to detect and classify
sentiment in tweets and most of them would be ig-
nored if the vocabulary were different in the train-
126
ing and test data or if, for example, a stressed word
would be written under different forms or a punctu-
ation sign would be repeated a different number of
times. We can see that the method employed obtains
good results, above the ones reported so far with the
state-of-the-art approaches. We have seen that the
use of affect and modifier lexica generalization has
an impact on both the quantitative performance of
the classification, as well as on the quality of the re-
sults, making the classifier less biased towards the
class with a significantly larger number of exam-
ples. In practice, datasets are not balanced, so it is
imporant that a classifier is able to assign (even in-
correctly) an example to a class that is semantically
similar and not to a class with totally opposite affec-
tive orientation. In this sense, as we have seen in the
detailed results obtained on the TweetEm dataset,
it is preferable that, e.g. the examples pertaining to
the emotion classes of anger and sadness are mis-
takenly classified as the other. However, it is not
acceptable to have such a high number of examples
from these classes labeled as ?joy?. Finally, by in-
specting some of the examples in the three datasets,
we noticed that a constant reason for error remains
the limited power of the method to correctly spot the
scope of the negations and modifiers. As such, we
plan to study the manner in which skip-bigrams (bi-
grams made up of non-consecutive tokens) can be
added and whether or not they will contribute to (at
least partially) solve this issue.
6 Conclusions and Future Work
In this article, we presented a method to classify
the sentiment in tweets, by taking into account their
peculiarities and adapting the features employed to
their structure and content. Specifically, we em-
ployed a pre-processing stage to normalize the lan-
guage and generalize the vocabulary employed to
express sentiment. This regarded spelling, slang,
punctuation, etc., and the use of sentiment dictio-
naries and modifier lists to generalize the patterns
of sentiment expression extracted from the training
data. We have shown that the use of such general-
ized features significantly improves the results of the
sentiment classification,when compared to the best-
performing approaches that do not use affect dictio-
naries. Additionally, we have shown that we can
obtain good results even though we employ min-
imal linguistic processing. The advantage of this
approach is that it makes the method easily appli-
cable to other languages. Finally, we have shown
that the use of a simple heuristic, concerning filter-
ing out features that appear only once, improves the
results. As such, the method is less dependent on the
dataset on which the classification model is trained
and the vocabulary it contains. Finally, we employed
a simple SVM SMO linear classifier to test our ap-
proach on three different data sets. Using such an
approach avoids overfitting the data and, as we have
shown, leads to comparable performances on differ-
ent datasets. In future work, we plan to evaluate
the use of higher-order n-grams (3-grams) and skip-
grams to extract more complex patterns of sentiment
expressions and be able to identify more precisely
the scope of the negation. Additionally, we plan to
evaluate the influence of deeper linguistic process-
ing on the results, by performing stemming, lem-
matizing and POS-tagging. Further on, we would
like to extend our approach on generalizing the se-
mantic classes of words and employing unique la-
bels to group them (e.g. label mouse, cat and dog as
?animal?). Finally, we would like to study the per-
formance of our approach in the context of tweets
related to specific news, in which case these short
texts can be contextualized by adding further con-
tent from other information sources.
References
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Proceedings of the
10th international conference on Text, speech and di-
alogue, TSD?07, pages 196?205, Berlin, Heidelberg.
Springer-Verlag.
Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,
Vanni Zavarella, Erik van der Goot, Matina Halkia,
Bruno Pouliquen, and Jenya Belyaeva. 2010. Sen-
timent analysis in the news. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini, 2007. Language resources and lin-
127
guistic theory: Typology, second language acquisition,
English linguistics., chapter Micro-WNOp: A gold
standard for the evaluation of automatically compiled
lexical resources for opinion mining. Franco Angeli
Editore, Milano, IT.
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3-4):169?200, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 151?160, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Saif Mohammad. 2012. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 246?255,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta; ELRA, may. European
Language Resources Association. 19-21.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
John C. Platt. 1998. Sequential minimal optimization:
A fast algorithm for training support vector machines.
Technical report, Advances in Kernel Methods - Sup-
port Vector Learning.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, ACLstudent ?05, pages 43?
48, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Comput-
erized Text Analysis Methods. Journal of Language
and Social Psychology, 29(1):24?54, March.
Cynthia Whissell. 1989. The Dictionary of Affect in
Language. In Robert Plutchik and Henry Kellerman,
editors, Emotion: theory, research and experience,
volume 4, The measurement of emotions. Academic
Press, London.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lexicon-
based and learning-based methods for twitter senti-
ment analysis. Technical Report HPL-2011-89, HP,
21/06/2011.
128
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, page 66,
Baltimore, Maryland, USA. June 27, 2014. c?2014 Association for Computational Linguistics
Challenges in Creating a Multilingual Sentiment Analysis Application 
for Social Media Mining 
 
 
 Alexandra Balahur, Hristo Tanev, Erik van der Goot 
European Commission Joint Research Centre 
Institute for the Protection and Security of the Citizen 
Via E. Fermi 2749, 21027 Ispra (VA), Italy 
Firstname.Lastname@jrc.ec.europa.eu 
 
 
  
 
Abstract of the talk 
In the past years, there has been an increasing 
amount of research done in the field of Sentiment 
Analysis. This was motivated by the growth in the 
volume of user-generated online data, the infor-
mation flood in Social Media and the applications 
Sentiment Analysis has to different fields ? Mar-
keting, Business Intelligence, e-Law Making, De-
cision Support Systems, etc. Although many 
methods have been proposed to deal with senti-
ment detection and classification in diverse types 
of texts and languages, many challenges still arise 
when passing these methods from the research 
settings to real-life applications.  
 
In this talk, we will describe the manner in which 
we employed machine translation together with 
human-annotated data to extend a sentiment anal-
ysis system to various languages. Additionally, 
we will describe how a joint multilingual model 
that detects and classifies sentiments expressed in 
texts from Social Media has been developed (at 
this point for Twitter and Facebook) and demo its 
use in a real-life application: a project aimed at 
detecting the citizens? attitude on Science and 
Technology. 
66
