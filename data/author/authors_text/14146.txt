Recognition Assistance 
Treating Errors in Texts Acquired from Various Recognition Processes 
 
G?bor PR?SZ?KY 
MorphoLogic  
K?sm?rki u. 8 
1118 Budapest, Hungary 
proszeky@morphologic.hu 
M?ty?s NASZ?DI 
MorphoLogic  
K?sm?rki u. 8 
1118 Budapest, Hungary 
naszodim@morphologic.hu 
Bal?zs KIS 
MorphoLogic  
K?sm?rki u. 8 
1118 Budapest, Hungary 
kis@morphologic.hu 
 
Abstract  
Texts acquired from recognition sources?conti-
nuous speech/handwriting recognition and 
OCR?generally have three types of errors re-
gardless of the characteristics of the source in 
particular. The output of the recognition process 
may be (1) poorly segmented or not segmented at 
all; (2) containing underspecified symbols (where 
the recognition process can only indicate that the 
symbol belongs to a specific group), e.g. shape 
codes; (3) containing incorrectly identified sym-
bols. The project presented in this paper addresses 
these errors by developing of a unified linguistic 
framework called the MorphoLogic Recognition 
Assistant that provides feedback and corrections 
for various recognition processes. The framework 
uses customized morpho-syntactic and syntactic 
analysis where the lexicons and their alphabets 
correspond to the symbol set acquired from the 
recognition process. The successful framework 
must provide three services: (1) proper disambi-
guated segmentation, (2) disambiguation for un-
derspecified symbols, (3) correction for incorr-
ectly recognized symbols. The paper outlines the 
methods of morpho-syntactic and syntactic 
post-processing currently in use. 
Introduction 
Recognition processes produce a sequence of dis-
crete symbols that usually do not entirely corre-
spond to characters of printed text. Further on, we 
refer to this sequence as an input sequence.1 A 
                                                   
1 This framework is actually a second tier of the data 
flow. The user receives a black box providing linguis-
tically sound and correctly recognized text. Inside the 
black box, the first tier performs the actual recognition, 
and the second tier carries out linguistic corrections and 
disambiguation. 
unified linguistic framework must perform a trans-
formation where (1) the symbols from the recogni-
tion process are converted into characters of written 
text, and (2) the correlation between the original 
analog source and the result is the closest possible. 
A post-processing framework must not simply 
perform a symbol-to-symbol conversion. A direct 
conversion is either impossible (phonetic symbols 
of any kind do not directly correspond to printed 
characters) or insufficient (source symbols are un-
derspecified or incorrectly recognized). Mor-
pho-lexical and syntactic models can help this pro-
cess as they recognize elements of the language, 
extracting meaningful passages from the input se-
quence. 
Lexical databases with fully inflected forms are 
fairly standard for speech recognition, mainly 
where a small closed vocabulary is used, and new, 
unknown or ad hoc word formations are not re-
quired (Gibbon et al, 1997). This procedure is 
convenient in languages with very small inflection-
al paradigms. An example of a language with few 
inflections is English, where, in general, three 
forms exist for nouns and four for verbs. English is 
therefore not a good example for illustrating in-
flectional morphology.  
Agglutinative languages such as Turkish, Fin-
nish or Hungarian, however, have complex inflec-
tional and derivational morphology, with signifi-
cantly more endings on all verbs, nouns, adjectives 
and pronouns. The number of endings increase the 
size of a basic vocabulary by a factor of thousands. 
Algorithmic morphological techniques have been 
developed for efficient composition of inflected 
forms and to avoid a finite but unmanageable explo-
sion of lexicon size. Still, according to Althoff et al 
(1996), these techniques have not been applied to 
any significant extent in speech technology.  
In this paper, we describe the application of a 
new method based on morphology and partial 
parsing. This method uses a unified error model 
with flexible symbol mapping, facilitating the use 
of any linguistic module with traditional ortho-
graphic lexicons?for any recognition process 
(OCR, handwriting, speech recognition), even for 
highly inflectional languages. The integrated sys-
tem uses our existing morpho-syntactic modules 
and lexicons. 
1 The error model  
The linguistic correction framework must be a-
ware of three classes of error sources occurring in 
the input sequence: (1) poor or nonexistent seg-
mentation, (2) underspecified symbols, (3) in-
correctly recognised symbols. 
The input sequence does not appear in the form 
of written text. It comprises of complex symbol 
codes in a normalized format, where the codes 
closely correspond to the signals recognized by the 
particular recognition process. In the case of OCR 
or handwriting recognition, this can be a shape 
code such as <lower> indicating a group of char-
acters. With speech recognition, this is rather a 
phoneme code such as <e:>. (Here we use the 
notation of the proposed framework.) Standard 
orthographic characters may also appear in the in-
put sequence. 
With all types of recognition processes, there 
exists no one-to-one mapping between the sym-
bols of the input sequence (the input alphabet) and 
the orthographic alphabet of the written text. The 
number of identified phonemes/phoneme comp-
lexes or characters/character complexes does not 
provide information about the number of charac-
ters to be used in the output text. 2  Unlike in 
two-level-morphology, the framework must pro-
vide n-to-m character (symbol) mapping, where n 
? m. Mapping between speech and text chunks of 
different length makes the system able to offer, for 
example, consonant sequences instead of affricates 
usually represented by single characters: 
me{ts,ts?}  metsz, metssz  
(?engraves,slits,cuts? in affirmative and 
imperative) 
                                                   
2 An example: in OCR outputs, the letter ?m? often oc-
curs in place of the ?rn? sequence. The correction mo-
dule must be able to transform single ?m?-s into ?rn?. 
With continuous speech recognition (and, though 
less frequently, continuous handwriting recogni-
tion) it is even possible that a written segment 
boundary?such as the end of a word or a 
sentence?occurs within a symbol. The framework 
must be aware of these schemes as well. 
The following sections present each error class 
with Hungarian examples to show the complexity 
of the linguistic model required by some languages. 
2 The basics: symbol mapping 
Atomic segments of input sequences are assumed to 
consist of (underspecified) symbols (phone-
mes/phoneme complexes, characters/character 
complexes). The correction framework must have a 
database of complex symbols?either phoneme 
codes or shape codes representing the classes of 
underspecified characters. An obvious approach to 
acquire a database of phonetic description of stems 
and suffixes (for morphological processing) is 
converting the existing (orthographical) lexicon. 
However, this conversion is very complicated and 
may result in an extremely large database. With 
speech recognition, for example, all orthographic 
representations must be converted into as many 
phonetic allomorphs as possible, on the basis of a 
grapheme-sequence-to-phoneme-sequence conver-
sion. This set contains every allomorph where the 
first or the last phoneme of which is subject to 
contextual change. E.g. k?t (?two?) is converted to 
{ke?t, ke?ty}, because of palatalization before cer-
tain words, like nyu?l (?rabbit?). 
ke?t
y
n
y
u?l  k?t ny?l (?two rabbits?) 
As the above method has some obvious disadvan-
tages, we decided to separate the symbol mapping 
from the linguistic processes. We have created a 
database mapping the recognized symbols to all 
possible orthographical characters/character se-
quences. In this scheme, the framework creates se-
veral possible orthographical sequences from the 
input sequence (implemented internally as a direc-
ted acyclic graph for performance reasons). The 
correction framework then segments and validates 
each sequence using ?traditional? linguistic modu-
les with the original orthographical lexicons. The 
conversion database uses a unified entry format 
suitable for all types of recognition processes. Ex-
ample: 
<ccs> ((<t>|((c|)c<s>)|)(c(<s>|)c<s>|ts)) 
This is a phoneme conversion entry. On the left 
side, a phonetic code is listed in the unified inter-
nal representation of the framework. (Note that 
this input symbol is the result of a mapping from 
the output of the recognition module.) On the right 
side, there is a directed acyclic graph (more or less 
a regular expression) describing all possible or-
thographic representations of the single phonetic 
entity. 
This is the core idea of the framework: the se-
parate conversion process provides for an open 
architecture where the framework can be attached 
to any recognition process, and even the linguistic 
modules are replaceable. 
3 Morpho-lexical segmentation 
For the simplest example, let us assume that the 
input sequence consists of phonetic symbols with 
no segmentation: however, pauses are indicated by 
the recognition process. The input sequence is 
processed symbol by symbol, and when the seg-
menter encounters a potential segment boundary, 
registers it and checks if the phonetic processor 
saw any pause, stress or other sign of segmentation 
at that particular position in the original speech 
signal. This might require some interaction with 
speech recognizer, but for the sake of simplicity, 
now we describe the operation of the linguistic 
subsystem only. 
The original architecture design devises the 
framework as a feedback service, one requesting 
further information from the recognition source. In 
the current implementation, however, the correction 
framework can be separated from the recognition 
process, and provide corrected and disambiguated 
text without feedback to the recognition module. 
In the analysis process of the unsegmented sig-
nal (see Figure 1), for example, the input slice 
vonate?r has three morpho-lexically equally likely 
segmentations: von a t?r, vonat ?r, vonat?r. Either 
the acoustic signal contains information confirming 
or rejecting any of them; or all of them will be 
temporarily kept, and the segmentation process 
itself will filter them out later on. In Figure 1, after 
reading some further symbols from the input, it 
becomes clear that the only orthographically correct 
word boundary is between vonat and ?rkezik . 
4 Underspecified forms 
It is quite common that the recognition process 
cannot perfectly identify segments in the original 
signal source. These are the cases of underspeci-
fication. Let us assume that a speech recognition 
Non-segmented phonetic input Proposed orthographic segmentation 
v 
vo 
von ...............................................................................von 
vona..............................................................................von a 
vonat.............................................................................vonat 
vonate? ..........................................................................vonat? 
vonate?r .........................................................................von a t?r, vonat ?r, vonat?r 
vonate?rk 
vonate?rke 
vonate?rkez 
vonate?rkezi ..................................................................vonat ?rkezi 
vonate?rkezik ................................................................vonat ?rkezik 
vonate?rkezika...............................................................vonat ?rkezik a 
vonate?rkezikam 
vonate?rkezikama? 
vonate?rkezikama?s ? ? .................................................................vonat ?rkezik a m?s 
vonate?rkezikama?s ? ?o 
vonate?rkezikama?s ? ?od ................................................................ vonat ?rkezik a m?sod 
vonate?rkezikama?s ? ?odi ............................................................... vonat ?rkezik a m?sodi 
vonate?rkezikama?s ? ?odik ............................................................ vonat ?rkezik a m?sodik 
vonate?rkezikama?s ? ?odikv 
vonate?rkezikama?s ? ?odikva? 
vonate?rkezikama?s ? ?odikva?g .................................................... vonat ?rkezik a m?sodik v?g 
vonate?rkezikama?s ? ?odikva?ga? 
vonate?rkezikama?s ? ?odikva?ga?ny ............................................ vonat ?rkezik a m?sodik v?g?ny 
vonate?rkezikama?s ? ?odikva?ga?nyr 
vonate?rkezikama?s ? ?odikva?ga?nyra......................................... vonat ?rkezik a m?sodik v?g?nyra 
Figure 1. Morpho-lexical segmentation of a Hungarian phonetic string 
process is unable to identify the value of the binary 
feature VOICED. In these cases, the linguistic 
subsystem attempts to find orthographically 
well-formed morpho-lexical constructions for both 
voiced and voiceless variants of the phoneme in 
question. In fact, underspecified forms of the input 
signal are represented either by lists of possible 
characters?like set representations in two-level 
morphology (Koskenniemi, 1983): 
vona{t,d}e?rkezikama?s? ?odi{g,k}{v,f}a?ga?nyra 
(?train is arriving to the second platform?) 
or by underspecified feature complexes: 
vonaDe?rkezikama?s? ?odiGVa?ga?nyra 
where D, G and V are d, g and v, respectively, but 
not specified as voiced or voiceless. 
5 Using higher-level linguistic processes 
The linguistic correction framework operates ra-
ther inefficiently if it uses morpho-lexical proces-
sing only. This results in extreme ambiguity: nu-
merous overgenerated orthographic patterns ap-
pear with grammatically incorrect segmentation. 
Thus the process must be improved by adding 
higher level linguistic analysis. Currently, the 
framework uses partial syntax similar to the me-
chanism applied in the Hungarian grammar che-
cker module. This partial syntax describes parti-
cular syntactic phenomena in order to identify in-
correct grammar beyond the word boundaries. 
A more efficient post-processing filter is being 
developed by applying the HumorESK parser mo-
dule (Pr?sz?ky, 1996). Figure 2 shows the possible 
segmentations of the morphology-only system. In 
this figure, an asterisk marks syntactically non-mo-
tivated word sequences filtered out by the partial 
syntax or the full parser?operating as a higher-le-
vel segmenter. 
In the first 10 segmentations, the personal pro-
noun ti (2nd person, pl.) does not agree with either 
the verb ?r (3rd person, sing.) or the verb ?rok (1st 
person, sing.). Syntactically the last two 
segmentations can be accepted (but semantically 
and according to topic-focus articulation, Nr. 11 is 
bizarre). In most cases it is true that the 
segmentation containing the longest matches in the 
input sequence is the best orthographical candidate. 
6 Further development 
Morpho-lexical and syntactic segmentation and 
correction can be very useful in improving the 
quality of ?traditional? recognition sources. How-
ever, it is important to emphasize that the proposed 
framework would only support existing recognition 
methods (e.g. likelihood-based mechanisms in 
speech recognition) rather than replacing them. The 
current breakdown of the framework makes no as-
sumptions on the operation of the underlying re-
cognition process, and does not prefer any methods 
to any other. In terms of architecture, the correction 
framework?s operation is separated from the rec-
ognition module. 
One of the aims of this project is, however, a 
better interaction between the linguistic and the re-
cognition subsystem. As the first step, it requires a 
standard feedback interface (yet to be developed). 
Because the current implementation of the Mor-
phoLogic Recognition Assistant framework does 
not make assumptions of the recognition subsys-
tem, it cannot influence its operation. A standard 
feedback interface consists of a formalism for de-
scribing the interaction between a recognition 
source and the correction framework, regardless of 
the characteristics of the recognition subsystem. 
Stub modules must be developed to communicate 
with existing recognition systems. 
An example for the dialogue between a phonetic 
and linguistic subsystem: first, a superficial acous-
tic-phonetic analysis offers some sequence of 
underspecified feature complexes, then the lin-
guistic subsystem attempts to transform them into 
potential orthographically correct units with surface 
word boundaries. Finally, the phonetic system 
Input: nyelve?setitsik?eti?rok 
1. *nyel v?sz e ti cikket ?r ok 
2. *nyel v?sz e ti cikket ?rok 
3. *nyel v?sze ti cikket ?r ok 
4. *nyel v?sze ti cikket ?rok 
5. *nyelv ?sz e ti cikket ?r ok 
6. *nyelv ?sz e ti cikket ?rok 
7. *nyelv?sz e ti cikket ?r ok 
8. *nyelv?sz e ti cikket ?r ok 
9. *nyelv?sze ti cikket ?r ok 
10. *nyelv?sze ti cikket ?rok  
11. nyelv?szeti cikket ?r ok  
12. nyelv?szeti cikket ?rok 
(?I am writing a linguistic paper.?) 
Figure 2. Syntactic filtering 
controls whether which of the offered segmen-
tation points can be confirmed acoustically. 
7 Implementation 
The first version of the MorphoLogic Recognition 
Assistant framework has been implemented along 
with a demonstration interface. This application 
takes symbolic codes of different recognized 
symbols (phonemes, OCR-read characters etc.), 
and provides orthographical output. It has been 
programmed in C++ using MS Visual Studio 6.0, 
and runs on 32-bit Windows systems. As service 
modules, the framework incorporates the Humor 
(morphological analyser), the Helyesebb (gram-
matical validator), and the HumorESK (full parser) 
technologies. With a standard programming in-
terface, it is ready to be integrated with existing 
recognition systems. 
Conclusion 
This paper has introduced a framework for treating 
common error classes occurring in the output of 
various recognition sources. We have shown that 
different types of recognition sources share the 
same error types: namely, (1) poor or nonexistent 
segmentation, (2) underspecified and (3) incor-
rectly recognized symbols. 
Our proposed solution is a post-processing 
phase performed on the output of the recognition 
source, where morpho-lexical and syntactic mod-
els validate (either accept or reject) different 
orthographical candidates derived from a single 
recognized symbol sequence. 
The system is language independent and com-
pletely data-driven: by replacing the databases, the 
MorphoLogic Recognition Assistant is imme-
diately ready to work with a different language. 
For the Humor system, descriptions exist for sev-
eral languages (Hungarian, English, German, 
Spanish, Czech, Polish and Romanian). Syntax 
descriptions are under development for Hungarian 
and English (prototypes exist). 
The proposed framework seems promising for 
continuous recognition systems. Its main advan-
tage is the ease of application of any linguistic 
module, thanks to the separate symbol mapping 
process and the open architecture. However, we 
must emphasize again that the MorphoLogic Re-
cognition Assistant supports existing recognition 
systems rather than replacing them.  
Acknowledgements 
This research was carries out within the framework 
of the IKTA-063/2000 Project supported by the 
Hungarian Ministry of Education. 
References  
Althoff, F., G. Drexel, H. L?ngen, M. Pampel & C. 
Schillo (1996). The treatment of compounds in a 
morphological component for speech recog-
nition. In: D. Gibbon (ed.) Natural language 
processing and speech technology, 71-76, 
Mouton de Gruyter, Berlin, New York  
Gibbon, D., R. Moore, R. Winski, eds. (1997). 
Handbook of Standards and Resources for 
Spoken Language Systems. Walter de Gruyter 
Publishers, Berlin & New York  
Koskenniemi, K. (1983) Two-level morphology: A 
general computational model for word-form 
recognition and production. University of Hel-
sinki, Department of General Linguistics, Hel-
sinki, Finland  
Nakayama, T. (1994). Modeling Content Identi-
fication from Document Images. Proceedings of 
the 4th Applied Natural Language Processing 
Conference, 22-27, Stuttgart, Germany 
Pr?sz?ky, G. (1994). Industrial Applications of 
Unification Morphology Proceedings of the 4th 
Conference on Applied Natural Language 
Processing, 157?159, Stuttgart, Germany  
Pr?sz?ky, G. (1996).  Humor: a Morphological 
System for Corpus Analysis First TELRI 
Seminar on Language Resources and Language 
Technology, 149?158, Tihany, Hungary 
Pr?sz?ky, G. (1996). Syntax As Meta-morphology 
Proceedings of COLING-96, Vol.2, 1123?1126, 
Copenhagen, Denmark 
Pr?sz?ky, G. & B. Kis (1999). Agglutinative and 
Other (Highly) Inflectional Languages. Pro-
ceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, 
261?268. College Park, Maryland, USA  
Sibun, P. & Spitz, A. L. (1994). Language Deter-
mination: Natural Language Processing from 
Scanned Document Images. Proceedings of the 
4th Applied Natural Language Processing 
Conference, 15-21,Stuttgart, Germany 
Context-Sensitive Electronic Dictionaries 
 
G?bor PR?SZ?KY 
MorphoLogic 
 K?sm?rki u. 8. 
1118 Budapest, Hungary 
 proszeky@morphologic.hu 
Bal?zs KIS 
 MorphoLogic 
K?sm?rki u. 8. 
1118 Budapest, Hungary 
   kis@morphologic.hu 
 
Abstract  
This paper introduces a context-sensitive 
electronic dictionary that provides transla-
tions for any piece of text displayed on a 
computer screen, without requiring user in-
teraction. This is achieved through a process 
of three phases: text acquisition from the 
screen, morpho-syntactic analysis of the 
context of the selected word, and the dic-
tionary lookup. As with other similar tools 
available, this program usually works with 
dictionaries adapted from one or more prin-
ted dictionaries. To implement context sen-
sitive features, however, traditional diction-
ary entries need to be restructured. By split-
ting up entries into smaller pieces and in-
dexing them in a special way, the program is 
able to display a restricted set of information 
that is relevant to the context. Based on the 
information in the dictionaries, the program 
is able to recognize?even discontinu-
ous?multiword expressions on the screen. 
The program has three major features which 
we believe make it unique for the time being, 
and which the development focused on: lin-
guistic flexibility (stemming, morphological 
analysis and shallow parsing), open archi-
tecture (three major architectural blocks, all 
replaceable along public documented APIs), 
and flexible user interface (replaceable dic-
tionaries, direct user feedback). 
In this paper, we assess the functional re-
quirements of a context-sensitive dictionary 
as a start; then we explain the program?s 
three phases of operation, focusing on the 
implementation of the lexicons and the con-
text-sensitive features. We conclude the pa-
per by comparing our tool to other similar 
publicly available products, and summarize 
plans for future development. 
1 Introduction 
With several instant comprehension tools pub-
licly available, we need not justify the usefulness 
of the type of device we are developing. The 
main idea behind the program is to help computer 
users understand the large number of foreign 
language texts they encounter. In most situations 
of computer usage, users do not need translations, 
nor do they have to provide translations. A dic-
tionary in such cases must not be another appli-
cation but a background process providing help 
when necessary. 
This help must be context-sensitive in two 
aspects: first, it should appear in the context 
where the need for translation occurred, the user 
must not be forced to switch to another context of 
a separate application; second, the output?the 
translation?should contain only information 
relevant to the textual context for which the 
translation is required. An entire dictionary entry 
should almost never be displayed since it con-
tains multiword examples irrelevant to the con-
text of translation. Adapting a bi-lingual diction-
ary to foreign language comprehension takes the 
recompilation of any dictionary to some extent 
before it is incorporated in the system (Feldweg 
and Breidt 1996). 
We define the context-sensitive electronic 
dictionary we devise here as a context-sensitive 
instant comprehension tool. It is more than a 
dictionary lookup engine as it tailors dictionary 
entries to the context of the translation point. It is 
less than a translation engine, however, as it 
performs no syntactic processing of the source 
text, only series of dictionary lookups. 
It is not only the textual context that our tool is 
sensitive to?like all major instant dictionaries: 
in a graphical computing environment, it reads 
text from anywhere on the computer screen, 
performs its linguistic analysis in the background, 
and then uses one or more dictionaries to find the 
translations. The output is displayed in a bubble, 
in front of the existing screen contents, leaving it 
otherwise intact. The program is activated with-
out a mouse click, simply by leaving the mouse 
pointer over the translation point for one second. 
There are several aspects of user interface design 
affecting the decision to use this mechanism. The 
obvious advantage of using no mouse clicks is 
that this never interferes with the extisting user 
interfaces of any other programs. 
2 Requirements of a comprehension 
assistant  
An instant comprehension assistant is completely 
left alone in the sense that it cannot ask for user 
interaction: it cannot require the user to choose 
from a list of ambiguous linguistic analyses, and, 
at the same time, it should keep the proportion of 
semantic ambiguities as low as possible. So such 
an application can only rely on its own linguistic 
knowledge. 
When the user leaves the mouse pointer over a 
word, it means that he needs information about 
that word and its context. The boundaries of the 
context are not precisely specified: it could be the 
entire sentence (or even a larger passage) which 
includes the selected word, or?more often?a 
smaller context such as a multi-word expression 
around it. It is therefore the task of the program to 
determine the largest possible context, analyze it, 
and provide as much information of it as possi-
ble?based on the dictionaries behind the system. 
The minimum requirement is that the program 
should recognize all obvious multi-word expres-
sions and idioms, and provide appropriate trans-
lations. All possible forms of the multi-word 
expressions should be identified?even if 
word-forms are inflected or the word order is 
different from the basic form. This is the matter 
of the quality of the linguistic parsing compo-
nents and the dictionaries. If no multi-word ex-
pressions are recognized in the context, the 
comprehension assistant should display a simple 
dictionary entry for the selected word only, list-
ing all possible translations found in the active 
dictionaries. 
There is another implication of the fact that 
the comprehension assistant is not allowed to ask 
for user interaction. The program has to acquire 
pieces of text from the screen regardless of the 
application that displayed them without relying 
on user input, clipboard or file contents, or spe-
cial application properties. As there is no direct 
access to the text, the program sees pieces of text 
as sequences of characters without formatting or 
other document-specific information, including 
the language of the source text. This requires 
implementing a language identification algo-
rithm, too. So far, it is clear that a well-behaved 
comprehension assistant is a rather special com-
bination of different techniques, involving lan-
guage technology in almost every bit of op-
eration. 
3 Phases of context-sensitive instant 
comprehension 
Phase 1: Text acquisition. When the user leaves 
the mouse pointer unmoved for one second, the 
text acquisition phase is activated. This is a task 
all instant dictionaries must face. Current im-
plementations rely on operating system (or 
graphical user interface) resources to acquire text 
displayed on the screen. Our implementation 
performs a combination of an OCR-like proce-
dure on the screen contents and applica-
tion-specific acquisition procedures. The former 
works with all applications, but is less accurate 
with nonstandard character sizes, while the latter 
communicates with known programs?this is 
very accurate, but limited to a closed set of pro-
grams. Depending on the version, text is acquired 
either one line or one paragraph at a time (when 
applicable). 
Phase 2: Linguistic analysis and dictionary 
lookups. Linguistic analysis is used to identify 
the word that was pointed at, and perform a 
morpho-syntactic analysis of its context to de-
termine what to look up in the dictionaries. Lin-
guistic analysis consists of several steps essential 
for proper dictionary lookup, because there is no 
initial information about the text other than the 
text itself?with a single word highlighted indi-
cating the position of the mouse pointer and thus 
the initial point of analysis. One must take into 
account that the initial data are often results of an 
OCR-like process whose errors require correc-
tion during subsequent linguistic analy-
sis?similarly to the procedure in common OCR 
programs.1 
The linguistic analyzer module performs 
morpho-syntactic analysis for the selected word 
in context?by means of the HUMOR engine 
(Pr?sz?ky and Kis 1999). At this point, morpho-
logical analysis has three main purposes: (a) lin-
guistic stemming for accurate dictionary lookups, 
(b) spelling correction and (c) preparation of 
shallow parsing of the context to identify candi-
dates for multi-word expressions.  
If linguistic analysis fails to recognize any 
multi-word expressions, words from the context 
are still passed on to the dictionary lookup phase 
as the dictionaries may contain idiomatic phrases 
that cannot be recognized on a linguistic basis. 
The dictionary lookup module receives lexical 
stems in the context of the translation point, and 
matches them against the installed dictionaries. 
The program uses the same robust dictionary 
engine as the one we use in our terminology 
management system. It is capable of handling 
multiple dictionaries at the same time (Pr?sz?ky 
1998).  
Dictionaries are compiled to facilitate the fil-
tering of multiword lexemes. This means two 
things: first, in addition to headwords, all lex-
emes (subheadwords, examples) within entries 
are indexed. Second, entries are split into smaller 
parts to retrieve only relevant information.  
The engine is capable of finding all multi-word 
lexemes which include one or more words with a 
single lookup. In some cases, this could be a 
rather lengthy list which must be filtered using 
the other words in the context. More precisely, 
(translations for) multi-word expressions will be 
displayed if and only if they include some sig-
nificant words of the context (and do not contain 
other significant words). By ?significant word?, 
we mean that there are also ?nonsignificant? 
words (or stop-words) that are skipped when 
forming a query expression for the dictionary 
engine.  
                                                   
1 According to our experience, however, recognition errors 
are very rare because there is a closed set of shapes (glyphs 
in the currently installed system fonts) that may occur in any 
text displayed by applications (except for pieces of text 
within bitmap images). Recognition errors are usually re-
sults of applications using nonstandard techniques (e.g. 
dynamically altering character spacing) to display text. 
The ambiguity of the output is reduced ?only? 
by this filtering process. If an entry is considered 
as relevant by the filtering procedure, it is dis-
played. In current implementations, different 
meanings of a single word or a multiword lexeme 
are not filtered out based on the context. 
Phase 3: Rendering and graphic output. The 
output of the program is displayed in a bub-
ble-shaped pop-up window on the screen that 
disappears if the user moves the mouse cursor 
again. The bubble contains formatted text: cur-
rent implementations use either a proprietary 
XML-to-RTF conversion procedure, or XSLT 
formatting, depending on the version. 
4 Some implementation details 
Dictionaries. Dictionaries in our system are rep-
resented as lexical databases where the structure 
of each dictionary is strictly preserved. This is 
achieved through using XML as the single dic-
tionary format. Dictionaries are either originally 
written in XML or transformed from a printed or 
another electronic format by means of automatic 
and semi-automatic tools. 
All dictionaries are bi-lingual. Currently 
available dictionaries use language pairs such as 
English-Hungarian, German-Hungarian. How-
ever, there are experimental dictionaries for other 
languages such as Spanish, Polish, and even 
Japanese. 
The largest dictionary currently available is an 
adaptation of Hungary?s newest academic Eng-
lish-Hungarian dictionary, which contains over 
400,000 entries in the electronic version. (Note 
that for the reasons mentioned earlier, original 
entries are split into multiple parts for filtering 
multiword lexemes.) 
We have mentioned earlier that a language 
identification module might be required for effi-
cient operation of an instant dictionary. One 
could notice, however, that we have not imple-
mented such a module. Although we have de-
veloped a language identifier called LangWitch, 
we use a much simpler approach in the instant 
comprehension tool: all dictionaries are looked 
up in both their languages. If a word is there in a 
dictionary in any language, there is a hit. There-
fore, if there is a word on the screen that is in-
cluded in any of the installed dictionaries in any 
language handled by them, it will be recognized 
and translated. 
Filtering. By using a heuristic procedure, the 
program is able to recognize continuous and dis-
continuous multiword lexemes. The size of the 
analysis window if configurable, but basically it 
is determined by the longest multiword example 
in the dictionary. 
Text acquisition accuracy. Most versions of our 
instant comprehension assistant use the 
OCR-based text acquisition technique mentioned 
earlier. This procedure is capable of recognizing 
text written in fonts installed on the computer. If 
a piece of text is written in an installed font and in 
a standard size between 8 and 16 points, the 
recognition accuracy is near 100 percent. With 
nonstandard text sizes (zoomed display, too small 
or too large character spacing), however, the 
accuracy radically declines. Some applica-
tions?like Microsoft Word or Adobe Acrobat 
Reader?display text in a nonstandard way. For 
these applications, we use alternative acquisition 
methods that communicate with the particular 
application using an application-specific protocol, 
which provides accurate text recognition. 
Processing user feedback. Our team does not 
regularly develop dictionary contents. Some dic-
tionaries, however, have been developed by us, 
and these dictionaries are continuously reviewed 
and updated. The update process is rather unique 
because it is built largely on user feedback. From 
the aspect of dictionary development (and even 
linguistic research), the comprehension assistant 
is an ideal source of linguistic information be-
cause it reaches a potentially large number of 
users (since it is not a special application but a 
utility that has its place in every computing en-
vironment). Based on this insight, we have im-
plemented an instant feedback feature, which 
comprises of two processes: 
(1) Logging: the program continuously logs 
words and multiword expressions it was 
unable to analyze or failed to find in the 
dictionaries. 
(2) Contacting the developers: the program 
automatically sends e-mails containing the 
current logs to the developer lab.2  
                                                   
2 This requires permission from the user which the program 
asks for during installation. 
Logs are gathered and analysed by further auto-
matic tools at the development site. Having been 
filtered to exclude obvious noise entries, the list 
is then sent to lexicographers for further analysis. 
This process effectively reveals errors and defi-
ciencies in the dictionaries and the morphological 
lexicons, and, at the same time, it helps defining 
directions of further improvements. 
5 Comparison to other systems 
There are two categories where our context-sen-
sitive instant comprehension tool?the brand 
name is MoBiMouse?might be compared to 
other systems: functionality and linguistic accu-
racy. There are a few pop-up dictionaries on the 
market: the most well-known are Babylon, 
WordPoint, CleverLearn, iFinger, Langen-
scheidt's Pop-up Dictionary and Techocraft?s 
RoboWord, but none of them have as many 
language technology features as MoBiMouse. 
There are some ?glossing? programs in research 
laboratories (RXCE, see Feldweg and Breidt 
1996; or Sharp, see Poznanski et al 1998) that 
access dictionaries with a context-sensitive 
look-up procedure. However, they present the 
information to the user through their own 
graphical interface, and none of them have the 
basic featuere of MoBiMouse, namely, being a 
context-sensitive instant comprehension tool for 
any running application. The above systems do 
not have access to more than one dictionary at the 
same time, unlike MoBiMouse. On the other 
hand, the treatment of multiword units in the 
IDAREX formalism (Segond and Breidt 1996) is 
more sophisticated than in MoBiMouse. Another 
project with instant understanding is GLOSSER, 
whose prototype (Nerbonne et al 1997) performs 
morphological analysis of the sentence contain-
ing the selected word in a similar manner. In 
GLOSSER?unlike in MoBiMouse?there is a 
stochastic disambiguation step but everything is 
shown in a.separate window. 
The text acquisition techniques used in Mo-
BiMouse are independent from both the language 
and the writing system. Hence it is rather differ-
ent from most known applications that work with 
English characters only. Most other pop-up dic-
tionary applications start by pressing a button or 
clicking the mouse. MoBiMouse is activated 
without mouse clicks (like RoboWord), therefore 
it can be used to acquire any text from the screen 
without affecting other running applications. 
MoBiMouse is even able to access user interface 
elements such as menus and buttons because it 
works from the graphical content of the entire 
screen, while others such as RoboWord access 
only the window contents displayed by applica-
tions.  
The speed of the text acquisition module is 
1000 character/s, stemming takes 0,002 s/word-
form, an average dictionary lookup 0,02 s. Mo-
BiMouse, unlike Babylon, can be used in both 
language directions of the dictionary due to its 
writing independence and liguistic components 
for many languages. 
6 Future development plans 
Most our development plans focus on improving 
the program?s user interface. Before MoBiMouse, 
we have developed an electronic dic-
tionary/terminology management program called 
MoBiDic. The new versions of both programs are 
integrated into a single package, where the full 
MoBiDic user interface is callable through the 
MoBiMouse technology.. 
As for the linguistic capabilities, we plan to 
exploit MoBiMouse?s open architecture and in-
tegrate the ?traditional? dictionary lookup module 
with a parser/translator engine capable of ana-
lysing and often translating an entire sentence or 
at least a part of it. The parser/translator engine 
(called MetaMorpho) is still under development. 
Conclusion 
MoBiMouse is a context-sensitive instant com-
prehension tool that offers translations for words 
and expressions displayed on computer screens. 
The program is activated without a mouse click 
when the user leaves the mouse pointer over the 
word in question. The translation is displayed in a 
tooltip-like bubble. If the mouse is moved again, 
the translation disappears promptly, so the user's 
work will not be disrupted by another program 
requiring a whole window.  
Although there are many similar programs 
publicly available, we believe MoBiMouse is 
quite unique thanks to many of its features: (1) 
the combined text acquisition procedure (using 
an application-independent and an applica-
tion-specific module), which makes it work in 
any application, (2) the rich linguistic processing 
with the linguistic stemming and the con-
text-sensitive filtering module, which makes the 
program the most linguisticly sophisticated of its 
kind, (3) and the open architecture which makes 
any major architectural element replaceable, 
providing for an easy development of any kind of 
instant information acquisition application. 
Acknowledgements 
The authors would like to thank Andr?s F?ldes, 
development project leader for MoBiMouse, and 
L?szl? Tihanyi, chief content developer at Mor-
phoLogic. 
References  
Feldweg, H. and E. Breidt (1996) 
COMPASS?An Intelligent Dictionary Sys-
tem for Reading Text in a Foreign Language. 
Papers in Computational Lexicography 
(COMPLEX 96), Linguistics Institute, HAS, 
Budapest, pp. 53?62. 
Nerbonne, L. Karttunen, E. Paskaleva, G. 
Pr?sz?ky and T. Roosmaa (1997) Reading 
More into Foreign Languages. Proceedings of 
the 5th Conference on Applied Natural Lan-
guage Processing (ANLP?97), Washington, pp. 
135?138. 
Poznanski, V., P. Whitelock, J. Udens and S. 
Corley (1998) Practical Glossing by Prioritised 
Tiling. Proceedings of the COLING-98, 
Montreal, pp. 1060?1066. 
Pr?sz?ky, G. (1998) An Intelligent Multi-Dic-
tionary Environment. Proceedings of the 
COLING-98, Montreal, pp. 1067?1071. 
Pr?sz?ky, G. and B. Kis (1999) A Unifica-
tion-based Approach to Morpho-Syntactic 
Parsing of Agglutinative and Other (Hughly) 
Inflectional Languages. Proceedings of the37th 
Annual Meeting of ACL, College Park, pp. 
261?268. 
Segond, F. and E. Breidt (1996) IDAREX: de-
scription formelle des expression ? mots mul-
tiples en fran?ais et en allemand. In: A. Clas, 
Ph. Thoiron and H. B?joint (eds.) Lexico-
matique et dictionnairiques, Montreal, Au-
pelf-Uref 
53
54
55
56
Proceedings of the Third Workshop on Statistical Machine Translation, pages 111?114,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The MetaMorpho translation system 
Attila Nov?k, L?szl? Tihanyi and G?bor Pr?sz?ky 
MorphoLogic 
Orb?nhegyi ?t 5, Budapest 1126, Hungary 
{novak,tihanyi,proszeky}@morphologic.hu 
 
 
 
Abstract 
In this article, we present MetaMorpho, a rule 
based machine translation system that was 
used to create MorphoLogic?s submission to 
the WMT08 shared Hungarian to English 
translation task. The architecture of Meta-
Morpho does not fit easily into traditional 
categories of rule based systems: the building 
blocks of its grammar are pairs of rules that 
describe source and target language structures 
in a parallel fashion and translated structures 
are created while parsing the input.  
1 Introduction 
Three rule-based approaches to MT are tradition-
ally distinguished: direct, interlingua and transfer. 
The direct method uses a primitive one-stage proc-
ess in which words in the source language are re-
placed with words in the target language and then 
some rearrangement is done. The main idea behind 
the interlingua method is that the analysis of any 
source language should result in a language-
independent representation. The target language is 
then generated from that language-neutral repre-
sentation. The transfer method first parses the sen-
tence of the source language. It then applies rules 
that map the lexical and grammatical segments of 
the source sentence to a representation in the target 
language. 
The MetaMorpho machine translation system de-
veloped at MorphoLogic (Pr?sz?ky and Tihanyi, 
2002), cannot be directly classified in either of the 
above categories, although it has the most in com-
mon with the transfer type architecture.  
2 Translation via immediate transfer 
In the MetaMorpho system, both productive 
rules of grammar and lexical entries are stored in 
the form of patterns, which are like context-free 
rules enriched with features. Patterns may contain 
more-or-less underspecified slots, ranging from 
general productive rules of grammar through more-
or-less idiomatic phrases to fully lexicalized items. 
The majority of the patterns (a couple of hundreds 
of thousands in the case of our English grammar) 
represent partially lexicalized items. 
The grammar operates with pairs of patterns 
that consist of one source pattern used during bot-
tom-up parsing and one or more target patterns that 
are applied during top-down generation of the 
translation. While traditional transfer and interlin-
gua based systems consist of separate parsing and 
generating rules, in a MetaMorpho grammar, each 
parsing rule has its associated generating counter-
part. The translation of the parsed structures is al-
ready determined during parsing the source 
language input. The actual generation of the target 
language representations does not involve any ad-
ditional transfer operations: target language struc-
tures corresponding to substructures of the source 
language parse tree are combined and the leaves of 
the resulting tree are interpreted by a morphologi-
cal generator. We call this solution ?immediate 
transfer? as it uses no separate transfer steps or 
target transformations. 
The idea behind this architecture has much in 
common with the way semantic compositionality 
was formalized by Bach (1976) in the from of his 
rule-to-rule hypothesis, stating that to every rule of 
syntax that combines constituents into a phrase 
pertains a corresponding rule of semantics that 
111
combines the meanings of the constituents. In the 
case of phrases with compositional meaning, the 
pair of rules of syntax and semantics are of a gen-
eral nature, while in the case of idioms, the pair of 
rules is specific and arbitrary. The architecture im-
plemented in the MetaMorpho system is based on 
essentially the same idea, except that the represen-
tation built during analysis of the input sentence is 
not expressed in a formal language of some seman-
tic representation but directly in the human target 
language of the translation system. 
3 System architecture  
The analysis of the input is performed in three 
stages. First the text to be translated is segmented 
into sentences, and each sentence is broken up into 
a sequence of tokens. This token sequence is the 
actual input of the parser. Morphosyntactic annota-
tion of the input word forms is performed by a 
morphological analyzer: it assigns morphosyntactic 
attribute vectors to word forms. We use the Humor 
morphological system (Pr?sz?ky and Kis, 1999; 
Pr?sz?ky and Nov?k, 2005) that performs an item-
and-arrangement style morphological analysis. 
Morphological synthesis of the target language 
word forms is performed by the same morphologi-
cal engine.  
The system also accepts unknown elements: 
they are treated as strings to be inflected at the tar-
get side. The (potentially ambiguous) output of the 
morphological analyzer is fed into the syntactic 
parser called Moose (Pr?sz?ky, Tihanyi and Ugray, 
2004), which analyzes this input sequence using 
the source language patterns and if it is recognized 
as a correct sentence, comes up with one or more 
root symbols on the source side.  
Every terminal and non-terminal symbol in the 
syntactic tree under construction has a set of fea-
tures. The number of features is normally up to a 
few dozen, depending on the category. These fea-
tures can either take their values from a finite set of 
symbolic items (e.g., values of case can be INS, 
ACC, DAT, etc.), or represent a string (e.g., 
lex="approach", the lexical form of a token). 
The formalism does not contain embedded feature 
structures. It is important to note that no structural 
or semantic information is amassed in the features 
of symbols: the interpretation of the input is con-
tained in the syntactic tree itself, and not in the fea-
tures of the node on the topmost level. Features are 
used to express constraints on the applicability of 
patterns and to store morphosyntactic valence and 
lexical information concerning the parsed input. 
More specific patterns (e.g. approach to) can 
override more general ones (e.g. approach), in that 
case subtrees containing symbols that were created 
by the general pattern are deleted. Every symbol 
that is created and is not eliminated by an overrid-
ing pattern is retained even if it does not form part 
of a correct sentence's syntactic tree. Each pattern 
can explicitly override other rules: if the overriding 
rule covers a specific range of the input, it blocks 
the overridden ones over the same range. This 
method can be used to eliminate spurious ambigui-
ties early during analysis. 
When the whole input is processed and no ap-
plicable patterns remain, translation is generated in 
a top-down fashion by combining the target struc-
tures corresponding to the source patterns consti-
tuting the source language parse tree.  
A source language pattern may have more than 
one associated target pattern. The selection of the 
target structure to apply relies on constraints on the 
actual values of features in the source pattern: the 
first target pattern whose conditions are satisfied is 
used for target structure generation. To handle 
complicated word-order changes, the target struc-
ture may need rearrangement of its elements within 
the scope of a single node and its children. There is 
another technique that can be used to handle word 
order differences between the source and the target 
language. A pointer to a subtree can be stored in a 
feature when applying a rule at parse time, and 
because this feature?s value can percolate up the 
parse-tree and down the target tree, just like any 
other feature, a phrase swallowed somewhere in 
the source side can be expanded at a different loca-
tion in the target tree. This technique can be used 
to handle both systematic word order differences 
(such as the different but fixed order of constitu-
ents in possessive constructions: possession of pos-
sessor in English versus possessor possession + 
possessive suffix in Hungarian) and accidental ones 
(such as the fixed order of subject verb and object 
in English, versus the ?free? order of these con-
stituents in Hungarian1). 
Unlike in classical transfer-based systems, 
however, these rearrangement operations are al-
                                                          
1 In fact the order is determined by various factors other than 
grammatical function. 
112
ready determined during parsing the source lan-
guage input. During generation, the already deter-
mined rearranged structures are simply spelled out. 
The morphosyntactic feature vectors on the termi-
nal level of the generated tree are interpreted by 
the morphological generator that synthesizes the 
corresponding target language word forms.  
The morphological generator is not a simple in-
verse of the corresponding analyzer. It accepts 
many alternative equivalent morphological de-
scriptions of each word form it can generate beside 
the one that the corresponding analyzer outputs.  
4 The rule database 
The rules used by the parser explicitly contain 
all the features of the daughter nodes to check, all 
the features to percolate to the mother node, all the 
features to set in the corresponding target struc-
tures and those to be checked on the source lan-
guage structure to decide on the applicability of a 
target structure. The fact that all this redundant 
information is present in the run-time rule database 
makes the operation of the parser efficient in terms 
of speed. However, it would be very difficult for 
humans to create and maintain the rule database in 
this redundant format.  
There is a high level version of the language: 
although it is not really different in terms of its 
syntax from the low-level one, it does not require 
default values and default correspondences to be 
explicitly listed. The rule database is maintained 
using this high level formalism. There is a rule 
converter for each language pair that extends the 
high-level rules with default information and may 
also create transformed rules (such as the passive 
version of verbal subcategorization frames) creat-
ing the rule database used by the parser.  
Rule conversion is also necessary because in 
order to be able to parse a free word order lan-
guage like Hungarian with a parser that uses con-
text free rules, you need to use run time rules that 
essentially differ in the way they operate from 
what would be suggested by the rules they are de-
rived from in the high level database. In Hungar-
ian, arguments of a predicate may appear in many 
different orders in actual sentences and they also 
freely mix with sentence level adjuncts. This 
means that a verbal argument structure of the high 
level rule database with its normal context free rule 
interpretation would only cover a fraction of its 
real world realizations. Rule conversion effectively 
handles this problem by converting rules describ-
ing lexical items with argument structures ex-
pressed using a context free rule formalism into 
run time rules that do not actually combine con-
stituents, but only check the saturation of valency 
frames. Constituents are combined by other more 
generic rules that take care of saturating the argu-
ment slots. This means that while the high level 
and the run time rules have a similar syntax, the 
semantics of some high level rules may be very 
different from similar rules in the low level rule 
database. 
5 Handling sentences with no full parse 
The system must not break down if the input 
sentence happens not to have a full parse (this in-
evitably happens in the case of real life texts). In 
that case, it reverts to using a heuristic process that 
constructs an output by combining the output of a 
selected set of partial structures covering the whole 
sentence stored during parsing the input. In the 
MetaMorpho terminology, this is called a ?mosaic 
translation?. Mosaic translations are usually subop-
timal, because in the absence of a full parse some 
structural information such as agreement is usually 
lost. There is much to improve on the current algo-
rithm used to create mosaic translations: e.g. it 
does not currently utilize a statistical model of the 
target language, which has a negative effect on the 
fluency of the output. Augmenting the system with 
such a component would probably improve its per-
formance considerably. 
6 Motivation for the MetaMorpho archi-
tecture 
An obvious drawback of the architecture de-
scribed above compared to the interlingua and 
transfer based systems is that the grammar compo-
nents of the system cannot be simply reused to 
build translation systems to new target languages 
without a major revision of the grammar. While in 
a classical transfer based system, the source lan-
guage grammar may cover phenomena that the 
transfer component does not cover, in the Meta-
Morpho architecture, this is not possible. In a 
transfer based system, there is a relatively cheaper 
way to handle coverage issues partially by aug-
menting only the source grammar (and postponing 
113
creation of the corresponding transfer rules). This 
is not an option in the MetaMorpho architecture. 
The main motivation for this system architec-
ture was that it makes it possible to integrate ma-
chine translation and translation memories in a 
natural way and to make the system easily extensi-
ble by the user. There is a grammar writer?s work-
bench component of MetaMorpho called Rule 
Builder. This makes it possible for users to add 
new, lexical or even syntactic patterns to the 
grammar in a controlled manner without the need 
to recompile the rest, using an SQL database for 
user added entries. The technology used in Rule-
Builder can also be applied to create a special 
combination of the MetaMorpho machine transla-
tion tool and translation memories (Hod?sz, 
Gr?bler and Kis 2004).  
Moreover, existing bilingual lexical databases 
(dictionaries of idioms and collocations) are rela-
tively easy to convert to the high level rule format 
of the system. The bulk of the grammar of the sys-
tem was created based on such resources. Another 
rationale for developing language pair specific 
grammars directly is that this way distinctions in 
the grammar of the source language not relevant 
for the translation to the target language at hand 
need not be addressed.  
7 Performance in the translation task 
During development of the system and its grammar 
components, regression testing has been performed 
using a test set unknown to the developers measur-
ing case insensitive BLEU with three human refer-
ence translations. Our usual test set for the system 
translating from Hungarian to English contains 274 
sentences of newswire text. We had never used 
single reference BLEU before, because, although 
creating multiple translations is expensive, single 
reference BLEU is quite unreliable usually produc-
ing very low scores especially if the target lan-
guage is morphologically rich, like Hungarian. 
The current version of the MetaMorpho system 
translating from Hungarian to English has a BLEU 
score of 22.14 on our usual newswire test set with 
three references. Obtaining a BLEU score of 7.8 on 
the WMT08 shared Hungarian to English transla-
tion task test set was rather surprising, so we 
checked single reference BLEU on our usual test 
set: the scores are 13.02, 14.15 and 16.83 with the 
three reference translations respectively.  
In the end, we decided to submit our results to the 
WMT08 shared translation task in spite of the low 
score. But we think, that these figures cast doubts 
on the quality of the texts and reference transla-
tions in the test set, especially in cases where both 
the English and the Hungarian text were translated 
from a third language, so we think that the scores 
on the WMT08 test set should be evaluated only 
relative to other systems? performance on the same 
data and the same language pair. 
References  
Emmon Bach. 1976. An extension of classical transfor-
mational grammar. In Saenz (ed.) Problems of Lin-
guistic Metatheory: Proceedings of the 1976 
Conference, 183?224. East Lansing, MI: Michigan 
State University. 
G?bor Hod?sz, Tam?s Gr?bler and Bal?zs Kis. 2004. 
Translation memory as a robust example-based trans-
lation system. In Hutchins (ed.), 82?89.  
John Hutchins (ed.) Broadening horizons of machine 
translation and its applications. Proceedings of the 
9th EAMT Workshop, 26?27 April 2004. La Val-
letta: Foundation for International Studies. 
G?bor Pr?sz?ky and Bal?zs Kis. 1999. Agglutinative 
and other (highly) inflectional languages. In Robert 
Dale & Kenneth W. Church (eds.) Proceedings of the 
37th Annual Meeting of the Association for Computa-
tional Linguistics, 261?268. Morristown, NJ: Asso-
ciation for Computational Linguistics. 
G?bor Pr?sz?ky and Attila Nov?k. 2005. Computational 
Morphologies for Small Uralic Languages. In: A. 
Arppe, L. Carlson, K. Lind?n, J. Piitulainen, M. 
Suominen, M. Vainio, H. Westerlund, A. Yli-Jyr? 
(eds.): Inquiries into Words, Constraints and Con-
texts Festschrift in the Honour of Kimmo Kosken-
niemi on his 60th Birthday, 116?125. Gummerus 
Printing, Saarij?rvi/CSLI Publications, Stanford. 
G?bor Pr?sz?ky and L?szl? Tihanyi. 2002 MetaMor-
pho: A Pattern-Based Machine Translation System. 
In: Proceedings of the 24th 'Translating and the 
Computer' Conference, 19?24. ASLIB, London, 
United Kingdom. 
G?bor Pr?sz?ky, L?szl? Tihanyi and G?bor Ugray. 
2004. Moose: A robust high-performance parser and 
generator. In Hutchins (ed.), 138?142. 
114
