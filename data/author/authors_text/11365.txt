Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1388?1397,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Automatic Acquisition of the Argument-Predicate Relations
from a Frame-Annotated Corpus
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Theodore Alexandrov
University of Bremen
theodore@
math.uni-bremen.de
Tonio Wandmacher
University of Osnabru?ck
twandmac@uos.de
Abstract
This paper presents an approach to au-
tomatic acquisition of the argument-
predicate relations from a semantically
annotated corpus. We use SALSA, a
German newspaper corpus manually an-
notated with role-semantic information
based on frame semantics. Since the rel-
atively small size of SALSA does not al-
low to estimate the semantic relatedness
in the extracted argument-predicate pairs,
we use a larger corpus for ranking. Two
experiments have been performed in or-
der to evaluate the proposed approach.
In the first experiment we compare au-
tomatically extracted argument-predicate
relations with the gold standard formed
from associations provided by human sub-
jects. In the second experiment we cal-
culate correlation between automatic relat-
edness measure and human ranking of the
extracted relations.
1 Introduction
There are many debates in lexical semantics about
what kind of world knowledge actually belongs
to the meaning of a lexeme. Nowadays, it is
widely accepted that predicates impose selectional
restrictions on their arguments. For example, since
we know that the predicate to be hungry mainly
takes expressions describing animate beings as ar-
guments, we can correctly resolve the anaphora
in the following sentence: We gave the bananas
to the monkeys because they were hungry. There
exists also multiple linguistic evidence showing
that the semantics of arguments can help to pre-
dict implicit predicates. For example, the sentence
John finished the cigarette usually means John fin-
ished smoking the cigarette because the meaning
of the noun cigarette is strongly associated with
the smoking activity.
It has been claimed that information about pred-
icates associated with nouns can be helpful for
a variety of tasks in natural language processing
(NLP), see for example (Pustejovsky et al, 1993;
Voorhees, 1994). However, at present there exists
no corresponding lexical semantic resource. Sev-
eral approaches have been presented that aim at
creating a knowledge base containing noun-verb
relations. There are two main research paradigms
for developing such knowledge bases. The first
paradigm assumes manual development of the re-
source (Pustejovsky et al, 2006), while the sec-
ond one relies on automatic acquisition methods,
see for example (Cimiano and Wenderoth, 2007).
In this paper we propose a procedure for auto-
matic acquisition of argument-predicate relations
from a semantically annotated corpus. In line with
(Lapata and Lascarides, 2003) our approach is
based on the assumption that predicates are omit-
ted in a discourse when they are highly predictable
from the semantics of their arguments. We exploit
SALSA (Burchardt et al, 2006), a German news-
paper corpus manually annotated with FrameNet
frames based on frame semantics. Using a man-
ually annotated corpus for relation extraction has
one particular advantage compared to extraction
from plain text: the type of an argument-predicate
relation is already annotated; there is no need to
determine it by automatic means which are usu-
ally error-prone. However, the relatively small
size of SALSA does not allow to make relevant
predictions about the degree of semantic related-
ness in the extracted argument-predicate pairs, see
section 4. We therefore employ a considerably
larger unannotated corpus for weighting. The re-
sults are evaluated quantitatively against human
judgments obtained experimentally. The proposed
evaluation procedure is similar to that presented in
(Cimiano and Wenderoth, 2007). First, we create
a gold standard for 30 words from the argument
list and evaluate our approach with respect to this
1388
gold standard. Second, we provide results from
an evaluation in which test subjects are asked to
rate automatically extracted relations using a four-
point scale.
The paper is structured as follows: Section 2
describes some linguistic phenomena requiring in-
ferences of an implicit predicate from the seman-
tics of an explicitly given argument. In section 3
we give a short overview of the related work. Sec-
tions 4 discusses the SALSA corpus. Section 5 in-
troduces our approach. Finally, section 6 describes
an experimental evaluation of the presented ap-
proach and section 7 concludes the paper.
2 Implicit Predicates
In this section we discuss some linguistic phenom-
ena requiring inferences of an implicit predicate
from the semantics of an explicitly given argument
for their resolution. One of the most studied phe-
nomena that Pustejovsky (1991) has called logical
metonymy is illustrated by the examples (1a) and
(1b) below. In the case of logical metonymy an im-
plicit predicate is inferable from particular verb-
noun and adjective-noun pairs in a systematic way.
The verb anfangen ?to start? and the adjective kom-
pliziert ?complicated? in the mentioned examples
semantically select for an event, while the nouns
(Buch ?book? and Frage ?question? respectively)
have a different semantic type. However, the set
of the most probable implicit predicates is pre-
dictable from the semantics of the nouns. Thus,
(1a) plausibly means Als ich angefangen habe,
dieses Buch zu lesen/schreiben... ?When I have
started to read/write this book...? and (2a) plau-
sibly means eine Frage die kompliziert zu beant-
worten ist ?a question which is complicated to an-
swer?.
Example 1
(a) Als ich mit diesem Buch angefangen habe...
?When I have started this book...?
(b) eine komplizierte Frage
?a complicated question?
(c) Studentenfutter
?student food?
(d) Nachrichtenagentur Xinhua u?ber Beziehun-
gen beider Seiten der Taiwan-Strasse
?News agency Xinhua about relations of both
sides of the Taiwan Strait?
(e) Hans ist beredt
?Hans is eloquent?
As we can see from Example 1, besides logi-
cal metonymy there are other linguistic phenom-
ena requiring knowledge about predicates associ-
ated with an argument for their resolution. Exam-
ple (1c) contains a noun compound which can be
interpreted on basis of the meaning of the noun
Futter ?food?. In general, noun compounds can be
interpreted in many different ways depending on
the semantics of the constituencies: morning cof-
fee is a coffee which is drunk in the morning, brick
house is a house which is made of bricks etc. In
case of (1c) the relation via the predicate essen ?to
eat? taking Studenten ?students? as a subject and
Futter ?food? as an object seems to be the most
plausible one.
The phrase (1d) is a title of a newspaper ar-
ticle. As in the previous examples, a predicate
is left out in (1d). The meaning of the prepo-
sition u?ber ?about? can help to narrow down the
set of possible predicates, but still allows an in-
adequately large range of interpretations. How-
ever, the semantics of the noun Nachrichtenagen-
tur ?news agency? supports such interpretations as
berichten ?to report? or informieren ?to inform?.
Most of the literature discusses predicates infer-
able from nouns. However, other parts of speech
can support similar inferences. In example (1e) a
predicate is predictable on the basis of the mean-
ing of the adjective beredt ?eloquent?. The sen-
tence (1e) most plausibly means that Hans speaks
eloquently.
Example 1 shows that knowledge about pred-
icates associated with explicitly given arguments
can help to deal with several linguistic phenom-
ena. The cases when a predictable predicate is left
out are not rare in natural language. For example,
for logical metonymy a corpus study has shown
that the constructions like begin V NP occur rarely
if the verb V corresponds to a highly plausible in-
terpretation of begin NP (Briscoe et al, 1990).
3 Related Work
The most influential account of logical metonymy
is provided by Pustejovsky?s theory of the Gen-
erative Lexicon, GL (Pustejovsky, 1991). Ac-
cording to Pustejovsky the meaning of a noun in-
cludes a qualia structure representing ?the essen-
tial attributes of an object as defined by the lexi-
cal item?. Thus, the lexical meaning of the noun
book includes read and write as qualia roles. In
the framework of GL, Pustejovsky et al (2006)
1389
are manually developing the Brandeis Semantic
Ontology which is a large generative lexicon on-
tology and dictionary. There also exist several ap-
proaches to automatic acquisition of qualia struc-
tures from text corpora which aim at supporting
the time-consuming manual work. For example,
Pustejovsky et al (1993) use generalized syntac-
tic patterns for extracting qualia structures from a
partially parsed corpus. Cimiano and Wenderoth
(2007) suggest a pattern-based method for auto-
matic extraction of qualia structures from the Web.
The results of the human judgment experiment re-
ported in (Cimiano and Wenderoth, 2007) suggest
that the automatic acquisition of qualia structures
is a difficult task. Human test subjects have shown
a very low agreement (11,8% average agreement)
in providing qualia structures for given nouns.
Another line of research on inferring implicit
predicates concerns using information about col-
locations derived from corpora. For example,
Lapata and Lascarides (2003) resolve logical
metonymy on the basis of the distribution of para-
phrases like finish the cigarette ? finish smok-
ing the cigarette and easy problem ? problem
which is easy to solve in a corpus. This approach
shows promising results, but it is limited to logi-
cal metonymy. Similarly, Nastase et al (2006) use
grammatical collocations for defining semantic re-
lations between constituents in noun compounds.
In our study we aim at extracting intuitively
plausible argument-predicate relations from a se-
mantically annotated corpus. Using an annotated
corpus we avoid problems of defining types of
these relations by automatic means which are usu-
ally error-prone. We represent argument-predicate
relations in terms of FrameNet frames which al-
low for a fine-grained and grounded representation
supporting paraphrasing, see next sections. Our
approach is not restricted to nouns. We also con-
cern relations where argument positions are filled
by adjectives, adverbs or even verbs.
4 The SALSA Corpus
For relation extraction we have chosen the SALSA
corpus (Burchardt et al, 2006) developed at Saar-
land University. SALSA is a German corpus
manually annotated with role-semantic informa-
tion, based on the syntactically annotated TIGER
newspaper corpus (Brants et al, 2002). The
2006 SALSA release which we have used con-
tains about 20 000 annotated predicate instances.
The corpus is annotated with the set of FrameNet
frames.
The FrameNet, FN (Ruppenhofer et al, 2006),
lexical resource is based on frame semantics (Fill-
more, 1976), see http://framenet.icsi.berkeley.edu.
The lexical meaning of predicates in FN is ex-
pressed in terms of frames (approx. 800 frames)
which are supposed to describe prototypical sit-
uations spoken about in natural language. Every
frame contains a set of roles (or frame elements,
FEs) corresponding to the participants of the de-
scribed situation. Predicates with similar seman-
tics are assigned to the same frame, e.g. to give
and to hand over refer to the GIVING frame. Con-
sider a FN annotation for the sentence (2a) below.
In this annotation DONOR, RECIPIENT and THEME
are roles in the frame GIVING and John, Mary and
a book are fillers of these roles. The FN anno-
tation generalizes across near meaning-preserving
transformations, see (2b).
Example 2
(a) [John]DONOR [gave]GIVING
[Mary]RECIPIENT [a book]THEME.
(b) [John]DONOR [gave]GIVING [a
book]THEME [to Mary]RECIPIENT.
In FN information about syntactic realization
patterns of frame elements as well as information
about frequency of occurrences of these patterns in
corpora is provided. For example, the role DONOR
in the frame GIVING is most frequently filled by a
noun phrase in the subject position or by a prepo-
sitional phrase with the preposition by as the head
in the complement position.
The FN project originally aimed at developing a
frame-semantic lexicon for English. Later on FN
frames turned out to be to a large extent language
independent (Burchardt et al, 2006). In most of
the cases German predicates could be successfully
described by the FN frames. However, some of
the frames required adaptation to the German data,
e.g. new FEs were introduced. Since FN does not
cover all possible word senses, new frames needed
to be added for some of the predicates.
We have chosen the SALSA corpus for our
experiments because to our knowledge it is the
only freely available corpus which contains both
syntactic and role-semantic annotation. However,
we are aware that SALSA (approx. 700 000
tokens) is too small to compute a reliable co-
occurrence model for measuring plausibility of the
extracted argument-predicate relations, though it
1390
is relatively large for a manually annotated cor-
pus. As it was shown in (Bullinaria and Levy,
2007), co-occurrence-based approaches need very
large training corpora in order to reliably compute
semantic relatedness. The SALSA corpus, com-
prising less than 1 million tokens, is too small for
this purpose. Moreover, a considerable number of
predicates in SALSA appeared to be unannotated.
Some of the high frequency pairs, as for exam-
ple Bombe, explodieren ?bomb, to explode?, occur
in SALSA only once, just as occasional pairs like
Deutsche, entdecken ?German, to discover?. We
have tried to overcome the size problems by using
a larger unannotated corpus for recomputing the
rating of our resulting relations, see next section.
5 Automatic Acquisition of the
Argument-Predicate Relations
In line with (Lapata and Lascarides, 2003), our ap-
proach to extraction of argument-predicate (AP)
relations is based on two assumptions:
A1: If predicates are highly predictable from the
semantics of their arguments then they can be
omitted in a discourse;
A2: If a predicate frequently takes a word as an
argument then it is highly predictable from the se-
mantics of this word.
In the proposed experimental setting argument-
predicate relations are defined in terms of the
FrameNet frames. Thus, we aim at extracting
from SALSA tuples of the form ?Argument, ROLE,
FRAME, Predicate? such that the Argument plau-
sibly fills the ROLE in the FRAME evoked by the
Predicate. As already mentioned in section 3,
our approach is not restricted to nouns. We also
treat arguments expressed by other content parts
of speech. The proposed relation extraction pro-
cedure consists in
? finding for every content word which occurs
in the corpus a set of predicates taking this
word as an argument with a high probability;
? defining a relation between the word and ev-
ery predicate from this set by finding which
roles the noun fills in frames evoked by the
predicate;
? estimating the degree of the semantic relat-
edness in the extracted argument-predicate
pairs.
For example, analyzing the following sentence
[Fu?nf Oppositionelle]SUSPECT sind in Ebe-
biyin [von der Polizei]AUTHORITIES [festgenom-
men]ARREST worden.
?Five members of the opposition have been
arrested by the police in Ebebiyin.?
we aim at extracting the following tuples:
Argument Role Frame Predicate
Oppositionell SUSPECT ARREST festnehmen
Polizei AUTHORITIES ARREST festnehmen
Relation Extraction
In SALSA, every sentence is annotated with a set
of frames in such a way that for every frame its
FEs refer to some syntactic constituents in the sen-
tence. In order to extract argument-predicate rela-
tions from SALSA we need 1) to find a content
head for every constituent corresponding to a FE;
2) to resolve possibly existing anaphora. Since
SALSA is syntactically annotated, the first task
proved to be relatively easy.1 On the contrary,
anaphora resolution is well-known to be one of
most challenging NLP tasks. In our study, we
do not focus on it, and we treat only pronominal
anaphora using the following straightforward res-
olution algorithm: given a pronoun the first noun
which agrees in number and gender with the pro-
noun is supposed to be its antecedent. In order
to evaluate this resolution procedure we have in-
spected 100 anaphoric cases. In approximately
three fourths of the cases the anaphora were re-
solved correctly. Therefore, we have assigned a
confidence rate of 0,75 to the FE fillers resulting
from a resolved anaphora. In non-anaphoric cases
a confidence rate of 1 was assigned.
For every extracted tuple of the form
?Argument, ROLE, FRAME, Predicate? we
have summed up the corresponding confidence
rates. Finally, we have obtained around 30 000
tuples with confidence rates ranging from 0,75
to 88. It is not surprising that most of the argu-
ments appeared to be nouns, while most of the
predicates are expressed by verbs. Since SALSA
has been annotated manually, there are almost
no mistakes in defining types of the semantic
1We have excluded from the consideration foreign-
language expressions, while proper nouns were treated in the
usual way. For verb phrases with auxiliary or modal verbs as
heads the main verb was taken as a corresponding role filler.
1391
relations between arguments and predicates.2 For
several pairs, the semantic relation between an
argument and a predicate is ambiguous. Consider
the tuples extracted for the word pair Buch,
schreiben ?book, to write? which are given below.
While the first tuple corresponds to phrases like
ein Buch schreiben ?to write a book?, the second
one abstracts from the expressions like in einem
Buch schreiben ?to write in a book?.
Argument Role Frame Predicate
Buch TEXT TEXT CREATION schreiben
Buch MEDIUM STATEMENT schreiben
Additionally, ambiguity can arise because of the
annotation disagreements in SALSA. For exam-
ple, the pair (Haft, sitzen) ?imprisonment?, ?to sit?
in Table 1 was annotated in SALSA both with the
BEING LOCATED and with the POSTURE frames.
As mentioned in section 4, a considerable num-
ber of predicates in SALSA is not annotated se-
mantically. In order to find out how many relevant
AP-relations get lost if we consider only seman-
tically annotated predicates, we have additionally
extracted AP-pairs on the basis of the syntactic an-
notation only. The anaphora resolution procedure
as described above was again applied to the syn-
tactic argument heads. We have obtained around
56 500 pairs with confidence rates ranging from
0,75 to 71,50.3
As one could expect, being a newspaper corpus
SALSA appeared to be thematically unbalanced.
The most frequent argument-predicate relations
occurring in SALSA reflect common topics dis-
cussed in newspapers: economics (e.g. (Prozent,
steigen), ?percent?, ?to increase?), criminality (e.g.
(Haft, verurteilen) ?imprisonment?, ?to sentence?),
catastrophes (e.g. (Mensch, to?ten) ?human?, ?to
kill?) etc.
Ranking
As mentioned in section 4, the size of SALSA
does not allow to make relevant predictions about
the distribution of frames and role fillers. Only
2% of the relations occur in SALSA more then
3 times. In order to overcome this problem we
have developed a measure of semantic relatedness
between the extracted arguments and predicates
2Mistakes can arise only because of the annotation errors
and errors in the anaphora resolution procedure.
3The comparison of the results obtained by the extraction
procedure based on the semantic annotation with the results
of the procedure based on the syntactic annotation only is
provided in the next section.
which takes into account their co-occurrence in a
larger and more representative corpus. For com-
puting semantic relatedness we have used a lem-
matized newspaper corpus (Su?ddeutsche Zeitung,
SZ) of 145 million words. Given a tuple t with a
confidence rate c containing an argument a and a
predicate p, the relatedness measure rm of t was
computed as follows:
rm(t) = lsa(a, p) + c/max(c),
where the lsa(a, p) is based on Latent Semantic
Analysis, LSA (Deerwester et al, 1990). LSA is
a vector-based technique that has been shown to
give reliable estimates on semantic relatedness. It
makes use of distributional similarities of words
in text and constructs a semantic space (or word
space) in which every word of a given vocabulary
is represented as a vector. Such vectors can then
be compared to one another by the usual vector
similarity measures (e.g. cosine). We calculated
the LSA word space using the Infomap toolkit10
v. 0.8.6 (http://infomap-nlp.sourceforge.net). The
co-occurrence matrix (window size: 5 words)
comprised 80 000?3 000 terms and was reduced
by SVD to 300 dimensions. For the vector com-
parisons the cosine measure was applied. To those
words which did not occur in the analyzed SZ cor-
pus (approx. 3500 words) a lsa measure of 0 was
assigned. To provide a comparable contribution to
rm, the confidence rates c extracted from SALSA
are divided by the maximal confidence rate. The
rm function is a linear interpolation of the lsa and
the normalized c measure. As mentioned above,
the c measure is a discriminative factor for only
2% of the relations. For the remaining 98% the
normalized c values are small (0,003 or 0,002 or
0,001). Therefore, calculating the rm measure we
mainly rely on lsa, while normalized c actually
plays a role only for the relations frequently oc-
curring in SALSA. Table 1 contains the 5 most se-
mantically related predicates for an example argu-
ment.
6 Evaluation
Since the extracted argument-predicate relations
are intended to be used for inferring intuitively ob-
vious predicates,we evaluate to which extend they
correspond to human intuition.
1392
Table 1: Examples of the extracted argument-predicate relations
Argument Role Frame Predicate rm
Haft FINDING VERDICT verurteilen ?to sentence? 0,939
?imprisonment? LOCATION BEING LOCATED sitzen ?to sit? 0,237
LOCATION POSTURE sitzen ?to sit? 0,226
MESSAGE REQUEST fordern ?to demand? 0,153
BAD OUTCOME RUN RISK-FNSALSA drohen ?to threaten? 0,144
Gold Standard
Similar to (Cimiano and Wenderoth, 2007) we
provide a gold standard for 30 test arguments oc-
curring in the SALSA corpus. The test argu-
ments were selected randomly from the set of
those arguments that have more than one pred-
icate associated with them such that a value of
argument-predicate relatedness exceeds the aver-
age one. These words were nearly uniformly dis-
tributed among 20 participants of the experiment,
who were all non-linguists. We also ensured that
each word was treated by three different subjects.
For every word we asked our subjects to write be-
tween 5 and 10 short phrases that contain a pred-
icate taking the given word as an argument, e.g.
book ? to read a book. The participants were asked
to provide phrases instead of single predicates, be-
cause we wanted to control the syntactic and se-
mantic position of the arguments. The participants
received an instruction informally describing the
notion of predicate and what kind of phrases they
are supposed to come up with. Besides the task
description they were shown examples containing
appropriate and inappropriate phrases. Some of
the examples are given below.
Example 3
(a) Aktie ?stock? : Kauf der Aktien ?buying of
stocks?, Aktien kaufen ?to buy stocks?, Aktien an
der Bo?rse ?stocks on the bourse? (is inappropriate
because the word ?bourse? describes a place and
not an event)
(b) beredt ?eloquent?: beredt sprechen ?to
speak eloquently?, ein beredter Sprecher ?an elo-
quent speaker? (is inappropriate because the word
?speaker? describes a person and not an event)
The test was conducted via e-mail. In or-
der to compare the human associations with the
extracted AP-relations, we have manually anno-
tated the obtained phrases with SALSA frames.
The agreement for the described task for every
cue word was calculated as the averaged pairwise
agreement between the AP-relations delivered by
the three subjects, S
1
, S
2
and S
3
, as follows:
Agr =
|S
1
?S
2
|
|S
1
?S
2
|
+
|S
2
?S
3
|
|S
2
?S
3
|
+
|S
2
?S
3
|
|S
2
?S
3
|
3
.
Agreement results for every cue word are re-
ported in table 2. Second column of the table
contains gold standard predicates which were pro-
vided by all 3 participants treating the same word.4
Averaging over all words, we got a mean agree-
ment of 13%. Though this value seems to be low,
it is consistent with a mean agreement of 11,8%
for a similar task reported in (Cimiano and Wen-
deroth, 2007), see section 3. Cimiano and Wen-
deroth (2007) show that the lowest agreement is
yielded for more abstract words, while the agree-
ment for very concrete words is reasonable. We
could not make a similar observation, see table 2.
Comparison with the Gold Standard
In the first experiment we checked whether pred-
icates which people associate with the test argu-
ments can be automatically extracted by our pro-
cedure. For this aim we compared the gold stan-
dard with all automatically extracted argument-
predicate relations5 containing some of the 30 cue
words as follows. These relations were ranked ac-
cording to the relatedness measure described in
previous section. In line with (Cimiano and Wen-
deroth, 2007) we exploited an approach common
in information retrieval for estimating the qual-
ity of correspondence of a ranked output to a
gold standard, see (Baeza-Yates and Ribeiro-Neto,
1999).
Given some n automatically extracted relations
with the highest ranking we calculated a precision-
recall curve expressing precision and recall of our
procedure compared to the gold standard. The pre-
cision characterizes the procedure exactness, i.e.
how many redundant relations are retrieved. The
4The overall gold standard consists of 33 tuples.
5In order to evaluate the procedure extracting AP-
relations on the basis of the semantic annotation we com-
pared automatically extracted tuples to the gold standard tu-
ples. For the procedure using the syntactic annotation only
the AP-pairs were considered without regarding frames and
FEs.
1393
recall measures the completeness, i.e. how many
relations of the gold standard are extracted auto-
matically. For each point of the curve (which is
a pair (p, r) of values of precision p and recall r)
we calculated the F -measure as F = 2pr/(p+ r)
which is the harmonic mean between recall and
precision. The precision-recall curve is a set of
precision values for the prespecified recall levels
varying from 0 to 1 with a step 0,1. Then, to pro-
duce only one value evaluating the quality of the
ranked output compared to the gold standard, for
each precision-recall curve we calculated F
max
,
the maximal value of the F -measure achieved
for the points of this curve. F
max
expresses the
best trade-off between precision and recall for the
given ranked output. Finally, among all possible
n (numbers of the considered relations with the
highest ranking) we selected that one which pro-
vides the maximal F
max
value.
The resulting maximal F
max
values are 0,47 for
the procedure extracting AP-relations on the basis
of the semantic annotation and 0,41 for the pro-
cedure using the syntactic annotation only. We
compared these results with the baseline results
of maximal F
max
values produced for the output
with random ranking. The calculation of the base-
line was repeated 100 times, each time a new ran-
dom ranking was generated. The lowest baseline
results are 0,08/0,06 (semantic/syntactic annota-
tion), the highest are 0,18/0,14 and the medians
are 0,1/0,07. One can see that the results produced
using the relatedness measure (0,47/0,41) greatly
exceed the baseline. Based on this comparison we
conclude that the ranking done using the related-
ness measure brings a significant advantage. The
values of precision and recall for the reported max-
imal F
max
values are 0,5/0,33 (semantic/syntactic
annotation) and 0,45/0,54 respectively. This re-
sults show that half of the AP-relations from the
gold standard appeared to be in the list of the top-
ranked tuples extracted by the ?semantic? proce-
dure, while the size of this list (n = 28) was al-
most equal to the size of the gold standard (33).
The differences in performance between the ?se-
mantic? and ?syntactic? procedures could be ex-
plained by the fact that the ?syntactic? procedure
finds in the corpus more related predicates for ev-
ery argument than the ?semantic? one. Neverthe-
less, the ?semantic? procedure shows better per-
formance.
Next we investigated the results for each argu-
ment used in the gold standard separately in the
same way as described above. For each argument
the F
max
measure has been computed. Because
of the low agreement between the subjects ques-
tioned for the gold standard (see above), in these
calculations we considered all predicates reported
by our subjects. The calculated F
max
values are
reported in table 2 which shows a correlation be-
tween F
max
values calculated for the ?semantic?
and ?syntactic? procedures. However, there is
no correlation with human agreement. This issue
needs a further investigation, see section 7.
Human Judgments of the Relatedness
Following (Cimiano and Wenderoth, 2007), in or-
der to check whether the calculated relatedness
is reasonable according to human intuition, we
have performed another experiment. For each of
the 30 words selected for the gold standard we
selected the 5 top ranked predicates. Since for
some of the cue arguments only 3 predicates were
found in the corpus, the final test set contains only
138 argument-predicate tuples. From these tuples
we generated short grammatically correct phrases
structurally similar to those in example 3. These
phrases were uniformly distributed among 10 sub-
jects so that every phrase was evaluated by one
subject. The participants were asked to rate the
phrases with respect to their naturalness using a
scale from 0 to 3, whereby 0 means ?unnatural?,
1 ?possible?, 2 ?natural? and 3 ?totally natural and
self-evident?.
Further on we investigated the relationship be-
tween the human estimates and the relatedness
values obtained automatically. For this aim we
used the Spearman rank correlation coefficient.
Because of four-points scale used, the human
rankings are equal for many tuples which lead to
the so-called effect of ties. For this reason we
computed the correlation coefficient with a cor-
rection for ties. The coefficient value is 0,30 and
this correlation is statistically significant with p-
value 0,0006. Based on these results we conclude
that our relatedness measure is correlated with hu-
man judgments. Taking into account the subjec-
tive character of human ranking in terms of nat-
uralness, the achieved correlation values can be
considered as high.
1394
Table 2: Evaluation results for 30 gold standard cue words.
Cue word Shared predicates Agr Sem. F
max
Syn. F
max
Name ?name? haben ?to have? 14% 0,2 0,48
Urlaub ?vacation? fahren ?to go? 8% 0,13 0,16
Sprache ?language? sprechen ?to speak?, lernen ?to learn? 14% 0,4 0,3
Strafe ?fine? verurteilen ?to sentence? 11% 0,21 0,3
Stuhl ?chair? sitzen ?to sit? 14% 0,1 0,2
Bombe ?bomb? hochgehen ?to blow up? 14% 0,11 0,22
Blatt ?gazette?, ?page?, ?leaf? ? 2% 0 0
Flughafen ?airport? ankommen ?to arrive?, fahren ?to go? 21% 0,17 0,17
Gesetz ?low? ? 8% 0,17 0,38
Polizei ?police? rufen ?to call? 11% 0,22 0,23
Kompromiss ?compromise? schliessen ?to make? 15% 0,07 0,29
Fluggesellschaft ?airline? ? 3% 0,11 0,38
Antrag ?proposal?, ?application? stellen ?to introduce?, ablehnen ?to decline? 24% 0,43 0,42
Zeitung ?newspaper? lesen ?to read? 13% 0,17 0,09
Brief ?letter? verschicken ?to send?, schreiben ?to write? 19% 0,23 0,12
Flu?chtling ?refugee? aufnehmen ?to accept? 13% 0 0,07
Buch ?book? schreiben ?to write?, lesen ?to read? 15% 0,44 0,39
Za?hler ?counter? ablesen ? to read? 11% 0 0
Anzahl ?number? ? 3% 0,23 0,19
Prozent ?percent? ? 3% 0,48 0,21
Ziel ?goal? verfehlen ?to miss?, erreichen ?to reach? 20% 0,3 0,48
Schule ?school? schwa?nzen ?to miss?, gehen ?to go? 22% 0,13 0,23
Amt ?position?, ?department? bekleiden, innehaben ?to hold?, gehen ?to go? 20% 0 0,17
Frage ?question? beantworten ?to answer?, stellen ?to ask? 20% 0,15 0,37
Mensch ?human? sein ?to be? 16% 0,09 0,03
Zeuge ?witness? aussagen ?to testify?, sein ?to be? 22% 0,13 0,19
Thema ?theme? ? 7% 0,14 0,26
Preistra?ger ?prize winner? ? 5% 0,08 0,08
Initiative ?initiative? ergreifen ?to take? 17% 0,1 0,13
Wohnung ?flat? ? 7% 0,09 0,17
7 Conclusion and Discussion
In this paper we presented an approach to auto-
matic extraction of argument-predicate relations
from a frame-annotated corpus.6 In our approach
we aimed to combine the advantages offered by
annotated and unannotated lexical resources. Be-
sides extracting AP-pairs the proposed method al-
lows us to define types of semantic relations in
terms of FrameNet frames. The proposed proce-
dure is not restricted to arguments expressed by
nouns and treats also other content parts of speech.
The main goal of this paper was to show that
though manually annotated corpora usually have
a relatively small size, they can be successfully
exploited for the relation extraction. An obvious
limitation of the presented approach is that it is
bounded to manual annotations which are hard
to obtain. However, since semantic annotations
are useful for many different goals in linguistics
and NLP, the number of reliable annotated cor-
pora constantly grows.7 Moreover, recently sev-
6The complete list of the extracted AP-relations as well
as the results of the experiment will be available online at
http://www.ikw.uni-osnabrueck.de/?eovchinn/APrels/.
7At present FrameNet annotated corpora are
eral tools have been developed which perform role
annotation automatically, for example see (Erk
and Pado, 2006). Therefore we believe that ap-
proaches using semantic annotation are valid and
promising. In the future we plan to experiment
with large role-annotated corpora for English such
as PropBank (approx. 300 000 words, (Palmer
et al, 2005)) and the FrameNet-annotated corpus
provided by the FN project (more than 135 000
annotated sentences, (Ruppenhofer et al, 2006)).
Since these corpora do not contain syntactic anno-
tation, for extracting argument-predicate relations
we will need to parse annotated sentences.
There are several ways to improve the proposed
procedure. First, an implementation of a more
advanced anaphora resolution algorithm treating
pronominal as well as nominal anaphora should
significantly raise the precision/recall characteris-
tics. Second, splitting German compounds occur-
ring in the corpus should provide additional ev-
idence. We have treated such words as Kunde
?client? and Privatkunde ?private client? as differ-
ent lexemes, while they are strongly related se-
available for English, German and Spanish, see
http://framenet.icsi.berkeley.edu.
1395
mantically and information about predicates co-
occurring with the second word could probably
be used for describing the semantics of the first
one. Concerning relatedness measure, additional
corpus-based measures such as Web-based mea-
sures (Cimiano and Wenderoth, 2007) or measures
based on syntactic relations (Pustejovsky et al,
1993) could appear to be useful for improving the
ranking of the extracted relations.
The presented procedure was evaluated quanti-
tatively against human judgments obtained experi-
mentally. The participants of the experiment were
asked to provide short phrases containing given
cue words and predicates associated with these
words as well as to rate phrases generated from the
automatically extracted AP-relations. Concerning
the first experiment, the low human agreement has
shown that the proposed association task appeared
to be difficult for the subjects. Nevertheless, the
described learning procedure proved to extract in-
tuitively reasonable relations.
The evaluation strategy presented in this pa-
per on relies on the underlying assumptions (A1
and A2 in section 5) and is compatible with the
other approaches to relation extraction, cf. (Cimi-
ano and Wenderoth, 2007). However, it is plau-
sible that human responses in the context of pro-
viding associated predicates for target words will
differ from the responses in the experimental set-
tings where subjects are asked to infer implicit
predicates, e.g. to extend phrases containing im-
plicit predicates. In the future we plan to im-
plement a procedure making use of the extracted
AP-relations which would automatically extend
phrases containing implicit predicates. Then we
intend to compare output results of the procedure
with the human responses. Additionally, a study of
a possible correspondence between human agree-
ment on associated predicates and a semantic type
of an argument (e.g. concrete/abstract, natural
kind/artifact) should be performed on more test ar-
guments.
Potential Applications
As already mentioned in the literature, see for ex-
ample (Lapata and Lascarides, 2003), knowledge
about implicit predicates could be potentially use-
ful for a variety of NLP tasks such as language
generation, information extraction, question an-
swering or machine translation. Many applica-
tions of semantic relations in NLP are connected
to paraphrasing or query expansion, see for ex-
ample (Voorhees, 1994). Suppose that a search
engine or a question answering system receives
the query schnelle Bombe ?quick bomb?. Prob-
ably, in this case the user is interested in find-
ing information about bombs that explode quickly
rather then about bombs in general. Knowledge
about predicates associated with the noun Bombe
?bomb? could be used for predicting a set of prob-
able implicit predicates. For generation of the se-
mantically and syntactically correct paraphrases it
is sometimes not enough to guess the most prob-
able argument-predicate pairs. Information about
types of an argument-predicate relation could be
helpful, i.e. which semantic and syntactic posi-
tion does the argument fill in the argument struc-
ture of the predicate. For example, compare
eine Bombe explodiert schnell ?a bomb explodes
quickly? for schnelle Bombe with ein Buch schnell
lesen/schreiben ?to read/write a book quickly? for
schnelles Buch ?quick book?. In the first case the
argument Bombe fills the subject position, while
in the second case Buch fills the object posi-
tion. Since FrameNet contains information about
syntactic realization patterns for frame elements,
representation of argument-predicate relations in
terms of frames directly supports generation of se-
mantically and syntactically correct paraphrases.
The described procedure could also support
manual development of a lexical resource, provid-
ing evidence from corpora as well as the distribu-
tional information.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
Harlow, 1. aufl. edition.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Ted Briscoe, Ann Copestake, and Bran Boguraev.
1990. Enjoy the paper: Lexical semantics via lex-
icology. In Proceedings of the 13th International
Conference on Computational Linguistics, pages
42?47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
1396
2006. The SALSA corpus: A German corpus re-
source for lexical semantics. In Proceedings of
LREC 2006, pages 969?974.
Philipp Cimiano and Johanna Wenderoth. 2007. Auto-
matic Acquisition of Ranked Qualia Structures from
the Web. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 888?895.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. American
Society of Information Science, 41(6):391?407.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a flexible toolbox for semantic role assignment. In
Proceedings of LREC 2006, Genoa, Italy.
Charles J. Fillmore. 1976. Frame semantics and the
nature of language. In Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, volume 280,
pages 20?32.
Mirella Lapata and Alex Lascarides. 2003. A Prob-
abilistic Account of Logical Metonymy. Computa-
tional Linguistics, 29(2):261?316.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina
Sokolova, and Stan Szpakowicz. 2006. Learning
noun-modifier semantic relations with corpus-based
and wordnet-based features. In Proceedings of the
AAAI 2006.
Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky, Peter Anick, and Sabine Bergler.
1993. Lexical semantic techniques for corpus anal-
ysis. Computational Linguistics, 19(2):331?358.
James Pustejovsky, Catherine Havasi, Roser Saur,
Patrick Hanks, Anna Rumshisky, Jessica Littman,
Jos Castao, and Marc Verhagen. 2006. Towards a
generative lexical resource: The Brandeis Semantic
Ontology. In Proceedings of the Fifth Language Re-
source and Evaluation Conference.
James Pustejovsky. 1991. The Generative Lexicon.
Computational Linguistics, 17(4):409?441.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 61?69.
1397
Abductive Reasoning with a Large Knowledge Base
for Discourse Processing
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Niloofar Montazeri
USC ISI
niloofar@isi.edu
Theodore Alexandrov
University of Bremen
theodore@uni-bremen.de
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Michael C. McCord
IBM Research
mcmccord@us.ibm.com
Rutu Mulkar-Mehta
USC ISI
me@rutumulkar.com
Abstract
This paper presents a discourse processing framework based on weighted abduction. We elabo-
rate on ideas described in Hobbs et al (1993) and implement the abductive inference procedure in a
system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowl-
edge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as
WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the
Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In
addition, we provide an evaluation of the semantic role labeling produced by the system taking the
Frame-Annotated Corpus for Textual Entailment as a gold standard.
1 Introduction
In this paper, we elaborate on a semantic processing framework based on a mode of inference called
abduction, or inference to the best explanation. In logics, abduction is a kind of inference which arrives
at an explanatory hypothesis given an observation. Hobbs et al (1993) describe how abductive reasoning
can be applied to the discourse processing problem viewing the process of interpreting sentences in
discourse as the process of providing the best explanation of why the sentence would be true. In this
framework, interpreting a sentence means 1) proving its logical form, 2) merging redundancies where
possible, and 3) making assumptions where necessary. As the reader will see later in this paper, abductive
reasoning as a discourse processing technique helps to solve many pragmatic problems such as reference
resolution, the interpretation of noun compounds, the resolution of some kinds of syntactic, and semantic
ambiguity as a by-product. We adopt this approach. Specifically, we use a system we have built called
Mini-TACITUS1 (Mulkar et al, 2007) that provides the expressivity of logical inference but also allows
probabilistic, fuzzy, or defeasible inference and includes measures of the ?goodness? of abductive proofs
and hence of interpretations of texts and other situations.
The success of a discourse processing system based on inferences heavily depends on a knowledge
base. The main contribution of this paper is in showing how a large and reliable knowledge base can be
obtained by exploiting existing lexical semantic resources and can be successfully applied to reasoning
tasks on a large scale. In particular, we experiment with axioms extracted from WordNet, see Fellbaum
(1998), and FrameNet, see Ruppenhofer et al (2006). In axiomatizing FrameNet we rely on the study
described in Ovchinnikova et al (2010).
We evaluate our inference system and the obtained knowledge base in recognizing textual entailment
(RTE). As the reader will see in the following sections, inferences carried out by Mini-TACITUS are
fairly general and not tuned for a particular application. We decided to test our approach on RTE because
this is a well-defined task that captures major semantic inference needs across many natural language
1http://www.rutumulkar.com/download/TACITUS/tacitus.php
225
processing applications, such as question answering, information retrieval, information extraction, and
document summarization. For evaluation, we have chosen the RTE-2 data set (Bar-Haim et al, 2006),
because besides providing text-hypothesis pairs and a gold standard this data set has been annotated with
FrameNet frame and role labels (Burchardt and Pennacchiotti, 2008) which gives us the possibility of
evaluating our frame and role labeling based on the axioms extracted from FrameNet.
2 NL Pipeline and Abductive Reasoning
Our natural language pipeline produces interpretations of texts given the appropriate knowledge base. A
text is first input to the English Slot Grammar (ESG) parser (McCord, 1990, 2010). For each segment,
the parse produced by ESG is a dependency tree that shows both surface and deep structure. The deep
structure is exhibited via a word sense predication for each node, with logical arguments. These logical
predications form a good start on a logical form (LF) for the whole segment. An add-on to ESG converts
the parse tree into a LF in the style of Hobbs (1985). The LF is a conjunction of predications, which have
generalized entity arguments that can be used for showing relationships among the predications. These
LFs are used by the downstream components.
The interpretation of the text is carried out by an inference system called Mini-TACITUS using
weighted abduction as described in detail in Hobbs et al (1993). Mini-TACITUS tries to prove the logical
form of the text, allowing assumptions where necessary. Where the system is able to prove parts of the
LF, it is anchoring it in what is already known from the overall discourse or from a knowledge base.
Where assumptions are necessary, it is gaining new information. Obviously, there are many possible
proofs in this procedure. A cost function on proofs enables the system to chose the ?best? (the cheapest)
interpretation. The key factors involved in assigning a cost are the following: 1) proofs with fewer
assumptions are favored, 2) short proofs are favored over long ones, 3) plausible axioms are favored over
less plausible axioms, and 4) proofs are favored that exploit the inherent implicit redundancy in text.
Let us illustrate the procedure with a simple example. Suppose that we want to construct the best
interpretation of the sentence John composed a sonata. As a by-product, the procedure will disambiguate
between two readings of compose, namely between the ?form? reading instantiated for example in the
sentence Three representatives composed a committee, and the ?create art? meaning instantiated in the
given sentence. After being processed by the parser, the sentence will be assigned the following logical
form where the numbers (20) after every proposition correspond to the default costs of these proposi-
tions.2 The total cost of this logical form is equal to 60.
John(x1):20 & compose(e1,x1,x2):20 & sonata(x2):20
Suppose our knowledge base contains the following axioms:
1) form(e0,x1,x2):90 ? compose(e0,x1,x2)
2) create art(e0,x1,x2):50 & art piece(x2):40 ? compose(e0,x1,x2)
3) art piece(x1):90 ? sonata(x1)
Unlike deductive axioms, abductive axioms should be read ?right to left?. Thus, the propositions on
the right hand side (compose, sonata) correspond to an input, whereas the left hand side propositions
will be assumed given the input. The number assigned to each proposition on the left hand side shows
what percentage of the total input cost the assumption of this proposition will cost.3 For example, if the
proposition compose costs 20 then the assumption of form will cost 18.
Two interpretations can be constructed for the given logical form. The first one is the result of the
application of axioms 1 and 3. Note that the costs of the backchained propositions (compose, sonata) are
2The actual value of the default costs of the input propositions does not matter, because, as the reader will see in this section,
the axiom weights which affect the costs of the resulting interpretations are given as percentages of the input proposition costs.
The only heuristic we use here concerns setting all costs of the input propositions to be equal (all propositions cost 20 in the
discussed example). This heuristic needs a further investigation to be approved or modified.
3The axiom weights in the given example are arbitrary.
226
set to 0, because their costs are now carried by the newly introduces assumptions (form, art piece). The
total cost of the first interpretation I1 is equal to 56.
I1: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & form(e1,x1,x2):18 & art piece(x2):18
The second interpretation is constructed in two steps. First, axioms 2 and 3 are applied as follows.
I21: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 &
create art(e1,x1,x2):10 & art piece(x2):8 & art piece(x2):18
The total cost of I21 is equal to 56. This interpretation is redundant, because it contains the propo-
sition art piece twice. The procedure will merge propositions with the same predicate, setting the cor-
responding arguments of these propositions to be equal and assigning the minimum of the costs to the
result of merging. The idea behind such mergings is that if an assumption has already been made then
there is no need to make it again. The final form of the second interpretation I22 with the cost of 38
is as follows. The ?create art? meaning of compose has been brought forward because of the implicit
redundancy in the sentence which facilitated the disambiguation.
I22: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & create art(e1,x1,x2):10 &
art piece(x2):8
Thus, on each reasoning step the procedure 1) applies axioms to propositions with non-zero costs
and 2) merges propositions with the same predicate, assigning the lowest cost to the result of merging.
Reasoning terminates when no more axioms can be applied.4 The procedure favors the cheapest inter-
pretations. Among them, the shortest proofs are favored, i.e. if two interpretations have the same cost
then the one which has been constructed with fewer axiom application steps is considered to be ?better?.
It is easy to see that changing weights of axioms can crucially influence the reasoning process. Axiom
weights can help to propagate more frequent and reliable inferences and to distinguish between ?real?
abduction and deduction. For example, an axiom backchaining from dog to animal should in the general
case have a weight below 100, because it is cheap to assume that there is an animal if there is a dog; it is
a reliable deduction. On the contrary, assuming dog given animal should have a weight above 100.
In order to avoid undesirable mergings, we introduce non-merge constraints. For example, in the
sentence John reads a book and Bill reads a book the two read propositions should not be merged
because they refer to different actions. This is ensured by the following non-merge constraint: if not all
arguments of two propositions (which are not nouns) with the same predicate can be merged, then these
propositions cannot be merged. The constraint implies that in the sentence above two read propositions
cannot be merged, because John being the first argument of the first read cannot be merged with Bill.5
This constraint is a heuristic; it corresponds to the intuition that it is unlikely that the same noun refers to
different objects in a short discourse, while for other parts of speech it is possible. An additional corpus
study is needed in order to prove or disprove it.
The described procedure provides solutions to a whole range of natural language pragmatics prob-
lems, such as resolving ambiguity, discovering implicit relations in nouns compounds, prepositional
phrases, or discourse structure. Moreover, this account of interpretation solves the problem of where to
stop drawing inferences, which could easily be unlimited in number; an inference is appropriate if it is
part of the lowest-cost proof of the logical form.
Adapting Mini-TACITUS to a Large-Scale Knowledge Base
Mini-TACITUS (Mulkar et al, 2007) began as a simple backchaining theorem-prover intended to be a
more transparent version of the original TACITUS system, which was based on Stickel?s PTTP system
(Stickel, 1988). Originally, Mini-TACITUS was not designed for treating large amounts of data. A clear
and clean reasoning procedure rather than efficiency was in the focus of its developers. In order to make
the system work with the large-scale knowledge base, we had to perform several optimization steps and
add a couple of new features.
4In practice, we use the depth parameter d and do not allow an inference chain with more that d steps.
5Recall that only propositions with the same predicate can be merged, therefore John and Bill cannot be merged.
227
For avoiding the reasoning complexity problem, we have introduced two parameters. The time pa-
rameter t is used to restrict the processing time. After the processing time exceeds t the reasoning
terminates and the best interpretation so far is output. The time parameter ensures that an interpretation
will be always returned by the procedure even if reasoning could not be completed in a reasonable time.
The depth parameter d restricts the depth of the inference chain. Suppose that a proposition p occurring
in the input has been backchained and a proposition p? has been introduced as a result. Then, p? will be
backchained and so on. The number of such iterations cannot exceed d. The depth parameter reduces
the number of reasoning steps.
Since Mini-TACITUS processing time increases exponentially with the input size (sentence length
and number of axioms), making such a large set of axioms work was an additional issue. For speeding
up reasoning it was necessary to reduce both the number of the input propositions and the number of
axioms. In order to reduce the number of axioms, a two-step reduction of the axiom set is performed.
First, only the axioms which could be evoked by the input propositions or as a result of backchaining
from the input are selected for each reasoning task. Second, the axioms which could never lead to any
merging are filtered out. Concerning the input propositions, those which could never be merged with the
others (even after backchaining) are excluded from the reasoning process.
3 Knowledge Base
As described in the previous section, the Mini-TACITUS inferences are based on a knowledge base (KB)
consisting of a set of axioms. In order to obtain a reliable KBwith a sufficient coverage we have exploited
existing lexical-semantic resources.
First, we have extracted axioms from WordNet (Fellbaum, 1998), version 3.0, which has already
proved itself to be useful in knowledge-intensive NLP applications. The central entity in WordNet is
called a synset. Synsets correspond to word senses, so that every lexeme can participate in several
synsets. For every word sense, WordNet indicates the frequency of this particular word sense in the
WordNet annotated corpora. We have used the lexeme-synset mapping for generating axioms, with the
corresponding frequencies of word senses converted into the axiom weights. For example, in the axioms
below, the verb compose is mapped to its sense 2 in WordNet which participates in synset-X.
compose-2(e1,x1,x2):80 ? compose(e1,x1,x2)
synset-X(e0,e1):100 ? compose-2(e1,x1,x2)
Moreover, we have converted the following WordNet relations defined on synsets into axioms: hy-
pernymy, instantiation, entailment, similarity, meronymy. Hypernymy and instantiation relations pre-
suppose that the related synsets refer to the same entity (the first axiom below), whereas other types of
relations relate synsets referring to different entities (the second axiom below). All axioms based on
WordNet relations have the weights equal to 100.
synset-1(e0,e1):100 ? synset-2(e0,e1)
synset-1(e0,e1):100 ? synset-2(e2,e3)
WordNet alo provides morphosemantic relations which relate verbs and nouns, e.g., buy-buyer.
WordNet distinguishes between 14 types of such relations.We use relation types in order to define the
direction of the entailment and map the arguments. For example, the ?agent? relation (buy-buyer) stands
for a bi-directional entailment such that the noun is the first (agentive) argument of the verb:
buy-1(e0,x1,x2):100 ? buyer-1(x1)
buyer-1(x1):100 ? buy-1(e0,x1,x2)
Additionally, we have exploited the WordNet synset definitions. In WordNet the definitions are given
in natural language form. We have used the extended WordNet resource6 which provides logical forms
for the definition in WordNet version 2.0. We have adapted logical forms from extended WordNet to our
6http://xwn.hlt.utdallas.edu/
228
representation format and converted them into axioms; for example the following axiom represents the
meaning of the synset containing such lexemes as horseback. These axioms have the total weight of 100.
on(e2,e1,x2):25 & back(e3,x2):25 & of (e4,x2,x1):25 & horse(e5,x1):25 ? synset-X(e0,x0)
The second resource which we have used as a source of axioms is FrameNet, release 1.5, see Rup-
penhofer et al (2006). FrameNet has a shorter history in NLP applications thanWordNet, but lately more
and more researchers have been demonstrating its potential to improve the quality of question answering
(Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). The lexical mean-
ing of predicates in FrameNet is represented in terms of frames which describe prototypical situations
spoken about in natural language. Every frame contains a set of roles corresponding to the participants of
the described situation. Predicates with similar semantics are assigned to the same frame; e.g. both give
and hand over refer to the GIVING frame. For most of the lexical elements FrameNet provides syntactic
patterns showing the surface realization of these lexical elements and their arguments. Syntactic patterns
also contain information about their frequency in the FrameNet annotated corpora. We have used the
patterns and the frequencies for deriving axioms such as for example the following.
GIVING(e1,x1,x2,x3):70 & DONOR(e1,x1):0 & RECIPIENT(e1,x2):0 & THEME(e1,x3):0 ?
give(e1,x1,x3) & to(e2,e1,x2)
HIRING(e1,x1,x3):90 & EMPLOYER(e1,x1) & EMPLOYEE(e1,x3) ?
give(e1,x1,x2,x3):10 & job(x2)
The first pattern above corresponds to the phrases like John gave a book to Mary and the second ?
less frequent ? to phrases like John gave Mary a job. It is interesting to note that application of such
axioms provides a solution to the problem of semantic role labeling as a by-product. As in the statis-
tical approaches, more frequent patterns will be favored. Moreover, patterns helping to detect implicit
redundancy will be brought forward.
FrameNet alo introduces semantic relations defined on frames such as inheritance, causation or
precedence; for example the GIVING and GETTING frames are connected with the causation relation.
Roles of the connected frames are also linked, e.g. DONOR in GIVING is linked with SOURCE in GETTING.
Frame relations have no formal semantics in FrameNet. In order to generate corresponding axioms, we
have used the previous work on axiomatizing frame relations and extracting new relations from corpora
(Ovchinnikova et al, 2010). Weights of the axioms derived from frame relations depend on corpus-based
similarity of the lexical items assigned to the corresponding frames. An example of an axiomatized
relation is given below.7
GIVING(e0,x1,x2,x3):120 & DONOR(e0,x1):0 & RECIPIENT(e0,x2):0 & THEME(e0,x3):0 &
causes(e0,e1):0 ? GETTING(e1,x2,x3,x1) & SOURCE(e1,x1) & RECIPIENT(e1,x2) & THEME(e1,x3)
Both WordNet and FrameNet are manually created resources which ensures a relatively high quality
of the resulting axioms as well as the possibility of exploiting the linguistic information provided for
structuring the axioms. Although manual creation of resources is a very time-consuming task, WordNet
and FrameNet, being long-term projects, have an extensive coverage of English vocabulary. The cover-
age of WordNet is currently larger than that of FrameNet (155 000 vs. 12 000 lexemes). However, the
fact that FrameNet introduces complex argument structures (roles) for frames and provides mappings of
these structures makes FrameNet especially valuable for reasoning.
The complete list of axioms we have extracted from these resources is given in table 1.
4 Recognizing Textual Entailment
As the reader can see from the previous sections, the discourse processing procedure we have presented
is fairly general and not tuned for any particular type of inferences. We have evaluated the procedure and
7The ?causes? predicate is supposed to be linked to an underlying causation theory, see for example
http://www.isi.edu/?hobbs/bgt-cause.text. However, in the described experimental settings we have left the abstract theories
out and evaluated only the axioms extracted from the lexical-semantic resources.
229
Table 1: Statistics for extracted axioms
Axiom type Source Numb. of axioms
Lexeme-synset mappings WN 3.0 422,000
Lexeme-synset mappings WN 2.0 406,000
Synset relations WN 3.0 141,000
Derivational relations WN 3.0 (annotated) 35,000
Synset definitions WN 2.0 (parsed, annotated) 120,500
Lexeme-frame mappings FN 1.5 50,000
Frame relations FN 1.5 + corpora 6,000
the KB derived from WordNet and FrameNet on the Recognizing Textual Entailment (RTE) task, which
is a generic task that seems to capture major semantic inference needs across many natural language
processing applications. In this task, the system is given a text and a hypothesis and must decide whether
the hypothesis is entailed by the text plus commonsense knowledge.
Our approach is to interpret both the text and the hypothesis using Mini-TACITUS, and then see
whether adding information derived from the text to the knowledge base will reduce the cost of the best
abductive proof of the hypothesis as compared to using the original knowledge base only. If the cost
reduction exceeds a threshold determined from a training set, then we predict entailment.
A simple example would be the text John gave a book to Mary and the hypothesis Mary got a book.
Our pipeline constructs the following logical forms for these two sentences.
T: John(x1):20 & give(e1,x1,x2):20 & book(x3):20 & to(e2,e1,x3):20 & Mary(x3):20
H: Mary(x1):20 & get(e1,x1,x2):20 & book(x2):20
These logical forms constitute the Mini-TACITUS input. Mini-TACITUS applies the axioms from
the knowledge base to the input logical forms in order to reduce the overall cost of the interpretations.
Suppose that we have three FrameNet axioms in our knowledge base. The first one maps give to to the
GIVING frame, the second one maps get to GETTING and the third one relates GIVING and GETTING with
the causation relation. The first two axioms have the weights of 90 and the third 120. As a result of the
application of the axioms the following best interpretations will be constructed for T and H.
I(T): John(x1):20 & give(e1,x1,x2):0 & book(x3):20 & to(e2,e1,x3):0 & Mary(x3):20 &
GIVING(e0,x1,x2,x3):18
I(H): Mary(x1):20 & get(e1,x1,x2):0 & book(x2):20 & GETTING(e0,x1,x2):18
The total cost of the best interpretation for H is equal to 58. Now the best interpretation of T will
be added to H with the zero costs (as if T has been totally proven) and we will try to prove H once
again. First of all, merging of the propositions with the same names will result in reducing costs of the
propositions Mary and book to 0, because they occur in T:
I(T+H): John(x1):0 & give(e1,x1,x2):0 & book(x3):0 & to(e2,e1,x3):0 & Mary(x3):0 &
GIVING(e0,x1,x2,x3):0 & get(e1,x1,x2):0 & GETTING(e0,x1,x2):18
The only proposition left to be proved is GETTING. Using the GETTING-GIVING relation as described
in the previous section, this proposition can be backchained on to GIVING which will merge with GIVING
coming from the T sentence. H appears to be proven completely with respect to T; the total cost of its
best interpretation given T is equal to 0. Thus, using knowledge from T helped to reduce the cost of the
best interpretation of H from 58 to 0.
The approach presented does not have any special account for logical connectors such as if, not, or
etc. Given a text If A then B and a hypothesis A and B our procedure will most likely predict entailment.
At the moment our RTE procedure mainly accounts for the informational content of texts, being able to
detect the ?aboutness? overlap of T and H. In our framework, a fuller treatment of the logical structure
230
of the natural language would presuppose a more complicated strategy of merging redundancies.
5 Evaluation Results
We have evaluated our procedure on the RTE-2 dataset 8, see Bar-Haim et al (2006) . The RTE-2
dataset contains the development and the test set, both including 800 text-hypothesis pairs. Each dataset
consists of four subsets, which correspond to typical success and failure settings in different applications:
information extraction (IE), information retrieval (IR), question answering (QA), and summarization
(SUM). In total, 200 pairs were collected for each application in each dataset.
As a baseline we have processed the datasets with an empty knowledge base. Then we have done 2
runs, first, using axioms extracted fromWordNet 3.0 plus FrameNet, and, second, using axioms extracted
from the WordNet 2.0 definitions. In both runs the depth parameter was set to 3. The development
set was used to train the threshold as described in the previous section.9 Table 2 contains results of
our experiments.10 Accuracy was calculated as the percentage of pairs correctly judged. The results
suggest that the proposed method seems to be promising as compared to the other systems evaluated
on the same task. Our best run gives 63% accuracy. Two systems participating the RTE-2 Challenge
had 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved
55%-61%, cf. Bar-Haim et al (2006). For our best run (WN 3.0 + FN), we present the accuracy data
for each application separately (table 2). The distribution of the performance of Mini-TACITUS on the
four datasets corresponds to the average performance of systems participating in RTE-2 as reported by
Garoufi (2007). The most challenging task in RTE-2 appeared to be IE. QA and IR follow, and finally,
SUM was titled the ?easiest? task, with a performance significantly higher than that of any other task.11
It is worth noting that the performance of Mini-TACITUS increases with the increasing time of pro-
cessing. This is not surprising. We use the time parameter t for restricting the processing time. The
smaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms. The experiments
carried out suggest that optimizing the system computationally could lead to producing significantly bet-
ter results. Tracing the reasoning process, we found out that given a long sentence and a short processing
time Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation was
not always among them.
The lower performance of the system using the KB based on axioms extracted from extended Word-
Net can be easily explained. At the moment we define non-merge constraints (see section 2) for the input
propositions only. The axioms extracted from the synset definitions introduce a lot of new lexemes into
the logical form, since these axioms define words with the help of other words rather than abstract con-
cepts. These new lexemes, especially those which are frequent in English, result in undesired mergings
(e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them. In order
to fix this problem, we will need to implement dynamic non-merge constraints which will be added on
the fly if a new lexeme is introduced during reasoning. The WN 3.0 + FN axiom set does not fall into
this problem, because these axioms operate on frames and synsets rather than on lexemes.
In addition, for the run using axioms derived from FrameNet, we have evaluated how well we do
in assigning frames and frame roles. For Mini-TACITUS, semantic role labeling is a by-product of
constructing the best interpretation. But since this task is considered to be important as such in the NLP
community, we provide an additional evaluation for it. As a gold standard we have used the Frame-
Annotated Corpus for Textual Entailment, FATE, see Burchardt and Pennacchiotti (2008). This corpus
provides frame and semantic role label annotations for the RTE-2 challenge test set.12 It is important to
8http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/
9Interpretation costs were normalized to the number of propositions in the input.
10?Time? stands for the value of the time parameter ? processing time per sentence, in minutes; ?Numb. of ax.? stands for
the average number of axioms per sentence.
11In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types of
entailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailment
which was produced by Garoufi (2007). We would like to thank one of our reviewers for giving us this idea.
12FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms. However,
231
Table 2: Evaluation results for the RTE-2 test set
KB Accuracy Time
Numb. of ax.
T H
No KB 57% 1 0 0
WN 3.0 + FN 62% 20 533 237
WN 3.0 + FN 63% 30 533 237
Ext. WN 2.0 60% 20 3700 1720
Ext. WN 2.0 61% 30 3700 1720
Task Accuracy
SUM 75%
IR 64%
QA 62%
IE 50%
Table 3: Evaluation of frames/roles labeling towards FATE
System
Frame match
Recall
Role match
Precision Recall
Shalmaneser 0.55 0.54 0.37
Shalmaneser + Detour 0.85 0.52 0.36
Mini-TACITUS 0.65 0.55 0.30
note that FATE annotates only those frames which are relevant for computing entailment. Since Mini-
TACITUS makes all possible frame assignments for a sentence, we provide only the recall measure for
the frame match and leave the precision out.
The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system (Erk and
Pado, 2006) which is a state-of-the-art system for assigning FrameNet frames and roles. In table 2 we
replicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet
(Burchardt et al, 2005). The WN-FN Detour extended the frame labels assigned by Shalmaneser with
the labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf. Burchardt et al
(2009). In frame matching, the number of frame labels in the gold standard annotation that can also be
found in the system annotation (recall) was counted. Role matching was evaluated only on the frames
that are correctly annotated by the system. The number of role labels in the gold standard annotation
that can also be found in the system annotation (recall) as well as the number of role labels found by
the system which also occur in the gold standard (precision) were counted.13 Table 3 shows that given
FrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with those
of the system specially designed to solve this task.
6 Conclusion and Future Work
This paper presents a discourse processing framework underlying the abductive reasoner called Mini-
TACITUS. We have shown that interpreting texts using weighted abduction helps solve pragmatic prob-
lems in discourse processing as a by-product. In this paper, particular attention was paid to the construc-
tion of a large and reliable knowledge base populated with axioms extracted from such lexical-semantic
resources as WordNet and FrameNet. The reasoning procedure as well as the knowledge base were eval-
uated in the Recognizing Textual Entailment task. The data for evaluation were taken from the RTE-2
Challenge. First, we have evaluated the accuracy of the entailment prediction. Second, we have eval-
in the new FN version the number of frames and roles increases and there is no message about removed frames in the General
Release Notes R1.5, see http://framenet.icsi.berkeley.edu. Therefore we suppose that most of the frames and roles used for the
FATE annotation are still present in FN 1.5.
13We do not compare filler matching, because the FATE syntactic annotation follows different standards as the one produced
by the ESG parser, which makes aligning fillers non-trivial.
232
uated frame and role labeling using the Frame-Annotated Corpora for Textual Entailment as the gold
standard. In both tasks our system showed performance compatible with those of the state-of-the art
systems. Since the inference procedure and the axiom set are general and not tuned for a particular task,
we consider the results of our experiments to be promising concerning possible manifold applications of
Mini-TACITUS.
The experiments we have carried out have shown that there is still a lot of space for improving the
procedure. First, for successful application of Mini-TACITUS on a large scale the system needs to be
computationally optimized. In its current state, Mini-TACITUS requires too much time for producing
satisfactory results. As our experiments suggest (cf. table 2), speeding up reasoning may lead to signif-
icant improvements in the system performance. Since Mini-TACITUS was not originally designed for
large-scale processing, its implementation is in many aspects not effective enough. We hope to improve
it by changing the data structure and re-implementing some of the main algorithms.
Second, in the future we plan to elaborate our treatment of natural language expressions standing for
logical connectors such as implication if, negation not, disjunction or and others. Quantifiers such as
all, each, some also require a special treatment. This advance is needed in order to achieve more precise
entailment inferences, which are at the moment based in our approach on the core information content
(?aboutness?) of texts. Concerning the heuristic non-merge constraints preventing undesired mergings
as well as the heuristic for assigning default costs (see section 2), in the future we would like to perform
a corpus study for evaluating and possibly changing these heuristics.
Another future direction concerns the enlargement of the knowledge base. Hand-crafted lexical-
semantic resources such as WordNet and FrameNet provide both an extensive lexical coverage and a
high-value semantic labeling. However, such resources still lack certain features essential for captur-
ing some of the knowledge required for linguistic inferences. First of all, manually created resources
are static; updating them with new information is a slow and time-consuming process. By contrast,
commonsense knowledge and the lexicon undergo daily updates. In order to accommodate dynamic
knowledge, we plan to make use of the distributional similarities of words in a large Web-corpus such
as for example Wikipedia. Many researchers working on RTE have already been using word similarity
for computing similarity between texts and hypotheses, e.g., Mehdad et al (2010). In our approach, we
plan to incorporate word similarities into the reasoning procedure making them affect proposition costs
so that propositions implied by the context (similar to other words in the context) will become cheaper
to prove. This extension might give us a performance improvement in RTE, because it will help to relate
those propositions from H for which there are no appropriate axioms in the KB to propositions in T.
Lexical-semantic resources as knowledge sources for reasoning have another shortcoming: They
imply too little structure. WordNet and FrameNet enable some argument mappings of related synsets or
frames, but they cannot provide a more detailed concept axiomatization. We are engaged in two types of
efforts to obtain more structured knowledge. The first effort is the manual encoding of abstract theories
explicating concepts that pervade natural language discourse, such as causality, change of state, and
scales, and the manual encoding of axioms linking lexical items to these theories. A selection of the core
theories can be found at http://www.isi.edu/ hobbs/csk.html. The second effort concerns making use of
the existing ontologies. The recent progress of the Semantic Web technologies has stimulated extensive
development of the domain-specific ontologies as well as development of inference machines specially
designed to reason with these ontologies.14 In practice, domain-specific ontologies usually represent
detailed and structured knowledge about particular domains (e.g. geography, medicine etc.). We intend
to make Mini-TACITUS able to use this knowledge through querying an externally stored ontology with
the help of an existing reasoner. This extension will give us a possibility to access elaborated domain-
specific knowledge which might be crucial for interpretation of domain-specific texts.
We believe that implementation of the mentioned improvements and extensions will make Mini-
TACITUS a powerful reasoning system equipped with enough knowledge to solve manifold NLP tasks on
a large scale. In our view, the experiments with the axioms extracted from the lexical-semantic resources
presented in this paper show the potential of weighted abduction for natural language reasoning and open
14www.w3.org/2001/sw/,http://www.cs.man.ac.uk/ sattler/reasoners.html
233
new ways for its application.
References
Bar-Haim, R., I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor (2006). The
second PASCAL recognising textual entailment challenge. In Proc. of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment.
Burchardt, A., K. Erk, and A. Frank (2005). A WordNet Detour to FrameNet. In Sprachtechnologie,
mobile Kommunikation und linguistische Resourcen, Volume 8.
Burchardt, A. and M. Pennacchiotti (2008). FATE: a FrameNet-Annotated Corpus for Textual Entail-
ment. In Proc. of LREC?08.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2009). Assessing the impact of frame seman-
tics on textual entailment. Natural Language Engineering 15(4), 527?550.
Erk, K. and S. Pado (2006). Shalmaneser - a flexible toolbox for semantic role assignment. In Proc. of
LREC?06, Genoa, Italy.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database (First ed.). MIT Press.
Garoufi, K. (2007). Towards a better understanding of applied textual entailment: Annotation and eval-
uation of the rte-2 dataset. Master?s thesis, Saarland University.
Hobbs, J. R. (1985). Ontological promiscuity. In Proceedings, 23rd Annual Meeting of the Association
for Computational Linguistics, Chicago, Illinois, pp. 61?69.
Hobbs, J. R., M. Stickel, and P. Martin (1993). Interpretation as abduction. Artificial Intelligence 63,
69?142.
McCord, M. C. (1990). Slot grammar: A system for simpler construction of practical natural language
grammars. In Natural Language and Logic: International Scientific Symposium, Lecture Notes in
Computer Science, pp. 118?145. Springer Verlag.
McCord, M. C. (2010). Using Slot Grammar. Technical report, IBM T. J. Watson Research Center. RC
23978Revised.
Mehdad, Y., A. Moschitti, and F. M. Zanzotto (2010). Syntactic/semantic structures for textual entailment
recognition. In Proc. of HLT ?10: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 1020?1028.
Mulkar, R., J. R. Hobbs, and E. Hovy (2007). Learning from Reading Syntactically Complex Biol-
ogy Texts. In Proc.of the 8th International Symposium on Logical Formalizations of Commonsense
Reasoning. Palo Alto.
Ovchinnikova, E., L. Vieu, A. Oltramari, S. Borgo, and T. Alexandrov (2010). Data-Driven and Onto-
logical Analysis of FrameNet for Natural Language Reasoning. In Proc. of LREC?10, Valletta, Malta.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk (2006). FrameNet II: Extended
Theory and Practice. International Computer Science Institute.
Shen, D. and M. Lapata (2007). Using Semantic Roles to Improve Question Answering. In Proc. of
EMNLP-CoNLL, pp. 12?21.
Stickel, M. E. (1988). A prolog technology theorem prover: Implementation by an extended prolog
compiler. Journal of Automated Reasoning 4(4), 353?380.
234
