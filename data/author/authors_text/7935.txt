Detecting Shifts in News Stories for Paragraph Extraction
Fumiyo Fukumoto Yoshimi Suzuki
Department of Computer Science and Media Engineering,
Yamanashi University
4-3-11, Takeda, Kofu, 400-8511, Japan
{fukumoto@skye.esb, ysuzuki@alps1.esi}.yamanashi.ac.jp
Abstract
For multi-document summarization where docu-
ments are collected over an extended period of time,
the subject in a document changes over time. This
paper focuses on subject shift and presents a method
for extracting key paragraphs from documents that
discuss the same event. Our extraction method uses
the results of event tracking which starts from a few
sample documents and finds all subsequent docu-
ments that discuss the same event. The method was
tested on the TDT1 corpus, and the result shows the
effectiveness of the method.
1 Introduction
Multi-document summarization of news stories dif-
fers from single document in that it is important
to identify differences and similarities across doc-
uments. This can be interpreted as the question
of how to identify an event and a subject in doc-
uments. According to the TDT project, an event is
something that occurs at a specific place and time as-
sociated with some specific actions, and it becomes
the background among documents. A subject, on
the other hand, refers to a theme of the document
itself. Another important factor, which is typical in
a stream of news, is recognizing and handling sub-
ject shift. The extracted paragraphs based on an
event and a subject may include the main points
of each document and the background among docu-
ments. However, when they are strung together, the
resulting summary still contains much overlapping
information.
This paper focuses on subject shift and presents
a method for extracting key paragraphs from docu-
ments that discuss the same event. We use the re-
sults of our tracking technique which automatically
detects subject shift, and produces the optimal win-
dow size in the training data so as to include only the
data which are sufficiently related to the current sub-
ject. The idea behind this is that, of two documents
from the target event which are close in chronological
order, the latter discusses (i) the same subject as an
earlier one, or (ii) a new subject related to the tar-
get event. This is particularly well illustrated by the
Kobe Japan quake event in the TDT1 data. The first
document says that a severe earthquake shook the
city of Kobe. It continues until the 5th document.
The 6th through 17th documents report damage, lo-
cation and nature of quake. The 18th document, on
the other hand, states that the Osaka area suffered
much less damage than Kobe. The subject of the
document is different from the earlier ones, while all
of these are related to the Kobe Japan quake event.
We use the leave-one-out estimator of Support Vec-
tor Machines(SVMs)(Vapnik, 1995) to make a clear
distinction between (i) and (ii) and thus estimate the
optimal window size in the training data. For the re-
sults of tracking where documents are divided into
several sets, each of which covers a different subject
related to the same event, we apply SVMs again and
induce classifiers. Using these classifiers, we extract
key paragraphs.
The next section explains why we need to detect
subject shift by providing notions of an event, a
subject class and a subject which are properties that
identify key paragraphs. After describing SVMs, we
present our system. Finally, we report some exper-
iments using the TDT1 and end with a very brief
summary of existing techniques.
2 An Event, A Subject Class and A
Subject
Our hypothesis about key paragraphs in multiple
documents related to the target event is that they
include words related to the subject of a document,
a subject class among documents, and the target
event. We call these words subject, subject class
and event words. The notion of a subject word
refers to the theme of the document itself, i.e., some-
thing a writer wishes to express, and it appears
across paragraphs, but does not appear in other doc-
uments(Luhn, 1958). A subject class word differen-
tiates it from a specific subject, i.e. it is a broader
class of subjects, but narrower than an event. It ap-
pears across documents, and these documents dis-
cuss related subjects. An event word, on the other
hand, is something that occurs at a specific place
and time associated with some specific actions, and
it appears across documents about the target event.
Let us take a look at the following three documents
concerning the Kobe Japan quake from the TDT1.
1. Emergency work continues after earthquake in Japan
1-1. Casualties are mounting in [Japan], where a strong
[earthquake] eight hours ago struck [Kobe]. Up to
400 {people} related {deaths} are confirmed, thou-
sands of {injuries}, and rescue crews are searching
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
2. Quake Collapses Buildings in Central Japan
2-1. At least two {people} died and dozens {injuries}
when a powerful [earthquake] rolled through central
[Japan] Tuesday morning, collapsing buildings and
setting off fires in the cities of [Kobe] and Osaka.
2-2. The [Japan] Meteorological Agency said the
[earthquake], which measured 7.2 on the open-ended
Richter scale, rumbled across Honshu Island from
the Pacific Ocean to the [Japan] Sea.
2-3. The worst hit areas were the port city of [Kobe]
and the nearby island of Awajishima where in
both places dozens of fires broke out and up to 50
buildings, including several apartment blocks,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
3. US forces to fly blankets to Japan quake survivors
3-1. United States forces based in [Kobe] [Japan] will take
blankets to help [earthquake] survivors Thursday, in
the U.S. military?s first disaster relief operation in
[Japan] since it set up bases here.
3-2. A military transporter was scheduled to take off in
the afternoon from Yokota air base on the outskirts
of Tokyo and fly to Osaka with 37,000 blankets.
3-3. Following the [earthquake] Tuesday, President Clin-
ton offered the assistance of U.S. military forces in
[Japan], and Washington provided the Japanese
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Figure 1: Documents from the TDT1
The underlined words in Figure 1 denote a subject
word in each document. Words marked with ?{}? and
?[]? refer to a subject class word and an event word,
respectively. Words such as ?Kobe? and ?Japan? are
associated with an event, since all of these docu-
ments concern the Kobe Japan quake. The first
document says that emergency work continues af-
ter the earthquake in Japan. Underlined words such
as ?rescue? and ?crews? denote the subject of the doc-
ument. The second document states that the quake
collapsed buildings in central Japan. These two doc-
uments mention the same thing: A powerful earth-
quake rolled through central Japan, and many peo-
ple were injured. Therefore, words such as ?people?
and ?injuries? which appear in both documents are
subject class words, and these documents are classi-
fied into the same set. If we can determine that these
documents discuss related subjects, we can eliminate
redundancy between them. The third document, on
the other hand, states that the US military will fly
blankets to Japan quake survivors. The subject of
the document is different from the earlier ones, i.e.,
the subject has shifted.
Though it is hard to make a clear distinction be-
tween a subject and a subject class, it is easier to
find properties to determine whether the later docu-
ment discusses the same subject as an earlier one or
not. Our method exploits this feature of documents.
3 SVMs
We use a supervised learning technique,
SVMs(Vapnik, 1995), in the tracking and paragraph
extraction task. SVMs are defined over a vector
space where the problem is to find a decision surface
that ?best? separates a set of positive examples
from a set of negative examples by introducing
the maximum ?margin? between two sets. Figure
2 illustrates a simple problem that is linearly
separable.
Margin
w
Positive examples
Negative examples
Figure 2: The decision surface of a linear SVM
Solid line denotes a decision surface, and two dashed
lines refer to the boundaries. The extra circles
are called support vectors, and their removal would
change the decision surface. Precisely, the decision
surface for linearly separable space is a hyperplane
which can be written as w?x+b = 0, where x is an ar-
bitrary data point(x?Rn) and w and b are learned
from a training set. In the linearly separable case
maximizing the margin can be expressed as an opti-
mization problem:
Minimize : ?
?l
i=1
?i + 1
2
?l
i,j=1
?i?jyiyjxi ? xj (1)
s.t :
?l
i=1
?iyi = 0 ?i : ?i ? 0
w =
?
l
i=1
?iyixi (2)
where xi = (xi1,? ? ?,xin) is the i-th training exam-
ple and yi is a label corresponding the i-th training
example. In formula (2), each element of w, wk (1
? k ? n) corresponds to each word in the training
examples, and the larger value of wk =
?l
i
?iyixik
is, the more the word wk features positive examples.
We use an upper bound value, E?loo of the leave-
one-out error of SVMs to estimate the optimal win-
dow size in the training data. E ?loo can estimate
the performance of a classifier. It is based on the
idea of leave-one-out technique: The first example is
removed from l training examples. The resulting ex-
ample is used for training, and a classifier is induced.
The classifier is tested on the held out example. The
process is repeated for all training examples. The
number of errors divided by l, Eloo, is the leave-
one-out estimate of the generalization error. E?loo
uses an upper bound on Eloo instead of calculating
them, which is computationally very expensive. Re-
call that the removal of support vectors change the
decision surface. Thus the worst happens when ev-
ery support vector will become an error. Let l be
the number of training examples of a set S, and m
be the number of support vectors. E?loo(S) is defined
as follows:
Eloo(S) ? E
?
loo(S) =
m
l
(3)
4 System Design
4.1 Tracking by Window Adjustment
Like much previous research, our hypotheses regard-
ing event tracking is that exploiting time will lead to
improved data adjustment because documents closer
together in the stream are more likely to discuss re-
lated subject than documents further apart. Let x1,
? ? ?, xp be positive training documents, i.e., being the
target event, which are in chronological order. Let
also y1, ? ? ?, yq be negative training documents. The
algorithm can be summarized as follows:
1. Scoring negative training documents
In the TDT tracking task, the number of labelled
positive training documents is small (at most 16
documents) compared to the negative training doc-
uments. Therefore, the choice of good training data
is an important issue to produce optimal results. We
first represent each document as a vector in an n di-
mensional space, where n is the number of words in
the collection. The cosine of the angle between two
vectors, xi and yj is shown in (4).
cos(xi, yj) =
?n
k=1
xik ? yjk
?
?
n
k=1
x2
ik
?
?
?
n
k=1
y2
jk
(4)
where xik and yjk are the term frequency of word k
in the document xi and yj , respectively. We com-
pute a relevance score for each negative training doc-
ument by the cosine of the angle between a vector of
the center of gravity on positive training documents
and a vector of the negative training document, i.e.,
cos(g, yj) (1 ? j ? q), where yj is the j-th negative
training document, and g is defined as follows:
g = (g
1
, ? ? ? , gn) = (
1
p
p
?
i=1
xi1, ? ? ? ,
1
p
p
?
i=1
xin) (5)
xij (1 ? j ? n) is the term frequency of word j
in the positive document xi. The negative training
documents are sorted in the descending order of their
relevance scores: y1, ? ? ?, yq?1 and yq .
2. Adjusting window size
We estimate that the most recent positive training
document, xp discusses either (i) the same subject
as the previous positive one, or (ii) a new subject.
To do this, we use the value of E?loo. Let y1, ? ? ?, yr
be negative training documents whose cosine simi-
larity values are the top r among q negative training
documents. Let alo Set1 be a set consisting of x1,
xp, y1, ? ? ?, yr , and Set2 be a set which consists of
xp?1, xp, y1, ? ? ?, yr. We compute E?loo on sets Set1
and Set2. If the value of E ?loo on Set2 is smaller
than that of Set1, this means that xp has the same
subject as the previous document xp?1, since a clas-
sifier which is induced by training Set2 is estimated
to generate a smaller error rate than that of Set1.
In this case, we need to find the optimal window size
so as to include only the positive documents which
are sufficiently related to the subject. The flow of
the algorithm is shown in Figure 3.
begin
for k = 1 to p-3
num = ?,
Let Seta = {x
1
, xp, ? ? ? , xp?k, y1, ? ? ? , yr?1, yr}.
Setb = {x1, x
(p?1), ? ? ? , x(p?1)?k, y1, ? ? ? , yr?1, yr}
if E?
loo
(Seta) < E?loo(Setb)
then num = k + 2 exit loop
end if
end for
if num = ?
then num = p
end if
end
Figure 3: Flow of window adjustment
On the other hand, if the value of E?loo of Set2 is
larger than that of Set1 , xp is regarded to discuss
a new subject. We use all previously seen positive
documents for training as a default strategy.
3. Tracking
Let num be the number of adjusted positive train-
ing documents. The top num negative documents
are extracted from q negative documents and merged
into num positive documents. The new set is trained
by SVMs, and a classifier is induced. Recall that
E?loo is computationally less expensive. However,
they are sometimes too tight for the small size of
training data. This causes a high F/A rate which
is signaled by the ratio of the documents that were
judged as negative but were evaluated as positive.
We then use a simple measure for the test document
which is determined to be positive by a classifier.
For each training document, we compute the cosine
between the test and the training document vectors.
If the cosine between the test and the negative doc-
uments is largest, the test document is judged to be
negative. Otherwise, it is truly positive and tracking
is terminated. The procedure 1, 2 and 3 is repeated
until the last test document is judged.
4.2 Paragraph Extraction
Our window adjustment algorithm is applied each
time the document discusses the target event.
Therefore, some documents are assigned to more
than one set of documents. We thus eliminate some
sets which completely overlap each other, and apply
paragraph extraction to the result. Our hypothesis
about key paragraphs is that they include subject,
subject class, and event words. Let xp be a para-
graph in the document x and x\1 be the resulting
document with xp removed. Let alo l be the total
number of documents in a set where each document
discusses subjects related to x. If xp includes subject
words, xp is related to x\1 rather than the other l-1
documents, since subject words appear across para-
graphs in x\1 rather than the other l?1 documents.
We apply SVMs to the training data, which consists
of l documents, and induce a classifier sbj(xp), which
identifies whether xp is related to x\1 or not.
sbj(xp) =
{
1 if xp is assigned to x\1
0 else
We note that SVMs are basically introduced for solv-
ing binary classification, while our paragraph ex-
traction is a multi-class classification problem, i.e.,
l classes. We use the pairwise technique for using
SVMs with multi-class data(Weston and C.Watkins,
1998), and assign xp to one of the l documents. In a
similar way, we apply SVMs to the other two train-
ing data and induce classifiers: sbj class(xp) and
event(xp).
sbj class(xp) =
{
1 if xp is assigned to sbj classx\1
0 else
event(xp) =
{
1 if xp is assigned to eventx\1
0 else
sbj class(xp) refers to a classifier which identifies
whether or not xp is assigned to the set sbj classx\1
including x\1. It is induced by training data
which consists of m different sets including the set
sbj classx\1 , each of which covers a different subject
related to the target event. The classifier event(xp)
is induced by training data which consists of two dif-
ferent sets: one is a set of all documents including
x\1, and concerning the target event. The other is
a set of documents which are not the target event.
We extract paragraphs for which (6) holds.
sbj(xp) = 1 & sbj class(xp) = 1 & event(xp) = 1 (6)
5 Experiments
We used the TDT1 corpus which comprises a set
of different sources, Reuters(7,965 documents) and
CNN(7,898 documents)(Allan et al, 1998a). A set
of 25 target events were defined. Each document is
labeled according to whether or not the document
discusses the target event. All 15,863 documents
were tagged by a part-of-speech tagger(Brill, 1992)
and stemmed using WordNet information(Fellbaum,
1998). We extracted all nouns in the documents.
5.1 Tracking Task
Table 1 summarizes the results which were obtained
using the standard TDT evaluation measure1.
Table 1: Tracking results
Nt Miss F/A Prec F1
1 31% 0.16% 70% 0.68
2 27% 0.16% 79% 0.78
4 24% 0.09% 87% 0.78
8 23% 0.09% 87% 0.79
16 22% 0.09% 86% 0.79
?Nt? denotes the number of initial positive training
documents where Nt takes on values 1, 2, 4, 8 and 16.
When Nt takes on value 1, we use the document d
and one negative training document y1 for training.
Here, y1 is a vector whose cosine value of d and y1
is the largest among the other negative documents.
The test set is always the collection minus the Nt =
16 documents. ?Miss? denotes Miss rate, which is the
ratio of the documents that were judged as Yes but
were not evaluated as Yes. ?F/A? shows false alarm
rate, which is the ratio of the documents judged as
No but were evaluated as Yes. ?Prec? stands for pre-
cision, which is the ratio of correct assignments by
the system divided by the total number of the sys-
tem?s assignments. ?F1? is a measure that balances
recall and precision, where recall denotes the ratio
of correct assignments by the system divided by the
total number of correct assignments. Table 1 shows
that there is no significant difference among Nt val-
ues except for 1, since F1 ranges from 0.78 to 0.79.
This shows that the method works well even for a
small number of initial positive training documents.
Furthermore, the results are comparable to the ex-
isting event tracking techniques, since the F1, Miss
and F/A score by CMU were 0.66, 29 and 0.40, and
those of UMass were 0.62, 39 and 0.27, respectively,
when Nt is 4(Allan et al, 1998b).
The contribution of the adaptive window algo-
rithm is best explained by looking at the window
sizes it estimates. Table 2 illustrates the sample re-
sult of tracking for ?Kobe Japan Quake? event on the
Nt = 16. This event has many documents, each of
these discusses a new subject related to the target
event. The result shows the first 10 documents in
chronological order which are evaluated as positive.
Columns 1-3 in Table 2 denote id number, dates,
and title of the document, respectively. ?id=1?, for
example, denotes the first document which is eval-
uated as positive. Columns 4 and 5 stand for the
result of our method, and the majority of three hu-
man judges, respectively. They take on three values:
?Yes? denotes that the document discusses the same
subject as an earlier one, ?New? indicates that the
document discusses a new subject, and ?No?, that
the document is not a positive document. We can
1http://www.nist.gov/speech/tests/tdt/index.htm
Table 2: The adaptive window size in Event 15, ?Kobe Japan Quake?
id date title shifts adjusted window size
system actual recall precision F1
1 01/17/95 Kobe Residents Unable to Commence Rescue Operations New New 100% 100% 1.00
2 01/17/95 Emergency Efforts Continue After Quake in Japan Yes Yes 100% 100% 1.00
3 01/17/95 Japan Helpline Worker Discusses Emergency Efforts Yes New 100% 5% 0.10
4 01/17/95 U.S. Businessman Describes Japan Earthquake Yes Yes 100% 80% 0.89
5 01/17/95 Osaka, Japan, Withstands Earthquake Better Than Others Yes New 100% 5% 0.09
6 01/17/95 President Clinton Drums Up Support in Humanitarian Trip No New 100% 5% 0.09
7 01/17/95 Engineer Examines Causes of Damage in Japan Quake Yes New 100% 50% 0.67
8 01/18/95 Mike Chinoy Updates Japan?s Earthquake Recovery Efforts Yes Yes 100% 100% 1.00
9 01/18/95 Smoke Hangs in a Pall Over Quake-, Fire-Ravaged Kobe New New 100% 4% 0.08
10 01/18/95 Japanese Wonder If Their Cities Are Really ?Quakeproof? New New 100% 4% 0.07
see that the method correctly recognizes a test doc-
ument as discussing an earlier subject or a new one,
since the result of our method(?system?) and human
judges(?actual?) coincide except for ?id=5, 6 and 7?.
Columns 6-8 stand for the accuracy of the ad-
justed window size. Recall denotes the number
of documents selected by both the system and hu-
man judges divided by the total number of doc-
uments selected by human judges, and precision
shows the number of documents selected by both
the system and human judges divided by the to-
tal number of documents selected by the system.
When the method correctly recognizes a test doc-
ument as discussing an earlier subject(?system = ac-
tual = Yes?), our algorithm selects documents which
are sufficiently related to the current subject, since
the total average of F1 was 0.82. We note that the
ratio of precision in ?system = New? is low. This
is because we use a default strategy, i.e., we use
all previously seen positive documents for training
when the most recent training document is judged
to discuss a new subject.
5.2 Paragraph Extraction
We used 15 out of 25 events which have more than 16
positive documents in the experiment. Table 3 de-
notes the number of documents and paragraphs in
each event. ?Avg.? in ?doc? shows the average num-
ber of documents per event, and ?Avg.? in ?para?
denotes the average number of paragraphs per doc-
ument. The maximum number of paragraphs per
document was 100.
Table 4 shows the result of paragraph extraction.
?CNN? refers to the results using the CNN corpus
as both training and test data. ?Reuters? denotes
the results using the Reuters corpus. ?Total? stands
for the results using both corpora. ?Tracking result?
refers to the F1 score obtained by using tracking re-
sults. ?Perfect analysis? stands for the F1 achieved
using the perfect (post-edited) output of the track-
ing method, i.e., the errors by both tracking and
detecting shifts were corrected. Precisely, the docu-
ments judged as Yes but were not evaluated as Yes
Table 3: Data
Event CNN Reuters
doc para doc para
3(Carter in Bosnia) 26 314 8 37
5(Clinic Murders (Salvi)) 36 416 5 34
6(Comet into Jupiter) 41 539 4 23
8(Death of Kim Jong Il) 28 337 39 353
9(DNA in OJ trial) 108 1,407 6 75
11(Hall?s copter (N. Korea)) 77 875 22 170
12(Humble, TX, flooding) 22 243 0 0
15(Kobe Japan quake) 72 782 12 64
16(Lost in Iraq) 34 395 10 78
17(NYC Subway bombing) 22 374 2 2
18(OK-City bombing) 214 3,209 59 439
21(Serbians down F-16) 50 572 15 135
22(Serbs violate Bihac) 56 669 35 349
24(USAir 427 crash) 32 435 7 98
25(WTC Bombing trial) 18 132 4 54
Avg. 55.4 12.7 15.2 9.7
were eliminated, and the documents judged as No
but were evaluated as Yes were added. Further,
the documents were divided by a human into sev-
eral sets, each of which covers a different subject
related to the same event. The evaluation is made
by three humans. The classification is determined
to be correct if the majority of three human judges
agrees. Table 4 shows that the average F1 of ?Track-
ing results?(0.68) in ?Total? was 0.06 lower than that
of ?Perfect analysis?(0.74). Overall, the result us-
ing ?CNN? was better than that of ?Reuters?. One
reason behind this lies in the difference between the
two corpora: CNN consists of a larger number of
words per paragraph than Reuters. This causes a
high recall rate, since a paragraph which consists of
a large number of words is more likely to include
event, subject-class, and subject words than a para-
graph containing a small number of words.
Recall that in SVMs each value of word wk is
calculated using formula (2), and the larger value
of wk is, the more the word wk features positive
examples. Table 5 illustrates sample words which
Table 4: Performance of paragraph extraction
N
t
Tracking results Perfect analysis
CNN Reuters Total CNN Reuters Total
1 0.70 0.56 0.62
2 0.75 0.60 0.67
4 0.76 0.61 0.70 0.78 0.62 0.74
8 0.76 0.62 0.70
16 0.77 0.62 0.72
Avg. 0.85 0.60 0.68
have the highest weighted value calculated using for-
mula (2). Each classifier, sbj(xp), sbj class(xp),
and event(xp) is the result using both corpora. The
event is the Kobe Japan quake, and the document
which includes xp states that the death toll has
risen to over 800 in the Kobe-Osaka earthquake,
and officials are concentrating on getting people
out. ?Words? denote words which have the highest
weighted value in each classifier and they are used to
determine whether xp is a key paragraph or not. We
assume these words are subject, subject class and
event words, while some words such as ?earthquake?
and ?activity? appear in more than one classifier.
Table 5: Sample words in the Kobe Japan quake
classifier words
sbj(xp) earthquake activity Japan seismologist
news conference living prime minister Mu-
rayama crew Bill Dorman
sbj class(xp) city something floor quake Tokyo after-
shock activity street injury fire seismolo-
gist police people building cry
event(xp) Kobe magnitude survivor earthquake col-
lapse death fire damage aftershock Kyoto
toll quake magnitude emergency Osaka-
Kobe Japan Osaka
Figure 4: F1 v.s. the number of documents
Figure 4 illustrates how the number of documents
influences extraction accuracy. The event is the US-
Air 427 crash, and F1 is 0.68, which is lower than
the average F1 of all events(0.79). The result is when
Nt is 16. ?P ana of tracking? refers to the result us-
ing the post-edited output of the tracking, i.e., only
the errors of tracking were corrected, while ?Perfect
analysis? refers to the result using the output: the
errors by both tracking and detecting shifts were
corrected. Figure 4 shows that our method does
not depend on the number of documents, since the
performance does not monotonically decrease when
the number of documents increases. Figure 4 also
shows that there is no significant difference between
?P ana of tracking? and ?Perfect analysis? compared
to the difference between ?Tracking results? and ?Per-
fect analysis?. This indicates that (i) subject shifts
are correctly detected, and (ii) the performance of
our paragraph extraction explicitly depends on the
tracking results.
We note the contribution of detecting shifts for
paragraph extraction. Figures 5 and 6 illustrate the
recall and precision with two methods: with and
without detecting shift. In the method without de-
tecting shift, we use the ?full memory? approach for
tracking, i.e., SVMs generate its classification model
from all previously seen documents. For the result of
tracking, we extract paragraphs for which sbj(xp) =
1 and sbj class(xp) = 1 hold. We can see from both
Figure 5 and Figure 6 that the method that detects
shifts outperformed the method without detecting
shifts in all Nt values. More surprisingly, Figure 6
shows that the precision scores in all Nt values using
the tracking results with detecting shift were higher
than that of ?P ana? without detecting shift. Fur-
ther, the difference in precision between two meth-
ods is larger than that of recall. This demonstrates
that it is necessary to detect subject shifts and thus
to identify subject class words for paragraph extrac-
tion, since the system without detecting shift ex-
tracts many documents, which yields redundancy.
Figure 5: Recall with and without detecting shift
Figure 6: Precision with and without detecting shift
6 Related Work
Most of the work on summarization task by para-
graph or sentence extraction has applied statistical
techniques based on word distribution to the target
document(Kupiec et al, 1995). More recently, other
approaches have investigated the use of machine
learning to find patterns in documents(Strzalkowski
et al, 1998) and the utility of parameterized mod-
ules so as to deal with different genres or cor-
pora(Goldstein et al, 2000). Some of these ap-
proaches to single document summarization have
been extended to deal with multi-document sum-
marization(Mani and E.Bloedorn, 1997), (Barzilay
et al, 1999), (McKeown et al, 1999).
Our work differs from the earlier work in several
important respects. First, our method focuses on
subject shift of the documents from the target event
rather than the sets of documents from different
events(Radev et al, 2000). Detecting subject shift
from the documents in the target event, however,
presents special difficulties, since these documents
are collected from a very restricted domain. We thus
present a window adjustment algorithm which auto-
matically adjusts the optimal window in the training
documents, so as to include only the data which are
sufficiently related to the current subject. Second,
our approach works in a living way, while many ap-
proaches are stable ones, i.e., they use documents
which are prepared in advance and apply a variety
of techniques to create summaries. We are interested
in a substantially smaller number of initial training
documents, which are then utilized to extract para-
graphs from documents relevant to the initial doc-
uments. Because the small number of documents
which are used for initial training is easy to col-
lect, and costly human intervention can be avoided.
To do this, we use a tracking technique. The small
size of the training corpus, however, requires sophis-
ticated parameters tuning for learning techniques,
since we can not make one or more validation sets
of documents from the initial training documents
which are required for optimal results. Instead we
use E?loo of SVMs to cope with this problem. Fur-
ther, our method does not use specific features for
training such as ?Presence and type of agent? and
?Presence of citation?, which makes it possible to be
extendable to other domains(Teufel, 2001).
7 Conclusion
This paper studied the effectiveness of detecting sub-
ject shifts in paragraph extraction. Future work
includes (i) incorporating Named Entity extraction
into the method, (ii) applying the method to the
TDT2 and TDT3 corpora for quantitative evalua-
tion, and (iii) extending the method to on-line para-
graph extraction for real-world applications, which
will extract key paragraphs each time the document
discusses the target event.
Acknowledgments
We would like to thank Prof. Virginia Teller of
Hunter College CUNY for her valuable comments
and the anonymous reviewers for their helpful sug-
gestions.
References
J. Allan, J.Carbonell, G.Doddington, J.Yamron,
and Y.Yang. 1998a. Topic Detection and Track-
ing pilot study final report. In Proc. of DARPA
Workshop.
J. Allan, R.Papka, and V.Lavrenko. 1998b. On-
line new event detection and tracking. In Proc.
of ACM SIGIR?98, pages 37?45.
R. Barzilay, K. R. McKeown, and M. Elhadad.
1999. Information fusion in the context of multi-
document summarization. In Proc. of ACL?99,
pages 550?557.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of ANLP?92, pages 152?155.
C. Fellbaum, editor. 1998. Nouns in WordNet, An
Electronic Lexical Database. MIT.
J. Goldstein, V.Mittal, J.Carbonell, and
M.Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proc. of the
ANLP/NAACL-2000 Workshop on Automatic
Summarization, pages 40?48.
J. Kupiec, J.Pedersen, and F.Chen. 1995. A train-
able document summarizer. In Proc. of ACM SI-
GIR?95, pages 68?73.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM journal, 2(1):159?165.
I. Mani and E.Bloedorn. 1997. Multi-document
summarization by graph search and merging. In
Proc. of AAAI-97, pages 622?628.
K. McKeown, J.Klavans, V.Hatzivassiloglou,
R.Barzilay, and E.Eskin. 1999. Towards mul-
tidocument summarization by reformulation:
Progress and prospects. In Proc. of the 16th
National Conference on AI, pages 18?22.
D. Radev, H.Jing, and M.Budzikowska. 2000.
Centroid-based summarization of multiple doc-
uments: Sentence extraction, utility-based eval-
uation, and user studies. In Proc. of the
ANLP/NAACL-2000 Workshop on Automatic
Summarization, pages 21?30.
T. Strzalkowski, J.Wang, and B.Wise. 1998. A
robust practical text summarization system. In
Proc. of AAAI Intelligent Text summarization
Workshop, pages 26?30.
S. Teufel. 2001. Task-based evaluation of summary
quality: Describing relationships between scien-
tific papers. In Proc. of NAACL 2001 Workshop
on Automatic Summarization, pages 12?21.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
J. Weston and C.Watkins. 1998. Multi-class Sup-
port Vector Machines. In Technical Report CSD-
TR-98-04.
 	 
   	
     		
   
 	
 

    
  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 233?240
Manchester, August 2008
Retrieving Bilingual Verb?noun Collocations by Integrating
Cross-Language Category Hierarchies
Fumiyo Fukumoto Yoshimi Suzuki Kazuyuki Yamashita?
Interdisciplinary Graduate School of
Medicine and Engineering
Univ. of Yamanashi
{fukumoto, ysuzuki}@yamanashi.ac.jp
?The Center for Educational Research
Faculty of Education and Human Sciences
Univ. of Yamanashi
kazuyuki@yamanashi.ac.jp
Abstract
This paper presents a method of retriev-
ing bilingual collocations of a verb and
its objective noun from cross-lingual docu-
ments with similar contents. Relevant doc-
uments are obtained by integrating cross-
language hierarchies. The results showed a
15.1% improvement over the baseline non-
hierarchy model, and a 6.0% improvement
over use of relevant documents retrieved
from a single hierarchy. Moreover, we
found that some of the retrieved colloca-
tions were domain-specific.
1 Introduction
A bilingual lexicon is important for cross-lingual
NLP applications, such as CLIR, and multilingual
topic tracking. Much of the previous work on find-
ing bilingual lexicons has made use of comparable
corpora, which exhibit various degrees of paral-
lelism. Fung et al (2004) described corpora rang-
ing from noisy parallel, to comparable, and finally
to very non-parallel. Obviously, the latter are easy
to collect because very non-parallel corpora con-
sist of sets of documents in two different languages
from the same period of dates. However, a good
solution is required to produce a higher quality of
lexicon retrieval.
In this paper, we focus on English and Japanese
bilingual verb?objective noun collocations which
we call verb?noun collocations and retrieve them
using very non-parallel corpora. The method first
finds cross-lingual relevant document pairs with
similar contents from non-parallel corpora, and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
then we estimate bilingual verb?noun collocations
within these relevant documents. Relevant doc-
uments are defined here as pairs of English and
Japanese documents that report identical or closely
related contents, e.g., a pair of documents describ-
ing an aircraft crash and the ensuing investigation
to compensate the victims? families or any safety
measures proposed as a result of the crash. In
the task of retrieving cross-lingual relevant docu-
ments, it is crucial to identify an event as some-
thing occurs at some specific place and time asso-
ciated with some specific action. One solution is to
use a topic, i.e., category in the hierarchical struc-
ture, such as Internet directories. Although a topic
is not an event, it can be a broader class of event.
Therefore, it is helpful for retrieving relevant docu-
ments, and thus bilingual verb?noun collocations.
Consider the Reuters?96 and Mainichi newspaper
documents shown in Figure 1. The documents re-
port on the same event, ?Russian space station col-
lides with cargo craft,? were published within two
days of each other, and have overlapping content.
Moreover, as indicated by the double-headed ar-
rows in the figure, there are a number of bilingual
collocations. However, as shown in Figure 1, the
Reuters document is classified into ?Science and
Technology,? while the Mainichi document is clas-
sified into ?Space Navigation?. This is natural be-
cause categories in the hierarchical structures are
defined by different human experts. Therefore, a
hierarchy tends to have some bias in both defining
hierarchical structure and classifying documents,
and as a result some hierarchies written in one lan-
guage are coarse-grained, while others written in
other languages are fine-grained. Our attempt us-
ing the results of integrating different hierarchies
for retrieving relevant documents was postulated
to be able to solve this defect of the differences in
233
Figure 1: Relevant document pairs
hierarchies, and to improve the efficiency and effi-
cacy of retrieving collocations.
2 System Description
The method consists of three steps: integrating cat-
egory hierarchies, retrieving cross-lingual relevant
documents, and retrieving collocations from rele-
vant documents.
2.1 Integrating Hierarchies
The method for integrating different category hier-
archies does not simply merge two different hier-
archies into a large hierarchy, but instead retrieves
pairs of categories, where each category is relevant
to each other.1 The procedure consists of two sub-
steps: Cross-language text classification (CLTC)
and estimating category correspondences.
2.1.1 Cross-language text classification
The corpora we used are the Reuters?96 and the
RWCP of the Mainichi Japanese newspapers. In
the CLTC task, we used English and Japanese data
to train the Reuters?96 categorical hierarchy and
the Mainichi UDC code hierarchy (Mainichi hier-
archy), respectively. In the Reuters?96 hierarchy,
the system was trained using labeled English doc-
uments, and classified translated labeled Japanese
1The reason for retrieving pairs of categories is that each
categorical hierarchy is defined by individual human experts,
and different linguists often identify different numbers of cat-
egories for the same concepts. Therefore, it is impossible to
handle full integration of hierarchies.
Figure 2: Cross-language text classification
documents. Similarly, for Mainichi hierarchy,
the system was trained using labeled Japanese
documents, and classified translated labeled En-
glish documents. We used Japanese-English and
English-Japanese MT software.
We used a learning model, Support Vector Ma-
chines (SVMs) (Vapnik, 1995), to classify docu-
ments, as SVMs have been shown to be effective
for text classification. We used the ?One-against-
the-Rest? version of the SVMs at each level of a hi-
erarchy. We classify test documents using a hierar-
chy by learning separate classifiers at each internal
node of the hierarchy. We used a Boolean func-
tion b(L
1
)&&? ? ?&&b(L
m
), where b(L
i
) is a deci-
sion threshold value of the i-th hierarchical level.
The process is repeated by greedily selecting sub-
branches until a leaf is reached.
We classified translated Mainichi documents
with Mainichi category m into Reuters categories
using SVMs classifiers. Similarly, each translated
Reuters document with category r was classified
into Mainichi categories. Figure 2 illustrates the
classification of Reuters and Mainichi documents.
A document with Mainichi category ?m1? is clas-
sified into Reuters category ?r12?, and a docu-
ment with Reuters category ?r1? is classified into
Mainichi category ?m21?. As a result, we ob-
tained category pairs, e.g., (r12, m1), and (m21,
r1), from the documents assigned to the categories
in each hierarchy.
2.1.2 Estimating category correspondences
The assumption of category correspondences is
that semantically similar categories, such as ?Eq-
uity markets? and ?Bond markets? exhibit simi-
lar statistical properties than dissimilar categories,
such as ?Equity markets? and ?Sports?. We ap-
plied ?2 statistics to the results of CLTC. Let us
take a look at the Reuters?96 hierarchy. Sup-
234
pose that the translated Mainichi document with
Mainichi category m ? M (where M is a set of
Mainichi categories) is assigned to Reuters cate-
gory r ? R (R is a set of Reuters?96 categories).
We can retrieve Reuters and Mainichi category
pairs, and estimate category correspondences ac-
cording to the ?2 statistics shown in Eq. (1).
?
2
(r, m) =
f(r, m)? E(r,m)
E(r,m)
(1)
where E(r, m) = S
r
?
S
m
S
R
,
S
r
=
?
k?M
f(r, k), S
R
=
?
r?R
S
r
.
Here, the co-occurrence frequency of r and m,
f(r,m) is equal to the number of category m doc-
uments assigned to r. Similar to the Reuters hier-
archy, we can estimate category correspondences
from Mainichi hierarchy, and extract a pair (r, m)
according to the ?2 value. We note that the sim-
ilarity obtained by each hierarchy does not have
a fixed range. Thus, we apply the normalization
strategy shown in Eq. (2) to the results obtained
by each hierarchy to bring the similarity value into
the range [0,1].
?
2
new
(r, m) =
?
2
old
(r,m)? ?
2
min
(r,m)
?
2
max
(r,m)? ?
2
min
(r,m)
. (2)
Let SP
r
and SP
m
are a set of pairs obtained by
Reuters hierarchy and Mainichi hierarchy, respec-
tively. We construct the set of r and m category
pairs, SP
(r,m)
= {(r,m) | (r,m) ? SP
r
? SP
m
},
where each pair is sorted in descending order of ?2
value. For each pair of SP
(r,m)
, if the value of ?2
is higher than a lower bound L
?
2
, two categories,
r and m, are regarded as similar.2
2.2 Retrieval of Relevant Documents
We used the results of category correspondences
from the Reuters and Mainichi hierarchies to re-
trieve relevant documents. Recall that we used
English and Japanese documents with quite dif-
ferent hierarchical structures. The task thus con-
sists of two criteria: retrieving relevant documents
based on English (we call this Int hi & Eng) and
in Japanese (Int hi & Jap). Let dr
i
(1 ? i ? s) be a
Reuters document that is classified into the Reuters
category r. Let dm
j
(1 ? j ? t) be a Mainichi
2We set ?2 value of each element of SP
(r,m)
to a higher
value of either (r,m) ? SP
r
or (r,m) ? SP
m
.
Figure 3: Retrieving relevant documents
document that belongs to the Mainichi category m.
Here, s and t are the number of documents classi-
fied into r and m, respectively. Each Reuters doc-
ument dr
i
is translated into a Japanese document
d
r mt
i
by an MT system. Each Mainichi document
d
m
j
is translated into an English document dm mt
j
.
Retrieving relevant documents itself is quite
simple. As illustrated in Figure 3, in ?Int hi
& Eng? with a set of similar categories consist-
ing of r and m, for each Reuters and translated
Mainichi document, we calculate BM25 similari-
ties between them.
BM25(dr
i
, d
m mt
j
) =
?
w?d
m mt
j
w
(1)
(k
1
+ 1)tf
K + tf
(k
3
+ 1)qtf
k
3
+ qtf
, (3)
where w is a word within dm mt
j
, and w(1) is the
weight of w, w(1) = log (N?n+0.5)
(n+0.5)
. N is the num-
ber of Reuters documents within the same category
r, and n is the number of documents which con-
tains w. K refers to k
1
((1 ? b) + b
dl
avdl
). k
1
, b,
and k
3
are parameters and set to 1, 1, and 1,000,
respectively. dl is the document length of dr
i
and
avdl is the average document length in words. tf
and qtf are the frequency of occurrence of w in dr
i
,
and dm mt
j
, respectively. If the similarity value be-
tween them is higher than a lower bound L
?
, we
regarded these as relevant documents. The pro-
cedure is applied to all documents belonging to
the sets of similar categories. ?Int hi & Jap? is
the same as ?Int hi & Eng? except for the use of
d
r mt
i
and dm
j
for comparison. We compared the
performance of these tasks, and found that ?Int hi
& Eng? was better than ?Int hi & Jap?. In section
3, we show results with ?Int hi & Eng? due to lack
of space.
235
2.3 Acquisition of Bilingual Collocations
The final step is to estimate bilingual correspon-
dences from relevant documents. All Japanese
documents were parsed using the syntactic ana-
lyzer CaboCha (Kudo and Matsumoto, 2003). En-
glish documents were parsed with the syntactic an-
alyzer (Lin, 1993). In both English and Japanese,
we extracted all the dependency triplets(obj, n, v).
Here, n refers to a noun which is an object(obj)
of a verb v in a sentence.3 Hereafter, we de-
scribe the Reuters English dependency triplet as
vn
r
, and that of Mainichi as vn
m
. The method to
retrieve bilingual correspondences consists of two
sub-steps: document-based retrieval and sentence-
based retrieval.
2.3.1 Document-based retrieval
We extract vn
r
and vn
m
pairs from the results
of relevant documents:
{vn
r
, vn
m
} s.t.
?
d
r
i
 vn
r
,
?
d
m
j
 vn
m
BM25(dr
i
, d
m mt
j
) ? L
?
. (4)
Next, we estimate the bilingual correspondences
according to the ?2(vn
r
, vn
m
) statistics shown in
Eq. (1). In Eq. (1), we replace r by vn
r
and m by
vn
m
. f(r,m) is replaced by f(vn
r
, vn
m
), i.e., the
co-occurrence frequency of vn
r
and vn
m
.
2.3.2 Sentence-based retrieval
We note that bilingual correspondences ob-
tained by document-based retrieval are not reli-
able. This is because many verb?noun colloca-
tions appear in a pair of relevant documents, as
can be seen from Figure 1. Therefore, we applied
sentence-based retrieval to the results obtained by
document-based retrieval. First, we extract vn
r
and vn
m
pairs the ?2 values of which are higher
than 0. Next, for each vn
r
and vn
m
pair, we as-
sign sentence-based similarity:
S sim(vn
r
, vn
m
) =
max
S vn
r
?Set
r
,S vn
m
?Set
m
sim(S vn
r
, S vn
m
) . (5)
Here, Set
r
and Set
m
are a set of sentences that
include vn
r
and vn
m
, respectively. The similarity
between S vn
r
and S vn
m
is shown in Eq. (6).
3We used the particle ?wo? as an object relationship in
Japanese.
sim(S vn
r
, S vn
m
) =
co(S vn
r
? S
mt
vn
m
)
| S vn
r
| + | S
mt
vn
m
| ?2co(S vn
r
? S
mt
vn
m
) + 2
, (6)
where |X| is the number of content words in a sen-
tence X, and co(S vn
r
? S
mt
vn
m
) refers to the
number of content words that appear in both S vn
r
and Smt vn
m
. S
mt
vn
m
is a translation result of
S vm
m
. We retrieved vn
r
and vn
m
as a bilingual
lexicon that satisfies:
{vn
r
, vn
m
} = argmax
{vn
r
?
?BP (vn
m
), vn
m
}
S sim(vn
r
?
, vn
m
) , (7)
where BP (vn
m
) is a set of bilingual verb?noun
pairs, each of which includes vn
m
on the Japanese
side.
3 Experiments
3.1 Integrating hierarchies
3.1.1 Experimental setup
We used Reuters?96 and UDC code hierarchies.
The Reuters?96 corpus from 20th Aug. 1996 to
19th Aug. 1997 consists of 806,791 documents
organized into coarse-grained categories, i.e., 126
categories with a four-level hierarchy. The RWCP
corpus labeled with UDC codes selected from
1994 Mainichi newspaper consists of 27,755 doc-
uments organized into a fine-grained categories,
i.e., 9,951 categories with a seven-level hierarchy
(RWCP., 1998). We used Japanese-English and
English-Japanese MT software (Internet Honyaku-
no-Ousama for Linux, Ver.5, IBM Corp.) for
CLTC. We divided both Reuters?96 (from 20th
Aug. 1996 to 19th May 1997) and RWCP corpora
into two equal sets: a training set to train SVM
classifiers, and a test set for TC to generate pairs
of similar categories. We divided the test set into
two parts: the first was used to estimate thresholds,
i.e., a decision threshold b used in CLTC, and lower
bound L
?
2
; and the second was used to generate
pairs of similar categories using the threshold. We
chose b = 0 for each level of a hierarchy. The lower
bound L
?
2
was .003.
We selected 109 categories from Reuters and
4,739 categories from Mainichi, which have at
least five documents in each set. We used con-
tent words for both English and Japanese docu-
ments. We compared the results obtained by hi-
erarchical approach to those obtained by the flat
236
Table 1: Performance of category correspondences
Hierarchy Flat
Prec Rec F1 Prec Rec F1
Mai & Reu .503 .463 .482 .462 .389 .422
Reu .342 .329 .335 .240 .296 .265
Mai .157 .293 .204 .149 .277 .194
non-hierarchical approach. Moreover, in the hier-
archical approach, we applied a Boolean function
to each test document.
For evaluation of category correspondences, we
used F1-score (F1) which is a measure that bal-
ances precision (Prec) and recall (Rec). Let Cor
be a set of correct category pairs.4 The precise def-
initions of the precision and recall of the task are
given below:
Prec =
| {(r, m) | (r,m) ? Cor, ?
2
(r, m) ? L
?
2
} |
| {(r, m) | ?
2
(r, m) ? L
?
2
} |
Rec =
| {(r, m) | (r,m) ? Cor, ?
2
(r, m) ? L
?
2
} |
| {(r, m) | (r,m) ? Cor} |
3.1.2 Results
Table 1 shows F1 of category correspondences
with L
?
2
= .003. ?Mai & Reu? shows the results
obtained by our method. ?Mai? and ?Reu? show
the results using only one hierarchy. For example,
?Mai? shows the results in which both Mainichi
and translated Reuters documents are classified
into categories with Mainichi hierarchy, and esti-
mated category correspondences.
Integrating hierarchies is more effective than
only a single hierarchy. Moreover, we found ad-
vantages in the F1 for the hierarchical approach
(?Hierarchy? in Table 1) in comparison with a
baseline flat approach (?Flat?). We note that the
result of ?Mai? was worse than that of ?Reu? in
both approaches. One reason is that the accuracy
of TC. The micro-average F1 of TC for Reuters hi-
erarchy was .815, while that of Mainichi was .673,
as Mainichi hierarchy consists of many categories,
and the number of training data for each category
were smaller than those of Reuters. The results ob-
tained by our method depend on the performance
of TC. Therefore, it will be necessary to examine
some semi-supervised learning techniques to im-
prove classification accuracy.
4The classification was determined to be correct if the two
human judges agreed on the evaluation.
Table 2: Data for retrieving documents
Jap ? Eng(? 3) Total # of doc. Total # of
Jap Eng relevant doc.
26/06/97 391 15,482 513
3.2 Relevant document retrieval
3.2.1 Experimental setup
The training data for choosing the lower bound
L
?
used in the relevant document retrieval is
Reuters and RWCP from 13th to 21st Jun. 1997.
The difference in dates between them is less than
? 3 days. For example, when the date of the
RWCP document is 18th Jun., the corresponding
Reuters date is from 15th to 21st Jun. We chose
L
?
that maximized the average F1 among them.
Table 2 shows the test data, i.e., the total number
of collected documents and the number of related
documents collected manually for the evaluation.5
We implemented the following approaches includ-
ing related work, and compared these results with
those obtained by our methods, Int hi & Eng.
1. No hierarchy: Categories with each hierar-
chy are not used in the approach. The ap-
proach is the same as the method reported by
Collier et al (1998) except for term weights
and similarities. We calculate similarities be-
tween Reuters and translated Mainichi docu-
ments, where the difference in dates is less
than ? 3 days. (No hi & Eng).
2. Hierarchy: The approach uses only Reuters
hierarchy (we call this Reu Hierarchy).
Reuters documents and translated Mainichi
documents are classified into categories with
Reuters hierarchy. We calculate BM25
between Reuters and Mainichi documents
within the same category. The procedure is
applied for all categories of the hierarchies.
The judgment of relevant documents was the
same as our method: if the value of similarity be-
tween two documents is higher than a lower bound
L
?
, we regarded them as relevant documents.
3.2.2 Results
The retrieval results are shown in Table 3 and
Figure 4. Table 3 shows best performance of each
method against L
?
. As can be seen clearly from
Table 3 and Figure 4, the results with integrating
hierarchies improved overall performance.
5The classification was determined by two human.
237
Table 3: Retrieval performance
Prec Rec F1-score L
?
No hi & Eng .417 .322 .363 40
Reu Hierarchy .356 .544 .430 20
Int hi & Eng .839 .585 .689 20
Figure 4: F1 of retrieving relevant documents
Table 4 shows the total number of document
pairs (P), Reuters (E), and Mainichi documents (J),
which satisfied the similarity lower bound L
?
. As
shown in Table 4, the number of retrieved pairs
by non-hierarchy approach was much greater than
that of ?Int hi & Eng? at all L
?
values. This is be-
cause pairs are retrieved by using only the BM25.
Therefore, many of the document pairs retrieved
do not have closely related contents, even if L
?
is
set to a higher value.
The results of a single hierarchy showed re-
call of .544, while that of the integrating hier-
archies was .585 at the same L
?
value (20), as
shown in Table 3. This is because in the sin-
gle hierarchy method, there are some translated
Mainichi documents that are not correctly clas-
sified into categories with the Reuters hierarchy.
For example, ?Hashimoto remarks on fx rates?
in Mainichi documents should be classified into
Reuters category ?Forex markets,? but it was clas-
sified into ?Government?. As a result, ?U.S. Trea-
sury has no comment on Hashimoto fx remarks?
in Reuters category ?Forex markets? and the doc-
ument ?Hashimoto? are not retrieved by a single
hierarchy approach. In contrast, in the integrat-
ing method, these two documents are classified
correctly into a pair of similar categories, i.e., the
?U.S Treasury? is classified into Reuters category
?Forex markets?, and the ?Hashimoto? is clas-
sified into Mainichi category ?Money and bank-
ing?. These observations show that our method
contributes to the retrieval of relevant documents.
Table 4: # of documents vs L
?
Approach Lower Bound L?
100 80 60 40 20
p 188 319 630 1,229 3,000
No hi & Eng E 150 272 543 987 2,053
J 13 16 19 22 25
p 12 17 25 47 186
Reu Hierarchy E 8 12 19 36 142
J 8 10 12 18 25
p 46 61 83 135 218
Int hi & Eng E 32 43 60 99 158
J 4 4 5 7 9
Table 5: # of J/E document pairs with L
?
Approach & (L
?
) pairs Eng Jap
No hi & Eng (40) 3,042,166 428,042 70,080
Reu Hierarchy (20) 27,181,243 43,0181 99,452
Int hi & Eng (20) 81,904,243 45,965 654,787
3.3 Bilingual Verb?noun Collocations
Finally, we report the results of bilingual verb?
noun collocations.
3.3.1 Experimental setup
The data for relevant document retrieval was the
Reuters and Mainichi corpora from the same pe-
riod, i.e., 20th Aug. 1996 to 19th Aug. 1997. The
total number of Reuters documents was 806,791,
and that of Mainichi was 119,822. As the num-
ber of Reuters documents was far greater than that
of Mainichi documents, we estimated collocations
from the results of cross-lingually retrieving rele-
vant English documents with Japanese query doc-
uments. The difference in dates between them was
less than ? 3 days. Table 5 shows retrieved rele-
vant documents that showed best performance of
each method against L
?
. From these data, we ex-
tracted bilingual verb-noun collocations.
3.3.2 Results
Table 6 shows the numbers of English and
Japanese monolingual verb?noun collocations,
those of candidate collocations against which
bilingual correspondences were estimated, and
those of correct collocations. ?D & S? of candidate
collocations indicates the number of collocations
when we applied both document- and sentence-
based retrieval. ?Doc? indicates the number of col-
locations when we applied only document-based
retrieval. ?D & S? and ?Doc? of correct colloca-
tions show the number of correct collocations in
the topmost 1,000 according to sentence similar-
ity and the ?2 statistics, respectively. As shown in
238
Table 6, the results obtained by integrating hierar-
chies showed a 15.1% (32.8 - 17.7) improvement
over the baseline non-hierarchy model, and a 6.0%
(32.8 - 26.8) improvement over use of a single hi-
erarchy. We manually compared those 328 bilin-
gual collocations with an existing bilingual lexi-
con where 78 of them (23.8%) were not included
in it.6 Moreover, 168 of 328 (51.2%) were not
correctly translated by Japanese-English MT soft-
ware.7 These observations clearly support the use-
fulness of the method.
It is very important to compare the column
?rate? for the numbers of candidate collocations
with that for the numbers of correct collocations.
In all approaches, sentence-based retrieval was
effective in removing useless collocations, espe-
cially in our method, about 1.5% of the size
obtained by ?Doc? was retrieved, while about
4.6(328/72) times the number of correct colloca-
tions were obtained in the topmost 1,000 colloca-
tions. These observations showed that sentence-
based retrieval contributes to a marked reduction
in the number of useless collocations without a de-
crease in accuracy.
The last column in Table 6 shows the results us-
ing Inverse Rank Score (IRS), which is a measure
of system performance by considering the rank of
correct bilingual collocations within the candidate
collocations. It is the sum of the inverse rank of
each matching collocations, e.g., correct colloca-
tions by manual evaluation matches at ranks 2 and
4 give an IRS of 1
2
+ 1
4
= 0.75. With at most 1,000
collocations, the maximum IRS score is 7.485, and
the higher the IRS value, the better the system per-
formance. As shown in Table 6, the performance
by integrating hierarchies was much better than
that of the non-hierarchical approach, and slightly
better than those obtained by a single hierarchy.
However, correct retrieved collocations were dif-
ferent from each other. Table 7 lists examples of
bilingual collocations obtained by a single hierar-
chy and integrating hierarchies. The category is
?Sport?.8 (x,y) of category pair in Table 7 refer to
Reuters and Mainichi category correspondences.
Examples in Table 7 denote only English verb?
6We used an existing bilingual lexicon, Eijiro on the Web,
1.91 million words, (http://www.alc.co.jp) for evaluation. If
collocations were not included, the estimation was deter-
mined by two human judges.
7The number of words in the Japanese-English dictionary
(Internet Honyaku-no-Ousama for Linux, Ver.5, IBM Corp.)
was about 250,000.
8We obtained 98 category pairs in the Sport category.
noun collocations.
It is interesting to note that 12 of 154 colloca-
tions, such as ?earn medal? and ?block shot? ob-
tained by integrating hierarchies were also ob-
tained by a single hierarchy approach. How-
ever, other collocations such as ?get strikeout? and
?make birdie? which were obtained in a particular
category (Sport, Baseball) and (Sport, Golf), did
not appear in either of the results using a single
hierarchy or a non hierarchical approach. These
observations again clearly support the usefulness
of our method.
4 Previous Work
Much of the previous work on finding bilingual
lexicons used comparable corpora. One attempt
involved directly retrieving bilingual lexicons from
corpora. One approach focused on extracting word
translations (Gaussier et al, 2004). The techniques
were based on the idea that semantically similar
words appear in similar contexts. Unlike paral-
lel corpora, the position of a word in a document
is useless for translation into the other language.
In these techniques, the frequency of words in the
monolingual document is calculated and their con-
textual similarity is measured across languages.
Another approach focused on sentence extraction
(Fung and Cheung, 2004). One limitation of all
these methods is that they need to control the ex-
perimental evaluation to avoid estimation of every
bilingual lexicon appearing in comparable corpora.
The alternative consists of two steps: first, cross-
lingual relevant documents are retrieved from
comparable corpora, then bilingual term corre-
spondences within these relevant documents are
estimated. Thus, the accuracy depends on the per-
formance of relevant documents retrieval. Much
of the previous work in finding relevant docu-
ments used MT systems or existing bilingual lexi-
cons to translate one language into another. Doc-
ument pairs are then retrieved using some mea-
sure of document similarity. Another approach to
retrieving relevant documents involves the collec-
tion of relevant document URLs from the WWW
(Resnik and Smith, 2003). Utsuro et al (2003)
proposed a method for acquiring bilingual lex-
icons that involved retrieval of relevant English
and Japanese documents from news sites on the
WWW. Our work is also applicable to retrieval
of relevant documents on the web because it es-
timates every bilingual lexicon only appearing in
239
Table 6: Numbers of monolingual and bilingual verb?noun collocations
Approach & (L
?
) # of Candidate collocations # of Correct collocations Inverse
(top 1,000) rank score
Monolingual patterns # of collocations rate # of collocations rate(D & S/ (D & S/ (top 1,000)
Jap Eng D & S Doc Doc) D & S Doc Doc) D & S Doc
No hi & Eng (40) 25,163 44,762 25,163 6,976,214 .361 177 62 2.9 1.35 0.71
Reu Hierarchy (20) 10,576 37,022 10,576 1,272,102 .831 268 64 4.2 2.24 1.41
Int hi & Eng (20) 8,347 21,524 8,347 560,472 1.489 328 72 4.6 2.33 1.46
Table 7: Examples of bilingual verb?noun collocations
Approach & (L
?
) Category or # of collocations # of correct Examples (English)
category pair D & S Doc collocations(%)
Reu Hierarchy (20) Sport 262 19,391 36(13.7) create chance, earn medal, feel pressure
block shot, establish record, take chance
(Sport, Baseball) 110 8,838 24(21.8) get strikeout, leave base, throw pitch
(Sport, Relay) 177 3,418 18(10.2) lead ranking, run km, win athletic
(Sport, Tennis) 115 2,656 32(27.8) lose prize money, play exhibition game
Int hi & Eng (20) (Sport, Golf) 131 2,654 28(21.4) make birdie, have birdie, hole putt, miss putt
(Sport, Soccer) 86 1,317 34(39.5) block shot, score defender, give free kick
(Sport, Sumo) 75 773 2(2.7) lead sumo, set championship
(Sport, Ski jump) 68 661 10(14.7) postpone downhill, earn medal
(Sport, Football) 37 461 6(16.2) play football, lease football stadium
a set of smaller documents belonging to pairs of
similar categories. Munteanu and Marcu (2006)
proposed a method for extracting parallel sub-
sentential fragments from very non-parallel bilin-
gual corpora. The method is based on the fact that
very non-parallel corpora has none or few good
sentence pairs, while existing methods for exploit-
ing comparable corpora look for parallel data at
the sentence level. Their methodology is the first
aimed at detecting sub-sentential correspondences,
while they have not reported that the method is
also applicable for large amount of data with good
performance, especially in the case of large-scale
evaluation such as that presented in this paper.
5 Conclusion
We have developed an approach to bilingual verb?
noun collocations from non-parallel corpora. The
results showed the effectiveness of the method.
Future work will include: (i) applying the method
to retrieve other types of collocations (Smadja,
1993), and (ii) evaluating the method using Inter-
net directories.
References
Collier, N., H. Hirakawa, and A. Kumano. 1998. Machine
Translation vs. Dictionary Term Translation - a Compar-
ison for English-Japanese News Article Alignment. In
Proc. of 36th ACL and 17th COLING., pages 263?267.
Fung, P. and P. Cheung. 2004. Mining Very Non-Parallel
Corpora: Parallel Sentence and Lexicon Extraction vie
Bootstrapping and EM. In Proc. of EMNLP2004., pages
57?63.
Gaussier, E., H-M. Renders, I. Matveeva, C. Goutte, and
H. De?jean. 2004. A Geometric View on Bilingual Lex-
icon Extraction from Comparable Corpora. In Proc. of
42nd ACL, pages 527?534.
Kudo, T. and Y. Matsumoto. 2003. Fast Methods for Kernel-
based Text Analysis. In Proc. of 41th ACL, pages 24?31.
Lin, D. 1993. Principle-based Parsing without Overgenera-
tion. In Proc. of 31st ACL, pages 112?120.
Munteanu, D. S. and D. Marcu. 2006. Extracting Parallel
Sub-Sentential Fragments from Non-Parallel Corpora. In
Proc. of 21st COLING and 44th ACL., pages 81?88.
Resnik, P. and N. A. Smith. 2003. The Web as a Parallel
Corpus. Computational Linguistics., 29(3):349?380.
RWCP. 1998. Rwc Text Database. In Real World Computing
Partnership.
Smadja, F. 1993. Retrieving Collocations from Text: Xtract.
Computational Linguistics., 19(1):243?178.
Utsuro, T., T. Horiuchi, T. Hamamoto, K. Hino, and
T. Nakayama. 2003. Effect of Cross-Language IR in
Bilingual Lexicon Acquisition from Comparable Corpora.
In Proc. of 10th EACL., pages 355?362.
Vapnik, V. 1995. The Nature of Statistical Learning Theory.
Springer.
240
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 231?238,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Bilingual Comparable Corpora and Semi-supervised Clustering for
Topic Tracking
Fumiyo Fukumoto
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
fukumoto@yamanashi.ac.jp
Yoshimi Suzuki
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
ysuzuki@yamanashi.ac.jp
Abstract
We address the problem dealing with
skewed data, and propose a method for
estimating effective training stories for the
topic tracking task. For a small number of
labelled positive stories, we extract story
pairs which consist of positive and its as-
sociated stories from bilingual comparable
corpora. To overcome the problem of a
large number of labelled negative stories,
we classify them into some clusters. This
is done by using k-means with EM. The
results on the TDT corpora show the ef-
fectiveness of the method.
1 Introduction
With the exponential growth of information on the
Internet, it is becoming increasingly difficult to
find and organize relevant materials. Topic Track-
ing defined by the TDT project is a research area
to attack the problem. It starts from a few sample
stories and finds all subsequent stories that discuss
the target topic. Here, a topic in the TDT con-
text is something that happens at a specific place
and time associated with some specific actions. A
wide range of statistical and ML techniques have
been applied to topic tracking(Carbonell et. al,
1999; Oard, 1999; Franz, 2001; Larkey, 2004).
The main task of these techniques is to tune the
parameters or the threshold to produce optimal re-
sults. However, parameter tuning is a tricky issue
for tracking(Yang, 2000) because the number of
initial positive training stories is very small (one
to four), and topics are localized in space and time.
For example, ?Taipei Mayoral Elections? and ?U.S.
Mid-term Elections? are topics, but ?Elections? is
not a topic. Therefore, the system needs to esti-
mate whether or not the test stories are the same
topic with few information about the topic. More-
over, the training data is skewed data, i.e. there
is a large number of labelled negative stories com-
pared to positive ones. The system thus needs to
balance the amount of positive and negative train-
ing stories not to hamper the accuracy of estima-
tion.
In this paper, we propose a method for esti-
mating efficient training stories for topic track-
ing. For a small number of labelled positive sto-
ries, we use bilingual comparable corpora (TDT1-
3 English and Japanese newspapers, Mainichi and
Yomiuri Shimbun). Our hypothesis using bilin-
gual corpora is that many of the broadcasting sta-
tion from one country report local events more fre-
quently and in more detail than overseas? broad-
casting stations, even if it is a world-wide famous
ones. Let us take a look at some topic from
the TDT corpora. A topic, ?Kobe Japan quake?
from the TDT1 is a world-wide famous one, and
89 stories are included in the TDT1. However,
Mainichi and Yomiuri Japanese newspapers have
much more stories from the same period of time,
i.e. 5,029 and 4,883 stories for each. These obser-
vations show that it is crucial to investigate the use
of bilingual comparable corpora based on the NL
techniques in terms of collecting more information
about some specific topics. We extract Japanese
stories which are relevant to the positive English
stories using English-Japanese bilingual corpora,
together with the EDR bilingual dictionary. The
associated story is the result of alignment of a
Japanese term association with an English term as-
sociation.
For a large number of labelled negative sto-
ries, we classify them into some clusters us-
ing labelled positive stories. We used a semi-
supervised clustering technique which combines
231
labeled and unlabeled stories during clustering.
Our goal for semi-supervised clustering is to clas-
sify negative stories into clusters where each clus-
ter is meaningful in terms of class distribution
provided by one cluster of positive training sto-
ries. We introduce k-means clustering that can be
viewed as instances of the EM algorithm, and clas-
sify negative stories into clusters. In general, the
number of clusters k for the k-means algorithm is
not given beforehand. We thus use the Bayesian
Information Criterion (BIC) as the splitting crite-
rion, and select the proper number for k.
2 Related Work
Most of the work which addresses the small num-
ber of positive training stories applies statistical
techniques based on word distribution and ML
techniques. Allan et. al explored on-line adaptive
filtering approaches based on the threshold strat-
egy to tackle the problem(Allan et. al, 1998). The
basic idea behind their work is that stories closer
together in the stream are more likely to discuss re-
lated topics than stories further apart. The method
is based on unsupervised learning techniques ex-
cept for its incremental nature. When a tracking
query is first created from the Nt training stories,
it is also given a threshold. During the tracking
phase, if a story S scores over that threshold, S
is regarded to be relevant and the query is regen-
erated as if S were among the Nt training sto-
ries. This method was tested using the TDT1 cor-
pus and it was found that the adaptive approach
is highly successful. But adding more than four
training stories provided only little help, although
in their approach, 12 training stories were added.
The method proposed in this paper is similar to
Allan?s method, however our method for collect-
ing relevant stories is based on story pairs which
are extracted from bilingual comparable corpora.
The methods for finding bilingual story pairs
are well studied in the cross-language IR task,
or MT systems/bilingual lexicons(Dagan, 1997).
Much of the previous work uses cosine similar-
ity between story term vectors with some weight-
ing techniques(Allan et. al, 1998) such as TF-IDF,
or cross-language similarities of terms. However,
most of them rely on only two stories in question
to estimate whether or not they are about the same
topic. We use multiple-links among stories to
produce optimal results.
In the TDT tracking task, classifying negative
stories into meaningful groups is also an im-
portant issue to track topics, since a large num-
ber of labelled negative stories are available in
the TDT context. Basu et. al. proposed a
method using k-means clustering with the EM al-
gorithm, where labeled data provides prior infor-
mation about the conditional distribution of hid-
den category labels(Basu, 2002). They reported
that the method outperformed the standard random
seeding and COP-k-means(Wagstaff, 2001). Our
method shares the basic idea with Basu et. al. An
important difference with their method is that our
method does not require the number of clusters k
in advance, since it is determined during cluster-
ing. We use the BIC as the splitting criterion, and
estimate the proper number for k. It is an impor-
tant feature because in the tracking task, no knowl-
edge of the number of topics in the negative train-
ing stories is available.
3 System Description
The system consists of four procedures: extracting
bilingual story pairs, extracting monolingual story
pairs, clustering negative stories, and tracking.
3.1 Extracting Bilingual Story Pairs
We extract story pairs which consist of positive
English story and its associated Japanese stories
using the TDT English and Mainichi and Yomi-
uri Japanese corpora. To address the optimal pos-
itive English and their associated Japanese stories,
we combine the output of similarities(multiple-
links). The idea comes from speech recognition
where two outputs are combined to yield a better
result in average. Fig.1 illustrates multiple-links.
The TDT English corpus consists of training and
test stories. Training stories are further divided
into positive(black box) and negative stories(doted
box). Arrows in Fig.1 refer to an edge with simi-
larity value between stories. In Fig.1, for example,
whether the story J
2
discusses the target topic, and
is related to E
1
or not is determined by not only the
value of similarity between E
1
and J
2
, but also the
similarities between J
2
and J
4
, E
1
and J
4
.
Extracting story pairs is summarized as follows:
Let initial positive training stories E
1
, ? ? ?, Em be
initial node, and each Japanese stories J
1
, ? ? ?, Jm?
be node or terminal node in the graph G. We cal-
culate cosine similarities between Ei(1 ? i ? m)
and Jj(1 ? j ? m?)1. In a similar way, we calcu-
1m? refers to the difference of dates between English and
232
training stories
test stories time lines
TDT English corpus
E1 E2 E3
edge(E1,J1)
edge(E1,J4)
time lines
Mainichi and Yomiuri Japanese corpora topic
J1 J2 J3 J4 J5 J6 Jm?
edge(J2,J4)
not topic
Figure 1: Multiple-links among stories
late similarities between Jk and Jl(1 ? k, l ? m?).
If the value of similarity between nodes is larger
than a certain threshold, we connect them by an
edge(bold arrow in Fig.1). Next, we delete an edge
which is not a constituent of maximal connected
sub-graph(doted arrow in Fig.1). After eliminat-
ing edges, we extract pairs of initial positive En-
glish story Ei and Japanese story Jj as a linked
story pair, and add associated Japanese story Jj
to the training stories. In Fig.1, E
1
, J
2
, and J
4
are extracted. The procedure for calculating co-
sine similarities between Ei and Jj consists of two
sub-steps: extracting terms, and estimating bilin-
gual term correspondences.
Extracting terms
The first step to calculate similarity between
Ei and Jj is to align a Japanese term with its
associated English term using the bilingual dic-
tionary, EDR. However, this naive method suf-
fers from frequent failure due to incompleteness
of the bilingual dictionary. Let us take a look at
the Mainichi Japanese newspaper stories. The to-
tal number of terms(words) from Oct. 1, 1998 to
Dec. 31, 1998, was 528,726. Of these, 370,013
terms are not included in the EDR bilingual dic-
tionary. For example, ?????? (Endeavour)?
which is a key term for the topic ?Shuttle Endeav-
our mission for space station? from the TDT3 cor-
pus is not included in the EDR bilingual dictio-
nary. New terms which fail to segment by dur-
ing a morphological analysis are also a problem in
calculating similarities between stories in mono-
lingual data. For example, a proper noun ????
????(Tokyo Metropolitan Univ.) is divided into
three terms, ???? (Metropolitan), ??? (Univ.)?,
Japanese story pairs.
Table 1: tE and tJ matrix
tE
tE ? siE tE ? s
i
E
tJ
tJ ? S
?i
J a b
tJ ? S
?i
J c d
and ??? (Tokyo)?. To tackle these problems, we
conducted term extraction from a large collection
of English and Japanese corpora. There are several
techniques for term extraction(Chen, 1996). We
used n-gram model with Church-Gale smoothing,
since Chen reported that it outperforms all existing
methods on bigram models produced from large
training data. The length of the extracted terms
does not have a fixed range2. We thus applied the
normalization strategy which is shown in Eq.(1)
to each length of the terms to bring the probabil-
ity value into the range [0,1]. We extracted terms
whose probability value is greater than a certain
threshold. Words from the TDT English(Japanese
newspaper) corpora are identified if they match the
extracted terms.
simnew =
simold ? simmin
simmax ? simmin
(1)
Bilingual term correspondences
The second step to calculate similarity between
Ei and Jj is to estimate bilingual term correspon-
dences using ?2 statistics. We estimated bilingual
term correspondences with a large collection of
English and Japanese data. More precisely, let Ei
be an English story (1 ? i ? n), where n is the
number of stories in the collection, and SiJ denote
the set of Japanese stories with cosine similarities
higher than a certain threshold value ?: SiJ = {Jj
| cos(Ei, Jj) ? ?}. Then, we concatenate con-
stituent Japanese stories of SiJ into one story S
?i
J ,
and construct a pseudo-parallel corpus PPCEJ of
English and Japanese stories: PPCEJ = { { Ei,
S
?i
J } | S
i
J = 0 }. Suppose that there are two crite-
ria, monolingual term tE in English story and tJ in
Japanese story. We can determine whether or not a
particular term belongs to a particular story. Con-
sequently, terms are divided into four classes, as
shown in Table 1. Based on the contingency table
of co-occurence frequencies of tE and tJ , we esti-
mate bilingual term correspondences according to
the statistical measure ?2.
?2(tE, tJ ) =
(ad ? bc)2
(a + b)(a + c)(b + d)(c + d)
(2)
2We set at most five noun words.
233
We extract term tJ as a pair of tE which satisfies
maximum value of ?2, i.e. maxtJ?TJ ?2(tE ,tJ ),
where TJ = {tJ | ?2(tE ,tJ )}. For the extracted En-
glish and Japanese term pairs, we conducted semi-
automatic acquisition, i.e. we manually selected
bilingual term pairs, since our source data is not
a clean parallel corpus, but an artificially gener-
ated noisy pseudo-parallel corpus, it is difficult to
compile bilingual terms full-automatically(Dagan,
1997). Finally, we align a Japanese term with its
associated English term using the selected bilin-
gual term correspondences, and again calculate
cosine similarities between Japanese and English
stories.
3.2 Extracting Monolingual Story Pairs
We noted above that our source data is not a clean
parallel corpus. Thus the difference of dates be-
tween bilingual stories is one of the key factors to
improve the performance of extracting story pairs,
i.e. stories closer together in the timeline are more
likely to discuss related subjects. We therefore ap-
plied a method for extracting bilingual story pairs
from stories closer in the timelines. However, this
often hampers our basic motivation for using bilin-
gual corpora: bilingual corpora helps to collect
more information about the target topic. We there-
fore extracted monolingual(Japanese) story pairs
and added them to the training stories. Extract-
ing Japanese monolingual story pairs is quite sim-
ple: Let Jj (1 ? j ? m?) be the extracted Japanese
story in the procedure, extracting bilingual story
pairs. We calculate cosine similarities between Jj
and Jk(1 ? k ? n). If the value of similarity be-
tween them is larger than a certain threshold, we
add Jk to the training stories.
3.3 Clustering Negative Stories
Our method for classifying negative stories into
some clusters is based on Basu et. al.?s
method(Basu, 2002) which uses k-means with the
EM algorithm. K-means is a clustering algo-
rithm based on iterative relocation that partitions
a dataset into the number of k clusters, locally
minimizing the average squared distance between
the data points and the cluster centers(centroids).
Suppose we classify X = { x
1
, ? ? ?, xN}, xi ?
Rd into k clusters: one is the cluster which con-
sists of positive stories, and other k-1 clusters
consist of negative stories. Here, which clusters
does each negative story belong to? The EM is
a method of finding the maximum-likelihood es-
timate(MLE) of the parameters of an underlying
distribution from a set of observed data that has
missing value. K-means is essentially an EM on
a mixture of k Gaussians under certain assump-
tions. In the standard k-means without any initial
supervision, the k-means are chosen randomly in
the initial M-step and the stories are assigned to
the nearest means in the subsequent E-step. For
positive training stories, the initial labels are kept
unchanged throughout the algorithm, whereas the
conditional distribution for the negative stories are
re-estimated at every E-step. We select the num-
ber of k initial stories: one is the cluster center of
positive stories, and other k-1 stories are negative
stories which have the top k-1 smallest value be-
tween the negative story and the cluster center of
positive stories. In Basu et. al?s method, the num-
ber of k is given by a user. However, for negative
training stories, the number of clusters is not given
beforehand. We thus developed an algorithm for
estimating k. It goes into action after each run of
k means3, making decisions about which sets of
clusters should be chosen in order to better fit the
data. The splitting decision is done by comput-
ing the Bayesian Information Criterion which is
shown in Eq.(3).
BIC(k = l) = ?lll(X)?
pl
2
? log N (3)
where l?ll(X) is the log-likelihood of X according
to the number of k is l, N is the total number of
training stories, and pl is the number of parame-
ters in k = l. We set pl to the sum of k class prob-
abilities,
?k
m=1 l?l(Xm) , the number of n ? k cen-
troid coordinates, and the MLE for the variance,
??2. Here, n is the number of dimensions. ??2, un-
der the identical spherical Gaussian assumption,
is:
??2 =
1
N ? k
?
i
(xi ? ?i)
2 (4)
where ?i denotes i-th partition center. The proba-
bilities are:
?P (xi) =
Ri
N
?
1
?
2???n
exp(?
1
2??2
|| xi ? ?i ||
2
) (5)
Ri is the number of stories that have ?i as their
closest centroid. The log-likelihood of ll(X)
3We set the maximum number of k to 100 in the experi-
ment.
234
cluster of positive training data
cluster of negative training datatest data
center of gravity
minimum distance between test data and the center of gravity
Figure 2: Each cluster and a test story
is log
?
i P (xi). It is taken at the maximum-
likelihood point(story), and thus, focusing just on
the set Xm ? X which belongs to the centroid m
and plugging in the MLE yields:
?ll(Xm) = ?
Rm
2
log(2?)?
Rm ? n
2
log(
??2)?
Rm ? k
2
+Rm log Rm ? Rm log N (1 ? m ? k) (6)
We choose the number of k whose value of BIC
is highest.
3.4 Tracking
Each story is represented as a vector of terms
with tf ? idf weights in an n dimensional space,
where n is the number of terms in the collection.
Whether or not each test story is positive is judged
using the distance (measured by cosine similarity)
between a vector representation of the test story
and each centroid g of the clusters. Fig.2 illus-
trates each cluster and a test story in the tracking
procedure. Fig.2 shows that negative training sto-
ries are classified into three groups. The centroid
g for each cluster is calculated as follows:
g = (g
1
, ? ? ? , gn) = (
1
p
p
?
i=1
xi1, ? ? ? ,
1
p
p
?
i=1
xin)(7)
where xij (1? j ? n) is the tf ?idf weighted value
of term j in the story xi. The test story is judged
by using these centroids. If the value of cosine
similarity between the test story and the centroid
with positive stories is smallest among others, the
test story is declared to be positive. In Fig.2, the
test story is regarded as negative, since the value
between them is smallest. This procedure, is re-
peated until the last test story is judged.
4 Experiments
4.1 Creating Japanese Corpus
We chose the TDT3 English corpora as our gold
standard corpora. TDT3 consists of 34,600 sto-
ries with 60 manually identified topics. We then
created Japanese corpora (Mainichi and Yomiuri
newspapers) to evaluate the method. We annotated
the total number of 66,420 stories from Oct.1, to
Dec.31, 1998, against the 60 topics. Each story
was labelled according to whether the story dis-
cussed the topic or not. Not all the topics were
present in the Japanese corpora. We therefore col-
lected 1 topic from the TDT1 and 2 topics from the
TDT2, each of which occurred in Japan, and added
them in the experiment. TDT1 is collected from
the same period of dates as the TDT3, and the first
story of ?Kobe Japan Quake? topic starts from Jan.
16th. We annotated 174,384 stories of Japanese
corpora from the same period for the topic. Ta-
ble 2 shows 24 topics which are included in the
Japanese corpora. ?TDT? refers to the evaluation
data, TDT1, 2, or 3. ?ID? denotes topic number de-
fined by the TDT. ?OnT.?(On-Topic) refers to the
number of stories discussing the topic. Bold font
stands for the topic which happened in Japan. The
evaluation of annotation is made by three humans.
The classification is determined to be correct if the
majority of three human judges agree.
4.2 Experiments Set Up
The English data we used for extracting terms
is Reuters?96 corpus(806,791 stories) including
TDT1 and TDT3 corpora. The Japanese data
was 1,874,947 stories from 14 years(from 1991
to 2004) Mainichi newspapers(1,499,936 stories),
and 3 years(1994, 1995, and 1998) Yomiuri
newspapers(375,011 stories). All Japanese sto-
ries were tagged by the morphological analysis
Chasen(Matsumoto, 1997). English stories were
tagged by a part-of-speech tagger(Schmid, 1995),
and stop word removal. We applied n-gram model
with Church-Gale smoothing to noun words, and
selected terms whose probabilities are higher than
a certain threshold4. As a result, we obtained
338,554 Japanese and 130,397 English terms. We
used the EDR bilingual dictionary, and translated
Japanese terms into English. Some of the words
had no translation. For these, we estimated term
correspondences. Each story is represented as a
vector of terms with tf ?idf weights. We calcu-
lated story similarities and extracted story pairs
between positive and its associated stories5. In
4The threshold value for both English and Japanese was
0.800. It was empirically determined.
5The threshold value for bilingual story pair was 0.65, and
that for monolingual was 0.48. The difference of dates be-
tween bilingual stories was ?4.
235
Table 2: Topic Name
TDT ID Topic name OnT. TDT ID Topic name OnT.
1 15 Kobe Japan quake 9,912
2 31015 Japan Apology to Korea 28 2 31023 Kyoto Energy Protocol 40
3 30001 Cambodian government coalition 48 3 30003 Pinochet trial 165
3 30006 NBA labor disputes 44 3 30014 Nigerian gas line fire 6
3 30017 North Korean food shortages 23 3 30018 Tony Blair visits China in Oct. 7
3 30022 Chinese dissidents sentenced 21 3 30030 Taipei Mayoral elections 353
3 30031 Shuttle Endeavour mission for space station 17 3 30033 Euro Introduced 152
3 30034 Indonesia-East Timor conflict 34 3 30038 Olympic bribery scandal 35
3 30041 Jiang?s Historic Visit to Japan 111 3 30042 PanAm lockerbie bombing trial 13
3 30047 Space station module Zarya launched 30 3 30048 IMF bailout of Brazil 28
3 30049 North Korean nuclear facility? 111 3 30050 U.S. Mid-term elections 123
3 30053 Clinton?s Gaza trip 74 3 30055 D?Alema?s new Italian government 37
3 30057 India train derailment 12
the tracking, we used the extracted terms together
with all verbs, adjectives, and numbers, and repre-
sented each story as a vector of these with tf ?idf
weights.
We set the evaluation measures used in the TDT
benchmark evaluations. ?Miss? denotes Miss rate,
which is the ratio of the stories that were judged
as YES but were not evaluated as such for the run
in question. ?F/A? shows false alarm rate, which is
the ratio of the stories judged as NO but were eval-
uated as YES. The DET curve plots misses and
false alarms, and better performance is indicated
by curves more to the lower left of the graph. The
detection cost function(CDet) is defined by Eq.(8).
CDet = (CMiss ? PMiss ? PTarget +
CFa ? PFa ? (1? PTarget))
PMiss = #Misses/#Targets
PFa = #FalseAlarms/#NonTargets (8)
CMiss, CFa, and PTarget are the costs of a missed
detection, false alarm, and priori probability of
finding a target, respectively. CMiss, CFa, and
PTarget are usually set to 10, 1, and 0.02, respec-
tively. The normalized cost function is defined by
Eq.(9), and lower cost scores indicate better per-
formance.
(CDet)Norm = CDet/MIN(CMiss ? PTarget, CFa
?(1? PTarget)) (9)
4.3 Basic Results
Table 3 summaries the tracking results. MIN
denotes MIN(CDet)Norm which is the value of
(CDet)Norm at the best possible threshold. Nt
is the number of initial positive training stories.
We recall that we used subset of the topics de-
fined by the TDT. We thus implemented Allan?s
method(Allan et. al, 1998) which is similar to
our method, and compared the results. It is based
   1  
   2  
   5  
  10  
  20  
  40  
  60  
  80  
  90  
  .01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90  
Mi
ss
 P
rob
ab
ilit
y (i
n %
)
False Alarm Probability (in %)
random performance
With story pairs
Baseline
Figure 3: Tracking result(23 topics)
on a tracking query which is created from the top
10 most commonly occurring features in the Nt
stories, with weight equal to the number of times
the term occurred in those stories multiplied by its
incremental idf value. They used a shallow tag-
ger and selected all nouns, verbs, adjectives, and
numbers. We added the extracted terms to these
part-of-speech words to make their results compa-
rable with the results by our method. ?Baseline?
in Table 3 shows the best result with their method
among varying threshold values of similarity be-
tween queries and test stories. We can see that the
performance of our method was competitive to the
baseline at every Nt value.
Fig.3 shows DET curves by both our method
and Allan?s method(baseline) for 23 topics from
the TDT2 and 3. Fig.4 illustrates the results for 3
topics from TDT2 and 3 which occurred in Japan.
To make some comparison possible, only the Nt =
4 is given for each. Both Figs. show that we have
an advantage using bilingual comparable corpora.
4.4 The Effect of Story Pairs
The contribution of the extracted story pairs, es-
pecially the use of two types of story pairs, bilin-
gual and monolingual, is best explained by look-
ing at the two results: (i) the tracking results with
two types of story pairs, with only English and
236
Table 3: Basic results
TDT1 (Kobe Japan Quake)
Baseline Bilingual corpora & clustering
Nt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN
1 27% .15% 73% 67% .70 .055 1 10% .42% 90% 74% .81 .023
2 20% .12% 80% 73% .76 .042 2 6% .27% 93% 76% .83 .013
4 9% .09% 91% 80% .85 .039 4 5% .18% 96% 81% .88 .012
TDT2 & TDT3(23 topics)
Baseline Bilingual corpora & clustering
Nt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN
1 41% .17% 59% 60% .60 .089 1 29% .25% 71% 54% .61 .059
2 40% .16% 60% 62% .61 .072 2 27% .25% 73% 55% .63 .054
4 29% .12% 71% 72% .71 .057 4 20% .13% 80% 73% .76 .041
   1  
   2  
   5  
  10  
  20  
  40  
  60  
  80  
  90  
  .01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90  
Mi
ss
 P
rob
ab
ilit
y (i
n %
)
False Alarm Probability (in %)
random performance
With story pairs(Japan)
Baseline(Japan)
Figure 4: 3 topics concerning to Japan
   1  
   2  
   5  
  10  
  20  
  40  
  60  
  80  
  90  
  .01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90  
Mi
ss
 P
rob
ab
ilit
y (i
n %
)
False Alarm Probability (in %)
random performance
two types of story pairs
With only J-E story pairs
Without story pairs
Figure 5: With and without story pairs
Japanese stories in question, and without story
pairs, and (ii) the results of story pairs by vary-
ing values of Nt. Fig.5 illustrates DET curves for
23 topics, Nt=4.
As can be clearly seen from Fig.5, the re-
sult with story pairs improves the overall perfor-
mance, especially the result with two types of
story pairs was better than that with only English
Table 4: Performance of story pairs(24 topics)
Two types of story pairs J-E story pairs
Nt Rec. Prec. F Rec. Prec. F
1 30% 82% .439 28% 80% .415
2 36% 85% .506 33% 82% .471
4 45% 88% .595 42% 79% .548
and Japanese stories in question. Table 4 shows
the performance of story pairs which consist of
positive and its associated story. Each result de-
notes micro-averaged scores. ?Rec.? is the ratio
of correct story pair assignments by the system di-
vided by the total number of correct assignments.
?Prec.? is the ratio of correct story pair assign-
ments by the system divided by the total number
of system?s assignments. Table 4 shows that the
system with two types of story pairs correctly ex-
tracted stories related to the target topic even for a
small number of positive training stories, since the
ratio of Prec. in Nt = 1 is 0.82. However, each re-
call value in Table 4 is low. One solution is to use
an incremental approach, i.e. by repeating story
pairs extraction, new story pairs that are not ex-
tracted previously may be extracted. This is a rich
space for further exploration.
The effect of story pairs for the tracking task
also depends on the performance of bilingual term
correspondences. We obtained 1,823 English and
Japanese term pairs in all when a period of days
was ?4. Fig.6 illustrates the result using differ-
ent period of days(?1 to ?10). For example, ??1?
shows that the difference of dates between English
and Japanese story pairs is less than ?1. Y-axis
shows the precision which is the ratio of correct
term pairs by the system divided by the total num-
ber of system?s assignments. Fig.6 shows that the
difference of dates between bilingual story pairs,
affects the overall performance.
4.5 The Effect of k-means with EM
The contribution of k-means with EM for classi-
fying negative stories is explained by looking at
the result without classifying negative stories. We
calculated the centroid using all negative training
stories, and a test story is judged to be negative or
237
???
??
??
??
????? ?? ?? ?? ?? ?? ?? ?? ?? ???
Prec. (%)
1.42
18.3
39.8
53.0
37.2
34.0
33.7
32.0
20.8
19.6
Figure 6: Prec. with different period of days
   1  
   2  
   5  
  10  
  20  
  40  
  60  
  80  
  90  
  .01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90  
Mi
ss
 P
rob
ab
ilit
y (i
n %
)
False Alarm Probability (in %)
Random Performance
BIC (with classifying)
k=0
k=100
Figure 7: BIC v.s. fixed k for k-means with EM
positive by calculating cosine similarities between
the test story and each centroid of negative and
positive stories. Further, to examine the effect of
using the BIC, we compared with choosing a pre-
defined k, i.e. k=10, 50, and 100. Fig.7 illustrates
part of the result for k=100. We can see that the
method without classifying negative stories(k=0)
does not perform as well and results in a high miss
rate. This result is not surprising, because the size
of negative training stories is large compared with
that of positive ones, and therefore, the test story is
erroneously judged as NO. Furthermore, the result
indicates that we need to run BIC, as the result was
better than the results with choosing any number
of pre-defined k, i.e. k=10, 50, and 100. We also
found that there was no correlation between the
number of negative training stories for each of the
24 topics and the number of clusters k obtained by
the BIC. The minimum number of clusters k was
44, and the maximum was 100.
5 Conclusion
In this paper, we addressed the issue of the differ-
ence in sizes between positive and negative train-
ing stories for the tracking task, and investigated
the use of bilingual comparable corpora and semi-
supervised clustering. The empirical results were
encouraging. Future work includes (i) extend-
ing the method to an incremental approach for
extracting story pairs, (ii) comparing our cluster-
ing method with the other existing methods such
as X-means(Pelleg, 2000), and (iii) applying the
method to the TDT4 for quantitative evaluation.
Acknowledgments
This work was supported by the Grant-in-aid for
the JSPS, Support Center for Advanced Telecom-
munications Technology Research, and Interna-
tional Communications Foundation.
References
J.Allan and R.Papka and V.Lavrenko, On-line new event
detection and tracking, Proc. of the DARPA Workshop,
1998.
J.Allan and V.Lavrenko and R.Nallapti, UMass at TDT
2002, Proc. of TDT Workshop, 2002.
S.Basu and A.Banerjee and R.Mooney, Semi-supervised
clustering by seeding, Proc. of ICML?02, 2002.
J.Carbonell et. al, CMU report on TDT-2: segmentation,
detection and tracking, Proc. of the DARPA Workshop,
1999.
S.F.Chen and J.Goodman, An empirical study of smoothing
techniques for language modeling, Proc. of the ACL?96,
pp. 310-318, 1996.
N.Collier and H.Hirakawa and A.Kumano, Machine trans-
lation vs. dictionary term translation - a comparison for
English-Japanese news article alignment, Proc. of COL-
ING?02, pp. 263-267, 2002.
I.Dagan and K.Church, Termight: Coordinating humans and
machines in bilingual terminology acquisition, Journal of
MT, Vol. 20, No. 1, pp. 89-107, 1997.
M.Franz and J.S.McCarley, Unsupervised and supervised
clustering for topic tracking, Proc. of SIGIR?01, pp. 310-
317, 2001.
L.S.Larkey et. al, Language-specific model in multilingual
topic tracking, Proc. of SIGIR?04, pp. 402-409, 2004.
Y.Matsumoto et. al, Japanese morphological analysis system
chasen manual, NAIST Technical Report, 1997.
D.W.Oard, Topic tracking with the PRISE information re-
trieval system, Proc. of the DARPA Workshop, pp. 94-
101, 1999.
D.Pelleg and A.Moore, X-means: Extending K-means with
efficient estimation of the number of clusters, Proc. of
ICML?00, pp. 727-734, 2000.
H.Schmid, Improvements in part-of-speech tagging with an
application to german, Proc. of the EACL SIGDAT Work-
shop, 1995.
K.Wagstaff et. al, Constrained K-means clustering with
background knowledge, Proc. of ICML?01, pp. 577-584,
2001.
Y.Yang et. al, Improving text categorization methods for
event tracking, Proc. of SIGIR?00, pp. 65-72, 2000.
238
Extract ing Key Paragraph based on Topic and Event Detect ion  
- -  Towards Mul t i -Document  Summarizat ion 
Fumiyo Fukumoto and Yoshimi Suzuki t
Department. ofComputer Science and Media Enginccring, 
Yamanashi University 
4-3-11 Takeda, Kofu 400-8511 Japan 
{j~tkumotoCo)skye. esb:. ysuzuki @aIps l. esi~ }. yamano.~hi, ac.jp 
Abstract  
This paper proposes a method for extracting key 
paragraph for multi-document summarization based 
on distinction between a topic and a~ event. A topic 
emd an event are identified using a simple criterion 
called domain dependency of words. The method 
was tested on the TDT1 corpus which has been de- 
veloped by the TDT Pilot Study and the result can 
be regarded as promising the idea of domain depen- 
dency of words effectively employed. 
1 Introduction 
As the volume of olfline documents has drastically 
increased, summarization techniques have become 
very importaalt in IR and NLP studies. Most of the 
summarization work has focused on a single docu- 
ment. Tiffs paper focuses on multi-document sum- 
marization: broadcast news documents about the 
same topic. One of the major problems in the multi- 
document summarization task is how to identify dif- 
ferences and similza'ities across documents. This can 
be interpreted as a question of how to make a clear 
distinction between an e~ent mM a topic in docu= 
meats. Here, an event is the subject of a document 
itself, i.e. a writer wants to express, in other words, 
notions of who, what, where, when. why and how in 
a document. On the other hand, a topic in this paper 
is some unique thing that happens at some specific 
time and place, and the unavoidable consequences. 
I t 'becomes background among documents. For ex- 
ample, in the documents of :Kobe Japan quake', the 
event includes early reports of damage, location and 
nature of quake, rescue efforts, consequences of the 
quake, a~ld on-site reports, while the topic is Kobe 
Japaa~ quake. The well-known past experience from 
IR ~ that notions of who, what, where, when, why 
and how may not make a great contribution to the 
topic detection and tracking task (Allan and Papka, 
1998) causes this fact, i.e. a topic and an event are 
different from each other 1 . 
1 Some topic words can also be an event. Fbr instance: 
in the document shown in Figure 1: 'Japan: and =quake' are 
topic words and also event words in the document. However, 
we regarded these words as a topic, i.e. not be an event. 
In this paper: we propose a. method fi)r extract- 
ing key paragraph for multi-document smnmariza- 
tion based on distinction between a topic and an 
event. We use a silnple criterion called domain de- 
pendency of words as a solution and present how the 
i.dea of domain dependency of words can be utilized 
effectively to identify a topic and an event: and thus 
allow multi-document summarization. 
The basic idea of our approach is that whether a 
word appeared in a document is a topic (an event) 
or not, depends on the domain to which the docu- 
ment belongs. Let us take a look at the following 
document from the TDT1 corpus. 
(1-2) Two Americans known dead in Japan quake 
1. The number of \[Americans\] known to have been 
killed in Tuesday's earthquake in Japan has risen to 
two, the \[State\] [Department\] said Thursday. 
2. The first was named Wednesday as Voni Lynn 
~Vong~ a teacher from California. \[State I \[De- 
partment\] spokswoman Christine Shelly declined 
to name the second: saying formalities of notifying 
the family had not been completed. 
3. With the death toll still mounting, at least 4:000 
people were killed in the earthquake which devas- 
tated the Japanese city of Kobe. 
4. \[U.S.\] diplomats were trying to locate the sevcrM 
thousand-strong \[U.S.\] community in the area: and 
some \[Americans\] who had been made homeless 
were found shelter in the \[U.S.\] consulate there: 
which was only lightly damaged in the quake. 
5. Shelly said an emergency \[State\] \[Department\] 
telephone number in Washington to provide infor- 
mation about private \[American\] citizens in Japan 
had received over 6,000 calls, more than half ot'Th-'e'm 
seeking direct assistance. 
6. The Pentagon has agreed to send 57:000 blankets 
to Japan and \[U.S.\] ambassador to Tokyo ~Valter 
Mondale has donated a $25,000 discretionary fund 
for emergencies to the Japanese Red Cross, Shelly 
said. 
7. Japan has also agreed to a visit by a team of \[U.S.\] 
experts headed by Richard Witt, national director 
of the Federal Emergency Management Agency. 
Figure 1: The document titled 'Two Americans 
l~lown dead in Japan quake' 
Figure I is the document whose topic is 'Kobe Japan 
quake', and the subject of the document (event 
31 
words) is 'Two Americans known dead in Japan 
quake'. Underlined words denote a topic, and the 
words marked with '\[ \]' are events. '1,,,7' of Figure 
1 is paragraph id. Like Lulm's technique of keyword 
extraction, our method assumes that an event asso- 
ciated with a document appears throughout parm 
graphs (Luhn, 1958), but a topic does not.  This is 
because an event is the subject of a document i self. 
while a topic is an event, along with all directly re- 
lated events. In Figure 1, event words 'Americans' 
and 'U.S.', for instance, appears across paragraphs, 
while a topic word, for example, 'Kobe' appears only 
the third paragraph. Let us consider further a broad 
coverage domain which consists of a small number of 
sanaple news documents about the same topic, 'Kobe 
Japan quake'. Figure 2 and 3 are documents with 
'Kobe Japan quake'. 
( l- l)  Quake collapses buildings in central Japan 
1. At lea.~t two people died and dozens were injured 
when a powerful earthquake rolled through central 
Japan Tue..~lay morning, collapsing buildings and 
setting off fires in the cities of Kobe and Osaka. 
2. The Japan Meteorological Agency said the 
earthquake, which measured 7.2 on the open-ended 
Richter scale: rmnbled across Honshu Island from 
the Pacific Ocean to the Japan Sea. 
Figure 2: The document itled 'Quake collapses 
buildings in central Japan' 
(1-3) Kobe quake leaves questions about medical system 
1. The earthquake that devastated Kobe in January 
raised serious questions about the efficiency of 
Japan's emergency medical system, a government 
report released on Tuesday said. 
2. 'The earthquake xposed many i~ues in terms 
of quantity, quality, promptness and efficiency of 
Japan's medical care in time of disaster,' the report 
on-'ff'h-~alth and welfare said. 
Figure 3: The document itled 'Kobe quake leaves 
questions about medical system' 
Underlined words in Figure 2 and 3 show the topic 
of these documents. In these two documents, :Kobe' 
which is a topic appears in eveD" document, while 
'Americans' and 'U.S.' which are events of the docu- 
ment shown in Figure 1, does not appear. Our  tech- 
nique for making the distinction between a topic and 
an event explicitly exploits this feature of the domain 
dependency of words: how strongly a word features 
a given set of data. 
The rest of the paper is organized as follows. 
The next section provides domain dependency of 
words which is used to identify a topic and an event 
for broadcast news documents. We then present a 
method for extracting topic and event words: and de- 
scribe a paragraph-based summarization algorithm 
using the result of topic and event extraction. Fi- 
nally~ we report some experiments using the TDT1 
corpus which has been developed by the TDT (Topic 
Detection and Tracking) Pilot Study (Allan and 
Carbonell, 1998) with a discussion of evaluation. 
2 Domain  Dependency  o f  Words  
The domain dependency of words that how strongly 
a word features a given set of data (documents) con- 
tributes to event extraction, as we previously re- 
ported (Fukumoto et al: 1997). In the study, we 
hypothesi~d that the articles from the Wall Street 
Journal corpus can be structured by three levels, i.e. 
Domain, Article and Paragraph. It'a word is nil event 
in a given article, it satisfies the two conditions: (1) 
The dispersion value of the word in the Paragraph 
level is smaller than that of the Art.iele, since the 
.word appears throughout paragr~q~hs in the Para- 
graph level rather than articles in the Article level. 
(2) The dispersion value of the word in the Arti- 
cle is smaller than that of the Domain, as the word 
appears across articles rather than domains. 
However, ~here are two problems to adapt it to 
multl-document summarization task. The first is 
that the method extracts only events in the docu- 
ment. Because the goal of the study is to summarize 
a single document, and thus there is no answer to 
the question of how to identi~' differences and sim- 
ilarities across documents. The second is that the 
performance of the method greatly depends on the 
structure of a given data itself. Like the Wall Street 
Journal corpus, (i) if a given data caal be structured 
by three levels, Paragraph, Article and Domain, each 
of which consists of several paragraphs, articles and 
domains, respectively, aaad (ii) if Domain consists of 
different subject domains, such as 'aerospace', 'en- 
vironment' and 'stock market', the method can be 
done with satisfactoD' accuracy. However, there is 
no guarantee to make such an appropriate structure 
from a given set of documents in the multi-document 
summarization task. 
The purpose of this paper is to define domain 
dependency of words for a number of sample doc- 
uments about the same topic, and thus for multi- 
document summarization task. Figure 4 illustrates 
the structure of broadcast news documents which 
have been developed by the TDT (Topic Detection 
and Tracking) Pilot Study (Allan and Carbonell, 
1998). It consists of two levels, Paragraph and Doc- 
ument. In Document level, there is a small number 
of sample news documents about the same topic. 
These documents are arranged in chronological or- 
der such as, ' ( l - l )  Quake collapses buildings in cen- 
tral ,Japan (Figure 2)', '(1-2) Two Americans known 
dead in Japan quake (Figure 1)' and '(1-3) gobe 
quake leaves questions about medical system (Fig- 
ure 3)'. A particular document consists of several 
II 
I 
II 
32 
I 
I 
I 
i 
I 
I 
i 
I 
paragraphs. We call it Paragraph level. Let words 
within a document be an event, a topic, or among 
others (We call it n .qeneraZ word). 
(H) 
0 0 
i:r 
~umedlevel 
(t.2) 
:0 x 
x 
:h0  
0 
; 5 A 
i=2 
0.3} 
o 
X 
0 
0 X ~ o o * . .  
0 ' - 
.. J  
i=m 
oo 
i Paragraphleve~ ! ' 0 / '  
, X 
i::r 
X : 
al 
k.2 
i o . ? l l  
i x  
i 
?~ lopic word 
& event word 
x: general word 
(1.1) 'Qu~e corpses b,.l~s in cen~ Japan' 
(1.2)'Two Americans known dead b Japan qu~e' 
(1-3) ~obe quake leaves quests about me&al system' 
Figure 4: The stnmture of broadcast news documents 
(event extraction) 
Given the structure shown in Figure 4, how can we 
identi~" every word in document (1-2) with an event, 
a topic or a general word? Our method assumes that 
aal event associated with a document appears across 
paragraphs, but a topic word does not. Then, we use 
domain dependency of words to extract event and 
topic words in document (1-2). Domain dependency 
of words is a measure showing how greatly each word 
features a given set of data. 
In Figure 4.. let 'C)', 'A '  and 'x '  denote a topicl 
an event and a general word in document (1-2), re- 
spectively. We recall the example shown in Figure 1. 
'A',  for instance, 'U.S.' appears across paragraphs. 
However, in the Document level, :A' frequently ap- 
pears in document, (1-2) itself. On the basis of this 
example, we hypothesize that if word i is an event, 
it"satisfies the following condition: 
\[1\] Word i greatly depends on a particular 
document in the Document level rather 
than a particular paragraph in the Para- 
graph. 
Next, we turn to identi~" the remains (words) wit.h 
a topic, or a general word. In Figure 5; a topic of 
documents (1-1) ~ (1-3), for instance, :Kobe' aP- 
pears in a particular paragraph in each level of Para- 
graphl, Paragraph2 and Paragraph3. Here, (1-1), (1- 
2) and (1-3) corresponds to Paragraph1, Paragraph2 
and Paragraph3, respectively. On the other hand, 
in Document level, a topic frequently appears acros.~ 
documents. Then: we hypothesize that if word i is a 
33 
(H) 
. x  x I 
.~e.nt  !e_ve\[. c. 
o ' : . i  
:~1 
Paragraph 1: C. 
level ~! C' xi x 
j=l 
? o? - . ?  
i 
j=2 ~ . . . . .  j=n 
ic. 
ParagraphZi O " 
level !! x i 
(1-2) p-3) 
? 
x x \[ 0 t i z 
L i 
:i=2 i=3 
m ~  
- - i  
X i , . , . o  
i=rn 
i 0 i ! ,  i 
O:topic word Paragraph 3 0 ; j 
x: general word i leve iJ ! o x !~ j !C': ....... 
l~igure 5: The structure of broadcast news documents 
(topic extraction) 
topic, it satisfies the following condition: 
\[2\] Word i greatly depends on a particu- 
lar paragraph in each Paragraph level 
rather than a particular document in 
Document. 
3 Topic and Event Extraction 
We hypothesized that  the domain dependency o f  
words is a key clue to make a distinction between 
a topic and an event. This can be broken down into 
two observations: (i) whether a word appears across 
paragraphs (documents), (it) whether or not a word 
appears frequently. We represented the former by 
using dispersion value, and the latter by deviation 
value. Topic and event words are extracted by using 
these values. 
The first step to extract opic and event words is 
to assign weight to the individual word in a docu- 
ment. We applied TF*IDF to each level of the Doc- 
ument and Paragraph, i.e. Paragraphl, Paragraph2 
and Paragraph3. 
N 
Wdit = TFdit * log Ndt (1) 
Wdit in formula (1) is TF*IDF of term t in the i-th 
document. In a similar way, Wpit denotes TF*IDF 
of the term t in the i-th paragraph. TFdit in (1) 
denotes term frequency of t in the i-th document. N 
is the number of documents and Ndt is the number 
of do(:uments where t occurs. The second step is to 
calculate domain dependency of words. We defined 
it by using formula (2) and (3). 
DispOt = /I/E'~=l(I4;dit - mean')2 (2) 
? Tn 
De vdi, = (Wdit - meant) ,10+50 (3) 
DispDt 
Formula (2) is dispersion value of term t in the level 
of Document which consists of m documents, and 
denotes how frequently t appears across documents. 
In a similar way, DispPt denotes dispersion of term 
t in the level of Paragraph. Formula (3) is the devia- 
tion value of t in the i-th document and denotes how 
frequently it appears in a particular document, he 
i-th document. Devpit is deviation of term t in the 
i-th paragraph. In (2) and (3), meant is the mean 
of the total TF*IDF values of term t in the level of 
Document. 
The last step is to extract a topic and an ever~t 
using fonmfla (2) and (3). We recall that if t is an 
event, it satisfies \[1\] described in section 2. This is 
shown by using formula (4) mad (5). 
DispPt < DispDt (4) 
for all Pi E di Devpjt < Devdit (5) 
Formula (4) shows that t frequently appears across 
paragraphs rather than documents. In formula (5), 
di is the i-th document and consists of the number 
of n paragraphs (see Figure 4). Pi is an element of 
di. (5) shows that t frequently appears in the i-th 
document di rather than paragraphs pj ( 1 < j < 
n). On the other hand: if t satisfies formula (6) and 
(7), then propose t as a topic. 
DispPt > DispDt (6) 
for all dl E D, 
Pit exists such that Devpjt >_ Devdlt (7) 
In formula (7), D consists of the number of rn doc- 
aments (see Figure 5). (7) denotes that t frequently 
appears in the particular paragraph pj rather than 
the document di which includes pj. 
4 Key Paragraph Extraction 
The summarization task in this paper is paragraph- 
based extraction (Stein et al, 1999). Basically, para- 
graphs which include not only event words but also 
topic words are considered to be significant para- 
graphs. The basic algorithm works as follows: 
1. For each document: extract topic and event 
words. 
2. Determine the paragraph weights for all para- 
graphs in the documents: 
(a) Compute the sum of topic weights over the 
total number of topic words for each para- 
graph. 
(b) Compute the sum of event weights over the 
total number of event words for each para- 
graph. 
A topic and an event weights are calculated 
by using Devdlt in formula (3). Here, t is a 
topic or an evcnt and i is the i-th document 
in the documents. 
(c) Compute the sum of (a) and (b) for each 
paragraph. 
3. Sort the paragraphs t~ccording to their weights 
and extract he N highest weighted paragrai~hs 
in documents in order to yield summarization 
of the documents. 
4. When their weights are the same, Compute the 
sum of all the topic and event word weights. 
Select a paragraph whose weight is higher than 
the others. 
5 Experiments 
Evaluation of extracting key paragraph based on 
multi-document is difficult. First, we have not found 
an existing collection of summaries of multiple doc- 
uments. Second, the maamal effort needed to judge 
system output is far more extensive than for single 
document summarization. Consequently, we focused 
on the TDT1 corpus. This is because (i) events have 
been defined to support the TDT study effort, (ii) 
it was completely annotated with respect o these 
events (Allan and Carbonell, 1997). Therefore, we 
do not need the manual effort to collect documents 
which discuss about the target event. 
We report the results of three experiments. The 
first experiment, Event Extraction, is concerned with 
event extraction technique, ha the second experi- 
ment, Tracking Task, we applied the extracted top- 
ics to tracking task (Allan and Carbonell, 1998). 
The third experiment: Key Paragraph Extraction is 
conducted to evaluate how the extracted topic and 
event words can be used effectively to extract key 
paragraph. 
5.1 Data 
The TDT1 corpus comprises a set of documents 
(.15,863) that includes both newswire (Reuters) 
7..965 and a manual transcription of the broadcast 
news speech (CNN) 7,898 documents. A set of 25 
target events were defined 2 
All documents were tagged by the tagger (Brill, 
1992). %Ve used nouns in the documents. 
h t t p://morph.ldc.upenn.edu/TDT 
I 
I 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
i 
I 
i 
! 
I 
i 
34 
I 
I 
I 
I 
I 
I 
I 
I 
I 
i 
I 
i 
I 
/ 
I 
I 
I 
I 
5.2  Event  Ext ract ion  
We collected 300 documents from the TDT1 corpus, 
each of which is mmolated with respect o one of 25 
events.' The result is shown in Table 1. 
In Table 1, 'Event type' illustrates the target events 
defined by the TDT Pilot Study. 'Doe' denotes the 
number of documents. 'Rec' (Recall) is the imm- 
ber of correct events divided by the total mnnber 
of events which are selected by a human, and 'Prec' 
(Precision) stands for the number of correctevents 
divided by the number of events which are selected 
by our method. The denominator 'Rec' is made by 
a hmnan judge. 'Accuracy' in Table 1 is the total 
average ratio. 
In Table 1, recall and precision values range from 
55.0/47.0 to 83.3/84.2, the average being 71.0/72.2. 
The worst result of recall and precision was when 
event type was 'Serbs violate Bihac' (55.0/59.3). We 
currently hypothesize that this drop of accuracy is 
due to the fhct that some documents are against our 
assumption of an event. Examining the documents 
whose event type is 'Serbs violate Bihac', 3 ( one 
from CNN and two from Reuters).out of 16 docu- 
ments has discussed the same event, i.e. 'Bosnian 
Muslim enclave hit by heavy shelling'. As a result, 
the event appears across these three documents? Fu- 
ture research will shed nmre light on that. 
5.3 T rack ing  Task 
Tracking task in the TDT project is starting from 
a few sample documents and finding all subsequent 
documents that discuss the same event (Allan and 
Carbonell, 1998), (Carbonell et al, 1999). The cor- 
pus is divided into two parts: training set and test 
set. Each of the documents i flagged as to whether 
it discusses the target event, and these flags ('YES', 
:'NO') axe the only information used for training the 
.system to correctly classiC" the target event. We ap- 
plied the extracted topic to the tracking task under 
? these conditions. The basic algorithm used in the 
experiment is as follows: 
1L Create a single document Sip and represent it as 
a term vector 
For the results of topic extraction, all the docu- 
ments that belong to the same topic are lmndled 
into a single document Stp and represent i by 
a term vector as follows: 
Stp -~ 
ttpl 
tip2 
ttpn 
s.t. itpj = 
{ f(ttpj) i f t t~j isatoplc 
of Stp 
0 otherwise 
. 
f(w) denotes term frequency of word w. 
Represent other training and test documents as 
te rm vectors  
= 
. S= = 
, 3. 
Let $1: --', S,, be all the other training docu- 
ments (where m is the number of training doc- 
uments which does not belong to the target 
event) and Sx be a test docmnent which should 
be classified as to whether or not it discusses the 
target event. 81, "" ", Sm mid Sz are represented " 
by term vectors as follows: 
I l l  
ti2 
s.t? li.i = { f ( t ,A  if t,~ (1 < i < m) appears ill S; and not, be a topic of Sip 
0 otherwise 
tzl 
i=2 
i=. 
f(t=j) if t.~j appears i,i t;~ 
s.t. txj = 0 otherwise 
Compute the similarity between a training docu- 
ment and a test document 
Given a vector epresentation f documents SI, 
? ? ", Sin, Sty and Sx, a similarity between two 
documents Si (1 < i < m, tp) and the test doc- 
ument S~ would be obtained by using formula 
(8), i.e. the inner product of their normalized 
vectors. 
Si. Sx 
s~m(s .s~)  - I S~ II S=l (s) 
The greater the value of Sim(Si, S=) is, the 
more similar Si and S,  are. If the similarity 
value between the test document Sx and the 
document Sip is largest among all the other 
pairs of documents, i.e. (&,  S=).---,  (S~, S=), 
Sx is judged to be a document hat discusses 
the target event. 
We used the standard TDT evaluation measure 
Table 2 illustrates the result. 
3. 
Table 2: The results of tracking task 
1 
2 
4 
8 
? 16 
Avg 
%Miss 
32.5 
23.7 
23.1 
12.0 
13.7 
21.0 
%F/A F1 %Rec %Prec 
0.16 0.68 67.5 70.0 
0~06 0.80 76.3 87.8 
0.05 0.81 76.9 90.1 
0.08 0,87 88.0 91.4 
0.06 0.89 86.3 93.6 
0.08 0.76 79.0 86.6 
In Table 2, 'Nt' denotes the number of positive train- 
ing documents where A~ takes on values 1, 2, 4, 8 
.3 h t tp : / /www.n is t .gov /speech/ td t98 .htm 
35 
I 
Table 1: The results of event words extraction I 
m 
Event type Doc Avg Rec/Avg Prec ' 
Karrigan/Harding 2 . 64.1/55.5 " ' I 
Kobe Japan quake 16 74.5/75.0 
Lost in Iraq 16 ~5.7/68.8 
NYC Subway bombing 16 68.0/84.2 ' 
OK-City bombing 16 78.8/47.0 ? I 
Pentium chip flaw 4 81.1/72.9 
Quayle lung clot 8 63.6/74.4 
Serbians down F- 16 16 .78"6/75"0 I 
Serbs violate Bihac 16 55.1)/59.3 
Shannon Faulker 4 11.4/82.4 
USAir 427 crash 16 72.6/86.3 
WTC Bombing trial 16 62.6/70.1 I 
71.0/72.2 - - -=  | 
In Table 3. 'Event' denotes event words in the first 
document in chronological order from A~ --- 4, and i 
the title of the document is 'Emergency Work Con- 
tinues After Earthquake in Japan'. Table 3 clearly 
demonstrates that the criterion, domain dependency 
of-''words effectively employed. 
Figure 6 illustrates the DET (Detection Evalua- 
tion Tradeoff) curves for a sample event (event ype. 
is 'Comet into Jupiter) runs at several values of Nt. \ ]~ 
'/l 
! ~"  i~'~-~.~ .1, ! .: ~ i i .: i 
| ! : : " ~ " t ~.'. "~ ~ " E -v*~ : ? .: " : ~ : ~ ~ :~ : : N=4 ....... : 
? - : ."1 : " , . .  : t ' t "  ~"  : H=8 . . . . . . .  : 
I t t : : ** ; I g t?  ~ t "~ * t -* : ? 
| ? * * " ? ~ ? ~ , . to  1~, ? t ? ? * ? 
? t I t l = i~ = . "~ t ' ~ t l t = 
| 2o t - . - . . . . . r . . ? - . . . - - - . .T . . . .~ : : : .  . . . . . .  . :  ; : :~ : . . . .T .~ .  . . . . . . . .  ! . . . . . . . . . .  ! . . . . . . . . . . .  ? . . . . . . .  . I  ; -  .. : ? : ? . ? ~ .  : . . .  : : : : I 
~o ~,..-.,...*...:......*....!....~ . . .. .... *.--'~.'~';'i""~-"': ......... ~ .......... ":...... "i. 
s i , .4 . - .1 . . .~ . . . i . . . . .4 . . . . | . . . . .~  . . . . . .  4 . . . . . .  4 . - . .  :4b  a .~. . i . . '~  . . . . . . . . .  | . . . . . . . . . .  ~ . . . . . . .  4 
: : : : : : : " : : "'1:~ : ::" : : : I 
P .4 . . . . i . . .~ . . . . | . . . . .~ . . . . . i . . . . . i  . . . . . .  4 . . . . . .  . ;  . . . . . .  ~..i......i....~.'...i . . . . . . . . . 4 . . . . . . .  
Event type Avg Rec/Avg Prec 
8 61.7/70.5 
8 60.7/73.3 
76.3/79.1 
65.7/80.0 
75.9/80.0 
65.2._/61.9 
65.2173.9 
83.3/71.4 
78.7/72.9 
62.0/74.0 
78.5/75.0. 
80.4/70.2 
8 75.9/72.2 
Di.spPt DispDt 
Doc 
Aldrich Ames 
Carlos the Jackal 
Carter in Bosnia 
Cessna on White House 
Clinic Murders 
' Comet into Jupiter 
Cuban riot in Panama 
Death of Kim Jong 
-DNA in OJ trial 
Haiti ousts observers 
Hall's copter 
16 
8 
16 
16 
2 
16 
16 
8 
16 
Humble: TX, flooding 16 
Justice-to-be Breyer 
Accuracy 
and 16. 'Miss' means Miss rate, which is the ra- 
tio of the doounents that were judged as YES but 
were not evahmted as YES for the run in question. 
'F/A' shows false alarm rate and 'FI' is a measure 
that balances recall and precision. 'Rec' denotes the 
ratio of the documents judged YES that were also 
evaluated as YES, and 'Prec' is the percent of the 
documents that were evaluated as YES which corre- 
spond to documents actually judged as YES. 
Table 2 shows that more training data helps the 
performance, as the best result was when we used 
:Yt = 16. 
Table 3 illustrates the extracted topic and event 
words in a sample document. The topic is 'Kobe 
Japan quake' and the number of positive training 
documents is 4. 'Devpzt', 'Devd\]t', DispPt' and 
'DispDt' denote values calculated by using formula 
(2) and (3). 
,Table 3: Topic and event words in :Kobe Japan 
quake' 
Topic word 
earthquake 
Japan 
Kobe 
fire 
Devplt 
53,5 
69,8 
56,6 
57.0 
Devdzt 
50.0 
50.0 
50.0 
46.4 
12.3 10.3 
13.3 9.8 
8.6 6.4 
2.3 1.5 
Event word 
emergency 
area  
worker  
rescue  
Devplt 
50.0 
40.6 
50.0 
43.3 
Devdzt 
74.7 
50.0 
66.1 
50.0 
DispP t 
0.9 
0.6 
0.4 
2.3 
DispDt 
1.5 
1.0 
1.0 
3.4 
.ol .(m .o6 o.1 o2. o.5 1 g s lo '2o 4o $o 8o 90 
Fatse Atarm p'rotm~Jity (in %) 
I I  
Eigure 6: DET curve for a sample tracking runs ? 
Overall, the curves also show that more training 
helps tile performance, while there is no significant B 
difference among -'Yt = 2, 4 and 8. 
5.4 Key  Paragraph Extract ion 
ro l l  
We used 4 different sets as a test data. Each set con- ? 
sists of 2, 4.. 8 and 16 documents. For each set, we 
36 
I 
I 
5.2 Event  Ext ract ion  
We collected 300 docmnents from the TDT1 corpus, 
each of which is annotated with respect o one of 25 
events.' The result is shown in Table 1. 
In Table 1.. 'Event type' illustrates the target events 
defined by the TDT Pilot Study. ~Doc' denotes the 
number of documents. 'Rec' (Recall) is the nmn- 
bet of correct events divided by the total number 
of events which are selected by a humaa, and :Pree ~ 
(Precision) stands for the number of correct-events 
divided by the number of events which are selected 
by our method. The denominator 'Rec: is made by 
a human judge. 'Accuracy' in Table 1 is the total 
average ratio. 
In Table 1, recall and precision values range, from 
55.0/47.0 to 83.3/84.2, the average being 71.0/72.2. 
The worst result of recall and precision was when 
event ype was 'Serbs violate Bihac' (55.0/59.3). We 
currently hypothesize that this drop of accuracy is 
due to the fact that some documents are against our 
assumption of an event. Examining the ctocuments 
whose event type is 'Serbs violate Bihac', 3 ( one 
from CNN and two from Reuters) out of 16 docu- 
ments has discussed the same evefit, i.e. 'Bosnian 
Muslim enclave hit by heavy shelling'. As a result, 
the event appears across these three documents. Fu- 
ture research will shed more light on that. 
5.3 T rack ing  Task  
Tracking task in the TDT project is starting from 
a few sample documents and finding all subsequent 
documents that discuss the same event (Allan and 
Carbonell, 1998), (Carbonell et al, 1999). The cor- 
pus is divided into two parts: training set and test 
~et. Each of the documents i flagged as to whether 
it discusses the target event, and these flags ('YES', 
'NO') are the only information used tbr training the 
system to correctly classiC" the target event. We ap- 
plied the extracted topic to the tracking task under 
these conditions. The basic algorithm used in the 
? experiment is as follows: 
1. Create a single document Stp and represent it as 
".a term vector 
For the results of topic extraction, all the docu- 
ments that belong to the sanae topic are bundled 
into a single document S,p and represent i by 
a term vector as follows: 
~tp -~ 
ttpl 
ttp2 
? s . t .  t tp j  = 
ttpn 
{ /(t,pj) ift,pj is atoplc 
of Stp 
0 otherwise 
f (w)  denotes term frequency of word w. 
2. Represent other training and test documents as 
term vectors 
35 
Let $1, ---, S,,, be all the other training docu- 
ments (where m is the number of training doc- 
uments which does not belong to the target 
event) and Sx be a test document which should 
be classified as to whether or not it discusses the 
target event. $1, "- -, Sm and Sx are represented " 
by term vectors as follows: 
~ = 
' "  { 
s . t .  l l j  = 
f(t~j) ift 0 (1 < i <m)  
appears in S~ and 
not be a topic of ,5"tp 
(I otherwise 
S= = 
tzl 
t~2 
? S . t .  t~ j  = 
f(t.r.j) ift~j ~ppears i , S, 
0 otherwise 
3. Compute the similarity between a training docu- 
ment and a test document 
Given a vector epresentation f documents SI, 
? .., S.,, Stp and S=; a similarity between two 
documents Si (1 < i < m,  tp) mad the test doc- 
ument S= would be obtained by using formula 
(8), i.e. the inner product of their normalized 
vectors. 
Si ? S= 
Sim(Si, S~) = I Si II S~ I (S) 
The greater the value of S im(S i ,S , )  is, the 
more similar 5"/ and Sz are. If the similarity 
value between the test document S, and the 
document Stp is largest among all the other 
pairs of documents, i,e. ($1, Sx), " ", (Sin, S=), 
S= is judged to be a document hat discusses 
the target event. 
We used the standard TDT evaluation measure 3
Table 2 illustrates the result? 
Table 2: The results of tracking task 
Nt %Miss %F/A F1 %Rec %Prec 
1 32.5 0.16 0.68 67.5 70:0 
2 23.7 0.06 0.80 76.3 87.8 
4 23.1 0.05 0.81 76.9 90.1 
8 12.0 0.08 0.87 88.0 91.4 
16 13.7 0.06 0.89 86.3 93.6 
"Avg 21.0 0.08 0.76 79.0 86.6 
In Table 2, 'Nt '  denotes the number of positive train- 
ing documents where A~ takes on values 1, 2, 4, 8 
z http://www.nist.gov/speech/tdt98.htrn 
Table 1: The results of event words extract ion 
I 
I 
Event type Doe Avg Rec /Avg Prec Event type Doc Avg Rec /Avg  Prec 
.. Aldrich Ames 8 61.7/70.5 Karr igan/Hard ing 2 64.7/55.5 . I I  
Carlos the Jackal 8 60.7/73.3 Kobe Japan  quake 16 74.5/75.0 
Carter  in Bosnia " 1-6 76.3/79.1 Lost in I raq 16 75.7/68.8 J 
Cessna on White House 8 65.7/80.0 NYC Subway bombing 16 68.0/84.2 
cl inic Murders 16 75.9/80.0 OK-Ci ty  bombing 16 78.8/47.0 i 
Comet into Jupi ter  16 6~o.2/61.9 Pent ium chip flaw 4 81.1/72.9 II 
Cuban riot in Panama 2 65.2/73.9 Quayle lung clot 8 63.6/74.4 
Death of K im Jong 16 83.3/71.4 Serbians down F-16 16 78.6/75.0 * l  
DNA in OJ  trial 16 78.7/72.9 Serbs violate Bihac 16 55.0/59.3 i 
Haiti  ousts observers 8 62.0/74.0 Shannon Faulker 4 71.4/82.4 l 
Hall 's copter 16 78.5/75.0 USAir 427 crash 16 72.6/86.3 
Humble,  TX,  f looding 16 ...... 80.21/70.2 WTC Bombing trial 16 62.6/70.1 
Just ice-to-be Breyer 8 75.9/72.2 I! 
Accuracy 71.0/72.2 
! 
and 16. 'Miss' means Miss rate, which is the ra- In Table 3, 'Event '  denotes event words in the first - 
rio of the documents that were, judged as YES but  document in chronological order from .,X~ = 4, and 
not evaluated as YES for the run in question, the tit le of the document  is 'Emergency Work Con- i were  
' F /A '  shows false Mann rate mad 'F I '  is a measure tinues After Earthquake in Japan ' .  Table 3 clearly i 
that  balances recall and precision. 'Rec'  denotes the demonstrate~ that  the criterion, domain dependency 
ratio of the documents  judged YES that  were also of words effectively employed, i 
evaluated as YES, and Tree '  is the percent of the Figure 6 i l lustrates the DET  (Detect ion Evalua- | 
documents that  were evaluated as YES which corre- tion Tradeoff)  curves for a sample event (event type 
spond to documents  actually judged as YES. is 'Comet  into Jupi ter ' )  runs at several values of Art. 
i 
Table 2 shows that  more training data helps the 
performance, as the best result was when we used 9o , . , . .  . . . . . . .  ---., , . . . . . .  , . . . .  ' I  
-'Vt = 16. ~" ? q .." ~, ~ "', .: ? .. , ,~m~'~,~' - - - -  
Table 3 i l lustrates the extracted topic and event E0 ~" ...... " ~"'*'''"=~''~'":"*'"'"'"'''~i .:',..: wt. ".....: : ................ i e ~  ....... ? B 
words in a sample  document.  The topic is 'Kobe  i i i i q  i ~a'!4 ~-  i i ~ \ [ ; \ [ :  W Japem quake' and the m~mber of posit ive training e0 ~.4....i..-~...~....s....i-...i~:u...i...~,.4 .... .  ... ... e ,~ ,  ? 
i . :  ~ " ~ . ' .~'~. '~ ;.~ " ~ .  ~-- ' , ' . :  documents  is 4. 'Devp\]t', 'Devd\]t', 'DispPt'  and ~ l :. :. : : :.'-q: ;~ ~.~.: 1 : m~s--  ! 
'DispDt' denote values calculated by using formula 4o : ~..~..~.~. : . . . : 
(2) and (3). : : : " : 
: : : : : : ~ : ~. . | .a .  : i  .: " -: : - -  
20 ~*.. '2. . . , . t . . . . ' * - - . : ,* ,* ,? . . . . ;~: :=~: : ; - -  : * ' * " : ' "~"  . . . . . . .  ! . . . . . . . . . .  ! . . . . . . . . . . .  9" " ' " *~ 
i i : : " : ~ : : " .: " .  :- " " ! : " i : .: : " .: : . !~ . - t .~_  :. : : : : Table 3: Topic and event words in 'Kobe Japan  ~o| i i i i i i i i i :41 t. ~. \[ i i i 
"quake'  ~ i,.L...i.,.;-..i.....;....|.....; .. 4......~;..... ~j.an..:~ ........ i ......... ,;....... d 
i " "  ": " : " " " " "  " I "  ":~ ": " ! I 
""  " " ~ " " i " " .,~. "~- , "  " " 
Topic word Devp~t Devd~t DispPt DispDt ~ i..4....i...~....i.....~....J.....i . .. 4 ..... 4 . . . . . .  ?..i.....-i....~..i . ........ 4...... q : : : ? ! :" ? .: - . . : . . :  ? . ,  ~ ." 
earthquake 53.5 50.0 12.3 10.3 ~ "=:"= ' . . . .  "- : =:" . . . . . . . . .  " ......... "==" ............ : : .01 .(\]2 .I\]6 0.1 0.2 0.5 1 2 S 10 20 40 60 80 90 
J apan  
Kobe 
fire 
69.8 
56.6 
57.0 
50.0 
50.0 
46.4 
13.3 
8.6 
2.3 
9.8 
6.4 
1.5 
Event word 
emergency 
axea  
worker 
rescue 
Devplt  
50.0 
40.6 
50.0 
43.3 
Devdlt  
74.7 
50.0 
66.1 
50.0 
Di s pP t 
0.9 
0.6 
0.4 
2.3 
bL~pDt  
1.5 
1.0 
1.0 
3.4 
Fat~ ~aan. Pr0ea~y fm ~) 
I I  
Figure 6: DET  curve for a sample tracking runs B 
'Overal l ,  the curves also show that  more trailfing 
helps tile performance,  while there is no significant 
difference anaong :Yt = 2, 4 and 8. il 
5.4 Key  Paragraph  Ext rac t ion  
We used 4 different sets as a test data. Each set con- I 
sists of 2, 4, 8 and 16 documents.  For each set, we I I  
36 
I 
I 
extracted 10% and 20% of the full-documents para- 
"graph length (Jing et al, 1998). Table 4 illustrates 
the result. 
In Table 4, 'Num ~ denotes the number of documents 
in a set. 10 and 20?~ indicate the extraction ratio. 
'Para' denotes the number of par~]graphs exr.racted 
by a humaa~ judge, and 'Correct' shows the accuracy 
ot" the method. 
The best result was 77.7% (the extraction ratio is 
20% and the number of documents i 2). 
Wc now turn our attention to the main question: 
how was the contribution of making the distinction 
between a topic and an event for summarization 
task? Figure 7 illustrates the results of the methods 
which used (i) the extracted topic artd event words, 
i.e. our method, and (ii) only the extracted event" 
words. 
75 
~, 70 
8 
<175 
60 
55 
1 4 8 16 
Num 
Figure 7: Accuracy with each method 
In Figure 7, '(10%): and '(20%)' denote the ex- 
tracted paragraph ratio. 'Event' is the result when 
we used only the extracted event words. Figure 7 
shows that our method consistently outperforms the 
method which used only the extra,.ted events. To 
summarize the evaluation: 
][: Event extraction effectively employed when 
each document discusses different subject about 
the same topic. This shows that the method will 
be applicable to other genres of corpora which 
consist of different subjects. 
2. The result of tracking task (79.0% average recall 
and 86.6% average precision) is comparable to 
the existing tracking techniques which tested on 
the TDT1 corpus (Allan and Carbonell, 1998). 
3. Distinction between a topic and an event im- 
proved the results of key paragraph extrac- 
tion, as our method consistently outperforms 
the method which used only the extracted event 
words (see Figure 7). 
37 
6 Re la ted  Work  
The majority of techniques for summarization fall 
within two broad categories: Those that rely on tem- 
plate instantiation and those that rely on passage 
extraction. 
Work in the former approach is the DARPA- 
sponsored TIPSTER program and, in particular, the 
message understanding conferences hag provided fer- 
tile groined for such work, by placing the emphasis 
of docunmnt analysis to the identification and ex- 
traction of certain core entities and facts in a doc- 
ument, while work on template-driven, knowledge. 
based summarization to date is hardly domain or 
genre-independent (Boguraev and Kennedy. 1997). 
The alternative approach largely escapes this con- 
straint, by viewing the task as one of identi~,ing 
certain passages(typically sentences) which, by some 
metric, are deemed to be the most representative, of 
the document's content. A variety of approaches ex- 
ist for determining the salient sentences in the text: 
statistical techniques based oll word distribution 
(Kupiec et al, 1995), (Zechner, 1996), (Salton et 
al., 1991), (Teufell and Moens, 1997), symbolic tech- 
niques based on discourse structure (Marcu, 1997) 
and semantic relations between words (Barzil~v and 
Elhadad, 1997). All of their results demonstrate hat 
passage xtraction techniques are a useful first step 
in document summarization, although most of them 
have focused on a single document. 
Some researchers have started to apply a 
single-document summarization technique to multi- 
document. Stein et. al. proposed a method for 
summarizing multi-document using single-document 
summarizer (Stralkowsik et al, 1998), (Stralkowski 
et al. 1999). Their method first summarizes each 
document of multi-document, then groups the sum- 
maries in clusters and finally, orders these summaries 
in a logical way (Stein et al, 1999). Their technique 
seems ensible. However, as she admits, (i) the order 
the information should not only depend on topic cov- 
ered, (ii) background information that helps clari~" 
related information should be placed first. More seri- 
ously, as Barzilay and Mani claim, summarization of
multiple documents requires information about sim- 
ilarities and differences across  documents. There- 
fore it is difficult to identi~" these information using 
a single-document summarizer technique (Mani and 
Bloedorn, 1997), (Barzilay et al, 1999). 
A method proposed by Mani et. al. deal with 
the problem, i.e. they tried to detect the similar- 
ities and differences in information content  among 
documents (Mani and Bloedorn, 1997). They used 
a spreading activation algorithm and graph match- 
ing in order to identify similarities and differences 
across documents. The output is presented as a set 
of paragraphs with similar and unique words high- 
lighted. However, if the same information is men- 
Nun: 
Table 4: The results of Key Paragraph Extraction 
Accuracy 
%10 
Paa'a Correct(%) Para 
2 58 44(75.8) 117 
4 107 80(74.7) 214 
8 202 138(68.3) 404 
16 281 175(62~) 563 
Total 648 437(67.4) 1,298 
%20 
Correct(%) Para 
91(77.7) 175 
160(74.7) 321 
278(68.8) 606 
361(64.1) 844 
890(68.5) 1,946 
Total 
Correct(%) 
135(77.1) 
240(74.7) 
416(68.6) 
536(63.5) 
1,327(68.1) 
"tioned several times in different documents, much of 
the summary will be redundant. 
Allan et. al. also address the problem aald pro- 
posed a method for event tracking using common 
words and surprising features by supplementing the 
corpus statistics (Allan and Papka, 1998) (Papka et 
al., 1999). One of the purpose of this study is to 
make a distinction between an event aald an event 
class using surprising features. Here event class fea- 
tures are broad news areas such as politics, death, 
destruction and ~,'~fare. The idea is considered to 
be necessary to obtain higti accuracy, while Allan 
claims that the surprising words do not provide a 
broad enough coverage to capture all documents on 
the event. 
A more recent approach dealing with this problem 
is Barzilav et. al's approach (Barzilay et al, 1999). 
They used paraphrasing rules which are maaaually 
derived from the result of syntactic analysis to iden- 
tify theme intersection and used language generation 
to reformulate them as a coherent, summary. While 
promising to obtain high accuracy: the result of sum- 
marization task has not been reported. 
Like Mani and Barzil~,'s techniques, our ap- 
proach focuses on the problem that how to identi~" 
differences and similarities across documents, rather 
than the problem that how to form the actual sum- 
mar:,, (Sparck, 1993), (McKeown and Radev, 1995), 
(Radev and McKeown, 1998). However, while Barzi- 
lav's approach used paraphrasing rules to eliminate 
redmadancy in a summary, we proposed omain de- 
pendency of words to address robustness of the tech- 
nique. 
7 Conc lus ion  
In this paper, we proposed a method for extract- 
ing key paragraph for summarization based on dis- 
tinction between a topic and an event. The results 
showed that the average accuracy was 68.1~ when 
we used the TDT1 corpus. TIPSTER Text Sum- 
marization Evaluation (SUMMAC) proposed vari- 
ous methods for evaluating document summariza- 
tion and tasks (Mani et al, 1999). Of these, par- 
ticipants submitted two summaries: a fixed-length 
summary limited to 10% of tile length of the source, 
and a summary which was not limited in length. Fu- 
ture work includes quantitative and qualitative val- 
uation. In addition, our method used single words 
rather thaaa phrases. These phrases, however, would 
be helpful to resolve ambiguity and reduce a lot of 
noise, i.e. yield much better accuracy. We plaal to 
apply our method to phrase-based topic and event 
extraction, then turn to focus on the problem that 
how to form the actual summary.. 
Acknowledgments  
The authors would like to thank the reviewers 
for their valuable comments. This work was sup- 
ported ~'  the Grant-in-aid for the Japan Society for 
the Promotion of Science(JSPS, No.11780258) and 
Tateisi Science and Technology Foundation. 
Re ferences  
J. Allan and J. Carbonell. 1997. The tdt pilot study 
corpus documentation. In TDT.Study. Carpus, 
V1.3.doc. 
J. Allan and J. Carbonell. 1998. Topic detection 
and tracking pilot study: Final report.. In Proc. 
of the DARPA Broadcast News Transcription and 
Understanding Workshop. 
J. Allan and R. Papka. 1998. On-line new event de- 
tection and tracking. In Proc. of 21st Annual b~- 
ternational A CM SIGIR Conference on Research 
and Development in Information Retrieval, pages 
37-45. 
R. Barzila.v and M. Elhadad. 1997. Using lexical 
chains for text summarization. In Proc. of ACL 
Workshop on b~telligent Scalable Text Summa- 
rization, pages 10-17. 
R. Barzilay, K. R. McKeown, and M. Elhadad. 
1999. Information fusion in the context of multi- 
document summarization. In Proc. of 87th An- 
nual Meet.ing of Association for Computational 
Linguistics, pages 550-557. 
38 
I 
I 
i 
i 
i 
I 
I 
i 
I 
I 
I 
l 
i 
i 
I 
I 
I 
I 
I 
B. Boguraev mad C. Kennedy. 1997. Saiience-based 
content characterization f text documents. Ixi 
Proc. of A CL Workshop on b,telligent Scalable 
Tezt Summarization: p~ges 2-9. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proc. of the 3rd Conference on Applied 
Natural Language Processing, pages 152-155. 
J. Carbonell, Y. Yang, mad J. Lafferty. 1999. CMU 
report on TDT-2: Segmentation, detection and 
tracking. In Proc. o/the DARPA Broadcast News 
Workshop. 
F. Fukumoto, Y. Suzuki, and J. Fukumoto. 1997. 
An automatic extraction of key paragraphs based 
oil context dependency. In Proc. of the 5th Con- 
ference on Applied Natural Language Processing, 
pages 291-298. 
H. Jing, R. Barzil~', K. R. McKeown, and M. E1- 
hadad. 1998. Summarization evaluation methods: 
Experiments and analysis, intelligent ext sum- / 
marization. In Proc. o/1998 American Associa- 
tion/or Artificial h~telligence Sprin 9 Symposium, 
pages 51-59. 
J. Kupiec, 3. Pedersen, and F..Chen. 1995. A 
trainable document summarizer. In Proc. of the 
18th Annual International ACM SIGIR Confer- 
ence on Research and Development in h~formation 
Retrieval, pages 68-73. 
H. P. Lutm. 1958. The automatic creation of litera- 
ture abstracts. IBM journal, 2(1):159-165. 
I. Mani and E. Bloedorn. 1997. Multi-document 
summarization bygraph search and matching. In 
Proc. o/the 15th National Conference on Artifi- 
cial h~telligence , pages 622-628. 
I. Mani, T. Firmin, and B. Sundheim. 1999. The 
TIPSTER SUMMAC text summarization evalu- 
ation. In Proc. o/Ninth Conference o/the Eu- 
ropean Chapter o/the Association/or Computa- 
tional Linguistics, pages 77-85. 
D. Marcu. 1997. From discourse structures to text 
summaries. In Proc. of A CI, Workshop on Intel- 
ligent Scalable Text Summarization, pages 82-88. 
K. R. McKeown and D. R. Radev. 1995. Generating 
summaries of multiple news articles. In Proc. of 
the 18th Annual h~ternational A CM SIGIR Con- 
ference on Research and Development in Informa- 
tion Retrieval, pages 74-82. 
R. Papka, J. Allan, and V. Lavrenko. 1999. UMASS 
approaches todetection and tracking at TDT2. In 
Proc. of the DARPA Broadcast News Workshop. 
D. R. Radev and K. R. McKeown. 1998. Gen- 
erating natural anguage summaries from multi- 
pie on-line sources. Computational Linguistics. 
24(3):469-500. 
G. Salton, J. Allan, C. Buckle); and A. Singhal. 
1991. Automatic aaaalysis, theme generation, and 
summarization of machine-readable texts. Sci- 
ence, 164:1421-1426. 
K. J. Sparck. 1993. What might be in a summary? 
In Proc. of h~forraation Retrieval98, pages 9-26. 
G. C. Stein, T. Strzalkowski, aald G. B. Wise. 1999. 
Summarizing multiple documents using text ex- 
traction and interactive clustering. In Proc. of. 
the Pacific Association for Computational Lin- 
guistics1999, pages 200-208. 
T. Stralkowsik, G. C. Stein, aaad G. B. Wise. 1998. 
A text-extractlon based summarizer. In Proc. of 
Tipster Workshop. 
T. Stralkowski.. G. C. Stein; and G. B. Wise. 1999. 
Getracker: A robust, lightweight topic tracking 
system. In Proe. o/the DARPA Broadcast News 
Workshop. 
S. Teufell and M. Moens. 1997. Sentence xtraction 
as a classification task. In Proc. of ACL Workshop 
on h~telligent Scalable Text Summarization, pages 
58-65. 
K. Zechner. 1996. Fast generation ofabstracts from 
general domain text corpora by extracting rele- 
vant sentences. In Proc. of the 16th International 
Gonference on Gomputational Lin9uistics, pages 
986-989. 
! 39 
Manipulating Large Corpora for Text Classification
Fumiyo Fukumoto and Yoshimi Suzuki
Department of Computer Science and Media Engineering,
Yamanashi University
4-3-11 Takeda, Kofu 400-8511 Japan
fukumoto@skye.esb.yamanashi.ac.jp ysuzuki@alps1.esi.yamanashi.ac.jp
Abstract
In this paper, we address the problem of
dealing with a large collection of data
and propose a method for text classifi-
cation which manipulates data using two
well-known machine learning techniques,
Naive Bayes(NB) and Support Vector Ma-
chines(SVMs). NB is based on the as-
sumption of word independence in a text,
which makes the computation of it far
more efficient. SVMs, on the other hand,
have the potential to handle large feature
spaces, which makes it possible to pro-
duce better performance. The training
data for SVMs are extracted using NB
classifiers according to the category hier-
archies, which makes it possible to reduce
the amount of computation necessary for
classification without sacrificing accuracy.
1 Introduction
As the volume of online documents has drastically
increased, text classification has become more im-
portant, and a growing number of statistical and ma-
chine learning techniques have been applied to the
task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker
and McCallum, 1998), (Lam and Ho, 1998), (Mc-
Callum, 1999), (Dumais and Chen, 2000). Most of
them use the Reuters-21578 articles1 in the evalu-
1The Reuters-21578, distribution 1.0, is comprised of
21,578 documents, representing what remains of the original
Reuters-22173 corpus after the elimination of 595 duplicates
by Steve Lynch and David Lewis in 1996.
ations of their methods, since the corpus has be-
come a benchmark, and their results are thus eas-
ily compared with other results. It is generally
agreed that these methods using statistical and ma-
chine learning techniques are effective for classifi-
cation task, since most of them showed significant
improvement (the performance over 0.85 F1 score)
for Reuters-21578(Joachims, 1998), (Dumais et al,
1998), (Yang and Liu, 1999).
More recently, some researchers have applied
their techniques to larger corpora such as web
pages in Internet applications(Mladenic and Grobel-
nik, 1998), (McCallum, 1999), (Dumais and Chen,
2000). The increasing number of documents and
categories, however, often hampers the develop-
ment of practical classification systems, mainly due
to statistical, computational, and representational
problems(Dietterich, 2000). There are at least two
strategies for solving these problems. One is to
use category hierarchies. The idea behind this is
that when humans organize extensive data sets into
fine-grained categories, category hierarchies are of-
ten employed to make the large collection of cate-
gories more manageable. McCallum et. al. pre-
sented a method called ?shrinkage? to improve pa-
rameter estimates by taking advantage of a hierar-
chy(McCallum, 1999). They tested their method us-
ing three different real-world datasets: 20,000 ar-
ticles from UseNet, 6,440 web pages from the in-
dustry sector, and 14,831 pages from Yahoo, and
showed improved performance. Dumais et. al.
used SVMs and classified hierarchical web content
consisting of 50,078 web pages for training, and
10,024 for testing, with promising results(Dumais
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 196-203.
                         Proceedings of the Conference on Empirical Methods in Natural
and Chen, 2000).
The other is to use   	
  methods which are
learning algorithms that construct a set of classifiers
and then classify new data by taking a (weighted)
vote of their predictions(Dietterich, 2000). One
of the methods for constructing ensembles manipu-
lates the training examples to generate multiple hy-
potheses. The most straightforward way is called



. It presents the learning algorithm with a
training set that consists of a sample of  examples
drawn randomly with replacement from the original
training set. The second method is to construct the
training sets by leaving out disjoint subsets of the
training data. The third is illustrated by the AD-
ABOOST algorithm(Freund and Schapire, 1996).
Dietterich has compared these methods(Dietterich,
2000). He reported that in low-noise data, AD-
ABOOST performs well, while in high-noise cases,
it yields overfitting because ADABOOST puts a
large amount of weight on the mislabeled examples.
Bagging works well on both the noisy and the noise-
free data because it focuses on the statistical prob-
lem which arises when the amount of training data
available is too small, and noise increases this sta-
tistical problem. However, it is not clear whether
?works well? means that it exponentially reduces the
amount of computation necessary for classification,
while sacrificing only a small amount of accuracy,
or whether it is statistically significantly better than
other methods.
In this paper, we address the problem of dealing
with a large collection of data and report on an em-
pirical study for text classification which manipu-
lates data using two well-known machine learning
techniques, Naive Bayes(NB) and Support Vector
Machines(SVMs). NB probabilistic classifiers are
based on the assumption of word independence in a
text which makes the computation of the NB classi-
fiers far more efficient. SVMs, on the other hand,
have the potential to handle large feature spaces,
since SVMs use overfitting protection which does
not necessarily depend on the number of features,
and thus makes it possible to produce better perfor-
mance. The basic idea of our approach is quite sim-
ple: We solve simple classification problems using
NB and more complex and difficult problems using
SVMs. As in previous research, we use category
hierarchies. We use all the training data for NB.
The training data for SVMs, on the other hand, is
extracted using NB classifiers. The training data is
learned by NB using cross-validation according to
the hierarchical structure of categories, and only the
documents which could not classify correctly by NB
classifiers in each category level are extracted as the
training data of SVMs.
The rest of the paper is organized as follows. The
next section provides the basic framework of NB and
SVMs. We then describe our classification method.
Finally, we report some experiments using 279,303
documents in the Reuters 1996 corpus with a discus-
sion of evaluation.
2 Classifiers
2.1 NB
Naive Bayes(NB) probabilistic classifiers are com-
monly studied in machine learning(Mitchell, 1996).
The basic idea in NB approaches is to use the joint
probabilities of words and categories to estimate the
probabilities of categories given a document. The
NB assumption is that all the words in a text are
conditionally independent given the value of a clas-
sification variable. There are several versions of the
NB classifiers. Recent studies on a Naive Bayes
classifier which is proposed by McCallum et. al.
reported high performance over some other com-
monly used versions of NB on several data collec-
tions(McCallum et al, 1998). We use the model of
NB by McCallum et. al. which is shown in formula
(1).
A Comparison of Manual and Automatic Constructions of Category
Hierarchy for Classifying Large Corpora
Fumiyo Fukumoto
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
fukumoto@skye.esb.yamanashi.ac.jp
Yoshimi Suzuki
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
ysuzuki@ccn.yamanashi.ac.jp
Abstract
We address the problem dealing with a large
collection of data, and investigate the use of
automatically constructing category hierarchy
from a given set of categories to improve clas-
sification of large corpora. We use two well-
known techniques, partitioning clustering,  -
means and a  	 to create category
hierarchy.  -means is to cluster the given cate-
gories in a hierarchy. To select the proper num-
ber of  , we use a  	 which mea-
sures the degree of our disappointment in any
differences between the true distribution over
inputs and the learner?s prediction. Once the
optimal number of   is selected, for each clus-
ter, the procedure is repeated. Our evaluation
using the 1996 Reuters corpus which consists
of 806,791 documents shows that automati-
cally constructing hierarchy improves classifi-
cation accuracy.
1 Introduction
Text classification has an important role to play, espe-
cially with the recent explosion of readily available on-
line documents. Much of the previous work on text clas-
sification use statistical and machine learning techniques.
However, the increasing number of documents and cate-
gories often hamper the development of practical classifi-
cation systems, mainly by statistical, computational, and
representational problems(Dietterich, 2000). One strat-
egy for solving these problems is to use category hierar-
chies. The idea behind this is that when humans organize
extensive data sets into fine-grained categories, category
hierarchies are often employed to make the large collec-
tion of categories more manageable.
McCallum et. al. presented a method called ?shrink-
age? to improve parameter estimates by taking advan-
tage of the hierarchy(McCallum, 1999). They tested their
method using three different real-world datasets: 20,000
articles from the UseNet, 6,440 web pages from the In-
dustry Sector, and 14,831 pages from the Yahoo, and
showed improved performance. Dumais et. al. also de-
scribed a method for hierarchical classification of Web
content consisting of 50,078 Web pages for training, and
10,024 for testing, with promising results(Dumais and
Chen, 2000). Both of them use hierarchies which are
manually constructed. Such hierarchies are costly human
intervention, since the number of categories and the size
of the target corpora are usually very large. Further, man-
ually constructed hierarchies are very general in order to
meet the needs of a large number of forthcoming acces-
sible source of text data, and sometimes constructed by
relying on human intuition. Therefore, it is difficult to
keep consistency, and thus, problematic for classifying
text automatically.
In this paper, we address the problem dealing with a
large collection of data, and propose a method to gener-
ate category hierarchy for text classification. Our method
uses two well-known techniques, partitioning clustering
method called  -means and a  	 to create
hierarchical structure.  -means partitions a set of given
categories into   clusters, locally minimizing the average
squared distance between the data points and the clus-
ter centers. The algorithm involves iterating through the
data that the system is permitted to classify during each
iteration and constructs category hierarchy. To select the
proper number of   during each iteration, we use a 
	 which measures the degree of our disappoint-
ment in any differences between the true distribution over
inputs and the learner?s prediction. Another focus of this
paper is whether or not a large collection of data, the
1996 Reuters corpus helps to generate a category hier-
archy which is used to classify documents.
The rest of the paper is organized as follows. The next
section presents a brief review the earlier work. We then
explain the basic framework for constructing category hi-
erarchy, and describe hierarchical classification. Finally,
we report some experiments using the 1996 Reuters cor-
pus with a discussion of evaluation.
2 Related Work
Automatically generating hierarchies is not a new goal
for NLP and their application systems, and there have
been several attempts to create various types of hier-
archies(Koller and Sahami, 1997), (Nevill-Manning et
al., 1999), (Sanderson and Croft, 1999). One attempt
is Crouch(Crouch, 1988), which automatically generates
thesauri. Cutting et al proposed a method called Scat-
ter/Gather in which clustering is used to create document
hierarchies(Cutting et al, 1992). Lawrie et al proposed a
method to create domain specific hierarchies that can be
used for browsing a document set and locating relevant
documents(Lawrie and Croft, 2000).
At about the same time, several researchers have in-
vestigated the use of automatically generating hierarchies
for a particular application, text classification. Iwayama
et al presented a probabilistic clustering algorithm called
Hierarchical Bayesian Clustering(HBC) to construct a set
of clusters for text classification(Iwayama and Tokunaga,
1995). The searching platform they focused on is the
probabilistic model of text categorisation that searches
the most likely clusters to which an unseen document is
classified. They tested their method using two data sets:
Japanese dictionary data called ?Gendai yogo no kiso-
tisiki? which contains 18,476 word entries, and a collec-
tion of English news stories from the Wall Street Jour-
nal which consists of 12,380 articles. The HBC model
showed 2 3% improvements in breakeven point over the
non-hierarchical model.
Weigend et al proposed a method to generate hi-
erarchies using a probabilistic approach(Weigend et al,
1999). They used an exploratory cluster analysis to create
hierarchies, and this was then verified by human assign-
ments. They used the Reuters-22173 and defined two-
level categories: 5 top-level categories (agriculture, en-
ergy, foreign exchange, metals and miscellaneous cate-
gory) called meta-topic, and other category groups as-
signed to its meta-topic. Their method is based on a prob-
abilistic approach that frames the learning problem as one
of function approximation for the posterior probability of
the topic vector given the input vector. They used a neu-
ral net architecture and explored several input represen-
tations. Information from each level of the hierarchy is
combined in a multiplicative fashion, so no hard decision
have to be made except at the leaf nodes. They found
a 5% advantage in average precision for the hierarchical
representation when using words.
All of these mentioned above perform well, while the
collection they tested is small compared with many real-
istic applications. In this paper, we investigate that a large
collection of data helps to generate a hierarchy, i.e. it is
statistically significant better than the results which uti-
lize hierarchical structure by hand, that has not previously
been explored in the context of hierarchical classification
except for the improvements of hierarchical model over
the flat model.
3 Generating Hierarchical Structure
3.1 Document Representation
To generate hierarchies, we need to address the question
of how to represent texts(Cutting et al, 1992), (Lawrie
and Croft, 2000). The total number of words we focus on
is too large and it is computationally very expensive.
We use two statistical techniques to reduce the number
of inputs. The first is to use 
  instead of
 . The number of input vectors is not
the number of the training documents but equals to the
number of different categories. This allows to make the
large collection of data more manageable. The second is
a well-known technique, i.e. mutual information measure
between a word and a category. We use it as the value in
each dimension of the vector(Cover and Thomas, 1991).
More formally, each category in the training set is rep-
resented using a vector of weighted words. We call it

 . Category vectors are used for repre-
senting as points in Euclidean space in  -means cluster-
ing algorithm. Let 
 
be one of the categories 
 
,   , 

,
and a vector assigned to 
 
be (
  
, 
 
,   , 
 
). The mu-
tual information  
 between a word  , and a
category 
 is defined as:
  

 
    
 
  
   
  
     
(1)
Each 
 
(1  	 ) is the value of mutual information
between 

and 
 
. We select the 1,000 words with the
largest mutual information for each category.
3.2 Clustering
Clustering has long been used to group data with many
applications(Jain and Dubes, 1988). We use a simple
clustering technique,  -means to group categories and
construct a category hierarchy(Duda and Hart, 1973).  -
means is based on iterative relocation that partitions a
dataset into   clusters. The algorithm keeps track of the
centroids, i.e. seed points, of the subsets, and proceeds in
iterations. In each iteration, the following is performed:
(i) for each point , find the seed point which is closest
to . Associate  with this seed point, (ii) re-estimate
each seed point locations by taking the center of mass
of points associated with it. Before the first iteration the
seed points are initialized to random values. However, a
bad choice of initial centers can have a great impact on
performance, since  -means is fully deterministic, given
the starting seed points. We note that by utilizing hierar-
chical structure, the classification problem can be decom-
posed into a set of smaller problems corresponding to hi-
erarchical splits in the tree. This indicates that one first
learns rough distinctions among classes at the top level,
then lower level distinctions are learned only within the
appropriate top level of the tree, and lead to more special-
ized classifiers. We thus selected the top   frequent cate-
gories as initial seed points. Figure 1 illustrates a sample
hierarchy obtained by  -means. The input is a set of cat-
egory vectors. Seed points assigned to each cluster are
underlined in Figure 1.
k=3
C1, C2, C3 C6, C7, C8,
C9, C10C4, C5
C2
C1 C3
C5
C4
C8
C6 C7 C9
C10
k=2
k=3
C1 C2
C6 C7 C9, C10
Figure 1: Hierarchical structure obtained by  -means
In general, the number of   is not given beforehand.
We thus use a  	which is derived from Naive
Bayes(NB) classifiers to evaluate the goodness of  .
3.3 NB
Naive Bayes(NB) probabilistic classifiers are commonly
studied in machine learning(Mitchell, 1996). The basic
idea in NB approaches is to use the joint probabilities of
words and categories to estimate the probabilities of cat-
egories given a document. The NB assumption is that all
the words in a text are conditionally independent given
the value of a classification variable. There are several
versions of the NB classifiers. Recent studies on a Naive
Bayes classifier which is proposed by McCallum et al re-
ported high performance over some other commonly used
versions of NB on several data collections(McCallum,
1999). We use the model of NB by McCallum et al
which is shown in formula (2).
  
 
 



Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 32?40,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Classifying Japanese Polysemous Verbs based on Fuzzy C-means
Clustering
Yoshimi Suzuki
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi, Japan
ysuzuki@yamanashi.ac.jp
Fumiyo Fukumoto
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi, Japan
fukumoto@yamanashi.ac.jp
Abstract
This paper presents a method for classify-
ing Japanese polysemous verbs using an
algorithm to identify overlapping nodes
with more than one cluster. The algo-
rithm is a graph-based unsupervised clus-
tering algorithm, which combines a gener-
alized modularity function, spectral map-
ping, and fuzzy clustering technique. The
modularity function for measuring cluster
structure is calculated based on the fre-
quency distributions over verb frames with
selectional preferences. Evaluations are
made on two sets of verbs including pol-
ysemies.
1 Introduction
There has been quite a lot of research concerned
with automatic clustering of semantically simi-
lar words or automatic retrieval of collocations
among them from corpora. Most of this work is
based on similarity measures derived from the dis-
tribution of words in corpora. However, the facts
that a single word does have more than one sense
and that the distribution of a word in a corpus is a
mixture of usages of different senses of the same
word often hamper such attempts. In general, re-
striction of the subject domain makes the problem
of polysemy less problematic. However, even in
texts from a restricted domain such as economics
or sports, one encounters quite a large number of
polysemous words. Therefore, semantic classifi-
cation of polysemies has been an interest since the
earliest days when a number of large scale corpora
have become available.
In this paper, we focus on Japanese polysemous
verbs, and present a method for polysemous verb
classification. We used a graph-based unsuper-
vised clustering algorithm (Zhang, 2007). The
algorithm combines the idea of modularity func-
tion Q, spectral relaxation and fuzzy c-means clus-
tering method to identify overlapping nodes with
more than one cluster. The modularity function
measures the quality of a cluster structure. Spec-
tral mapping performs a dimensionality reduction
which makes it possible to cluster in the very high
dimensional spaces. The fuzzy c-means allows for
the detection of nodes with more than one cluster.
We applied the algorithm to cluster polysemous
verbs. The modularity function for measuring the
quality of a cluster structure is calculated based
on the frequency distributions over verb frames
with selectional preferences. We collected seman-
tic classes from IPAL Japanese dictionary (IPAL,
1987), and used them as a gold standard data.
IPAL lists about 900 Japanese basic verbs, and cat-
egorizes each verb into multiple senses. Moreover,
the categorization is based on verbal syntax with
respect to the choice of its arguments. Therefore,
if the clustering algorithm induces a polysemous
verb classification on the basis of verbal syntax,
then the resulting classification should agree the
IPAL classes. We used a large Japanese newspaper
corpus and EDR (Electronic Dictionary Research)
dictionary (EDR, 1986) to obtain verbs and their
subcategorization frames with selectional prefer-
ences 1. The results obtained using two data sets
were better than the baseline, EM algorithm.
The rest of the paper is organized as follows.
The next section presents related work. After
describing Japanese verb with selectional pref-
erences, we present a distributional similarity in
Section 4, and a graph-based unsupervised clus-
tering algorithm in Section 5. Results using two
data sets are reported in Section 6. We give our
conclusion in Section 7.
1We did not use IPAL, but instead EDR sense dictionary.
Because IPAL did not have senses for the case filler which
were used to create selectional preferences.
32
2 Related Work
Graph-based algorithms have been widely used
to classify semantically similar words (Jannink,
1999; Galley, 2003; Widdows, 2002; Muller,
2006). Sinha and Mihalcea proposed a graph-
based algorithm for unsupervised word sense
disambiguation which combines several seman-
tic similarity measures including Resnik?s metric
(Resnik, 1995), and algorithms for graph central-
ity (Sinha, 2007). They reported that the results
using the SENSEVAL-2 and SENSEVAL-3 En-
glish all-words data sets lead to relative error rate
reductions of 5 - 8% as compared to the previsous
work (Mihalcea, 2005). More recently, Matsuo
et al (2006) presented a method of word clus-
tering based on Web counts using a search en-
gine. They applied Newman clustering (New-
man, 2004) for identifying word clusters. They
reported that the results obtained by the algorithm
were better than those obtained by average-link
agglomerative clustering using 90 Japanese noun
words. However, their method relied on hard-
clustering models, and thus have largely ignored
the issue of polysemy that word belongs to more
than one cluster.
In contrast to hard-clustering algorithms, soft
clustering allows that words to belong to more
than one cluster. Much of the previous work on
word classification with soft clustering is based
on the EM algorithm (Pereira, 1993). Torisawa
et al, (2002) presented a method to detect asso-
ciative relationships between verb phrases. They
used the EM algorithm to calculate the likelihood
of co-occurrences, and reported that the EM is ef-
fective to produce associative relationships with
a certain accuracy. More recent work in this di-
rection is that of Schulte et al, (2008). They
proposed a method for semantic verb classifica-
tion based on verb frames with selectional prefer-
ences. They combined the EM training with the
MDL principle. The MDL principle is used to
induce WordNet-based selectional preferences for
arguments within subcategorization frames. The
results showed the effectiveness of the method.
Our work is similar to their method in the use of
verb frames with selectional preferences. Korho-
nen et al (2003) used verb?frame pairs to clus-
ter verbs into Levin-style semantic classes (Ko-
rhonen, 2003). They used the Information Bottle-
neck, and classified 110 test verbs into Levin-style
classes. They had a focus on the interpretation of
verbal polysemy as represented by the soft clus-
ters: they interpreted polysemy as multiple-hard
assignments.
In the context of Japanese taxonomy of verbs
and their classes, Utsuro et al (1995) proposed a
class-based method for sense classification of ver-
bal polysemy in case frame acquisition from paral-
lel corpora (Utsuro, 1995). A measure of bilingual
class/class association is introduced and used for
discovering sense clusters in the sense distribution
of English predicates and Japanese case element
nouns. They used the test data consisting of 10 En-
glish and Japanese verbs taken from Roget?s The-
saurus and BGH (Bunrui Goi Hyo) (BGH, 1989).
They reported 92.8% of the discovered clusters
were correct. Tokunaga et al (1997) presented
a method for extending an existing thesaurus by
classifying new words in terms of that thesaurus.
New words are classified on the basis of relative
probabilities of a word belonging to a given word
class, with the probabilities calculated using noun-
verb co-occurrence pairs. Experiments using the
Japanese BGH thesaurus showed that new words
can be classified correctly with a maximum accu-
racy of more than 80%, while they did not report
in detail whether the clusters captured polysemies.
3 Selectional Preferences
A major approach on word clustering task is to use
distribution of a word in a corpus, i.e., words are
classified into classes based on their distributional
similarity. Similarity measures based on distribu-
tional hypothesis compare a pair of weighted fea-
ture vectors that characterize two words (Hindle,
1990; Lin, 1998; Dagan, 1999).
Like previous work on verb classification, we
used subcategorization frame distributions with
selectional preferences to calculate similarity be-
tween verbs (Schulte, 2008). We used the EDR
dictionary of selectional preferences consisting of
5,269 basic Japanese verbs and the EDR concept
dictionary (EDR, 1986). For selectional prefer-
ences, the dictionary has each concept of a verb,
the group of possible co-occurrence surface-level
case particles, the types of concept relation label
that correspond to the surface-level case as well
as the range of possible concepts that may fill the
deep-level case. Figure 1 illustrates an example of
a verb ?taberu (eat)?.
In Figure 1, ?Sentence pattern? refers to the co-
occurrence pattern between a verb and a noun
33
[Sentence pattern] <word1> ga <word2> wo taberu (eat)
[Sense relation] agent object
[Case particle] ga (nominative) wo (accusative)
[Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants);
30f6e5(parts of plants);
3f9639(food and drink);
3f963a(feed)
Figure 1: An example of a verb ?taberu (eat)?
with a case marker. ?Sense relation? expresses the
deep-level case, while ?Case particle? shows the
surface-level case. ?Sense identifier? refers to the
range of possible concepts for the case filler. The
subcategorization frame pattern of a sentence (1),
for example consists of two arguments with selec-
tional preferences and is given below:
(1) Nana ga apple wo taberu.
?Nana eats an apple.?
taberu 30f6b0 ga 3f9639 wo
eat human nom entity acc
In the above frame pattern, x of the argument
?x y? refers to sense identifier and y denotes case
particle.
4 Distributional Similarity
Various similarity measures have been proposed
and used for NLP tasks (Korhonen, 2002). In
this paper, we concentrate on three distance-based,
and entropy-based similarity measures. In the fol-
lowing formulae, x and y refer to the verb vec-
tors, their subscripts to the verb subcategorization
frame values.
1. The Cosine measure (Cos): The cosine
measures the similarity of the two vectors x
and y by calculating the cosine of the an-
gle between vectors, where each dimension
of the vector corresponds to each frame with
selectional preferences patterns of verbs and
each value of the dimension is the frequency
of each pattern.
2. The Cosine measure based on probability
of relative frequencies (rfCos): The differ-
ences between the cosine and the value based
on relative frequencies of verb frames with
selectional preferences are the values of each
dimension, i.e., the former are frequencies of
each pattern and the latter are the fraction of
the total number of verb frame patterns be-
longing to the verb.
3. L
1
Norm (L
1
): The L
1
Norm is a mem-
ber of a family of measures known as the
Minkowski Distance, for measuring the dis-
tance between two points in space. The L
1
distance between two verbs can be written as:
L
1
(x, y) =
n
?
i=1
| x
i
? y
i
| .
4. Kullback-Leibler (KL): Kullback-Leibler is
a measure from information theory that deter-
mines the inefficiency of assuming a model
probability distribution given the true distri-
bution.
KL(x, y) =
n
?
i=1
P (x
i
) ? log
P (x
i
)
P (y
i
)
.
where P (x
i
) =
x
i
|x|
. KL is not defined in
case y
i
= 0. So, the probability distribu-
tions must be smoothed (Korhonen, 2002).
We used two smoothing methods, i.e., Add-
one smoothing and Witten and Bell smooth-
ing (Witten, 1991).2 Moreover, two variants
of KL, ?-skew divergence and the Jensen-
Shannon, were used to perform smoothing.
5. ?-skew divergence (? div.): The ?-skew di-
vergence measure is a variant of KL, and is
defined as:
?div(x, y) = KL(y, ? ? x + (1 ? ?) ? y).
Lee (1999) reported the best results with ? =
0.9. We used the same value.
6. The Jensen-Shannon (JS): The Jensen-
Shannon is a measure that relies on the as-
sumption that if x and y are similar, they are
close to their average. It is defined as:
2We report Add-one smoothing results in the evaluation,
as it was better than Witten and Bell smoothing.
34
JS(x, y) =
1
2
[KL(x,
x + y
2
) + KL(y,
x + y
2
)].
All measures except Cos and rfCos showed that
smaller values indicate a closer relation between
two verbs. Thus, we used inverse of each value.
5 Clustering Method
The clustering algorithm used in this study was a
graph-based unsupervised clustering reported by
(Zhang, 2007). This algorithm detects overlap-
ping nodes by the combination of a modularity
function based on Newman Girvan?s Q function
(Newman, 2004), spectral mapping that maps in-
put nodes into Euclidean space, and fuzzy c-means
clustering which allows node to belong to more
than one cluster. They evaluated their method by
applying several data including the American col-
lege football team network, and found that the al-
gorithm successfully detected overlapping nodes.
We thus used the algorithm to cluster verbs.
Here are the key steps of the algorithm: Given
a set of input verbs V = {v
1
, v
2
, ? ? ? v
n
}, an up-
per bound K of the number of clusters, the adja-
cent matrix A = (a
ij
)
n?n
of an input verbs and a
threshold ? that can convert a soft assignment into
final clustering, i.e., the value of ? decreases, each
verb is distributed into larger number of clusters.
We calculated the adjacent matrix A by using one
of the similarity measures mentioned in Section 4,
i.e., the value of the edge between v
i
and v
j
. a
ij
refers to the similarity value between them.
1. Form a diagonal matrix D = (d
ii
), where d
ii
=
?
k
a
ik
.
2. Form the eigenvector matrix E
K
=
[e
1
, e
2
, ? ? ? , e
K
] by calculating the top K
eigenvectors of the generalized eigensystem
Ax = tDx.
3. For each value of k, 2 ? k ? K:
(a) Form the matrix E
k
= [e
2
, ? ? ? , e
k
] where
e
k
refers to the top k-th eigenvector.
(b) Normalize the rows of E
k
to unit length
using Euclidean distance norm.
(c) Cluster the row vectors of E
k
using
fuzzy c-means to obtain a soft assign-
ment matrix U
k
. Fuzzy c-means is
carried out through an iterative opti-
mization (minimization) of the objective
function J
m
with the update of member-
ship degree u
ij
and the cluster centers
c
j
. J
m
is defined as:
J
m
=
n
?
i=1
k
?
j=1
u
m
ij
|| v
i
? c
j
||
2
,
where u
ij
is the membership degree of
v
i
in the cluster j, and
?
j
u
ij
= 1. m ?
[1,?] is a weight exponent controlling
the degree of fuzzification. c
j
is the d-
dimensional center of the cluster j.
|| v
i
? c
j
|| is defined as:
|| v
i
? c
j
||
2
= (v
i
? c
j
)E(v
i
? c
j
)
T
.
where E denotes an unit matrix. The
procedure converges to a saddle point of
J
m
.
4. Pick the k and the corresponding n ? k
soft assignment matrix U
k
that maximizes
the modularity function ?Q(U
k
). Here U
k
=
[u
1
, ? ? ?u
k
] with 0 ? u
ic
? 1 for each c = 1,
? ? ?, k, and
?
k
1
u
ic
= 1 for each i = 1, ? ? ?, n.
A modularity function of a soft assignment
matrix is defined as:
?
Q(U
k
) =
k
?
c=1
[
A(
?
V
c
,
?
V
c
)
A(V, V )
? (
A(
?
V
c
, V )
A(V, V )
)
2
],
where
A(
?
V
c
,
?
V
c
) =
?
i?
?
V
c
,j?
?
V
c
{
(u
ic
+ u
jc
)
2
}a
ij
,
A(
?
V
c
, V ) = A(
?
V
c
,
?
V
c
) +
?
i?
?
V
c
,j?V \
?
V
c
{
(u
ic
+ (1 ? u
jc
))
2
}a
ij
,
A(V, V ) =
?
i?V,j?V
a
ij
.
?
Q(U
k
) shows comparison of the actual val-
ues of internal or external edges with its re-
spective expectation value under the assump-
tion of equally probable links and given data
sizes.
35
6 Experiments
6.1 Experimental setup
We created test verbs using two sets of Japanese
Mainichi newspaper corpus. One is a set con-
sisting one year (2007) newspapers (We call it a
set from 2007), and another is a set of 17 years
(from 1991 to 2007) Japanese Mainichi newspa-
pers (We call it a set from 1991 2007). For each
set, all Japanese documents were parsed using the
syntactic analyzer Cabocha (Kudo, 2003). We
selected verbs, each frequency f(v) is, 500 ?
f(v) ? 10,000. As a result, we obtained 279
verbs for a set from 2007 and 1,692 verbs for
a set from 1991 2007. From these verbs, we
chose verbs which appeared in the machine read-
able dictionary, IPAL. This selection resulted in
a total of 81 verbs for a set from 2007, and 170
verbs, for a set from 1991 2007. We obtained
Japanese verb frames with selectional preferences
using these two sets. We extracted sentence pat-
terns with their frequencies. Noun words within
each sentence were tagged sense identifier by us-
ing the EDR Japanese sense dictionary. As a re-
sult, we obtained 56,400 verb frame patterns for a
set from 2007, and 300,993 patterns for a set from
1991 2007.
We created the gold standard data, verb classes,
using IPAL. IPAL lists about 900 Japanese verbs
and categorizes each verb into multiple senses,
based on verbal syntax and semantics. It also
listed synonym verbs. Table 1 shows a fragment of
the entry associated with the Japanese verb taberu.
The verb ?taberu? has two senses, ?eat? and
?live?. ?pattern? refers to the case frame(s) associ-
ated with each verb sense. According to the IPAL,
we obtained verb classes, each class corresponds
to a sense of each verb. There are 87 classes for
a set from 2007, and 152 classes for a set from
1991 2007. The examples of the test verbs and
their senses are shown in Table 2.
For evaluation of verb classification, we used
the precision, recall, and F-score, which were de-
fined by (Schulte, 2000), especially to capture
how many verbs does the algorithm actually de-
tect more than just the predominant sense.
For comparison against polysemies, we utilized
the EM algorithm which is widely used as a soft
clustering technique (Schulte, 2008). We followed
the method presented in (Rooth, 1999). We used
a probability distribution over verb frames with
selectional preferences. The initial probabilities
Table 3: Results for a set from 2007
Method m ? C Prec Rec F
FCM 2.0 0.09 74 .815 .483 .606
FCM(none) 1.5 0.07 74 .700 .477 .567
EM ? ? 87 .308 .903 .463
Table 4: Results against each measure
Measure m ? C Prec Rec F
cos 3.0 0.02 74 .660 .517 .580
rfcos 2.0 0.04 74 .701 .488 .576
L
1
2.0 0.04 74 .680 .500 .576
KL 2.0 0.09 74 .815 .483 .606
? div. 2.0 0.04 74 .841 .471 .604
JS 1.5 0.03 74 .804 .483 .603
EM ? ? 87 .308 .903 .463
were often determined randomly. We set the ini-
tial probabilities by using the result of the standard
k-means. For k-means, we used 50 random repli-
cations of the initialization, each time initializing
the cluster center with k randomly chosen. We
used up to 20 iterations to learn the model prob-
abilities.
6.2 Basic results
The results using a set from 2007 are shown in
Table 3. We used KL as a similarity measure in
FCM. ?FCM(none)? shows the result not applying
a spectral mapping, i.e., we applied fuzzy c-means
to each vector of verb, where each dimension of
the vector corresponds to each frame with selec-
tional preferences. ?m? and ??? refer to the pa-
rameters used by Fuzzy C-means. ?C? refers to
the number of clusters obtained by each method.
?m?, ??? and ?C? in Table 3 denote the value that
maximized the F-score. ?C? in the EM is fixed
in advance. The result of EM shows the best re-
sult among 20 iterations. As can be seen clearly
from Table 3, the result obtained by fuzzy c-means
was better to the result by EM algorithm. Table
3 also shows that a dimensionality reduction, i.e.,
spectral mapping improved overall performance,
especially we have obtained better precision. The
result suggests that a dimensionality reduction is
effective for clustering. Table 4 shows the results
obtained by using each similarity measure. As we
can see from Table 4, the overall results obtained
by information theory based measures, KL, ? div.,
and JS were slightly better to the results obtained
by other distance based measures.
We note that the fuzzy c-means has two param-
eters ? and m, where ? is a threshold of the as-
36
Table 1: A fragment of the entry associated with the Japanese verb ?taberu?
Sense id Pattern Synonyms
1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat)
2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live)
Table 2: Examples of test verbs and their polysemic gold standard senses
Id Sense Verb Classes Id Sense Verb Classes
1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru}
2 prey {negau, inoru} 12 persuade {oshieru, satosu}
3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru}
4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru}
5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru}
6 move {saru, utsuru} 16 lose {ushinau, nakusu}
7 pass {saru, kieru, sugiru} 17 miss {ushinau, torinogasu, itusuru}
8 go {saru, sugiru, iku} 18 survive, lose {ushinau, nakusu, shinareru}
9 remove {saru, hanareru, toozakeru 19 give {kubaru, watasu, wakeru}
torinozoku}
10 lead {oshieru, michibiku, tugeru} 20 arrange {kubaru, haichisuru}
Figure 2: F-score against ?
signment in the fuzzy c-means, and m is a weight
controlling the degree of fuzzification. To exam-
ine how these parameters affect the overall per-
formance of the algorithm, we performed exper-
iments by varying these parameters. Figure 2 il-
lustrates F-score of polysemies against the value
of ?. We used KL as a similarity measure, m = 2,
and C = 74.
As shown in Figure 2, the best result was ob-
tained when the value of ? was 0.09. When ?
value was larger than 0.09, the overall perfor-
mance decreased, and when it exceeded 1.2, no
verbs were assigned to multiple sense. Figure 3
illustrates F-score against the value of m. As il-
lustrated in Figure 3, we could not find effects on
accuracy against the value of m. It is necessary to
investigate on the influence of the parameter m by
performing further quantitative evaluation.
Figure 3: F-score against m
6.3 Error analysis against polysemy
We examined whether 46 polysemous verbs in
a set from 2007 were correctly classified into
classes. We manually analyzed clustering results
obtained by running fuzzy c-means with KL as a
similarity measure. They were classified into three
types of error.
1. Partially correct: Some senses of a poly-
semous verb were correctly identified, but
others were not. The first example of this
pattern is that ?nigiru? has at least two
senses, ?motsu (have)? and ?musubu (dou-
ble)?. However, only one sense was identi-
fied correctly. The second example is that one
of the senses of the verb ?watasu? was clas-
sified correctly into the class ?ataeru (give)?,
while it was classified incorrectly into the
class ?uru (sell)?. This was the most frequent
error type.
37
{nigiru, motsu (have)}
?
{watasu, ataeru (give)}
{watasu, uru (sell)}
2. Polysemous verbs classified into only one
cluster: ?hakobu? has two senses ?carry?,
and ?progress?. However, it was classified
into one cluster including verbs ?motuteiku
(carry)?, and ?susumu (progress)?. Because
it often takes the same nominative subjects
such as ?human? and accusative object such
as ?abstract?.
{hakobu (carry, progress),
motuteiku (carry), susumu (progress)}
3. Polysemous verb incorrectly classified into
clusters: The polysemous verb ?hataraku?
has two senses, ?work?, and ?operate?. How-
ever, it was classified incorrectly into ?ochiru
(fall)? and ?tsukuru (make)?.
{hataraku (work, operate), ochiru (fall),
tsukuru (make)}
Apart from the above error analysis, we found
that we should improve the definition and demar-
cation of semantic classes by using other exist-
ing thesaurus, e.g., EDR or BGH (Bunrui Goi
Hyo) (BGH, 1989). We recall that we created
the gold standard data by using synonymous infor-
mation. However, the algorithm classified some
antonymous words such as ?uketoru? (receive) and
?watasu? (give) into one cluster. Similarly, transi-
tive and intransitive verbs are classified into the
same cluster. For example, intransitive verb of the
verb ?ochiru? (drop) is ?otosu?. They were clas-
sified into one cluster. It would provide further
potential, i.e., not only to improve the accuracy
of classification, but also to reveal the relationship
between semantic verb classes and their syntactic
behaviors.
An investigation of the resulting clusters re-
vealed another interesting direction of the method.
We found that some senses of a polysemous verb
Table 5: Results for a set from 1991 2007
Method m ? C Prec Rec F
FCM 2.0 0.24 152 .792 .477 .595
FCM(none) 2.0 0.07 147 .687 .459 .550
EM ? ? 152 .284 .722 .408
which is not listed in the IPAL are correctly identi-
fied by the algorithm. For example, ?ukeireru? and
?yurusu? (forgive) were correctly classified into
one cluster. Figure 4 illustrates a sample of verb
frames with selectional preferences extracted by
our method.
?ukeireru? and ?yurusu? in Table 4 have the same
frame pattern, and the sense identifiers of the case
filler ?wo?, for example, are ?a human being?
(0f0157) and ?human? (30f6b0). However, these
verbs are not classified into one class in the IPAL:
?ukeireru? is not listed in the IPAL as a synonym
verb of ?yurusu?. The example illustrates that
these verbs within a cluster are semantically re-
lated, and that they share obvious verb frames with
intuitively plausible selectional preferences. This
indicates that we can extend the algorithm to solve
this resource scarcity problem: semantic classifi-
cation of words which do not appear in the re-
source, but appear in corpora.
6.4 Results for a set of verbs from 1991 2007
corpus
One goal of this work was to develop a cluster-
ing methodology with respect to the automatic
recognition of Japanese verbal polysemies cover-
ing large-scale corpora. For this task, we tested a
set of 170 verbs including 82 polysemies. The re-
sults are shown in Table 5. We used KL as a simi-
larity measure in FCM. Each value of the parame-
ter shows the value that maximized the F-score.
As shown in Table 5, the result obtained by fuzzy
c-means was as good as for the smaller set, a set
of 78 verbs. Moreover, we can see that the fuzzy
c-means is better than the EM algorithm and the
method not applying a spectral mapping, as an in-
crease in the F-score of 18.7% compared with the
EM, and 4.5% compared with a method without
spectral mapping. This shows that our method is
effective for a size of the input test data consisting
178 verbs.
One thing should be noted is that when the al-
gorithm is applied to large data, it is computation-
ally expensive. There are at least two ways to ad-
dress the problem. One is to use several methods
38
[Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive)
[Concept relation] agent object
[Case particle] ga (nominative) wo (accusative)
[Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0
0ee0de: the part of a something written that makes reference to a particular matter
0f58b4: a generally-held opinion
0f98ee: the people who citizens of a nation
0f0157: a human being
30f6b0: human
Figure 4: Extracted Verb frames of ?ukeireru? and ?yurusu? (forgive)
of fuzzy c-means acceleration. Kelen et al (2002)
presented an efficient implementation of the fuzzy
c-means algorithm, and showed that the algorithm
had the worse-case complexity of O(nK2), where
n is the number of nodes, and K is the number of
eigenvectors. Another approach is to parallelize
the algorithm by using the Message Passing Inter-
face (MPI) to estimate the optimal number of k (2
? k ? K). This is definitely worth trying with our
method.
7 Conclusion
We have developed an approach for classifying
Japanese polysemous verbs using fuzzy c-means
clustering. The results were comparable to other
unsupervised techniques. Future work will assess
by a comparison against other existing soft clus-
tering algorithms such as the Clique Percolation
method (Palla, 2005). Moreover, it is necessary
to apply the method to other verbs for quantitative
evaluation. New words including polysemies are
generated daily. We believe that classifying these
words into semantic classes potentially enhances
many semantic-oriented NLP applications. It is
necessary to apply the method to other verbs, espe-
cially low frequency of verbs to verify that claim.
Acknowledgments
This work was supported by the Grant-in-aid for
the Japan Society for the Promotion of Science
(JSPS).
References
E. Iwabuchi. 1989. Word List by Semantic Principles,
National Language Research Institute Publications,
Shuei Shuppan.
I. Dagan and L. Lee and F. C. N. Pereira. 1999.
Similarity-based Models of Word Cooccurrence
Probabilities. Machine Learning, 34(1-3), pages
43?69.
Japan Electronic Dictionary Research Institute, Ltd.
http://www2.nict.go.jp/r/r312/EDR/index.html
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining, In Proc.
of 19th International Joint Conference on Artificial
Intelligence, pages 1486?1488.
D. Hindle. 1990. Noun Classification from Predicate-
Argument Structures, In Proc. of 28th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 268?275.
GSK2007-D. http://www.gsk.or.jp/catalog/GSK2007-
D/catalog.html
J. Jannink and G. Wiederhold. 1999. Thesaurus Entry
Extraction from an On-line Dictionary, In Proc. of
Fusion?99.
J. F. Kelen and T. Hutcheson. 2002. Reducing the
Time Complexity of the Fuzzy C-means Algorithm,
In Trans. of IEEE Fuzzy Systems, 10(2), pages 263?
267.
A. Korhonen and Y. Krymolowski. 2002. On the
Robustness of Entropy-based Similarity Measures
in Evaluation of Subcategorization Acquisiton Sys-
tems. In Proc. of the 6th Conference on Natural
Language Learning, pages 91?97.
A. Korhonen and Y. Krymolowski and Z. Marx. 2003.
Clustering Polysemic Subcategorization Frame Dis-
tributions Semantically. In Proc. of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 64?71.
T.Kudo and Y.Matsumoto. 2003. Fast Methods for
Kernel-based Text Analysis. In Proc. of 41th ACL,
pages 24?31.
L. Lee. 1999. Measures of Distributional Similarity.
In Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 25?32.
D. Lin. 1998. Automatic Retrieval and Clustering
of Similar Words, In Proc. of 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, pages 768?773.
39
Y. Matsuo and T. Sakaki and K. Uchiyama and M.
Ishizuka. 2006. Graph-based Word Clustering us-
ing a Web Search Engine, In Proc. of 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2006), pages 542?550.
R. Mihalcea. 2005. Unsupervised Large Vocabulary
Word Sense Disambiguation with Graph-based Al-
gorithms for Sequence Data Labeling, In Proc. of
the Human Language Technology / Empirical Meth-
ods in Natural Language Processing Conference,
pages 411?418.
P. Muller and N. Hathout and B. Gaume. 2006. Syn-
onym Extraction Using a Semantic Distance on a
Dictionary, In Proc. of the Workshop on TextGraphs,
pages 65?72.
M.E.J.Newman. 2004. Fast Algorithm for Detecting
Community Structure in Networks, Physical Re-
view, E 2004, 69, 066133.
G. Palla and I. Dere?nyi and I. Farkas and T. Vic-
sek. 2005. Uncovering the Overlapping Commu-
nity Structure of Complex Networks in Nature and
Society, Nature. 435(7043), 814?8.
F. Pereira and N. Tishby and L. Lee. 1993. Distribu-
tional Clustering of English Words. In Proc. of the
31st Annual Meeting of the Association for Compu-
tational Linguistics, pages 183?190.
P. Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proc.
of 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
M. Rooth et al 1999. Inducing a Semantically Anno-
tated Lexicon via EM-Based Clustering, In Proc. of
37th ACL, pages 104?111.
R. Sinha and R. Mihalcea. 2007. Unsupervised Graph-
based Word Sense Disambiguation Using Measures
of Word Semantic Similarity. In Proc. of the IEEE
International Conference on Semantic Computing,
pages 46?54.
S. Schulte im Walde. 2000. Clustering Verbs Seman-
tically according to their Alternation Behaviour. In
Proc. of the 18th COLING, pages 747?753.
S. Schulte im Walde et al 2008. Combining EM
Training and the MDL Principle for an Automatic
Verb Classification Incorporating Selectional Pref-
erences. In Proc. of the 46th ACL, pages 496?504.
T. Tokunaga and A. Fujii and M. Iwayama and N. Saku-
rai and H. Tanaka. 1997. Extending a thesaurus
by classifying words. In Proc. of the ACL-EACL
Workshop on Automatic Information Extraction and
Building of Lexical Semantic Resources, pages 16?
21.
K. Torisawa. 2002. An Unsupervised Learning
Method for Associative Relationships between Verb
Phrases, In Proc. of 19th International Confer-
ence on Computational Linguistics (COLING2002),
pages 1009?1015.
T. Utsuro. 1995. Class-based sense classification of
verbal polysemy in case frame acquisition from par-
allel corpora. In Proc. of the 3rd Natural Language
Processing Pacific Rim Symposium, pages 671?677.
D. Widdows and B. Dorow. 2002. A Graph Model for
Unsupervised Lexical Acquisition. In Proc. of 19th
International conference on Computational Linguis-
tics (COLING2002), pages 1093?1099.
I. H. Witten and T. C. Bell. 1991. The Zero-
Frequency Problem: Estimating the Probabilities of
Novel Events in Adaptive Text Compression. IEEE
Transactions on Information Theory, 37(4), pages
1085?1094.
S. Zhang et al 2007. Identification of Overlapping
Community Structure in Complex Networks using
Fuzzy C-means Clustering. PHYSICA A, 374, pages
483?490.
40
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 552?557,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identification of Domain-Specific Senses in a Machine-Readable Dictionary
Fumiyo Fukumoto
Interdisciplinary Graduate School of
Medicine and Engineering,
Univ. of Yamanashi
fukumoto@yamanashi.ac.jp
Yoshimi Suzuki
Interdisciplinary Graduate School of
Medicine and Engineering,
Univ. of Yamanashi
ysuzuki@yamanashi.ac.jp
Abstract
This paper focuses on domain-specific senses
and presents a method for assigning cate-
gory/domain label to each sense of words in
a dictionary. The method first identifies each
sense of a word in the dictionary to its cor-
responding category. We used a text classifi-
cation technique to select appropriate senses
for each domain. Then, senses were scored by
computing the rank scores. We used Markov
Random Walk (MRW) model. The method
was tested on English and Japanese resources,
WordNet 3.0 and EDR Japanese dictionary.
For evaluation of the method, we compared
English results with the Subject Field Codes
(SFC) resources. We also compared each En-
glish and Japanese results to the first sense
heuristics in the WSD task. These results
suggest that identification of domain-specific
senses (IDSS) may actually be of benefit.
1 Introduction
Domain-specific sense of a word is crucial informa-
tion for many NLP tasks and their applications, such
asWord Sense Disambiguation (WSD) and Informa-
tion Retrieval (IR). For example, in the WSD task,
McCarthy et al presented a method to find predom-
inant noun senses automatically using a thesaurus
acquired from raw textual corpora and the Word-
Net similarity package (McCarthy et al, 2004; Mc-
Carthy et al, 2007). They used parsed data to find
words with a similar distribution to the target word.
Unlike Buitelaar et al approach (Buitelaar and
Sacaleanu, 2001), they evaluated their method us-
ing publically available resources, namely SemCor
(Miller et al, 1998) and the SENSEVAL-2 English
all-words task. The major motivation for their work
was similar to ours, i.e., to try to capture changes in
ranking of senses for documents from different do-
mains.
Domain adaptation is also an approach for fo-
cussing on domain-specific senses and used in the
WSD task (Chand and Ng, 2007; Zhong et al, 2008;
Agirre and Lacalle, 2009). Chan et. al. proposed
a supervised domain adaptation on a manually se-
lected subset of 21 nouns from the DSO corpus hav-
ing examples from the Brown corpus and Wall Street
Journal corpus. They used active learning, count-
merging, and predominant sense estimation in order
to save target annotation effort. They showed that
for the set of nouns which have different predomi-
nant senses between the training and target domains,
the annotation effort was reduced up to 29%. Agirre
et. al. presented a method of supervised domain
adaptation (Agirre and Lacalle, 2009). They made
use of unlabeled data with SVM (Vapnik, 1995),
a combination of kernels and SVM, and showed
that domain adaptation is an important technique for
WSD systems. The major motivation for domain
adaptation is that the sense distribution depends on
the domain in which a word is used. Most of them
adapted textual corpus which is used for training on
WSD.
In the context of dictionary-based approach, the
first sense heuristic applied to WordNet is often used
as a baseline for supervised WSD systems (Cotton et
al., 1998), as the senses in WordNet are ordered ac-
cording to the frequency data in the manually tagged
resource SemCor (Miller et al, 1998). The usual
552
drawback in the first sense heuristic applied to the
WordNet is the small size of the SemCor corpus.
Therefore, senses that do not occur in SemCor are
often ordered arbitrarily. More seriously, the deci-
sion is not based on the domain but on the frequency
of SemCor data. Magnini et al presented a lexi-
cal resource where WordNet 2.0 synsets were anno-
tated with Subject Field Codes (SFC) by a procedure
that exploits the WordNet structure (Magnini and
Cavaglia, 2000; Bentivogli et al, 2004). The results
showed that 96% of the WordNet synsets of the noun
hierarchy could have been annotated using 115 dif-
ferent SFC, while identification of the domain labels
for word senses was required a considerable amount
of hand-labeling.
In this paper, we focus on domain-specific
senses and propose a method for assigning cate-
gory/domain label to each sense of words in a dictio-
nary. Our approach is automated, and requires only
documents assigned to domains/categories, such as
Reuters corpus, and a dictionary with gloss text,
such as WordNet. Therefore, it can be applied easily
to a new domain, sense inventory or different lan-
guages, given sufficient documents.
2 Identification of Domain-Specific Senses
Our approach, IDSS consists of two steps: selection
of senses and computation of rank scores.
2.1 Selection of senses
The first step to find domain-specific senses is to se-
lect appropriate senses for each domain. We used
a corpus where each document is classified into do-
mains. The selection is done by using a text classi-
fication technique. We divided documents into two
sets, i.e., training and test sets. The training set is
used to train SVM classifiers, and the test set is to
test SVM classifiers. For each domain, we collected
noun words. Let D be a domain set, and S be a set
of senses that the word w ? W has. Here, W is a set
of noun words. The senses are obtained as follows:
1. For each sense s ? S, and for each d ? D, we
applied word replacement, i.e., we replaced w
in the training documents assigning to the do-
main d with its gloss text in a dictionary.
2. All the training and test documents are tagged
by a part-of-speech tagger, and represented as
term vectors with frequency.
3. The SVM was applied to the two types of train-
ing documents, i.e., with and without word re-
placement, and classifiers for each category are
generated.
4. SVM classifiers are applied to the test data. If
the classification accuracy of the domain d is
equal or higher than that without word replace-
ment, the sense s of a word w is judged to be a
candidate sense in the domain d.
The procedure is applied to all w ? W .
2.2 Computation of rank scores
We note that text classification accuracy used in se-
lection of senses depends on the number of words
consisting gloss in a dictionary. However, it is not
so large. As a result, many of the classification ac-
curacy with word replacement were equal to those
without word replacement1. Then in the second pro-
cedure, we scored senses by using MRW model.
Given a set of senses Sd in the domain d, Gd =
(Sd, E) is a graph reflecting the relationships be-
tween senses in the set. Each sense si in Sd is a
gloss text assigned from a dictionary. E is a set of
edges, which is a subset of Sd ? Sd. Each edge eij
in E is associated with an affinity weight f(i ? j)
between senses si and sj (i 6= j). The weight is com-
puted using the standard cosine measure between
two senses. The transition probability from si to
sj is then defined by normalizing the corresponding
affinity weight p(i ? j) = f(i?j)
P
|Sd|
k=1 f(i?k)
, if ?f 6= 0,
otherwise, 0.
We used the row-normalized matrix Uij =
(Uij)
|Sd|?|Sd| to describe G with each entry corre-
sponding to the transition probability, where Uij =
p(i ? j). To make U a stochastic matrix, the rows
with all zero elements are replaced by a smooth-
ing vector with all elements set to 1
|Sd| . The matrix
form of the saliency score Score(si) can be formu-
lated in a recursive form as in the MRW model: ~?
= ?UT~? + (1??)
|Sd| ~e, where
~? = [Score(si)]
|Sd|?1
is a vector of saliency scores for the senses. ~e is a
column vector with all elements equal to 1. ? is a
1In the experiment, the classification accuracy of more than
50% of words has not changed.
553
damping factor. We set ? to 0.85, as in the PageR-
ank (Brin and Page, 1998). The final transition ma-
trix is given by the formula (1), and each score of the
sense in a specific domain is obtained by the princi-
pal eigenvector of the new transition matrix M .
M = ?UT + (1 ? ?)
| Sd |
~e~eT (1)
We applied the algorithm for each domain. We
note that the matrix M is a high-dimensional space.
Therefore, we used a ScaLAPACK, a library of
high-performance linear algebra routines for dis-
tributed memory MIMD parallel computing (Netlib,
2007)2. We selected the topmost K% senses accord-
ing to rank score for each domain and make a sense-
domain list. For each word w in a document, find
the sense s that has the highest score within the list.
If a domain with the highest score of the sense s and
a domain in a document appearing w match, s is re-
garded as a domain-specific sense of the word w.
3 Experiments
3.1 WordNet 3.0
We assigned Reuters categories to each sense of
words in WordNet 3.0 3. The Reuters documents
are organized into 126 categories (Rose et al, 2002).
We selected 20 categories consisting a variety of
genres. We used one month of documents, from
20th Aug to 19th Sept 1996 to train the SVM model.
Similarly, we classified the following one month of
documents into these 20 categories. All documents
were tagged by Tree Tagger (Schmid, 1995).
Table 1 shows 20 categories, the number of train-
ing and test documents, and F-score (Baseline)
by SVM. For each category, we collected noun
words with more than five frequencies from one-
year Reuters corpus. We randomly divided these
into two: 10% for training and the remaining 90%
for test data. The training data is used to estimate K
according to rank score, and test data is used to test
the method using the estimated value K. We man-
ually evaluated a sense-domain list. As a result, we
set K to 50%. Table 2 shows the result using the
2For implementation, we used a supercomputer, SPARCEn-
terprise M9000, 64CPU, 1TB memory.
3http://wordnet/princeton.edu/
test data, i.e., the total number of words and senses,
and the number of selected senses (Select S) that the
classification accuracy of each domain was equal or
higher than the result without word replacement. We
used these senses as an input of MRW.
There are no existing sense-tagged data for these
20 categories that could be used for evaluation.
Therefore, we selected a limited number of words
and evaluated these words qualitatively. To do
this, we used SFC resources (Magnini and Cavaglia,
2000), which annotate WordNet 2.0 synsets with do-
main labels. We manually corresponded Reuters
and SFC categories. Table 3 shows the results of
12 Reuters categories that could be corresponded to
SFC labels. In Table 3, ?Reuters? shows categories,
and ?IDSS? shows the number of senses assigned by
our approach. ?SFC? refers to the number of senses
appearing in the SFC resource. ?S & R? denotes the
number of senses appearing in both SFC and Reuters
corpus. ?Prec? is a ratio of correct assignments by
?IDSS? divided by the total number of ?IDSS? as-
signments. We manually evaluated senses not ap-
pearing in SFC resource. We note that the corpus
used in our approach is different from SFC. There-
fore, recall denotes a ratio of the number of senses
matched in our approach and SFC divided by the
total number of senses appearing in both SFC and
Reuters.
As shown in Table 3, the best performance was
?weather? and recall was 0.986, while the result
for ?war? was only 0.149. Examining the result
of text classification by word replacement, the for-
mer was 0.07 F-score improvement by word replace-
ment, while that of the later was only 0.02. One rea-
son is related to the length of the gloss in WordNet:
the average number of words consisting the gloss as-
signed to ?weather? was 8.62, while that for ?war?
was 5.75. IDSS depends on the size of gloss text in
WordNet. Efficacy can be improved if we can assign
gloss sentences to WordNet based on corpus statis-
tics. This is a rich space for further exploration.
In the WSD task, a first sense heuristic is often
applied because of its powerful and needless of ex-
pensive hand-annotated data sets. We thus compared
the results obtained by our method to those obtained
by the first sense heuristic. For each of the 12 cat-
egories, we randomly picked up 10 words from the
senses assigned by our approach. For each word, we
554
Cat Train Test F-score Cat Train Test F-score
Legal/judicial 897 808 .499 Funding 3,245 3,588 .709
Production 2,179 2,267 .463 Research 204 180 .345
Advertising 113 170 .477 Management 923 812 .753
Employment 1,224 1,305 .703 Disasters 757 522 .726
Arts/entertainments 326 295 .536 Environment 532 420 .476
Fashion 13 50 .333 Health 524 447 .513
Labour issues 1,278 1,343 .741 Religion 257 251 .665
Science 158 128 .528 Sports 2,311 2,682 .967
Travel 47 64 .517 War 3,126 2,674 .678
Elections 1,107 1,208 .689 Weather 409 247 .688
Table 1: Classification performance (Baseline)
Cat Words Senses S senses Cat Words Senses S senses
Legal/judicial 10,920 62,008 25,891 Funding 11,383 28,299 26,209
Production 13,967 31,398 30,541 Research 7,047 19,423 18,600
Advertising 7,960 23,154 20,414 Management 9,386 24,374 22,961
Employment 11,056 28,413 25,915 Disasters 10,176 28,420 24,266
Arts 12,587 29,303 28,410 Environment 10,737 26,226 25,413
Fashion 4,039 15,001 12,319 Health 10,408 25,065 24,630
Labour issues 11,043 28,410 25,845 Religion 8,547 21,845 21,468
Science 8,643 23,121 21,861 Sports 12,946 31,209 29,049
Travel 5,366 16,216 15,032 War 13,864 32,476 30,476
Elections 11,602 29,310 26,978 Weather 6,059 18,239 16,402
Table 2: The # of candidate senses (WordNet)
Reuters IDSS SFC S&R Rec Prec
Legal/judicial 25,715 3,187 809 .904 .893
Funding 2,254 2,944 747 .632 .650
Arts 3,978 3,482 576 .791 .812
Environment 3,725 56 7 .857 .763
Fashion 12,108 2,112 241 .892 .793
Sports 935 1,394 338 .800 .820
Health 10,347 79 79 .329 .302
Science 21,635 62,513 2,736 .810 .783
Religion 1,766 3,408 213 .359 .365
Travel 14,925 506 86 .662 .673
War 2,999 1,668 301 .149 .102
Weather 16,244 253 72 .986 .970
Average 9,719 6,800 517 .686 .661
Table 3: The results against SFC resource
selected 10 sentences from the documents belonging
to each corresponding category. Thus, we tested 100
sentences for each category. Table 4 shows the re-
sults. ?Sense? refers to the number of average senses
par a word. Table 4 shows that the average preci-
sion by our method was 0.648, while the result ob-
tained by the first sense heuristic was 0.581. Table
4 also shows that overall performance obtained by
our method was better than that with the first sense
heuristic in all categories.
3.2 EDR dictionary
We assigned categories from Japanese Mainichi
newspapers to each sense of words in EDR Japanese
dictionary 4. The Mainichi documents are organized
into 15 categories. We selected 4 categories, each
of which has sufficient number of documents. All
documents were tagged by a morphological analyzer
Chasen (Matsumoto et al, 2000), and nouns are ex-
tracted. We used 10,000 documents for each cate-
gory from 1991 to 2000 year to train SVM model.
We classified other 600 documents from the same
period into one of these four categories. Table 5
shows categories and F-score (Baseline) by SVM.
We used the same ratio used in English data to es-
timate K . As a result, we set K to 30%. Table 6
shows the result of IDSS. ?Prec? refers to the preci-
sion of IDSS, i.e., we randomly selected 300 senses
4http://www2.nict.go.jp/r/r312/EDR/index.html
555
Cat Sense IDSS First sense
Correct Wrong Prec Correct Wrong Prec
Legal/judicial 5.3 69 31 .69 63 37 .63
Funding 5.6 60 40 .60 43 57 .43
Arts/entertainments 4.5 62 38 .62 48 52 .48
Environment 6.5 72 28 .72 70 30 .70
Fashion 4.7 74 26 .74 73 27 .73
Sports 4.3 72 28 .72 70 30 .70
Health 4.5 68 32 .68 62 38 .62
Science 5.0 69 31 .69 65 35 .65
Religion 4.1 54 46 .54 52 48 .52
Travel 4.8 75 25 .75 68 32 .68
War 4.9 53 47 .53 30 70 .30
Weather 5.3 60 40 .60 53 47 .53
Average 4.95 64.8 35.1 0.648 58.0 41.9 0.581
Table 4: IDSS against the first sense heuristic (WordNet)
Cat Precision Recall F-score
International .650 .853 .778
Economy .703 .804 .750
Science .867 .952 .908
Sport .808 .995 .892
Table 5: Text classification performance (Baseline)
Cat Words Senses S senses Prec
International 3,607 11,292 10,647 .642
Economy 3,180 9,921 9,537 .571
Science 4,759 17,061 13,711 .673
Sport 3,724 12,568 11,074 .681
Average 3,818 12,711 11,242 .642
Table 6: The # of selected senses (EDR)
for each category and evaluated these senses qualita-
tively. The average precision for four categories was
0.642.
In the WSD task, we randomly picked up 30
words from the senses assigned by our method. For
each word, we selected 10 sentences from the doc-
uments belonging to each corresponding category.
Table 7 shows the results. As we can see from
Table 7 that IDSS was also better than the first
sense heuristics in Japanese data. For the first sense
heuristics, there was no significant difference be-
tween English and Japanese, while the number of
senses par a word in Japanese resource was 3.191,
and it was smaller than that with WordNet (4.950).
One reason is the same as SemCor data, i.e., the
Cat Sense IDSS First sense
International 2.873 .630 .587
Economy 2.793 .677 .637
Science 4.223 .723 .610
Sports 2.873 .620 .477
Average 3.191 .662 .593
Table 7: IDSS against the first sense heuristic (EDR)
small size of the EDR corpus. Therefore, there are
many senses that do not occur in the corpus. In fact,
there are 62,460 nouns which appeared in both EDR
and Mainichi newspapers (from 1991 to 2000 year),
164,761 senses in all. Of these, there are 114,267
senses not appearing in the EDR corpus. This also
demonstrates that automatic IDSS is more effective
than the frequency-based first sense heuristics.
4 Conclusion
We presented a method for assigning categories to
each sense of words in a machine-readable dictio-
nary. For evaluation of the method using Word-
Net 3.0, the average precision was 0.661, and recall
against the SFC was 0.686. Moreover, the result of
WSD obtained by our method outperformed against
the first sense heuristic in both English and Japanese.
Future work will include: (i) applying the method
to other part-of-speech words, (ii) comparing the
method with existing other automated method, and
(iii) extending the method to find domain-specific
senses with unknown words.
556
References
E. Agirre and O. L. Lacalle. 2009. Supervised domain
adaption for wsd. In Proc. of the 12th Conference of
the European Chapter of the ACL, pages 42?50.
L. Bentivogli, P. Forner, B. Magnini, and E. Pianta. 2004.
Revising the WORDNET DOMAINS Hierarchy: Se-
mantics, Coverage and Balancing. In In Proc. of COL-
ING 2004 Workshop on Multilingual Linguistic Re-
sources, pages 101?108.
S. Brin and L. Page. 1998. The Anatomy of a Large-
scale Hypertextual Web Search Engine. In Computer
Networks and ISDN Systems, volume 30, pages 1?7.
P. Buitelaar and B. Sacaleanu. 2001. Ranking and Se-
lecting Synsets by Domain Relevance. In Proc. of
WordNet and Other Lexical Resources: Applications,
Extensions and Customization, pages 119?124.
Y. S. Chand and H. T. Ng. 2007. Domain adaptation
with active learning for word sense disambiguation. In
Proc. of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 49?56.
S. Cotton, P. Edmonds, A. Kilgarriff, and
M. Palmer. 1998. SENSEVAL-2,
http://www.sle.sharp.co.uk/senseval2/.
B. Magnini and G. Cavaglia. 2000. Integrating Subject
Field Codes into WordNet. In In Proc. of LREC-2000.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano,
Y. Matsuda, K. Takaoka, and M. Asahara. 2000.
Japanese Morphological Analysis System ChaSen
Version 2.2.1. In NAIST Technical Report NAIST.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding Predominant Senses in Untagged Text. In
Proc. of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 280?287.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2007. Unsupervised Acquisition of Predominant
Word Senses. Computational Linguistics, 33(4):553?
590.
G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.
1998. A Semantic Concordance. In Proc. of the ARPA
Workshop on Human Language Technology, pages
303?308.
Netlib. 2007. http://www.netlib.org/scalapack/index.html.
In Netlib Repository at UTK and ORNL.
T. G. Rose, M. Stevenson, and M. Whitehead. 2002.
The Reuters Corpus Volume 1 - from yesterday?s news
to tomorrow?s language resources. In Proc. of Third
International Conference on Language Resources and
Evaluation.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proc. of the
EACL SIGDAT Workshop.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
Z. Zhong, H. T. Ng, and Y. S. Chan. 2008. Word sense
disambiguation using ontonotes: An empirical study.
In Proc. of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 1002?1010.
557
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474?478,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Text Classification from Positive and Unlabeled Data using Misclassified
Data Correction
Fumiyo Fukumoto and Yoshimi Suzuki and Suguru Matsuyoshi
Interdisciplinary Graduate School of Medicine and Engineering
University of Yamanashi, Kofu, 400-8511, JAPAN
{fukumoto,ysuzuki,sugurum}@yamanashi.ac.jp
Abstract
This paper addresses the problem of deal-
ing with a collection of labeled training
documents, especially annotating negative
training documents and presents a method
of text classification from positive and un-
labeled data. We applied an error detec-
tion and correction technique to the re-
sults of positive and negative documents
classified by the Support Vector Machines
(SVM). The results using Reuters docu-
ments showed that the method was compa-
rable to the current state-of-the-art biased-
SVM method as the F-score obtained by
our method was 0.627 and biased-SVM
was 0.614.
1 Introduction
Text classification using machine learning (ML)
techniques with a small number of labeled data has
become more important with the rapid increase in
volume of online documents. Quite a lot of learn-
ing techniques e.g., semi-supervised learning, self-
training, and active learning have been proposed.
Blum et al proposed a semi-supervised learn-
ing approach called the Graph Mincut algorithm
which uses a small number of positive and nega-
tive examples and assigns values to unlabeled ex-
amples in a way that optimizes consistency in a
nearest-neighbor sense (Blum et al, 2001). Cabr-
era et al described a method for self-training text
categorization using the Web as the corpus (Cabr-
era et al, 2009). The method extracts unlabeled
documents automatically from the Web and ap-
plies an enriched self-training for constructing the
classifier.
Several authors have attempted to improve clas-
sification accuracy using only positive and unla-
beled data (Yu et al, 2002; Ho et al, 2011). Liu
et al proposed a method called biased-SVM that
uses soft-margin SVM as the underlying classi-
fiers (Liu et al, 2003). Elkan and Noto proposed
a theoretically justified method (Elkan and Noto,
2008). They showed that under the assumption
that the labeled documents are selected randomly
from the positive documents, a classifier trained on
positive and unlabeled documents predicts proba-
bilities that differ by only a constant factor from
the true conditional probabilities of being positive.
They reported that the results were comparable to
the current state-of-the-art biased SVM method.
The methods of Liu et al and Elkan et al model
a region containing most of the available positive
data. However, these methods are sensitive to the
parameter values, especially the small size of la-
beled data presents special difficulties in tuning
the parameters to produce optimal results.
In this paper, we propose a method for elimi-
nating the need for manually collecting training
documents, especially annotating negative train-
ing documents based on supervised ML tech-
niques. Our goal is to eliminate the need for manu-
ally collecting training documents, and hopefully
achieve classification accuracy from positive and
unlabeled data as high as that from labeled posi-
tive and labeled negative data. Like much previous
work on semi-supervised ML, we apply SVM to
the positive and unlabeled data, and add the classi-
fication results to the training data. The difference
is that before adding the classification results, we
applied the MisClassified data Detection and Cor-
rection (MCDC) technique to the results of SVM
learning in order to improve classification accu-
racy obtained by the final classifiers.
2 Framework of the System
The MCDC method involves category error cor-
rection, i.e., correction of misclassified candidates,
while there are several strategies for automati-
cally detecting lexical/syntactic errors in corpora
(Abney et al, 1999; Eskin, 2000; Dickinson and
474
training
UPP1 N1N1
training
SVM
MCDC
N1RC1
U?N1
CP1
CN1SVM
MCDC
? Final results
SVMtraining
selection
classification
PCPN1RC1
N1RC2
N1CN
MCDC
Figure 1: Overview of the system
Meurers., 2005; Boyd et al, 2008) or categorical
data errors (Akoglu et al, 2013). The method first
detects error candidates. As error candidates, we
focus on support vectors (SVs) extracted from the
training documents by SVM. Training by SVM is
performed to find the optimal hyperplane consist-
ing of SVs, and only the SVs affect the perfor-
mance. Thus, if some training document reduces
the overall performance of text classification be-
cause of an outlier, we can assume that the docu-
ment is a SV.
Figure 1 illustrates our system. First, we ran-
domly select documents from unlabeled data (U )
where the number of documents is equal to that of
the initial positive training documents (P1). We set
these selected documents to negative training doc-
uments (N1), and apply SVM to learn classifiers.
Next, we apply the MCDC technique to the re-
sults of SVM learning. For the result of correction
(RC1)1, we train SVM classifiers, and classify the
remaining unlabeled data (U \ N1). For the re-
sult of classification, we randomly select positive
(CP1) and negative (CN1) documents classified
by SVM and add to the SVM training data (RC1).
We re-train SVM classifiers with the training doc-
uments, and apply the MCDC. The procedure is
repeated until there are no unlabeled documents
judged to be either positive or negative. Finally,
the test data are classified using the final classi-
fiers. In the following subsections, we present the
MCDC procedure shown in Figure 2. It consists
of three steps: extraction of misclassified candi-
dates, estimation of error reduction, and correction
of misclassified candidates.
1The manually annotated positive examples are not cor-
rected.
Extraction
of miss-
classified
candidates
Training data D
test
learning
D ?SV (Support vectors)
Estimation of 
error reduction
classification
SV label
?
NB label
D ?Error candidates
Correction of
misclassified candidates
D1
D2
Final results
Error candidates
SVM NB
Loss function
Judgment using loss values
Figure 2: The MCDC procedure
2.1 Extraction of misclassified candidates
Let D be a set of training documents and xk ?
{x1, x2, ? ? ?, xm} be a SV of negative or positive
documents obtained by SVM.We remove ?mk=1xk
from the training documents D. The resulting
D \ ?mk=1xk is used for training Naive Bayes
(NB) (McCallum, 2001), leading to a classifica-
tion model. This classification model is tested on
each xk, and assigns a positive or negative label.
If the label is different from that assigned to xk,
we declare xk an error candidate.
2.2 Estimation of error reduction
We detect misclassified data from the extracted
candidates by estimating error reduction. The es-
timation of error reduction is often used in ac-
tive learning. The earliest work is the method of
Roy and McCallum (Roy and McCallum, 2001).
They proposed a method that directly optimizes
expected future error by log-loss or 0-1 loss, using
the entropy of the posterior class distribution on
a sample of unlabeled documents. We used their
method to detect misclassified data. Specifically,
we estimated future error rate by log-loss function.
It uses the entropy of the posterior class distribu-
tion on a sample of the unlabeled documents. A
loss function is defined by Eq (1).
EP?D2?(xk,yk) = ?
1
| X |
?
x?X
?
y?Y
P (y|x)
? log(P?D2?(xk,yk)(y|x)). (1)
Eq (1) denotes the expected error of the learner.
P (y | x) denotes the true distribution of out-
put classes y ? Y given inputs x. X denotes a
475
set of test documents. P?D2?(xk,yk)(y | x) shows
the learner?s prediction, and D2 denotes the train-
ing documents D except for the error candidates
?lk=1xk. If the value of Eq (1) is sufficiently
small, the learner?s prediction is close to the true
output distribution.
We used bagging to reduce variance of P (y | x)
as it is unknown for each test document x. More
precisely, from the training documents D, a dif-
ferent training set consisting of positive and nega-
tive documents is created2. The learner then cre-
ates a new classifier from the training documents.
The procedure is repeated m times3, and the final
class posterior for an instance is taken to be the un-
weighted average of the class posteriori for each of
the classifiers.
2.3 Correction of misclassified candidates
For each error candidate xk, we calculated the ex-
pected error of the learner, EP?D2?(xk,yk old) and
EP?D2?(xk,yk new) by using Eq (1). Here, yk old
refers to the original label assigned to xk, and
yk new is the resulting category label estimated by
NB classifiers. If the value of the latter is smaller
than that of the former, we declare the document
xk to be misclassified, i.e., the label yk old is an
error, and its true label is yk new. Otherwise, the
label of xk is yk old.
3 Experiments
3.1 Experimental setup
We chose the 1996 Reuters data (Reuters, 2000)
for evaluation. After eliminating unlabeled doc-
uments, we divided these into three. The data
(20,000 documents) extracted from 20 Aug to 19
Sept is used as training data indicating positive
and unlabeled documents. We set the range of ?
from 0.1 to 0.9 to create a wide range of scenar-
ios, where ? refers to the ratio of documents from
the positive class first selected from a fold as the
positive set. The rest of the positive and negative
documents are used as unlabeled data. We used
categories assigned to more than 100 documents
in the training data as it is necessary to examine
a wide range of ? values. These categories are 88
in all. The data from 20 Sept to 19 Nov is used
2We set the number of negative documents extracted ran-
domly from the unlabeled documents to the same number of
positive training documents.
3We set the number of m to 100 in the experiments.
as a test set X, to estimate true output distribu-
tion. The remaining data consisting 607,259 from
20 Nov 1996 to 19 Aug 1997 is used as a test data
for text classification. We obtained a vocabulary
of 320,935 unique words after eliminating words
which occur only once, stemming by a part-of-
speech tagger (Schmid, 1995), and stop word re-
moval. The number of categories per documents is
3.21 on average. We used the SVM-Light package
(Joachims, 1998)4. We used a linear kernel and set
all parameters to their default values.
We compared our method, MCDC with three
baselines: (1) SVM, (2) Positive Example-Based
Learning (PEBL) proposed by (Yu et al, 2002),
and (3) biased-SVM (Liu et al, 2003). We chose
PEBL because the convergence procedure is very
similar to our framework. Biased-SVM is the
state-of-the-art SVM method, and often used for
comparison (Elkan and Noto, 2008). To make
comparisons fair, all methods were based on a lin-
ear kernel. We randomly selected 1,000 positive
and 1,000 negative documents classified by SVM
and added to the SVM training data in each itera-
tion5. For biased-SVM, we used training data and
classified test documents directly. We empirically
selected values of two parameters, ?c? (trade-off
between training error and margin) and ?j?, i.e.,
cost (cost-factor, by which training errors on posi-
tive examples) that optimized the F-score obtained
by classification of test documents.
The positive training data in SVM are assigned
to the target category. The negative training data
are the remaining data except for the documents
that were assigned to the target category, i.e., this
is the ideal method as we used all the training data
with positive/negative labeled documents. The
number of positive training data in other three
methods depends on the value of ?, and the rest
of the positive and negative documents were used
as unlabeled data.
3.2 Text classification
Classification results for 88 categories are shown
in Figure 3. Figure 3 shows micro-averaged F-
score against the ? value. As expected, the re-
sults obtained by SVM were the best among all
? values. However, this is the ideal method
that requires 20,000 documents labeled posi-
tive/negative, while other methods including our
4http://svmlight.joachims.org
5We set the number of documents up to 1,000.
476
SVM PEBL Biased-SVM MCDC
Level (# of Cat) Cat F Cat F (Iter) Cat F (Iter) Cat F (Iter)
Best GSPO .955 GSPO .802 (26) CCAT .939 GSPO .946 (9)
Top (22) Worst GODD .099 GODD .079 (6) GODD .038 GODD .104 (4)
Avg .800 .475 (19) .593 .619 (8)
Best M14 .870 E71 .848 (7) M14 .869 M14 .875 (9)
Second (32) Worst C16 .297 E14 .161 (14) C16 .148 C16 .150 (3)
Avg .667 .383 (22) .588 .593 (7)
Best M141 .878 C174 .792 (27) M141 .887 M141 .885 (8)
Third (33) Worst G152 .102 C331 .179 (16) G155 .130 C331 .142 (6)
Avg .717 .313 (18) .518 .557 (8)
Fourth (1) ? C1511 .738 C1511 .481 (16) C1511 .737 C1511 .719 (4)
Micro Avg F-score .718 .428 (19) .614 .627 (8)
Table 1: Classification performance (? = 0.7)
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
F-
sc
or
e
Delta Value
SVM
PEBL
Biased-SVM
MCDC
Figure 3: F-score against the value of ?
method used only positive and unlabeled docu-
ments. Overall performance obtained by MCDC
was better for those obtained by PEBL and biased-
SVM methods in all ? values, especially when the
positive set was small, e.g., ? = 0.3, the improve-
ment of MCDC over biased-SVM and PEBL was
significant.
Table 1 shows the results obtained by each
method with a ? value of 0.7. ?Level? indi-
cates each level of the hierarchy and the numbers
in parentheses refer to the number of categories.
?Best? and ?Worst? refer to the best and the low-
est F-scores in each level of a hierarchy, respec-
tively. ?Iter? in PEBL indicates the number of it-
erations until the number of negative documents
is zero in the convergence procedure. Similarly,
?Iter? in the MCDC indicates the number of it-
erations until no unlabeled documents are judged
to be either positive or negative. As can be seen
clearly from Table 1, the results with MCDC were
better than those obtained by PEBL in each level
of the hierarchy. Similarly, the results were bet-
? SV Ec Err Correct
Prec Rec F
0.3 227,547 54,943 79,329 .693 .649 .670
0.7 141,087 34,944 42,385 .712 .673 .692
Table 2: Miss-classified data correction results
ter than those of biased-SVM except for the fourth
level, ?C1511?(Annual results). The average num-
bers of iterations with MCDC and PEBL were 8
and 19 times, respectively. In biased-SVM, it is
necessary to run SVM many times, as we searched
?c? and ?j?. In contrast, MCDC does not require
such parameter tuning.
3.3 Correction of misclassified candidates
Our goal is to achieve classification accuracy from
only positive documents and unlabeled data as
high as that from labeled positive and negative
data. We thus applied a miss-classified data de-
tection and correction technique for the classifica-
tion results obtained by SVM. Therefore, it is im-
portant to examine the accuracy of miss-classified
correction. Table 2 shows detection and correction
performance against all categories. ?SV? shows
the total number of SVs in 88 categories in all iter-
ations. ?Ec? refers to the total number of extracted
error candidates. ?Err? denotes the number of doc-
uments classified incorrectly by SVM and added
to the training data, i.e., the number of documents
that should be assigned correctly by the correction
procedure. ?Prec? and ?Rec? show the precision
and recall of correction, respectively.
Table 2 shows that precision was better than re-
call with both ? values, as the precision obtained
by ? value = 0.3 and 0.7 were 4.4% and 3.9%
improvement against recall values, respectively.
These observations indicated that the error candi-
dates extracted by our method were appropriately
477
corrected. In contrast, there were still other doc-
uments that were miss-classified but not extracted
as error candidates. We extracted error candidates
using the results of SVM and NB classifiers. En-
semble of other techniques such as boosting and
kNN for further efficacy gains seems promising to
try with our method.
4 Conclusion
The research described in this paper involved text
classification using positive and unlabeled data.
Miss-classified data detection and correction tech-
nique was incorporated in the existing classifica-
tion technique. The results using the 1996 Reuters
corpora showed that the method was comparable
to the current state-of-the-art biased-SVM method
as the F-score obtained by our method was 0.627
and biased-SVM was 0.614. Future work will in-
clude feature reduction and investigation of other
classification algorithms to obtain further advan-
tages in efficiency and efficacy in manipulating
real-world large corpora.
References
S. Abney, R. E. Schapire, and Y. Singer. 1999. Boost-
ing Applied to Tagging and PP Attachment. In Proc.
of the Joint SIGDAT Conference on EMNLP and
Very Large Corpora, pages 38?45.
L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos.
2013. Fast and Reliable Anomaly Detection in Cate-
gorical Data. In Proc. of the CIKM, pages 415?424.
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2001. Learning from Labeled and Unlabeled Data
using Graph Mincuts. In Proc. of the 18th ICML,
pages 19?26.
A. Boyd, M. Dickinson, and D. Meurers. 2008. On
Detecting Errors in Dependency Treebanks. Re-
search on Language and Computation, 6(2):113?
137.
R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V.
Pineda. 2009. Using the Web as Corpus for
Self-Training Text Categorization. Information Re-
trieval, 12(3):400?415.
M. Dickinson and W. D. Meurers. 2005. Detecting
Errors in Discontinuous Structural Annotation. In
Proc. of the ACL?05, pages 322?329.
C. Elkan and K. Noto. 2008. Learning Classifiers from
Only Positive and Unlabeled Data. In Proc. of the
KDD?08, pages 213?220.
E. Eskin. 2000. Detectiong Errors within a Corpus us-
ing Anomaly Detection. In Proc. of the 6th ANLP
Conference and the 1st Meeting of the NAACL,
pages 148?153.
C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active
Learning and Experimental Design with SVMs. In
Proc. of the JMLRWorkshop on Active Learning and
Experimental Design, pages 71?84.
T. Joachims. 1998. SVM Light Support Vector Ma-
chine. In Dept. of Computer Science Cornell Uni-
versity.
B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003.
Building Text Classifiers using Positive and Unla-
beled Examples. In Proc. of the ICDM?03, pages
179?188.
A. K. McCallum. 2001. Multi-label Text Classifica-
tion with a Mixture Model Trained by EM. In Re-
vised Version of Paper Appearing in AAAI?99 Work-
shop on Text Learning, pages 135?168.
Reuters. 2000. Reuters Corpus Volume1 English Lan-
guage. 1996-08-20 to 1997-08-19 Release Date
2000-11-03 Format Version 1.
N. Roy and A. K. McCallum. 2001. Toward Optimal
Active Learning through Sampling Estimation of Er-
ror Reduction. In Proc. of the 18th ICML, pages
441?448.
H. Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. In Proc. of
the EACL SIGDAT Workshop, pages 47?50.
H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Pos-
itive Example based Learning for Web Page Classi-
fication using SVM. In Proc. of the ACM Special
Interest Group on Knowledge Discovery and Data
Mining, pages 239?248.
478
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 241?246,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Detection of Topic and its Extrinsic Evaluation Through Multi-Document
Summarization
Yoshimi Suzuki
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi
Kofu, 400-8511, JAPAN
ysuzuki@yamanashi.ac.jp
Fumiyo Fukumoto
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi
Kofu, 400-8511, JAPAN
fukumoto@yamanashi.ac.jp
Abstract
This paper presents a method for detect-
ing words related to a topic (we call them
topic words) over time in the stream of
documents. Topic words are widely dis-
tributed in the stream of documents, and
sometimes they frequently appear in the
documents, and sometimes not. We pro-
pose a method to reinforce topic words
with low frequencies by collecting docu-
ments from the corpus, and applied Latent
Dirichlet Allocation (Blei et al, 2003) to
these documents. For the results of LDA,
we identified topic words by using Mov-
ing Average Convergence Divergence. In
order to evaluate the method, we applied
the results of topic detection to extractive
multi-document summarization. The re-
sults showed that the method was effective
for sentence selection in summarization.
1 Introduction
As the volume of online documents has drastically
increased, the analysis of topic bursts, topic drift
or detection of topic is a practical problem attract-
ing more and more attention (Allan et al, 1998;
Swan and Allan, 2000; Allan, 2003; Klinken-
berg, 2004; Lazarescu et al, 2004; Folino et al,
2007). The earliest known approach is the work
of Klinkenberg and Joachims (Klinkenberg and
Joachims, 2000). They have attempted to han-
dle concept changes by focusing a window with
documents sufficiently close to the target concept.
Mane et. al. proposed a method to generate
maps that support the identification of major re-
search topics and trends (Mane and Borner, 2004).
The method used Kleinberg?s burst detection al-
gorithm, co-occurrences of words, and graph lay-
out technique. Scholz et. al. have attempted to
use different ensembles obtained by training sev-
eral data streams to detect concept drift (Scholz,
2007). However the ensemble method itself re-
mains a problem that how to manage several clas-
sifiers effectively. He and Parket attempted to find
bursts, periods of elevated occurrence of events as
a dynamic phenomenon instead of focusing on ar-
rival rates (He and Parker, 2010). However, the
fact that topics are widely distributed in the stream
of documents, and sometimes they frequently ap-
pear in the documents, and sometimes not often
hamper such attempts.
This paper proposes a method for detecting
topic over time in series of documents. We rein-
forced words related to a topic with low frequen-
cies by collecting documents from the corpus, and
applied Latent Dirichlet Allocation (LDA) (Blei
et al, 2003) to these documents in order to ex-
tract topic candidates. For the results of LDA, we
applied Moving Average Convergence Divergence
(MACD) to find topic words while He et. al., ap-
plied it to find bursts. The MACD is a technique
to analyze stock market trends (Murphy, 1999). It
shows the relationship between two moving av-
erages of prices modeling bursts as intervals of
topic dynamics, i.e., positive acceleration. Fuku-
moto et. al also applied MACD to find topics.
However, they applied it only to the words with
high frequencies in the documents (Fukumoto et
al., 2013). In contrast, we applied it to the topic
candidates obtained by LDA.
We examined our method by extrinsic evalua-
tion, i.e., we applied the results of topic detection
to extractive multi-document summarization. We
assume that a salient sentence includes words re-
lated to the target topic, and an event of each doc-
uments. Here, an event is something that occurs
at a specific place and time associated with some
specific actions(Allan et al, 1998). We identified
event words by using the traditional tf?idf method
applied to the results of named entities. Each sen-
tence in documents is represented using a vector
of frequency weighted words that can be event
241
or topic words. We used Markov Random Walk
(MRW) to compute the rank scores for the sen-
tences (Page et al, 1998). Finally, we selected a
certain number of sentences according to the rank
score into a summary.
2 Topic Detection
2.1 Extraction of Topic Candidates
LDA presented by (Blei et al, 2003) models
each document as a mixture of topics (we call
it lda topic to discriminate our topic candidates),
and generates a discrete probability distribution
over words for each lda topic. The generative pro-
cess for LDA can be described as follows:
1. For each topic k = 1, ? ? ? , K, generate ?
k
,
multinomial distribution of words specific to
the topic k from a Dirichlet distribution with
parameter ?;
2. For each document d = 1, ? ? ? , D, generate ?
d
,
multinomial distribution of topics specific to
the document d from a Dirichlet distribution
with parameter ?;
3. For each word n = 1, ? ? ? , N
d
in document d;
(a) Generate a topic z
dn
of the nth word
in the document d from the multinomial
distribution ?
d
(b) Generate a word w
dn
, the word associ-
ated with the nth word in document d
from multinomial ?
zdn
Like much previous work on LDA, we used Gibbs
sampling to estimate ? and ?. The sampling prob-
ability for topic z
i
in document d is given by:
P (z
i
| z
\i
,W ) =
(n
v
\i,j
+ ?)(n
d
\i,j
+ ?)
(n
?
\i,j
+ W?)(n
d
\i,?
+ T?)
. (1)
z
\i
refers to a topic set Z, not including the cur-
rent assignment z
i
. n
v
\i,j
is the count of word v
in topic j that does not include the current assign-
ment z
i
, and n?
\i,j
indicates a summation over that
dimension. W refers to a set of documents, and T
denotes the total number of unique topics. After
a sufficient number of sampling iterations, the ap-
proximated posterior can be used to estimate ? and
? by examining the counts of word assignments to
topics and topic occurrences in documents. The
.........
.........
(i) Lda_topic clusters
(ii) Task clusters
topic id0 topic id1
cluster
task1 task3 task2
task2task1
task1doc ....
doc
....
....
task1 task2
topic id2
topic id1
topic id1
topic id0
topic id0
topic id2... ...
....
cluster
Figure 1: Lda topic cluster and task cluster
approximated probability of topic k in the docu-
ment d, ??k
d
, and the assignments word w to topic
k,
?
?
w
k
are given by:
?
?
k
d
=
N
dk
+ ?
N
d
+ ?K
. (2)
?
?
w
k
=
N
kw
+ ?
N
k
+ ?V
. (3)
We used documents prepared by summarization
tasks, NTCIR and DUC data as each task consists
of series of documents with the same topic. We
applied LDA to the set consisting of all documents
in the summarization tasks and documents from
the corpus. We need to estimate the appropriate
number of lda topic.
Let k? be the number of lda topics and d? be
the number of topmost d? documents assigned to
each lda topic. We note that the result obtained
by LDA can be regarded as the two types of clus-
tering result shown in Figure 1: (i) each cluster
corresponds to each lda topic (topic id0, topic id1
? ? ? in Figure 1), and each element of the clusters
is the document in the summarization tasks (task1,
task2, ? ? ? in Figure 1) or from the corpus (doc in
Figure 1), and (ii) each cluster corresponds to the
summarization task and each element of the clus-
ters is the document in the summarization tasks
or the document from the corpus assigned topic
id. For example, DUC2005 consists of 50 tasks.
Therefore the number of different clusters is 50.
We call the former lda topic cluster and the latter
task cluster. We estimated k? and d? by using En-
tropy measure given by:
E = ?
1
log l
?
j
N
j
N
?
i
P (A
i
, C
j
) logP (A
i
, C
j
).(4)
242
l refers to the number of clusters. P (A
i
, C
j
) is a
probability that the elements of the cluster C
j
as-
signed to the correct class A
i
. N denotes the total
number of elements and N
j
shows the total num-
ber of elements assigned to the cluster C
j
. The
value of E ranges from 0 to 1, and the smaller
value of E indicates better result. Let E
topic
and
E
task
are entropy value of lda topic cluster and
task cluster, respectively. We chose the parame-
ters k? and d? whose value of the summation of
E
topic
and E
task
is smallest. For each lda topic,
we extracted words whose probabilities are larger
than zero, and regarded these as topic candidates.
2.2 Topic Detection by MACD
The proposed method does not simply use MACD
to find bursts, but instead determines topic words
in series of documents. Unlike Dynamic Topic
Models (Blei and Lafferty, 2006), it does not as-
sume Gaussian distribution so that it is a natural
way to analyze bursts which depend on the data.
We applied it to extract topic words in series of
documents. MACD histogram defined by Eq. (6)
shows a difference between the MACD and its
moving average. MACD of a variable x
t
is defined
by the difference of n
1
-day and n
2
-day moving
averages, MACD(n
1
,n
2
) = EMA(n
1
) - EMA(n
2
).
Here, EMA(n
i
) refers to n
i
-day Exponential Mov-
ing Average (EMA). For a variable x = x(t) which
has a corresponding discrete time series x = {x
t
| t
= 0,1,? ? ? }, the n-day EMA is defined by Eq. (5).
EMA(n)[x]
t
= ?x
t
+ (1? ?)EMA(n? 1)[x]
t?1
=
n
?
k=0
?(1? ?)
k
x
t?k
. (5)
? refers to a smoothing factor and it is often taken
to be 2
(n+1)
. MACD histogram shows a difference
between the MACD and its moving average1.
hist(n
1
, n
2
, n
3
) = MACD(n
1
, n
2
)?
EMA(n
3
)[MACD(n
1
, n
2
)]. (6)
The procedure for topic detection with MACD
is illustrated in Figure 2. Let A be a series of doc-
uments and w be one of the topic candidates ob-
tained by LDA. Each document in A is sorted in
chronological order. We set A to the documents
from the summarization task. Whether or not a
word w is a topic word is judged as follows:
1In the experiment, we set n
1
, n
2
, and n
3
to 4, 8 and 5,
respectively (He and Parker, 2010).
T T
T
Correct histogram Bursts histogram
Histogram similarity
bursts bursts
bursts
Figure 2: Topic detection with MACD
1. Create document-based MACD histogram
where X-axis refers to T , i.e., a period of time
(numbered from day 1 to 365). Y-axis is the
document count in A per day. Hereafter, re-
ferred to as correct histogram.
2. Create term-based MACD histogram where
X-axis refers to T , and Y-axis denotes bursts
of word w in A. Hereafter, referred to as
bursts histogram.
3. We assume that if a term w is informative
for summarizing a particular documents in
a collection, its burstiness approximates the
burstiness of documents in the collection.
Because w is a representative word of each
document in the task. Based on this assump-
tion, we computed similarity between correct
and word histograms by using KL-distance2.
Let P and Q be a normalized distance of
correct histogram, and bursts histogram, re-
spectively. KL-distance is defined by D(P ||
Q) =
?
i=1
P (x
i
) log
P (x
i
)
Q(x
i
)
where x
i
refers
bursts in time i. If the value of D(P || Q)
is smaller than a certain threshold value, w is
regarded as a topic word.
3 Extrinsic Evaluation to Summarization
3.1 Event detection
An event word is something that occurs at a spe-
cific place and time associated with some spe-
cific actions (Allan, 2003; Allan et al, 1998). It
refers to notions of who(person), where(place),
2We tested KL-distance, histogram intersection and Bhat-
tacharyya distance to obtain similarities. We reported only
the result obtained by KL-distance as it was the best results
among them.
243
when(time) including what, why and how in a doc-
ument. Therefore, we can assume that named en-
tities(NE) are linguistic features for event detec-
tion. An event word refers to the theme of the
document itself, and frequently appears in the doc-
ument but not frequently appear in other docu-
ments. Therefore, we first applied NE recogni-
tion to the target documents to be summarized, and
then calculated tf?idf to the results of NE recogni-
tion. We extracted words whose tf?idf values are
larger than a certain threshold value, and regarded
these as event words.
3.2 Sentence extraction
We recall that our hypothesis about key sentences
in multiple documents is that they include topic
and event words. Each sentence in the docu-
ments is represented using a vector of frequency
weighted words that can be event or topic words.
Like much previous work on extractive sum-
marization (Erkan and Radev, 2004; Mihalcea
and Tarau, 2005; Wan and Yang, 2008), we used
Markov Random Walk (MRW) model to compute
the rank scores for the sentences. Given a set
of documents to be summarized, G = (S, E) is
a graph reflecting the relationships between two
sentences. S is a set of vertices, and each vertex
s
i
in S is a sentence. E is a set of edges, and each
edge e
ij
in E is associated with an affinity weight
f(i ? j) between sentences s
i
and s
j
(i 6= j). The
affinity weight is computed using cosine measure
between the two sentences, s
i
and s
j
. Two ver-
tices are connected if their affinity weight is larger
than 0 and we let f(i ? i)= 0 to avoid self tran-
sition. The transition probability from s
i
to s
j
is
then defined as follows:
p(i ? j) =
?
?
?
?
?
?
?
f(i?j)
|S|
?
k=1
f(i?k)
, if ?f 6= 0
0 , otherwise.
(7)
We used the row-normalized matrix U
ij
=
(U
ij
)
|S|?|S|
to describe G with each entry corre-
sponding to the transition probability, where U
ij
=
p(i ? j). To make U a stochastic matrix, the rows
with all zero elements are replaced by a smoothing
vector with all elements set to 1
|S|
. The final transi-
tion matrix is given by formula (8), and each score
of the sentence is obtained by the principal eigen-
vector of the matrix M .
M = ?U
T
+
(1? ?)
| S |
~e~e
T
. (8)
We selected a certain number of sentences accord-
ing to rank score into the summary.
4 Experiments
4.1 Experimental settings
We applied the results of topic detection to ex-
tractive multi-document summarization task, and
examined how the results of topic detection af-
fect the overall performance of the salient sen-
tence selection. We used two tasks, Japanese and
English summarization tasks, NTCIR-33 SUMM
Japanese and DUC4 English data. The baselines
are (i) MRW model (MRW): The method ap-
plies the MRW model only to the sentences con-
sisted of noun words, (ii) Event detection (Event):
The method applies the MRW model to the result
of event detection, (iii) Topic Detection by LDA
(LDA): MRW is applied to the result of topic can-
didates detection by LDA and (iv) Topic Detec-
tion by LDA and MACD (LDA & MACD): MRW
is applied to the result of topic detection by LDA
and MACD only, i.e., the method does not include
event detection.
4.2 NTCIR data
The data used in the NTCIR-3 multi-document
summarization task is selected from 1998 to 1999
of Mainichi Japanese Newspaper documents. The
gold standard data provided to human judges con-
sists of FBFREE DryRun and FormalRun. Each
data consists of 30 tasks. There are two types of
correct summary according to the character length,
?long? and ?short?, All series of documents were
tagged by CaboCha (Kudo and Matsumoto, 2003).
We used person name, organization, place and
proper name extracted from NE recognition (Kudo
and Matsumoto, 2003) for event detection, and
noun words including named entities for topic de-
tection. FBFREE DryRun data is used to tuning
parameters, i.e., the number of extracted words ac-
cording to the tf?idf value, and the threshold value
of KL-distance. The size that optimized the aver-
age Rouge-1(R-1) score across 30 tasks was cho-
sen. As a result, we set tf?idf and KL-distance to
100 and 0.104, respectively.
We used FormalRun as a test data, and another
set consisted of 218,724 documents from 1998 to
1999 of Mainichi newspaper as a corpus used in
3http://research.nii.ac.jp/ntcir/
4http://duc.nist.gov/pubs.html
244
????
????
????
????
????
????
????
??? ??? ??? ??? ??? ??? ??? ???
???????????? ????????????
???????????? ????????????
???????????? ????????????
???????????? ????????????
Number of documents
Entropy
Figure 3: Entropy against the # of topics and doc-
uments
Method Short Long
R-1 R-1
MRW .369 .454
Event .625 .724
LDA .525 .712
LDA & MACD .630 .742
Event & Topic .678 .744
Table 1: Sentence Extraction (NTCIR-3 test data)
LDA and MACD. We estimated the number of k?
and d? in LDA, i.e., we searched k? and d? in steps
of 100 from 200 to 900. Figure 3 illustrates en-
tropy value against the number of topics k? and
documents d? using 30 tasks of FormalRun data.
Each plot shows that at least one of the docu-
ments for each summarization task is included in
the cluster. We can see from Figure 3 that the
value of entropy depends on the number of doc-
uments rather than the number of topics. From
the result shown in Figure 3, the minimum entropy
value was 0.025 and the number of topics and doc-
uments were 400 and 300, respectively. We used
them in the experiment. The summarization re-
sults are shown in Table 1.
Table 1 shows that our approach, ?Event &
Topic? outperforms other baselines, regardless of
the summary type (long/short). Topic candidates
include surplus words that are not related to the
topic because the results obtained by ?LDA? were
worse than those obtained by ?LDA & MACD?,
and even worse than ?Event? in both short and
long summary. This shows that integration of
LDA and MACD is effective for topic detection.
4.3 DUC data
We used DUC2005 consisted of 50 tasks for train-
ing, and 50 tasks of DUC2006 data for testing in
order to estimate parameters. We set tf?idf and
Method R-1 Method R-1
MRW .381 Event .407
LDA .402 LDA & MACD .428
Event & Topic .438
PYTHY .426 HybHSum .456
hPAM .412 TTM .447
Table 2: Comparative results (DUC2007 test data)
KL-distance to 80 and 0.9. The minimum en-
tropy value was 0.050 and the number of topics
and documents were 500 and 600, respectively.
45 tasks from DUC2007 were used to evaluate
the performance of the method. All documents
were tagged by Tree Tagger (Schmid, 1995) and
Stanford Named Entity Tagger 5 (Finkel et al,
2005). We used person name, organization and lo-
cation for event detection, and noun words includ-
ing named entities for topic detection. AQUAINT
corpus6 which consists of 1,033,461 documents
are used as a corpus in LDA and MACD. Table
2 shows Rouge-1 against unigrams.
We can see from Table 2 that Rouge-1 obtained
by our approach was also the best compared to the
baselines. Table 2 also shows the performance of
other research sites reported by (Celikylmaz and
Hakkani-Tur, 2010). The top site was ?HybH-
Sum? by (Celikylmaz and Hakkani-Tur, 2010).
However, the method is a semi-supervised tech-
nique that needs a tagged training data. Our ap-
proach achieves performance approaching the top-
performing unsupervised method, ?TTM? (Ce-
likylmaz and Hakkani-Tur, 2011), and is compet-
itive to ?PYTHY? (Toutanoval et al, 2007) and
?hPAM? (Li and McCallum, 2006). Prior work
including ?TTM? has demonstrated the usefulness
of semantic concepts for extracting salient sen-
tences. For future work, we should be able to
obtain further advantages in efficacy in our topic
detection and summarization approach by disam-
biguating topic senses.
5 Conclusion
The research described in this paper explores a
method for detecting topic words over time in se-
ries of documents. The results of extrinsic evalu-
ation showed that integration of LDA and MACD
is effective for topic detection.
5http://nlp.stanford.edu/software/CRF-NER.shtml
6http://catalog.ldc.upenn.edu/LDC2002T31
245
References
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic Detection and Tracking Pilot
Study Final Report. In Proc. of the DARPA Broad-
cast News Transcription and Understanding Work-
shop.
J. Allan, editor. 2003. Topic Detection and Tracking.
Kluwer Academic Publishers.
D. M. Blei and J. D. Lafferty. 2006. Dynamic Topic
Models. In Proc. of the 23rd International Confer-
ence on Machine Learning, pages 113?120.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. In The Journal of Machine
Learning Research, volume 3, pages 993?1022.
A. Celikylmaz and D. Hakkani-Tur. 2010. A Hy-
bird Hierarchical Model for Multi-Document Sum-
marization. In Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 815?824.
A. Celikylmaz and D. Hakkani-Tur. 2011. Discovery
of Topically Coherent Sentences for Extractive Sum-
marization. In Proc. of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 491?499.
G. Erkan and D. Radev. 2004. LexPageRank: Prestige
in Multi-Document Text Summarization. In Proc. of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 365?371.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 363?370.
G. Folino, C. Pizzuti, and G. Spezzano. 2007. An
Adaptive Distributed Ensemble Approach to Mine
Concept-Drifting Data Streams. In Proc. of the 19th
IEEE International Conference on Tools with Artifi-
cial Intelligence, pages 183?188.
F. Fukumoto, Y. Suzuki, A. Takasu, and S. Matsuyoshi.
2013. Multi-document summarization based on
event and topic detection. In Proc. of the 6th Lan-
guage and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics, pages 117?121.
D. He and D. S. Parker. 2010. Topic Dynamics: An
Alternative Model of Bursts in Streams of Topics.
In Proc. of the 16th ACM Special Interest Group on
Knowledge Discovery and Data Mining, pages 443?
452.
R. Klinkenberg and T. Joachims. 2000. Detecting
Concept Drift with Support Vector Machines. In
Proc. of the 17th International Conference on Ma-
chine Learning, pages 487?494.
R. Klinkenberg. 2004. Learning Drifting Concepts:
Example Selection vs. Example Weighting. Intel-
leginet Data Analysis, 8(3):281?300.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proc. of 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 24?31.
M. M. Lazarescu, S. Venkatesh, and H. H. Bui. 2004.
Using Multiple Windows to Track Concept Drift.
Intelligent Data Analysis, 8(1):29?59.
W. Li and A. McCallum. 2006. Pachinko Alloca-
tion: Dag-Structure Mixture Model of Topic Cor-
relations. In Proc. of the 23rd International Confer-
ence on Machine Learning, pages 577?584.
K. Mane and K. Borner. 2004. Mapping Topics
and Topic Bursts in PNAS. Proc. of the National
Academy of Sciences of the United States of Amer-
ica, 101:5287?5290.
R. Mihalcea and P. Tarau. 2005. Language Indepen-
dent Extractive Summarization. In In Proc. of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 49?52.
J. Murphy. 1999. Technical Analysis of the Financial
Markets. Prentice Hall.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The Pagerank Citation Ranking: Bringing Order to
the Web. In Technical report, Stanford Digital Li-
braries.
H. Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. In Proc. of
the European chapter of the Association for Compu-
tational Linguistics SIGDAT Workshop.
M. Scholz. 2007. Boosting Classifiers for Drifting
Concepts. Intelligent Data Analysis, 11(1):3?28.
R. Swan and J. Allan. 2000. Automatic Generation
of Overview Timelines. In Proc. of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 38?45.
K. Toutanoval, C. Brockett, M. Gammon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. 2007. The
Phthy Summarization System: Microsoft Research
at DUC. In Proc. of Document Understanding Con-
ference 2007.
X. Wan and J. Yang. 2008. Multi-Document Summa-
rization using Cluster-based Link Analysis. In Proc.
of the 31st Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 299?306.
246
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 98?102,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Eliminating Redundancy by Spectral Relaxation for Multi-Document
Summarization
Fumiyo Fukumoto Akina Sakai Yoshimi Suzuki
Interdisciplinary Graduate School of Medicine and Engineering
University of Yamanashi
{fukumoto, t05kg014, ysuzuki}@yamanashi.ac.jp
Abstract
This paper focuses on redundancy, over-
lapping information in multi-documents,
and presents a method for detecting
salient, key sentences from documents
that discuss the same event. To elimi-
nate redundancy, we used spectral clus-
tering and classified each sentence into
groups, each of which consists of seman-
tically related sentences. Then, we ap-
plied link analysis, the Markov Random
Walk (MRW) Model to deciding the im-
portance of a sentence within documents.
The method was tested on the NTCIR
evaluation data, and the result shows the
effectiveness of the method.
1 Introduction
With the exponential growth of information on the
Internet, it is becoming increasingly difficult for a
user to read and understand all the materials from
a series of large-scale document streams that is po-
tentially of interest. Multi-document summariza-
tion is an issue to attack the problem. It differs
from single document summarization in that it is
important to identify differences and similarities
across documents. Graph-based ranking methods,
such as PageRank (Page et al, 1998) and HITS
(Kleinberg, 1999) have recently applied and been
successfully used for multi-document summariza-
tion (Erkan and Radev, 2004; Mihalcea and Tarau,
2005). Given a set of documents, the model con-
structs graph consisting vertices and edges where
vertices are sentences and edges reflect the rela-
tionships between sentences. The model then ap-
plies a graph-based ranking method to obtain the
rank scores for the sentences. Finally, the sen-
tences with large rank scores are chosen into the
summary. However, when they are strung to-
gether, the resulting summary still contains much
overlapping information. Because all the sen-
tences are ranked based on a sentence as unit of
information. Therefore, for example, semanti-
cally related two sentences with ?high recommen-
dation? are ranked with high score, and thus are
regarded as a summary sentence. To attack the
problem, Wan et al proposed two models, i.e., the
Cluster-based conditional Markov Random Walk
model and the Cluster-based HITS model, both
make use of the theme clusters in the document set
(Wan and Yang, 2008). Their model first groups
documents into theme clusters by using a simple
clustering method, k-means. Next, the model con-
structs a directed or undirected graph to reflect the
relationships between sentences and clusters by
using link analysis. They reported that the results
on the DUC2001 and DUC2002 datasets showed
the effectiveness of their models. However, one
of the problems using multivariate clustering such
as k-means is that it is something of a black art
when applied to high-dimensional data. The avail-
able techniques for searching this large space do
not offer guarantees of global optimality, thus the
resulting summary still contains much overlapping
information, especially for a large amount of doc-
uments.
This paper focuses extractive summarization,
and present a method for detecting key sentences
from documents that discuss the same event. Like
Wan et al?s approach, we applied link analysis,
the Markov Random Walk (MRW) model (Bre-
maud, 1999) to a graph consisting sentences and
clusters. To attack the problem dealing with the
high dimensional spaces, we applied spectral clus-
tering technique (Ng et al, 2002) to the sentences
from a document set. Spectral clustering is a trans-
formation of the original sentences into a set of or-
thogonal eigenvectors. We worked in the space de-
fined by the first few eigenvectors, using standard
clustering techniques in the transformed space.
98
2 Spectral Clustering
Similar to other clustering algorithms, the spec-
tral clustering takes as input a matrix formed from
a pairwise similarity function over a set of data
points. Given a set of points S = {s
1
, ? ? ? , s
n
}
in a high dimensional space, the algorithm is as
follows:
1. Form a distance matrix D ? R2. We used
cosine similarity as a distance measure.
2. D is transformed to an affinity matrix A
ij
.
A
ij
=
{
exp(?
D
2
ij
?
2
), if i 6= j
0, otherwise.
?2 is a parameter and controls the rate at
which affinity drops off with distance.
3. The matrix L = D?1/2AD?1/2 is created. D
is a diagonal matrix whose (i,i) element is the
sum of A?s i-th row.
4. The eigenvectors and eigenvalues of L are
computed, and a new matrix is created from
the vectors associated with the number of l
largest eigenvalues.
5. Each item now has a vector of l coordinates
in the transformed space. These vectors are
normalized to unit length.
6. K-means is applied to S in the l-dimensional
space.
3 Cluster-based Link Analysis
The link analysis we used is an approach presented
by Wan et. al (Wan and Yang, 2008). The model
called ?Cluster-based Conditional Markov Ran-
dom Walk Model? incorporates the cluster-level
information into the process of sentence rank-
ing. The model is summarized as follows: Let
?(clus(s
i
)) ? [0, 1] be the importance of clus-
ter clus(s
i
) in the whole document set D. Let
also ?(s
i
, clus(s
i
)) ? [0, 1] denote the strength of
the correlation between sentence s
i
and its cluster
clus(s
i
). clus(s
i
) refers to the cluster containing
sentence s
i
. The transition probability from s
i
to
s
j
is defined by formula (1).
p(i?j|clus(si), clus(sj))
=
8
>
>
<
>
>
:
f(i?j|clus(si), clus(sj))
|S|
X
k=1
f(i?k|clus(si), clus(sk))
, if ?f 6= 0
0 , otherwise.
(1)
f(i ? j | clus(s
i
), clus(s
j
)) in formula (1) refers
to the weight between two sentences s
i
and s
j
,
conditioned on the two clusters containing the two
sentences, and defined by formula (2).
f(i?j|clus(si), clus(sj))
= f(i?j)?{???(clus(si))??(clus(si))
+(1 ? ?)??(clus(sj))??(clus(sj))} (2)
? ? [0, 1] in formula (2) is the combination
weight controlling the relative contributions from
the source cluster and the destination cluster.
?(clus(s
i
)) denotes the value indicating the im-
portance of the cluster clus(s
i
) in the document
set D. Similarly, ?(s
i
, clus(s
i
)) refers to the sim-
ilarity value between the sentence s
i
and its cluster
clus(s
i
). These values are obtained by using the
cosine similarity. The new row-normalized matrix
M is defined by formula (3).
Mij = p(i ? j | clus(si), clus(sj)) (3)
The saliency scores for the sentences are com-
puted based on formula (3) by using the iterative
form in formula (4).
Score(si) = ?
X
allj 6=i
Score(sj) ? Mji +
(1 ? ?)
| S |
(4)
? in formula (4) is the damping factor, which we
set to 0.85. The above process can be considered
as a Markov chain by taking the sentences as the
states and the final transition matrix is given by
formula (5), and each score of the sentences is ob-
tained by the principle eigenvector of the new tran-
sition matrix A.
A = ?M
T
+
(1 ? ?)
| V |
~e~e
T (5)
~e in formula (5) is a column vector with all ele-
ments equal to 1. We selected a certain number
of sentences according to rank score into the sum-
mary.
99
4 Experiments
We had an experiment by using the NTCIR-31
SUMM to evaluate our approach. NTCIR-3 has
two tasks, single, and multi-document summariza-
tion. The data is collected from two years(1998-
1999) Mainichi Japanese Newspaper articles. We
used multi-document summarization task. There
are two types of gold standard data provided to
human judges, FBFREE DryRun and FormalRun,
each of which consists of 30 topics. There are two
types of correct summary according to the charac-
ter length, i.e., ?long? and ?short?. All documents
were tagged by a morphological analysis, ChaSen
(Matsumoto et al, 1997) and noun words are ex-
tracted.
We used FormalRun consisting of 30 topics as a
test data. Similarly, we randomly chose 10 topics
from the FBFREEDryRun data to tuning a param-
eter ? in Spectral Clustering, and the number of l
in the l-dimensional space obtained by the Spec-
tral Clustering. ? is searched in steps of 0.01 from
1.0 to 5.0. l in the l-dimensional space is searched
in steps 10% from 0 to 80% against the total num-
ber of words in the training data. The size that op-
timized the average F-score of 10 topics was cho-
sen. Here, F-score is the standard measure used
in the clustering algorithm, and it combines recall
and precision with an equal weight. Precision is a
ratio of the number of correct pair of sentences ob-
tained by the k-means divided by the total number
of pairs obtained by the k-means. Recall indicates
a ratio of the number of correct pair of sentences
obtained by the k-means divided by the total num-
ber of correct pairs. As a result, ? and l are set to
4.5 and 80%, respectively.
It is difficult to predict the actual cluster number
k in a given input sentences to produce optimal
results. The usual drawback in many clustering
algorithms is that they cannot give a valid criterion
for measuring class structure. Therefore, similar
to Wan et. al?s method (Wan and Yang, 2008), we
typically set the number of k of expected clusters
as
?
N where N is the number of all sentences
in the document set. We used these values of the
parameters and evaluated by using test data.
We used two evaluation measures. One is co-
sine similarity between the generated summary by
the system and the human generated summary.
Another is ROUGE score used in DUC (Liu and
Hovy, 2003).
1http://research.nii.ac.jp/ntcir/
ROUGE =
X
s?Cand
X
ngram?s
Countmatch(ngram)
X
s?Cand
X
ngram?s
Count(ngram)
(6)
We used a word instead of n-gram sequence in
formula (6). The results are shown in Table 1. ?#
of doc? and ?# of sent? refer to the average number
of documents and sentences, respectively. ?# of
sum? denotes to the average number of summary
sentences provided by NTCIR3 SUMM. ?cos? and
?ROUGE? refer to the results evaluated by using
cosine, and ROUGE score, respectively. ?MRW?
indicates the results obtained by directly applying
MRW model to the input sentences.
We can see from Table 1 that our approach
(Spectral) outperforms the baselines, ?MRW?
and ?k-means?, regardless of the types of sum-
mary (long/short) and evaluation measures (co-
sine/ROUGE). The results obtained by three ap-
proaches show that ?short? was better than ?long?.
This indicates that the rank score of correct sen-
tences within the candidate sentences obtained
by the MRW model works well. Comparing
the results evaluated by ?ROUGE? were worse
than those of ?cos? at any approaches. One rea-
son is that the difference of summarization tech-
nique, i.e., our work is extractive summarization,
while the gold standard data provided by NTCIR-
3 SUMM is the abstracts written by human pro-
fessionals. As a result, a large number of words
in a candidate summary are extracted by our ap-
proaches. For future work, it is necessary to ex-
tend our method to involve paraphrasing for ex-
tracted key sentences to reduce the gap between
automatically generated summaries and human-
written abstracts (Barzilay et al, 1993; Carenini
and Cheung, 2008).
It is interesting to note how our approach affects
for the number of sentences as an input. Figure
1 illustrates the results of summary ?long? with
evaluated ROUGE score. We can see from Figure
1 that our approach is more robust than k-means
and the MRW model, even for a large number of
input data. We have seen the same observations
from other three results, i.e., the results of short
and long with evaluated cos and short with evalu-
ated ROUGE.
We recall that the cluster number k is set to the
square root of the sentence number. We tested dif-
ferent number of k to see how the cluster number
100
Table 1: Results against 30 topics
# of doc # of sent # of sum cos ROUGE
MRW k-means Spectral MRW k-means Spectral
Short 7.5 83.0 11.9 0.431 0.575 0.632 0.330 0.334 0.360
Long 20.4 0.371 0.408 0.477 0.180 0.186 0.209
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  50  100  150  200  250  300  350
R
O
U
G
E
# of sentences
k-means
Spectral
MRW
Figure 1: Long with ROUGE vs. # of sentences
affects the summarization performance. In the ex-
periment, we set k = r? | N | where r is a pa-
rameter ranged from 0 to 1 (Wan and Yang, 2008).
Because of space is limited, we report only the re-
sult with summary ?long? and ROUGE score. The
result is shown in Figure 2.
Overall the results obtained by our approach
and k-means outperformed the results obtained
by directly applying MRW model, while the re-
sults by k-means was worse than the results by
MRW model when the ratio of the number of sen-
tences was larger than 0.8. This shows that cluster-
based summarization is effective reduce redun-
dancy, overlapping information. Figure 2 also
shows that our approach always outperforms, re-
gardless of how many number of sentences were
used. This indicates that the MRW model with
spectral clustering is more robust than that with
the baseline, k-means, with respect to the differ-
ent number of clusters.
5 Conclusion
We have developed an approach to detect salient
sentences from documents that discuss the same
 0.175
 0.18
 0.185
 0.19
 0.195
 0.2
 0.205
 0.21
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
A
vg
. R
O
U
G
E
ratio of the # of k
MRW
k-means
Spectral
Figure 2: Long with ROUGE score measure vs. #
of k
event. The results showed the effectiveness of the
method. Future work will include: (i) compar-
ing other approaches that uses link analysis to re-
duce redundancy, such as (Zhu et al, 2007), (ii)
applying the method to the DUC evaluation data
for quantitative evaluation, and (iii) extending the
method to classify sentences into more than one
classes by using soft-clustering techniques such as
EM (Dempster et al, 1977) and fuzzy c-means al-
gorithms (Zhang and Wang, 2007).
References
R. Barzilay, K. R. McKeown, and M. Elhadad.
1993. Information Fusion in the Context of Multi-
document Summarization. In Proc. of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 550?557.
P. Bremaud. 1999. Markov Chains: Gibbs Fields,
Monte Carlo Simulation, and Queues. Springer-
Verlag.
G. Carenini and J. C. K. Cheung. 2008. Extractive
vs. NLG-based Abstractive Summarization of Eval-
uative Text: The Effect of Corpus Controversiality.
101
In Proc. of the 5th International Natural Language
Generation Conference, pages 33?41.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
MaximumLikelihood from Incomplete Data Via the
EM Algorithm. Royal Statistical Society, 39(B):1?
38.
G. Erkan and D. Radev. 2004. LexPageRank: Prestige
in Multi-document Text Summarization. In Proc. of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 365?371.
J. M. Kleinberg. 1999. Authoritative Sources in a Hy-
perlinked Environment. ACM, 46(5):604?632.
C-Y. Liu and E. H. Hovy. 2003. Automatic Evalu-
ation of Summaries Using N-gram Co-occurrence
Statistics. In Proc. of Human Language Technolo-
gies: The Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 71?78.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Haruno,
O. Imaichi, and T. Imamura. 1997. Japanese Mor-
phological Analysis System Chasen Mannual.
R. Mihalcea and P. Tarau. 2005. Language Indepen-
dent Extractive Summarization. In In Proc. of the
43nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 49?52.
A. Y. Ng, M. I. Jordan, and Y. Weiss. 2002. On Spec-
tral Clustering: Analysis and an Algorithm, vol-
ume 14. MIT Press.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The Pagerank Citation Ranking: Bringing Order to
the Web. In Technical report, Stanford Digital Li-
braries.
X. Wan and J. Yang. 2008. Multi-document Sum-
marization Using Cluster-based Link Analysis. In
Proc. of the 31st Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 299?306.
Z. Zhang and R. Wang. 2007. Identification of
Overlapping Community Structure in Complex Net-
works using Fuzzy C-means Clustering. PHYSICA,
A(374):483?490.
X. Zhu, A. Goldberg, J. V. Gael, and D. Andrzejew-
ski. 2007. Improving Diversity in Ranking using
Absorbing Random Walks. In In Human Language
technologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 97?104.
102
