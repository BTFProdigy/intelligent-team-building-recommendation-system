Coling 2010: Poster Volume, pages 1256?1264,
Beijing, August 2010
Extraction of Multi-word Expressions from Small Parallel Corpora
Yulia Tsvetkov
Department of Computer Science
University of Haifa
yulia.tsvetkov@gmail.com
Shuly Wintner
Department of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We present a general methodology for ex-
tracting multi-word expressions (of vari-
ous types), along with their translations,
from small parallel corpora. We auto-
matically align the parallel corpus and fo-
cus on misalignments; these typically in-
dicate expressions in the source language
that are translated to the target in a non-
compositional way. We then use a large
monolingual corpus to rank and filter the
results. Evaluation of the quality of the ex-
traction algorithm reveals significant im-
provements over na??ve alignment-based
methods. External evaluation shows an
improvement in the performance of ma-
chine translation that uses the extracted
dictionary.
1 Introduction
Multi-word Expressions (MWEs) are lexical
items that consist of multiple orthographic words
(e.g., ad hoc, by and large, New York, kick the
bucket). MWEs are numerous and constitute a
significant portion of the lexicon of any natural
language. They are a heterogeneous class of con-
structions with diverse sets of characteristics, dis-
tinguished by their idiosyncratic behavior. Mor-
phologically, some MWEs allow some of their
constituents to freely inflect while restricting (or
preventing) the inflection of other constituents. In
some cases MWEs may allow constituents to un-
dergo non-standard morphological inflections that
they would not undergo in isolation. Syntactically,
some MWEs behave like words while other are
phrases; some occur in one rigid pattern (and a
fixed order), while others permit various syntactic
transformations. Semantically, the compositional-
ity of MWEs is gradual, ranging from fully com-
positional to idiomatic (Bannard et al, 2003).
Because of their prevalence and irregularity,
MWEs must be stored in lexicons of natural lan-
guage processing applications. Handling MWEs
correctly is beneficial for a variety of applications,
including information retrieval, building ontolo-
gies, text alignment, and machine translation.
Identifying MWEs and extracting them from
corpora is therefore both important and difficult.
In Hebrew (which is the subject of our research),
this is even more challenging due to two reasons:
the rich and complex morphology of the language;
and the dearth of existing language resources, in
particular parallel corpora, semantic dictionaries
and syntactic parsers.
We propose a novel algorithm for identifying
MWEs in bilingual corpora, using automatic word
alignment as our main source of information. In
contrast to existing approaches, we do not limit
the search to one-to-many alignments, and pro-
pose an error-mining strategy to detect misalign-
ments in the parallel corpus. We also consult a
large monolingual corpus to rank and filter out
the expressions. The result is fully automatic ex-
traction of MWEs of various types, lengths and
syntactic patterns, along with their translations.
We demonstrate the utility of the methodology on
Hebrew-English MWEs by incorporating the ex-
tracted dictionary into an existing machine trans-
lation system.
The main contribution of the paper is thus a
new alignment-based algorithm for MWE extrac-
tion that focuses on misalignments, augmented by
validating statistics computed from a monolingual
corpus. After discussing related work, we detail in
Section 3 the methodology we propose. Section 4
provides a thorough evaluation of the results. We
then extract translations of the identified MWEs
and evaluate the contribution of the extracted dic-
tionary in Section 5. We conclude with sugges-
tions for future research.
1256
2 Related Work
Early approaches to identifying MWEs concen-
trated on their collocational behavior (Church and
Hanks, 1989). Pecina (2008) compares 55 dif-
ferent association measures in ranking German
Adj-N and PP-Verb collocation candidates. This
work shows that combining different collocation
measures using standard statistical classification
methods improves over using a single collocation
measure. Other results (Chang et al, 2002; Villav-
icencio et al, 2007) suggest that some collocation
measures (especially PMI and Log-likelihood) are
superior to others for identifying MWEs. Soon,
however, it became clear that mere co-occurrence
measurements are not enough to identify MWEs,
and their linguistic properties should be exploited
as well (Piao et al, 2005). Hybrid methods that
combine word statistics with linguistic informa-
tion exploit morphological, syntactic and seman-
tic idiosyncrasies to extract idiomatic MWEs.
Semantic properties of MWEs can be used
to distinguish between compositional and non-
compositional (idiomatic) expressions. Katz and
Giesbrecht (2006) and Baldwin et al (2003) use
Latent Semantic Analysis for this purpose. They
show that compositional MWEs appear in con-
texts more similar to their constituents than non-
compositional MWEs. Van de Cruys and Vil-
lada Moiro?n (2007) use unsupervised learning
methods to identify non-compositional MWEs by
measuring to what extent their constituents can be
substituted by semantically related terms. Such
techniques typically require lexical semantic re-
sources that are unavailable for Hebrew.
An alternative approach to using semantics cap-
italizes on the observation that an expression
whose meaning is non-compositional tends to be
translated into a foreign language in a way that
does not result from a combination of the literal
translations of its component words. Alignment-
based techniques explore to what extent word
alignment in parallel corpora can be used to dis-
tinguish between idiomatic expressions and more
transparent ones. A significant added value of
such works is that MWEs can thus be both iden-
tified in the source language and associated with
their translations in the target language.
Villada Moiro?n and Tiedemann (2006) focus
on Dutch expressions and their English, Spanish
and German translations in the Europarl corpus
(Koehn, 2005). To extract the candidates, they use
syntactic properties (based on full parsing of the
Dutch text) and statistical association measures.
This approach requires syntactic resources that are
unavailable for Hebrew.
Some recent works concentrate on exploit-
ing translational correspondences of MWEs from
(small) parallel corpora. MWE candidates and
their translations are extracted as a by-product of
automatic word alignment of parallel texts. Un-
like Villada Moiro?n and Tiedemann (2006), who
use aligned parallel texts to rank MWE candi-
dates, Caseli et al (2009) actually use them to
extract the candidates. After the texts are word-
aligned, Caseli et al (2009) extract sequences of
length 2 or more in the source language that are
aligned with sequences of length 1 or more in the
target. Candidates are then filtered out of this set if
they comply with pre-defined part-of-speech pat-
terns, or if they are not sufficiently frequent in the
parallel corpus. Even with the most aggressive fil-
tering, precision is below 40% and recall is ex-
tremely low (F-score is below 10 for all experi-
ments). Our setup is similar, but we extract MWE
candidates from the aligned corpus in a very dif-
ferent way; and we use statistics collected from a
monolingual corpus to filter and rank the results.
Zarrie? and Kuhn (2009) also use aligned par-
allel corpora but only focus on one-to-many word
alignments. To restrict the set of candidates, they
focus on specific syntactic patterns as determined
by parsing both sides of the corpus (again, us-
ing resources unavailable to us). The results show
high precision but very low recall.
3 Methodology
We propose an alternative approach to existing
alignment-based techniques for MWE extraction.
Using a small bilingual corpus, we extract MWE
candidates from noisy word alignments in a novel
way. We then use statistics from a large mono-
lingual corpus to rank and filter the list of candi-
dates. Finally, we extract the translation of candi-
date MWEs from the parallel corpus and use them
in a machine translation (MT) system.
1257
3.1 Motivation
Parallel texts are an obvious resource from which
to extract MWEs. By definition, idiomatic ex-
pressions have a non-compositional meaning, and
hence may be translated to a single word (or to
an expression with a different meaning) in a for-
eign language. The underlying assumption of
alignment-based approaches to MWE extraction
is that MWEs are aligned across languages in a
way that differs from compositional expressions;
we share this assumption. However, existing ap-
proaches focus on the results of word alignment
in their quest for MWEs, and in particular con-
sider 1:n and n:m alignments as potential areas
in which to look for MWEs. This is problematic
for two reasons: first, word alignment algorithms
have difficulties aligning MWEs, and hence 1:n
and n:m alignments are often noisy; while these
environments provide cues for identifying MWEs,
they also include much noise. Second, our exper-
imental scenario is such that our parallel corpus is
particularly small, and we cannot fully rely on the
quality of word alignments, but we have a bilin-
gual dictionary that compensates for this limita-
tion. In contrast to existing approaches, then, we
focus on misalignments: we trust the quality of
1:1 alignments, which we verify with the dictio-
nary; and we search for MWEs exactly in the ar-
eas that word alignment failed to properly align,
not relying on the alignment in these cases.
Moreover, in contrast to existing alignment-
based approaches, we also make use of a large
monolingual corpus from which statistics on the
distribution of word sequences in Hebrew are
drawn. This has several benefits: of course, mono-
lingual corpora are easier to obtain than parallel
ones, and hence tend to be larger and provide more
accurate statistics. Furthermore, this provides val-
idation of the MWE candidates that are extracted
from the parallel corpus: rare expressions that
are erroneously produced by the alignment-based
technique can thus be eliminated on account of
their low frequency in the monolingual corpus.
Specifically, we use pointwise mutual informa-
tion (PMI) as our association measure. While PMI
has been proposed as a good measure for identi-
fying MWEs, it is also known not to discriminate
accurately between MWEs and other frequent col-
locations. This is because it promotes collocations
whose constituents rarely occur in isolation (e.g.,
typos and grammar errors), and expressions con-
sisting of some word that is very frequently fol-
lowed by another (e.g., say that). However, such
cases do not have idiomatic meanings, and hence
at least one of their constituents is likely to have a
1:1 alignment in the parallel corpus; we only use
PMI after such alignments have been removed.
An added value of our methodology is the au-
tomatic production of an MWE translation dictio-
nary. Since we start with a parallel corpus, we
can go back to that corpus after MWEs have been
identified, and extract their translations from the
parallel sentences in which they occur.
Finally, alignment-based approaches can be
symmetric, and ours indeed is. While our main
motivation is to extract MWEs in Hebrew, a by-
product of our system is the extraction of English
MWEs, along with their translations to Hebrew.
This, again, contributes to the task of enriching
our existing bilingual dictionary.
3.2 Resources
Our methodology is in principle language-
independent and appropriate for medium-density
languages (Varga et al, 2005). We assume the
following resources: a small bilingual, sentence-
aligned parallel corpus; large monolingual cor-
pora in both languages; morphological processors
(analyzers and disambiguation modules) for the
two languages; and a bilingual dictionary. Our
experimental setup is Hebrew-English. We use
a small parallel corpus (Tsvetkov and Wintner,
2010) consisting of 19,626 sentences, mostly from
newspapers. The corpus consists of 271,787 En-
glish tokens (14,142 types) and 280,508 Hebrew
tokens (12,555 types), and is similar in size to that
used by Caseli et al (2009).
We also use data extracted from two mono-
lingual corpora. For Hebrew, we use the
morphologically-analyzed MILA corpus (Itai and
Wintner, 2008) with part-of-speech tags produced
by Bar-Haim et al (2005). This corpus is much
larger, consisting of 46,239,285 tokens (188,572
types). For English we use Google?s Web 1T cor-
pus (Brants and Franz, 2006).
Finally, we use a bilingual dictionary consist-
1258
ing of 78,313 translation pairs. Some of the en-
tries were collected manually, while others are
produced automatically (Itai and Wintner, 2008;
Kirschenbaum and Wintner, 2010).
3.3 Preprocessing the corpora
Automatic word alignment algorithms are noisy,
and given a small parallel corpus such as ours,
data sparsity is a serious problem. To minimize
the parameter space for the alignment algorithm,
we attempt to reduce language specific differences
by pre-processing the parallel corpus. The impor-
tance of this phase should not be underestimated,
especially for alignment of two radically different
languages such as English and Hebrew (Dejean et
al., 2003).
Hebrew,1 like other Semitic languages, has a
rich, complex and highly productive morphology.
Information pertaining to gender, number, defi-
niteness, person, and tense is reflected morpho-
logically on base forms of words. In addition,
prepositions, conjunctions, articles, possessives,
etc., may be concatenated to word forms as pre-
fixes or suffixes. This results in a very large num-
ber of possible forms per lexeme. We therefore to-
kenize the parallel corpus and then remove punc-
tuation. We analyze the Hebrew corpus morpho-
logically and select the most appropriate analysis
in context. Adopting this selection, the surface
form of each word is reduced to its base form,
and bound morphemes (prefixes and suffixes) are
split to generate stand-alone ?words?. We also to-
kenize and lemmatize the English side of the cor-
pus, using the Natural Language Toolkit package
(Bird et al, 2009).
Then, we remove some language-specific dif-
ferences automatically. We remove frequent func-
tion words: in English, the articles a, an and the,
the infinitival to and the copulas am, is and are; in
Hebrew, the accusative marker at. These forms do
not have direct counterparts in the other language.
For consistency, we pre-process the monolin-
gual corpora in the same way. We then compute
the frequencies of all word bi-grams occurring in
each of the monolingual corpora.
1To facilitate readability we use a transliteration of He-
brew using Roman characters; the letters used, in Hebrew
lexicographic order, are abgdhwzxTiklmns?pcqrs?t.
3.4 Identifying MWE candidates
The motivation for our MWE identification algo-
rithm is the assumption that there may be three
sources to misalignments (anything that is not
a 1:1 word alignment) in parallel texts: either
MWEs (which trigger 1:n or n:m alignments);
or language-specific differences (e.g., the source
language lexically realizes notions that are re-
alized morphologically, syntactically or in some
other way in the target language); or noise (e.g.,
poor translations, low-quality sentence alignment,
and inherent limitations of word alignment algo-
rithms).
This motivation induces the following algo-
rithm. Given a parallel, sentence-aligned corpus,
it is first pre-processed as described above, to re-
duce the effect of language-specific differences.
We then use Giza++ (Och and Ney, 2003) to word-
align the text, employing union to merge the align-
ments in both directions. We look up all 1:1 align-
ments in the dictionary. If the pair exists in our
bilingual dictionary, we remove it from the sen-
tence and replace it with a special symbol, ?*?.
Such word pairs are not parts of MWEs. If the
pair is not in the dictionary, but its alignment score
is very high (above 0.5) and it is sufficiently fre-
quent (more than 5 occurrences), we add the pair
to the dictionary but also retain it in the sentence.
Such pairs are still candidates for being (parts of)
MWEs.
Example 1 Figure 1-a depicts a Hebrew sentence
with its word-by-word gloss, and its English trans-
lation in the parallel corpus. Here, bn adm ?per-
son? is a MWE that cannot be translated literally.
After pre-processing (Section 3.3), the English is
represented as ?and i tell her keep away from per-
son? (note that to and the were deleted). The He-
brew, which is aggressively segmented, is repre-
sented as in Figure 1-b. Note how this reduces the
level of (morphological and orthographic) differ-
ence between the two languages. Consequently,
Giza++ finds the alignment depicted in Figure 1-
c. Once 1:1 alignments are replaced by ?*?, the
alignment of Figure 1-d is obtained.
If our resources were perfect, i.e., if word align-
ment made no errors, the dictionary had perfect
coverage and our corpora induced perfect statis-
1259
a. wamrti lh lhzhr mbn adm kzh
and-I-told to-her to-be-careful from-child man like-this
?and I told her to keep away from the person?
b. w ani amr lh lhzhr m bn adm k zh
and I tell to-her to-be-careful from child man like this
c. w ani amr lh lhzhr m bn adm k zh
and I told her keep away from person {} {}
d. * * * * lhzhr * bn adm k zh
* * * * keep away * person
Figure 1: Example sentence pair (a); after pre-processing (b); after word alignment (c); and after 1:1
alignments are replaced by ?*? (d)
tics, then all remaining text (other than the spe-
cial symbol) in the parallel text would be part of
MWEs. In other words, all sequences of remain-
ing source words, separated by ?*?, are MWE can-
didates. As our resources are far from perfect, fur-
ther processing is required in order to prune these
candidates. For this, we use association measures
computed from the monolingual corpus.
3.5 Ranking and filtering MWE candidates
The algorithm described above produces se-
quences of Hebrew word forms (free and bound
morphemes produced by the pre-processing stage)
that are not 1:1-aligned, separated by ?*?s. Each
such sequence is a MWE candidate. In order to
rank the candidates we use statistics from a large
monolingual corpus. We do not rely on the align-
ments produced by Giza++ in this stage.
We extract all word bi-grams from the remain-
ing candidates. Each bi-gram is associated with its
PMI-based score,2 computed from the monolin-
gual corpus. Interestingly, about 20,000 candidate
MWEs are removed in this stage because they do
not occur at all in the monolingual corpus.
We then experimentally determine a threshold
(see Section 4). A word sequence of any length
is considered MWE if all the adjacent bi-grams it
2PMIk is a heuristic variant of the PMI measure, pro-
posed and studied by Daille (1994), where k, the exponent, is
a frequency-related factor, used to demote collocations with
low-frequency constituents. The value of the parameter k can
be chosen freely (k > 0) in order to tune the properties of the
PMI to the needs of specific applications. We conducted ex-
periments with k = 0, 0.1, ... , 3 and found k = 2.7 to give the
best results for our application.
contains score above the threshold. Finally, we
restore the original forms of the Hebrew words
in the candidates, combining together bound mor-
phemes that were split during pre-processing; and
we restore the function words. Many of the
candidate MWEs produced in the previous stage
are eliminated now, since they are not genuinely
multi-word in the original form.
Example 2 Refer back to Figure 1-d. The se-
quence bn adm k zh is a MWE candidate. Two
bi-grams in this sequence score above the thresh-
old: bn adm, which is indeed a MWE, and k zh,
which is converted to the original form kzh and is
hence not considered a candidate. We also con-
sider adm k, whose score is low. Note that the
same aligned sentence can be used to induce the
English MWE keep away, which is aligned to a
single Hebrew word.
3.6 Results
As an example of the results obtained with this
setup, we list in Table 1 the 15 top-ranking ex-
tracted MWEs. For each instance we list an indi-
cation of the type of MWE: person name (PN),
geographical term (GT), noun-noun compound
(NNC) or noun-adjective combination (N-ADJ).
Of the top 100 candidates, 99 are clearly MWEs,3
including mzg awir (temper-of air) ?weather?,
kmw kn (like thus) ?furthermore?, bit spr (house-
of book ) ?school?, s?dh t?wph (field-of flying)
?airport?, ts?wmt lb (input-of heart) ?attention?,
ai aps?r (not possible) ?impossible? and b?l ph
3This was determined by two annotators.
1260
(in-on mouth ) ?orally?. Longer MWEs include
ba lidi biTwi (came to-the-hands-of expression)
?was expressed?; xzr ?l ?cmw (returned on itself )
?recurred?; ixd ?m zat (together with it) ?in ad-
dition?; and h?crt hkllit s?l haw?m (the general as-
sembly of the UN ) ?the UN general assembly?.
Hebrew Gloss Type
xbr hknst MP NNC
tl abib Tel Aviv GT
gws? qTip Gush Katif NNC-GT
awpir pins Ophir Pines PN
hc?t xwq Legislation NNC
axmd Tibi Ahmad Tibi PN
zhwh glawn Zehava Galon PN
ras? hmms?lh Prime Minister NNC
abs?lwm wiln Avshalom Vilan PN
br awn Bar On PN
mair s?Trit Meir Shitrit PN
limwr libnt Limor Livnat PN
hiw?c hms?pTi Attorney General N-ADJ
twdh rbh thanks a lot N-ADJ
rcw?t ?zh Gaza Strip NNC-GT
Table 1: Results: extracted MWEs
4 Evaluation
MWEs are notoriously hard to define, and no
clear-cut criteria exist to distinguish between
MWEs and other frequent collocations. In order
to evaluate the utility of our methodology, we con-
ducted three different types of evaluations that we
detail below and in Section 5.
First, we use a small annotated corpus of
Hebrew noun-noun constructions that was made
available to us (Al-Haj and Wintner, 2010). The
corpus consists of 463 high-frequency bi-grams of
the same syntactic construction; of those, 202 are
tagged as MWEs (in this case, noun compounds)
and 258 as non-MWEs. This corpus consolidates
the annotation of three annotators: only instances
on which all three agreed were included. Since it
includes both positive and negative instances, this
corpus facilitates a robust evaluation of precision
and recall. Of the 202 positive examples, only 121
occur in our parallel corpus; of the 258 negative
examples, 91 occur in our corpus. We therefore
limit the discussion to those 212 examples whose
MWE status we can determine, and ignore other
results produced by the algorithm we evaluate.
On this corpus, we compare the performance
of our algorithm to four baselines: using only
PMI to rank the bi-grams in the parallel cor-
pus; using PMI computed from the monolingual
corpus to rank the bi-grams in the parallel cor-
pus; and using Giza++ 1:n alignments, ranked
by their PMI (with bi-grams statistics computed
once from parallel and once from monolingual
corpora). ?MWE? refers to our algorithm. For
each of the above methods, we set the threshold
at various points, and count the number of true
MWEs above the threshold (true positives) and the
number of non-MWEs above the threshold (false
positives), as well as the number of MWEs and
non-MWEs below the threshold (false positives
and true negatives, respectively). From these four
figures we compute precision, recall and their har-
monic mean, f -score, which we plot against (the
number of results above) the threshold in Figure 2.
Clearly, the performance of our algorithm is con-
sistently above the baselines.
Second, we evaluate the algorithm on addi-
tional datasets. We compiled three small corpora
of Hebrew two-word MWEs. The first corpus,
PN, contains 785 person names (names of Knesset
members and journalists), of which 157 occur in
the parallel corpus. The second, Phrases, consists
of 571 entries beginning with the letter x from a
dictionary of Hebrew phrases (Rosenthal, 2009),
and a set of 331 idioms we collected from internet
resources. Of those, 154 occur in the corpus. The
third set, NN, consists of the positive examples in
the annotated corpus of noun-noun constructions
described above.
Since we do not have negative examples for
these sets, we only evaluate recall, using a thresh-
old reflecting 2750 results. For each of these
datasets, we report the number of MWEs in the
dataset (which also occur in the parallel corpus,
of course) our algorithm detected. We compare
in Table 2 the recall of our method (MWE) to
Giza++ alignments, as above, and list also the
upper bound (UB), obtained by taking all above-
threshold bi-grams in the corpus.
1261
Figure 2: Evaluation results compared with baselines: noun-noun compounds
Method PN Phrases NN
# % # % # %
UB 74 100 40 100 89 100
MWE 66 89.2 35 87.5 67 75.3
Giza 7 9.5 33 82.5 37 41.6
Table 2: Recall evaluation
5 Extraction of MWE translations
An obvious benefit of using parallel corpora for
MWE extraction is that the translations of ex-
tracted MWEs are available in the corpus. We use
a na??ve approach to identify these translations. For
each MWE in the source-language sentence, we
consider as translation all the words in the target-
language sentence (in their original order) that
are aligned to the word constituents of the MWE,
as long as they form a contiguous string. Since
the quality of word alignment, especially in the
case of MWEs, is rather low, we remove ?trans-
lations? that are longer than four words (these are
most often wrong). We then associate each ex-
tracted MWE in Hebrew with all its possible En-
glish translations.
The result is a bilingual dictionary contain-
ing 2,955 MWE translation pairs, and also 355
translation pairs produced by taking high-quality
1:1 word alignments (Section 3.4). We used
the extracted MWE bilingual dictionary to aug-
ment the existing (78,313-entry) dictionary of a
transfer-based Hebrew-to-English statistical ma-
chine translation system (Lavie et al, 2004b). We
report in Table 3 the results of evaluating the per-
formance of the MT system with its original dic-
tionary and with the augmented dictionary. The
results show a statistically-significant (p < 0.1)
improvement in terms of both BLEU (Papineni et
al., 2002) and Meteor (Lavie et al, 2004a) scores.
Dictionary BLEU Meteor
Original 13.69 33.38
Augmented 13.79 33.99
Table 3: External evaluation
As examples of improved translations, a sen-
tence that was originally translated as ?His teach-
ers also hate to the Zionism and besmirch his
HRCL and Gurion? (fully capitalized words in-
dicate lexical omissions that are transliterated by
the MT system) is translated with the new dic-
tionary as ?His teachers also hate to the Zionism
and besmirch his Herzl and David Ben-Gurion?;
a phrase originally translated as ?when so? is now
properly translated as ?likewise?; and several oc-
currences of ?down spring? and ?height of spring?
are corrected to ?Tel Aviv?.
1262
6 Conclusion
We described a methodology for extracting multi-
word expressions from parallel corpora. The al-
gorithm we propose capitalizes on semantic cues
provided by ignoring 1:1 word alignments, and
viewing all other material in the parallel sentence
as potential MWE. It also emphasizes the impor-
tance of properly handling the morphology and
orthography of the languages involved, reducing
wherever possible the differences between them
in order to improve the quality of the alignment.
We use statistics computed from a large mono-
lingual corpus to rank and filter the results. We
used the algorithm to extract MWEs from a small
Hebrew-English corpus, demonstrating the ability
of the methodology to accurately extract MWEs
of various lengths and syntactic patterns. We also
demonstrated that the extracted MWE bilingual
dictionary can improve the quality of MT.
This work can be extended in various ways.
While several works address the choice of asso-
ciation measure for MWE identification and for
distinguishing between MWEs and other frequent
collocations, it is not clear which measure would
perform best in our unique scenario, where candi-
dates are produced by word (mis)alignment. We
intend to explore some of the measures discussed
by Pecina (2008) in this context. The algorithm
used for extracting the translations of candidate
MWEs is obviously na??ve, and we intend to ex-
plore more sophisticated algorithms for improved
performance. Also, as our methodology is com-
pletely language-symmetric, it can be used to pro-
duce MWE candidates in English. In fact, we al-
ready have such a list of candidates, whose qual-
ity we will evaluate in the future. Finally, as our
main motivation is high-precision, high-recall ex-
traction of Hebrew MWEs, we develop other, non-
alignment-based approaches to the task (Al-Haj
and Wintner, 2010), and would like to explore the
utility of combining different approaches to the
same task under a unified framework. We are ac-
tively pursuing these research directions.
Acknowledgments
This research was supported by THE ISRAEL
SCIENCE FOUNDATION (grants No. 137/06,
1269/07). We are grateful to Hassan Al-Haj for
providing the noun compound annotated corpus
and to Gennadi Lembersky for his help with the
machine translation system.
References
Al-Haj, Hassan and Shuly Wintner. 2010. Identify-
ing multi-word expressions by leveraging morpho-
logical and syntactic idiosyncrasy. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING 2010), August.
Baldwin, Timothy, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions, pages 89?96. Association for Compu-
tational Linguistics.
Bannard, Colin, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Francis Bond, Anna Ko-
rhonen, Diana McCarthy and Aline Villavicencio,
editors, Proceedings of the ACL 2003 Workshop on
Multiword Expressions: Analysis, Acquisition and
Treatment, pages 65?72.
Bar-Haim, Roy, Khalil Sima?an, and Yoad Winter.
2005. Choosing an optimal architecture for segmen-
tation and POS-tagging of Modern Hebrew. In Pro-
ceedings of the ACL Workshop on Computational
Approaches to Semitic Languages, pages 39?46,
Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Bird, Steven, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Sebastopol, CA.
Brants, Thorsten and Alex Franz. 2006. Web 1T 5-
gram version 1.1. LDC Catalog No. LDC2006T13.
Caseli, Helena, Aline Villavicencio, Andre? Machado,
and Maria Jose? Finatto. 2009. Statistically-driven
alignment-based multiword expression identifica-
tion for technical domains. In Proceedings of the
Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applica-
tions, pages 1?8, Singapore, August. Association
for Computational Linguistics.
Chang, Baobao, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
Chinese-English parallel corpora. In Proceedings
of the first SIGHAN workshop on Chinese language
processing, pages 1?5, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
1263
Church, Kenneth. W. and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy (rev). Computational Linguistics, 19(1):22?
29.
Daille, Be?atrice. 1994. Approche mixte pour
l?extraction automatique de terminologie : statis-
tiques lexicales et filtres linguistiques. Ph.D. thesis,
Universite? Paris 7.
Dejean, Herve, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for
word alignment. In Proceedings of the HLT-NAACL
2003 Workshop on Building and using parallel texts,
pages 23?26, Morristown, NJ, USA. Association for
Computational Linguistics.
Itai, Alon and Shuly Wintner. 2008. Language re-
sources for Hebrew. Language Resources and Eval-
uation, 42:75?98, March.
Katz, Graham and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 12?19, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Kirschenbaum, Amit and Shuly Wintner. 2010. A
general method for creating a bilingual translitera-
tion dictionary. In Proceedings of The seventh in-
ternational conference on Language Resources and
Evaluation (LREC-2010), May.
Koehn, Philipp. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of the MT Summit X, Phuket, Thailand.
Lavie, Alon, Kenji Sagae, and Shyamsundar Jayara-
man. 2004a. The significance of recall in automatic
metrics for mt evaluation. In Frederking, Robert E.
and Kathryn Taylor, editors, AMTA, volume 3265 of
Lecture Notes in Computer Science, pages 134?143.
Springer.
Lavie, Alon, Shuly Wintner, Yaniv Eytani, Erik Peter-
son, and Katharina Probst. 2004b. Rapid prototyp-
ing of a transfer-based Hebrew-to-English machine
translation system. In Proceedings of TMI-2004:
The 10th International Conference on Theoretical
and Methodological Issues in Machine Translation,
Baltimore, MD, October.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In ACL
?02: Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages
311?318, Morristown, NJ, USA. Association for
Computational Linguistics.
Pecina, Pavel. 2008. A machine learning approach
to multiword expression extraction. In Proceedings
of the LREC Workshop Towards a Shared Task for
Multiword Expressions.
Piao, Scott Songlin, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining a
semantic tagger and a statistical tool for mwe extrac-
tion. Computer Speech and Language, 19(4):378?
397.
Rosenthal, Ruvik. 2009. Milon HaTserufim (Dic-
tionary of Hebrew Idioms and Phrases). Keter,
Jerusalem. In Hebrew.
Tsvetkov, Yulia and Shuly Wintner. 2010. Automatic
acquisition of parallel corpora from websites with
dynamic content. In Proceedings of The seventh in-
ternational conference on Language Resources and
Evaluation (LREC-2010), May.
Van de Cruys, Tim and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 25?
32, Prague, Czech Republic, June. Association for
Computational Linguistics.
Varga, Da?niel, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, and Viktor Tro?n. 2005. Par-
allel corpora for medium density languages. In Pro-
ceedings of RANLP?2005, pages 590?596.
Villada Moiro?n, Begon?a and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word alignment. In Proceedings of the EACL 2006
Workshop on Multi-word-expressions in a multilin-
gual context. Association for Computational Lin-
guistics.
Villavicencio, Aline, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1034?1043.
Zarrie?, Sina and Jonas Kuhn. 2009. Exploit-
ing Translational Correspondences for Pattern-
Independent MWE Identification. In Proceedings
of the Workshop on Multiword Expressions: Identi-
fication, Interpretation, Disambiguation and Appli-
cations, pages 23?30, Singapore, August. Associa-
tion for Computational Linguistics.
1264
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 836?845,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identification of Multi-word Expressions by
Combining Multiple Linguistic Information Sources
Yulia Tsvetkov
Language Technologies Institute
Carnegie Mellon University
yulia.tsvetkov@gmail.com
Shuly Wintner
Department of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We propose an architecture for expressing
various linguistically-motivated features that
help identify multi-word expressions in nat-
ural language texts. The architecture com-
bines various linguistically-motivated clas-
sification features in a Bayesian Network.
We introduce novel ways for computing
many of these features, and manually de-
fine linguistically-motivated interrelationships
among them, which the Bayesian network
models. Our methodology is almost en-
tirely unsupervised and completely language-
independent; it relies on few language re-
sources and is thus suitable for a large num-
ber of languages. Furthermore, unlike much
recent work, our approach can identify ex-
pressions of various types and syntactic con-
structions. We demonstrate a significant im-
provement in identification accuracy, com-
pared with less sophisticated baselines.
1 Introduction
Multi-word Expressions (MWEs) are lexical items
that consist of multiple orthographic words (e.g.,
ad hoc, by and large, New York, kick the bucket).
MWEs are numerous and constitute a significant
portion of the lexicon of any natural language (Jack-
endoff, 1997; Erman and Warren, 2000; Sag et
al., 2002). They are a heterogeneous class of con-
structions with diverse sets of characteristics, dis-
tinguished by their idiosyncratic behavior. Mor-
phologically, some MWEs allow some of their con-
stituents to freely inflect while restricting (or pre-
venting) the inflection of other constituents. In
some cases MWEs may allow constituents to un-
dergo non-standard morphological inflections that
they would not undergo in isolation. Syntactically,
some MWEs behave like words while other are
phrases; some occur in one rigid pattern (and a fixed
order), while others permit various syntactic trans-
formations. Semantically, the compositionality of
MWEs is gradual, ranging from fully compositional
to idiomatic (Bannard et al, 2003).
Because of their prevalence and irregularity,
MWEs must be stored in lexicons of natural lan-
guage processing applications. Correct handling of
MWEs has been proven beneficial for various ap-
plications, including information retrieval, building
ontologies, text alignment, and machine translation.
We propose a novel architecture for identifying
MWEs of various types and syntactic categories in
monolingual corpora. Unlike much existing work,
which focuses on a particular syntactic construction,
our approach addresses MWEs of all types by focus-
ing on the general idiosyncratic properties of MWEs
rather than on specific properties of each sub-class
thereof. While we only evaluate our methodol-
ogy on bi-grams, it can in principle be extended
to longer MWEs. The architecture uses Bayesian
Networks (BN) to express multiple interdependent
linguistically-motivated features.
First, we automatically generate a small (training)
set of MWE and non-MWE bi-grams (positive and
negative instances, respectively). We then define a
set of linguistically-motivated features that embody
observed characteristics of MWEs. We augment
these by features that reflect collocation measures.
Finally, we define dependencies among these fea-
tures, expressed in the structure of a Bayesian Net-
work model, which we then use for classification.
This is a directed graph, whose nodes express the
features used for classification, and whose edges de-
836
fine causal relationships among these features. In
this architecture, learning does not result in a black
box, expressed solely as feature weights. Rather, the
structure of the BN allows us to learn the impact of
different MWE features on the classification. The
result is a new unsupervised method for identifying
MWEs of various types in text corpora. It com-
bines statistics with a large array of linguistically-
motivated features, organized in an architecture that
reflects interdependencies among the features.
The contribution of this work is manifold. First,
we show how to generate training material (al-
most) automatically, so the method is almost com-
pletely unsupervised. The methodology we advo-
cate is thus language-independent, requiring rela-
tively few language resources, and is therefore op-
timal for medium-density languages (Varga et al,
2005). Second, we propose several linguistically-
motivated features that can be computed from data
and that are demonstrably productive for improv-
ing the accuracy of MWE identification. These fea-
ture focus on the expression of linguistic idiosyn-
crasies of various types, a phenomenon typical of
MWEs. We propose novel computational model-
ing of many of these features; in particular, we ac-
count for the morphological idiosyncrasy of MWEs
using a histogram of the number of inflected forms,
in a technique that draws from image processing.
Third, we advocate the use of Bayesian Networks
as a mechanism for expressing manually-crafted de-
pendencies among features; the use of BN signifi-
cantly improves the classification accuracy. Finally,
we demonstrate the utility of our methodology by
applying it to Hebrew.1 Our evaluation shows that
the use of linguistically-motivated features results in
reduction of 23% of the errors compared with a col-
location baseline; organizing the knowledge in a BN
reduces the error rate by additional 8.7%.
After discussing related work in the next section,
we describe in Section 3 the methodology we pro-
pose, including a detailed discussion of the features
and their implementation. Section 4 provides a thor-
ough evaluation of the results. We conclude with
suggestions for future research.
1To facilitate readability we use a transliteration of Hebrew
using Roman characters; the letters used, in Hebrew lexico-
graphic order, are abgdhwzxTiklmns?pcqrs?t.
2 Related Work
Early approaches to MWEs identification concen-
trated on their collocational behavior (Church and
Hanks, 1990). Pecina (2008) compares 55 differ-
ent association measures in ranking German Adj-
N and PP-Verb collocation candidates. He shows
that combining different collocation measures using
standard statistical classification methods improves
over using a single collocation measure. Other re-
sults (Chang et al, 2002; Villavicencio et al, 2007)
suggest that some collocation measures (especially
PMI and Log-likelihood) are superior to others for
identifying MWEs.
Soon, however, it became clear that mere co-
occurrence measurements are not enough to identify
MWEs, and their linguistic properties should be ex-
ploited as well (Piao et al, 2005). Hybrid methods
that combine word statistics with linguistic informa-
tion exploit morphological, syntactic and semantic
idiosyncrasies to extract idiomatic MWEs.
Ramisch et al (2008) evaluate a number of asso-
ciation measures on the task of identifying English
Verb-Particle Constructions and German Adjective-
Noun pairs. They show that adding linguistic infor-
mation (mostly POS and POS-sequence patterns) to
the association measure yields a significant improve-
ment in performance over using pure frequency.
Several works address the lexical fixedness or syn-
tactic fixedness of (certain types of) MWEs in order
to extract them from texts. An expression is con-
sidered lexically fixed if replacing any of its con-
stituents by a semantically (and syntactically) sim-
ilar word generally results in an invalid or literal
expression. Syntactically fixed expressions prohibit
(or restrict) syntactic variation. For example, Van de
Cruys and Villada Moiro?n (2007) use lexical fixed-
ness to extract Dutch Verb-Noun idiomatic com-
binations (VNICs). Bannard (2007) uses syntac-
tic fixedness to identify English VNICs. Another
work uses both the syntactic and the lexical fixed-
ness of VNICs in order to distinguish them from
non-idiomatic ones, and eventually to extract them
from corpora (Fazly and Stevenson, 2006).
While these approaches are in line with ours, they
require lexical semantic resources (e.g., a database
that determines semantic similarity among words)
and syntactic resources (parsers) that are unavail-
837
able for Hebrew (and many other languages). Our
approach only requires morphological processing
and a bilingual dictionary, which are more readily-
available for several languages. Note also that
these approaches target a specific syntactic construc-
tion, whereas ours is adequate for various types of
MWEs.
Several properties of Hebrew MWEs are de-
scribed by Al-Haj (2010); Al-Haj and Wintner
(2010) use them in order to construct an SVM-based
classifier that can distinguish between MWE and
non-MWE noun-noun constructions in Hebrew. The
features of the SVM reflect several morphological
and morpho-syntactic properties of such construc-
tions. The resulting classifier performs much bet-
ter than a na??ve baseline, reducing over one third of
the errors. We rely on some of these insights, as
we implement more of the linguistic properties of
MWEs. Again, our methodology is not limited to a
particular construction: indeed, we demonstrate that
our general methodology, trained on automatically-
generated, general training data, performs almost as
well as the noun-noun-specific approach of Al-Haj
and Wintner (2010) on the very same dataset.
Recently, Tsvetkov and Wintner (2010b) intro-
duced a general methodology for extracting MWEs
from bilingual corpora, and applied it to Hebrew.
The results were a highly accurate set of Hebrew
MWEs, of various types, along with their English
translations. A major limitation of this work is that
it can only be used to identify MWEs in the bilingual
corpus, and is thus limited in its scope. We use this
methodology to extract both positive and negative
instances for our training set in the current work; but
we extrapolate the results much further by extend-
ing the method to monolingual corpora, which are
typically much larger than bilingual ones.
Bayesian Networks have only scarcely been used
for classification in natural language applications.
For example, BN were used for POS tagging of un-
known words (Peshkin et al, 2003); dependency
parsing (Savova and Peshkin, 2005); and docu-
ment classification (Lam et al, 1997; Calado et al,
2003; Denoyer and Gallinari, 2004). Very recently,
Ramisch et al (2010) have used BN for Portuguese
MWE identification. The features used for classi-
fication were of two kinds: (1) various collocation
measures; (2) bi-grams aligned together by an auto-
matic word aligner applied to a parallel (Portuguese-
English) corpus. A BN was used to combine the pre-
dictions of the various features on the test set, but
the structure of the network is not described. The
combined classifier resulted in a much higher accu-
racy than any of the two methods alone. However,
the BN does not play any special role in this work,
and its structure does not reflect any insights or intu-
itions on the structure of the problem domain or on
interdependencies among features.
We, too, acknowledge the importance of combin-
ing different types of knowledge in the hard task of
MWE identification. In particular, we also believe
that collocation measures are highly important for
this task, but cannot completely solve the problem:
linguistically-motivated features are mandatory in
order to improve the accuracy of the classifier. In
this work we focus on various properties of different
types of MWEs, and define general features that may
accurately apply to some, but not necessarily all of
them. An architecture of Bayesian Networks is op-
timal for this task: it enables us to define weighted
dependencies among features, such that certain fea-
tures are more significant for identifying some class
of MWEs, whereas others are more prominent in
identifying other classes. As we show below, this ar-
chitecture results in significant improvements over a
more na??ve combination of features.
3 Methodology
3.1 Motivation
The task we address is identification of MWEs, of
various types and syntactic constructions, in mono-
lingual corpora.2 Several properties of MWEs make
this task challenging: MWEs exhibit idiosyncrasies
on a variety of levels, orthographic, morphological,
syntactic and of course semantic (Al-Haj, 2010).
They are also extremely diverse: for example, on
the semantic dimension alone, MWEs cover an en-
tire spectrum, ranging from frozen, fixed idioms to
free combinations of words (Bannard et al, 2003).
Such a complex task calls for a combination of
multiple approaches, and much research indeed sug-
gests ?hybrid? approaches to MWE identification
2For simplicity, we focus on bi-grams of tokens (MWEs of
length 2) in this work; the methodology, however, is easily ex-
tensible to longer n-grams.
838
(Duan et al, 2009; Weller and Fritzinger, 2010;
Ramisch et al, 2010; Hazelbeck and Saito, 2010).
We believe that Bayesian Networks provide an op-
timal architecture for expressing various pieces of
knowledge aimed at MWE identification, for the fol-
lowing reasons (Heckerman, 1995):
? In contrast to many other classification meth-
ods, BN can learn (and express) causal relation-
ships between features. This facilitates better
understanding of the problem domain.
? BN can encode not only statistical data, but also
prior domain knowledge and human intuitions,
in the form of interdependencies among fea-
tures. We do indeed use this possibility here.
3.2 Linguistically-motivated Features
Based on the observations of Al-Haj (2010), we
define several linguistically-motivated features that
are aimed at capturing some of the unique proper-
ties of MWEs. While many idiosyncratic properties
of MWEs have been previously studied, we intro-
duce novel ways to express those properties as com-
putable features informing a classifier. Note that
many of the features we describe below are com-
pletely language-independent; others are applicable
to a wide range of languages, while few are specific
to morphologically-rich languages, and can be ex-
hibited in different ways in different languages. The
methodology we advocate, however, is completely
universal.
A common theme for all these features is idiosyn-
cracy: they are all aimed at locating some linguis-
tic property on which MWEs may differ from non-
MWEs. Below we detail these properties, along
with the features that we define to reflect them. In
all cases, the feature is applied to a candidate MWE,
defined here as a bi-gram of tokens (all possible bi-
grams are potential candidates). To compute the fea-
tures, we use a 46M-token monolingual Hebrew cor-
pus (Itai and Wintner, 2008), which we pre-process
as in Tsvetkov and Wintner (2010b). All statistics
are computed from this large corpus. Likewise, we
compute these features on a small training corpus,
which we generate automatically (see Section 3.4).
Orthographic variation Sometimes, MWEs are
written with dashes instead of inter-token spaces.
We define a binary feature, DASH, whose value is 1
iff the dash character appears in some surface form
of the candidate MWE. For example, xd-cddi (one
sided ) ?unilateral?.
Hapax legomena MWEs sometimes include con-
stituents that have no usage outside the particular
expression, and are hence not included in lexicons.
We define a feature, HAPAX, whose value is a binary
vector with 1 in the i-th place iff the i-th word of the
candidate is not in the lexicon, and does not occur
in other bi-grams at the same location. For exam-
ple, hwqws pwqws ?hocus-pocus?. In order to filter
out potential errors, candidates must occur at least 5
times in the corpus in order for this feature to fire.
Frozen form MWE constituents sometimes occur
in one fixed, frozen form. We define a feature,
FROZEN, whose value is a binary vector with 1 in the
i-th place iff the i-th word of the candidate never in-
flects in the context of this expression. Example: bit
xwlim (house-of sick-people) ?hospital?; the noun
xwlim must be in the plural in this MWE.
Partial morphological inflection In some cases,
MWE constituents undergo a (strict but non-empty)
subset of the full inflections that they would undergo
in isolation. We capture this property with a tech-
nique that has been proven useful in the area of im-
age processing (Jain, 1989, Section 7.3). We com-
pute a histogram of the distribution in the corpus of
all the possible surface forms of each constituent of
an MWE candidate. Such histograms can compactly
represent distributional information on morphologi-
cal behavior, in the same way that histograms of the
distribution of gray levels in a picture are used to
represent the picture itself.
Our assumption is that the inflection histograms
of non-MWEs are more uniform than the histograms
of MWEs, in which some inflections may be more
frequent and others may be altogether missing. Of
course, restrictions on the histogram may stem from
the part of speech of the expression; such constraints
are captured by dependencies in the BN structure.
Since each MWE is idiosyncratic in its own
way, we do not expect the histograms of MWEs to
have some specific pattern, except non-uniformity.
We therefore sort the columns of each histogram,
thereby losing information pertaining to the specific
839
inflections, and retaining only information about the
idiosyncrasy of the histogram. Offline, we compute
the average histogram for positive and negative ex-
amples: The average histogram of MWEs is shorter
and less uniform than the average histogram of non-
MWEs. We define as feature, HIST, the L1 (Manhat-
tan) distance between the histogram of the candidate
and the closest average histogram.
For example, the MWE bit mepv (house-of law)
?court? occurs in the following inflected forms:
bit hmepv ?the court? (75%); bit mepv ?a court?
(15%); bti hmepv ?the courts? (8%); and bti mepv
?courts? (2%). The histogram for this candidate
is thus (75, 15, 8, 2). In contrast, the non-MWE
txwm mepv (domain-of law) ?domain of the law?,
which is syntactically identical, occurs in nine dif-
ferent inflected forms, and its sorted histogram is
(59, 14, 7, 7, 5, 2, 2, 2, 2).
Context We hypothesize that MWEs tend to con-
strain their (semantic) context more strongly than
non-MWEs. We expect words that occur imme-
diately after MWEs to vary less freely than words
that immediately follow other expressions. One mo-
tivation for this hypothesis is the observation that
MWEs tend to be less polysemous than free com-
binations of words, thereby limiting the possible se-
mantic context in which they can occur.
We define a feature, CONTEXT, as follows. We
first compute a histogram of the frequencies of
words following each candidate MWE. We trim the
tail of the histogram by removing words whose fre-
quency is lower than 0.1% (the expectation is that
non-MWEs would have a much longer tail). Off-
line, we compute the same histograms for positive
and negative examples and average them as above.
The value of CONTEXT is 1 iff the histogram of the
candidate is closer (in terms of L1 distance) to the
positive average.
For example, the histogram of bit mepv ?court?
includes 15 values, dominated by bit mepv yliwn
?supreme court? (20%) and bit mepv mxwzi ?dis-
trict court? (13%), followed by contexts whose fre-
quency ranges between 5% and 0.6%. In con-
trast, the non-MWE txwm mepv ?domain-of law?
has a much shorter histogram, namely (12, 11, 6):
over 70% of the words following this expression oc-
cur less than 0.1% and are hence in the trimmed tail.
Syntactic diversity MWEs can belong to various
part of speech categories. We define as feature, POS,
the category of the candidate, with values obtained
by selecting frequent tuples of POS tags. For exam-
ple, Noun-Noun, PropN-PropN, Noun-Adj, etc.
Translational equivalents Since MWEs are of-
ten idiomatic, they tend to be translated in a non-
literal way, sometimes to a single word. We use
a dictionary to generate word-by-word translations
of candidate MWEs to English, and check the num-
ber of occurrences of the English literal translation
in a large English corpus.3 Due to differences in
word order between the two languages, we create
two variants for each translation, corresponding to
both possible orders. We expect non-MWEs to have
some literal translational equivalent (possibly with
frequency that correlates with their frequency in He-
brew), whereas for MWEs we expect no (or few) lit-
eral translations. We define a binary feature, TRANS,
whose value is 1 iff some literal translation of the
candidate occurs more than 5 times in the corpus.
For example, the MWE htxtn ym (marry with )
?marry? is literally translated as with marry, marry
with, together marry and marry together, none of
which occurs in the corpus.
Collocation As a baseline, statistical association
measure, we use a heuristic variant of pointwise mu-
tual information (PMI), promoting also collocations
whose constituents are frequent (Tsvetkov and Wint-
ner, 2010b). We define a binary feature, PMI, with
values (low and high) reflecting the threshold that
maximizes the accuracy of MWE classification in
Tsvetkov and Wintner (2010b).
3.3 Feature Interdependencies Expressed as a
Bayesian Network
A Bayesian Network (Jensen and Nielsen, 2007) is
organized as a graph whose nodes are random vari-
ables and whose edges represent interdependencies
among those variables. We use a particular type
of BN, known as causal networks, in which di-
rected edges lead to a variable from each of its direct
causes. This facilitates the expression of domain
knowledge (and intuitions, beliefs, etc.) as struc-
tural properties of the network. We use the BN as
3We use a 120M-token newspaper corpus.
840
a classification device: training amounts to comput-
ing the joint probability distribution of the training
set, whereas classification maximizes the posterior
probability of the particular node (variable) being
queried.
For MWE identification we define a BN whose
nodes correspond to the features described in Sec-
tion 3.2. In addition, we define a node MWE for
the complete classification task. Over these nodes
we impose the structure depicted graphically in Fig-
ure 1. This structure, which we motivate below, is
manually defined: it reflects our understanding of
the problem domain and is a result of thorough ex-
perimentations. That said, it can of course be mod-
ified in various ways, and in particular, new nodes
can be easily added to reflect additional features.
MWE
HAPAXDASH CNTXT
POS
HIST
PMI
TRANS
FRZN
Figure 1: Bayesian Network for MWE identification
All nodes depend on MWE, as all are affected
by whether or not the candidate is a MWE. The
POS of an expression influences its morphological
inflection, hence the edges from POS to HIST and
to FROZEN. For example, Hebrew noun-noun con-
structions allow their constituents to undergo the full
inflectional paradigm, but when such a construction
is a MWE, inflection is severely constrained (Al-Haj
and Wintner, 2010); similarly, when one of the con-
stituents of a MWE is a conjunction, the entire ex-
pression is very likely to be frozen.
Hapaxes clearly affect all statistical metrics,
hence the edge from HAPAX to PMI, and also the
existence of literal translation, since if a word is not
in the lexicon, it does not have a translation, hence
the edge from HAPAX to TRANS. Also, we assume
that there is a correlation between the frequency (and
PMI) of a candidate and whether or not a literal
translation of the expression exists, hence the edge
from PMI to TRANS. The edges from PMI and HIST
to CONTEXT are justified by the correlation between
the frequency and variability of an expression and
the variability of the context in which it occurs.
Once the structure of the network is established,
the conditional probabilities of each dependency
have to be determined. We compute the conditional
probability tables from our training data (see below)
using Weka (Hall et al, 2009), and obtain values
for P (X | X1, . . . , Xk) for each variable X and all
variables Xi, 1 ? i ? k, such that the graph in-
cludes an edge from Xi to X (parents of X). We
then perform inference on the network in order to
compute P (Xmwe | X1, . . . , Xk), where Xmwe
corresponds to the node MWE, and X1, . . . , Xk are
the variables corresponding to all other nodes in the
network. Using Bayes Rule,
P (Xmwe | X1, . . . , Xk) ?
P (X1, . . . , Xk | Xmwe)? P (Xmwe)
We define the prior, P (Xmwe), to be 0.41:
this is the percentage of MWEs in WordNet 1.7
(Fellbaum, 1998). The conditional probabilities
P (X1, . . . , Xk | Xmwe) are determined by Weka
from the conditional probability tables:
P (X1, . . . , Xk | Xmwe) = ?ki=1P (Xi | pai)
where k is the number of nodes in the BN (other than
Xmwe) and pai is the set of parents of Xi.
3.4 Automatic Generation of Training Data
For training we need samples of positive and nega-
tive instances of MWEs, each associated with a vec-
tor of the values of all features discussed in Sec-
tion 3.2. We generate this training material auto-
matically. We use a small Hebrew-English bilin-
gual corpus (Tsvetkov and Wintner, 2010a). We
word-align the corpus with Giza++ (Och and Ney,
2003), and then apply the (completely unsupervised)
841
algorithm of Tsvetkov and Wintner (2010b), which
extracts MWE candidates from the aligned corpus
and re-ranks them using statistics computed from a
large monolingual corpus. The core idea behind this
method is that MWEs tend to be translated in non-
literal ways; in a parallel corpus, words that are 1:1
aligned typically indicate literal translations and are
hence unlikely constituents of MWEs.
The result is a set of 134,001 Hebrew bi-gram
types (from the bilingual corpus), classified as either
1:1 aligned (implying they are likely not MWEs)
or unaligned (in which case they may or may not
be MWEs). In addition, for each bi-gram we
have a PMI score; naturally, higher PMI scores
are indicative of MWEs. We thus divide the set
into four classes: aligned bi-grams with high PMI
score, aligned bi-grams with low PMI score, mis-
aligned with high PMI and misaligned with low
PMI. Aligned bi-grams, independently of their PMI
score, are more likely non-MWEs; high-PMI mis-
aligned bi-grams are very likely MWEs; and the sta-
tus of low-PMI misaligned bi-grams is unclear, and
must be further investigated. This is summarized in
Table 1.
Misaligned Aligned
High PMI MWE non-MWE
Low PMI unclear non-MWE
Table 1: Classification of bi-grams
We set the threshold that separates low PMI from
high PMI as in Tsvetkov and Wintner (2010b). The
results of this classification is depicted in Table 2.
Misaligned Aligned Total
High PMI 2,203 493 2,696
Low PMI 61,314 69,991 131,305
Total 63,517 70,484 134,001
Table 2: Statistics of the sample space from which the
training set is generated
We assume that all bi-grams in the ?Aligned? col-
umn are non-MWEs. Additionally, we assume that
the 2,203 misaligned bi-grams with high PMI scores
are likely MWEs. As for the set of over 61,000 mis-
aligned low-PMI bi-grams, certainly many of them
are non-MWEs, but some may be MWEs, and we
are interested in including them as positive examples
of MWEs with low PMI scores. We therefore manu-
ally annotate a sample of 50 MWEs from this partic-
ular set (we had to manually go over a few thousands
of bi-grams to select this sample). This is the only
supervision provided in this work.
The remaining question is how to determine the
sizes of samples from each of the other three classes.
We use two guidelines: first, we would like the ra-
tio of MWEs to non-MWEs in the training set to be
41 : 59, reflecting the ratio in WordNet (the prior
MWE probability). Second, we would like classifi-
cation by PMI score only to yield a reasonable base-
line; the baseline is defined as the ratio of the sum of
high-PMI MWEs plus low-PMI non-MWEs to the
size of the training set. We choose 67%, the PMI
baseline reported by Al-Haj and Wintner (2010). As
a result of these two considerations, we end up with
training sets whose sizes are depicted in Table 3. We
randomly select from the sample space this many in-
stances for each class. Since much of the procedure
of preparing training data is automatic, the results
may be somewhat noisy. As Bayesian Network are
known to be robust to noisy data, we expect the BN
to compensate for this problem.
MWE non-MWE Total
High PMI 300 232 532
Low PMI 50 272 322
Total 350 504 854
Table 3: Sizes of each training set
4 Results and Evaluation
We use the training set described above for train-
ing and evaluation: we perform 10-fold cross vali-
dation experiments, reporting Precision, Recall, Ac-
curacy and F-measure in three setups: one (SVM)
in which we train an SVM classifier4 with the
features described in Section 3.2; one (BN-auto)
in which we train a BN but let Weka determine
its structure (using the K2 algorithm); and one
(BN) in which we train a Bayesian Network whose
structure reflects manually-crafted linguistically-
motivated knowledge, as depicted in Figure 1. The
4We use Weka SMO with the PolyKernel setup; experimen-
tation with several other kernels yielded worse results.
842
results, along with the PMI baseline figures, are
listed in Table 4.
Accuracy Prec. Recall F-score
PMI 66.98% 0.73 0.67 0.67
BN-auto 71.19% 0.71 0.71 0.71
SVM 74.59% 0.75 0.75 0.75
BN 76.82% 0.77 0.77 0.77
Table 4: 10-fold cross validation evaluation results
The linguistically-motivated features defined in
Section 3.2 are clearly helpful in the classification
task: the accuracy of the SVM, informed by these
features, is close to 75%, reducing the error rate
of the PMI baseline by 23%. The contribution
of the Bayesian Network is also highly significant,
reducing almost 7% more errors (8.7% of the er-
rors made by the SVM classifier), or a total of al-
most 30% error-rate reduction with respect to the
baseline. Interestingly, a BN whose structure does
not reflect prior knowledge, but is rather learned au-
tomatically, performs poorly. It is the combination
of linguistically-motivated features with feature in-
terdependencies reflecting domain knowledge that
contribute to the best performance.
As a further demonstration of the utility of our
approach, we evaluate the algorithm on an addi-
tional test set that was used for evaluation in the past
(Tsvetkov and Wintner, 2010b; Al-Haj and Wintner,
2010). This is a small annotated corpus, NN, of He-
brew noun-noun constructions. The corpus consists
of 413 high-frequency bi-grams of the same syntac-
tic construction; of those, 178 are tagged as MWEs
(in this case, noun compounds) and 235 as non-
MWEs. This corpus consolidates the annotation of
three annotators: only instances on which all three
agreed were included. Since it includes both posi-
tive and negative instances, this corpus facilitates a
robust evaluation of precision and recall.
We train a Bayesian Network on the training set
described in Section 3.4 and use it to classify the set
NN. We compare the results of this classifier with a
PMI baseline (using the same threshold as above),
and also with the classification results reported by
Al-Haj and Wintner (2010) (AW); the latter reflects
10-fold cross-validation evaluation using the entire
set, so it should be considered an upper bound for
any classifier that uses a general training corpus.
The results are depicted in Table 5. They clearly
demonstrate that the linguistically-motivated fea-
tures we define provide a significant improvement in
classification accuracy over the baseline PMI mea-
sure. Note that our F-score, 0.77, is very close to
the best result of 0.79 obtained by Al-Haj and Wint-
ner (2010) as the average of 10-fold cross valida-
tion runs, using only high-frequency noun-noun con-
structions for training. We interpret this result as a
further proof of the robustness of our architecture.
Accuracy Precision Recall F-score
PMI 71.43% 0.71 0.71 0.71
BN 77.00% 0.77 0.77 0.77
AW 80.77% 0.77 0.81 0.79
Table 5: Evaluation results: noun-noun constructions
Finally, we have used the trained BN to classify
the entire set of bi-grams present in the (Hebrew
side of the) parallel corpus described in Tsvetkov
and Wintner (2010a). Of the 134,000 candidates,
only 4,000 are classified as MWEs. We sort this
list of potential MWEs by the probability assigned
by the BN to the positive value of the variable
Xmwe. The resulting sorted list is dominated by
high-PMI bi-grams, especially proper names, all of
which are indeed MWEs. The first non-MWE (false
positive) occurs in the 50th place on the list; it is
crpt niqwla ?France Nicolas?, which is obviously a
sub-sequence of the larger MWE, neia crpt niqwla
srqwzi ?French president Nicolas Sarkozy?. Simi-
lar sub-sequences are also present, but only five are
in the top-100. Such false positives can be reduced
when longer MWEs are extracted, as it can be as-
sumed that a sub-sequence of a longer MWE does
not have to be identified. Other false positives in the
top-100 include some highly frequent expressions,
but over 85 of the top-100 are clearly MWEs.
While more careful evaluation is required in order
to estimate the rate of true positives in this list, we
trust that the vast majority of the positive results are
indeed MWEs.
5 Conclusions and future work
We presented a novel architecture for identifying
MWEs in text corpora. The main insights we em-
843
phasize are sophisticated computational encoding of
linguistic knowledge that focuses on the idiosyn-
cratic behavior of such expressions. This is reflected
in two ways in our work: by defining computable
features that reflect different facets of irregulari-
ties; and by framing the features as part of a larger
Bayesian Network that accounts for interdependen-
cies among them. We also introduce a method for
automatically generating a training set for this task,
which renders the classification almost entirely un-
supervised. The result is a nearly-unsupervised,
language-independent classification method that can
identify MWEs of various lengths, types and con-
structions. Evaluation on Hebrew shows significant
improvement in the accuracy of the classifier com-
pared with the state of the art.
The modular architecture of BN facilitates easy
exploration with more features. We are currently in-
vestigating the contribution of various other sources
of information to the classification task. For exam-
ple, Hebrew lacks large-scale lexical semantic re-
sources. However, it is possible to literally trans-
late a MWE candidate to English and rely on the
English WordNet for generating synonyms of the lit-
eral translation. Such ?literal synonyms? can then be
back-translated to Hebrew. The assumption is that
if a back-translated expression has a high PMI, the
original candidate is very likely not a MWE. While
such a feature may contribute little on its own, in-
corporating it in a well-structured BN may improve
performance.
While our methodology is applicable to MWEs
of any length, we have so far only evaluated it on bi-
grams. In the future, we intend to extend the evalu-
ation to longer n-grams. We also plan to apply the
methodology to languages other than Hebrew.
Acknowledgments
This research was supported by THE ISRAEL
SCIENCE FOUNDATION (grants No. 137/06,
1269/07). We are grateful to Gennadi Lembersky
for his continuous help.
References
Hassan Al-Haj and Shuly Wintner. 2010. Identifying
multi-word expressions by leveraging morphological
and syntactic idiosyncrasy. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING 2010), pages 10?18, Beijing, China,
August. Coling 2010 Organizing Committee.
Hassan Al-Haj. 2010. Hebrew multiword expressions:
Linguistic properties, lexical representation, morpho-
logical processing, and automatic acquisition. Mas-
ter?s thesis, University of Haifa, February.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Diana McCarthy Francis Bond, Anna Ko-
rhonen and Aline Villavicencio, editors, Proceedings
of the ACL 2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 65?72.
Colin Bannard. 2007. A measure of syntactic flexibility
for automatically identifying multiword expressions in
corpora. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 1?8. As-
sociation for Computational Linguistics.
Pa?vel Calado, Marco Cristo, Edleno Silva De Moura,
Nivio Ziviani, Berthier A. Ribeiro-Neto, and Mar-
cos Andre? Gonc?alves. 2003. Combining link-based
and content-based methods for web document classifi-
cation. In Proceedings of CIKM-03, 12th ACM Inter-
national Conference on Information and Knowledge
Management, pages 394?401, New Orleans, US. ACM
Press, New York, US.
Baobao Chang, Pernilla Danielsson, and Wolfgang Teu-
bert. 2002. Extraction of translation unit from
Chinese-English parallel corpora. In Proceedings of
the first SIGHAN workshop on Chinese language pro-
cessing, pages 1?5, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Ludovic Denoyer and Patrick Gallinari. 2004. Bayesian
network model for semi-structured document classi-
fication. Information Processing and Management,
40(5):807?827.
Jianyong Duan, Mei Zhang, Lijing Tong, and Feng Guo.
2009. A hybrid approach to improve bilingual mul-
tiword expression extraction. In Thanaruk Theera-
munkong, Boonserm Kijsirikul, Nick Cercone, and
Tu-Bao Ho, editors, Advances in Knowledge Discov-
ery and Data Mining, volume 5476 of Lecture Notes
in Computer Science, pages 541?547. Springer, Berlin
and Heidelberg.
Britt Erman and Beatrice Warren. 2000. The idiom prin-
ciple and the open choice principle. Text, 20(1):29?62.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 337?344.
844
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Gregory Hazelbeck and Hiroaki Saito. 2010. A hybrid
approach for functional expression identification in a
japanese reading assistant. In Proceedings of the 2010
Workshop on Multiword Expressions: from Theory to
Applications, pages 81?84, Beijing, China, August.
Coling 2010 Organizing Committee.
David Heckerman. 1995. A tutorial on learning with
Bayesian networks. Technical Report MSR-TR-95-
06, Microsoft Research, March.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Ray Jackendoff. 1997. The Architecture of the Language
Faculty. MIT Press, Cambridge, USA.
Anil K. Jain. 1989. Fundamentals of digital image pro-
cessing. Prentice-Hall, Inc., NJ, USA.
Finn V. Jensen and Thomas D. Nielsen. 2007. Bayesian
Networks and Decision Graphs. Springer, 2nd edition.
Wai Lam, Kon F. Low, and Chao Y. Ho. 1997. Using a
bayesian network induction approach for text catego-
rization. In Martha E. Pollack, editor, Proceedings of
IJCAI-97, 15th International Joint Conference on Ar-
tificial Intelligence, pages 745?750, Nagoya, JP. Mor-
gan Kaufmann Publishers, San Francisco, US.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Pavel Pecina. 2008. A machine learning approach to
multiword expression extraction. In Proceedings of
the LREC Workshop Towards a Shared Task for Multi-
word Expressions.
Leonid Peshkin, Avi Pfeffer, and Virginia Savova. 2003.
Bayesian nets in syntactic categorization of novel
words. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume of the Proceedings of HLT-
NAACL 2003?short papers - Volume 2, NAACL ?03,
pages 79?81, Morristown, NJ, USA. Association for
Computational Linguistics.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and Tony
McEnery. 2005. Comparing and combining a se-
mantic tagger and a statistical tool for mwe extraction.
Computer Speech and Language, 19(4):378?397.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and
Alline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions. In
Proceedings of the LREC Workshop Towards a Shared
Task for Multiword Expressions.
Carlos Ramisch, Helena de Medeiros Caseli, Aline
Villavicencio, Andre? Machado, and Maria Finatto.
2010. A hybrid approach for multiword expression
identification. In Thiago Pardo, Anto?nio Branco,
Aldebaro Klautau, Renata Vieira, and Vera de Lima,
editors, Computational Processing of the Portuguese
Language, volume 6001 of Lecture Notes in Computer
Science, pages 65?74. Springer.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent Text
Processing and Computational Linguistics (CICLING
2002), pages 1?15, Mexico City, Mexico.
Virginia Savova and Leonid Peshkin. 2005. Dependency
parsing with dynamic bayesian network. In Proceed-
ings of the 20th national conference on Artificial intel-
ligence - Volume 3, pages 1112?1117. AAAI Press.
Yulia Tsvetkov and Shuly Wintner. 2010a. Automatic
acquisition of parallel corpora from websites with dy-
namic content. In Proceedings of the Seventh confer-
ence on International Language Resources and Eval-
uation (LREC?10), pages 3389?3392. European Lan-
guage Resources Association (ELRA), May.
Yulia Tsvetkov and Shuly Wintner. 2010b. Extraction
of multi-word expressions from small parallel corpora.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), Au-
gust.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 25?32, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, and Viktor Tro?n. 2005. Par-
allel corpora for medium density languages. In Pro-
ceedings of RANLP?2005, pages 590?596.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1034?1043.
Marion Weller and Fabienne Fritzinger. 2010. A hy-
brid approach for the identification of multiword ex-
pressions. In Proceedings of the SLTC 2010 Workshop
on Compounds and Multiword Expressions, October.
845
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616?625,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Augmenting Translation Models with Simulated Acoustic Confusions for
Improved Spoken Language Translation
Yulia Tsvetkov Florian Metze Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213; U.S.A.
{ytsvetko, fmetze, cdyer}@cs.cmu.edu
Abstract
We propose a novel technique for adapting
text-based statistical machine translation
to deal with input from automatic speech
recognition in spoken language translation
tasks. We simulate likely misrecognition
errors using only a source language pro-
nunciation dictionary and language model
(i.e., without an acoustic model), and use
these to augment the phrase table of a stan-
dard MT system. The augmented sys-
tem can thus recover from recognition er-
rors during decoding using synthesized
phrases. Using the outputs of five differ-
ent English ASR systems as input, we find
consistent and significant improvements in
translation quality. Our proposed tech-
nique can also be used in conjunction with
lattices as ASR output, leading to further
improvements.
1 Introduction
Spoken language translation (SLT) systems gen-
erally consist of two components: (i) an auto-
matic speech recognition (ASR) system that tran-
scribes source language utterances and (ii) a ma-
chine translation (MT) system that translates the
transcriptions into the target language. These two
components are usually developed independently
and then combined and integrated (Ney, 1999;
Matusov et al., 2006; Casacuberta et al., 2008;
Zhou, 2013; He and Deng, 2013).
While this architecture is attractive since it re-
lies only on components that are independently
useful, such systems face several challenges. First,
spoken language tends to be quite different from
the highly edited parallel texts that are available to
train translation systems. For example, disfluen-
cies, such as repeated words or phrases, restarts,
and revisions of content, are frequent in spon-
taneous speech,
1
while these are usually absent
in written texts. In addition, ASR outputs typi-
cally lack explicit segmentation into sentences, as
well as reliable casing and punctuation informa-
tion, which are crucial for MT and other text-based
language processing applications (Ostendorf et al.,
2008). Second, ASR systems are imperfect and
make recognition errors. Even high quality sys-
tems make recognition errors, especially in acous-
tically similar words with similar language model
scores, for example morphological substitutions
like confusing bare stem and past tense forms, and
in high-frequency short words (function words)
which often lack both disambiguating context and
are subject to reduced pronunciations (Goldwater
et al., 2010).
One would expect that training an MT system
on ASR outputs (rather than the usual written-
style texts) would improve matters. Unfortunately,
there are few corpora of speech paired with text
translations into a second language that could be
used for this purpose. This has been an incentive
to various MT adaptation approaches and devel-
opment of speech-input MT systems. MT adapta-
tion has been done via input text pre-processing,
by transformation of spoken language (ASR out-
put) into written language (MT input) (Peitz et
al., 2012; Xu et al., 2012); via decoding ASR n-
best lists (Quan et al., 2005), or confusion net-
works (Bertoldi et al., 2007; Casacuberta et al.,
2008), or lattices (Dyer et al., 2008; Onishi et al.,
2010); via additional translation features captur-
ing acoustic information (Zhang et al., 2004); and
with methods that follow a paradigm of unified de-
coding (Zhou et al., 2007; Zhou, 2013). In line
with the previous research, we too adapt a standard
MT system to a speech-input MT, but by altering
the translation model itself so it is better able to
1
Disfluencies constitute about 6% of word tokens in spon-
taneous speech, not including silent pauses (Tree, 1995; Kasl
and Mahl, 1965)
616
deal with ASR output (Callison-Burch et al., 2006;
Tsvetkov et al., 2013a).
We address speech translation in a resource-
deficient scenario, specifically, adapting MT sys-
tems to SLT when ASR is unavailable. We aug-
ment a discriminative set that translation models
rescore with synthetic translation options. These
automatically generated translation rules (hence-
forth synthetic phrases) are noisy variants of ob-
served translation rules with simulated plausible
speech recognition errors (?2). To simulate ASR
errors we generate acoustically and distribution-
ally similar phrases to a source (English) phrase
with a phonologically-motivated algorithm (?4).
Likely phonetic substitutions are learned with an
unsupervised algorithm that produces clusters of
similar phones (?3). We show that MT systems
augmented with synthetic phrases increase the
coverage of input sequences that can be translated,
and yield significant improvement in the quality of
translated speech (?6).
This work makes several contributions. Primary
is our framework to adapt MT to SLT by popu-
lating translation models with synthetic phrases.
2
Second, we propose a novel method to generate
acoustic confusions that are likely to be encoun-
tered in ASR transcription hypotheses. Third, we
devise simple and effective phone clustering al-
gorithm. All aforementioned algorithms work in
a low-resource scenario, without recourse to au-
dio data, speech transcripts, or ASR outputs: our
method to predict likely recognition errors uses
phonological rather than acoustic information and
does not depend on a specific ASR system. Since
our source language is English, we operate on a
phone level and employ a pronunciation dictionary
and a language model, but the algorithm can in
principle be applied without pronunciation dictio-
nary for languages with a phonemic orthography.
2 Methodology
We adopt a standard ASR-MT cascading approach
and then augment translation models with syn-
thetic phrases. Our proposed system architecture
is depicted in Figure 1.
Synthetic phrases are generated from entries in
the original translation model?phrase translation
2
We augment phrase tables only with synthetic phrases
that capture simulated ASR errors, the methodology that we
advocate, however, is applicable to many problems in transla-
tion (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau
et al., 2013).
ASR
ASR LM
(source lang.)
Acoustic 
Model
cat ????
MT
MT LM
(target lang.)
TM+TM'
Translation Model 
augmented with 
simulated ASR errors
Figure 1: SLT architecture: ASR and MT are
trained independently and then cascaded. We im-
prove SLT by populating MT translation model
with synthetic phrases. Each synthetic phrase is
a variant of an original phrase pair with simulated
ASR errors on the source side.
pairs acquired from parallel data. From a source
side of an original phrase pair we generate list of
its plausible misrecognition variants (pseudo-ASR
outputs with recognition errors) and add them as
a source side of a synthetic phrase. For k-best
simulated ASR outputs we construct k synthetic
phrases: a simulated ASR output in the source
side is coupled with its translation?an original tar-
get phrase (identical for all k phrases). Synthetic
phrases are annotated with five standard phrasal
translation features (forward and reverse phrase
and lexical translation probabilities and phrase
penalty); these were found in the original phrase
and remain unchanged. In addition, we add three
new features to all phrase pairs, both synthetic and
original. First, we add a boolean feature indi-
cating the origin of a phrase: synthetic or origi-
nal. Two other features correspond to an ASR lan-
guage model score of the source side. One is LM
score of the synthetic phrase, another is a score
of a phrase from which the source side was gener-
ated. We then append synthetic phrases to a phrase
table: k synthetic phrases for each original phrase
pair, with eight features attached to each phrase.
We show synthetic phrases example in Figure 2.
3 Acoustically confusable phones
The phonetic context of a given phone affects its
acoustic realization, and a variability in a produc-
tion of the same phone is possible depending on
coarticulation with its neighboring phones.
3
In ad-
dition, there are phonotactic constraints that can
restrict allowed sequences of phones. English has
strong constraints on sequences of consonants; the
sequence [zdr], for example, cannot be a legal En-
3
These are the reasons why in context-dependent acous-
tic modeling different HMM models are trained for different
contexts.
617
Source
phrase
Target
phrase
Original phrase
translation features
Synthetic
indicator
Synthetic
LM score
Original
LM score
tells the story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
0 3.9?10
?3
3.9?10
?3
tell their story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 5.9?10
?3
3.9?10
?3
tells a story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 2.2?10
?3
3.9?10
?3
tell the story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.7?10
?3
3.9?10
?3
tell a story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.3?10
?3
3.9?10
?3
tell that story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.0?10
?3
3.9?10
?3
tell their stories raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.9?10
?3
3.9?10
?3
tells the stories raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.8?10
?3
3.9?10
?3
tells her story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.7?10
?3
3.9?10
?3
chelsea star raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.5?10
?3
3.9?10
?3
Figure 2: Example of acoustically confusable synthetic phrases. Phrases were synthesized from the
original phrase pair in Row 1 by generating acoustically similar phrases for the English phrase tells the
story. All phrases have the same (target) French translation me raconte l?histoire and the same five basic
phrase-based translation rule features. To these, three additional features are added: a synthetic phrase
indicator, the source language LM score of the source phrase, and the source language LM score of a
source phrase in the original phrase pair.
T
left
T
right
TW
left
WIH
right
. . .
T P (T |WIH) . . .
W P (W |T ) . . .
IH P (IH|T ) P (IH|TW ) . . .
ER P (ER|T ) . . .
. . . . . . . . . . . . . . . . . .
Figure 3: A fragment of the co-occurrence matrix
for phone sequence [T W IH T ER]. Rows corre-
spond to phones; columns correspond to left/right
context phones of lengths one and two.
glish syllable onset (Jurafsky and Martin, 2000).
Motivated by the constraining effect of context
on phonetic distribution, we cluster phones using a
distance-based measure. To do so, we build a vec-
tor space model representation of each phone by
creating a co-occurrence matrix from a corpus of
phonetic forms where each row represents a phone
and columns indicate the contextual phones. We
take into account left/right context windows of
lengths one and two. A cell r
p,c
in the vector space
dictionary matrix represents phone p and context c
using the empirical relative frequency f(p | c), as
estimated from a pronunciation dictionary. Fig-
ure 3 shows a fragment of the co-occurrence ma-
trix constructed from a dictionary containing just
the pronunciation of Twitter ? [T W IH T ER].
Under this representation, the similarity of
phones can be easily quantified by measuring their
distance in the vector space, the cosine of the angle
between them:
Sim(p
1
, p
2
) =
p
1
?p
2
||p
1
||?||p
2
||
Armed with this similarity function, we apply the
K-means algorithm
4
to partition the phones into
disjoint sets.
4 Plausible misrecognition variants
For an input English sequence we generate top-k
pseudo-ASR outputs, that are added as a source
side of a synthetic phrase. Every ASR output that
we simulate is a plausible misrecognition that has
two distinguishing characteristics: it is acousti-
cally and linguistically confusable with the input
sequence. Former corresponds to phonetic simi-
larity and latter to distributional similarity of these
two phrases in corpus.
Given a reference string?a word or sequence
of words w in the source language, we generate
k-best hypotheses v. This can be modeled as a
weighted finite state transducer:
{v} = G ?D
?1
? T ?D ? {w} (1)
where
? D maps from words to pronunciations
? T is a phone confusion transducer
? D
?1
maps from pronunciations to words
? G is an ASR language model
D maps words to their phonetic representation
5
,
or multiple representations for words with several
4
Value of K=12 was determined empirically.
5
Using the CMU pronounciation dictionary
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
618
pronunciation variants. To create a phone con-
fusion transducer T maps source to target phone
sequences by performing a number of edit opera-
tions. Allowed edits are:
? Deletion of a consonant (mapping to ).
? Doubling of a vowel.
? Insertion of one or two phones in the end of a
sequence from the list of possible suffixes: S
(-s), IX NG (-ing), D (-ed).
? Substitution of a phone by an acousti-
cally similar phone. The clusters of the
similar phones are {Z,S}, {XL,L,R},
{AA,AO,EY,UH}, {AXR,AX}, {XN,XM},
{P,B,F}, {DH,CH,ZH,T,SH}, {OY,AE},
{IY,AY,OW}, {EH,AH,IH,AW,ER,UW}.
The phone clustering algorithm that pro-
duced these is detailed in the previous
section.
After a series of edit operations, D
?1
trans-
ducer maps new phonetic sequences from pronun-
ciations to n-grams of words. The k-best variants
resulting from the weighted composition are the
k-best plausible misrecognitions.
One important property of this method is that it
maps words in decoding vocabulary (41,487 types
are possible inputs to transducer D) into CMU
dictionary which is substantially larger (141,304
types are possible outputs of transducer D
?1
).
This allows to generate out-of-vocabulary (OOV)
words and phrases, which are not only recogni-
tion errors, but also plausible variants of different
source phrases that can be translated to one tar-
get phrase, e.g., verb past tense forms or function
words.
Consider a bigram tells the from our synthetic
phrase example in Figure 2. We first obtain
its phonetic representation [T EH L Z] [DH IY],
and then a sequence of possible edit operations
is Substitute(T, CH), Substitute(Z, S), Delete(DH)
and translation of phonetic sequence [CH EH L S
IY] back to words brings us to chelsea. See Fig-
ure 4 for visualization.
5 Experimental setups
To establish the effectiveness and ro-
bustness of our approach, we conducted
two sets of experiments?expASR and
expMultilingual?with transcribed and
tells the  T EH L Z DH IY
chelsea CH EH L S    IY
Figure 4: Pseudo-ASR output generation exam-
ple for a bigram tells the. Phonetic edits are
Substitute(T, CH), Substitute(Z, S), Delete(DH).
translated TED talks (Cettolo et al., 2012b).
6
En-
glish is the source language in all the experiments.
In expASR we used tst2011?the official test
set of the SLT track of the IWSLT 2011 evalu-
ation campaign on the English-French language
pair (Federico et al., 2011).
7
This test set com-
prises reference transcriptions of 8 talks (approx-
imately 1.1h of speech, segmented to 818 utter-
ances), 1-best hypotheses from five different ASR
systems, a ROVER combination of four systems
(Fiscus, 1997), and three sets of lattices produced
by the participants of the IWSLT 2011 ASR track.
In this set of experiments we compare baseline
systems performance to a performance of systems
augmented with synthetic phrases on (1) reference
transcriptions, (2) 1-best hypotheses from all re-
leased ASR systems, and (3) a set of ASR lattices
produced by FBK (Ruiz et al., 2011).
8
Experi-
ments with individual systems are aimed to val-
idate that MT augmented with synthetic phrases
can better translate ASR outputs with recogni-
tion errors and sequences that were not observed
in the MT training data. Consistency in perfor-
mance across different ASRs is expected if our ap-
proach to generate plausible misrecognition vari-
ants is universal, rather than biased to a specific
system. Comparison of 1-best system with syn-
thetic phrases to lattice decoding setup without
synthetic phrases should demonstrate whether n-
best plausible misrecognition variants that we gen-
erate assemble multiple paths through a lattice.
The purpose of expMultilingual is to
show that translation improvement is consistent
across different target languages. This multilin-
gual experiment is interesting because typologi-
cally different languages pose different challenges
to translation (degree and locality of reordering,
morphological richness, etc.). By showing that
we improve results across languages (even with
6
http://www.ted.com/
7
http://iwslt2011.org/doku.php?id=06_evaluation#slt_track_
english_to_french
8
Pruning threshold for lattices is 0.08.
619
the same underlying ASR system), we show that
our technique is robust to the different demands
that languages place on the translation model. We
could not find any publicly available multilingual
data sets of the translated speech,
9
therefore we
constructed a new test set.
We use our in-house speech recognizer and
evaluate on locally crawled and pre-processed
TED audio and text data. We build SLT systems
for five target languages: French, German, Rus-
sian, Hebrew, and Hindi. Consequently, our test
systems are diverse typologically and trained on
corpora of different sizes. We sample a test set of
seven talks, representing approximately two hours
of English speech, for which we have translations
to all five languages;
10
talks are listed in Table 1.
Due to segmentation differences in the released
TED (text) corpora and then several automatic
preprocessing stages, numbers of sentences for
the same talks are not identical across languages.
Therefore, we select English-French system as an
oracle (this is the largest dataset), and first align it
with the ASR output. Then, we filter out test sets
for non-French MT systems, to retain only sen-
tence pairs that are included in the English-French
test set. Thus, our test sets for non-French MT
systems are smaller, and source-side sentences in
the English-French MT is a superset of source-side
sentences in all five languages. Training, tuning,
and test corpora sizes are listed in Table 2. Same
training and development sets were used in both
expASR and expMultilingual experiments.
Training Dev Test
EN?FR 140,816 2,521 843
EN?DE 130,010 2,373 501
EN?RU 117,638 2,380 735
EN?HE 135,366 2,501 540
EN?HI 126,117 2,000 300
Table 2: Number of sentences in training, dev and
expMultilingual test corpora.
5.1 ASR
In the expMultilingual set of experiments,
we employ the JANUS Recognition Toolkit that
features the IBIS single pass decoder (Soltau et
9
After we conducted our experiments, a new multilingual
parallel corpus of translated speech was released for SLT
track of IWSLT 2013 Evaluation Campaign, however, this
data set does not include Russian, Hebrew and Hindi, which
are a subject of this research.
10
Since TED translation is a voluntary effort, not all talks
are available in all languages.
al., 2001). The acoustic model is maximum
likelihood system, no speaker adaptation or dis-
criminative training applied. The acoustic model
training data is 186h of Broadcast News-style
data. 5-gram language model with modified
Kneser-Ney smoothing is trained with the SRILM
toolkit (Stolcke, 2002) on the EPPS, TED, News-
Commentary, and the Gigaword corpora. The
Broadcast News test set contains 4h of audio; we
obtain 25.6% word error rate (WER) on this test
set.
We segment the TED test audio by the times-
tamps of transcripts appearance on the screen.
Then, we manually detect and discard noisy hy-
potheses around segmentation boundaries, and
manually align the remaining hypotheses with
the references which are the source side of the
English-French MT test set. The resulting test
set of 843 hypotheses, sentence aligned with tran-
scripts, yields 30.7% WER. Higher error rates (rel-
atively to the Broadcast News baseline) can be
explained by the idiosyncratic nature of the TED
genre, and the fact that our ASR system was not
trained on the TED data.
For the expASR set of experiments the ASR
outputs and lattices in standard lattice format
(SLF) were produces by the participants of IWSLT
2011 evaluation campaign.
5.2 MT
We train and test MT using the TED corpora in
all five languages. For French, German and Rus-
sian we use sentence-aligned training and develop-
ment sets (without our test talks) released for the
IWSLT 2012 evaluation campaign (Cettolo et al.,
2012a); we split Hebrew and Hindi to training and
development respectively.
11
We split Hebrew and
Hindi to sentences with simple heuristics, and then
sentence-align with the Microsoft Bilingual Sen-
tence Aligner (Moore, 2002). Punctuation marks
were removed, corpora were lowercased, and tok-
enized using the cdec scripts (Dyer et al., 2010).
In all MT experiments, both for sentence and
lattice translation, we employ the Moses toolkit
(Koehn et al., 2007), implementing the phrase-
based statistical MT model (Koehn et al., 2003)
and optimize parameters with MERT (Och, 2003).
Target language 3-gram Kneser-Ney smoothed
11
Since TED Hindi corpus is very small (only about 6K
sentences) we augment it with additional parallel data (Bojar
et al., 2010); however, this improved Hindi system quality
only marginally, probably owing to domain mismatch.
620
TED id TED talk
1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006
39 Aubrey de Grey: A roadmap to end aging, 2005
142 Alan Russell: The potential of regenerative medicine, 2006
228 Alan Kay shares a powerful idea about ideas, 2007
248 Alisa Miller: The news about the news, 2008
451 Bill Gates: Mosquitos, malaria and education, 2009
535 Al Gore warns on latest climate trends, 2009
Table 1: Test set of TED talks.
language models are trained on the training part
of each corpus. Results are reported using case-
insensitive BLEU with a single reference and no
punctuation (Papineni et al., 2002). To verify
that our improvements are consistent and are not
just an effect of optimizer instability (Clark et al.,
2011), we train three systems for each MT setup.
Statistical significance is measured with the Mul-
tEval toolkit.
12
Reported BLEU scores are aver-
aged over three systems.
In MT adaptation experiments we augment
baseline phrase tables with synthetic phrases. For
each entry in the original phrase table we add (at
most) five
13
best acoustic confusions, detailed in
Section 4. Table 3 contains sizes of phrase tables,
original and augmented with synthetic phrases.
Original Synthetic
EN?FR 4,118,702 24,140,004
EN?DE 2,531,556 14,807,308
EN?RU 1,835,553 10,743,818
EN?HE 2,169,397 12,692,641
EN?HI 478,281 2,674,025
Table 3: Sizes of phrase tables from the baseline
systems, and phrase tables with synthetic phrases.
6 Experiments
6.1 expASR
We first measure the phrasal coverage of recog-
nition errors that our technique is able to predict.
We compute a number of 1- and 2-gram phrases
in ASR hypotheses from the tst2011 that are
not in the references: these are ASR errors. Then,
we compare their OOV rate in the English-French
phrase tables, original vs. synthetic. The pur-
pose of synthetic phrases is to capture misrecog-
nized sequences, ergo, reduction in OOV rate of
12
https://github.com/jhclark/multeval
13
This threshold is of course rather arbitrary. In future ex-
periments we are planning to conduct an in-depth investiga-
tion of the threshold value, based on ASR LM score and pho-
netic distance from the original phrase.
ASR errors in synthetic phrase tables corresponds
to the portion of errors that our method was able
to predict. Table 4 shows that the OOV rate of n-
grams in phrase tables augmented with synthetic
phrases drops dramatically, up to 54%. Consis-
tent reduction of recognized errors across outputs
from five different ASR systems confirms that our
error-prediction approach is ASR-independent.
tst2011 #1-grams #2-grams
system0 29 (50.9%) 230 (20.3%)
system1 27 (41.5%) 234 (21.3%)
system2 36 (36.0%) 230 (20.1%)
system3 34 (44.1%) 275 (20.1%)
system4 46 (52.9%) 182 (16.8%)
ROVER 30 (54.5%) 183 (18.7%)
Table 4: Phrasal coverage of recognition errors
that our technique is able to predict. These are
raw counts of 1-gram and 2-gram types that are
OOVs in the baseline system and are recovered
by our method when we augment the system with
plausible misrecognitions. Percentages in paren-
theses show OOV rate reduction due to recovered
n-grams.
Next, we explore the effect of synthetic phrases
on translation performance, across different (1-
best) ASR outputs. For references, ASR hypothe-
ses, and ROVERed hypotheses we compare trans-
lations produced by MT systems trained with and
without synthetic phrases. We detail our findings
in Table 5.
Improvements in translation are significant for
all systems with synthetic phrases. This experi-
ment corroborates the underlying assumption that
simulated ASR errors are paired with correct tar-
get phrases. Moreover, this experiment supports
the claim that incorporating noisier translations in
the translation model successfully adapts MT to
SLT scenario and has indeed a positive effect on
speech translation. Interestingly, improvement of
reference translations is also observed. We spec-
ulate that this stems from better lexical selection
due to a smoothing effect that our technique may
621
WER
BLEU
Baseline
BLEU
Synthetic
p
references - 30.8 31.2 0.05
system0 22.0 24.3 25.0 <0.01
system1 23.3 23.8 24.3 <0.01
system2 21.1 23.9 24.4 0.02
system3 32.4 20.8 21.3 <0.01
system4 19.5 24.5 25.0 0.01
ROVER 17.4 25.0 25.6 0.01
Table 5: Comparison of the baseline translation
systems with the systems augmented with syn-
thetic phrases. We measure EN?FR MT perfor-
mance on the tst2011 test set: reference tran-
scripts and ASR outputs on from five systems
and their ROVER combination. Improvements in
translation of all ASR outputs are statistically sig-
nificant. This confirms the claim that incorporat-
ing simulated ASR errors via synthetic phrases ef-
fectively adapts MT to SLT scenario.
have.
Finally, we contrast the proposed approach of
translation models adaptation to a conventional
method of lattice translation. We decode FBK lat-
tices produced for IWSLT 2011 Evaluation Cam-
paign, and compare results to FBK 1-best transla-
tion results, which correspond to system1 in Table
5. Table 6 summarizes our main finding: 1-best
system with synthetic phrases significantly outper-
forms lattice decoding setup with baseline trans-
lation table.
14
The additional small improvement
in lattice decoding with synthetic phrases suggests
that lattice decoding and phrase table adaptation
are two complementary strategies and their com-
bination is beneficial.
6.2 expMultilingual
In the multilingual experiment we train ten MT se-
tups: five baseline setups and five systems with
synthetic phrases, three systems per setup. For
each system we compare translations of the refer-
ence transcripts and ASR hypotheses on the multi-
lingual test set described in Section 6. We evaluate
translations produced by MT systems trained with
and without synthetic phrases. Table 7 summa-
rizes experimental results, along with the test set
WER for each language.
14
Automatic evaluation results (in terms of BLEU) pub-
lished during the IWSLT 2011 Evaluation Campaign (Fed-
erico et al., 2011) (p. 21) are 26.1 for FBK systems. Unsur-
prisingly, performance of our systems is lower, as we focus
only on translation table and do not optimize factors, such as
LMs and others.
BLEU
Baseline
BLEU
Synthetic
FBK 1-best 23.8 24.3
FBK lattices 24.0 24.4
Table 6: Comparison of the baseline EN?FR trans-
lation systems with the systems augmented with
synthetic phrases, in 1-best and lattice decoding
setups. 1-best synthetic system significantly out-
performs baseline lattice decoding setup. Addi-
tional improvement in lattice decoding with syn-
thetic phrases suggests that lattice decoding and
phrase table adaptation are two complementary
strategies.
WER Baseline Synthetic
Ref ASR Ref ASR
EN?FR 30.7
23.3 17.8 23.9 18.1
EN?DE 33.6
14.0 11.1 14.2 11.4
EN?RU 30.7
12.3 10.7 12.2 10.6
EN?HE 29.7
9.2 7.0 9.5 7.2
EN?HI 32.1
5.5 4.5 5.6 4.8
Table 7: Comparison of the baseline translation
systems with the systems augmented with syn-
thetic phrases. We measure MT performance on
the reference transcripts and ASR outputs. Con-
sistent improvements are observed in four out of
five languages.
Modest but consistent improvements are ob-
served in four out of five setups with synthetic
phrases. Only French setup yielded statistically
significant improvement (p < .01). However,
if we concatenate the outputs of all languages,
the improvement in translation of references with
BLEU score averaged over all systems becomes
statistically significant (p = .03), improving from
16.8 for the baseline system to 17.3 for the adapted
MT outputs. While more careful evaluation is re-
quired in order to estimate the effect of acous-
tic confusions, the accumulated result show that
synthetic phrases facilitate MT adaptation to SLT
across languages.
7 Analysis
We conducted careful manual analysis of actual
usages of synthetic phrases in translation. The pur-
pose of this qualitative analysis is to verify that
predicted ASR errors are paired with phrases that
contribute to better translation to a target language.
Table 8 shows some examples. In the first sentence
from the tst2011 test set (output from system 4)
the word area was erroneously recognized as airy,
622
English ref so what they do is they move into an area
ASR output so what they do is they move into an airy
Baseline MT donc ce qu?ils font c?est qu?ils se d?placer dans un airy
Synthetic MT donc ce qu?ils font c?est qu?ils se d?placer dans une zone
French ref donc ce qu?ils font c?est qu?ils emm?nagent dans une zone
English ref so i started thinking and listing what all it was that i thought would make a perfect biennial
ASR output so on i started a thinking and listing was all it was that i thought would make a pretty by neil
Baseline MT donc j?ai commenc? ? une pens?e et listing ?tait tout c??tait que je pensais ferait un assez par neil
Synthetic MT donc j?ai commenc? ? penser et une liste ?tait tout c??tait que je pensais ferait un assez par neil
French ref alors j?ai commenc? ? penser et ? lister tout ce qui selon moi ferait une biennale parfaite
Table 8: Examples of translations improved with synthetic phrases.
which is an OOV word for the baseline system.
Our confusion generation algorithm also produced
the word airy as a plausible misrecognition variant
for the word area and attached it to a correct tar-
get phrase zone, and this synthetic phrase was se-
lected during decoding, yielding to a correct trans-
lation for the ASR error. Second example shows a
similar behavior for an indefinite article a. Third
example is taken from the English-Russian system
in the multilingual test set. Gauge was produced
as a plausible misrecognition variant to age, and
therefore correctly translated (albeit incorrectly in-
flected) as ????????(age+sg+m+acc). Synthetic
phrases were also used in translations contain-
ing misrecognized function words, segmentation-
related examples, and longer n-grams.
8 Related work
Predicting ASR errors to improve speech recog-
nition quality has been explored in several previ-
ous studies. Jyothi and Fosler-Lussier (2009) de-
velop weighted finite-state transducer framework
for error prediction. They build a confusion ma-
trix FST between phones to model acoustic errors
made by the recognizer. Costs in the confusion
matrix combine acoustic variations in the HMM
representations of the phones (information from
the acoustic model) and word-based phone confu-
sions (information from the pronunciation model).
In their follow-up work, Jyothi and Fosler-Lussier
(2010) employ this error-predicting framework to
train the parameters of a global linear discrimina-
tive language model that improves ASR.
Sagae et al. (2012) examined three protocols
for ?hallucinating? ASR n-best lists. First ap-
proach generates confusions on the phone level,
with a phone-based finite-state transducer that em-
ploys real n-best lists produced by the ASR sys-
tem. Second is generating confusions at the word
level with a MT-based approach. Third is a phrasal
cohorts approach, in which acoustically confus-
able phrases are extracted from ASR n-best lists,
based on pivots?identical left and right contexts of
a phrase. All three methods were evaluated on the
task of ASR improvement through decoding with
discriminative language models. Discriminative
language models trained on simulated n-best lists
produced with phrasal cohorts method yielded the
largest WER reduction on the telephone speech
recognition task.
Our approach to generating plausible ASR mis-
recognitions is similar to previously explored FST-
based methods. The fundamental difference, how-
ever, is in speech-free phonetic confusion trans-
ducer that does not employ any data extracted
from acoustic models or ASR outputs. Simulated
ASR errors are typically used to improve ASR ap-
plications. To the best of our knowledge no prior
work has been done on integrating ASR errors di-
rectly in the translation models.
9 Conclusion
The idea behind the novel ASR error-prediction
algorithm that we devise is to identify phonolog-
ical neighbors with similar distributional proper-
ties, i.e. similar sounding words for which lan-
guage model probabilities are insufficient for their
disambiguation. These sequences have been iden-
tified as significant contributors to ASR errors
(Goldwater et al., 2010). Additional and even
more important factors that cause recognition er-
rors are disfluencies in speech (Tsvetkov et al.,
2013b). In the task of adapting MT to SLT these
and other irregularities can effectively be incor-
porated in a useful general framework: synthetic
phrases that augment phrase tables. Our exper-
iments show that simulated acoustic confusions
capture real ASR errors and that proposed frame-
work effectively exploits them to improve transla-
tion.
623
Acknowledgments
We are grateful to Jo?o Miranda and Alan Black for providing
us the TED audio with transcriptions, and to Zaid Sheikh for
his help with ASR decoding. This work was supported in part
by the U. S. Army Research Laboratory and the U. S. Army
Research Office under contract/grant number W911NF-10-1-
0533.
References
Waleed Ammar, Victor Chahuneau, Michael
Denkowski, Greg Hanneman, Wang Ling, Austin
Matthews, Kenton Murray, Nicola Segall, Yulia
Tsvetkov, Alon Lavie, and Chris Dyer. 2013.
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proc. ICASSP, pages 1297?1300. IEEE.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In Proceedings of LREC.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL, pages 17?24. Association for Compu-
tational Linguistics.
Francisco Casacuberta, Marcello Federico, Hermann
Ney, and Enrique Vidal. 2008. Recent efforts
in spoken language translation. Signal Processing
Magazine, IEEE, 25(3):80?88.
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Michael Paul, and Sebastian St?ker. 2012a.
Overview of the IWSLT 2012 evaluation campaign.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012b. WIT
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT, pages
261?268, Trento, Italy.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of ACL, pages
176?181. Association for Computational Linguis-
tics.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012?1020. Asso-
ciation for Computational Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian St?ker. 2011. Overview of the
IWSLT 2011 evaluation campaign. In Proc. IWSLT,
pages 8?9.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. ASRU,
pages 347?352. IEEE.
Sharon Goldwater, Dan Jurafsky, and Christopher D
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Xiaodong He and Li Deng. 2013. Speech-centric in-
formation processing: An optimization-oriented ap-
proach. IEEE, 101(5):1116?1135.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
Preethi Jyothi and Eric Fosler-Lussier. 2009. A com-
parison of audio-free speech recognition error pre-
diction methods. In Proc. INTERSPEECH, pages
1211?1214.
Preethi Jyothi and Eric Fosler-Lussier. 2010. Discrimi-
native language modeling using simulated asr errors.
In Proc. INTERSPEECH, pages 1049?1052.
Stanislav V Kasl and George F Mahl. 1965. The re-
lationship of disturbances and hesitations in spon-
taneous speech to anxiety. In Journal of Personal-
ity and Social Psychology, volume 1(5), pages 425?
433.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 48?54. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL, pages 177?180. Asso-
ciation for Computational Linguistics.
Evgeny Matusov, Stephan Kanthak, and Hermann Ney.
2006. Integrating speech recognition and machine
translation: Where do we stand? In Proc. ICASSP,
pages V?1217?V?1220. IEEE.
624
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings
of AMTA, pages 135?144, London, UK. Springer-
Verlag.
Hermann Ney. 1999. Speech translation: Coupling of
recognition and translation. In Proc. ICASSP, vol-
ume 1, pages 517?520. IEEE.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine
translation. In Proceedings of ACL.
Mari Ostendorf, Beno?t Favre, Ralph Grishman, Dilek
Hakkani-Tur, Mary Harper, Dustin Hillard, Julia
Hirschberg, Heng Ji, Jeremy G Kahn, Yang Liu,
Sameer Maskey, Evgeny Matusov, Hermann Ney,
Andrew Rosenberg, Elizabeth Shriberg, Wen Wang,
and Chuck Wooters. 2008. Speech segmentation
and spoken document processing. Signal Process-
ing Magazine, IEEE, 25(3):59?69.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318. Association for Computa-
tional Linguistics.
Stephan Peitz, Simon Wiesler, Markus Nu?baum-
Thom, and Hermann Ney. 2012. Spoken language
translation using automatically transcribed text in
training. In Proc. IWSLT.
Vu H Quan, Marcello Federico, and Mauro Cettolo.
2005. Integrated n-best re-ranking for spoken lan-
guage translation. In Proc. INTERSPEECH, pages
3181?3184. IEEE.
Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele
Falavigna, Diego Giuliani, Suhel Jaber, Roberto
Gretter, and Marcello Federico. 2011. FBK@
IWSLT 2011. In Proc. IWSLT.
Kenji Sagae, M. Lehr, E. Prud?hommeaux, P. Xu,
N. Glenn, D. Karakos, S. Khudanpur, B. Roark,
M. Sara?lar, I. Shafran, D. Bikel, C. Callison-Burch,
Y. Cao, K. Hall, E. Hasler, P. Koehn, A. Lopez,
M. Post, and D. Riley. 2012. Hallucinated n-best
lists for discriminative language modeling. In Proc.
ICASSP. IEEE.
H. Soltau, F. Metze, C. F?gen, and A. Waibel. 2001.
A one-pass decoder based on polymorphic linguistic
context assignment. In Proc. ASRU.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proc. ICSLP, pages 901?
904.
Jean E Fox Tree. 1995. The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. Journal of memory and lan-
guage, 34(6):709?738.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Bhatia. 2013a. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of WMT. Association for
Computational Linguistics.
Yulia Tsvetkov, Zaid Sheikh, and Florian Metze.
2013b. Identification and modeling of word frag-
ments in spontaneous speech. In Proc. ICASSP.
IEEE.
Ping Xu, Pascale Fung, and Ricky Chan. 2012.
Phrase-level transduction model with reordering for
spoken to written language transformation. In Proc.
ICASSP, pages 4965?4968. IEEE.
Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya-
mamoto, Taro Watanabe, Frank Soong, and Wai Kit
Lo. 2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. In Proceedings
of COLING, page 1168. Association for Computa-
tional Linguistics.
Bowen Zhou, Laurent Besacier, and Yuqing Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. In Proc. ICASSP, volume 4,
pages IV?101. IEEE.
Bowen Zhou. 2013. Statistical machine translation for
speech: A perspective on structures, learning, and
decoding. IEEE, 101(5):1180?1202.
625
Identification of Multiword Expressions
by Combining Multiple Linguistic
Information Sources
Yulia Tsvetkov?
Carnegie Mellon University
Shuly Wintner??
University of Haifa
We propose a framework for using multiple sources of linguistic information in the task of
identifying multiword expressions in natural language texts. We define various linguistically
motivated classification features and introduce novel ways for computing them. We then man-
ually define interrelationships among the features, and express them in a Bayesian network.
The result is a powerful classifier that can identify multiword expressions of various types
and multiple syntactic constructions in text corpora. Our methodology is unsupervised and
language-independent; it requires relatively few language resources and is thus suitable for a
large number of languages. We report results on English, French, and Hebrew, and demonstrate
a significant improvement in identification accuracy, compared with less sophisticated baselines.
1. Introduction
Multiword expressions (MWEs) are lexical items that consist of multiple orthographic
words (ad hoc, New York, look up). MWEs constitute a significant portion of the lexicon
of any natural language (Jackendoff 1997; Erman and Warren 2000; Sag et al. 2002). They
are a heterogeneous class of constructions with diverse sets of characteristics, distin-
guished by their idiosyncratic behavior. Morphologically, some MWEs allow some of
their constituents to freely inflect while restricting (or preventing) the inflection of other
constituents. In some cases MWEs may allow constituents to undergo non-standard
morphological inflections that they would not undergo in isolation. Syntactically, some
MWEs behave like words and other are phrases; some occur in one rigid pattern (and a
fixed order), and others permit various syntactic transformations. The most characteris-
tic property of MWEs is their semantic opacity, although the compositionality of MWEs
is gradual, and ranges from fully compositional to completely idiomatic (Bannard,
Baldwin, and Lascarides 2003).
? 2014 Association for Computational Linguistics
? Language Technologies Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA
15213-3891. E-mail: ytsvetko@cs.cmu.edu.
?? Department of Computer Science, University of Haifa Mount Carmel, 31905 Haifa, Israel.
E-mail: shuly@cs.haifa.ac.il.
Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication:
16 August 2013.
doi:10.1162/COLI a 00177
Computational Linguistics Volume 40, Number 2
Because of their prevalence and irregularity, MWEs must be stored in lexicons
of natural language processing (NLP) applications. Awareness of MWEs was proven
beneficial for a variety of applications, including information retrieval (Doucet and
Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo
2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin
and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010).
We propose a novel architecture for identifying MWEs, of various types and syn-
tactic categories, in monolingual corpora. Unlike much existing work, which focuses
on a particular syntactic construction, our approach addresses MWEs of various types
by zooming in on the general idiosyncratic properties of MWEs rather than on spe-
cific properties of each subclass thereof. Addressing multiple types of MWEs has its
limitations: The task is less well-defined, one cannot rely on specific properties of
a particular construction, and the type of the MWE is not extracted along with the
candidate expression. Nevertheless, there are clear benefits to such an approach. Certain
applications can benefit from a large, albeit untyped, mixed bag of MWEs; machine
translation is an obvious candidate (Lambert and Banchs 2005; Ren et al. 2009; Bouamor,
Semmar, and Zweigenbaum 2012). Another use, which motivates our current work, is
the construction of computational lexicons. Clearly, manual supervision is required be-
fore MWE candidates are added to a high-precision lexicon, but our approach provides
the lexicographer with a large-scale set of potential candidates.
We focus on bigrams only in this work, that is, on MWEs consisting of two consec-
utive tokens. Many of the features we design, as well as the general architecture, can in
principle be extended to longer MWEs, but we do not address longer (and, in particular,
the harder case of non-contiguous) MWEs here. The architecture uses Bayesian net-
works (Pearl 1985) to express multiple interdependent linguistically motivated features.
First, we automatically generate a small (training) set of MWE and non-MWE
bigrams (positive and negative instances, respectively) from a small parallel corpus.
We then define a set of linguistically motivated features that embody observed char-
acteristics of MWEs. We augment these by features that reflect collocation measures.
Finally, we define dependencies among these features, expressed in the structure of a
Bayesian network model, which we then use for classification. A Bayesian network (BN)
is a directed graph whose nodes express the features used for classification and whose
edges define causal relationships among these features. In this architecture, learning
does not result in a black box, expressed solely as feature weights. Rather, the structure
of the BN allows us to study the impact of different MWE features on the classification.
The result is a new method for identifying MWEs of various types in text corpora. It
combines statistics with an array of linguistically motivated features, organized in an
architecture that reflects interdependencies among the features.
The contribution of this work is manifold.1 First, we use existing approaches to
MWE extraction to automatically generate training material. Specifically, we use our
earlier work (Tsvetkov and Wintner 2012) to extract a set of positive and negative MWE
candidates from a small parallel corpus, and use them for training a BN that can then
extract a new set of MWEs from a potentially much larger monolingual corpus. As
1 This article is a thoroughly revised and extended version of Tsvetkov and Wintner (2011). Whereas the
methodology of that paper required minor supervision, we now present a completely unsupervised
approach. We added several linguistically motivated features to the classification task. We demonstrate
results on two new languages, English and French, to emphasize the generality of the method.
Additional extensions include a more complete literature survey and, because new languages are added,
different, more reliable data sets for evaluating our results.
450
Tsvetkov and Wintner Identification of Multiword Expressions
a result, our method is completely unsupervised (more precisely, it does not require
manual annotation; we do need several language resources, see Section 3.2).
Second, we propose several linguistically motivated features that can be computed
from data and that are demonstrably productive for improving the accuracy of MWE
identification. These features focus on the expression of linguistic idiosyncrasies of var-
ious types, a phenomenon typical of MWEs. Some of these features are commonplace,
but others are new, or are implemented in novel ways. In particular, we account for
the morphological idiosyncrasy of MWEs using a histogram of the number of inflected
forms, in a technique that draws from image processing. We also use frequency his-
tograms to model the semantic contexts of MWEs.
Finally, the methodology we advocate is not language-specific; given relatively few
language resources, it can be easily adapted to new languages. We demonstrate the
generality of our methodology by applying it to three languages: English, French, and
Hebrew. Our evaluation shows that the use of linguistically motivated features results
in a reduction of between one quarter and one third of the errors compared with a
collocation baseline; organizing the knowledge in a Bayesian network reduces the error
rate by an additional 3?9%.
After discussing related work in the next section (borrowing from Tsvetkov and
Wintner [2012]), we motivate in Section 3 the methodology we propose, and list the re-
sources needed for implementing it. Section 4 discusses the linguistically motivated fea-
tures and their implementation; the organization of the Bayesian network is described
in Section 5. We explain how we generate training materials in Section 6. Section 7
provides a thorough evaluation of the results. We conclude with suggestions for future
research.
2. Related Work
Early approaches to MWE identification concentrated on their collocational behavior
(Church and Hanks 1990). One of the first approaches was implemented as Xtract
(Smadja 1993): Here, word pairs that occur with high frequency within a context of
five words in a corpus are first collected, and are then ranked and filtered according
to contextual considerations, including the parts of speech of their neighbors. Pecina
(2008) compares 55 different association measures in ranking German Adj-N and PP-
Verb collocation candidates. He shows that combining different collocation measures
using standard statistical classification methods improves over using a single colloca-
tion measure. Other results (Chang, Danielsson, and Teubert 2002; Villavicencio et al.
2007) suggest that some collocation measures (especially point-wise mutual information
and log-likelihood) are superior to others for identifying MWEs.
Co-occurrence measures alone are probably not enough to identify MWEs, and their
linguistic properties should be exploited as well (Piao et al. 2005). Hybrid methods that
combine word statistics with linguistic information exploit morphological, syntactic,
and semantic idiosyncrasies to extract idiomatic MWEs.
Cook, Fazly, and Stevenson (2007), for example, use prior knowledge about the
overall syntactic behavior of an idiomatic expression to determine whether an instance
of the expression is used literally or idiomatically. They assume that in most cases,
idiomatic usages of an expression tend to occur in a small number of canonical forms
for that idiom; in contrast, the literal usages of an expression are less syntactically
restricted, and are expressed in a greater variety of patterns, involving inflected forms
of the constituents.
451
Computational Linguistics Volume 40, Number 2
Ramisch et al. (2008) evaluate a number of association measures on the task of
identifying English verb-particle constructions and German adjective-noun pairs. They
show that adding linguistic information (mostly POS and POS-sequence patterns) to the
association measure yields a significant improvement in performance over using pure
frequency.
Several works address the lexical fixedness or syntactic fixedness of (certain types
of) MWEs in order to extract them from texts. An expression is considered lexically
fixed if replacing any of its constituents by a semantically (and syntactically) similar
word generally results in an invalid or literal expression. Syntactically fixed expressions
prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moiro?n
(2007) use lexical fixedness to extract Dutch verb-noun idiomatic combinations (VNICs).
Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses
both the syntactic and the lexical fixedness of VNICs in order to distinguish them from
non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson
2006). Recently, Green et al. (2011) use parsing, and in particular Tree Substitution
Grammars, for identifying MWEs in French.
Semantic properties of MWEs can be used to distinguish between compositional
and non-compositional (idiomatic) expressions. Katz and Giesbrecht (2006) and
Baldwin et al. (2003) use Latent Semantic Analysis (LSA) for this purpose. They show
that compositional MWEs appear in contexts more similar to their constituents than
non-compositional MWEs. For example, the co-occurrence measured by LSA between
the expression kick the bucket and the word die is much higher than co-occurrence
of this expression and its component words. The disadvantage of this methodology is
that to distinguish between idiomatic and non-idiomatic usages of the MWE it relies on
the MWE?s known idiomatic meaning, and this information is usually not available. In
addition, this approach fails when only idiomatic or only literal usages of the MWE are
overwhelmingly frequent.
Although these approaches are in line with ours, they require lexical semantic
resources (e.g., a database that determines semantic similarity among words) and
syntactic resources (parsers) that are unavailable for many languages. Our approach
only requires morphological processing and a bilingual dictionary, which are more
readily available for several languages. Note also that these approaches target a specific
syntactic construction, whereas ours is appropriate for various types of MWEs.
Several properties of Hebrew MWEs are described by Al-Haj (2010); Al-Haj and
Wintner (2010) use them in order to construct a support vector machine (SVM) classifier
that can distinguish between MWE and non-MWE noun-noun constructions in Hebrew.
The features of the SVM reflect several morphological and morphosyntactic properties
of such constructions. The resulting classifier performs much better than a naive base-
line, reducing the error rate by over one third. We rely on some of these insights, as we
implement more of the linguistic properties of MWEs. Again, our methodology is not
limited to a particular construction: Indeed, we demonstrate that our general methodol-
ogy, trained on automatically generated, general training data, performs almost as well
as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same data
set (Section 7).
Recently, Tsvetkov and Wintner (2010b, 2012) introduced a general methodology
for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results
were a highly accurate set of Hebrew MWEs, of various types, along with their English
translations. A major limitation of this work is that it can only be used to identify MWEs
in the bilingual corpus, and is thus limited in its scope. We use this methodology to
extract both positive and negative instances for our training set in the current work;
452
Tsvetkov and Wintner Identification of Multiword Expressions
but we extrapolate the results much further by extending the method to monolingual
corpora, which are typically much larger than bilingual ones.
Probabilistic graphical models are widely used in statistical machine learning in
general, and natural language processing in particular (Smith 2011). Bayesian networks
are an instance of such models, and have been used for classification in several natural
language applications. For example, BNs have been used for POS tagging of unknown
words (Peshkin, Pfeffer, and Savova 2003), dependency parsing (Savova and Peshkin
2005), and document classification (Lam, Low, and Ho 1997; Calado et al. 2003; Denoyer
and Gallinari 2004). Very recently, Ramisch et al. (2010) used BN for Portuguese MWE
identification. The features used for classification were of two kinds: (1) various colloca-
tion measures; (2) bigrams aligned together by an automatic word aligner applied to a
parallel (Portuguese?English) corpus. A BN was used to combine the predictions of the
various features on the test set, but the structure of the network is not described. The
combined classifier resulted in a much higher accuracy than any of the two methods
alone. However, the use of BN is not central to this work, and its structure does not
reflect any insights or intuitions on the structure of the problem domain or on inter-
dependencies among features.
We, too, acknowledge the importance of combining different sources of knowledge
in the hard task of MWE identification. In particular, we also believe that collocation
measures are highly important for this task, but cannot completely solve the problem:
Linguistically motivated features are crucial in order to improve the accuracy of the
classifier. In this work we focus on various properties of different types of MWEs,
and define general features that may accurately apply to some, but not necessarily all,
of them. An architecture of Bayesian networks is optimal for this task: It enables us
to define weighted dependencies among features, such that certain features are more
significant for identifying some class of MWEs, whereas others are more prominent
in identifying other classes (although we never predefine these classes). As we show
herein, this architecture results in significant improvements over a more naive combi-
nation of features.
3. Methodology
3.1 Motivation
The task we address is identification of MWEs, of various types and syntactic construc-
tions, in monolingual corpora. These include proper names, noun phrases, verb-particle
pairs, and so forth. We focus on bigrams (MWEs consisting of two consecutive tokens)
in this work; the methodology, however, can be extended to longer n-grams. Several
properties of MWEs make this task challenging: MWEs exhibit idiosyncrasies on a
variety of levels, orthographic, morphological, syntactic, and of course semantic. Such a
complex task calls for a combination of multiple approaches, and much research indeed
suggests ?hybrid? approaches to MWE identification (Duan et al. 2009; Hazelbeck and
Saito 2010; Ramisch et al. 2010; Weller and Fritzinger 2010). We believe that Bayesian
networks provide an optimal architecture for expressing various pieces of knowledge
aimed at MWE identification, for the following reasons (noted, e.g., by Heckerman
1995):
 In contrast to many other classification methods, Bayesian networks can
learn (and express) causal relationships between features. This facilitates
better understanding of the problem domain.
453
Computational Linguistics Volume 40, Number 2
 Bayesian networks can encode not only statistical data, but also
prior domain knowledge and human intuitions, in the form of
interdependencies among features (a possibility that we use here).
Furthermore, we try in this work to leverage the idiosyncrasy of MWEs and use it as a
tool for identifying them.
Our definition of MWEs is operational: An expression is considered a MWE if it
has to be stored in the lexicon of some NLP application; typically, this is because the
expression exhibits some level of idiosyncratic behavior (semantic, syntactic, morpho-
logical, orthographic, etc.). In order to properly handle such expressions in downstream
applications, the lexicon must store some specific information about the expression. This
working definition motivates and drives our methodology: We leverage the idiosyn-
cratic behavior of MWEs and define (Section 4) an array of features that capture and
reflect this idiosyncrasy in order to extract MWEs from corpora.
3.2 Resources
Although our approach is in general not language-specific, applying it to any partic-
ular language requires several language resources which we specify in this section. In
general, we require corpora (both monolingual and bilingual), morphological analyzers
or stemmers, part-of-speech taggers, and bilingual dictionaries. No deeper processing
is assumed (e.g., no parsers or lexical semantic resources are needed). The method we
advocate is thus appropriate for medium-density languages (Varga et al. 2005).
To compute the features discussed in Section 4, we need large monolingual corpora.
For English and French, we use the 109 corpora released for WMT-11 (Callison-Burch
et al. 2011); the corpora were syntactically parsed using the Berkeley parser (Petrov
and Klein 2007), but we only use the POS tags in this work. For Hebrew, we use a
monolingual corpus (Itai and Wintner 2008), which we pre-process as in Tsvetkov and
Wintner (2012): We use a morphological analyzer (Itai and Wintner 2008) to segment
word forms (separating prefixes and suffixes) and induce POS tags. Summary statistics
for each corpus are listed in Table 1.
For some features we need access to the lemma of word tokens. In Hebrew, the
MILA morphological analyzer (Itai and Wintner 2008) provides the lemmas, but the
parsed corpora we use in English and French do not. We therefore use the DELA
dictionaries of English and French, available from LADL as part of the Unitex project
(http://www-igm.univ-mlv.fr/~unitex/). The French dictionary lists 683,824 single-
word entries corresponding to 102,073 lemmas, and 108,436 multiword entries corre-
sponding to 83,604 MWEs. The English dictionary is smaller, with 296,606 single-word
forms corresponding to 150,145 lemmas, and 132,990 multiword entries, corresponding
Table 1
Statistics of the monolingual corpora.
English French Hebrew
Tokens 447,073,250 522,964,336 46,239,285
Types 2,421,181 2,416,269 188,572
Bigram tokens 429,550,149 505,441,224 45,858,152
Bigram types 22,929,768 21,428,007 5,698,581
454
Tsvetkov and Wintner Identification of Multiword Expressions
Table 2
Statistics of the bilingual corpora.
English?French English?Hebrew
Sentences 30,000 30,000 19,626 19,626
Tokens 834,707 895,632 271,787 280,508
Types 22,787 27,880 14,142 12,555
Bigram tokens 804,704 865,632 252,183 280,506
Bigram types 218,108 225,660 128,987 149,688
to 69,912 MWEs. If the corpus surface form is not listed in the dictionary, we use the
surface form in lieu of its lemma. The multiword entries of the DELA dictionaries are
only used for evaluation.
For some features we also need a bilingual dictionary. For English?Hebrew, we
use a small dictionary consisting of 78,313 translation pairs. Some of the entries are
collected manually, whereas others are produced automatically (Itai and Wintner
2008; Kirschenbaum and Wintner 2010). For English?French, because we are unable to
obtain a good-quality dictionary, we use instead Giza++ (Och and Ney 2000) 1-1 word
alignments computed automatically from the entire WMT-11 parallel corpus.
In order to prepare training material automatically (Section 6), we use small bilin-
gual corpora. For English?French, we use a random sample of 30,000 parallel sentences
from the WMT-11 corpus. For English-Hebrew, we use the parallel corpus of Tsvetkov
and Wintner (2010a). Statistics of the parallel corpora are listed in Table 2.
For evaluation we need lists of MWEs, ideally augmented by lists of non-MWE
bigrams. Such lists are notoriously hard to obtain. As a general method of evaluation,
we run 10-fold cross-validation evaluation using the training materials (which we
generate automatically). Additionally, we use three sets of MWEs for evaluation. First,
we extract all the MWE entries from the English WordNet (Miller et al. 1990); we use the
WordNet version that is distributed with NLTK (Bird, Klein, and Loper 2009). Second,
we use the MWEs listed in the DELA dictionaries of English and French (see above).
These sets only include positive examples, of course, so we only report recall results on
them. For Hebrew, we use a small set that was used for evaluation in the past (Al-Haj
and Wintner 2010; Tsvetkov and Wintner 2012). This is a small annotated corpus,
NN, of Hebrew noun-noun constructions. The corpus consists of 413 high-frequency
bigrams of the same syntactic construction; of those, 178 are tagged as MWEs (in this
case, noun compounds) and 235 as non-MWEs. This corpus consolidates the annotation
of three annotators: Only instances on which all three agreed were included. Because it
includes both positive and negative instances, this corpus facilitates a robust evaluation
of precision and recall.
4. Linguistically Motivated Features
We define several linguistically motivated features that are aimed at capturing some
of the unique properties of MWEs. Although many idiosyncratic properties of MWEs
have been previously studied, we introduce novel ways to express these properties as
computable features that inform a classifier. Note that many of the features we describe
in the following are completely language-independent; others are applicable to a wide
range of languages, whereas few are specific to morphologically rich languages, and can
455
Computational Linguistics Volume 40, Number 2
be exhibited in different ways in different languages. We provide examples in English,
French, and Hebrew, drawn from the resources listed in Section 3.2. The methodology
we advocate, however, is completely general.
A common theme for all these features is idiosyncracy: They are all aimed at locating
some linguistic property on which MWEs may differ from non-MWEs. We begin by
detailing these properties, along with the features that we define to reflect them. In all
cases, the feature is applied to a candidate MWE, defined here as a bigram of tokens (all
possible bigrams are potential candidates). The features are computed from the large
monolingual corpora described in Section 3.2. In order for a feature to fire, at least five
instances of the candidate MWE have to be present in the corpus.
Orthographic variation. Sometimes, MWEs are written with hyphens instead of inter-
token spaces. Examples include Hebrew2 xd-cddi (one sided) ?unilateral?, English
elephant-bird, and French aide-soignant (help carer) ?caregiver?. Of course, this feature
is only relevant for languages that use the hyphen in this way.
We define a binary feature, HYPHEN, whose value is 1 iff the corpus includes
instances of the candidate MWE in which the hyphen character connects the two tokens
of the bigram.
Capitalization. MWEs are often named entities, and in languages such as English and
French a large number of MWEs involve words whose first letter is capital. We therefore
define a feature, CAPS, whose value is a binary vector with 1 in the i-th place iff the
i-th word of the MWE candidate is capitalized.3 For example, the White House will
have the value ?0, 1, 1?. This feature is of course irrelevant for languages that do not use
capitalization.
Fossil words. MWEs sometimes include constituents that have no usage outside the
particular expression. Examples include Hebrew ird lTmiwn (went-down to-treasury)
?was lost?, French night club, and English hocus pocus; as far as we know, this is a rather
universal property.
We define a feature, FOSSIL, whose value is a binary vector with 1 in the i-th place
iff the i-th word of the candidate only occurs in this particular bigram; the other words
of the candidate expression can be morphological variants of each other, but must share
the same lemma. For example, the value of FOSSIL for hocus pocus is ?1, 1?, whereas
for French night club it is ?1, 0?. In order to filter out potential typos, candidates must
occur at least five times in the corpus in order for this feature to fire.
Frozen form. MWE constituents sometimes occur in one fixed, frozen form, where the
language?s morphology licenses also other forms. For example, spill the beans does
not license spill the bean, although bean is a valid form. Similarly, Hebrew bit xwlim
(house-of sick-people) ?hospital? requires that the noun xwlim be in the plural; the
variant bit xwlh (house-of sick-person) ?a sick person?s house? only has the literal
meaning. This feature is of use for languages that are not isolating.
2 To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used,
in Hebrew lexicographic order, are abgdhwzxTiklmnsypcqrs?t.
3 Here and in subsequent examples we do not assume that the length of an MWE is limited to 2.
In the present work, however, the vector is of length exactly 2.
456
Tsvetkov and Wintner Identification of Multiword Expressions
We define a feature, FROZEN, whose value is a binary vector with 1 in the i-th place
iff the i-th word of the candidate never inflects in the context of this expression. For
example, the value of FROZEN for spill the beans is ?0, 1, 1?, and for Hebrew bit xwlim
(house-of sick-people) ?hospital? it is ?0, 1?.
Partial morphological inflection. In some cases, MWE constituents undergo a (strict but
non-empty) subset of the full inflections that they would undergo in isolation. For
example, the Hebrew bit ms?pT (house-of law) ?court? occurs in the following inflected
forms: bit hms?pT ?the court? (75%); bit ms?pT ?a court? (15%); bti hms?pT ?the courts? (8%);
and bti ms?pT ?courts? (2%). Crucially, forms in which the second word, ms?pT ?law,? is
in the plural are altogether missing. Our assumption is that the inflection histograms of
non-MWEs are more uniform than the histograms of MWEs, in which some inflections
may be more frequent and others may be altogether missing. Of course, restrictions on
the histogram may stem from the part of speech of the expression; such constraints are
captured by dependencies in the BN structure.
We capture this property, which is again relevant for all non-isolating languages,
with a technique that has been proven useful in the area of image processing (Jain
1989, Section 7.3). We compute a histogram of the distribution in the corpus of all
the possible surface forms of each MWE candidate. Such histograms can compactly
represent distributional information on morphological behavior, in the same way that
histograms of the distribution of gray levels in a picture are used to represent the picture
itself. For example, the histogram corresponding to bit ms?pT (house-of law) ?court?
would be
?(bit hms?pT, 0.75), (bit ms?pT, 0.15), (bti hms?pT, 0.08), (bti ms?pT, 0.02)?
Because each MWE is idiosyncratic in its own way, we do not expect the histograms
of MWEs to have some specific pattern, except non-uniformity. We therefore sort the
columns of each histogram, thereby losing information pertaining to the specific inflec-
tions, and retaining only information about the idiosyncrasy of the histogram. For the
example given, the obtained histogram is ?75, 15, 8, 2?. In contrast, the non-MWE txwm
ms?pT (domain-of law) ?domain of the law?, which is syntactically identical, occurs in
nine different inflected forms, and its sorted histogram is ?59, 14, 7, 7, 5, 2, 2, 2, 2?. The
longer ?tail? of the histogram is typical of compositional expressions.
Off-line, we compute the average histogram for positive and negative examples:
The average histogram of MWEs is shorter and less uniform than the average histogram
of non-MWEs. We define a binary feature, HIST, that determines whether the histo-
gram of the candidate is closer, in terms of L1 (Manhattan) distance, to the average
histogram of positive or of negative examples.
In our corpora, the average histogram of English positive examples has exactly four
elements: 93.62, 5.86, 0.45, and 0.05. This shows a clear tendency (93.62%) of English
MWEs to occur in a single form only; and it also implies that no English MWE occurs in
more than four variants. The English negative instances, in contrast, have a much longer
histogram (12 elements); the first element is 85.83, much lower than the dominating
element of the positive examples. In French, which is morphologically much richer, the
number of elements in the average histogram of positive examples is 32 (the domi-
nating elements are 90.8, 6.9, 1.1, 0.4), whereas the number of elements in the average
histogram of negative examples is 92 (dominated by 75.6, 14.5, 3.9, 2.0).
457
Computational Linguistics Volume 40, Number 2
Context. We hypothesize that MWEs tend to constrain their (semantic) context more
strongly than non-MWEs. We expect words that occur immediately after MWEs to vary
less freely than words that immediately follow other expressions. One motivation for
this hypothesis is the observation that MWEs tend to be less polysemous than free
combinations of words, thereby limiting the possible semantic context in which they
can occur. This seems to us to be a universal property.
We define a feature, CONTEXT, as follows. We first compute a histogram of the
frequencies of words following each candidate MWE. We trim the tail of the histogram
by removing words whose frequency is lower than 0.1% (the expectation is that non-
MWEs would have a much longer tail). Off-line, we compute the same histograms for
positive and negative examples and average them as before. The value of CONTEXT
is 1 iff the histogram of the candidate is closer (in terms of L1 distance) to the positive
average.
For example, the histogram of Hebrew bit ms?pT ?court? includes 15 values, dom-
inated by bit ms?pT yliwn ?supreme court? (20%) and bit ms?pT mxwzi ?district court?
(13%), followed by contexts whose frequency ranges between 5% and 0.6%. In contrast,
the non-MWE txwm ms?pT ?domain-of law? has a much shorter histogram, namely
(12, 11, 6): Over 70% of the words following this expression occur with frequency lower
than 0.1% and are hence in the trimmed tail.
Syntactic diversity. MWEs can belong to various part of speech categories. We define as
feature, POS, the category of the candidate, with values obtained by selecting frequent
tuples of POS tags. For example, English heart attack is Noun-Noun, dark blue is
Adj-Adj, Al Capone is PropN-PropN; French chant fune`bre (song funeral ) ?dirge? is
Noun-Adj, en bas (in low) ?down? is Prep-Adj; Hebrew rkbt hrim (train-of mountains)
?roller-coaster? is Noun-Noun, and so forth.
Translational equivalents. Because MWEs are often idiomatic, they tend to be translated in
a non-literal way, sometimes to a single word. We use a bilingual dictionary to generate
word-by-word translations of candidate MWEs from Hebrew to English, and check
the number of occurrences of the English literal translation in a large English corpus.
For French?English, we check whether the literal translation occurs in the Giza++ (Och
and Ney 2000) alignment results (we use grow-diag-final-and for symmetrization in this
case, to improve the precision). Due to differences in word order between the two
languages, we create two variants for each translation, corresponding to both possible
orders. We expect non-MWEs to have some literal translational equivalent (possibly
with frequency that correlates with their frequency in the source language), whereas for
MWEs we expect no (or few) literal translations. For example, consider Hebrew sprwt
iph (literature pretty) ?belles lettres?. Literal translation of the expression to English
yields literature pretty and pretty literature; we expect these phrases to occur rarely
in an English corpus. In contrast, the compositional tmwnh iph (picture pretty) ?pretty
picture? is much more likely to occur literally in English.
We define a binary feature, TRANS, whose value is 1 iff some literal translation
of the candidate occurs more than five times in the corpus. Although this feature is
not language-specific, we assume that it should work best for pairs of rather distinct
languages.
Collocation. As a baseline, statistical association measure, we use pointwise mutual
information (PMI). We define a binary feature, PMI, with two values, low and high,
458
Tsvetkov and Wintner Identification of Multiword Expressions
MWECAPS
FOSSILHYPHEN CNTXT
POS
HIST
PMI
TRANS
FRZN
Figure 1
The Bayesian network for MWE identification.
reflecting an experimentally determined threshold. Clearly, other association measures
(as well as combinations of more than one) could be used (Pecina 2005).
5. Feature Interdependencies Expressed as a Bayesian Network
A Bayesian network (Jensen and Nielsen 2007) is organized as a directed acyclic
graph whose nodes are random variables and whose edges represent interdependencies
among those variables. We use a particular view of BN, known as causal networks, in
which directed edges lead to a variable from each of its direct causes.4 This facilitates the
expression of domain knowledge (and intuitions, beliefs, etc.) as structural properties of
the network. We use the BN as a classification device: Training amounts to computing
the joint probability distribution of the training set, and classification maximizes the
posterior probability of the particular node (variable) being queried.
For MWE identification we define a BN whose nodes correspond to the features de-
scribed in Section 4. In addition, we define a node, MWE, for the complete classification
task. Over these nodes we impose the structure depicted graphically in Figure 1. This
structure, which we motivate below, is manually defined: It reflects our understanding
of the problem domain and is a result of our linguistic intuition. That said, it can of
course be modified in various ways, and, in particular, new nodes can be easily added
to reflect additional features.
All nodes depend on MWE, as all are affected by whether or not the candidate is
an MWE. The POS of an expression influences its morphological inflection, hence the
edges from POS to HIST and to FROZEN. For example, Hebrew noun-noun constructions
allow their constituents to undergo the full inflectional paradigm, but when such a
construction is a MWE, inflection is severely constrained (Al-Haj and Wintner 2010);
4 The direction of edges is from the target to the observable; this is compatible with the use of BNs in
latent-variable generative models.
459
Computational Linguistics Volume 40, Number 2
similarly, when one of the constituents of a MWE is a conjunction, the entire expression
is very likely to be frozen, as in English by and large and more or less.
Fossil words clearly affect all statistical metrics, hence the edge from FOSSIL to
PMI. They also affect the existence of literal translations, because if a word is not in
the lexicon, it does not have a translation, hence the edge from FOSSIL to TRANS. Also,
we assume that there is a correlation between the frequency (and PMI) of a candidate
and whether or not a literal translation of the expression exists, hence the edge from
PMI to TRANS. The edges from PMI and HIST to CONTEXT are justified by the correlation
between the frequency and variability of an expression and the variability of the context
in which it occurs.
Clearly, the process of determining the structure of the graph, and in particular the
direction of some of the edges, is somewhat arbitrary. Having said that, it does give the
designer of the system a clear and explicit way of expressing linguistically motivated
intuitions about dependencies among features.
Once the structure of the network is established, the conditional probabilities of
each dependency have to be determined. We compute the conditional probability tables
from our training data (see Section 6) using Weka (Hall et al. 2009), and obtain values for
P(X | X1, . . . ,Xk) for each variable X and all variables Xi, 1 ? i ? k (parents of X), such
that the graph includes an edge from Xi to X. We then use the network for classification
by maximizing P(Xmwe | X1, . . . ,Xk), where Xmwe corresponds to the node MWE, and
X1, . . . ,Xk are the variables corresponding to all other nodes in the network. According
to Bayes rule, we have
P(Xmwe | X1, . . . ,Xk) ?
P(X1, . . . ,Xk | Xmwe)? P(Xmwe)
We define the prior, P(Xmwe), to be 0.41: This is the percentage of MWEs in WordNet 1.7
(Fellbaum 1998). This figure is of course rather arbitrary, but several studies indicate that
the percentage of MWEs in the (mental) lexicon is approximately one half (Jackendoff
1997; Erman and Warren 2000; Sag et al. 2002). Post factum, we experimented with
various other values for this parameter. We chose values between 0.3 and 0.55, in
increments of 0.05, and computed the F-score of the system on the task of extracting
English MWEs (see Section 7). As Table 3 shows, the differences are small (and not
statistically significant), meaning that the accuracy of the system seems to be rather
robust to the actual value of the prior. Given a small tuning set, it should be possible to
optimize the choice of the prior more systematically.
The conditional probabilities P(X1, . . . ,Xk | Xmwe) are determined by Weka from
the conditional probability tables:
P(X1, . . . ,Xk | Xmwe) = ?ki=1P(Xi | pai)
where k is the number of nodes in the BN (other than Xmwe) and pai is the set of parents
of Xi.
Table 3
F-score as a function of the value of the prior.
P(Xmwe) 0.3 0.35 0.4 0.41 0.45 0.5 0.55
F-score 0.848 0.84 0.833 0.835 0.831 0.836 0.843
460
Tsvetkov and Wintner Identification of Multiword Expressions
Table 4
Sizes of the training sets.
MWE non-MWE Total
English 1,381 2,004 3,385
French 1,445 2,089 3,534
Hebrew 350 504 854
6. Automatic Generation of Training Data
For training we need samples of positive and negative instances of MWEs, each asso-
ciated with a vector of the values of all features discussed in Section 4. We generate
this training material automatically, using the small bilingual corpora described in
Section 3.2. Each parallel corpus is first word-aligned with IBM Model 4 (Brown et al.
1993), implemented in Giza++ (Och and Ney 2003); we use union for symmetrization
here, to improve the recall. Then, we apply the (completely unsupervised) algorithm of
Tsvetkov and Wintner (2012), which extracts MWE candidates from the aligned corpus
and re-ranks them using statistics computed from a large monolingual corpus.
The core idea behind this algorithm is that MWEs tend to be translated in non-
literal ways; in a parallel corpus, words that are 1:1 aligned typically indicate literal
translations and are hence unlikely constituents of MWEs. The algorithm hence focuses
on misalignments: It trusts the quality of 1:1 alignments (which are further verified with
a bilingual dictionary) and searches for MWEs exactly in the areas that word alignment
failed to properly align, not relying on the alignment in these cases. Specifically, the
algorithm views all words that are not included in 1:1 alignments as potential areas
in which to search for MWEs, independently of how these words were aligned by the
word-aligner. Then, it uses statistics computed from a large monolingual corpus to rank
the MWE candidates; specifically, we use the PMI score of candidates based on counts
from the monolingual corpora. Finally, the algorithm extracts maximally long sequences
of words from the unaligned parallel phrases, in which each bigram has a PMI score
above some threshold (determined experimentally). All bigrams in those sequences are
considered MWEs. See Tsvetkov and Wintner (2012) for more details.
The set of MWEs that is determined in this way constitutes the positive examples
in the training set. For negative examples, we use two sets of bigrams: Those that are 1:1
aligned and have high PMI; and those that are misaligned but have low PMI. To decide
how many negative examples to generate, we rely on the ratio between MWE and non-
MWE entries in WordNet, as mentioned above: P(Xmwe) = 0.41. We thus select from the
negative set approximately 50% more negative examples than positive ones, such that
the ratio between the sizes of the sets is 0.41 : 0.59. The sizes of the resulting training
sets are listed in Table 4.
7. Results and Evaluation
We use the training data described in Section 6 for training and evaluation: We perform
10-fold cross validation experiments, reporting accuracy and (balanced) F-score in three
set-ups: One (SVM) in which we train an SVM classifier5 with the features described
5 We use Weka SMO with the PolyKernel set-up; experimentation with several other kernels yielded
worse results.
461
Computational Linguistics Volume 40, Number 2
Table 5
10-fold cross validation evaluation results.
Hebrew French English
Accuracy (%) F-score Accuracy (%) F-score Accuracy (%) F-score
PMI 66.98 0.67 70.88 0.762 74.15 0.737
BN-auto 71.19 0.71 77.45 0.775 82.16 0.822
SVM 74.59 0.75 78.38 0.736 82.95 0.828
BN 76.82 0.77 79.04 0.778 83.52 0.835
in Section 4; one (BN-auto) in which we train a Bayesian network with these features,
but let Weka determine its structure (using the K2 algorithm); and one (BN) in which
we train a Bayesian network whose structure reflects manually crafted linguistically
motivated knowledge, as depicted in Figure 1. The results are listed in Table 5; they are
compared with a PMI baseline, obtained by defining a Bayesian network with only two
nodes, MWE and PMI.
The linguistically motivated features defined in Section 4 are clearly helpful in the
classification task: The accuracy of an SVM, informed by these features, is close to 75%
for Hebrew, over 78% for French, and as high as 83% for English, reducing the error rate
of the PMI baseline by 23% (Hebrew) to 34% (English). The contribution of the BN is
also highly significant, reducing 3?9% more errors (with respect to the errors made by
the SVM classifier).6 In total, the best method, BN, reduces the error rate of the PMI-
based classifier by one third. Interestingly, a BN whose structure does not reflect prior
knowledge, but is rather learned automatically, performs worse than these two methods
(but still much better than relying on PMI alone).7 It is the combination of linguistically
motivated features with feature interdependencies reflecting domain knowledge that
contribute to the best performance.
We did not investigate the contribution of each of the features to the classification
task. However, we did analyze the weights assigned by the SVM classifier to specific
features. Unsurprisingly, the most distinctive feature is PMI. Among the POS features,
the strongest feature is VB NNS, an indication of a negative instance. Capitalization is
also unsurprisingly a very strong feature. We leave a more systematic analysis of the
contribution of each feature to future work.
To further assess the quality of the results, we performed a human evaluation on
the English data set. We first produced the results in the BN set-up, and then sorted
both the (predicted) positive and the (predicted) negative instances by their PMI. We
randomly picked 100 instances of both lists, at the same positions in the ranked lists,
to constitute an evaluation set. We asked three English-speaking annotators to deter-
mine whether the 200 expressions were indeed MWEs. The annotation guidelines are
given in Appendix A. Comparing the three annotators? labels, we found out that they
agreed on 141 of the 200 (70.5%). This should probably be taken as an upper bound for
the task.
6 The improvement of both BN and SVM over the baseline is highly significant statistically (sign test,
p < 0.01 in all three cases); the improvement of BN over SVM is significant for English (p < 0.01)
but not for French.
7 We are not sure why this is the case. One possible explanation is that our training set contains noisy
examples, and as the BN-auto classifier learns the dependencies from noisy data, it performs worse
than the SVM classifier. Another possible explanation is that it attempts to learn more dependencies,
thereby increasing the parameter space of the model.
462
Tsvetkov and Wintner Identification of Multiword Expressions
We then computed the majority label and compared it with our predicted label.
Exactly 142 of the predicted labels were annotated as correct; that?s an accuracy of
71%. Of the 141 instances that the three annotators agreed on, our results predict the
correct label for 112 instances (79.4%). We take these figures as a strong indication of the
accuracy of the results.
As an additional evaluation measure, we use the sets of bigrams in the English
WordNet, and the bigram MWEs in the DELA dictionaries of English and French
(Section 3.2). Because we only have positive instances in these evaluation sets, we can
only report recall. We therefore use the Bayesian network classifier to extract MWEs
from the large monolingual corpora discussed in Section 3.2. For each evaluation set
(WordNet, DELA English, and DELA French), we divide the number of bigrams in the
set that are classified as MWEs by the size of the intersection of the evaluation set with
the monolingual corpus. In other words, we exclude from the evaluation those MWEs
in the evaluation set that never occur in our corpora. The results are listed in Table 6.
As examples of correctly identified MWEs, consider English advisory board, air
cargo, adoption agency, air ticket, crude oil, and so on, and French accord international
?international agreement?, acte final ?final act?, banque centrale ?central bank?, ce soir
?tonight?, and so forth, all taken from the DELA dictionaries. The relatively low recall of
our method on these dictionaries is to a large extent due to a very liberal definition of
MWEs that the dictionaries use.Many entries that are listed asMWEs are actually highly
compositional, and hence our method fails to identify them. DELA entries that are not
identified by our classifier include examples such as English abnormal behavior, abso-
lute necessity, academic research, and so on. The French DELA dictionary is especially
extensive, with examples such as action sociale, action antitumorale, action associa-
tive, action caritative, action collective, action commerciale, action communautaire,
and many more, all listed as MWEs. Our system only recognizes the first of these.
The WordNet results are obviously much better. Correctly identified MWEs include
ad hoc, outer space, web site, inter alia, road map, and so forth. WordNet MWEs that
our system failed to identify include has been, as well, in this, a few, set up, and so on.
A more involved error analysis is required in order to propose potential directions for
improvement on this set.
As a further demonstration of the utility of our approach, we evaluate the algorithm
on the set NN of Hebrew noun-noun constructions described in Section 3.2. We train a
Bayesian network on the training set described in Section 6 and use it to classify the
set NN. We compare the results of this classifier with a PMI baseline, and also with
the classification results reported by Al-Haj and Wintner (2010); the latter reflects 10-
fold cross-validation evaluation using the entire set, so it may be considered an upper
bound for any classifier that uses a general training corpus.
The results are depicted in Table 7. They clearly demonstrate that the linguistically
motivated features we define provide a significant improvement in classification accu-
racy over the baseline PMI measure. Note that our F-score, 0.77, is very close to the
Table 6
Evaluation results: WordNet and DELA dictionaries.
True positives Evaluation set size Recall (%)
WordNet 25,549 42,403 60
DELA English 11,955 26,460 45
DELA French 886 4,798 18
463
Computational Linguistics Volume 40, Number 2
Table 7
Evaluation results: noun-noun constructions.
Accuracy Precision Recall F-score
PMI 71.43% 0.71 0.71 0.71
BN 77.00% 0.77 0.77 0.77
AW 80.77% 0.77 0.81 0.79
AW = results from Al-Haj and Wintner (2010)
best result of 0.79 obtained by Al-Haj and Wintner (2010) as the average of 10-fold cross
validation runs, using only high-frequency noun-noun constructions for training. We
interpret this result as a further proof of the robustness of our architecture.
Finally, we conduct an analysis of the quality of extracted (Hebrew) MWEs. We
used the trained BN to classify the entire set of bigrams present in the (Hebrew side of
the) Hebrew?English parallel corpus described in Section 3.2. Of the more than 140,000
candidates, only 4,000 are classified as MWEs. We sort this list of potential MWEs by the
probability assigned by the BN to the positive value of the variable Xmwe. The resulting
sorted list is dominated by high-PMI bigrams, especially proper names, all of which are
indeed MWEs. The first non-MWE (false positive) occurs in the 50th place on the list; it
is crpt niqwla ?France Nicolas?, which is obviously a sub-sequence of the larger MWE,
neia crpt niqwla srqwzi ?French president Nicolas Sarkozy?. Similar sub-sequences are
also present, but only five are in the top 100. Such false positives can be reduced when
longer MWEs are extracted, as it can be assumed that a sub-sequence of a longer MWE
does not have to be identified. Other false positives in the top 100 include some highly
frequent expressions, but over 85 of the top 100 are clearly MWEs.
Although more careful evaluation is required in order to estimate the rate of true
positives in this list, we trust that the vast majority of the positive results are indeed
MWEs.
8. Conclusions and Future Work
We presented a novel architecture for identifying MWEs in text corpora. The main
insights we emphasize are sophisticated computational encoding of linguistic knowl-
edge that focuses on the idiosyncratic behavior of such expressions. This is reflected
in two ways in our work: by defining computable features that reflect different facets
of irregularities; and by framing the features as part of a larger Bayesian network that
accounts for interdependencies among them. We also introduce a method for automat-
ically generating a training set for this task, which renders the classification entirely
unsupervised. The result is a classifier that can identify MWEs of several types and
constructions. Evaluation on three languages (English, French, and Hebrew) shows sig-
nificant improvement in the accuracy of the classifier compared with less sophisticated
baselines.
The modular architecture of Bayesian networks facilitates easy exploration with
more features. We are currently investigating the contribution of various other sources
of information to the classification task. For example, Hebrew lacks large-scale lexical
semantic resources. However, it is possible to literally translate an MWE candidate to
English and rely on the English WordNet for generating synonyms of the literal transla-
tion. Such ?literal synonyms? can then be back-translated to Hebrew. The assumption is
464
Tsvetkov and Wintner Identification of Multiword Expressions
that if a back-translated expression has a low PMI, the original candidate is very likely
not a MWE. Although such a feature may contribute little on its own, incorporating it
in a well-structured BN may improve performance. Another feature that can easily be
implemented in this way is whether the POS of MWE constituents is retained when the
expression is translated to another language; we hypothesize that this is much more
likely when the expression is compositional.
Appendix A. Annotation Guidelines
These are the instructions given to the annotators.
The task is to annotate each line as either a multi-word expression, in which case mark
1 in the first field; or not, in which case the value is 0. It?s a hard task, but you are
requested to be decisive. Please do not change the file in any other way.
The main criterion for determining whether an expression is a MWE is whether it
has to be stored in a computational lexicon. Typically, expressions are stored in lexicons
if they exhibit idiosyncratic (irregular) behavior. This could be due to:
 non-compositional meaning. For example, ?green light? is an MWE because it is not
a light. ?kill time? is not a violent action. A good indication of non-compositional
meaning is limited reference. For example, if someone gives you a green light, you
can?t then refer to it as ?the light I was given?.
 non-substitutability of elements. For example, ?breast cancer? is an MWE because
while ?breast? and ?chest? can often be substituted, ?breast cancer? and ?chest
cancer? cannot.
 fossil words, i.e., words that only occur in the context of the expression. For
example, ?mutatis mutandis?.
 nominalization. If the expression can occur as a single word, or with a connecting
hyphen, it is a strong indication that it is an MWE. For example, ?road map? can be
written ?roadmap?.
 irregular syntactic and/or morphological behavior. For example, ?look up? is an
MWE because while ordinarily you can convert ?I walked up the alley? to ?Up the
alley I walked?, you can?t convert ?I looked up that word in a dictionary? to ?Up that
word I looked?.
 proper names. All proper names are by definition MWEs. This includes people
(?Barack Obama?), places (?Tel Aviv?), organizations (?United Nations?), etc.
But really, the best criterion is: if I hadn?t known this expression, would I be able to use
it properly simply by knowing its two constituents? Would I understand its meaning,
be able to inflect it properly, construct syntactic constructions, and in general use it in
the right context in the right way?
Acknowledgments
This research was supported by The Israel
Science Foundation (grants 137/06 and
1269/07). We are grateful to Gennadi
Lembersky for his continuous help, and to
the three anonymous Computational
Linguistics reviewers for very constructive
comments that greatly improved this article.
All remaining errors are of course our own.
References
Al-Haj, Hassan. 2010. Hebrew multiword
expressions: Linguistic properties, lexical
representation, morphological processing,
and automatic acquisition. Master?s thesis,
University of Haifa.
Al-Haj, Hassan and Shuly Wintner. 2010.
Identifying multi-word expressions by
leveraging morphological and syntactic
465
Computational Linguistics Volume 40, Number 2
idiosyncrasy. In Proceedings of the 23rd
International Conference on Computational
Linguistics (COLING 2010), pages 10?18,
Beijing.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003. An
empirical model of multiword expression
decomposability. In Proceedings of the ACL
2003 Workshop on Multiword Expressions,
pages 89?96, Sapporo.
Baldwin, Timothy and Takaaki Tanaka.
2004. Translation by machine of complex
nominals: Getting it right. In Second ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 24?31,
Barcelona.
Bannard, Colin. 2007. A measure of syntactic
flexibility for automatically identifying
multiword expressions in corpora. In
Proceedings of the Workshop on a Broader
Perspective on Multiword Expressions,
pages 1?8, Prague.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
approach to the semantics of
verb-particles. In Proceedings of the ACL
2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 65?72, Sapporo, Japan.
Bird, Steven, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with
Python. O?Reilly Media, Sebastopol, CA.
Bouamor, Dhouha, Nasredine Semmar, and
Pierre Zweigenbaum. 2012. Identifying
bilingual multi-word expressions
for statistical machine translation.
In Proceedings of the Eight International
Conference on Language Resources and
Evaluation (LREC?12), pages 674?679,
Istanbul.
Brown, Peter F., Stephen Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematic of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Calado, Pa?vel, Marco Cristo, Edleno Silva
De Moura, Nivio Ziviani, Berthier A.
Ribeiro-Neto, and Marcos Andre?
Gonc?alves. 2003. Combining link-based
and content-based methods for web
document classification. In Proceedings
of CIKM-03, 12th ACM International
Conference on Information and Knowledge
Management, pages 394?401,
New Orleans, LA.
Callison-Burch, Chris, Philipp Koehn,
Christof Monz, and Omar F. Zaidan,
editors. 2011. Proceedings of the Sixth
Workshop on Statistical Machine
Translation. Association for Computational
Linguistics, Edinburgh.
Carpuat, Marine and Mona Diab. 2010.
Task-based evaluation of multiword
expressions: A pilot study in statistical
machine translation. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 242?245, Los Angeles, CA.
Chang, Baobao, Pernilla Danielsson, and
Wolfgang Teubert. 2002. Extraction of
translation unit from Chinese-English
parallel corpora. In Proceedings of the First
SIGHAN Workshop on Chinese Language
Processing, pages 1?5, Morristown, NJ.
Church, Kenneth Ward and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Cook, Paul, Afsaneh Fazly, and Suzanne
Stevenson. 2007. Pulling their weight:
Exploiting syntactic forms for the
automatic identification of idiomatic
expressions in context. In Proceedings of
the ACL Workshop on a Broader Perspective
on Multiword Expressions (MWE 2007),
pages 41?48, Prague.
Denoyer, Ludovic and Patrick Gallinari.
2004. Bayesian network model for
semi-structured document classification.
Information Processing and Management,
40(5):807?827.
Doucet, Antoine and Helana Ahonen-Myka.
2004. Non-contiguous word sequences
for information retrieval. In Second ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 88?95,
Barcelona.
Duan, Jianyong, Mei Zhang, Lijing Tong,
and Feng Guo. 2009. A hybrid approach to
improve bilingual multiword expression
extraction. In Thanaruk Theeramunkong,
Boonserm Kijsirikul, Nick Cercone, and
Tu-Bao Ho, editors, Advances in Knowledge
Discovery and Data Mining, volume 5476
of Lecture Notes in Computer Science.
Springer, Berlin and Heidelberg,
pages 541?547.
Erman, Britt and Beatrice Warren. 2000.
The idiom principle and the open choice
principle. Text, 20(1):29?62.
Fazly, Afsaneh and Suzanne Stevenson.
2006. Automatically constructing a lexicon
of verb phrase idiomatic combinations.
In Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 337?344, Trento.
466
Tsvetkov and Wintner Identification of Multiword Expressions
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. Language,
Speech and Communication. MIT Press,
Cambridge, MA.
Green, Spence, Marie-Catherine de Marneffe,
John Bauer, and Christopher D. Manning.
2011. Multiword expression identification
with tree substitution grammars:
A parsing tour de force with French.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 725?735, Edinburgh.
Hall, Mark, Eibe Frank, Geoffrey Holmes,
Bernhard Pfahringer, Peter Reutemann,
and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD
Explorations, 11(1):10?18.
Hazelbeck, Gregory and Hiroaki Saito.
2010. A hybrid approach for functional
expression identification in a Japanese
reading assistant. In Proceedings of the 2010
Workshop on Multiword Expressions: From
Theory to Applications, pages 81?84, Beijing.
Heckerman, David. 1995. A tutorial on
learning with Bayesian networks.
Technical Report MSR-TR-95-06,
Microsoft Research, Redmond, WA.
Itai, Alon and Shuly Wintner. 2008.
Language resources for Hebrew. Language
Resources and Evaluation, 42(1):75?98.
Jackendoff, Ray. 1997. The Architecture
of the Language Faculty. MIT Press,
Cambridge, MA.
Jain, Anil K. 1989. Fundamentals of Digital
Image Processing. Prentice-Hall, Inc.,
Upper Saddle River, NJ.
Jensen, Finn V. and Thomas D. Nielsen. 2007.
Bayesian Networks and Decision Graphs.
Springer, 2nd edition.
Katz, Graham and Eugenie Giesbrecht.
2006. Automatic identification of
non-compositional multi-word
expressions using latent semantic analysis.
In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting
Underlying Properties, pages 12?19, Sydney.
Kirschenbaum, Amit and Shuly Wintner.
2010. A general method for creating a
bilingual transliteration dictionary.
In Proceedings of the Seventh Conference
on International Language Resources and
Evaluation (LREC?10), pages 273?276,
Valletta.
Lam, Wai, Kon F. Low, and Chao Y. Ho.
1997. Using a Bayesian network
induction approach for text categorization.
In Proceedings of IJCAI-97, 15th International
Joint Conference on Artificial Intelligence,
pages 745?750, Nagoya.
Lambert, Patrik and Rafael Banchs. 2005.
Data inferred multi-word expressions
for statistical machine translation.
In Proceedings of the MT Summit X,
pages 396?403, Phuket.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine Miller. 1990. Five papers
on WordNet. International Journal of
Lexicography, 3(4):235?312.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In ACL ?00: Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Pearl, Judea. 1985. Bayesian networks:
A model of self-activated memory for
evidential reasoning. In Proceedings of the
7th Conference of the Cognitive Science
Society, pages 329?334, University of
California, Irvine, CA.
Pecina, Pavel. 2005. An extensive empirical
study of collocation extraction methods.
In Proceedings of the ACL Student Research
Workshop, pages 13?18, Ann Arbor, MI.
Pecina, Pavel. 2008. A machine learning
approach to multiword expression
extraction. In Proceedings of the LREC
Workshop Towards a Shared Task for
Multiword Expressions, pages 54?57,
Marrakech.
Peshkin, Leonid, Avi Pfeffer, and
Virginia Savova. 2003. Bayesian nets in
syntactic categorization of novel words.
In Proceedings of the 2003 Conference of the
North American Chapter of the Association
for Computational Linguistics on Human
Language Technology: Companion Volume of
the Proceedings of HLT-NAACL 2003?Short
Papers - Volume 2, NAACL ?03,
pages 79?81, Edmonton.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In Proceedings of HLT-NAACL,
pages 404?411, Rochester, NY.
Piao, Scott Songlin, Paul Rayson, Dawn
Archer, and Tony McEnery. 2005.
Comparing and combining a semantic
tagger and a statistical tool for MWE
extraction. Computer Speech and Language,
19(4):378?397.
Ramisch, Carlos, Helena de Medeiros Caseli,
Aline Villavicencio, Andre? Machado, and
Maria Finatto. 2010. A hybrid approach
for multiword expression identification.
467
Computational Linguistics Volume 40, Number 2
In Thiago Pardo, Anto?nio Branco,
Aldebaro Klautau, Renata Vieira, and
Vera de Lima, editors, Computational
Processing of the Portuguese Language,
volume 6001 of Lecture Notes in Computer
Science. Springer, Berlin and Heidelberg,
pages 65?74.
Ramisch, Carlos, Paulo Schreiner, Marco
Idiart, and Alline Villavicencio. 2008. An
evaluation of methods for the extraction of
multiword expressions. In Proceedings of
the LREC Workshop Towards a Shared Task for
Multiword Expressions, pages 50?53,
Marrakech.
Ren, Zhixiang, Yajuan Lu?, Jie Cao, Qun Liu,
and Yun Huang. 2009. Improving
statistical machine translation using
domain bilingual multiword expressions.
In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation,
Disambiguation and Applications,
pages 47?54, Singapore.
Sag, Ivan, Timothy Baldwin, Francis Bond,
Ann Copestake, and Dan Flickinger. 2002.
Multiword expressions: A pain in the
neck for NLP. In Proceedings of the Third
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLING 2002), pages 1?15, Mexico City.
Savova, Virginia and Leonid Peshkin. 2005.
Dependency parsing with dynamic
Bayesian network. In Proceedings of the
20th National Conference on Artificial
Intelligence?Volume 3, pages 1,112?1,117,
Pittsburgh, PA.
Smadja, Frank A. 1993. Retrieving
collocations from text: Xtract.
Computational Linguistics, 19(1):143?177.
Smith, Noah A. 2011. Linguistic Structure
Prediction. Synthesis Lectures on Human
Language Technologies. Morgan and
Claypool.
Tsvetkov, Yulia and Shuly Wintner. 2010a.
Automatic acquisition of parallel corpora
from websites with dynamic content.
In Proceedings of the Seventh Conference
on International Language Resources and
Evaluation (LREC?10), pages 3,389?3,392,
Valletta.
Tsvetkov, Yulia and Shuly Wintner. 2010b.
Extraction of multi-word expressions from
small parallel corpora. In Proceedings
of the 23rd International Conference on
Computational Linguistics (COLING 2010),
pages 1256?1264, Beijing.
Tsvetkov, Yulia and Shuly Wintner. 2011.
Identification of multi-word expressions
by combining multiple linguistic
information sources. In Proceedings
of the 2011 Conference on Empirical Methods
in Natural Language Processing,
pages 836?845, Edinburgh.
Tsvetkov, Yulia and Shuly Wintner. 2012.
Extraction of multi-word expressions
from small parallel corpora. Natural
Language Engineering, 18(4):549?573.
Uchiyama, Kiyoko, Timothy Baldwin, and
Shun Ishizaki. 2005. Disambiguating
Japanese compound verbs. Computer
Speech & Language, 19(4):497?512.
Van de Cruys, Tim and Begon?a
Villada Moiro?n. 2007. Semantics-based
multiword expression extraction.
In Proceedings of the Workshop on a Broader
Perspective on Multiword Expressions,
pages 25?32, Prague.
Varga, Da?niel, Pe?ter Hala?csy, Andra?s Kornai,
Viktor Nagy, La?szlo? Ne?meth, and Viktor
Tro?n. 2005. Parallel corpora for medium
density languages. In Proceedings of
RANLP?2005, pages 590?596, Borovet.
Venkatapathy, Sriram and Aravind Joshi.
2006. Using information about multi-word
expressions for the word-alignment
task. In Proceedings of the COLING/ACL
Workshop on Multiword Expressions:
Identifying and Exploiting Underlying
Properties, pages 20?27, Sydney.
Venkatsubramanyan, Shailaja and Jose
Perez-Carballo. 2004. Multiword
expression filtering for building
knowledge. In Second ACL Workshop
on Multiword Expressions: Integrating
Processing, pages 40?47, Barcelona.
Villavicencio, Aline, Valia Kordoni, Yi Zhang,
Marco Idiart, and Carlos Ramisch. 2007.
Validation and evaluation of automatically
acquired multiword expressions for
grammar engineering. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1,034?1,043,
Prague.
Weller, Marion and Fabienne Fritzinger. 2010.
A hybrid approach for the identification
of multiword expressions. In Proceedings
of the SLTC 2010 Workshop on Compounds
and Multiword Expressions, pages 1?2,
Linko?ping.
468
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 248?258,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Metaphor Detection with Cross-Lingual Model Transfer
Yulia Tsvetkov Leonid Boytsov Anatole Gershman Eric Nyberg Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ytsvetko, srchvrs, anatoleg, ehn, cdyer}@cs.cmu.edu
Abstract
We show that it is possible to reliably dis-
criminate whether a syntactic construction
is meant literally or metaphorically using
lexical semantic features of the words that
participate in the construction. Our model
is constructed using English resources,
and we obtain state-of-the-art performance
relative to previous work in this language.
Using a model transfer approach by piv-
oting through a bilingual dictionary, we
show our model can identify metaphoric
expressions in other languages. We pro-
vide results on three new test sets in Span-
ish, Farsi, and Russian. The results sup-
port the hypothesis that metaphors are
conceptual, rather than lexical, in nature.
1 Introduction
Lakoff and Johnson (1980) characterize metaphor
as reasoning about one thing in terms of another,
i.e., a metaphor is a type of conceptual mapping,
where words or phrases are applied to objects and
actions in ways that do not permit a literal inter-
pretation. They argue that metaphors play a fun-
damental communicative role in verbal and writ-
ten interactions, claiming that much of our every-
day language is delivered in metaphorical terms.
There is empirical evidence supporting the claim:
recent corpus studies have estimated that the pro-
portion of words used metaphorically ranges from
5% to 20% (Steen et al, 2010), and Thibodeau and
Boroditsky (2011) provide evidence that a choice
of metaphors affects decision making.
Given the prevalence and importance of
metaphoric language, effective automatic detec-
tion of metaphors would have a number of ben-
efits, both practical and scientific. Language pro-
cessing applications that need to understand lan-
guage or preserve meaning (information extrac-
tion, machine translation, dialog systems, senti-
ment analysis, and text analytics, etc.) would have
access to a potentially useful high-level bit of in-
formation about whether something is to be under-
stood literally or not. Second, scientific hypothe-
ses about metaphoric language could be tested
more easily at a larger scale with automation.
However, metaphor detection is a hard problem.
On one hand, there is a subjective component: hu-
mans may disagree whether a particular expres-
sion is used metaphorically or not, as there is no
clear-cut semantic distinction between figurative
and metaphorical language (Shutova, 2010). On
the other, metaphors can be domain- and context-
dependent.
1
Previous work has focused on metaphor identi-
fication in English, using both extensive manually-
created linguistic resources (Mason, 2004; Gedi-
gian et al, 2006; Krishnakumaran and Zhu, 2007;
Turney et al, 2011; Broadwell et al, 2013) and
corpus-based approaches (Birke and Sarkar, 2007;
Shutova et al, 2013; Neuman et al, 2013; Shutova
and Sun, 2013; Hovy et al, 2013). We build on
this foundation and also extend metaphor detec-
tion into other languages in which few resources
may exist. Our work makes the following con-
tributions: (1) we develop a new state-of-the-art
English metaphor detection system that uses con-
ceptual semantic features, such as a degree of ab-
stractness and semantic supersenses;
2
(2) we cre-
ate new metaphor-annotated corpora for Russian
and English;
3
(3) using a paradigm of model trans-
fer (McDonald et al, 2011; T?ackstr?om et al, 2013;
Kozhenikov and Titov, 2013), we provide sup-
port for the hypothesis that metaphors are concep-
1
For example, drowning students could be used metaphor-
ically to describe the situation where students are over-
whelmed with work, but in the sentence a lifeguard saved
drowning students, this phrase is used literally.
2
https://github.com/ytsvetko/metaphor
3
http://www.cs.cmu.edu/
?
ytsvetko/
metaphor/datasets.zip
248
tual (rather than lexical) in nature by showing that
our English-trained model can detect metaphors in
Spanish, Farsi, and Russian.
2 Methodology
Our task in this work is to define features that dis-
tinguish between metaphoric and literal uses of
two syntactic constructions: subject-verb-object
(SVO) and adjective-noun (AN) tuples.
4
We give
examples of a prototypical metaphoric usage of
each type:
? SVO metaphors. A sentence containing a
metaphoric SVO relation is my car drinks
gasoline. According to Wilks (1978), this
metaphor represents a violation of selectional
preferences for the verb drink, which is nor-
mally associated with animate subjects (the
car is inanimate and, hence, cannot drink in
the literal sense of the verb).
? AN metaphors. The phrase broken promise
is an AN metaphor, where attributes from
a concrete domain (associated with the con-
crete word broken) are transferred to a more
abstract domain, which is represented by the
relatively abstract word promise. That is, we
map an abstract concept promise to a concrete
domain of physical things, where things can
be literally broken to pieces.
Motivated by Lakoff?s (1980) argument that
metaphors are systematic conceptual mappings,
we will use coarse-grained conceptual, rather than
fine-grained lexical features, in our classifier. Con-
ceptual features pertain to concepts and ideas as
opposed to individual words or phrases expressed
in a particular language. In this sense, as long as
two words in two different languages refer to the
same concepts, their conceptual features should
be the same. Furthermore, we hypothesize that
our coarse semantic features give us a language-
invariant representation suitable for metaphor de-
tection. To test this hypothesis, we use a cross-
lingual model transfer approach: we use bilingual
dictionaries to project words from other syntactic
constructions found in other languages into En-
glish and then apply the English model on the de-
rived conceptual representations.
4
Our decision to focus on SVO and AN metaphors is jus-
tified by corpus studies that estimate that verb- and adjective-
based metaphors account for a substantial proportion of all
metaphoric expressions, approximately 60% and 24%, re-
spectively (Shutova and Teufel, 2010; Gandy et al, 2013).
Each SVO (or AN) instance will be represented
by a triple (duple) from which a feature vector
will be extracted.
5
The vector will consist of the
concatenation of the conceptual features (which
we discuss below) for all participating words, and
conjunction features for word pairs.
6
For example,
to generate the feature vector for the SVO triple
(car, drink, gasoline), we compute all the features
for the individual words car, drink, gasoline and
combine them with the conjunction features for
the pairs car drink and drink gasoline.
We define three main feature categories (1) ab-
stractness and imageability, (2) supersenses, (3)
unsupervised vector-space word representations;
each category corresponds to a group of features
with a common theme and representation.
? Abstractness and imageability. Abstract-
ness and imageability were shown to be use-
ful in detection of metaphors (it is easier to
invoke mental pictures of concrete and im-
ageable words) (Turney et al, 2011; Broad-
well et al, 2013). We expect that abstract-
ness, used in conjunction features (e.g., a
feature denoting that the subject is abstract
and the verb is concrete), is especially use-
ful: semantically, an abstract agent perform-
ing a concrete action is a strong signal of
metaphorical usage.
Although often correlated with abstractness,
imageability is not a redundant property.
While most abstract things are hard to visu-
alize, some call up images, e.g., vengeance
calls up an emotional image, torture calls up
emotions and even visual images. There are
concrete things that are hard to visualize too,
for example, abbey is harder to visualize than
banana (B. MacWhinney, personal commu-
nication).
? Supersenses. Supersenses
7
are coarse se-
mantic categories originating in WordNet.
For nouns and verbs there are 45 classes:
26 for nouns and 15 for verbs, for example,
5
Looking at components of the syntactic constructions in-
dependent of their context has its limitations, as discussed
above with the drowning students example; however, it sim-
plifies the representation challenges considerably.
6
If word one is represented by features u ? R
n
and word
two by features v ? R
m
then the conjunction feature vector
is the vectorization of the outer product uv
>
.
7
Supersenses are called ?lexicographer classes? in Word-
Net documentation (Fellbaum, 1998), http://wordnet.
princeton.edu/man/lexnames.5WN.html
249
noun.body, noun.animal, verb.consumption,
or verb.motion (Ciaramita and Altun, 2006).
English adjectives do not, as yet, have a sim-
ilar high-level semantic partitioning in Word-
Net, thus we use a 13-class taxonomy of ad-
jective supersenses constructed by Tsvetkov
et al (2014) (discussed in ?3.2).
Supersenses are particularly attractive fea-
tures for metaphor detection: coarse sense
taxonomies can be viewed as semantic con-
cepts, and since concept mapping is a pro-
cess in which metaphors are born, we
expect different supersense co-occurrences
in metaphoric and literal combinations.
In ?drinks gasoline?, for example, map-
ping to supersenses would yield a pair
<verb.consumption, noun.substance>, con-
trasted with <verb.consumption, noun.food>
for ?drinks juice?. In addition, this coarse
semantic categorization is preserved in trans-
lation (Schneider et al, 2013), which makes
supersense features suitable for cross-lingual
approaches such as ours.
? Vector space word representations. Vec-
tor space word representations learned us-
ing unsupervised algorithms are often effec-
tive features in supervised learning methods
(Turian et al, 2010). In particular, many such
representations are designed to capture lex-
ical semantic properties and are quite effec-
tive features in semantic processing, includ-
ing named entity recognition (Turian et al,
2009), word sense disambiguation (Huang et
al., 2012), and lexical entailment (Baroni et
al., 2012). In a recent study, Mikolov et
al. (2013) reveal an interesting cross-lingual
property of distributed word representations:
there is a strong similarity between the vec-
tor spaces across languages that can be eas-
ily captured by linear mapping. Thus, vector
space models can also be seen as vectors of
(latent) semantic concepts, that preserve their
?meaning? across languages.
3 Model and Feature Extraction
In this section we describe a classification model,
and provide details on mono- and cross-lingual
implementation of features.
3.1 Classification using Random Forests
To make classification decisions, we use a random
forest classifier (Breiman, 2001), an ensemble of
decision tree classifiers learned from many inde-
pendent subsamples of the training data. Given
an input, each tree classifier assigns a probabil-
ity to each label; those probabilities are averaged
to compute the probability distribution across the
ensemble. Random forest ensembles are partic-
ularly suitable for our resource-scarce scenario:
rather than overfitting, they produce a limiting
value of the generalization error as the number
of trees increases,
8
and no hyperparameter tuning
is required. In addition, decision-tree classifiers
learn non-linear responses to inputs and often out-
perform logistic regression (Perlich et al, 2003).
9
Our random forest classifier models the probabil-
ity that the input syntactic relation is metaphorical.
If this probability is above a threshold, the relation
is classified as metaphoric, otherwise it is literal.
We used the scikit-learn toolkit to train our
classifiers (Pedregosa et al, 2011).
3.2 Feature extraction
Abstractness and imageability. The MRC psy-
cholinguistic database is a large dictionary listing
linguistic and psycholinguistic attributes obtained
experimentally (Wilson, 1988).
10
It includes,
among other data, 4,295 words rated by the de-
grees of abstractness and 1,156 words rated by the
imageability. Similarly to Tsvetkov et al (2013),
we use a logistic regression classifier to propagate
abstractness and imageability scores from MRC
ratings to all words for which we have vector space
representations. More specifically, we calculate
the degree of abstractness and imageability of all
English items that have a vector space representa-
tion, using vector elements as features. We train
two separate classifiers for abstractness and im-
ageability on a seed set of words from the MRC
database. Degrees of abstractness and imageabil-
ity are posterior probabilities of classifier predic-
tions. We binarize these posteriors into abstract-
concrete (or imageable-unimageable) boolean in-
dicators using pre-defined thresholds.
11
Perfor-
8
See Theorem 1.2 in (Breiman, 2001) for details.
9
In our experiments, random forests model slightly out-
performed logistic regression and SVM classifiers.
10
http://ota.oucs.ox.ac.uk/headers/
1054.xml
11
Thresholds are equal to 0.8 for abstractness and to 0.9
for imageability. They were chosen empirically based on ac-
250
mance of these classifiers, tested on a sampled
held-out data, is 0.94 and 0.85 for the abstractness
and imageability classifiers, respectively.
Supersenses. In the case of SVO relations, we
incorporate supersense features for nouns and
verbs; noun and adjective supersenses are used in
the case of AN relations.
Supersenses of nouns and verbs. A lexical item
can belong to several synsets, which are associ-
ated with different supersenses. Degrees of mem-
bership in different supersenses are represented
by feature vectors, where each element corre-
sponds to one supersense. For example, the word
head (when used as a noun) participates in 33
synsets, three of which are related to the super-
sense noun.body. The value of the feature corre-
sponding to this supersense is 3/33 ? 0.09.
Supersenses of adjectives. WordNet lacks
coarse-grained semantic categories for adjectives.
To divide adjectives into groups, Tsvetkov et al
(2014) use 13 top-level classes from the adapted
taxonomy of Hundsnurscher and Splett (1982),
which is incorporated in GermaNet (Hamp and
Feldweg, 1997). For example, the top-level
classes in GermaNet include: adj.feeling (e.g.,
willing, pleasant, cheerful); adj.substance (e.g.,
dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-
gantic).
12
For each adjective type in WordNet,
they produce a vector with a classifier posterior
probabilities corresponding to degrees of mem-
bership of this word in one of the 13 semantic
classes,
13
similar to the feature vectors we build
for nouns and verbs. For example, for a word
calm the top-2 categories (with the first and second
highest degrees of membership) are adj.behavior
and adj.feeling.
Vector space word representations. We em-
ploy 64-dimensional vector-space word represen-
tations constructed by Faruqui and Dyer (2014).
14
Vector construction algorithm is a variation on
traditional latent semantic analysis (Deerwester
et al, 1990) that uses multilingual information
to produce representations in which synonymous
words have similar vectors. The vectors were
curacy during cross-validation.
12
For the full taxonomy see http://www.sfs.
uni-tuebingen.de/lsd/adjectives.shtml
13
http://www.cs.cmu.edu/
?
ytsvetko/
adj-supersenses.tar.gz
14
http://www.cs.cmu.edu/
?
mfaruqui/soft.
html
trained on the news commentary corpus released
by WMT-2011,
15
comprising 180,834 types.
3.3 Cross-lingual feature projection
For languages other than English, feature vectors
are projected to English features using translation
dictionaries. We used the Babylon dictionary,
16
which is a proprietary resource, but any bilingual
dictionary can in principle be used. For a non-
English word in a source language, we first ob-
tain all translations into English. Then, we av-
erage all feature vectors related to these transla-
tions. Consider an example related to projection
of WordNet supersenses. A Russian word ??????
is translated as head and brain. Hence, we select
all the synsets of the nouns head and brain. There
are 38 such synsets (33 for head and 5 for brain).
Four of these synsets are associated with the su-
persense noun.body. Therefore, the value of the
feature noun.body is 4/38 ? 0.11.
4 Datasets
In this section we describe a training and testing
dataset as well a data collection procedure.
4.1 English training sets
To train an SVO metaphor classifier, we employ
the TroFi (Trope Finder) dataset.
17
TroFi includes
3,737 manually annotated English sentences from
the Wall Street Journal (Birke and Sarkar, 2007).
Each sentence contains either literal or metaphori-
cal use for one of 50 English verbs. First, we use a
dependency parser (Martins et al, 2010) to extract
subject-verb-object (SVO) relations. Then, we fil-
ter extracted relations to eliminate parsing-related
errors, and relations with verbs which are not in
the TroFi verb list. After filtering, there are 953
metaphorical and 656 literal SVO relations which
we use as a training set.
In the case of AN relations, we construct and
make publicly available a training set contain-
ing 884 metaphorical AN pairs and 884 pairs
with literal meaning. It was collected by two
annotators using public resources (collections of
metaphors on the web). At least one additional
person carefully examined and culled the col-
lected metaphors, by removing duplicates, weak
metaphors, and metaphorical phrases (such as
15
http://www.statmt.org/wmt11/
16
http://www.babylon.com
17
http://www.cs.sfu.ca/
?
anoop/students/
jbirke/
251
drowning students) whose interpretation depends
on the context.
4.2 Multilingual test sets
We collect and annotate metaphoric and literal test
sentences in four languages. Thus, we compile
eight test datasets, four for SVO relations, and
four for AN relations. Each dataset has an equal
number of metaphors and non-metaphors, i.e., the
datasets are balanced. English (EN) and Russian
(RU) datasets have been compiled by our team
and are publicly available. Spanish (ES) and Farsi
(FA) datasets are published elsewhere (Levin et al,
2014). Table 1 lists test set sizes.
SVO AN
EN 222 200
RU 240 200
ES 220 120
FA 44 320
Table 1: Sizes of the eight test sets. Each dataset is
balanced, i.e., it has an equal number of metaphors
and non-metaphors. For example, English SVO
dataset has 222 relations: 111 metaphoric and 111
literal.
We used the following procedure to compile the
EN and RU test sets. A moderator started with seed
lists of 1000 most common verbs and adjectives.
18
Then she used the SketchEngine, which pro-
vides searching capability for the TenTen Web cor-
pus,
19
to extract sentences with words that fre-
quently co-occurred with words from the seed
lists. From these sentences, she removed sen-
tences that contained more than one metaphor, and
sentences with non-SVO and non-AN metaphors.
Remaining sentences were annotated by several
native speakers (five for English and six for Rus-
sian), who judged AN and SVO phrases in con-
text. The annotation instructions were general:
?Please, mark in bold all words that, in your opin-
ion, are used non-literally in the following sen-
tences. In many sentences, all the words may be
used literally.? The Fleiss? Kappas for 5 English
and 6 Russian annotators are: EN-AN = .76, RU-
18
Selection of 1000 most common verbs and adjectives
achieves much broader lexical and domain coverage than
what can be realistically obtained from continuous text. Our
test sentence domains are, therefore, diverse: economic, po-
litical, sports, etc.
19
http://trac.sketchengine.co.uk/wiki/
Corpora/enTenTen
AN = .85, EN-SVO = .75, RU-SVO = .78. For the fi-
nal selection, we filtered out low-agreement (<.8)
sentences.
The test candidate sentences were selected by
a person who did not participate in the selection
of the training samples. No English annotators of
the test set, and only one Russian annotator out
of 6 participated in the selection of the training
samples. Thus, we trust that annotator judgments
were not biased towards the cases that the system
is trained to process.
5 Experiments
5.1 English experiments
Our task, as defined in Section 2, is to classify
SVO and AN relations as either metaphoric or lit-
eral. We first conduct a 10-fold cross-validation
experiment on the training set defined in Section
4.1. We represent each candidate relation using
the features described in Section 3.2, and evalu-
ate performance of the three feature categories and
their combinations. This is done by computing an
accuracy in the 10-fold cross validation. Experi-
mental results are given in Table 2, where we also
provide the number of features in each feature set.
SVO AN
# FEAT ACC # FEAT ACC
AbsImg 20 0.73
?
16 0.76
?
Supersense 67 0.77
?
116 0.79
?
AbsImg+Sup. 87 0.78
?
132 0.80
?
VSM 192 0.81 228 0.84
?
All 279 0.82 360 0.86
Table 2: 10-fold cross validation results for three
feature categories and their combination, for clas-
sifiers trained on English SVO and AN training
sets. # FEAT column shows a number of features.
ACC column reports an accuracy score in the 10-
fold cross validation. Statistically significant dif-
ferences (p < 0.01) from the all-feature combina-
tion are marked with a star.
These results show superior performance over
previous state-of-the-art results, confirming our
hypothesis that conceptual features are effective
in metaphor classification. For the SVO task, the
cross-validation accuracy is about 10% better than
that of Tsvetkov et al (2013). For the AN task,
the cross validation accuracy is better by 8% than
the result of Turney et al (2011) (two baseline
252
methods are described in Section 5.2). We can
see that all types of features have good perfor-
mance on their own (VSM is the strongest feature
type). Noun supersense features alone allows us to
achieve an accuracy of 75%, i.e., adjective super-
sense features contribute 4% to adjective-noun su-
persense feature combination. Experiments with
the pairs of features yield better results than in-
dividual features, implying that the feature cate-
gories are not redundant. Yet, combining all fea-
tures leads to even higher accuracy during cross-
validation. In the case of the AN task, a difference
between the All feature combination and any other
combination of features listed in Table 2 is statis-
tically significant (p < 0.01 for both the sign and
the permutation test).
Although the first experiment shows very high
scores, the 10-fold cross-validation cannot fully
reflect the generality of the model, because all
folds are parts of the same corpus. They are col-
lected by the same human judges and belong to the
same domain. Therefore, experiments on out-of-
domain data are crucial. We carry out such exper-
iments using held-out SVO and AN EN test sets,
described in Section 4.2 and Table 1. In this ex-
periment, we measure the f -score. We classify
SVO and AN relations using a classifier trained on
the All feature combination and balanced thresh-
olds. The values of the f -score are 0.76, both for
SVO and AN tasks. This out-of-domain experi-
ment suggests that our classifier is portable across
domains and genres.
However, (1) different application may have
different requirements for recall/precision, and (2)
classification results may be skewed towards hav-
ing high precision and low recall (or vice versa). It
is possible to trade precision for recall by choos-
ing a different threshold. Thus, in addition to
giving a single f -score value for balanced thresh-
olds, we present a Receiver Operator Characteris-
tic (ROC) curve, where we plot a fraction of true
positives against the fraction of false positives for
100 threshold values in the range from zero to one.
The area under the ROC curve (AUC) can be in-
terpreted as the probability that a classifier will as-
sign a higher score to a randomly chosen positive
example than to a randomly chosen negative ex-
ample.
20
For a randomly guessing classifier, the
ROC curve is a dashed diagonal line. A bad classi-
20
Assuming that positive examples are labeled by ones,
and negative examples are labeled by zeros.
fier has an ROC curve that goes close to the dashed
diagonal or even below it.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
Supersenses (area = 0.77)
AbsImg (area = 0.73)
VSM (area = 0.8)
All (area = 0.79)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
AbsImg (area = 0.9)
Supersenses (area = 0.86)
VSM (area = 0.89)
All (area = 0.92)
(b) AN
Figure 1: ROC curves for classifiers trained using
different feature sets (English SVO and AN test
sets).
According to ROC plots in Figure 1, all three
feature sets are effective, both for SVO and for
AN tasks. Abstractness and Imageability features
work better for adjectives and nouns, which is in
line with previous findings (Turney et al, 2011;
Broadwell et al, 2013). It can be also seen that
VSM features are very effective. This is in line
with results of Hovy et al (2013), who found that
it is hard to improve over the classifier that uses
only VSM features.
5.2 Comparison to baselines
In this section, we compare our method to state-of-
the-art methods of Tsvetkov et al (2013) and of
Turney et al (2011), who focused on classifying
SVO and AN relations, respectively.
In the case of SVO relations, we use software
253
and datasets from Tsvetkov et al (2013). These
datasets, denoted as an SVO-baseline, consist of
98 English and 149 Russian sentences. We train
SVO metaphor detection tools on SVO relations
extracted from TroFi sentences and evaluate them
on the SVO-baseline dataset. We also use the same
thresholds for classifier posterior probabilities as
Tsvetkov et al (2013). Our approach is different
from that of Tsvetkov et al (2013) in that it uses
additional features (vector space word representa-
tions) and a different classification method (we use
random forests while Tsvetkov et al (2013) use
logistic regression). According to Table 3, we ob-
tain higher performance scores for both Russian
and English.
EN RU
SVO-baseline 0.78 0.76
This work 0.86 0.85
Table 3: Comparing f -scores of our SVO
metaphor detection method to the baselines.
In the case of AN relations, we use the dataset
(denoted as an AN-baseline) created by Turney
et al (2011) (see Section 4.1 in the referred pa-
per for details). Turney et al (2011) manu-
ally annotated 100 pairs where an adjective was
one of the following: dark, deep, hard, sweet,
and worm. The pairs were presented to five
human judges who rated each pair on a scale
from 1 (very literal/denotative) to 4 (very non-
literal/connotative). Turney et al (2011) train
logistic-regression employing only abstractness
ratings as features. Performance of the method
was evaluated using the 10-fold cross-validation
separately for each judge.
We replicate the above described evaluation
procedure of Turney et al (2011) using their
model and features. In our classifier, we use the
All feature combination and the balanced thresh-
old as described in Section 5.1.
According to results in Table 4, almost all of the
judge-specific f -scores are slightly higher for our
system, as well as the overall average f -score.
In both baseline comparisons, we obtain perfor-
mance at least as good as in previously published
studies.
5.3 Cross-lingual experiments
In the next experiment we corroborate the main
hypothesis of this paper: a model trained on En-
AN-baseline This work
Judge 1 0.73 0.75
Judge 2 0.81 0.84
Judge 3 0.84 0.88
Judge 4 0.79 0.81
Judge 5 0.78 0.77
average 0.79 0.81
Table 4: Comparing AN metaphor detection
method to the baselines: accuracy of the 10-
fold cross validation on annotations of five human
judges.
glish data can be successfully applied to other
languages. Namely, we use a trained English
model discussed in Section 5.1 to classify literal
and metaphoric SVO and AN relations in English,
Spanish, Farsi and Russian test sets, listed in Sec-
tion 4.2. This time we used all available features.
Experimental results for all four languages, are
given in Figure 2. The ROC curves for SVO and
AN tasks are plotted in Figure 2a and Figure 2b,
respectively. Each curve corresponds to a test set
described in Table 1. In addition, we perform an
oracle experiment, to obtain actual f -score values
for best thresholds. Detailed results are shown in
Table 5.
Consistent results with high f -scores are ob-
tained across all four languages. Note that higher
scores are obtained for the Russian test set. We hy-
pothesize that this happens due to a higher-quality
translation dictionary (which allows a more accu-
rate model transfer). Relatively lower (yet rea-
sonable) results for Farsi can be explained by a
smaller size of the bilingual dictionary (thus, fewer
feature projections can be obtained). Also note
that, in our experience, most of Farsi metaphors
are adjective-noun constructions. This is why the
AN FA dataset in Table 1 is significantly larger
than SVO FA. In that, for the AN Farsi task we
observe high performance scores.
Figure 2 and Table 5 confirm, that we ob-
tain similar, robust results on four very differ-
ent languages, using the same English classi-
fiers. We view this result as a strong evidence of
language-independent nature of our metaphor de-
tection method. In particular, this shows that pro-
posed conceptual features can be used to detect se-
lectional preferences violation across languages.
To summarize the experimental section, our
metaphor detection approach obtains state-of-the-
254
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.79)
ES (area = 0.71)
FA (area = 0.69)
RU (area = 0.89)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.92)
ES (area = 0.73)
FA (area = 0.83)
RU (area = 0.8)
(b) AN
Figure 2: Cross-lingual experiment: ROC curves
for classifiers trained on the English data using a
combination of all features, and applied to SVO
and AN metaphoric and literal relations in four test
languages: English, Russian, Spanish, and Farsi.
art performance in English, is effective when ap-
plied to out-of-domain English data, and works
cross-lingually.
5.4 Examples
Manual data analysis on adjective-noun pairs sup-
ports an abstractness-concreteness hypothesis for-
mulated by several independent research studies.
For example, in English we classify as metaphoric
dirty word and cloudy future. Word pairs dirty
diaper and cloudy weather have same adjectives.
Yet they are classified as literal. Indeed, diaper
is a more concrete term than word and weather
is more concrete than future. Same pattern is ob-
served in non-English datasets. In Russian, ????-
??? ???????? ?sick society? and ?????? ????
?empty sound? are classified as metaphoric, while
SVO AN
EN 0.79 0.85
RU 0.84 0.77
ES 0.76 0.72
FA 0.75 0.74
Table 5: Cross-lingual experiment: f -scores for
classifiers trained on the English data using a com-
bination of all features, and applied, with optimal
thresholds, to SVO and AN metaphoric and literal
relations in four test languages: English, Russian,
Spanish, and Farsi.
??????? ??????? ?sick grandmother? and ??-
???? ????? ?empty cup? are classified as literal.
Spanish example of an adjective-noun metaphor
is a well-known m?usculo econ?omico ?economic
muscle?. We also observe that non-metaphoric ad-
jective noun pairs tend to have more imageable ad-
jectives, such as literal derecho humano ?human
right?. In Spanish, human is more imageable than
economic.
Verb-based examples that are correctly clas-
sified by our model are: blunder escaped no-
tice (metaphoric) and prisoner escaped jail (lit-
eral). We hypothesize that supersense features are
instrumental in the correct classification of these
examples: <noun.person,verb.motion> is usually
used literally, while <noun.act,verb.motion> is
used metaphorically.
6 Related Work
For a historic overview and a survey of
common approaches to metaphor detection,
we refer the reader to recent reviews by
Shutova et al (Shutova, 2010; Shutova et al,
2013). Here we focus only on recent approaches.
Shutova et al (2010) proposed a bottom-up
method: one starts from a set of seed metaphors
and seeks phrases where verbs and/or nouns be-
long to the same cluster as verbs or nouns in seed
examples.
Turney et al (2011) show how abstractness
scores could be used to detect metaphorical AN
phrases. Neuman et al (2013) describe a Concrete
Category Overlap algorithm, where co-occurrence
statistics and Turney?s abstractness scores are used
to determine WordNet supersenses that corre-
spond to literal usage of a given adjective or verb.
For example, given an adjective, we can learn that
it modifies concrete nouns that usually have the
255
supersense noun.body. If this adjective modifies
a noun with the supersense noun.feeling, we con-
clude that a metaphor is found.
Broadwell et al (2013) argue that metaphors
are highly imageable words that do not belong
to a discussion topic. To implement this idea,
they extend MRC imageability scores to all dic-
tionary words using links among WordNet super-
senses (mostly hypernym and hyponym relations).
Strzalkowski et al (2013) carry out experiments
in a specific (government-related) domain for four
languages: English, Spanish, Farsi, and Russian.
Strzalkowski et al (2013) explain the algorithm
only for English and say that is the same for Span-
ish, Farsi, and Russian. Because they heavily
rely on WordNet and availability of imageability
scores, their approach may not be applicable to
low-resource languages.
Hovy et al (2013) applied tree kernels to
metaphor detection. Their method also employs
WordNet supersenses, but it is not clear from the
description whether WordNet is essential or can
be replaced with some other lexical resource. We
cannot compare directly our model with this work
because our classifier is restricted to detection of
only SVO and AN metaphors.
Tsvetkov et al (2013) propose a cross-lingual
detection method that uses only English lexical re-
sources and a dependency parser. Their study fo-
cuses only on the verb-based metaphors. Tsvetkov
et al (2013) employ only English and Russian
data. Current work builds on this study, and incor-
porates new syntactic relations as metaphor candi-
dates, adds several new feature sets and different,
more reliable datasets for evaluating results. We
demonstrate results on two new languages, Span-
ish and Farsi, to emphasize the generality of the
method.
A words sense disambiguation (WSD) is a re-
lated problem, where one identifies meanings of
polysemous words. The difference is that in the
WSD task, we need to select an already existing
sense, while for the metaphor detection, the goal
is to identify cases of sense borrowing. Studies
showed that cross-lingual evidence allows one to
achieve a state-of-the-art performance in the WSD
task, yet, most cross-lingual WSD methods em-
ploy parallel corpora (Navigli, 2009).
7 Conclusion
The key contribution of our work is that we show
how to identify metaphors across languages by
building a model in English and applying it?
without adaptation?to other languages: Spanish,
Farsi, and Russian. This model uses language-
independent (rather than lexical or language spe-
cific) conceptual features. Not only do we estab-
lish benchmarks for Spanish, Farsi, and Russian,
but we also achieve state-of-the-art performance
in English. In addition, we present a comparison
of relative contributions of several types of fea-
tures. We concentrate on metaphors in the con-
text of two kinds of syntactic relations: subject-
verb-object (SVO) relations and adjective-noun
(AN) relations, which account for a majority of all
metaphorical phrases.
Future work will expand the scope of metaphor
identification by including nominal metaphoric re-
lations as well as explore techniques for incor-
porating contextual features, which can play a
key role in identifying certain kinds of metaphors.
Second, cross-lingual model transfer can be im-
proved with more careful cross-lingual feature
projection.
Acknowledgments
We are extremely grateful to Shuly Wintner for a
thorough review that helped us improve this draft;
we also thank people who helped in creating the
datasets and/or provided valuable feedback on this
work: Ed Hovy, Vlad Niculae, Davida Fromm,
Brian MacWhinney, Carlos Ram??rez, and other
members of the CMU METAL team. This work
was supported by the U.S. Army Research Labo-
ratory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proc. of
EACL, pages 23?32.
Julia Birke and Anoop Sarkar. 2007. Active learning
for the identification of nonliteral language. In Proc.
of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?
28.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
256
George Aaron Broadwell, Umit Boz, Ignacio Cases,
Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,
Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.
2013. Using imageability and topic chaining to lo-
cate metaphors in linguistic corpora. In Social Com-
puting, Behavioral-Cultural Modeling and Predic-
tion, pages 102?110. Springer.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder,
Newton Howard, Sergey Kanareykin, Moshe Kop-
pel, Mark Last, Yair Neuman, and Shlomo Arga-
mon. 2013. Automatic identification of conceptual
metaphors with limited knowledge. In Proc. of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 328?334.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48.
Birgit Hamp and Helmut Feldweg. 1997. Germanet-
a lexical-semantic net for German. In Proc. of
ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proc. of
the First Workshop on Metaphor in NLP, page 52.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL, pages 873?882.
Franz Hundsnurscher and Jochen Splett. 1982. Se-
mantik der Adjektive des Deutschen. Number 3137.
Westdeutscher Verlag.
Mikhail Kozhenikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models. In
Proc. of ACL, pages 1190?1200.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proc. of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
Lori Levin, Teruko Mitamura, Davida Fromm, Brian
MacWhinney, Jaime Carbonell, Weston Feely,
Robert Frederking, Anatole Gershman, and Carlos
Ramirez. 2014. Resources for the detection of con-
ventionalized metaphors in four languages. In Proc.
of LREC.
Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?ario A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proc. of ENMLP, pages 34?
44.
Zachary J Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proc. of EMNLP.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for Ma-
chine Translation. CoRR, abs/1309.4168.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,
Shlomo Argamon, Newton Howard, and Ophir
Frieder. 2013. Metaphor identification in large texts
corpora. PloS one, 8(4):e62343.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Si-
monoff. 2003. Tree induction vs. logistic regres-
sion: a learning-curve analysis. Journal of Machine
Learning Research, 4:211?255.
Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal
Oflazer, and Noah A Smith. 2013. Supersense tag-
ging for Arabic: the MT-in-the-middle attack. In
Proc. of NAACL-HLT, pages 661?667.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proc. of NAACL-HLT,
pages 978?988.
257
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target domain
mappings. In Proc. of LREC, pages 3255?3261.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proc. of COLING, pages 1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proc. of ACL, pages 688?697.
Gerard J Steen, Aletta G Dorst, J Berenike Her-
rmann, Anna A Kaal, and Tina Krennmayr.
2010. Metaphor in usage. Cognitive Linguistics,
21(4):765?796.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al 2013. Robust extraction of metaphors from
novel data. In Proc. of the First Workshop on
Metaphor in NLP, page 67.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Paul H Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in
reasoning. PLoS One, 6(2):e16782.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection using
common semantic features. In The 1st Workshop on
Metaphor in NLP 2013, page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting English adjective senses with super-
senses. In Proc. of LREC.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL, pages
384?394.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proc. of EMNL, pages 680?690.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
Michael Wilson. 1988. MRC Psycholinguistic
Database: Machine-usable dictionary, version 2.00.
Behavior Research Methods, Instruments, & Com-
puters, 20(1):6?10.
258
Proceedings of the First Workshop on Metaphor in NLP, pages 45?51,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Metaphor Detection Using Common Semantic Features
Yulia Tsvetkov Elena Mukomel Anatole Gershman
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{ytsvetko,helenm,anatoleg}@cs.cmu.edu
Abstract
We present the CSF - Common Semantic Fea-
tures method for metaphor detection. This
method has two distinguishing characteristics:
it is cross-lingual and it does not rely on the
availability of extensive manually-compiled
lexical resources in target languages other than
English. A metaphor detecting classifier is
trained on English samples and then applied to
the target language. The method includes pro-
cedures for obtaining semantic features from
sentences in the target language. Our exper-
iments with Russian and English sentences
show comparable results, supporting our hy-
pothesis that a CSF-based classifier can be ap-
plied across languages. We obtain state-of-
the-art performance in both languages.
1 Introduction
Metaphors are very powerful pervasive communica-
tion tools that help deliver complex concepts and
ideas simply and effectively (Lakoff and Johnson,
1980). Automatic detection and interpretation of
metaphors is critical for many practical language
processing tasks such as information extraction,
summarization, opinion mining, and translation. In
this paper, we focus on the automatic metaphor de-
tection task. This problem gained much attention
in natural language processing research mostly us-
ing the detection principles articulated by the Prag-
glejaz Group (2007). According to these princi-
ples, a lexical unit (a word or expression) is used
metaphorically if its contextual meaning is different
from its ?basic contemporary? meaning. To apply
this method, we need to be able to determine the ba-
sic meaning of a lexical unit and then test if this in-
terpretation makes sense in the current context.
Several approaches to automatic detection of
metaphors have been proposed (Gedigian et al,
2006; Krishnakumaran and Zhu, 2007; Shutova et
al., 2010), all of which rely on the availability of
extensive manually crafted lexical resources such
as WordNet, VerbNet, FrameNet, TreeBank, etc.
Unfortunately, such resources exist only for a few
resource-rich languages such as English. For most
other languages, such resources either do not exist
or are of a low quality.
To our knowledge this work is the first empiri-
cal study of cross-lingual metaphor detection. We
present the Common Semantic Features (CSF) ap-
proach to metaphor detection in languages without
extensive lexical resources. In a target language
it requires only a dependency parser and a target-
English dictionary. We classify sentences into lit-
eral and metaphoric using automatically extracted
coarse-grained semantic properties of words such as
their propensity to refer to abstract versus concrete
concepts, animate entities, artifacts, body parts, etc.
These properties serve as features for the key re-
lations in a sentence, which include Subject-Verb-
Object (SVO) and Adjective-Noun (AN). A clas-
sifier trained on English sentences obtains a 0.78
F -score. The same classifier, trained solely on
English sentences, achieves a similar level of per-
formance on sentences from other languages such
as Russian; this is the central contribution of this
work. An additional important contribution is that in
Russian we obtain the necessary semantic features
45
without recourse to sophisticated non-English lexi-
cal resources. In this paper, we focus on the sen-
tences where verbs are used metaphorically, leaving
Adjective-Noun relations for future work. Based on
our examination of over 500 metaphorical sentences
in English and Russian collected from general news
articles, we estimate that verb-based metaphors con-
stitute about 40-50% of all metaphors.
We present and discuss our experiments with
three sets of features: (1) features corresponding to
the lexicographer file names defined in WordNet
3.0 (Fellbaum, 1998), (2) features based on abstract-
ness vs. concreteness computed using Vector Space
Models (VSM), and (3) features based on the types
of named entities, if present. Our main target lan-
guage in these experiments has been Russian, but we
also present preliminary experiments with Spanish.
The paper is organized as follows: Section 2 con-
tains an overview of the resources we use; Sec-
tion 3 discusses the methodology; Section 4 presents
the experiments; in Section 5, we discuss related
work, and we conclude with suggestions for future
research in Section 6.
2 Datasets
We use the following English lexical resources to
train our model:
TroFi Example Base1 (Birke and Sarkar, 2007) of
3,737 English sentences from the Wall Street Jour-
nal. Each sentence contains one of the seed verbs
and is marked L by human annotators if the verb
is used in a literal sense. Otherwise, the sentence
is marked N (non-literal). The model was evalu-
ated on 25 target verbs with manually annotated 1
to 115 sentences per verb. TroFi does not define the
basic meanings of these verbs, but provides exam-
ples of literal and metaphoric sentences which we
use to train and evaluate our metaphor identification
method.
WordNet (Fellbaum, 1998) is an English lexical
database where each entry contains a set of syn-
onyms (a synset) all representing the same con-
cept. This database is compiled from a set of
1http://www.cs.sfu.ca/ anoop/students/jbirke/
45 lexicographer files2 such as ?noun.body? or
?verb.cognition? identified by a number from 0 to
44, called lexicographer file number (henceforth
lexFN ). The lexFN of each synset is contained in
the database. We use lexFNs as coarse-grain se-
mantic features of nouns and verbs.
MRC Psycholinguistic Database3 (Wilson, 1988)
is a dictionary containing 150,837 words with up to
26 linguistic and psycholinguistic attributes rated by
human subjects in psycholinguistic experiments. It
includes 4,295 words rated with degrees of abstract-
ness; the ratings range from 158 (highly abstract)
to 670 (highly concrete). We use these words as a
seed when we calculate the values of abstractness
and concreteness features for nouns and verbs in our
training and test sets.
Word Representations via Global Context is a
collection of 100,232 words and their vector rep-
resentations.4 These representations were extracted
from a statistical model embedding both local and
global contexts of words (Huang et al, 2012), in-
tended to capture better the semantics of words. We
use these vectors to calculate the values of abstract-
ness and concreteness features of a word.
3 Methodology
We treat the metaphor detection problem as a task
of binary classification of sentences. A sentence
is represented by one or more key relations such
as Subject-Verb-Object triples and Adjective-Noun
pairs. In this paper, we focus only on the SVO rela-
tions and we allow either the S part or the O part to
be empty. If all relations representing a sentence are
classified literal by our model then the whole sen-
tence is tagged literal. Otherwise, the sentence is
tagged metaphoric.
2See http://wordnet.princeton.edu/man/lexnames.5WN.html
for a full list of lexicographer file names.
3http://ota.oucs.ox.ac.uk/headers/1054.xml
4http://www.socher.org/index.php/Main/Improving-
WordRepresentationsViaGlobalContextAndMultipleWordPrototypes
46
3.1 Model
We classify an SVO relation x as literal vs.
metaphorical using a logistic regression classifier:
p(y | x) ? exp
?
j
?jhj(y, x),
where hj(?) are feature values computed for each
word in x, ?j are the corresponding weights, and
y ? {L,M} refer to our classes: L for literal and
M for metaphoric. The parameters ?j are learned
during training.
3.2 Features
An SVO relation is a concatenation of features for
the S, V, and O parts. The S and O parts contain
three types of features: (1) semantic categories of a
word, (2) degree of abstractness of a word, and (3)
types of named entities. The V part contains only
the first two types of features.
Semantic categories are features corresponding
to the WordNet lexFNs, introduced in Section 2.
Since S and O are assumed to be nouns,5 each has
26 semantic category features corresponding to the
lexFNs for nouns (3 through 28). These categories
include noun.animal, noun.artefact, noun.body,
noun.cognition, noun.food, noun.location, etc. The
V part has 15 semantic category features corre-
sponding to lexical ids for verbs (29 through 43),
for example, verb.motion and verb.cognition. A lex-
ical item can belong to several synsets with different
lexFNs. For example, the word ?head? when used
as a noun participates in 33 synsets, 3 of which have
lexFN 08 (noun.body). The value of the feature
corresponding to this lexFN is 3/33 = 0.09.
For a non-English word, we first obtain its most
common translations to English and then select all
corresponding English WordNet synsets. For exam-
ple, when Russian word `??????' is translated as
?head? and ?brain?, we select all the synsets for the
nouns head and brain. There are 38 such synsets (33
for head and 5 for brain). Four of these synsets have
lexFN 08 (noun.body). Therefore, the value of
the feature corresponding to this lexFN is 4/38 =
0.10. This dictionary-based mapping of non-English
5We currently exclude pronouns from the relations that we
learn.
words into WN synsets is rather coarse. A more dis-
criminating approach may improve the overall per-
formance. In addition, WN synsets may not always
capture all the meanings of non-English words. For
example, Russian word `????' refers to both the
?foot? and the ?leg?. WN has synsets for foot, leg
and extremity, but not for lower extremity.
Degree of abstractness According to Turney et al
(2011), ?Abstract words refer to ideas and concepts
that are distant from immediate perception, such as
economics, calculating and disputable.? Concrete
words refer to physical objects and actions. Words
with multiple senses can refer to both concrete and
abstract concepts. Evidence from several languages
suggests that concrete verbs tend to have concrete
subjects and objects. If either the subject or an object
of a concrete verb is abstract, then the verb is typi-
cally used in a figurative sense, indicating the pres-
ence of a metaphor. For example, when we hear that
?an idea was born?, we know that the word ?born?
is used figuratively. This observation motivates our
decision to include the degree of abstractness in our
feature set.
To calculate the degree of abstractness of English
lexical items we use the vector space representations
of words computed by Huang et al (2012) and a sep-
arate supervised logistic regression classifier trained
on a set of abstract and concrete words from the
MRC dataset. Each value in a word?s vector is a fea-
ture, thus, semantically similar words have similar
feature values. Degrees of abstractness are posterior
probabilities of the classifier predictions.
For non-English words, we use the following pro-
cedure. Suppose word w has n English transla-
tions whose degrees of abstractness are a1, a2, . . . an
in decreasing order. If the majority is deemed
abstract then ABSTRACT (w) = a1, otherwise
ABSTRACT (w) = an. This heuristic prefers the
extreme interpretations, and is based on an observa-
tion that translations tend to be skewed to one side or
the other of ?abstractness?. Our results may improve
if we map non-English words more precisely into the
most contextually-appropriate English senses.
Named entities (NE) is an additional category
of features instrumental in metaphor identification.
Specifically, we would like to distinguish whether
an action (a verb in SVO) is performed by a human,
47
an organization or a geographical entity. These dis-
tinctions are often needed to detect metonymy, as in
?the White House said?. Often, these entities are
mentioned by their names which are not found in
common dictionaries. Fortunately, there are many
named entity recognizers (NER) for all major lan-
guages. In addition, Shah et al (2010) showed
that named entities tend to survive popular machine
translation engines and can be relatively reliably de-
tected even without a native NER. Based on these
observations, we decided to include three boolean
features corresponding to these NE categories: per-
son, organization, and location.
4 Experiments
We train two classifiers: the first to calculate the de-
gree of abstractness of a given word and the second
to classify an SVO relation as metaphoric or literal.
Both are logistic regression classifiers trained with
the creg regression modeling framework.6 To min-
imize the number of free parameters in our model we
use `1 regularization.
4.1 Measuring abstractness
To train the abstractness classifier, we normalize ab-
stractness scores of nouns from the MRC dataset
to probabilities, and select 1,225 most abstract and
1,225 most concrete words. From these words, we
set aside 25 randomly selected samples from each
category for testing. We obtain the vector space rep-
resentations of the remaining 1,400 samples and use
the dimensions of these representations as features.
We train the abstractness classifier on the 1,400 la-
beled samples and test it on the 50 samples that were
set aside, obtaining 76% accuracy. The degree of ab-
stractness of a word is the posterior probability pro-
duced by the abstractness classifier.
4.2 Metaphor detection
We train the metaphor classifier using labeled En-
glish SVO relations. To obtain these relations,
we use the Turbo parser (Martins et al, 2010) to
parse 1,592 literal and 1,609 metaphorical man-
ually annotated sentences from the TroFi Exam-
ple Base and extract 1,660 sentences that have
SVO relations that contain annotated verbs: 696
6https://github.com/redpony/creg
literal and 964 metaphorical training instances.
For example, the verb flourish is used literally in
?Methane-making bacteria flourish in the stom-
ach? and metaphorically in ?Economies flourish in
free markets?. From the first sentence we extract
SVO relation <bacteria, flourish, NIL>,
and <economies, flourish, NIL> from the
second. We then build feature vectors, using feature
categories described in Section 3.
We train several versions of the metaphor classi-
fier for each feature category and for their combina-
tions. The feature categories are designated as fol-
lows:
? WN - Semantic categories based on WordNet lexFNs
? VSM - Degree of abstractness based on word vectors
? NE - Named Entity categories
We evaluate the metaphor classifiers using 10-fold
cross validation. The results are listed in Table 1.
Feature categories Accuracy
WN 63.7%
VSM 64.1%
WN+VSM 67.7%
WN+NE 64.5%
WN+VSM+NE 69.0%
Table 1: 10-fold cross validation results of the
metaphor classifier.
Our results are comparable to the accuracy of
64.9% reported by Birke and Sarkar (2007) on the
TroFi dataset. The combination of all feature cate-
gories significantly improves over this baseline.
4.2.1 English metaphor detection
We compute precision, recall and F -score on a
test set of 98 English sentences. This test set consists
of 50 literal and 48 metaphorical sentences, where
each metaphoric sentence contains a verb used in a
figurative sense. The test sentences were selected
from general news articles by independent collec-
tors. Table 2 shows the results.
In this experiment, the WN group of features con-
tributes the most. The addition of NE, while not im-
proving the overall F -score, helps to reduce false
positives and better balance precision and recall.
The VSM features are considerably weaker perhaps
48
Feature categories Precision Recall F -score
WN 0.75 0.81 0.78
VSM 0.57 0.71 0.63
WN+VSM 0.66 0.90 0.76
WN+NE 0.78 0.79 0.78
WN+VSM+NE 0.68 0.71 0.69
Table 2: Evaluation of the metaphor classifier on
the test set of 50 literal and 48 metaphoric English
sentences from news articles.
because we used single model vector space repre-
sentations where each word uses only one vector that
combines all its senses.
4.2.2 Russian metaphor detection
In a cross-lingual experiment, we evaluate our al-
gorithm on a set of 140 Russian sentences: 62 literal
and 78 metaphoric, selected from general news arti-
cles by two independent collectors. As in English,
each metaphoric sentence contains a verb used in a
figurative sense. We used the AOT parser7 to ob-
tain the SVO relations and the Babylon dictionary8
to obtain English translations of individual words.
The example sentence in Figure 1 contains one SVO
relation with missing O part. We show the set of fea-
tures and their values that were extracted from words
in this relation.
The results of the Russian test set, listed in Ta-
ble 3, are similar to the English results, supporting
our hypothesis that a semantic classifier can work
across languages. As in the previous experiment, the
WN features are the most effective and the NE fea-
tures contribute to improved precision.
Feature categories Precision Recall F -score
WN 0.74 0.76 0.75
VSM 0.66 0.73 0.69
WN+VSM 0.70 0.73 0.71
WN+NE 0.82 0.71 0.76
WN+VSM+NE 0.74 0.72 0.73
Table 3: Evaluation of the metaphor classifier on
the test set of 62 literal and 78 metaphoric Russian
sentences from news articles.
While we did not conduct a full-scale experiment
7www.aot.ru
8www.babylon.com
with Spanish, we ran a pilot using 51 sentences: 24
literal and 27 metaphoric. We obtained the F -score
of 0.66 for the WN+VSM combination. We take it as
a positive sign and will conduct more experiments.
5 Related work
Our work builds on the research of Birke and Sarkar
(2007) who used an active learning approach to cre-
ate an annotated corpus of sentences with literal
and figurative senses of 50 common English verbs.
The result was the TroFi Example Base set of 3,737
labeled sentences, which was used by the authors
to train several classifiers. These algorithms were
tested on sentences containing 25 English verbs not
included in the original set. The authors report F -
scores around 64.9%. We used this dataset for train-
ing and evaluation, and Birke and Sarkar?s (2007)
results as a baseline.
In a more recent work, Turney et al (2011) sug-
gested that the degree of abstractness of a word?s
context is correlated with the likelihood that the
word is used metaphorically. To compute the ab-
stractness of a word, the authors use a variation
of Turney and Littman?s (2003) algorithm compar-
ing the word to twenty typically abstract words and
twenty typically concrete words. Latent Semantic
Analysis (Deerwester et al, 1990) is used to mea-
sure semantic similarity between each pair of words.
A feature vector is generated for each word and a
logistic regression classifier is used. The result is
an average F -score of 63.9% on the TroFi dataset,9
compared to Birke and Sarkar?s (2007) 64.9%. In
another experiment on 100 adjective-noun phrases
labeled as literal or non-literal, according to the
sense of the adjective, this algorithm obtains an av-
erage accuracy of 79%. While we obtain compara-
ble results, our work extends this method in several
important directions. First, we show how to apply
a metaphor classifier across languages. Second, we
extend our feature set beyond abstractness criteria.
Finally, we propose an alternative technique to mea-
sure degrees of abstractness.
9Turney et al (2011) report on two experimental setups with
TroFi, our setup is closer to their first experiment.
49
???????? ????? ????????????? .
?Society ripens over decades?
SVO = <????????, ?????, NIL>
Subject Verb
WN
noun.group 0.54
noun.state 0.23
noun.possession 0.15
noun.location 0.08
verb.change 0.75
verb.body 0.125
verb.communication 0.125
VSM Abstractness 0.87 Abstractness 0.93
Figure 1: Features extracted for a Russian test sentence classified as metaphoric by our model.
6 Conclusions and future work
We presented CSF ? an approach to metaphor de-
tection based on semantic rather than lexical fea-
tures. We described our experiments with an ini-
tial set of fairly coarse-grained features and showed
how these features can be obtained in languages that
lack extensive lexical resources. Semantic, as op-
posed to lexical features, are common to all lan-
guages which allows a classifier trained to detect
metaphors in one language to be successfully ap-
plied to sentences in another language. Our results
suggest that metaphors can be detected on a con-
ceptual level, independently of whether they are ex-
pressed in Russian or English, supporting Lakoff
and Johnson?s (1980) claim that metaphors are parts
of a pervasive conceptual system.
Our current work has been limited to the detection
of figurative SVO relations, which account for about
half of all metaphors in English and Russian. Other
languages such as Farsi have a greater proportion of
metaphors based on figurative use of adjectives and
nouns. We plan to include more relations and ex-
pand our set of semantic features as part of the future
research.
Acknowledgments
We are grateful to Chris Dyer for his invaluable advice.
We are also grateful to the three anonymous reviewers for
their constructive suggestions. Supported by the Intelli-
gence Advanced Research Projects Activity (IARPA) via
Department of Defense US Army Research Laboratory
contract number W911NF-12-C-0020. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and conclu-
sions contained herein are those of the authors and should
not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
References
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?28.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural Language
Understanding, pages 41?48.
Pragglejaz Group. 2007. MIP: A method for identify-
ing metaphorically used words in discourse. Metaphor
and Symbol, 22(1):1?39.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2012.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
50
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 34?44.
Rushin Shah, Bo Lin, Anatole Gershman, and Robert
Frederking. 2010. SYNERGY: a named entity recog-
nition system for resource-scarce languages such as
Swahili using online machine translation. In Proceed-
ings of the Second Workshop on African Language
Technology, AfLaT 2010, pages 21?26.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics, pages 1002?1010.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation and System Security, 21(4):315?346.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages
680?690.
Michael Wilson. 1988. MRC Psycholinguistic Database:
Machine-usable dictionary, version 2.00. Behav-
ior Research Methods, Instruments, & Computers,
20(1):6?10.
51
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Identifying the L1 of non-native writers: the CMU-Haifa system
Yulia Tsvetkov? Naama Twitto? Nathan Schneider? Noam Ordan?
Manaal Faruqui? Victor Chahuneau? Shuly Wintner? Chris Dyer?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA
cdyer@cs.cmu.edu
?Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
We show that it is possible to learn to identify, with
high accuracy, the native language of English test
takers from the content of the essays they write.
Our method uses standard text classification tech-
niques based on multiclass logistic regression, com-
bining individually weak indicators to predict the
most probable native language from a set of 11 pos-
sibilities. We describe the various features used for
classification, as well as the settings of the classifier
that yielded the highest accuracy.
1 Introduction
The task we address in this work is identifying the
native language (L1) of non-native English (L2) au-
thors. More specifically, given a dataset of short
English essays (Blanchard et al, 2013), composed
as part of the Test of English as a Foreign Lan-
guage (TOEFL) by authors whose native language is
one out of 11 possible languages?Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, or Turkish?our task is to identify
that language.
This task has a clear empirical motivation. Non-
native speakers make different errors when they
write English, depending on their native language
(Lado, 1957; Swan and Smith, 2001); understand-
ing the different types of errors is a prerequisite for
correcting them (Leacock et al, 2010), and systems
such as the one we describe here can shed interest-
ing light on such errors. Tutoring applications can
use our system to identify the native language of
students and offer better-targeted advice. Forensic
linguistic applications are sometimes required to de-
termine the L1 of authors (Estival et al, 2007b; Es-
tival et al, 2007a). Additionally, we believe that the
task is interesting in and of itself, providing a bet-
ter understanding of non-native language. We are
thus equally interested in defining meaningful fea-
tures whose contribution to the task can be linguis-
tically interpreted. Briefly, our features draw heav-
ily on prior work in general text classification and
authorship identification, those used in identifying
so-called translationese (Volansky et al, forthcom-
ing), and a class of features that involves determin-
ing what minimal changes would be necessary to
transform the essays into ?standard? English (as de-
termined by an n-gram language model).
We address the task as a multiway text-
classification task; we describe our data in ?3 and
classification model in ?4. As in other author attri-
bution tasks (Juola, 2006), the choice of features for
the classifier is crucial; we discuss the features we
define in ?5. We report our results in ?6 and con-
clude with suggestions for future research.
2 Related work
The task of L1 identification was introduced by Kop-
pel et al (2005a; 2005b), who work on the Inter-
national Corpus of Learner English (Granger et al,
2009), which includes texts written by students from
5 countries, Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts range from 500 to
850 words in length. Their classification method
is a linear SVM, and features include 400 standard
function words, 200 letter n-grams, 185 error types
and 250 rare part-of-speech (POS) bigrams. Ten-
279
fold cross-validation results on this dataset are 80%
accuracy.
The same experimental setup is assumed by Tsur
and Rappoport (2007), who are mostly interested
in testing the hypothesis that an author?s choice of
words in a second language is influenced by the
phonology of his or her L1. They confirm this hy-
pothesis by carefully analyzing the features used by
Koppel et al, controlling for potential biases.
Wong and Dras (2009; 2011) are also motivated
by a linguistic hypothesis, namely that syntactic er-
rors in a text are influenced by the author?s L1.
Wong and Dras (2009) analyze three error types sta-
tistically, and then add them as features in the same
experimental setup as above (using LIBSVM with a
radial kernel for classification). The error types are
subject-verb disagreement, noun-number disagree-
ment and misuse of determiners. Addition of these
features does not improve on the results of Kop-
pel et al. Wong and Dras (2011) further extend
this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic struc-
ture. This improves the results significantly, yielding
78% accuracy compared with less than 65% using
only lexical features.
Kochmar (2011) uses a different corpus, the Cam-
bridge Learner Corpus, in which texts are 200-400
word long, and are authored by native speakers of
five Germanic languages (German, Swiss German,
Dutch, Swedish and Danish) and five Romance lan-
guages (French, Italian, Catalan, Spanish and Por-
tuguese). Again, SVMs are used as the classification
device. Features include POS n-grams, character n-
grams, phrase-structure rules (extracted from parse
trees), and two measures of error rate. The classi-
fier is evaluated on its ability to distinguish between
pairs of closely-related L1s, and the results are usu-
ally excellent.
A completely different approach is offered by
Brooke and Hirst (2011). Since training corpora for
this task are rare, they use mainly L1 (blog) cor-
pora. Given English word bigrams ?e1, e2?, they try
to assess, for each L1, how likely it is that an L1 bi-
gram was translated literally by the author, resulting
in ?e1, e2?. Working with four L1s (French, Span-
ish, Chinese, and Japanese), and evaluating on the
International Corpus of Learner English, accuracy is
below 50%.
3 Data
Our dataset in this work consists of TOEFL essays
written by speakers of eleven different L1s (Blan-
chard et al, 2013), distributed as part of the Na-
tive Language Identification Shared Task (Tetreault
et al, 2013). The training data consists of 1000
essays from each native language. The essays are
short, consisting of 10 to 20 sentences each. We
used the provided splits of 900 documents for train-
ing and 100 for development. Each document is an-
notated with the author?s English proficiency level
(low, medium, high) and an identification (1 to 8) of
the essay prompt. All essays are tokenized and split
into sentences. In table 1 we provide some statistics
on the training corpora, listed by the authors? profi-
ciency level. All essays were tagged with the Stan-
ford part-of-speech tagger (Toutanova et al, 2003).
We did not parse the dataset.
Low Medium High
# Documents 1,069 5,366 3,456
# Tokens 245,130 1,819,407 1,388,260
# Types 13,110 37,393 28,329
Table 1: Training set statistics.
4 Model
For our classification model we used the creg re-
gression modeling framework to train a 11-class lo-
gistic regression classifier.1 We parameterize the
classifier as a multiclass logistic regression:
p?(y | x) =
exp
?
j ? jh j(x, y)
Z?(x)
,
where x are documents, h j(?) are real-valued feature
functions of the document being classified, ? j are the
corresponding weights, and y is one of the eleven L1
class labels. To train the parameters of our model,
we minimized the following objective,
L = ?
`2 reg.
?????
j
?2j ?
?
{(xi,yi)}
|D|
i=1
(
log likelihood
?          ??          ?
log p?(yi | xi) +
?Ep?(y? |xi) log p?(y
? | xi)
?                      ??                      ?
?conditional entropy
)
,
1https://github.com/redpony/creg
280
which combines the negative log likelihood of the
training dataset D, an `2 (quadratic) penalty on the
magnitude of ? (weighted by ?), and the negative en-
tropy of the predictive model (weighted by ?). While
an `2 weight penalty is standard in regression prob-
lems like this, we found that the the additional en-
tropy term gave more reliable results. Intuitively,
the entropic regularizer encourages the model to re-
main maximally uncertain about its predictions. In
the metaphor of ?maximum entropy?, the entropic
prior finds a solution that has more entropy than the
?maximum? model that is compatible with the con-
straints.
The objective cannot be minimized in closed
form, but it does have a unique minimum and
is straightforwardly differentiable, so we used L-
BFGS to find the optimal weight settings (Liu et al,
1989).
5 Feature Overview
We define a large arsenal of features, our motivation
being both to improve the accuracy of classification
and to be able to interpret the characteristics of the
language produced by speakers of different L1s.
While some of the features were used in prior
work (?2), we focus on two broad novel categories
of features: those inspired by the features used
to identify translationese by Volansky et al (forth-
coming) and those extracted by automatic statisti-
cal ?correction? of the essays. Refer to figure 1 to
see the set of features and their values that were ex-
tracted from an example sentence.
POS n-grams Part-of-speech n-grams were used in
various text-classification tasks.
Prompt Since the prompt contributes information
on the domain, it is likely that some words (and,
hence, character sequences) will occur more fre-
quently with some prompts than with others. We
therefore use the prompt ID in conjunction with
other features.
Document length The number of tokens in the text
is highly correlated with the author?s level of flu-
ency, which in turn is correlated with the author?s
L1.
Pronouns The use of pronouns varies greatly
among different authors. We use the same list
of 25 English pronouns that Volansky et al (forth-
coming) use for identifying translationese.
Punctuation Similarly, different languages use
punctuation differently, and we expect this to taint
the use of punctuation in non-native texts. Of
course, character n-grams subsume this feature.
Passives English uses passive voice more fre-
quently than other languages. Again, the use of
passives in L2 can be correlated with the author?s
L1.
Positional token frequency The choice of the first
and last few words in a sentence is highly con-
strained, and may be significantly influenced by
the author?s L1.
Cohesive markers These are 40 function words
(and short phrases) that have a strong discourse
function in texts (however, because, in fact,
etc.). Translators tend to spell out implicit utter-
ances and render them explicitly in the target text
(Blum-Kulka, 1986). We use the list of Volansky
et al (forthcoming).
Cohesive verbs This is a list of manually compiled
verbs that are used, like cohesive markers, to spell
out implicit utterances (indicate, imply, contain,
etc.).
Function words Frequent tokens, which are mostly
function words, have been used successfully for
various text classification tasks. Koppel and Or-
dan (2011) define a list of 400 such words, of
which we only use 100 (using the entire list was
not significantly different). Note that pronouns
are included in this list.
Contextual function words To further capitalize
on the ability of function words to discriminate,
we define pairs consisting of a function word from
the list mentioned above, along with the POS tag
of its adjacent word. This feature captures pat-
terns such as verbs and the preposition or particle
immediately to their right, or nouns and the deter-
miner that precedes them. We also define 3-grams
consisting of one or two function words and the
POS tag of the third word in the 3-gram.
Lemmas The content of the text is not considered a
good indication of the author?s L1, but many text
categorization tasks use lemmas (more precisely,
the stems produced by the tagger) as features ap-
proximating the content.
Misspelling features Learning to perceive, pro-
duce, and encode non-native phonemic contrasts
281
Firstly the employers live more savely because they are going to have more money to spend for luxury .
Presence Considered alternatives/edits
Characters
"CHAR_l_y_ ": log 2 + 1
"CharPrompt_P5_g_o_i": log 1 + 1
"MFChar_e_ ": log 1 + 1
"Punc_period": log 1 + 1
"DeleteP_p_.": 1.0
"InsertP_p_,": 1.0
"MID:SUBST:v:f": log 1 + 1
"SUBST:v:f": log 1 + 1
Words
"DocLen_": log 19 + 1
"MeanWordRank": 422.6
"CohMarker_because": log 1 + 1
"MostFreq_have": log 1 + 1
"PosToken_last_luxury": log 1 + 1
"Pronouns_they": log 1 + 1
"MSP:safely": log 1 + 1
"Match_p_to": 0.5
"Delete_p_to": 0.5
"Delete_p_are": 1.0
"Delete_p_because": 1.0
"Delete_p_for": 1.0
POS "POS
_VBP_VBG_TO": log 1 + 1
"POS_p_VBP_VBG_TO": 0.059
Words + POS "VBP
_VBG_to": log 1 + 1
"FW__more RB": log 1 + 1
Figure 1: Some of the features extracted for an L1 German sentence.
is extremely difficult for L2 learners (Hayes-Harb
and Masuda, 2008). Since English?s orthogra-
phy is largely phonemic?even if it is irregular
in many places, we expect leaners whose na-
tive phoneme contrasts are different from those
of English to make characteristic spelling errors.
For example, since Japanese and Korean lack a
phonemic /l/-/r/ contrast, we expect native speak-
ers of those languages to be more likely to make
spelling errors that confuse l and r relative to
native speakers of languages such as Spanish in
which that pair is contrastive. To make this in-
formation available to our model, we use a noisy
channel spelling corrector (Kernighan, 1990) to
identify and correct misspelled words in the train-
ing and test data. From these corrections, we ex-
tract minimal edit features that show what inser-
tions, deletions, substitutions and joinings (where
two separate words are written merged into a sin-
gle orthographic token) were made by the author
of the essay.
Restored tags We focus on three important token
classes defined above: punctuation marks, func-
tion words and cohesive verbs. We first remove
words in these classes from the texts, and then
recover the most likely hidden tokens in a se-
quence of words, according to an n-gram lan-
guage model trained on all essays in the training
corpus corrected with a spell checker and con-
taining both words and hidden tokens. This fea-
ture should capture specific words or punctuation
marks that are consistently omitted (deletions),
or misused (insertions, substitutions). To restore
hidden tokens we use the hidden-ngram util-
ity provided in SRI?s language modeling toolkit
(Stolcke, 2002).
Brown clusters (Brown et al, 1992) describe an al-
gorithm that induces a hierarchical clustering of
a language?s vocabulary based on each vocabu-
lary item?s tendency to appear in similar left and
right contexts in a training corpus. While origi-
nally developed to reduce the number of parame-
ters required in n-gram language models, Brown
clusters have been found to be extremely effective
as lexical representations in a variety of regres-
sion problems that condition on text (Koo et al,
2008; Turian et al, 2010; Owoputi et al, 2013).
Using an open-source implementation of the al-
gorithm,2 we clustered 8 billion words of English
into 600 classes.3 We included log counts of all
4-grams of Brown clusters that occurred at least
100 times in the NLI training data.
5.1 Main Features
We use the following four feature types as the base-
line features in our model. For features that are sen-
sitive to frequency, we use the log of the (frequency-
plus-one) as the feature?s value. Table 2 reports the
accuracy of using each feature type in isolation (with
2https://github.com/percyliang/brown-cluster
3http://www.ark.cs.cmu.edu/cdyer/en-600/
cluster_viewer.html
282
Feature Accuracy (%)
POS 55.18
FreqChar 74.12
CharPrompt 65.09
Brown 72.26
DocLen 11.81
Punct 27.41
Pron 22.81
Position 53.03
PsvRatio 12.26
CxtFxn (bigram) 62.79
CxtFxn (trigram) 62.32
Misspell 37.29
Restore 47.67
CohMark 25.71
CohVerb 22.85
FxnWord 42.47
Table 2: Independent performance of feature types de-
tailed in ?5.1, ?5.2 and ?5.3. Accuracy is averaged over
10 folds of cross-validation on the training set.
10-fold cross-validation on the training set).
POS Part-of-speech n-grams. Features were ex-
tracted to count every POS 1-, 2-, 3- and 4-gram
in each document.
FreqChar Frequent character n-grams. We exper-
imented with character n-grams: To reduce the
number of parameters, we removed features only
those character n-grams that are observed more
than 5 times in the training corpus, and n ranges
from 1 to 4. High-weight features include:
TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.
CharPrompt Conjunction of the character n-gram
features defined above with the prompt ID.
Brown Substitutions, deletions and insertions
counts of Brown cluster unigrams and bigrams in
each document.
The accuracy of the classifier on the development set
using these four feature types is reported in table 3.4
5.2 Additional Features
To the basic set of features we now add more spe-
cific, linguistically-motivated features, each adding
a small number of parameters to the model. As
above, we indicate the accuracy of each feature type
in isolation.
4For experiments in this paper combining multiple types of
features, we used Jonathan Clark?s workflow management tool,
ducttape (https://github.com/jhclark/ducttape).
Feature Group # Params Accuracy (%) `2
POS 540,947 55.18 1.0
+ FreqChar 1,036,871 79.55 1.0
+ CharPrompt 2,111,175 79.82 1.0
+ Brown 5,664,461 81.09 1.0
Table 3: Dev set accuracy with main feature groups,
added cumulatively. The number of parameters is always
a multiple of 11 (the number of classes). Only `2 regular-
ization was used for these experiments; the penalty was
tuned on the dev set as well.
DocLen Document length in tokens.
Punct Counts of each punctuation mark.
Pron Counts of each pronoun.
Position Positional token frequency. We use the
counts for the first two and last three words be-
fore the period in each sentence as features. High-
weight features for the second word include:
ARA:2<,>; CHI:2<is>; HIN:2<can>.
PsvRatio The proportion of passive verbs out of all
verbs.
CxtFxn Contextual function words. High-weight
features include: CHI:<some JJ>;
HIN:<as VBN>.
Misspell Spelling correction edits. Features
included substitutions, deletions, insertions,
doubling of letters and missing doublings of
letters, and splittings (alot?a lot), as well as the
word position where the error occurred.
High-weight features include: ARA:DEL<e>,
ARA:INS<e>, ARA:SUBST<e>/<i>;
GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,
JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,
SPA:MID_INS<s>, SPA:INS<s>.
Restore Counts of substitutions, deletions and
insertions of predefined tokens that we restored
in the texts. High-weight features include:
CHI:DELWORD<do>; GER:DELWORD<on>;
ITA:DELWORD<be>
Table 4 reports the empirical improvement that each
of these brings independently when added to the
main features (?5.1).
5.3 Discarded Features
We also tried several other feature types that did not
improve the accuracy of the classifier on the devel-
opment set.
CohMark Counts of each cohesive marker.
283
Feature Group # Params Accuracy (%) `2
main + Position 6,153,015 81.00 1.0
main + PsvRatio 5,664,472 81.00 1.0
main 5,664,461 81.09 1.0
main + DocLen 5,664,472 81.09 1.0
main + Pron 5,664,736 81.09 1.0
main + Punct 5,664,604 81.09 1.0
main + Misspell 5,799,860 81.27 5.0
main + Restore 5,682,589 81.36 5.0
main + CxtFxn 7,669,684 81.73 1.0
Table 4: Dev set accuracy with main features plus addi-
tional feature groups, added independently. `2 regulariza-
tion was tuned as in table 3 (two values, 1.0 and 5.0, were
tried for each configuration; more careful tuning might
produce slightly better accuracy). Results are sorted by
accuracy; only three groups exhibited independent im-
provements over the main feature set.
CohVerb Counts of each cohesive verb.
FxnWord Counts of function words. These features
are subsumed by the highly discriminative CxtFxn
features.
6 Results
The full model that we used to classify the test set
combines all features listed in table 4. Using all
these features, the accuracy on the development set
is 84.55%, and on the test set it is 81.5%. The values
for ? and ? were tuned to optimize development set
performance, and found to be ? = 5, ? = 2.
Table 5 lists the confusion matrix on the test set,
as well as precision, recall and F1-score for each L1.
The largest error type involved predicting Telugu
when the true label was Hindi, which happened 18
times. This error is unsurprising since many Hindi
and Telugu speakers are arguably native speakers of
Indian English.
Production of L2 texts, not unlike translating from
L1 to L2, involves a tension between the impos-
ing models of L1 (and the source text), on the one
hand, and a set of cognitive constraints resulting
from the efforts to generate the target text, on the
other. The former is called interference in Trans-
lation Studies (Toury, 1995) and transfer in second
language acquisition (Selinker, 1972). Volansky et
al. (forthcoming) designed 32 classifiers to test the
validity of the forces acting on translated texts, and
found that features sensitive to interference consis-
tently yielded the best performing classifiers. And
indeed, in this work too, we find fingerprints of the
source language are dominant in the makeup of L2
texts. The main difference, however, between texts
translated by professionals and the texts we address
here, is that more often than not professional trans-
lators translate into their mother tongue, whereas L2
writers write out of their mother tongue by defini-
tion. So interference is ever more exaggerated in
this case, for example, also phonologically (Tsur and
Rappoport, 2007).
We explore the effects of interference by analyz-
ing several patterns we observe in the features. Our
classifier finds that the character sequence alot is
overrepresented in Arabic L2 texts. Arabic has no
indefinite article and we speculate that Arabic speak-
ers conceive a lot as a single word; the Arabic equiv-
alent for a lot is used adverbially like an -ly suffix
in English. For the same reason, another promi-
nent feature is a missing definite article before nouns
and adjectives. Additionally, Arabic, being an Ab-
jad language, rarely indicates vowels, and indeed we
find many missing e?s and i?s in the texts of Arabic
speakers. Phonologically, because Arabic conflates
/I/ and /@/ into /i/ (at least in Modern Standard Ara-
bic), we see that many e?s are indeed substituted for
i?s in these texts.
We find that essays that contain hyphens are more
likely to be from German authors. We again find
evidence of interference from the native language
here. First, relative clauses are widely used in Ger-
man, and we see this pattern in L2 English of L1
German speakers. For example, any given rational
being ? let us say Immanual Kant ? we find that.
Another source of extra hyphens stems from com-
pounding convention. So, for example, we find well-
known, community-help, spare-time, football-club,
etc. Many of these reflect an effort to both connect
and separate connected forms in the original (e.g.,
Fussballklub, which in English would be more natu-
rally rendered as football club). Another unexpected
feature of essays by native Germans is a frequent
substitution of the letter y for z and vice versa. We
suspect this owes to their switched positions on Ger-
man keyboards.
Lexical item frequency also provides clues to the
L1 of the essay writers. The word that occurs more
frequently in the texts of German L1 speakers. We
284
true? ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)
ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4
CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2
FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5
GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3
HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9
ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5
JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5
KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6
SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6
TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9
TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4
Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.
hypothesize that in English it is optional in rela-
tive clauses whereas in German it is not, so Ger-
man speakers are less comfortable using the non-
obligatory form. Also, often is over represented. We
hypothesize that since it is cognate of German oft, it
is not cognitively expensive to retrieve it. We find
many times?a literal translation of muchas veces?
in Spanish essays.
Other informative features that reflect L1 features
include frequent misspellings involving confusions
of l and r in Japanese essays. More mysteriously,
the characters r and s are misused in Chinese and
Spanish, respectively. The word then is dominant
in the texts of Hindi speakers. Finally, it is clear
that authors refer to their native cultures (and, conse-
quently, native languages and countries); the strings
Turkish, Korea, and Ita were dominant in the texts of
Turkish, Korean and Italian native speakers, respec-
tively.
7 Discussion
We experimented with different classifiers and a
large set of features to solve an 11-way classifica-
tion problem. We hope that studying this problem
will improve to facilitate human assessment, grad-
ing, and teaching of English as a second language.
While the core features used are sparse and sensitive
to lexical and even orthographic features of the writ-
ing, many of them are linguistically informed and
provide insight into how L1 and L2 interact.
Our point of departure was the analogy between
translated texts as a genre in its own and L2 writ-
ers as pseudo translators, relying heavily on their
mother tongue and transferring their native models
to a second language. In formulating our features,
we assumed that like translators, L2 writers will
write in a simplified manner and overuse explicit
markers. Although this should be studied vis-?-vis
comparable outputs of mother tongue writers in En-
glish, we observe that the best features of our clas-
sifiers are of the ?interference? type, i.e. phonolog-
ical, morphological and syntactic in nature, mostly
in the form of misspelling features, restoration tags,
punctuation and lexical and syntactic modeling.
We would like to stress that certain features indi-
cating a particular L1 have no bearing on the quality
of the English produced. This has been discussed
extensively in Translation Studies (Toury, 1995),
where interference is observed by the overuse or un-
deruse of certain features reflecting the typological
differences between a specific pair of languages, but
which is still within grammatical limits. For exam-
ple, the fact that Italian native speakers favor the
syntactic sequence of determiner + adjective + noun
(e.g., a big risk or this new business) has little pre-
scriptive value for teachers.
A further example of how L2 quality and the
ability to predict L1 are uncorrelated, we noted
that certain L2 writers often repeat words appear-
ing in their essay prompts, and including informa-
tion about whether the writer was reusing prompt
words improved classification accuracy. We suggest
this reflects different educational backgrounds. This
feature says nothing about the quality of the text, just
as the tendency of Korean and Italian writers to men-
tion their home country more often does not.
285
Acknowledgments
This research was supported by a grant from the Is-
raeli Ministry of Science and Technology.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Shoshana Blum-Kulka. 1986. Shifts of cohesion and co-
herence in translation. In Juliane House and Shoshana
Blum-Kulka, editors, Interlingual and intercultural
communication Discourse and cognition in translation
and second language acquisition studies, volume 35,
pages 17?35. Gunter Narr Verlag.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author profil-
ing for English emails. In Proc. of PACLING, pages
263?272, Melbourne, Australia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An author
profiling tool with application to Arabic emails. In
Proc. of the Australasian Language Technology Work-
shop, pages 21?30, Melbourne, Australia, December.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English. Presses universitaires de Louvain,
Louvain-la-Neuve.
Rachel Hayes-Harb and Kyoko Masuda. 2008. Devel-
opment of the ability to lexically encode novel second
language phonemic contrasts. Second Language Re-
search, 24(1):5?33.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Mark D. Kernighan. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
COLING.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of ACL-HLT, pages 1318?
1326, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proc. of KDD, pages 624?628,
Chicago, IL. ACM.
Robert Lado. 1957. Linguistics across cultures: applied
linguistics for language teachers. University of Michi-
gan Press, Ann Arbor, Michigan, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan and
Claypool.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL.
Larry Selinker. 1972. Interlanguage. International
Review of Applied Linguistics in Language Teaching,
10(1?4):209?232.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Procedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Michael Swan and Bernard Smith. 2001. Learner En-
glish: A Teacher?s Guide to Interference and Other
Problems. Cambridge Handbooks for Language
Teachers. Cambridge University Press.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proc. of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, Atlanta, GA, USA, June. Association for Com-
putational Linguistics.
Gideon Toury. 1995. Descriptive Translation Studies
and beyond. John Benjamins, Amsterdam / Philadel-
phia.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180, Edmonton, Canada,
June. Association for Computational Linguistics.
286
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proc. of
the Workshop on Cognitive Aspects of Computational
Language Acquisition, pages 9?16, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Vered Volansky, Noam Ordan, and Shuly Wintner. forth-
coming. On the features of translationese. Literary
and Linguistic Computing.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Proc.
of the Australasian Language Technology Association
Workshop, pages 53?61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. of EMNLP, pages 1600?1610, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
287
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271?280,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Generating English Determiners in Phrase-Based Translation with
Synthetic Translation Options
Yulia Tsvetkov Chris Dyer Lori Levin Archna Bhatia
Language Technologies Institute
Carnegie Mellon University
Pittspurgh, PA, 15213, USA
{ytsvetko, cdyer, lsl, archna}@cs.cmu.edu
Abstract
We propose a technique for improving
the quality of phrase-based translation
systems by creating synthetic translation
options?phrasal translations that are gen-
erated by auxiliary translation and post-
editing processes?to augment the de-
fault phrase inventory learned from par-
allel data. We apply our technique to
the problem of producing English deter-
miners when translating from Russian and
Czech, languages that lack definiteness
morphemes. Our approach augments the
English side of the phrase table using a
classifier to predict where English arti-
cles might plausibly be added or removed,
and then we decode as usual. Doing
so, we obtain significant improvements in
quality relative to a standard phrase-based
baseline and to a to post-editing complete
translations with the classifier.
1 Introduction
Phrase-based translation works as follows. A set
of candidate translations for an input sentence is
created by matching contiguous spans of the in-
put against an inventory of phrasal translations,
reordering them into a target-language appropri-
ate order, and choosing the best one according to a
discriminative model that combines features of the
phrases used, reordering patterns, and target lan-
guage model (Koehn et al, 2003). This relatively
simple approach to translation can be remarkably
effective, and, since its introduction, it has been
the basis for further innovations, including devel-
oping better models for distinguishing the good
translations from bad ones (Chiang, 2012; Gim-
pel and Smith, 2012; Cherry and Foster, 2012;
Eidelman et al, 2013), improving the identifica-
tion of phrase pairs in parallel data (DeNero et al,
2008; DeNero and Klein, 2010), and formal gen-
eralizations to gapped rules and rich nonterminal
types (Chiang, 2007; Galley et al, 2006). This
paper proposes a different mechanism for improv-
ing phrase-based translation: the use of synthetic
translation options to supplement the standard
phrasal inventory used in phrase-based translation
systems.
In the following, we argue that phrase tables ac-
quired in usual way will be expected to have gaps
in their coverage in certain language pairs and
that supplementing these with synthetic translation
options is a priori preferable to alternative tech-
niques, such as post processing, for generalizing
beyond the translation pairs observable in training
data (?2). As a case study, we consider the prob-
lem of producing English definite/indefinite arti-
cles (the, a, and an) when translating from Russian
and Czech, two languages that lack overt definite-
ness morphemes (?3). We develop a classifier that
predicts the presence and absence of English arti-
cles (?4). This classifier is used to generate syn-
thetic translation options that are used to augment
phrase tables used the usual way (?5). We eval-
uate their performance relative to post-processing
approach and to a baseline phrase-based system,
finding that synthetic translation options reliably
outperform the other approaches (?6). We then
discuss how our approach relates to previous work
(?7) and conclude by discussing further applica-
tions of our technique (?8).
2 Why Synthetic Translation Options?
Before turning to the problem of generating En-
glish articles, we give arguments for why syn-
thetic translation options are a useful extension of
271
standard phrase-based translation approaches, and
why this technique might be better than some al-
ternative proposals that been made for generaliz-
ing beyond translation examples directly observ-
able in the training data.
In language pairs that are typologically sim-
ilar (i.e., when both languages lexicalize the
same kinds of semantic and syntactic informa-
tion), words and phrases map relatively directly
from source to target languages, and the standard
approach to learning phrase pairs is quite effec-
tive.1 However, in language pairs in which in-
dividual source language words have many dif-
ferent possible translations (e.g., when the target
language word could have many different inflec-
tions or could be surrounded by different func-
tion words that have no direct correspondence in
the source language), we can expect the standard
phrasal inventory to be incomplete, except when
very large quantities of parallel data are available
or for very frequent words. There simply will not
be enough examples from which to learn the ideal
set of translation options. Therefore, since phrase
based translation can only generate input/output
word pairs that were directly observed in the train-
ing corpus, the decoder?s only hope for produc-
ing a good output is to find a fluent, meaning-
preserving translation using incomplete transla-
tion lexicons. Synthetic translation option genera-
tion seeks to fill these gaps using secondary gener-
ation processes that produce possible phrase trans-
lation alternatives that are not directly extractable
from the training data. We hypothesize that by
filling in gaps in the translation options, discrim-
inative translation models will be more effective
(leading to better translation quality).
The creation of synthetic translation options can
be understood as a kind of translation or post-
editing of phrasal units/translations. This raises
a question: if we have the ability to post-edit a
phrasal translation or retranslate a source phrase
so as to fill in gaps in the phrasal inventory, we
should be able to use the same technique to trans-
late the sentence; why not do this? While the ef-
fectiveness of this approach will ultimately be as-
sessed empirically, translation option generation is
appealing because the translation option synthe-
sizer need not produce only single-best guesses?
1When translating from a language with a richer lexical
inventory to a simpler one, approximate matching or backing
off to (e.g.) morphologically simpler forms likewise reliably
produces good translations.
??????
?
?????
saw +1SG +PST cat+ACC1SG+NOM
I
saw
saw a
saw the
cat
a
the cat
cat
saw the cat
saw a cat
I saw
I saw a
I saw the
Figure 1: Russian-English phrase-based transla-
tion example. Since Russian lacks a definiteness
morpheme the determiners a, the must be part of
a translation option containing ?????? or ?????
in order to be present in the right place in the En-
glish output. Translation options that are in dashed
boxes should exist but were not observed in the
training data. This work seeks to produce such
missing translation options synthetically.
if multiple possibilities appear to be equally good
(say, multiple inflections of a translated lemma),
then multiple translation options may be synthe-
sized. Ultimately, of course, the global translation
model must select one translation for every phrase
it uses, but the decoder will have access to global
information that it can use to pick better transla-
tion options.
3 Case Study: English Definite Articles
We now turn to a translation problem that we will
use to assess the value of synthetic translation op-
tions: generating English in/definite articles when
translating from Russian.
Definiteness is a semantic property of noun
phrases that expresses information such as iden-
tifiability, specificity, familiarity and unique-
ness (Lyons, 1999). In English, it is expressed
through the use of article determiners and non-
article determiners. Although languages may ex-
press definiteness through such morphemes, many
languages use alternative mechanisms. For exam-
ple they may use noncanonical word orders (Mo-
hanan, 1994)2 or different constructions such as
existentials, differential object marking (Aissen,
2003), and the ba (?) construction in Chinese
2See pp. 11?12 for an example in Hindi, a language with-
out articles.
272
(Chen, 2004). While these languages lack arti-
cles, they may use demonstratives and the quan-
tifier one to emphasize definiteness and indefinite-
ness, respectively.
Russian and Czech are examples of languages
that use non-lexical means to express definiteness.
As such, in Russian to English translation systems,
we expect that most Russian nouns should have at
least three translation options?the bare noun, the
noun preceded by the, and the noun preceded a/an.
Fig. 1 illustrates how the definiteness mismatch
between Russian and English can result in ?gaps?
in the phrasal inventory learned from a relatively
large parallel corpus. The Russian input should
translate (depending on context) as either I saw a
cat or I saw the cat; however, the phrase table we
learned is only able to generate the former.3
4 Predicting English Definite Articles
Although English articles express semantic con-
tent, their use is largely predictable in context,
both for native English speakers and for automated
systems (Knight and Chander, 1994).4 In this sec-
tion we describe a classifier that uses local contex-
tual features to predict whether an article belongs
in a particular position in a sequence of words, and
if so, whether it is definite or indefinite (the form
of the indefinite article is deterministic given the
pronunciation of the following word).
4.1 Model
The classifier takes an English word sequence w =
?w1, w2, . . . , w|w|?with missing articles and an in-
dex i and predicts whether no article, a definite ar-
ticle, or an indefinite article should appear before
wi. We parameterize the classifier as a multiclass
3The phrase table for this example was extracted from the
WMT 2013 shared task training data consisting of 1.2M sen-
tence pairs.
4An interesting contribution of this work is a discussion
on lower and upper bounds that can be achieved by native
English speakers in predicting determiners. 67% is a lower
bound, obtained by guessing the for every instance. The up-
per bound was obtained experimentally, and was measured on
noun phrases (NP) without context, in a context of 4 words
(2 before and 2 after NP), and given full context. Human
subjects achieved an accuracy of 94-96% given full context,
83-88% for NPs in a context of 4 words, and 79-80% for NPs
without context. Since in the current state-of-the-art building
an automated determiners prediction in a full context (repre-
senting meaning computationally) is not a feasible task, we
view 83-88% accuracy as our goal, and 88% as an upper
bound for our method.
logistic regression:
p(y | w, i) ? exp?
j
?jhj(y,w, i),
where hj(?) are feature functions, ?j are the corre-
sponding weights, and y ? {D,I,N} refer, respec-
tively, to the outputs: definite article, indefinite ar-
ticle, and no article.5
4.2 Features
The English article system is extremely com-
plex (as non-native English speakers will surely
know!): in addition to a general placement rule
that articles must precede a noun or its modifiers
in an NP, multiple other factors can also affect ar-
ticle selection, including countability of the head
noun, syntactic properties of an adjective modi-
fying a noun (superlative, ordinal), discourse fac-
tors, general knowledge, etc. In this section, we
define morphosyntactic features aimed at reflect-
ing basic grammatical rules, we define statistical,
semantic and shallow lexical features to capture
additional regular and idiosyncratic usages of def-
inite and indefinite articles in English. Below we
provide brief details of the features and their mo-
tivation.
Lexical. Because training data can be con-
structed inexpensively (from any unannotated En-
glish corpus), n-gram indicator features, such as
[[wi?1ywiwi+1 = with y lot of]], can be es-
timated reliably and capture construction-specific
article use.
Morphosyntactic. We used part-of-speech
(POS) tags produced by the Stanford POS tagger
(Toutanova and Manning, 2000) to capture gen-
eral article patterns. These are relevant features
in the prediction of articles as we observe certain
constraints regarding the use of articles in the
neighborhood of certain POS tags. For example,
we do not expect to predict an article following an
adjective (JJ).
Semantic. We extract further information indi-
cating whether a named entity, as identified by the
Stanford NE Recognizer (Finkel et al, 2005) be-
gins at wi. These features are relevant as there
5Realization of the classes D and N as lexical items is
straightforward. To convert I into a or an, we use the
CMU pronouncing dictionary (http://www.speech.
cs.cmu.edu/cgi-bin/cmudict) and select an if wi
starts with a phonetic vowel.
273
is, in general, a constraint on the co-occurrence
of articles with named entities which can help us
predict the use of articles in such constructions.
For example, proper nouns do not tend to co-
occur with articles in English. Although there are
some proper nouns that have an article included in
them, such as the Netherlands, the United States
of America, but these are fixed expressions and the
model is easily able to capture such cases with lex-
ical features.
Statistical. Statistical features capture probabil-
ity of co-occurrences of a sample with each of
the determiner classes, e.g., for wi?1ywi we
collect probabilities of wi?1Iwi, wi?1Dwi, and
wi?1Nwi.6
4.3 Training and evaluation
We employ the creg regression modeling frame-
work to train a ternary logistic regression classi-
fier.7 All features were computed for the target-
side of the Russian-English TED corpus (Cettolo
et al, 2012); from 117,527 sentences we removed
5K sentences used as tuning and test sets in the
MT system. We extract statistical features from
monolingual English corpora released for WMT-
11 (Callison-Burch et al, 2011).
In the training corpus there are 65,075 I in-
stances, 114,571 D instances, and 2,435,287 N in-
stances. To create a balanced training set we
randomly sample 65K instances from each set of
collected instances.8 This training set of feature
vectors has 142,604 features and 285,210 param-
eters. To minimize the number of free parame-
ters in our model we use `1 regularization. We
perform 10-fold cross validation experiments with
various feature combinations, evaluating the clas-
sifier accuracy for all classes and for each class
independently. The performance of the classifier
on individual classes and consolidated results for
all classes are listed in Table 1.
We observe that morphosyntactic and lexical
features are highly significant, reducing the er-
ror rate of statistical features by 25%. A combi-
6Although statistical features are de rigeur in NLP, they
are arguably justified for this problem on linguistic grounds
since human subjects use frequency-based in addition to their
grammatical knowledge. For example, we say He is at school
rather than He is at the school, but Americans say He is in
the hospital while UK English speakers might prefer He is in
hospital.
7https://github.com/redpony/creg
8Preliminary experiments indicated that the excess of N
labels resulted in poor performance.
Feature combination All I D N
Statistical 0.80 0.76 0.79 0.87
Lexical 0.82 0.79 0.80 0.87
Morphosyntactic 0.75 0.71 0.64 0.86
Semantic 0.35 0.99 0.02 0.04
Statistical+Lexical 0.85 0.83 0.82 0.89
+ Morphosyntactic 0.87 0.86 0.83 0.92
+ Semantic 0.87 0.86 0.83 0.92
Table 1: 10-fold cross validation accuracy of the
classifier over all and by class.
nation of morphosyntactic, lexical, and statistical
features is also helpful, reducing 13% more errors.
Semantic features do not contribute to the classi-
fier accuracy (we believe, mainly due to the feature
sparsity).
5 Experimental Setup
Our experimental workflow includes the follow-
ing steps. First, we select a phrase table PTsource
from which we generate synthetic phrases. For
each phrase pair ?f, e? in PTsource we generate n
synthetic variants of the target side phrase e which
we then append to PTbaseline. We annotate both
the original and synthetic phrases with additional
translation features in PTbaseline.
For this language pair, we have several options
for how to construct PTsource. The most straight-
forward way is to extract the phrasal inventory as
usual; a second option is to extract phrases from
training data from which definite articles have
been removed (since we will rely on the classifier
to reinsert them where they belong).
To synthesize phrases, we employ two differ-
ent techniques: LM-based and classifier-based.
We use a LM for one- or two-word phrases or
an auxiliary classifier for longer phrases and cre-
ate a new phrase in which we insert, remove or
substitute an article between each adjacent pair of
words in the original phrase. Such distinction be-
tween short and longer phrases has clear motiva-
tion: phrases without context may allow alterna-
tive, equally plausible options for article selection,
therefore we can just rely on a LM, trained on
large monolingual corpora, to identify phrases un-
observed in MT training corpus. Longer context
restricts determiners usage and statistical model
decisions are less prone to generating ungrammat-
ical synthetic phrases.
LM-based method is applied to phrases shorter
than three words. These phrases are numerous,
roughly 20% of a phrase table, and extracted from
274
many sites in the training data. For each short (tar-
get) phrase we add all possible alternative entries
observed in the LM and not observed in the orig-
inal translation model. For example, for a short
target phrase a cat we extract the cat.
We apply an auxiliary classifier to longer
phrases, containing three or more words. Based
on the classifier prediction, we use the maximally
probable class to insert, remove or substitute an
article between each adjacent pair of words in
the original phrase. Synthetic phrases are gener-
ated by linguistically-informed features and can
introduce alternative grammatically-correct trans-
lations of source phrases by adding or removing
existing articles (since the English article selection
in a local context is often ambiguous and not cat-
egorical). We add a synthetic phrase only if the
phrase pair not observed in the original model.
We compare two possible applications of a clas-
sifier: one-pass and iterative prediction. With
one-pass prediction we decide on the prediction
for each position independently of other deci-
sions. With iterative update we adopt the best
first (greedy) strategy, selecting in each iteration
the update-location in which the classifier obtains
highest confidence score. In each iteration we in-
corporate a prediction in a target phrase, and in the
next iteration the best first decision is made on an
updated phrase. Iterative prediction stops when no
updates are introduced.
Synthetic phrases are added to a phrase table
with the five standard phrasal translation features
that were found in the source phrase, and with sev-
eral new features. First, we add a boolean fea-
ture indicating the origin of a phrase: synthetic or
original. Second, we experiment with a posterior
probability of a classifier averaged over all loca-
tions where it could be extracted from the training
data. The next feature is derived from this score:
it is a boolean feature indicating a confidence of
the classifier: the feature value is 1 iff the average
classifier score is higher than some threshold.
Consider again a phrase I saw a cat discussed
in Section 1. Synthetic entry generation from the
original phrase table entry is illustrated in Fig-
ure 2.
6 Translation Results
We now review the results of experiments using
synthetic translation options in a machine trans-
lation system. We use the Moses toolkit (Koehn
et al, 2007) to train a baseline phrase-based SMT
system. Each configuration we compare has a dif-
ferent phrase table, with synthetic phrases gen-
erated with best-first or iterative strategies, from
a phrase table with- or without-determiners, with
variable number of translation features. To verify
that system improvement is consistent, and is not a
result of optimizer instability (Clark et al, 2011),
we replicate each experimental setup three times,
and then estimate the translation quality of the me-
dian MT system using the MultEval toolkit.9
The corpus is the same as in Section 4.3:
the training part contains 112,527 sentences from
Russian-English TED corpus, randomly sampled
3K sentences are used for tuning and a disjoint set
of 2K sentences is used for test. We lowercase
both sides, and use Stanford CoreNLP10 tools to
tokenize the corpora. We employ SRILM toolkit
(Stolcke, 2002) to linearly interpolate the target
side of the training corpus with the WMT En-
glish corpus, optimizing towards the MT tuning
set. This LM is used in all experiments.
The rest of this section is organized as follows.
First, we compare two approaches to the deter-
miners classifier application. Then, we provide
detailed description of experiments with synthetic
phrases. We evaluate various aspects of synthetic
phrases generation and summarize all the results
in Table 3. In Table 5 we show examples of im-
proved translations.
Classifier application: one-pass vs. iterative.
First, as an intrinsic evaluation of the prediction
strategy we remove definite and indefinite articles
from the reference translations (2K test sentences)
and then employ the determiners classifier to re-
produce the original sentences. In Table 2 we re-
port on the word error rate (WER) derived from
the Levenshtein distance between the original sen-
tences and the sentences (1) without articles, (2)
with articles recovered using one-pass prediction,
and (3) articles recovered using iterative predic-
tion. The WER is averaged over all test sentences.
Both one-pass and iterative approaches are effec-
tive in the task of determiners prediction, reducing
the number of errors by 44%. The iterative ap-
proach yields slightly lower WER, hence we em-
ploy the iterative prediction in the future experi-
ments with synthetic phrases.
9https://github.com/jhclark/multeval
10http://nlp.stanford.edu/software/corenlp.shtml
275
the
? ?????? ????? ||| i saw the cat ||| f0 f1 f2 f3 f4 exp(1) exp(0) |||
<s>      I      saw   a   cat </s>
None
None
? ?????? ????? ||| i saw a cat ||| f0 f1 f2 f3 f4 exp(0) exp(0) |||
original phrase
post-processing
synthetic phrase
is synthetic
is no-context
Figure 2: Synthetic entry generation example. The original parallel phrase has two additional boolean
features (set to false) indicating that this is not a synthetic phrase and not a short phrase. We apply
our determiners classifier to predict an article at each location marked with a dashed box. Based on a
classifier prediction we derive a new phrase I saw the cat. Since corresponding parallel entry is not in
the original phrase table, we set the synthetic indicator feature to 1.
Post-processing WER
None 5.6%
One-pass 3.2%
Iterative 3.1%
Table 2: WER (lower is better) of reference trans-
lations without articles and of post-processed ref-
erence translations. Both one-pass and iterative
approaches are effective in the task of determin-
ers prediction.
MT output post-processing. We then evaluate
the post-processing strategy directly on the MT
output. We experiment with one-pass and itera-
tive post-processing of two variants of the base-
line system outputs: original output and the out-
put without articles (we remove the articles prior
to post-processing). The results are listed in Ta-
ble 3. Interestingly, we do not obtain any improve-
ments applying the determiners classifier in a con-
ventional way of a MT output post-processing. It
is the combination of linguistically-motivated fea-
tures with synthetic phrases that contribute to the
best performance.
LM-based synthetic phrases. As discussed
above, LM-based (short) phrases are shorter than
3 tokens and their synthetic variants contain same
words with articles inserted or deleted between
each adjacent pair of words. The phrase table
of the baseline system contains 2,441,678 phrase
pairs. There are 518,453 original short phrases,
and our technique yields 842,252 new synthetic
entries which we append to the baseline phrase ta-
ble. Table 3 shows the evaluation of the median
SMT system (derived from three systems) with
short phrases. In these systems the five phrasal
translation features are the same as in the base-
line systems. Improvement in the BLEU score
(Papineni et al, 2002) is statistically significant
(p < .05), compared to the baseline system
Classifier-generated synthetic phrases We ap-
ply classifier with the iterative prediction directly
on the baseline phrase table entries and synthe-
size 944,145 new parallel phrases, increasing the
phrase table size by 38%. The phrasal transla-
tion features in each synthetic phrase are the same
as in the phrase it was derived from. The BLEU
score of the median SMT system with synthetic
phrases is 22.9 ? .1, the improvement is statisti-
cally significant (p < .01). Post-processing of a
phrase table created from corpora without articles
and adding synthetic phrases to the baseline phrase
table yielded similar results.
Translation features for synthetic phrases In
the following experiments we aim to establish the
optimal set of translation features that should be
used with synthetic phrases. We train several SMT
systems, each containing synthetic phrases derived
from the original phrase table by iterative classifi-
cation, and with LM-based short phrases. Each
synthetic phrase has five translation features as an
original phrase it was derived from. The additional
features that we evaluate are:
1. Boolean feature for LM-based synthetic
phrases
276
MT System BLEU
Baseline 22.6? .1
MT output post-processing
one-pass, MT output with articles 20.8
one-pass, MT output without articles 19.7
iterative, MT output with articles 22.6
iterative, MT output without articles 21.8
With synthetic phrases
LM-based phrases 22.9? .1
+ classifier-generated phrases 22.9? .1
+ features 1,2 23.0 ? .1
+ features 1,2,3 22.8? .1
+ features 1,2,3,4 22.8? .1
+ feature 5 22.9? .1
Table 3: Summary of experiments with MT out-
put post-processing and with synthetic translation
options in a phrase table. Post-processing of the
MT output do not improve translations. Best per-
forming system with synthetic phrases has five
original phrase translation features and two addi-
tional boolean features indicating if the phrase is
LM-based or not, is classifier-generated or not. All
the synthetic systems are significantly better than
the baseline system.
2. Boolean feature for classifier-generated syn-
thetic phrases
3. Classifier confidence: posterior probability of
the classifier averaged over all samples in a tar-
get phrase.
4. Boolean feature indicating a confidence of the
classifier: the feature value is 1 iff the Fea-
ture 3 scores higher than some threshold. The
threshold was set to 0.8, we did not experiment
with other values.
5. Boolean feature for a synthetic phrase of any
type: LM-based or classifier-generated
Table 3 details the change in the BLEU score
of each experimental setup. The best perform-
ing system has five original phrase translation fea-
tures and two additional boolean features indicat-
ing if the phrase is LM-based or not, is classifier-
generated or not. Note that all the synthetic sys-
tems are significantly better than the baseline.
Czech-English. Our technique was developed
using Russian-English system in the TED domain,
so we want to see how our method generalizes to a
different domain when translating from a different
language. We therefore applied our most success-
ful configuration to a Czech-English news transla-
tion task.11 For training, we use the WMT Czech-
English parallel corpus CzEng0.7; we tune using
the WMT2011 test set and test on the WMT2012
test set. The LM is trained on the target side of the
training corpus. Determiners classifier, re-trained
on the English side of this corpus, with statistical,
lexical, morphosyntactic and dependency features
obtained an accuracy of 88%.
In Table 4, we report the results of evaluat-
ing the performance of the Russian-to-English
and Czech-to-English MT systems with synthetic
phrases. The results of both systems show a statis-
tically significant (p < .01) improvement in terms
of BLEU score.
Russian Czech
Baseline 22.6? .1 16.0? .05
Synthetic 23.0? .1 16.2? .03
Table 4: BLEU score of Russian-to-English
and Czech-to-English MT systems with synthetic
phrases and features 1 and 2 show a significant im-
provement.
Qualitative analysis. Table 5 shows some ex-
amples from the output of our Russian-to-English
systems. Although both systems produce compre-
hensible translations, the system augmented with
determiner classifier is more fluent. The first ex-
ample represents a case where a singular count
noun (piece) is present which requires an article.
The baseline is not able to identify this require-
ment and hence does not insert the article an be-
fore the phrase extraordinary engineering piece.
Our system, however, correctly identifies the con-
struction requiring an article and thus provides an
appropriate form of the article (an- Indefinite arti-
cle for lexical items beginning with a vowel). Thus
we see that our system is able to capture the lin-
guistic requirement of the singular count nouns to
co-occur with an article. In the second row, the
lexical item poor is used as an adjective. The base-
line has inserted an article in front of it, chang-
ing it to a noun. Our system, however, is able to
maintain the status of poor as an adjective since
it has the option not to insert an article. Thus we
see that besides fluency, our system also does bet-
ter in maintaining the grammatical category of a
lexical item. In the third row, the phrase three
11Like Russian, Czech is a Slavic language that does not
have definite or indefinite articles.
277
Source: ?? ??? ?? ????? , ??? ?????????? ???????????? ??????????? ????????? .
Reference: but nonetheless , it ?s an extraordinary piece of engineering .
Baseline: but nevertheless , it ?s extraordinary engineering piece of art .
Ours: but nevertheless , it ?s an extraordinary piece of engineering art .
Source: ? ?? ?????? ?????????? ??? ??? ?? ?????? .
Reference: and by many definitions she is no longer poor .
Baseline: and in a lot definitions , it ?s not a poor .
Ours: and in a lot definitions she ?s not poor .
Source: ??? ????? ????????? ??? ????????? ????????? ??????? .
Reference: we must feed three billion people in cities .
Baseline: we need to feed the three billion urban hundreds of them .
Ours: we need to feed three billion people in the city .
Table 5: Examples of translations with improved articles handling.
billion people refers to a nonidentifiable referent.
The baseline inserts the definite article the. If a
human subject reads this translation, it would mis-
lead him/her to interpret the object three billion
people as referring to a specific identifiable set.
Our system, on the other hand, correctly selects
the determiner class N and hence does not insert an
article. Thus we see that our system does not just
add fluency but it also captures a semantic distinc-
tion, namely identifiability, that a human subject
makes when producing or interpreting a phrase.
7 Related Work
Automated determiner prediction has been found
beneficial in a variety of applications, including
postediting of MT output (Knight and Chander,
1994), text generation (Elhadad, 1993; Minnen
et al, 2000), and more recently identification and
correction of ESL errors (Han et al, 2006; De Fe-
lice and Pulman, 2008; Gamon et al, 2009; Ro-
zovskaya and Roth, 2010). Our work on determin-
ers extends previous studies in several dimensions.
While all previous approaches were tested only on
NP constructions, we evaluate our classifier on any
sequence of tokens.
To the best of our knowledge, the only stud-
ies that directly address generation of synthetic
phrase table entries was conducted by Chen et al
(2011) and Koehn and Hoang (2007). The former
find semantically similar source phrases and pro-
duce ?fabricated? translations by combining these
source phrases with a set of their target phrases;
however, they do not observe improvements. The
later work integrates the synthesis of translation
options into the decoder. While related in spirit,
their method only supports a limited set of gen-
erative processes for producing the candidate set
(lacking, for instance, the simple and effective
phrase post-editing process we have used), and
their implementation has been plagued by compu-
tational challenges.
Post-processing techniques have been ex-
tremely popular. These can be understood as using
a translation model to generate a translation skele-
ton (or k-best skeletons) and then post-editing
these in various ways. These have been applied
to translation into morphologically rich languages,
such as Japanese, German, Turkish, and Finnish
(de Gispert et al, 2005; Suzuki and Toutanova,
2006; Suzuki and Toutanova, 2007; Fraser et al,
2012; Clifton and Sarkar, 2011; Oflazer and Dur-
gar El-Kahlout, 2007).
8 Conclusions and future work
The contribution of this work is twofold. First, we
propose a new supervised method to predict defi-
nite and indefinite articles. Our log-linear model
trained on a linguistically-motivated set of fea-
tures outperforms previously reported results, and
obtains an upper bound of an accuracy achieved
by human subjects given a context of four words.
However, more important result of this work is the
experimentally verified idea of improving phrase-
based SMT via synthetic phrases. While we have
focused on a limited problem in this paper, there
are numerous alternative applications including
translation into morphologically rich languages, as
a method for incorporating (source) contextual in-
formation in making local translation decisions,
enriching the target language lexicon using lexical
translation resources, and many others.
Acknowledgments
We are grateful to Shuly Wintner for insightful sugges-
tions and support. This work was supported in part by the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-1-
0533.
278
References
J. Aissen. 2003. Differential object marking: Iconic-
ity vs. economy. Natural Language and Linguistic
Theory, 21(3):435?483.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland, July. Association for
Computational Linguistics.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
B. Chen, R. Kuhn, and G. Foster. 2011. Semantic
smoothing and fabrication of phrase pairs for SMT.
In Proceedings of the International Workshop on
Spoken Lanuage Translation (IWSLT-2011).
P. Chen. 2004. Identifiability and definiteness in chi-
nese. Linguistics, 42(6):1129?1184.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proceedings of
HLT-NAACL 2012, volume 12, pages 34?35.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. The Jour-
nal of Machine Learning Research, 98888:1159?
1187.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In In Proc. of ACL.
A. Clifton and A. Sarkar. 2011. Combining
morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
ACL.
R. De Felice and S. G. Pulman. 2008. A classifier-
based approach to preposition and determiner error
correction in L2 English. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 169?176. Association
for Computational Linguistics.
A. de Gispert, J. B. Marin?o, and J. M. Crego. 2005.
Improving statistical machine translation by classi-
fying and generalizing inflected verb forms. In Pro-
ceedings of InterSpeech.
J. DeNero and D. Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1453?
1463. Association for Computational Linguistics.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 314?323. Association for Com-
putational Linguistics.
V. Eidelman, Y. Marton, and P. Resnik. 2013. Online
relative margin maximization for statistical machine
translation. In Proceedings of ACL.
M. Elhadad. 1993. Generating argumentative judg-
ment determiners. In AAAI, pages 344?349.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 363?
370, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012.
Modeling inflection and word-formation in SMT. In
Proceedings of EACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In ACL-44: Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 961?968,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2009. Us-
ing contextual speller techniques and language mod-
eling for ESL error correction. Urbana, 51:61801.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies HLT-
NAACL 2012, Montreal, Canada.
N.-R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers.
K. Knight and I. Chander. 1994. Automated poste-
diting of documents. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
779?779, Seattle, WA.
P. Koehn and H. Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
279
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
C. Lyons. 1999. Definiteness. Cambridge University
Press.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Compu-
tational natural language learning-Volume 7, pages
43?48. Association for Computational Linguistics.
T. Mohanan. 1994. Argument Structure in Hindi.
CSLI Publications.
K. Oflazer and I. Durgar El-Kahlout. 2007. Explor-
ing different representational units in English-to-
Turkish statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
A. Rozovskaya and D. Roth. 2010. Training
paradigms for correcting errors in grammar and us-
age. Urbana, 51:61801.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Procedings of International
Conference on Spoken Language Processing, pages
901?904.
H. Suzuki and K. Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 1049?
1056. Association for Computational Linguistics.
H. Suzuki and K. Toutanova. 2007. Generating case
markers in machine translation. In Proceedings of
HLT-NAACL 2007, pages 49?56.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the 2000
Joint SIGDAT conference on Empirical methods in
natural language processing and very large corpora,
pages 63?70, Morristown, NJ, USA. Association for
Computational Linguistics.
280
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
