Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1392?1401, Dublin, Ireland, August 23-29 2014.
An Empirical Evaluation of Automatic Conversion from Constituency to
Dependency in Hungarian
Katalin Ilona Simk
?
o
1
, Veronika Vincze
1,2
, Zsolt Sz
?
ant
?
o
1
, Rich
?
ard Farkas
1
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
kata.simko@gmail.com
{vinczev,szantozs,rfarkas}@inf.u-szeged.hu
Abstract
In this paper, we investigate the differences between Hungarian sentence parses based on auto-
matically converted and manually annotated dependency trees. We also train constituency parsers
on the manually annotated constituency treebank and then convert their output to dependency
trees. We argue for the importance of training on gold standard corpora, and we also demon-
strate that although the results obtained by training on the constituency treebank and converting
the output to dependency format and those obtained by training on the automatically converted
dependency treebank are similar in terms of accuracy scores, the typical errors made by different
systems differ from each other.
1 Introduction
Nowadays, two popular approaches to data-driven syntactic parsing are based on constituency grammar
on the one hand and dependency grammar on the other hand. There exist constituency-based treebanks
for many languages and dependency treebanks for most of these languages are converted automatically
from constituent trees with the help of conversion rules, which is the case for e.g. the languages used in
the SPMRL-2013 Shared Task (Seddah et al., 2013) with the exception of Basque, where constituency
trees are converted from manually annotated dependency trees (Aduriz et al., 2003), and Hungarian,
where both treebanks are manually annotated (Csendes et al., 2005; Vincze et al., 2010). However, the
quality of automatic dependency conversion is hardly investigated.
Hungarian is one of those rare examples where there exist manual annotations for both constituency
and dependency syntax on the same bunch of texts, the Szeged (Dependency) Treebank (Csendes et al.,
2005; Vincze et al., 2010), which makes it possible to evaluate the quality of a rule-based automatic con-
version from constituency to dependency trees, to compare the two sets of manual annotations and also
the output of constituency and dependency parsers trained on converted and gold standard dependency
trees.
We investigate the effect of automatic conversions related to the two parsing paradigms as well. It is
well known that for English, the automatic conversion of a constituency parser?s output to dependency
format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output
trained on automatically converted trees
1
(cf. Petrov et al. (2010)). One of the possible explanations for
this is that English is a configurational language, hence constituency parsers have advantages over depen-
dency parsers here. We check whether this hypothesis holds for Hungarian too, which is the prototype
of free word order languages.
In this paper, we compare three pairs of dependency analyses in order to evaluate the usefulness
of converted trees. First, we examine the errors of the conversion itself by comparing the converted
dependency trees with the manually annotated gold standard ones. Second, we argue for the importance
of training parsers on gold standard trees by looking at the typical differences between the outputs of
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
However, it has been pointed out that errors in the conversion script may significantly influence the results of parsing, see
e.g. Petrov and McDonald (2012) and Pitler (2012)
1392
dependency parsers trained on converted (silver standard) trees, parsers trained on gold standard trees and
the manual annotation itself. Third, we demonstrate that similar to English, training on a constituency
treebank and converting the results to dependency format can achieve similar results in terms of ULA to
the dependency parser trained on the automatically converted treebank, but the typical errors they make
differ in both cases.
2 Parsing Hungarian on the Szeged Treebank
Hungarian is a morphologically rich language, where word order encodes information structure, which
makes its syntactic analysis very different from English?s as the arguments in a sentence cannot be
determined by their position but by their suffixes, cf.
?
E. Kiss (2002). Words? grammatical functions
are signified by case suffixes and verbs are marked for the number and person of their subject and the
definiteness of their object, thus these arguments may be often omitted from the sentence: L?atlak (see-
1SG2OBJ) ?I see you?. Due to word order reasons, words that form one syntactic phrase may not be
adjacent (long-distance dependencies), which is true for the possessive construction as well: the posses-
sor and the possessed may be situated in two distant positions: A fi?unak elvette a kalapj?at (the boy-DAT
take-PAST-3SGOBJ the hat-POSS3SG-ACC) ?He took the boy?s hat?. Verbless clauses are also com-
mon in Hungarian, as the copula in third person singular present tense indicative form is phonologically
empty, while it is present in all other moods and tenses: A kalap piros (the hat red) ?The hat is red?, but
A kalap piros volt (the hat red was) ?The hat was red?.
The Szeged Treebank (Csendes et al., 2005) is a manually annotated constituency treebank for Hun-
garian consisting of 82,000 sentences. Besides the phrase structure, grammatical roles of the verbs?
arguments and morphological information are also annotated. It incorporates texts from six different
domains: short business news, newspaper, law, literature, compositions and informatics, however, in this
paper, we just focus on the short business news domain.
The Szeged Dependency Treebank (Vincze et al., 2010) contains manual dependency syntax annota-
tions for the same texts. Certain linguistic phenomena ? such as discontinuous structures ? are annotated
in this treebank, but not in the constituency treebank. In the dependency treebank, the possessor is linked
to the possession while this connection is not annotated in the constituency treebank. The two types of
trees can be seen in Figure 1.
CP
PUNC
.
NP-ACC
N
kalapj?at
T
a
V
V0
V
elvette
NP-GEN
N
fi?unak
T
A
A fi?unak elvette a kalapj?at .
ROOT
DET
GEN
DET
OBJ
PUNCT
Figure 1: Discontinuous structure A fi?unak elvette a kalapj?at (the boy-DAT take-past3SGOBJ the hat-
POSS3SG-ACC) ?He took the boy?s hat? in constituency and dependency analysis.
Another difference between the two treebanks is the way they represent different types of complex
sentences, as can be seen in Figure 2. In the dependency treebank subordinations and coordinations are
1393
handled very similarly. The head of one of the clauses (the subordinated clause or the second clause in
the case of coordination) is linked to the head of the other clause (the matrix clause of the subordination
or the first clause of the coordination), only the type of relation between the two heads differs in the
two structures, in the dependency tree in Figure 2, the heads of the three clauses (?atj?ott ?came over?,
meg??g?erte ?promised? and elj?on ?come?) are linked to one another through their conjunctions with either
an ATT relation in the case of subordination or COORD for coordination. In the constituency treebank
these sentences are represented very differently: in the case of subordination, the subordinated clause is
within the matrix clause: CP
3
is within CP
2
in the constituency tree in Figure 2. Coordinated clauses
appear at the same level in the structure, in the same figure CP
1
and CP
2
are coordinated clauses.
CP
PUNC
.
CP
2
CP
3
elj?on velem
C0
hogy
PUNC
,
V
meg??g?erte
C0
?es
CP
1
?
Atj?ott hozz?am
?
Atj?ott hozz?am ?es meg??g?erte , hogy elj?on velem .
ROOT
OBL
CONJ
COORD
PUNCT
CONJ
ATT
OBL
PUNCT
Figure 2: Constituency and dependency analysis of coordination and subordination in the sentence
?
Atj?ott
hozz?am ?es meg??g?erte, hogy elj?on velem (through.come-PAST-3SG to.me and promise-PAST-3SG-OBJ
that away.come-3SG with.me) ?He came over and promised that he will come with me?.
The parallels of these two manually annotated treebanks make them suitable for testing our hypotheses
about automatic dependency conversion. The differences between them originate from the characteristics
of constituent and dependency syntax.
3 Converting Constituency Trees to Dependency Trees
In this section, we present our methods to convert constituency trees to dependency trees and we also
discuss the most typical sources of errors during conversion.
3.1 Conversion rules
In order to convert constituency trees to dependency trees, we used a rule based system. Sentences with
virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their
treatment in dependency trees is also problematic (Farkas et al., 2012; Seeker et al., 2012). As a result,
we worked with 7,372 sentences and 162,960 tokens.
First, we determined the head of each clause (CP) and the relations between CPs in complex sentences.
In most cases the head of the CP is a finite verb, if the CP contains no finite verb, the head is the either an
infinitive verb or a participle, if none of these are present in the CP, the head can be a nominal expression.
The relations between the CP heads make up the base of the dependency structure using ROOT relation
for the sentence?s main verb, COORD for coordination and ATT for subordination, as well as CONJ in
the case of conjunctions between the CPs.
1394
The arguments of verbs, infinitives and participles in the CP were linked to their governor and marked
for their grammatical role in the Szeged Treebank. We used this information to construct the appropriate
dependency relations between governors and their arguments. The main grammatical roles such as sub-
ject, object, dative have their own label in dependency syntax, while minor ones are assigned the oblique
(OBL) relation. The argument?s modifiers were then linked to the head or other modifiers based on the
phrase structure with relations according to their morphological code.
Long distance dependencies, like the connection between a genitive case possessor and the possessed
are not annotated in the constituency treebank. In these cases we used morphological information to link
these elements together in the dependency tree. Figure 3 shows an example of converting a constituency
tree to a dependency tree.
CP
PUNC
.
V
V0
V
volt
NEG
R
nem
NP
N
?uzletk?ot?es
NP
N
h?uspiacon
T
A
A h?uspiacon ?uzletk?ot?es nem volt .
ROOT
DET
OBL
SUBJ
NEG
PUNCT
Figure 3: Conversion of the sentence A h?uspiacon ?uzletk?ot?es nem volt (the meat.market-SUP transaction
not was) ?There were no transactions at the meat market.? from constituency to dependency trees.
3.2 Error Analysis
We automatically converted the constituency treebank into dependency trees following the prin-
ciples described above and detailed at our website (http://www.inf.u-szeged.hu/rgai/
SzegedTreebank). For evaluation, we applied the metrics labeled attachment score (LAS) and un-
labeled attachment score (ULA), without punctuation marks. The accuracy of the conversion was 96.51
(ULA) and 93.85 (LAS). The errors made during conversion were categorized manually in 200 sentences
selected randomly from the short business news subcorpus of the Szeged Dependency Treebank, and the
most typical ones are listed in Table 1, Column convError.
As it is shown, the most common source of error was when more than one modifier was within a
phrase as the example in Figure 4 shows. In each figure, the gold standard parse can be seen on the left
hand side while the erroneous one can be seen on the right hand side.
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
ATT
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
COORD
Figure 4: Multiple modifier error in eur?opai, olcs?o utakat k??n?al?o l?egit?arsas?ag (European cheap trips-
ACC offering airline) ?European airline offering cheap trips?.
1395
Error type convError goldTrain silverTrain BerkeleyConv convDep
# % # % # % # % # %
Coordination 26 13.00 39 13.22 59 14.82 55 16.37 64 19.57
Multiple modifiers 26 13.00 30 10.17 49 12.31 52 15.48 47 14.37
Determiner 7 3.50 28 9.49 25 6.28 31 9.23 31 9.48
Conj./adverb attached 33 16.50 23 7.80 45 11.31 39 11.61 42 12.84
Arg. of verbal element 10 5.00 27 9.15 34 8.54 59 17.56 44 13.46
Sub- vs. coordination 7 3.50 9 3.05 12 3.02 ? ? ? ?
Possessor 9 4.50 14 4.75 16 4.02 28 8.33 22 6.73
Wrong root 14 7.00 17 5.76 23 5.78 35 10.42 27 8.26
Consecutive nouns 4 2.00 11 3.73 14 3.52 13 3.87 15 4.59
Multiword NE 8 4.00 25 8.47 33 8.29 8 2.38 19 5.81
Wrong MOD label 25 12.50 26 8.81 34 8.54 ? ? ? ?
Wrong other label 17 8.50 33 11.19 30 7.54 ? ? ? ?
Other errors 14 7.00 13 4.41 24 6.03 16 4.76 16 4.89
Total 200 100 295 100 398 100 336 100 327 100
Table 1: Error Types. convError: errors made during converting constituency trees to dependency trees.
goldTrain: errors in the output got by training the Bohnet parser on the gold standard data. silverTrain:
errors in the output got by training the Bohnet parser on the silver standard data. BerkeleyConv: errors in
the output got by training the Berkeley parser on the gold standard constituency data and converting the
output into dependency format. convDep: errors in the output got by training the Bohnet parser without
dependency labels on the silver standard data.
Coordination errors occurred when multiple members of a coordination were wrongly connected. On
the other hand, the attachment of conjunctions and some adverbs was also problematic, for example in
Figure 5 the conjunction is ?also? is connected to the verb in the gold standard and to the noun in the
converted version.
a miniszt?erium is besz?all
DET
SUBJ
CONJ
a miniszt?erium is besz?all
DET
SUBJ
CONJ
Figure 5: Conjunction attachment error in a miniszt?erium is besz?all (the ministry also steps.in) ?the
ministry also steps in?.
Also, the constituency treebank did not mark all the grammatical relations (e.g. numerals and deter-
miners were simply parts of an NP but had no distinct labeling, like [NP az ?ot [ADJP fekete] kutya]
(the five black dog) ?the five black dogs?), but it was necessary to assign them a dependency label and
a parent node during conversion. However, in some cases it was not straightforward which modifier
modifies which parent node: for instance, in [NP nem [ADJP megfelel?o] m?odszerek] (not appropriate
methods) ?inappropriate methods?, the negation word nem is erroneously attached to the noun instead of
the adjective in the converted phrase. Determiner errors were those where the determiner was attached
to the wrong noun in a NP with a noun modifier. In CPs with multiple verbal elements (both a finite verb
and an infinitive or a participle in the CP) the arguments were sometimes linked to the wrong verb, as in
Figure 6.
1396
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
Figure 6: Verbal argument error in a saj?at pecseny?ej?ukkel voltak elfoglalva (the own roast-3PLPOSS-INS
were busy) ?they were busy with their own thing?.
Possessors are sometimes wrongly identified during conversion as long distance dependencies are not
marked in the constituency treebank (see Figure 7).
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
SUBJ
OBL
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
ATT
OBL
Figure 7: Possessor attachment error in a gy?art?o sz?ar??t?o?uzem?eben hasznos??t (the manufacturer
drying.plant-3SGPOSS-INE utilizes) ?the manufacturer utilizes it in its drying plant?.
In CPs with more verbal element, sometimes the wrong word is selected as the root, as in Figure 8.
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
ATT
ATT
SUBJ
ATT
OBJ
PREVERB
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
COORD
ATT
SUBJ
ATT
OBJ
PREVERB
Figure 8: Root error in a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be (the
tender-SUB applied other bidder invalid application-ACC submit-PAST-3SG) ?the other bidder applying
to the tender submitted an invalid application?.
In some cases, consecutive (but separate) noun phrases were taken as one unit as if one noun modified
the other, for example in Figure 9.
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
Figure 9: Consecutive noun error in a tervezettn?el t?obb munkahelyet sz?untet meg (the planned-ADE more
workplace-ACC terminates) ?it terminates more workplaces than planned?.
Multiword NEs also caused some problems in the conversion, as in Figure 10.
1397
Besz?all??t?oi Befektet?o Rt.
NE
NE
Besz?all??t?oi Befektet?o Rt.
ATT
NE
Figure 10: Multiword NE error in Besz?all??t?oi Befektet?o Rt. (a name of a company) .
In other cases, divergences between the gold standard and the converted trees are due to some erro-
neous annotations either in the constituency treebank or in the dependency treebank. A typical example
of this is the wrong MOD (modifier) label. In the treebank, locative and temporal modifiers were classi-
fied according to the tridirectionality typical of Hungarian adverbs and case suffixes: where, from where
and to where (or when, from what time and till what time) the action is taken place. Thus, there are
six dependency relations dedicated to these aspects and all the other adverbials are grouped under the
relation MOD. However, this distinction is rather semantic in nature and was sometimes erroneously
annotated in the constituency treebank, which was later corrected in the dependency one and thus now
resulted in conversion errors, as shown in Figure 11.
ny?ar v?ege fel?e kezdik
ATT
ATT
MODE
ny?ar v?ege fel?e kezdik
ATT
ATT
TO
Figure 11: MOD label error in ny?ar v?ege fel?e kezdik (summer end-3SGPOSS around begin) ?they begin
around the end of the summer?.
There were also some atypical errors that occurred too rarely to categorize them in a different class,
like cases when an article or determiner got erroneously attached to a verb and so on, so they were
lumped into the category of ?other errors? in Table 1.
4 Training on Gold Standard and Silver Standard Trees
We also experimented with training the Bohnet dependency parser (Bohnet, 2010) on the manually an-
notated (gold standard) and the converted (silver standard) treebank. The Bohnet parser (Bohnet, 2010)
is a state-of-the-art
2
graph-based parser, which employs online training with a perceptron. The parser
contains a feature function for the first order factor, one for the sibling factor, and one for the grandchil-
dren.
From the corpus, 5,892 sentences (130,211 tokens) were used in the training dataset and the remaining
1,480 sentences (32,749 tokens) in the test dataset. For evaluation, we again applied the metrics LAS
and ULA. Results are shown in Table 2, Rows goldTrain and silverTrain.
As the numbers show, better results can be achieved when the gold standard data are used as training
database than when the parser is trained on the silver standard data, the differences being 1.6% (ULA)
and 3.16% (LAS). Besides evaluation scores, we also compared the outputs of the two scenarios: we
used the same set of randomly selected sentences as when investigating conversion errors and carried out
a manual error analysis against the gold standard data in each case: see Table 1, Columns goldTrain and
silverTrain.
There are some common error types that seem to cause problems for both ways of parsing. For
instance, coordination and multiple modifiers are among the most frequent sources of errors in both
cases as for the error rates are concerned. However, with regard to the absolute numbers, we can see
that both error types are reduced when the gold standard dataset is used for training. On the other hand,
finding the parent node of a conjunction or an adverb seems to improve significantly when the parser is
trained on gold standard data. This is probably due to the fact that they are not marked in the constituency
treebank and thus training data for these grammatical phenomena are very noisy in the silver standard
treebank. All in all, we argue that there are some grammatical phenomena ? e.g. the attachment of
2
For a comparative evaluation with other dependency parsers on the same treebank see Farkas et al. (2012). According to
their results, the Bohnet parser achieved the best scores on the treebank hence we also used this parser in our experiments.
1398
Setting LAS ULA
Conversion 93.85 96.51
goldTrain 93.48 95.17
silverTrain 90.32 93.57
BerkeleyConv ? 92.78
convDep ? 93.23
Table 2: Results of the experiments. Conversion: converting constituency trees to dependency trees.
goldTrain: training the Bohnet parser on the gold standard data. silverTrain: training the Bohnet parser
on the silver standard data. BerkeleyConv: training the Berkeley parser on the gold standard constituency
data and converting the output into dependency format. convDep: training the Bohnet parser without
dependency labels on the silver standard data.
conjunctions or adverbs ? that require manual checking even if automatic conversion from constituency
to dependency is applied.
5 Pre- or Post Conversion?
It is well known that for English, converting a constituency parser?s output to dependency format (post
conversion) can achieve competitive ULA scores to a dependency parser?s output trained on automati-
cally converted trees (pre conversion) (Petrov et al., 2010; Farkas and Bohnet, 2012). One of the pos-
sible reasons for this may be that English is a configurational language, hence constituency parsers are
expected to perform better here. In this paper, we investigate whether this is true for Hungarian, which
is the prototype of morphologically rich languages with free word order.
We employed the product-of-grammars procedure (Petrov, 2010) of the Berkeleyparser (Petrov et al.,
2006), where grammars are trained on the same dataset but with different initialization setups, which
leads to different grammars. We trained 8 grammars and used tree-level inference. The output of the
parser was then automatically converted to dependency format, based on the rules described in Section
3 (BerkeleyConv). Second, we used the silver standard dependency treebank for training the Bohnet
parser (convDep). Since our constituency parser did not produce grammatical functions for the nodes,
we trained the Bohnet parser on unlabeled dependency trees in order to ensure a fair comparison here
(that is the difference between the columns BerkeleyConv and convDep in Table 1).
As the numbers show, competitive results can be obtained with both methods, yielding an ULA score
of 92.78 and 93.23, respectively. This means that the same holds for Hungarian as for English and the
surprisingly good results of post conversion are not related to the configurational level of the language.
Manually analysing the errors on the same set of sentences as before, there are again some error cate-
gories that occur frequently in both cases such as coordination, the attachment of conjunctions, modifiers
and determiners. On the other hand, training on constituency trees seems to have some specific sources
of errors. First, the possessor in possessive constructions is less frequently attached to its possessed,
which may be due to the fact that the genitive possessor is not linked to the possessed in the constituency
treebank and thus the parser is not able to learn this relationship. Second, arguments of verbal elements
(i.e. verbs, participles and infinitives) are also somewhat more difficult to find when there are at least two
verbal elements within the clause, which is especially true for adverbial participles and infinitives. In
Figure 6, the differences between the two trees are shown. The noun pecseny?ej?ukkel (roast-3PLPOSS-
INS) ?with their thing? is linked to the adverbial participle in the correct analysis, but it connects to the
main verb in the other. Third, identifying the root node of the sentence may also be problematic for this
setting. As Farkas and Bohnet (2012) reported that preconversion can achieve better results for finding
the root node in English, this seems to be a language-specific issue and it represents an interesting differ-
ence between English and Hungarian. Nevertheless, training on constituency trees has a beneficial effect
on finding multiword named entities. Hence, it can be concluded that although the evaluation scores are
similar, the errors the two systems make differ from each other.
1399
6 Discussion and Conclusions
Here, we compared dependency analyses of Hungarian obtained in different ways. It was revealed that
although the accuracy scores are similar to each other, each system makes different types of errors. On
the other hand, there are some specific linguistic phenomena that seem to be difficult for dependency
parsing generally as they were among the most frequent sources of errors in each case (e.g. coordination,
multiple modifiers and the attachment of conjunctions and adverbs).
Converting constituency trees into dependency trees enabled us to experiment with a silver standard
dependency corpus as well. Our results empirically showed that better results can be achieved on the
gold standard corpus, hence manual annotation of dependency trees is desirable. However, when there
is no access to manually annotated dependency data, converting the output of a constituency parser into
dependency format or training the dependency parser on converted data may also be viable: similar to
English, both solutions result in competitive scores but the errors the systems make differ from each
other.
In the future, we would like to investigate how the advantages of constituency and dependency repre-
sentations may be further exploited in parsing Hungarian and we also plan to carry out some uptraining
experiments with both types of parsers.
Acknowledgements
This work was supported in part by the European Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013).
References
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola, Aitziber Atutxa, A. Diaz de Ilarraza, Aitzpea Garmendia,
and Maite Oronoz. 2003. Construction of a Basque dependency treebank. In Proceedings of the 2nd Workshop
on Treebanks and Linguistic Theories (TLT), pages 201?204, V?axj?o, Sweden.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd
International Conference on Computational Linguistics (Coling 2010), pages 89?97.
D?ora Csendes, J?anos Csirik, Tibor Gyim?othy, and Andr?as Kocsor. 2005. The Szeged TreeBank. In V?aclav
Matousek, Pavel Mautner, and Tom?as Pavelka, editors, Proceedings of the 8th International Conference on
Text, Speech and Dialogue, TSD 2005, Lecture Notes in Computer Science, pages 123?132, Berlin / Heidelberg,
September. Springer.
Katalin
?
E. Kiss. 2002. The Syntax of Hungarian. Cambridge University Press, Cambridge.
Rich?ard Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Rich?ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency Parsing of Hungarian: Baseline Re-
sults and Challenges. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 55?65, Avignon, France, April. Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Linguistics, pages 433?440.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 705?713, Cambridge, MA, October. Association for Computational Linguistics.
Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
1400
Emily Pitler. 2012. Conjunction representation and ease of domain adaptation. Notes of the First Workshop on
Syntactic Analysis of Non-Canonical Language (SANCL).
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie Candito, Jinho D. Choi, Rich?ard Farkas, Jennifer Foster,
Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Yuval Marton, Joakim Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Woli?nski, and Alina Wr?oblewska. 2013. Overview of the SPMRL 2013
shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the
Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182, Seattle, Washing-
ton, USA, October. Association for Computational Linguistics.
Wolfgang Seeker, Rich?ard Farkas, Bernd Bohnet, Helmut Schmid, and Jonas Kuhn. 2012. Data-driven depen-
dency parsing with empty heads. In Proceedings of COLING 2012: Posters, pages 1081?1090, Mumbai, India,
December. The COLING 2012 Organizing Committee.
Veronika Vincze, D?ora Szauter, Attila Alm?asi, Gy?orgy M?ora, Zolt?an Alexin, and J?anos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings of LREC 2010, Valletta, Malta, May. ELRA.
1401
LAW VIII - The 8th Linguistic Annotation Workshop, pages 64?69,
Dublin, Ireland, August 23-24 2014.
Annotating Uncertainty in Hungarian Webtext
Veronika Vincze
1,2
, Katalin Ilona Simk
?
o
1
, Viktor Varga
1
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
{kata.simko,viktor.varga.1991}@gmail.com
Abstract
Uncertainty detection has been a popular topic in natural language processing, which manifested
in the creation of several corpora for English. Here we show how the annotation guidelines origi-
nally developed for English standard texts can be adapted to Hungarian webtext. We annotated a
small corpus of Facebook posts for uncertainty phenomena and we illustrate the main character-
istics of such texts, with special regard to uncertainty annotation. Our results may be exploited
in adapting the guidelines to other languages or domains and later on, in the construction of
automatic uncertainty detectors.
1 Background
Detecting uncertainty in natural language texts has received a considerable amount of attention in the
last decade (Farkas et al., 2010; Morante and Sporleder, 2012). Several manually annotated corpora have
been created, which serve as training and test databases of state-of-the-art uncertainty detectors based
on supervised machine learning techniques. Most of these corpora are constructed for English, however,
their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008;
Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner
et al., 2009), pieces of news (Saur?? and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin,
2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze,
2013), reviews (Konstantinova et al., 2012; Cruz D??az, 2013) and tweets (Wei et al., 2013) have been
annotated for uncertainty, just to mention a few examples.
The diversity of the resources also manifests in the fact that the annotation principles behind the cor-
pora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three
corpora (BioScope, FactBank and WikiWeasel) and they offered a unified classification of semantic
uncertainty phenomena, on the basis of which these corpora were reannotated, using uniform guide-
lines. Some other uncertainty-related linguistic phenomena are described as discourse-level uncertainty
in Vincze (2013). As a first objective of our paper, we will carry out a pilot study and investigate how
these unified guidelines can be adapted to texts written in a language that is typologically different from
English, namely, Hungarian.
As a second goal, we will also focus on annotating texts in a new domain: social media texts ?
apart from Wei et al. (2013) ? have not been extensively investigated from the uncertainty detection
perspective. As the use and communication through the internet is becoming more and more important
in people?s lives, the huge amount of data available from this domain is a valuable source of information
for computation linguistics. However, processing texts from the web ? especially social media texts from
blogs, status updates, chat logs and comments ? revealed that they are very challenging for applications
trained on standard texts. Most studies in this area focus on English, for instance, sentiment analysis
from tweets has been the focus of recent challenges (Wilson et al., 2013) and Facebook posts have
been analysed from the perspective of computational psychology (Celli et al., 2013). A syntactically
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
64
annotated treebank of webtext has been also created for English (Bies et al., 2012). However, methods
developed for processing English webtext require serious alterations to be applicable to other languages,
for example Hungarian, which is very different from English syntactically and morphologically. Thus,
in our pilot study we will annotate Hungarian webtext for uncertainty and examine the possible effects
of the domain and the language on uncertainty detection.
In the following, we will present the uncertainty categories that were annotated in Hungarian webtext
and we will illustrate the difficulties of both annotating Hungarian webtext and annotating uncertainty
phenomena in them.
2 Uncertainty Categories
Here we just briefly summarize uncertainty categories that we applied in the annotation, based on Szarvas
et al. (2012) and Vincze (2013).
Linguistic uncertainty is related to modality and the semantics of the sentence. For instance, the
sentence It may be raining does not contain enough information to determine whether it is really raining
(semantic uncertainty). There are several phenomena that are categorized as semantic uncertainty. A
proposition is epistemically uncertain if its truth value cannot be determined on the basis of world
knowledge. Conditionals and investigations also belong to this group ? the latter is characteristic of
research papers, where research questions usually express this type of uncertainty. Non-epistemic types
of modality are also be listed here such as doxastic uncertainty, which is related to beliefs.
However, there are other linguistic phenomena that only become uncertain within the context of com-
munication. For instance, the sentence Many people think that Dublin is the best city in the world does
not reveal who exactly think that, hence the source of the proposition about Dublin remains uncertain.
This is a type of discourse-level uncertainty, more specifically, it is called weasel (Ganter and Strube,
2009). On the other hand, hedges make the meaning of words fuzzy: they blur the exact meaning of some
quality/quantity. Finally, peacock cues express unprovable evaluations, qualifications, understatements
and exaggerations.
The above categories proved to be applicable to Hungarian texts as well. However, the morpholog-
ically rich nature of Hungarian required some slight changes in the annotation process. For instance,
modal auxiliaries like may correspond to a derivational suffix in Hungarian, which required that in the
case of j?ohet ?may come? the whole word was annotated as uncertain, not just the suffix -het.
3 Annotating Hungarian Webtext
Annotating uncertainty in webtexts comes with the usual difficulties of working with this domain. We
annotated Hungarian posts and comments from Facebook, which made the uncertainty annotation more
challenging than on standard texts. Texts were randomly selected from the public posts available at the
Facebook-sites of some well-known brands (like mobile companies, electronic devices, nutrition expert
companies etc.) and from the comments that users made on these posts. For our pilot annotation, we
used 1373 sentences and 18,327 tokens (as provided by magyarlanc, a linguistic preprocessing toolkit
developed for standard Hungarian texts (Zsibrita et al., 2013)).
One fundamental property of social media texts is their similarity to oral communication despite their
written form. The communication is online and multimodal; its speed causing a number of possibilities
for error. The quick typing makes typos, abbreviations and lack of capitalization, punctuation and accen-
tuated letters more common in these texts. Accentuated and unaccentuated vowels represent different
sounds in Hungarian that can change the meaning of words (kerek ?round?, ker?ek ?wheel? and k?erek ?I
want?). Other types of linguistic creativity are also common, such as the use of emoticons and English
words and abbreviations in Hungarian texts. However, these attributes do not characterize social media
texts homogeneously. For instance, blog posts are closer to standard texts since they are usually written
by a PR expert from the side of the brand, who presumably spends more time with elaborating on the
text of the posts than an average user. On the other hand, comments and chat texts are closer to oral
communication because users here want to react as quickly as possible, making them harder to analyze.
65
Our corpus of Facebook posts and comments exhibited a number of these properties. It contained
a lot of typos, abbreviations and letters that should have been accentuated. These sometimes caused
interpretation problems even for the human annotators; especially as these posts and comments were
annotated without broader context. Lack of capitalization and punctuation was more common in the
comment section of the corpus than in the posts. Emoticons were also frequent in both parts of the
corpus.
Example 1: Typos in our corpus.
ugya ilynem van csak fekete el?ol ?es sz?urke h?at ?ul ? original
ugyanilyenem van csak fekete el
?
ol ?es sz?urke h
?
atul ? standardized
(same.kind-POSS1SG have but black front and grey back)
?I have the same kind but its front is black and its back is grey.?
Example 2: Abbreviation in our corpus.
Am?ugy meg sztem Nektek nem kellene a Saj?at oldalatokon magyar?azkodni! ? original
Am?ugy meg szerintem Nektek nem kellene a saj?at oldalatokon magyar?azkodni! ? standard-
ized
(by.the.way PART according.to-POSS1SG you-DAT not should the own site-POSS3PL-SUP
explain.yourselves-INF)
?By the way I think you should not be explaining yourselves on your own site.?
Example 3: Lack of accentuation in our corpus.
es Marai Sandornak is ma van a szuletesnapja. ? original
?es M?arai S?andornak is ma van a sz?ulet?esnapja. ? standardized
(and M?arai S?andor-GEN also today has the birthday-POSS3SG)
?And today is also M?arai S?andor?s birthday?
4 Uncertainty in Hungarian Webtext
Apart from the above mentioned usual problems when dealing with webtext, other difficulties emerged
during their uncertainty annotation. Uncertainty is often related to opinions, but writers of these texts do
not usually express these as opinions, but as factual elements. Linguistic uncertainty is not annotated in
these cases, as these sentences do not hold uncertain meanings semantically, even if certain facts in them
are clearly not true or at least the writers obviously lack evidence to back them up.
Example 4: Information without evidence in our corpus.
?
Uj megfigyel?es, hogy az elektronok ?ugy viselkednek, mint az antioxid?ansok.
(new observation that the electrons that.way behave as the antioxidants)
?It is a new observation that electrons behave as antioxidants.?
The uncertainty annotation of this text differed greatly from our corpus of Hungarian Wikipedia arti-
cles and news (Vincze, 2014), which domains are much closer to standard language use. Table 1 shows
the distribution of the different types of uncertainty cues in these domains. Comparing this new subcorpus
with the other two shows certain domain specific characteristics. Unlike Facebook posts and comments,
the other two domains should not contain subjective opinions according to the objective nature of news
media and encyclopedias. This is consistent with the difference in the proportion of peacock cues in each
subcorpus: Facebook posts abound in them but their number is low in the other types of texts.
The relatively small number of hedges and epistemic uncertainty may be attributed to the previously
mentioned observation that the writers of these posts and comments often make confident statements,
even if these are not actual facts.
66
The resemblance of Facebook posts and comments to oral communication also means that elements
that could also signify uncertainty can have different uses in this context. Certain phrases may indi-
cate politeness or other pragmatic functions that in a different domain would mean and be annotated as
linguistic uncertainty.
Example 5: The use of uncertain elements for politeness reasons in our corpus.
sajnos ?ugy t?unik a fut?araink valami?ert val?oban nem ?erkeztek meg hozz?atok szombaton
(unfortunately that.way seems the carriers-POSS1PL something-CAU really not arrive-PAST-
3PL you-ALL Saturday-SUP)
?Unfortunately it seems like our carriers did not get to you on Saturday for some reason.?
The phrase ?ugy t?unik ?it seems? can express uncertainty in some contexts, but in the above example,
it is used as a marker of politeness, in order to apologize for and mitigate the inconvenience they caused
to their customers by not delivering some package in time.
Uncertainty cue Wikipedia News Webtext
# % # % # %
Weasel 1801 32.02 258 10.93 50 9.72
Hedge 2098 37.3 799 33.86 147 28.59
Peacock 787 14 94 3.98 192 37.35
Discourse-level total 4686 83.3 1151 48.77 389 75.6
Epistemic 439 7.8 358 15.16 21 4.08
Doxastic 315 5.6 710 30.08 44 8.56
Conditional 154 2.74 128 5.42 59 11.47
Investigation 31 0.55 13 0.55 1 0.19
Semantic total 939 16.69 1209 51.22 125 24.3
Total 5625 100 2360 100 514 100
Table 1: Uncertainty cues.
5 Conclusions
In this paper, we focused on annotating Hungarian Facebook posts and comments for uncertainty phe-
nomena. We adapted guidelines proposed for uncertainty annotation of standard English texts to Hun-
garian, and we also showed that this domain exhibit certain characteristics which are not present in other
domains that are more similar to standard language use. First, users usually express their opinions as
facts, thus relatively less markers of hedges or epistemic uncertainty occur in the corpus. Second, uncer-
tainty cue candidates can fulfill politeness functions, and apparently they do not signal uncertainty in
these contexts. Third, the characteristics of webtext may cause difficulties in annotation since in some
cases, the meaning of the text is vague due to typos or other errors.
Our pilot study of annotating Hungarian webtext for uncertainty leads us to conclude that the annota-
tion guidelines are mostly applicable to Hungarian as well and webtexts also exhibit the same uncertainty
categories as more standard texts, although the distribution of uncertainty categories differ among differ-
ent types of text. Besides, politeness factors should get more attention in this domain. Our results may be
employed in adapting annotation guidelines of uncertainty to other languages or domains as well. Later
on, we would like to extend our corpus and we would like to implement machine learning methods to
automatically detect uncertainty in Hungarian webtext, for which these findings will be most probably
fruitfully exploited.
Acknowledgements
This research was funded in part by the European Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013). Veronika Vincze was par-
67
tially funded by the National Excellence Program T
?
AMOP-4.2.4.A/2-11/1-2012-0001 of the State of
Hungary, co-financed by the European Social Fund.
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. Linguistic Data Consortium,
Philadelphia.
Fabio Celli, Fabio Pianesi, David Stilwell, and Michal Kosinski. 2013. Extracting evaluative conditions from
online reviews: Toward enhancing opinion mining. In Workshop on Computational Personality Recognition,
Boston, July.
Noa P. Cruz D??az. 2013. Detecting negated and uncertain information in biomedical and review texts. In Proceed-
ings of the Student Research Workshop associated with RANLP 2013, pages 45?50, Hissar, Bulgaria, September.
RANLP 2013 Organising Committee.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the
Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1?
12, Uppsala, Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia
Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 173?176, Suntec, Singapore, August. Association for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008. Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(Suppl 10).
Natalia Konstantinova, Sheila C.M. de Sousa, Noa P. Cruz, Manuel J. Mana, Maite Taboada, and Ruslan Mitkov.
2012. A review corpus annotated for negation, speculation and their scope. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Ben Medlock and Ted Briscoe. 2007. Weakly Supervised Learning for Hedge Classification in Scientific Litera-
ture. In Proceedings of the ACL, pages 992?999, Prague, Czech Republic, June.
Roser Morante and Caroline Sporleder. 2012. Modality and negation: An introduction to the special issue.
Computational Linguistics, 38:223?260, June.
Raheel Nawaz, Paul Thompson, and Sophia Ananiadou. 2010. Evaluating a meta-knowledge annotation scheme
for bio-events. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,
pages 69?77, Uppsala, Sweden, July. University of Antwerp.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando. 2005. Certainty identification in texts: Categorization
model and manual tagging results. In J.G. Shanahan, J. Qu, and J. Wiebe, editors, Computing attitude and affect
in text: Theory and applications (the information retrieval series), New York. Springer Verlag.
Victoria L. Rubin. 2010. Epistemic modality: From uncertainty to certainty in the context of information seeking
as interactions with texts. Information Processing & Management, 46(5):533?540.
Roser Saur?? and James Pustejovsky. 2009. FactBank: a corpus annotated with event factuality. Language
Resources and Evaluation, 43:227?268.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings
of the NIPS Workshop on Cost-Sensitive Learning, pages 1?10.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and W. John Wilbur. 2008. Multi-dimensional classification of
biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics,
24(18):2086?2093.
Gy?orgy Szarvas, Veronika Vincze, Rich?ard Farkas, Gy?orgy M?ora, and Iryna Gurevych. 2012. Cross-genre and
cross-domain detection of semantic uncertainty. Computational Linguistics, 38:335?367, June.
?
Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda. 2009. Machine Learning and Rule-based Approaches to
Assertion Classification. Journal of the American Medical Informatics Association, 16(1):109?115, January.
68
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas, Gy?orgy M?ora, and J?anos Csirik. 2008. The BioScope Corpus:
Biomedical Texts Annotated for Uncertainty, Negation and their Scopes. BMC Bioinformatics, 9(Suppl 11):S9.
Veronika Vincze. 2013. Weasels, Hedges and Peacocks: Discourse-level Uncertainty in Wikipedia Articles.
In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 383?391,
Nagoya, Japan, October. Asian Federation of Natural Language Processing.
Veronika Vincze. 2014. Uncertainty detection in Hungarian texts. In Proceedings of Coling 2014.
Zhongyu Wei, Junwen Chen, Wei Gao, Binyang Li, Lanjun Zhou, Yulan He, and Kam-Fai Wong. 2013. An empir-
ical study on uncertainty identification in social media context. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pages 58?62, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ?13, June.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh, Pittsburgh.
J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas. 2013. magyarlanc: A Toolkit for Morphological and Depen-
dency Parsing of Hungarian. In Proceedings of RANLP-2013, pages 763?771, Hissar, Bulgaria.
69
