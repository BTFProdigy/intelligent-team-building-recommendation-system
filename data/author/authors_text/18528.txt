Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1133?1143, Dublin, Ireland, August 23-29 2014.
A Structured Language Model for Incremental Tree-to-String Translation
Heng Yu1
1Institute of Computing Technology. CAS
University of Chinese Academy of Sciences
yuheng@ict.ac.cn
Haitao Mi
T.J. Watson Research Center
IBM
hmi@us.ibm.com
Liang Huang
Queens College & Grad. Center
City University of New York
huang@cs.qc.cuny.edu
Qun Liu1,2
2Centre for Next Generation Localisation.
Faculty of Engineering and Computing
Dublin City University
qliu@computing.dcu.ie
Abstract
Tree-to-string systems have gained significant popularity thanks to their simplicity and efficien-
cy by exploring the source syntax information, but they lack in the target syntax to guarantee
the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate
a structured language model, a left-to-right shift-reduce parser in specific, into an incremental
tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integra-
tion. Large-scale experiments on various Chinese-English test sets show that with a reasonable
speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
1 Introduction
Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained
significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et
al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster
by proposing an incremental tree-to-string model, which generates the target translation exactly in a left-
to-right manner. Although, tree-to-string models have made those progresses, they can not utilize the
target syntax information to guarantee the grammaticality of the output, as they only generate strings on
the target side.
One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree
models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches
still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al.,
2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010).
Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Char-
niak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved
better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is inde-
pendent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the
problems that tree-to-tree models have. However, integration is not easy, as the following two questions
arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures.
Second, hypotheses in the same bin may not be comparable, since their syntactic structures may not be
comparable, and the future costs are hard to estimate. Hassan et al. (2009) skip those problems by only
keeping the best parsing structure for each hypothesis.
In this paper, we integrate a shift-reduce parser into an incremental tree-to-string model, and intro-
duce an efficient grouping and pruning method to handle the growing search space and incomparable
hypotheses problems. Large-scale experiments on various Chinese-English test sets show that with a rea-
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1133
sonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
2 Linear-time Shift-reduce Parsing
parsing
action signature dependency structure
s1 s0 q0
Bush S 0
sh Bush held S 1: Bush
sh Bush held a S 2: Bush held
re
x
held
Bush
a S 3: Bush held
sh held
Bush
a meeting S 4: Bush held a
sh a meeting with S 5: Bush held a meeting
re
x
held
Bush
meeting
a
with S 6: Bush held a meeting
re
y
held
Bush meeting
with S 7: Bush held a meeting
sh held
Bush meeting
with Sharon S 8: Bush held a meeting with
sh with Sharon S 9: Bush held a meeting with Sharon
re
y
held
Bush meeting
with
Sharon
S 10: Bush held a meeting with Sharon
re
y
held
Bush meeting with
S 11: Bush held a meeting with Sharon
Figure 1: Linear-time left-to-right dependency parsing.
A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step,
chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re)
the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing
scenario, the reduce action is further divided into two cases: left-reduce (re
x
) and right-reduce (re
y
),
depending on which one of the two items becomes the head after reduction. Each parsing derivation can
be represented by a sequence of parsing actions.
1134
2.1 Shift-reduce Dependency Parsing
We will use the following sentence as the running example:
Bush held a meeting with Sharon
Given an input sentence e, where ei is the ith token, ei...e j is the substring of e from i to j, a shift-reduce
parser searches for a dependency tree with a sequence of shift-reduce moves (see Figure 1). Starting
from an initial structure S 0, we first shift (sh) a word e1, ?Bush?, onto the parsing stack s0, and form a
structure S 1 with a singleton tree. Then e2, ?held?, is shifted, and there are two or more structures in the
parsing stack, we can use re
x
or re
y
step to combine the top two trees on the stack, replace them with
dependency structure e1 x e0 or e1 y e0 (shown as S 3), and add one more dependency edge between
e0 and e1.
Note that the shade nodes are exposed heads on which re
x
or re
y
parsing actions can be performed.
The middle columns in Figure 1 are the parsing signatures: q0 (parsing queue), s0 and s1 (parsing stack),
where s0 and s1 only have one level dependency. Take the line of S 11 for example, ?a? is not in the
signature. As each action results in an update of cost, we can pick the best one (or few, with beam) after
each action. Costs are accumulated in each step by extracting contextual features from the structure and
the action. As the sentence gets longer, the number of partial structures generated at each steps grows
exponentially, which makes it impossible to search all of the hypothesis. In practice, we usually use beam
search instead.
(a) atomic features
s0.w s0.t
s1.w s1.t
s0.lc.t s0.rc.t
q0.w q0.t
(b) feature templates
unigram
s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
bigram
s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.w ? s1.w ? s1.t s0.t ? s1.w ? s1.t
s0.w ? s0.t ? s1.w
trigram s0.t ? s1.t ? q0.t s1.t ? s0.t ? s0.lc.t
s1.t ? s0.t ? q0.t s1.t ? s0.t ? s0.rc.t
(c) ?? parsing stack parsing queue ??
. . . s1 s0
s0.lc ? ? ? s0.rc
q0
Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from
Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree,
x.lc and x.rc denote x?s leftmost and rightmost child respectively. (c) the feature window.
2.2 Features
We view features as ?abstractions? or (partial) observations of the current structure. Feature templates f
are functions that draw information from the feature window, consisting of current partial tree and first
word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic
1135
IP
NP
Bu`sh??
VP
PP
P
yu?
NP
Sha?lo?ng
VP
VV
ju?x??ng
AS
le
NP
hu?`ta?n
Figure 2: A parse tree
features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way
classification based on f, and conjoin these feature instances with each action:
[f ? (action=sh/re
x
/re
y
)]
We extract all the feature templates from training data, and use the average perceptron algorithm and
early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model.
3 Incremental Tree-to-string Translation with Slm
The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps:
parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and
the linear incremental decoder then searches for the best derivation that generates a target-language string
in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the
following section.
3.1 Decoding with Slm
Since the incremental tree-to-string model generates translation in strictly left-to-right fashion, and the
shift-reduce dependency parser also processes an input sentence in left-to-right order, it is intuitive to
combine them together. The last two columns in Figure 3 show the dependency structures for the corre-
sponding hypotheses. Start at the root translation stack with a dot  before the root node IP:
[ IP ],
we first predict (pr) with rule r1,
(r1) IP (x1:NP x2:VP)? x1 x2,
and push its English-side to the translation stack, with variables replaced by matched tree nodes, here
x1 for NP and x2 for VP. Since this translation action does not generate any translation string, we don?t
perform any dependency parsing actions. So we have the following translation stack
[ IP ][ NP VP],
where the dot  indicates the next symbol to process in the English word-order. Since node NP is the next
symbol, we then predict with rule r2,
(r2) NP(Bu`sh??)? Bush,
and add it to the translation stack:
[ IP ] [ NP VP ] [ Bush]
Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current
translation, which results in the new translation stack
[ IP ] [ NP VP ] [Bush  ]
1136
translation parsing
stack string dependency structure Slm
[  IP ] S 0
1 pr [  IP ] [  NP VP] S 0
2 pr [  IP ] [ NP VP ] [  Bush ] S 0
3 sc [  IP ] [ NP VP] [Bush  ] Bush S 1: Bush P(Bush | S 0)
4 co [  IP ] [NP  VP] S 1:
5 pr [  IP ] [NP  VP] [ held NP with NP] S 1:
6 sc [  IP ] [NP  VP] [held  NP with NP] held S 3: Bush held P(held | S 1)
7 pr [ IP] [NP VP] [held NP with NP] [ a meeting] S 3
8 sc [ IP] [NP VP] [held  NP with NP] [a meeting  ] a meeting S 7: Bush held a meeting P(a meeting | S 3)
9 co [ IP ] [NP VP] [held NP  with NP] S 7
10 sc [ IP] [NP VP] [held NP with  NP] with S 8: Bush held a meeting with P(with | S 7)
S ?8: Bush held a meeting with P
? (with | S 7)
11 pr [ IP] [NP VP] [held NP with  NP] [ Sharon] S 8
S 8?
12 sc [ IP ] [NP  VP] [held NP with  NP] [Sharon ] Sharon S 11: Bush held a meeting with Sharon P(Sharon | S 8)
S ?11? : Bush held a meeting with Sharon P
? (Sharon | S ?8)
13 co [  IP ] [NP  VP] [held NP with NP ] S 11
14 co [  IP ] [NP VP ] S 11
15 co [ IP  ] S 11
Figure 3: Simulation of the integraton of an Slm into an incremental tree-to-string decoding. The first
column is the line number. The second column shows the translation actions: predict (pr), scan (sc), and
complete (co). S i denotes a dependency parsing structure. The shaded nodes are exposed roots of S i.
Immediately after each sc translation action, our shift-reduce parser is triggered. Here, our parser applies
the parsing action sh, and shift ?Bush? into a partial dependency structure S 1 as a root ?Bush? (shaded
node) in Figure 3. Now the top rule on the translation stack has finished (dot is at the end), so we complete
(co) it, pop the top rule and advance the dot in the second-to-top rule, denoting that NP is completed:
[ IP ] [NP  VP].
Following this procedure, we have a dependency structure S 3 after we scan (sc) the word ?held? and
take a shift (sh) and a left reduce (re
x
) parsing actions. The shaded node ?held? means exposed roots,
that the shift-reduce parser takes actions on.
Following Huang and Mi (2010), the hypotheses with same translation step1 fall into the same bin.
Thus, only the prediction (pr) actions actually make a jump from a bin to another. Here line 2 to 4 fall
into one bin (translation step = 4, as there are 4 nodes, IP, NP, VP and Bu`sh??, in the source tree are
covered). Similarly, lines from 7 to 10 fall into another bin (translation step = 15).
1The step number is defined by the number of tree nodes covered in the source tree, and it is not equal to the number of
translation actions taken so far.
1137
Noted that as we number the bins by the translation step, only pr actions make progress, the sc and
co actions are treated as ?closure? operators in practice. Thus we always do as many sc/co actions as
possible immediately after a pr step until the symbol after the dot is another non-terminal. The total
number of bins is equal to the size of the parse tree, and each hypothesis has a constant number of
outgoing hyper-edges to predict, so the time complexity is linear in the sentence length.
After adding our Slm to this translation, an interesting branch occurs after we scan the word ?with?,
we have two different partial dependency structures S 8 and S
?
8 for the same translation. If we denote
N(S i) as the number of re actions that S i takes, N(S 8) is 3, while N(S ?8) is 4. Here N(S i) does not take
into account the number of sh parsing actions, since all partial structures with same translations should
shift the same number of translations. Thus, N(S i) determines the score of dependency structures, and
only the hypotheses with same N(S i) are comparable to each other. In this case, we should distinguish
S 8 with S
?
8, and if we make a prediction over the hypothesis of S 8, we can reach the correct parsing state
S 11 (shown in the red dashed line in Figure 3).
So the key problem of our integration is that, after each translation step, we will apply different se-
quences of parsing actions, which result in different and incomparable dependency structures with the
same translation. In the following two Sections, we introduce three ways for this integration.
3.2 Na??ve: Adding Parsing Signatures into Translation Signatures
One straightforward approach is to add the parsing signatures (in Figure 1) of each dependency structure
(in Figure 1 and Figure 3) to translation signatures. Here, we only take into account of the s0 and s1 in
the parsing stack, as the q0 is the future word that is not available in translation strings. For example, the
dependency structure S 8 has parsing signatures:
held
Bush meeting
with
We add those information to its translation signature, and only the hypothesis that have same translation
and parsing signatures can be recombined.
So, in each translation bin, different dependency structures with same translation strings are treated as
different hypothesis, and all the hypothesis are sorted and ranked in the same way. For example, S 8 and
S ?8 are compared in the bin, and we only keep top b (the beam size) hypothesis for each bin.
Obviously, this simple approach suffers from the incomparable problem for those hypothesis that have
different number of parsing actions (e.g. S 8 and S ?8). Moreover, it may result in very low translation
variance in each beam.
3.3 Best-parse: Keeping the Best Dependency Structure for Each Translation
Following Hassan et al. (2009), we only keep the best parsing tree for each translation. That means after
a consecutive translation sc actions, our shift-reduce parser applies all the possible parsing actions, and
generates a set of new partial dependency structures. Then we only choose the best one with the highest
Slm score, and only use this dependency structure for future predictions.
For example, for the translation in line 10 in Figure 3, we only keep S 8, if the parsing score of S 8 is
higher than S ?8, although they are not comparable. Another complicate example is shown in Figure 4,
within the translation step 15, there are many alternatives with different parsing structures for the same
translation (?a meeting with?) in the third column, but we can only choose the top one in the final.
3.4 Grouping: Regrouping Hypothesis by N(S ) in Each Bin
In order to do comparable sorting and pruning, our basic idea is to regroup those hypotheses in a same
bin into small groups by N(S ). For each translation, we first apply all the possible parsing actions,
and generate all dependency structures. Then we regroup all the hypothesis with different dependency
structures based on the size of N(S ).
1138
Bush held al Bush held a meetingl i
sh
Bush held al
re
Bush held a meetingl i
Bush held a meetingl i
re
sh
Bush held a meetingl i
re
Bush held a meeting withl i i
sh
Bush held a meeting withl i i
sh
sh
Bush held a meeting withl i i
Bush held a meeting withl i i
sh
re
re
Bush held a meeting withl i i
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
Bush held a meeting with Sharonl i i
re
sh
Bush held a meeting with Sharonl i i
sh
......
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
re
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i ish
......
Bush held a meeting with Sharonl i i
re
Step 15 Step 16
G1: N(S)=1
......
Bush held a meeting withl i i
G2: N(S)=2
G3: N(S)=3
G4: N(S)=4
Figure 4: Multi-beam structures of two bins with different translation steps (15 and 16). The first three
columns show the parsing movements in bin 15. Each dashed box is a group based on the number of
reduce actions over the new translation strings (?a meeting with? for bin 15, and ?Sharon? for bin 16).
G2 means two reduce actions have been applied. After this regrouping, we perform the pruning in two
phases: 1) keep top b states in each group, and labeled each group with the state with the highest parsing
score in this group; 2) sort the different groups, and keep top g groups.
For example, Figure 4 shows two bins with two different translation steps (15 and 16). In bin 15, the
graph shows the parsing movements after we scan three new words (?a?, ?meeting?, and ?with?). The
parsing sh action happens from a parsing state in one column to another state in the next column, while
re happens from a state to another state in the same column. The third column in bin 15 lists some partial
dependency structures that have all new words parsed. Here each dashed box is a group of hypothesis
with a same N(S ), e.g. the G2 contains all the dependency structures that have two reduce actions after
parsed all the new words. Then, we sort and prune each group by the beam size b, and each group labeled
as the highest hypothesis in this group. Finally, we sort those groups and only keep top g groups for the
future predictions. Again, in Figure 4, we can keep the whole group G3 and partial group of G2 if b = 2.
In our experiments, we set the group size g to 5.
3.5 Log-linear Model
We integrate our dependency parser into the log-linear model as an additional feature. So the decoder
searches for the best translation e? with a latent tree structure (evaluated by our Slm) according to the
following equation:
e? = argmax
e?E
exp(Slm(e) ? ws +
?
i
fi ? wi) (1)
where Slm(e) is the dependency parsing score calculated by our parser, ws is the weight of Slm(e), fi are
the features in the baseline model and wi are the weights.
1139
4 Experiments
4.1 Data Preparation
The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respec-
tively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08
(newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement
option ?grow-diag-and? (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley
parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string
translation rules. Our trigram word language model was trained on the target side of the training corpus
using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we
again parse the input sentences using the Berkeley parser, and convert them into translation forests using
rule pattern-matching (Mi et al., 2008).
Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the
same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate
training (Och, 2003) to maximize the Bleu score on the development set.
Our dependency parser is an implementation of the ?arc-standard? shift-reduce parser (Nivre, 2004),
and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training
set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and
Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We
add the structured language model as an additional feature into the baseline system.
We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the scrip-
t mteval-v13a.pl. We also report the Ter scores.
4.2 Complete Comparisons on MT08
To explore the soundness of our approach, we carry out some experiments in Table 2. With a beam size
100, the baseline decoder achieves a Bleu score of 21.06 with a speed of 1.7 seconds per sentence.
Since our dependency parser is trained on the English PTB, which is not included in the MT training
set, there is a chance that the gain of Bleu score is due to the increase of new n-grams in the PTB data.
In order to rule out this possibility, we use the tool SRILM to train another tri-gram language model on
English PTB and use it as a secondary language model for the decoder. The Bleu score is 21.10, which
is similar to the baseline result. Thus we can conclude that any gain of the following +Slm experiments
is not because of the using of the additional English PTB.
Our second experiment re-ranks the 100-best translations of the baseline with our structured language
model trained on PTB. The improvement is less than 0.2 Bleu, which is not statistically significant, as
the search space for re-ranking is relatively small compared with the decoding space.
As shown in Section 3, we have three different ways to integrate an Slm to the baseline system:
? na??ve: adding the parsing signature to the translation signature;
? best-parse: keeping the best dependency structure for each translation;
? grouping: regrouping the hypothesis by N(S ) in each bin.
The na??ve approach achieves a Bleu score of 19.12, which is significantly lower than the baseline. The
main reason is that adding parsing signatures leads to very restricted translation variance in each beam.
We also tried to increase the beam size to 1000, but we do not see any improvement.
The fourth line in Table 2 shows the result of the best-parse (Hassan et al., 2009). This approach only
slows the speed by a factor of two, but the improvement is not statistically significant. We manually
looked into some dependency trees this approach generates, and found this approach always introduce
local parsing errors.
The last line shows our efficient beam grouping scheme with a grouping size 5, it achieves a significant
improvement with an acceptable speed, which is about 6 times slower than the baseline system.
1140
System Bleu Speed
baseline 21.06 1.7
+Slm
re-ranking 21.23 1.73
na??ve 19.12 2.6
best-parse 21.30 3.4
grouping (g=5) 21.64 10.6
Table 2: Results on MT08. The bold score is significantly better than the baseline result at level p < 0.05.
System MT03 MT04 MT05 MT08 Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
baseline 19.94 10.73 22.03 18.63 19.92 11.45 21.06 10.37 12.80
+Slm 21.49 9.44 22.33 18.38 20.51 10.71 21.64 9.88 12.10
Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p < 0.5).
4.3 Final Results on All Test Sets
Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points
in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p < 0.05, using
bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times
slower than the baseline.
5 Related Work
The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways.
First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to
a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can
only tune their system on short sentences less than 20 words. Furthermore, their results are from a much
bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the
larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-the-
art tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions
beyond their work.
Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant
improvement. Different from their work, we pay more attention to the dependency parser, and we also
test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they
are suffering from the local parsing errors.
Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005)
to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental
parser remains in quadratic time.
Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008)
and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and
tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in
translation rules. Both papers integrate dependency structures into translation model, we instead model
the dependency structures with a monolingual parsing model over translation strings.
6 Conclusion
In this paper, we presented an efficient algorithm to integrate a structured language model (an incremen-
tal shift-reduce parser in specific) into an incremental tree-to-string system. We calculate the structured
language model scores incrementally at the decoding step, rather than re-scoring a complete transla-
tion. Our experiments suggest that it is important to design efficient pruning strategies, which have been
1141
overlooked in previous work. Experimental results on large-scale data set show that our approach signif-
icantly improves the translation quality at a reasonable slower speed than a state-of-the-art tree-to-string
system.
The structured language model introduced in our work only takes into account the target string, and
ignores the reordering information in the source side. Thus, our future work seeks to incorporate more
source side syntax information to guide the parsing of the target side, and tune a structured language
model for both Bleu and paring accuracy. Another potential work lies in the more efficient searching and
pruning algorithms for integration.
Acknowledgments
We thank the three anonymous reviewers for helpful suggestions, and Dan Gildea and Licheng Fang for
discussions. Yu and Liu were supported in part by CAS Action Plan for the Development of Western
China (No. KGZD-EW-501) and a grant from Huawei Noah?s Ark Lab, Hong Kong. Liu was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin C-
ity University. Huang was supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty Research
Award, and a PSC-CUNY Award, and Mi by DARPA HR0011-12-C-0015. The views and findings in
this paper are those of the authors and are not endorsed by the US or Chinese governments.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. Parsing of series in automatic computation. In The Theory of Parsing,
Translation, and Compiling, page Volume I.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine
translation. In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. volume 14, pages 283 ? 332.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of
ACL.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of ACL 2009 and AFNLP, pages 773?781, Suntec, Singapore, August.
Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In Proceed-
ings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-
ACL, pages 961?968.
Hany Hassan, Khalil Sima?an, and Andy Way. 2009. A syntactified direct translation model with linear-time de-
coding. In Proceedings of EMNLP 2009, pages 1182?1191, Singapore, August. Association for Computational
Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of ACL 2010, pages 1077?1086, Uppsala, Sweden, July. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings
of NAACL 2012, Montreal, Quebec.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of NAACL, pages 127?133.
1142
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP,
pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of COLING-ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of ACL/IJCNLP, pages 558?566, Suntec, Singapore, August.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523?530, Vancouver, British Columbia,
Canada, October.
Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of ACL, pages
1433?1442, Uppsala, Sweden, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL: HLT, pages
192?199.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational Linguis-
tics.
Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411.
Matt Post and Daniel Gildea. 2008. Language modeling with tree substitution grammars. In Proceedings of
AMTA.
Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In Proceedings of NIPS
workshop on Grammar Induction, Representation of Language, and Language Learning.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Arbor, MI, June.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of ACL 2011, pages 620?631, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
1143
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112?1123,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Violation Perceptron and Forced Decoding for Scalable MT Training
Heng Yu1?
1Institute of Computing Tech.
Chinese Academy of Sciences
yuheng@ict.ac.cn
Liang Huang2? Haitao Mi3
2Queens College & Grad. Center
City University of New York
{huang@cs.qc,kzhao@gc}.cuny.edu
Kai Zhao2
3T.J. Watson Research Center
IBM
hmi@us.ibm.com
Abstract
While large-scale discriminative training has
triumphed in many NLP problems, its defi-
nite success on machine translation has been
largely elusive. Most recent efforts along this
line are not scalable (training on the small
dev set with features from top ?100 most fre-
quent words) and overly complicated. We in-
stead present a very simple yet theoretically
motivated approach by extending the recent
framework of ?violation-fixing perceptron?,
using forced decoding to compute the target
derivations. Extensive phrase-based transla-
tion experiments on both Chinese-to-English
and Spanish-to-English tasks show substantial
gains in BLEU by up to +2.3/+2.0 on dev/test
over MERT, thanks to 20M+ sparse features.
This is the first successful effort of large-scale
online discriminative training for MT.
1 Introduction
Large-scale discriminative training has witnessed
great success in many NLP problems such as pars-
ing (McDonald et al, 2005) and tagging (Collins,
2002), but not yet for machine translation (MT) de-
spite numerous recent efforts. Due to scalability is-
sues, most of these recent methods can only train
on a small dev set of about a thousand sentences
rather than on the full training set, and only with
2,000?10,000 rather ?dense-like? features (either
unlexicalized or only considering highest-frequency
words), as in MIRA (Watanabe et al, 2007; Chiang
et al, 2008; Chiang, 2012), PRO (Hopkins and May,
2011), and RAMP (Gimpel and Smith, 2012). How-
ever, it is well-known that the most important fea-
tures for NLP are lexicalized, most of which can not
?Work done while visiting City University of New York.
?Corresponding author.
be seen on a small dataset. Furthermore, these meth-
ods often involve complicated loss functions and
intricate choices of the ?target? derivations to up-
date towards or against (e.g. k-best/forest oracles, or
hope/fear derivations), and are thus hard to replicate.
As a result, the classical method of MERT (Och,
2003) remains the default training algorithm for MT
even though it can only tune a handful of dense fea-
tures. See also Section 6 for other related work.
As a notable exception, Liang et al (2006) do
train a structured perceptron model on the train-
ing data with sparse features, but fail to outperform
MERT. We argue this is because structured percep-
tron, like many structured learning algorithms such
as CRF and MIRA, assumes exact search, and search
errors inevitably break theoretical properties such as
convergence (Huang et al, 2012). Empirically, it
is now well accepted that standard perceptron per-
forms poorly when search error is severe (Collins
and Roark, 2004; Zhang et al, 2013).
To address the search error problem we propose a
very simple approach based on the recent framework
of ?violation-fixing perceptron? (Huang et al, 2012)
which is designed specifically for inexact search,
with a theoretical convergence guarantee and excel-
lent empirical performance on beam search pars-
ing and tagging. The basic idea is to update when
search error happens, rather than at the end of the
search. To adapt it to MT, we extend this framework
to handle latent variables corresponding to the hid-
den derivations. We update towards ?gold-standard?
derivations computed by forced decoding so that
each derivation leads to the exact reference transla-
tion. Forced decoding is also used as a way of data
selection, since those reachable sentence pairs are
generally more literal and of higher quality, which
the training should focus on. When the reachable
subset is small for some language pairs, we augment
1112
it by including reachable prefix-pairs when the full
sentence pair is not.
We make the following contributions:
1. Our work is the first successful effort to scale
online structured learning to a large portion of
the training data (as opposed to the dev set).
2. Our work is the first to use a principled learning
method customized for inexact search which
updates on partial derivations rather than full
ones in order to fix search errors. We adapt it
to MT using latent variables for derivations.
3. Contrary to the common wisdom, we show that
simply updating towards the exact reference
translation is helpful, which is much simpler
than k-best/forest oracles or loss-augmented
(e.g. hope/fear) derivations, avoiding sentence-
level BLEU scores or other loss functions.
4. We present a convincing analysis that it is the
search errors and standard perceptron?s inabil-
ity to deal with them that prevent previous
work, esp. Liang et al (2006), from succeed-
ing.
5. Scaling to the training data enables us to engi-
neer a very rich feature set of sparse, lexical-
ized, and non-local features, and we propose
various ways to alleviate overfitting.
For simplicity and efficiency reasons, in this paper
we use phrase-based translation, but our method has
the potential to be applicable to other translation
paradigms. Extensive experiments on both Chinese-
to-English and Spanish-to-English tasks show statis-
tically significant gains in BLEU by up to +2.3/+2.0
on dev/test over MERT, and up to +1.5/+1.5 over
PRO, thanks to 20M+ sparse features.
2 Phrase-Based MT and Forced Decoding
We first review the basic phrase-based decoding al-
gorithm (Koehn, 2004), which will be adapted for
forced decoding.
2.1 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English from Mi et al (2008):
0 1 2 3 4 5 6
Figure 1: Standard beam-search phrase-based decoding.
Bu`sh??
Bush
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
-ed
hu?`ta?n
meeting
?Bush held a meeting with Sharon?
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (or states) (Koehn, 2004). Each hy-
pothesis has a coverage vector capturing the source-
language words translated so far, and can be ex-
tended into a longer hypothesis by a phrase-pair
translating an uncovered segment. For example, the
following is one possible derivation:
(0 ) : (0, ??)
(?1 ) : (s1, ?Bush?)
r1
(? ???6) : (s2, ?Bush held talks?)
r2
(???3???) : (s3, ?Bush held talks with Sharon?)
r3
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where each
si is the score of each state, each adding the rule
score and the distortion cost (dc) to the score of the
previous state. To compute the distortion cost we
also need to maintain the ending position of the last
phrase (e.g., the 3 and 6 in the coverage vectors).
In phrase-based translation there is also a distortion-
limit which prohibits long-distance reorderings.
The above states are called?LM states since they
do not involve language model costs. To add a bi-
gram model, we split each ?LM state into a series
of +LM states; each +LM state has the form (v,a)
where a is the last word of the hypothesis. Thus a
+LM version of the above derivation might be:
(0 ,<s>) : (0, ?<s>?)
(?1 ,Bush) : (s?1, ?<s> Bush?)
r1
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
(???3???,Sharon) : (s?3, ?<s> Bush held ... with Sharon?)
r3
1113
0 1 2 3 4 5 6
Bush held
held talks
talks with
with Sharon
Sharon
Figure 2: Forced decoding and y-good derivation lattice.
where the score of applying each rule now also in-
cludes a combination cost due to the bigrams formed
when applying the phrase-pair, e.g.
s?3 = s?2 + s(r3) +dc(|6?3|)? logPlm(with | talk)
To make this exponential-time algorithm practi-
cal, beam search is the standard approximate search
method (Koehn, 2004). Here we group +LM states
into n bins, with each bin Bi hosting at most b states
that cover exactly i Chinese words (see Figure 1).
2.2 Forced Decoding
The idea of forced decoding is to consider only those
(partial) derivations that can produce (a prefix of)
the exact reference translation (assuming single ref-
erence). We call these partial derivations ?y-good?
derivations (Daume?, III and Marcu, 2005), and those
that deviate from the reference translation ?y-bad?
derivations. The forced decoding algorithm is very
similar to +LM decoding introduced above, with the
new ?forced decoding LM? to be defined as only
accepting two consecutive words on the reference
translation, ruling out any y-bad hypothesis:
Pforced (b | a) =
{
1 if ?j, s.t. a = yj and b = yj+1
0 otherwise
In the +LM state, we can simply replace the
boundary word by the index on the reference trans-
lation:
(0 ,0) : (0, ?<s>?)
(?1 ,1) : (w?1, ?<s> Bush?)
r1
(? ???6,3) : (w?2, ?<s> Bush held talks?)
r2
(???3???,5) : (w?3, ?<s> Bush held talks with Sharon?)
r3
The complexity of this forced decoding algorithm
is reduced to O(2nn3) where n is the source sen-
tence length, without the expensive bookkeeping for
English boundary words.
Lia?
nhe?
guo?
pa`i
qia?
n
50 gua?
nch
a?iy
ua?n
jia?n
du?
Bo?l
?`we?
iya`
hu??
fu`
m??n
zhu?
zhe`
ngz
h?`
y??la?
i
sho?
uc?`
qua?
ngu?
o
da`x
ua?n
P            
U.N.
 P           
sent
  P          
50
   P         
observers
             
to
    P        
monitor
          P  
the
          P  
1st
           PP
election
         P   
since
     P       
Bolivia
      P      
restored
       PP    
democracy
5
33
4
1
Figure 3: Example of unreachable sentence pair and
reachable prefix-pair. The first big jump is disallowed for
a distortion limit of 4, but we can still extract the top-left
box as a reachable prefix-pair. Note that this example is
perfectly reachable in syntax-based MT.
2.3 Reachable Prefix-Pairs
In practice, many sentence pairs in the parallel text
fail in forced decoding due to two reasons:
1. distortion limit: long-distance reorderings are
disallowed but are very common between lan-
guages with very different word orders such as
English and Chinese.
2. noisy alignment and phrase limit: the word-
alignment quality (typically from GIZA++) are
usually very noisy, which leads to unnecessar-
ily big chunks of rules beyond the phrase limit.
If we only rely on the reachable whole sentence
pairs, we will not be able to use much of the training
set for Chinese-English. So we propose to augment
the set of reachable examples by considering reach-
able prefix-pairs (see Figure 3 for an example).
3 Violation-Fixing Perceptron for MT
Huang et al (2012) establish a theoretical frame-
work called ?violation-fixing perceptron? which is
tailored for structured learning with inexact search
and has provable convergence properties. The high-
level idea is that standard full update does not fix
search errors; to do that we should instead up-
date when search error occurs, e.g., when the gold-
1114
standard derivation falls below the beam. Huang et
al. (2012) show dramatic improvements in the qual-
ity of the learned model using violation-fixing per-
ceptron (compared to standard perceptron) on incre-
mental parsing and part-of-speech tagging.
Since phrase-based decoding is also an incremen-
tal search problem which closely resembles beam-
search incremental parsing, it is very natural to em-
ploy violation-fixing perceptron here for MT train-
ing. Our goal is to produce the exact reference trans-
lation, or in other words, we want at least one y-good
derivation to survive in the beam search.
To adapt the violation-fixing perceptron frame-
work to MT we need to extend the framework
to handle latent variables since the gold-standard
derivation is not observed. This is done in a way
similar to the latent variable structured perceptron
(Zettlemoyer and Collins, 2005; Liang et al, 2006;
Sun et al, 2009) where each update is from the best
(y-bad) derivation towards the best y-good deriva-
tion in the current model; the latter is a constrained
search which is exactly forced decoding in MT.
3.1 Notations
We first establish some necessary notations. Let
?x, y? be a sentence pair in the training data, and
d = r1 ? r2 ? . . . ? r|d|
be a (partial) derivation, where each ri =
?c(ri), e(ri)? is a rule, i.e., a Chinese-English
phrase-pair. Let |c(d)| ?= ?i |c(ri)| be the num-
ber of Chinese words covered by this derivation, and
e(d) ?= e(r1) ? e(r2) . . . ? e(r|d|) be the English pre-
fix generated so far. Let D(x) be the set of all pos-
sible partial derivations translating part of the input
sentence x. Let pre(y) ?= {y[0:j] | 0 ? j ? |y|}
be the set of prefixes of the reference translation y,
and good i(x, y) be the set of partial y-good deriva-
tions whose English side is a prefix of the reference
translation y, and whose Chinese projection covers
exactly i words on the input sentence x, i.e.,
good i(x, y)
?
= {d ? D(x) | e(d)?pre(y), |c(d)|= i}.
Conversely, we define the set of y-bad partial deriva-
tions covering i Chinese words to be:
bad i(x, y) ?= {d ? D(x) | e(d) /?pre(y), |c(d)|= i}.
Basically, at each bin Bi, y-good derivations
good i(x, y) and y-bad ones bad i(x, y) compete for
the b slots in the bin:
B0 = {} (1)
Bi = topb
?
j=1..l
{d ? r | d ? Bi?j , |c(r)| = j} (2)
where r is a rule covering j Chinese words, l is
the phrase-limit, and topb S is a shorthand for
argtopbd?S w ? ?(x, d) which selects the top b
derivations according to the current model w.
3.2 Algorithm 1: Early Update
As a special case of violation-fixing perceptron,
early update (Collins and Roark, 2004) stops decod-
ing whenever the gold derivation falls off the beam,
makes an update on the prefix so far and move on
to the next example. We adapt it to MT as fol-
lows: if at a certain bin Bi, all y-good derivations
in good i(x, y) have fallen off the bin, then we stop
and update, rewarding the best y-good derivation in
good i(x, y) (with respect to current model w), and
penalizing the best y-bad derivation in the same step:
d+i (x, y)
?
= argmax
d?goodi(x,y)
w ??(x, d) (3)
d?i (x, y)
?
= argmax
d?badi(x,y)?Bi
w ??(x, d) (4)
w? w + ??(x, d+i (x, y), d?i (x, y)) (5)
where ??(x, d, d?) ?= ?(x, d)??(x, d?) is a short-
hand notation for the difference of feature vectors.
Note that the set good i(x, y) is independent of the
beam search and current model and is instead pre-
computed in the forced decoding phase, whereas the
negative signal d?i (x, y) depends on the beam.
In practice, however, there are exponentially
many y-good derivations for each reachable sen-
tence pair, and our goal is just to make sure (at least)
one y-good derivation triumphs at the end. So it
is possible that at a certain bin, all y-good partial
derivations fall off the bin, but the search can still
continue and produce the exact reference translation
through some other y-good path that avoids that bin.
For example, in Figure 1, the y-good states in steps
3 and 5 are not critical; it is totally fine to miss them
in the search as long as we save the y-good states
1115
ea
r
l
y
m
a
x
-
v
i
o
l
a
t
i
o
n
best in the beam
worst in the beam
d i
d+i d+i?
d i?
d+|x|
dy|x|
s
t
d
l
o
c
a
l
standard update 
is invalid
m
o
d
e
l
w
d |x|
Figure 4: Illustration of four update methods. The blue
paths denote (possibly lots of) gold-standard derivations
from forced decoding. Standard update in this case is
invalid as it reinforces the error of w (Huang et al, 2012).
in bins 1, 4 and 6. So we actually use a ?softer?
version of the early update algorithm: only stop and
update when there is no hope to continue. To be
more concrete, let l denote the phrase-limit then we
stop where there are l consecutive bins without any
y-good states, and update on the first among them.
3.3 Algorithm 2: Max-Violation Update
While early update learns substantially better mod-
els than standard perceptron in the midst of inex-
act search, it is also well-known to be converging
much slower than the latter, since each update is
on a (short) prefix. Huang et al (2012) propose an
improved method ?max-violation? which updates at
the worst mistake instead of the first, and converges
much faster than early update with similar or better
accuracy. We adopt this idea here as follows: decode
the whole sentence, and find the step i? where the
difference between the best y-good derivation and
the best y-bad one is the biggest. This amount of dif-
ference is called the amount of ?violation? in Huang
et al (2012), and the place of maximum violation is
intuitively the site of the biggest mistake during the
search. More formally, the update rule is:
i? ?= argmin
i
w ???(x, d+i (x, y), d?i (x, y)) (6)
w? w + ??(x, d+i?(x, y), d?i?(x, y)) (7)
3.4 Previous Work: Standard and Local Updates
We compare the above new update methods with the
two existing ones from Liang et al (2006).
Standard update (also known as ?bold update?
in Liang et al (2006)) simply updates at the very
end, from the best derivation in the beam towards the
best gold-standard derivation (regardless of whether
it survives the beam search):
w? w + ??(x, d+|x|(x, y), d
?
|x|(x, y)) (8)
Local update, however, updates towards the
derivation in the final bin that is most similar to the
reference y, denoted dy|x|(x, y):
dy|x|(x, y) = argmax
d?B|x|
Bleu+1(y, e(d)) (9)
w? w + ??(x, dy|x|(x, y), d
?
|x|(x, y))
(10)
where Bleu+1(?, ?) returns the sentence-level BLEU.
Liang et al (2006) observe that standard update
performs worse than local update, which they at-
tribute to the fact that the former often update to-
wards a gold derivation made up of ?unreasonable?
rules. Here we give a very different but theoreti-
cally more reasonable explanation based on the the-
ory of Huang et al (2012), who define an update
??(x, d+, d?) to be invalid if d+ scores higher
than d? (i.e., w ? ??(x, d+, d?) > 0, or update
?w points to the same direction as w in Fig. 4), in
which case there is no ?violation? or mistake to fix.
Perceptron is guaranteed to converge if all updates
are valid. Clearly, early and max-violation updates
are valid. But standard update is not: it is possible
that at the end of search, the best y-good derivation
d+|x|(x, y), though pruned earlier in the search, rankseven higher in the current model than anything in the
final bin (see Figure 4). In other words, there is no
mistake at the final step, while there must be some
search error in earlier steps which expels the y-good
subderivation. We will see in Section 5.3 that invalid
updates due to search errors are indeed the main rea-
son why standard update fails. Local update, how-
ever, is always valid in that definition.
Finally, it is worth noting that in terms of imple-
mentation, standard and max-violation are the easi-
est, while early update is more involved.
4 Feature Design
Our feature set includes the following 11 dense fea-
tures: LM, four conditional and lexical translation
probabilities (pc(e|f), pc(f |e), pl(e|f), pl(f |e)),
length and phrase penalties, distortion cost, and
three lexicalized reordering features. All these fea-
tures are inherited from Moses (Koehn et al, 2007).
1116
(?1 ,Bush) : (s?1, ?<s> Bush?)
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
</s>ju?x??ng le hu?`ta?nyu? Sha?lo?ngBu`sh??<s>
held talksBush<s>
r1 r2
features for applying r2 on span x[3:6]
WordEdges
c(r2)[0] = ju?x??ng, c(r2)[?1] = hu?`ta?n
e(r2)[0] = held, e(r2)[?1] = talks
x[2:3] = Sha?lo?ng, x[6:7] = </s>, |c(r2)| = 3
... (combos of the above atomic features) ...
non-local e(r0 ? r1)[?2:] ? id(r2)id(r1) ? id(r2)
Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax.
4.1 Local Sparse Features: Ruleid & WordEdges
We first add the rule identification feature for each
rule: id(ri). We also introduce lexicalized Word-
Edges features, which are shown to be very effec-
tive in parsing (Charniak and Johnson, 2005) and
MT (Liu et al, 2008; He et al, 2008) literatures.
We use the following atomic features when apply-
ing a rule ri = ?c(ri), e(ri)?: the source-side length
|c(ri)|, the boundary words of both c(ri) and e(ri),
and the surrounding words of c(ri) on the input sen-
tence x. See Figure 5 for examples. These atomic
features are concatenated to generate all kinds of
combo features.
Chinese English class size budget
word 52.9k 64.2k 5
characters - 3.7k - 3
Brown cluster, full string 200 3
Brown cluster, prefix 6 6 8 2
Brown cluster, prefix 4 4 4 2
POS tag 52 36 2
word type - 4 - 1
Table 1: Various levels of backoff for WordEdges fea-
tures. Class size is estimated on the small Chinese-
English dataset (Sec. 5.3). The POS tagsets are ICT-
CLAS for Chinese (Zhang et al, 2003) and Penn Tree-
bank for English (Marcus et al, 1993).
4.2 Addressing Overfitting
With large numbers of lexicalized combo features
we will face the overfitting problem, where some
combo features found in the training data are too
rare to be seen in the test data. Thus we propose
three ways to alleviate this problem.
First, we introduce various levels of backoffs for
each word w (see Table 1). We include w?s Brown
cluster and its prefixes of lengths 4 and 6 (Brown et
al., 1992), and w?s part-of-speech tag. If w is Chi-
nese we also include its word type (punctuations,
digits, alpha, or otherwise) and (leftmost or right-
most) character. In such a way, we significantly in-
crease the feature coverage on unseen data.
However, if we allow arbitrary combinations, we
can extract a hexalexical feature (4 Chinese + 2 En-
glish words) for a local window in Figure 5, which
is unlikely to be seen at test time. To control model
complexity we introduce a feature budget for each
level of backoffs, shown in the last column in Ta-
ble 1. The total budget for a combo feature is the
sum of the budgets of all atomic features. In our ex-
periments, we only use the combo features with a
total budget of 10 or less, i.e., we can only include
bilexical but not trilexical features, and we can in-
clude for example combo features with one Chinese
word plus two English tags (total budget: 9).
Finally, we use two methods to alleviate overfit-
ting due to one-count rules: for large datasets, we
simply remove all one-count rules, but for small
datasets where out-of-vocabulary words (OOVs)
abound, we use a simple leave-one-out method:
when training on a sentence pair (x, y), do not use
the one-count rules extracted from (x, y) itself.
4.3 Non-Local Features
Following the success of non-local features in pars-
ing (Huang, 2008) and MT (Vaswani et al, 2011),
we also introduce them to capture the contextual in-
formation in MT. Our non-local features, shown in
Figure 5, include bigram rule-ids and the concatena-
tion of a rule id with the translation history, i.e. the
last two English words. Note that we also use back-
offs (Table 1) for the words included. Experiments
(Section 5.3) show that although the set of non-local
features is just a tiny fraction of all features, it con-
tributes substantially to the improvement in BLEU.
1117
Scale Language Training Data Reachability ?BLEU SectionsPair # sent. # words sent. words # feats # refs dev/test
small CH-EN 30K 0.8M/1.0M 21.4% 8.8% 7M 4 +2.2/2.0 5.2, 5.3large 230K 6.9M/8.9M 32.1% 12.7% 23M +2.3/2.0 5.2, 5.4
large SP-EN 174K 4.9M/4.3M 55.0% 43.9% 21M 1 +1.3/1.1 5.5
Table 2: Overview of all experiments. The ?BLEU column shows the absolute improvements of our method MAX-
FORCE on dev/test sets over MERT. The Chinese datasets also use prefix-pairs in training (see Table 3).
5 Experiments
In order to test our approach in different language
pairs, we conduct three experiments, shown in Ta-
ble 2, on two significantly different language pairs
(long vs. short distance reorderings), Chinese-to-
English (CH-EN) and Spanish-to-English (SP-EN).
5.1 System Preparation and Data
We base our experiments on Cubit, a state-of-art
phrase-based system in Python (Huang and Chiang,
2007).1 We set phrase-limit to 7 in rule extraction,
and beam size to 30 and distortion limit 6 in de-
coding. We compare our violation-fixing percep-
tron with two popular tuning methods: MERT (Och,
2003) and PRO (Hopkins and May, 2011).
For word alignments we use GIZA++-`0
(Vaswani et al, 2012) which produces sparser align-
ments, alleviating the garbage collection problem.
We use the SRILM toolkit (Stolcke, 2002) to train a
trigram language model with modified Kneser-Ney
smoothing on 1.5M English sentences.
Our dev and test sets for CH-EN task are from the
newswire portion of 2006 and 2008 NIST MT Eval-
uations (616/691 sentences, 18575/18875 words),
with four references.2 The dev and test sets for SP-
EN task are from newstest2012 and newstest2013,
with only one reference. Below both MERT and PRO
tune weights on the dev set, while our method on the
training set. Specifically, our method only uses the
dev set to know when to stop training.
5.2 Forced Decoding Reachability on Chinese
As mentioned in Section 2.2, we perform forced de-
coding to select reachable sentences from the train-
1http://www.cis.upenn.edu/?lhuang3/cubit/. We
will release the new version at http://acl.cs.qc.edu.
2We use the ?average? reference length to compute the
brevity penalty factor, which does not decrease with more ref-
erences unlike the ?shortest? heuristic.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 10  20  30  40  50  60  70
R
at
io
 o
f c
om
pl
et
e 
co
ve
ra
ge
Sentence length
dist-unlimited
dist-6
dist-4
dist-2
dist-0
Figure 6: Reachability ratio vs. sentence length on the
small CH-EN training set.
small large
sent. words sent. words
full 21.4% 8.8% 32.1% 12.7%
+prefix 61.3% 24.6% 67.3% 32.8%
Table 3: Ratio of sentence reachability and word cover-
age on the two CH-EN training data (distortion limit: 6).
ing data; this part is done with exact search with-
out any beam pruning. Figure 6 shows the reacha-
bility ratio vs. sentence length on the small CH-EN
training data, where the ratio decreases sharply with
sentence length, and increases with distortion limit.
We can see that there are a lot of long distance re-
orderings beyond small distortion limits. In the ex-
treme case of unlimited distortion, a large amount of
sentences will be reachable, but at the cost of much
slower decoding (O(n2V 2) in beam search decod-
ing, andO(2nn3) in forced decoding). In fact forced
decoding is too slow in the unlimited mode that we
only plot reachability for sentences up to 30 words.
Table 3 shows the statistics of forced decoding on
both small and large CH-EN training sets. In the
1118
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
 5  10  15  20  25  30  35  40  45  50
A
ve
ra
ge
 n
um
be
r 
of
 d
er
iv
at
io
ns
Sentence length
dist-6
dist-4
dist-2
dist-0
Figure 7: Average number of derivations in gold lattices.
small data-set, 21.4% sentences are fully reachable
which only contains 8.8% words (since shorter sen-
tences are more likely to be reachable). Larger data
improves reachable ratios significantly thanks to bet-
ter alignment quality, but still only 12.7% words can
be used. In order to add more examples for per-
ceptron training, we pick all non-trivial reachable
prefix-pairs (with 5 or more Chinese words) as addi-
tional training examples (see Section 2.2). As shown
in Table 3, with prefix-pairs we can use about 1/4 of
small data and 1/3 of large data for training, which is
10x and 120x bigger than the 616-sentence dev set.
After running forced decoding, we obtain gold
translation lattice for each reachable sentence (or
prefix) pair. Figure 7 shows, as expected, the av-
erage number of gold derivations in these lattices
grows exponentially with sentence length.
5.3 Analysis on Small Chinese-English Data
Figure 8 shows the BLEU scores of different learn-
ing algorithms on the dev set. MAXFORCE3 per-
forms the best, peaking at iteration 13 while early
update learns much slower (the first few iterations
are faster than other methods due to early stopping
but this difference is immaterial later). The local and
standard updates, however, underperform MERT; in
particular, the latter gets worse as training goes on.
As analysized in Section 3.4, the reason why stan-
dard update (or ?bold update? in Liang et al (2006))
fails is that inexact search leads to many invalid up-
dates. This is confirmed by Figure 9, where more
3Stands for Max-Violation Perceptron w/ Forced Decoding
17
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16  18  20
B
LE
U
Number of iteration
Max
Forc
e
MERT
early
local
standard
Figure 8: BLEU scores on the heldout dev set for different
update methods (trained on small CH-EN data).
50%
60%
70%
80%
90%
 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
R
at
io
beam size
+non-local features
standard perceptron
Figure 9: Ratio of invalid updates in standard update.
than half of the updates remain invalid even at a
beam of 30. These analyses provide an alternative
but theoretically more reasonable explanation to the
findings of Liang et al (2006): while they blame
?unreasonable? gold derivations for the failure of
standard update, we observe that it is the search er-
rors that make the real difference, and that an up-
date that respects search errors towards a gold sub-
derivation is indeed helpful, even if that subderiva-
tion might be ?unreasonable?.
In order to speedup training, we use mini-batch
parallelization of Zhao and Huang (2013) which has
been shown to be much faster than previous paral-
lelization methods. We set the mini-batch size to
24 and train MAXFORCE with 1, 6, and 24 cores
on a small subset of the our original reachable sen-
1119
 22
 23
 24
 0  0.5  1  1.5  2  2.5  3  3.5  4
B
LE
U
Time
MERT PRO-dense
minibatch(24-core)
minibatch(6-core)
minibatch(1 core)
single processor
Figure 10: Minibatch parallelization speeds up learning.
10
12
14
16
18
20
22
24
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MaxForce
MERT
PRO-dense
PRO-medium
PRO-large
Figure 11: Comparison between different training meth-
ods. Ours trains the training set while others on dev set.
tences. The number of sentence pairs in this subset
is 1,032, which contains similar number of words to
our 616-sentence dev set (since reachable sentences
are much shorter). Thus, it is reasonable to compare
different learning algorithms in terms of speed and
performance. Figure 10 shows that first of all, mini-
batch improves BLEU even in the serial setting, and
when run on 24 cores, it leads to a speedup of about
7x. It is also interesting to know that on 1 CPU,
minibatch perceptron takes similar amount of time
to reach the same performance as MERT and PRO.
Figure 11 compares the learning curves of MAX-
FORCE, MERT, and PRO. We test PRO in three
different ways: PRO-dense (dense features only),
PRO-medium (dense features plus top 3K most fre-
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MERT
+non-local
+word-edges
+ruleid
dense
Figure 12: Incremental contributions of different feature
sets (dense features, ruleid, WordEdges, and non-local).
type count % BLEU
dense 11 - 22.3
+ruleid +9,264 +0.1% +0.8
+WordEdges +7,046,238 +99.5% +2.0
+non-local +22,536 +0.3% +0.7
all 7,074,049 100% 25.8
Table 4: Feature counts and incremental BLEU improve-
ments. MAXFORCE with all features is +2.2 over MERT.
quent sparse features4), and PRO-large (dense fea-
tures plus all sparse features). The results show that
PRO-dense performs almost the same as MERT but
with a stabler learning curve while PRO-medium im-
proves by +0.6. However, PRO-large decreases the
performance significantly, which indicates PRO is
not scalable to truly sparse features. By contrast,
our method handles large-scale sparse features well
and outperforms all other methods by a large margin
and with a stable learning curve.
We also investigate the individual contribution
from each group of features (ruleid, WordEdges, and
non-local features). So we perform experiments by
adding each group incrementally. Figure 12 shows
the learning curves and Table 4 lists the counts and
incremental contributions of different feature sets.
With dense features alone MAXFORCE does not do
4To prevent overfitting we remove all lexicalized features
and only use Brown clusters. It is difficult to engineer the right
feature set for PRO, whereas MAXFORCE is much more robust.
1120
system algorithm # feat. dev test
Moses MERT 11 25.5 22.5
Cubit
MERT 11 25.4 22.5
PRO
11 25.6 22.6
3K 26.3 23.0
36K 17.7 14.3
MAXFORCE 23M 27.8 24.5
Table 5: BLEU scores (with four references) using the
large CH-EN data. Our approach is +2.3/2.0 over MERT.
well because perceptron is known to suffer from fea-
tures of vastly different scales. Adding ruleid helps,
but still not enough. WordEdges (which is the vast
majority of features) improves BLEU by +2.0 points
and outperforms MERT, when sparse features totally
dominate dense features. Finally, the 0.3% non-local
features contribute a final +0.7 in BLEU.
5.4 Results on Large Chinese-English Data
Table 5 shows all BLEU scores for different learn-
ing algorithms on the large CH-EN data. The MERT
baseline on Cubit is essentially the same as Moses.
Our MAXFORCE activates 23M features on reach-
able sentences and prefixes in the training data, and
takes 35 hours to finish 15 iterations on 24 cores,
peaking at iteration 13. It achieves significant im-
provements over other approaches: +2.3/+2.0 points
over MERT and +1.5/+1.5 over PRO-medium on de-
v/test sets, respectively.
5.5 Results on Large Spanish-English Data
In SP-EN translation, we first run forced decod-
ing on the training set, and achieve a very high
reachability of 55% (with the same distortion limit
of 6), which is expected since the word order be-
tween Spanish and English are more similar than
than between Chinese and English, and most SP-
EN reorderings are local. Table 6 shows that MAX-
FORCE improves the translation quality over MERT
by +1.3/+1.1 BLEU on dev/test. These gains are
comparable to the improvements on the CH-EN task,
since it is well accepted in MT literature that a
change of ? in 1-reference BLEU is roughly equiva-
lent to a change of 2? with 4 references.
system algorithm # feat. dev test
Moses MERT 11 27.4 24.4
Cubit MAXFORCE 21M 28.7 25.5
Table 6: BLEU scores (with one reference) on SP-EN.
6 Related Work
Besides those discussed in Section 1, there are also
some research on tuning sparse features on the train-
ing data, but they integrate those sparse features into
the MT log-linear model as a single feature weight,
and tune its weight on the dev set (e.g. (Liu et al,
2008; He et al, 2008; Wuebker et al, 2010; Simi-
aner et al, 2012; Flanigan et al, 2013; Setiawan
and Zhou, 2013; He and Deng, 2012; Gao and He,
2013)). By contrast, our approach learns sparse fea-
tures only on the training set, and use dev set as held-
out to know when to stop.
Forced decoding has been used in the MT litera-
ture. For example, open source MT systems Moses
and cdec have implemented it. Liang et al (2012)
also use the it to boost the MERT tuning by adding
more y-good derivations to the standard k-best list.
7 Conclusions and Future Work
We have presented a simple yet effective approach
of structured learning for machine translation which
scales, for the first time, to a large portion of the
whole training data, and enables us to tune a rich set
of sparse, lexical, and non-local features. Our ap-
proach results in very significant BLEU gains over
MERT and PRO baselines. For future work, we will
consider other translation paradigms such as hierar-
chical phrase-based or syntax-based MT.
Acknowledgement
We thank the three anonymous reviewers for helpful sug-
gestions. We are also grateful to David Chiang, Dan
Gildea, Yoav Goldberg, Yifan He, Abe Ittycheriah, and
Hao Zhang for discussions, and Chris Callison-Burch,
Philipp Koehn, Lemao Liu, and Taro Watanabe for help
with datasets. Huang, Yu, and Zhao are supported by
DARPA FA8750-13-2-0041 (DEFT), a Google Faculty
Research Award, and a PSC-CUNY Award, and Mi by
DARPA HR0011-12-C-0015. Yu is also supported by the
China 863 State Key Project (No. 2011AA01A207). The
views and findings in this paper are those of the authors
and are not endorsed by the US or Chinese governments.
1121
References
Peter Brown, Peter Desouza, Robert Mercer, Vincent
Pietra, and Jenifer Lai. 1992. Class-based n-gram
models of natural language. Computational linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of NAACL 2013.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient ascent.
In Proceedings of NAACL:HLT, pages 450?459, At-
lanta, Georgia, June.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proceedings of ACL.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of COLING, pages
321?328, Manchester, UK, August.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of ACL:
Demonstrations.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
COLING-ACL, Sydney, Australia, July.
Huashen Liang, Min Zhang, and Tiejun Zhao. 2012.
Forced decoding for minimum error rate training in
statistical machine translation. Journal of Computa-
tional Information Systems, (8):861868.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of EMNLP, pages 89?97, Honolulu,
Hawaii, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Hendra Setiawan and Bowen Zhou. 2013. Discrimi-
native training of 150 million translation parameters
and its application to pruning. In Proceedings of
NAACL:HLT, pages 335?341, Atlanta, Georgia, June.
ACL.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proceedings of ACL, Jeju Island, Korea.
1122
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Xu Sun, Takuya Matsuzaki, and Daisuke Okanohara.
2009. Latent variable perceptron algorithm for struc-
tured classification. In Proceedings of IJCAI.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of ACL 2011, Port-
land, OR.
Ashish Vaswani, Liang Huang, and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of ACL.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of EMNLP-
CoNLL.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of ACL, pages 475?484, Uppsala,
Sweden, July.
Luke Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In Pro-
ceedings of UAI.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. In Proceedings of the second SIGHAN workshop
on Chinese language processing, pages 184?187.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-
ald. 2013. Online learning with inexact hypergraph
search. In Proceedings of EMNLP 2013.
Kai Zhao and Liang Huang. 2013. Minibatch and paral-
lelization for online large margin structured learning.
In Proceedings of NAACL 2013.
1123
