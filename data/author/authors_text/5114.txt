Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Minimum Cut Model for Spoken Lecture Segmentation
Igor Malioutov and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{igorm,regina}@csail.mit.edu
Abstract
We consider the task of unsupervised lec-
ture segmentation. We formalize segmen-
tation as a graph-partitioning task that op-
timizes the normalized cut criterion. Our
approach moves beyond localized com-
parisons and takes into account long-
range cohesion dependencies. Our results
demonstrate that global analysis improves
the segmentation accuracy and is robust in
the presence of speech recognition errors.
1 Introduction
The development of computational models of text
structure is a central concern in natural language
processing. Text segmentation is an important in-
stance of such work. The task is to partition a
text into a linear sequence of topically coherent
segments and thereby induce a content structure
of the text. The applications of the derived rep-
resentation are broad, encompassing information
retrieval, question-answering and summarization.
Not surprisingly, text segmentation has been ex-
tensively investigated over the last decade. Fol-
lowing the first unsupervised segmentation ap-
proach by Hearst (1994), most algorithms assume
that variations in lexical distribution indicate topic
changes. When documents exhibit sharp varia-
tions in lexical distribution, these algorithms are
likely to detect segment boundaries accurately.
For example, most algorithms achieve high per-
formance on synthetic collections, generated by
concatenation of random text blocks (Choi, 2000).
The difficulty arises, however, when transitions
between topics are smooth and distributional vari-
ations are subtle. This is evident in the perfor-
mance of existing unsupervised algorithms on less
structured datasets, such as spoken meeting tran-
scripts (Galley et al, 2003). Therefore, a more
refined analysis of lexical distribution is needed.
Our work addresses this challenge by casting
text segmentation in a graph-theoretic framework.
We abstract a text into a weighted undirected
graph, where the nodes of the graph correspond
to sentences and edge weights represent the pair-
wise sentence similarity. In this framework, text
segmentation corresponds to a graph partitioning
that optimizes the normalized-cut criterion (Shi
and Malik, 2000). This criterion measures both the
similarity within each partition and the dissimilar-
ity across different partitions. Thus, our approach
moves beyond localized comparisons and takes
into account long-range changes in lexical distri-
bution. Our key hypothesis is that global analysis
yields more accurate segmentation results than lo-
cal models.
We tested our algorithm on a corpus of spo-
ken lectures. Segmentation in this domain is chal-
lenging in several respects. Being less structured
than written text, lecture material exhibits digres-
sions, disfluencies, and other artifacts of sponta-
neous communication. In addition, the output of
speech recognizers is fraught with high word er-
ror rates due to specialized technical vocabulary
and lack of in-domain spoken data for training.
Finally, pedagogical considerations call for fluent
transitions between different topics in a lecture,
further complicating the segmentation task.
Our experimental results confirm our hypothe-
sis: considering long-distance lexical dependen-
cies yields substantial gains in segmentation per-
formance. Our graph-theoretic approach com-
pares favorably to state-of-the-art segmentation al-
gorithms and attains results close to the range of
human agreement scores. Another attractive prop-
25
erty of the algorithm is its robustness to noise: the
accuracy of our algorithm does not deteriorate sig-
nificantly when applied to speech recognition out-
put.
2 Previous Work
Most unsupervised algorithms assume that frag-
ments of text with homogeneous lexical distribu-
tion correspond to topically coherent segments.
Previous research has analyzed various facets of
lexical distribution, including lexical weighting,
similarity computation, and smoothing (Hearst,
1994; Utiyama and Isahara, 2001; Choi, 2000;
Reynar, 1998; Kehagias et al, 2003; Ji and Zha,
2003).
The focus of our work, however, is on an or-
thogonal yet fundamental aspect of this analysis
? the impact of long-range cohesion dependen-
cies on segmentation performance. In contrast to
previous approaches, the homogeneity of a seg-
ment is determined not only by the similarity of its
words, but also by their relation to words in other
segments of the text. We show that optimizing our
global objective enables us to detect subtle topical
changes.
Graph-Theoretic Approaches in Vision Seg-
mentation Our work is inspired by minimum-cut-
based segmentation algorithms developed for im-
age analysis. Shi and Malik (2000) introduced
the normalized-cut criterion and demonstrated its
practical benefits for segmenting static images.
Our method, however, is not a simple applica-
tion of the existing approach to a new task. First,
in order to make it work in the new linguistic
framework, we had to redefine the underlying rep-
resentation and introduce a variety of smoothing
and lexical weighting techniques. Second, the
computational techniques for finding the optimal
partitioning are also quite different. Since the min-
imization of the normalized cut is NP -complete
in the general case, researchers in vision have to
approximate this computation. Fortunately, we
can find an exact solution due to the linearity con-
straint on text segmentation.
3 Minimum Cut Framework
Linguistic research has shown that word repeti-
tion in a particular section of a text is a device for
creating thematic cohesion (Halliday and Hasan,
1976), and that changes in the lexical distributions
usually signal topic transitions.
Figure 1: Sentence similarity plot for a Physics
lecture, with vertical lines indicating true segment
boundaries.
Figure 1 illustrates these properties in a lec-
ture transcript from an undergraduate Physics
class. We use the text Dotplotting representation
by (Church, 1993) and plot the cosine similar-
ity scores between every pair of sentences in the
text. The intensity of a point (i, j) on the plot in-
dicates the degree to which the i-th sentence in
the text is similar to the j-th sentence. The true
segment boundaries are denoted by vertical lines.
This similarity plot reveals a block structure where
true boundaries delimit blocks of text with high
inter-sentential similarity. Sentences found in dif-
ferent blocks, on the other hand, tend to exhibit
low similarity.
u1 u2 u3 un
Figure 2: Graph-based Representation of Text
Formalizing the Objective Whereas previous
unsupervised approaches to segmentation rested
on intuitive notions of similarity density, we for-
malize the objective of text segmentation through
cuts on graphs. We aim to jointly maximize the
intra-segmental similarity and minimize the simi-
larity between different segments. In other words,
we want to find the segmentation with a maximally
homogeneous set of segments that are also maxi-
26
mally different from each other.
Let G = {V,E} be an undirected, weighted
graph, where V is the set of nodes correspond-
ing to sentences in the text and E is the set of
weighted edges (See Figure 2). The edge weights,
w(u, v), define a measure of similarity between
pairs of nodes u and v, where higher scores in-
dicate higher similarity. Section 4 provides more
details on graph construction.
We consider the problem of partitioning the
graph into two disjoint sets of nodes A and B. We
aim to minimize the cut, which is defined to be the
sum of the crossing edges between the two sets of
nodes. In other words, we want to split the sen-
tences into two maximally dissimilar classes by
choosing A and B to minimize:
cut(A,B) =
?
u?A,v?B
w(u, v)
However, we need to ensure that the two parti-
tions are not only maximally different from each
other, but also that they are themselves homoge-
neous by accounting for intra-partition node simi-
larity. We formulate this requirement in the frame-
work of normalized cuts (Shi and Malik, 2000),
where the cut value is normalized by the volume
of the corresponding partitions. The volume of the
partition is the sum of its edges to the whole graph:
vol(A) =
?
u?A,v?V
w(u, v)
The normalized cut criterion (Ncut) is then de-
fined as follows:
Ncut(A,B) =
cut(A,B)
vol(A)
+
cut(A,B)
vol(B)
By minimizing this objective we simultane-
ously minimize the similarity across partitions and
maximize the similarity within partitions. This
formulation also allows us to decompose the ob-
jective into a sum of individual terms, and formu-
late a dynamic programming solution to the mul-
tiway cut problem.
This criterion is naturally extended to a k-way
normalized cut:
Ncutk(V ) =
cut(A1, V ?A1)
vol(A1)
+ . . .+
cut(Ak, V ?Ak)
vol(Ak)
where A1 . . . Ak form a partition of the graph,
and V ?Ak is the set difference between the entire
graph and partition k.
Decoding Papadimitriou proved that the prob-
lem of minimizing normalized cuts on graphs is
NP -complete (Shi and Malik, 2000). However,
in our case, the multi-way cut is constrained to
preserve the linearity of the segmentation. By seg-
mentation linearity, we mean that all of the nodes
between the leftmost and the rightmost nodes of
a particular partition have to belong to that par-
tition. With this constraint, we formulate a dy-
namic programming algorithm for exactly finding
the minimum normalized multiway cut in polyno-
mial time:
C [i, k] = min
j<k
[
C [i? 1, j] +
cut [Aj,k, V ?Aj,k]
vol [Aj,k]
]
(1)
B [i, k] = argmin
j<k
[
C [i? 1, j] +
cut [Aj,k, V ?Aj,k]
vol [Aj,k]
]
(2)
s.t. C [0, 1] = 0, C [0, k] = ?, 1 < k ? N (3)
B [0, k] = 1, 1 ? k ? N (4)
C [i, k] is the normalized cut value of the op-
timal segmentation of the first k sentences into i
segments. The i-th segment, Aj,k, begins at node
uj and ends at node uk. B [i, k] is the back-pointer
table from which we recover the optimal sequence
of segment boundaries. Equations 3 and 4 capture
respectively the condition that the normalized cut
value of the trivial segmentation of an empty text
into one segment is zero and the constraint that the
first segment starts with the first node.
The time complexity of the dynamic program-
ming algorithm is O(KN2), where K is the num-
ber of partitions and N is the number of nodes in
the graph or sentences in the transcript.
4 Building the Graph
Clearly, the performance of our model depends
on the underlying representation, the definition of
the pairwise similarity function, and various other
model parameters. In this section we provide fur-
ther details on the graph construction process.
Preprocessing Before building the graph, we
apply standard text preprocessing techniques to
the text. We stem words with the Porter stem-
mer (Porter, 1980) to alleviate the sparsity of word
counts through stem equivalence classes. We also
remove words matching a prespecified list of stop
words.
27
Graph Topology As we noted in the previ-
ous section, the normalized cut criterion considers
long-term similarity relationships between nodes.
This effect is achieved by constructing a fully-
connected graph. However, considering all pair-
wise relations in a long text may be detrimen-
tal to segmentation accuracy. Therefore, we dis-
card edges between sentences exceeding a certain
threshold distance. This reduction in the graph
size also provides us with computational savings.
Similarity Computation In computing pair-
wise sentence similarities, sentences are repre-
sented as vectors of word counts. Cosine sim-
ilarity is commonly used in text segmentation
(Hearst, 1994). To avoid numerical precision
issues when summing a series of very small
scores, we compute exponentiated cosine similar-
ity scores between pairs of sentence vectors:
w(si, sj) = e
si?sj
||si||?||sj ||
We further refine our analysis by smoothing the
similarity metric. When comparing two sentences,
we also take into account similarity between their
immediate neighborhoods. The smoothing is
achieved by adding counts of words that occur in
adjoining sentences to the current sentence feature
vector. These counts are weighted in accordance
to their distance from the current sentence:
s?i =
i+k?
j=i
e??(j?i)sj ,
where si are vectors of word counts, and ? is a
parameter that controls the degree of smoothing.
In the formulation above we use sentences as
our nodes. However, we can also represent graph
nodes with non-overlapping blocks of words of
fixed length. This is desirable, since the lecture
transcripts lack sentence boundary markers, and
short utterances can skew the cosine similarity
scores. The optimal length of the block is tuned
on a heldout development set.
Lexical Weighting Previous research has
shown that weighting schemes play an important
role in segmentation performance (Ji and Zha,
2003; Choi et al, 2001). Of particular concern
are words that may not be common in general En-
glish discourse but that occur throughout the text
for a particular lecture or subject. For example, in
a lecture about support vector machines, the oc-
currence of the term ?SVM? is not going to con-
vey a lot of information about the distribution of
Segments per Total Word ASR WER
Corpus Lectures Lecture Tokens Accuracy
Physics 33 5.9 232K 19.4%
AI 22 12.3 182K ?
Table 1: Lecture Corpus Statistics
sub-topics, even though it is a fairly rare term
in general English and bears much semantic con-
tent. The same words can convey varying degrees
of information across different lectures, and term
weighting specific to individual lectures becomes
important in the similarity computation.
In order to address this issue, we introduce a
variation on the tf-idf scoring scheme used in the
information-retrieval literature (Salton and Buck-
ley, 1988). A transcript is split uniformly into N
chunks; each chunk serves as the equivalent of
documents in the tf-idf computation. The weights
are computed separately for each transcript, since
topic and word distributions vary across lectures.
5 Evaluation Set-Up
In this section we present the different corpora
used to evaluate our model and provide a brief
overview of the evaluation metrics. Next, we de-
scribe our human segmentation study on the cor-
pus of spoken lecture data.
5.1 Parameter Estimation
A heldout development set of three lectures is-
used for estimating the optimal word block length
for representing nodes, the threshold distances for
discarding node edges, the number of uniform
chunks for estimating tf-idf lexical weights, the
alpha parameter for smoothing, and the length of
the smoothing window. We use a simple greedy
search procedure for optimizing the parameters.
5.2 Corpora
We evaluate our segmentation algorithm on three
sets of data. Two of the datasets we use are new
segmentation collections that we have compiled
for this study,1 and the remaining set includes a
standard collection previously used for evaluation
of segmentation algorithms. Various corpus statis-
tics for the new datasets are presented in Table 1.
Below we briefly describe each corpus.
Physics Lectures Our first corpus consists of
spoken lecture transcripts from an undergraduate
1Our materials are publicly available at http://www.
csail.mit.edu/
?
igorm/acl06.html
28
Physics class. In contrast to other segmentation
datasets, our corpus contains much longer texts.
A typical lecture of 90 minutes has 500 to 700
sentences with 8500 words, which corresponds to
about 15 pages of raw text. We have access both
to manual transcriptions of these lectures and also
output from an automatic speech recognition sys-
tem. The word error rate for the latter is 19.4%,2
which is representative of state-of-the-art perfor-
mance on lecture material (Leeuwis et al, 2003).
The Physics lecture transcript segmentations
were produced by the teaching staff of the intro-
ductory Physics course at the Massachusetts In-
stitute of Technology. Their objective was to fa-
cilitate access to lecture recordings available on
the class website. This segmentation conveys the
high-level topical structure of the lectures. On av-
erage, a lecture was annotated with six segments,
and a typical segment corresponds to two pages of
a transcript.
Artificial Intelligence Lectures Our second
lecture corpus differs in subject matter, lecturing
style, and segmentation granularity. The gradu-
ate Artificial Intelligence class has, on average,
twelve segments per lecture, and a typical segment
is about half of a page. One segment roughly cor-
responds to the content of a slide. This time the
segmentation was obtained from the lecturer her-
self. The lecturer went through the transcripts of
lecture recordings and segmented the lectures with
the objective of making the segments correspond
to presentation slides for the lectures.
Due to the low recording quality, we were un-
able to obtain the ASR transcripts for this class.
Therefore, we only use manual transcriptions of
these lectures.
Synthetic Corpus Also as part of our anal-
ysis, we used the synthetic corpus created by
Choi (2000) which is commonly used in the eval-
uation of segmentation algorithms. This corpus
consists of a set of concatenated segments ran-
domly sampled from the Brown corpus. The
length of the segments in this corpus ranges from
three to eleven sentences. It is important to note
that the lexical transitions in these concatenated
texts are very sharp, since the segments come from
texts written in widely varying language styles on
completely different topics.
2A speaker-dependent model of the lecturer was trained
on 38 hours of lectures from other courses using the SUM-
MIT segment-based Speech Recognizer (Glass, 2003).
5.3 Evaluation Metric
We use the Pk and WindowDiff measures to eval-
uate our system (Beeferman et al, 1999; Pevzner
and Hearst, 2002). The Pk measure estimates the
probability that a randomly chosen pair of words
within a window of length k words is inconsis-
tently classified. The WindowDiff metric is a vari-
ant of the Pk measure, which penalizes false posi-
tives on an equal basis with near misses.
Both of these metrics are defined with re-
spect to the average segment length of texts and
exhibit high variability on real data. We fol-
low Choi (2000) and compute the mean segment
length used in determining the parameter k on
each reference text separately.
We also plot the Receiver Operating Character-
istic (ROC) curve to gauge performance at a finer
level of discrimination (Swets, 1988). The ROC
plot is the plot of the true positive rate against the
false positive rate for various settings of a decision
criterion. In our case, the true positive rate is the
fraction of boundaries correctly classified, and the
false positive rate is the fraction of non-boundary
positions incorrectly classified as boundaries. In
computing the true and false positive rates, we
vary the threshold distance to the true boundary
within which a hypothesized boundary is consid-
ered correct. Larger areas under the ROC curve
of a classifier indicate better discriminative perfor-
mance.
5.4 Human Segmentation Study
Spoken lectures are very different in style from
other corpora used in human segmentation studies
(Hearst, 1994; Galley et al, 2003). We are inter-
ested in analyzing human performance on a corpus
of lecture transcripts with much longer texts and a
less clear-cut concept of a sub-topic. We define a
segment to be a sub-topic that signals a prominent
shift in subject matter. Disregarding this sub-topic
change would impair the high-level understanding
of the structure and the content of the lecture.
As part of our human segmentation analysis,
we asked three annotators to segment the Physics
lecture corpus. These annotators had taken the
class in the past and were familiar with the subject
matter under consideration. We wrote a detailed
instruction manual for the task, with annotation
guidelines for the most part following the model
used by Gruenstein et al (2005). The annotators
were instructed to segment at a level of granularity
29
O A B C
MEAN SEG. COUNT 6.6 8.9 18.4 13.8
MEAN SEG. LENGTH 69.4 51.5 24.9 33.2
SEG. LENGTH DEV. 39.6 37.4 34.5 39.4
Table 2: Annotator Segmentation Statistics for the
first ten Physics lectures.
REF/HYP O A B C
O 0 0.243 0.418 0.312
A 0.219 0 0.400 0.355
B 0.314 0.337 0 0.332
C 0.260 0.296 0.370 0
Table 3: Pk annotation agreement between differ-
ent pairs of annotators.
that would identify most of the prominent topical
transitions necessary for a summary of the lecture.
The annotators used the NOMOS annotation
software toolkit, developed for meeting segmenta-
tion (Gruenstein et al, 2005). They were provided
with recorded audio of the lectures and the corre-
sponding text transcriptions. We intentionally did
not provide the subjects with the target number of
boundaries, since we wanted to see if the annota-
tors would converge on a common segmentation
granularity.
Table 2 presents the annotator segmentation
statistics. We see two classes of segmentation
granularities. The original reference (O) and anno-
tator A segmented at a coarse level with an average
of 6.6 and 8.9 segments per lecture, respectively.
Annotators B and C operated at much finer levels
of discrimination with 18.4 and 13.8 segments per
lecture on average. We conclude that multiple lev-
els of granularity are acceptable in spoken lecture
segmentation. This is expected given the length of
the lectures and varying human judgments in se-
lecting relevant topical content.
Following previous studies, we quantify the
level of annotator agreement with the Pk measure
(Gruenstein et al, 2005).3 Table 3 shows the an-
notator agreement scores between different pairs
of annotators. Pk measures ranged from 0.24 and
0.42. We observe greater consistency at similar
levels of granularity, and less so across the two
3Kappa measure would not be the appropriate measure in
this case, because it is not sensitive to near misses, and we
cannot make the required independence assumption on the
placement of boundaries.
EDGE CUTOFF
10 25 50 100 200 NONE
PHYSICS (MANUAL)
PK 0.394 0.373 0.341 0.295 0.311 0.330
WD 0.404 0.383 0.352 0.308 0.329 0.350
PHYSICS (ASR)
PK 0.440 0.371 0.343 0.330 0.322 0.359
WD 0.456 0.383 0.356 0.343 0.342 0.398
AI
PK 0.480 0.422 0.408 0.416 0.393 0.397
WD 0.493 0.435 0.420 0.440 0.424 0.432
CHOI
PK 0.222 0.202 0.213 0.216 0.208 0.208
WD 0.234 0.222 0.233 0.238 0.230 0.230
Table 4: Edges between nodes separated beyond a
certain threshold distance are removed.
classes. Note that annotator A operated at a level
of granularity consistent with the original refer-
ence segmentation. Hence, the 0.24 Pk measure
score serves as the benchmark with which we can
compare the results attained by segmentation al-
gorithms on the Physics lecture data.
As an additional point of reference we note that
the uniform and random baseline segmentations
attain 0.469 and 0.493 Pk measure, respectively,
on the Physics lecture set.
6 Experimental Results
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
False Positive Rate
Tru
e P
osit
ive 
Rat
e
Cutoff=5Cutoff=100
Figure 3: ROC plot for the Minimum Cut Seg-
menter on thirty Physics Lectures, with edge cut-
offs set at five and hundred sentences.
Benefits of global analysis We first determine
the impact of long-range pairwise similarity de-
pendencies on segmentation performance. Our
30
CHOI UI MINCUT
PHYSICS (MANUAL)
PK 0.372 0.310 0.298
WD 0.385 0.323 0.311
PHYSICS (ASR TRANSCRIPTS)
PK 0.361 0.352 0.322
WD 0.376 0.364 0.340
AI
PK 0.445 0.374 0.383
WD 0.478 0.420 0.417
CHOI
PK 0.110 0.105 0.212
WD 0.121 0.116 0.234
Table 5: Performance analysis of different algo-
rithms using the Pk and WindowDiff measures,
with three lectures heldout for development.
key hypothesis is that considering long-distance
lexical relations contributes to the effectiveness of
the algorithm. To test this hypothesis, we discard
edges between nodes that are more than a cer-
tain number of sentences apart. We test the sys-
tem on a range of data sets, including the Physics
and AI lectures and the synthetic corpus created by
Choi (2000). We also include segmentation results
on Physics ASR transcripts.
The results in Table 4 confirm our hypothesis ?
taking into account non-local lexical dependencies
helps across different domains. On manually tran-
scribed Physics lecture data, for example, the al-
gorithm yields 0.394 Pk measure when taking into
account edges separated by up to ten sentences.
When dependencies up to a hundred sentences are
considered, the algorithm yields a 25% reduction
in Pk measure. Figure 3 shows the ROC plot
for the segmentation of the Physics lecture data
with different cutoff parameters, again demon-
strating clear gains attained by employing long-
range dependencies. As Table 4 shows, the im-
provement is consistent across all lecture datasets.
We note, however, that after some point increas-
ing the threshold degrades performance, because
it introduces too many spurious dependencies (see
the last column of Table 4). The speaker will oc-
casionally return to a topic described at the begin-
ning of the lecture, and this will bias the algorithm
to put the segment boundary closer to the end of
the lecture.
Long-range dependencies do not improve the
performance on the synthetic dataset. This is ex-
pected since the segments in the synthetic dataset
are randomly selected from widely-varying doc-
uments in the Brown corpus, even spanning dif-
ferent genres of written language. So, effectively,
there are no genuine long-range dependencies that
can be exploited by the algorithm.
Comparison with local dependency models
We compare our system with the state-of-the-art
similarity-based segmentation system developed
by Choi (2000). We use the publicly available im-
plementation of the system and optimize the sys-
tem on a range of mask-sizes and different param-
eter settings described in (Choi, 2000) on a held-
out development set of three lectures. To control
for segmentation granularity, we specify the num-
ber of segments in the reference (?O?) segmen-
tation for both our system and the baseline. Ta-
ble 5 shows that the Minimum Cut algorithm con-
sistently outperforms the similarity-based baseline
on all the lecture datasets. We attribute this gain
to the presence of more attenuated topic transi-
tions in spoken language. Since spoken language
is more spontaneous and less structured than writ-
ten language, the speaker needs to keep the listener
abreast of the changes in topic content by intro-
ducing subtle cues and references to prior topics in
the course of topical transitions. Non-local depen-
dencies help to elucidate shifts in focus, because
the strength of a particular transition is measured
with respect to other local and long-distance con-
textual discourse relationships.
Our system does not outperform Choi?s algo-
rithm on the synthetic data. This again can be at-
tributed to the discrepancy in distributional prop-
erties of the synthetic corpus which lacks coher-
ence in its thematic shifts and the lecture corpus
of spontaneous speech with smooth distributional
variations. We also note that we did not try to ad-
just our model to optimize its performance on the
synthetic data. The smoothing method developed
for lecture segmentation may not be appropriate
for short segments ranging from three to eleven
sentences that constitute the synthetic set.
We also compared our method with another
state-of-the-art algorithm which does not explic-
itly rely on pairwise similarity analysis. This algo-
rithm (Utiyama and Isahara, 2001) (UI) computes
the optimal segmentation by estimating changes in
the language model predictions over different par-
titions. We used the publicly available implemen-
31
tation of the system that does not require parame-
ter tuning on a heldout development set.
Again, our method achieves favorable perfor-
mance on a range of lecture data sets (See Ta-
ble 5), and both algorithms attain results close to
the range of human agreement scores. The attrac-
tive feature of our algorithm, however, is robust-
ness to recognition errors ? testing it on the ASR
transcripts caused only 7.8% relative increase in
Pk measure (from 0.298 to 0.322), compared to
a 13.5% relative increase for the UI system. We
attribute this feature to the fact that the model is
less dependent on individual recognition errors,
which have a detrimental effect on the local seg-
ment language modeling probability estimates for
the UI system. The block-level similarity func-
tion is not as sensitive to individual word errors,
because the partition volume normalization factor
dampens their overall effect on the derived mod-
els.
7 Conclusions
In this paper we studied the impact of long-range
dependencies on the accuracy of text segmenta-
tion. We modeled text segmentation as a graph-
partitioning task aiming to simultaneously opti-
mize the total similarity within each segment and
dissimilarity across various segments. We showed
that global analysis of lexical distribution im-
proves the segmentation accuracy and is robust
in the presence of recognition errors. Combin-
ing global analysis with advanced methods for
smoothing (Ji and Zha, 2003) and weighting could
further boost the segmentation performance.
Our current implementation does not automati-
cally determine the granularity of a resulting seg-
mentation. This issue has been explored in the
past (Ji and Zha, 2003; Utiyama and Isahara,
2001), and we will explore the existing strategies
in our framework. We believe that the algorithm
has to produce segmentations for various levels of
granularity, depending on the needs of the appli-
cation that employs it.
Our ultimate goal is to automatically generate
tables of content for lectures. We plan to in-
vestigate strategies for generating titles that will
succinctly describe the content of each segment.
We will explore how the interaction between the
generation and segmentation components can im-
prove the performance of such a system as a
whole.
8 Acknowledgements
The authors acknowledge the support of the National Sci-
ence Foundation (CAREER grant IIS-0448168, grant IIS-
0415865, and the NSF Graduate Fellowship). Any opinions,
findings, conclusions or recommendations expressed in this
publication are those of the author(s) and do not necessar-
ily reflect the views of the National Science Foundation. We
would like to thank Masao Utiyama for providing us with an
implementation of his segmentation system and Alex Gru-
enstein for assisting us with the NOMOS toolkit. We are
grateful to David Karger for an illuminating discussion on
the Minimum Cut algorithm. We also would like to acknowl-
edge the MIT NLP and Speech Groups, the three annotators,
and the three anonymous reviewers for valuable comments,
suggestions, and help.
References
D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical
models for text segmentation. Machine Learning, 34(1-
3):177?210.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent se-
mantic analysis for text segmentation. In Proceedings of
EMNLP, 109?117.
F. Y. Y. Choi. 2000. Advances in domain independent linear
text segmentation. In Proceedings of the NAACL, 26?33.
K. W. Church. 1993. Char align: A program for aligning
parallel texts at the character level. In Proceedings of the
ACL, 1?8.
M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003.
Discourse segmentation of multi-party conversation. In
Proceedings of the ACL, 562?569.
J. R. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17(2?3):137?152.
A. Gruenstein, J. Niekrasz, M. Purver. 2005. Meeting struc-
ture annotation: Data and tools. In Proceedings of the
SIGdial Workshop on Discourse and Dialogue, 117?127.
M. A. K. Halliday, R. Hasan. 1976. Cohesion in English.
Longman, London.
M. Hearst. 1994. Multi-paragraph segmentation of exposi-
tory text. In Proceedings of the ACL, 9?16.
X. Ji, H. Zha. 2003. Domain-independent text segmentation
using anisotropic diffusion and dynamic programming. In
Proceedings of SIGIR, 322?329.
A. Kehagias, P. Fragkou, V. Petridis. 2003. Linear text seg-
mentation using a dynamic programming algorithm. In
Proceedings of the EACL, 171?178.
E. Leeuwis, M. Federico, M. Cettolo. 2003. Language mod-
eling and transcription of the ted corpus lectures. In Pro-
ceedings of ICASSP, 232?235.
L. Pevzner, M. Hearst. 2002. A critique and improvement
of an evaluation metric for text segmentation. Computa-
tional Linguistics, 28(1):pp. 19?36.
M. F. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3):130?137.
J. Reynar. 1998. Topic segmentation: Algorithms and appli-
cations. Ph.D. thesis, University of Pennsylvania.
G. Salton, C. Buckley. 1988. Term weighting approaches
in automatic text retrieval. Information Processing and
Management, 24(5):513?523.
J. Shi, J. Malik. 2000. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8):888?905.
J. Swets. 1988. Measuring the accuracy of diagnostic sys-
tems. Science, 240(4857):1285?1293.
M. Utiyama, H. Isahara. 2001. A statistical model for
domain-independent text segmentation. In Proceedings of
the ACL, 499?506.
32
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504?511,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Making Sense of Sound:
Unsupervised Topic Segmentation over Acoustic Input
Igor Malioutov, Alex Park, Regina Barzilay, and James Glass
Massachusetts Institute of Technology
{igorm,malex,regina,glass}@csail.mit.edu
Abstract
We address the task of unsupervised topic
segmentation of speech data operating over
raw acoustic information. In contrast to ex-
isting algorithms for topic segmentation of
speech, our approach does not require in-
put transcripts. Our method predicts topic
changes by analyzing the distribution of re-
occurring acoustic patterns in the speech sig-
nal corresponding to a single speaker. The
algorithm robustly handles noise inherent in
acoustic matching by intelligently aggregat-
ing information about the similarity profile
from multiple local comparisons. Our ex-
periments show that audio-based segmen-
tation compares favorably with transcript-
based segmentation computed over noisy
transcripts. These results demonstrate the
desirability of our method for applications
where a speech recognizer is not available,
or its output has a high word error rate.
1 Introduction
An important practical application of topic segmen-
tation is the analysis of spoken data. Paragraph
breaks, section markers and other structural cues
common in written documents are entirely missing
in spoken data. Insertion of these structural markers
can benefit multiple speech processing applications,
including audio browsing, retrieval, and summariza-
tion.
Not surprisingly, a variety of methods for
topic segmentation have been developed in the
past (Beeferman et al, 1999; Galley et al, 2003;
Dielmann and Renals, 2005). These methods typi-
cally assume that a segmentation algorithm has ac-
cess not only to acoustic input, but also to its tran-
script. This assumption is natural for applications
where the transcript has to be computed as part of the
system output, or it is readily available from other
system components. However, for some domains
and languages, the transcripts may not be available,
or the recognition performance may not be adequate
to achieve reliable segmentation. In order to process
such data, we need a method for topic segmentation
that does not require transcribed input.
In this paper, we explore a method for topic seg-
mentation that operates directly on a raw acoustic
speech signal, without using any input transcripts.
This method predicts topic changes by analyzing the
distribution of reoccurring acoustic patterns in the
speech signal corresponding to a single speaker. In
the same way that unsupervised segmentation algo-
rithms predict boundaries based on changes in lexi-
cal distribution, our algorithm is driven by changes
in the distribution of acoustic patterns. The central
hypothesis here is that similar sounding acoustic se-
quences produced by the same speaker correspond
to similar lexicographic sequences. Thus, by ana-
lyzing the distribution of acoustic patterns we could
approximate a traditional content analysis based on
the lexical distribution of words in a transcript.
Analyzing high-level content structure based on
low-level acoustic features poses interesting compu-
tational and linguistic challenges. For instance, we
need to handle the noise inherent in matching based
on acoustic similarity, because of possible varia-
504
tions in speaking rate or pronunciation. Moreover,
in the absence of higher-level knowledge, informa-
tion about word boundaries is not always discernible
from the raw acoustic input. This causes problems
because we have no obvious unit of comparison. Fi-
nally, noise inherent in the acoustic matching pro-
cedure complicates the detection of distributional
changes in the comparison matrix.
The algorithm presented in this paper demon-
strates the feasibility of topic segmentation over raw
acoustic input corresponding to a single speaker. We
first apply a variant of the dynamic time warping al-
gorithm to find similar fragments in the speech input
through alignment. Next, we construct a compari-
son matrix that aggregates the output of the align-
ment stage. Since aligned utterances are separated
by gaps and differ in duration, this representation
gives rise to sparse and irregular input. To obtain ro-
bust similarity change detection, we invoke a series
of transformations to smooth and refine the compar-
ison matrix. Finally, we apply the minimum-cut seg-
mentation algorithm to the transformed comparison
matrix to detect topic boundaries.
We compare the performance of our method
against traditional transcript-based segmentation al-
gorithms. As expected, the performance of the lat-
ter depends on the accuracy of the input transcript.
When a manual transcription is available, the gap
between audio-based segmentation and transcript-
based segmentation is substantial. However, in
a more realistic scenario when the transcripts are
fraught with recognition errors, the two approaches
exhibit similar performance. These results demon-
strate that audio-based algorithms are an effective
and efficient solution for applications where tran-
scripts are unavailable or highly errorful.
2 Related Work
Speech-based Topic Segmentation A variety of
supervised and unsupervised methods have been
employed to segment speech input. Some of these
algorithms have been originally developed for pro-
cessing written text (Beeferman et al, 1999). Others
are specifically adapted for processing speech input
by adding relevant acoustic features such as pause
length and speaker change (Galley et al, 2003; Diel-
mann and Renals, 2005). In parallel, researchers ex-
tensively study the relationship between discourse
structure and intonational variation (Hirschberg and
Nakatani, 1996; Shriberg et al, 2000). However,
all of the existing segmentation methods require as
input a speech transcript of reasonable quality. In
contrast, the method presented in this paper does
not assume the availability of transcripts, which pre-
vents us from using segmentation algorithms devel-
oped for written text.
At the same time, our work is closely related to
unsupervised approaches for text segmentation. The
central assumption here is that sharp changes in lex-
ical distribution signal the presence of topic bound-
aries (Hearst, 1994; Choi et al, 2001). These ap-
proaches determine segment boundaries by identi-
fying homogeneous regions within a similarity ma-
trix that encodes pairwise similarity between textual
units, such as sentences. Our segmentation algo-
rithm operates over a distortion matrix, but the unit
of comparison is the speech signal over a time in-
terval. This change in representation gives rise to
multiple challenges related to the inherent noise of
acoustic matching, and requires the development of
new methods for signal discretization, interval com-
parison and matrix analysis.
Pattern Induction in Acoustic Data Our work
is related to research on unsupervised lexical acqui-
sition from continuous speech. These methods aim
to infer vocabulary from unsegmented audio streams
by analyzing regularities in pattern distribution (de
Marcken, 1996; Brent, 1999; Venkataraman, 2001).
Traditionally, the speech signal is first converted into
a string-like representation such as phonemes and
syllables using a phonetic recognizer.
Park and Glass (2006) have recently shown the
feasibility of an audio-based approach for word dis-
covery. They induce the vocabulary from the au-
dio stream directly, avoiding the need for phonetic
transcription. Their method can accurately discover
words which appear with high frequency in the au-
dio stream. While the results obtained by Park and
Glass inspire our approach, we cannot directly use
their output as proxies for words in topic segmen-
tation. Many of the content words occurring only
a few times in the text are pruned away by this
method. Our results show that this data that is too
sparse and noisy for robustly discerning changes in
505
lexical distribution.
3 Algorithm
The audio-based segmentation algorithm identifies
topic boundaries by analyzing changes in the dis-
tribution of acoustic patterns. The analysis is per-
formed in three steps. First, we identify recurring
patterns in the audio stream and compute distortion
between them (Section 3.1). These acoustic patterns
correspond to high-frequency words and phrases,
but they only cover a fraction of the words that ap-
pear in the input. As a result, the distributional pro-
file obtained during this process is too sparse to de-
liver robust topic analysis. Second, we generate an
acoustic comparison matrix that aggregates infor-
mation from multiple pattern matches (Section 3.2).
Additional matrix transformations during this step
reduce the noise and irregularities inherent in acous-
tic matching. Third, we partition the matrix to iden-
tify segments with a homogeneous distribution of
acoustic patterns (Section 3.3).
3.1 Comparing Acoustic Patterns
Given a raw acoustic waveform, we extract a set of
acoustic patterns that occur frequently in the speech
document. Continuous speech includes many word
sequences that lack clear low-level acoustic cues to
denote word boundaries. Therefore, we cannot per-
form this task through simple counting of speech
segments separated by silence. Instead, we use a lo-
cal alignment algorithm to search for similar speech
segments and quantify the amount of distortion be-
tween them. In what follows, we first present a vec-
tor representation used in this computation, and then
specify the alignment algorithm that finds similar
segments.
MFCC Representation We start by transforming
the acoustic signal into a vector representation that
facilitates the comparison of acoustic sequences.
First, we perform silence detection on the original
waveform by registering a pause if the energy falls
below a certain threshold for a duration of 2s. This
enables us to break up the acoustic stream into con-
tinuous spoken utterances.
This step is necessary as it eliminates spurious
alignments between silent regions of the acoustic
waveform. Note that silence detection is not equiv-
alent to word boundary detection, as segmentation
by silence detection alone only accounts for 20% of
word boundaries in our corpus.
Next, we convert each utterance into a time se-
ries of vectors consisting of Mel-scale cepstral co-
efficients (MFCCs). This compact low-dimensional
representation is commonly used in speech process-
ing applications because it approximates human au-
ditory models.
The process of extracting MFCCs from the speech
signal can be summarized as follows. First, the 16
kHz digitized audio waveform is normalized by re-
moving the mean and scaling the peak amplitude.
Next, the short-time Fourier transform is taken at
a frame interval of 10 ms using a 25.6 ms Ham-
ming window. The spectral energy from the Fourier
transform is then weighted by Mel-frequency fil-
ters (Huang et al, 2001). Finally, the discrete cosine
transform of the log of these Mel-frequency spec-
tral coefficients is computed, yielding a series of 14-
dimensional MFCC vectors. We take the additional
step of whitening the feature vectors, which normal-
izes the variance and decorrelates the dimensions of
the feature vectors (Bishop, 1995). This whitened
spectral representation enables us to use the stan-
dard unweighted Euclidean distance metric. After
this transformation, the distances in each dimension
will be uncorrelated and have equal variance.
Alignment Now, our goal is to identify acoustic
patterns that occur multiple times in the audio wave-
form. The patterns may not be repeated exactly, but
will most likely reoccur in varied forms. We capture
this information by extracting pairs of patterns with
an associated distortion score. The computation is
performed using a sequence alignment algorithm.
Table 1 shows examples of alignments automati-
cally computed by our algorithm. The correspond-
ing phonetic transcriptions1 demonstrate that the
matching procedure can robustly handle variations
in pronunciations. For example, two instances of the
word ?direction? are matched to one another despite
different pronunciations, (?d ay? vs. ?d ax? in the
first syllable). At the same time, some aligned pairs
form erroneous matches, such as ?my prediction?
matching ?y direction? due to their high acoustic
1Phonetic transcriptions are not used by our algorithm and
are provided for illustrative purposes only.
506
Aligned Word(s) Phonetic Transcription
the x direction dh iy eh kcl k s dcl d ax r eh kcl sh ax n
D iy Ek^k s d^d @r Ek^S@n
the y direction dh ax w ay dcl d ay r eh kcl sh epi en
D @w ay d^ay r Ek^k S@n
of my prediction ax v m ay kcl k r iy l iy kcl k sh ax n
@v m ay k^k r iy l iy k^k S@n
acceleration eh kcl k s eh l ax r ey sh epi en
Ek^k s El @r Ey S- n
"
acceleration ax kcl k s ah n ax r eh n epi sh epi en
@k^k s 2n @r En - S- n
"
the derivation dcl d ih dx ih z dcl dh ey sh epi en
d^d IRIz d^D Ey S- n
"
a demonstration uh dcl d eh m ax n epi s tcl t r ey sh en
Ud^d Em @n - s t^t r Ey Sn
"
Table 1: Aligned Word Paths. Each group of rows
represents audio segments that were aligned to one
another, along with their corresponding phonetic
transcriptions using TIMIT conventions (Garofolo et
al., 1993) and their IPA equivalents.
similarity.
The alignment algorithm operates on the audio
waveform represented by a list of silence-free utter-
ances (u1, u2, . . . , un). Each utterance u? is a time
series of MFCC vectors ( ~x?1, ~x?2, . . . , ~x?m). Given
two input utterances u? and u??, the algorithm out-
puts a set of alignments between the corresponding
MFCC vectors. The alignment distortion score is
computed by summing the Euclidean distances of
matching vectors.
To compute the optimal alignment we use a vari-
ant of the dynamic time warping algorithm (Huang
et al, 2001). For every possible starting alignment
point, we optimize the following dynamic program-
ming objective:
D(ik, jk) = d(ik, jk) + min
?
?
?
?
?
D(ik ? 1, jk)
D(ik, jk ? 1)
D(ik ? 1, jk ? 1)
In the equation above, ik and jk are alignment end-
points in the k-th subproblem of dynamic program-
ming.
This objective corresponds to a descent through
a dynamic programming trellis by choosing right,
down, or diagonal steps at each stage.
During the search process, we consider not only
the alignment distortion score, but also the shape of
the alignment path. To limit the amount of temporal
warping, we enforce the following constraint:
?
?
(
ik ? i1
)
?
(
jk ? j1
)
?
? ? R,?k, (1)
ik ? Nx and jk ? Ny,
where Nx and Ny are the number of MFCC samples
in each utterance. The value 2R + 1 is the width of
the diagonal band that controls the extent of tempo-
ral warping. The parameter R is tuned on a develop-
ment set.
This alignment procedure may produce paths with
high distortion subpaths. Therefore, we trim each
path to retain the subpath with lowest average dis-
tortion and length at least L. More formally, given
an alignment of length N , we seek to find m and n
such that:
arg min
1?m?n?N
1
n ? m + 1
n
?
k=m
d(ik, jk) n?m ? L
We accomplish this by computing the length con-
strained minimum average distortion subsequence
of the path sequence using an O(N log(L)) algo-
rithm proposed by Lin et al(2002). The length
parameter, L, allows us to avoid overtrimming and
control the length of alignments that are found. Af-
ter trimming, the distortion of each alignment path
is normalized by the path length.
Alignments with a distortion exceeding a prespec-
ified threshold are pruned away to ensure that the
aligned phrasal units are close acoustic matches.
This parameter is tuned on a development set.
In the next section, we describe how to aggregate
information from multiple noisy matches into a rep-
resentation that facilitates boundary detection.
3.2 Construction of Acoustic Comparison
Matrix
The goal of this step is to construct an acoustic com-
parison matrix that will guide topic segmentation.
This matrix encodes variations in the distribution of
acoustic patterns for a given speech document. We
construct this matrix by first discretizing the acoustic
signal into constant-length blocks and then comput-
ing the distortion between pairs of blocks.
507
Figure 1: a) Similarity matrix for a Physics lecture constructed using a manual transcript. b) Similarity
matrix for the same lecture constructed from acoustic data. The intensity of a pixel indicates the degree of
block similarity. c) Acoustic comparison matrix after 2000 iterations of anisotropic diffusion. Vertical lines
correspond to the reference segmentation.
Unfortunately, the paths and distortions generated
during the alignment step (Section 3.1) cannot be
mapped directly to an acoustic comparison matrix.
Since we compare only commonly repeated acous-
tic patterns, some portions of the signal correspond
to gaps between alignment paths. In fact, in our cor-
pus only 67% of the data is covered by alignment
paths found during the alignment stage. Moreover,
many of these paths are not disjoint. For instance,
our experiments show that 74% of them overlap with
at least one additional alignment path. Finally, these
alignments vary significantly in duration, ranging
from 0.350 ms to 2.7 ms in our corpus.
Discretization and Distortion Computation To
compensate for the irregular distribution of align-
ment paths, we quantize the data by splitting the in-
put signal into uniform contiguous time blocks. A
time block does not necessarily correspond to any
one discovered alignment path. It may contain sev-
eral complete paths and also portions of other paths.
We compute the aggregate distortion score D(x, y)
of two blocks x and y by summing the distortions of
all alignment paths that fall within x and y.
Matrix Smoothing Equipped with a block dis-
tortion measure, we can now construct an acoustic
comparison matrix. In principle, this matrix can be
processed employing standard methods developed
for text segmentation. However, as Figure 1 illus-
trates, the structure of the acoustic matrix is quite
different from the one obtained from text. In a tran-
script similarity matrix shown in Figure 1 a), refer-
ence boundaries delimit homogeneous regions with
high internal similarity. On the other hand, looking
at the acoustic similarity matrix2 shown in Figure 1
b), it is difficult to observe any block structure cor-
responding to the reference segmentation.
This deficiency can be attributed to the sparsity of
acoustic alignments. Consider, for example, the case
when a segment is interspersed with blocks that con-
tain very few or no complete paths. Even though the
rest of the blocks in the segment could be closely
related, these path-free blocks dilute segment homo-
geneity. This is problematic because it is not always
possible to tell whether a sudden shift in scores sig-
nifies a transition or if it is just an artifact of irreg-
ularities in acoustic matching. Without additional
matrix processing, these irregularities will lead the
system astray.
We further refine the acoustic comparison matrix
using anisotropic diffusion. This technique has been
developed for enhancing edge detection accuracy in
image processing (Perona and Malik, 1990), and has
been shown to be an effective smoothing method in
text segmentation (Ji and Zha, 2003). When ap-
plied to a comparison matrix, anisotropic diffusion
reduces score variability within homogeneous re-
2We converted the original comparison distortion matrix to
the similarity matrix by subtracting the component distortions
from the maximum alignment distortion score.
508
gions of the matrix and makes edges between these
regions more pronounced. Consequently, this trans-
formation facilitates boundary detection, potentially
increasing segmentation accuracy. In Figure 1 c), we
can observe that the boundary structure in the dif-
fused comparison matrix becomes more salient and
corresponds more closely to the reference segmen-
tation.
3.3 Matrix Partitioning
Given a target number of segments k, the goal of
the partitioning step is to divide a matrix into k
square submatrices along the diagonal. This pro-
cess is guided by an optimization function that max-
imizes the homogeneity within a segment or mini-
mizes the homogeneity across segments. This opti-
mization problem can be solved using one of many
unsupervised segmentation approaches (Choi et al,
2001; Ji and Zha, 2003; Malioutov and Barzilay,
2006).
In our implementation, we employ the minimum-
cut segmentation algorithm (Shi and Malik, 2000;
Malioutov and Barzilay, 2006). In this graph-
theoretic framework, segmentation is cast as a prob-
lem of partitioning a weighted undirected graph
that minimizes the normalized-cut criterion. The
minimum-cut method achieves robust analysis by
jointly considering all possible partitionings of a
document, moving beyond localized decisions. This
allows us to aggregate comparisons from multiple
locations, thereby compensating for the noise of in-
dividual matches.
4 Evaluation Set-Up
Data We use a publicly available3 corpus of intro-
ductory Physics lectures described in our previous
work (Malioutov and Barzilay, 2006). This mate-
rial is a particularly appealing application area for an
audio-based segmentation algorithm ? many aca-
demic subjects lack transcribed data for training,
while a high ratio of in-domain technical terms lim-
its the use of out-of-domain transcripts. This corpus
is also challenging from the segmentation perspec-
tive because the lectures are long and transitions be-
tween topics are subtle.
3See http://www.csail.mit.edu/?igorm/
acl06.html
The corpus consists of 33 lectures, with an aver-
age length of 8500 words and an average duration
of 50 minutes. On average, a lecture was anno-
tated with six segments, and a typical segment cor-
responds to two pages of a transcript. Three lectures
from this set were used for development, and 30 lec-
tures were used for testing. The lectures were deliv-
ered by the same speaker.
To evaluate the performance of traditional
transcript-based segmentation algorithms on this
corpus, we also use several types of transcripts at
different levels of recognition accuracy. In addi-
tion to manual transcripts, our corpus contains two
types of automatic transcripts, one obtained using
speaker-dependent (SD) models and the other ob-
tained using speaker-independent (SI) models. The
speaker-independent model was trained on 85 hours
of out-of-domain general lecture material and con-
tained no speech from the speaker in the test set.
The speaker-dependent model was trained by us-
ing 38 hours of audio data from other lectures given
by the speaker. Both recognizers incorporated word
statistics from the accompanying class textbook into
the language model. The word error rates for the
speaker-independent and speaker-dependent models
are 44.9% and 19.4%, respectively.
Evaluation Metrics We use the Pk and WindowD-
iff measures to evaluate our system (Beeferman et
al., 1999; Pevzner and Hearst, 2002). The Pk mea-
sure estimates the probability that a randomly cho-
sen pair of words within a window of length k words
is inconsistently classified. The WindowDiff met-
ric is a variant of the Pk measure, which penalizes
false positives and near misses equally. For both of
these metrics, lower scores indicate better segmen-
tation accuracy.
Baseline We use the state-of-the-art mincut seg-
mentation system by Malioutov and Barzilay (2006)
as our point of comparison. This model is an appro-
priate baseline, because it has been shown to com-
pare favorably with other top-performing segmenta-
tion systems (Choi et al, 2001; Utiyama and Isa-
hara, 2001). We use the publicly available imple-
mentation of the system.
As additional points of comparison, we test the
uniform and random baselines. These correspond
to segmentations obtained by uniformly placing
509
Pk WindowDiff
MAN 0.298 0.311
SD 0.340 0.351
AUDIO 0.358 0.370
SI 0.378 0.390
RAND 0.472 0.497
UNI 0.476 0.484
Table 2: Segmentation accuracy for audio-based
segmentor (AUDIO), random (RAND), uniform
(UNI) and three transcript-based segmentation algo-
rithms that use manual (MAN), speaker-dependent
(SD) and speaker-independent (SI) transcripts. For
all of the algorithms, the target number of segments
is set to the reference number of segments.
boundaries along the span of the lecture and select-
ing random boundaries, respectively.
To control for segmentation granularity, we spec-
ify the number of segments in the reference segmen-
tation for both our system and the baselines.
Parameter Tuning We tuned the number of quan-
tized blocks, the edge cutoff parameter of the min-
imum cut algorithm, and the anisotropic diffusion
parameters on a heldout set of three development
lectures. We used the same development set for the
baseline segmentation systems.
5 Results
The goal of our evaluation experiments is two-fold.
First, we are interested in understanding the condi-
tions in which an audio-based segmentation is ad-
vantageous over a transcript-based one. Second, we
aim to analyze the impact of various design deci-
sions on the performance of our algorithm.
Comparison with Transcript-Based Segmenta-
tion Table 2 shows the segmentation accuracy
of the audio-based segmentation algorithm and three
transcript-based segmentors on the set of 30 Physics
lectures. Our algorithm yields an average Pk mea-
sure of 0.358 and an average WindowDiff mea-
sure of 0.370. This result is markedly better than
the scores attained by uniform and random seg-
mentations. As expected, the best segmentation re-
sults are obtained using manual transcripts. How-
ever, the gap between audio-based segmentation
and transcript-based segmentation narrows when the
recognition accuracy decreases. In fact, perfor-
mance of the audio-based segmentation beats the
transcript-based segmentation baseline obtained us-
ing speaker-independent (SI) models (0.358 for AU-
DIO versus Pk measurements of 0.378 for SI).
Analysis of Audio-based Segmentation A cen-
tral challenge in audio-based segmentation is how to
overcome the noise inherent in acoustic matching.
We addressed this issue by using anisotropic diffu-
sion to refine the comparison matrix. We can quan-
tify the effects of this smoothing technique by gener-
ating segmentations directly from the similarity ma-
trix. We obtain similarities from the distortions in
the comparison matrix by subtracting the distortion
scores from the maximum distortion:
S(x, y) = max
si,sj
[D(si, sj)] ? D(x, y)
Using this matrix with the min-cut algorithm, seg-
mentation accuracy drops to a Pk measure of 0.418
(0.450 WindowDiff). This difference in perfor-
mance shows that anisotropic diffusion compensates
for noise introduced during acoustic matching.
An alternative solution to the problem of irregu-
larities in audio-based matching is to compute clus-
ters of acoustically similar utterances. Each of the
derived clusters can be thought of as a unique word
type.4 We compute these clusters, employing a
method for unsupervised vocabulary induction de-
veloped by Park and Glass (2006). Using the out-
put of their algorithm, the continuous audio stream
is transformed into a sequence of word-like units,
which in turn can be segmented using any stan-
dard transcript-based segmentation algorithm, such
as the minimum-cut segmentor. On our corpus, this
method achieves disappointing results ? a Pk mea-
sure of 0.423 (0.424 WindowDiff). The result can
be attributed to the sparsity of clusters5 generated by
this method, which focuses primarily on discovering
the frequently occurring content words.
6 Conclusion and Future Work
We presented an unsupervised algorithm for audio-
based topic segmentation. In contrast to existing
4In practice, a cluster can correspond to a phrase, word, or
word fragment (See Table 1 for examples).
5We tuned the number of clusters on the development set.
510
algorithms for speech segmentation, our approach
does not require an input transcript. Thus, it can
be used in domains where a speech recognizer is
not available or its output is too noisy. Our ap-
proach approximates the distribution of cohesion
ties by considering the distribution of acoustic pat-
terns. Our experimental results demonstrate the util-
ity of this approach: audio-based segmentation com-
pares favorably with transcript-based segmentation
computed over noisy transcripts.
The segmentation algorithm presented in this pa-
per focuses on one source of linguistic information
for discourse analysis ? lexical cohesion. Multiple
studies of discourse structure, however, have shown
that prosodic cues are highly predictive of changes
in topic structure (Hirschberg and Nakatani, 1996;
Shriberg et al, 2000). In a supervised framework,
we can further enhance audio-based segmentation
by combining features derived from pattern analy-
sis with prosodic information. We can also explore
an unsupervised fusion of these two sources of in-
formation; for instance, we can induce informative
prosodic cues by using distributional evidence.
Another interesting direction for future research
lies in combining the results of noisy recogni-
tion with information obtained from distribution of
acoustic patterns. We hypothesize that these two
sources provide complementary information about
the audio stream, and therefore can compensate for
each other?s mistakes. This combination can be par-
ticularly fruitful when processing speech documents
with multiple speakers or background noise.
7 Acknowledgements
The authors acknowledge the support of the Microsoft Faculty
Fellowship and the National Science Foundation (CAREER
grant IIS-0448168, grant IIS-0415865, and the NSF Graduate
Fellowship). Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of the au-
thor(s) and do not necessarily reflect the views of the National
Science Foundation. We would like to thank T.J. Hazen for
his assistance with the speech recognizer and to acknowledge
Tara Sainath, Natasha Singh, Ben Snyder, Chao Wang, Luke
Zettlemoyer and the three anonymous reviewers for their valu-
able comments and suggestions.
References
D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 34(1-3):177?
210.
C. Bishop, 1995. Neural Networks for Pattern Recognition,
pg. 38. Oxford University Press, New York, 1995.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34(1-3):71?105.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic
analysis for text segmentation. In Proceedings of EMNLP,
109?117.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, Massachusetts Institute of Technology.
A. Dielmann, S. Renals. 2005. Multistream dynamic Bayesian
network for meeting segmentation. In Proceedings Mul-
timodal Interaction and Related Machine Learning Algo-
rithms Workshop (MLMI?04), 76?86.
M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003.
Discourse segmentation of multi-party conversation. In Pro-
ceedings of the ACL, 562?569.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,
N. Dahlgren, V. Zue. 1993. TIMIT Acoustic-Phonetic Con-
tinuous Speech Corpus. Linguistic Data Consortium, 1993.
M. Hearst. 1994. Multi-paragraph segmentation of expository
text. In Proceedings of the ACL, 9?16.
J. Hirschberg, C. H. Nakatani. 1996. A prosodic analysis of
discourse segments in direction-giving monologues. In Pro-
ceedings of the ACL, 286?293.
X. Huang, A. Acero, H.-W. Hon. 2001. Spoken Language Pro-
cessing. Prentice Hall.
X. Ji, H. Zha. 2003. Domain-independent text segmentation
using anisotropic diffusion and dynamic programming. In
Proceedings of SIGIR, 322?329.
Y.-L. Lin, T. Jiang, K.-M. Chao. 2002. Efficient algorithms
for locating the length-constrained heaviest segments with
applications to biomolecular sequence analysis. J. Computer
and System Sciences, 65(3):570?586.
I. Malioutov, R. Barzilay. 2006. Minimum cut model for
spoken lecture segmentation. In Proceedings of the COL-
ING/ACL, 25?32.
A. Park, J. R. Glass. 2006. Unsupervised word acquisition from
speech using pattern discovery. In Proceedings of ICASSP.
P. Perona, J. Malik. 1990. Scale-space and edge detection using
anisotropic diffusion. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 12(7):629?639.
L. Pevzner, M. Hearst. 2002. A critique and improvement of
an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.
J. Shi, J. Malik. 2000. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8):888?905.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, 32(1-2):127?
154.
M. Utiyama, H. Isahara. 2001. A statistical model for domain-
independent text segmentation. In Proceedings of the ACL,
499?506.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):353?372.
511
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321?1330,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Acquisition and Probabilistic Models: keeping it simple
Aline Villavicencio?, Marco Idiart?Robert Berwick?, Igor Malioutov?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA)
? CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA)
avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br
berwick@csail.mit.edu, igorm@mit.edu
Abstract
Hierarchical Bayesian Models (HBMs)
have been used with some success
to capture empirically observed pat-
terns of under- and overgeneralization
in child language acquisition. How-
ever, as is well known, HBMs are
?ideal? learning systems, assuming ac-
cess to unlimited computational re-
sources that may not be available
to child language learners. Conse-
quently, it remains crucial to carefully
assess the use of HBMs along with al-
ternative, possibly simpler, candidate
models. This paper presents such
an evaluation for a language acquisi-
tion domain where explicit HBMs have
been proposed: the acquisition of En-
glish dative constructions. In particu-
lar, we present a detailed, empirically-
grounded model-selection compari-
son of HBMs vs. a simpler alternative
based on clustering along with max-
imum likelihood estimation that we
call linear competition learning (LCL).
Our results demonstrate that LCL can
match HBM model performance with-
out incurring on the high computa-
tional costs associated with HBMs.
1 Introduction
In recent years, with advances in probability
and estimation theory, there has been much
interest in Bayesian models (BMs) (Chater,
Tenenbaum, and Yuille, 2006; Jones and
Love, 2011) and their application to child lan-
guage acquisition with its challenging com-
bination of structured information and in-
complete knowledge, (Perfors, Tenenbaum,
and Wonnacott, 2010; Hsu and Chater, 2010;
Parisien, Fazly, and Stevenson, 2008; Parisien
and Stevenson, 2010) as they offer several ad-
vantages in this domain. They can readily
handle the evident noise and ambiguity of ac-
quisition input, while at the same time pro-
viding efficiency via priors that mirror known
pre-existing language biases. Further, hierar-
chical Bayesian Models (HBMs) can combine
distinct abstraction levels of linguistic knowl-
edge, from variation at the level of individ-
ual lexical items, to cross-item variation, using
hyper-parameters to capture observed pat-
terns of both under- and over-generalization
as in the acquisition of e.g. dative alterna-
tions in English (Hsu and Chater, 2010; Per-
fors, Tenenbaum, and Wonnacott, 2010), and
verb frames in a controlled artificial language
(Wonnacott, Newport, and Tanenhaus, 2008).
HBMs can thus be viewed as providing a
?rational? upper bound on language learn-
ability, yielding optimal models that account
for observed data while minimizing any re-
quired prior information. In addition, the
clustering implicit in HBM modeling intro-
duces additional parameters that can be tuned
to specific data patterns. However, this comes
at a well-known price: HBMs generally are
also ideal learning systems, known to be
computationally infeasible (Kwisthout, Ware-
ham, and van Rooij, 2011). Approximations
proposed to ensure computational tractabil-
ity, like reducing the number of classes that
need to be learned may also be linguisti-
cally and cognitively implausible. For in-
stance, in terms of verb learning, this could
1321
take the form of reducing the number of sub-
categorization frames to the relevant subset,
as in (Perfors, Tenenbaum, and Wonnacott,
2010), where only 2 frames are considered for
?take?, when in fact it is listed in 6 frames
by Levin (1993). Finally, comparison of vari-
ous Bayesian models of the same task is rare
(Jones and Love, 2011) and Bayesian infer-
ence generally can be demonstrated as sim-
ply one class of regularization or smooth-
ing techniques among many others; given the
problem at hand, there may well be other,
equally compelling regularization methods
for dealing with the bias-variance dilemma
(e.g., SVMs (Shalizi, 2009)). Consequently, the
relevance of HBMs for cognitively accurate ac-
counts of human learning remains uncertain
and needs to be carefully assessed.
Here we argue that the strengths of HBMs
for a given task must be evaluated in light of
their computational and cognitive costs, and
compared to other viable alternatives. The fo-
cus should be on finding the simplest statis-
tical models consistent with a given behav-
ior, particularly one that aligns with known
cognitive limitations. In the case of many
language acquisition tasks this behavior often
takes the form of overgeneralization, but with
eventual convergence to some target language
given exposure to more data.
In particular, in this paper we consider how
children acquire English dative verb construc-
tions, comparing HBMs to a simpler alterna-
tive, a linear competition learning (LCL) al-
gorithm that models the behavior of a given
verb as the linear competition between the ev-
idence for that verb, and the average behav-
ior of verbs belonging to its same class. The
results show that combining simple cluster-
ing methods along with ordinary maximum
likelihood estimation yields a result compara-
ble to HBM performance, providing an alter-
native account of the same facts, without the
computational costs incurred by HBM models
that must rely, for example, on Markov Chain
Monte Carlo (MCMC) methods for numeri-
cally integrating complex likelihood integrals,
or on Chinese Restaurant Process (CRP) for
producing partitions.
In terms of Marr?s hierarchy (Marr, 1982)
learning verb alternations is an abstract com-
putational problem (Marr?s type I), solvable
by many type II methods combining repre-
sentations (models, viz. HBMs or LCLs) with
particular algorithms. The HBM convention
of adopting ideal learning amounts to invok-
ing unbounded algorithmic resources, solv-
ability in principle, even though in practice
such methods, even approximate ones, are
provably NP-hard (cf. (Kwisthout, Wareham,
and van Rooij, 2011)). Assuming cognitive
plausibility as a desideratum, we therefore ex-
amine whether HBMs can also be approxi-
mated by another type II method (LCLs) that
does not demand such intensive computa-
tion. Any algorithm that approximates an
HBM can be viewed as implementing a some-
what different underlying model; if it repli-
cates HBM prediction performance but is sim-
pler and less computationally complex then
we assume it is preferable.
This paper is organized as follows: we start
with a discussion of formalizations of lan-
guage acquisition tasks, ?2. We present our
experimental framework for the dative acqui-
sition task, formalizing a range of learning
models from simple MLE methods to HBM
techniques, ?3, and a computational evalua-
tion of each model, ?4. We finish with conclu-
sions and possibilities for future work, ?5.
2 Evidence in Language Acquisition
A familiar problem for language acquisition is
how children learn which verbs participate in
so-called dative alternations, exemplified by
the child-produced sentences 1 to 3, from the
Brown (1973) corpus in CHILDES (MacWhin-
ney, 1995).
1. you took me three scrambled eggs (a direct object da-
tive (DOD) from Adam at age 3;6)
2. Mommy can you fix dis for me ? (a prepositional da-
tive (PD) from Adam at age 4;7)
3. *Mommy, fix me my tiger (from Adam at age 5;2)
Examples like these show that children gen-
eralize their use of verbs. For example, in sen-
tence (1), the child Adam uses take as a DOD
before any recorded occurrence of a similar
use of take in adult speech to Adam. Such
verbs alternate because they can also occur
with a prepositional form, as in sentence (2).
However, sometimes a child?s use of verbs like
1322
these amounts to an overgeneralization ? that
is, their productive use of a verb in a pattern
that does not occur in the adult grammar, as in
sentence (3), above. Faced with these two verb
frames the task for the learner is to decide for a
particular verb if it is a non-alternating DOD
only verb, a PD only verb, or an alternating
verb that allows both forms.
This ambiguity raises an important learn-
ability question, conventionally known as
Baker?s paradox (Baker, 1979). On the as-
sumption that children only receive positive
examples of verb forms, then it is not clear
how they might recover from the overgener-
alization exhibited in sentence (3) above, be-
cause they will never receive positive sen-
tences from adults like (3), using fix in a DOD
form. As has long been noted, if negative ex-
amples were systematically available to learn-
ers, then this problem would be solved, since
the child would be given evidence that the
DOD form is not possible in the adult gram-
mar. However, although parental correction
could be considered to be a source of negative
evidence, it is neither systematic nor generally
available to all children (Marcus, 1993). Even
when it does occur, all careful studies have in-
dicated that it seems mostly concerned with
semantic appropriateness rather than syntax.
In the cases where it is related to syntax, it
is often difficult to determine what the cor-
rection refers to in the utterance and besides
children seem to be oblivious to the correction
(Brown and Hanlon, 1970; Ingram, 1989).
One alternative solution to Baker?s paradox
that has been widely discussed at least since
Chomsky (1981) is the use of indirect negative
evidence. On the indirect negative evidence
model, if a verb is not found where it would
be expected to occur, the learner may con-
clude it is not part of the adult grammar. Cru-
cially, the indirect evidence model is inher-
ently statistical. Different formalizations of in-
direct negative evidence have been incorpo-
rated in several computational learning mod-
els for learning e.g. grammars (Briscoe, 1997;
Villavicencio, 2002; Kwiatkowski et al, 2010);
dative verbs (Perfors, Tenenbaum, and Won-
nacott, 2010; Hsu and Chater, 2010); and mul-
tiword verbs (Nematzadeh, Fazly, and Steven-
son, 2013). Since a number of closely related
models can all implement the indirect nega-
tive evidence approach, the decision of which
one to choose for a given task may not be en-
tirely clear. In this paper we compare a range
of statistical models consistent with a certain
behavior: early overgeneralization, with even-
tual convergence to the correct target on the
basis of exposure to more data.
3 Materials and Methods
3.1 Dative Corpora
To emulate a child language acquisition en-
vironment we use naturalistic longitudinal
child-directed data, from the Brown corpus in
CHILDES, for one child (Adam) for a subset
of 19 verbs in the DOD and PD verb frames,
figure 1. This dataset was originally reported
in Perfors, Tenenbaum, and Wonnacott (2010),
and longitudinal and incremental aspects to
acquisition are approximated by dividing the
data available into 5 incremental epochs (E1 to
E5 in the figures), where at the final epoch the
learner has seen the full corpus.
Model comparison requires a gold standard
database for acquisition, reporting which
frames have been learned for which verbs at
each stage, and how likely a child is of mak-
ing creative uses of a particular verb in a new
frame. An independent gold standard with
developmental information (e.g. Gropen et
al. (1989)) would clearly be ideal. Absent
this, a first step is demonstrating that sim-
pler alternative models can replicate HBM
performance on their own terms. Therefore,
the gold standard we use for evaluation is
the classification predicted by Perfors, Tenen-
baum, and Wonnacott (2010). The evaluations
reported in our analysis take into account in-
trinsic characteristics of each model in rela-
tion to the likelihoods of the verbs, to deter-
mine the extent to which the models go be-
yond the data they were exposed to, discussed
in section 2. Further, since it has been ar-
gued that very low frequency verbs may not
yet be firmly placed in a child?s lexicon (Yang,
2010; Gropen et al, 1989), at each epoch we
also impose a low-frequency threshold of 5
occurrences, considering only verbs that the
learner has seen at least 5 times. This use of a
low-frequency threshold for learning has ex-
tensive support in the literature for learning
1323
of all kinds in both human and non-human
animals, e.g. (Gallistel, 2002). A cut-off fre-
quency in this range has also commonly been
used in NLP tasks like POS tagging (Ratna-
parkhi, 1999).
3.2 The learners
We selected a set of representative statistical
models that are capable in principle of solv-
ing this classification task, ranging from what
is perhaps the simplest possible, a simple bi-
nomial, all the way to multi-level hierarchical
Bayesian approaches.
A Binomial distribution serves as the sim-
plest model for capturing the behavior of a
verb occurring in either DOD or PD frame.
Representing the probability of DOD as ?, af-
ter n occurrences of the verb the probability
that y of them are DOD is:
p( y| ?,n) =
(n
y
)
?y (1 ? ?)n?y (1)
Considering that p(y| ?,n) is the likelihood
in a Bayesian framework, the simplest and the
most intuitive estimator of ?, given y in n verb
occurrences, is the Maximum Likelihood Esti-
mator (MLE):
?MLE =
y
n (2)
?MLE is viable as a learning model in the sensethat its accuracy increases as the amount of ev-
idence for a verb grows (n ? ?), reflecting
the incremental, on-line character of language
learning. However, one well known limita-
tion of MLE is that it assigns zero probability
mass to unseen events. Ruling out events on
the grounds that they did not occur in a finite
data set early in learning may be too strong ?
though it should be noted that this is simply
one (overly strong) version of the indirect neg-
ative evidence position.
Again as is familiar, to overcome zero
count problem, models adopt one or another
method of smoothing to assign a small prob-
ability mass to unseen events. In a Bayesian
formulation, this amounts to assigning non-
zero probability mass to some set of priors;
smoothing also captures the notion of gener-
alization, making predictions about data that
has never been seen by the learner. In the
context of verb learning smoothing could be
based on several principles:
? an (innate) expectation as to how verbs in
general should behave;
? an acquired class-based expectation of
the behavior of a verb, based on its associ-
ation to similar but more frequent verbs.
The former can be readily implemented
in terms of prior probability estimates. As
we discuss below, class-based estimates arise
from one or another clustering method, and
can produce more accurate estimates for less
frequent verbs based on patterns already
learned for more frequent verbs in the same
class; see (Perfors, Tenenbaum, and Wonna-
cott, 2010). In this case, smoothing is a side-
effect of the behavior of a class as a whole.
When learning begins, the prior probability
is the only source of information for a learner
and, as such, dominates the value of the poste-
rior probability. However, in the large sample
limit, it is the likelihood that dominates the
posterior distribution regardless of the prior.
In Hierarchical Bayesian Models both effects
are naturally incorporated. The prior distri-
bution is structured as a chain of distributions
of parameters and hyper-parameters, and the
data may be divided into classes that share
some of the hyper-parameters, as defined be-
low for the case of a three levels model:
? ? Exponential(1)
? ? Exponential(1)
?k ? Exponential(?)
?k ? Beta(?, ?)
?ik ? Beta(?k?k, ?k(1 ? ?k))
yi|ni ? Binomial(?ik)
The indices refer to the possible hierarchies
among the hyper-parameters. ? and ? are in
the top, and they are shared by all verbs. Then
there are classes of different ?k, ?k, and theprobabilities for the DOD frame for the dif-
ferent verbs (?ik) are drawn according to theclasses k assigned to them. An estimate for
(?ik) for a given configuration of clusters isgiven by
1324
Figure 1: Verb tokens per epoch (E1 to E5)
Figure 2: Verb tokens ? 5 per epoch (E1 to E5)
where P(Y) is the evidence of the data,
the unnormalized posterior for the hyper-
parameters is
and the likelihood for ? and ? is
The Hierarchical Bayesian Model prediction
for?i is the average of the estimate?ikHBM overall possible partitions of the verbs in the task.
To simplify the notation we can write
?HBM = E
[ y + ??
n + ?
]
(3)
where in the expression E[. . . ] are included
the integrals described above and the average
of all possible class partitions. Due to this
complexity, in practice even small data sets re-
quire the use of MCMC methods, and statisti-
cal models for partitions, like CRP (Gelman et
al., 2003; Perfors, Tenenbaum, and Wonnacott,
2010). This complexity also calls into question
the cognitive fidelity of such approaches.
Eq.3 is particularly interesting because by
fixing? and ? (instead of averaging over them)
it is possible to deduce simpler (and classical)
models: MLE corresponds to ? = 0; the so
called ?add-one? smoothing (referred in this
paper as L1) corresponds to ? = 2 and ? = 1/2.
From Eq.3 it is also clear that if ? and ? (or
their distributions) are unchanged, as the evi-
dence of a verb grows (n??), the HBM esti-
mate approaches MLE?s, (?HBM ? ?MLE). Onthe other hand, when ? >> n, ?HBM ? ?, sothat ? can be interpreted as a prior value for ?
in the low frequency limit.
Following this reasoning, we propose an
alternative approach, a linear competition
learner (LCL), that explicitly models the be-
havior of a given verb as the linear competi-
tion between the evidence for the verb, and
the average behavior of verbs of the same
class. As clustering is defined independently
from parameter estimation, the advantages of
the proposed approach are twofold. First, it
is computationally much simpler, not requir-
ing approximations by Monte Carlo meth-
ods. Second, differently from HBMs where
the same attributes are used for clustering and
parameter estimation (in this case the DOD
and PD counts for each verb), in LCL cluster-
1325
ing may be done using more general contexts
that employ a variety of linguistic and envi-
ronmental attributes.
For LCL the prior and class-based informa-
tion are incorporated as:
?LCL =
yi + ?C?C
ni + ?C (4)
where ?C and ?C are defined via justifiableheuristic expressions dependent solely on the
statistics of the class attributed to each verb i.
The strength of the prior (?C) is a mono-tonic function of the number of elements (mC)in the class C, excluding the target verb vi.To approximate the gold standard behavior of
the HBM for this task (Perfors, Tenenbaum,
and Wonnacott, 2010) we chose the following
function for ?C:
?C = mC3/2(1 ?mC?1/5) + 0.1 (5)
with the strength of the prior for the LCL
model depending on the number of verbs in
the class, not on their frequency. Eq.5 was
chosen as a good fit to HBMs, without incur-
ring their complexity. The powers are simple
fractions, not arbitrary numbers. A best fit
was not attempted due to the lack of assess-
ment of how accurate HBMs are on real data.
The prior value (?C) is a smoothed estima-tion of the probability of DOD in a given class,
combining the evidence for all verbs in that
class:
?C =
YC + 1/2
NC + 1 (6)
in this case YC is the number of DOD occur-rences in the class, and NC the total numberof verb occurrences in the class, in both cases
excluding the target verb vi.The interpretation of these parameters is
as follows: ?C is the estimate of ? in the ab-sence of any data for a verb; and ?C controlsthe crossover between this estimate and MLE,
with a large ?C requiring a larger sample (ni)to overcome the bias given by ?C.For comparative purposes, in this paper we
examine alternative models for (a) probability
estimation and (b) clustering. The models are
the following:
? two models without clusters: MLE and
L1;
? two models where clusters are performed
independently: LCL and MLE??; and
? the full HBM described before.
MLE?? corresponds to replacing ?, ? in eq.3by their maximal likelihood values calculated
from P({yi,ni}i?k|?, ?) described before.For models without clustering, estimation
is based solely on the observed behavior of
verbs. With clustering, same-cluster verbs
share some parameters, influencing one an-
other. HBMs place distributions over pos-
sible clusters, with estimation derived from
averages over distributions. In HBMs, clus-
tering and probability estimation are calcu-
lated jointly. In the other models these two
estimates are calculated separately, permit-
ting ?plug-and-play? use of external cluster-
ing methods, like X-means (Pelleg and Moore,
2000)1. However, to further assess the impact
of cluster assignment on alternative model
performance, we also used the clusters that
maximize the evidence of the HBM for the
DOD and PD counts of the target verbs, and
we refer to these as Maximum Evidence (ME)
clusters. In MWE clusters, verbs are separated
into 3 classes: one if they have counts for both
frames; another for only the DOD frame; and
a final for only the PD frame.
4 Evaluation
The learning task consists of estimating the
probability that a given verb occurs in a partic-
ular frame, using previous occurrences as the
basis for this estimation. In this context, over-
generalization can be viewed as the model?s
predictions that a given verb seen only in one
frame (say, a PD) can also occur in the other
(say, a DOD) as well, and it decreases as the
learner receives more data. In one extreme
we have MLE, which does not overgeneralize,
and in the other the L1 model, which assigns
uniform probability for all unseen cases. The
other 3 models fall somewhere in between,
overgeneralizing beyond the observed data,
using the prior and class-based smoothing to
assign some (low) probability mass to an un-
seen verb-frame pair. The relevant models?
1Other clustering algorithms were also used; here
we report X-means results as representative of these
models. X-means is available from http://www.cs.
waikato.ac.nz/ml/weka/
1326
predictions for each of the target verbs in the
DOD frame, given the full corpus, are in fig-
ure 3. In either end of the figure are the verbs
that were attested in only one of the frames
(PD only at the left-hand end, and DOD only
at the right-hand end). For these verbs, LCL
and HBM exhibit similar behavior. When the
low-frequency threshold is applied, MLE??,HBM and LCL work equally well, figure 4.
Figure 4: Probability of verbs in DOD frame,
Low Frequency Threshold.
To examine how overgeneralization pro-
gresses during the course of learning as the
models were exposed to increasing amounts
of data, we used the corpus divided by cumu-
lative epochs, as described in ?3.1. For each
epoch, verbs seen in only one of the frames
were divided in 5 frequency bins, and the
models were assessed as to how much over-
generalization they displayed for each of these
verbs. Following Perfors, Tenenbaum, and
Wonnacott (2010) overgeneralization is calcu-
lated as the absolute difference between the
models predicted ? and ?MLE, for each of theepochs, figure 5, and for comparative pur-
poses their alternating/non-alternating clas-
sification is also adopted. For non-alternating
verbs, overgeneralization reflects the degree
of smoothing of each model. As expected, the
more frequent a verb is, the more confident
the model is in the indirect negative evidence
it has for that verb, and the less it overgeneral-
izes, shown in the lighter bars in all epochs. In
addition, the overall effect of larger amounts
of data are indicated by a reduction in over-
generalization epoch by epoch. The effects of
class-based smoothing can be assessed com-
paring L1, a model without clustering which
displays a constant degree of overgeneraliza-
tion regardless of the epoch, while HBM uses
a distribution over clusters and the other mod-
els X-means. If a low-frequency threshold is
applied, the differences between the models
decrease significantly and so does the degree
of overgeneralization in the models? predic-
tions, as shown in the 3 lighter bars in the fig-
ure.
Figure 5: Overgeneralization, per epoch, per
frequency bin, where 0.5 corresponds to the
maximum overgeneralization.
While the models differ somewhat in their
predictions, the quantitative differences need
to be assessed more carefully. To compare
the models and provide an overall difference
measure, we use the predictions of the more
complex model, HBM, as a baseline and then
calculate the difference between its predic-
tions and those of the other models. We
used three different measures for comparing
models, one for their standard difference; one
that prioritizes agreement for high frequency
verbs; and one that focuses more on low fre-
quency verbs.
The first measure, denoted Difference, cap-
tures a direct comparison between two mod-
els, M1 and M2 as the average prediction dif-ference among the verbs, and is defined as:
This measure treats all differences uniformly,
regardless of whether they relate to high or
low frequency verbs in the learning sample
(e.g. for bring with 150 counts and serve with
only 1 have the same weight). To focus on high
frequency verbs, we also define the Weighted
Difference between two models as:
Here we expect Dn < D since models tend to
1327
Figure 3: Probability of verbs in DOD frame.
agree as the amount of evidence for each verb
increases. Conversely, our third measure, de-
noted Inverted, prioritizes the agreement be-
tween two models on low frequency verbs, de-
fined as follows:
D1/n captures the degree of similarity in over-generalization between two models. The re-
sults of applying these three difference mea-
sures are shown in figure 6 for the relevant
models, where grey is for D(M1,M2), blackfor Dn(M1,M2) and white for D1/n(M1,M2).Given the probabilistic nature of Monte Carlo
methods, there is also a variation between dif-
ferent runs of the HBM model (HBM to HBM-
2), and this indicates that models that per-
form within these bounds can be considered
to be equivalent (e.g. HBMs and ME-MLE??for Weighted Difference, and the HBMs and
X-MLE?? for the Inverted Difference).
Comparing the prediction agreement, the
strong influence of clustering is clear: the
models that have compatible clusters have
similar performances. For instance, all the
models that adopt the ME clusters for the
data perform closest to HBMs. Moreover, the
weighted differences tend to be smaller than
0.01 and around 0.02 for the inverted differ-
ences. The results for these measures become
even closer in most cases when the low fre-
quency threshold is adopted, figure 7, as the
Figure 6: Model Comparisons.
Figure 7: Model Comparison - Low Frequency
Threshold.
0 5 10 15 20 25 30 35 40 45 500.5
0.6
0.7
0.8
0.9
1
number of examples
DO
D p
rob
abi
lity
 
 
MLE
L1
HBM
LCLMLE
L1
HBM
LCL
Figure 8: DOD probability evolution for mod-
els with increase in evidence
evidence reduces the influence of the prior.
To examine the decay of overgeneralization
with the increase in evidence for these mod-
els, two simulated scenarios are defined for a
single generic verb: one where the evidence
for DOD amounts to 75% of the data (dashed
lines) and in the other to 100% (solid lines),
figures 9 and 8. Unsurprisingly, the perfor-
mance of the models is dependent on the
amount of evidence available. This is a con-
sequence of the decrease in the influence of
the priors as the sample size increases in a rate
of 1/N, as shown in figure 9 for the decrease
in overgeneralization. Ultimately it is the ev-
1328
100 101 102
10?4
10?3
10?2
10?1
100
number of examples
ove
rge
ner
aliz
atio
n
 
 L1HBMLCLL1HBMLCL
Figure 9: Overgeneralization reduction with
increase in evidence
idence that dominates the posterior probabil-
ity. Although the Bayesian model exhibits fast
convergence, after 10 examples, the simpler
model L1 is only approximately 3% below the
Bayesian model in performance for scenario 1
and is still 90% accurate in scenario 2, figure 8.
These results suggest that while these mod-
els all differ slightly in the degree of overgen-
eralization for low frequency data and noise,
these differences are small, and as evidence
reaches approximately 10 examples per verb,
the overall performance for all models ap-
proaches that of MLE.
5 Conclusions and Future Work
HBMs have been successfully used for a
number of language acquisition tasks captur-
ing both patterns of under- and overgeneral-
ization found in child language acquisition.
Their (hyper)parameters provide robustness
for dealing with low frequency events, noise,
and uncertainty and a good fit to the data,
but this fidelity comes at the cost of complex
computation. Here we have examined HBMs
against computationally simpler approaches
to dative alternation acquisition, which imple-
ment the indirect negative approach. We also
advanced several measures for model com-
parison in order to quantify their agreement
to assist in the task of model selection. The re-
sults show that the proposed LCL model, in
particular, that combines class-based smooth-
ing with maximum likelihood estimation, ob-
tains results comparable to those of HBMs,
in a much simpler framework. Moreover,
when a cognitively-viable frequency thresh-
old is adopted, differences in the performance
of all models decrease, and quite rapidly ap-
proach the performance of MLE.
In this paper we used standard clustering
techniques grounded solely on verb counts to
enable comparison with previous work. How-
ever, a variety of additional linguistic and dis-
tributional features could be used for cluster-
ing verbs into more semantically motivated
classes, using a larger number of frames and
verbs. This will be examined in future work.
We also plan to investigate the use of cluster-
ing methods more targeted to language tasks
(Sun and Korhonen, 2009).
Acknowledgements
We would like to thank the support of
projects CAPES/COFECUB 707/11, CNPq
482520/2012-4, 478222/2011-4, 312184/2012-
3, 551964/2011-1 and 312077/2012-2. We also
want to thank Amy Perfors for kindly sharing
the input data.
References
Baker, Carl L. 1979. Syntactic Theory and the Pro-
jection Problem. Linguistic Inquiry, 10(4):533?
581.
Briscoe, Ted. 1997. Co-evolution of language and
the language acquisition device. In Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 418?427.
Morgan Kaufmann.
Brown, Roger. 1973. A first language: Ehe early
stages. Harvard University Press, Cambridge,
Massachusetts.
Brown, Roger and Camille Hanlon. 1970. Deriva-
tional complexity and the order of acquisition of
child?s speech. In J. Hays, editor, Cognition and
the Development of Language. NY: John Wiley.
Chater, Nick, Joshua B. Tenenbaum, and Alan
Yuille. 2006. Probabilistic models of cogni-
tion: where next? Trends in Cognitive Sciences,
10(7):292 ? 293.
Chomsky, Noam. 1981. Lectures on government and
binding. Mouton de Gruyter.
1329
Gallistel, Charles R. 2002. Frequency, contin-
gency, and the information processing theory of
conditioning. In P.Sedlmeier and T. Betsch, ed-
itors, Frequency processing and cognition. Oxford
University Press, pages 153?171.
Gelman, Andrew, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2003. Bayesian Data Analy-
sis, Second Edition (Chapman & Hall/CRC Texts in
Statistical Science). Chapman and Hall/CRC, 2
edition.
Gropen, Jess, Steve Pinker, Michael Hollander,
Richard Goldberg, and Ronald Wilson. 1989.
The learnability and acquisition of the dative al-
ternation in English. Language, 65(2):203?257.
Hsu, Anne S. and Nick Chater. 2010. The logi-
cal problem of language acquisition: A proba-
bilistic perspective. Cognitive Science, 34(6):972?
1016.
Ingram, David. 1989. First Language Acquisition:
Method, Description and Explanation. Cambridge
University Press.
Jones, Matt and Bradley C. Love. 2011. Bayesian
Fundamentalism or Enlightenment? On the ex-
planatory status and theoretical contributions
of Bayesian models of cognition. Behavioral and
Brain Sciences, 34(04):169?188.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2010. Induc-
ing probabilistic CCG grammars from logical
form with higher-order unification. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1223?1233.
Kwisthout, Johan, Todd Wareham, and Iris van
Rooij. 2011. Bayesian intractability is not an
ailment that approximation can cure. Cognitive
Science, 35(5):779?1007.
Levin, B. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
MacWhinney, Brian. 1995. The CHILDES project:
tools for analyzing talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Marcus, Gary F. 1993. Negative evidence in lan-
guage acquisition. Cognition, 46:53?85.
Marr, D. 1982. Vision. San Francisco, CA: W. H.
Freeman.
Nematzadeh, Aida, Afsaneh Fazly, and Suzanne
Stevenson. 2013. Child acquisition of multi-
word verbs: A computational investigation. In
A. Villavicencio, T. Poibeau, A. Korhonen, and
A. Alishahi, editors, Cognitive Aspects of Com-
putational Language Acquisition. Springer, pages
235?256.
Parisien, Christopher, Afsaneh Fazly, and Suzanne
Stevenson. 2008. An incremental bayesian
model for learning syntactic categories. In Pro-
ceedings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages
89?96, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Parisien, Christopher and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the 32nd
Annual Conference of the Cognitive Science Society.
Pelleg, Dan and Andrew Moore. 2000. X-means:
Extending k-means with efficient estimation of
the number of clusters. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 727?734, San Francisco. Morgan
Kaufmann.
Perfors, Amy, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, nega-
tive evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
(37):607?642.
Ratnaparkhi, Adwait. 1999. Learning to parse nat-
ural language with maximum entropy models.
Machine Learning, pages 151?175.
Shalizi, Cosma R. 2009. Dynamics of bayesian
updating with dependent data and misspeci-
fied models. ElectroCosmanic Journal of Statistics,
3:1039?1074.
Sun, Lin and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired se-
lectional preferences. In EMNLP, pages 638?
647.
Villavicencio, Aline. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, Computer Laboratory, University
of Cambridge.
Wonnacott, Elizabeth, Elissa L. Newport, and
Michael K. Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cogni-
tive Psychology, 56:165?209.
Yang, Charles. 2010. Three factors in language
variation. Lingua, 120:1160?1177.
1330
