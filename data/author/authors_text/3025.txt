Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108?116,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Coarse-to-Fine Syntactic Machine Translation
using Language Projections
Slav Petrov Aria Haghighi Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, aria42, klein}@eecs.berkeley.edu
Abstract
The intersection of tree transducer-based
translation models with n-gram language
models results in huge dynamic programs for
machine translation decoding. We propose a
multipass, coarse-to-fine approach in which
the language model complexity is incremen-
tally introduced. In contrast to previous order-
based bigram-to-trigram approaches, we fo-
cus on encoding-based methods, which use
a clustered encoding of the target language.
Across various encoding schemes, and for
multiple language pairs, we show speed-ups of
up to 50 times over single-pass decoding while
improving BLEU score. Moreover, our entire
decoding cascade for trigram language models
is faster than the corresponding bigram pass
alone of a bigram-to-trigram decoder.
1 Introduction
In the absence of an n-gram language model, decod-
ing a synchronous CFG translation model is very
efficient, requiring only a variant of the CKY al-
gorithm. As in monolingual parsing, dynamic pro-
gramming items are simply indexed by a source lan-
guage span and a syntactic label. Complexity arises
when n-gram language model scoring is added, be-
cause items must now be distinguished by their ini-
tial and final few target language words for purposes
of later combination. This lexically exploded search
space is a root cause of inefficiency in decoding, and
several methods have been suggested to combat it.
The approach most relevant to the current work is
Zhang and Gildea (2008), which begins with an ini-
tial bigram pass and uses the resulting chart to guide
a final trigram pass. Substantial speed-ups are ob-
tained, but computation is still dominated by the ini-
tial bigram pass. The key challenge is that unigram
models are too poor to prune well, but bigram mod-
els are already huge. In short, the problem is that
there are too many words in the target language. In
this paper, we propose a new, coarse-to-fine, mul-
tipass approach which allows much greater speed-
ups by translating into abstracted languages. That
is, rather than beginning with a low-order model of
a still-large language, we exploit language projec-
tions, hierarchical clusterings of the target language,
to effectively reduce the size of the target language.
In this way, initial passes can be very quick, with
complexity phased in gradually.
Central to coarse-to-fine language projection is
the construction of sequences of word clusterings
(see Figure 1). The clusterings are deterministic
mappings from words to clusters, with the property
that each clustering refines the previous one. There
are many choice points in this process, including
how these clusterings are obtained and how much
refinement is optimal for each pass. We demon-
strate that likelihood-based hierarchical EM train-
ing (Petrov et al, 2006) and cluster-based language
modeling methods (Goodman, 2001) are superior
to both rank-based and random-projection methods.
In addition, we demonstrate that more than two
passes are beneficial and show that our computa-
tion is equally distributed over all passes. In our
experiments, passes with less than 16-cluster lan-
guage models are most advantageous, and even a
single pass with just two word clusters can reduce
decoding time greatly.
108
To follow related work and to focus on the effects
of the language model, we present translation re-
sults under an inversion transduction grammar (ITG)
translation model (Wu, 1997) trained on the Eu-
roparl corpus (Koehn, 2005), described in detail in
Section 3, and using a trigram language model. We
show that, on a range of languages, our coarse-to-
fine decoding approach greatly outperforms base-
line beam pruning and bigram-to-trigram pruning on
time-to-BLEU plots, reducing decoding times by up
to a factor of 50 compared to single pass decoding.
In addition, coarse-to-fine decoding increases BLEU
scores by up to 0.4 points. This increase is a mixture
of improved search and subtly advantageous coarse-
to-fine effects which are further discussed below.
2 Coarse-to-Fine Decoding
In coarse-to-fine decoding, we create a series of ini-
tially simple but increasingly complex search prob-
lems. We then use the solutions of the simpler prob-
lems to prune the search spaces for more complex
models, reducing the total computational cost.
2.1 Related Work
Taken broadly, the coarse-to-fine approach is not
new to machine translation (MT) or even syntactic
MT. Many common decoder precomputations can
be seen as coarse-to-fine methods, including the A*-
like forward estimates used in the Moses decoder
(Koehn et al, 2007). In an ITG framework like
ours, Zhang and Gildea (2008) consider an approach
in which the results of a bigram pass are used as
an A* heuristic to guide a trigram pass. In their
two-pass approach, the coarse bigram pass becomes
computationally dominant. Our work differs in two
ways. First, we use posterior pruning rather than
A* search. Unlike A* search, posterior pruning
allows multipass methods. Not only are posterior
pruning methods simpler (for example, there is no
need to have complex multipart bounds), but they
can be much more effective. For example, in mono-
lingual parsing, posterior pruning methods (Good-
man, 1997; Charniak et al, 2006; Petrov and Klein,
2007) have led to greater speedups than their more
cautious A* analogues (Klein and Manning, 2003;
Haghighi et al, 2007), though at the cost of guaran-
teed optimality.
L
M
 
O
r
d
e
r
Bits in language model
the,report-NP-these,states
1
pi
2
3
2 3
the-NP-states0-NP-1 01-NP-10 010-NP-100
0,1-NP-0,1 01,10-NP-00,10 010,100-NP-000,100
...
...
?
Figure 2: Possible state projections pi for the target noun
phrase ?the report for these states? using the clusters
from Figure 1. The number of bits used to encode the tar-
get language vocabulary is varied along the x-axis. The
language model order is varied along the y-axis.
Second, we focus on an orthogonal axis of ab-
straction: the size of the target language. The in-
troduction of abstract languages gives better control
over the granularity of the search space and provides
a richer set of intermediate problems, allowing us
to adapt the level of refinement of the intermediate,
coarse passes to minimize total computation.
Beyond coarse-to-fine approaches, other related
approaches have also been demonstrated for syntac-
tic MT. For example, Venugopal et al (2007) con-
siders a greedy first pass with a full model followed
by a second pass which bounds search to a region
near the greedy results. Huang and Chiang (2007)
searches with the full model, but makes assumptions
about the the amount of reordering the language
model can trigger in order to limit exploration.
2.2 Language Model Projections
When decoding in a syntactic translation model with
an n-gram language model, search states are spec-
ified by a grammar nonterminal X as well as the
the n-1 left-most target side words ln?1, . . . , l1 and
right-most target side words r1, . . . , rn?1 of the gen-
erated hypothesis. We denote the resulting lexical-
ized state as ln?1, . . . , l1-X-r1, . . . , rn?1. Assum-
ing a vocabulary V and grammar symbol set G, the
state space size is up to |V |2(n?1)|G|, which is im-
mense for a large vocabulary when n > 1. We
consider two ways to reduce the size of this search
space. First, we can reduce the order of the lan-
guage model. Second, we can reduce the number
of words in the vocabulary. Both can be thought
of as projections of the search space to smaller ab-
109
these
one
we
they
the
a
that
for
states
report
of
to
also
been
will
must
0
1
00
01
000 001
010 011 100 101 110 111
10
11
Figure 1: An example of hierarchical clustering of target language vocabulary (see Section 4). Even with a small
number of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes.
stracted spaces. Figure 2 illustrates those two or-
thogonal axes of abstraction.
Order-based projections are simple. As shown
in Figure 2, they simply strip off the appropriate
words from each state, collapsing dynamic program-
ming items which are identical from the standpoint
of their left-to-right combination in the lower or-
der language model. However, having only order-
based projections is very limiting. Zhang and Gildea
(2008) found that their computation was dominated
by their bigram pass. The only lower-order pass
possible uses a unigram model, which provides no
information about the interaction of the language
model and translation model reorderings. We there-
fore propose encoding-based projections. These
projections reduce the size of the target language vo-
cabulary by deterministically projecting each target
language word to a word cluster. This projection ex-
tends to the whole search state in the obvious way:
assuming a bigram language model, the state l-X-r
projects to c(l)-X-c(r), where c(?) is the determin-
istic word-to-cluster mapping.
In our multipass approach, we will want a se-
quence c1 . . . cn of such projections. This requires a
hierarchical clustering of the target words, as shown
in Figure 1. Each word?s cluster membership can be
represented by an n-bit binary string. Each prefix of
length k declares that word?s cluster assignment at
the k-bit level. As we vary k, we obtain a sequence
of projections ck(?), each one mapping words to a
more refined clustering. When performing inference
in a k-bit projection, we replace the detailed original
language model over words with a coarse language
model LMk over the k-bit word clusters. In addition,
we replace the phrase table with a projected phrase
table, which further increases the speed of projected
passes. In Section 4, we describe the various clus-
tering schemes explored, as well as how the coarse
LMk are estimated.
2.3 Multipass Decoding
Unlike previous work, where the state space exists
only at two levels of abstraction (i.e. bigram and tri-
gram), we have multiple levels to choose from (Fig-
ure 2). Because we use both encoding-based and
order-based projections, our options form a lattice
of coarser state spaces, varying from extremely sim-
ple (a bigram model with just two word clusters) to
nearly the full space (a trigram model with 10 bits or
1024 word clusters).
We use this lattice to perform a series of coarse
passes with increasing complexity. More formally,
we decode a source sentence multiple times, in a
sequence of state spaces S0, S1, . . . , Sn=S, where
each Si is a refinement of Si?1 in either language
model order, language encoding size, or both. The
state spaces Si and Sj (i < j) are related to each
other via a projection operator pij?i(?) which maps
refined states deterministically to coarser states.
We start by decoding an input x in the simplest
state space S0. In particular, we compute the chart
of the posterior distributions p0(s) = P (s|x) for all
states s ? S0. These posteriors will be used to prune
the search space S1 of the following pass. States s
whose posterior falls below a threshold t trigger the
removal of all more refined states s? in the subse-
quent pass (see Figure 3). This technique is poste-
rior pruning, and is different from A* methods in
two main ways. First, it can be iterated in a multi-
pass setting, and, second, it is generally more effi-
110
0-X-0
11-X-10 10-X-11 11-X-1100-X-11 10-X-1011-X-01 01-X-1010-X-00 11-X-00 10-X-0100-X-00 01-X-00 00-X-01
1-X-0 0-X-1 1-X-1
2-Bit Pass
1-Bit Pass
 < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?
< t ?
< t ? < t ? < t ?
01-X-1100-X-1001-X-01
Figure 3: Example of state pruning in coarse-to-fine decoding using the language encoding projection (see Section 2.2).
During the coarse one-bit word cluster pass, two of the four possible states are pruned. Every extension of the pruned
one-bit states (indicated by the grey shading) are not explored during the two-bit word cluster pass.
cient with a potential cost of increased search errors
(see Section 2.1 for more discussion).
Looking at Figure 2, multipass coarse-to-fine de-
coding can be visualized as a walk from a coarse
point somewhere in the lower left to the most re-
fined point in the upper right of the grid. Many
coarse-to-fine schedules are possible. In practice,
we might start decoding with a 1-bit word bigram
pass, followed by an 3-bit word bigram pass, fol-
lowed by a 5-bit word trigram pass and so on (see
Section 5.3 for an empirical investigation). In terms
if time, we show that coarse-to-fine gives substantial
speed-ups. There is of course an additional mem-
ory requirement, but it is negligible. As we will see
in our experiments (Section 5) the largest gains can
be obtained with extremely coarse language mod-
els. In particular, the largest coarse model we use in
our best multipass decoder uses a 4-bit encoding and
hence has only 16 distinct words (or at most 4096
trigrams).
3 Inversion Transduction Grammars
While our approach applies in principle to a vari-
ety of machine translation systems (phrase-based or
syntactic), we will use the inversion transduction
grammar (ITG) approach of Wu (1997) to facili-
tate comparison with previous work (Zens and Ney,
2003; Zhang and Gildea, 2008) as well as to focus on
language model complexity. ITGs are a subclass of
synchronous context-free grammars (SCFGs) where
there are only three kinds of rules. Preterminal unary
productions produce terminal strings on both sides
(words or phrases): X ? e/f . Binary in-order pro-
ductions combine two phrases monotonically (X ?
[Y Z]). Finally, binary inverted productions invert
the order of their children (X ? ?Y Z?). These pro-
ductions are associated with rewrite weights in the
standard way.
Without a language model, SCFG decoding is just
like (monolingual) CFG parsing. The dynamic pro-
gramming states are specified by iXj , where ?i, j? is
a source sentence span and X is a nonterminal. The
only difference is that whenever we apply a CFG
production on the source side, we need to remem-
ber the corresponding synchronous production on
the target side and store the best obtainable transla-
tion via a backpointer. See Wu (1996) or Melamed
(2004) for a detailed exposition.
Once we integrate an n-gram language model, the
state space becomes lexicalized and combining dy-
namic programming items becomes more difficult.
Each state is now parametrized by the initial and
final n?1 words in the target language hypothesis:
ln?1, ..., l1-iXj-r1, ..., rn?1. Whenever we combine
two dynamic programming items, we need to score
the fluency of their concatentation by incorporat-
ing the score of any language model features which
cross the target side boundaries of the two concate-
nated items (Chiang, 2005). Decoding with an in-
tegrated language model is computationally expen-
sive for two reasons: (1) the need to keep track of
a large number of lexicalized hypotheses for each
source span, and (2) the need to frequently query the
large language model for each hypothesis combina-
tion.
Multipass coarse-to-fine decoding can alleviate
both computational issues. We start by decoding
in an extremely coarse bigram search space, where
there are very few possible translations. We com-
pute standard inside/outside probabilities (iS/oS),
as follows. Consider the application of non-inverted
binary rule: we combine two items lb-iBk-rb and
lc-kCj-rc spanning ?i, k? and ?k, j? respectively to
form a larger item lb-iAj-rc, spanning ?i, j?. The
111
lb-iAj -rc lb-iBk-rb lc-kCj-rc
rclb
+
lb rc
+=
iS(lb-iAj -rc) += iS(lb-iBk-rb) ? iS(lc-kCj-rc)LM(rb, lc) ?p(X?[Y Z]) ?
lcrb
Figure 4: Monotonic combination of two hypotheses dur-
ing the inside pass involves scoring the fluency of the con-
catenation with the language model.
inside score of the new item is incremented by:
iS(lb-iAj-rc) += p(X ? [Y Z]) ? iS(lb-iBk-rb) ?
iS(lc-kCj-rc) ? LM(rb, lc)
This process is also illustrated in Figure 4. Of
course, we also loop over the split point k and ap-
ply the other two rule types (inverted concatenation,
terminal generation). We omit those cases from this
exposition, as well as the update for the outside pass;
they are standard and similar. Once we have com-
puted the inside and outside scores, we compute pos-
terior probabilities for all items:
p(la-iAj-ra) =
iS(la-iAj-ra)oS(la-iAj-ra)
iS(root)
where iS(root) is sum of all translations? scores.
States with low posteriors are then pruned away.
We proceed to compute inside/outside score in the
next, more refined search space, using the projec-
tions pii?i?1 to map between states in Si and Si?1.
In each pass, we skip all items whose projection into
the previous stage had a probability below a stage-
specific threshold. This process is illustrated in Fig-
ure 3. When we reach the most refined search space
S?, we do not prune, but rather extract the Viterbi
derivation instead.1
4 Learning Coarse Languages
Central to our encoding-based projections (see Sec-
tion 2.2) are hierarchical clusterings of the tar-
get language vocabulary. In the present work,
these clusterings are each k-bit encodings and yield
sequences of coarse language models LMk and
phrasetables PTk.
1Other final decoding strategies are possible, of course, in-
cluding variational methods and minimum-risk methods (Zhang
and Gildea, 2008).
Given a hierarchical clustering, we estimate the
corresponding LMk from a corpus obtained by re-
placing each token in a target language corpus with
the appropriate word cluster. As with our original
refined language model, we estimate each coarse
language model using the SRILM toolkit (Stolcke,
2002). The phrasetables PTk are similarly estimated
by replacing the words on the target side of each
phrase pair with the corresponding cluster. This pro-
cedure can potentially map two distinct phrase pairs
to the same coarse translation. In such cases we keep
only one coarse phrase pair and sum the scores of the
colliding originals.
There are many possible schemes for creating hi-
erarchical clusterings. Here, we consider several di-
visive clustering methods, where coarse word clus-
ters are recursively split into smaller subclusters.
4.1 Random projections
The simplest approach to splitting a cluster is to ran-
domly assign each word type to one of two new sub-
clusters. Random projections have been shown to be
a good and computationally inexpensive dimension-
ality reduction technique, especially for high dimen-
sional data (Bingham andMannila, 2001). Although
our best performance does not come from random
projections, we still obtain substantial speed-ups
over a single pass fine decoder when using random
projections in coarse passes.
4.2 Frequency clustering
In frequency clustering, we allocate words to clus-
ters by frequency. At each level, the most frequent
words go into one cluster and the rarest words go
into another one. Concretely, we sort the words in
a given cluster by frequency and split the cluster so
that the two halves have equal token mass. This ap-
proach can be seen as a radically simplified version
of Brown et al (1992). It can, and does, result in
highly imbalanced cluster hierarchies.
4.3 HMM clustering
An approach found to be effective by Petrov and
Klein (2007) for coarse-to-fine parsing is to use
likelihood-based hierarchical EM training. We
adopt this approach here by identifying each clus-
ter with a latent state in an HMM and determiniz-
ing the emissions so that each word type is emitted
112
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 0  1  2  3  4  5  6  7  8  9  10
Per
ple
xity
Number of bits in coarse language model
HMMJClusterFrequencyRandom
Figure 5: Results of coarse language model perplexity
experiment (see Section 4.5). HMM and JClustering have
lower perplexity than frequency and random clustering
for all number of bits in the language encoding.
by only one state. When splitting a cluster s into
s1 and s2, we initially clone and mildly perturb its
corresponding state. We then use EM to learn pa-
rameters, which splits the state, and determinize the
result. Specifically, each word w is assigned to s1 if
P (w|s1) > P (w|s2) and s2 otherwise. Because of
this determinization after each round of EM, a word
in one cluster will be allocated to exactly one of that
cluster?s children. This process not only guarantees
that the clusters are hierarchical, it also avoids the
state drift discussed by Petrov and Klein (2007). Be-
cause the emissions are sparse, learning is very effi-
cient. An example of some of the words associated
with early splits can be seen in Figure 1.
4.4 JCluster
Goodman (2001) presents a clustering scheme
which aims to minimize the entropy of a word given
a cluster. This is accomplished by incrementally
swapping words between clusters to locally mini-
mize entropy.2 This clustering algorithm was devel-
oped with a slightly different application in mind,
but fits very well into our framework, because the
hierarchical clusters it produces are trained to maxi-
mize predictive likelihood.
4.5 Clustering Results
We applied the above clustering algorithms to our
monolingual language model data to obtain hierar-
2The software for this clustering technique is available at
http://research.microsoft.com/?joshuago/.
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000  100000
BL
EU
Total time in seconds
HMMJClusterFrequenceRandomSingle pass (no clustering)
Figure 6: Coarse-to-fine decoding with HMM or JClus-
tering coarse language models reduce decoding times
while increasing accuracy.
chical clusters. We then trained coarse language
models of varying granularity and evaluated them on
a held-out set. To measure the quality of the coarse
language models we use perplexity (exponentiated
cross-entropy).3 Figure 5 shows that HMM clus-
tering and JClustering have lower perplexity than
frequency and random based clustering for all com-
plexities. In the next section we will present a set of
machine translation experiments using these coarse
language models; the clusterings with better per-
plexities generally produce better decoders.
5 Experiments
We ran our experiments on the Europarl corpus
(Koehn, 2005) and show results on Spanish, French
and German to English translation. We used the
setup and preprocessing steps detailed in the 2008
Workshop on Statistical Machine Translation.4 Our
baseline decoder uses an ITG with an integrated tri-
gram language model. Phrase translation parame-
ters are learned from parallel corpora with approx-
imately 8.5 million words for each of the language
pairs. The English language model is trained on the
entire corpus of English parliamentary proceedings
provided with the Europarl distribution. We report
results on the 2000 development test set sentences
of length up to 126 words (average length was 30
words).
3We assumed that each cluster had a uniform distribution
over all the words in that cluster.
4See http://www.statmt.org/wmt08 for details.
113
 0
 50
 100
 150
 200
 250
 300
1-2-3-f1-3-f2-3-f1-f2-f3-f4-ff
Tot
al t
ime
 in 
min
ute
s
Language model bits for coarse passes
fine4 bits3 bits2 bits1 bit
Figure 7: Many passes with extremely simple language
models produce the highest speed-ups.
Our ITG translation model is broadly competitive
with state-of-the-art phrase-based-models trained on
the same data. For example, on the Europarl devel-
opment test set, we fall short of Moses (Koehn et al,
2007) by less than one BLEU point. On Spanish-
English we get 29.47 BLEU (compared to Moses?s
30.40), on French-English 29.34 (vs. 29.95), and
23.80 (vs. 24.64) on German-English. These differ-
ences can be attributed primarily to the substantially
richer distortion model used by Moses.
The multipass coarse-to-fine architecture that we
have introduced presents many choice points. In
the following, we investigate various axes individu-
ally. We present our findings as BLEU-to-time plots,
where the tradeoffs were generated by varying the
complexity and the number of coarse passes, as well
as the pruning thresholds and beam sizes. Unless
otherwise noted, the experiments are on Spanish-
English using trigram language models. When
different decoder settings are applied to the same
model, MERT weights (Och, 2003) from the unpro-
jected single pass setup are used and are kept con-
stant across runs. In particular, the same MERT
weights are used for all coarse passes; note that this
slightly disadvantages the multipass runs, which use
MERT weights optimized for the single pass de-
coder.
5.1 Clustering
In section Section 4, HMM clustering and JCluster-
ing gave lower perplexities than frequency and ran-
dom clustering when using the same number of bits
for encoding the language model. To test how these
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 29.6
 100  1000  10000  100000
BL
EU
Total time in seconds
Encoding+OrderOrderEncodingSingle pass
Figure 8: A combination of order-based and encoding-
based coarse-to-fine decoding yields the best results.
models perform at pruning, we ran our decoder sev-
eral times, varying only the clustering source. In
each case, we used a 2-bit trigram model as a sin-
gle coarse pass, followed by a fine output pass. Fig-
ure 6 shows that we can obtain significant improve-
ments over the single-pass baseline regardless of the
clustering. To no great surprise, HMM clustering
and JClustering yield better results, giving a 30-fold
speed-up at the same accuracy, or improvements of
about 0.3 BLEU when given the same time as the
single pass decoder. We discuss this increase in ac-
curacy over the baseline in Section 5.5. Since the
performance differences between those two cluster-
ing algorithms are negligible, we will use the sim-
pler HMM clustering in all subsequent experiments.
5.2 Spacing
Given a hierarchy of coarse language models, all
trigam for the moment, we need to decide on the
number of passes and the granularity of the coarse
language models used in each pass. Figure 7 shows
how decoding time varies for different multipass
schemes to achieve the same translation quality.
A single coarse pass with a 4-bit language model
cuts decoding time almost in half. However, one
can further cut decoding time by starting with even
coarser language models. In fact, the best results
are achieved by decoding in sequence with 1-, 2-
and 3-bit language models before running the final
fine trigram pass. Interestingly, in this setting, each
pass takes about the same amount of time. A simi-
lar observation was reported in the parsing literature,
where coarse-to-fine inference with multiple passes
114
 28 28.2
 28.4 28.6
 28.8 29
 29.2 29.4
 29.6
 100  1000  10000
BLE
U
Total time in seconds
Spanish
Coarse-To-FineFine Baseline  28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000
BLE
U
Total time in seconds
French
Coarse-To-FineFine Baseline  22
 22.5
 23
 23.5
 24
 100  1000  10000
BLE
U
Total time in seconds
German
Coarse-To-FineFine Baseline
Figure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to better
BLEU scores on all language pairs and for all parameter settings.
of roughly equal complexity produces tremendous
speed-ups (Petrov and Klein, 2007).
5.3 Encoding vs. Order
As described in Section 2, the language model com-
plexity can be reduced either by decreasing the vo-
cabulary size (encoding-based projection) or by low-
ering the language model order from trigram to bi-
gram (order-based projection). Figure 7 shows that
both approaches alone yield comparable improve-
ments over the single pass baseline. Fortunately,
the two approaches are complimentary, allowing us
to obtain further improvements by combining both.
We found it best to first do a series of coarse bigram
passes, followed by a fine bigram pass, followed by
a fine trigram pass.
5.4 Final Results
Figure 9 compares our multipass coarse-to-fine de-
coder using language refinement to single pass de-
coding on three different languages. On each lan-
guage we get significant improvements in terms of
efficiency as well as accuracy. Overall, we can
achieve up to 50-fold speed-ups at the same accu-
racy, or alternatively, improvements of 0.4 BLEU
points over the best single pass run.
In absolute terms, our decoder translates on aver-
age about two Spanish sentences per second at the
highest accuracy setting.5 This compares favorably
to the Moses decoder (Koehn et al, 2007), which
takes almost three seconds per sentence.
5Of course, the time for an average sentence is much lower,
since long sentences dominate the overall translation time.
5.5 Search Error Analysis
In multipass coarse-to-fine decoding, we noticed
that in addition to computational savings, BLEU
scores tend to improve. A first hypothesis is
that coarse-to-fine decoding simply improves search
quality, where fewer good items fall off the beam
compared to a simple fine pass. However, this hy-
pothesis turns out to be incorrect. Table 1 shows
the percentage of test sentences for which the BLEU
score or log-likelihood changes when we switch
from single pass decoding to coarse-to-fine multi-
pass decoding. Only about 30% of the sentences
get translated in the same way (if much faster) with
coarse-to-fine decoding. For the rest, coarse-to-fine
decoding mostly finds translations with lower likeli-
hood, but higher BLEU score, than single pass de-
coding.6 An increase of the underlying objectives of
interest when pruning despite an increase in model-
score search errors has also been observed in mono-
lingual coarse-to-fine syntactic parsing (Charniak et
al., 1998; Petrov and Klein, 2007). This effect may
be because coarse-to-fine approximates certain min-
imum Bayes risk objective. It may also be an effect
of model intersection between the various passes?
models. In any case, both possibilities are often per-
fectly desirable. It is also worth noting that the num-
ber of search errors incurred in the coarse-to-fine
approach can be dramatically reduced (at the cost
of decoding time) by increasing the pruning thresh-
olds. However, the fortuitous nature of coarse-to-
fine search errors seems to be a substantial and de-
sirable effect.
6We compared the influence of multipass decoding on the
TM score and the LM score; both decrease.
115
LL
> = <
B
L
E
U > 3.6% - 26.3%
= 1.5% 29.6 % 12.9 %
< 2.2% - 24.1%
Table 1: Percentage of sentences for which the BLEU
score/log-likelihood improves/drops during coarse-to-
fine decoding (compared to single pass decoding).
6 Conclusions
We have presented a coarse-to-fine syntactic de-
coder which utilizes a novel encoding-based lan-
guage projection in conjunction with order-based
projections to achieve substantial speed-ups. Un-
like A* methods, a posterior pruning approach al-
lows multiple passes, which we found to be very
beneficial for total decoding time. When aggres-
sively pruned, coarse-to-fine decoding can incur ad-
ditional search errors, but we found those errors to
be fortuitous more often than harmful. Our frame-
work applies equally well to other translation sys-
tems, though of course interesting new challenges
arise when, for example, the underlying SCFGs be-
come more complex.
References
E. Bingham and H.i Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In KDD ?01.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ?06.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ?05.
J. Goodman. 1997. Global thresholding and multiple-
pass parsing. In EMNLP ?97.
J. Goodman. 2001. A bit of progress in language model-
ing. Technical report, Microsoft Research.
A. Haghighi, J. DeNero, and D. Klein. 2007. A* search
via approximate factoring. In NAACL ?07.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL
?07.
D. Klein and C. Manning. 2003. A* parsing: fast exact
viterbi parse selection. In NAACL ?03.
P. Koehn, H. Hoang, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL ?07.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In ACL ?04.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In ICSLP ?02.
A. Venugopal, A. Zollmann, and S. Vogel. 2007. An ef-
ficient two-pass approach to synchronous-CFG driven
statistical MT. In HLT-NAACL ?07.
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In ACL ?96.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. In
Computational Linguistics.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL ?03.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
ACL ?08.
116
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152?1161,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Simple Coreference Resolution with Rich Syntactic and Semantic Features
Aria Haghighi and Dan Klein
Computer Science Division
UC Berkeley
{aria42, klein}@cs.berkeley.edu
Abstract
Coreference systems are driven by syntactic, se-
mantic, and discourse constraints. We present
a simple approach which completely modularizes
these three aspects. In contrast to much current
work, which focuses on learning and on the dis-
course component, our system is deterministic and
is driven entirely by syntactic and semantic com-
patibility as learned from a large, unlabeled corpus.
Despite its simplicity and discourse naivete, our
system substantially outperforms all unsupervised
systems and most supervised ones. Primary con-
tributions include (1) the presentation of a simple-
to-reproduce, high-performing baseline and (2) the
demonstration that most remaining errors can be at-
tributed to syntactic and semantic factors external
to the coreference phenomenon (and perhaps best
addressed by non-coreference systems).
1 Introduction
The resolution of entity reference is influenced by
a variety of constraints. Syntactic constraints like
the binding theory, the i-within-i filter, and appos-
itive constructions restrict reference by configura-
tion. Semantic constraints like selectional compat-
ibility (e.g. a spokesperson can announce things)
and subsumption (e.g. Microsoft is a company)
rule out many possible referents. Finally, dis-
course phenomena such as salience and centering
theory are assumed to heavily influence reference
preferences. As these varied factors have given
rise to a multitude of weak features, recent work
has focused on how best to learn to combine them
using models over reference structures (Culotta et
al., 2007; Denis and Baldridge, 2007; Klenner and
Ailloud, 2007).
In this work, we break from the standard view.
Instead, we consider a vastly more modular system
in which coreference is predicted from a determin-
istic function of a few rich features. In particu-
lar, we assume a three-step process. First, a self-
contained syntactic module carefully represents
syntactic structures using an augmented parser and
extracts syntactic paths from mentions to potential
antecedents. Some of these paths can be ruled in
or out by deterministic but conservative syntactic
constraints. Importantly, the bulk of the work in
the syntactic module is in making sure the parses
are correctly constructed and used, and this mod-
ule?s most important training data is a treebank.
Second, a self-contained semantic module evalu-
ates the semantic compatibility of headwords and
individual names. These decisions are made from
compatibility lists extracted from unlabeled data
sources such as newswire and web data. Finally,
of the antecedents which remain after rich syntac-
tic and semantic filtering, reference is chosen to
minimize tree distance.
This procedure is trivial where most systems are
rich, and so does not need any supervised corefer-
ence data. However, it is rich in important ways
which we argue are marginalized in recent coref-
erence work. Interestingly, error analysis from our
final system shows that its failures are far more
often due to syntactic failures (e.g. parsing mis-
takes) and semantic failures (e.g. missing knowl-
edge) than failure to model discourse phenomena
or appropriately weigh conflicting evidence.
One contribution of this paper is the exploration
of strong modularity, including the result that our
system beats all unsupervised systems and ap-
proaches the state of the art in supervised ones.
Another contribution is the error analysis result
that, even with substantial syntactic and semantic
richness, the path to greatest improvement appears
to be to further improve the syntactic and semantic
modules. Finally, we offer our approach as a very
strong, yet easy to implement, baseline. We make
no claim that learning to reconcile disparate fea-
tures in a joint model offers no benefit, only that it
must not be pursued to the exclusion of rich, non-
reference analysis.
2 Coreference Resolution
In coreference resolution, we are given a docu-
ment which consists of a set of mentions; each
1152
mention is a phrase in the document (typically
an NP) and we are asked to cluster mentions ac-
cording to the underlying referent entity. There
are three basic mention types: proper (Barack
Obama), nominal (president), and pronominal
(he).
1
For comparison to previous work, we eval-
uate in the setting where mention boundaries are
given at test time; however our system can easily
annotate reference on all noun phrase nodes in a
parse tree (see Section 3.1.1).
2.1 Data Sets
In this work we use the following data sets:
Development: (see Section 3)
? ACE2004-ROTH-DEV: Dev set split of the ACE
2004 training set utilized in Bengston and
Roth (2008). The ACE data also annotates
pre-nominal mentions which we map onto
nominals. 68 documents and 4,536 mentions.
Testing: (see Section 4)
? ACE2004-CULOTTA-TEST: Test set split of the
ACE 2004 training set utilized in Culotta et
al. (2007) and Bengston and Roth (2008).
Consists of 107 documents.
2
? ACE2004-NWIRE: ACE 2004 Newswire set to
compare against Poon and Domingos (2008).
Consists of 128 documents and 11,413 men-
tions; intersects with the other ACE data sets.
? MUC-6-TEST: MUC6 formal evaluation set
consisting of 30 documents and 2,068 men-
tions.
Unlabeled: (see Section 3.2)
? BLIPP: 1.8 million sentences of newswire
parsed with the Charniak (2000) parser. No
labeled coreference data; used for mining se-
mantic information.
? WIKI: 25k articles of English Wikipedia ab-
stracts parsed by the Klein and Manning
(2003) parser.
3
No labeled coreference data;
used for mining semantic information.
1
Other mention types exist and are annotated (such as pre-
nominal), which are treated as nominals in this work.
2
The evaluation set was not made available to non-
participants.
3
Wikipedia abstracts consist of roughly the first paragraph
of the corresponding article
2.2 Evaluation
We will present evaluations on multiple corefer-
ence resolution metrics, as no single one is clearly
superior:
? Pairwise F1: precision, recall, and F1 over
all pairs of mentions in the same entity clus-
ter. Note that this over-penalizes the merger
or separation of clusters quadratically in the
size of the cluster.
? b
3
(Amit and Baldwin, 1998): For each men-
tion, form the intersection between the pre-
dicted cluster and the true cluster for that
mention. The precision is the ratio of the in-
tersection and the true cluster sizes and recall
the ratio of the intersection to the predicted
sizes; F1 is given by the harmonic mean over
precision and recall from all mentions.
? MUC (Vilain et al, 1995): For each true clus-
ter, compute the number of predicted clusters
which need to be merged to cover the true
cluster. Divide this quantity by true cluster
size minus one. Recall is given by the same
procedure with predicated and true clusters
reversed.
4
? CEAF (Luo, 2005): For a similarity function
between predicted and true clusters, CEAF
scores the best match between true and pre-
dicted clusters using this function. We use
the ?
3
similarity function from Luo (2005).
3 System Description
In this section we develop our system and re-
port developmental results on ACE2004-ROTH-
DEV (see Section 2.1); we report pairwise F1 fig-
ures here, but report on many more evaluation
metrics in Section 4. At a high level, our system
resembles a pairwise coreference model (Soon et
al., 1999; Ng and Cardie, 2002; Bengston and
Roth, 2008); for each mention m
i
, we select ei-
ther a single-best antecedent amongst the previ-
ous mentions m
1
, . . . ,m
i?1
, or the NULL men-
tion to indicate the underlying entity has not yet
been evoked. Mentions are linearly ordered ac-
cording to the position of the mention head with
ties being broken by the larger node coming first.
4
The MUC measure is problematic when the system pre-
dicts many more clusters than actually exist (Luo, 2005;
Finkel and Manning, 2008); also, singleton clusters do not
contribute to evaluation.
1153
While much research (Ng and Cardie, 2002; Cu-
lotta et al, 2007; Haghighi and Klein, 2007; Poon
and Domingos, 2008; Finkel and Manning, 2008)
has explored how to reconcile pairwise decisions
to form coherent clusters, we simply take the tran-
sitive closure of our pairwise decision (as in Ng
and Cardie (2002) and Bengston and Roth (2008))
which can and does cause system errors.
In contrast to most recent research, our pair-
wise decisions are not made with a learned model
which outputs a probability or confidence, but in-
stead for each mentionm
i
, we select an antecedent
amongst m
1
, . . . ,m
i?1
or the NULL mention as
follows:
? Syntactic Constraint: Based on syntac-
tic configurations, either force or disallow
coreference between the mention and an an-
tecedent. Propagate this constraint (see Fig-
ure 4).
? Semantic/Syntactic Filter: Filter the re-
maining possible antecedents based upon
compatibility with the mention (see Fig-
ure 2).
? Selection: Select the ?closest? mention from
the set of remaining possible antecedents (see
Figure 1) or the NULL antecedent if empty.
Initially, there is no syntactic constraint (im-
proved in Section 3.1.3), the antecedent com-
patibility filter allows proper and nominal men-
tions to corefer only with mentions that have the
same head (improved in Section 3.2), and pro-
nouns have no compatibility constraints (improved
in Section 3.1.2). Mention heads are determined
by parsing the given mention span with the Stan-
ford parser (Klein and Manning, 2003) and us-
ing the Collins head rules (Collins, 1999); Poon
and Domingos (2008) showed that using syntactic
heads strongly outperformed a simple rightmost
headword rule. The mention type is determined
by the head POS tag: proper if the head tag is NNP
or NNPS, pronoun if the head tag is PRP, PRP$, WP,
or WP$, and nominal otherwise.
For the selection phase, we order mentions
m
1
, . . . ,m
i?1
according to the position of the
head word and select the closest mention that re-
mains after constraint and filtering are applied.
This choice reflects the intuition of Grosz et al
(1995) that speakers only use pronominal men-
tions when there are not intervening compatible
S!!!!!!!"""""""
NP#1###$$$
NP
NNP
Nintendo
PP%%&&IN
of
NP#2
NNP
America
VP''''((((VBD
announced
NP#3)))***NP#1
PRP$
its
NP%%&&JJ
new
NN
console
Figure 1: Example sentence where closest tree dis-
tance between mentions outperforms raw distance.
For clarity, each mention NP is labeled with the
underlying entity id.
mentions. This system yields a rather low 48.9
pairwise F1 (see BASE-FLAT in Table 2). There
are many, primarily recall, errors made choos-
ing antecedents for all mention types which we
will address by adding syntactic and semantic con-
straints.
3.1 Adding Syntactic Information
In this section, we enrich the syntactic represen-
tation and information in our system to improve
results.
3.1.1 Syntactic Salience
We first focus on fixing the pronoun antecedent
choices. A common error arose from the use of
mention head distance as a poor proxy for dis-
course salience. For instance consider the exam-
ple in Figure 1, the mention America is closest
to its in flat mention distance, but syntactically
Nintendo of America holds a more prominent syn-
tactic position relative to the pronoun which, as
Hobbs (1977) argues, is key to discourse salience.
MappingMentions to Parse Nodes: In order to
use the syntactic position of mentions to determine
anaphoricity, we must associate each mention in
the document with a parse tree node. We parse
all document sentences with the Stanford parser,
and then for each evaluation mention, we find the
largest-span NP which has the previously deter-
mined mention head as its head.
5
Often, this re-
sults in a different, typically larger, mention span
than annotated in the data.
Now that each mention is situated in a parse
tree, we utilize the length of the shortest tree path
between mentions as our notion of distance. In
5
If there is no NP headed by a given mention head, we
add an NP over just that word.
1154
S!!!!!!!!""""""""NP-ORG#1###$$$The Israelis
VP!!!!!!!!!!!%%%% """""""""""VBP
regard
NP#2###$$$NP
the site
PP&&''IN
as
NP#2&&''a shrine
SBAR(((((()******IN
because
PP++,,TO
to
NP#1
PRP
them
S###$$$it is sacred
Figure 2: Example of a coreference decision fixed
by agreement constraints (see Section 3.1.2). The
pronoun them is closest to the sitemention, but has
an incompatible number feature with it. The clos-
est (in tree distance, see Section 3.1.1) compatible
mention is The Israelis, which is correct
particular, this fixes examples such as those in
Figure 1 where the true antecedent has many em-
bedded mentions between itself and the pronoun.
This change by itself yields 51.7 pairwise F1 (see
BASE-TREE in Table 2), which is small overall, but
reduces pairwise pronoun antecedent selection er-
ror from 51.3% to 42.5%.
3.1.2 Agreement Constraints
We now refine our compatibility filtering to in-
corporate simple agreement constraints between
coreferent mentions. Since we currently allow
proper and nominal mentions to corefer only with
matching head mentions, agreement is only a con-
cern for pronouns. Traditional linguistic theory
stipulates that coreferent mentions must agree in
number, person, gender, and entity type (e.g. an-
imacy). Here, we implement person, number and
entity type agreement.
6
A number feature is assigned to each mention
deterministically based on the head and its POS
tag. For entity type, we use NER labels. Ideally,
we would like to have information about the en-
tity type of each referential NP, however this in-
formation is not easily obtainable. Instead, we opt
to utilize the Stanford NER tagger (Finkel et al,
2005) over the sentences in a document and anno-
tate each NP with the NER label assigned to that
mention head. For each mention, when its NP is
assigned an NER label we allow it to only be com-
patible with that NER label.
7
For pronouns, we
deterministically assign a set of compatible NER
values (e.g. personal pronouns can only be a PER-
6
Gender agreement, while important for general corefer-
ence resolution, did not contribute to the errors in our largely
newswire data sets.
7
Or allow it to be compatible with all NER labels if the
NER tagger doesn?t predict a label.
gore president florida state
bush governor lebanese territory
nation people arafat leader
inc. company aol company
nation country assad president
Table 1: Most common recall (missed-link) errors
amongst non-pronoun mention heads on our de-
velopment set. Detecting compatibility requires
semantic knowledge which we obtain from a large
corpus (see Section 3.2).
S
`
`
`
 
 
 
NP#1
NNP
Wal-Mart
VP
h
h
h
h
(
(
(
(
VBZ
says
S
h
h
h
h
(
(
(
(
NP#2
X
X


NP
NNP
Gitano
,
,
NP-APPOS#2
P
P


NP#1
PRP
its
JJ
top
NNS
brand
VP
P
P


is underselling
Figure 4: Example of interaction between the ap-
positive and i-within-i constraint. The i-within-
i constraint disallows coreference between parent
and child NPs unless the child is an appositive.
Hashed numbers indicate ground truth but are not
in the actual trees.
SON, but its can be an ORGANIZATION or LOCA-
TION). Since the NER tagger typically does not
label non-proper NP heads, we have no NER com-
patibility information for nominals.
We incorporate agreement constraints by filter-
ing the set of possible antecedents to those which
have compatible number and NER types with the
target mention. This yields 53.4 pairwise F1, and
reduces pronoun antecedent errors to 42.5% from
34.4%. An example of the type of error fixed by
these agreement constraints is given by Figure 2.
3.1.3 Syntactic Configuration Constraints
Our system has so far focused only on improving
pronoun anaphora resolution. However, a plurality
of the errors made by our system are amongst non-
pronominal mentions.
8
We take the approach that
in order to align a non-pronominal mention to an
antecedent without an identical head, we require
evidence that the mentions are compatible.
Judging compatibility of mentions generally re-
quires semantic knowledge, to which we return
later. However, some syntactic configurations
8
There are over twice as many nominal mentions in our
development data as pronouns.
1155
NP#1!!!!!!!"""""""
NP####$$$$NN#1
painter
NNP
Pablo
NNP
Picasso
,
,
NP#1%%%%%%&&&&&&subject of the [exhibition]2
NP-PERS#1!!!!!!!!""########NP$$$$%%%%NP-APPOS#1
NN
painter
NP-PERS&&''NNP
Pablo
NNP
Picasso
,
,
NP-APPOS#1(((((())))))subject of the [exhibition]2
(a) (b)
Figure 3: NP structure annotation: In (a) we have the raw parse from the Klein and Manning (2003)
parser with the mentions annotated by entity. In (b), we demonstrate the annotation we have added. NER
labels are added to all NP according to the NER label given to the head (see Section 3.1.1). Appositive
NPs are also annotated. Hashes indicate forced coreferent nodes
guarantee coreference. The one exploited most
in coreference work (Soon et al, 1999; Ng and
Cardie, 2002; Luo et al, 2004; Culotta et al, 2007;
Poon and Domingos, 2008; Bengston and Roth,
2008) is the appositive construction. Here, we rep-
resent apposition as a syntactic feature of an NP
indicating that it is coreferent with its parent NP
(e.g. it is an exception to the i-within-i constraint
that parent and child NPs cannot be coreferent).
We deterministically mark a node as NP-APPOS
(see Figure 3) when it is the third child in of a par-
ent NP whose expansion begins with (NP , NP),
and there is not a conjunction in the expansion (to
avoid marking elements in a list as appositive).
Role Appositives: During development, we dis-
covered many errors which involved a variant of
appositives which we call ?role appositives? (see
painter in Figure 3), where an NP modifying the
head NP describes the role of that entity (typi-
cally a person entity). There are several challenges
to correctly labeling these role NPs as being ap-
positives. First, the NPs produced by Treebank
parsers are flat and do not have the required inter-
nal structure (see Figure 3(a)). While fully solving
this problem is difficult, we can heuristically fix
many instances of the problem by placing an NP
around maximum length sequences of NNP tags
or NN (and JJ) tags within an NP; note that this
will fail for many constructions such as U.S. Pres-
ident Barack Obama, which is analyzed as a flat
sequence of proper nouns. Once this internal NP
structure has been added, whether the NP immedi-
ately to the left of the head NP is an appositive de-
pends on the entity type. For instance, Rabbi Ashi
is an apposition but Iranian army is not. Again, a
full solution would require its own model, here we
mark as appositions any NPs immediately to the
left of a head child NP where the head child NP is
identified as a person by the NER tagger.
9
We incorporate NP appositive annotation as a
constraint during filtering. Any mention which
corresponds to an appositive node has its set of
possible antecedents limited to its parent. Along
with the appositive constraint, we implement the
i-within-i constraint that any non-appositive NP
cannot be be coreferent with its parent; this con-
straint is then propagated to any node its parent
is forced to agree with. The order in which these
constraints are applied is important, as illustrated
by the example in Figure 4: First the list of pos-
sible antecedents for the appositive NP is con-
strained to only its parent. Now that all apposi-
tives have been constrained, we apply the i-within-
i constraint, which prevents its from having the NP
headed by brand in the set of possible antecedents,
and by propagation, also removes the NP headed
by Gitano. This leaves the NP Wal-Mart as the
closest compatible mention.
Adding these syntactic constraints to our system
yields 55.4 F1, a fairly substantial improvement,
but many recall errors remain between mentions
with differing heads. Resolving such cases will
require external semantic information, which we
will automatically acquire (see Section 3.2).
Predicate Nominatives: Another syntactic con-
straint exploited in Poon and Domingos (2008) is
the predicate nominative construction, where the
object of a copular verb (forms of the verb be) is
constrained to corefer with its subject (e.g. Mi-
crosoft is a company in Redmond). While much
less frequent than appositive configurations (there
are only 17 predicate nominatives in our devel-
9
Arguably, we could also consider right modifying NPs
(e.g., [Microsoft [Company]
1
]
1
) to be role appositive, but we
do not do so here.
1156
Path Example
NP!!!"""
NP-NNP PRN-NNP
NP#####$$
%%%%%
NP-president CC NP-NNP
America Online Inc. (AOL)
NP
NP-NNP PRN-NNP
NP
$$
NP-president CC NP-NNP
[President and C.E.O] Bill Gates
Figure 5: Example paths extracted via semantic compatibility mining (see Section 3.2) along with exam-
ple instantiations. In both examples the left child NP is coreferent with the rightmost NP. Each category
in the interior of the tree path is annotated with the head word as well as its subcategorization. The
examples given here collapse multiple instances of extracted paths.
opment set), predicate nominatives are another
highly reliable coreference pattern which we will
leverage in Section 3.2 to mine semantic knowl-
edge. As with appositives, we annotate object
predicate-nominative NPs and constrain corefer-
ence as before. This yields a minor improvement
to 55.5 F1.
3.2 Semantic Knowledge
While appositives and related syntactic construc-
tions can resolve some cases of non-pronominal
reference, most cases require semantic knowledge
about the various entities as well as the verbs used
in conjunction with those entities to disambiguate
references (Kehler et al, 2008).
However, given a semantically compatible men-
tion head pair, say AOL and company, one
might expect to observe a reliable appositive
or predicative-nominative construction involving
these mentions somewhere in a large corpus.
In fact, the Wikipedia page for AOL
10
has a
predicate-nominative construction which supports
the compatibility of this head pair: AOL LLC (for-
merly America Online) is an American global In-
ternet services and media company operated by
Time Warner.
In order to harvest compatible head pairs, we
utilize our BLIPP and WIKI data sets (see Sec-
tion 2), and for each noun (proper or common) and
pronoun, we assign a maximal NP mention node
for each nominal head as in Section 3.1.1; we then
annotate appositive and predicate-nominative NPs
as in Section 3.1.3. For any NP which is annotated
as an appositive or predicate-nominative, we ex-
tract the head pair of that node and its constrained
antecedent.
10
http://en.wikipedia.org/wiki/AOL
The resulting set of compatible head words,
while large, covers a little more than half of the
examples given in Table 1. The problem is that
these highly-reliable syntactic configurations are
too sparse and cannot capture all the entity infor-
mation present. For instance, the first sentence of
Wikipedia abstract for Al Gore is:
Albert Arnold ?Al? Gore, Jr. is an
American environmental activist who
served as the 45th Vice President of the
United States from 1993 to 2001 under
President Bill Clinton.
The required lexical pattern X who served as Y is
a general appositive-like pattern that almost surely
indicates coreference. Rather than opt to manu-
ally create a set of these coreference patterns as in
Hearst (1992), we instead opt to automatically ex-
tract these patterns from large corpora as in Snow
et al (2004) and Phillips and Riloff (2007). We
take a simple bootstrapping technique: given a
set of mention pairs extracted from appositives
and predicate-nominative configurations, we ex-
tract counts over tree fragments between nodes
which have occurred in this set of head pairs (see
Figure 5); the tree fragments are formed by an-
notating the internal nodes in the tree path with
the head word and POS along with the subcatego-
rization. We limit the paths extracted in this way
in several ways: paths are only allowed to go be-
tween adjacent sentences and have a length of at
most 10. We then filter the set of paths to those
which occur more than a hundred times and with
at least 10 distinct seed head word pairs.
The vast majority of the extracted fragments are
variants of traditional appositives and predicate-
nominatives with some of the structure of the NPs
1157
MUC b
3
Pairwise CEAF
System P R F1 P R F1 P R F1 P R F1
ACE2004-ROTH-DEV
BASIC-FLAT 73.5 66.8 70.0 80.6 68.6 74.1 63.6 39.7 48.9 68.4 68.4 68.4
BASIC-TREE 75.8 68.9 72.2 81.9 69.9 75.4 65.6 42.7 51.7 69.8 69.8 69.8
+SYN-COMPAT 77.8 68.5 72.9 84.1 69.7 76.2 71.0 43.1 53.4 69.8 69.8 69.8
+SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8
+SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5
ACE2004-CULOTTA-TEST
BASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5
BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9
+SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2
+SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6
+SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3
Supervised Results
Culotta et al (2007) - - - 86.7 73.2 79.3 - - - - - -
Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - -
MUC6-TEST
+SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0
Unsupervised Results
Poon and Domingos (2008) 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - -
Supervised Results
Finkel and Manning (2008) 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - -
ACE2004-NWIRE
+SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5
Unsupervised Results
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - -
Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the
largest result is bolded. The CEAF measure has equal values for precision, recall, and F1.
specified. However there are some tree fragments
which correspond to the novel coreference pat-
terns (see Figure 5) of parenthetical alias as well
as conjunctions of roles in NPs.
We apply our extracted tree fragments to our
BLIPP and WIKI data sets and extract a set of com-
patible word pairs which match these fragments;
these words pairs will be used to relax the seman-
tic compatibility filter (see the start of the section);
mentions are compatible with prior mentions with
the same head or with a semantically compatible
head word. This yields 58.5 pairwise F1 (see SEM-
COMPAT in Table 2) as well as similar improve-
ments across other metrics.
By and large the word pairs extracted in this
way are correct (in particular we now have cov-
erage for over two-thirds of the head pair recall
errors from Table 1.) There are however word-
pairs which introduce errors. In particular city-
state constructions (e.g. Los Angeles, California)
appears to be an appositive and incorrectly allows
our system to have angeles as an antecedent for
california. Another common error is that the %
symbol is made compatible with a wide variety of
common nouns in the financial domain.
4 Experimental Results
We present formal experimental results here
(see Table 2). We first evaluate our model
on the ACE2004-CULOTTA-TEST dataset used in
the state-of-the-art systems from Culotta et al
(2007) and Bengston and Roth (2008). Both of
these systems were supervised systems discrimi-
natively trained to maximize b
3
and used features
from many different structured resources includ-
ing WordNet, as well as domain-specific features
(Culotta et al, 2007). Our best b
3
result of 79.0
is broadly in the range of these results. We should
note that in our work we use neither the gold men-
tion types (we do not model pre-nominals sepa-
rately) nor do we use the gold NER tags which
Bengston and Roth (2008) does. Across metrics,
the syntactic constraints and semantic compatibil-
ity components contribute most to the overall final
result.
On the MUC6-TEST dataset, our system outper-
1158
PR
O
P
E
R
N
O
M
I
N
A
L
P
R
O
N
O
U
N
N
U
L
L
T
O
T
A
L
PROPER 21/451 8/20 - 72/288 101/759
NOMINAL 16/150 99/432 - 158/351 323/933
PRONOUN 29/149 60/128 15/97 1/2 105/376
Table 3: Errors for each type of antecedent deci-
sion made by the system. Each row is a mention
type and the column the predicted mention type
antecedent. The majority of errors are made in the
NOMINAL category.
forms both Poon and Domingos (2008) (an un-
supervised Markov Logic Network system which
uses explicit constraints) and Finkel and Manning
(2008) (a supervised system which uses ILP in-
ference to reconcile the predictions of a pairwise
classifier) on all comparable measures.
11
Simi-
larly, on the ACE2004-NWIRE dataset, we also out-
perform the state-of-the-art unsupervised system
of Poon and Domingos (2008).
Overall, we conclude that our system outper-
forms state-of-the-art unsupervised systems
12
and
is in the range of the state-of-the art systems of Cu-
lotta et al (2007) and Bengston and Roth (2008).
5 Error Analysis
There are several general trends to the errors made
by our system. Table 3 shows the number of
pairwise errors made on MUC6-TEST dataset by
mention type; note these errors are not equally
weighted in the final evaluations because of the
transitive closure taken at the end. The most er-
rors are made on nominal mentions with pronouns
coming in a distant second. In particular, we most
frequently say a nominal is NULL when it has an
antecedent; this is typically due to not having the
necessary semantic knowledge to link a nominal
to a prior expression.
In order to get a more thorough view of the
cause of pairwise errors, we examined 20 random
errors made in aligning each mention type to an
antecedent. We categorized the errors as follows:
? SEM. COMPAT: Missing information about
the compatibility of two words e.g. pay and
wage. For pronouns, this is used to mean that
11
Klenner and Ailloud (2007) took essentially the same ap-
proach but did so on non-comparable data.
12
Poon and Domingos (2008) outperformed Haghighi and
Klein (2007). Unfortunately, we cannot compare against Ng
(2008) since we do not have access to the version of the ACE
data used in their evaluation.
we incorrectly aligned a pronoun to a men-
tion with which it is not semantically com-
patible (e.g. he aligned to board).
? SYN. COMPAT: Error in assigning linguistic
features of nouns for compatibility with pro-
nouns (e.g. disallowing they to refer to team).
? HEAD: Errors involving the assumption that
mentions with the same head are always com-
patible. Includes modifier and specificity er-
rors such as allowing Lebanon and Southern
Lebanon to corefer. This also includes errors
of definiteness in nominals (e.g. the people
in the room and Chinese people). Typically,
these errors involve a combination of missing
syntactic and semantic information.
? INTERNAL NP: Errors involving lack of inter-
nal NP structure to mark role appositives (see
Section 3.1.3).
? PRAG. / DISC.: Errors where discourse salience
or pragmatics are needed to disambiguate
mention antecedents.
? PROCESS ERROR: Errors which involved a tok-
enization, parse, or NER error.
The result of this error analysis is given in Ta-
ble 4; note that a single error may be attributed to
more than one cause. Despite our efforts in Sec-
tion 3 to add syntactic and semantic information
to our system, the largest source of error is still
a combination of missing semantic information or
annotated syntactic structure rather than the lack
of discourse or salience modeling.
Our error analysis suggests that in order to im-
prove the state-of-the-art in coreference resolu-
tion, future research should consider richer syntac-
tic and semantic information than typically used in
current systems.
6 Conclusion
Our approach is not intended as an argument
against the more complex, discourse-focused ap-
proaches that typify recent work. Instead, we note
that rich syntactic and semantic processing vastly
reduces the need to rely on discourse effects or ev-
idence reconciliation for reference resolution. In-
deed, we suspect that further improving the syn-
tactic and semantic modules in our system may
produce greater error reductions than any other
1159
Mention Type SEM. COMPAT SYN. COMPAT HEAD INTENAL NP PRAG / DISC. PROCESS ERROR OTHER Comment
NOMINAL 7 - 5 6 2 2 1 2 general appos. patterns
PRONOUN 6 3 - 6 3 3 3 2 cataphora
PROPER 6 - 3 4 4 4 1
Table 4: Error analysis on ACE2004-CULOTTA-TEST data by mention type. The dominant errors are in
either semantic or syntactic compatibility of mentions rather than discourse phenomena. See Section 5.
route forward. Of course, a system which is rich
in all axes will find some advantage over any sim-
plified approach.
Nonetheless, our coreference system, despite
being relatively simple and having no tunable pa-
rameters or complexity beyond the non-reference
complexity of its component modules, manages
to outperform state-of-the-art unsupervised coref-
erence resolution and be broadly comparable to
state-of-the-art supervised systems.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
Eric Bengston and Dan Roth. 2008. Understanding the
value of features for corefernce resolution. In Em-
pirical Methods in Natural Language Processing.
E. Charniak. 2000. Maximum entropy inspired parser.
In North American Chapter of the Association of
Computational Linguistics (NAACL).
Mike Collins. 1999. Head-driven statistical models for
natural language parsing.
A Culotta, M Wick, R Hall, and A McCallum. 2007.
First-order probabilistic models for coreference res-
olution. In NAACL-HLT.
Pascal Denis and Jason Baldridge. 2007. Global,
Joint Determination of Anaphoricity and Corefer-
ence Resolution using Integer Programming. In
HLT-NAACL.
Jenny Finkel and Christopher Manning. 2008. Enforc-
ing transitivity in coreference resolution. In Associ-
ation of Computational Linguists (ACL).
Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by gibbs sam-
pling. In ACL.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for modelling
the local coherence of discourse.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. As-
sociation for Computational Linguistics.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Conference on
Natural Language Learning (COLING).
J. R. Hobbs. 1977. Resolving pronoun references.
Lingua.
Andrew Kehler, Laura Kertz, Hannah Rohde, and Jef-
frey Elman. 2008. Coherence and coreference re-
visited.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association of Computational Lin-
guists (ACL).
Manfred Klenner and Etienne Ailloud. 2007. Op-
timization in coreference resolution is not needed:
A nearly-optimal algorithm with intensional con-
straints. In Recent Advances in Natural Language
Processing.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Association of
Computational Linguists.
X Luo. 2005. On coreference resolution performance
metrics. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Resolu-
tion. In Association of Computational Linguists.
Vincent Ng. 2008. Unsupervised models of corefer-
ence resolution. In EMNLP.
W. Phillips and E. Riloff. 2007. Exploiting role-
identifying nouns and expressions for information
extraction. In Recent Advances in Natural Language
Processing (RANLP).
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
R. Snow, D. Jurafsky, and A. Ng. 2004. Learning syn-
tactic patterns for automatic hypernym discovery. In
Neural Information Processing Systems (NIPS).
W.H. Soon, H. T. Ng, and D. C. Y. Lim. 1999. A
machine learning approach to coreference resolution
of noun phrases.
1160
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC-6.
1161
A Global Joint Model for Semantic
Role Labeling
Kristina Toutanova?
Microsoft Research
Aria Haghighi??
University of California Berkeley
Christopher D. Manning?
Stanford University
We present a model for semantic role labeling that effectively captures the linguistic intuition
that a semantic argument frame is a joint structure, with strong dependencies among the
arguments. We show how to incorporate these strong dependencies in a statistical joint model
with a rich set of features over multiple argument phrases. The proposed model substantially
outperforms a similar state-of-the-art local model that does not include dependencies among
different arguments.
We evaluate the gains from incorporating this joint information on the Propbank corpus,
when using correct syntactic parse trees as input, and when using automatically derived parse
trees. The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments
for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are
8.3% and 10.3% on all and core arguments, respectively. We also present results on the CoNLL
2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to
cope with parser noise and uncertainty.
1. Introduction
Since the release of the FrameNet (Baker, Fillmore, and Lowe 1998) and Propbank
(Palmer, Gildea, and Kingsbury 2005) corpora, there has been a large amount of work
on statistical models for semantic role labeling. Most of this work relies heavily on local
classifiers: ones that decide the semantic role of each phrase independently of the roles
of other phrases.
However, linguistic theory tells us that a core argument frame is a joint struc-
ture, with strong dependencies between arguments. For instance, in the sentence
? One Microsoft Way, Redmond, WA 98052, USA. E-mail: kristout@microsoft.com.
?? Department of Electrical Engineering and Computer Sciences, Soda Hall, Berkeley, CA 94720, USA.
E-mail: aria42@cs.berkeley.edu.
? Department of Computer Science, Gates Building 1A, 353 Serra Mall, Stanford CA 94305, USA. E-mail:
manning@cs.stanford.edu.
Submission received: 15 July 2006; Revised submission received: 1 May 2007; Accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
[Final-hour trading]THEME accelerated [to 108.1 million shares]TARGET [yesterday]ARGM-TMP, the
first argument is the subject noun phrase final-hour trading of the active verb accelerated.
If we did not consider the rest of the sentence, it would look more like an AGENT argu-
ment, but when we realize that there is no other good candidate for a THEME argument,
because to 108.1 million sharesmust be a TARGET and yesterday is most likely ARGM-TMP,
we can correctly label it THEME.
Even though previous work has modeled some correlations between the labels of
parse tree nodes (see Section 2), many important phenomena have not been modeled.
The key properties needed to model this joint structure are: (1) no finite Markov horizon
assumption for dependencies among node labels, (2) features looking at the labels of
multiple argument nodes and internal features of these nodes, and (3) a statistical model
capable of incorporating these long-distance dependencies and generalizing well. We
show how to build a joint model of argument frames, incorporating novel features into
a discriminative log-linear model. This system achieves an error reduction of 24.1%
on ALL arguments and 36.8% on CORE arguments over a state-of-the-art independent
classifier for gold-standard parse trees on Propbank.
If we consider the linguistic basis for joint modeling of a verb?s arguments (includ-
ing modifiers), there are at least three types of information to be captured. The most
basic is to limit occurrences of each kind of argument. For instance, there is usually
at most one argument of a verb that is an ARG0 (agent), and although some modifier
roles such as ARGM-TMP can fairly easily be repeated, others such as ARGM-MNR also
generally occur at most once.1 The remaining two types of information apply mainly to
core arguments (the strongly selected arguments of a verb: ARG0?ARG5 in Propbank),
which in most linguistic theories are modeled as belonging together in an argument
frame (set of arguments). The information is only marginally useful for adjuncts (the
ARGM arguments of Propbank), which are usually treated as independent realizational
choices not included in the argument frame of a verb.
Firstly, many verbs take a number of different argument frames. Previous work
has shown that these are strongly correlated with the word sense of the verb (Roland
and Jurafsky 2002). If verbs were disambiguated for sense, the semantic roles of
phrases would be closer to independent given the sense of the verb. However, be-
cause in almost all semantic role labeling work (including ours), the word sense is
unknown and the model conditions only on the lemma, there is much joint informa-
tion between arguments when conditioning only on the verb lemma. For example,
compare:
(1) Britain?s House of Commons passed a law on steroid use.
(2) The man passed the church on his way to the factory.
In the first case the noun phrase after passed is an ARG1, whereas in the second case it is a
ARGM-LOC, with the choice governed by the sense of the verb pass. Secondly, even with
same sense of a verb, different patterns of argument realization lead to joint information
between arguments. Consider:
(3) The day that the ogre cooked the children is still remembered.
1 The Propbank semantic role names which we use here are defined in Section 3.
162
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
(4) The meal that the ogre cooked the children is still remembered.
Despite both examples having an identical surface syntax, knowing that the ARG1 of
cook is expressed by the initial noun meal in the second example gives evidence that the
children is the ARG2 (beneficiary), not the ARG1 in this case.
Let us think of a graphical model over a set of m variables, one for each node in
the parse tree t, representing the labels of the nodes and the dependencies between
them. In order for a model over these variables to capture, for example, the statistical
tendency of some semantic roles to occur at most once (e.g., that there is usually at most
one constituent labeled AGENT), there must be a dependency link between any two
variables. To estimate the probability that a certain node gets the role AGENT, we need
to know if any of the other nodes were labeled with this role.
We propose such a model, with a very rich graphical model structure, which is
globally conditioned on the observation (the parse tree).2 Such a model is formally
a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However,
note that in practice this term has previously been used almost exclusively to describe
the restricted case of linear chain Conditional Markov Random Fields (sequence mod-
els) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models
that have strong Markov properties, which allow efficient dynamic programming al-
gorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF
structure, with no Markov properties, and use approximate inference by re-ranking the
n-best solutions of a simpler model with stronger independence assumptions (for which
exact inference is possible).
Such a rich graphical model can represent many dependencies but there are two
dangers?one is that the computational complexity of training the model and search-
ing for the most likely labeling given the tree can be prohibitive, and the other is
that if too many dependencies are encoded, the model will over-fit the training
data and will not generalize well. We propose a model which circumvents these
two dangers and achieves significant performance gains over a similar local model
that does not add any dependency arcs among the random variables. To tackle the
efficiency problem, we adopt dynamic programming and re-ranking algorithms. To
avoid overfitting we encode only a small set of linguistically motivated dependencies
in features over sets of the random variables. Our re-ranking approach, like the ap-
proach to parse re-ranking of Collins (2000), employs a simpler model?a local semantic
role labeling algorithm?as a first pass to generate a set of n likely complete assign-
ments of labels to all parse tree nodes. The joint model is restricted to these n assign-
ments and does not have to search the exponentially large space of all possible joint
labelings.
2. Related Work
There has been a substantial amount of work on automatic semantic role labeling,
starting with the statistical model of Gildea and Jurafsky (2002). Researchers have
worked on defining new useful features, and different system architectures andmodels.
Here we review the work most closely related to ours, concentrating on methods for
incorporating joint information and for increasing robustness to parser error.
2 That is, it defines a conditional distribution of labels of all nodes given the parse tree.
163
Computational Linguistics Volume 34, Number 2
2.1 Methods for Incorporating Joint Information
Gildea and Jurafsky (2002) propose a method to model global dependencies by includ-
ing a probability distribution over multi-sets of semantic role labels given a predicate.
In this way the model can consider the assignment of all nodes in the parse tree and
evaluate whether the set of realized semantic roles is likely. If a necessary role is missing
or if an unusual set of arguments is assigned by the local model, this additional factor
can correct some of the mistakes. The distribution over label multi-sets is estimated
using interpolation of a relative frequency and a back-off distribution. The back-off
distribution assumes each argument label is present or absent independently of the
other labels, namely, it assumes a Bernoulli Naive Bayes model.
The most likely assignment of labels according to such a joint model is found ap-
proximately using re-scoring of the top k = 10 assignments according to a local model,
which does not include dependencies among arguments. Using this model improves
the performance of the system in F-measure from 59.2 to 62.85. This shows that adding
global information improves the performance of a role labeling system considerably.
However, the type of global information in this model is limited to label multi-sets.
We will show that much larger gains are possible from joint modeling, adding richer
sources of joint information using a more flexible statistical model.
The model of Pradhan, Hacioglu, et al (2004, 2005) is a state-of-the-art model, based
on Support Vector Machines, and incorporating a large set of structural and lexical
features. At the heart of the model lies a local classifier, which labels each parse tree
node with one of the possible argument labels or NONE. Joint information is integrated
into the model in two ways:
Dynamic class context: Using the labels of the two nodes to the left as features for
classifying the current node. This is similar to the Conditional Markov Models (CMM)
often used in information extraction (McCallum, Freitag, and Pereira 2000). Notice
that here the previous two nodes classified are not in general the previous two nodes
assigned non-NONE labels. If a linear order on all nodes is imposed, then the previous
two nodes classified most likely bear the label NONE.
Language model lattice re-scoring: Re-scoring of an N-best lattice with a trigram
language model over semantic role label sequences. The target predicate is also part of
the sequence.
These ways of incorporating joint information resulted in small gains over a base-
line system using only the features of Gildea and Jurafsky (2002). The performance
gain due to joint information over a system using all features was not reported. The
joint information captured by this model is limited by the n-gram Markov assumption
of the language model over labels. In our work, we improve the modeling of joint
dependencies by looking at longer-distance context, by defining richer features over
the sequence of labels and input features, and by estimating the model parameters
discriminatively.
A system which can integrate longer-distance dependencies is that of Punyakanok
et al (2004) and Punyakanok, Roth, and Yih (2005). The idea is to build a semantic role
labeling system that is based on local classifiers but also uses a global component that
ensures that several linguistically motivated global constraints on argument frames
are satisfied. The constraints are categorical and specified by hand. For example, one
global constraint is that the argument phrases cannot overlap?that is, if a node is
labeled with a non-NONE label, all of its descendants have to be labeled NONE. The
proposed framework is integer linear programming (ILP), which makes it possible
to find the most likely assignment of labels to all nodes of the parse tree subject to
164
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
specified constraints. Solving the ILP problem is NP-hard but it is very fast in practice
(Punyakanok et al 2004). The authors report substantial gains in performance due
to these global consistency constraints. This method was applied to improve the
performance both of a system based on labeling syntactic chunks and one based on
labeling parse tree nodes. Our work differs from that work in that our constraints are
not categorical (either satisfied or not), but are rather statistical preferences, and that
they are learned automatically based on features specified by the knowledge engineer.
On the other hand, we solve the search/estimation problem through re-ranking and
n-best search only approximately, not exactly.
So far we have mainly discussed systems which label nodes in a parse tree. Many
systems that only use shallow syntactic information have also been presented (Hacioglu
2004; Punyakanok et al 2004); using full syntactic parse information was not allowed in
the CoNLL 2004 shared task on Semantic Role Labeling and description of such systems
can be found in (Carreras and Ma`rquez 2004). Most systems which use only shallow
syntactic information represent the input sentence as a sequence of tokens (words or
phrases), which they label with a BIO tagging representation (beginning, inside, and
outside argument labels) (Hacioglu 2004). Limited joint information is used by such sys-
tems, provided as a fixed size context of tags on previous tokens; for example, a length
five window is used in the chunk-based system in (Pradhan, Hacioglu et al 2005).
A method that models joint information in a different way was proposed by Cohn
and Blunsom (2005). It uses a tree-structured CRF, where the statistical dependency
structure is exactly defined by the edges in the syntactic parse tree. The only depen-
dencies captured are between the label of a node and the label of each of its chil-
dren. However, the arguments of a predicate can be arbitrarily far from each other
in the syntactic parse tree and therefore a tree-CRF model is limited in its ability to
model dependencies among different arguments. For instance, the dependency between
the meal and the children for the sentence in example (4) will not be captured because
these phrases are not in the same local tree according to Penn Treebank syntax.
2.2 Increasing Robustness to Parser Error
There have been multiple approaches to reducing the sensitivity of semantic role
labeling systems to syntactic parser error. Promising approaches have been to consider
multiple syntactic analyses?the top k parses from a single or multiple full parsers
(Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (Ma`rquez et al
2005; Pradhan et al 2005), or several types of full syntactic parses (Pradhan, Ward
et al 2005). Such techniques are important for achieving good performance: The top
four systems in the CoNLL 2005 shared task competition all used multiple syntactic
analyses (Carreras and Ma`rquez 2005).
These previous methods develop special components to combine the labeling deci-
sions obtained using different syntactic annotation. The method of Punyakanok, Roth,
and Yih (2005) uses ILP to derive a consistent set of arguments, each of which could
be derived using a different parse tree. Pradhan, Ward et al (2005) use stacking to
train a classifier which combines decisions based on different annotations, andMa`rquez
et al (2005) use special-purpose filtering and inference stages which combine arguments
proposed by systems using shallow and full analyses.
Our approach to increasing robustness uses the top k parses from a single parser
and is a simple general method to factor in the uncertainty of the parser by apply-
165
Computational Linguistics Volume 34, Number 2
ing Bayesian inference. It is most closely related to the method described in Finkel,
Manning, and Ng (2006) and can be seen as an approximation of that method.
We describe our system in detail by first introducing simpler local semantic role
labeling models in Section 4, and later building on them to define joint models in Sec-
tion 5. Before we start presenting models, we describe the data and evaluation measures
used in Section 3. Readers can skip the next section and continue on to Section 4 if they
are not interested in the details of the evaluation.
3. Data and Evaluation Measures
3.1 Data
For most of our experiments we used the February 2004 release of Propbank. We also
report results on the CoNLL 2005 shared task data (Propbank I) in Section 6.2. For
the latter, we used the standard CoNLL evaluation measures, and we refer readers
to the description of that task for details of the evaluation (Carreras and Ma`rquez
2005). In this section we describe the data and evaluation measures we used for the
February 2004 data. We use our own set of measures on the February 2004 data for
three reasons. Firstly, we wish to present a richer set of measures, which can better
illustrate the performance of the system on core arguments as against adjuncts and
the performance on identifying versus classifying arguments. Secondly, we technically
could not use the CoNLL measure on the February 2004 data, because this earlier
data was not available in a format which specifies which arguments should have the
additional R-ARGX labels used in the CoNLL evaluation.3 Finally, these measures are
better for comparison with early papers, because most research before 2005 did not
distinguish referring arguments. We describe our argument-based measures in detail
here in case researchers are interested in replicating our results for the February 2004
data.
For the February 2004 data, we used the standard split into training, development,
and test sets?the annotations from sections 02?21 formed the training set, section 24 the
development, and section 23 the test set. The set of argument labels considered is the
set of core argument labels (ARG0 through ARG5) plus the modifier labels (see Figure 1).
The training set contained 85,392 propositions, the test set 4,615, and the development
set 2,626.
We evaluate semantic role labeling models on gold-standard parse trees and parse
trees produced by Charniak?s automatic parser (Charniak 2000). For gold-standard
parse trees, we preprocess the trees to discard empty constituents and strip functional
tags. Using the trace information provided by empty constituents is very useful for
improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al
2005), but we have not used this information so that we can compare our results to
previous work and since automatic systems that recover it are not widely available.
3.2 Evaluation Measures
Since 2004, there has been a precise, standard evaluation measure for semantic role la-
beling, formulated by the organizers of the CoNLL shared tasks (Carreras and Ma`rquez
3 Currently, a script which can convert original Propbank annotations to CoNLL format is available as part
of the CoNLL software distribution.
166
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 1
Labels of modifying arguments occurring in Propbank.
2004, 2005). An evaluation script is also distributed as part of the provided software for
the shared task and can be used to evaluate systems on Propbank I data.
For papers published between 2000 and 2005, there are several details of the eval-
uation measures for semantic role labeling that make it difficult to compare results
obtained by different researchers, because researchers use their own implementations
of evaluation measures, without making all the exact details clear in their papers. The
first issue is the existence of arguments consisting of multiple constituents. In this case
it is not clear whether partial credit is to be given for guessing only some of the con-
stituents comprising the argument correctly. The second issue is whether the bracketing
of constituents should be required to be recovered correctly, in other words, whether
pairs of labelings, such as [the]ARG0 [man]ARG0 and [the man]ARG0 are to be considered the
same or not. If they are considered the same, there are multiple labelings of nodes in
a parse tree that are equivalent. The third issue is that when using automatic parsers,
some of the constituents that are fillers of semantic roles are not recovered by the parser.
In this case it is not clear how various research groups have scored their systems (using
headword match, ignoring these arguments altogether, or using exact match). If we
vary the choice taken for these three issues, we can come up with many (at least eight)
different evaluationmeasures, and these details are important, because different choices
can lead to rather large differences in reported performance.
Here we describe in detail our evaluation measures for the results on the February
2004 data reported in this article. The measures are similar to the CoNLL evaluation
measure, but report a richer set of statistics; the exact differences are discussed at the
end of this section.
For both gold-standard and automatic parses we use one evaluation measure,
which we call argument-based evaluation. To describe the evaluation measure, we will
use as an example the correct and guessed semantic role labelings shown in Figures 2(a)
and 2(b). Both are shown as labelings on parse tree nodes with labels of the form ARGX
and C-ARGX. The label C-ARGX is used to represent multi-constituent arguments. A con-
stituent labeled C-ARGX is assumed to be a continuation of the closest constituent to the
left labeled ARGX. Our semantic role labeling system produces labelings of this form and
the gold standard Propbank annotations are converted to this form as well.4 The evalua-
tion is carried out individually for each predicate and its associated argument frame. If a
sentence contains several clauses, the several argument frames are evaluated separately.
4 This representation is not powerful enough to represent all valid labelings of multi-constituent
arguments, because it cannot represent the case where a new argument with label ARGX starts before
a previous multi-constituent argument with the same label ARGX has finished. This case, however, is
very rare.
167
Computational Linguistics Volume 34, Number 2
Our argument-based measures do not require exact bracketing (if the set of words
constituting an argument is correct, there is no need to know how this set is broken
into constituents) and do not give partial credit for labeling correctly only some of
several constituents in a multi-constituent argument. They are illustrated in Figure 2.
For these measures, a semantic role labeling of a sentence is viewed as a labeling on sets
of words. These sets can encompass several non-contiguous spans. Figure 2(c) gives the
representation of the correct and guessed labelings shown in Figures 2(a) and 2(b), in the
first and second rows of the table, respectively. To convert a labeling on parse tree nodes
to this form, we create a labeled set for each possibly multi-constituent argument. All
remaining sets of words are implicitly labeled with NONE. We can see that, in this way,
exact bracketing is not necessary and also no partial credit is given when only some of
several constituents in a multi-constituent argument are labeled correctly.
We will refer to word sets as ?spans.? To compute the measures, we are comparing
a guessed set of labeled spans to a correct set of labeled spans. We briefly define the
various measures of comparison used herein, using the example guessed and correct
Figure 2
Argument-based scoring measures for the guessed labeling.
168
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
labelings shown in Figure 2(c). All spans not listed explicitly are assumed to have label
NONE. The scoring measures are illustrated in Figure 2(d).
The figure shows performance measures?F-Measure (F1) and Whole Frame Accu-
racy (Acc.)?across nine different conditions. When the sets of labeled spans are com-
pared directly, we obtain the complete taskmeasures, corresponding to the ID&CLS row
and ALL column in Figure 2(d). We also define several other measures to understand the
performance of the system on different types of labels. We measure the performance on
identification (ID), classification (CLS), and the complete task (ID&CLS), when consider-
ing only the core arguments (CORE), all arguments but with a single ARGM label for the
modifier arguments (COARSEARGM), and all arguments (ALL). This defines nine sub-tasks,
which we now describe. For each of them, we compute the Whole Frame Accuracy and
F-Measure as follows:
Whole Frame Accuracy (Acc.). This is the percentage of propositions for which
there is an exact match between the proposed and correct labelings. For example, the
whole frame accuracy for ID&CLS and ALL is 0, because the correct and guessed sets
of labeled spans shown in Figure 2(c) do not match exactly. In the figures, ?Acc.? is
always an abbreviation for this whole frame accuracy. Even though this measure has
not been used extensively in previous work, we find it useful to track. Most importantly,
potential applications of role labeling may require correct labeling of all (or at least the
core) arguments in a sentence in order to be effective, and partially correct labelings may
not be very useful. Moreover, a joint model for semantic role labeling optimizes Whole
Frame Accuracy more directly than a local model does.
F-Measure (F1). Because there may be confusion about what wemean by F-Measure
in this multi-class setting, we define it here. F-Measure is defined as the harmonic mean
of precision and recall: f = 2?p?rp+r ; p =
true positive
true positive+false positive ; r =
true positive
true positve+false negative .
This formula uses the number of true positive, false positive, and false nega-
tive spans in a given guessed labeling. True positive is the number of spans whose correct
label is one of the core or modifier argument labels (not NONE) and whose guessed label
is the same as the correct label. False positive is the number of spans whose guessed
label is non-NONE and whose correct label is different from the guessed label (possibly
NONE). False negative is the number of spans whose correct label is non-NONE andwhose
guessed label is not the same as the correct one (possibly NONE). In the figures in this
paper we show F-Measure multiplied by 100 so that it is in the same range as Whole
Frame Accuracy.
Core Argument Measures (CORE). These measures score the system on core
arguments only, without regard to modifier arguments. They can be obtained by first
mapping all non-core argument labels in the guessed and correct labelings to NONE.
Coarse Modifier Argument Measures (COARSEARGM). Sometimes it is sufficient to
know a given span has a modifier role, without knowledge of the specific role label. In
addition, deciding exact modifier argument labels was one of the decisions with highest
disagreement among annotators (Palmer, Gildea, and Kingsbury 2005). To estimate
performance under this setting, we relabel all ARGM-X arguments to ARGM in the
proposed and correct labeling. Such a performance measure was also used by Xue and
Palmer (2004). Note that these measures do not exclude the core arguments but instead
consider the core plus a coarse version of the modifier arguments. Thus for COARSEARGM
169
Computational Linguistics Volume 34, Number 2
ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive,
and {1, 2, 3, 4} and {7, 8, 9} as false negative.
Identification Measures (ID). These measure how well we do on the ARG vs. NONE
distinction. For the purposes of this evaluation, all spans labeled with a non-NONE label
are considered to have the generic label ARG. For example, to compute CORE ID, we
compare the following sets of labeled spans:
Correct: {0}-ARG, {7, 8, 9}-ARG
Guessed: {0}-ARG, {7, 8, 9}-ARG
The F-Measure is 1.0 and the Whole Frame Accuracy is 100%.
Classification Measures (CLS). These are performance on argument spans which
were also guessed to be argument spans (but possibly the exact label was wrong). In
other words, thesemeasures ignore the ARG vs. NONE confusions. They ignore all spans,
which were incorrectly labeled NONE, or incorrectly labeled with an argument label,
when the correct label was NONE. This is different from ?classification accuracy? used
in previous work to mean the accuracy of the system in classifying spans when the
correct set of argument spans is given. To compute CLS measures, we remove all spans
from Sguessed and Scorrect that do not occur in both sets, and compare the resulting sets.
For example, to compute the ALL CLS measures, we need to compare the following sets
of labeled spans:
Correct: {0}-ARG0, {7, 8, 9}-ARG2
Guessed: {0}-ARG0, {7, 8, 9}-ARG3
The rest of the spans were removed from both sets because they were labeled NONE
according to one of the labelings and non-NONE according to the other. The F-Measure
is .50 and the Whole Frame Accuracy is 0%.
As we mentioned before, we label and evaluate the semantic frame of every
predicate in the sentence separately. It is possible for a sentence to contain several
propositions?annotations of predicates occurring in the sentence. For example, in the
sentence The spacecraft faces a six-year journey to explore Jupiter, there are two propositions,
for the verbs faces and explore. These are:
[The spacecraft]ARG0 [faces]PRED [a six-year journey to explore Jupiter]ARG1.
[The spacecraft]ARG0 faces a six-year journey to [explore]PRED [Jupiter]ARG1.
Our evaluation measures compare the guessed and correct set of labeled spans for each
proposition.
3.3 Relation to the CoNLL Evaluation Measure
The CoNLL evaluation measure (Carreras and Ma`rquez 2004, 2005) is almost the same
as our argument-based measure. The only difference is that the CoNLL measure intro-
duces an additional label type for arguments, of the form R-ARGX, used for referring ex-
170
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
pressions. The Propbank distribution contains a specification of whichmulti-constituent
arguments are in a coreference chain. The CoNLL evaluation script considers these
multi-constituent arguments as several separate arguments having different labels,
where one argument has an ARGX label and the others have R-ARGX labels. The decision
of which constituents were to be labeled with referring labels was made using a set of
rules expressed with regular expressions.5 A script that converts Propbank annotations
to CoNLL format is available as part of the shared task software.
For example, in the following sentence, the CoNLL specification annotates the
arguments of began as follows:
[The deregulation]ARG1 of railroads [that]R-ARG1 [began]PRED enabled shippers to bargain
for transportation.
In contrast, we treat all multi-constituent arguments in the same way, and do not
distinguish coreferential versus non-coreferential split arguments. According to our
argument-based evaluation, the annotation of the arguments of the verb began is:
[The deregulation]ARG1 of railroads [that]C-ARG1 [began]PRED enabled shippers to bargain
for transportation.
The difference between our argument based measure and the CoNLL evaluation
measure is such that we cannot say that the value of one is always higher than the value
of the other. Either measure could be higher depending on the kinds of errors made.
For example, if the guessed labeling is: [The deregulation]ARG0 of railroads [that]R-ARG1
[began]PRED enabled shippers to bargain for transportation, the CoNLL script would
count the argument that as correct and report precision and recall of .5, whereas our
argument-based measure would not count any argument correct and report precision
and recall of 0. On the other hand, if the guessed labeling is [The deregulation]ARG1 of
railroads [that]C-ARG1 [began]PRED enabled shippers to bargain for transportation, the CoNLL
measure would report a precision and recall of 0, whereas our argument-based measure
would report precision and recall of 1. If the guessed labeling is [The deregulation]ARG1
of railroads [that]R-ARG1 [began]PRED enabled shippers to bargain for transportation, both
measures would report precision and recall of 1. (For our argument-based measure
it does not make sense to propose R-ARGX labels and we assume such labels would
be converted to C-ARGX labels if they are after the phrase they refer to.) Nevertheless,
overall we expect the two measures to yield very similar results.
4. Local Classifiers
A classifier is local if it assigns a probability (or score) to the label of an individual parse
tree node ni independently of the labels of other nodes.
In defining our models, we use the standard separation of the task of semantic role
labeling into identification and classification phases. Formally, let L denote a mapping
of the nodes in a tree t to a label set of semantic roles (including NONE) with respect to
a predicate v. Let Id(L) be the mapping which collapses L?s non-NONE values into ARG.
5 The regular expressions look for phrases containing pronouns with part-of-speech tags WDT, WRB, WP,
or WP$ (Xavier Carreras, personal communication).
171
Computational Linguistics Volume 34, Number 2
Then, like the Gildea and Jurafsky (2002) system, we decompose the probability of a
labeling L into probabilities according to an identification model PID and a classification
model PCLS.
PSRL(L|t, v) = PID(Id(L)|t, v)PCLS(L|t, v, Id(L)) (1)
This decomposition does not encode any independence assumptions, but is a use-
ful way of thinking about the problem. Our local models for semantic role labeling
use this decomposition. We use the same features for local identification and classi-
fication models, but use the decomposition for efficiency of training. The identifica-
tion models are trained to classify each node in a parse tree as ARG or NONE, and
the classification models are trained to label each argument node in the training set
with its specific label. In this way the training set for the classification models is
smaller. Note that we do not do any hard pruning at the identification stage in testing
and can find the exact labeling of the complete parse tree, which is the maximizer of
Equation (1).
We use log-linear models for multi-class classification for the local models. Because
they produce probability distributions, identification and classification models can be
chained in a principled way, as in Equation (1). The baseline features we used for the
local identification and classification models are outlined in Figure 3. These features are
a subset of the features used in previous work. The standard features at the top of the
figure were defined by Gildea and Jurafsky (2002), and the rest are other useful lexical
and structural features identified in more recent work (Surdeanu et al 2003; Pradhan
et al 2004; Xue and Palmer 2004). We also incorporated several novel features which
we describe next.
Figure 3
Baseline features.
172
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 4
Example of displaced arguments.
4.1 Additional Features for Displaced Constituents
We found that a large source of errors for ARG0 and ARG1 stemmed from cases such
as those illustrated in Figure 4, where arguments were dislocated by raising or control
verbs. Here, the predicate, expected, does not have a subject in the typical position?
indicated by the empty NP?because the auxiliary is has raised the subject to its current
position. In order to capture this class of examples, we use a binary feature, MISSING
SUBJECT, indicating whether the predicate is ?missing? its subject, and use this feature
in conjunction with the PATH feature, so that we learn typical paths to raised subjects
conditioned on the absence of the subject in its typical position.6
In the particular case of Figure 4, there is another instance of an argument being
quite far from its predicate. The predicate widen shares the phrase the trade gap with
expect as an ARG1 argument. However, as expect is a raising verb, widen?s subject is not
in its typical position either, and we should expect to find it in the same position as
expected?s subject. This indicates it may be useful to use the path relative to expected
to find arguments for widen. In general, to identify certain arguments of predicates
embedded in auxiliary and infinitival VPs we expect it to be helpful to take the path
from the maximum extended projection of the predicate?the highest VP in the chain
of VPs dominating the predicate. We introduce a new path feature, PROJECTED PATH,
which takes the path from the maximal extended projection to an argument node. This
feature applies only when the argument is not dominated by the maximal projection
(e.g., direct objects). These features also handle other cases of discontinuous and non-
local dependencies, such as those arising due to control verbs. The performance gain
from these new features was notable, especially in identification. The performance on
ALL arguments for the model using only the features in Figure 3, and the model using
the additional features as well, are shown in Figure 5. For these results, the constraint
that argument phrases do not overlap was enforced using the algorithm presented in
Section 4.2.
4.2 Enforcing the Non-Overlapping Constraint
The most direct way to use trained local identification and classification models in
testing is to select a labeling L of the parse tree that maximizes the product of the
6 We consider a verb to be missing its subject if the highest VP in the chain of VPs dominating the verb
does not have an NP or S(BAR) as its immediate left sister.
173
Computational Linguistics Volume 34, Number 2
Figure 5
Performance of local classifiers on ALL arguments, using the features in Figure 3 only and using
the additional local features. Using gold standard parse trees on Section 23.
probabilities according to the two models, as in Equation (1). Because these models are
local, this is equivalent to independently maximizing the product of the probabilities of
the twomodels for the label li of each parse tree node ni as shown below in Equation (2).
PSRL(L|t, v) =
?
ni?t
PID(Id(li)|t, v)
?
ni?t
PCLS(li|t, v, Id(li)) (2)
A problem with this approach is that a maximizing labeling of the nodes could possibly
violate the constraint that argument nodes should not overlap with each other. There-
fore, to produce a consistent set of arguments with local classifiers, we must have a way
of enforcing the non-overlapping constraint.
When labeling parse tree nodes, previous work has either used greedy algorithms
to find a non-overlapping assignment, or the general-purpose ILP approach of
Punyakanok et al (2004). For labeling chunks an exact algorithm based on shortest
paths was proposed in Punyakanok and Roth (2001). Its complexity is quadratic in the
length of the sentence.
Here we describe a faster exact dynamic programming algorithm to find the most
likely non-overlapping (consistent) labeling of all nodes in the parse tree, according
to a product of probabilities from local models, as in Equation (2). For simplicity, we
describe the dynamic program for the case where only two classes are possible: ARG and
NONE. The generalization to more classes is straightforward. Intuitively, the algorithm
is similar to the Viterbi algorithm for context-free grammars, because we can describe
the non-overlapping constraint by a ?grammar? that disallows ARG nodes having ARG
descendants.
Subsequently, we will talk about maximizing the sum of the logs of local proba-
bilities rather than the product of local probabilities, which is equivalent. The dynamic
program works from the leaves of the tree up and finds a best assignment for each
subtree, using already computed assignments for its children. Suppose we want the
most likely consistent assignment for subtree t with child trees t1, . . . , tk each storing
the most likely consistent assignment of its nodes, as well as the log-probability of the
ALLNONE assignment: the assignment of NONE to all nodes in the tree. The most likely
assignment for t is the one that corresponds to the maximum of:
 The sum of the log-probabilities of the most likely assignments of the child
subtrees t1, . . . , tk plus the log-probability of assigning the node t to NONE.
 The sum of the log-probabilities of the ALLNONE assignments of t1, . . . , tk
plus the log-probability of assigning the node t to ARG.
174
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 6
Performance of local model on ALL arguments when enforcing the non-overlapping constraint
or not.
The log-probability of the ALLNONE assignment for a tree t is the log-probability
of assigning the root node of t to NONE plus the sum of the log-probabilities of the
ALLNONE assignments of the child subtrees of t.
Propagating this procedure from the leaves to the root of t we have our most
likely non-overlapping assignment. By slightly modifying this procedure, we obtain the
most likely assignment according to a product of local identification and classification
models. We use the local models in conjunction with this search procedure to select a
most-likely labeling in testing.
The complexity of this algorithm is linear in the number of nodes in the parse tree,
which is usually much less than the square of the number of words in the sentence (l2),
the complexity of the Punyakanok and Roth (2001) algorithm. For example, for a binary-
branching parse tree, the number of nodes is approximately 2l. The speedup is due to
the fact that when we label parse tree nodes, we make use of the bracketing constraints
imposed by the parse tree. The shortest path algorithm proposed by Punyakanok and
Roth can also be adapted to achieve this lower computational complexity.
It turns out that enforcing the non-overlapping constraint does not lead to large
gains in performance. The results in Figure 5 are frommodels that use the dynamic pro-
gram for selecting non-overlapping arguments. To evaluate the gain from enforcing the
constraint, Figure 6 shows the performance of the same local model using all features,
when the dynamic program is used versus when a most likely possibly overlapping
assignment is chosen in testing.
The local model with basic plus additional features is our first pass model used
in re-ranking. The non-overlapping constraint is enforced using the dynamic program.
This is a state-of-the-art model. Its F-Measure on ALL arguments is 88.4 according to our
argument-based scoring measure. This is very similar to the best reported results (as of
2004) using gold-standard parse trees without null constituents and functional tags: 89.4
F-Measure reported for the Pradhan et al (2004) model.7
A more detailed analysis of the results obtained by the local model is given in Fig-
ure 7(a), and the two confusion matrices in Figures 7(b) and 7(c), which display the
number of errors of each type that the model made. The first confusion matrix con-
centrates on CORE arguments and merges all modifying argument labels into a single
ARGM label. The second concentrates on confusions among modifying arguments.
From the confusion matrix in Figure 7(b), we can see that the largest number of
errors are confusions of argument labels with NONE. The number of confusions between
pairs of core arguments is low, as is the number of confusions between core andmodifier
labels. If we ignore the column and row corresponding to NONE in Figure 7(b), the
number of off-diagonal entries is very small. This corresponds to the high F-Measures
7 The results in Pradhan et al (2004) are based on a measure which is more lenient than our
argument-based scoring (Sameer Pradhan, personal communication, July 2005). Our estimated
performance using his measure is 89.9.
175
Computational Linguistics Volume 34, Number 2
Figure 7
Performance measures for local model using all local features and enforcing the
non-overlapping constraint. Results are on Section 23 using gold standard parse trees.
on COARSEARGM CLS and CORE CLS, 98.1 and 98.0 respectively, shown in Figure 7(a).
The number of confusions of argument labels with NONE, shown in the NONE column,
is larger than the number of confusions of NONE with argument labels, shown in the
NONE row. This shows that the model generally has higher precision than recall. We
experimented with the precision?recall tradeoff but this did not result in an increase in
F-Measure.
From the confusion matrix in Figure 7(c) we can see that the number of confusions
between modifier argument labels is higher than the number of confusions between
core argument labels. This corresponds to the ALL CLS F-Measure of 95.7 versus the
CORE CLS F-Measure of 98.0. The per-label F-Measures in the last column show that the
performance on some very frequent modifier labels is in the low sixties or seventies.
The confusions between modifier labels and NONE are quite numerous.
Thus, to improve the performance on CORE arguments, we need to improve recall
without lowering precision. In particular, when the model is uncertain which of several
likely CORE labels to assign, we need to find additional sources of evidence to improve
its confidence. To improve the performance on modifier arguments, we also need to
lower the confusions among different modifier arguments. We will see that our joint
model improves the overall performance mainly by improving the performance on
176
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
CORE arguments, through increasing recall and precision by looking at wider sentence
context.
4.3 On Split Constituents
As discussed in Section 3, multiple constituents can be part of the same semantic
argument as specified by Propbank. An automatic system that has to recover such
information needs to have a way of indicating when multiple constituents labeled with
the same semantic role are a part of the same argument. Some researchers (Pradhan et al
2004; Punyakanok et al 2004) have chosen to make labels of the form C-ARGX distinct
argument labels that become additional classes in a multi-class constituent classifier.
These C-ARGX are used to indicate continuing arguments as illustrated in the two trees
in Figure 2. We chose to not introduce additional labels of this form, because they might
unnecessarily fragment the training data. Our automatic classifiers label constituents
with one of the core or modifier semantic role labels, and a simple post-processing
rule is applied to the output of the system to determine which constituents that are
labeled the same are to be merged as the same argument. The post-processing rule is
the following: For every constituent that bears a core argument label ARGX, if there is
a preceding constituent with the same label, re-label the current constituent C-ARGX.
Therefore, according to our algorithm, all constituents having the same core argument
label are part of the same argument, and all constituents having the samemodifier labels
are separate arguments by themselves. This rule is fairly accurate for core arguments but
is not always correct; it fails more often on modifier arguments. An evaluation of this
rule using the CoNLL data set and evaluation measure shows that our upper bound in
performance because of this rule is approximately 99.0 F-Measure on ALL arguments.
5. Joint Classifiers
We proceed to describe our models incorporating dependencies between labels of nodes
in the parse tree. As we discussed briefly before, the dependencies we would like to
model are highly non-local. A factorized sequence model that assumes a finite Markov
horizon, such as a chain CRF (Lafferty, McCallum, and Pereira 2001), would not be
able to encode such dependencies. We define a CRF with a much richer dependency
structure.
5.1 Form of the Joint Classifiers
Motivation for Re-Ranking. For argument identification, the number of possible as-
signments for a parse tree with n nodes is 2n. This number can run into the hundreds
of billions for a normal-sized tree. For argument labeling, the number of possible
assignments is ? 20m, if m is the number of arguments of a verb (typically between
2 and 5), and 20 is the approximate number of possible labels if considering both core
and modifying arguments. Training a model which has such a huge number of classes
is infeasible if the model does not factorize due to strong independence assumptions.
Therefore, in order to be able to incorporate long-range dependencies in our models,
we chose to adopt a re-ranking approach (Collins 2000), which selects from likely as-
signments generated by a model which makes stronger independence assumptions. We
utilize the top n assignments of our local semantic role labeling model PSRL to generate
likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our
177
Computational Linguistics Volume 34, Number 2
re-ranking approach does not present a serious bottleneck to performance. We used a
value of n = 10 for training. In Figure 8(a) we can see that if we could pick, using an
oracle, the best assignment out of the top 10 assignments according to the local model,
we would achieve an F-Measure of 97.3 on all arguments. Increasing the number of n to
30 results in a very small gain in the upper bound on performance and a large increase
in memory requirements. We therefore selected n = 10 as a good compromise.
Generation of top n Most Likely Joint Assignments. We generate the top n most
likely non-overlapping joint assignments of labels to nodes in a parse tree according
to a local model PSRL, using an exact dynamic programming algorithm, which is a
direct generalization of the algorithm for finding the top non-overlapping assignment
described in Section 4.2.
Parametric Models.We learn log-linear re-ranking models for joint semantic role label-
ing, which use feature maps from a parse tree and label sequence to a vector space. The
form of the models is as follows. Let ?(t, v,L) ? Rs denote a feature map from a tree t,
target verb v, and joint assignment L of the nodes of the tree, to the vector space Rs. Let
L1,L2, ? ? ? ,LN denote the top N possible joint assignments. We learn a log-linear model
with a parameter vectorW, with one weight for each of the s dimensions of the feature
vector. The probability (or score) of an assignment L according to this re-ranking model
is defined as
PrSRL(L|t, v) =
e<?(t,v,L),W>
?N
j=1 e
<?(t,v,Lj ),W>
(3)
The score of an assignment L not in the top n is zero. We train the model to maximize the
sum of log-likelihoods of the best assignments minus a quadratic regularization term.
In this framework, we can define arbitrary features of labeled trees that capture general
properties of predicate?argument structure.
5.2 Joint Model Features
Wewill introduce the features of the joint re-rankingmodel in the context of the example
parse tree shown in Figure 9. We model dependencies not only between the label of a
Figure 8
Oracle upper bounds for top n non-overlapping assignments from local model on CORE and ALL
arguments, using gold-standard parse trees.
178
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 9
An example tree from Propbank with semantic role annotations, for the sentence Final-hour
trading accelerated to 108.1 million shares yesterday.
node and the labels of other nodes, but also dependencies between the label of a node
and input features of other argument nodes. The features are specified by instantiation
of templates and the value of a feature is the number of times a particular pattern occurs
in the labeled tree.
For a tree t, predicate v, and joint assignment L of labels to the nodes of the tree, we
define the candidate argument sequence as the sequence of non-NONE labeled nodes
[n1, l1, . . . , vPRED, . . . ,nm, lm] (li is the label of node ni). A reasonable candidate argument
sequence usually contains very few of the nodes in the tree?about 2 to 7?as this is
the typical number of arguments for a verb. To make it more convenient to express
our feature templates, we include the predicate node v in the sequence. This sequence
of labeled nodes is defined with respect to the left-to-right order of constituents in the
parse tree. Because non-NONE labeled nodes do not overlap, there is a strict left-to-right
order among these nodes. The candidate argument sequence that corresponds to the
correct assignment in Figure 9 is then:
[NP1-ARG1, VBD1-PRED, PP1-ARG4, NP3-ARGM-TMP]
Features from Local Models.All features included in the local models are also included
in our joint models. In particular, each template for local features is included as a joint
template that concatenates the local template and the node label. For example, for the
local feature PATH, we define a joint feature template that extracts PATH from every
node in the candidate argument sequence and concatenates it with the label of the
node. Both a feature with the specific argument label and a feature with the generic
back-off ARG label are created. This is similar to adding features from identification
and classification models. In the case of the example candidate argument sequence
provided, for the node NP1 we have the features:
{(NP?S?VP?VBD)-ARG1, (NP?S?VP?VBD)-ARG}
When comparing a local and a joint model, we use the same set of local feature
templates in the two models. If these were the only features that a joint model used,
we would expect its performance to be roughly the same as the performance of a
local model. This is because the two models will in fact be in the same parametric
family but will only differ slightly in the way the parameters are estimated. In
particular, the likelihood of an assignment according to the joint model with local
features will differ from the likelihood of the same assignment according to the local
model only in the denominator (the partition function). The joint model sums over
179
Computational Linguistics Volume 34, Number 2
a few likely assignments in the denominator, whereas the local model sums over all
assignments; also, the joint model does not treat the decomposition into identification
and classification models in exactly the same way as the local model.
Whole Label Sequence Features. As observed in previous work (Gildea and Jurafsky
2002; Pradhan et al 2004), including information about the set or sequence of labels
assigned to argument nodes should be very helpful for disambiguation. For example,
including such information will make the model less likely to pick multiple nodes to
fill the same role or to come up with a labeling that does not contain an obligatory
argument. We added a whole label sequence feature template that extracts the labels
of all argument nodes, and preserves information about the position of the predicate.
Two templates for whole label sequences were added: one having the predicate voice
only, and another also including the predicate lemma. These templates are instantiated
as follows for the example candidate argument sequence:
[voice:active, ARG1, PRED, ARG4, ARGM-TMP]
[voice:active, lemma:accelerate, ARG1, PRED, ARG4, ARGM-TMP]
We also add variants of these templates that use a generic ARG label instead of
specific labels for the arguments. These feature templates have the effect of counting the
number of arguments to the left and right of the predicate, which provides useful global
information about argument structure. A local model is not able to represent the count
of arguments since the label of each node is decided independently. This feature can
very directly and succinctly encode preferences for required arguments and expected
number of arguments.
As previously observed (Pradhan et al 2004), including modifying arguments in
sequence features is not helpful. This corresponds to the standard linguistic understand-
ing that there are no prevalent constraints on the position or presence of adjuncts in an
argument frame, and was confirmed in our experiments. We redefined the whole label
sequence features to exclude modifying arguments.
The whole label sequence features are the first type of features we add to relax
the independence assumptions of the local model. Because these features look at
the sequence of labels of all arguments, they capture joint information. There is no
limit on the length of the label sequence and thus there is no n-gram Markov order
independence assumption (in practice the candidate argument sequences in the top
n complete assignments are rarely more than 7 nodes long). Additionally, the nodes
in the candidate argument sequences are in general not in the same local tree in the
syntactic analysis and a tree-CRF model (Cohn and Blunsom 2005) would not be able
to encode these dependencies.
Joint Syntactic?Semantic Features. This class of features is similar to the whole label
sequence features, but in addition to labels of argument nodes, it includes syntactic
features of the nodes. These features can capture the joint mapping from the syntactic
realization of the predicate?s arguments to its semantic frame. The idea of these features
is to capture knowledge about the label of a constituent given the syntactic realization
and labels of all other arguments of the verb. This is helpful in capturing syntactic
alternations, such as the dative alternation. For example, consider the sentence (i) [Shaw
Publishing]ARG0 [offered]PRED [Mr. Smith]ARG2 [a reimbursement]ARG1 and the alternative re-
alization (ii) [Shaw Publishing]ARG0 [offered]PRED [a reimbursement]ARG1 [to Mr. Smith]ARG2.
180
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
When classifying the NP in object position, it is useful to know whether the following
argument is a PP. If it is, the NP will more likely be an ARG1, and if not, it will more
likely be an ARG2. A feature template that captures such information extracts, for each
candidate argument node, its phrase type and label. For example, the instantiations of
such templates in (ii), including only the predicate voice or also the predicate lemma,
would be:
[voice:active, NP-ARG0, PRED, NP-ARG1, PP-ARG2]
[voice:active,lemma:offer, NP-ARG0, PRED, NP-ARG1, PP-ARG2]
We experimented with extracting several kinds of features from each argument
node and found that the phrase type and the head of a directly dominating PP?if one
exists?were most helpful.
Local models normally consider only features of the phrase being classified in
addition to features of the predicate. They cannot take into account the features of other
argument nodes, because they are only given the input (parse tree), and the identity of
the argument nodes is unknown. It is conceivable that a local model could condition on
the features of all nodes in the tree but the number of parameters (features) would be
extremely large. The joint syntactic?semantic features proposed here encode important
dependencies using a very small number of parameters, as we will show in Section 5.4.
We should note that Xue and Palmer (2004) define a similar feature template, called
syntactic frame, which often captures similar information. The important difference is
that their template extracts contextual information from noun phrases surrounding the
predicate, rather than from the sequence of argument nodes. Because we use a joint
model, we are able to use information about other argument nodes when labeling a
node.
Repetition Features. We also add features that detect repetitions of the same label in
a candidate argument sequence, together with the phrase types of the nodes labeled
with that label. For example, (NP-ARG0, WHNP-ARG0) is a common pattern of this form.
Variants of this feature template also indicate whether all repeated arguments are sisters
in the parse tree, or whether all repeated arguments are adjacent in terms of word spans.
These features can provide robustness to parser errors, making it more likely to assign
the same label to adjacent phrases that may have been incorrectly split by the parser. In
Section 5.4 we report results from the joint model and an ablation study to determine
the contribution of each of the types of joint features.
5.3 Applying Joint Models in Testing
Here we describe the application in testing of a joint model for semantic role labeling,
using a local model PSRL and a joint re-ranking model P
r
SRL. The local model P

SRL is used
to generate N non-overlapping joint assignments L1, . . . ,LN.
One option is to select the best Li according to P
r
SRL, as in Equation (3), ignoring the
score from the local model. In our experiments, we noticed that for larger values of N,
the performance of our re-ranking model PrSRL decreased. This was probably due to the
fact that at test time the local classifier produces very poor argument frames near the
bottom of the top n for large n. Because the re-ranking model is trained on relatively
181
Computational Linguistics Volume 34, Number 2
few good argument frames, it cannot easily rule out very bad frames. It makes sense
then to incorporate the local model into our final score. Our final score is given by:
PSRL(L|t, v) = (PSRL(L|t, v))
? PrSRL(L|t, v)
where ? is a tunable parameter determining the amount of influence the local score has
on the final score (we found ? = 1.0 to work best). Such interpolation with a score from
a first-pass model was also used for parse re-ranking in (Collins 2000). Given this score,
at test time we choose among the top n local assignments L1, . . . ,Ln according to:
argmaxL?L1,...,Ln ? logP

SRL(L|t, v)+ logP
r
SRL(L|t, v) (4)
5.4 Joint Model Results
We compare the performance of joint re-ranking models and local models. We used
n = 10 joint assignments for training re-ranking models, and n = 15 for testing. The
weight ? of the local model was set to 1. Using different numbers of joint assignments
in training and testing is in general not ideal, but due to memory requirements, we
could not experiment with larger values of n for training.
Figure 10 shows the summary performance of the local model (LOCAL), repeated
from earlier figures, a joint model using only local features (JOINTLOCAL), a joint model
using local + whole label sequence features (LABELSEQ), and a joint model using all
described types of features (ALLJOINT). The evaluation is on gold-standard parse trees.
In addition to performance measures, the figure shows the number of binary features
included in the model. The number of features is a measure of the complexity of the
hypothesis space of the parametric model.
We can see that a joint model using only local features outperforms a local model
by .5 points of F-Measure. The joint model using local features estimates the feature
weights only using the top n consistent assignments, thus making the labels of different
nodes non-independent according to the estimation procedure, which may be a cause
of the improved performance. Another factor could be that the model JOINTLOCAL is a
combination of two models as specified in Equation (4), which may lead to gains (as is
usual for classifier combination).
The label sequence features added in Model LABELSEQ result in another 1.5 points
jump in F-Measure on all arguments. An additional .8 gain results from the inclusion
of syntactic?semantic and repetition features. The error reduction of model ALLJOINT
Figure 10
Performance of local and joint models on ID&CLS on Section 23, using gold-standard parse trees.
The number of features of each model is shown in thousands.
182
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
over the local model is 36.8% in CORE arguments F-Measure, 33.3% in CORE arguments
whole frame accuracy, 24.1% in ALL arguments F-Measure, and 21.7% in ALL arguments
whole frame accuracy. All differences in ALL arguments F-Measure are statistically
significant according to a paired Wilcoxon signed rank test. JOINTLOCAL is significantly
better than LOCAL (p < .001), LABELSEQ is significantly better than JOINTLOCAL (p <
.001), and ALLJOINT is significantly better than LABELSEQ (p < .001). We performed the
Wilcoxon signed rank test on per-proposition ALL arguments F-Measure for all models.
We also note that the joint models have fewer features than the local model. This is
due to the fact that the local model has seenmanymore negative examples and therefore
more unique features. The joint features are not very numerous compared to the local
features in the joint models. The ALLJOINT model has around 30% more features than
the JOINTLOCAL model.
These experiments showed that the label sequence features were very useful, es-
pecially on CORE arguments, increasing the F-Measure on these arguments by two
points when added to the JOINTLOCAL model. This shows that even though the
local model is optimized to use a large set of features and achieve state-of-the-art
performance, it is still advantageous to model the joint information in the sequence
of labels in a predicate?s argument frame. Additionally, the joint syntactic?semantic
features improved performance further, showing that when predicting the label of an
argument, it is useful to condition on the features of other arguments, in addition to
their labels.
A more detailed analysis of the results obtained by the joint model ALLJOINT
is given in Figure 11(a) (Summary results), and the two confusion matrices in Fig-
ures 11(b) and 11(c), which display the number of errors of each type that the model
made. The first confusion matrix concentrates on CORE arguments and merges all
modifying argument labels into a single ARGM label. The second confusion matrix
concentrates on confusions among modifying arguments. This figure can be compared
to Figure 7, which summarizes the results for the local model in the same form. The
biggest differences are in the performance on CORE arguments, which can be seen by
comparing the confusion matrices in Figures 7(b) and 11(b). The F-Measure on each
of the core argument labels has increased by at least three points: the F-Measure on
ARG2 by 5.7 points, and the F-Measure on ARG3 by eight points. The confusions of
core argument labels with NONE have gone down significantly, and also there is a
large decrease in the confusions of NONE with ARG1. There is generally a slight increase
in F-Measure on modifier labels as well, but the performance on some of the modifier
labels has gone down. This makes sense because our joint features are targeted at
capturing the dependencies among core arguments. There may be useful regularities
for modifier arguments as well, but capturing them may require different joint feature
templates.
Figure 12 lists the frequency with which each of the top k assignments from the
LOCAL model was ranked first by the re-ranking model ALLJOINT. For example, for
84.1% of the propositions, the re-ranking model chose the same assignment that the
LOCAL model would have chosen. The second best assignment according to the LOCAL
model was promoted to first 8.6% of the time. The figure shows statistics for the top ten
assignments only. The rest of the assignments, ranked 11 through 15, were chosen as
best by the re-ranking model for a total of 0.3% of the propositions.
The labeling of the tree in Figure 9 is a specific example of the kind of errors
fixed by the joint models. The local classifier labeled the first argument in the tree as
ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject
position.
183
Computational Linguistics Volume 34, Number 2
6. Semantic Role Labeling of Automatic Parses
We now evaluate our models when trained and tested using automatic parses produced
by Charniak?s parser. The Propbank training set Sections 2?21 is also the training set of
the parser. The performance of the parser is therefore better on the training set.When the
constituents of an argument do not have corresponding constituents in an automatically
produced parse tree, it will be very hard for a model to get the semantic role labeling
correct. However, this is not impossible and systems which are more robust to parser
error have been proposed (Pradhan et al 2005; Ma`rquez et al 2005). Our system can also
theoretically guess the correct set of words by labeling a set of constituents that cover
Figure 11
Performance measures for joint model using all features (AllJoint). Results are on Section 23
using gold-standard parse trees.
Figure 12
Percentage of test set propositions for which each of the top ten assignments from the Local
model was selected as best by the joint model AllJoint.
184
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 13
Percentage of argument constituents that are not present in the automatic parses of Charniak?s
parser. Constituents shows the percentage of missing constituents and Propositions shows the
percentage of propositions that have missing constituents.
the argument words, but we found that this rarely happens in practice. Figure 13 shows
the percentage of argument constituents that are missing in the automatic parse trees
produced by Charniak?s parser. We can see that the percentage of missing constituents
is quite high.
We report local and joint model results in Figures 14(a) and 14(b), respectively. As
for gold-standard parses, we test on all arguments regardless of whether they corre-
spond to constituents that have been recovered by the parser and use the samemeasures
detailed in Section 3.2. We also compare the confusion matrices for the local and joint
models, ignoring the confusions among modifier argument labels (COARSEARGM setting)
in Figure 15. The error reduction of the joint over the local model is 10.3% in CORE
arguments F-Measure and 8.3% in ALL arguments F-Measure.
6.1 Using Multiple Automatic Parse Guesses
Semantic role labeling is very sensitive to the correctness of the given parse tree, as the
results show. If an argument does not correspond to any constituent in a parse tree, or
a constituent exists but is not attached or labeled correctly, our model will have a very
hard time guessing the correct labeling.
Thus, if the syntactic parser makes errors, these errors influence directly the seman-
tic role labeling system. The theoretically correct way to propagate the uncertainty of
the syntactic parser is to consider (sum over) multiple possible parse trees, weighted by
their likelihood. In Finkel, Manning, and Ng (2006), this is approximated by sampling
Figure 14
Comparison of local and joint model results on Section 23 using Chaniak?s automatic parser.
185
Computational Linguistics Volume 34, Number 2
Figure 15
COARSEARGM argument confusion matrices for local and joint model using Charniak?s
automatic parses.
parse trees. We implement this idea by an argmax approximation, using the top k parse
trees from the parser of Charniak (2000).
We use these alternative parses as follows: Suppose t1, . . . , tk are trees for sentence
swith probabilities P(ti|s) given by the parser. Then for a fixed predicate v, let Li denote
the best joint labeling of tree ti, with score scoreSRL(Li|ti) according to our final joint
model. Then we choose the labeling Lwhich maximizes
argmaxi?{1,...,k}? logP(ti|S)+ scoreSRL(Li|ti)
This method of using multiple parse trees is very simple to implement and factors
in the uncertainty of the parser to some extent. However, according to this method (due
to the argmax operation) we are choosing a single parse and a complete semantic frame
derived from that parse. Other methods are able to derive different arguments of the
semantic frame from different syntactic annotations which maymake themmore robust
(Ma`rquez et al 2005; Pradhan, Ward et al 2005; Punyakanok, Roth, and Yih 2005).
Figure 16 shows summary results for the test set when using the top ten parses and
the joint model. The weighting parameter for the parser probabilities was ? = 1. We did
not experiment extensively with different values of ?. Preliminary experiments showed
that considering 15 parses was a bit better, and considering the top 20 was a bit worse.
6.2 Evaluation on the CoNLL 2005 Shared Task
The CoNLL 2005 data is derived from Propbank version I, which is the first official
release in 2005, whereas the results we have been reporting in the previous sections used
the pre-final February 2004 data. Using the CoNLL 2005 evaluation standard ensures
that results obtained by different groups are evaluated in exactly the same way. In
186
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 16
Performance of the joint model using the top ten parses from Charniak?s parser. Results are on
Section 23.
Propbank I, there have been several changes in the annotation conventions, as well as
error fixes and addition of new propositions. There was also a change in the way PP
arguments are annotated: In the February 2004 data some PP arguments are annotated
at the head NP child, but in Propbank I all PP arguments are annotated at the PP nodes.
In order to achieve maximal performance with respect to these annotations, it would
probably be best to change the feature definitions to account for the changes. However,
we did no adaptation of the features.
The training set consists of the annotations in Sections 2 to 21, the development set
is section 24 (Devset), and one of the test sets is section 23 (Test WSJ). The other test set
is from the Brown corpus (Test Brown). The CoNLL annotations distinguish referring
arguments, of the form R-ARGX, as discussed in Section 3.
Our approach to dealing with referring arguments and deciding when multiple
identically labeled constituents are part of the same argument was to label constituents
with only the set of argument labels and NONE and then map some of these labels
into referring or continuation labels. We converted an ARGX into a R-ARGX if and
only if the label of the constituent began with ?WH?. The rule for deciding when to
add continuation labels was the same as for our systems for the February 2004 data
described in Section 4.3: A constituent label becomes continuing if and only if it is a
core argument label and there is another constituent with the same core argument label
to the left. Therefore, for the CoNLL 2005 shared task we employ the same semantic
role labeling system, just using a different post-processing rule to map to CoNLL-style
labelings of sets of words.
We tested the upper bound in performance due to our conversion scheme in the
following way: Take the gold-standard CoNLL annotations for the development set
(including referring and continuing labels), convert these to basic argument labels of the
form ARGX, then convert the resulting labeling to CoNLL-style labeling using our rules
to recover the referring and continuing annotations. The F-Measure obtained was 99.0.
Figure 17 shows the performance of the local and joint model on one of the CoNLL
test sets?Test WSJ (Section 23)?when using gold-standard parse trees. Performance
on gold-standard parse trees was not measured in the CoNLL 2005 shared task, but we
report it here to provide a basis for comparison with the results of other researchers.
Figure 17
Results on the CoNLL WSJ Test set, when using gold-standard parse trees.
187
Computational Linguistics Volume 34, Number 2
Figure 18
Results on the CoNLL data set, when using Charniak automatic parse trees as provided in the
CoNLL 2005 shared task data.
Figure 19
Results on the CoNLL data set, using automatic parse trees from the May 2005 version of the
Charniak parser with correct treatment of forward quotes.
Next we present results using Charniak?s automatic parses on the development
and two test sets. We present results for the local and joint models using the max-
scoring Charniak parse tree. Additionally, we report results for the joint model using
the top five Charniak parse trees according to the algorithm described in Section 6.1.
The performance measures reported here are higher than the results of our submission
in the CoNLL 2005 shared task (Haghighi, Toutanova, and Manning 2005), because of
two changes. One was changing the rule that produces continuing arguments to only
add continuation labels to core argument labels; in the previous version the rule added
continuation labels to all repeated labels. Another was fixing a bug in the way the
sentences were passed in as input to Charniak?s parser, leading to incorrect analyses
of forward quotes.8
We first present results of our local and joint model using the parses provided as
part of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18. We
then report results from the same local and joint model, and the joint model using the
top five Charniak parses, where the parses have correct representation of the forward
quotes in Figure 19. For these results we used the version of the Charniak parser from
4 May 2005. The results were very similar to the results we obtained with the version
from 18March 2005. We did not experiment with the new re-ranking model of Charniak
and Johnson (2005), even though it improves upon Charniak (2000) significantly.
For comparison, the system we submitted to CoNLL 2005 had an F-Measure of
78.45 on the WSJ Test set. The winning system (Punyakanok, Roth, and Yih 2005) had
an F-Measure of 79.44 and our current system has an F-Measure of 80.32. For the Brown
Test set, our submitted version had an F-Measure of 67.71, the winning system had
67.75, and our current system has 68.81.
Figure 20 shows the per-label performance of our joint model using the top five
Charniak parse trees on the Test WSJ test set. The columns show the Precision, Recall,
F-Measure, and the total number of arguments for each label.
8 The Charniak parses provided as part of the CoNLL shared task data uniformly ignore the distinction
between forward and backward quotes and all quotes are backward. We re-ran the parser and obtained
analyses with correct treatment of quotes.
188
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
7. Conclusions
In accord with standard linguistic assumptions, we have shown that there are sub-
stantial gains to be had by jointly modeling the argument frames of verbs. This is
especially true when we model the dependencies with discriminative models capable
of incorporating non-local features. We incorporated joint information by using two
types of features: features of the complete sequence of argument labels and features
modeling dependencies between the labels of arguments and syntactic features of other
arguments.We showed that both types of features yielded significant performance gains
over a state-of-the-art local model.
For further improving performance in the presence of perfect syntactic parses, we
see at least three promising avenues for improvement. First, one could improve the
identification of argument nodes, by better handling of long-distance dependencies; for
example, by incorporatingmodels which recover the trace and null element information
in Penn Treebank parse trees, as in Levy andManning (2004). Second, it may be possible
to improve the accuracy on modifier labels, by enhancing the knowledge about the
semantic characteristics of specific words and phrases, such as by improving lexical
statistics; for instance, our performance on ARGM-TMP roles is rather worse than that
of some other groups. Finally, it is worth exploring alternative handling of multi-
constituent arguments; our current model uses a simple rule in a post-processing step
Figure 20
Per-label performance of joint model using the top five Charniak automatic parse trees on the
Test WSJ test set.
189
Computational Linguistics Volume 34, Number 2
to decide which constituents given the same label are part of the same argument. This
could be done more intelligently by the machine learning model.
Because perfect syntactic parsers do not yet exist and the major bottleneck to the
performance of current semantic role labeling systems is syntactic parser performance,
the more important question is how to improve performance in the presence of parser
errors. We explored a simple approach of choosing from among the top k parses from
Charniak?s parser, which resulted in an improvement. Other methods have also been
proposed, as we discussed in Section 2 (Ma`rquez et al 2005; Pradhan, Ward et al 2005;
Punyakanok, Roth, and Yih 2005; Yi and Palmer 2005; Finkel, Manning, and Ng 2006).
This is a very promising line of research.
Acknowledgments
This research was carried out while all the
authors were at Stanford University. We
thank the journal reviewers and the
reviewers and audience at ACL 2005 and
CoNLL 2005 for their helpful comments. We
also thank Dan Jurafsky for his insightful
comments and useful discussions. This work
was supported in part by the Disruptive
Technology Organization (DTO)?s Advanced
Question Answering for Intelligence
(AQUAINT) Program.
References
Baker, Collin, Charles Fillmore, and John
Lowe. 1998. The Berkeley Framenet
project. In Proceedings of COLING-ACL,
pages 86?90, San Francisco, CA.
Carreras, Xavier and Lu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of CoNLL, pages 89?97, Boston, MA.
Carreras, Xavier and Lu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of CoNLL, pages 152?164, Ann Arbor, MI.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL, pages 132?139,
Seattle, WA.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In Proceedings of
ACL, pages 173?180, Ann Arbor, MI.
Cohn, Trevor and Philip Blunsom. 2005.
Semantic role labelling with tree
conditional random fields. In Proceedings of
CoNLL, pages 169?172, Ann Arbor, MI.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of ICML, pages 175?182,
Stanford, CA.
Finkel, Jenny, Christopher Manning, and
Andrew Ng. 2006. Solving the problem of
cascading errors: Approximate bayesian
inference for linguistic annotation
pipelines. In Proceedings of EMNLP,
pages 618?626, Sydney, Australia.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Hacioglu, Kadri. 2004. A lightweight
semantic chunking model based on
tagging. In Proceedings of HLT-NAACL:
Short Papers, pages 145?148, Boston, MA.
Haghighi, Aria, Kristina Toutanova, and
Christopher D. Manning. 2005. A joint
model for semantic role labeling. In
Proceedings of CoNLL, pages 173?176,
Ann Arbor, MI.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML, pages 282?289,
Williamstown, MA.
Levy, Roger and Chris Manning. 2004.
Deep dependencies from context-free
statistical parsers: correcting the
surface dependency approximation.
In Proceedings of ACL, pages 327?334,
Barcelona, Spain.
Ma`rquez, Lu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of EMNLP,
pages 644?651, Vancouver, Canada.
McCallum, Andrew, Dayne Freitag, and
Fernando Pereira. 2000. Maximum entropy
Markov models for information extraction
and segmentation. In Proceedings of ICML,
pages 591?598, Stanford, CA.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?105.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James Martin,
and Dan Jurafsky. 2005. Support
vector learning for semantic argument
190
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
classification.Machine Learning Journal,
60(1):11?39.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
Proceedings of HLT-NAACL, pages 233?240,
Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Daniel
Jurafsky. 2005. Semantic role labeling using
different syntactic views. In Proceedings of
ACL, pages 581?588, Ann Arbor, MI.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference.
In Proceedings of NIPS, pages 995?1001,
Vancouver, Canada.
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2005. The necessity of syntactic
parsing for semantic role labeling. In
Proceedings of IJCAI, pages 1117?1123,
Acapulco, Mexico.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
Dav Zimak, and Yuancheng Tu. 2004.
Semantic role labeling via generalized
inference over classifiers. In Proceedings
of CoNLL, pages 130?133, Boston, MA.
Roland, Douglas and Daniel Jurafsky. 2002.
Verb sense and verb subcategorization
probabilities. In Paola Merlo and
Suzanne Stevenson, editors, The Lexical
Basis of Sentence Processing: Formal,
Computational, and Experimental
Issues. John Benjamins, Amsterdam,
pages 325?345.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In Proceedings of
HLT-NAACL, pages 134?141,
Edmonton, Canada.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth.
2003. Using predicate-argument
structures for information extraction.
In Proceedings of ACL, pages 8?15,
Sapporo, Japan.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of EMNLP,
pages 88?94, Barcelona, Spain.
Yi, Szu-ting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings of
CoNLL, pages 237?240, Ann Arbor, MI.
191

Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 320?327,
New York, June 2006. c?2006 Association for Computational Linguistics
Prototype-Driven Learning for Sequence Models
Aria Haghighi
Computer Science Division
University of California Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California Berkeley
klein@cs.berkeley.edu
Abstract
We investigate prototype-driven learning for pri-
marily unsupervised sequence modeling. Prior
knowledge is specified declaratively, by provid-
ing a few canonical examples of each target an-
notation label. This sparse prototype information
is then propagated across a corpus using distri-
butional similarity features in a log-linear gener-
ative model. On part-of-speech induction in En-
glish and Chinese, as well as an information extrac-
tion task, prototype features provide substantial er-
ror rate reductions over competitive baselines and
outperform previous work. For example, we can
achieve an English part-of-speech tagging accuracy
of 80.5% using only three examples of each tag
and no dictionary constraints. We also compare to
semi-supervised learning and discuss the system?s
error trends.
1 Introduction
Learning, broadly taken, involves choosing a good
model from a large space of possible models. In su-
pervised learning, model behavior is primarily de-
termined by labeled examples, whose production
requires a certain kind of expertise and, typically,
a substantial commitment of resources. In unsu-
pervised learning, model behavior is largely deter-
mined by the structure of the model. Designing
models to exhibit a certain target behavior requires
another, rare kind of expertise and effort. Unsuper-
vised learning, while minimizing the usage of la-
beled data, does not necessarily minimize total ef-
fort. We therefore consider here how to learn mod-
els with the least effort. In particular, we argue for a
certain kind of semi-supervised learning, which we
call prototype-driven learning.
In prototype-driven learning, we specify prototyp-
ical examples for each target label or label configu-
ration, but do not necessarily label any documents or
sentences. For example, when learning a model for
Penn treebank-style part-of-speech tagging in En-
glish, we may list the 45 target tags and a few exam-
ples of each tag (see figure 4 for a concrete prototype
list for this task). This manner of specifying prior
knowledge about the task has several advantages.
First, is it certainly compact (though it remains to
be proven that it is effective). Second, it is more or
less the minimum one would have to provide to a
human annotator in order to specify a new annota-
tion task and policy (compare, for example, with the
list in figure 2, which suggests an entirely different
task). Indeed, prototype lists have been used ped-
agogically to summarize tagsets to students (Man-
ning and Schu?tze, 1999). Finally, natural language
does exhibit proform and prototype effects (Radford,
1988), which suggests that learning by analogy to
prototypes may be effective for language tasks.
In this paper, we consider three sequence mod-
eling tasks: part-of-speech tagging in English and
Chinese and a classified ads information extraction
task. Our general approach is to use distributional
similarity to link any given word to similar pro-
totypes. For example, the word reported may be
linked to said, which is in turn a prototype for the
part-of-speech VBD. We then encode these pro-
totype links as features in a log-linear generative
model, which is trained to fit unlabeled data (see
section 4.1). Distributional prototype features pro-
vide substantial error rate reductions on all three
tasks. For example, on English part-of-speech tag-
ging with three prototypes per tag, adding prototype
features to the baseline raises per-position accuracy
from 41.3% to 80.5%.
2 Tasks and Related Work: Tagging
For our part-of-speech tagging experiments, we used
data from the English and Chinese Penn treebanks
(Marcus et al, 1994; Ircs, 2002). Example sentences
320
(a) DT VBN NNS RB MD VB NNS TO VB NNS IN NNS RBR CC RBR RB .
The proposed changes also would allow executives to report exercises of options later and less often .
(b) NR AD VV AS PU NN VV DER VV PU PN AD VV DER VV PU DEC NN VV PU
! " # $ % & ? ( ) * + , - . / 0 * + , 1 2 3 4 5 6 7
(c) FEAT FEAT FEAT FEAT NBRHD NBRHD NBRHD NBRHD NBRHD SIZE SIZE SIZE SIZE
Vine covered cottage , near Contra Costa Hills . 2 bedroom house ,
FEAT FEAT FEAT FEAT FEAT RESTR RESTR RESTR RESTR RENT RENT RENT RENT
modern kitchen and dishwasher . No pets allowed . 1050 / month$
Figure 1: Sequence tasks: (a) English POS, (b) Chinese POS, and (c) Classified ad segmentation
are shown in figure 1(a) and (b). A great deal of re-
search has investigated the unsupervised and semi-
supervised induction of part-of-speech models, es-
pecially in English, and there is unfortunately only
space to mention some highly related work here.
One approach to unsupervised learning of part-
of-speech models is to induce HMMs from un-
labeled data in a maximum-likelihood framework.
For example, Merialdo (1991) presents experiments
learning HMMs using EM. Merialdo?s results most
famously show that re-estimation degrades accu-
racy unless almost no examples are labeled. Less
famously, his results also demonstrate that re-
estimation can improve tagging accuracies to some
degree in the fully unsupervised case.
One recent and much more successful approach
to part-of-speech learning is contrastive estimation,
presented in Smith and Eisner (2005). They utilize
task-specific comparison neighborhoods for part-of-
speech tagging to alter their objective function.
Both of these works require specification of the
legal tags for each word. Such dictionaries are large
and embody a great deal of lexical knowledge. A
prototype list, in contrast, is extremely compact.
3 Tasks and Related Work: Extraction
Grenager et al (2005) presents an unsupervised
approach to an information extraction task, called
CLASSIFIEDS here, which involves segmenting clas-
sified advertisements into topical sections (see fig-
ure 1(c)). Labels in this domain tend to be ?sticky?
in that the correct annotation tends to consist of
multi-element fields of the same label. The over-
all approach of Grenager et al (2005) typifies the
process involved in fully unsupervised learning on
new domain: they first alter the structure of their
HMM so that diagonal transitions are preferred, then
modify the transition structure to explicitly model
boundary tokens, and so on. Given enough refine-
Label Prototypes
ROOMATES roommate respectful drama
RESTRICTIONS pets smoking dog
UTILITIES utilities pays electricity
AVAILABLE immediately begin cheaper
SIZE 2 br sq
PHOTOS pictures image link
RENT $ month *number*15*1
CONTACT *phone* call *time*
FEATURES kitchen laundry parking
NEIGHBORHOOD close near shopping
ADDRESS address carlmont *ordinal*5
BOUNDARY ; . !
Figure 2: Prototype list derived from the develop-
ment set of the CLASSIFIEDS data. The BOUND-
ARY field is not present in the original annotation,
but added to model boundaries (see Section 5.3).
The starred tokens are the results of collapsing of
basic entities during pre-processing as is done in
(Grenager et al, 2005)
ments the model learns to segment with a reasonable
match to the target structure.
In section 5.3, we discuss an approach to this
task which does not require customization of model
structure, but rather centers on feature engineering.
4 Approach
In the present work, we consider the problem of
learning sequence models over text. For each doc-
ument x = [xi], we would like to predict a sequence
of labels y = [yi], where xi ? X and yi ? Y . We
construct a generative model, p(x, y|?), where ? are
the model?s parameters, and choose parameters to
maximize the log-likelihood of our observed dataD:
L(?;D) =
?
x?D
log p(x|?)
=
?
x?D
log
?
y
p(x, y|?)
321
yi?1
?DT,NN?
yi
?NN,VBD?
xi
reported
xi?1
witness
f(xi, yi) =
?
?
?
?
?
?
?
?
?
word = reported
suffix-2 = ed
proto = said
proto = had
?
?
?
?
?
?
?
?
?
?VBD
f(yi?1, yi) = DT ?NN ?VBD
Figure 3: Graphical model representation of trigram
tagger for English POS domain.
4.1 Markov Random Fields
We take our model family to be chain-structured
Markov random fields (MRFs), the undirected
equivalent of HMMs. Our joint probability model
over (x, y) is given by
p(x, y|?) = 1Z(?)
n
?
i=1
?(xi, yi)?(yi?1, yi)
where ?(c) is a potential over a clique c, taking the
form exp
{
?T f(c)
}
, and f(c) is the vector of fea-
tures active over c. In our sequence models, the
cliques are over the edges/transitions (yi?1, yi) and
nodes/emissions (xi, yi). See figure 3 for an exam-
ple from the English POS tagging domain.
Note that the only way an MRF differs from
a conditional random field (CRF) (Lafferty et al,
2001) is that the partition function is no longer ob-
servation dependent; we are modeling the joint prob-
ability of x and y instead of y given x. As a result,
learning an MRF is slightly harder than learning a
CRF; we discuss this issue in section 4.4.
4.2 Prototype-Driven Learning
We assume prior knowledge about the target struc-
ture via a prototype list, which specifies the set of
target labels Y and, for each label y ? Y , a set of
prototypes words, py ? Py. See figures 2 and 4 for
examples of prototype lists.1
1Note that this setting differs from the standard semi-
supervised learning setup, where a small number of fully la-
beled examples are given and used in conjunction with a larger
amount of unlabeled data. In our prototype-driven approach, we
never provide a single fully labeled example sequence. See sec-
tion 5.3 for further comparison of this setting to semi-supervised
learning.
Broadly, we would like to learn sequence models
which both explain the observed data and meet our
prior expectations about target structure. A straight-
forward way to implement this is to constrain each
prototype word to take only its given label(s) at
training time. As we show in section 5, this does
not work well in practice because this constraint on
the model is very sparse.
In providing a prototype, however, we generally
mean something stronger than a constraint on that
word. In particular, we may intend that words which
are in some sense similar to a prototype generally be
given the same label(s) as that prototype.
4.3 Distributional Similarity
In syntactic distributional clustering, words are
grouped on the basis of the vectors of their pre-
ceeding and following words (Schu?tze, 1995; Clark,
2001). The underlying linguistic idea is that replac-
ing a word with another word of the same syntactic
category should preserve syntactic well-formedness
(Radford, 1988). We present more details in sec-
tion 5, but for now assume that a similarity function
over word types is given.
Suppose further that for each non-prototype word
type w, we have a subset of prototypes, Sw, which
are known to be distributionally similar to w (above
some threshold). We would like our model to relate
the tags of w to those of Sw.
One approach to enforcing the distributional as-
sumption in a sequence model is by supplementing
the training objective (here, data likelihood) with a
penalty term that encourages parameters for which
each w?s posterior distribution over tags is compati-
ble with it?s prototypes Sw. For example, we might
maximize,
?
x?D
log p(x|?) ?
?
w
?
z?Sw
KL( t|z || t|w)
where t|w is the model?s distribution of tags for
word w. The disadvantage of a penalty-based ap-
proach is that it is difficult to construct the penalty
term in a way which produces exactly the desired
behavior.
Instead, we introduce distributional prototypes
into the learning process as features in our log-linear
model. Concretely, for each prototype z, we intro-
duce a predicate PROTO = z which becomes active
322
at each w for which z ? Sw (see figure 3). One ad-
vantage of this approach is that it allows the strength
of the distributional constraint to be calibrated along
with any other features; it was also more successful
in our experiments.
4.4 Parameter Estimation
So far we have ignored the issue of how we learn
model parameters ? which maximizeL(?;D). If our
model family were HMMs, we could use the EM al-
gorithm to perform a local search. Since we have
a log-linear formulation, we instead use a gradient-
based search. In particular, we use L-BFGS (Liu
and Nocedal, 1989), a standard numerical optimiza-
tion technique, which requires the ability to evaluate
L(?;D) and its gradient at a given ?.
The density p(x|?) is easily calculated up to the
global constant Z(?) using the forward-backward
algorithm (Rabiner, 1989). The partition function
is given by
Z(?) =
?
x
?
y
n
?
i=1
?(xi, yi)?(yi?1, yi)
=
?
x
?
y
score(x, y)
Z(?) can be computed exactly under certain as-
sumptions about the clique potentials, but can in all
cases be bounded by
Z?(?) =
K
?
`=1
Z?`(?) =
K
?
`=1
?
x:|x|=`
score(x, y)
WhereK is a suitably chosen large constant. We can
efficiently compute Z?`(?) for fixed ` using a gener-
alization of the forward-backward algorithm to the
lattice of all observations x of length ` (see Smith
and Eisner (2005) for an exposition).
Similar to supervised maximum entropy prob-
lems, the partial derivative of L(?;D) with respect
to each parameter ?j (associated with feature fj) is
given by a difference in feature expectations:
?L(?;D)
??j
=
?
x?D
(
Ey|x,?fj ? Ex,y|?fj
)
The first expectation is the expected count of the fea-
ture under the model?s p(y|x, ?) and is again eas-
ily computed with the forward-backward algorithm,
Num Tokens
Setting 48K 193K
BASE 42.2 41.3
PROTO 61.9 68.8
PROTO+SIM 79.1 80.5
Table 1: English POS results measured by per-
position accuracy
just as for CRFs or HMMs. The second expectation
is the expectation of the feature under the model?s
joint distribution over all x, y pairs, and is harder to
calculate. Again assuming that sentences beyond a
certain length have negligible mass, we calculate the
expectation of the feature for each fixed length ` and
take a (truncated) weighted sum:
Ex,y|?fj =
K
?
`=1
p(|x| = `)Ex,y|`,?fj
For fixed `, we can calculate Ex,y|`,?fj using the lat-
tice of all inputs of length `. The quantity p(|x| = `)
is simply Z?`(?)/Z?(?).
As regularization, we use a diagonal Gaussian
prior with variance ?2 = 0.5, which gave relatively
good performance on all tasks.
5 Experiments
We experimented with prototype-driven learning in
three domains: English and Chinese part-of-speech
tagging and classified advertisement field segmenta-
tion. At inference time, we used maximum poste-
rior decoding,2 which we found to be uniformly but
slightly superior to Viterbi decoding.
5.1 English POS Tagging
For our English part-of-speech tagging experiments,
we used the WSJ portion of the English Penn tree-
bank (Marcus et al, 1994). We took our data to be
either the first 48K tokens (2000 sentences) or 193K
tokens (8000 sentences) starting from section 2. We
used a trigram tagger of the model form outlined in
section 4.1 with the same set of spelling features re-
ported in Smith and Eisner (2005): exact word type,
2At each position choosing the label which has the highest
posterior probability, obtained from the forward-backward al-
gorithm.
323
Label Prototype Label Prototype
NN % company year NNS years shares companies
JJ new other last VBG including being according
MD will would could -LRB- -LRB- -LCB-
VBP are ?re ?ve DT the a The
RB n?t also not WP$ whose
-RRB- -RRB- -RCB- FW bono del kanji
WRB when how where RP Up ON
IN of in for VBD said was had
SYM c b f $ $ US$ C$
CD million billion two # #
TO to To na : ? : ;
VBN been based compared NNPS Philippines Angels Rights
RBR Earlier duller ? ? ? non-?
VBZ is has says VB be take provide
JJS least largest biggest RBS Worst
NNP Mr. U.S. Corp. , ,
POS ?S CC and or But
PRP$ its their his JJR smaller greater larger
PDT Quite WP who what What
WDT which Whatever whatever . . ? !
EX There PRP it he they
? ? UH Oh Well Yeah
Figure 4: English POS prototype list
Correct Tag Predicted Tag % of Errors
CD DT 6.2
NN JJ 5.3
JJ NN 5.2
VBD VBN 3.3
NNS NN 3.2
Figure 5: Most common English POS confusions for
PROTO+SIM on 193K tokens
character suffixes of length up to 3, initial-capital,
contains-hyphen, and contains-digit. Our only edge
features were tag trigrams.
With just these features (our baseline BASE) the
problem is symmetric in the 45 model labels. In
order to break initial symmetry we initialized our
potentials to be near one, with some random noise.
To evaluate in this setting, model labels must be
mapped to target labels. We followed the common
approach in the literature, greedily mapping each
model label to a target label in order to maximize
per-position accuracy on the dataset. The results of
BASE, reported in table 1, depend upon random ini-
tialization; averaging over 10 runs gave an average
per-position accuracy of 41.3% on the larger training
set.
We automatically extracted the prototype list by
taking our data and selecting for each annotated la-
bel the top three occurring word types which were
not given another label more often. This resulted
in 116 prototypes for the 193K token setting.3 For
comparison, there are 18,423 word types occurring
in this data.
Incorporating the prototype list in the simplest
possible way, we fixed prototype occurrences in the
data to their respective annotation labels. In this
case, the model is no longer symmetric, and we
no longer require random initialization or post-hoc
mapping of labels. Adding prototypes in this way
gave an accuracy of 68.8% on all tokens, but only
47.7% on non-prototype occurrences, which is only
a marginal improvement over BASE. It appears as
though the prototype information is not spreading to
non-prototype words.
In order to remedy this, we incorporated distri-
butional similarity features. Similar to (Schu?tze,
1995), we collect for each word type a context vector
of the counts of the most frequent 500 words, con-
joined with a direction and distance (e.g +1,-2). We
then performed an SVD on the matrix to obtain a re-
duced rank approximation. We used the dot product
between left singular vectors as a measure of distri-
butional similarity. For each word w, we find the set
of prototype words with similarity exceeding a fixed
threshold of 0.35. For each of these prototypes z,
we add a predicate PROTO = z to each occurrence of
w. For example, we might add PROTO = said to each
token of reported (as in figure 3).4
Each prototype word is also its own prototype
(since a word has maximum similarity to itself), so
when we lock the prototype to a label, we are also
pushing all the words distributionally similar to that
prototype towards that label.5
3To be clear: this method of constructing a prototype list
required statistics from the labeled data. However, we believe
it to be a fair and necessary approach for several reasons. First,
we wanted our results to be repeatable. Second, we did not want
to overly tune this list, though experiments below suggest that
tuning could greatly reduce the error rate. Finally, it allowed us
to run on Chinese, where the authors have no expertise.
4Details of distributional similarity features: To extract con-
text vectors, we used a window of size 2 in either direction and
use the first 250 singular vectors. We collected counts from
all the WSJ portion of the Penn Treebank as well as the entire
BLIPP corpus. We limited each word to have similarity features
for its top 5 most similar prototypes.
5Note that the presence of a prototype feature does not en-
sure every instance of that word type will be given its proto-
type?s label; pressure from ?edge? features or other prototype
features can cause occurrences of a word type to be given differ-
ent labels. However, rare words with a single prototype feature
are almost always given that prototype?s label.
324
This setting, PROTO+SIM, brings the all-tokens
accuracy up to 80.5%, which is a 37.5% error re-
duction over PROTO. For non-prototypes, the accu-
racy increases to 67.8%, an error reduction of 38.4%
over PROTO. The overall error reduction from BASE
to PROTO+SIM on all-token accuracy is 66.7%.
Table 5 lists the most common confusions for
PROTO+SIM. The second, third, and fourth most
common confusions are characteristic of fully super-
vised taggers (though greater in number here) and
are difficult. For instance, both JJs and NNs tend to
occur after determiners and before nouns. The CD
and DT confusion is a result of our prototype list not
containing a contains-digit prototype for CD, so the
predicate fails to be linked to CDs. Of course in a
realistic, iterative design setting, we could have al-
tered the prototype list to include a contains-digit
prototype for CD and corrected this confusion.
Figure 6 shows the marginal posterior distribu-
tion over label pairs (roughly, the bigram transi-
tion matrix) according to the treebank labels and the
PROTO+SIM model run over the training set (using
a collapsed tag set for space). Note that the broad
structure is recovered to a reasonable degree.
It is difficult to compare our results to other sys-
tems which utilize a full or partial tagging dictio-
nary, since the amount of provided knowledge is
substantially different. The best comparison is to
Smith and Eisner (2005) who use a partial tagging
dictionary. In order to compare with their results,
we projected the tagset to the coarser set of 17 that
they used in their experiments. On 24K tokens, our
PROTO+SIM model scored 82.2%. When Smith and
Eisner (2005) limit their tagging dictionary to words
which occur at least twice, their best performing
neighborhood model achieves 79.5%. While these
numbers seem close, for comparison, their tagging
dictionary contained information about the allow-
able tags for 2,125 word types (out of 5,406 types)
and the their system must only choose, on average,
between 4.4 tags for a word. Our prototype list,
however, contains information about only 116 word
types and our tagger must on average choose be-
tween 16.9 tags, a much harder task. When Smith
and Eisner (2005) include tagging dictionary entries
for all words in the first half of their 24K tokens, giv-
ing tagging knowledge for 3,362 word types, they do
achieve a higher accuracy of 88.1%.
Setting Accuracy
BASE 46.4
PROTO 53.7
PROTO+SIM 71.5
PROTO+SIM+BOUND 74.1
Figure 7: Results on test set for ads data in
(Grenager et al, 2005).
5.2 Chinese POS Tagging
We also tested our POS induction system on the Chi-
nese POS data in the Chinese Treebank (Ircs, 2002).
The model is wholly unmodified from the English
version except that the suffix features are removed
since, in Chinese, suffixes are not a reliable indi-
cator of part-of-speech as in English (Tseng et al,
2005). Since we did not have access to a large aux-
iliary unlabeled corpus that was segmented, our dis-
tributional model was built only from the treebank
text, and the distributional similarities are presum-
ably degraded relative to the English. On 60K word
tokens, BASE gave an accuracy of 34.4, PROTO gave
39.0, and PROTO+SIM gave 57.4, similar in order if
not magnitude to the English case.
We believe the performance for Chinese POS tag-
ging is not as high as English for two reasons: the
general difficulty of Chinese POS tagging (Tseng et
al., 2005) and the lack of a larger segmented corpus
from which to build distributional models. Nonethe-
less, the addition of distributional similarity features
does reduce the error rate by 35% from BASE.
5.3 Information Field Segmentation
We tested our framework on the CLASSIFIEDS data
described in Grenager et al (2005) under conditions
similar to POS tagging. An important characteristic
of this domain (see figure 1(a)) is that the hidden la-
bels tend to be ?sticky,? in that fields tend to consist
of runs of the same label, as in figure 1(c), in con-
trast with part-of-speech tagging, where we rarely
see adjacent tokens given the same label. Grenager
et al (2005) report that in order to learn this ?sticky?
structure, they had to alter the structure of their
HMM so that a fixed mass is placed on each diag-
onal transition. In this work, we learned this struc-
ture automatically though prototype similarity fea-
tures without manually constraining the model (see
325
INPUNC
PRT
TO
VBN
LPUNC
W
DET
ADV
V
POS
ENDPUNC
VBG
PREP
ADJ
RPUNC
N
CONJ
IN
PU
N
C
PR
T
TO V
BN
LP
U
N
C
W D
ET
A
DV
V PO
S
EN
D
PU
N
C
V
BG
PR
EP
A
D
J
RP
U
N
C
N CO
N
J
INPUNC
PRT
TO
VBN
LPUNC
W
DET
ADV
V
POS
ENDPUNC
VBG
PREP
ADJ
RPUNC
N
CONJ
IN
PU
N
C
PR
T
TO V
BN
LP
U
N
C
W D
ET
A
DV
V PO
S
EN
D
PU
N
C
V
BG
PR
EP
A
D
J
RP
U
N
C
N CO
N
J
(a) (b)
Figure 6: English coarse POS tag structure: a) corresponds to ?correct? transition structure from labeled
data, b) corresponds to PROTO+SIM on 24K tokens
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
(a) (b) (c)
Figure 8: Field segmentation observed transition structure: (a) labeled data, (b) BASE(c)
BASE+PROTO+SIM+BOUND (after post-processing)
figure 8), though we did change the similarity func-
tion (see below).
On the test set of (Grenager et al, 2005),
BASE scored an accuracy of 46.4%, comparable to
Grenager et al (2005)?s unsupervised HMM base-
line. Adding the prototype list (see figure 2) without
distributional features yielded a slightly improved
accuracy of 53.7%. For this domain, we utilized
a slightly different notion of distributional similar-
ity: we are not interested in the syntactic behavior
of a word type, but its topical content. Therefore,
when we collect context vectors for word types in
this domain, we make no distinction by direction
or distance and collect counts from a wider win-
dow. This notion of distributional similarity is more
similar to latent semantic indexing (Deerwester et
al., 1990). A natural consequence of this definition
of distributional similarity is that many neighboring
words will share the same prototypes. Therefore
distributional prototype features will encourage la-
bels to persist, naturally giving the ?sticky? effect
of the domain. Adding distributional similarity fea-
tures to our model (PROTO+SIM) improves accuracy
substantially, yielding 71.5%, a 38.4% error reduc-
tion over BASE.6
Another feature of this domain that Grenager et
al. (2005) take advantage of is that end of sen-
tence punctuation tends to indicate the end of a
field and the beginning of a new one. Grenager et
al. (2005) experiment with manually adding bound-
ary states and biasing transitions from these states
to not self-loop. We capture this ?boundary? ef-
fect by simply adding a line to our protoype-list,
adding a new BOUNDARY state (see figure 2) with
a few (hand-chosen) prototypes. Since we uti-
lize a trigram tagger, we are able to naturally cap-
ture the effect that the BOUNDARY tokens typically
indicate transitions between the fields before and
after the boundary token. As a post-processing
step, when a token is tagged as a BOUNDARY
6Distributional similarity details: We collect for each word
a context vector consisting of the counts for words occurring
within three token occurrences of a word. We perform a SVD
onto the first 50 singular vectors.
326
Correct Tag Predicted Tag % of Errors
FEATURES SIZE 11.2
FEATURES NBRHD 9.0
SIZE FEATURES 7.7
NBRHD FEATURES 6.4
ADDRESS NBRHD 5.3
UTILITIES FEATURES 5.3
Figure 9: Most common classified ads confusions
token it is given the same label as the previous
non-BOUNDARY token, which reflects the annota-
tional convention that boundary tokens are given the
same label as the field they terminate. Adding the
BOUNDARY label yields significant improvements,
as indicated by the PROTO+SIM+BOUND setting in
Table 5.3, surpassing the best unsupervised result
of Grenager et al (2005) which is 72.4%. Further-
more, our PROTO+SIM+BOUND model comes close
to the supervised HMM accuracy of 74.4% reported
in Grenager et al (2005).
We also compared our method to the most ba-
sic semi-supervised setting, where fully labeled doc-
uments are provided along with unlabeled ones.
Roughly 25% of the data had to be labeled
in order to achieve an accuracy equal to our
PROTO+SIM+BOUND model, suggesting that the use
of prior knowledge in the prototype system is partic-
ularly efficient.
In table 5.3, we provide the top confusions made
by our PROTO+SIM+BOUND model. As can be seen,
many of our confusions involve the FEATURE field,
which serves as a general purpose background state,
which often differs subtly from other fields such as
SIZE. For instance, the parenthical comment: ( mas-
ter has walk - in closet with vanity ) is labeled as
a SIZE field in the data, but our model proposed
it as a FEATURE field. NEIGHBORHOOD and AD-
DRESS is another natural confusion resulting from
the fact that the two fields share much of the same
vocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs.
[NBRHD near Telegraph]).
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by aMicrosoft / CITRIS grant and
by an equipment donation from Intel.
6 Conclusions
We have shown that distributional prototype features
can allow one to specify a target labeling scheme
in a compact and declarative way. These features
give substantial error reduction on several induction
tasks by allowing one to link words to prototypes ac-
cording to distributional similarity. Another positive
property of this approach is that it tries to reconcile
the success of sequence-free distributional methods
in unsupervised word clustering with the success of
sequence models in supervised settings: the similar-
ity guides the learning of the sequence model.
References
Alexander Clark. 2001. The unsupervised induction of stochas-
tic context-free grammars using distributional clustering. In
CoNLL.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the American
Society of Information Science, 41(6):391?407.
Trond Grenager, Dan Klein, and Christopher Manning. 2005.
Unsupervised learning of field segmentation models for in-
formation extraction. In Proceedings of the 43rd Meeting of
the ACL.
Nianwen Xue Ircs. 2002. Building a large-scale annotated chi-
nese corpus.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International Con-
ference on Machine Learning (ICML).
Dong C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Mathematical
Programming.
Christopher D. Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. The MIT
Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313?330.
Bernard Merialdo. 1991. Tagging english text with a proba-
bilistic model. In ICASSP, pages 809?812.
L.R Rabiner. 1989. A tutorial on hidden markov models and
selected applications in speech recognition. In IEEE.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Hinrich Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
Noah Smith and Jason Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proceed-
ings of the 43rd Meeting of the ACL.
Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.
2005. Morphological features help pos tagging of unknown
words across language varieties. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Process-
ing.
327
Proceedings of NAACL HLT 2007, pages 412?419,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Approximate Factoring for A? Search
Aria Haghighi, John DeNero, Dan Klein
Computer Science Division
University of California Berkeley
{aria42, denero, klein}@cs.berkeley.edu
Abstract
We present a novel method for creating A? esti-
mates for structured search problems. In our ap-
proach, we project a complex model onto multiple
simpler models for which exact inference is effi-
cient. We use an optimization framework to es-
timate parameters for these projections in a way
which bounds the true costs. Similar to Klein and
Manning (2003), we then combine completion es-
timates from the simpler models to guide search
in the original complex model. We apply our ap-
proach to bitext parsing and lexicalized parsing,
demonstrating its effectiveness in these domains.
1 Introduction
Inference tasks in NLP often involve searching for
an optimal output from a large set of structured out-
puts. For many complex models, selecting the high-
est scoring output for a given observation is slow or
even intractable. One general technique to increase
efficiency while preserving optimality is A? search
(Hart et al, 1968); however, successfully using A?
search is challenging in practice. The design of ad-
missible (or nearly admissible) heuristics which are
both effective (close to actual completion costs) and
also efficient to compute is a difficult, open prob-
lem in most domains. As a result, most work on
search has focused on non-optimal methods, such
as beam search or pruning based on approximate
models (Collins, 1999), though in certain cases ad-
missible heuristics are known (Och and Ney, 2000;
Zhang and Gildea, 2006). For example, Klein and
Manning (2003) show a class of projection-based A?
estimates, but their application is limited to models
which have a very restrictive kind of score decom-
position. In this work, we broaden their projection-
based technique to give A? estimates for models
which do not factor in this restricted way.
Like Klein and Manning (2003), we focus on
search problems where there are multiple projec-
tions or ?views? of the structure, for example lexical
parsing, in which trees can be projected onto either
their CFG backbone or their lexical attachments. We
use general optimization techniques (Boyd and Van-
denberghe, 2005) to approximately factor a model
over these projections. Solutions to the projected
problems yield heuristics for the original model.
This approach is flexible, providing either admissi-
ble or nearly admissible heuristics, depending on the
details of the optimization problem solved. Further-
more, our approach allows a modeler explicit control
over the trade-off between the tightness of a heuris-
tic and its degree of inadmissibility (if any). We de-
scribe our technique in general and then apply it to
two concrete NLP search tasks: bitext parsing and
lexicalized monolingual parsing.
2 General Approach
Many inference problems in NLP can be solved
with agenda-based methods, in which we incremen-
tally build hypotheses for larger items by combining
smaller ones with some local configurational struc-
ture. We can formalize such tasks as graph search
problems, where states encapsulate partial hypothe-
ses and edges combine or extend them locally.1 For
example, in HMM decoding, the states are anchored
labels, e.g. VBD[5], and edges correspond to hidden
transitions, e.g. VBD[5] ? DT[6].
The search problem is to find a minimal cost path
from the start state to a goal state, where the path
cost is the sum of the costs of the edges in the path.
1In most complex tasks, we will in fact have a hypergraph,
but the extension is trivial and not worth the added notation.
412
( a
a?
)
?
( b
b?
)
( b
a?
)
?
( c
b?
)
1
( a
a?
)
?
( b
b?
)
( b
b?
)
?
( c
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
Local Configurations
a' ? b' b'  c'
a ? b
b ? c
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 4.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 5.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
(a) (b) (c)
Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row and
column of the cell). In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration. The
bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the
actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along
two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost.
Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.
For probabilistic inference problems, the cost of an
edge is typically a negative log probability which de-
pends only on some local configuration type. For
instance, in PCFG parsing, the (hyper)edges refer-
ence anchored spans X[i, j], but the edge costs de-
pend only on the local rule type X ? Y Z. We will
use a to refer to a local configuration and use c(a)
to refer to its cost. Because edge costs are sensi-
tive only to local configurations, the cost of a path
is
?
a c(a). A? search requires a heuristic function,
which is an estimate h(s) of the completion cost, the
cost of a best path from state s to a goal.
In this work, following Klein and Manning
(2003), we consider problems with projections or
?views,? which define mappings to simpler state and
configuration spaces. For instance, suppose that we
are using an HMM to jointly model part-of-speech
(POS) and named-entity-recognition (NER) tagging.
There might be one projection onto the NER com-
ponent and another onto the POS component. For-
mally, a projection pi is a mapping from states to
some coarser domain. A state projection induces
projections of edges and of the entire graph pi(G).
We are particularly interested in search problems
with multiple projections {pi1, . . . , pi`} where each
projection, pii, has the following properties: its state
projections induce well-defined projections of the
local configurations pii(a) used for scoring, and the
projected search problem admits a simpler infer-
ence. For instance, the POS projection in our NER-
POS HMM is a simpler HMM, though the gains
from this method are greater when inference in the
projections have lower asymptotic complexity than
the original problem (see sections 3 and 4).
In defining projections, we have not yet dealt with
the projected scoring function. Suppose that the
cost of local configurations decomposes along pro-
jections as well. In this case,
c (a) =
?`
i=1
ci(a) , ?a ? A (1)
where A is the set of local configurations and ci(a)
represents the cost of configuration a under projec-
tion pii. A toy example of such a cost decomposi-
tion in the context of a Markov process over two-part
states is shown in figure 1(b), where the costs of the
joint transitions equal the sum of costs of their pro-
jections. Under the strong assumption of equation
(1), Klein and Manning (2003) give an admissible
A? bound. They note that the cost of a path decom-
poses as a sum of projected path costs. Hence, the
following is an admissible additive heuristic (Felner
et al, 2004),
h(s) =
?`
i=1
h?i (s) (2)
where h?i (s) denote the optimal completion costs in
the projected search graph pii(G). That is, the com-
pletion cost of a state bounds the sum of the comple-
tion costs in each projection.
In virtually all cases, however, configuration costs
will not decompose over projections, nor would we
expect them to. For instance, in our joint POS-NER
task, this assumption requires that the POS and NER
413
transitions and observations be generated indepen-
dently. This independence assumption undermines
the motivation for assuming a joint model. In the
central contribution of this work, we exploit the pro-
jection structure of our search problem without mak-
ing any assumption about cost decomposition.
Rather than assuming decomposition, we propose
to find scores ? for the projected configurations
which are pointwise admissible:
?`
i=1
?i(a) ? c(a), ?a ? A (3)
Here, ?i(a) represents a factored projection cost of
pii(a), the pii projection of configuration a. Given
pointwise admissible ?i?s we can again apply the
heuristic recipe of equation (2). An example of
factored projection costs are shown in figure 1(c),
where no exact decomposition exists, but a point-
wise admissible lower bound is easy to find.
Claim. If a set of factored projection costs
{?1, . . . , ?`} satisfy pointwise admissibility, then
the heuristic from (2) is an admissible A? heuristic.
Proof. Assume a1, . . . , ak are configurations used
to optimally reach the goal from state s. Then,
h?(s) =
kX
j=1
c(aj) ?
kX
j=1
X`
i=1
?i(aj)
=
X`
i=1
 
kX
j=1
?i(aj)
!
?
X`
i=1
h?i (s) = h(s)
The first inequality follows from pointwise admis-
sibility. The second inequality follows because each
inner sum is a completion cost for projected problem
pii and therefore h?i (s) lower bounds it. Intuitively,
we can see two sources of slack in such projection
heuristics. First, there may be slack in the pointwise
admissible scores. Second, the best paths in the pro-
jections will be overly optimistic because they have
been decoupled (see figure 5 for an example of de-
coupled best paths in projections).
2.1 Finding Factored Projections for
Non-Factored Costs
We can find factored costs ?i(a) which are point-
wise admissible by solving an optimization problem.
We think of our unknown factored costs as a block
vector ? = [?1, .., ?`], where vector ?i is composed
of the factored costs, ?i(a), for each configuration
a ? A. We can then find admissible factored costs
by solving the following optimization problem,
minimize
?
??? (4)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
?a ? 0, ?a ? A
We can think of each ?a as the amount by which
the cost of configuration a exceeds the factored pro-
jection estimates (the pointwise A? gap). Requiring
?a ? 0 insures pointwise admissibility. Minimiz-
ing the norm of the ?a variables encourages tighter
bounds; indeed if ??? = 0, the solution corresponds
to an exact factoring of the search problem. In the
case where we minimize the 1-norm or ?-norm, the
problem above reduces to a linear program, which
can be solved efficiently for a large number of vari-
ables and constraints.2
Viewing our procedure decision-theoretically, by
minimizing the norm of the pointwise gaps we are
effectively choosing a loss function which decom-
poses along configuration types and takes the form
of the norm (i.e. linear or squared losses). A com-
plete investigation of the alternatives is beyond the
scope of this work, but it is worth pointing out that
in the end we will care only about the gap on entire
structures, not configurations, and individual config-
uration factored costs need not even be pointwise ad-
missible for the overall heuristic to be admissible.
Notice that the number of constraints is |A|, the
number of possible local configurations. For many
search problems, enumerating the possible configu-
rations is not feasible, and therefore neither is solv-
ing an optimization problem with all of these con-
straints. We deal with this situation in applying our
technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search
optimality for efficiency. In our approach, we can
explicitly make this trade-off by designing an alter-
native optimization problem which allows for slack
2We used the MOSEK package (Andersen and Andersen,
2000).
414
in the admissibility constraints. We solve the follow-
ing soft version of problem (4):
minimize
?
??+?+ C???? (5)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
where ?+ = max{0, ?} and ?? = max{0,??}
represent the componentwise positive and negative
elements of ? respectively. Each ??a > 0 represents
a configuration where our factored projection esti-
mate is not pointwise admissible. Since this situa-
tion may result in our heuristic becoming inadmis-
sible if used in the projected completion costs, we
more heavily penalize overestimating the cost by the
constant C.
2.2 Bounding Search Error
In the case where we allow pointwise inadmissibil-
ity, i.e. variables ??a , we can bound our search er-
ror. Suppose ??max = maxa?A ??a and that L? is
the length of the longest optimal solution for the
original problem. Then, h(s) ? h?(s) + L???max,
?s ? S. This ?-admissible heuristic (Ghallab and
Allard, 1982) bounds our search error by L???max.3
3 Bitext Parsing
In bitext parsing, one jointly infers a synchronous
phrase structure tree over a sentence ws and its
translation wt (Melamed et al, 2004; Wu, 1997).
Bitext parsing is a natural candidate task for our
approximate factoring technique. A synchronous
tree projects monolingual phrase structure trees onto
each sentence. However, the costs assigned by
a weighted synchronous grammar (WSG) G do
not typically factor into independent monolingual
WCFGs. We can, however, produce a useful surro-
gate: a pair of monolingual WCFGs with structures
projected by G and weights that, when combined,
underestimate the costs of G.
Parsing optimally relative to a synchronous gram-
mar using a dynamic program requires time O(n6)
in the length of the sentence (Wu, 1997). This high
degree of complexity makes exhaustive bitext pars-
ing infeasible for all but the shortest sentences. In
3This bound may be very loose if L is large.
contrast, monolingual CFG parsing requires time
O(n3) in the length of the sentence.
3.1 A? Parsing
Alternatively, we can search for an optimal parse
guided by a heuristic. The states in A? bitext pars-
ing are rooted bispans, denoted X [i, j] :: Y [k, l].
States represent a joint parse over subspans [i, j] of
ws and [k, l] of wt rooted by the nonterminals X and
Y respectively.
Given a WSG G, the algorithm prioritizes a state
(or edge) e by the sum of its inside cost ?G(e) (the
negative log of its inside probability) and its outside
estimate h(e), or completion cost.4 We are guaran-
teed the optimal parse if our heuristic h(e) is never
greater than ?G(e), the true outside cost of e.
We now consider a heuristic combining the com-
pletion costs of the monolingual projections of G,
and guarantee admissibility by enforcing point-wise
admissibility. Each state e = X [i, j] :: Y [k, l]
projects a pair of monolingual rooted spans. The
heuristic we propose sums independent outside costs
of these spans in each monolingual projection.
h(e) = ?s(X [i, j]) + ?t(Y [k, l])
These monolingual outside scores are computed rel-
ative to a pair of monolingual WCFG grammars Gs
and Gt given by splitting each synchronous rule
r =
(
X(s)
Y(t)
)
?
(
? ?
? ?
)
into its components pis(r) = X? ?? and pit(r) =
Y??? and weighting them via optimized ?s(r) and
?t(r), respectively.5
To learn pointwise admissible costs for the mono-
lingual grammars, we formulate the following opti-
mization problem:6
minimize
?,?s,?t
???1
such that, ?r = c(r)? [?s(r) + ?t(r)]
for all synchronous rules r ? G
?s ? 0, ?t ? 0, ? ? 0
4All inside and outside costs are Viterbi, not summed.
5Note that we need only parse each sentence (monolin-
gually) once to compute the outside probabilities for every span.
6The stated objective is merely one reasonable choice
among many possibilities which require pointwise admissibil-
ity and encourage tight estimates.
415
ij
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
? ?
Cost under Gt Cost under G
Synchronized completion 
scored by original model
Synchronized completion 
scored by factored model
Monolingual completions 
scored by factored model
Cost under Gs
Figure 2: The gap between the heuristic (left) and true comple-
tion cost (right) comes from relaxing the synchronized problem
to independent subproblems and slack in the factored models.
Figure 2 diagrams the two bounds that enforce the
admissibility of h(e). For any outside cost ?G(e),
there is a corresponding optimal completion struc-
ture o under G, which is an outer shell of a syn-
chronous tree. o projects monolingual completions
os and ot which have well-defined costs cs(os) and
ct(ot) under Gs and Gt respectively. Their sum
cs(os) + ct(ot) will underestimate ?G(e) by point-
wise admissibility.
Furthermore, the heuristic we compute underesti-
mates this sum. Recall that the monolingual outside
score ?s(X [i, j]) is the minimal costs for any com-
pletion of the edge. Hence, ?s(X [i, j]) ? cs(os)
and ?t(X [k, l]) ? ct(ot). Admissibility follows.
3.2 Experiments
We demonstrate our technique using the syn-
chronous grammar formalism of tree-to-tree trans-
ducers (Knight and Graehl, 2004). In each weighted
rule, an aligned pair of nonterminals generates two
ordered lists of children. The non-terminals in each
list must align one-to-one to the non-terminals in the
other, while the terminals are placed freely on either
side. Figure 3(a) shows an example rule.
Following Galley et al (2004), we learn a gram-
mar by projecting English syntax onto a foreign lan-
guage via word-level alignments, as in figure 3(b).7
We parsed 1200 English-Spanish sentences using
a grammar learned from 40,000 sentence pairs of
the English-Spanish Europarl corpus.8 Figure 4(a)
shows that A? expands substantially fewer states
while searching for the optimal parse with our op-
7The bilingual corpus consists of translation pairs with fixed
English parses and word alignments. Rules were scored by their
relative frequencies.
8Rare words were replaced with their parts of speech to limit
the memory consumption of the parser.
(a)
?
NP(s)
NP(t)
?
?
 
NN(s)1 NNS
(s)
2
NNS(t)2 de NN
(t)
1
!
(b)
T
r
a
n
s
l
a
t
i
o
n
s
y
s
t
e
m
s
s
o
m
e
t
i
m
e
s
w
o
r
k
sistemas
traduccion
funcionan
a
veces
de
NNS
NN
NP
NNSNN
NP
RB VB
S
Figure 3: (a) A tree-to-tree transducer rule. (b) An example
training sentence pair that yields rule (a).
timization heuristic. The exhaustive curve shows
edge expansions using the null heuristic. The in-
termediate result, labeled English only, used only
the English monolingual outside score as a heuris-
tic. Similar results using only Spanish demonstrate
that both projections contribute to parsing efficiency.
All three curves in figure 4 represent running times
for finding the optimal parse.
Zhang and Gildea (2006) offer a different heuris-
tic for A? parsing of ITG grammars that provides a
forward estimate of the cost of aligning the unparsed
words in both sentences. We cannot directly apply
this technique to our grammar because tree-to-tree
transducers only align non-terminals. Instead, we
can augment our synchronous grammar model to in-
clude a lexical alignment component, then employ
both heuristics. We learned the following two-stage
generative model: a tree-to-tree transducer generates
trees whose leaves are parts of speech. Then, the
words of each sentence are generated, either jointly
from aligned parts of speech or independently given
a null alignment. The cost of a complete parse un-
der this new model decomposes into the cost of the
synchronous tree over parts of speech and the cost
of generating the lexical items.
Given such a model, both our optimization heuris-
tic and the lexical heuristic of Zhang and Gildea
(2006) can be computed independently. Crucially,
the sum of these heuristics is still admissible. Re-
sults appear in figure 4(b). Both heuristics (lexi-
cal and optimization) alone improve parsing perfor-
mance, but their sum opt+lex substantially improves
upon either one.
416
(a) 050
100150
200
5 7 9 11 13 15Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 11 13 15Sentence lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
(b) 050
100150
200
5 7 9 1 13 15Sent ce lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 1 13 15Sent ce lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efficiency.
4 Lexicalized Parsing
We next apply our technique to lexicalized pars-
ing (Charniak, 1997; Collins, 1999). In lexical-
ized parsing, the local configurations are lexicalized
rules of the form X[h, t] ? Y [h?, t?] Z[h, t], where
h, t, h?, and t? are the head word, head tag, ar-
gument word, and argument tag, respectively. We
will use r = X ? Y Z to refer to the CFG back-
bone of a lexicalized rule. As in Klein and Man-
ning (2003), we view each lexicalized rule, `, as
having a CFG projection, pic(`) = r, and a de-
pendency projection, pid(`) = (h, t, h?, t?)(see fig-
ure 5).9 Broadly, the CFG projection encodes con-
stituency structure, while the dependency projection
encodes lexical selection, and both projections are
asymptotically more efficient than the original prob-
lem. Klein and Manning (2003) present a factored
model where the CFG and dependency projections
are generated independently (though with compati-
ble bracketing):
P (Y [h, t]Z[h?, t?] | X[h, t]) = (6)
P (Y Z|X)P (h?, t?|t, h)
In this work, we explore the following non-factored
model, which allows correlations between the CFG
and dependency projections:
P (Y [h, t]Z[h?, t?] | X[h, t]) = P (Y Z|X, t, h) (7)
P (t?|t, Z, h?, h) P (h?|t?, t, Z, h?, h)
This model is broadly representative of the suc-
cessful lexicalized models of Charniak (1997) and
9We assume information about the distance and direction of
the dependency is encoded in the dependency tuple, but we omit
it from the notation for compactness.
Collins (1999), though simpler.10
4.1 Choosing Constraints and Handling
Unseen Dependencies
Ideally we would like to be able to solve the op-
timization problem in (4) for this task. Unfortu-
nately, exhaustively listing all possible configura-
tions (lexical rules) yields an impractical number of
constraints. We therefore solve a relaxed problem in
which we enforce the constraints for only a subset
of the possible configurations, A? ? A. Once we
start dropping constraints, we can no longer guaran-
tee pointwise admissibility, and therefore there is no
reason not to also allow penalized violations of the
constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we
first include all configurations observed in the gold
training trees. We then sample novel configurations
by choosing (X,h, t) from the training distribution
and then using the model to generate the rest of the
configuration. In our experiments, we ended up with
434,329 observed configurations, and sampled the
same number of novel configurations. Our penalty
multiplier C was 10.
Even if we supplement our training set with many
sample configurations, we will still see new pro-
jected dependency configurations at test time. It is
therefore necessary to generalize scores from train-
ing configurations to unseen ones. We enrich our
procedure by expressing the projected configuration
costs as linear functions of features. Specifically, we
define feature vectors fc(r) and fd(h, t, h?t?) over
the CFG and dependency projections, and intro-
10All probability distributions for the non-factored model are
estimated by Witten-Bell smoothing (Witten and Bell, 1991)
where conditioning lexical items are backed off first.
417
SXXXXXXNPSaaa!!!NPNP
DT
These
PPNPHHHNNS
stocks
NPPP
RB
eventually
VPS
VBD
reopened
reopened-VBDhhhhhhhh""((((((((These-DT
These
stocks-NNS
stocks
reopened-VBDPPPP
eventually-RB
eventually
reopened-VBD
reopened
S, reopened-VBDhhhhhhhhhh
((((((((((NPS , stocks-NNSbb""DT
These
NNS
stocks
ADVPS , eventually-RB
RB
eventually
VPS , reopened-VBD
VBD
reopened
Actual Cost: 18.7
Best Projected CFG Cost: 4.1 Best Projected Dep. Cost: 9.5 CFG Projection Cost : 6.9
Dep. Projection Cost: 11.1(a) (b) (c)
Figure 5: Lexicalized parsing projections. The figure in (a) is the optimal CFG projection solution and the figure in (b) is the
optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the
CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.
duce corresponding weight vectors wc and wd. The
weight vectors are learned by solving the following
optimization problem:
minimize
?,wc,wd
??+?2 + C????2 (8)
such that, wc ? 0, wd ? 0
?` = c(`)? [w
T
c fc(r) + w
T
d fd(h, t, h
?, t?)]
for ` = (r, h, t, h?, t?) ? A?
Our CFG feature vector has only indicator features
for the specific rule. However, our dependency fea-
ture vector consists of an indicator feature of the tu-
ple (h, t, h?, t?) (including direction), an indicator of
the part-of-speech type (t, t?) (also including direc-
tion), as well as a bias feature.
4.2 Experimental Results
We tested our approximate projection heuristic on
two lexicalized parsing models. The first is the fac-
tored model of Klein and Manning (2003), given
by equation (6), and the second is the non-factored
model described in equation (7). Both models
use the same parent-annotated head-binarized CFG
backbone and a basic dependency projection which
models direction, but not distance or valence.11
In each case, we compared A? using our approxi-
mate projection heuristics to exhaustive search. We
measure efficiency in terms of the number of ex-
panded hypotheses (edges popped); see figure 6.12
In both settings, the factored A? approach substan-
tially outperforms exhaustive search. For the fac-
11The CFG and dependency projections correspond to the
PCFG-PA and DEP-BASIC settings in Klein and Manning
(2003).
12All models are trained on section 2 through 21 of the En-
glish Penn treebank, and tested on section 23.
tored model of Klein and Manning (2003), we can
also compare our reconstructed bound to the known
tight bound which would result from solving the
pointwise admissible problem in (4) with all con-
straints. As figure 6 shows, the exact factored
heuristic does outperform our approximate factored
heuristic, primarily because of many looser, backed-
off cost estimates for unseen dependency tuples. For
the non-factored model, we compared our approxi-
mate factored heuristic to one which only bounds the
CFG projection as suggested by Klein and Manning
(2003). They suggest,
?c(r) = min
`?A:pic(`)=r
c(`)
where we obtain factored CFG costs by minimizing
over dependency projections. As figure 6 illustrates,
this CFG only heuristic is substantially less efficient
than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be
admissible, we evaluated its effect on search in sev-
eral ways. The first is to check for search errors,
where the model-optimal parse is not found. In the
case of the factored model, we can find the optimal
parse using the exact factored heuristic and compare
it to the parse found by our learned heuristic. In our
test set, the approximate projection heuristic failed
to return the model optimal parse in less than 1% of
sentences. Of these search errors, none of the costs
were more than 0.1% greater than the model optimal
cost in negative log-likelihood. For the non-factored
model, the model optimal parse is known only for
shorter sentences which can be parsed exhaustively.
For these sentences up to length 15, there were no
search errors. We can also check for violations of
pointwise admissibility for configurations encoun-
418
(a)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
(b)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
Figure 6: Edges popped by exhaustive versus factored A? search. The chart in (a) is using the factored lexicalized model from
Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4.
tered during search. For both the factored and non-
factored model, less than 2% of the configurations
scored by the approximate projection heuristic dur-
ing search violated pointwise admissibility.
While this is a paper about inference, we also
measured the accuracy in the standard way, on sen-
tences of length up to 40, using EVALB. The fac-
tored model with the approximate projection heuris-
tic achieves an F1 of 82.2, matching the performance
with the exact factored heuristic, though slower. The
non-factored model, using the approximate projec-
tion heuristic, achieves an F1 of 83.8 on the test set,
which is slightly better than the factored model.13
We note that the CFG and dependency projections
are as similar as possible across models, so the in-
crease in accuracy is likely due in part to the non-
factored model?s coupling of CFG and dependency
projections.
5 Conclusion
We have presented a technique for creating A? es-
timates for inference in complex models. Our tech-
nique can be used to generate provably admissible
estimates when all search transitions can be enumer-
ated, and an effective heuristic even for problems
where all transitions cannot be efficiently enumer-
ated. In the future, we plan to investigate alterna-
tive objective functions and error-driven methods for
learning heuristic bounds.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a DHS fellowship to the first
13Since we cannot exhaustively parse with this model, we
cannot compare our F1 to an exact search method.
author and a Microsoft new faculty fellowship to the
third author.
References
E. D. Andersen and K. D. Andersen. 2000. The MOSEK in-
terior point optimizer for linear programming. In H. Frenk
et al, editor, High Performance Optimization. Kluwer Aca-
demic Publishers.
Stephen Boyd and Lieven Vandenberghe. 2005. Convex Opti-
mization. Cambridge University Press.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In National Conference on Ar-
tificial Intelligence.
Michael Collins. 1999. Head-driven statistical models for nat-
ural language parsing.
Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive
pattern database heuristics. JAIR.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
Malik Ghallab and Dennis G. Allard. 1982. A?? - an efficient
near admissible heuristic search algorithm. In IJCAI.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for
the heuristic determination of minimum cost paths. In IEEE
Transactions on Systems Science and Cybernetics. IEEE.
Dan Klein and Christopher D. Manning. 2003. Factored A*
search for models over sequences and trees. In IJCAI.
Kevin Knight and Jonathan Graehl. 2004. Training tree trans-
ducers. In HLT-NAACL.
I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004.
Generalized multitext grammars. In ACL.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In ACL.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comput. Linguist.
Hao Zhang and Daniel Gildea. 2006. Efficient search for inver-
sion transduction grammar. In EMNLP.
419
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362?370,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Content Models for Multi-Document Summarization
Aria Haghighi
UC Berkeley, CS Division
aria42@cs.berkeley.edu
Lucy Vanderwende
Microsoft Research
Lucy.Vanderwende@microsoft.com
Abstract
We present an exploration of generative prob-
abilistic models for multi-document summa-
rization. Beginning with a simple word fre-
quency based model (Nenkova and Vander-
wende, 2005), we construct a sequence of
models each injecting more structure into the
representation of document set content and ex-
hibiting ROUGE gains along the way. Our
final model, HIERSUM, utilizes a hierarchi-
cal LDA-style model (Blei et al, 2004) to
represent content specificity as a hierarchy of
topic vocabulary distributions. At the task
of producing generic DUC-style summaries,
HIERSUM yields state-of-the-art ROUGE per-
formance and in pairwise user evaluation
strongly outperforms Toutanova et al (2007)?s
state-of-the-art discriminative system. We
also explore HIERSUM?s capacity to produce
multiple ?topical summaries? in order to facil-
itate content discovery and navigation.
1 Introduction
Over the past several years, there has been much in-
terest in the task of multi-document summarization.
In the common Document Understanding Confer-
ence (DUC) formulation of the task, a system takes
as input a document set as well as a short descrip-
tion of desired summary focus and outputs a word
length limited summary.1 To avoid the problem of
generating cogent sentences, many systems opt for
an extractive approach, selecting sentences from the
document set which best reflect its core content.2
1In this work, we ignore the summary focus. Here, the word
topic will refer to elements of our statistical model rather than
summary focus.
2Note that sentence extraction does not solve the problem of
selecting and ordering summary sentences to form a coherent
There are several approaches to modeling docu-
ment content: simple word frequency-based meth-
ods (Luhn, 1958; Nenkova and Vanderwende,
2005), graph-based approaches (Radev, 2004; Wan
and Yang, 2006), as well as more linguistically mo-
tivated techniques (Mckeown et al, 1999; Leskovec
et al, 2005; Harabagiu et al, 2007). Another strand
of work (Barzilay and Lee, 2004; Daume? III and
Marcu, 2006; Eisenstein and Barzilay, 2008), has
explored the use of structured probabilistic topic
models to represent document content. However, lit-
tle has been done to directly compare the benefit of
complex content models to simpler surface ones for
generic multi-document summarization.
In this work we examine a series of content
models for multi-document summarization and ar-
gue that LDA-style probabilistic topic models (Blei
et al, 2003) can offer state-of-the-art summariza-
tion quality as measured by automatic metrics (see
section 5.1) and manual user evaluation (see sec-
tion 5.2). We also contend that they provide con-
venient building blocks for adding more structure
to a summarization model. In particular, we uti-
lize a variation of the hierarchical LDA topic model
(Blei et al, 2004) to discover multiple specific ?sub-
topics? within a document set. The resulting model,
HIERSUM (see section 3.4), can produce general
summaries as well as summaries for any of the
learned sub-topics.
2 Experimental Setup
The task we will consider is extractive multi-
document summarization. In this task we assume
a document collection D consisting of documents
D1, . . . , Dn describing the same (or closely related)
narrative (Lapata, 2003).
362
set of events. Our task will be to propose a sum-
mary S consisting of sentences in D totaling at most
L words.3 Here as in much extractive summariza-
tion, we will view each sentence as a bag-of-words
or more generally a bag-of-ngrams (see section 5.1).
The most prevalent example of this data setting is
document clusters found on news aggregator sites.
2.1 Automated Evaluation
For model development we will utilize the DUC
2006 evaluation set4 consisting of 50 document sets
each with 25 documents; final evaluation will utilize
the DUC 2007 evaluation set (section 5).
Automated evaluation will utilize the standard
DUC evaluation metric ROUGE (Lin, 2004) which
represents recall over various n-grams statistics from
a system-generated summary against a set of human-
generated peer summaries.5 We compute ROUGE
scores with and without stop words removed from
peer and proposed summaries. In particular, we
utilize R-1 (recall against unigrams), R-2 (recall
against bigrams), and R-SU4 (recall against skip-4
bigrams)6. We present R-2 without stop words in the
running text, but full development results are pre-
sented in table 1. Official DUC scoring utilizes the
jackknife procedure and assesses significance using
bootstrapping resampling (Lin, 2004). In addition to
presenting automated results, we also present a user
evaluation in section 5.2.
3 Summarization Models
We present a progression of models for multi-
document summarization. Inference details are
given in section 4.
3.1 SumBasic
The SUMBASIC algorithm, introduced in Nenkova
and Vanderwende (2005), is a simple effective pro-
cedure for multi-document extractive summariza-
tion. Its design is motivated by the observation that
the relative frequency of a non-stop word in a doc-
ument set is a good predictor of a word appearing
in a human summary. In SUMBASIC, each sentence
3For DUC summarization tasks, L is typically 250.
4http://www-nlpir.nist.gov/projects/duc/data.html
5All words from peer and proposed summaries are lower-
cased and stemmed.
6Bigrams formed by skipping at most two words.
S is assigned a score reflecting how many high-
frequency words it contains,
Score(S) = ?
w?S
1
|S|PD(w) (1)
where PD(?) initially reflects the observed unigram
probabilities obtained from the document collection
D. A summary S is progressively built by adding
the highest scoring sentence according to (1).7
In order to discourage redundancy, the words
in the selected sentence are updated PnewD (w) ?
P oldD (w)2. Sentences are selected in this manner un-
til the summary word limit has been reached.
Despite its simplicity, SUMBASIC yields 5.3 R-2
without stop words on DUC 2006 (see table 1).8 By
comparison, the highest-performing ROUGE sys-
tem at the DUC 2006 evaluation, SUMFOCUS, was
built on top of SUMBASIC and yielded a 6.0, which
is not a statistically significant improvement (Van-
derwende et al, 2007).9
Intuitively, SUMBASIC is trying to select a sum-
mary which has sentences where most words have
high likelihood under the document set unigram dis-
tribution. One conceptual problem with this objec-
tive is that it inherently favors repetition of frequent
non-stop words despite the ?squaring? update. Ide-
ally, a summarization criterion should be more recall
oriented, penalizing summaries which omit moder-
ately frequent document set words and quickly di-
minishing the reward for repeated use of word.
Another more subtle shortcoming is the use of the
raw empirical unigram distribution to represent con-
tent significance. For instance, there is no distinc-
tion between a word which occurs many times in the
same document or the same number of times across
several documents. Intuitively, the latter word is
more indicative of significant document set content.
3.2 KLSum
The KLSUM algorithm introduces a criterion for se-
lecting a summary S given document collection D,
S? = min
S:words(S)?L
KL(PD?PS) (2)
7Note that sentence order is determined by the order in
which sentences are selected according to (1).
8This result is presented as 0.053 with the official ROUGE
scorer (Lin, 2004). Results here are scaled by 1,000.
9To be fair obtaining statistical significance in ROUGE
scores is quite difficult.
363
?B Z
W
?C
?D
?t
Document Set
Document
Sentence
Word
Figure 1: Graphical model depiction of TOPIC-
SUM model (see section 3.3). Note that many hyper-
parameter dependencies are omitted for compactness.
where PS is the empirical unigram distribution of
the candidate summary S and KL(P?Q) repre-
sents the Kullback-Lieber (KL) divergence given by?
w P (w) log P (w)Q(w) .10 This quantity represents thedivergence between the true distribution P (here the
document set unigram distribution) and the approx-
imating distribution Q (the summary distribution).
This criterion casts summarization as finding a set
of summary sentences which closely match the doc-
ument set unigram distribution. Lin et al (2006)
propose a related criterion for robust summarization
evaluation, but to our knowledge this criteria has
been unexplored in summarization systems. We ad-
dress optimizing equation (2) as well as summary
sentence ordering in section 4.
KLSUM yields 6.0 R-2 without stop words, beat-
ing SUMBASIC but not with statistical significance. It
is worth noting however that KLSUM?s performance
matches SUMFOCUS (Vanderwende et al, 2007), the
highest R-2 performing system at DUC 2006.
3.3 TopicSum
As mentioned in section 3.2, the raw unigram dis-
tribution PD(?) may not best reflect the content of
D for the purpose of summary extraction. We
propose TOPICSUM, which uses a simple LDA-like
topic model (Blei et al, 2003) similar to Daume?
III and Marcu (2006) to estimate a content distribu-
10In order to ensure finite values of KL-divergence we
smoothe PS(?) so that it has a small amount of mass on all doc-
ument set words.
System ROUGE -stop ROUGE all
R-1 R-2 R-SU4 R-1 R-2 R-SU4
SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3
KLSUM 30.6 6.0 8.9 38.9 8.3 13.7
TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6
HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3
Table 1: ROUGE results on DUC2006 for models pre-
sented in section 3. Results in bold represent results sta-
tistically significantly different from SUMBASIC in the
appropriate metric.
tion for summary extraction.11 We extract summary
sentences as before using the KLSUM criterion (see
equation (2)), plugging in a learned content distribu-
tion in place of the raw unigram distribution.
First, we describe our topic model (see figure 1)
which generates a collection of document sets. We
assume a fixed vocabulary V :12
1. Draw a background vocabulary distribution ?B
from DIRICHLET(V ,?B) shared across docu-
ment collections13 representing the background
distribution over vocabulary words. This distri-
bution is meant to flexibly model stop words
which do not contribute content. We will refer
to this topic as BACKGROUND.
2. For each document set D, we draw a content
distribution ?C from DIRICHLET(V ,?C) repre-
senting the significant content of D that we
wish to summarize. We will refer to this topic
as CONTENT.
3. For each document D in D, we draw a
document-specific vocabulary distribution ?D
from DIRICHLET(V ,?D) representing words
which are local to a single document, but do
not appear across several documents. We will
refer to this topic as DOCSPECIFIC.
11A topic model is a probabilistic generative process that gen-
erates a collection of documents using a mixture of topic vo-
cabulary distributions (Steyvers and Griffiths, 2007). Note this
usage of topic is unrelated to the summary focus given for doc-
ument collections; this information is ignored by our models.
12In contrast to previous models, stop words are not removed
in pre-processing.
13DIRICHLET(V ,?) represents the symmetric Dirichlet
prior distribution over V each with a pseudo-count of ?. Con-
crete pseudo-count values will be given in section 4.
364
{ star: 0.21, wars: 0.15, phantom: 0.10, ... }  
General Content Topic
?C1
{ $: 0.39, million: 0.15, record: 0.8, ... }  
Specific Content Topic               "Financial"
?C1
{ toys: 0.22, spend: 0.18, sell: 0.10, ... }  
{ fans: 0.16, line: 0.12, film: 0.09, ... }  
Specific Content Topic               "Merchandise"
Specific Content Topic               "Fans"
?C2
?C3
Document Set
?C0
?CK?C1
ZS
?D
Document
Sentence
Word
Z
W
?T
?G
.........
?B
(a) Content Distributions (b) HIERSUM Graphical Model
Figure 2: (a): Examples of general versus specific content distributions utilized by HIERSUM (see section 3.4). The
general content distribution ?C0 will be used throughout a document collection and represents core concepts in a
story. The specific content distributions represent topical ?sub-stories? with vocabulary tightly clustered together but
consistently used across documents. Quoted names of specific topics are given manually to facilitate interpretation. (b)
Graphical model depiction of the HIERSUM model (see section 3.4). Similar to the TOPICSUM model (see section 3.3)
except for adding complexity in the content hierarchy as well as sentence-specific prior distributions between general
and specific content topics (early sentences should have more general content words). Several dependencies are
missing from this depiction; crucially, each sentence?s specific topic ZS depends on the last sentence?s ZS .
4. For each sentence S of each document
D, draw a distribution ?T over topics
(CONTENT,DOCSPECIFIC, BACKGROUND)
from a Dirichlet prior with pseudo-counts
(1.0, 5.0, 10.0).14 For each word position in
the sentence, we draw a topic Z from ?T ,
and a word W from the topic distribution Z
indicates.
Our intent is that ?C represents the core con-
tent of a document set. Intuitively, ?C does
not include words which are common amongst
several document collections (modeled with the
BACKGROUND topic), or words which don?t appear
across many documents (modeled with the DOCSPE-
CIFIC topic). Also, because topics are tied together
at the sentence level, words which frequently occur
with other content words are more likely to be con-
sidered content words.
We ran our topic model over the DUC 2006
document collections and estimated the distribution
?C(?) for each document set.15 Then we extracted
14The different pseudo-counts reflect the intuition that most
of the words in a document come from the BACKGROUND and
DOCSPECIFIC topics.
15While possible to obtain the predictive posterior CON-
a summary using the KLSUM criterion with our es-
timated ?C in place of the the raw unigram distribu-
tion. Doing so yielded 6.3 R-2 without stop words
(see TOPICSUM in table 1); while not a statistically
significant improvement over KLSUM, it is our first
model which outperforms SUMBASIC with statistical
significance.
Daume? III and Marcu (2006) explore a topic
model similar to ours for query-focused multi-
document summarization.16 Crucially however,
Daume? III andMarcu (2006) selected sentences with
the highest expected number of CONTENT words.17
We found that in our model using this extraction
criterion yielded 5.3 R-2 without stop words, sig-
nificantly underperforming our TOPICSUM model.
One reason for this may be that Daume? III and
Marcu (2006)?s criterion encourages selecting sen-
tences which have words that are confidently gener-
ated by the CONTENT distribution, but not necessar-
ily sentences which contain a plurality of it?s mass.
TENT distribution by analytically integrating over ?C (Blei et
al., 2003), doing so gave no benefit.
16Daume? III and Marcu (2006) note their model could be
used outside of query-focused summarization.
17This is phrased as selecting the sentence which has the
highest posterior probability of emitting CONTENT topic
words, but this is equivalent.
365
(a) HIERSUM output
The French government
Saturday announced sev-
eral emergency measures
to support the jobless
people, including sending
an additional 500 million
franc (84 million U.S. dol-
lars) unemployment aid
package. The unem-
ployment rate in France
dropped by 0.3 percent
to stand at 12.4 percent
in November, said the
Ministry of Employment
Tuesday.
(b) PYTHY output
Several hundred people
took part in the demon-
stration here today against
the policies of the world?s
most developed nations.
The 12.5 percent unem-
ployment rate is haunt-
ing the Christmas sea-
son in France as militants
and unionists staged sev-
eral protests over the past
week against unemploy-
ment.
(c) Ref output
High unemployment is
France?s main economic
problem, despite recent
improvements. A top
worry of French people,
it is a factor affecting
France?s high suicide rate.
Long-term unemployment
causes social exclusion
and threatens France?s
social cohesion.
(d) Reference Unigram Coverage
word Ref PYTHY HIERSUMunemployment 8 9 10france?s 6 1 4francs 4 0 1high 4 1 2economic 2 0 1french 2 1 3problem 2 0 1benefits 2 0 0social 2 0 2jobless 2 1 2
Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two
sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent
non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.
Note that many content words in the reference summary absent from PYTHY?s proposal are present in HIERSUM?s.
3.4 HIERSUM
Previous sections have treated the content of a doc-
ument set as a single (perhaps learned) unigram dis-
tribution. However, as Barzilay and Lee (2004) ob-
serve, the content of document collections is highly
structured, consisting of several topical themes, each
with its own vocabulary and ordering preferences.
For concreteness consider the DUC 2006 docu-
ment collection describing the opening of Star Wars:
Episode 1 (see figure 2(a)).
While there are words which indicate the general
content of this document collection (e.g. star, wars),
there are several sub-stories with their own specific
vocabulary. For instance, several documents in this
collection spend a paragraph or two talking about
the financial aspect of the film?s opening and use a
specific vocabulary there (e.g. $, million, record). A
user may be interested in general content of a docu-
ment collection or, depending on his or her interests,
one or more of the sub-stories. We choose to adapt
our topic modeling approach to allow modeling this
aspect of document set content.
Rather than drawing a single CONTENT distribu-
tion ?C for a document collection, we now draw
a general content distribution ?C0 from DIRICH-
LET(V ,?G) as well as specific content distribu-
tions ?Ci for i = 1, . . . ,K each from DIRICH-
LET(V ,?S).18 Our intent is that ?C0 represents the
18We choose K=3 in our experiments, but one could flexibly
general content of the document collection and each
?Ci represents specific sub-stories.
As with TOPICSUM, each sentence
has a distribution ?T over topics
(BACKGROUND,DOCSPECIFIC, CONTENT). When
BACKGROUND or DOCSPECIFIC topics are chosen,
the model works exactly as in TOPICSUM. However
when the CONTENT topic is drawn, we must decide
whether to emit a general content word (from ?C0)
or from one of the specific content distributions
(from one of ?Ci for i = 1, . . . ,K). The generative
story of TOPICSUM is altered as follows in this case:
? General or Specific? We must first decide
whether to use a general or specific content
word. Each sentence draws a binomial distribu-
tion ?G determining whether a CONTENT word
in the sentence will be drawn from the general
or a specific topic distribution. Reflecting the
intuition that the earlier sentences in a docu-
ment19 describe the general content of a story,
we bias ?G to be drawn from BETA(5,2), pre-
ferring general content words, and every later
sentence from BETA(1,2).20
? What Specific Topic? If ?G decides we are
choose K as Blei et al (2004) does.
19In our experiments, the first 5 sentences.
20BETA(a,b) represents the beta prior over binomial random
variables with a and b being pseudo-counts for the first and sec-
ond outcomes respectively.
366
emitting a topic specific content word, we must
decide which of ?C1 , . . . , ?CK to use. In or-
der to ensure tight lexical cohesion amongst the
specific topics, we assume that each sentence
draws a single specific topic ZS used for every
specific content word in that sentence. Reflect-
ing intuition that adjacent sentences are likely
to share specific content vocabulary, we uti-
lize a ?sticky? HMM as in Barzilay and Lee
(2004) over the each sentences? ZS . Con-
cretely, ZS for the first sentence in a docu-
ment is drawn uniformly from 1, . . . ,K, and
each subsequent sentence?s ZS will be identi-
cal to the previous sentence with probability ?,
and with probability 1 ? ? we select a succes-
sor topic from a learned transition distribution
amongst 1, . . . ,K.21
Our intent is that the general content distribution
?C0 now prefers words which not only appear in
many documents, but also words which appear con-
sistently throughout a document rather than being
concentrated in a small number of sentences. Each
specific content distribution ?Ci is meant to model
topics which are used in several documents but tend
to be used in concentrated locations.
HIERSUM can be used to extract several kinds
of summaries. It can extract a general summary
by plugging ?C0 into the KLSUM criterion. It can
also produce topical summaries for the learned spe-
cific topics by extracting a summary over each ?Ci
distribution; this might be appropriate for a user
who wants to know more about a particular sub-
story. While we found the general content distribu-
tion (from ?C0) to produce the best single summary,
we experimented with utilizing topical summaries
for other summarization tasks (see section 6.1). The
resulting system, HIERSUM yielded 6.4 R-2 without
stop words. While not a statistically significant im-
provement in ROUGE over TOPICSUM, we found the
summaries to be noticeably improved.
4 Inference and Model Details
Since globally optimizing the KLSUM criterion in
equation (equation (2)) is exponential in the total
number of sentences in a document collection, we
21We choose ? = 0.75 in our experiments.
opted instead for a simple approximation where sen-
tences are greedily added to a summary so long as
they decrease KL-divergence. We attempted more
complex inference procedures such as McDonald
(2007), but these attempts only yielded negligible
performance gains. All summary sentence order-
ing was determined as follows: each sentence in the
proposed summary was assigned a number in [0, 1]
reflecting its relative sentence position in its source
document, and sorted by this quantity.
All topic models utilize Gibbs sampling for in-
ference (Griffiths, 2002; Blei et al, 2004). In gen-
eral for concentration parameters, the more specific
a distribution is meant to be, the smaller its con-
centration parameter. Accordingly for TOPICSUM,
?G = ?D = 1 and ?C = 0.1. For HIERSUM we
used ?G = 0.1 and ?S = 0.01. These parameters
were minimally tuned (without reference to ROUGE
results) in order to ensure that all topic distribution
behaved as intended.
5 Formal Experiments
We present formal experiments on the DUC 2007
data main summarization task, proposing a general
summary of at most 250 words22 which will be eval-
uated automatically and manually in order to simu-
late as much as possible the DUC evaluation envi-
ronment.23 DUC 2007 consists of 45 document sets,
each consisting of 25 documents and 4 human refer-
ence summaries.
We primarily evaluate the HIERSUM model, ex-
tracting a single summary from the general con-
tent distribution using the KLSUM criterion (see sec-
tion 3.2). Although the differences in ROUGE be-
tween HIERSUM and TOPICSUM were minimal, we
found HIERSUM summary quality to be stronger.
In order to provide a reference for ROUGE
and manual evaluation results, we compare against
PYTHY, a state-of-the-art supervised sentence ex-
traction summarization system. PYTHY uses human-
generated summaries in order to train a sentence
ranking system which discriminatively maximizes
22Since the ROUGE evaluation metric is recall-oriented, it is
always advantageous - with respect to ROUGE - to use all 250
words.
23Although the DUC 2007 main summarization task provides
an indication of user intent through topic focus queries, we ig-
nore this aspect of the data.
367
System ROUGE w/o stop ROUGE w/ stop
R-1 R-2 R-SU4 R-1 R-2 R-SU4
HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3
HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7
PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5
PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8
Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1). While HIER-
SUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stop
words), HIERSUM bigram?s performance is comparable and statistically no worse.
ROUGE scores. PYTHY uses several features to
rank sentences including several variations of the
SUMBASIC score (see section 3.1). At DUC 2007,
PYTHY was ranked first overall in automatic ROUGE
evaluation and fifth in manual content judgments.
As PYTHY utilizes a sentence simplification com-
ponent, which we do not, we also compare against
PYTHY without sentence simplification.
5.1 ROUGE Evaluation
ROUGE results comparing variants of HIERSUM and
PYTHY are given in table 3. The HIERSUM system
as described in section 3.4 yields 7.3 R-2 without
stop words, falling significantly short of the 8.7 that
PYTHY without simplification yields. Note that R-2
is a measure of bigram recall and HIERSUM does not
represent bigrams whereas PYTHY includes several
bigram and higher order n-gram statistics.
In order to put HIERSUM and PYTHY on equal-
footing with respect to R-2, we instead ran HIER-
SUM with each sentence consisting of a bag of bi-
grams instead of unigrams.24 All the details of the
model remain the same. Once a general content
distribution over bigrams has been determined by
hierarchical topic modeling, the KLSUM criterion
is used as before to extract a summary. This sys-
tem, labeled HIERSUM bigram in table 3, yields 9.3
R-2 without stop words, significantly outperform-
ing HIERSUM unigram. This model outperforms
PYTHY with and without sentence simplification, but
not with statistical significance. We conclude that
both PYTHY variants and HIERSUM bigram are com-
parable with respect to ROUGE performance.
24Note that by doing topic modeling in this way over bi-
grams, our model becomes degenerate as it can generate incon-
sistent bags of bigrams. Future work may look at topic models
over n-grams as suggested by Wang et al (2007).
Question PYTHY HIERSUM
Overall 20 49
Non-Redundancy 21 48
Coherence 15 54
Focus 28 41
Table 4: Results of manual user evaluation (see sec-
tion 5.2). 15 participants expressed 69 pairwise prefer-
ences between HIERSUM and PYTHY. For all attributes,
HIERSUM outperforms PYTHY; all results are statisti-
cally significant as determined by pairwise t-test.
5.2 Manual Evaluation
In order to obtain a more accurate measure of sum-
mary quality, we performed a simple user study. For
each document set in the DUC 2007 collection, a
user was given a reference summary, a PYTHY sum-
mary, and a HIERSUM summary;25 note that the orig-
inal documents in the set were not provided to the
user, only a reference summary. For this experiment
we use the bigram variant of HIERSUM and compare
it to PYTHY without simplification so both systems
have the same set of possible output summaries.
The reference summary for each document set
was selected according to highest R-2 without stop
words against the remaining peer summaries. Users
were presented with 4 questions drawn from the
DUC manual evaluation guidelines:26 (1) Overall
quality: Which summary was better overall? (2)
Non-Redundancy: Which summary was less redun-
dant? (3) Coherence: Which summary was more
coherent? (4) Focus: Which summary was more
25The system identifier was of course not visible to the user.
The order of automatic summaries was determined randomly.
26http://www-nlpir.nist.gov/projects/duc/duc2007/quality-
questions.txt
368
Figure 3: Using HIERSUM to organize content of document set into topics (see section 6.1). The sidebar gives key
phrases salient in each of the specific content topics in HIERSUM (see section 3.4). When a topic is clicked in the right
sidebar, the main frame displays an extractive ?topical summary? with links into document set articles. Ideally, a user
could use this interface to quickly find content in a document collection that matches their interest.
focused in its content, not conveying irrelevant de-
tails? The study had 16 users and each was asked
to compare five summary pairs, although some did
fewer. A total of 69 preferences were solicited. Doc-
ument collections presented to users were randomly
selected from those evaluated fewest.
As seen in table 5.2, HIERSUM outperforms
PYTHY under all questions. All results are statis-
tically significant as judged by a simple pairwise
t-test with 95% confidence. It is safe to conclude
that users in this study strongly preferred the HIER-
SUM summaries over the PYTHY summaries.
6 Discussion
While it is difficult to qualitatively compare one
summarization system over another, we can broadly
characterize HIERSUM summaries compared to some
of the other systems discussed. For example out-
put from HIERSUM and PYTHY see table 2. On the
whole, HIERSUM summaries appear to be signifi-
cantly less redundant than PYTHY and moderately
less redundant than SUMBASIC. The reason for this
might be that PYTHY is discriminatively trained to
maximize ROUGE which does not directly penalize
redundancy. Another tendency is for HIERSUM to se-
lect longer sentences typically chosen from an early
sentence in a document. As discussed in section 3.4,
HIERSUM is biased to consider early sentences in
documents have a higher proportion of general con-
tent words and so this tendency is to be expected.
6.1 Content Navigation
A common concern in multi-document summariza-
tion is that without any indication of user interest or
intent providing a single satisfactory summary to a
user may not be feasible. While many variants of
the general summarization task have been proposed
which utilize such information (Vanderwende et al,
2007; Nastase, 2008), this presupposes that a user
knows enough of the content of a document collec-
tion in order to propose a query.
As Leuski et al (2003) and Branavan et al (2007)
suggest, a document summarization system should
facilitate content discovery and yield summaries rel-
evant to a user?s interests. We may use HIERSUM in
order to facilitate content discovery via presenting
a user with salient words or phrases from the spe-
cific content topics parametrized by ?C1 , . . . , ?CK
(for an example see figure 3). While these topics are
not adaptive to user interest, they typically reflect
lexically coherent vocabularies.
Conclusion
In this paper we have presented an exploration of
content models for multi-document summarization
and demonstrated that the use of structured topic
models can benefit summarization quality as mea-
sured by automatic and manual metrics.
Acknowledgements The authors would like to
thank Bob Moore, Chris Brockett, Chris Quirk, and
Kristina Toutanova for their useful discussions as
well as the reviewers for their helpful comments.
369
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. JMLR.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
NIPS.
S.R.K. Branavan, Pawan Deshpande, and Regina Barzi-
lay. 2007. Generating a table-of-contents. In ACL.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP-
SIGDAT.
Thomas Griffiths. 2002. Gibbs sampling in the genera-
tive model of latent dirichlet alocation.
Sanda Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2007. Satisfying information needs with multi-
document summaries. Inf. Process. Manage., 43(6).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL.
Jurij Leskovec, Natasa Milic-frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the se-
mantic graph coverage and learning of document ex-
tracts. In In AAAI 05.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: Interactive multi-document summarization. In
ACL.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In HLT-NAACL.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In ECIR.
Kathleen R. Mckeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by re-
formulation: Progress and prospects. In In Proceed-
ings of AAAI-99.
Vivi Nastase. 2008. Topic-driven multi-document sum-
marization with encyclopedic knowledge and spread-
ing activation. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, Mi-
crosoft Research.
Dragomir R. Radev. 2004. Lexrank: graph-based cen-
trality as salience in text summarization. Journal of
Artificial Intelligence Research (JAIR.
M. Steyvers and T. Griffiths, 2007. Probabilistic Topic
Models.
Kristina Toutanova, Chris Brockett, Michael Gamon Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In DUC.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lexi-
cal expansion. volume 43.
Xiaojun Wan and Jianwu Yang. 2006. Improved affinity
graph based multi-document summarization. In HLT-
NAACL.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In ICDM.
370
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881?888,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Prototype-Driven Grammar Induction
Aria Haghighi
Computer Science Division
University of California Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California Berkeley
klein@cs.berkeley.edu
Abstract
We investigate prototype-driven learning for pri-
marily unsupervised grammar induction. Prior
knowledge is specified declaratively, by providing a
few canonical examples of each target phrase type.
This sparse prototype information is then propa-
gated across a corpus using distributional similar-
ity features, which augment an otherwise standard
PCFG model. We show that distributional features
are effective at distinguishing bracket labels, but not
determining bracket locations. To improve the qual-
ity of the induced trees, we combine our PCFG in-
duction with the CCM model of Klein and Manning
(2002), which has complementary stengths: it iden-
tifies brackets but does not label them. Using only
a handful of prototypes, we show substantial im-
provements over naive PCFG induction for English
and Chinese grammar induction.
1 Introduction
There has been a great deal of work on unsuper-
vised grammar induction, with motivations rang-
ing from scientific interest in language acquisi-
tion to engineering interest in parser construc-
tion (Carroll and Charniak, 1992; Clark, 2001).
Recent work has successfully induced unlabeled
grammatical structure, but has not successfully
learned labeled tree structure (Klein and Manning,
2002; Klein and Manning, 2004; Smith and Eis-
ner, 2004) .
In this paper, our goal is to build a system capa-
ble of producing labeled parses in a target gram-
mar with as little total effort as possible. We in-
vestigate a prototype-driven approach to grammar
induction, in which one supplies canonical ex-
amples of each target concept. For example, we
might specify that we are interested in trees which
use the symbol NP and then list several examples
of prototypical NPs (determiner noun, pronouns,
etc., see figure 1 for a sample prototype list). This
prototype information is similar to specifying an
annotation scheme, which even human annotators
must be provided before they can begin the con-
struction of a treebank. In principle, prototype-
driven learning is just a kind of semi-supervised
learning. However, in practice, the information we
provide is on the order of dozens of total seed in-
stances, instead of a handful of fully parsed trees,
and is of a different nature.
The prototype-driven approach has three
strengths. First, since we provide a set of target
symbols, we can evaluate induced trees using
standard labeled parsing metrics, rather than the
far more forgiving unlabeled metrics described in,
for example, Klein and Manning (2004). Second,
knowledge is declaratively specified in an inter-
pretable way (see figure 1). If a user of the system
is unhappy with its systematic behavior, they can
alter it by altering the prototype information (see
section 7.1 for examples). Third, and related to
the first two, one does not confuse the ability of
the system to learn a consistent grammar with its
ability to learn the grammar a user has in mind.
In this paper, we present a series of experiments
in the induction of labeled context-free trees us-
ing a combination of unlabeled data and sparse
prototypes. We first affirm the well-known re-
sult that simple, unconstrained PCFG induction
produces grammars of poor quality as measured
against treebank structures. We then augment a
PCFGwith prototype features, and show that these
features, when propagated to non-prototype se-
quences using distributional similarity, are effec-
tive at learning bracket labels on fixed unlabeled
trees, but are still not enough to learn good tree
structures without bracketing information. Finally,
we intersect the feature-augmented PCFGwith the
CCM model of Klein and Manning (2002), a high-
quality bracketing model. The intersected model
is able to learn trees with higher unlabeled F1 than
those in Klein and Manning (2004). More impor-
881
tantly, its trees are labeled and can be evaluated
according to labeled metrics. Against the English
Penn Treebank, our final trees achieve a labeled F1
of 65.1 on short sentences, a 51.7% error reduction
over naive PCFG induction.
2 Experimental Setup
The majority of our experiments induced tree
structures from the WSJ section of the English
Penn treebank (Marcus et al, 1994), though see
section 7.4 for an experiment on Chinese. To fa-
cilitate comparison with previous work, we ex-
tracted WSJ-10, the 7,422 sentences which con-
tain 10 or fewer words after the removal of punc-
tuation and null elements according to the scheme
detailed in Klein (2005). We learned models on all
or part of this data and compared their predictions
to the manually annotated treebank trees for the
sentences on which the model was trained. As in
previous work, we begin with the part-of-speech
(POS) tag sequences for each sentence rather than
lexical sequences (Carroll and Charniak, 1992;
Klein and Manning, 2002).
Following Klein and Manning (2004), we report
unlabeled bracket precision, recall, and F1. Note
that according to their metric, brackets of size 1
are omitted from the evaluation. Unlike that work,
all of our induction methods produce trees labeled
with symbols which are identified with treebank
categories. Therefore, we also report labeled pre-
cision, recall, and F1, still ignoring brackets of
size 1.1
3 Experiments in PCFG induction
As an initial experiment, we used the inside-
outside algorithm to induce a PCFG in the
straightforward way (Lari and Young, 1990; Man-
ning and Schu?tze, 1999). For all the experiments
in this paper, we considered binary PCFGs over
the nonterminals and terminals occuring in WSJ-
10. The PCFG rules were of the following forms:
? X ? Y Z, for nonterminal types X,Y, and
Z, with Y 6= X or Z 6= X
? X ? t Y , X ? Y t, for each terminal t
? X ? t t?, for terminals t and t?
For a given sentence S, our CFG generates la-
beled trees T over S.2 Each tree consists of binary
1In cases where multiple gold labels exist in the gold trees,
precision and recall were calculated as in Collins (1999).
2Restricting our CFG to a binary branching grammar re-
sults in an upper bound of 88.1% on unlabeled F1.
productions X(i, j) ? ? over constituent spans
(i, j), where ? is a pair of non-terminal and/or
terminal symbols in the grammar. The generative
probability of a tree T for S is:
PCFG(T, S) =
?
X(i,j)???T
P (?|X)
In the inside-outside algorithm, we iteratively
compute posterior expectations over production
occurences at each training span, then use those
expectations to re-estimate production probabili-
ties. This process is guaranteed to converge to a
local extremum of the data likelihood, but initial
production probability estimates greatly influence
the final grammar (Carroll and Charniak, 1992). In
particular, uniform initial estimates are an (unsta-
ble) fixed point. The classic approach is to add a
small amount of random noise to the initial prob-
abilities in order to break the symmetry between
grammar symbols.
We randomly initialized 5 grammars using tree-
bank non-terminals and trained each to conver-
gence on the first 2000 sentences of WSJ-10.
Viterbi parses were extracted for each of these
2000 sentences according to each grammar. Of
course, the parses? symbols have nothing to anchor
them to our intended treebank symbols. That is, an
NP in one of these grammars may correspond to
the target symbol VP, or may not correspond well
to any target symbol. To evaluate these learned
grammars, we must map the models? phrase types
to target phrase types. For each grammar, we fol-
lowed the common approach of greedily mapping
model symbols to target symbols in the way which
maximizes the labeled F1. Note that this can, and
does, result in mapping multiple model symbols
to the most frequent target symbols. This experi-
ment, labeled PCFG? NONE in figure 4, resulted in
an average labeled F1 of 26.3 and an unlabeled F1
of 45.7. The unlabeled F1 is better than randomly
choosing a tree (34.7), but not better than always
choosing a right branching structure (61.7).
Klein and Manning (2002) suggest that the task
of labeling constituents is significantly easier than
identifying them. Perhaps it is too much to ask
a PCFG induction algorithm to perform both of
these tasks simultaneously. Along the lines of
Pereira and Schabes (1992), we reran the inside-
outside algorithm, but this time placed zero mass
on all trees which did not respect the bracketing
of the gold trees. This constraint does not fully
882
Phrase Prototypes Phrase Prototypes
NP DT NN VP VBN IN NN
JJ NNS VBD DT NN
NNP NNP MD VB CD
S PRP VBD DT NN QP CD CD
DT NN VBD IN DT NN RB CD
DT VBZ DT JJ NN DT CD CD
PP IN NN ADJP RB JJ
TO CD CD JJ
IN PRP JJ CC JJ
ADVP RB RB
RB CD
RB CC RB
VP-INF VB NN NP-INF NN POS
Figure 1: English phrase type prototype list man-
ually specified (The entire supervision for our sys-
tem). The second part of the table is additional
prototypes discussed in section 7.1.
eliminate the structural uncertainty since we are
inducing binary trees and the gold trees are flat-
ter than binary in many cases. This approach of
course achieved the upper bound on unlabeled F1,
because of the gold bracket constraints. However,
it only resulted in an average labeled F1 of 52.6
(experiment PCFG ? GOLD in figure 4). While this
labeled score is an improvement over the PCFG ?
NONE experiment, it is still relatively disappoint-
ing.
3.1 Encoding Prior Knowledge with
Prototypes
Clearly, we need to do something more than
adding structural bias (e.g. bracketing informa-
tion) if we are to learn a PCFG in which the sym-
bols have the meaning and behaviour we intend.
How might we encode information about our prior
knowledge or intentions?
Providing labeled trees is clearly an option. This
approach tells the learner how symbols should re-
cursively relate to each other. Another option is to
provide fully linearized yields as prototypes. We
take this approach here, manually creating a list
of POS sequences typical of the 7 most frequent
categories in the Penn Treebank (see figure 1).3
Our grammar is limited to these 7 phrase types
plus an additional type which has no prototypes
and is unconstrained.4 This list grounds each sym-
3A possible objection to this approach is the introduction
of improper reasearcher bias via specifying prototypes. See
section 7.3 for an experiment utilizing an automatically gen-
erated prototype list with comparable results.
4In our experiments we found that adding prototypes for
more categories did not improve performance and took more
bol in terms of an observable portion of the data,
rather than attempting to relate unknown symbols
to other unknown symbols.
Broadly, we would like to learn a grammar
which explains the observed data (EM?s objec-
tive) but also meets our prior expectations or re-
quirements of the target grammar. How might
we use such a list to constrain the learning of
a PCFG with the inside-outside algorithm? We
might require that all occurences of a prototype
sequence, say DT NN, be constituents of the cor-
responding type (NP). However, human-elicited
prototypes are not likely to have the property that,
when they occur, they are (nearly) always con-
stituents. For example, DT NN is a perfectly rea-
sonable example of a noun phrase, but is not a con-
stituent when it is part of a longer DT NN NN con-
stituent. Therefore, when summing over trees with
the inside-outside algorithm, we could require a
weaker property: whenever a prototype sequence
is a constituent it must be given the label specified
in the prototype file.5 This constraint is enough to
break the symmetry between the model labels, and
therefore requires neither random initialization for
training, nor post-hoc mapping of labels for eval-
uation. Adding prototypes in this way and keep-
ing the gold bracket constraint gave 59.9 labeled
F1. The labeled F1 measure is again an improve-
ment over naive PCFG induction, but is perhaps
less than we might expect given that the model has
been given bracketing information and has proto-
types as a form of supervision to direct it.
In response to a prototype, however, we may
wish to conclude something stronger than a con-
straint on that particular POS sequence. We might
hope that sequences which are similar to a proto-
type in some sense are generally given the same
label as that prototype. For example, DT NN is a
noun phrase prototype, the sequence DT JJ NN is
another good candidate for being a noun phrase.
This kind of propagation of constraints requires
that we have a good way of defining and detect-
ing similarity between POS sequences.
3.2 Phrasal Distributional Similarity
A central linguistic argument for constituent types
is substitutability: phrases of the same type appear
time. We note that we still evaluate against all phrase types
regardless of whether or not they are modeled by our gram-
mar.
5Even this property is likely too strong: prototypes may
have multiple possible labels, for example DT NN may also
be a QP in the English treebank.
883
Yield Prototype Skew KL Phrase Type Skew KL
DT JJ NN DT NN 0.10 NP 0.39
IN DT VBG NN IN NN 0.24 PP 0.45
DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58
CC NN IN NN 0.43 PP 0.71
MD NNS PRP VBD DT NN 1.43 NONE -
Figure 2: Yields along with most similar proto-
types and phrase types, guessed according to (3).
in similar contexts and are mutually substitutable
(Harris, 1954; Radford, 1988). For instance, DT
JJ NN and DT NN occur in similar contexts, and
are indeed both common NPs. This idea has been
repeatedly and successfully operationalized using
various kinds of distributional clustering, where
we define a similarity measure between two items
on the basis of their immediate left and right con-
texts (Schu?tze, 1995; Clark, 2000; Klein and Man-
ning, 2002).
As in Clark (2001), we characterize the distribu-
tion of a sequence by the distribution of POS tags
occurring to the left and right of that sequence in
a corpus. Each occurence of a POS sequence ?
falls in a context x ? y, where x and y are the ad-
jacent tags. The distribution over contexts x ? y
for a given ? is called its signature, and is denoted
by ?(?). Note that ?(?) is composed of context
counts from all occurences, constitiuent and dis-
tituent, of ?. Let ?c(?) denote the context dis-
tribution for ? where the context counts are taken
only from constituent occurences of ?. For each
phrase type in our grammar,X , define ?c(X) to be
the context distribution obtained from the counts
of all constituent occurences of type X:
?c(X) = Ep(?|X) ?c(?) (1)
where p(?|X) is the distribution of yield types for
phrase type X . We compare context distributions
using the skewed KL divergence:
DSKL(p, q) = DKL(p??p + (1? ?)q)
where ? controls how much of the source distribu-
tions is mixed in with the target distribution.
A reasonable baseline rule for classifying the
phrase type of a POS yield is to assign it to the
phrase from which it has minimal divergence:
type(?) = argmin
X
DSKL(?c(?), ?c(X)) (2)
However, this rule is not always accurate, and,
moreover, we do not have access to ?c(?) or
?c(X). We chose to approximate ?c(X) us-
ing the prototype yields for X as samples from
p(?|X). Letting proto(X) denote the (few) pro-
totype yields for phrase type X , we define ??(X):
??(X) =
1
|proto(X)|
?
??proto(X)
?(?)
Note ??(X) is an approximation to (1) in sev-
eral ways. We have replaced an expectation over
p(?|X) with a uniform weighting of proto(X),
and we have replaced ?c(?) with ?(?) for each
term in that expectation. Because of this, we will
rely only on high confidence guesses, and allow
yields to be given a NONE type if their divergence
from each ??(X) exceeds a fixed threshold t. This
gives the following alternative to (2):
type(?) = (3)
{
NONE, if minX DSKL(?(?), ??(X)) < t
argminX DSKL(?(?), ??(X)), otherwise
We built a distributional model implementing
the rule in (3) by constructing ?(?) from context
counts in the WSJ portion of the Penn Treebank
as well as the BLIPP corpus. Each ??(X) was ap-
proximated by a uniform mixture of ?(?) for each
of X?s prototypes ? listed in figure 1.
This method of classifying constituents is very
precise if the threshold is chosen conservatively
enough. For instance, using a threshold of t =
0.75 and ? = 0.1, this rule correctly classifies the
majority label of a constituent-type with 83% pre-
cision, and has a recall of 23% over constituent
types. Figure 2 illustrates some sample yields, the
prototype sequence to which it is least divergent,
and the output of rule (3).
We incorporated this distributional information
into our PCFG induction scheme by adding a pro-
totype feature over each span (i, j) indicating the
output of (3) for the yield ? in that span. Asso-
ciated with each sentence S is a feature map F
specifying, for each (i, j), a prototype feature pij .
These features are generated using an augmented
CFG model, CFG+, given by:6
PCFG+(T, F ) =
?
X(i,j)???T
P (pij |X)P (?|X)
=
?
X(i,j)???T
?CFG+(X ? ?, pij)
6Technically, all features in F must be generated for each
assignment to T , which means that there should be terms in
this equation for the prototype features on distituent spans.
However, we fixed the prototype distribution to be uniform
for distituent spans so that the equation is correct up to a con-
stant depending on F .
884
P (S|ROOT) ? ROOT
S
? P (NP VP|S)
P (P = NONE|S)XXXXX
P (NN NNS|NP)
P (P = NP|NP)
ff
NP
 HHH
NNN
payrolls
NN
Factory
VP
? P (VBD PP|VP)
P (P = VP|VP)aaa
!!!
VBD
fell
PP
? P (IN NN|PP)
P (P = PP|PP)!!! aaa
NN
November
IN
in
Figure 3: Illustration of PCFG augmented with
prototype similarity features.
where ?CFG+(X ? ?, pij) is the local factor for
placing X ? ? on a span with prototype feature
pij . An example is given in figure 3.
For our experiments, we fixed P (pij |X) to be:
P (pij |X) =
{
0.60, if pij = X
uniform, otherwise
Modifying the model in this way, and keeping the
gold bracketing information, gave 71.1 labeled F1
(see experiment PROTO ? GOLD in figure 4), a
40.3% error reduction over naive PCFG induction
in the presence of gold bracketing information.
We note that the our labeled F1 is upper-bounded
by 86.0 due to unary chains and more-than-binary
configurations in the treebank that cannot be ob-
tained from our binary grammar.
We conclude that in the presence of gold bracket
information, we can achieve high labeled accu-
racy by using a CFG augmented with distribu-
tional prototype features.
4 Constituent Context Model
So far, we have shown that, given perfect per-
fect bracketing information, distributional proto-
type features allow us to learn tree structures with
fairly accurate labels. However, such bracketing
information is not available in the unsupervised
case.
Perhaps we don?t actually need bracketing con-
straints in the presence of prototypes and distri-
butional similarity features. However this exper-
iment, labeled PROTO ? NONE in figure 4, gave
only 53.1 labeled F1 (61.1 unlabeled), suggesting
that some amount of bracketing constraint is nec-
essary to achieve high performance.
Fortunately, there are unsupervised systems
which can induce unlabeled bracketings with rea-
sonably high accuracy. One such model is
the constituent-context model (CCM) of Klein
and Manning (2002), a generative distributional
model. For a given sentence S, the CCM generates
a bracket matrix, B, which for each span (i, j), in-
dicates whether or not it is a constituent (Bij = c)
or a distituent (Bij = d). In addition, it generates
a feature map F ?, which for each span (i, j) in S
specifies a pair of features, F ?ij = (yij , cij), where
yij is the POS yield of the span, and cij is the con-
text of the span, i.e identity of the conjoined left
and right POS tags:
PCCM (B,F
?) = P (B)
?
(i,j)
P (yij |Bij)P (cij |Bij)
The distribution P (B) only places mass on brack-
etings which correspond to binary trees. We
can efficiently compute PCCM (B,F ?) (up to
a constant) depending on F ? using local fac-
tors ?CCM (yij , cij) which decomposes over con-
stituent spans:7
PCCM (B,F
?) ?
?
(i,j):Bij=c
P (yij |c)P (cij |c)
P (yij |d)P (cij |d)
=
?
(i,j):Bij=c
?CCM (yij , cij)
The CCM by itself yields an unlabeled F1 of 71.9
on WSJ-10, which is reasonably high, but does not
produce labeled trees.
5 Intersecting CCM and PCFG
The CCM and PCFG models provide complemen-
tary views of syntactic structure. The CCM explic-
itly learns the non-recursive contextual and yield
properties of constituents and distituents. The
PCFG model, on the other hand, does not explic-
itly model properties of distituents but instead fo-
cuses on modeling the hierarchical and recursive
properties of natural language syntax. One would
hope that modeling both of these aspects simulta-
neously would improve the overall quality of our
induced grammar.
We therefore combine the CCM with our feature-
augmented PCFG, denoted by PROTO in exper-
iment names. When we run EM on either of
the models alone, at each iteration and for each
training example, we calculate posteriors over that
7Klein (2005) gives a full presentation.
885
model?s latent variables. For CCM, the latent vari-
able is a bracketing matrix B (equivalent to an un-
labeled binary tree), while for the CFG+ the latent
variable is a labeled tree T . While these latent
variables aren?t exactly the same, there is a close
relationship between them. A bracketing matrix
constrains possible labeled trees, and a given la-
beled tree determines a bracketing matrix. One
way to combine these models is to encourage both
models to prefer latent variables which are com-
patible with each other.
Similar to the approach of Klein and Manning
(2004) on a different model pair, we intersect CCM
and CFG+ by multiplying their scores for any la-
beled tree. For each possible labeled tree over a
sentence S, our generative model for a labeled tree
T is given as follows:
P (T, F, F ?) = (4)
PCFG+(T, F )PCCM (B(T ), F
?)
where B(T ) corresponds to the bracketing ma-
trix determined by T . The EM algorithm for the
product model will maximize:
P (S,F, F ?) =
?
T?T (S)
PCCM (B,F
?)PCFG+(T, F )
=
?
B
PCCM (B,F
?)
?
T?T (B,S)
PCFG+(T, F )
where T (S) is the set of labeled trees consistent
with the sentence S and T (B,S) is the set of la-
beled trees consistent with the bracketing matrix
B and the sentence S. Notice that this quantity in-
creases as the CCM and CFG+ models place proba-
bility mass on compatible latent structures, giving
an intuitive justification for the success of this ap-
proach.
We can compute posterior expectations over
(B, T ) in the combined model (4) using a variant
of the inside-outside algorithm. The local factor
for a binary rule r = X ? Y Z, over span (i, j),
with CCM features F ?ij = (yij , cij) and prototype
feature pij , is given by the product of local factors
for the CCM and CFG+ models:
?(r, (i, j)) = ?CCM (yij , cij)?CFG+(r, pij)
From these local factors, the inside-outside al-
gorithm produces expected counts for each binary
rule, r, over each span (i, j) and split point k, de-
noted by P (r, (i, j), k|S, F, F ?). These posteriors
are sufficient to re-estimate all of our model pa-
rameters.
Labeled Unlabeled
Setting Prec. Rec. F1 Prec. Rec. F1
No Brackets
PCFG ? NONE 23.9 29.1 26.3 40.7 52.1 45.7
PROTO ? NONE 51.8 62.9 56.8 59.6 76.2 66.9
Gold Brackets
PCFG ? GOLD 47.0 57.2 51.6 78.8 100.0 88.1
PROTO ? GOLD 64.8 78.7 71.1 78.8 100.0 88.1
CCM Brackets
CCM - - - 64.2 81.6 71.9
PCFG ? CCM 32.3 38.9 35.3 64.1 81.4 71.8
PROTO ? CCM 56.9 68.5 62.2 68.4 86.9 76.5
BEST 59.4 72.1 65.1 69.7 89.1 78.2
UBOUND 78.8 94.7 86.0 78.8 100.0 88.1
Figure 4: English grammar induction results. The
upper bound on labeled recall is due to unary
chains.
6 CCM as a Bracketer
We tested the product model described in sec-
tion 5 on WSJ-10 under the same conditions as
in section 3. Our initial experiment utilizes no
protoype information, random initialization, and
greedy remapping of its labels. This experiment,
PCFG ? CCM in figure 4, gave 35.3 labeled F1,
compared to the 51.6 labeled F1 with gold brack-
eting information (PCFG ? GOLD in figure 4).
Next we added the manually specified proto-
types in figure 1, and constrained the model to give
these yields their labels if chosen as constituents.
This experiment gave 48.9 labeled F1 (73.3 unla-
beled). The error reduction is 21.0% labeled (5.3%
unlabeled) over PCFG ? CCM.
We then experimented with adding distributional
prototype features as discussed in section 3.2 us-
ing a threshold of 0.75 and ? = 0.1. This experi-
ment, PROTO ? CCM in figure 4, gave 62.2 labeled
F1 (76.5 unlabeled). The error reduction is 26.0%
labeled (12.0% unlabeled) over the experiment us-
ing prototypes without the similarity features. The
overall error reduction from PCFG? CCM is 41.6%
(16.7%) in labeled (unlabeled) F1.
7 Error Analysis
The most common type of error by our PROTO ?
CCM system was due to the binary grammar re-
striction. For instance common NPs, such as DT JJ
NN, analyzed as [NP DT [NP JJ NN] ], which pro-
poses additional N constituents compared to the
flatter treebank analysis. This discrepancy greatly,
and perhaps unfairly, damages NP precision (see
figure 6). However, this is error is unavoidable
886
SXXXXXNP
NNP
France
VPXXXXXMD
can
VPhhhhhhh(((((((VB
boast
NPXXXXXNPaaa!!!NPaaa!!!DT
the
NN
lion
POS
?s
NN
share
PPPPPPIN
of
NPHHHJJ
high-priced
NNS
bottles
Shhhhhhhhh(((((((((NNP
France
VPhhhhhhhh((((((((VPXXXXXVPZZProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848?855,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Aria Haghighi and Dan Klein
Computer Science Division
UC Berkeley
{aria42, klein}@cs.berkeley.edu
Abstract
We present an unsupervised, nonparamet-
ric Bayesian approach to coreference reso-
lution which models both global entity iden-
tity across a corpus as well as the sequen-
tial anaphoric structure within each docu-
ment. While most existing coreference work
is driven by pairwise decisions, our model
is fully generative, producing each mention
from a combination of global entity proper-
ties and local attentional state. Despite be-
ing unsupervised, our system achieves a 70.3
MUC F1 measure on the MUC-6 test set,
broadly in the range of some recent super-
vised results.
1 Introduction
Referring to an entity in natural language can
broadly be decomposed into two processes. First,
speakers directly introduce new entities into dis-
course, entities which may be shared across dis-
courses. This initial reference is typically accom-
plished with proper or nominal expressions. Second,
speakers refer back to entities already introduced.
This anaphoric reference is canonically, though of
course not always, accomplished with pronouns, and
is governed by linguistic and cognitive constraints.
In this paper, we present a nonparametric generative
model of a document corpus which naturally con-
nects these two processes.
Most recent coreference resolution work has fo-
cused on the task of deciding which mentions (noun
phrases) in a document are coreferent. The domi-
nant approach is to decompose the task into a col-
lection of pairwise coreference decisions. One then
applies discriminative learning methods to pairs of
mentions, using features which encode properties
such as distance, syntactic environment, and so on
(Soon et al, 2001; Ng and Cardie, 2002). Although
such approaches have been successful, they have
several liabilities. First, rich features require plen-
tiful labeled data, which we do not have for corefer-
ence tasks in most domains and languages. Second,
coreference is inherently a clustering or partitioning
task. Naive pairwise methods can and do fail to pro-
duce coherent partitions. One classic solution is to
make greedy left-to-right linkage decisions. Recent
work has addressed this issue in more global ways.
McCallum and Wellner (2004) use graph partion-
ing in order to reconcile pairwise scores into a final
coherent clustering. Nonetheless, all these systems
crucially rely on pairwise models because cluster-
level models are much harder to work with, combi-
natorially, in discriminative approaches.
Another thread of coreference work has focused
on the problem of identifying matches between
documents (Milch et al, 2005; Bhattacharya and
Getoor, 2006; Daume and Marcu, 2005). These
methods ignore the sequential anaphoric structure
inside documents, but construct models of how and
when entities are shared between them.1 These
models, as ours, are generative ones, since the fo-
cus is on cluster discovery and the data is generally
unlabeled.
In this paper, we present a novel, fully genera-
tive, nonparametric Bayesian model of mentions in a
document corpus. Our model captures both within-
and cross-document coreference. At the top, a hi-
erarchical Dirichlet process (Teh et al, 2006) cap-
1Milch et al (2005) works with citations rather than dis-
courses and does model the linear structure of the citations.
848
tures cross-document entity (and parameter) shar-
ing, while, at the bottom, a sequential model of
salience captures within-document sequential struc-
ture. As a joint model of several kinds of discourse
variables, it can be used to make predictions about
either kind of coreference, though we focus experi-
mentally on within-document measures. To the best
of our ability to compare, our model achieves the
best unsupervised coreference performance.
2 Experimental Setup
We adopt the terminology of the Automatic Context
Extraction (ACE) task (NIST, 2004). For this paper,
we assume that each document in a corpus consists
of a set of mentions, typically noun phrases. Each
mention is a reference to some entity in the domain
of discourse. The coreference resolution task is to
partition the mentions according to referent. Men-
tions can be divided into three categories, proper
mentions (names), nominal mentions (descriptions),
and pronominal mentions (pronouns).
In section 3, we present a sequence of increas-
ingly enriched models, motivating each from short-
comings of the previous. As we go, we will indicate
the performance of each model on data from ACE
2004 (NIST, 2004). In particular, we used as our
development corpus the English translations of the
Arabic and Chinese treebanks, comprising 95 docu-
ments and about 3,905 mentions. This data was used
heavily for model design and hyperparameter selec-
tion. In section 5, we present final results for new
test data from MUC-6 on which no tuning or devel-
opment was performed. This test data will form our
basis for comparison to previous work.
In all experiments, as is common, we will assume
that we have been given as part of our input the true
mention boundaries, the head word of each mention
and the mention type (proper, nominal, or pronom-
inal). For the ACE data sets, the head and mention
type are given as part of the mention annotation. For
the MUC data, the head was crudely chosen to be
the rightmost mention token, and the mention type
was automatically detected. We will not assume
any other information to be present in the data be-
yond the text itself. In particular, unlike much re-
lated work, we do not assume gold named entity
recognition (NER) labels; indeed we do not assume
observed NER labels or POS tags at all. Our pri-
?
?
K
?
K
Zi
H i
J
I
?
?
?
?
?
Zi
H i
I
J
(a) (b)
Figure 1: Graphical model depiction of document level en-
tity models described in sections 3.1 and 3.2 respectively. The
shaded nodes indicate observed variables.
mary performance metric will be the MUC F1 mea-
sure (Vilain et al, 1995), commonly used to evalu-
ate coreference systems on a within-document basis.
Since our system relies on sampling, all results are
averaged over five random runs.
3 Coreference Resolution Models
In this section, we present a sequence of gener-
ative coreference resolution models for document
corpora. All are essentially mixture models, where
the mixture components correspond to entities. As
far as notation, we assume a collection of I docu-
ments, each with Ji mentions. We use random vari-
ables Z to refer to (indices of) entities. We will use
?z to denote the parameters for an entity z, and ?
to refer to the concatenation of all such ?z . X will
refer somewhat loosely to the collection of variables
associated with a mention in our model (such as the
head or gender). We will be explicit about X and ?z
shortly.
Our goal will be to find the setting of the entity
indices which maximize the posterior probability:
Z? = argmax
Z
P (Z|X) = argmax
Z
P (Z,X)
= argmax
Z
?
P (Z,X,?) dP (?)
where Z,X, and ? denote all the entity indices, ob-
served values, and parameters of the model. Note
that we take a Bayesian approach in which all pa-
rameters are integrated out (or sampled). The infer-
ence task is thus primarily a search problem over the
index labels Z.
849
(a)
(b)
(c)
The Weir Group
1
, whose
2
  headquarters
3
 is in the US
4
, is a large, specialized corporation
5
 investing in the area of electricity 
generation. This  power plant
6
, which
7
  will be situated in Rudong
8
, Jiangsu
9
, has an annual generation capacity of 2.4 million kilowatts.  
The Weir Group
1
, whose
1
  headquarters
2
 is in the US
3
, is a large, specialized corporation
4
 investing in the area of electricity 
generation. This  power plant
5
, which
1
  will be situated in Rudong
6
, Jiangsu
7
, has an annual generation capacity of 2.4 million kilowatts.  
The Weir Group
1
, whose
1
  headquarters
2
 is in the US
3
, is a large, specialized corporation
4
 investing in the area of electricity 
generation. This  power plant
5
, which
5
  will be situated in Rudong
6
, Jiangsu
7
, has an annual generation capacity of 2.4 million kilowatts.  
Figure 2: Example output from various models. The output from (a) is from the infinite mixture model of section 3.2. It incorrectly
labels both boxed cases of anaphora. The output from (b) uses the pronoun head model of section 3.3. It correctly labels the first
case of anaphora but incorrectly labels the second pronominal as being coreferent with the dominant document entity The Weir
Group. This error is fixed by adding the salience feature component from section 3.4 as can be seen in (c).
3.1 A Finite Mixture Model
Our first, overly simplistic, corpus model is the stan-
dard finite mixture of multinomials shown in fig-
ure 1(a). In this model, each document is indepen-
dent save for some global hyperparameters. Inside
each document, there is a finite mixture model with
a fixed numberK of components. The distribution ?
over components (entities) is a draw from a symmet-
ric Dirichlet distribution with concentration ?. For
each mention in the document, we choose a compo-
nent (an entity index) z from ?. Entity z is then asso-
ciated with a multinomial emission distribution over
head words with parameters ?hZ , which are drawn
from a symmetric Dirichlet over possible mention
heads with concentration ?H .2 Note that here the X
for a mention consists only of the mention head H .
As we enrich our models, we simultaneously de-
velop an accompanying Gibbs sampling procedure
to obtain samples from P (Z|X).3 For now, all heads
H are observed and all parameters (? and ?) can be
integrated out analytically: for details see Teh et al
(2006). The only sampling is for the values of Zi,j ,
the entity index of mention j in document i. The
relevant conditional distribution is:4
P (Zi,j |Z?i,j ,H) ? P (Zi,j |Z?i,j)P (Hi,j |Z,H?i,j)
where Hi,j is the head of mention j in document i.
Expanding each term, we have the contribution of
the prior:
P (Zi,j = z|Z?i,j) ? nz + ?
2In general, we will use a subscripted ? to indicate concen-
tration for finite Dirichlet distributions. Unless otherwise spec-
ified, ? concentration parameters will be set to e?4 and omitted
from diagrams.
3One could use the EM algorithm with this model, but EM
will not extend effectively to the subsequent models.
4Here, Z?i,j denotes Z? {Zi,j}
where nz is the number of elements of Z?i,j with
entity index z. Similarly we have for the contribu-
tion of the emissions:
P (Hi,j = h|Z,H?i,j) ? nh,z + ?H
where nh,z is the number of times we have seen head
h associated with entity index z in (Z,H?i,j).
3.2 An Infinite Mixture Model
A clear drawback of the finite mixture model is the
requirement that we specify a priori a number of en-
tities K for a document. We would like our model
to select K in an effective, principled way. A mech-
anism for doing so is to replace the finite Dirichlet
prior on ? with the non-parametric Dirichlet process
(DP) prior (Ferguson, 1973).5 Doing so gives the
model in figure 1(b). Note that we now list an in-
finite number of mixture components in this model
since there can be an unbounded number of entities.
Rather than a finite ? with a symmetric Dirichlet
distribution, in which draws tend to have balanced
clusters, we now have an infinite ?. However, most
draws will have weights which decay exponentially
quickly in the prior (though not necessarily in the
posterior). Therefore, there is a natural penalty for
each cluster which is actually used.
With Z observed during sampling, we can inte-
grate out ? and calculate P (Zi,j |Z?i,j) analytically,
using the Chinese restaurant process representation:
P (Zi,j = z|Z?i,j) ?
{
?, if z = znew
nz, otherwise
(1)
where znew is a new entity index not used in Z?i,j
and nz is the number of mentions that have entity in-
dex z. Aside from this change, sampling is identical
5We do not give a detailed presentation of the Dirichlet pro-
cess here, but see Teh et al (2006) for a presentation.
850
PERS : 0.97,   LOC : 0.01,  ORG: 0.01,  MISC: 0.01 
Entity Type
SING: 0.99, PLURAL: 0.01
Number
     MALE: 0.98, FEM: 0.01, NEUTER: 0.01
Gender
Bush : 0.90,   President : 0.06,  .....
Head
?t
?h
?n
?g
X =
Z Z
M T N G
H
? ??
(a) (b)
Figure 3: (a) An entity and its parameters. (b)The head model
described in section 3.3. The shaded nodes indicate observed
variables. The mention type determines which set of parents are
used. The dependence of mention variable on entity parameters
? and pronoun head model ? is omitted.
to the finite mixture case, though with the number
of clusters actually occupied in each sample drifting
upwards or downwards.
This model yielded a 54.5 F1 on our develop-
ment data.6 This model is, however, hopelessly
crude, capturing nothing of the structure of coref-
erence. Its largest empirical problem is that, un-
surprisingly, pronoun mentions such as he are given
their own clusters, not labeled as coreferent with any
non-pronominal mention (see figure 2(a)).
3.3 Pronoun Head Model
While an entity-specific multinomial distribution
over heads makes sense for proper, and some nom-
inal, mention heads, it does not make sense to gen-
erate pronominal mentions this same way. I.e., all
entities can be referred to by generic pronouns, the
choice of which depends on entity properties such as
gender, not the specific entity.
We therefore enrich an entity?s parameters ? to
contain not only a distribution over lexical heads
?h, but also distributions (?t, ?g, ?n) over proper-
ties, where ?t parametrizes a distribution over en-
tity types (PER, LOC, ORG, MISC), and ?g for gen-
der (MALE, FEMALE, NEUTER), and ?n for number
(SG, PL).7 We assume each of these property distri-
butions is drawn from a symmetric Dirichlet distri-
bution with small concentration parameter in order
to encourage a peaked posterior distribution.
6See section 4 for inference details.
7It might seem that entities should simply have, for exam-
ple, a gender g rather than a distribution over genders ?g . There
are two reasons to adopt the softer approach. First, one can
rationalize it in principle, for entities like cars or ships whose
grammatical gender is not deterministic. However, the real rea-
son is that inference is simplified. In any event, we found these
property distributions to be highly determinized in the posterior.
?
?
?
?
?
Z1 Z 3
L 1
S 1
T
1
 
N
1
G
1
M
1
 =
NAM
Z 2
L 2
S 2
N
2
G
2
M
2
 =
NOM
T
2
H
2 =
"president"
H
1 =
"Bush"
H
3 =
"he"
N
2 =
SG
G
2 =
MALE
M
3
 =
PRO
T
2
L 3
S 3
?
I
Figure 4: Coreference model at the document level with entity
properties as well salience lists used for mention type distri-
butions. The diamond nodes indicate deterministic functions.
Shaded nodes indicate observed variables. Although it appears
that each mention head node has many parents, for a given men-
tion type, the mention head depends on only a small subset. De-
pendencies involving parameters ? and ? are omitted.
Previously, when an entity z generated a mention,
it drew a head word from ?hz . It now undergoes a
more complex and structured process. It first draws
an entity type T , a gender G, a number N from the
distributions ?t, ?g, and ?n, respectively. Once the
properties are fetched, a mention type M is chosen
(proper, nominal, pronoun), according to a global
multinomial (again with a symmetric Dirichlet prior
and parameter ?M ). This corresponds to the (tem-
porary) assumption that the speaker makes a random
i.i.d. choice for the type of each mention.
Our head model will then generate a head, con-
ditioning on the entity, its properties, and the men-
tion type, as shown in figure 3(b). If M is not a
pronoun, the head is drawn directly from the en-
tity head multinomial with parameters ?hz . Other-
wise, it is drawn based on a global pronoun head dis-
tribution, conditioning on the entity properties and
parametrized by ?. Formally, it is given by:
P (H|Z, T,G,N,M,?,?) =
{
P (H|T,G,N,?), if M =PRO
P (H|?hZ), otherwise
Although we can observe the number and gen-
der draws for some mentions, like personal pro-
nouns, there are some for which properties aren?t
observed (e.g., it). Because the entity prop-
erty draws are not (all) observed, we must now
sample the unobserved ones as well as the en-
tity indices Z. For instance, we could sample
851
Salience Feature Pronoun Proper Nominal
TOP 0.75 0.17 0.08
HIGH 0.55 0.28 0.17
MID 0.39 0.40 0.21
LOW 0.20 0.45 0.35
NONE 0.00 0.88 0.12
Table 1: Posterior distribution of mention type given salience
by bucketing entity activation rank. Pronouns are preferred for
entities which have high salience and non-pronominal mentions
are preferred for inactive entities.
Ti,j , the entity type of pronominal mention j in
document i, using, P (Ti,j |Z,N,G,H,T?i,j) ?
P (Ti,j |Z)P (Hi,j |T,N,G,H), where the posterior
distributions on the right hand side are straight-
forward because the parameter priors are all finite
Dirichlet. Sampling G and N are identical.
Of course we have prior knowledge about the re-
lationship between entity type and pronoun head
choice. For example, we expect that he is used for
mentions with T = PERSON. In general, we assume
that for each pronominal head we have a list of com-
patible entity types, which we encode via the prior
on ?. We assume ? is drawn from a Dirichlet distri-
bution where each pronoun head is given a synthetic
count of (1 + ?P ) for each (t, g, n) where t is com-
patible with the pronoun and given ?P otherwise.
So, while it will be possible in the posterior to use
he to refer to a non-person, it will be biased towards
being used with persons.
This model gives substantially improved predic-
tions: 64.1 F1 on our development data. As can be
seen in figure 2(b), this model does correct the sys-
tematic problem of pronouns being considered their
own entities. However, it still does not have a pref-
erence for associating pronominal references to en-
tities which are in any way local.
3.4 Adding Salience
We would like our model to capture how mention
types are generated for a given entity in a robust and
somewhat language independent way. The choice of
entities may reasonably be considered to be indepen-
dent given the mixing weights ?, but how we realize
an entity is strongly dependent on context (Ge et al,
1998).
In order to capture this in our model, we enrich
it as shown in figure 4. As we proceed through a
document, generating entities and their mentions,
we maintain a list of the active entities and their
saliences, or activity scores. Every time an entity is
mentioned, we increment its activity score by 1, and
every time we move to generate the next mention,
all activity scores decay by a constant factor of 0.5.
This gives rise to an ordered list of entity activations,
L, where the rank of an entity decays exponentially
as new mentions are generated. We call this list a
salience list. Given a salience list, L, each possible
entity z has some rank on this list. We discretize
these ranks into five buckets S: TOP (1), HIGH (2-
3), MID (4-6), LOW (7+), and NONE. Given the entity
choices Z, both the list L and buckets S are deter-
ministic (see figure 4). We assume that the mention
type M is conditioned on S as shown in figure 4.
We note that correctly sampling an entity now re-
quires that we incorporate terms for how a change
will affect all future salience values. This changes
our sampling equation for existing entities:
P (Zi,j = z|Z?i,j) ? nz
?
j??j
P (Mi,j? |Si,j? ,Z) (2)
where the product ranges over future mentions in the
document and Si,j? is the value of future salience
feature given the setting of all entities, including set-
ting the current entity Zi,j to z. A similar equation
holds for sampling a new entity. Note that, as dis-
cussed below, this full product can be truncated as
an approximation.
This model gives a 71.5 F1 on our development
data. Table 1 shows the posterior distribution of the
mention type given the salience feature. This model
fixes many anaphora errors and in particular fixes the
second anaphora error in figure 2(c).
3.5 Cross Document Coreference
One advantage of a fully generative approach is that
we can allow entities to be shared between docu-
ments in a principled way, giving us the capacity to
do cross-document coreference. Moreover, sharing
across documents pools information about the prop-
erties of an entity across documents.
We can easily link entities across a corpus by as-
suming that the pool of entities is global, with global
mixing weights ?0 drawn from a DP prior with
concentration parameter ?. Each document uses
852
??
?
?
?
Z1 Z 3
L 1
S 1
T
1 
N
1
G
1
M
1
 =
NAM
Z 2
L 2
S 2
N
2
G
2
M
2
 =
NOM
T
2
H
2 =
"president"
H
1 =
"Bush"
H
3 =
"he"
N
2 =
SG
G
2 =
MALE
M
3
 =
PRO
T
2
L 3
S 3
?0
?
?
?
I
Figure 5: Graphical depiction of the HDP coreference model
described in section 3.5. The dependencies between the global
entity parameters ? and pronoun head parameters ? on the men-
tion observations are not depicted.
the same global entities, but each has a document-
specific distribution ?i drawn from a DP centered on
?0 with concentration parameter ?. Up to the point
where entities are chosen, this formulation follows
the basic hierarchical Dirichlet process prior of Teh
et al (2006). Once the entities are chosen, our model
for the realization of the mentions is as before. This
model is depicted graphically in figure 5.
Although it is possible to integrate out ?0 as we
did the individual ?i, we instead choose for ef-
ficiency and simplicity to sample the global mix-
ture distribution ?0 from the posterior distribution
P (?0|Z).8 The mention generation terms in the
model and sampler are unchanged.
In the full hierarchical model, our equation (1) for
sampling entities, ignoring the salience component
of section 3.4, becomes:
P (Zi,j = z|Z?i,j , ?0)?
{
??u0 , if z = znew
nz + ??z0 , otherwise
where ?z0 is the probability of the entity z under the
sampled global entity distribution and ?u0 is the un-
known component mass of this distribution.
The HDP layer of sharing improves the model?s
predictions to 72.5 F1 on our development data. We
should emphasize that our evaluation is of course
per-document and does not reflect cross-document
coreference decisions, only the gains through cross-
document sharing (see section 6.2).
8We do not give the details here; see Teh et al (2006) for de-
tails on how to implement this component of the sampler (called
?direct assignment? in that reference).
4 Inference Details
Up until now, we?ve discussed Gibbs sampling, but
we are not interested in sampling from the poste-
rior P (Z|X), but in finding its mode. Instead of
sampling directly from the posterior distribution, we
instead sample entities proportionally to exponen-
tiated entity posteriors. The exponent is given by
exp cik?1 , where i is the current round number (start-
ing at i = 0), c = 1.5 and k = 20 is the total num-
ber of sampling epochs. This slowly raises the pos-
terior exponent from 1.0 to ec. In our experiments,
we found this procedure to outperform simulated an-
nealing. We also found sampling the T , G, and N
variables to be particularly inefficient, so instead we
maintain soft counts over each of these variables and
use these in place of a hard sampling scheme. We
also found that correctly accounting for the future
impact of salience changes to be particularly ineffi-
cient. However, ignoring those terms entirely made
negligible difference in final accuracy.9
5 Final Experiments
We present our final experiments using the full
model developed in section 3. As in section 3, we
use true mention boundaries and evaluate using the
MUC F1 measure (Vilain et al, 1995). All hyper-
parameters were tuned on the development set only.
The document concentration parameter ? was set by
taking a constant proportion of the average number
of mentions in a document across the corpus. This
number was chosen to minimize the squared error
between the number of proposed entities and true
entities in a document. It was not tuned to maximize
the F1 measure. A coefficient of 0.4 was chosen.
The global concentration coefficient ? was chosen
to be a constant proportion of ?M , where M is the
number of documents in the corpus. We found 0.15
to be a good value using the same least-square pro-
cedure. The values for these coefficients were not
changed for the experiments on the test sets.
5.1 MUC-6
Our main evaluation is on the standard MUC-6 for-
mal test set.10 The standard experimental setup for
9This corresponds to truncating equation (2) at j? = j.
10Since the MUC data is not annotated with mention types,
we automatically detect this information in the same way as Luo
853
Dataset Num Docs. Prec. Recall F1
MUC-6 60 80.8 52.8 63.9
+DRYRUN-TRAIN 251 79.1 59.7 68.0
+ENGLISH-NWIRE 381 80.4 62.4 70.3
Dataset Prec. Recall F1
ENGLISH-NWIRE 66.7 62.3 64.2
ENGLISH-BNEWS 63.2 61.3 62.3
CHINESE-NWIRE 71.6 63.3 67.2
CHINESE-BNEWS 71.2 61.8 66.2
(a) (b)
Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al (1995). The table in (a) is
our performance on the thirty document MUC-6 formal test set with increasing amounts of training data. In all cases for the table,
we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised. The
table in (b) is our performance on the ACE 2004 training sets.
this data is a 30/30 document train/test split. Train-
ing our system on all 60 documents of the training
and test set (as this is in an unsupervised system,
the unlabeled test documents are present at train-
ing time), but evaluating only on the test documents,
gave 63.9 F1 and is labeled MUC-6 in table 2(a).
One advantage of an unsupervised approach is
that we can easily utilize more data when learning a
model. We demonstrate the effectiveness of this fact
by evaluating on the MUC-6 test documents with in-
creasing amounts of unannotated training data. We
first added the 191 documents from the MUC-6
dryrun training set (which were not part of the train-
ing data for official MUC-6 evaluation). This model
gave 68.0 F1 and is labeled +DRYRUN-TRAIN in ta-
ble 2(a). We then added the ACE ENGLISH-NWIRE
training data, which is from a different corpora than
the MUC-6 test set and from a different time period.
This model gave 70.3 F1 and is labeled +ENGLISH-
NWIRE in table 2(a).
Our results on this test set are surprisingly com-
parable to, though slightly lower than, some recent
supervised systems. McCallum and Wellner (2004)
report 73.4 F1 on the formal MUC-6 test set, which
is reasonably close to our best MUC-6 number of
70.3 F1. McCallum and Wellner (2004) also report
a much lower 91.6 F1 on only proper nouns men-
tions. Our system achieves a 89.8 F1 when evalu-
ation is restricted to only proper mentions.11 The
et al (2004). A mention is proper if it is annotated with NER
information. It is a pronoun if the head is on the list of En-
glish pronouns. Otherwise, it is a nominal mention. Note we do
not use the NER information for any purpose but determining
whether the mention is proper.
11The best results we know on the MUC-6 test set using the
standard setting are due to Luo et al (2004) who report a 81.3
F1 (much higher than others). However, it is not clear this is a
comparable number, due to the apparent use of gold NER fea-
tures, which provide a strong clue to coreference. Regardless, it
is unsurprising that their system, which has many rich features,
would outperform ours.
HEAD ENT TYPE GENDER NUMBER
Bush: 1.0 PERS MALE SG
AP: 1.0 ORG NEUTER PL
viacom: 0.64, company: 0.36 ORG NEUTER SG
teamsters: 0.22, union: 0.78, MISC NEUTER PL
Table 3: Frequent entities occurring across documents along
with head distribution and mode of property distributions.
closest comparable unsupervised system is Cardie
and Wagstaff (1999) who use pairwise NP distances
to cluster document mentions. They report a 53.6 F1
on MUC6 when tuning distance metric weights to
maximize F1 on the development set.
5.2 ACE 2004
We also performed experiments on ACE 2004 data.
Due to licensing restrictions, we did not have access
to the ACE 2004 formal development and test sets,
and so the results presented are on the training sets.
We report results on the newswire section (NWIRE
in table 2b) and the broadcast news section (BNEWS
in table 2b). These datasets include the prenomi-
nal mention type, which is not present in the MUC-
6 data. We treated prenominals analogously to the
treatment of proper and nominal mentions.
We also tested our system on the Chinese
newswire and broadcast news sections of the ACE
2004 training sets. Our relatively higher perfor-
mance on Chinese compared to English is perhaps
due to the lack of prenominal mentions in the Chi-
nese data, as well as the presence of fewer pronouns
compared to English.
Our ACE results are difficult to compare exactly
to previous work because we did not have access
to the restricted formal test set. However, we can
perform a rough comparison between our results on
the training data (without coreference annotation) to
supervised work which has used the same training
data (with coreference annotation) and evaluated on
the formal test set. Denis and Baldridge (2007) re-
854
port 67.1 F1 and 69.2 F1 on the English NWIRE and
BNEWS respectively using true mention boundaries.
While our system underperforms the supervised sys-
tems, its accuracy is nonetheless promising.
6 Discussion
6.1 Error Analysis
The largest source of error in our system is between
coreferent proper and nominal mentions. The most
common examples of this kind of error are appos-
itive usages e.g. George W. Bush, president of the
US, visited Idaho. Another error of this sort can be
seen in figure 2, where the corporation mention is
not labeled coreferent with the The Weir Groupmen-
tion. Examples such as these illustrate the regular (at
least in newswire) phenomenon that nominal men-
tions are used with informative intent, even when the
entity is salient and a pronoun could have been used
unambiguously. This aspect of nominal mentions is
entirely unmodeled in our system.
6.2 Global Coreference
Since we do not have labeled cross-document coref-
erence data, we cannot evaluate our system?s cross-
document performance quantitatively. However, in
addition to observing the within-document gains
from sharing shown in section 3, we can manually
inspect the most frequently occurring entities in our
corpora. Table 3 shows some of the most frequently
occurring entities across the English ACE NWIRE
corpus. Note that Bush is the most frequent entity,
though his (and others?) nominal cluster president
is mistakenly its own entity. Merging of proper and
nominal clusters does occur as can be seen in table 3.
6.3 Unsupervised NER
We can use our model to for unsupervised NER
tagging: for each proper mention, assign the mode
of the generating entity?s distribution over entity
types. Note that in our model the only way an en-
tity becomes associated with an entity type is by
the pronouns used to refer to it.12 If we evaluate
our system as an unsupervised NER tagger for the
proper mentions in the MUC-6 test set, it yields a
12Ge et al (1998) exploit a similar idea to assign gender to
proper mentions.
per-label accuracy of 61.2% (on MUC labels). Al-
though nowhere near the performance of state-of-
the-art systems, this result beats a simple baseline of
always guessing PERSON (the most common entity
type), which yields 46.4%. This result is interest-
ing given that the model was not developed for the
purpose of inferring entity types whatsoever.
7 Conclusion
We have presented a novel, unsupervised approach
to coreference resolution: global entities are shared
across documents, the number of entities is deter-
mined by the model, and mentions are generated by
a sequential salience model and a model of pronoun-
entity association. Although our system does not
perform quite as well as state-of-the-art supervised
systems, its performance is in the same general
range, despite the system being unsupervised.
References
I. Bhattacharya and L. Getoor. 2006. A latent dirichlet model
for unsupervised entity resolution. SIAM conference on data
mining.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. EMNLP.
Hal Daume and Daniel Marcu. 2005. A Bayesian model for su-
pervised clustering with the Dirichlet process prior. JMLR.
Pascal Denis and Jason Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution using inte-
ger programming. HLT-NAACL.
Thomas Ferguson. 1973. A bayesian analysis of some non-
parametric problems. Annals of Statistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. Sixth Workshop on Very
Large Corpora.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the bell tree. ACL.
Andrew McCallum and Ben Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. NIPS.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. Blog: Proba-
bilistic models with unknown objects. IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. ACL.
NIST. 2004. The ACE evaluation plan.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics.
Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei.
2006. Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly,
and Lynette Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. MUC-6.
855
Proceedings of ACL-08: HLT, pages 771?779,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Bilingual Lexicons from Monolingual Corpora
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,pliang,tberg,klein }@cs.berkeley.edu
Abstract
We present a method for learning bilingual
translation lexicons from monolingual cor-
pora. Word types in each language are charac-
terized by purely monolingual features, such
as context counts and orthographic substrings.
Translations are induced using a generative
model based on canonical correlation analy-
sis, which explains the monolingual lexicons
in terms of latent matchings. We show that
high-precision lexicons can be learned in a va-
riety of language pairs and from a range of
corpus types.
1 Introduction
Current statistical machine translation systems use
parallel corpora to induce translation correspon-
dences, whether those correspondences be at the
level of phrases (Koehn, 2004), treelets (Galley et
al., 2006), or simply single words (Brown et al,
1994). Although parallel text is plentiful for some
language pairs such as English-Chinese or English-
Arabic, it is scarce or even non-existent for most
others, such as English-Hindi or French-Japanese.
Moreover, parallel text could be scarce for a lan-
guage pair even if monolingual data is readily avail-
able for both languages.
In this paper, we consider the problem of learning
translations from monolingual sources alone. This
task, though clearly more difficult than the standard
parallel text approach, can operate on language pairs
and in domains where standard approaches cannot.
We take as input two monolingual corpora and per-
haps some seed translations, and we produce as out-
put a bilingual lexicon, defined as a list of word
pairs deemed to be word-level translations. Preci-
sion and recall are then measured over these bilin-
gual lexicons. This setting has been considered be-
fore, most notably in Koehn and Knight (2002) and
Fung (1995), but the current paper is the first to use
a probabilistic model and present results across a va-
riety of language pairs and data conditions.
In our method, we represent each language as a
monolingual lexicon (see figure 2): a list of word
types characterized by monolingual feature vectors,
such as context counts, orthographic substrings, and
so on (section 5). We define a generative model over
(1) a source lexicon, (2) a target lexicon, and (3) a
matching between them (section 2). Our model is
based on canonical correlation analysis (CCA)1 and
explains matched word pairs via vectors in a com-
mon latent space. Inference in the model is done
using an EM-style algorithm (section 3).
Somewhat surprisingly, we show that it is pos-
sible to learn or extend a translation lexicon us-
ing monolingual corpora alone, in a variety of lan-
guages and using a variety of corpora, even in the
absence of orthographic features. As might be ex-
pected, the task is harder when no seed lexicon is
provided, when the languages are strongly diver-
gent, or when the monolingual corpora are from dif-
ferent domains. Nonetheless, even in the more diffi-
cult cases, a sizable set of high-precision translations
can be extracted. As an example of the performance
of the system, in English-Spanish induction with our
best feature set, using corpora derived from topically
similar but non-parallel sources, the system obtains
89.0% precision at 33% recall.
1See Hardoon et al (2003) for an overview.
771
state
society
enlarge-
ment
control
import-
ance
sociedad
estado
amplifi-
caci?n
import-
ancia
control
.
.
.
.
.
.
s
t
m
Figure 1: Bilingual lexicon induction: source word types
s are listed on the left and target word types t on the
right. Dashed lines between nodes indicate translation
pairs which are in the matching m.
2 Bilingual Lexicon Induction
As input, we are given a monolingual corpus S (a
sequence of word tokens) in a source language and
a monolingual corpus T in a target language. Let
s = (s1, . . . , snS ) denote nS word types appearing
in the source language, and t = (t1, . . . , tnT ) denote
word types in the target language. Based on S and
T , our goal is to output a matching m between s
and t. We represent m as a set of integer pairs so
that (i, j) ?m if and only if si is matched with tj .
2.1 Generative Model
We propose the following generative model over
matchings m and word types (s, t), which we call
matching canonical correlation analysis (MCCA).
MCCA model
m ? MATCHING-PRIOR [matching m]
For each matched edge (i, j) ?m:
?z
i,j
? N (0, I
d
) [latent concept]
?f
S
(s
i
) ? N (W
S
z
i,j
,?
S
) [source features]
?f
T
(t
i
) ? N (W
T
z
i,j
,?
T
) [target features]
For each unmatched source word type i:
?f
S
(s
i
) ? N (0, ?
2
I
d
S
) [source features]
For each unmatched target word type j:
?f
T
(t
j
) ? N (0, ?
2
I
d
T
) [target features]
First, we generate a matching m ?M, whereM
is the set of matchings in which each word type is
matched to at most one other word type.2 We take
MATCHING-PRIOR to be uniform overM.3
Then, for each matched pair of word types (i, j) ?
m, we need to generate the observed feature vectors
of the source and target word types, fS(si) ? RdS
and fT (tj) ? RdT . The feature vector of each word
type is computed from the appropriate monolin-
gual corpus and summarizes the word?s monolingual
characteristics; see section 5 for details and figure 2
for an illustration. Since si and tj are translations of
each other, we expect fS(si) and fT (tj) to be con-
nected somehow by the generative process. In our
model, they are related through a vector zi,j ? Rd
representing the shared, language-independent con-
cept.
Specifically, to generate the feature vectors, we
first generate a random concept zi,j ? N (0, Id),
where Id is the d ? d identity matrix. The source
feature vector fS(si) is drawn from a multivari-
ate Gaussian with mean WSzi,j and covariance ?S ,
where WS is a dS ? d matrix which transforms the
language-independent concept zi,j into a language-
dependent vector in the source space. The arbitrary
covariance parameter ?S  0 explains the source-
specific variations which are not captured by WS ; it
does not play an explicit role in inference. The target
fT (tj) is generated analogously using WT and ?T ,
conditionally independent of the source given zi,j
(see figure 2). For each of the remaining unmatched
source word types si which have not yet been gen-
erated, we draw the word type features from a base-
line normal distribution with variance ?2IdS , with
hyperparameter ?2  0; unmatched target words
are similarly generated.
If two word types are truly translations, it will be
better to relate their feature vectors through the la-
tent space than to explain them independently via
the baseline distribution. However, if a source word
type is not a translation of any of the target word
types, we can just generate it independently without
requiring it to participate in the matching.
2Our choice ofM permits unmatched word types, but does
not allow words to have multiple translations. This setting facil-
itates comparison to previous work and admits simpler models.
3However, non-uniform priors could encode useful informa-
tion, such as rank similarities.
772
1.0
1.0
20.0
5.0
100.0
50.0
.
.
.
Source 
Space
Canonical 
Space
R
d
s
R
d
t
1.0
1.0
.
.
.
1.0
Target 
Space
R
d
1.0
{
{
O
r
t
h
o
g
r
a
p
h
i
c
 
F
e
a
t
u
r
e
s
C
o
n
t
e
x
t
u
a
l
 
F
e
a
t
u
r
e
s
time
tiempo
#ti
#ti
ime
mpo
me#
pe#
change
dawn
period
necessary
40.0
65.0
120.0
45.0
suficiente
per?odo
mismo
adicional
s
i
t
j
z
f
S
(s
i
)
f
T
(t
j
)
Figure 2: Illustration of our MCCA model. Each latent concept z
i,j
originates in the canonical space. The observed
word vectors in the source and target spaces are generated independently given this concept.
3 Inference
Given our probabilistic model, we would like to
maximize the log-likelihood of the observed data
(s, t):
`(?) = log p(s, t; ?) = log
?
m
p(m, s, t; ?)
with respect to the model parameters ? =
(WS ,WT ,?S ,?T ).
We use the hard (Viterbi) EM algorithm as a start-
ing point, but due to modeling and computational
considerations, we make several important modifi-
cations, which we describe later. The general form
of our algorithm is as follows:
Summary of learning algorithm
E-step: Find the maximum weighted (partial) bi-
partite matching m ?M
M-step: Find the best parameters ? by performing
canonical correlation analysis (CCA)
M-step Given a matching m, the M-step opti-
mizes log p(m, s, t; ?) with respect to ?, which can
be rewritten as
max
?
?
(i,j)?m
log p(si, tj ; ?). (1)
This objective corresponds exactly to maximizing
the likelihood of the probabilistic CCA model pre-
sented in Bach and Jordan (2006), which proved
that the maximum likelihood estimate can be com-
puted by canonical correlation analysis (CCA). In-
tuitively, CCA finds d-dimensional subspaces US ?
R
dS?d of the source and UT ? RdT?d of the tar-
get such that the components of the projections
U
>
S fS(si) and U
>
T fT (tj) are maximally correlated.
4
US and UT can be found by solving an eigenvalue
problem (see Hardoon et al (2003) for details).
Then the maximum likelihood estimates are as fol-
lows: WS = CSSUSP 1/2, WT = CTTUTP 1/2,
?S = CSS ?WSW
>
S , and ?T = CTT ?WTW
>
T ,
where P is a d? d diagonal matrix of the canonical
correlations, CSS = 1|m|
?
(i,j)?m fS(si)fS(si)
> is
the empirical covariance matrix in the source do-
main, and CTT is defined analogously.
E-step To perform a conventional E-step, we
would need to compute the posterior over all match-
ings, which is #P-complete (Valiant, 1979). On the
other hand, hard EM only requires us to compute the
best matching under the current model:5
m = argmax
m?
log p(m?, s, t; ?). (2)
We cast this optimization as a maximum weighted
bipartite matching problem as follows. Define the
edge weight between source word type i and target
word type j to be
wi,j = log p(si, tj ; ?) (3)
? log p(si; ?)? log p(tj ; ?),
4Since dS and dT can be quite large in practice and of-
ten greater than |m|, we use Cholesky decomposition to re-
represent the feature vectors as |m|-dimensional vectors with
the same dot products, which is all that CCA depends on.
5If we wanted softer estimates, we could use the agreement-
based learning framework of Liang et al (2008) to combine two
tractable models.
773
which can be loosely viewed as a pointwise mutual
information quantity. We can check that the ob-
jective log p(m, s, t; ?) is equal to the weight of a
matching plus some constant C:
log p(m, s, t; ?) =
?
(i,j)?m
wi,j + C. (4)
To find the optimal partial matching, edges with
weight wi,j < 0 are set to zero in the graph and the
optimal full matching is computed inO((nS+nT )3)
time using the Hungarian algorithm (Kuhn, 1955). If
a zero edge is present in the solution, we remove the
involved word types from the matching.6
Bootstrapping Recall that the E-step produces a
partial matching of the word types. If too few
word types are matched, learning will not progress
quickly; if too many are matched, the model will be
swamped with noise. We found that it was helpful
to explicitly control the number of edges. Thus, we
adopt a bootstrapping-style approach that only per-
mits high confidence edges at first, and then slowly
permits more over time. In particular, we compute
the optimal full matching, but only retain the high-
est weighted edges. As we run EM, we gradually
increase the number of edges to retain.
In our context, bootstrapping has a similar moti-
vation to the annealing approach of Smith and Eisner
(2006), which also tries to alter the space of hidden
outputs in the E-step over time to facilitate learn-
ing in the M-step, though of course the use of boot-
strapping in general is quite widespread (Yarowsky,
1995).
4 Experimental Setup
In section 5, we present developmental experiments
in English-Spanish lexicon induction; experiments
6Empirically, we obtained much better efficiency and even
increased accuracy by replacing these marginal likelihood
weights with a simple proxy, the distances between the words?
mean latent concepts:
wi,j = A? ||z
?
i ? z
?
j ||2, (5)
where A is a thresholding constant, z?i = E(zi,j | fS(si)) =
P 1/2U>S fS(si), and z
?
j is defined analogously. The increased
accuracy may not be an accident: whether two words are trans-
lations is perhaps better characterized directly by how close
their latent concepts are, whereas log-probability is more sensi-
tive to perturbations in the source and target spaces.
are presented for other languages in section 6. In
this section, we describe the data and experimental
methodology used throughout this work.
4.1 Data
Each experiment requires a source and target mono-
lingual corpus. We use the following corpora:
? EN-ES-W: 3,851 Wikipedia articles with both
English and Spanish bodies (generally not di-
rect translations).
? EN-ES-P: 1st 100k sentences of text from the
parallel English and Spanish Europarl corpus
(Koehn, 2005).
? EN-ES(FR)-D: English: 1st 50k sentences of
Europarl; Spanish (French): 2nd 50k sentences
of Europarl.7
? EN-CH-D: English: 1st 50k sentences of Xin-
hua parallel news corpora;8 Chinese: 2nd 50k
sentences.
? EN-AR-D: English: 1st 50k sentences of 1994
proceedings of UN parallel corpora;9 Ara-
bic: 2nd 50k sentences.
? EN-ES-G: English: 100k sentences of English
Gigaword; Spanish: 100k sentences of Spanish
Gigaword.10
Note that even when corpora are derived from par-
allel sources, no explicit use is ever made of docu-
ment or sentence-level alignments. In particular, our
method is robust to permutations of the sentences in
the corpora.
4.2 Lexicon
Each experiment requires a lexicon for evaluation.
Following Koehn and Knight (2002), we consider
lexicons over only noun word types, although this
is not a fundamental limitation of our model. We
consider a word type to be a noun if its most com-
mon tag is a noun in our monolingual corpus.11 For
7Note that the although the corpora here are derived from a
parallel corpus, there are no parallel sentences.
8LDC catalog # 2002E18.
9LDC catalog # 2004E13.
10These corpora contain no parallel sentences.
11We use the Tree Tagger (Schmid, 1994) for all POS tagging
except for Arabic, where we use the tagger described in Diab et
al. (2004).
774
 0.6 0.65 0.7 0.75 0.8 0.85
 0.9 0.95 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8Precision Recall EN-ES-PEN-ES-W
Figure 3: Example precision/recall curve of our system
on EN-ES-P and EN-ES-W settings. See section 6.1.
all languages pairs except English-Arabic, we ex-
tract evaluation lexicons from the Wiktionary on-
line dictionary. As we discuss in section 7, our ex-
tracted lexicons have low coverage, particularly for
proper nouns, and thus all performance measures are
(sometimes substantially) pessimistic. For English-
Arabic, we extract a lexicon from 100k parallel sen-
tences of UN parallel corpora by running the HMM
intersected alignment model (Liang et al, 2008),
adding (s, t) to the lexicon if s was aligned to t at
least three times and more than any other word.
Also, as in Koehn and Knight (2002), we make
use of a seed lexicon, which consists of a small, and
perhaps incorrect, set of initial translation pairs. We
used two methods to derive a seed lexicon. The
first is to use the evaluation lexicon Le and select
the hundred most common noun word types in the
source corpus which have translations in Le. The
second method is to heuristically induce, where ap-
plicable, a seed lexicon using edit distance, as is
done in Koehn and Knight (2002). Section 6.2 com-
pares the performance of these two methods.
4.3 Evaluation
We evaluate a proposed lexicon Lp against the eval-
uation lexicon Le using the F1 measure in the stan-
dard fashion; precision is given by the number of
proposed translations contained in the evaluation
lexicon, and recall is given by the fraction of pos-
sible translation pairs proposed.12 Since our model
12We should note that precision is not penalized for (s, t) if
s does not have a translation in Le, and recall is not penalized
for failing to recover multiple translations of s.
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ?- 47.4
ORTHO 76.0 81.3 80.1 52.3 55.0
CONTEXT 91.1 81.3 80.2 65.3 58.0
MCCA 87.2 89.7 89.0 89.7 72.0
Table 1: Performance of EDITDIST and our model with
various features sets on EN-ES-W. See section 5.
naturally produces lexicons in which each entry is
associated with a weight based on the model, we can
give a full precision/recall curve (see figure 3). We
summarize these curves with both the best F1 over
all possible thresholds and various precisions px at
recalls x. All reported numbers exclude evaluation
on the seed lexicon entries, regardless of how those
seeds are derived or whether they are correct.
In all experiments, unless noted otherwise, we
used a seed of size 100 obtained from Le and
considered lexicons between the top n = 2, 000
most frequent source and target noun word types
which were not in the seed lexicon; each system
proposed an already-ranked one-to-one translation
lexicon amongst these n words. Where applica-
ble, we compare against the EDITDIST baseline,
which solves a maximum bipartite matching prob-
lem where edge weights are normalized edit dis-
tances. We will use MCCA (for matching CCA) to
denote our model using the optimal feature set (see
section 5.3).
5 Features
In this section, we explore feature representations of
word types in our model. Recall that fS(?) and fT (?)
map source and target word types to vectors in RdS
and RdT , respectively (see section 2). The features
used in each representation are defined identically
and derived only from the appropriate monolingual
corpora. For a concrete example of a word type to
feature vector mapping, see figure 2.
5.1 Orthographic Features
For closely related languages, such as English and
Spanish, translation pairs often share many ortho-
graphic features. One direct way to capture ortho-
graphic similarity between word pairs is edit dis-
tance. Running EDITDIST (see section 4.3) on EN-
775
ES-W yielded 61.1 p0.33, but precision quickly de-
grades for higher recall levels (see EDITDIST in ta-
ble 1). Nevertheless, when available, orthographic
clues are strong indicators of translation pairs.
We can represent orthographic features of a word
type w by assigning a feature to each substring of
length ? 3. Note that MCCA can learn regular or-
thographic correspondences between source and tar-
get words, which is something edit distance cannot
capture (see table 5). Indeed, running our MCCA
model with only orthographic features on EN-ES-
W, labeled ORTHO in table 1, yielded 80.1 p0.33, a
31% error-reduction over EDITDIST in p0.33.
5.2 Context Features
While orthographic features are clearly effective for
historically related language pairs, they are more
limited for other language pairs, where we need to
appeal to other clues. One non-orthographic clue
that word types s and t form a translation pair is
that there is a strong correlation between the source
words used with s and the target words used with t.
To capture this information, we define context fea-
tures for each word type w, consisting of counts of
nouns which occur within a window of size 4 around
w. Consider the translation pair (time, tiempo)
illustrated in figure 2. As we become more con-
fident about other translation pairs which have ac-
tive period and periodico context features, we
learn that translation pairs tend to jointly generate
these features, which leads us to believe that time
and tiempo might be generated by a common un-
derlying concept vector (see section 2).13
Using context features alone on EN-ES-W, our
MCCA model (labeled CONTEXT in table 1) yielded
a 80.2 p0.33. It is perhaps surprising that context fea-
tures alone, without orthographic information, can
yield a best-F1comparable to EDITDIST.
5.3 Combining Features
We can of course combine context and orthographic
features. Doing so yielded 89.03 p0.33 (labeled
MCCA in table 1); this represents a 46.4% error re-
duction in p0.33 over the EDITDIST baseline. For the
remainder of this work, we will use MCCA to refer
13It is important to emphasize, however, that our current
model does not directly relate a word type?s role as a partici-
pant in the matching to that word?s role as a context feature.
(a) Corpus Variation
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES-G 75.0 71.2 68.3 ?- 49.0
EN-ES-W 87.2 89.7 89.0 89.7 72.0
EN-ES-D 91.4 94.3 92.3 89.7 63.7
EN-ES-P 97.3 94.8 93.8 92.9 77.0
(b) Seed Lexicon Variation
Corpus p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ? 47.4
MCCA 91.4 94.3 92.3 89.7 63.7
MCCA-AUTO 91.2 90.5 91.8 77.5 61.7
(c) Language Variation
Languages p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES 91.4 94.3 92.3 89.7 63.7
EN-FR 94.5 89.1 88.3 78.6 61.9
EN-CH 60.1 39.3 26.8 ?- 30.8
EN-AR 70.0 50.0 31.1 ?- 33.1
Table 2: (a) varying type of corpora used on system per-
formance (section 6.1), (b) using a heuristically chosen
seed compared to one taken from the evaluation lexicon
(section 6.2), (c) a variety of language pairs (see sec-
tion 6.3).
to our model using both orthographic and context
features.
6 Experiments
In this section we examine how system performance
varies when crucial elements are altered.
6.1 Corpus Variation
There are many sources from which we can derive
monolingual corpora, and MCCA performance de-
pends on the degree of similarity between corpora.
We explored the following levels of relationships be-
tween corpora, roughly in order of closest to most
distant:
? Same Sentences: EN-ES-P
? Non-Parallel Similar Content: EN-ES-W
? Distinct Sentences, Same Domain: EN-ES-D
? Unrelated Corpora: EN-ES-G
Our results for all conditions are presented in ta-
ble 2(a). The predominant trend is that system per-
formance degraded when the corpora diverged in
776
content, presumably due to context features becom-
ing less informative. However, it is notable that even
in the most extreme case of disjoint corpora from
different time periods and topics (e.g. EN-ES-G),
we are still able to recover lexicons of reasonable
accuracy.
6.2 Seed Lexicon Variation
All of our experiments so far have exploited a small
seed lexicon which has been derived from the eval-
uation lexicon (see section 4.3). In order to explore
system robustness to heuristically chosen seed lexi-
cons, we automatically extracted a seed lexicon sim-
ilarly to Koehn and Knight (2002): we ran EDIT-
DIST on EN-ES-D and took the top 100 most con-
fident translation pairs. Using this automatically de-
rived seed lexicon, we ran our system on EN-ES-
D as before, evaluating on the top 2,000 noun word
types not included in the automatic lexicon.14 Us-
ing the automated seed lexicon, and still evaluat-
ing against our Wiktionary lexicon, MCCA-AUTO
yielded 91.8 p0.33 (see table 2(b)), indicating that
our system can produce lexicons of comparable ac-
curacy with a heuristically chosen seed. We should
note that this performance represents no knowledge
given to the system in the form of gold seed lexicon
entries.
6.3 Language Variation
We also explored how system performance varies
for language pairs other than English-Spanish. On
English-French, for the disjoint EN-FR-D corpus
(described in section 4.1), MCCA yielded 88.3 p0.33
(see table 2(c) for more performance measures).
This verified that our model can work for another
closely related language-pair on which no model de-
velopment was performed.
One concern is how our system performs on lan-
guage pairs where orthographic features are less ap-
plicable. Results on disjoint English-Chinese and
English-Arabic are given as EN-CH-D and EN-AR
in table 2(c), both using only context features. In
these cases, MCCA yielded much lower precisions
of 26.8 and 31.0 p0.33, respectively. For both lan-
guages, performance degraded compared to EN-ES-
14Note that the 2,000 words evaluated here were not identical
to the words tested on when the seed lexicon is derived from the
evaluation lexicon.
(a) English-Spanish
Rank Source Target Correct
1. education educaci?n Y
2. pacto pact Y
3. stability estabilidad Y
6. corruption corrupci?n Y
7. tourism turismo Y
9. organisation organizaci?n Y
10. convenience conveniencia Y
11. syria siria Y
12. cooperation cooperaci?n Y
14. culture cultura Y
21. protocol protocolo Y
23. north norte Y
24. health salud Y
25. action reacci?n N
(b) English-French
Rank Source Target Correct
3. xenophobia x?nophobie Y
4. corruption corruption Y
5. subsidiarity subsidiarit? Y
6. programme programme-cadre N
8. traceability tra?abilit? Y
(c) English-Chinese
Rank Source Target Correct
1. prices ? Y
2. network ? Y
3. population ? Y
4. reporter ? N
5. oil ? Y
Table 3: Sample output from our (a) Spanish, (b) French,
and (c) Chinese systems. We present the highest con-
fidence system predictions, where the only editing done
is to ignore predictions which consist of identical source
and target words.
D and EN-FR-D, presumably due in part to the
lack of orthographic features. However, MCCA still
achieved surprising precision at lower recall levels.
For instance, at p0.1, MCCA yielded 60.1 and 70.0
on Chinese and Arabic, respectively. Figure 3 shows
the highest-confidence outputs in several languages.
6.4 Comparison To Previous Work
There has been previous work in extracting trans-
lation pairs from non-parallel corpora (Rapp, 1995;
Fung, 1995; Koehn and Knight, 2002), but gener-
ally not in as extreme a setting as the one consid-
ered here. Due to unavailability of data and speci-
ficity in experimental conditions and evaluations, it
is not possible to perform exact comparisons. How-
777
(a) Example Non-Cognate Pairs
health salud
traceability rastreabilidad
youth juventud
report informe
advantages ventajas
(b) Interesting Incorrect Pairs
liberal partido
Kirkhope Gorsel
action reaccio?n
Albanians Bosnia
a.m. horas
Netherlands Bretan?a
Table 4: System analysis on EN-ES-W: (a) non-cognate
pairs proposed by our system, (b) hand-selected represen-
tative errors.
(a) Orthographic Feature
Source Feat. Closest Target Feats. Example Translation
#st #es, est (statue, estatua)
ty# ad#, d# (felicity, felicidad)
ogy g??a, g?? (geology, geolog??a)
(b) Context Feature
Source Feat. Closest Context Features
party partido, izquierda
democrat socialistas, demo?cratas
beijing pek??n, kioto
Table 5: Hand selected examples of source and target fea-
tures which are close in canonical space: (a) orthographic
feature correspondences, (b) context features.
ever, we attempted to run an experiment as similar
as possible in setup to Koehn and Knight (2002), us-
ing English Gigaword and German Europarl. In this
setting, our MCCA system yielded 61.7% accuracy
on the 186 most confident predictions compared to
39% reported in Koehn and Knight (2002).
7 Analysis
We have presented a novel generative model for
bilingual lexicon induction and presented results un-
der a variety of data conditions (section 6.1) and lan-
guages (section 6.3) showing that our system can
produce accurate lexicons even in highly adverse
conditions. In this section, we broadly characterize
and analyze the behavior of our system.
We manually examined the top 100 errors in the
English-Spanish lexicon produced by our system
on EN-ES-W. Of the top 100 errors: 21 were cor-
rect translations not contained in the Wiktionary
lexicon (e.g. pintura to painting), 4 were
purely morphological errors (e.g. airport to
aeropuertos), 30 were semantically related (e.g.
basketball to be?isbol), 15 were words with
strong orthographic similarities (e.g. coast to
costas), and 30 were difficult to categorize and
fell into none of these categories. Since many of
our ?errors? actually represent valid translation pairs
not contained in our extracted dictionary, we sup-
plemented our evaluation lexicon with one automat-
ically derived from 100k sentences of parallel Eu-
roparl data. We ran the intersected HMM word-
alignment model (Liang et al, 2008) and added
(s, t) to the lexicon if s was aligned to t at least
three times and more than any other word. Evaluat-
ing against the union of these lexicons yielded 98.0
p0.33, a significant improvement over the 92.3 us-
ing only the Wiktionary lexicon. Of the true errors,
the most common arose from semantically related
words which had strong context feature correlations
(see table 4(b)).
We also explored the relationships our model
learns between features of different languages. We
projected each source and target feature into the
shared canonical space, and for each projected
source feature we examined the closest projected
target features. In table 5(a), we present some of
the orthographic feature relationships learned by our
system. Many of these relationships correspond to
phonological and morphological regularities such as
the English suffix ing mapping to the Spanish suf-
fix g??a. In table 5(b), we present context feature
correspondences. Here, the broad trend is for words
which are either translations or semantically related
across languages to be close in canonical space.
8 Conclusion
We have presented a generative model for bilingual
lexicon induction based on probabilistic CCA. Our
experiments show that high-precision translations
can be mined without any access to parallel corpora.
It remains to be seen how such lexicons can be best
utilized, but they invite new approaches to the statis-
tical translation of resource-poor languages.
778
References
Francis R. Bach and Michael I. Jordan. 2006. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, University of California, Berkeley.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of arabic text: From raw text to
base phrase chunks. In HLT-NAACL.
Pascale Fung. 1995. Compiling bilingual lexicon entries
from a non-parallel english-chinese corpus. In Third
Annual Workshop on Very Large Corpora.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
David R. Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2003. Canonical correlation analysis an
overview with application to learning methods. Tech-
nical Report CSD-TR-03-02, Royal Holloway Univer-
sity of London.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of ACL Workshop on Unsupervised Lexical
Acquisition.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
Reinhard Rapp. 1995. Identifying word translation in
non-parallel texts. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
L. G. Valiant. 1979. The complexity of computing
the permanent. Theoretical Computer Science, 8:189?
201.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL.
779
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923?931,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Better Word Alignments with Supervised ITG Models
Aria Haghighi, John Blitzer, John DeNero and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,blitzer,denero,klein }@cs.berkeley.edu
Abstract
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
1 Introduction
Inversion transduction grammar (ITG) con-
straints (Wu, 1997) provide coherent structural
constraints on the relationship between a sentence
and its translation. ITG has been extensively
explored in unsupervised statistical word align-
ment (Zhang and Gildea, 2005; Cherry and
Lin, 2007a; Zhang et al, 2008) and machine
translation decoding (Cherry and Lin, 2007b;
Petrov et al, 2008). In this work, we investigate
large-scale, discriminative ITG word alignment.
Past work on discriminative word alignment
has focused on the family of at-most-one-to-one
matchings (Melamed, 2000; Taskar et al, 2005;
Moore et al, 2006). An exception to this is the
work of Cherry and Lin (2006), who discrim-
inatively trained one-to-one ITG models, albeit
with limited feature sets. As they found, ITG
approaches offer several advantages over general
matchings. First, the additional structural con-
straint can result in superior alignments. We con-
firm and extend this result, showing that one-to-
one ITG models can perform as well as, or better
than, general one-to-one matching models, either
using heuristic weights or using rich, learned fea-
tures.
A second advantage of ITG approaches is that
they admit a range of training options. As with
general one-to-one matchings, we can optimize
margin-based objectives. However, unlike with
general matchings, we can also efficiently com-
pute expectations over the set of ITG derivations,
enabling the training of conditional likelihood
models. A major challenge in both cases is that
our training alignments are often not one-to-one
ITG alignments. Under such conditions, directly
training to maximize margin is unstable, and train-
ing to maximize likelihood is ill-defined, since the
target algnment derivations don?t exist in our hy-
pothesis class. We show how to adapt both margin
and likelihood objectives to learn good ITG align-
ers.
In the case of likelihood training, two innova-
tions are presented. The simple, two-rule ITG
grammar exponentially over-counts certain align-
ment structures relative to others. Because of this,
Wu (1997) and Zens and Ney (2003) introduced a
normal form ITG which avoids this over-counting.
We extend this normal form to null productions
and give the first extensive empirical comparison
of simple and normal form ITGs, for posterior de-
coding under our likelihood models. Additionally,
we show how to deal with training instances where
the gold alignments are outside of the hypothesis
class by instead optimizing the likelihood of a set
of minimum-loss alignments.
Perhaps the greatest advantage of ITG mod-
els is that they straightforwardly permit block-
923
structured alignments (i.e. phrases), which gen-
eral matchings cannot efficiently do. The need for
block alignments is especially acute in Chinese-
English data, where oracle AERs drop from 10.2
without blocks to around 1.2 with them. Indeed,
blocks are the primary reason for gold alignments
being outside the space of one-to-one ITG align-
ments. We show that placing linear potential func-
tions on many-to-one blocks can substantially im-
prove performance.
Finally, to scale up our system, we give a com-
bination of pruning techniques that allows us to
sum ITG alignments two orders of magnitude
faster than naive inside-outside parsing.
All in all, our discriminatively trained, block
ITG models produce alignments which exhibit
the best AER on the NIST 2002 Chinese-English
alignment data set. Furthermore, they result in
a 1.1 BLEU-point improvement over GIZA++
alignments in an end-to-end Hiero (Chiang, 2007)
machine translation system.
2 Alignment Families
In order to structurally restrict attention to rea-
sonable alignments, word alignment models must
constrain the set of alignments considered. In this
section, we discuss and compare alignment fami-
lies used to train our discriminative models.
Initially, as in Taskar et al (2005) and Moore
et al (2006), we assume the score a of a potential
alignment a) decomposes as
s(a) = ?
(i,j)?a
sij +
?
i/?a
si +
?
j /?a
sj (1)
where sij are word-to-word potentials and si and
sj represent English null and foreign null poten-
tials, respectively.
We evaluate our proposed alignments (a)
against hand-annotated alignments, which are
marked with sure (s) and possible (p) alignments.
The alignment error rate (AER) is given by,
AER(a, s,p) = 1? |a ? s|+ |a ? p||a|+ |s|
2.1 1-to-1 Matchings
The class of at most 1-to-1 alignment match-
ings, A1-1, has been considered in several works
(Melamed, 2000; Taskar et al, 2005; Moore et al,
2006). The alignment that maximizes a set of po-
tentials factored as in Equation (1) can be found
in O(n3) time using a bipartite matching algo-
rithm (Kuhn, 1955).1 On the other hand, summing
over A1-1 is #P -hard (Valiant, 1979).
Initially, we consider heuristic alignment poten-
tials given by Dice coefficients
Dice(e, f) = 2CefCe + Cf
where Cef is the joint count of words (e, f) ap-
pearing in aligned sentence pairs, and Ce and Cf
are monolingual unigram counts.
We extracted such counts from 1.1 million
French-English aligned sentence pairs of Hansards
data (see Section 6.1). For each sentence pair in
the Hansards test set, we predicted the alignment
from A1-1 which maximized the sum of Dice po-
tentials. This yielded 30.6 AER.
2.2 Inversion Transduction Grammar
Wu (1997)?s inversion transduction grammar
(ITG) is a synchronous grammar formalism in
which derivations of sentence pairs correspond to
alignments. In its original formulation, there is a
single non-terminal X spanning a bitext cell with
an English and foreign span. There are three rule
types: Terminal unary productions X ? ?e, f?,
where e and f are an aligned English and for-
eign word pair (possibly with one being null);
normal binary rules X ? X(L)X(R), where the
English and foreign spans are constructed from
the children as ?X(L)X(R), X(L)X(R)?; and in-
verted binary rules X ; X(L)X(R), where the
foreign span inverts the order of the children
?X(L)X(R), X(R)X(L)?.2 In general, we will call
a bitext cell a normal cell if it was constructed with
a normal rule and inverted if constructed with an
inverted rule.
Each ITG derivation yields some alignment.
The set of such ITG alignments,AITG, are a strict
subset of A1-1 (Wu, 1997). Thus, we will view
ITG as a constraint on A1-1 which we will ar-
gue is generally beneficial. The maximum scor-
ing alignment from AITG can be found in O(n6)
time with synchronous CFG parsing; in practice,
we can make ITG parsing efficient using a variety
of pruning techniques. One computational advan-
tage of AITG over A1-1 alignments is that sum-
mation overAITG is tractable. The corresponding
1We shall use n throughout to refer to the maximum of
foreign and English sentence lengths.
2The superscripts on non-terminals are added only to in-
dicate correspondence of child symbols.
924
In
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
 ?
 ?
 ??
 ??
 ??
 ??
 ?
 ?
 ??
 ??
 ??
 ??
I
n
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
(a) Max-Matching Alignment (b) Block ITG Alignment
Figure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively. The 1-1
matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is rendered
as two words in Chinese or that in court is rendered as a single word in Chinese.
dynamic program allows us to utilize likelihood-
based objectives for learning alignment models
(see Section 4).
Using the same heuristic Dice potentials on
the Hansards test set, the maximal scoring align-
ment from AITG yields 28.4 AER?2.4 better
than A1-1 ?indicating that ITG can be beneficial
as a constraint on heuristic alignments.
2.3 Block ITG
An important alignment pattern disallowed by
A1-1 is the many-to-one alignment block. While
not prevalent in our hand-aligned French Hansards
dataset, blocks occur frequently in our hand-
aligned Chinese-English NIST data. Figure 1
contains an example. Extending A1-1 to include
blocks is problematic, because finding a maximal
1-1 matching over phrases is NP-hard (DeNero
and Klein, 2008).
With ITG, it is relatively easy to allow contigu-
ous many-to-one alignment blocks without added
complexity.3 This is accomplished by adding ad-
ditional unary terminal productions aligning a for-
eign phrase to a single English terminal or vice
versa. We will use BITG to refer to this block
ITG variant and ABITG to refer to the alignment
family, which is neither contained in nor contains
A1-1. For this alignment family, we expand the
alignment potential decomposition in Equation (1)
to incorporate block potentials sef and sef which
represent English and foreign many-to-one align-
ment blocks, respectively.
One way to evaluate alignment families is to
3In our experiments we limited the block size to 4.
consider their oracle AER. In the 2002 NIST
Chinese-English hand-aligned data (see Sec-
tion 6.2), we constructed oracle alignment poten-
tials as follows: sij is set to +1 if (i, j) is a sure
or possible alignment in the hand-aligned data, -
1 otherwise. All null potentials (si and sj) are
set to 0. A max-matching under these potentials is
generally a minimal loss alignment in the family.
The oracle AER computed in this was is 10.1 for
A1-1 and 10.2 for AITG. The ABITG alignment
family has an oracle AER of 1.2. These basic ex-
periments show that AITG outperforms A1-1 for
heuristic alignments, and ABITG provide a much
closer fit to true Chinese-English alignments than
A1-1.
3 Margin-Based Training
In this and the next section, we discuss learning
alignment potentials. As input, we have a training
set D = (x1,a?1), . . . , (xn,a?n) of hand-aligned
data, where x refers to a sentence pair. We will as-
sume the score of a alignment is given as a linear
function of a feature vector ?(x,a). We will fur-
ther assume the feature representation of an align-
ment, ?(x,a) decomposes as in Equation (1),
?
(i,j)?a
?ij(x) +
?
i/?a
?i(x) +
?
j /?a
?j(x)
In the framework of loss-augmented margin
learning, we seek a w such that w ? ?(x,a?) is
larger than w ? ?(x,a) + L(a,a?) for all a in an
alignment family, where L(a,a?) is the loss be-
tween a proposed alignment a and the gold align-
ment a?. As in Taskar et al (2005), we utilize a
925
loss that decomposes across alignments. Specif-
ically, for each alignment cell (i, j) which is not
a possible alignment in a?, we incur a loss of 1
when aij 6= a?ij ; note that if (i, j) is a possible
alignment, our loss is indifferent to its presence in
the proposal alignment.
A simple loss-augmented learning pro-
cedure is the margin infused relaxed algo-
rithm (MIRA) (Crammer et al, 2006). MIRA
is an online procedure, where at each time step
t+ 1, we update our weights as follows:
wt+1 = argminw||w ?wt||22 (2)
s.t. w ? ?(x,a?) ? w ? ?(x, a?) + L(a?,a?)
where a? = argmax
a?A
wt ? ?(x,a)
In our data sets, many a? are not in A1-1 (and
thus not in AITG), implying the minimum in-
family loss must exceed 0. Since MIRA oper-
ates in an online fashion, this can cause severe
stability problems. On the Hansards data, the
simple averaging technique described by Collins
(2002) yields a reasonable model. On the Chinese
NIST data, however, where almost no alignment
is in A1-1, the update rule from Equation (2) is
completely unstable, and even the averaged model
does not yield high-quality results.
We instead use a variant of MIRA similar to
Chiang et al (2008). First, rather than update
towards the hand-labeled alignment a?, we up-
date towards an alignment which achieves mini-
mal loss within the family.4 We call this best-
in-class alignment a?p. Second, we perform loss-
augmented inference to obtain a?. This yields the
modified QP,
wt+1 = argminw||w ?wt||22 (3)
s.t. w ? ?(x,a?p) ? w ? ?(x, a?) + L(a,a?p)
where a? = argmax
a?A
wt ? ?(x,a) + ?L(a,a?p)
By setting ? = 0, we recover the MIRA update
from Equation (2). As ? grows, we increase our
preference that a? have high loss (relative to a?p)
rather than high model score. With this change,
MIRA is stable, but still performs suboptimally.
The reason is that initially the score for all align-
ments is low, so we are biased toward only using
very high loss alignments in our constraint. This
slows learning and prevents us from finding a use-
ful weight vector. Instead, in all the experiments
4There might be several alignments which achieve this
minimal loss; we choose arbitrarily among them.
we report here, we begin with ? = 0 and slowly
increase it to ? = 0.5.
4 Likelihood Objective
An alternative to margin-based training is a likeli-
hood objective, which learns a conditional align-
ment distribution Pw(a|x) parametrized as fol-
lows,
logPw(a|x)=w??(x,a)?log
?
a??A
exp(w??(x,a?))
where the log-denominator represents a sum over
the alignment family A. This alignment probabil-
ity only places mass on members ofA. The likeli-
hood objective is given by,
max
w
?
(x,a?)?A
logPw(a?|x)
Optimizing this objective with gradient methods
requires summing over alignments. ForAITG and
ABITG, we can efficiently sum over the set of ITG
derivations inO(n6) time using the inside-outside
algorithm. However, for the ITG grammar pre-
sented in Section 2.2, each alignment has multiple
grammar derivations. In order to correctly sum
over the set of ITG alignments, we need to alter
the grammar to ensure a bijective correspondence
between alignments and derivations.
4.1 ITG Normal Form
There are two ways in which ITG derivations dou-
ble count alignments. First, n-ary productions are
not binarized to remove ambiguity; this results in
an exponential number of derivations for diagonal
alignments. This source of overcounting is con-
sidered and fixed by Wu (1997) and Zens and Ney
(2003), which we briefly review here. The result-
ing grammar, which does not handle null align-
ments, consists of a symbol N to represent a bi-
text cell produced by a normal rule and I for a cell
formed by an inverted rule; alignment terminals
can be either N or I . In order to ensure unique
derivations, we stipulate that a N cell can be con-
structed only from a sequence of smaller inverted
cells I . Binarizing the rule N ? I2+ introduces
the intermediary symbolN (see Figure 2(a)). Sim-
ilarly for inverse cells, we insist an I cell only be
built by an inverted combination of N cells; bina-
rization of I ; N2+ requires the introduction of
the intermediary symbol I (see Figure 2(b)).
Null productions are also a source of double
counting, as there are many possible orders in
926
N ? I2+N ? IN
N ? I
}
N ? IN
I
I
I
N
N
N
(a) Normal Domain Rules
} I ! N2+
I ! NI
I ! NI
I ! N N
N
N
I
I
I
(b) Inverted Domain Rules
N11 ? ??, f?N11
N11 ? N10
N10 ? N10?e, ??
N10 ? N00
}N11 ? ??, f?
?N10
}N10 ? N00?e, ???
}
N00 ? I11N
N ? I11N
N ? I00
N00 ? I+11I00
N00 N10 N10
N11
N
NI11
I11
I00
N00
N11
(c) Normal Domain with Null Rules
}
}
}
I11 ! ??, f?I11
I11 ! I10 I11 ! ??, f?
?I10
I10 ! I10?e, ??
I10 ! I00 I10 ! I00?e, ??
?
I00 ! N+11N00 I
I
N00
N11
N11
I00 ! N11I
I ! N11I
I ! N00
I00
I00 I10 I10
I11
I11
(d) Inverted Domain with Null Rules
Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar
without nulls (presented in Wu (1997) and Zens and Ney (2003)). In (c) and (d), we present a normal form grammar
that accounts for null alignments.
which to attach null alignments to a bitext cell;
we address this by adapting the grammar to force
a null attachment order. We introduce symbols
N00, N10, and N11 to represent whether a normal
cell has taken no nulls, is accepting foreign nulls,
or is accepting English nulls, respectively. We also
introduce symbols I00, I10, and I11 to represent
inverse cells at analogous stages of taking nulls.
As Figures 2 (c) and (d) illustrate, the directions
in which nulls are attached to normal and inverse
cells differ. The N00 symbol is constructed by
one or more ?complete? inverted cells I11 termi-
nated by a no-null I00. By placing I00 in the lower
right hand corner, we allow the larger N00 to un-
ambiguously attach nulls. N00 transitions to the
N10 symbol and accepts any number of ?e, ?? En-
glish terminal alignments. Then N10 transitions to
N11 and accepts any number of ??, f? foreign ter-
minal alignments. An analogous set of grammar
rules exists for the inverted case (see Figure 2(d)
for an illustration). Given this normal form, we
can efficiently compute model expectations over
ITG alignments without double counting.5 To our
knowledge, the alteration of the normal form to
accommodate null emissions is novel to this work.
5The complete grammar adds sentinel symbols to the up-
per left and lower right, and the root symbol is constrained to
be a N00.
4.2 Relaxing the Single Target Assumption
A crucial obstacle for using the likelihood objec-
tive is that a given a? may not be in the alignment
family. As in our alteration to MIRA (Section 3),
we could replace a? with a minimal loss in-class
alignment a?p. However, in contrast to MIRA, the
likelihood objective will implicitly penalize pro-
posed alignments which have loss equal to a?p. We
opt instead to maximize the probability of the set
of alignmentsM(a?) which achieve the same op-
timal in-class loss. Concretely, let m? be the min-
imal loss achievable relative to a? in A. Then,
M(a?) = {a ? A|L(a,a?) = m?}
When a? is an ITG alignment (i.e., m? is 0),
M(a?) consists only of alignments which have all
the sure alignments in a?, but may have some sub-
set of the possible alignments in a?. See Figure 3
for a specific example where m? = 1.
Our modified likelihood objective is given by,
max
w
?
(x,a?)?D
log ?
a?M(a?)
Pw(a|x)
Note that this objective is no longer convex, as it
involves a logarithm of a summation, however we
still utilize gradient-based optimization. Summing
and obtaining feature expectations over M(a?)
can be done efficiently using a constrained variant
927
MIRA Likelihood
1-1 ITG ITG-S ITG-N
Features P R AER P R AER P R AER P R AER
Dice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6
+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4
+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1
Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows
indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar
(Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.
T
h
a
t
i
s
n
o
t
g
o
o
d
e
n
o
u
g
h
 Se
ne
est
pas
 suffisant
a?Gold Alignment Target AlignmentsM(a?)
Figure 3: Often, the gold alignment a? isn?t in our
alignment family, here ABITG. For the likelihood ob-
jective (Section 4.2), we maximize the probability of
the setM(a?) consisting of alignments ABITG which
achieve minimal loss relative to a?. In this example,
the minimal loss is 1, and we have a choice of remov-
ing either of the sure alignments to the English word
not. We also have the choice of whether to include the
possible alignment, yielding 4 alignments inM(a?).
of the inside-outside algorithm where sure align-
ments not present in a? are disallowed, and the
number of missing sure alignments is appended to
the state of the bitext cell.6
One advantage of the likelihood-based objec-
tive is that we can obtain posteriors over individual
alignment cells,
Pw((i, j)|x) =
?
a?A:(i,j)?a
Pw(a|x)
We obtain posterior ITG alignments by including
all alignment cells (i, j) such that Pw((i, j)|x) ex-
ceeds a fixed threshold t. Posterior thresholding
allows us to easily trade-off precision and recall in
our alignments by raising or lowering t.
5 Dynamic Program Pruning
Both discriminative methods require repeated
model inference: MIRA depends upon loss-
augmented Viterbi parsing, while conditional like-
6Note that alignments that achieve the minimal loss would
not introduce any alignments not either sure or possible, so it
suffices to keep track only of the number of sure recall errors.
lihood uses the inside-outside algorithm for com-
puting cell posteriors. Exhaustive computation
of these quantities requires an O(n6) dynamic
program that is prohibitively slow even on small
supervised training sets. However, most of the
search space can safely be pruned using posterior
predictions from a simpler alignment models. We
use posteriors from two jointly estimated HMM
models to make pruning decisions during ITG in-
ference (Liang et al, 2006). Our first pruning tech-
nique is broadly similar to Cherry and Lin (2007a).
We select high-precision alignment links from the
HMM models: those word pairs that have a pos-
terior greater than 0.9 in either model. Then, we
prune all bitext cells that would invalidate more
than 8 of these high-precision alignments.
Our second pruning technique is to prune all
one-by-one (word-to-word) bitext cells that have a
posterior below 10?4 in both HMM models. Prun-
ing a one-by-one cell also indirectly prunes larger
cells containing it. To take maximal advantage of
this indirect pruning, we avoid explicitly attempt-
ing to build each cell in the dynamic program. In-
stead, we track bounds on the spans for which we
have successfully built ITG cells, and we only iter-
ate over larger spans that fall within those bounds.
The details of a similar bounding approach appear
in DeNero et al (2009).
In all, pruning reduces MIRA iteration time
from 175 to 5 minutes on the NIST Chinese-
English dataset with negligible performance loss.
Likelihood training time is reduced by nearly two
orders of magnitude.
6 Alignment Quality Experiments
We present results which measure the quality of
our models on two hand-aligned data sets. Our
first is the English-French Hansards data set from
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003). Here we use the same 337/100
train/test split of the labeled data as Taskar et al
928
MIRA Likelihood
1-1 ITG BITG BITG-S BITG-N
Features P R AER P R AER P R AER P R AER P R AER
Dice, dist,
blcks, dict, lex 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1
+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4
Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment
family. The first row represents our best model without external alignment models and the second row includes
features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N
uses the normal form grammar (Section 4.1).
(2005); we compute external features from the
same unlabeled data, 1.1 million sentence pairs.
Our second is the Chinese-English hand-aligned
portion of the 2002 NIST MT evaluation set. This
dataset has 491 sentences, which we split into a
training set of 150 and a test set of 191. When we
trained external Chinese models, we used the same
unlabeled data set as DeNero and Klein (2007), in-
cluding the bilingual dictionary.
For likelihood based models, we set the L2 reg-
ularization parameter, ?2, to 100 and the thresh-
old for posterior decoding to 0.33. We report re-
sults using the simple ITG grammar (ITG-S, Sec-
tion 2.2) where summing over derivations dou-
ble counts alignments, as well as the normal form
ITG grammar (ITG-N,Section 4.1) which does
not double count. We ran our annealed loss-
augmented MIRA for 15 iterations, beginning
with ? at 0 and increasing it linearly to 0.5. We
compute Viterbi alignments using the averaged
weight vector from this procedure.
6.1 French Hansards Results
The French Hansards data are well-studied data
sets for discriminative word alignment (Taskar et
al., 2005; Cherry and Lin, 2006; Lacoste-Julien
et al, 2006). For this data set, it is not clear
that improving alignment error rate beyond that of
GIZA++ is useful for translation (Ganchev et al,
2008). Table 1 illustrates results for the Hansards
data set. The first row uses dice and the same dis-
tance features as Taskar et al (2005). The first
two rows repeat the experiments of Taskar et al
(2005) and Cherry and Lin (2006), but adding ITG
models that are trained to maximize conditional
likelihood. The last row includes the posterior of
the jointly-trained HMM of Liang et al (2006)
as a feature. This model alone achieves an AER
of 5.4. No model significantly improves over the
HMM alone, which is consistent with the results
of Taskar et al (2005).
6.2 Chinese NIST Results
Chinese-English alignment is a much harder task
than French-English alignment. For example, the
HMM aligner achieves an AER of 20.7 when us-
ing the competitive thresholding heuristic of DeN-
ero and Klein (2007). On this data set, our block
ITG models make substantial performance im-
provements over the HMM, and moreover these
results do translate into downstream improve-
ments in BLEU score for the Chinese-English lan-
guage pair. Because of this, we will briefly de-
scribe the features used for these models in de-
tail. For features on one-by-one cells, we con-
sider Dice, the distance features from (Taskar et
al., 2005), dictionary features, and features for the
50 most frequent lexical pairs. We also trained an
HMM aligner as described in DeNero and Klein
(2007) and used the posteriors of this model as fea-
tures. The first two columns of Table 2 illustrate
these features for ITG and one-to-one matchings.
For our block ITG models, we include all of
these features, along with variants designed for
many-to-one blocks. For example, we include the
average Dice of all the cells in a block. In addi-
tion, we also created three new block-specific fea-
tures types. The first type comprises bias features
for each block length. The second type comprises
features computed from N-gram statistics gathered
from a large monolingual corpus. These include
features such as the number of occurrences of the
phrasal (multi-word) side of a many-to-one block,
as well as pointwise mutual information statistics
for the multi-word parts of many-to-one blocks.
These features capture roughly how ?coherent? the
multi-word side of a block is.
The final block feature type consists of phrase
shape features. These are designed as follows: For
each word in a potential many-to-one block align-
ment, we map an individual word to X if it is not
one of the 25 most frequent words. Some example
features of this type are,
929
? English Block: [the X, X], [in X of, X]
? Chinese Block: [ X, X] [X|, X]
For English blocks, for example, these features
capture the behavior of phrases such as in spite
of or in front of that are rendered as one word in
Chinese. For Chinese blocks, these features cap-
ture the behavior of phrases containing classifier
phrases like? orP, which are rendered as
English indefinite determiners.
The right-hand three columns in Table 2 present
supervised results on our Chinese English data set
using block features. We note that almost all of
our performance gains (relative to both the HMM
and 1-1 matchings) come from BITG and block
features. The maximum likelihood-trained nor-
mal form ITG model outperforms the HMM, even
without including any features derived from the
unlabeled data. Once we include the posteriors
of the HMM as a feature, the AER decreases to
14.4. The previous best AER result on this data set
is 15.9 from Ayan and Dorr (2006), who trained
stacked neural networks based on GIZA++ align-
ments. Our results are not directly comparable
(they used more labeled data, but did not have the
HMM posteriors as an input feature).
6.3 End-To-End MT Experiments
We further evaluated our alignments in an end-to-
end Chinese to English translation task using the
publicly available hierarchical pipeline JosHUa
(Li and Khudanpur, 2008). The pipeline extracts
a Hiero-style synchronous context-free grammar
(Chiang, 2007), employs suffix-array based rule
extraction (Lopez, 2007), and tunes model pa-
rameters with minimum error rate training (Och,
2003). We trained on the FBIS corpus using sen-
tences up to length 40, which includes 2.7 million
English words. We used a 5-gram language model
trained on 126 million words of the Xinhua section
of the English Gigaword corpus, estimated with
SRILM (Stolcke, 2002). We tuned on 300 sen-
tences of the NIST MT04 test set.
Results on the NIST MT05 test set appear in
Table 3. We compared four sets of alignments.
The GIZA++ alignments7 are combined across di-
rections with the grow-diag-final heuristic, which
outperformed the union. The joint HMM align-
ments are generated from competitive posterior
7We used a standard training regimen: 5 iterations of
model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3
iterations of Model 4.
Alignments Translations
Model Prec Rec Rules BLEU
GIZA++ 62 84 1.9M 23.22
Joint HMM 79 77 4.0M 23.05
Viterbi ITG 90 80 3.8M 24.28
Posterior ITG 81 83 4.2M 24.32
Table 3: Results on the NIST MT05 Chinese-English
test set show that our ITG alignments yield improve-
ments in translation quality.
thresholding (DeNero and Klein, 2007). The ITG
Viterbi alignments are the Viterbi output of the
ITG model with all features, trained to maximize
log likelihood. The ITG Posterior alignments
result from applying competitive thresholding to
alignment posteriors under the ITG model. Our
supervised ITG model gave a 1.1 BLEU increase
over GIZA++.
7 Conclusion
This work presented the first large-scale applica-
tion of ITG to discriminative word alignment. We
empirically investigated the performance of con-
ditional likelihood training of ITG word aligners
under simple and normal form grammars. We
showed that through the combination of relaxed
learning objectives, many-to-one block alignment
potential, and efficient pruning, ITG models can
yield state-of-the art word alignments, even when
the underlying gold alignments are highly non-
ITG. Our models yielded the lowest published er-
ror for Chinese-English alignment and an increase
in downstream translation performance.
References
Necip Fazil Ayan and Bonnie Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In ACL.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In ACL.
Colin Cherry and Dekang Lin. 2007a. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In NAACL-HLT 2007.
Colin Cherry and Dekang Lin. 2007b. A scalable in-
version transduction grammar for joint phrasal trans-
lation modeling. In SSST Workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
930
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai S. Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In ACL Short Paper
Track.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In NAACL.
Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008.
Better alignments = better translations? In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word alignment via
quadratic assignment. In NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
SSST Workshop at ACL.
Percy Liang, Dan Klein, and Dan Klein. 2006. Align-
ment by agreement. In NAACL-HLT.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT/NAACL
Workshop on Building and Using Parallel Texts.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In ACL-COLING.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Empirical Methods in Nat-
ural Language Processing.
Andreas Stolcke. 2002. Srilm: An extensible language
modeling toolkit. In ICSLP 2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In NAACL-HLT.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Dan Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment.
In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
931
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 387?394, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Textual Inference via Graph Matching
Aria D. Haghighi
Dept. of Computer Science
Stanford University
Stanford, CA
aria42@stanford.edu
Andrew Y. Ng
Dept. of Computer Science
Stanford University
Stanford, CA
ang@cs.stanford.edu
Christopher D. Manning
Dept. of Computer Science
Stanford University
Stanford, CA
manning@cs.stanford.edu
Abstract
We present a system for deciding whether
a given sentence can be inferred from
text. Each sentence is represented as a
directed graph (extracted from a depen-
dency parser) in which the nodes repre-
sent words or phrases, and the links repre-
sent syntactic and semantic relationships.
We develop a learned graph matching ap-
proach to approximate entailment using
the amount of the sentence?s semantic
content which is contained in the text. We
present results on the Recognizing Textual
Entailment dataset (Dagan et al, 2005),
and show that our approach outperforms
Bag-Of-Words and TF-IDF models. In ad-
dition, we explore common sources of er-
rors in our approach and how to remedy
them.
1 Introduction
A fundamental stumbling block for several NLP ap-
plications is the lack of robust and accurate seman-
tic inference. For instance, question answering sys-
tems must be able to recognize, or infer, an answer
which may be expressed differently from the query.
Information extraction systems must also be able
recognize the variability of equivalent linguistic ex-
pressions. Document summarization systems must
generate succinct sentences which express the same
content as the original document. In Machine Trans-
lation evaluation, we must be able to recognize legit-
imate translations which structurally differ from our
reference translation.
One sub-task underlying these applications is the
ability to recognize semantic entailment; whether
one piece of text follows from another. In contrast
to recent work which has successfully utilized logic-
based abductive approaches to inference (Moldovan
et al, 2003; Raina et al, 2005b), we adopt a graph-
based representation of sentences, and use graph
matching approach to measure the semantic over-
lap of text. Graph matching techniques have proven
to be a useful approach for tractable approximate
matching in other domains including computer vi-
sion. In the domain of language, graphs provide
a natural way to express the dependencies between
words and phrases in a sentence. Furthermore,
graph matching also has the advantage of providing
a framework for structural matching of phrases that
would be difficult to resolve at the level of individual
words.
2 Task Definition and Data
We describe our approach in the context of the 2005
Recognizing Textual Entailment (RTE) Challenge
(Dagan et al, 2005), but note that our approach eas-
ily extends to other related inference tasks. The sys-
tem presented here was one component of our re-
search group?s 2005 RTE submission (Raina et al,
2005a) which was the top-ranking system according
to one of the two evaluation metrics.
In the 2005 RTE domain, we are given a set of
pairs, each consisting of two parts: 1) the text, a
387
SNP-Bezos
NNP
Bezos
VP-established
VBD
established
NP-company
DT
a
NN
company
Bezos
(person)
company
(organization)
establish
(VBD)
Subj (Agent) Obj (Patient)
Figure 1: An example parse tree and the correspond-
ing dependency graph. Each phrase of the parse tree
is annotated with its head word, and the parentheti-
cal edge labels in the dependency graph correspond
to semantic roles.
small passage,1 and the hypothesis, a single sen-
tence. Our task is to decide if the hypothesis is ?en-
tailed? by the text. Here, ?entails? does not mean
strict logical implication, but roughly means that
a competent speaker with basic world-knowledge
would be happy to conclude the hypothesis given the
text. This criterion has an aspect of relevance logic
as opposed to material implication: while various
additional background information may be needed
for the hypothesis to follow, the text must substan-
tially support the hypothesis.
Despite the informality of the criterion and the
fact that the available world knowledge is left
unspecified, human judges show extremely good
agreement on this task ? 3 human judges indepen-
dent of the organizers calculated agreement rates
with the released data set ranging from 91?96% (Da-
gan et al, 2005). We believe that this in part reflects
that the task is fairly natural to human beings. For
a flavor of the nature (and difficulty) of the task, see
Table 1.
We give results on the data provided for the RTE
task which consists of 567 development pairs and
800 test pairs. In both sets the pairs are divided into
7 tasks ? each containing roughly the same number
of entailed and not-entailed instances ? which were
used as both motivation and means for obtaining and
constructing the data items. We will use the follow-
ing toy example to illustrate our representation and
matching technique:
Text: In 1994, Amazon.com was founded by Jeff Bezos.
Hypothesis: Bezos established a company.
1Usually a single sentence, but occasionally longer.
3 Semantic Representation
3.1 The Need for Dependencies
Perhaps the most common representation of text for
assessing content is ?Bag-Of-Words? or ?Bag-of-N-
Grams? (Papineni et al, 2002). However, such rep-
resentations lose syntactic information which can
be essential to determining entailment. Consider a
Question Answer system searching for an answer
to When was Israel established? A representation
which did not utilize syntax would probably enthusi-
astically return an answer from (the 2005 RTE text):
The National Institute for Psychobiology in Israel
was established in 1979.
In this example, it?s important to try to match rela-
tionships as well as words. In particular, any answer
to the question should preserve the dependency be-
tween Israel and established. However, in the pro-
posed answer, the expected dependency is missing
although all the words are present.
Our approach is to view sentences as graphs be-
tween words and phrases, where dependency rela-
tionships, as in (Lin and Pantel, 2001), are charac-
terized by the path between vertices.
Given this representation, we judge entailment by
measuring not only how many of the hypothesis ver-
tices are matched to the text but also how well the
relationships between vertices in the hypothesis are
preserved in their textual counterparts. For the re-
mainder of the section we outline how we produce
graphs from text, and in the next section we intro-
duce our graph matching model.
3.2 From Text To Graphs
Starting with raw English text, we use a version of
the parser described in (Klein and Manning, 2003),
to obtain a parse tree. Then, we derive a dependency
tree representation of the sentence using a slightly
modified version of Collins? head propagation rules
(Collins, 1999), which make main verbs not auxil-
iaries the head of sentences. Edges in the depen-
dency graph are labeled by a set of hand-created
tgrep expressions. These labels represent ?sur-
face? syntax relationships such as subj for subject
and amod for adjective modifier, similar to the rela-
tions in Minipar (Lin and Pantel, 2001). The depen-
dency graph is the basis for our graphical represen-
tation, but it is enhanced in the following ways:
388
Task Text Hypothesis Entailed
Question An-
swer (QA)
Prince Charles was previously married to Princess
Diana, who died in a car crash in Paris in August
1997.
Prince Charles and Princess Diana got
married in August 1997.
False
Machine
Translation
(MT)
Sultan Al-Shawi, a.k.a the Attorney, said during a
funeral held for the victims, ?They were all chil-
dren of Iraq killed during the savage bombing.?.
The Attorney, said at the funeral, ?They
were all Iraqis killed during the brutal
shelling.?.
True
Comparable
Documents
(CD)
Napster, which started as an unauthorized song-
swapping Web site, has transformed into a legal
service offering music downloads for a monthly
fee.
Napster illegally offers music down-
loads.
False
Paraphrase
Recognition
(PP)
Kerry hit Bush hard on his conduct on the war in
Iraq.
Kerry shot Bush. False
Information
Retrieval (IR)
The country?s largest private employer, Wal-Mart
Stores Inc., is being sued by a number of its female
employees who claim they were kept out of jobs in
management because they are women.
Wal-Mart sued for sexual discrimina-
tion.
True
Table 1: Some Textual Entailment examples. The last three demonstrate some of the harder instances.
1. Collapse Collocations and Named-Entities: We
?collapse? dependency nodes which represent
named entities (e.g., Jeff Bezos in Figure fig-
example) and also collocations listed in Word-
Net, including verbs and their adjacent particles
(e.g., blow off in He blew off his work) .
2. Dependency Folding: As in (Lin and Pan-
tel, 2001), we found it useful to fold cer-
tain dependencies (such as modifying preposi-
tions) so that modifiers became labels connect-
ing the modifier?s governor and dependent di-
rectly. For instance, in the text graph in Figure
2, we have changed in from a word into a rela-
tion between its head verb and the head of its
NP complement.
3. Semantic Role Labeling: We also augment
the graph representation with Probank-style
semantic roles via the system described in
(Toutanova et al, 2005). Each predicate adds
an arc labeled with the appropriate seman-
tic role to the head of the argument phrase.
This helps to create links between words which
share a deep semantic relation not evident in
the surface syntax. Additionally, modifying
phrases are labeled with their semantic types
(e.g., in 1991 is linked by a Temporal edge in
the text graph of Figure 2), which should be
useful in Question Answering tasks.
4. Coreference Links: Using a co-rereference res-
olution tagger, coref links are added through-
out the graph. These links allowed connecting
the referent entity to the vertices of the referring
vertex. In the case of multiple sentence texts, it
is our only ?link? in the graph between entities
in the two sentences.
For the remainder of the paper, we will refer to
the text as T and hypothesis as H , and will speak
of them in graph terminology. In addition we will
use HV and HE to denote the vertices and edges,
respectively, of H .
4 Entailment by Graph Matching
We take the view that a hypothesis is entailed from
the text when the cost of matching the hypothesis
graph to the text graph is low. For the remainder of
this section, we outline a general model for assign-
ing a match cost to graphs.
For hypothesis graph H , and text graph T , a
matching M is a mapping from the vertices of H to
those of T . For vertex v in H , we will use M(v) to
denote its ?match? in T . As is common in statistical
machine translation, we allow nodes in H to map to
fictitious NULL vertices in T if necessary. Suppose
the cost of matching M is Cost(M). If M is the set
of such matchings, we define the cost of matching
H to T to be
MatchCost(H,T ) = min
M?M
Cost(M) (1)
Suppose we have a model, VertexSub(v,M(v)),
which gives us a cost in [0, 1], for substituting ver-
tex v in H for M(v) in T . One natural cost model
389
is to use the normalized cost for each of the vertex
substitutions in M :
VertexCost(M) = 1Z
?
v?HV
w(v)VertexSub(v,M(v))
(2)
Here, w(v) represents the weight or relative im-
portance for vertex v, and Z = ?v?HV w(v) is
a normalization constant. In our implementation,
the weight of each vertex was based on the part-of-
speech tag of the word or the type of named entity,
if applicable. However, there are several other pos-
sibilities including using TF-IDF weights for words
and phrases.
Notice that when Cost(M) takes the form of
(2), computing MatchCost(H,T ) is equivalent to
finding the minimal cost bipartite graph-matching,
which can be efficiently computed using linear pro-
gramming.
We would like our cost-model to incorporate
some measure of how relationships in H are pre-
served in T under M . Ideally, a matching should
preserve all local relationships; i.e, if v ? v? ? HE ,
then M(v) ? M(v?) ? TE . When this condition
holds for all edges in H , H is isomorphic to a sub-
graph of T .
What we would like is an approximate notion of
isomorphism, where we penalize the distortion of
each edge relation in H . Consider an edge e =
(v, v?) ? HE , and let ?M (e) be the path from M(v)
to M(v?) in T .
Again, suppose we have a model,
PathSub(e, ?M (e)) for assessing the ?cost? of
substituting a direct relation e ? HE for its coun-
terpart, ?M (e), under the matching. This leads to
a formulation similar to (2), where we consider the
normalized cost of substituting each edge relation
in H with a path in T :
RelationCost(M) = 1Z
?
e?HE
w(e)PathSub(e, ?M (e))
(3)
where Z = ?e?HE w(e) is a normalization con-
stant. As in the vertex case, we have weights
for each hypothesis edge, w(e), based upon the
edge?s label; typically subject and object relations
are more important to match than others. Our fi-
nal matching cost is given by a convex mixture of
Subj (Agent)
establish
(VBD)
Bezos
(person)
Company
(organization)
Obj (Patient) 
Subj (Agent)
found
(VBD)
Jeff Bezos
(person)
Amazon.com
(organization)
Obj (Patient)
In (Temporal)
1991
(date)
Synonym 
Match
Cost: 0.4
Hyponym
Match
Cost: 0.0
Exact
Match
Cost: 0.0
Vertex Cost: (0.0 + 0.2 + 0.4)/3 = 0.2
Relation Cost: 0  (Graphs Isomorphic)  
Match Cost: 0.55 (0.2) + (.45) 0.0 = 0.11
Figure 2: Example graph matching (? = 0.55) for
example pair. Dashed lines represent optimal match-
ing.
the vertex and relational match costs: Cost(M) =
?VertexCost(M) + (1 ? ?)RelationCost(M).
Notice that minimizing Cost(M) is computa-
tionally hard since if our PathSub model as-
signs zero cost only for preserving edges, then
RelationCost(M) = 0 if and only if H is isomorphic
to a subgraph of T . Since subgraph isomophism is
an NP-complete problem, we cannot hope to have an
efficient exact procedure for minimizing the graph
matching cost. As an approximation, we can ef-
ficiently find the matching M? which minimizes
VertexCost(?); we then perform local greedy hill-
climbing search, beginning from M?, to approxi-
mate the minimal matching. The allowed operations
are changing the assignment of any hypothesis ver-
tex to a text one, and, to avoid ridges, swapping two
hypothesis assignments
5 Node and Edge Substitution Models
In the previous section we described our graph
matching model in terms of our VertexSub model,
which gives a cost for substituting one graph vertex
for another, and PathSub, which gives a cost for sub-
stituting the path relationship between two paths in
one graph for that in another. We now outline these
models.
5.1 Vertex substitution cost model
Our VertexSub(v,M(v)) model is based upon a
sliding scale, where progressively higher costs are
390
given based upon the following conditions:
? Exact Match: v and M(v) are identical words/
phrases.
? Stem Match: v and M(v)?s stems match or one
is a derivational form of the other; e.g., matching
coaches to coach.
? Synonym Match: v and M(v) are synonyms ac-
cording to WordNet (Fellbaum, 1998). In particu-
lar we use the top 3 senses of both words to deter-
mine synsets.
? Hypernym Match: v is a ?kind of? M(v), as
determined by WordNet. Note that this feature is
asymmetric.
? WordNet Similarity: v and M(v) are similar ac-
cording to WordNet::Similarity (Peder-
sen et al, 2004). In particular, we use the measure
described in (Resnik, 1995). We found it useful
to only use similarities above a fixed threshold to
ensure precision.
? LSA Match: v and M(v) are distributionally
similar according to a freely available Latent Se-
mantic Indexing package,2 or for verbs similar
according to VerbOcean (Chklovski and Pantel,
2004).
? POS Match: v and M(v) have the same part of
speech.
? No Match: M(v) is NULL.
Although the above conditions often produce rea-
sonable matchings between text and hypothesis, we
found the recall of these lexical resources to be far
from adequate. More robust lexical resources would
almost certainly boost performance.
5.2 Path substitution cost model
Our PathSub(v ? v?,M(v) ? M(v?)) model is
also based upon a sliding scale cost based upon the
following conditions:
? Exact Match: M(v) ? M(v?) is an en edge in
T with the same label.
? Partial Match: M(v) ? M(v?) is an en edge in
T , not necessarily with the same label.
? Ancestor Match: M(v) is an ancestor of M(v?).
We use an exponentially increasing cost for longer
distance relationships.
2Available at http://infomap.stanford.edu
? Kinked Match: M(v) and M(v?) share a com-
mon parent or ancestor in T . We use an exponen-
tially increasing cost based on the maximum of
the node?s distances to their least common ances-
tor in T .
These conditions capture many of the common
ways in which relationships between entities are dis-
torted in semantically related sentences. For in-
stance, in our system, a partial match will occur
whenever an edge type differs in detail, for instance
use of the preposition towards in one case and to in
the other. An ancestor match will occur whenever an
indirect relation leads to the insertion of an interven-
ing node in the dependency graph, such as matching
John is studying French farming vs. John is studying
French farming practices.
5.3 Learning Weights
Is it possible to learn weights for the relative impor-
tance of the conditions in the VertexSub and PathSub
models? Consider the case where match costs are
given only by equation (2) and vertices are weighted
uniformly (w(v) = 1). Suppose that ?(v,M(v))
is a vector of features3 indicating the cost accord-
ing to each of the conditions listed for matching v
to M(v). Also let w be weights for each element
of ?(v,M(v)). First we can model the substitution
cost for a given matching as:
VertexSub(v,M(v)) = exp (w
T ?(v,M(v)))
1 + exp (wT ?(v,M(v)))
Letting s(?) be the 1-sigmoid function used in the
right hand side of the equation above, our final
matching cost as a function of w is given by
c(H,T ;w) = min
M?M
1
|HV |
?
v?H
s(wT ?(v,M(v)))
(4)
Suppose we have a set of text/hypothesis pairs,
{(T (1),H(1)), . . . , (T (n),H(n))}, with labels y(i)
which are 1 if H(i) is entailed by T (i) and 0
otherwise. Then we would like to choose w to
minimize costs for entailed examples and maximize
it for non-entailed pairs:
3In the case of our ?match? conditions, these features will
be binary.
391
?(w) =
?
i:y(i)=1
log c(H(i), T (i);w) +
?
i:y(i)=0
log(1 ? c(H(i), T (i);w))
Unfortunately, ?(w) is not a convex function. No-
tice that the cost of each matching, M , implicitly
depends on the current setting of the weights w. It
can be shown that since each c(H,T ;w) involves
minimizing M ? M, which depends on w, it is not
convex. Therefore, we can?t hope to globally opti-
mize our cost functions over w and must settle for
an approximation.
One approach is to use coordinate ascent over M
and w. Suppose that we begin with arbitrary weights
and given these weights choose M (i) to minimize
each c(H(i), T (i);w). Then we use a relaxed form of
the cost function where we use the matchings found
in the last step:
c?(H(i), T (i);w) = 1|HV |
?
v?H
s(wT?(v,M (i)(v)))
Then we maximize w with respect to ?(w) with
each c(?) replaced with the cost-function c?(?). This
step involves only logistic regression. We repeat this
procedure until our weights converge.
To test the effectiveness of the above procedure
we compared performance against baseline settings
using a random split on the development set. Picking
each weight uniformly at random resulted in 53%
accuracy. Setting all weights identically to an arbi-
trary value gave 54%. The procedure above, where
the weights are initialized to the same value, resulted
in an accuracy of 57%. However, we believe there
is still room for improvement since carefully-hand
chosen weights results in comparable performance
to the learned weights on the final test set. We be-
lieve this setting of learning under matchings is a
rather general one and could be beneficial to other
domains such as Machine Translation. In the future,
we hope to find better approximation techniques for
this problem.
6 Checks
One systematic source of error coming from our ba-
sic approach is the implicit assumption of upwards
monotonicity of entailment; i.e., if T entails H then
adding more words to T should also give us a sen-
tence which entails H . This assumption, also made
by other recent abductive approaches (Moldovan et
al., 2003), does not hold for several classes of exam-
ples. Our formalism does not at present provide a
general solution to this issue, but we include special
case handling of the most common types of cases,
which we outline below.4 These checks are done af-
ter graph matching and assume we have stored the
minimal cost matching.
Negation Check
Text: Clinton?s book is not a bestseller
Hypothesis: Clinton?s book is a bestseller
To catch such examples, we check that each hy-
pothesis verb is not matched to a text word which
is negated (unless the verb pairs are antonyms) and
vice versa. In this instance, the is in H , denoted by
isH , is matched to isT which has a negation modifier,
notT , absent for isH . So the negation check fails.
Factive Check
Text: Clonaid claims to have cloned 13 babies worldwide.
Hypothesis: Clonaid has cloned 13 babies.
Non-factive verbs (claim, think, charged, etc.) in
contrast to factive verbs (know, regret, etc.) have
sentential complements which do not represent true
propositions. We detect such cases, by checking that
each verb in H that is matched in T does not have a
non-factive verb for a parent.
Superlative Check
Text: The Osaka World Trade Center is the tallest building in
Western Japan.
Hypothesis: The Osaka World Trade Center is the tallest build-
ing in Japan.
In general, superlative modifiers (most, biggest,
etc.) invert the typical monotonicity of entailment
and must be handled as special cases. For any
noun n with a superlative modifier (part-of-speech
JJS) in H , we must ensure that all modifier relations
of M(n) are preserved in H . In this example, build-
ingH has a superlative modifier tallestH , so we must
ensure that each modifier relation of JapanT , a noun
4All the examples are actual, or slightly altered, RTE exam-
ples.
392
Method Accuracy CWS
Random 50.0% 0.500
Bag-Of-Words 49.5% 0.548
TF-IDF 51.8% 0.560
GM-General 56.8% 0.614
GM-ByTask 56.7% 0.620
Table 2: Accuracy and confidence weighted score
(CWS) for test set using various techniques.
dependent of buildingT , has a WesternT modifier not
in H . So its fails the superlative check.
Additionally, during error analysis on the devel-
opment set, we spotted the following cases where
our VertexSub function erroneously labeled vertices
as similar, and required special case consideration:
? Antonym Check: We consistently found that the
WordNet::Similarity modules gave high-
similarity to antonyms.5 We explicitly check
whether a matching involved antonyms and reject
unless one of the vertices had a negation modifier.
? Numeric Mismatch: Since numeric expressions
typically have the same part-of-speech tag (CD),
they were typically matched when exact matches
could not be found. However, mismatching nu-
merical tokens usually indicated that H was not
entailed, and so pairs with a numerical mismatch
were rejected.
7 Experiments and Results
For our experiments we used the devolpement and
test sets from the Recognizing Textual Entailment
challenge (Dagan et al, 2005). We give results for
our system as well as for the following systems:
? Bag-Of-Words: We tokenize the text and hypoth-
esis and strip the function words, and stem the re-
sulting words. The cost is given by the fraction of
the hypothesis not matched in the text.
? TF-IDF: Similar to Bag-Of-Words except that
there is a tf.idf weight associated with each hy-
pothesis word so that more ?important? words are
higher weight for matching.
5This isn?t necessarily incorrect, but is simply not suitable
for textual inference.
Task GM-General GM-ByTask
Accuracy CWS Accuracy CWS
CD 72.0% 0.742 76.0% 0.771
IE 55.9% 0.583 55.8% 0.595
IR 52.2% 0.564 51.1% 0.572
MT 50.0% 0.497 43.3% 0.489
PP 58.0% 0.741 58.0% 0.746
QA 53.8% 0.537 55.4% 0.556
RC 52.1% 0.539 52.9% 0.523
Table 3: Accuracy and confidence weighted score
(CWS) split by task on the RTE test set.
We also present results for two graph matching
(GM) systems. The GM-General system fits a sin-
gle global threshold from the development set. The
GM-ByTask system fits a different threshold for
each of the tasks.
Our results are summarized in Table 2. As the re-
sult indicates, the task is particularly hard; all RTE
participants scored between 50% and 60% in terms
of overall accuracy (Dagan et al, 2005). Nevever-
theless, both GM systems perform better than either
Bag-Of-Words or TF-IDF. CWS refers to Confi-
dence Weighted Score (also known as average pre-
cision). This measure is perhaps a more insightful
measure, since it allows the inclusion of a ranking
of answers by confidence and assesses whether you
are correct on the pairs that you are most confident
that you know the answer to. To assess CWS, our
n answers are sorted in decreasing order by the con-
fidence we return, and then for each i, we calculate
ai, our accuracy on our i most confident predictions.
Then CWS = 1n
?n
i=1 ai.
We also present results on a per-task basis in Ta-
ble 3. Interestingly, there is a large variation in per-
formance depending on the task.
8 Conclusion
We have presented a learned graph matching ap-
proach to approximating textual entailment which
outperforms models which only match at the word
level, and is competitive with recent weighed ab-
duction models (Moldovan et al, 2003). In addition,
we explore problematic cases of nonmonotonicity in
entailment, which are not naturally handled by ei-
ther subgraph matching or the so-called ?logic form?
393
Text Hypothesis True Ans. Our Ans. Conf Comments
A Filipino hostage in Iraq was re-
leased.
A Filipino hostage
was freed in Iraq.
True True 0.84 Verb rewrite is handled.
Phrasal ordering does not
affect cost.
The government announced last
week that it plans to raise oil
prices.
Oil prices drop. False False 0.95 High cost given for substituting
word for its antonym.
Shrek 2 rang up $92 million. Shrek 2 earned $92
million.
True False 0.59 Collocation ?rang up? is
not known to be similar to
?earned?.
Sonia Gandhi can be defeated in
the next elections in India by BJP.
Sonia Gandhi is de-
feated by BJP.
False True 0.77 ?can be? does not indicate the
complement event occurs.
Fighters loyal to Moqtada al-Sadr
shot down a U.S. helicopter Thurs-
day in the holy city of Najaf.
Fighters loyal to
Moqtada al-Sadr
shot down Najaf.
False True 0.67 Should recognize non-Location
cannot be substituted for Loca-
tion.
C and D Technologies announced
that it has closed the acquisition of
Datel, Inc.
Datel Acquired C
and D technologies.
False True 0.64 Failed to penalize switch in se-
mantic role structure enough
Table 4: Analysis of results on some RTE examples along with out guesses and confidence probabilities
inference of (Moldovan et al, 2003) and have pro-
posed a way to capture common cases of this phe-
nomenon. We believe that the methods employed
in this work show much potential for improving the
state-of-the-art in computational semantic inference.
9 Acknowledgments
Many thanks to Rajat Raina, Christopher Cox,
Kristina Toutanova, Jenny Finkel, Marie-Catherine
de Marneffe, and Bill MacCartney for providing us
with linguistic modules and useful discussions. This
work was supported by the Advanced Research and
Development Activity (ARDA)?s Advanced Ques-
tion Answering for Intelligence (AQUAINT) pro-
gram.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In EMNLP.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognizing textual entailment
challenge. In Proceedings of the PASCAL Challenges
Workshop Recognizing Textual Entailment.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323?328.
Dan I. Moldovan, Christine Clark, Sanda M. Harabagiu,
and Steven J. Maiorano. 2003. Cogex: A logic prover
for question answering. In HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL.
Ted Pedersen, Siddharth Parwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity ? measuring the relat-
edness of concepts. In AAAI.
Rajat Raina, Aria Haghighi, Christopher Cox, Jenny
Finkel, Jeff Michels, Kristina Toutanova, Bill Mac-
Cartney, Marie-Catherine de Marneffe, Christopher D.
Manning, and Andrew Y. Ng. 2005a. Robust textual
inference using diverse knowledge sources. In Pro-
ceedings of the First PASCAL Challenges Workshop.
Southampton, UK.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005b. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In IJCAI, pages
448?453.
Kristina Toutanova, Aria Haghighi, and Cristiopher Man-
ning. 2005. Joint learning improves semantic role la-
beling. In Association of Computational Linguistics
(ACL).
394
Proceedings of the 43rd Annual Meeting of the ACL, pages 589?596,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Joint Learning Improves Semantic Role Labeling
Kristina Toutanova
Dept of Computer Science
Stanford University
Stanford, CA, 94305
kristina@cs.stanford.edu
Aria Haghighi
Dept of Computer Science
Stanford University
Stanford, CA, 94305
aria42@stanford.edu
Christopher D. Manning
Dept of Computer Science
Stanford University
Stanford, CA, 94305
manning@cs.stanford.edu
Abstract
Despite much recent progress on accu-
rate semantic role labeling, previous work
has largely used independent classifiers,
possibly combined with separate label se-
quence models via Viterbi decoding. This
stands in stark contrast to the linguistic
observation that a core argument frame is
a joint structure, with strong dependen-
cies between arguments. We show how to
build a joint model of argument frames,
incorporating novel features that model
these interactions into discriminative log-
linear models. This system achieves an
error reduction of 22% on all arguments
and 32% on core arguments over a state-
of-the art independent classifier for gold-
standard parse trees on PropBank.
1 Introduction
The release of semantically annotated corpora such
as FrameNet (Baker et al, 1998) and PropBank
(Palmer et al, 2003) has made it possible to develop
high-accuracy statistical models for automated se-
mantic role labeling (Gildea and Jurafsky, 2002;
Pradhan et al, 2004; Xue and Palmer, 2004). Such
systems have identified several linguistically mo-
tivated features for discriminating arguments and
their labels (see Table 1). These features usually
characterize aspects of individual arguments and the
predicate.
It is evident that the labels and the features of ar-
guments are highly correlated. For example, there
are hard constraints ? that arguments cannot overlap
with each other or the predicate, and also soft con-
straints ? for example, is it unlikely that a predicate
will have two or more AGENT arguments, or that a
predicate used in the active voice will have a THEME
argument prior to an AGENT argument. Several sys-
tems have incorporated such dependencies, for ex-
ample, (Gildea and Jurafsky, 2002; Pradhan et al,
2004; Thompson et al, 2003) and several systems
submitted in the CoNLL-2004 shared task (Carreras
and Ma`rquez, 2004). However, we show that there
are greater gains to be had by modeling joint infor-
mation about a verb?s argument structure.
We propose a discriminative log-linear joint
model for semantic role labeling, which incorpo-
rates more global features and achieves superior
performance in comparison to state-of-the-art mod-
els. To deal with the computational complexity of
the task, we employ dynamic programming and re-
ranking approaches. We present performance re-
sults on the February 2004 version of PropBank on
gold-standard parse trees as well as results on auto-
matic parses generated by Charniak?s parser (Char-
niak, 2000).
2 Semantic Role Labeling: Task Definition
and Architectures
Consider the pair of sentences,
? [The GM-Jaguar pact]AGENT gives
[the car market]RECIPIENT
[a much-needed boost]THEME
? [A much-needed boost]THEME was given to
[the car market]RECIPIENT
by [the GM-Jaguar pact]AGENT
Despite the different syntactic positions of the la-
beled phrases, we recognize that each plays the same
589
role ? indicated by the label ? in the meaning of
this sense of the verb give. We call such phrases
fillers of semantic roles and our task is, given a sen-
tence and a target verb, to return all such phrases
along with their correct labels. Therefore one sub-
task is to group the words of a sentence into phrases
or constituents. As in most previous work on se-
mantic role labeling, we assume the existence of a
separate parsing model that can assign a parse tree t
to each sentence, and the task then is to label each
node in the parse tree with the semantic role of the
phrase it dominates, or NONE, if the phrase does not
fill any role. We do stress however that the joint
framework and features proposed here can also be
used when only a shallow parse (chunked) represen-
tation is available as in the CoNLL-2004 shared task
(Carreras and Ma`rquez, 2004).
In the February 2004 version of the PropBank cor-
pus, annotations are done on top of the Penn Tree-
Bank II parse trees (Marcus et al, 1993). Possi-
ble labels of arguments in this corpus are the core
argument labels ARG[0-5], and the modifier argu-
ment labels. The core arguments ARG[3-5] do not
have consistent global roles and tend to be verb spe-
cific. There are about 14 modifier labels such as
ARGM-LOC and ARGM-TMP, for location and tem-
poral modifiers respectively.1 Figure 1 shows an ex-
ample parse tree annotated with semantic roles.
We distinguish between models that learn to la-
bel nodes in the parse tree independently, called lo-
cal models, and models that incorporate dependen-
cies among the labels of multiple nodes, called joint
models. We build both local and joint models for se-
mantic role labeling, and evaluate the gains achiev-
able by incorporating joint information. We start
by introducing our local models, and later build on
them to define joint models.
3 Local Classifiers
In the context of role labeling, we call a classifier
local if it assigns a probability (or score) to the label
of an individual parse tree node ni independently of
the labels of other nodes.
We use the standard separation of the task of se-
mantic role labeling into identification and classifi-
1For a full listing of PropBank argument labels see (Palmer
et al, 2003)
cation phases. In identification, our task is to clas-
sify nodes of t as either ARG, an argument (includ-
ing modifiers), or NONE, a non-argument. In clas-
sification, we are given a set of arguments in t and
must label each one with its appropriate semantic
role. Formally, let L denote a mapping of the nodes
in t to a label set of semantic roles (including NONE)
and let Id(L) be the mapping which collapses L?s
non-NONE values into ARG. Then we can decom-
pose the probability of a labeling L into probabili-
ties according to an identification model PID and a
classification model PCLS .
PSRL(L|t, v) = PID(Id(L)|t, v) ?
PCLS(L|t, v, Id(L)) (1)
This decomposition does not encode any indepen-
dence assumptions, but is a useful way of thinking
about the problem. Our local models for semantic
role labeling use this decomposition. Previous work
has also made this distinction because, for example,
different features have been found to be more effec-
tive for the two tasks, and it has been a good way
to make training and search during testing more ef-
ficient.
Here we use the same features for local identifi-
cation and classification models, but use the decom-
position for efficiency of training. The identification
models are trained to classify each node in a parse
tree as ARG or NONE, and the classification models
are trained to label each argument node in the train-
ing set with its specific label. In this way the train-
ing set for the classification models is smaller. Note
that we don?t do any hard pruning at the identifica-
tion stage in testing and can find the exact labeling
of the complete parse tree, which is the maximizer
of Equation 1. Thus we do not have accuracy loss
as in the two-pass hard prune strategy described in
(Pradhan et al, 2005).
In previous work, various machine learning meth-
ods have been used to learn local classifiers for role
labeling. Examples are linearly interpolated rela-
tive frequency models (Gildea and Jurafsky, 2002),
SVMs (Pradhan et al, 2004), decision trees (Sur-
deanu et al, 2003), and log-linear models (Xue and
Palmer, 2004). In this work we use log-linear mod-
els for multi-class classification. One advantage of
log-linear models over SVMs for us is that they pro-
duce probability distributions and thus identification
590
Standard Features (Gildea and Jurafsky, 2002)
PHRASE TYPE: Syntactic Category of node
PREDICATE LEMMA: Stemmed Verb
PATH: Path from node to predicate
POSITION: Before or after predicate?
VOICE: Active or passive relative to predicate
HEAD WORD OF PHRASE
SUB-CAT: CFG expansion of predicate?s parent
Additional Features (Pradhan et al, 2004)
FIRST/LAST WORD
LEFT/RIGHT SISTER PHRASE-TYPE
LEFT/RIGHT SISTER HEAD WORD/POS
PARENT PHRASE-TYPE
PARENT POS/HEAD-WORD
ORDINAL TREE DISTANCE: Phrase Type with
appended length of PATH feature
NODE-LCA PARTIAL PATH Path from constituent
to Lowest Common Ancestor with predicate node
PP PARENT HEAD WORD If parent is a PP
return parent?s head word
PP NP HEAD WORD/POS For a PP, retrieve
the head Word / POS of its rightmost NP
Selected Pairs (Xue and Palmer, 2004)
PREDICATE LEMMA & PATH
PREDICATE LEMMA & HEAD WORD
PREDICATE LEMMA & PHRASE TYPE
VOICE & POSITION
PREDICATE LEMMA & PP PARENT HEAD WORD
Table 1: Baseline Features
and classification models can be chained in a princi-
pled way, as in Equation 1.
The features we used for local identification and
classification models are outlined in Table 1. These
features are a subset of features used in previous
work. The standard features at the top of the table
were defined by (Gildea and Jurafsky, 2002), and
the rest are other useful lexical and structural fea-
tures identified in more recent work (Pradhan et al,
2004; Surdeanu et al, 2003; Xue and Palmer, 2004).
The most direct way to use trained local identifi-
cation and classification models in testing is to se-
lect a labeling L of the parse tree that maximizes
the product of the probabilities according to the two
models as in Equation 1. Since these models are lo-
cal, this is equivalent to independently maximizing
the product of the probabilities of the two models
for the label li of each parse tree node ni as shown
below in Equation 2.
P `SRL(L|t, v) =
?
ni?t
PID(Id(li)|t, v) (2)
?
?
ni?t
PCLS(li|t, v, Id(li))
A problem with this approach is that a maximizing
labeling of the nodes could possibly violate the con-
straint that argument nodes should not overlap with
each other. Therefore, to produce a consistent set of
arguments with local classifiers, we must have a way
of enforcing the non-overlapping constraint.
3.1 Enforcing the Non-overlapping Constraint
Here we describe a fast exact dynamic programming
algorithm to find the most likely non-overlapping
(consistent) labeling of all nodes in the parse tree,
according to a product of probabilities from local
models, as in Equation 2. For simplicity, we de-
scribe the dynamic program for the case where only
two classes are possible ? ARG and NONE. The gen-
eralization to more classes is straightforward. In-
tuitively, the algorithm is similar to the Viterbi al-
gorithm for context-free grammars, because we can
describe the non-overlapping constraint by a ?gram-
mar? that disallows ARG nodes to have ARG descen-
dants.
Below we will talk about maximizing the sum of
the logs of local probabilities rather than the prod-
uct of local probabilities, which is equivalent. The
dynamic program works from the leaves of the tree
up and finds a best assignment for each tree, using
already computed assignments for its children. Sup-
pose we want the most likely consistent assignment
for subtree t with children trees t1, . . . , tk each stor-
ing the most likely consistent assignment of nodes
it dominates as well as the log-probability of the as-
signment of all nodes it dominates to NONE. The
most likely assignment for t is the one that corre-
sponds to the maximum of:
? The sum of the log-probabilities of the most
likely assignments of the children subtrees
t1, . . . , tk plus the log-probability for assigning
the node t to NONE
? The sum of the log-probabilities for assign-
ing all of ti?s nodes to NONE plus the log-
probability for assigning the node t to ARG.
Propagating this procedure from the leaves to the
root of t, we have our most likely non-overlapping
assignment. By slightly modifying this procedure,
we obtain the most likely assignment according to
591
a product of local identification and classification
models. We use the local models in conjunction with
this search procedure to select a most likely labeling
in testing. Test set results for our local model P `SRL
are given in Table 2.
4 Joint Classifiers
As discussed in previous work, there are strong de-
pendencies among the labels of the semantic argu-
ment nodes of a verb. A drawback of local models
is that, when they decide the label of a parse tree
node, they cannot use information about the labels
and features of other nodes in the tree.
Furthermore, these dependencies are highly non-
local. For instance, to avoid repeating argument la-
bels in a frame, we need to add a dependency from
each node label to the labels of all other nodes.
A factorized sequence model that assumes a finite
Markov horizon, such as a chain Conditional Ran-
dom Field (Lafferty et al, 2001), would not be able
to encode such dependencies.
The need for Re-ranking
For argument identification, the number of possi-
ble assignments for a parse tree with n nodes is
2n. This number can run into the hundreds of bil-
lions for a normal-sized tree. For argument label-
ing, the number of possible assignments is ? 20m,
if m is the number of arguments of a verb (typi-
cally between 2 and 5), and 20 is the approximate
number of possible labels if considering both core
and modifying arguments. Training a model which
has such huge number of classes is infeasible if the
model does not factorize due to strong independence
assumptions. Therefore, in order to be able to in-
corporate long-range dependencies in our models,
we chose to adopt a re-ranking approach (Collins,
2000), which selects from likely assignments gener-
ated by a model which makes stronger independence
assumptions. We utilize the top N assignments of
our local semantic role labeling model P `SRL to gen-
erate likely assignments. As can be seen from Table
3, for relatively small values of N , our re-ranking
approach does not present a serious bottleneck to
performance. We used a value of N = 20 for train-
ing. In Table 3 we can see that if we could pick, us-
ing an oracle, the best assignment out for the top 20
assignments according to the local model, we would
achieve an F-Measure of 98.8 on all arguments. In-
creasing the number of N to 30 results in a very
small gain in the upper bound on performance and
a large increase in memory requirements. We there-
fore selected N = 20 as a good compromise.
Generation of top N most likely joint
assignments
We generate the top N most likely non-
overlapping joint assignments of labels to nodes in
a parse tree according to a local model P `SRL, by
an exact dynamic programming algorithm, which
is a generalization of the algorithm for finding the
top non-overlapping assignment described in section
3.1.
Parametric Models
We learn log-linear re-ranking models for joint se-
mantic role labeling, which use feature maps from a
parse tree and label sequence to a vector space. The
form of the models is as follows. Let ?(t, v, L) ?
Rs denote a feature map from a tree t, target verb
v, and joint assignment L of the nodes of the tree,
to the vector space Rs. Let L1, L2, ? ? ? , LN denote
top N possible joint assignments. We learn a log-
linear model with a parameter vector W , with one
weight for each of the s dimensions of the feature
vector. The probability (or score) of an assignment
L according to this re-ranking model is defined as:
P rSRL(L|t, v) =
e??(t,v,L),W ?
?N
j=1 e??(t,v,Lj ).W ?
(3)
The score of an assignment L not in the top N
is zero. We train the model to maximize the sum
of log-likelihoods of the best assignments minus a
quadratic regularization term.
In this framework, we can define arbitrary fea-
tures of labeled trees that capture general properties
of predicate-argument structure.
Joint Model Features
We will introduce the features of the joint re-
ranking model in the context of the example parse
tree shown in Figure 1. We model dependencies not
only between the label of a node and the labels of
592
S1
NP1-ARG1
Final-hour trading
VP1
VBD1 PRED
accelerated
PP1 ARG4
TO1
to
NP2
108.1 million shares
NP3 ARGM-TMP
yesterday
Figure 1: An example tree from the PropBank with Semantic Role Annotations.
other nodes, but also dependencies between the la-
bel of a node and input features of other argument
nodes. The features are specified by instantiation of
templates and the value of a feature is the number of
times a particular pattern occurs in the labeled tree.
Templates
For a tree t, predicate v, and joint assignment L
of labels to the nodes of the tree, we define the can-
didate argument sequence as the sequence of non-
NONE labeled nodes [n1, l1, . . . , vPRED, nm, lm] (li
is the label of node ni). A reasonable candidate ar-
gument sequence usually contains very few of the
nodes in the tree ? about 2 to 7 nodes, as this is the
typical number of arguments for a verb. To make
it more convenient to express our feature templates,
we include the predicate node v in the sequence.
This sequence of labeled nodes is defined with re-
spect to the left-to-right order of constituents in the
parse tree. Since non-NONE labeled nodes do not
overlap, there is a strict left-to-right order among
these nodes. The candidate argument sequence that
corresponds to the correct assignment in Figure 1
will be:
[NP1-ARG1,VBD1-PRED,PP1-ARG4,NP3-ARGM-TMP]
Features from Local Models: All features included
in the local models are also included in our joint
models. In particular, each template for local fea-
tures is included as a joint template that concatenates
the local template and the node label. For exam-
ple, for the local feature PATH, we define a joint fea-
ture template, that extracts PATH from every node in
the candidate argument sequence and concatenates
it with the label of the node. Both a feature with
the specific argument label is created and a feature
with the generic back-off ARG label. This is similar
to adding features from identification and classifi-
cation models. In the case of the example candidate
argument sequence above, for the node NP1 we have
the features:
(NP?S?)-ARG1, (NP?S?)-ARG
When comparing a local and a joint model, we use
the same set of local feature templates in the two
models.
Whole Label Sequence: As observed in previous
work (Gildea and Jurafsky, 2002; Pradhan et al,
2004), including information about the set or se-
quence of labels assigned to argument nodes should
be very helpful for disambiguation. For example, in-
cluding such information will make the model less
likely to pick multiple fillers for the same role or
to come up with a labeling that does not contain an
obligatory argument. We added a whole label se-
quence feature template that extracts the labels of
all argument nodes, and preserves information about
the position of the predicate. The template also
includes information about the voice of the predi-
cate. For example, this template will be instantiated
as follows for the example candidate argument se-
quence:
[ voice:active ARG1,PRED,ARG4,ARGM-TMP]
We also add a variant of this feature which uses a
generic ARG label instead of specific labels. This
feature template has the effect of counting the num-
ber of arguments to the left and right of the predi-
cate, which provides useful global information about
argument structure. As previously observed (Prad-
han et al, 2004), including modifying arguments in
sequence features is not helpful. This was confirmed
in our experiments and we redefined the whole label
sequence features to exclude modifying arguments.
One important variation of this feature uses the
actual predicate lemma in addition to ?voice:active?.
Additionally, we define variations of these feature
templates that concatenate the label sequence with
features of individual nodes. We experimented with
593
variations, and found that including the phrase type
and the head of a directly dominating PP ? if one
exists ? was most helpful. We also add a feature that
detects repetitions of the same label in a candidate
argument sequence, together with the phrase types
of the nodes labeled with that label. For example,
(NP-ARG0,WHNP-ARG0) is a common pattern of this
form.
Frame Features: Another very effective class of fea-
tures we defined are features that look at the label of
a single argument node and internal features of other
argument nodes. The idea of these features is to cap-
ture knowledge about the label of a constituent given
the syntactic realization of all arguments of the verb.
This is helpful to capture syntactic alternations, such
as the dative alternation. For example, consider
the sentence (i) ?[Shaw Publishing]ARG0 offered [Mr.
Smith]ARG2 [a reimbursement]ARG1 ? and the alterna-
tive realization (ii) ?[Shaw Publishing]ARG0 offered
[a reimbursement]ARG1 [to Mr. Smith]ARG2 ?. When
classifying the NP in object position, it is useful to
know whether the following argument is a PP. If
yes, the NP will more likely be an ARG1, and if not,
it will more likely be an ARG2. A feature template
that captures such information extracts, for each ar-
gument node, its phrase type and label in the con-
text of the phrase types for all other arguments. For
example, the instantiation of such a template for [a
reimbursement] in (ii) would be
[ voice:active NP,PRED,NP-ARG1,PP]
We also add a template that concatenates the identity
of the predicate lemma itself.
We should note that Xue and Palmer (2004) define
a similar feature template, called syntactic frame,
which often captures similar information. The im-
portant difference is that their template extracts con-
textual information from noun phrases surrounding
the predicate, rather than from the sequence of ar-
gument nodes. Because our model is joint, we are
able to use information about other argument nodes
when labeling a node.
Final Pipeline
Here we describe the application in testing of a
joint model for semantic role labeling, using a local
model P `SRL, and a joint re-ranking model P rSRL.
P `SRL is used to generate top N non-overlapping
joint assignments L1, . . . , LN .
One option is to select the best Li according to
P rSRL, as in Equation 3, ignoring the score from
the local model. In our experiments, we noticed that
for larger values of N , the performance of our re-
ranking model P rSRL decreased. This was probably
due to the fact that at test time the local classifier
produces very poor argument frames near the bot-
tom of the top N for large N . Since the re-ranking
model is trained on relatively few good argument
frames, it cannot easily rule out very bad frames. It
makes sense then to incorporate the local model into
our final score. Our final score is given by:
PSRL(L|t, v) = (P `SRL(L|t, v))? P rSRL(L|t, v)
where ? is a tunable parameter 2 for how much in-
fluence the local score has in the final score. Such in-
terpolation with a score from a first-pass model was
also used for parse re-ranking in (Collins, 2000).
Given this score, at test time we choose among the
top N local assignments L1, . . . , LN according to:
arg max
L?{L1,...,LN}
? log P `SRL(L|t, v) + log P rSRL(L|t, v)
5 Experiments and Results
For our experiments we used the February 2004 re-
lease of PropBank. 3 As is standard, we used the
annotations from sections 02?21 for training, 24 for
development, and 23 for testing. As is done in
some previous work on semantic role labeling, we
discard the relatively infrequent discontinuous argu-
ments from both the training and test sets. In addi-
tion to reporting the standard results on individual
argument F-Measure, we also report Frame Accu-
racy (Acc.), the fraction of sentences for which we
successfully label all nodes. There are reasons to
prefer Frame Accuracy as a measure of performance
over individual-argument statistics. Foremost, po-
tential applications of role labeling may require cor-
rect labeling of all (or at least the core) arguments
in a sentence in order to be effective, and partially
correct labelings may not be very useful.
2We found ? = 0.5 to work best
3Although the first official release of PropBank was recently
released, we have not had time to test on it.
594
Task CORE ARGM
F1 Acc. F1 Acc.
Identification 95.1 84.0 95.2 80.5
Classification 96.0 93.3 93.6 85.6
Id+Classification 92.2 80.7 89.9 71.8
Table 2: Performance of local classifiers on identification, classification, and identification+classification on
section 23, using gold-standard parse trees.
N CORE ARGM
F1 Acc. F1 Acc.
1 92.2 80.7 89.9 71.8
5 97.8 93.9 96.8 89.5
20 99.2 97.4 98.8 95.3
30 99.3 97.9 99.0 96.2
Table 3: Oracle upper bounds for performance on the complete identification+classification task, using
varying numbers of top N joint labelings according to local classifiers.
Model CORE ARGM
F1 Acc. F1 Acc.
Local 92.2 80.7 89.9 71.8
Joint 94.7 88.2 92.1 79.4
Table 4: Performance of local and joint models on identification+classification on section 23, using gold-
standard parse trees.
We report results for two variations of the seman-
tic role labeling task. For CORE, we identify and
label only core arguments. For ARGM, we identify
and label core as well as modifier arguments. We
report results for local and joint models on argu-
ment identification, argument classification, and the
complete identification and classification pipeline.
Our local models use the features listed in Table 1
and the technique for enforcing the non-overlapping
constraint discussed in Section 3.1.
The labeling of the tree in Figure 1 is a specific
example of the kind of errors fixed by the joint mod-
els. The local classifier labeled the first argument in
the tree as ARG0 instead of ARG1, probably because
an ARG0 label is more likely for the subject position.
All joint models for these experiments used the
whole sequence and frame features. As can be seen
from Table 4, our joint models achieve error reduc-
tions of 32% and 22% over our local models in F-
Measure on CORE and ARGM respectively. With re-
spect to the Frame Accuracy metric, the joint error
reduction is 38% and 26% for CORE and ARGM re-
spectively.
We also report results on automatic parses (see
Table 5). We trained and tested on automatic parse
trees from Charniak?s parser (Charniak, 2000). For
approximately 5.6% of the argument constituents
in the test set, we could not find exact matches in
the automatic parses. Instead of discarding these
arguments, we took the largest constituent in the
automatic parse having the same head-word as the
gold-standard argument constituent. Also, 19 of the
propositions in the test set were discarded because
Charniak?s parser altered the tokenization of the in-
put sentence and tokens could not be aligned. As our
results show, the error reduction of our joint model
with respect to the local model is more modest in this
setting. One reason for this is the lower upper bound,
due largely to the the much poorer performance of
the identification model on automatic parses. For
ARGM, the local identification model achieves 85.9
F-Measure and 59.4 Frame Accuracy; the local clas-
sification model achieves 92.3 F-Measure and 83.1
Frame Accuracy. It seems that the largest boost
would come from features that can identify argu-
ments in the presence of parser errors, rather than
the features of our joint model, which ensure global
coherence of the argument frame. We still achieve
10.7% and 18.5% error reduction for CORE argu-
ments in F-Measure and Frame Accuracy respec-
tively.
595
Model CORE ARGM
F1 Acc. F1 Acc.
Local 84.1 66.5 81.4 55.6
Joint 85.8 72.7 82.9 60.8
Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak
automatically generated parse trees.
6 Related Work
Several semantic role labeling systems have success-
fully utilized joint information. (Gildea and Juraf-
sky, 2002) used the empirical probability of the set
of proposed arguments as a prior distribution. (Prad-
han et al, 2004) train a language model over label
sequences. (Punyakanok et al, 2004) use a linear
programming framework to ensure that the only ar-
gument frames which get probability mass are ones
that respect global constraints on argument labels.
The key differences of our approach compared
to previous work are that our model has all of the
following properties: (i) we do not assume a finite
Markov horizon for dependencies among node la-
bels, (ii) we include features looking at the labels
of multiple argument nodes and internal features of
these nodes, and (iii) we train a discriminative model
capable of incorporating these long-distance depen-
dencies.
7 Conclusions
Reflecting linguistic intuition and in line with cur-
rent work, we have shown that there are substantial
gains to be had by jointly modeling the argument
frames of verbs. This is especially true when we
model the dependencies with discriminative models
capable of incorporating long-distance features.
8 Acknowledgements
The authors would like to thank the review-
ers for their helpful comments and Dan Juraf-
sky for his insightful suggestions and useful dis-
cussions. This work was supported in part by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley Framenet project. In Proceedings of COLING-
ACL-1998.
Xavier Carreras and Lu??s M a`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL, pages 132?139.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML-2000.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?288.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-2001.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,
and Dan Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of HLT/NAACL-
2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support vec-
tor learning for semantic argument classification. Machine
Learning Journal.
Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zimak, and
Yuancheng Tu. 2004. Semantic role labeling via generalized
inference over classifiers. In Proceedings of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL-2003.
Cynthia A. Thompson, Roger Levy, and Christopher D. Man-
ning. 2003. A generative model for semantic role labeling.
In Proceedings of ECML-2003.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP-2004.
596
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 173?176, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Joint Model for Semantic Role Labeling
Aria Haghighi
Dept of Computer Science
Stanford University
Stanford, CA, 94305
aria42@stanford.edu
Kristina Toutanova
Dept of Computer Science
Stanford University
Stanford, CA, 94305
kristina@cs.stanford.edu
Christopher D. Manning
Dept of Computer Science
Stanford University
Stanford, CA, 94305
manning@cs.stanford.edu
Abstract
We present a semantic role labeling sys-
tem submitted to the closed track of the
CoNLL-2005 shared task. The system, in-
troduced in (Toutanova et al, 2005), im-
plements a joint model that captures de-
pendencies among arguments of a predi-
cate using log-linear models in a discrimi-
native re-ranking framework. We also de-
scribe experiments aimed at increasing the
robustness of the system in the presence
of syntactic parse errors. Our final system
achieves F1-Measures of 76.68 and 78.45
on the development and the WSJ portion
of the test set, respectively.
1 Introduction
It is evident that there are strong statistical patterns
in the syntactic realization and ordering of the argu-
ments of verbs; for instance, if an active predicate
has an A0 argument it is very likely to come before
an A1 argument. Our model aims to capture such de-
pendencies among the labels of nodes in a syntactic
parse tree.
However, building such a model is computation-
ally expensive. Since the space of possible joint la-
belings is exponential in the number of parse tree
nodes, a model cannot exhaustively consider these
labelings unless it makes strong independence as-
sumptions. To overcome this problem, we adopt
a discriminative re-ranking approach reminiscent of
(Collins, 2000). We use a local model, which la-
bels arguments independently, to generate a smaller
number of likely joint labelings. These candidate la-
belings are in turn input to a joint model which can
use global features and re-score the candidates. Both
the local and global re-ranking models are log-linear
(maximum entropy) models.
In the following sections, we briefly describe our
local and joint models and the system architecture
for combining them. We list the features used by our
models, with an emphasis on new features, and com-
pare the performance of a local and a joint model on
the CoNLL shared task. We also study an approach
to increasing the robustness of the semantic role la-
beling system to syntactic parser errors, by consid-
ering multiple parse trees generated by a statistical
parser.
2 Local Models
Our local model labels nodes in a parse tree inde-
pendently. We decompose the probability over la-
bels (all argument labels plus NONE), into a product
of the probability over ARG and NONE, and a prob-
ability over argument labels given that a node is an
ARG. This can be seen as chaining an identification
and a classification model. The identification model
classifies each phrase as either an argument or non-
argument and our classification model labels each
potential argument with a specific argument label.
The two models use the same features.
Previous research (Gildea and Jurafsky, 2002;
Pradhan et al, 2004; Carreras and Ma`rquez, 2004)
has identified many useful features for local iden-
tification and classification. Below we list the fea-
tures and hand-picked conjunctions of features used
in our local models. The ones denoted with asterisks
(*) were not present in (Toutanova et al, 2005). Al-
though most of these features have been described in
previous work, some features, described in the next
section, are ? to our knowledge ? novel.
173
? Phrase-Type Syntactic category of node
? Predicate Lemma Stemmed target verb
? Path Sequence of phrase types between the predicate and
node, with ?, ? to indicate direction
? Position Before or after predicate
? Voice Voice of predicate
? Head-Word of Phrase
? Head-POS POS tag of head word
? Sub-Cat CFG expansion of predicate?s parent
? First/Last Word
? Left/Right Sister Phrase-Type
? Left/Right Sister Head-Word/Head-POS
? Parent Phrase-Type
? Parent POS/Head-Word
? Ordinal Tree Distance Phrase-type concatenated with the
length of the Path feature
? Node-LCA Partial Path Path from the node to the lowest
common ancestor of the predicate and the node
? PP Parent Head-Word If the parent of the node is a PP, the
parent?s head-word
? PP NP Head-Word/Head-POS For a PP, retrieve the head-
word /head-POS of its rightmost NP
? Temporal Keywords* Is the head of the node a temporal
word e.g ?February? or ?afternoon?
? Missing subject* Is the predicate missing a subject in
the?standard? location
? Projected path* Path from the maximal extended projection
of the predicate to the node
? Predicate Lemma & Path
? Predicate Lemma & Head-Word
? Predicate Lemma & Phrase-Type
? Voice & Position
? Predicate Lemma & PP Parent Head-Word
? Path & Missing subject*
? Projected path & Missing subject*
2.1 Additional Local Features
We found that a large source of errors for A0 and A1
stemmed from cases such as those illustrated in Fig-
ure 1, where arguments were dislocated by raising
or controlling verbs. Here, the predicate, expected,
does not have a subject in the typical position ? in-
dicated by the empty NP ? since the auxiliary is has
raised the subject to its current position. In order to
capture this class of examples, we use a binary fea-
ture, Missing Subject, indicating whether the pred-
icate is ?missing? its subject, and use this feature in
conjunction with the Path feature, so that we learn
typical paths to raised subjects conditioned on the
absence of the subject in its typical position.
In the particular case of Figure 1, there is an-
other instance of an argument being quite far from
SPPPP

NPi-A1aaa
!!!
the trade gap
VPPPPP

is SPPPP

NPi-A1
-NONE-
VP
HHH

expected VP
QQ
to widen
Figure 1: Example of displaced arguments
its predicate. The predicate widen shares the trade
gap with expect as a A1 argument. However, as ex-
pect is a raising verb, widen?s subject is not in its
typical position either, and we should expect to find
it in the same positions as expected?s subject. This
indicates it may be useful to use the path relative to
expected to find arguments for widen. In general,
to identify certain arguments of predicates embed-
ded in auxiliary and infinitival VPs we expect it to
be helpful to take the path from the maximum ex-
tended projection of the predicate ? the highest VP
in the chain of VP?s dominating the predicate. We
introduce a new path feature, Projected Path, which
takes the path from the maximal extended projec-
tion to an argument node. This feature applies only
when the argument is not dominated by the maxi-
mal projection, (e.g., direct objects). These features
also handle other cases of discontinuous and non-
local dependencies, such as those arising due to con-
troller verbs. For a local model, these new features
and their conjunctions improved F1-Measure from
73.80 to 74.52 on the development set. Notably, the
F1-Measure of A0 increased from 81.02 to 83.08.
3 Joint Model
Our joint model, in contrast to the local model, col-
lectively scores a labeling of all nodes in the parse
tree. The model is trained to re-rank a set of N likely
labelings according to the local model. We find the
exact top N consistent1 most likely local model la-
belings using a simple dynamic program described
in (Toutanova et al, 2005).
1A labeling is consistent if satisfies the constraint that argu-
ment phrases do not overlap.
174
SNP1-A1
Crude oil prices
VP
VBD-V
fell
PP1-A3
TO
to
NP
$27.80
PP2-A4
FROM
from
NP
$37.80
NP2-AM-TMP
yesterday
Figure 2: An example tree with semantic role annotations.
Most of the features we use are described in more
detail in (Toutanova et al, 2005). Here we briefly
describe these features and introduce several new
joint features (denoted by *). A labeling L of all
nodes in the parse tree specifies a candidate argu-
ment frame ? the sequence of all nodes labeled with
a non-NONE label according to L. The joint model
features operate on candidate argument frames, and
look at the labels and internal features of the candi-
date arguments. We introduce them in the context
of the example in Figure 2. The candidate argument
frame corresponding to the correct labeling for the
tree is: [NP1-A1,VBD-V,PP1-A3,PP2-A4,NP2-AM-TMP].
? Core arguments label sequence: The sequence
of labels of core arguments concatenated with
the predicate voice. Example: [voice:active:
A1,V,A3,A4] A back-off feature which substitutes
specific argument labels with a generic argument
(A) label is also included.
? Flattened core arguments label sequence*:
Same as the previous but merging consecutive
equal labels.
? Core arguments label and annotated phrase
type sequence: The sequence of labels of core
arguments together with annotated phrase types.
Phrase types are annotated with the head word for
PP nodes, and with the head POS tag for S and VP
nodes. Example: [voice:active: NP-A1,V,PP-to-
A3,PP-from-A4]. A back-off to generic A labels
is also included. Also a variant that adds the pred-
icate stem.
? Repeated core argument labels with phrase
types: Annotated phrase types for nodes with
the same core argument label. This feature cap-
tures, for example, the tendency of WHNP refer-
ring phrases to occur as the second phrase having
the same label as a preceding NP phrase.
? Repeated core argument labels with phrase
types and sister/adjacency information*: Sim-
ilar to the previous feature, but also indicates
whether all repeated arguments are sisters in the
parse tree, or whether all repeated arguments are
adjacent in terms of word spans. These features
can provide robustness to parser errors, making it
more likely to label adjacent phrases incorrectly
split by the parser with the same label.
4 Combining Local and Joint Models
It is useful to combine the joint model score with
a local model score, because the local model has
been trained using all negative examples, whereas
the joint model has been trained only on likely
argument frames . Our final score is given by
a mixture of the local and joint model?s log-
probabilities: scoreSRL(L|t) = ? score`(L|t) +
scoreJ(L|t), where score`(L|t) is the local score of
L, scoreJ(L|t) is the corresponding joint score, and
? is a tunable parameter. We search among the top
N candidate labelings proposed by the local model,
for the labeling that maximizes the final score.
5 Increasing Robustness to Parser Errors
It is apparent that role labeling is very sensitive to the
correctness of the given parse tree. If an argument
does not correspond to a constituent in a parse tree,
our model will not be able to consider the correct
phrase.
One way to address this problem is to utilize alter-
native parses. Recent releases of the Charniak parser
(Charniak, 2000) have included an option to provide
the top k parses of a given sentence according to
the probability model of the parser. We use these
alternative parses as follow: Suppose t1, . . . , tk are
trees for sentence s with given probabilities P (ti|s)
by the parser. Then for a fixed predicate v, let Li
175
Precision Recall F?=1
Development 77.66% 75.72% 76.68
Test WSJ 79.54% 77.39% 78.45
Test Brown 70.24% 65.37% 67.71
Test WSJ+Brown 78.34% 75.78% 77.04
Test WSJ Precision Recall F?=1
Overall 79.54% 77.39% 78.45
A0 88.32% 88.30% 88.31
A1 78.61% 78.40% 78.51
A2 72.55% 68.11% 70.26
A3 73.08% 54.91% 62.71
A4 77.42% 70.59% 73.85
A5 100.00% 80.00% 88.89
AM-ADV 58.20% 51.19% 54.47
AM-CAU 63.93% 53.42% 58.21
AM-DIR 52.56% 48.24% 50.31
AM-DIS 76.56% 80.62% 78.54
AM-EXT 73.68% 43.75% 54.90
AM-LOC 61.52% 55.92% 58.59
AM-MNR 58.33% 56.98% 57.65
AM-MOD 97.85% 99.09% 98.47
AM-NEG 97.41% 98.26% 97.84
AM-PNC 49.50% 43.48% 46.30
AM-PRD 100.00% 20.00% 33.33
AM-REC 0.00% 0.00% 0.00
AM-TMP 74.85% 67.34% 70.90
R-A0 92.63% 89.73% 91.16
R-A1 81.53% 82.05% 81.79
R-A2 61.54% 50.00% 55.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 28.57% 33.33% 30.77
R-AM-TMP 61.54% 76.92% 68.38
V 97.32% 97.32% 97.32
Table 1: Overall results (top) and detailed results
on the WSJ test (bottom) on the closed track of the
CoNLL shared task.
denote the best joint labeling of tree ti, with score
scoreSRL(Li|ti) according to our final joint model.
Then we choose the labeling L which maximizes:
arg max
i?{1,...,k}
? log P (ti|S) + scoreSRL(Li|ti) (1)
Considering top k = 5 parse trees using this al-
gorithm resulted in up to 0.4 absolute increase in
F-Measure. In future work, we plan to experiment
with better ways to combine information from mul-
tiple parse trees.
6 Experiments and Results
For our final results we used a joint model with ? =
1.5 (local model weight), ? = 1 (parse tree log-
probability weight) , N = 15 (candidate labelings
from the local model to consider) , and k = 5 (num-
ber of alternative parses). The whole training set for
the CoNLL-2005 task was used to train the mod-
els. It takes about 2 hours to train a local identifi-
cation model, 40 minutes to train a local classifica-
tion model, and 7 hours to train a joint re-ranking
model.2
In Table 1, we present our final development and
test results using this model. The percentage of
perfectly labeled propositions for the three sets is
55.11% (development), 56.52% (test), and 37.06%
(Brown test). The improvement achieved by the
joint model relative to the local model is about 2
points absolute in F-Measure, similar to the im-
provement when gold-standard syntactic parses are
used (Toutanova et al, 2005). The relative error re-
duction is much lower for automatic parses, possi-
bly due to a lower upper bound on performance. It
is clear from the drop in performance from the WSJ
to Brown test set that our learned model?s features
do not generalize very well to related domains.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML-2000.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of HLT/NAACL-2004.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
2On a 3.6GHz machine with 4GB of RAM.
176
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 377?387,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Incorporating Content Structure into Text Analysis Applications
Christina Sauper, Aria Haghighi, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{csauper, aria42, regina}@csail.mit.edu
Abstract
In this paper, we investigate how modeling
content structure can benefit text analysis ap-
plications such as extractive summarization
and sentiment analysis. This follows the lin-
guistic intuition that rich contextual informa-
tion should be useful in these tasks. We
present a framework which combines a su-
pervised text analysis application with the in-
duction of latent content structure. Both of
these elements are learned jointly using the
EM algorithm. The induced content struc-
ture is learned from a large unannotated cor-
pus and biased by the underlying text analysis
task. We demonstrate that exploiting content
structure yields significant improvements over
approaches that rely only on local context.1
1 Introduction
In this paper, we demonstrate that leveraging doc-
ument structure significantly benefits text analysis
applications. As a motivating example, consider
the excerpt from a DVD review shown in Table 1.
This review discusses multiple aspects of a product,
such as audio and video properties. While the word
?pleased? is a strong indicator of positive sentiment,
the sentence in which it appears does not specify the
aspect to which it relates. Resolving this ambiguity
requires information about global document struc-
ture.
A central challenge in utilizing such informa-
tion lies in finding a relevant representation of con-
tent structure for a specific text analysis task. For
1Code and processed data presented here are available at
http://groups.csail.mit.edu/rbg/code/content structure.html
Audio Audio choices are English, Spanish and French
Dolby Digital 5.1 ... Bass is still robust and powerful,
giving weight to just about any scene ? most notably
the film?s exciting final fight. Fans should be pleased
with the presentation.
Extras This single-disc DVD comes packed in a black
amaray case with a glossy slipcover. Cover art has
clearly been designed to appeal the Twilight crowd ...
Finally, we?ve got a deleted scenes reel. Most of the
excised scenes are actually pretty interesting.
Table 1: An excerpt from a DVD review.
instance, when performing single-aspect sentiment
analysis, the most relevant aspect of content struc-
ture is whether a given sentence is objective or sub-
jective (Pang and Lee, 2004). In a multi-aspect
setting, however, information about the sentence
topic is required to determine the aspect to which
a sentiment-bearing word relates (Snyder and Barzi-
lay, 2007). As we can see from even these closely re-
lated applications, the content structure representa-
tion should be intimately tied to a specific text anal-
ysis task.
In this work, we present an approach in which a
content model is learned jointly with a text analy-
sis task. We assume complete annotations for the
task itself, but we learn the content model from raw,
unannotated text. Our approach is implemented in
a discriminative framework using latent variables to
represent facets of content structure. In this frame-
work, the original task features (e.g., lexical ones)
are conjoined with latent variables to enrich the fea-
tures with global contextual information. For ex-
ample, in Table 1, the feature associated with the
377
word ?pleased? should contribute most strongly to
the sentiment of the audio aspect when it is aug-
mented with a relevant topic indicator.
The coupling of the content model and the task-
specific model allows the two components to mutu-
ally influence each other during learning. The con-
tent model leverages unannotated data to improve
the performance of the task-specific model, while
the task-specific model provides feedback to im-
prove the relevance of the content model. The com-
bined model can be learned effectively using a novel
EM-based method for joint training.
We evaluate our approach on two complementary
text analysis tasks. Our first task is a multi-aspect
sentiment analysis task, where a system predicts the
aspect-specific sentiment ratings (Snyder and Barzi-
lay, 2007). Second, we consider a multi-aspect ex-
tractive summarization task in which a system ex-
tracts key properties for a pre-specified set of as-
pects. On both tasks, our method for incorporating
content structure consistently outperforms structure-
agnostic counterparts. Moreover, jointly learning
content and task parameters yields additional gains
over independently learned models.
2 Related Work
Prior research has demonstrated the usefulness of
content models for discourse-level tasks. Examples
of such tasks include sentence ordering (Barzilay
and Lee, 2004; Elsner et al, 2007), extraction-based
summarization (Haghighi and Vanderwende, 2009)
and text segmentation (Chen et al, 2009). Since
these tasks are inherently tied to document structure,
a content model is essential to performing them suc-
cessfully. In contrast, the applications considered in
this paper are typically developed without any dis-
course information, focusing on capturing sentence-
level relations. Our goal is to augment these models
with document-level content information.
Several applications in information extraction
and sentiment analysis are close in spirit to our
work (Pang and Lee, 2004; Patwardhan and Riloff,
2007; McDonald et al, 2007). These approaches
consider global contextual information when de-
termining whether a given sentence is relevant to
the underlying analysis task. All assume that rele-
vant sentences have been annotated. For instance,
Pang and Lee (2004) refine the accuracy of sen-
timent analysis by considering only the subjective
sentences of a review as determined by an indepen-
dent classifier. Patwardhan and Riloff (2007) take
a similar approach in the context of information ex-
traction. Rather than applying their extractor to all
the sentences in a document, they limit it to event-
relevant sentences. Since these sentences are more
likely to contain information of interest, the extrac-
tion performance increases.
Another approach, taken by Choi and Cardie
(2008) and Somasundaran et al (2009) uses lin-
guistic resources to create a latent model in a task-
specific fashion to improve performance, rather than
assuming sentence-level task relevancy. Choi and
Cardie (2008) address a sentiment analysis task by
using a heuristic decision process based on word-
level intermediate variables to represent polarity.
Somasundaran et al (2009) similarly uses a boot-
strapped local polarity classifier to identify sentence
polarity.
McDonald et al (2007) propose a model
which jointly identifies global polarity as well as
paragraph- and sentence-level polarity, all of which
are observed in training data. While our approach
uses a similar hierarchy, McDonald et al (2007) is
concerned with recovering the labels at all levels,
whereas in this work we are interested in using la-
tent document content structure as a means to benefit
task predictions.
While our method also incorporates contextual
information into existing text analysis applications,
our approach is markedly different from the above
approaches. First, our representation of context en-
codes more than the relevance-based binary distinc-
tion considered in the past work. Our algorithm ad-
justs the content model dynamically for a given task
rather than pre-specifying it. Second, while previ-
ous work is fully supervised, in our case relevance
annotations are readily available for only a few ap-
plications and are prohibitively expensive to obtain
for many others. To overcome this drawback, our
method induces a content model in an unsupervised
fashion and connects it via latent variables to the
target model. This design not only eliminates the
need for additional annotations, but also allows the
algorithm to leverage large quantities of raw data for
training the content model. The tight coupling of rel-
378
evance learning with the target analysis task leads to
further performance gains.
Finally, our work relates to supervised topic mod-
els in Blei and McAullife (2007). In this work, la-
tent topic variables are used to generate text as well
as a supervised sentiment rating for the document.
However, this architecture does not permit the usage
of standard discriminative models which condition
freely on textual features.
3 Model
3.1 Problem Formulation
In this section, we describe a model which incorpo-
rates content information into a multi-aspect sum-
marization task.2 Our approach assumes that at
training time we have a collection of labeled doc-
uments DL, each consisting of the document text
s and true task-specific labeling y?. For the multi-
aspect summarization task, y? consists of sequence
labels (e.g., value or service) for the tokens of a
document. Specifically, the document text s is
composed of sentences s1, . . . , sn and the label-
ings y? consists of corresponding label sequences
y1, . . . , yn.3
As is common in related work, we model each yi
using a CRF which conditions on the observed doc-
ument text. In this work, we also assume a content
model, which we fix to be the document-level HMM
as used in Barzilay and Lee (2004). In this content
model, each sentence si is associated with a hidden
topic variable Ti which generates the words of the
sentence. We will use T = (T1, . . . , Tn) to refer to
the hidden topic sequence for a document. We fix
the number of topics to a pre-specified constant K.
3.2 Model Overview
Our model, depicted in Figure 1, proceeds as fol-
lows: First the document-level HMM generates
a hidden content topic sequence T for the sen-
tences of a document. This content component is
parametrized by ? and decomposes in the standard
2In Section 3.6, we discuss how this framework can be used
for other text analysis applications.
3Note that each yi is a label sequence across the words in si,
rather than an individual label.
y
1
i
y
m
i
y
2
i
. . .
T
i
w
1
i
w
m
i
w
2
i
. . .
T
i?1
T
i+1
(w
2
i
= pleased) ? (T
i
= 3)
w
2
i
= pleased
...
s
i
Figure 1: A graphical depiction of our model for
sequence labeling tasks. The Ti variable represents
the content model topic for the ith sentence si. The
words of si, (w1i , . . . , w
m
i ), each have a task label
(y
1
i , . . . , y
m
i ). Note that each token label has an
undirected edge to a factor containing the words of
the current sentence, si as well as the topic of the
current sentence Ti.
HMM fashion:4
P?(s,T ) =
n?
i=1
P?(Ti|Ti?1)
?
w?si
P?(w|Ti) (1)
Then the label sequences for each sentence in
the document are independently modeled as CRFs
which condition on both the sentence features and
the sentence topic:
P?(y|s,T ) =
n?
i=1
P?(yi|si, Ti) (2)
Each sentence CRF is parametrized by ? and takes
the standard form:
P?(y|s, T ) ?
exp
?
?
?
?
j
?
T [
fN (y
j
, s, T ) + fE(y
j
, y
j+1
)
]
?
?
?
4We also utilize a hierarchical emission model so that each
topic distribution interpolates between a topic-specific distribu-
tion as well as a shared background model; this is intended to
capture domain-specific stop words.
379
Ts
y
?
?
?
Content
Parameters
Task
Parameters
Task Labels
Text
Content
Structure
Figure 2: A graphical depiction of the generative
process for a labeled document at training time (See
Section 3); shaded nodes indicate variables which
are observed at training time. First the latent un-
derlying content structure T is drawn. Then, the
document text s is drawn conditioned on the content
structure utilizing content parameters ?. Finally, the
observed task labels for the document are modeled
given s and T using the task parameters ?. Note that
the arrows for the task labels are undirected since
they are modeled discriminatively.
where fN (?) and fE(?) are feature functions associ-
ated with CRF nodes and transitions respectively.
Allowing the CRF to condition on the sentence
topic Ti permits predictions to be more sensitive to
content. For instance, using the example from Ta-
ble 1, we could have a feature that indicates the word
?pleased? conjoined with the segment topic (see Fig-
ure 1). These topic-specific features serve to disam-
biguate word usage.
This joint process, depicted graphically in Fig-
ure 2, is summarized as:
P (T , s,y?) = P?(T , s)P?(y
?
|s,T ) (3)
Note that this probability decomposes into a
document-level HMM term (the content component)
as well as a product of CRF terms (the task compo-
nent).
3.3 Learning
During learning, we would like to find the
document-level HMM parameters ? and the summa-
rization task CRF parameters ? which maximize the
likelihood of the labeled documents. The only ob-
served elements of a labeled document are the docu-
ment text s and the aspect labels y?. This objective
is given by:
LL(?, ?) =
?
(s,y?)?DL
logP (s,y?)
=
?
(s,y?)?DL
log
?
T
P (T , s,y?)
We use the EM algorithm to optimize this objec-
tive.
E-Step The E-Step in EM requires computing the
posterior distribution over latent variables. In this
model, the only latent variables are the sentence top-
ics T . To compute this term, we utilize the decom-
position in Equation (3) and rearrange HMM and
CRF terms to obtain:
P (T , s,y?) = P?(T , s)P?(y
?
|T , s)
=
(
n?
i=1
P?(Ti|Ti?1)
?
w?si
P?(w|Ti)
)
?
(
n?
i=1
P?(y
?
i |si, Ti)
)
=
n?
i=1
P?(Ti|Ti?1)?
(
?
w?si
P?(w|Ti)P?(y
?
i |si, Ti)
)
We note that this expression takes the same form as
the document-level HMM, except that in addition to
emitting the words of a sentence, we also have an
observation associated with the sentence sequence
labeling. We treat each P?(y?i |si, Ti) as part of the
node potential associated with the document-level
HMM. We utilize the Forward-Backward algorithm
as one would with the document-level HMM in iso-
lation, except that each node potential incorporates
this CRF term.
M-Step We perform separate M-Steps for content
and task parameters. The M-Step for the content pa-
rameters is identical to the document-level HMM
380
content model: topic emission and transition dis-
tributions are updated with expected counts derived
from E-Step topic posteriors.
The M-Step for the task parameters does not have
a closed-form solution. Recall that in the M-Step,
we maximize the log probability of all random vari-
ables given expectations of latent variables. Using
the decomposition in Equation (3), it is clear that
the only component of the joint labeled document
probability which relies upon the task parameters is
logP?(y?|s,T ). Thus for the M-Step, it is sufficient
to optimize the following with respect to ?:
ET |s,y? logP?(y
?
|s,T )
=
n?
i=1
E
T
i
|s
i
, y
?
i
logP?(y
?
i |si, Ti)
=
n?
i=1
K?
k=1
P (Ti = k|si, y
?
i ) logP?(y
?
i |si, Ti)
The first equality follows from the decomposition
of the task component into independent CRFs (see
Equation (2)). Optimizing this objective is equiva-
lent to a weighted version of the conditional likeli-
hood objective used to train the CRF in isolation. An
intuitive explanation of this process is that there are
multiple CRF instances, one for each possible hid-
den topic T . Each utilizes different content features
to explain the sentence sequence labeling. These in-
stances are weighted according to the posterior over
T obtained during the E-Step. While this objective
is non-convex due to the summation over T , we can
still optimize it using any gradient-based optimiza-
tion solver; in our experiments, we used the LBFGS
algorithm (Liu et al, 1989).
3.4 Inference
We must predict a label sequence y for each sen-
tence s of the document. We assume a loss function
over a sequence labeling y and a proposed labeling
y?, which decomposes as:
L(y, y?) =
?
j
L(y
j
, y?
j
)
where each position loss is sensitive to the kind of
error which is made. Failing to extract a token is
penalized to a greater extent than extracting it with
an incorrect label:
L(y
j
, y?
j
) =
?
??
??
0 if y?j = yj
c if yj 6= NONE and y?j = NONE
1 otherwise
In this definition, NONE represents the background
label which is reserved for tokens which do not cor-
respond to labels of interest. The constant c repre-
sents a user-defined trade-off between precision and
recall errors. For our multi-aspect summarization
task, we select c = 4 for Yelp and c = 5 for Amazon
to combat the high-precision bias typical of condi-
tional likelihood models.
At inference time, we select the single labeling
which minimizes the expected loss with respect to
model posterior over label sequences:
y? = min
y?
E
y|sL(y, y?)
= min
y?
?
j=1
E
y
j
|sL(y
j
, y?
j
)
In our case, we must marginalize out the sentence
topic T :
P (y
j
|s) =
?
T
P (y
j
, T |s)
=
?
T
P?(T |s)P?(y
j
|s, T )
This minimum risk criterion has been widely used in
NLP applications such as parsing (Goodman, 1999)
and machine translation (DeNero et al, 2009). Note
that the above formulation differs from the stan-
dard CRF due to the latent topic variables. Other-
wise the inference task could be accomplished by
directly obtaining posteriors over each yj state using
the Forward-Backwards algorithm on the sentence
CRF.
Finding y? can be done efficiently. First, we ob-
tain marginal token posteriors as above. Then, the
expected loss of a token prediction is computed as
follows: ?
y?j
P (y
j
|s)L(y
j
, y?
j
)
Once we obtain expected losses of each token pre-
diction, we compute the minimum risk sequence la-
beling by running the Viterbi algorithm. The po-
tential for each position and prediction is given by
381
the negative expected loss. The maximal scoring se-
quence according to these potentials minimizes the
expected risk.
3.5 Leveraging unannotated data
Our model allows us to incorporate unlabeled doc-
uments, denoted DU , to improve the learning of the
content model. For an unlabeled document we only
observe the document text s and assume it is drawn
from the same content model as our labeled docu-
ments. The objective presented in Section 3.3 as-
sumed that all documents were labeled; here we sup-
plement this objective by capturing the likelihood
of unlabeled documents according to the content
model:
LU (?) =
?
s?DU
logP?(s)
=
?
s?DU
log
?
T
P?(s,T )
Our overall objective function is to maximize the
likelihood of both our labeled and unlabeled data.
This objective corresponds to:
L(?, ?) =LU (?) + LL(?, ?)
This objective can also be optimized using the EM
algorithm, where the E-Step for labeled and unla-
beled documents is outlined above.
3.6 Generalization
The approach outlined can be applied to a wider
range of task components. For instance, in Sec-
tion 4.1 we apply this approach to multi-aspect sen-
timent analysis. In this task, the target y consists of
numeric sentiment ratings (y1, . . . , yK) for each of
K aspects. The task component consists of indepen-
dent linear regression models for each aspect sen-
timent rating. For the content model, we associate
a topic with each paragraph; T consists of assign-
ments of topics to each document paragraph.
The model structure still decomposes as in Fig-
ure 2, but the details of learning are slightly differ-
ent. For instance, because the task label (aspect sen-
timent ratings) is not localized to any region of the
document, all content model variables influence the
target response. Conditioned on the target label, all
topic variables become correlated. Thus when learn-
ing, the E-Step requires computing a posterior over
paragraph topic tuples T :
P (T |y, s) ? P (s,T )P (y|T , s)
For the case of our multi-aspect sentiment task, this
computation can be done exactly by enumerating
T tuples, since the number of sentences and pos-
sible topics is relatively small. If summation is in-
tractable, the posterior may be approximated using
variational techniques (Bishop, 2006), which is ap-
plicable to a broad range of potential applications.
4 Experimental Set-Up
We apply our approach to two text analysis tasks that
stand to benefit from modeling content structure:
multi-aspect sentiment analysis and multi-aspect re-
view summarization.
4.1 Tasks
In the following section, we define each task in de-
tail, explain the task-specific adaptation of the model
and describe the data sets used in the experiments.
Table 2 summarizes statistics for all the data sets.
For all tasks, when using a content model with a
task model, we utilize a new set of features which
include all the original features as well as a copy
of each feature conjoined with the content topic as-
signment (see Figure 1). We also include a fea-
ture which indicates whether a given word was most
likely emitted from the underlying topic or from a
background distribution.
Multi-Aspect Sentiment Ranking The goal of
multi-aspect sentiment classification is to predict a
set of numeric ranks that reflects the user satisfaction
for each aspect (Snyder and Barzilay, 2007). One of
the challenges in this task is to attribute sentiment-
bearing words to the aspects they describe. Informa-
tion about document structure has the potential to
greatly reduce this ambiguity.
Following standard sentiment ranking ap-
proaches (Wilson et al, 2004; Pang and Lee, 2005;
Goldberg and Zhu, 2006; Snyder and Barzilay,
2007), we employ ordinary linear regression to
independently map bag-of-words representations
into predicted aspect ranks. In addition to com-
monly used lexical features, this set is augmented
382
Task
Labeled
Unlabeled
Avg. Size
Train Test Words Sents
Multi-aspect sentiment 600 65 ? 1,027 20.5
Multi-aspect summarization
Amazon 35 24 12,684 214 11.7
Yelp 48 48 33,015 178 11.2
Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and
unlabeled documents are used for training the content model, while only the labeled training corpus is used
to train the task model. Note that the entire data set for the multi-aspect sentiment analysis task is labeled.
with content features as described above. For this
application, we fix the number of HMM states to be
equal to the predefined number of aspects.
We test our sentiment ranker on a set of DVD re-
views from the website IGN.com.5 Each review is
accompanied by 1-10 scale ratings in four categories
that assess the quality of a movie?s content, video,
audio, and DVD extras. In this data set, segments
corresponding to each of the aspects are clearly de-
lineated in each document. Therefore, we can com-
pare the performance of the algorithm using auto-
matically induced content models against the gold
standard structural information.
Multi-Aspect Review Summarization The goal
of this task is to extract informative phrases that
identify information relevant to several predefined
aspects of interest. In other words, we would like our
system to both extract important phrases (e.g., cheap
food) and label it with one of the given aspects (e.g.,
value). For concrete examples and lists of aspects
for each data set, see Figures 3b and 3c. Variants of
this task have been considered in review summariza-
tion in previous work (Kim and Hovy, 2006; Brana-
van et al, 2009).
This task has elements of both information extrac-
tion and phrase-based summarization ? the phrases
we wish to extract are broader in scope than in stan-
dard template-driven IE, but at the same time, the
type of selected information is restricted to the de-
fined aspects, similar to query-based summarization.
The difficulty here is that phrase selection is highly
context-dependent. For instance, in TV reviews such
as in Figure 3b, the highlighted phrase ?easy to read?
might refer to either the menu or the remote; broader
5http://dvd.ign.com/index/reviews.html
context is required for correct labeling.
We evaluated our approach for this task on two
data sets: Amazon TV reviews (Figure 3b) and Yelp
restaurant reviews (Figure 3c). To eliminate noisy
reviews, we only retain documents that have been
rated ?helpful? by the users of the site; we also re-
move reviews which are abnormally short or long.
Each data set was manually annotated with aspect
labels using Mechanical Turk, which has been used
in previous work to annotate NLP data (Snow et al,
2008). Since we cannot select high-quality annota-
tors directly, we included a control document which
had been previously annotated by a native speaker
among the documents assigned to each annotator.
The work of any annotator who exhibited low agree-
ment on the control document annotation was ex-
cluded from the corpus. To test task annotation
agreement, we use Cohen?s Kappa (Cohen, 1960).
On the Amazon data set, two native speakers anno-
tated a set of four documents. The agreement be-
tween the judges was 0.54. On the Yelp data set, we
simply computed the agreement between all pairs of
reviewers who received the same control documents;
the agreement was 0.49.
4.2 Baseline Comparison and Evaluation
Baselines For all the models, we obtain a baseline
system by eliminating content features and only us-
ing a task model with the set of features described
above. We also compare against a simplified vari-
ant of our method wherein a content model is in-
duced in isolation rather than learned jointly in the
context of the underlying task. In our experiments,
we refer to the two methods as the No Content
Model (NoCM) and Independent Content Model
(IndepCM) settings, respectively. The Joint Content
383
M = Movie
V = Video
A = Audio
E = Extras
M This collection certainly offers some nostalgic 
fun, but at the end of the day, the shows themselves, 
for the most part, just don't hold up. (5)
V Regardless, this is a fairly solid presentation, but 
it's obvious there was room for improvement.  (7)
A Bass is still robust and powerful. Fans should be 
pleased with this presentation. (8)
E The deleted scenes were quite lengthy, but only 
shelled out a few extra laughs. (4) 
(a) Sample labeled text from the multi-aspect sentiment corpus
[R Big multifunction remote] with [R easy-to-
read keys].   The on-screen menu is [M easy to 
use] and you [M can rename the inputs] to one 
of several options (DVD, Cable, etc.).
R = Remote
M = Menu
I = Inputs
E = Economy
V = Video
S = Sound
A = Appearance
F = Features
I bought this TV because the [V overall picture 
quality is good] and it's [A unbelievably thin].
[I Plenty of inputs], including [I 2 HDMI ports], 
which is [E unheard of in this price range].
(b) Sample labeled text from the Amazon multi-aspect summa-
rization corpus
[F All the ingredients are fresh], [V the sizes are 
huge] and [V the price is cheap]. 
F = Food
A = Atmosphere
V = Value
S = Service
O = Overall
[O This place rocks!]  [V Pricey, but worth it] .
[A The place is a pretty good size] and
[S the staff is super friendly].
(c) Sample labeled text from the Yelp multi-aspect summarization
corpus
Figure 3: Excerpts from the three corpora with the
corresponding labels. Note that sentences from the
multi-aspect summarization corpora generally focus
on only one or two aspects. The multi-aspect senti-
ment corpus has labels per paragraph rather than per
sentence.
Model (JointCM) setting refers to our full model de-
scribed in Section 3, where content and task compo-
nents are learned jointly.
Evaluation Metrics For multi-aspect sentiment
ranking, we report the average L2 (squared differ-
ence) and L1 (absolute difference) between system
prediction and true 1-10 sentiment rating across test
documents and aspects.
For the multi-aspect summarization task, we mea-
sure average token precision and recall of the label
assignments (Multi-label). For the Amazon corpus,
we also report a coarser metric which measures ex-
traction precision and recall while ignoring labels
(Binary labels) as well as ROUGE (Lin, 2004). To
compute ROUGE, we control for length by limiting
L1 L2
NoCM 1.37 3.15
IndepCM 1.28?* 2.80?*
JointCM 1.25? 2.65?*
Gold 1.18?* 2.48?*
Table 3: The error rate on the multi-aspect sentiment
ranking. We report mean L1 and L2 between system
prediction and true values over all aspects. Marked
results are statistically significant with p < 0.05: *
over the previous model and ? over NoCM.
F1 F2 Prec. Recall
NoCM 28.8% 34.8% 22.4% 40.3%
IndepCM 37.9% 43.7% 31.1%?* 48.6%?*
JointCM 39.2% 44.4% 32.9%?* 48.6%?
Table 4: Results for multi-aspect summarization on
the Yelp corpus. Marked precision and recall are
statistically significant with p < 0.05: * over the
previous model and ? over NoCM.
each system to predict the same number of tokens as
the original labeled document.
Our metrics of statistical significance vary by
task. For the sentiment task, we use Student?s t-
test. For the multi-aspect summarization task, we
perform chi-square analysis on the ROUGE scores
as well as on precision and recall separately, as
is commonly done in information extraction (Fre-
itag, 2004; Weeds et al, 2004; Finkel and Manning,
2009).
5 Results
In this section, we present the results of the methods
on the tasks described above (see Tables 3, 4, and 5).
Baseline Comparisons Adding a content model
significantly outperforms the NoCM baseline on
both tasks. The highest F1 error reduction ? 14.7%
? is achieved on multi-aspect summarization on the
Yelp corpus, followed by the reduction of 11.5% and
8.75%, on multi-aspect summarization on the Ama-
zon corpus and multi-aspect sentiment ranking, re-
spectively.
We also observe a consistent performance boost
when comparing against the IndepCM baseline.
This result confirms our hypothesis about the ad-
384
Multi-label Binary labels
F1 F2 Prec. Recall F1 F2 Prec. Recall ROUGE
NoCM 18.9% 18.0% 20.4% 17.5% 35.1% 33.6% 38.1% 32.6% 43.8%
IndepCM 24.5% 23.8% 25.8%?* 23.3%?* 43.0% 41.8% 45.3%?* 40.9%?* 47.4%?*
JointCM 28.2% 31.3% 24.3%? 33.7%?* 47.8% 53.0% 41.2%? 57.1%?* 47.6%?*
Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and
recall are statistically significant with p < 0.05: * over the previous model and ? over NoCM.
vantages of jointly learning the content model in the
context of the underlying task.
Comparison with additional context features
One alternative to an explicit content model is to
simply incorporate additional features into NoCM
as a proxy for contextual information. In the
multi-aspect summarization case, this can be accom-
plished by adding unigram features from the sen-
tences before and after the current one.6
When testing this approach, however, the perfor-
mance of NoCM actually decreases on both Ama-
zon (to 15.0% F1) and Yelp (to 24.5% F1) corpora.
This result is not surprising for this particular task ?
by adding these features, we substantially increase
the feature space without increasing the amount of
training data. An advantage of our approach is
that our learned representation of context is coarse,
and we can leverage large quantities of unannotated
training data.
Impact of content model quality on task per-
formance In the multi-aspect sentiment ranking
task, we have access to gold standard document-
level content structure annotation. This affords us
the ability to compare the ideal content structure,
provided by the document authors, with one that is
learned automatically. As Table 3 shows, the manu-
ally created document structure segmentation yields
the best results. However, the performance of our
JointCM model is not far behind the gold standard
content structure.
The quality of the induced content model is de-
termined by the amount of training data. As Fig-
ure 4 shows, the multi-aspect summarizer improves
with the increase in the size of raw data available for
learning content model.
6This type of feature is not applicable to our multi-aspect
sentiment ranking task, as we already use unigram features from
the entire document.
10
20
30
0% 50% 100%
Multi
-label
 F 1
Percentage of unlabeled data
22.8 26.0
28.2
Figure 4: Results on the Amazon corpus using the
complete annotated set with varying amounts of ad-
ditional unlabeled data.7
Compensating for annotation sparsity We hy-
pothesize that by incorporating rich contextual in-
formation, we can reduce the need for manual task
annotation. We test this by reducing the amount of
annotated data available to the model and measur-
ing performance at several quantities of unannotated
data. As Figure 5 shows, the performance increase
achieved by doubling the amount of annotated data
can also be achieved by adding only 12.5% of the
unlabeled data.
6 Conclusion
In this paper, we demonstrate the benefits of incor-
porating content models in text analysis tasks. We
also introduce a framework to allow the joint learn-
ing of an unsupervised latent content model with a
supervised task-specific model. On multiple tasks
and datasets, our results empirically connect model
quality and task performance, suggesting that fur-
7Because we append the unlabeled versions of the labeled
data to the unlabeled set, even with 0% additional unlabeled
data, there is a small data set to train the content model.
385
10
12
32
0% 1352% 32%
Multi
-label
 F 1
Percentage of unlabeled data
30
2.82
608 66828
Figure 5: Results on the Amazon corpus using half
of the annotated training documents. The content
model is trained with 0%, 12.5%, and 25% of addi-
tional unlabeled data.7 The dashed horizontal line
represents NoCM with the complete annotated set.
ther improvements in content modeling may yield
even further gains.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168) and NIH (grant 5-
R01-LM009723-02). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113?120.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc.
David M. Blei and Jon D. McAullife. 2007. Supervised
Topic Models. In NIPS.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2009. Learning document-level se-
mantic properties from free-text annotations. JAIR,
34:569?603.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. JAIR, 36:129?163.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for sub-
sentential sentiment analysis. In Proceedings of the
EMNLP, pages 793?801.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the ACL/IJCNLP, pages 567?575.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of the NAACL/HLT, pages
436?443.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the NAACL.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
the EMNLP, pages 262?269.
Andrew B. Goldberg and Xiaojin Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In Proceedings of the NAACL/HLT Workshop on
TextGraphs, pages 45?52.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of the NAACL/HLT, pages 362?370.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483?490.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proceedings of the ACL,
pages 74?81.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory bfgs method for
large scale optimization. Mathematical Programming,
45:503?528.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the ACL, pages 432?439.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115?124.
386
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
EMNLP/CoNLL, pages 717?727.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In Pro-
ceedings of the NAACL/HLT, pages 300?307.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the EMNLP, pages 170?179.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional simi-
larity. In Proceedings of the COLING, page 1015.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opin-
ion clauses. In Proceedings of the AAAI, pages 761?
769.
387
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 853?861,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Simple Type-Level Unsupervised POS Tagging
Yoong Keok Lee Aria Haghighi Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{yklee, aria42, regina}@csail.mit.edu
Abstract
Part-of-speech (POS) tag distributions are
known to exhibit sparsity ? a word is likely
to take a single predominant tag in a corpus.
Recent research has demonstrated that incor-
porating this sparsity constraint improves tag-
ging accuracy. However, in existing systems,
this expansion come with a steep increase in
model complexity. This paper proposes a sim-
ple and effective tagging method that directly
models tag sparsity and other distributional
properties of valid POS tag assignments. In
addition, this formulation results in a dramatic
reduction in the number of model parame-
ters thereby, enabling unusually rapid training.
Our experiments consistently demonstrate that
this model architecture yields substantial per-
formance gains over more complex tagging
counterparts. On several languages, we report
performance exceeding that of more complex
state-of-the art systems.1
1 Introduction
Since the early days of statistical NLP, researchers
have observed that a part-of-speech tag distribution
exhibits ?one tag per discourse? sparsity ? words
are likely to select a single predominant tag in a cor-
pus, even when several tags are possible. Simply
assigning to each word its most frequent associated
tag in a corpus achieves 94.6% accuracy on the WSJ
portion of the Penn Treebank. This distributional
sparsity of syntactic tags is not unique to English
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/typetagging/.
? similar results have been observed across multi-
ple languages. Clearly, explicitly modeling such a
powerful constraint on tagging assignment has a po-
tential to significantly improve the accuracy of an
unsupervised part-of-speech tagger learned without
a tagging dictionary.
In practice, this sparsity constraint is difficult
to incorporate in a traditional POS induction sys-
tem (Me?rialdo, 1994; Johnson, 2007; Gao and John-
son, 2008; Grac?a et al, 2009; Berg-Kirkpatrick
et al, 2010). These sequence models-based ap-
proaches commonly treat token-level tag assignment
as the primary latent variable. By design, they read-
ily capture regularities at the token-level. However,
these approaches are ill-equipped to directly repre-
sent type-based constraints such as sparsity. Pre-
vious work has attempted to incorporate such con-
straints into token-level models via heavy-handed
modifications to inference procedure and objective
function (e.g., posterior regularization and ILP de-
coding) (Grac?a et al, 2009; Ravi and Knight, 2009).
In most cases, however, these expansions come with
a steep increase in model complexity, with respect
to training procedure and inference time.
In this work, we take a more direct approach and
treat a word type and its allowed POS tags as a pri-
mary element of the model. The model starts by gen-
erating a tag assignment for each word type in a vo-
cabulary, assuming one tag per word. Then, token-
level HMM emission parameters are drawn condi-
tioned on these assignments such that each word is
only allowed probability mass on a single assigned
tag. In this way we restrict the parameterization of a
853
Language Original case
English 94.6
Danish 96.3
Dutch 96.6
German 95.5
Spanish 95.4
Swedish 93.3
Portuguese 95.6
Table 1: Upper bound on tagging accuracy assuming each
word type is assigned to majority POS tag. Across all
languages, high performance can be attained by selecting
a single tag per word type.
token-level HMM to reflect lexicon sparsity. This
model admits a simple Gibbs sampling algorithm
where the number of latent variables is proportional
to the number of word types, rather than the size of
a corpus as for a standard HMM sampler (Johnson,
2007).
There are two key benefits of this model architec-
ture. First, it directly encodes linguistic intuitions
about POS tag assignments: the model structure
reflects the one-tag-per-word property, and a type-
level tag prior captures the skew on tag assignments
(e.g., there are fewer unique determiners than unique
nouns). Second, the reduced number of hidden vari-
ables and parameters dramatically speeds up learn-
ing and inference.
We evaluate our model on seven languages ex-
hibiting substantial syntactic variation. On several
languages, we report performance exceeding that of
state-of-the art systems. Our analysis identifies three
key factors driving our performance gain: 1) select-
ing a model structure which directly encodes tag
sparsity, 2) a type-level prior on tag assignments,
and 3) a straightforward na??ve-Bayes approach to
incorporate features. The observed performance
gains, coupled with the simplicity of model imple-
mentation, makes it a compelling alternative to ex-
isting more complex counterparts.
2 Related Work
Recent work has made significant progress on unsu-
pervised POS tagging (Me?rialdo, 1994; Smith and
Eisner, 2005; Haghighi and Klein, 2006; Johnson,
2007; Goldwater and Griffiths, 2007; Gao and John-
son, 2008; Ravi and Knight, 2009). Our work is
closely related to recent approaches that incorporate
the sparsity constraint into the POS induction pro-
cess. This line of work has been motivated by em-
pirical findings that the standard EM-learned unsu-
pervised HMM does not exhibit sufficient word tag
sparsity.
The extent to which this constraint is enforced
varies greatly across existing methods. On one end
of the spectrum are clustering approaches that assign
a single POS tag to each word type (Schutze, 1995;
Lamar et al, 2010). These clusters are computed us-
ing an SVD variant without relying on transitional
structure. While our method also enforces a singe
tag per word constraint, it leverages the transition
distribution encoded in an HMM, thereby benefiting
from a richer representation of context.
Other approaches encode sparsity as a soft con-
straint. For instance, by altering the emission distri-
bution parameters, Johnson (2007) encourages the
model to put most of the probability mass on few
tags. This design does not guarantee ?structural ze-
ros,? but biases towards sparsity. A more force-
ful approach for encoding sparsity is posterior reg-
ularization, which constrains the posterior to have
a small number of expected tag assignments (Grac?a
et al, 2009). This approach makes the training ob-
jective more complex by adding linear constraints
proportional to the number of word types, which
is rather prohibitive. A more rigid mechanism for
modeling sparsity is proposed by Ravi and Knight
(2009), who minimize the size of tagging grammar
as measured by the number of transition types. The
use of ILP in learning the desired grammar signif-
icantly increases the computational complexity of
this method.
In contrast to these approaches, our method di-
rectly incorporates these constraints into the struc-
ture of the model. This design leads to a significant
reduction in the computational complexity of train-
ing and inference.
Another thread of relevant research has explored
the use of features in unsupervised POS induc-
tion (Smith and Eisner, 2005; Berg-Kirkpatrick et
al., 2010; Hasan and Ng, 2009). These methods
demonstrated the benefits of incorporating linguis-
tic features using a log-linear parameterization, but
requires elaborate machinery for training. In our
854
work, we demonstrate that using a simple na??ve-
Bayes approach also yields substantial performance
gains, without the associated training complexity.
3 Generative Story
We consider the unsupervised POS induction prob-
lem without the use of a tagging dictionary. A graph-
ical depiction of our model as well as a summary
of random variables and parameters can be found in
Figure 1. As is standard, we use a fixed constant K
for the number of tagging states.
Model Overview The model starts by generating
a tag assignment T for each word type in a vocab-
ulary, assuming one tag per word. Conditioned on
T , features of word types W are drawn. We refer
to (T ,W ) as the lexicon of a language and ? for
the parameters for their generation; ? depends on a
single hyperparameter ?.
Once the lexicon has been drawn, the model pro-
ceeds similarly to the standard token-level HMM:
Emission parameters ? are generated conditioned on
tag assignments T . We also draw transition param-
eters ?. Both parameters depend on a single hy-
perparameter ?. Once HMM parameters (?, ?) are
drawn, a token-level tag and word sequence, (t, w),
is generated in the standard HMM fashion: a tag se-
quence t is generated from ?. The corresponding
token words w are drawn conditioned on t and ?.2
Our full generative model is given by:
P (T ,W , ?, ?, ?, t,w|?, ?) =
P (T ,W , ?|?) [Lexicon]
P (?, ?|T , ?, ?) [Parameter]
P (w, t|?, ?) [Token]
We refer to the components on the right hand side
as the lexicon, parameter, and token component re-
spectively. Since the parameter and token compo-
nents will remain fixed throughout experiments, we
briefly describe each.
Parameter Component As in the standard
Bayesian HMM (Goldwater and Griffiths, 2007),
all distributions are independently drawn from
symmetric Dirichlet distributions:
2Note that t and w denote tag and word sequences respec-
tively, rather than individual tokens or tags.
P (?, ?|T , ?, ?) =
K?
t=1
(P (?t|?)P (?t|T , ?))
The transition distribution ?t for each tag t is drawn
according to DIRICHLET(?,K), where ? is the
shared transition and emission distribution hyperpa-
rameter. In total there are O(K2) parameters asso-
ciated with the transition parameters.
In contrast to the Bayesian HMM, ?t is not
drawn from a distribution which has support for
each of the n word types. Instead, we condition
on the type-level tag assignments T . Specifically,
let St = {i|Ti = t} denote the indices of the
word types which have been assigned tag t accord-
ing to the tag assignments T . Then ?t is drawn from
DIRICHLET(?, St), a symmetric Dirichlet which
only places mass on word types indicated by St.
This ensures that each word will only be assigned
a single tag at inference time (see Section 4).
Note that while the standard HMM, has O(Kn)
emission parameters, our model has O(n) effective
parameters.3
Token Component Once HMM parameters (?, ?)
have been drawn, the HMM generates a token-level
corpus w in the standard way:
P (w, t|?, ?) =
?
(w,t)?(w,t)
?
?
?
j
P (tj |?tj?1)P (wj |tj , ?tj )
?
?
Note that in our model, conditioned on T , there is
precisely one t which has non-zero probability for
the token component, since for each word, exactly
one ?t has support.
3.1 Lexicon Component
We present several variations for the lexical com-
ponent P (T ,W |?), each adding more complex pa-
rameterizations.
Uniform Tag Prior (1TW) Our initial lexicon
component will be uniform over possible tag assign-
ments as well as word types. Its only purpose is
3This follows since each ?t has St ? 1 parameters andP
t St = n.
855
?K
T
W
N
T
Y
P
E
T
O
K
E
N
w
1
t
1
?
?
w
2
t
2
?
?
w
m
t
m
?
?
N
?
?
?
K
: Word types                           (obs)    
VARIABLES
W
: Tag assigns                         
(T
1
, . . . , T
n
)
(W
1
, . . . ,W
n
)
T
: Token word seqs  (obs)
w
: Token tag assigns (det by     )
t
?
?
PARAMETERS
T
?
: Lexicon parameters
: Token word emission parameters
: Token tag transition parameters
?
?
Figure 1: Graphical depiction of our model and summary of latent variables and parameters. The type-level tag
assignments T generate features associated with word types W . The tag assignments constrain the HMM emission
parameters ?. The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.
The hyperparameters ? and ? represent the concentration parameters of the token- and type-level components of the
model respectively. They are set to fixed constants.
to explore how well we can induce POS tags using
only the one-tag-per-word constraint. Specifically,
the lexicon is generated as:
P (T ,W |?) =P (T )P (W |T )
=
n?
i=1
P (Ti)P (Wi|Ti) =
(
1
Kn
)n
This model is equivalent to the standard HMM ex-
cept that it enforces the one-word-per-tag constraint.
Learned Tag Prior (PRIOR) We next assume
there exists a single prior distribution ? over tag as-
signments drawn from DIRICHLET(?,K). This al-
ters generation of T as follows:
P (T |?) =
n?
i=1
P (Ti|?)
Note that this distribution captures the frequency of
a tag across word types, as opposed to tokens. The
P (T |?) distribution, in English for instance, should
have very low mass for the DT (determiner) tag,
since determiners are a very small portion of the vo-
cabulary. In contrast, NNP (proper nouns) form a
large portion of vocabulary. Note that these observa-
tions are not modeled by the standard HMM, which
instead can model token-level frequency.
Word Type Features (FEATS): Past unsuper-
vised POS work have derived benefits from features
on word types, such as suffix and capitalization fea-
tures (Hasan and Ng, 2009; Berg-Kirkpatrick et al,
2010). Past work however, has typically associ-
ated these features with token occurrences, typically
in an HMM. In our model, we associate these fea-
tures at the type-level in the lexicon. Here, we con-
sider suffix features, capitalization features, punctu-
ation, and digit features. While possible to utilize
the feature-based log-linear approach described in
Berg-Kirkpatrick et al (2010), we adopt a simpler
na??ve Bayes strategy, where all features are emitted
independently. Specifically, we assume each word
type W consists of feature-value pairs (f, v). For
each feature type f and tag t, a multinomial ?tf is
drawn from a symmetric Dirichlet distribution with
concentration parameter ?. The P (W |T , ?) term
in the lexicon component now decomposes as:
P (W |T , ?) =
n?
i=1
P (Wi|Ti, ?)
=
n?
i=1
?
?
?
(f,v)?Wi
P (v|?Tif )
?
?
856
4 Learning and Inference
For inference, we are interested in the posterior
probability over the latent variables in our model.
During training, we treat as observed the language
word types W as well as the token-level corpus w.
We utilize Gibbs sampling to approximate our col-
lapsed model posterior:
P (T ,t|W ,w, ?, ?) ? P (T , t,W ,w|?, ?)
=
?
P (T , t,W ,w, ?, ?, ?,w|?, ?)d?d?d?
Note that given tag assignments T , there is only one
setting of token-level tags t which has mass in the
above posterior. Specifically, for the ith word type,
the set of token-level tags associated with token oc-
currences of this word, denoted t(i), must all take
the value Ti to have non-zero mass. Thus in the con-
text of Gibbs sampling, if we want to block sample
Ti with t(i), we only need sample values for Ti and
consider this setting of t(i).
The equation for sampling a single type-level as-
signment Ti is given by,
P (Ti, t
(i)|T?i,W , t
(?i)
,w, ?, ?) =
P (Ti|W ,T?i, ?)P (t
(i)|Ti, t
(?i)
,w, ?)
where T?i denotes all type-level tag assignment ex-
cept Ti and t(?i) denotes all token-level tags except
t
(i). The terms on the right-hand-side denote the
type-level and token-level probability terms respec-
tively. The type-level posterior term can be com-
puted according to,
P (Ti|W ,T?i, ?) ?
P (Ti|T?i, ?)
?
(f,v)?Wi
P (v|Ti, f,W?i,T?i, ?)
All of the probabilities on the right-hand-side are
Dirichlet, distributions which can be computed an-
alytically given counts.
The token-level term is similar to the standard
HMM sampling equations found in Johnson (2007).
The relevant variables are the set of token-level tags
that appear before and after each instance of the ith
word type; we denote these context pairs with the set
{(tb, ta)} and they are contained in t(?i). We use w
0 5 10 15 20 25 300.2
0.3
0.4
0.5
0.6
0.7
Iteration
1?1
 Acc
urac
y
 
 
EnglishDanishDutchGermanyPortugueseSpanishSwedish
Figure 2: Graph of the one-to-one accuracy of our full
model (+FEATS) under the best hyperparameter setting
by iteration (see Section 5). Performance typically stabi-
lizes across languages after only a few number of itera-
tions.
to represent the ith word type emitted by the HMM:
P (t
(i)|Ti, t
(?i)
,w, ?) ?
?
(tb,ta)
P (w|Ti, t
(?i)
,w
(?i)
, ?)
P (Ti|t
b
, t
(?i)
, ?)P (t
a|Ti, t
(?i)
, ?)
All terms are Dirichlet distributions and parameters
can be analytically computed from counts in t(?i)
and w(?i) (Johnson, 2007).
Note that each round of sampling Ti variables
takes time proportional to the size of the corpus, as
with the standard token-level HMM. A crucial dif-
ference is that the number of parameters is greatly
reduced as is the number of variables that are sam-
pled during each iteration. In contrast to results re-
ported in Johnson (2007), we found that the per-
formance of our Gibbs sampler on the basic 1TW
model stabilized very quickly after about 10 full it-
erations of sampling (see Figure 2 for a depiction).
5 Experiments
We evaluate our approach on seven languages: En-
glish, Danish, Dutch, German, Portuguese, Spanish,
and Swedish. On each language we investigate the
contribution of each component of our model. For
all languages we do not make use of a tagging dic-
tionary.
857
Model Hyper- English Danish Dutch German Portuguese Spanish Swedishparam. 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1
1TW best 45.2 62.6 37.2 56.2 47.4 53.7 44.2 62.2 49.0 68.4 34.3 54.4 36.0 55.3median 45.1 61.7 32.1 53.8 43.9 61.0 39.3 68.4 48.5 68.1 33.6 54.3 34.9 50.2
+PRIOR best 47.9 65.5 42.3 58.3 51.4 65.9 50.7 62.2 56.2 70.7 42.8 54.8 38.9 58.0median 46.5 64.7 40.0 57.3 48.3 60.7 41.7 68.3 52.0 70.9 37.1 55.8 36.8 57.3
+FEATS best 50.9 66.4 52.1 61.2 56.4 69.0 55.4 70.4 64.1 74.5 58.3 68.9 43.3 61.7median 47.8 66.4 43.2 60.7 51.5 67.3 46.2 61.7 56.5 70.1 50.0 57.2 38.5 60.6
Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages
under several experimental settings (Section 5). For each language and setting, we report one-to-one (1-1) and many-
to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice,
where best is defined by the 1-1 metric. The second row represents the performance of the median hyperparameter
setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see
Section 3).
Language # Tokens # Word Types # Tags
English 1173766 49206 45
Danish 94386 18356 25
Dutch 203568 28393 12
German 699605 72325 54
Portuguese 206678 28931 22
Spanish 89334 16458 47
Swedish 191467 20057 41
Table 2: Statistics for various corpora utilized in exper-
iments. See Section 5. The English data comes from
the WSJ portion of the Penn Treebank and the other lan-
guages from the training set of the CoNLL-X multilin-
gual dependency parsing shared task.
5.1 Data Sets
Following the set-up of Johnson (2007), we use
the whole of the Penn Treebank corpus for train-
ing and evaluation on English. For other languages,
we use the CoNLL-X multilingual dependency pars-
ing shared task corpora (Buchholz and Marsi, 2006)
which include gold POS tags (used for evaluation).
We train and test on the CoNLL-X training set.
Statistics for all data sets are shown in Table 2.
5.2 Setup
Models To assess the marginal utility of each com-
ponent of the model (see Section 3), we incremen-
tally increase its sophistication. Specifically, we
evaluate three variants: The first model (1TW) only
encodes the one tag per word constraint and is uni-
form over type-level tag assignments. The second
model (+PRIOR) utilizes the independent prior over
type-level tag assignments P (T |?). The final model
(+FEATS) utilizes the tag prior as well as features
(e.g., suffixes and orthographic features), discussed
in Section 3, for the P (W |T , ?) component.
Hyperparameters Our model has two Dirichlet
concentration hyperparameters: ? is the shared hy-
perparameter for the token-level HMM emission and
transition distributions. ? is the shared hyperparam-
eter for the tag assignment prior and word feature
multinomials. We experiment with four values for
each hyperparameter resulting in 16 (?, ?) combi-
nations:
? ?
0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10
Iterations In each run, we performed 30 iterations
of Gibbs sampling for the type assignment variables
W .4 We use the final sample for evaluation.
Evaluation Metrics We report three metrics to
evaluate tagging performance. As is standard, we
report the greedy one-to-one (Haghighi and Klein,
2006) and the many-to-one token-level accuracy ob-
tained from mapping model states to gold POS tags.
We also report word type level accuracy, the fraction
of word types assigned their majority tag (where the
mapping between model state and tag is determined
by greedy one-to-one mapping discussed above).5
For each language, we aggregate results in the fol-
lowing way: First, for each hyperparameter setting,
4Typically, the performance stabilizes after only 10 itera-
tions.
5We choose these two metrics over the Variation Informa-
tion measure due to the deficiencies discussed in Gao and John-
son (2008).
858
we perform five runs with different random initial-
ization of sampling state. Hyperparameter settings
are sorted according to the median one-to-one met-
ric over runs. We report results for the best and me-
dian hyperparameter settings obtained in this way.
Specifically, for both settings we report results on
the median run for each setting.
Tag set As is standard, for all experiments, we set
the number of latent model tag states to the size of
the annotated tag set. The original tag set for the
CoNLL-X Dutch data set consists of compounded
tags that are used to tag multi-word units (MWUs)
resulting in a tag set of over 300 tags. We tokenize
MWUs and their POS tags; this reduces the tag set
size to 12. See Table 2 for the tag set size of other
languages. With the exception of the Dutch data set,
no other processing is performed on the annotated
tags.
6 Results and Analysis
We report token- and type-level accuracy in Table 3
and 6 for all languages and system settings. Our
analysis and comparison focuses primarily on the
one-to-one accuracy since it is a stricter metric than
many-to-one accuracy, but also report many-to-one
for completeness.
Comparison with state-of-the-art taggers For
comparison we consider two unsupervised tag-
gers: the HMM with log-linear features of Berg-
Kirkpatrick et al (2010) and the posterior regular-
ization HMM of Grac?a et al (2009). The system
of Berg-Kirkpatrick et al (2010) reports the best
unsupervised results for English. We consider two
variants of Berg-Kirkpatrick et al (2010)?s richest
model: optimized via either EM or LBFGS, as their
relative performance depends on the language. Our
model outperforms theirs on four out of five lan-
guages on the best hyperparameter setting and three
out of five on the median setting, yielding an aver-
age absolute difference across languages of 12.9%
and 3.9% for best and median settings respectively
compared to their best EM or LBFGS performance.
While Berg-Kirkpatrick et al (2010) consistently
outperforms ours on English, we obtain substantial
gains across other languages. For instance, on Span-
ish, the absolute gap on median performance is 10%.
Top 5 Bottom 5
Gold NNP NN JJ CD NNS RBS PDT # ? ,
1TW CD WRB NNS VBN NN PRP$ WDT : MD .
+PRIOR CD JJ NNS WP$ NN -RRB- , $ ? .
+FEATS JJ NNS CD NNP UH , PRP$ # . ?
Table 5: Type-level English POS Tag Ranking: We list
the top 5 and bottom 5 POS tags in the lexicon and the
predictions of our models under the best hyperparameter
setting.
Our second point of comparison is with Grac?a
et al (2009), who also incorporate a sparsity con-
straint, but does via altering the model objective us-
ing posterior regularization. We can only compare
with Grac?a et al (2009) on Portuguese (Grac?a et al
(2009) also report results on English, but on the re-
duced 17 tag set, which is not comparable to ours).
Their best model yields 44.5% one-to-one accuracy,
compared to our best median 56.5% result. How-
ever, our full model takes advantage of word features
not present in Grac?a et al (2009). Even without fea-
tures, but still using the tag prior, our median result
is 52.0%, still significantly outperforming Grac?a et
al. (2009).
Ablation Analysis We evaluate the impact of
incorporating various linguistic features into our
model in Table 3. A novel element of our model is
the ability to capture type-level tag frequencies. For
this experiment, we compare our model with the uni-
form tag assignment prior (1TW) with the learned
prior (+PRIOR). Across all languages, +PRIOR
consistently outperforms 1TW, reducing error on av-
erage by 9.1% and 5.9% on best and median settings
respectively. Similar behavior is observed when
adding features. The difference between the feature-
less model (+PRIOR) and our full model (+FEATS)
is 13.6% and 7.7% average error reduction on best
and median settings respectively. Overall, the differ-
ence between our most basic model (1TW) and our
full model (+FEATS) is 21.2% and 13.1% for the
best and median settings respectively. One striking
example is the error reduction for Spanish, which
reduces error by 36.5% and 24.7% for the best and
median settings respectively. We observe similar
trends when using another measure ? type-level ac-
curacy (defined as the fraction of words correctly
assigned their majority tag), according to which
859
Language Metric BK10 EM BK10 LBFGS G10 FEATS Best FEATS Median
English
1-1 48.3 56.0 ? 50.9 47.8
m-1 68.1 75.5 ? 66.4 66.4
Danish
1-1 42.3 42.6 ? 52.1 43.2
m-1 66.7 58.0 ? 61.2 60.7
Dutch
1-1 53.7 55.1 ? 56.4 51.5
m-1 67.0 64.7 ? 69.0 67.3
Portuguese
1-1 50.8 43.2 44.5 64.1 56.5
m-1 75.3 74.8 69.2 74.5 70.1
Spanish
1-1 ? 40.6 ? 58.3 50.0
m-1 ? 73.2 ? 68.9 57.2
Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg-
Kirkpatrick et al, 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS
optimization algorithm; Posterior regulariation model (Grac?a et al, 2009): The G10 model uses the posterior regular-
ization approach to ensure tag sparsity constraint.
Language 1TW +PRIOR +FEATS
English 21.1 28.8 42.8
Danish 10.1 20.7 45.9
Dutch 23.8 32.3 44.3
German 12.8 35.2 60.6
Portuguese 18.4 29.6 61.5
Spanish 7.3 27.6 49.9
Swedish 8.9 14.2 33.9
Table 6: Type-level Results: Each cell report the type-
level accuracy computed against the most frequent tag of
each word type. The state-to-tag mapping is obtained
from the best hyperparameter setting for 1-1 mapping
shown in Table 3.
our full model yields 39.3% average error reduction
across languages when compared to the basic con-
figuration (1TW).
Table 5 provides insight into the behavior of dif-
ferent models in terms of the tagging lexicon they
generate. The table shows that the lexicon tag fre-
quency predicated by our full model are the closest
to the gold standard.
7 Conclusion and Future Work
We have presented a method for unsupervised part-
of-speech tagging that considers a word type and its
allowed POS tags as a primary element of the model.
This departure from the traditional token-based tag-
ging approach allows us to explicitly capture type-
level distributional properties of valid POS tag as-
signments as part of the model. The resulting model
is compact, efficiently learnable and linguistically
expressive. Our empirical results demonstrate that
the type-based tagger rivals state-of-the-art tag-level
taggers which employ more sophisticated learning
mechanisms to exploit similar constraints.
In this paper, we make a simplifying assump-
tion of one-tag-per-word. This assumption, how-
ever, is not inherent to type-based tagging models.
A promising direction for future work is to explicitly
model a distribution over tags for each word type.
We hypothesize that modeling morphological infor-
mation will greatly constrain the set of possible tags,
thereby further refining the representation of the tag
lexicon.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, and grant IIS-
0904684). We are especially grateful to Taylor Berg-
Kirkpatrick for running additional experiments. We
thank members of the MIT NLP group for their sug-
gestions and comments. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors, and do not necessar-
ily reflect the views of the funding organizations.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless un-
860
supervised learning with features. In Proceedings of
NAACL-HLT, pages 582?590.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In In Proc.
of CoNLL, pages 149?164.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the EMNLP,
pages 344?352.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs. parameter sparsity in la-
tent variable models. In Proceeding of NIPS, pages
664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of the
HLT-NAACL, pages 320?327.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of EACL,
pages 363?371.
Mark Johnson. 2007. Why doesn?t em find good hmm
pos-taggers? In Proceedings of EMNLP-CoNLL,
pages 296?305.
Michael Lamar, Yariv Maron, Marko Johnson, and Elie
Bienstock. 2010. Svd Clustering for Unsupervised
POS Tagging. In Proceedings of ACL, pages 215?219.
Bernard Me?rialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP, pages 504?512.
Hinrich Schutze. 1995. Distributional part of speech tag-
ging. In Proceedings of the EACL, pages 141?148.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
861
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456?1466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Relation Discovery using Generative Models
Limin Yao? Aria Haghighi+ Sebastian Riedel? Andrew McCallum?
? Department of Computer Science, University of Massachusetts at Amherst
+ CSAIL, Massachusetts Institute of Technology
{lmyao,riedel,mccallum}@cs.umass.edu
{aria42}@csail.mit.edu
Abstract
We explore unsupervised approaches to rela-
tion extraction between two named entities;
for instance, the semantic bornIn relation be-
tween a person and location entity. Con-
cretely, we propose a series of generative
probabilistic models, broadly similar to topic
models, each which generates a corpus of ob-
served triples of entity mention pairs and the
surface syntactic dependency path between
them. The output of each model is a cluster-
ing of observed relation tuples and their as-
sociated textual expressions to underlying se-
mantic relation types. Our proposed models
exploit entity type constraints within a relation
as well as features on the dependency path be-
tween entity mentions. We examine effective-
ness of our approach via multiple evaluations
and demonstrate 12% error reduction in preci-
sion over a state-of-the-art weakly supervised
baseline.
1 Introduction
Many NLP applications would benefit from large
knowledge bases of relational information about
entities. For instance, knowing that the entity
Steve Balmer bears the leaderOf relation to the
entity Microsoft, would facilitate question answer-
ing (Ravichandran and Hovy, 2002), data mining,
and a host of other end-user applications. Due to
these many potential applications, relation extrac-
tion has gained much attention in information ex-
traction (Kambhatla, 2004; Culotta and Sorensen,
2004; Mintz et al, 2009; Riedel et al, 2010; Yao et
al., 2010). We propose a series of generative prob-
abilistic models, broadly similar to standard topic
models, which generate a corpus of observed triples
of entity mention pairs and the surface syntactic de-
pendency path between them. Our proposed mod-
els exploit entity type constraints within a relation
as well as features on the dependency path between
entity mentions. The output of our approach is a
clustering over observed relation paths (e.g. ?X was
born in Y? and ?X is from Y?) such that expressions
in the same cluster bear the same semantic relation
type between entities.
Past work has shown that standard supervised
techniques can yield high-performance relation de-
tection when abundant labeled data exists for a
fixed inventory of individual relation types (e.g.
leaderOf ) (Kambhatla, 2004; Culotta and Sorensen,
2004; Roth and tau Yih, 2002). However, less ex-
plored are open-domain approaches where the set
of possible relation types are not fixed and little to
no labeled is given for each relation type (Banko et
al., 2007; Banko and Etzioni, 2008). A more re-
lated line of research has explored inducing rela-
tion types via clustering. For example, DIRT (Lin
and Pantel, 2001) aims to discover different repre-
sentations of the same semantic relation using dis-
tributional similarity of dependency paths. Poon
and Domingos (2008) present an Unsupervised se-
mantic parsing (USP) approach to partition depen-
dency trees into meaningful fragments (or ?parts?
to use their terminology). The combinatorial nature
of this dependency partition model makes it difficult
for USP to scale to large data sets despite several
necessary approximations during learning and infer-
1456
ence. Our work is similar to DIRT and USP in that
we induce relation types from observed dependency
paths, but our approach is a straightforward and
principled generative model which can be efficiently
learned. As we show empirically, our approach out-
performs these related works when trained with the
same amount of data and further gains are observed
when trained with more data.
We evaluate our approach using ?intrinsic? clus-
tering evaluation and ?extrinsic? evaluation settings.1
The former evaluation is performed using subset of
induced clusters against Freebase relations, a large
manually-built entity and relational database. We
also show some clusters which are not included as
Freebase relations, as well as some entity clusters
found by our approach. The latter evaluation uses
the clustering induced by our models as features for
relation extraction in distant supervision framework.
Empirical results show that we can find coherent
clusters. In relation extraction, we can achieve 12%
error reduction in precision over a state-of-the-art
weakly supervised baseline and we show that using
features from our proposed models can find more
facts for a relation without significant accuracy loss.
2 Problem and Experimental Setup
The task of relation extraction is mapping surface
textual relations to underlying semantic relations.
For instance, the textual expression ?X was born in
Y? indicates a semantic relation bornIn between en-
tities ?X? and ?Y?. This relation can be expressed
textually in several ways: for instance, ?X, a native
of Y? or ?X grew up in Y?. There are several com-
ponents to a coherent relation type, including a tight
small number of textual expressions as well as con-
straints on the entities involved in the relation. For
instance, in the bornIn relation ?X? must be a person
entity and ?Y? a location (typically a city or nation).
In this work, we present an unsupervised probabilis-
tic generative model for inducing clusters of relation
types and recognizing their textual expressions. The
set of relation types is not pre-specified but induced
from observed unlabeled data. See Table 4 for ex-
amples of learned semantic relations.
Our observed data consists of a corpus of docu-
ments and each document is represented by a bag
1See Section 4 for a fuller discussion of evaluation.
of relation tuples. Each tuple represents an ob-
served syntactic relationship between two Named
Entities (NE) and consists of three components: the
dependency path between two NE mentions, the
source argument NE, and the destination argument
NE. A dependency path is a concatenation of depen-
dency relations (edges) and words (nodes) along a
path in a dependency tree. For instance, the sentence
?John Lennnon was born in Liverpool? would yield
the relation tuple (Lennon, [? ?nsubjpass, born, ?
?in], Liverpool). This relation tuple reflects a se-
mantic bornIn relation between the John Lennon and
Liverpool entities. The dependency path in this ex-
ample corresponds to the ?X was born in Y? textual
expression given earlier. Note that for the above ex-
ample, the bornIn relation can only occur between a
person and a location. The relation tuple is the pri-
mary observed random variable in our model and we
construct our models (see Section 3) so that clusters
consist of textual expressions representing the same
underlying relation type.
3 Models
We propose three generative models for modeling
tuples of entity mention pairs and the syntactic de-
pendency path between them (see Section 2). The
first two models, Rel-LDA and Rel-LDA1 are sim-
ple extensions of the standard LDA model (Blei et
al., 2003). At the document level, our model is iden-
tical to standard LDA; a multinomial distribution
is drawn over a fixed number of relation types R.
Changes lie in the observations. In standard LDA,
the atomic observation is a word drawn from a la-
tent topic distribution determined by a latent topic
indicator variable for that word position. In our ap-
proach, a document consists of an exchangeable set
of relation tuples. Each relation tuple is drawn from
a relation type ?topic? distribution selected by a la-
tent relation type indicator variable. Relation tuples
are generated using a collection of independent fea-
tures drawn from the underlying relation type distri-
bution. These changes to standard LDA are intended
to have the effect that instead of representing seman-
tically related words, the ?topic? latent variable rep-
resents a relation type.
Our third model exploits entity type constraints
within a relation and induces clusters of relations
1457
and entities jointly. For each tuple, a set of rela-
tion level features and two latent entity type indica-
tors are drawn independently from the relation type
distribution; a collection of entity mention features
for each argument is drawn independently from the
entity type distribution selected by the entity type
indicator.
Path X, made by Y
Source Gamma Knife
Dest Elekta
Trigger make
Lex , made by the Swedish
medical technology firm
POS , VBN IN DT JJ JJ NN NN
NER pair MISC-ORG
Sync pair partmod-pobj
Table 1: The features of tuple ?(Gamma Knife, made
by, Elekta)? in sentence ?Gamma Knife, made by the
Swedish medical technology firm Elekta, focuses low-
dosage gamma radiation ...?
3.1 Rel-LDA Model
This model is an extension to the standard LDA
model. At the document level, a multinomial dis-
tribution over relations ?doc is drawn from a prior
Dir(?). To generate a relation tuple, we first draw a
relation ?topic? r from Multi(?). Then we generate
each feature f of a tuple independently from a multi-
nomial distribution Multi(?rf ) selected by r. In this
model, each tuple has three features, i.e. its three
components, shown in the first three rows in Table 1.
Figure 1 shows the graphical representation of Rel-
LDA. Table 2 lists all the notation used in describing
our models.
The learning process of the models is an EM pro-
cess. The procedure is similar to that used by the
standard topic model. In the variational E-step (in-
ference), we sample the relation type indicator for
each tuple using p(r|f):
P (r|f(p, s, d)) ? p(r)?f p(f |r)
? (?r + nr|d)
?
f
?f+nf |rP
f ? (?f ?+nf ?|r)
|R| Number of relations
|D| Number of documents
r A relation
doc A document
p, s, d Dep path, source and dest args
f A feature/feature type
T Entity type of one argument
? Dirichlet prior for ?doc
?x Dirichlet prior for ?rx
? Dirichlet prior for ?t
?doc p(r|doc)
?rx p(x|r)
?t p(fs|T ), p(fd|T )
Table 2: The notation used in our models
      
  
             |R|  
      
  
  
......
                                        N 
r
f
?
?
rf
?
?
f
f
      
  
  
                           
                           
                                             |D|  
Figure 1: Rel-LDA model. Shaded circles are observa-
tions, and unshaded ones are hidden variables. A docu-
ment consists of N tuples. Each tuple has a set of fea-
tures. Each feature of a tuple is generated independently
from a hidden relation variable r.
p(r) and p(f |r) are estimated in the M-step:
?doc =
?+ nr|doc?
r?(?+ nr?|doc)
?rf =
?f + nf |r?
f ?(?f ? + nf ?|r)
where nf |r indicates the number of times a feature f
is assigned with r.
3.2 Rel-LDA1
Looking at results of Rel-LDA, we find the clus-
ters sometimes are in need of refinement, and we
can address this by adding more features. For in-
stance, adding trigger features can encourage spar-
sity over dependency paths. We define trigger words
as all the words on the dependency path except stop
words. For example, from path ?X, based in Y?,
?base? is extracted as a trigger word. The intuition
1458
for using trigger words is that paths sharing the same
set of trigger words should go to one cluster. Adding
named entity tag pair can refine the cluster too. For
example, a cluster found by Rel-LDA contains ?X
was born in Y? and ?X lives in Y?; but it also con-
tains ?X, a company in Y?. In this scenario, adding
features ?PER-LOC? and ?ORG-LOC? can push the
model to split the clusters into two and put the third
case into a new cluster.
Hence we propose Rel-LDA1. It is similar to
Rel-LDA, except that each tuple is represented with
more features. Besides p, s, and d, we introduce
trigger words, lexical pattern, POS tag pattern, the
named entity pair and the syntactic category pair fea-
tures for each tuple. Lexical pattern is the word se-
quence between the two arguments of a tuple and
POS tag pattern is the POS tag sequence of the lexi-
cal pattern. See Table 1 as an example.
Following typical EM learning(Charniak and El-
sner, 2009), we start with a much simpler genera-
tive model, expose the model to fewer features first,
and iteratively add more features. First, we train a
Rel-LDA model, i.e. the model only generates the
dependency path, source and destination arguments.
After each interval of 10 iterations, we introduce one
additional feature. We add the features in the order
of trigger, lexical pattern, POS, NER pair, and syn-
tactic pair.
3.3 Type-LDA model
We know that relations can only hold between
certain entity types, known as selectional prefer-
ences (Ritter et al, 2010; Seaghdha, 2010; Kozareva
and Hovy, 2010). Hence we propose Type-LDA
model. This model can capture the selectional pref-
erences of relations to their arguments. In the mean
time, it clusters tuples into relational clusters, and
arguments into different entity clusters. The entity
clusters could be interesting in many ways, for ex-
ample, defining fine-grained entity types and finding
new concepts.
We split the features of a tuple into relation level
features and entity level features. Relation level fea-
tures include the dependency path, trigger, lex and
POS features; entity level features include the entity
mention itself and its named entity tag.
The generative storyline is as follows. At the doc-
ument level, a multinomial distribution over rela-
      
  
  
                                        N   
      
  
  
        
                      
                                                   |D|  
      
  
             |R|  
r
f
f
s
?
?
rf
?
t
f
d
     
  
              |R| 
?
rt2
?
?
t2
?
?
f
T
1
T
2
     
  
              |T| 
     
  
              |R| 
?
rt1
?
t1
Figure 2: Type-LDA model. Each document consists of
N tuples. Each tuple has a set of features, relation level
features f and entity level features of source argument fs
and destination argument fd. Relation level features and
two hidden entity types T1 and T2 are generated from
hidden relation variable r independently. Source entity
features are generated from T1 and destination features
are generated from T2.
tions ?doc is drawn from a Dirichlet prior. A doc-
ument consists of N relation tuples. Each tuple is
represented by relation level features (f ) and entity
level features of source argument (fs) and destina-
tion argument (fd). For each tuple, a relation r is
drawn from Multi(?doc). The relation level features
and two hidden entity types T1 and T2 are indepen-
dently generated from r. Features fs are generated
from T1 and fd from T2. Figure 2 shows the graphi-
cal representation of this model.
At inference time, we sample r, T1 and T2 for
each tuple. For efficient inference, we first initialize
the model without T1 and T2, i.e. all the features are
generated directly from r. Here the model degener-
ates to Rel-LDA1. After some iterations, we intro-
duce T1 and T2. We sample the relation variable (r)
and two mention types variables (T1,T2) iteratively
for each tuple. We can sample them together, but
this is not very efficient. In addition, we found that
it does not improve performance.
4 Experiments
Our experiments are carried out on New York Times
articles from year 2000 to 2007 (Sandhaus, 2008).
We filter out some noisy documents, for example,
1459
obituary content, lists and so on. Obituary arti-
cles often contain syntax that diverges from stan-
dard newswire text. This leads to parse errors with
WSJ-trained parsers and in turn, makes extraction
harder. We also filter out documents that contain
lists or tables of items (such as books, movies) be-
cause this semi-structured information is not the fo-
cus of our current work. After filtering we are left
with approximately 428K documents. They are pre-
processed in several steps. First we employ Stanford
tools to tokenize, sentence-split and Part-Of-Speech
tag (Toutanova et al, 2003) a document. Next we
recognize named entities (Finkel et al, 2005) by
labelling tokens with PERSON, ORGANIZATION,
LOCATION, MISC and NONE tags. Consecutive
tokens which share the same category are assembled
into entity mentions. They serve as source and des-
tination arguments of the tuples we seek to model.
Finally we parse each sentence of a document using
MaltParser (Nivre et al, 2004) and extract depen-
dency paths for each pair of named entity mentions
in one sentence.
Following DIRT (Lin and Pantel, 2001), we fil-
ter out tuples that do not satisfy the following con-
straints. First, the path needs to be shorter than
10 edges, since longer paths occur less frequently.
Second, the dependency relations in the path should
connect two content words, i.e. nouns, verbs, ad-
jectives and adverbs. For example, in phrase ?solve
a problem?, ?obj(solve, problem)? is kept, while
?det(problem, a)? is discarded. Finally, the de-
pendency labels on the path must not be: ?conj?,
?ccomp?, ?parataxis?, ?xcomp?, ?pcomp?, ?advcl?,
?punct?, and ?infmod?. This selection is based on the
observation that most of the times the corresponding
dependency relations do not explicitly state a rela-
tion between two candidate arguments.
After all entity mentions are generated and paths
are extracted, we have nearly 2.5M tuples. After
clustering (inference), each of these tuple will be-
long to one cluster/relation and is associated with its
clusterID.
We experimented with the number of clusters and
find that in a range of 50-200 the performance does
not vary significantly with different numbers. In our
experiments, we cluster the tuples into 100 relation
clusters for all three models. For Type-LDA model,
we use 50 entity clusters.
We evaluate our models in two ways. The first
aims at measuring the clustering quality by mapping
clusters to Freebase relations. The second seeks to
assess the utility of our predicted clusters as features
for relation extraction.
4.1 Relations discovered by different models
Looking closely at the clusters we predict, we find
that some of them can be mapped to Freebase rela-
tions. We discover clusters that roughly correspond
to the parentCom (parent company relation), filmDi-
rector, authorOf, comBase (base of a company rela-
tion) and dieIn relations in Freebase. We treat Free-
base annotations as ground truth and measure recall.
We count each tuple in a cluster as true positive if
Freebase states the corresponding relation between
its argument pair. We find that precision numbers
against Freebase are low, below 10%. However,
these numbers are not reliable mainly because many
correct instances found by our models are missing
in Freebase. One reason why our predictions are
missing in Freebase is coreference. For example,
we predict parentCom relation between ?Linksys?
and ?Cisco?, while Freebase only considers ?Cisco
Systems, Inc.? as the parent company of ?Linksys?.
It does not corefer ?Cisco? to ?Cisco Systems, Inc.?.
Incorporating coreference in our model may fix this
problem and is a focus of future work. Instead of
measuring precision against Freebase, we ask hu-
mans to label 50 instances for each cluster and report
precision according to this annotated data. Table 3
shows the scores.
We can see that in most cases Rel-LDA1 and
Type-LDA substantially outperform the Rel-LDA
model. This is due to the fact that both models can
exploit more features to make clustering decisions.
For example, in Rel-LDA1 model, the NER pair fea-
ture restricts the entity types the two arguments can
take.
In the following, we take parentCom relation as
an example to analyze the behaviors of different
models. Rel-LDA includes spurious instances such
as ?A is the chief executive of B?, while Rel-LDA1
has fewer such instances due to the NER pair fea-
ture. Similarly, by explicitly modeling entity type
constraints, Type-LDA makes fewer such errors. All
our models make mistakes when sentences have co-
ordination structures on which the parser has failed.
1460
Rel. Sys. Rec. Prec.
parentCom
Rel-LDA 51.4 76.0
Rel-LDA1 49.5 78.0
Type-LDA 55.3 72.0
filmDirector
Rel-LDA 42.5 32.0
Rel-LDA1 70.5 40.0
Type-LDA 74.2 26.0
comBase
Rel-LDA 31.5 12.0
Rel-LDA1 54.2 22.0
Type-LDA 57.1 30.0
authorOf
Rel-LDA 25.2 84.0
Rel-LDA1 46.9 86.0
Type-LDA 20.2 68.0
dieIn
Rel-LDA 26.5 34.0
Rel-LDA1 55.9 40.0
Type-LDA 50.2 28.0
Table 3: Clustering quality evaluation (%), Rec. is mea-
sured against Freebase, Prec. is measured according to
human annotators
For example, when a sentence has the following pat-
tern ?The winners are A, a part of B; C, a part of
D; E, a part of F?, our models may predict parent-
Com(A,F), because the parser connects A with F via
the pattern ?a part of?.
Some clusters found by our models cannot be
mapped to Freebase relations. Consider the Free-
base relation worksFor as one example. This re-
lation subsumes all types of employment relation-
ships, irrespective of the role the employee plays for
the employer. By contrast, our models discover clus-
ters such as leaderOf, editorOf that correspond to
more specific roles an employee can have. We show
some example relations in Table 4. In the table, the
2nd row shows a cluster of employees of news media
companies; the 3rd row shows leaders of companies;
the last one shows birth and death places of persons.
We can see that the last cluster is noisy since we
do not handle antonyms in our models. The argu-
ments of the clusters have noise too. For example,
?New York? occurs as a destination argument in the
2nd cluster. This is because ?New York? has high
frequency in the corpus and it brings noise to the
clustering results. In Table 5 some entity clusters
found by Type-LDA are shown. We find different
types of companies, such as financial companies and
news companies. We also find subclasses of person,
for example, reviewer and politician, because these
different entity classes participate in different rela-
tions. The last cluster shown in the table is a mix-
ture of news companies and government agencies.
This may be because this entity cluster is affected
by many relations.
4.2 Distant Supervision based Relation
Extraction
Our generative models detect clusters of dependency
paths and their arguments. Such clusters are inter-
esting in their own right, but we claim that they can
also be used to help a supervised relation extractor.
We validate this hypothesis in the context of relation
extraction with distant supervision using predicted
clusters as features.
Following previous work (Mintz et al, 2009), we
use Freebase as our distant supervision source, and
align related entity pairs to the New York Times arti-
cles discussed earlier. Our training and test instances
are pairs of entities for which both arguments appear
in at least one sentence together. Features of each
instance are extracted from all sentences in which
both entities appear together. The gold label for each
instance comes from Freebase. If a pair of entities
is not related according to Freebase, we consider it
a negative example. Note that this tends to create
some amount of noise: some pairs may be related,
but their relationships are not yet covered in Free-
base.
After filtering out relations with fewer than 10 in-
stances we have 65 relations and an additional ?O?
label for unrelated pairs of entities. We call related
instances positive examples and unrelated instances
negative examples.
We train supervised classifiers using maximum
entropy. The baseline classifier employs features
that Mintz et al (2009) used. To extract features
from the generative models we proceed as follows.
For each pair of entities, we collect all tuples asso-
ciated with it. For each of these tuples we extract its
clusterID, and use this ID as a binary feature.
The baseline system without generative model
features is called Distant. The classifiers with ad-
ditional features from generative models are named
after the generative models. Thus we have Rel-LDA,
Rel-LDA1 and Type-LDA classifiers. We compare
1461
Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB Worldwide
Path X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of Y
Dest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis Groupe
Source Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrath
Path X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;
Dest The Times, The New York Times, Vogue, Vanity Fair, New York
Source Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill Gates
Path X, the executive of Y; X, Y?s executive; X, Y executive; X, the chairman of Y; X, Y?s chairman
Dest Enron, Microsoft, WorldCom, Citigroup, Nassau County
Source Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve Schmidt
Path X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of Y
Dest White House, Justice Department, Pentagon, United States, State Department
Source United Nations, Microsoft, Intel, Internet, M. D. Anderson
Path X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in Y
Dest New York, Washington, Manhattan, Chicago, London
Source Army, Shiite, Navy, John, David
Path X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at Y
Dest Manhattan, World War II, Brooklyn, Los Angeles, New York
Table 4: The path, source and destination arguments of some relations found by Rel-LDA1.
Company Microsoft, Enron, NBC, CBS, Disney
FinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First Boston
News Notebook, New Yorker, Vogue, Vanity Fair, Newsweek
SportsTeam Yankees, Mets, Giants, Knicks, Jets
University University of California, Harvard, Columbia University, New York University, University of Penn.
Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck
Games World Series, Olympic, World Cup, Super Bowl, Olympics
Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove
Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals
News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review
Table 5: The entity clusters found by Type-LDA
these against Distant and the DIRT database. For
the latter we parse our data using Minipar (Lin,
1998) and extract dependency paths between pairs
of named entity mentions. For each path, the top 3
similar paths are extracted from DIRT database. The
Minipar path and the similar paths are used as addi-
tional features.
For held-out evaluation, we construct the training
data from half of the positive examples and half of
the negative examples. The remaining examples are
used as test data. Note that the number of negative
instances is more than 10 times larger than the num-
ber of positive instances. At test time, we rank the
predictions by the conditional probabilities obtained
from the Maximum Entropy classifier. We report
precision of top ranked 50 instances for each relation
in table 6. From the table we can see that all systems
using additional features outperform the Distant sys-
tem. In average, our best model achieves 4.1%
improvement over the distant supervision baseline,
12% error reduction. The precision of bornIn is low
because in most cases we predict bornIn instances
as liveIn.
We expect systems using generative model fea-
tures to have higher recall than the baseline. This
is difficult to measure, but precision in the high re-
call area is a signal. We look at top ranked 1000
instances of each system and show the precision in
the last row of the table. We can see that our best
model Type-LDA outperforms the distant supervi-
sion baseline by 4.5%.
Why do generative model features help to im-
1462
Relation Dist Rel Rel1 Type DIRT
worksFor 80.0 92.0 86.0 90.0 84.0
authorOf 98.0 98.0 98.0 98.0 98.0
containedBy 92.0 96.0 96.0 92.0 96.0
bornIn 16.0 18.0 22.0 24.0 10.0
dieIn 28.0 30.0 28.0 24.0 24.0
liveIn 50.0 52.0 54.0 54.0 56.0
nationality 92.0 94.0 90.0 90.0 94.0
parentCom 94.0 96.0 96.0 96.0 90.0
founder 65.2 76.3 61.2 64.0 68.3
parent 52.0 54.0 50.0 52.0 52.0
filmDirector 54.0 60.0 60.0 64.0 62.0
Avg 65.6 69.7 67.4 68.0 66.8
Prec@1K 82.8 85.8 85.3 87.3 82.8
Table 6: Precision (%) of some frequent relations
prove relation extraction? One reason is that gen-
erative models can transfer information from known
patterns to unseen patterns. For example, given
?Sidney Mintz, the great food anthropologist at
Johns Hopkins University?, we want to predict the
relation between ?Sidney Mintz? and ?Johns Hopkins
University?. The distant supervision system incor-
rectly predicts the pair as ?O? since it has not seen
the path ?X, the anthropologist at Y? in the training
data. By contrast, Rel-LDA can predict this pair cor-
rectly as worksFor because the dependency path of
this pair is in a cluster which contains the path ?X, a
professor at Y?.
In addition to held-out evaluation we also carry
out manual evaluation. To this end, we use all the
positive examples and randomly select five times
the number of positive examples as negative ex-
amples to train a classifier. The remaining nega-
tive examples are candidate instances. We rank the
predicted instances according to their classification
scores. For each relation, we ask human annotators
to judge its top ranked 50 instances.
Table 7 lists the manual evaluation results for
some frequent relations. We also list how many in-
stances are found for each relation. For almost all
the relations, systems using generative model fea-
tures find more instances. In terms of precision, our
models perform comparatively to the baseline, even
better for some relations.
We also notice that clustering quality is not con-
sistent with distant supervision performance. Rel-
LDA1 can find better clusters than Rel-LDA but it
has lower precision in held-out evaluation. Type-
LDA underperforms Rel-LDA in average precision
but it gets higher precision in a higher recall area, i.e.
precision at 1K. One possible reason for the incon-
sistency is that the baseline distant supervision sys-
tem already employs features that are used in Rel-
LDA1. Another reason may be that the clusters do
not overlap with Freebase relations very well, see
section 4.1.
4.3 Comparing against USP
We also try to compare against USP (Poon and
Domingos, 2008). Due to memory requirements of
USP, we are only able to run it on a smaller data
set consisting of 1,000 NYT documents; this is three
times the amount of data Poon and Domingos (2008)
used to train USP.2 For distant supervision based re-
lation extraction, we only match about 500 Freebase
instances to this small data set.
USP provides a parse tree for each sentence and
for each mention pair we can extract a path from
the tree. Since USP provides clusters of words and
phrases, we use the USP clusterID associated with
the words on the path as binary features in the clas-
sifier.
All models are less accurate when trained on this
smaller dataset; we can do as well as USP does,
even a little better. USP achieves 8.6% in F1, Rel-
LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% and
Distant 10.3%. Of course, given larger datasets,
the performance of Rel-LDA, Rel-LDA1, and Type-
LDA improves considerably. In summary, compar-
ing against USP, our approach scales much more
easily to large data.
5 Related Work
Many approaches have been explored in relation ex-
traction, including bootstrapping, supervised classi-
fication, distant supervision, and unsupervised ap-
proaches.
Bootstrapping employs a few labeled examples
for each relation, iteratively extracts patterns from
the labeled seeds, and uses the patterns to extract
2Using the publicly released USP code, training a model
with 1,000 documents resulted in about 45 gigabytes of heap
space in the JVM.
1463
Relation Top 50 (%) #InstancesDist Rel Type Dist Rel Type
worksFor 100.0 100.0 100.0 314 349 349
authorOf 94.0 94.0 96.0 185 208 229
containedBy 98.0 98.0 98.0 670 714 804
bornIn 82.6 88.2 88.0 46 36 56
dieIn 100.0 100.0 100.0 167 176 231
liveIn 98.0 98.0 94.0 77 86 109
nationality 78.0 82.0 76.0 84 92 114
parentCom 79.2 77.4 85.7 24 31 28
founder 80.0 80.0 50.0 5 5 14
parent 97.0 92.3 94.7 33 39 38
filmDirector 92.6 96.9 97.1 27 32 34
Table 7: Manual evaluation, Precision and recall of some frequent relations
more seeds (Brin, 1998). This approach may suffer
from low recall since the patterns can be too specific.
Supervised learning can discover more general
patterns (Kambhatla, 2004; Culotta and Sorensen,
2004). However, this approach requires labeled data,
and most work only carry out experiments on small
data set.
Distant supervision for relation extraction re-
quires no labeled data. The approach takes some
existing knowledge base as supervision source,
matches its relational instances against the text cor-
pus to build the training data, and extracts new in-
stances using the trained classifiers (Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010; Yao et al, 2010).
All these approaches can not discover new rela-
tions and classify instances which do not belong to
any of the predefined relations. Other past work has
explored inducing relations using unsupervised ap-
proaches.
For example, DIRT (Lin and Pantel, 2001) aims
to discover different representations of the same se-
mantic relation, i.e. similar dependency paths. They
employ the distributional similarity based approach
while we use generative models. Both DIRT and our
approach take advantage of the arguments of depen-
dency paths to find semantic relations. Moreover,
our approach can cluster the arguments into differ-
ent types.
Unsupervised semantic parsing (USP) (Poon and
Domingos, 2008) discovers relations by merging
predicates which have similar meanings; it proceeds
to recursively cluster dependency tree fragments (or
?parts?) to best explain the observed sentence. It is
not focused on capturing any particular kind of re-
lation between sentence constituents, but to capture
repeated patterns. Our approach differs in that we
are focused on capturing a narrow range of binary
relations between named entities; some of our mod-
els (see Section 3) utilize entity type information to
constraint relation type induction. Also, our models
are built to be scalable and trained on a very large
corpus. In addition, we use a distant supervision
framework for evaluation.
Relation duality (Bollegala et al, 2010) employs
co-clustering to find clusters of entity pairs and pat-
terns. They identify each cluster of entity pairs as a
relation by selecting representative patterns for that
relation. This approach is related to our models,
however, it does not identify any entity clusters.
Generative probabilistic models are widely em-
ployed in relation extraction. For example, they are
used for in-domain relation discovery while incorpo-
rating constraints via posterior regularization (Chen
et al, 2011). We are focusing on open domain re-
lation discovery. Generative models are also ap-
plied to selectional preference discovery (Ritter et
al., 2010; Seaghdha, 2010). In this scenario, the
authors assume relation labels are given while we
automatically discover relations. Generative models
are also used in unsupervised coreference (Haghighi
and Klein, 2010).
1464
Clustering is also employed in relation extraction.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words
intervening between them. Their approach is not
probabilistic. Researchers also use topic models to
perform dimension reduction on features when they
cluster relations (Hachey, 2009). However, they do
not explicitly model entity types.
Open information extraction aims to discover re-
lations independent of specific domains and rela-
tions (Banko et al, 2007; Banko and Etzioni, 2008).
A self-learner is employed to extract relation in-
stances but the systems do not cluster the instances
into relations. Yates and Etzioni (2009) present RE-
SOLVER for discovering relational synonyms as a
post processing step. Our approach integrates entity
and relation discovery in a probabilistic model.
6 Conclusion
We have presented an unsupervised probabilistic
generative approach to relation extraction between
two named entities. Our proposed models exploit
entity type constraints within a relation as well
as features on the dependency path between entity
mentions to cluster equivalent textual expressions.
We demonstrate the effectiveness of this approach
by comparing induced relation clusters against a
large knowledge base. We also show that using clus-
ters of our models as features in distant supervised
framework yields 12% error reduction in precision
over a weakly supervised baseline and outperforms
other state-of-the art relation extraction techniques.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181, ITR#1, and NSF
MALLET. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors? and do not necessarily reflect those of the
sponsor.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proc. of WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Eugene Charniak and Micha Elsner. 2009. Em works for
pronoun anaphora resolution. In Proceedings of ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
1465
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of ACL 10.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Hoifung Poon and Pedro Domingos. 2008. Unsuper-
vised semantic parsing. In Proceedings of the Confer-
ence on Empirical methods in natural language pro-
cessing (EMNLP).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of ACL10.
Dan Roth and Wen tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In Proceedings
of Coling.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL, pages 252?259.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
1466
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 385?393,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Coreference Resolution in a Modular, Entity-Centered Model
Aria Haghighi
Computer Science Division
University of California, Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California, Berkeley
klein@cs.berkeley.edu
Abstract
Coreference resolution is governed by syntac-
tic, semantic, and discourse constraints. We
present a generative, model-based approach in
which each of these factors is modularly en-
capsulated and learned in a primarily unsu-
pervised manner. Our semantic representation
first hypothesizes an underlying set of latent
entity types, which generate specific entities
that in turn render individual mentions. By
sharing lexical statistics at the level of abstract
entity types, our model is able to substantially
reduce semantic compatibility errors, result-
ing in the best results to date on the complete
end-to-end coreference task.
1 Introduction
Coreference systems exploit a variety of informa-
tion sources, ranging from syntactic and discourse
constraints, which are highly configurational, to se-
mantic constraints, which are highly contingent on
lexical meaning and world knowledge. Perhaps be-
cause configurational features are inherently easier
to learn from small data sets, past work has often
emphasized them over semantic knowledge.
Of course, all state-of-the-art coreference systems
have needed to capture semantic compatibility to
some degree. As an example of nominal headword
compatibility, a ?president? can be a ?leader? but
cannot be not an ?increase.? Past systems have of-
ten computed the compatibility of specific headword
pairs, extracted either from lexical resources (Ng,
2007; Bengston and Roth, 2008; Rahman and Ng,
2009), web statistics (Yang et al, 2005), or sur-
face syntactic patterns (Haghighi and Klein, 2009).
While the pairwise approach has high precision, it is
neither realistic nor scalable to explicitly enumerate
all pairs of compatible word pairs. A more compact
approach has been to rely on named-entity recog-
nition (NER) systems to give coarse-grained entity
types for each mention (Soon et al, 1999; Ng and
Cardie, 2002). Unfortunately, current systems use
small inventories of types and so provide little con-
straint. In general, coreference errors in state-of-the-
art systems are frequently due to poor models of se-
mantic compatibility (Haghighi and Klein, 2009).
In this work, we take a primarily unsupervised ap-
proach to coreference resolution, broadly similar to
Haghighi and Klein (2007), which addresses this is-
sue. Our generative model exploits a large inven-
tory of distributional entity types, including standard
NER types like PERSON and ORG, as well as more
refined types like WEAPON and VEHICLE. For each
type, distributions over typical heads, modifiers, and
governors are learned from large amounts of unla-
beled data, capturing type-level semantic informa-
tion (e.g. ?spokesman? is a likely head for a PER-
SON). Each entity inherits from a type but captures
entity-level semantic information (e.g. ?giant? may
be a likely head for the Microsoft entity but not all
ORGs). Separately from the type-entity semantic
module, a log-linear discourse model captures con-
figurational effects. Finally, a mention model assem-
bles each textual mention by selecting semantically
appropriate words from the entities and types.
Despite being almost entirely unsupervised, our
model yields the best reported end-to-end results on
a range of standard coreference data sets.
2 Key Abstractions
The key abstractions of our model are illustrated in
Figure 1 and described here.
Mentions: A mention is an observed textual ref-
erence to a latent real-world entity. Mentions are as-
385
Person
[0: 0.30,
 1:0.25,
 2:0.20, ...]
NOM-HEAD
[1: 0.39,
 0:0.18,
 2:0.13, ...]
[Obama: 0.02,
  Smith:0.015,
  Jr.: 0.01, ...]
[president: 0.14,
 painter:0.11,
 senator: 0.10,...]
NAM-HEAD
r ?r fr
NOM-HEAD [president, leader]
NAM-HEAD [Obama, Barack]
r Lr
Barack Obama
NOM-HEAD [painter]
NAM-HEAD [Picasso, Pablo]
r Lr
Pablo Picasso
NN-MOD Mr.
NAM-HEAD Obama
r wr
NOM-HEAD president
r wr
Types
Entities
Mentions
(c)
(b)
(a)
?Mr. Obama? ?the president?
... ... ...
Figure 1: The key abstractions of our model (Section 2).
(a) Mentions map properties (r) to words (wr). (b) Enti-
ties map properties (r) to word lists (Lr). (c) Types map
properties (r) to distributions over property words (?r)
and the fertilities of those distributions (fr). For (b) and
(c), we only illustrate a subset of the properties.
sociated with nodes in a parse tree and are typically
realized as NPs. There are three basic forms of men-
tions: proper (denoted NAM), nominal (NOM), and
pronominal (PRO). We will often describe proper
and nominal mentions together as referring men-
tions.
We represent each mention M as a collection of
key-value pairs. The keys are called properties and
the values are words. For example, the left mention
in Figure 1(a) has a proper head property, denoted
NAM-HEAD, with value ?Obama.? The set of prop-
erties we consider, denoted R, includes several va-
rieties of heads, modifiers, and governors (see Sec-
tion 5.2 for details). Not every mention has a value
for every property.
Entities: An entity is a specific individual or ob-
ject in the world. Entities are always latent in text.
Where a mention has a single word for each prop-
erty, an entity has a list of signature words. For-
mally, entities are mappings from properties r ? R
to lists Lr of ?canonical? words which that entity
uses for that property. For instance in Figure 1(b),
the list of nominal heads for the Barack Obama en-
tity includes ?president.?
Types: Coreference systems often make a men-
tion / entity distinction. We extend this hierarchy
to include types, which represent classes of entities
(PERSON, ORGANIZATION, and so on). Types allow
the sharing of properties across entities and mediate
the generation of entities in our model (Section 3.1).
See Figure 1(c) for a concrete example.
We represent each type ? as a mapping between
properties r and pairs of multinomials (?r, fr). To-
gether, these distributions control the lists Lr for en-
tities of that type. ?r is a unigram distribution of
words that are semantically licensed for property r.
fr is a ?fertility? distribution over the integers that
characterizes entity list lengths. For example, for the
type PERSON, ?r for proper heads is quite flat (there
are many last names) but fr is peaked at 1 (people
have a single last name).
3 Generative Model
We now describe our generative model. At the pa-
rameter level, we have one parameter group for the
types ? = (?, ?1, . . . , ?t), where ? is a multinomial
prior over a fixed number t of types and the {?i} are
the parameters for each individual type, described in
greater detail below. A second group comprises log-
linear parameters pi over discourse choices, also de-
scribed below. Together, these two groups are drawn
according to P (? |?)P (pi|?2), where ? and ?2 are a
small number of scalar hyper-parameters described
in Section 4.
Conditioned on the parameters (? ,pi), a docu-
ment is generated as follows: A semantic module
generates a sequence E of entities. E is in prin-
ciple infinite, though during inference only a finite
number are ever instantiated. A discourse module
generates a vector Z which assigns an entity in-
dex Zi to each mention position i. Finally, a men-
tion generation module independently renders the
sequence of mentions (M) from their underlying en-
tities. The syntactic position and structure of men-
tions are treated as observed, including the mention
forms (pronominal, etc.). We use X to refer to this
ungenenerated information. Our model decomposes
as follows:
P (E,Z,M|? ,pi,X) =
P (E|? ) [Semantic, Section 3.1]
P (Z|pi,X) [Discourse, Section 3.2]
P (M|Z,E, ? ) [Mention, Section 3.3]
We detail each of these components in subsequent
sections.
386
TLr
?
fr ?r
ORG: 0.30
 PERS: 0.22
GPE: 0.18
LOC: 0.15
WEA: 0.12
VEH: 0.09
...
T = PERS
0: 0.30
1: 0.25
2: 0.20
3: 0.18
...
PERS
For T = PERS
president: 0.14
 painter: 0.11
senator: 0.10
minister: 0.09
leader: 0.08
official: 0.06
executive: 0.05
...
president 
leader
official
R
E
Figure 2: Depiction of the entity generation process (Sec-
tion 3.1). Each entity draws a type (T ) from ?, and, for
each property r ? R, forms a word list (Lr) by choosing
a length from T ?s fr distribution and then independently
drawing that many words from T ?s ?r distribution. Ex-
ample values are shown for the person type and the nom-
inal head property (NOM-HEAD).
3.1 Semantic Module
The semantic module is responsible for generating
a sequence of entities. Each entity E is generated
independently and consists of a type indicator T , as
well as a collection {Lr}r?R of word lists for each
property. These elements are generated as follows:
Entity Generation
Draw entity type T ? ?
For each mention property r ? R,
Fetch {(fr, ?r)} for ? T
Draw word list length |Lr| ? fr
Draw |Lr| words from w ? ?r
See Figure 2 for an illustration of this process. Each
word list Lr is generated by first drawing a list
length from fr and then independently populating
that list from the property?s word distribution ?r.1
Past work has employed broadly similar distribu-
tional models for unsupervised NER of proper men-
1There is one exception: the sizes of the proper and nomi-
nal head property lists are jointly generated, but their word lists
are still independently populated.
tions (Collins and Singer, 1999; Elsner et al, 2009).
However, to our knowledge, this is the first work
to incorporate such a model into an entity reference
process.
3.2 Discourse Module
The discourse module is responsible for choosing
an entity to evoke at each of the n mention posi-
tions. Formally, this module generates an entity as-
signment vector Z = (Z1, . . . , Zn), where Zi indi-
cates the entity index for the ith mention position.
Most linguistic inquiry characterizes NP anaphora
by the pairwise relations that hold between a men-
tion and its antecedent (Hobbs, 1979; Kehler et al,
2008). Our discourse module utilizes this pairwise
perspective to define each Zi in terms of an interme-
diate ?antecedent? variable Ai. Ai either points to a
previous antecedent mention position (Ai < i) and
?steals? its entity assignment or begins a new entity
(Ai = i). The choice ofAi is parametrized by affini-
ties spi(i, j;X) between mention positions i and j.
Formally, this process is described as:
Entity Assignment
For each mention position, i = 1, . . . , n,
Draw antecedent position Ai ? {1, . . . , i}:
P (Ai = j|X) ? spi(i, j;X)
Zi =
{
ZAi , if Ai < i
K + 1, otherwise
Here, K denotes the number of entities allocated in
the first i-1 mention positions. This process is an in-
stance of the sequential distance-dependent Chinese
Restaurant Process (DD-CRP) of Blei and Frazier
(2009). During inference, we variously exploit both
the A and Z representations (Section 4).
For nominal and pronoun mentions, there are sev-
eral well-studied anaphora cues, including centering
(Grosz et al, 1995), nearness (Hobbs, 1978), and
deterministic constraints, which have all been uti-
lized in prior coreference work (Soon et al, 1999;
Ng and Cardie, 2002). In order to combine these
cues, we take a log-linear, feature-based approach
and parametrize spi(i, j;X) = exp{pi>fX(i, j)},
where fX(i, j) is a feature vector over mention po-
sitions i and j, and pi is a parameter vector; the fea-
tures may freely condition on X. We utilize the
following features between a mention and an an-
387
tecedent: tree distance, sentence distance, and the
syntactic positions (subject, object, and oblique) of
the mention and antecedent. Features for starting a
new entity include: a definiteness feature (extracted
from the mention?s determiner), the top CFG rule
of the mention parse node, its syntactic role, and a
bias feature. These features are conjoined with the
mention form (nominal or pronoun). Additionally,
we restrict pronoun antecedents to the current and
last two sentences, and the current and last three sen-
tences for nominals. Additionally, we disallow nom-
inals from having direct pronoun antecedents.
In addition to the above, if a mention is in a de-
terministic coreference configuration, as defined in
Haghighi and Klein (2009), we force it to take the
required antecedent. In general, antecedent affini-
ties learn to prefer close antecedents in prominent
syntactic positions. We also learn that new entity
nominals are typically indefinite or have SBAR com-
plements (captured by the CFG feature).
In contrast to nominals and pronouns, the choice
of entity for a proper mention is governed more by
entity frequency than antecedent distance. We cap-
ture this by setting spi(i, j;X) in the proper case to
1 for past positions and to a fixed ? otherwise. 2
3.3 Mention Module
Once the semantic module has generated entities and
the discourse model selects entity assignments, each
mention Mi generates word values for a set of ob-
served properties Ri:
Mention Generation
For each mention Mi, i = 1, . . . , n
Fetch (T, {Lr}r?R) from EZi
Fetch {(fr, ?r)}r?R from ? T
For r ? Ri :
w ? (1? ?r)UNIFORM(Lr) + (?r)?r
For each property r, there is a hyper-parameter ?r
which interpolates between selecting a word from
the entity list Lr and drawing from the underlying
type property distribution ?r. Intuitively, a small
value of ?r indicates that an entity prefers to re-use
2As Blei and Frazier (2009) notes, when marginalizing out
theAi in this trivial case, the DD-CRP reduces to the traditional
CRP (Pitman, 2002), so our discourse model roughly matches
Haghighi and Klein (2007) for proper mentions.
? 1
Person
Organization
[software]
NN-
NOD
ORG
[Microsoft]
[company,
 firm]
NOM-
HEAD
NAM-
HEAD
T
? 2
E1 E2
Z1 Z2 Z3
M1 M2 M3
[Steve,chief, 
Microsoft]
NN-
NOD
PERS
[Ballmer,
 CEO]
[officer,
 executive]
NOM-
HEAD
NAM-
HEAD
T
joined
GOV-
SUBJ
Ballmer
Steve
NN-
HEAD
NAM-
HEAD
joined
GOV-
DOBJ
Microsoft
NAM-
HEAD
became
GOV-
DOBJ
CEO
NAM-
HEAD
? ?
E, ? E, ? E, ?
E,M E,M
E2E1 E1
M M
Figure 3: Depiction of the discourse module (Sec-
tion 3.2); each random variable is annotated with an ex-
ample value. For each mention position, an entity as-
signment (Zi) is made. Conditioned on entities (EZi ),
mentions (Mi) are rendered (Section 3.3). The Y sym-
bol denotes that a random variable is the parent of all Y
random variables.
a small number of words for property r. This is typ-
ically the case for proper and nominal heads as well
as modifiers. At the other extreme, setting ?r to 1
indicates the property isn?t particular to the entity
itself, but rather only on its type. We set ?r to 1
for pronoun heads as well as for the governor of the
head properties.
4 Learning and Inference
Our learning procedure involves finding parame-
ters and assignments which are likely under our
model?s posterior distribution P (E,Z, ? ,pi|M,X).
The model is modularized in such a way that run-
ning EM on all variables simultaneously would be
very difficult. Therefore, we adopt a variational ap-
proach which optimizes various subgroups of the
variables in a round-robin fashion, holding approx-
imations to the others fixed. We first describe the
variable groups, then the updates which optimize
them in turn.
Decomposition: We decompose the entity vari-
388
ables E into types, T, one for each entity, and word
lists, L, one for each entity and property. We decom-
pose the mentions M into referring mentions (prop-
ers and nominals), Mr, and pronominal mentions,
Mp (with sizes nr and np respectively). The en-
tity assignments Z are similarly divided into Zr and
Zp components. For pronouns, rather than use Zp,
we instead work with the corresponding antecedent
variables, denoted Ap, and marginalize over an-
tecedents to obtain Zp.
With these variable groups, we would
like to approximation our model posterior
P (T,L,Zr,Ap, ? ,pi|M,X) using a simple fac-
tored representation. Our variational approximation
takes the following form:
Q(T,L,Zr,Ap, ? ,pi) = ?r(Zr,L)
(
n?
k=1
qk(Tk)
)( np?
i=1
ri(Api )
)
?s(? )?d(pi)
We use a mean field approach to update each of the
RHS factors in turn to minimize the KL-divergence
between the current variational posterior and the
true model posterior. The ?r, ?s, and ?d factors
place point estimates on a single value, just as in
hard EM. Updating these factors involves finding the
value which maximizes the model (expected) log-
likelihood under the other factors. For instance, the
?s factor is a point estimate of the type parameters,
and is updated with:3
?s(? )? argmax? EQ??s lnP (E,Z,M, ? ,pi) (1)
where Q??s denotes all factors of the variational
approximation except for the factor being updated.
The ri (pronoun antecedents) and qk (type indica-
tor) factors maintain a soft approximation and so are
slightly more complex. For example, the ri factor
update takes the standard mean field form:
ri(Api ) ? exp{EQ?ri lnP (E,Z,M, ? ,pi)} (2)
We briefly describe the update for each additional
factor, omitting details for space.
Updating type parameters ?s(? ): The type pa-
rameters ? consist of several multinomial distri-
butions which can be updated by normalizing ex-
pected counts as in the EM algorithm. The prior
3Of course during learning, the argmax is performed over
the entire document collection, rather than a single document.
P (? |?) consists of several finite Dirichlet draws for
each multinomial, which are incorporated as pseu-
docounts.4 Given the entity type variational poste-
riors {qk(?)}, as well as the point estimates of the
L and Zr elements, we obtain expected counts from
each entity?s attribute word lists and referring men-
tion usages.
Updating discourse parameters ?d(pi): The
learned parameters for the discourse module rely on
pairwise antecedent counts for assignments to nom-
inal and pronominal mentions.5 Given these ex-
pected counts, which can be easily obtained from
other factors, the update reduces to a weighted max-
imum entropy problem, which we optimize using
LBFGS. The prior P (pi|?2) is a zero-centered nor-
mal distribution with shared diagonal variance ?2,
which is incorporated via L2 regularization during
optimization.
Updating referring assignments and word lists
?r(Zr,L): The word lists are usually concatena-
tions of the words used in nominal and proper
mentions and so are updated together with the
assignments for those mentions. Updating the
?r(Zr,L) factor involves finding the referring men-
tion entity assignments, Zr, and property word
lists L for instantiated entities which maximize
EQ??r lnP (T,L,Zr,Ap,M, ? ,pi). We actually
only need to optimize over Zr, since for any Zr, we
can compute the optimal set of property word lists
L. Essentially, for each entity we can compute the
Lr which optimizes the probability of the referring
mentions assigned to the entity (indicated by Zr). In
practice, the optimal Lr is just the set of property
words in the assigned mentions. Of course enumer-
ating and scoring all Zr hypotheses is intractable,
so we instead utilize a left-to-right sequential beam
search. Each partial hypothesis is an assignment to a
prefix of mention positions and is scored as though
it were a complete hypothesis. Hypotheses are ex-
tended via adding a new mention to an existing en-
tity or creating a new one. For our experiments, we
limited the number of hypotheses on the beam to the
top fifty and did not notice an improvement in model
score from increasing beam size.
4See software release for full hyper-parameter details.
5Propers have no learned discourse parameters.
389
Updating pronominal antecedents ri(Api ) and en-
tity types qk(Tk): These updates are straightfor-
ward instantiations of the mean-field update (2).
To produce our final coreference partitions, we as-
sign each referring mention to the entity given by the
?r factor and each pronoun to the most likely entity
given by the ri.
4.1 Factor Staging
In order to facilitate learning, some factors are ini-
tially set to fixed heuristic values and only learned
in later iterations. Initially, the assignment factors
?r and {ri} are fixed. For ?r, we use a determin-
istic entity assignment Zr, similar to the Haghighi
and Klein (2009)?s SYN-CONSTR setting: each re-
ferring mention is coreferent with any past men-
tion with the same head or in a deterministic syn-
tactic configuration (appositives or predicative nom-
inatives constructions).6 The {ri} factors are heuris-
tically set to place most of their mass on the closest
antecedent by tree distance. During training, we pro-
ceed in stages, each consisting of 5 iterations:
Stage Learned Fixed B3All
1 ?s, ?d, {qk} {ri},?r 74.6
2 ?s, ?d, {qk}, ?r {ri} 76.3
3 ?s, ?d, {qk}, ?r, {ri} ? 78.0
We evaluate our system at the end of stage using the
B3All metric on the A05CU development set (see
Section 5 for details).
5 Experiments
We considered the challenging end-to-end system
mention setting, where in addition to predicting
mention partitions, a system must identify the men-
tions themselves and their boundaries automati-
cally. Our system deterministically extracts mention
boundaries from parse trees (Section 5.2). We uti-
lized no coreference annotation during training, but
did use minimal prototype information to prime the
learning of entity types (Section 5.3).
5.1 Datasets
For evaluation, we used standard coreference data
sets derived from the ACE corpora:
6Forcing appositive coreference is essential for tying proper
and nominal entity type vocabulary.
? A04CU: Train/dev/test split of the newswire
portion of the ACE 2004 training set7 utilized
in Culotta et al (2007), Bengston and Roth
(2008) and Stoyanov et al (2009). Consists of
90/68/38 documents respectively.
? A05ST: Train/test split of the newswire portion
of the ACE 2005 training set utilized in Stoy-
anov et al (2009). Consists of 57/24 docu-
ments respectively.
? A05RA: Train/test split of the ACE 2005 train-
ing set utilized in Rahman and Ng (2009). Con-
sists of 482/117 documents respectively.
For all experiments, we evaluated on the dev and test
sets above. To train, we included the text of all doc-
uments above, though of course not looking at ei-
ther their mention boundaries or reference annota-
tions in any way. We also trained on the following
much larger unlabeled datasets utilized in Haghighi
and Klein (2009):
? BLLIP: 5k articles of newswire parsed with the
Charniak (2000) parser.
? WIKI: 8k abstracts of English Wikipedia arti-
cles parsed by the Berkeley parser (Petrov et
al., 2006). Articles were selected to have sub-
jects amongst the frequent proper nouns in the
evaluation datasets.
5.2 Mention Detection and Properties
Mention boundaries were automatically detected as
follows: For each noun or pronoun (determined by
parser POS tag), we associated a mention with the
maximal NP projection of that head or that word it-
self if no NP can be found. This procedure recovers
over 90% of annotated mentions on the A05CU dev
set, but also extracts many unannotated ?spurious?
mentions (for instance events, times, dates, or ab-
stract nouns) which are not deemed to be of interest
by the ACE annotation conventions.
Mention properties were obtained from parse
trees using the the Stanford typed dependency ex-
tractor (de Marneffe et al, 2006). The mention prop-
erties we considered are the mention head (anno-
tated with mention type), the typed modifiers of the
head, and the governor of the head (conjoined with
7Due to licensing restriction, the formal ACE test sets are
not available to non-participants.
390
MUC B3All B3None Pairwise F1
System P R F1 P R F1 P R F1 P R F1
ACE2004-STOYANOV-TEST
Stoyanov et al (2009) - - 62.0 - - 76.5 - - 75.4 - - -
Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 77.4 67.1 71.3 58.3 44.5 50.5
THIS WORK 67.4 66.6 67.0 81.2 73.3 77.0 80.6 75.2 77.3 59.2 50.3 54.4
ACE2005-STOYANOV-TEST
Stoyanov et al (2009) - - 67.4 - - 73.7 - - 72.5 - - -
Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 81.2 61.6 70.1 66.1 37.9 48.1
THIS WORK 74.6 62.7 68.1 83.2 68.4 75.1 82.7 66.3 73.6 64.3 41.4 50.4
ACE2005-RAHMAN-TEST
Rahman and Ng (2009) 75.4 64.1 69.3 - - - 54.4 70.5 61.4 - - -
Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6 52.0 72.6 60.6 57.0 44.6 50.0
THIS WORK 77.0 66.9 71.6 55.4 74.8 63.8 54.0 74.7 62.7 60.1 47.7 53.0
Table 1: Experimental results with system mentions. All systems except Haghighi and Klein (2009) and current work
are fully supervised. The current work outperforms all other systems, supervised or unsupervised. For comparison pur-
poses, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman
and Ng (2009).
the mention?s syntactic position). We discard deter-
miners, but make use of them in the discourse com-
ponent (Section 3.2) for NP definiteness.
5.3 Prototyping Entity Types
While it is possible to learn type distributions in a
completely unsupervised fashion, we found it use-
ful to prime the system with a handful of important
types. Rather than relying on fully supervised data,
we took the approach of Haghighi and Klein (2006).
For each type of interest, we provided a (possibly-
empty) prototype list of proper and nominal head
words, as well as a list of allowed pronouns. For
instance, for the PERSON type we might provide:
NAM Bush, Gore, Hussein
NOM president, minister, official
PRO he, his, she, him, her, you, ...
The prototypes were used as follows: Any entity
with a prototype on any proper or nominal head
word attribute list (Section 3.1) was constrained to
have the specified type; i.e. the qk factor (Section 4)
places probability one on that single type. Simi-
larly to Haghighi and Klein (2007) and Elsner et al
(2009), we biased these types? pronoun distributions
to the allowed set of pronouns.
In general, the choice of entity types to prime
with prototypes is a domain-specific question. For
experiments here, we utilized the types which are
annotated in the ACE coreference data: person
(PERS), organization (ORG), geo-political entity
(GPE), weapon (WEA), vehicle (VEH), location
(LOC), and facility (FAC). Since the person type
in ACE conflates individual persons with groups
of people (e.g., soldier vs. soldiers), we added
the group (GROUP) type and generated a prototype
specification.
We obtained our prototype list by extracting at
most four common proper and nominal head words
from the newswire portions of the 2004 and 2005
ACE training sets (A04CU and A05ST); we chose
prototype words to be minimally ambiguous with
respect to type.8 When there are not at least three
proper heads for a type (WEA for instance), we
did not provide any proper prototypes and instead
strongly biased the type fertility parameters to gen-
erate empty NAM-HEAD lists.
Because only certain semantic types were anno-
tated under the arbitrary ACE guidelines, there are
many mentions which do not fall into those limited
categories. We therefore prototype (refinements of)
the ACE types and then add an equal number of un-
constrained ?other? types which are automatically
induced. A nice consequence of this approach is
that we can simply run our model on all mentions,
discarding at evaluation time any which are of non-
prototyped types.
5.4 Evaluation
We evaluated on multiple coreference resolution
metrics, as no single one is clearly superior, partic-
8Meaning those headwords were assigned to the target type
for more than 75% of their usages.
391
ularly in dealing with the system mention setting.
We utilized MUC (Vilain et al, 1995), B3All (Stoy-
anov et al, 2009), B3None (Stoyanov et al, 2009),
and Pairwise F1. The B3All and B3None are B3
variants (Bagga and Baldwin, 1998) that differ in
their treatment of spurious mentions. For Pairwise
F1, precision measures how often pairs of predicted
coreferent mentions are in the same annotated entity.
We eliminated any mention pair from this calcula-
tion where both mentions were spurious.9
5.5 Results
Table 1 shows our results. We compared to two
state-of-the-art supervised coreference systems. The
Stoyanov et al (2009) numbers represent their
THRESHOLD ESTIMATION setting and the Rahman
and Ng (2009) numbers represent their highest-
performing cluster ranking model. We also com-
pared to the strong deterministic system of Haghighi
and Klein (2009).10 Across all data sets, our model,
despite being largely unsupervised, consistently out-
performs these systems, which are the best previ-
ously reported results on end-to-end coreference res-
olution (i.e. including mention detection). Perfor-
mance on the A05RA dataset is generally lower be-
cause it includes articles from blogs and web forums
where parser quality is significantly degraded.
While Bengston and Roth (2008) do not report on
the full system mention task, they do report on the
more optimistic setting where mention detection is
performed but non-gold mentions are removed for
evaluation using an oracle. On this more lenient set-
ting, they report 78.4B3 on the A04CU test set. Our
model yields 80.3.
6 Analysis
We now discuss errors and improvements made
by our system. One frequent source of error is
the merging of mentions with explicitly contrasting
modifiers, such as new president and old president.
While it is not unusual for a single entity to admit
multiple modifiers, the particular modifiers new and
old are incompatible in a way that new and popular
9Note that we are still penalized for marking a spurious
mention coreferent with an annotated one.
10Haghighi and Klein (2009) reports on true mentions; here,
we report performance on automatically detected mentions.
are not. Our model does not represent the negative
covariance between these modifiers.
We compared our output to the deterministic sys-
tem of Haghighi and Klein (2009). Many improve-
ments arise from correctly identifying mentions
which are semantically compatible but which do
not explicitly appear in an appositive or predicate-
nominative configuration in the data. For example,
analyst and it cannot corefer in our system because
it is not a likely pronoun for the type PERSON.
While the focus of our model is coreference res-
olution, we can also isolate and evaluate the type
component of our model as an NER system. We
test this component by presenting our learned model
with boundary-annotated non-pronominal entities
from the A05ST dev set and querying their predicted
type variable T . Doing so yields 83.2 entity clas-
sification accuracy under the mapping between our
prototyped types and the coarse ACE types. Note
that this task is substantially more difficult than the
unsupervised NER in Elsner et al (2009) because
the inventory of named entities is larger (7 vs. 3)
and because we predict types over nominal mentions
that are more difficult to judge from surface forms.
In this task, the plurality of errors are confusions be-
tween the GPE (geo-political entity) and ORG entity
types, which have very similar distributions.
7 Conclusion
Our model is able to acquire and exploit knowledge
at either the level of individual entities (?Obama? is
a ?president?) and entity types (?company? can refer
to a corporation). As a result, it leverages semantic
constraints more effectively than systems operating
at either level alone. In conjunction with reasonable,
but simple, factors capturing discourse and syntac-
tic configurational preferences, our entity-centric se-
mantic model lowers coreference error rate substan-
tially, particularly on semantically disambiguated
references, giving a sizable improvement over the
state-of-the-art.11
Acknowledgements: This project is funded in
part by the Office of Naval Research under MURI
Grant No. N000140911081.
11See nlp.cs.berkeley.edu and aria42.com/software.html for
software release.
392
References
A Bagga and B Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Work-
shop (LREC).
Eric Bengston and Dan Roth. 2008. Understanding
the Value of Features for Corefernce Resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
David Blei and Peter I. Frazier. 2009. Dis-
tance Dependent Chinese Restaurant Processes.
http://arxiv.org/abs/0910.1022/.
Eugene Charniak. 2000. Maximum Entropy Inspired
Parser. In North American Chapter of the Association
of Computational Linguistics (NAACL).
Michael Collins and Yoram Singer. 1999. Unsupervised
Models for Named Entity Classification. In Empirical
Methods in Natural Language Processing (EMNLP).
Mike Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
A Culotta, M Wick, R Hall, and A McCallum. 2007.
First-order Probabilistic Models for Coreference Res-
olution. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (NAACL-HLT).
M. C. de Marneffe, B. Maccartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In LREC.
M Elsner, E Charniak, and M Johnson. 2009. Structured
generative models for unsupervised named-entity clus-
tering. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 164?172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203?225.
Aria Haghighi and Dan Klein. 2006. Prototype-Driven
Learning for Sequence Models. In HLT-NAACL. As-
sociation for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
Coreference Resolution in a Nonparametric Bayesian
Model. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics. Associ-
ation for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple Coreference
Resolution with Rich Syntactic and Semantic Features.
In Proceedings of the 2009 Conference on Empirical
Conference in Natural Language Processing.
J. R. Hobbs. 1978. Resolving Pronoun References. Lin-
gua, 44.
J. R. Hobbs. 1979. Coherence and Coreference. Cogni-
tive Science, 3:67?90.
Andrew Kehler, Laura Kertz, Hannah Rohde, and Jeffrey
Elman. 2008. Coherence and Coreference Revisited.
Vincent Ng and Claire Cardie. 2002. Improving
Machine Learning Approaches to Coreference Res-
olution. In Association of Computational Linguists
(ACL).
Vincent Ng. 2005. Machine Learning for Corefer-
ence Resolution: From Local Classification to Global
Ranking. In Association of Computational Linguists
(ACL).
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 1689?1694.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
J. Pitman. 2002. Combinatorial Stochastic Processes. In
Lecture Notes for St. Flour Summer School.
A Rahman and V Ng. 2009. Supervised models for
coreference resolution. In Proceedings of the 2009
Conference on Empirical Conference in Natural Lan-
guage Processing.
W.H. Soon, H. T. Ng, and D. C. Y. Lim. 1999. A Ma-
chine Learning Approach to Coreference Resolution
of Noun Phrases.
V Stoyanov, N Gilbert, C Cardie, and E Riloff. 2009.
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-art. In Associate of
Computational Linguistics (ACL).
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC-6.
X Yang, J Su, and CL Tan. 2005. Improving pronoun
resolution using statistics-based semantic compatibil-
ity information. In Association of Computational Lin-
guists (ACL).
393
Proceedings of the ACL 2010 Conference Short Papers, pages 291?295,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
An Entity-Level Approach to Information Extraction
Aria Haghighi
UC Berkeley, CS Division
aria42@cs.berkeley.edu
Dan Klein
UC Berkeley, CS Division
klein@cs.berkeley.edu
Abstract
We present a generative model of
template-filling in which coreference
resolution and role assignment are jointly
determined. Underlying template roles
first generate abstract entities, which in
turn generate concrete textual mentions.
On the standard corporate acquisitions
dataset, joint resolution in our entity-level
model reduces error over a mention-level
discriminative approach by up to 20%.
1 Introduction
Template-filling information extraction (IE) sys-
tems must merge information across multiple sen-
tences to identify all role fillers of interest. For
instance, in the MUC4 terrorism event extrac-
tion task, the entity filling the individual perpetra-
tor role often occurs multiple times, variously as
proper, nominal, or pronominal mentions. How-
ever, most template-filling systems (Freitag and
McCallum, 2000; Patwardhan and Riloff, 2007)
assign roles to individual textual mentions using
only local context as evidence, leaving aggrega-
tion for post-processing. While prior work has
acknowledged that coreference resolution and dis-
course analysis are integral to accurate role identi-
fication, to our knowledge no model has been pro-
posed which jointly models these phenomena.
In this work, we describe an entity-centered ap-
proach to template-filling IE problems. Our model
jointly merges surface mentions into underlying
entities (coreference resolution) and assigns roles
to those discovered entities. In the generative pro-
cess proposed here, document entities are gener-
ated for each template role, along with a set of
non-template entities. These entities then generate
mentions in a process sensitive to both lexical and
structural properties of the mention. Our model
outperforms a discriminative mention-level base-
line. Moreover, since our model is generative, it
[S CSR] has said that [S it] has sold [S its]  [B oil 
interests] held in  [A Delhi Fund].  [P Esso Inc.] did not 
disclose how much [P they] paid for [A Dehli].
(a)
(b)
Document
Esso Inc.
PURCHASERACQUIRED
Delhi FundOil and Gas
BUSINESS
CSR Limited
SELLER
Template
Figure 1: Example of the corporate acquisitions role-filling
task. In (a), an example template specifying the entities play-
ing each domain role. In (b), an example document with
coreferent mentions sharing the same role label. Note that
pronoun mentions provide direct clues to entity roles.
can naturally incorporate unannotated data, which
further increases accuracy.
2 Problem Setting
Figure 1(a) shows an example template-filling
task from the corporate acquisitions domain (Fre-
itag, 1998).1 We have a template of K roles
(PURCHASER, AMOUNT, etc.) and we must iden-
tify which entity (if any) fills each role (CSR Lim-
ited, etc.). Often such problems are modeled at the
mention level, directly labeling individual men-
tions as in Figure 1(b). Indeed, in this data set,
the mention-level perspective is evident in the gold
annotations, which ignore pronominal references.
However, roles in this domain appear in several lo-
cations throughout the document, with pronominal
mentions often carrying the critical information
for template filling. Therefore, Section 3 presents
a model in which entities are explicitly modeled,
naturally merging information across all mention
types and explicitly representing latent structure
very much like the entity-level template structure
from Figure 1(a).
1In Freitag (1998), some of these fields are split in two to
distinguish a full versus abbreviated name, but we ignore this
distinction. Also we ignore the status field as it doesn?t apply
to entities and its meaning is not consistent.
291
R1 R2 RK
Z1 Z2 Zn
M1 M2 Mn...........
Document
Role Entity Parameters
Mentions
?
Role 
Priors
E1 E2
M3
Z3 ...........
EK
Other 
Entities
.... ....
Other Entity Parameters
.... ....
Entity Indicators
1
[1: 0.02, 
  0:0.015,
     2: 0.01,...]
MOD-APPOS
[company: 0.02, 
 firm:0.015,
  group: 0.01,...]
[1: 0.19, 
2:0.14,
     0: 0.08,...]
HEAD-NAM
[Inc.: 0.02, 
 Corp.:0.015,
  Ltd.: 0.01,...]
[2: 0.18, 
3:0.12,
     1: 0.09,...]
GOV-NSUBJ
fr?rr
[bought: 0.02, 
 obtained:0.015,
acquired: 0.01,...]
Purchaser Role
Role
Entites
CaliforniaMOD-PREP
MOD-NN search, giant
companyHEAD-NOM
HEAD-NAM
Lrr
Google, GOOG
Purchaser Entity
GOV-NSUBJ bought
HEAD-NAM Google
wr
Purchaser Mention
Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key
parameters and latent variables.
3 Model
We describe our generative model for a document,
which has many similarities to the coreference-
only model of Haghighi and Klein (2010), but
which integrally models template role-fillers. We
briefly describe the key abstractions of our model.
Mentions: A mention is an observed textual
reference to a latent real-world entity. Mentions
are associated with nodes in a parse tree and are
typically realized as NPs. There are three ba-
sic forms of mentions: proper (NAM), nominal
(NOM), and pronominal (PRO). Each mention M
is represented as collection of key-value pairs.
The keys are called properties and the values are
words. The set of properties utilized here, de-
noted R, are the same as in Haghighi and Klein
(2010) and consist of the mention head, its depen-
dencies, and its governor. See Figure 2 for a con-
crete example. Mention types are trivially deter-
mined from mention head POS tag. All mention
properties and their values are observed.
Entities: An entity is a specific individual or
object in the world. Entities are always latent in
text. Where a mention has a single word for each
property, an entity has a list of signature words.
Formally, entities are mappings from properties
r ? R to lists Lr of ?canonical? words which that
entity uses for that property.
Roles: The elements we have described so far
are standard in many coreference systems. Our
model performs role-filling by assuming that each
entity is drawn from an underlying role. These
roles include theK template roles as well as ?junk?
roles to represent entities which do not fill a tem-
plate role (see Section 5.2). Each role R is rep-
resented as a mapping between properties r and
pairs of multinomials (?r, fr). ?r is a unigram dis-
tribution of words for property r that are seman-
tically licensed for the role (e.g., being the sub-
ject of ?acquired? for the ACQUIRED role). fr is a
?fertility? distribution over the integers that char-
acterizes entity list lengths. Together, these distri-
butions control the lists Lr for entities which in-
stantiate the role.
We first present a broad sketch of our model?s
components and then detail each in a subsequent
section. We temporarily assume that all men-
tions belong to a template role-filling entity; we
lift this restriction in Section 5.2. First, a se-
mantic component generates a sequence of enti-
ties E = (E1, . . . , EK), where each Ei is gen-
erated from a corresponding role Ri. We use
R = (R1, . . . , RK) to denote the vector of tem-
plate role parameters. Note that this work assumes
that there is a one-to-one mapping between entities
and roles; in particular, at most one entity can fill
each role. This assumption is appropriate for the
domain considered here.
Once entities have been generated, a dis-
course component generates which entities will be
evoked in each of the n mention positions. We
represent these choices using entity indicators de-
noted by Z = (Z1, . . . , Zn). This component uti-
lizes a learned global prior ? over roles. The Zi in-
292
dicators take values in 1, . . . ,K indicating the en-
tity number (and thereby the role) underlying the
ith mention position. Finally, a mention genera-
tion component renders each mention conditioned
on the underlying entity and role. Formally:
P (E,Z,M|R, ?) =
(
K?
i=1
P (Ei|Ri)
)
[Semantic, Sec. 3.1]
?
?
n?
j=1
P (Zj |Z<j , ?)
?
? [Discourse, Sec. 3.2]
?
?
n?
j=1
P (Mj |EZj , RZj )
?
? [Mention, Sec. 3.3]
3.1 Semantic Component
Each role R generates an entity E as follows: for
each mention property r, a word list, Lr, is drawn
by first generating a list length from the corre-
sponding fr distribution in R.2 This list is then
populated by an independent draw from R?s uni-
gram distribution ?r. Formally, for each r ? R, an
entity word list is drawn according to,3
P (Lr|R) = P (len(Lr)|fr)
?
w?Lr
P (w|?r)
3.2 Discourse Component
The discourse component draws the entity indica-
tor Zj for the jth mention according to,
P (Zj |Z<j , ?) =
{
P (Zj |?), if non-pronominal
?
j? 1[Zj = Zj? ]P (j?|j), o.w.
When the jth mention is non-pronominal, we draw
Zj from ?, a global prior over the K roles. When
Mj is a pronoun, we first draw an antecedent men-
tion position j?, such that j? < j, and then we set
Zj = Zj? . The antecedent position is selected ac-
cording to the distribution,
P (j?|j) ? exp{??TREEDIST(j?, j)}
where TREEDIST(j?,j) represents the tree distance
between the parse nodes forMj andMj? .4 Mass is
2There is one exception: the sizes of the proper and nom-
inal head property lists are jointly generated, but their word
lists are still independently populated.
3While, in principle, this process can yield word lists with
duplicate words, we constrain the model during inference to
not allow that to occur.
4Sentence parse trees are merged into a right-branching
document parse tree. This allows us to extend tree distance to
inter-sentence nodes.
restricted to antecedent mention positions j? which
occur earlier in the same sentence or in the previ-
ous sentence.5
3.3 Mention Generation
Once the entity indicator has been drawn, we gen-
erate words associated with mention conditioned
on the underlying entity E and role R. For each
mention property r associated with the mention,
a word w is drawn utilizing E?s word list Lr as
well as the multinomials (fr, ?r) from roleR. The
word w is drawn according to,
P (w|E,R)=(1? ?r)
1 [w ? Lr]
len(Lr)
+ ?rP (w|?r)
For each property r, there is a hyper-parameter ?r
which interpolates between selecting a word uni-
formly from the entity list Lr and drawing from
the underlying role distribution ?r. Intuitively, a
small ?r indicates that an entity prefers to re-use a
small number of words for property r. This is typi-
cally the case for proper and nominal heads as well
as modifiers. At the other extreme, setting ?r to 1
indicates the property isn?t particular to the entity
itself, but rather always drawn from the underly-
ing role distribution. We set ?r to 1 for pronoun
heads as well as for the governor properties.
4 Learning and Inference
Since we will make use of unannotated data (see
Section 5), we utilize a variational EM algorithm
to learn parameters R and ?. The E-Step re-
quires the posterior P (E,Z|R,M, ?), which is
intractable to compute exactly. We approximate
it using a surrogate variational distribution of the
following factored form:
Q(E,Z) =
(
K?
i=1
qi(Ei)
)?
?
n?
j=1
rj(Zj)
?
?
Each rj(Zj) is a distribution over the entity in-
dicator for mention Mj , which approximates the
true posterior of Zj . Similarly, qi(Ei) approxi-
mates the posterior over entity Ei which is asso-
ciated with role Ri. As is standard, we iteratively
update each component distribution to minimize
KL-divergence, fixing all other distributions:
qi ? argmin
qi
KL(Q(E,Z)|P (E,Z|M,R, ?)
? exp{EQ/qi lnP (E,Z|M,R, ?))}
5The sole parameter ? is fixed at 0.1.
293
Ment Acc. Ent. Acc.
INDEP 60.0 43.7
JOINT 64.6 54.2
JOINT+PRO 68.2 57.8
Table 1: Results on corporate acquisition tasks with given
role mention boundaries. We report mention role accuracy
and entity role accuracy (correctly labeling all entity men-
tions).
For example, the update for a non-pronominal
entity indicator component rj(?) is given by:6
ln rj(z) ? EQ/rj lnP (E,Z,M|R, ?)
? Eqz ln (P (z|?)P (Mj |Ez, Rz))
= lnP (z|?) + Eqz lnP (Mj |Ez, Rz)
A similar update is performed on pronominal en-
tity indicator distributions, which we omit here for
space. The update for variational entity distribu-
tion is given by:
ln qi(ei) ? EQ/qi lnP (E,Z,M|R, ?)
? E{rj} ln
?
?P (ei|Ri)
?
j:Zj=i
P (Mj |ei, Ri)
?
?
= lnP (ei|Ri) +
?
j
rj(i) lnP (Mj |ei, Ri)
It is intractable to enumerate all possible entities
ei (each consisting of several sets of words). We
instead limit the support of qi(ei) to several sam-
pled entities. We obtain entity samples by sam-
pling mention entity indicators according to rj .
For a given sample, we assume that Ei consists
of the non-pronominal head words and modifiers
of mentions such that Zj has sampled value i.
During the E-Step, we perform 5 iterations of
updating each variational factor, which results in
an approximate posterior distribution. Using ex-
pectations from this approximate posterior, our M-
Step is relatively straightforward. The role param-
eters Ri are computed from the qi(ei) and rj(z)
distributions, and the global role prior ? from the
non-pronominal components of rj(z).
5 Experiments
We present results on the corporate acquisitions
task, which consists of 600 annotated documents
split into a 300/300 train/test split. We use 50
training documents as a development set. In all
6For simplicity of exposition, we omit terms where Mj is
an antecedent to a pronoun.
documents, proper and (usually) nominal men-
tions are annotated with roles, while pronouns are
not. We preprocess each document identically to
Haghighi and Klein (2010): we sentence-segment
using the OpenNLP toolkit, parse sentences with
the Berkeley Parser (Petrov et al, 2006), and ex-
tract mention properties from parse trees and the
Stanford Dependency Extractor (de Marneffe et
al., 2006).
5.1 Gold Role Boundaries
We first consider the simplified task where role
mention boundaries are given. We map each la-
beled token span in training and test data to a parse
tree node that shares the same head. In this set-
ting, the role-filling task is a collective classifica-
tion problem, since we know each mention is fill-
ing some role.
As our baseline, INDEP, we built a maxi-
mum entropy model which independently classi-
fies each mention?s role. It uses features as similar
as possible to the generative model (and more), in-
cluding the head word, typed dependencies of the
head, various tree features, governing word, and
several conjunctions of these features as well as
coarser versions of lexicalized features. This sys-
tem yields 60.0 mention labeling accuracy (see Ta-
ble 1). The primary difficulty in classification is
the disambiguation amongst the acquired, seller,
and purchaser roles, which have similar internal
structure, and differ primarily in their semantic
contexts. Our entity-centered model, JOINT in Ta-
ble 1, has no latent variables at training time in this
setting, since each role maps to a unique entity.
This model yields 64.6, outperforming INDEP.7
During development, we noted that often the
most direct evidence of the role of an entity was
associated with pronoun usage (see the first ?it?
in Figure 1). Training our model with pronominal
mentions, whose roles are latent variables at train-
ing time, improves accuracy to 68.2.8
5.2 Full Task
We now consider the more difficult setting where
role mention boundaries are not provided at test
time. In this setting, we automatically extract
mentions from a parse tree using a heuristic ap-
7We use the mode of the variational posteriors rj(Zj) to
make predictions (see Section 4).
8While this approach incorrectly assumes that all pro-
nouns have antecedents amongst our given mentions, this did
not appear to degrade performance.
294
ROLE ID OVERALL
P R F1 P R F1
INDEP 79.0 65.5 71.6 48.6 40.3 44.0
JOINT+PRO 80.3 69.2 74.3 53.4 46.4 49.7
BEST 80.1 70.1 74.8 57.3 49.2 52.9
Table 2: Results on corporate acquisitions data where men-
tion boundaries are not provided. Systems must determine
which mentions are template role-fillers as well as label them.
ROLE ID only evaluates the binary decision of whether a
mention is a template role-filler or not. OVERALL includes
correctly labeling mentions. Our BEST system, see Sec-
tion 5, adds extra unannotated data to our JOINT+PRO sys-
tem.
proach. Our mention extraction procedure yields
95% recall over annotated role mentions and 45%
precision.9 Using extracted mentions as input, our
task is to label some subset of the mentions with
template roles. Since systems can label mentions
as non-role bearing, only recall is critical to men-
tion extraction. To adapt INDEP to this setting, we
first use a binary classifier trained to distinguish
role-bearing mentions. The baseline then classi-
fies mentions which pass this first phase as before.
We add ?junk? roles to our model to flexibly model
entities that do not correspond to annotated tem-
plate roles. During training, extracted mentions
which are not matched in the labeled data have
posteriors which are constrained to be amongst the
?junk? roles.
We first evaluate role identification (ROLE ID in
Table 2), the task of identifying mentions which
play some role in the template. The binary clas-
sifier for INDEP yields 71.6 F1. Our JOINT+PRO
system yields 74.3. On the task of identifying and
correctly labeling role mentions, our model out-
performs INDEP as well (OVERALL in Table 2). As
our model is generative, it is straightforward to uti-
lize totally unannotated data. We added 700 fully
unannotated documents from the mergers and ac-
quisitions portion of the Reuters 21857 corpus.
Training JOINT+PRO on this data as well as our
original training data yields the best performance
(BEST in Table 2).10
To our knowledge, the best previously pub-
lished results on this dataset are from Siefkes
(2008), who report 45.9 weighted F1. Our BEST
system evaluated in their slightly stricter way
yields 51.1.
9Following Patwardhan and Riloff (2009), we match ex-
tracted mentions to labeled spans if the head of the mention
matches the labeled span.
10We scaled expected counts from the unlabeled data so
that they did not overwhelm those from our (partially) labeled
data.
6 Conclusion
We have presented a joint generative model of
coreference resolution and role-filling information
extraction. This model makes role decisions at
the entity, rather than at the mention level. This
approach naturally aggregates information across
multiple mentions, incorporates unannotated data,
and yields strong performance.
Acknowledgements: This project is funded in
part by the Office of Naval Research under MURI
Grant No. N000140911081.
References
M. C. de Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC.
Dayne Freitag and Andrew McCallum. 2000. Infor-
mation extraction with hmm structures learned by
stochastic optimization. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
Dayne Freitag. 1998. Machine learning for informa-
tion extraction in informal domains.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In North
American Association of Computational Linguistics
(NAACL).
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tuto-
rial). In Association for Computational Linguistics
(ACL).
S. Patwardhan and E. Riloff. 2007. Effective infor-
mation extraction with semantic affinity patterns and
relevant regions. In Joint Conference on Empirical
Methods in Natural Language Processing.
S. Patwardhan and E Riloff. 2009. A unified model of
phrasal and sentential evidence for information ex-
traction. In Empirical Methods in Natural Language
Processing (EMNLP).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Christian Siefkes. 2008. An Incrementally Train-
able Statistical Approach to Information Extraction:
Based on Token Classification and Rich Context
Model. VDM Verlag, Saarbru?cken, Germany, Ger-
many.
295
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350?358,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Content Models with Attitude
Christina Sauper, Aria Haghighi, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
csauper@csail.mit.edu, me@aria42.com, regina@csail.mit.edu
Abstract
We present a probabilistic topic model for
jointly identifying properties and attributes of
social media review snippets. Our model
simultaneously learns a set of properties of
a product and captures aggregate user senti-
ments towards these properties. This approach
directly enables discovery of highly rated or
inconsistent properties of a product. Our
model admits an efficient variational mean-
field inference algorithm which can be paral-
lelized and run on large snippet collections.
We evaluate our model on a large corpus of
snippets from Yelp reviews to assess property
and attribute prediction. We demonstrate that
it outperforms applicable baselines by a con-
siderable margin.
1 Introduction
Online product reviews have become an increasingly
valuable and influential source of information for
consumers. Different reviewers may choose to com-
ment on different properties or aspects of a product;
therefore their reviews focus on different qualities of
the product. Even when they discuss the same prop-
erties, their experiences and, subsequently, evalua-
tions of the product can differ dramatically. Thus,
information in any single review may not provide
a complete and balanced view representative of the
product as a whole. To address this need, online re-
tailers often use simple aggregation mechanisms to
represent the spectrum of user sentiment. For in-
stance, product pages on Amazon prominently dis-
play the distribution of numerical scores across re-
Coherent property cluster
+
The martinis were very good.
The drinks - both wine and martinis - were tasty.
-
The wine list was pricey.
Their wine selection is horrible.
Incoherent property cluster
+
The sushi is the best I?ve ever had.
Best paella I?d ever had.
The fillet was the best steak we?d ever had.
It?s the best soup I?ve ever had.
Table 1: Example clusters of restaurant review snippets.
The first cluster represents a coherent property of the un-
derlying product, namely the cocktail property, and as-
sesses distinctions in user sentiment. The latter cluster
simply shares a common attribute expression and does
not represent snippets discussing the same product prop-
erty. In this work, we aim to produce the first type of
property cluster with correct sentiment labeling.
views, providing access to reviews at different levels
of satisfaction.
The goal of our work is to provide a mechanism
for review content aggregation that goes beyond nu-
merical scores. Specifically, we are interested in
identifying fine-grained product properties across
reviews (e.g., battery life for electronics or pizza for
restaurants) as well as capturing attributes of these
properties, namely aggregate user sentiment.
For this task, we assume as input a set of prod-
uct review snippets (i.e., standalone phrases such as
?battery life is the best I?ve found?) rather than com-
plete reviews. There are many techniques for ex-
tracting this type of snippet in existing work; we use
the Sauper et al (2010) system.
350
At first glance, this task can be solved using ex-
isting methods for review analysis. These methods
can effectively extract product properties from indi-
vidual snippets along with their corresponding sen-
timent. While the resulting property-attribute pairs
form a useful abstraction for cross-review analysis,
in practice direct comparison of these pairs is chal-
lenging.
Consider, for instance, the two clusters of restau-
rant review snippets shown in Figure 1. While both
clusters have many words in common among their
members, only the first describes a coherent prop-
erty cluster, namely the cocktail property. The snip-
pets of the latter cluster do not discuss a single prod-
uct property, but instead share similar expressions
of sentiment. To solve this issue, we need a method
which can correctly identify both property and sen-
timent words.
In this work, we propose an approach that jointly
analyzes the whole collection of product review
snippets, induces a set of learned properties, and
models the aggregate user sentiment towards these
properties. We capture this idea using a Bayesian
topic model where a set of properties and corre-
sponding attribute tendencies are represented as hid-
den variables. The model takes product review snip-
pets as input and explains how the observed text
arises from the latent variables, thereby connecting
text fragments with corresponding properties and at-
tributes.
The advantages of this formulation are twofold.
First, this encoding provides a common ground for
comparing and aggregating review content in the
presence of varied lexical realizations. For instance,
this representation allows us to directly compare
how many reviewers liked a given property of a
product. Second, our model yields an efficient
mean-field variational inference procedure which
can be parallelized and run on a large number of re-
view snippets.
We evaluate our approach in the domain of snip-
pets taken from restaurant reviews on Yelp. In this
collection, each restaurant has on average 29.8 snip-
pets representing a wide spectrum of opinions about
a restaurant. The evaluation we present demon-
strates that the model can accurately retrieve clusters
of review fragments that describe the same property,
yielding 20% error reduction over a standalone clus-
tering baseline. We also show that the model can ef-
fectively identify binary snippet attributes with 9.2%
error reduction over applicable baselines, demon-
strating that learning to identify attributes in the con-
text of other product reviews yields significant gains.
Finally, we evaluate our model on its ability to iden-
tify product properties for which there is significant
sentiment disagreement amongst user snippets. This
tests our model?s capacity to jointly identify proper-
ties and assess attributes.
2 Related Work
Our work on review aggregation has connections to
three lines of work in text analysis.
First, our work relates to research on extraction of
product properties with associated sentiment from
review text (Hu and Liu, 2004; Liu et al, 2005a;
Popescu et al, 2005). These methods identify rele-
vant information in a document using a wide range
of methods such as association mining (Hu and Liu,
2004), relaxation labeling (Popescu et al, 2005) and
supervised learning (Kim and Hovy, 2006). While
our method also extracts product properties and sen-
timent, our focus is on multi-review aggregation.
This task introduces new challenges which were
not addressed in prior research that focused on per-
document analysis.
A second related line of research is multi-
document review summarization. Some of
these methods directly apply existing domain-
independent summarization methods (Seki et al,
2006), while others propose new methods targeted
for opinion text (Liu et al, 2005b; Carenini et al,
2006; Hu and Liu, 2006; Kim and Zhai, 2009). For
instance, these summaries may present contrastive
view points (Kim and Zhai, 2009) or relay average
sentiment (Carenini et al, 2006). The focus of this
line of work is on how to select suitable sentences,
assuming that relevant review features (such as nu-
merical scores) are given. Since our emphasis is on
multi-review analysis, we believe that the informa-
tion we extract can benefit existing summarization
systems.
Finally, a number of approaches analyze review
documents using probabilistic topic models (Lu and
Zhai, 2008; Titov and McDonald, 2008; Mei et al,
2007). While some of these methods focus primar-
351
ily on modeling ratable aspects (Titov and McDon-
ald, 2008), others explicitly capture the mixture of
topics and sentiments (Mei et al, 2007). These ap-
proaches are capable of identifying latent topics in
the collection in opinion text (e.g., weblogs) as well
as associated sentiment. While our model captures
similar high-level intuition, it analyzes fine-grained
properties expressed at the snippet level, rather than
document-level sentiment. Delivering analysis at
such a fine granularity requires a new technique.
3 Problem Formulation
In this section, we discuss the core random variables
and abstractions of our model. We describe the gen-
erative models over these elements in Section 4.
Product: A product represents a reviewable ob-
ject. For the experiments in this paper, we use
restaurants as products.
Snippets: A snippet is a user-generated short se-
quence of tokens describing a product. Input snip-
pets are deterministically taken from the output of
the Sauper et al (2010) system.
Property: A property corresponds to some fine-
grained aspect of a product. For instance, the snippet
?the pad thai was great? describes the pad thai prop-
erty. We assume that each snippet has a single prop-
erty associated with it. We assume a fixed number
of possible properties K for each product.
For the corpus of restaurant reviews, we assume
that the set of properties are specific to a given prod-
uct, in order to capture fine-grained, relevant proper-
ties for each restaurant. For example, reviews from a
sandwich shop may contrast the club sandwich with
the turkey wrap, while for a more general restau-
rant, the snippets refer to sandwiches in general. For
other domains where the properties are more consis-
tent, it is straightforward to alter our model so that
properties are shared across products.
Attribute: An attribute is a description of a prop-
erty. There are multiple attribute types, which may
correspond to semantic differences. We assume a
fixed, pre-specified number of attributes N . For
example, in the case of product reviews, we select
N = 2 attributes corresponding to positive and neg-
ative sentiment. In the case of information extrac-
tion, it may be beneficial to use numeric and alpha-
betic types.
One of the goals of this work in the review do-
main is to improve sentiment prediction by exploit-
ing correlations within a single property cluster. For
example, if there are already many snippets with the
attribute representing positive sentiment in a given
property cluster, additional snippets are biased to-
wards positive sentiment as well; however, data can
always override this bias.
Snippets themselves are always observed; the
goal of this work is to induce the latent property and
attribute underlying each snippet.
4 Model
Our model generates the words of all snippets for
each product in a collection of products. We use
s
i,j,w to represent the wth word of the jth snippet
of the ith product. We use s to denote the collec-
tion of all snippet words. We also assume a fixed
vocabulary of words V .
We present an overview of our generative model
in Figure 1 and describe each component in turn:
Global Distributions: At the global level, we
draw several unigram distributions: a global back-
ground distribution ?B and attribute distributions
?
a
A for each attribute. The background distribution
is meant to encode stop-words and domain white-
noise, e.g., food in the restaurants domain. In this
domain, the positive and negative attribute distribu-
tions encode words with positive and negative senti-
ments (e.g., delicious or terrible).
Each of these distributions are drawn from Dirich-
let priors. The background distribution is drawn
from a symmetric Dirichlet with concentration
?B = 0.2. The positive and negative attribute dis-
tributions are initialized using seed words (Vseeda
in Figure 1). These seeds are incorporated into
the attribute priors: a non-seed word gets  hyper-
parameter and a seed word gets  + ?A, where
 = 0.25 and ?A = 1.0.
Product Level: For the ith product, we draw
property unigram distributions ?i,1P , . . . , ?
i,K
P for
each of the possibleK product properties. The prop-
erty distribution represents product-specific content
distributions over properties discussed in reviews of
the product; for instance in the restaurant domains,
properties may correspond to distinct menu items.
Each ?i,kP is drawn from a symmetric Dirichlet prior
352
Global Level:
- Draw background distribution ?B ? DIRICHLET(?BV )
- For each attribute type a,
- Draw attribute distribution ?aA ? DIRICHLET(V + ?AVseeda)
Product Level:
- For each product i,
- Draw property distributions ?kP ? DIRICHLET(?PV ) for k = 1, . . . ,K
- Draw property attribute binomial ?i,k ? BETA(?A, ?A) for k = 1, . . . ,K
- Draw property multinomial ?i ? DIRICHLET(?MK)
Snippet Level:
- For each snippet j in ith product,
- Draw snippet property Zi,jP ? ?i
- Draw snippet attribute Zi,jA ? ?Z
ij
P
- Draw sequence of word topic indicators Zi,j,wW ? ?|Z
i,j,w?1
W
- Draw snippet word given property Zi,jP and attribute Z
i,j
A
si,j,w ?
?
???
???
?
i,Zi,jP
P , when Z
i,j,w
W = P
?
Zi,jA
A , when Z
i,j,w
W = A
?B, when Zi,j,wW = B
?
B
?
a
A
?
?
k
Z
i?1
W
Z
i
W
Z
i+1
W
w
i?1
w
i
w
i+1
HMM over snippet words
Background word 
distribution 
Attribute word 
distributions 
Product
Snippet
Z
P
Z
A
Property
multinomial
Property attribute 
binomials
?
k
P
Property word
distributions
Property
Snippet attributeSnippet property
?
a
A
Z
P
, ?
P
Z
A
, ?
A
?
B
Attribute
Figure 1: A high-level verbal and graphical description for our model in Section 4. We use DIRICHLET(?V ) to denote
a finite Dirichlet prior where the hyper-parameter counts are a scalar times the unit vector of vocabulary items. For
the global attribute distribution, the prior hyper-parameter counts are  for all vocabulary items and ?
A
for V
seeda , the
vector of vocabulary items in the set of seed words for attribute a.
with hyper-parameter ?P = 0.2.
For each property k = 1, . . . ,K. ?i,k, we draw a
binomial distribution ?i,k. This represents the dis-
tribution over positive and negative attributes for
that property; it is drawn from a beta prior using
hyper-parameters ?A = 2 and ?A = 2. We also
draw a multinomial ?i over K possible properties
from a symmetric Dirichlet distribution with hyper-
parameter ?M = 1, 000. This distribution is used to
draw snippet properties.
Snippet Level: For the jth snippet of the ith prod-
uct, a property random variable Zi,jP is drawn ac-
cording to the multinomial ?i. Conditioned on this
choice, we draw an attribute Zi,jA (positive or nega-
tive) from the property attribute distribution ?i,Z
j,j
P .
Once the property Zi,jP and attribute Z
i,j
A have
been selected, the tokens of the snippet are gener-
ated using a simple HMM. The latent state underly-
ing a token, Zi,j,wW , indicates whether the wth word
comes from the property distribution, attribute dis-
tribution, or background distribution; we use P , A,
or B to denote these respective values of Zi,j,wW .
The sequence Zi,j,1W , . . . , Z
i,j,m
W is generated us-
ing a first-order Markov model. The full transition
parameter matrix ? parametrizes these decisions.
Conditioned on the underlying Zi,j,wW , a word, s
i,j,w
is drawn from ?i,jP , ?
i,Zi,jP
A , or ?B for the values P ,A,
or B respectively.
5 Inference
The goal of inference is to predict the snippet prop-
erty and attribute distributions over each snippet
given all the observed snippets P (Zi,jP , Z
i,j
A |s) for
all products i and snippets j. Ideally, we would like
to marginalize out nuisance random variables and
distributions. Specifically, we approximate the full
353
model posterior using variational inference:1
P (?,?P , ?B,?A,?, |s) ?
Q(?,?P , ?B,?A,?)
where?, ?P ,? denote the collection of latent distri-
butions in our model. Here, we assume a full mean-
field factorization of the variational distribution; see
Figure 2 for the decomposition. Each variational
factor q(?) represents an approximation of that vari-
able?s posterior given observed random variables.
The variational distribution Q(?) makes the (incor-
rect) assumption that the posteriors amongst factors
are independent. The goal of variational inference is
to set factors q(?) so that it minimizes the KL diver-
gence to the true model posterior:
min
Q(?)
KL(P (?,?P , ?B,?A,?, |s)?
Q(?,?P , ?B,?A,?)
We optimize this objective using coordinate descent
on the q(?) factors. Concretely, we update each fac-
tor by optimizing the above criterion with all other
factors fixed to current values. For instance, the up-
date for the factor q(Zi,j,wW ) takes the form:
q(Zi,j,wW )?
EQ/q(Zi,j,wW )
lgP (?,?P , ?B,?A,?, s)
The full factorization of Q(?) and updates for
all random variable factors are given in Figure 2.
Updates of parameter factors are omitted; however
these are derived through simple counts of the ZA,
ZP , and ZW latent variables. For related discussion,
see Blei et al (2003).
6 Experiments
In this section, we describe in detail our data set and
present three experiments and their results.
Data Set Our data set consists of snippets from
Yelp reviews generated by the system described in
Sauper et al (2010). This system is trained to ex-
tract snippets containing short descriptions of user
sentiment towards some aspect of a restaurant.2 We
1See Liang and Klein (2007) for an overview of variational tech-
niques.
2For exact training procedures, please reference that paper.
The [P noodles ] and the [P meat ] were actually [+ pretty good ].
I [+ recommend ] the [P chicken noodle pho ].
The [P noodles ] were [- soggy ].
The [P chicken pho ] was also [+ good ].
The [P spring rolls ] and [P coffee ] were [+ good ] though.
The [P spring roll wrappers ] were a [- little dry tasting ].
My [+ favorites ] were the [P crispy spring rolls ].
The [P Crispy Tuna Spring Rolls ] are [+ fantastic ]!
The [P lobster roll ] my mother ordered was [- dry ] and [- scant ].
The [P portabella mushroom ] is my [+ go-to ] [P sandwich ].
The [P bread ] on the [P sandwich ] was [- stale ].
The slice of [P tomato ] was [- rather measly ].
The [P shumai ] and [P California maki sushi ] were [+ decent ].
The [P spicy tuna roll ] and [P eel roll ] were [+ perfect ].
The [P rolls ] with [P spicy mayo ] were [- not so great ].
I [+ love ] [P Thai rolls ].
Figure 3: Example snippets from our data set, grouped
according to property. Property words are labeled P and
colored blue, NEGATIVE attribute words are labeled - and
colored red, and POSITIVE attribute words are labeled +
and colored green. The grouping and labeling are not
given in the data set and must be learned by the model.
select only the snippets labeled by that system as ref-
erencing food, and we ignore restaurants with fewer
than 20 snippets. There are 13,879 snippets in to-
tal, taken from 328 restaurants in and around the
Boston/Cambridge area. The average snippet length
is 7.8 words, and there are an average of 42.1 snip-
pets per restaurant, although there is high variance
in number of snippets for each restaurant. Figure 3
shows some example snippets.
For sentiment attribute seed words, we use 42 and
33 words for the positive and negative distributions
respectively. These are hand-selected based on the
restaurant review domain; therefore, they include
domain-specific words such as delicious and gross.
Tasks We perform three experiments to evaluate
our model?s effectiveness. First, a cluster predic-
tion task is designed to test the quality of the learned
property clusters. Second, an attribute analysis task
will evaluate the sentiment analysis portion of the
model. Third, we present a task designed to test
whether the system can correctly identify properties
which have conflicting attributes, which tests both
clustering and sentiment analysis.
354
Mean-field Factorization
Q(?,?P , ?B,?A,?) = q(?B)
(
N?
a=1
q(?aA)
)?
?
n?
i
(
K?
k=1
q(?i,kP )q(?
i,k)
)?
?
?
j
q(Zi,jA )q(Z
i,j
P )
?
w
q(Zi,j,wW )
?
?
?
?
Snippet Property Indicator
lg q(Zi,jP = k) ? Eq(?i) lg?
i(p) +
?
w
q(Zi,j,wW = P )Eq(?i,kP )
lg ?i,kP (s
i,j,w) +
N?
a=1
q(Zi,jA = a)Eq(?i,k) lg ?
i,k(a)
Snippet Attribute Indicator
lg q(Zi,jA = a) =
?
k
q(Zi,jP = k)Eq(?i,k) lg ?
i,k(a) +
?
w
q(Zi,j,wW = A)Eq(?aA) lg ?
a
A(s
i,j,w)
Word Topic Indicator
lg q(Zi,j,wW = P ) ? lgP (ZW = P ) +
?
k
q(Zi,jP = k)Eq(?i,kP )
lg ?i,jP (s
i,j,w)
lg q(Zi,j,wW = A) ? lgP (ZW = A) +
?
a?{+,?}
q(Zi,jA = a)Eq(?aA) lg ?
a
A(s
i,j,w)
lg q(Zi,j,wW = B) ? lgP (ZW = B) + Eq(?B) lg ?B(s
i,j,w)
Figure 2: The mean-field variational algorithm used during learning and inference to obtain posterior predictions over
snippet properties and attributes, as described in Section 5. Mean-field inference consists of updating each of the latent
variable factors as well as a straightforward update of latent parameters in round robin fashion.
6.1 Cluster prediction
The goal of this task is to evaluate the quality of
property clusters; specifically the Zi,jP variable in
Section 4. In an ideal clustering, the predicted clus-
ters will be cohesive (i.e., all snippets predicted for
a given property are related to each other) and com-
prehensive (i.e., all snippets which are related to a
property are predicted for it). For example, a snip-
pet will be assigned the property pad thai if and only
if that snippet mentions some aspect of the pad thai.
Annotation For this task, we use a set of gold
clusters over 3,250 snippets across 75 restaurants
collected through Mechanical Turk. In each task, a
worker was given a set of 25 snippets from a single
restaurant and asked to cluster them into as many
clusters as they desired, with the option of leaving
any number unclustered. This yields a set of gold
clusters and a set of unclustered snippets. For verifi-
cation purposes, each task was provided to two dif-
ferent workers. The intersection of both workers?
judgments was accepted as the gold standard, so the
model is not evaluated on judgments which disagree.
In total, there were 130 unique tasks, each of which
were provided to two workers, for a total output of
210 generated clusters.
Baseline The baseline for this task is a cluster-
ing algorithm weighted by TF*IDF over the data set
as implemented by the publicly available CLUTO
package.3 This baseline will put a strong connec-
tion between things which are lexically similar. Be-
cause our model only uses property words to tie
together clusters, it may miss correlations between
words which are not correctly identified as property
words. The baseline is allowed 10 property clusters
per restaurant.
We use the MUC cluster evaluation metric for
this task (Vilain et al, 1995). This metric measures
the number of cluster merges and splits required to
recreate the gold clusters given the model?s output.
3Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
with agglomerative clustering, using the cosine similarity
distance metric.
355
Precision Recall F1
Baseline 80.2 61.1 69.3
Our model 72.2 79.1 75.5
Table 2: Results using the MUC metric on the cluster
prediction task. Note that while the precision of the base-
line is higher, the recall and overall F1 of our model out-
weighs that. While MUC has a deficiency in that putting
everything into a single cluster will artificially inflate the
score, parameters on our model are set so that the model
uses the same number of clusters as the baseline system.
Therefore, it can concisely show how accurate our
clusters are as a whole. While it would be possible
to artificially inflate the score by putting everything
into a single cluster, the parameters on our model
and the likelihood objective are such that the model
prefers to use all available clusters, the same number
as the baseline system.
Results Results for our cluster prediction task are
in Table 2. While our system does suffer on preci-
sion in comparison to the baseline system, the recall
gains far outweigh this loss, for a total error reduc-
tion of 20% on the MUC measure.
The most common cause of poor cluster choices
in the baseline system is its inability to distinguish
property words from attribute words. For example,
if many snippets in a given restaurant use the word
delicious, there may end up being a cluster based on
that alone. Because our system is capable of dis-
tinguishing which words are property words (i.e.,
words relevant to clustering), it can choose clusters
which make more sense overall. We show an exam-
ple of this in Table 3.
6.2 Attribute analysis
We also evaluate the system?s predictions of snip-
pet attribute using the predicted posterior over the
attribute distribution for the snippet (i.e., Zi,jA ). For
this task, we consider the binary judgment to be sim-
ply the one with higher value in q(Zi,jA ) (see Sec-
tion 5). The goal of this task is to evaluate whether
our model correctly distinguishes attribute words.
Annotation For this task, we use a set of 260 to-
tal snippets from the Yelp reviews for 30 restaurants,
evenly split into a training and test sets of 130 snip-
pets each. These snippets are manually labeled POS-
The martini selection looked delicious
The s?mores martini sounded excellent
The martinis were good
The martinis are very good
The mozzarella was very fresh
The fish and various meets were very well made
The best carrot cake I?ve ever eaten
Carrot cake was deliciously moist
The carrot cake was delicious.
It was rich, creamy and delicious.
The pasta Bolognese was rich and robust.
Table 3: Example phrases from clusters in both the base-
line and our model. For each pair of clusters, the dashed
line indicates separation by the baseline model, while the
solid line indicates separation by our model. In the first
example, the baseline mistakenly clusters some snippets
about martinis with those containing the word very. In
the second example, the same occurs with the word deli-
cious.
ITIVE or NEGATIVE. Neutral snippets are ignored
for the purpose of this experiment.
Baseline We use two baselines for this task, one
based on a standard discriminative classifier and one
based on the seed words from our model.
The DISCRIMINATIVE baseline for this task is
a standard maximum entropy discriminative bi-
nary classifier over unigrams. Given enough snip-
pets from enough unrelated properties, the classifier
should be able to identify that words like great in-
dicate positive sentiment and those like bad indi-
cate negative sentiment, while words like chicken
are neutral and have no effect.
The SEED baseline simply counts the number of
words from the positive and negative seed lists used
by the model, Vseed+ and Vseed? . If there are more
words from Vseed+ , the snippet is labeled positive,
and if there are more words from Vseed? , the snip-
pet is labeled negative. If there is a tie or there are
no seed words, we split the prediction. Because
the seed word lists are specifically slanted toward
restaurant reviews (i.e., they contain words such as
delicious), this baseline should perform well.
Results For this experiment, we measure the over-
all classification accuracy of each system (see Table
356
Accuracy
DISCRIMINATIVE baseline 75.9
SEED baseline 78.2
Our model 80.2
Table 4: Attribute prediction accuracy of the full system
compared to the DISCRIMINATIVE and SEED baselines.
The advantage of our system is its ability to distinguish
property words from attribute words in order to restrict
judgment to only the relevant terms.
The naan was hot and fresh
All the veggies were really fresh and crisp.
Perfect mix of fresh flavors and comfort food
The lo main smelled and tasted rancid
My grilled cheese sandwich was a little gross
Table 5: Examples of sentences correctly labeled by our
system but incorrectly labeled by the DISCRIMINATIVE
baseline; the key sentiment words are highlighted. No-
tice that these words are not the most common sentiment
words; therefore, it is difficult for the classifier to make a
correct generalization. Only two of these words are seed
words for our model (fresh and gross).
4). Our system outperforms both supervised base-
lines.
As in the cluster prediction case, the main flaw
with the DISCRIMINATIVE baseline system is its in-
ability to recognize which words are relevant for the
task at hand, in this case the attribute words. By
learning to separate attribute words from the other
words in the snippets, our full system is able to more
accurately judge their sentiment. Examples of these
cases are found in Table 5.
The obvious flaw in the SEED baseline is the in-
ability to pre-specify every possible sentiment word;
our model?s performance indicates that it is learning
something beyond just these basic words.
6.3 Conflict identification
Our final task requires both correct cluster prediction
and correct sentiment judgments. In many domains,
it is interesting to know not only whether a product
is rated highly, but also whether there is conflicting
sentiment or debate. In the case of restaurant re-
views, it is relevant to know whether the dishes are
consistently good or whether there is some variation
in quality.
Judgment
P A Attribute / Snippet
Yes Yes
- The salsa isn?t great
+ Chips and salsa are sublime
- The grits were good, but not great.
+ Grits were the perfect consistency
- The tom yum kha was bland
+ It?s the best Thai soup I ever had
- The naan is a bit doughy and undercooked
+ The naan was pretty tasty
- My reuben was a little dry.
+ The reuben was a good reuben.
Yes No
- Belgian frites are crave-able
+ The frites are very, very good.
No Yes
- The blackened chicken was meh
+ Chicken enchiladas are yummy!
- The taste overall was mediocre
+ The oysters are tremendous
No No - The cream cheese wasn?t bad+ Ice cream was just delicious
Table 6: Example property-attribute correctness for the
conflict identification task, over both property and at-
tribute. Property judgment (P) indicates whether the snip-
pets are discussing the same item; attribute judgment (A)
indicates whether there is a correct difference in attribute
(sentiment), regardless of properties.
To evaluate this, we examine the output clusters
which contain predictions of both positive and neg-
ative snippets. The goal is to identify whether these
are true conflicts of sentiment or there was a failure
in either property clustering or attribute classifica-
tion.
For this task, the output clusters are manually an-
notated for correctness of both property and attribute
judgments, as in Table 6. As there is no obvious
baseline for this experiment, we treat it simply as an
analysis of errors.
Results For this task, we examine the accuracy of
conflict prediction, both with and without the cor-
rectly identified properties. The results by property-
attribute correctness are shown in Table 7. From
these numbers, we can see that 50% of the clusters
are correct in both property (cohesiveness) and at-
tribute (difference in sentiment) dimensions.
Overall, the properties are correctly identified
(subject of NEG matches the subject of POS) 68%
of the time and a correct difference in attribute is
identified 67% of the time. Of the clusters which
are correct in property, 74% show a correctly labeled
357
Judgment
P A # Clusters
Yes Yes 52
Yes No 18
No Yes 17
No No 15
Table 7: Results of conflict analysis by correctness of
property label (P) and attribute conflict (A). Examples
of each type of correctness pair are show in in Table 6.
50% of the clusters are correct in both labels, and there
are approximately the same number of errors toward both
property and attribute.
difference in attribute.
7 Conclusion
We have presented a probabilistic topic model for
identifying properties and attitudes of product re-
view snippets. The model is relatively simple and
admits an efficient variational mean-field inference
procedure which is parallelized and can be run on
a large number of snippets. We have demonstrated
on multiple evaluation tasks that our model outper-
forms applicable baselines by a considerable mar-
gin.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168), NIH (grant 5-
R01-LM009723-02), Nokia, and the DARPA Ma-
chine Reading Program (AFRL prime contract no.
FA8750-09-C-0172). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006. Multi-document summarization of evaluative
text. In Proceedings of EACL, pages 305?312.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168?177.
Minqing Hu and Bing Liu. 2006. Opinion extraction and
summarization on the web. In Proceedings of AAAI.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of COLING/ACL, pages 483?490.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generat-
ing comparative summaries of contradictory opinions
in text. In Proceedings of CIKM, pages 385?394.
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tutorial).
In Proceedings of ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005a.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342?351.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005b.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342?351.
Yue Lu and ChengXiang Zhai. 2008. Opinion integra-
tion through semi-supervised topic modeling. In Pro-
ceedings of WWW, pages 121?130.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW, pages 171?180.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339?346.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of EMNLP, pages
377?387.
Yohei Seki, Koji Eguchi, Noriko K, and Masaki Aono.
2006. Opinion-focused summarization and its analysis
at DUC 2006. In Proceedings of DUC, pages 122?
130.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of ACL, pages 308?316.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC, pages 45?52.
358
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 389?398,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Event Discovery in Social Media Feeds
Edward Benson, Aria Haghighi, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{eob, aria42, regina}@csail.mit.edu
Abstract
We present a novel method for record extrac-
tion from social streams such as Twitter. Un-
like typical extraction setups, these environ-
ments are characterized by short, one sentence
messages with heavily colloquial speech. To
further complicate matters, individual mes-
sages may not express the full relation to be
uncovered, as is often assumed in extraction
tasks. We develop a graphical model that ad-
dresses these problems by learning a latent set
of records and a record-message alignment si-
multaneously; the output of our model is a
set of canonical records, the values of which
are consistent with aligned messages. We
demonstrate that our approach is able to accu-
rately induce event records from Twitter mes-
sages, evaluated against events from a local
city guide. Our method achieves significant
error reduction over baseline methods.1
1 Introduction
We propose a method for discovering event records
from social media feeds such as Twitter. The task
of extracting event properties has been well studied
in the context of formal media (e.g., newswire), but
data sources such as Twitter pose new challenges.
Social media messages are often short, make heavy
use of colloquial language, and require situational
context for interpretation (see examples in Figure 1).
Not all properties of an event may be expressed in
a single message, and the mapping between mes-
sages and canonical event records is not obvious.
1Data and code available at http://groups.csail.
mit.edu/rbg/code/twitter
Carnegie Hall
Artist Venue
Craig Ferguson
DJ Pauly D Terminal 5
Seated at @carnegiehall waiting for @CraigyFerg?s show to begin
RT @leerader : getting REALLY stoked for #CraigyAtCarnegie 
sat night. Craig, , want to join us for dinner at the pub across the 
street? 5pm, be there!
@DJPaulyD absolutely killed it at Terminal 5 last night. 
@DJPaulyD : DJ Pauly D Terminal 5 NYC Insanity ! #ohyeah 
@keadour @kellaferr24
Craig, nice seeing you at #noelnight this weekend @becksdavis!
Twitter Messages
Records
Figure 1: Examples of Twitter messages, along with
automatically extracted records.
These properties of social media streams make exist-
ing extraction techniques significantly less effective.
Despite these challenges, this data exhibits an im-
portant property that makes learning amenable: the
multitude of messages referencing the same event.
Our goal is to induce a comprehensive set of event
records given a seed set of example records, such as
a city event calendar table. While such resources
are widely available online, they are typically high
precision, but low recall. Social media is a natural
place to discover new events missed by curation, but
mentioned online by someone planning to attend.
We formulate our approach as a structured graphi-
cal model which simultaneously analyzes individual
messages, clusters them according to event, and in-
duces a canonical value for each event property. At
the message level, the model relies on a conditional
random field component to extract field values such
389
as location of the event and artist name. We bias lo-
cal decisions made by the CRF to be consistent with
canonical record values, thereby facilitating consis-
tency within an event cluster. We employ a factor-
graph model to capture the interaction between each
of these decisions. Variational inference techniques
allow us to effectively and efficiently make predic-
tions on a large body of messages.
A seed set of example records constitutes our only
source of supervision; we do not observe alignment
between these seed records and individual messages,
nor any message-level field annotation. The output
of our model consists of an event-based clustering of
messages, where each cluster is represented by a sin-
gle multi-field record with a canonical value chosen
for each field.
We apply our technique to construct entertain-
ment event records for the city calendar section of
NYC.com using a stream of Twitter messages. Our
method yields up to a 63% recall against the city
table and up to 85% precision evaluated manually,
significantly outperforming several baselines.
2 Related Work
A large number of information extraction ap-
proaches exploit redundancy in text collections to
improve their accuracy and reduce the need for man-
ually annotated data (Agichtein and Gravano, 2000;
Yangarber et al, 2000; Zhu et al, 2009; Mintz
et al, 2009a; Yao et al, 2010b; Hasegawa et al,
2004; Shinyama and Sekine, 2006). Our work most
closely relates to methods for multi-document infor-
mation extraction which utilize redundancy in in-
put data to increase the accuracy of the extraction
process. For instance, Mann and Yarowsky (2005)
explore methods for fusing extracted information
across multiple documents by performing extraction
on each document independently and then merg-
ing extracted relations by majority vote. This idea
of consensus-based extraction is also central to our
method. However, we incorporate this idea into our
model by simultaneously clustering output and la-
beling documents rather than performing the two
tasks in serial fashion. Another important difference
is inherent in the input data we are processing: it is
not clear a priori which extraction decisions should
agree with each other. Identifying messages that re-
fer to the same event is a large part of our challenge.
Our work also relates to recent approaches for re-
lation extraction with distant supervision (Mintz et
al., 2009b; Bunescu and Mooney, 2007; Yao et al,
2010a). These approaches assume a database and a
collection of documents that verbalize some of the
database relations. In contrast to traditional super-
vised IE approaches, these methods do not assume
that relation instantiations are annotated in the input
documents. For instance, the method of Mintz et al
(2009b) induces the mapping automatically by boot-
strapping from sentences that directly match record
entries. These mappings are used to learn a classi-
fier for relation extraction. Yao et al (2010a) further
refine this approach by constraining predicted rela-
tions to be consistent with entity types assignment.
To capture the complex dependencies among assign-
ments, Yao et al (2010a) use a factor graph repre-
sentation. Despite the apparent similarity in model
structure, the two approaches deal with various types
of uncertainties. The key challenge for our method
is modeling message to record alignment which is
not an issue in the previous set up.
Finally, our work fits into a broader area of
text processing methods designed for social-media
streams. Examples of such approaches include
methods for conversation structure analysis (Ritter
et al, 2010) and exploration of geographic language
variation (Eisenstein et al, 2010) from Twitter mes-
sages. To our knowledge no work has yet addressed
record extraction from this growing corpus.
3 Problem Formulation
Here we describe the key latent and observed ran-
dom variables of our problem. A depiction of all
random variables is given in Figure 2.
Message (x): Each message x is a single posting to
Twitter. We use xj to represent the jth token of x,
and we use x to denote the entire collection of mes-
sages. Messages are always observed during train-
ing and testing.
Record (R): A record is a representation of the
canonical properties of an event. We use Ri to de-
note the ith record and R`i to denote the value of the
`th property of that record. In our experiments, each
record Ri is a tuple ?R1i , R2i ? which represents that
390
Mercury Lounge
Yonder Mountain 
String Band
Craig Ferguson Carnegie Hall
Artist Venue
1
2
k R?k R?+1k
.
.
.
 Really     excited      for    #CraigyAtCarnegie
Seeing     Yonder    Mountain        at             8
@YonderMountain  rocking  Mercury  Lounge
None None None Artist
None NoneArtist Artist None
Venue VenueNoneArtist
xi
yi
xi?1
yi?1
xi+1
yi+1
Ai?1
Ai+1
Ai
Figure 2: The key variables of our model. A collection ofK latent recordsRk, each consisting of a set ofL properties.
In the figure above, R11 =?Craig Ferguson? and R21 =?Carnegie Hall.? Each tweet xi is associated with a labeling
over tokens yi and is aligned to a record via the Ai variable. See Section 3 for further details.
record?s values for the schema ?ARTIST, VENUE?.
Throughout, we assume a known fixed number K
of records R1, . . . , RK , and we use R to denote this
collection of records. For tractability, we consider
a finite number of possibilities for each R`k which
are computed from the input x (see Section 5.1 for
details). Records are observed during training and
latent during testing.
Message Labels (y): We assume that each message
has a sequence labeling, where the labels consist of
the record fields (e.g., ARTIST and VENUE) as well
as a NONE label denoting the token does not corre-
spond to any domain field. Each token xj in a mes-
sage has an associated label yj . Message labels are
always latent during training and testing.
Message to Record Alignment (A): We assume
that each message is aligned to some record such
that the event described in the message is the one
represented by that record. Each message xi is as-
sociated with an alignment variable Ai that takes a
value in {1, . . . ,K}. We use A to denote the set of
alignments across all xi. Multiple messages can and
do align to the same record. As discussed in Sec-
tion 4, our model will encourage tokens associated
with message labels to be ?similar? to corresponding
aligned record values. Alignments are always latent
during training and testing.
4 Model
Our model can be represented as a factor graph
which takes the form,
P (R,A, y|x) ?
(
?
i
?SEQ(xi, yi)
)
(Seq. Labeling)
(
?
`
?UNQ(R`)
)
(Rec. Uniqueness)
?
?
?
i,`
?POP (xi, yi, R`Ai)
?
? (Term Popularity)
(
?
i
?CON (xi, yi, RAi)
)
(Rec. Consistency)
where R` denotes the sequence R`1, . . . , R`K of
record values for a particular domain field `. Each
of the potentials takes a standard log-linear form:
?(z) = ?T f(z)
where ? are potential-specific parameters and f(?)
is a potential-specific feature function. We describe
each potential separately below.
4.1 Sequence Labeling Factor
The sequence labeling factor is similar to a standard
sequence CRF (Lafferty et al, 2001), where the po-
tential over a message label sequence decomposes
391
XiYi
?SEQ
R?k
R?k+1
R?k?1?UNQ ?th field(across records)
?
?POP
R?k
Ai
Yi
Xi
Ai
Yi
Xi
R?k R?+1k
?CON
k
kth record
Figure 3: Factor graph representation of our model. Circles represent variables and squares represent factors. For
readability, we depict the graph broken out as a set of templates; the full graph is the combination of these factor
templates applied to each variable. See Section 4 for further details.
over pairwise cliques:
?SEQ(x, y) = exp{?TSEQfSEQ(x, y)}
=exp
?
?
?
?TSEQ
?
j
fSEQ(x, yj , yj+1)
?
?
?
This factor is meant to encode the typical message
contexts in which fields are evoked (e.g. going to see
X tonight). Many of the features characterize how
likely a given token label, such as ARTIST, is for a
given position in the message sequence conditioning
arbitrarily on message text context.
The feature function fSEQ(x, y) for this compo-
nent encodes each token?s identity; word shape2;
whether that token matches a set of regular expres-
sions encoding common emoticons, time references,
and venue types; and whether the token matches a
bag of words observed in artist names (scraped from
Wikipedia; 21,475 distinct tokens from 22,833 dis-
tinct names) or a bag of words observed in New
York City venue names (scraped from NYC.com;
304 distinct tokens from 169 distinct names).3 The
only edge feature is label-to-label.
4.2 Record Uniqueness Factor
One challenge with Twitter is the so-called echo
chamber effect: when a topic becomes popular, or
?trends,? it quickly dominates the conversation on-
line. As a result some events may have only a few
referent messages while other more popular events
may have thousands or more. In such a circum-
stance, the messages for a popular event may collect
to form multiple identical record clusters. Since we
2e.g.: xxx, XXX, Xxx, or other
3These are just features, not a filter; we are free to extract
any artist or venue regardless of their inclusion in this list.
fix the number of records learned, such behavior in-
hibits the discovery of less talked-about events. In-
stead, we would rather have just two records: one
with two aligned messages and another with thou-
sands. To encourage this outcome, we introduce a
potential that rewards fields for being unique across
records.
The uniqueness potential ?UNQ(R`) encodes the
preference that each of the values R`, . . . , R`K for
each field ` do not overlap textually. This factor fac-
torizes over pairs of records:
?UNQ(R`) =
?
k 6=k?
?UNQ(R`k, R`k?)
where R`k and R`k? are the values of field ` for two
records Rk and Rk? . The potential over this pair of
values is given by:
?UNQ(R`k, R`k?) = exp{??TSIMfSIM (R`k, R`k?)}
where fSIM is computes the likeness of the two val-
ues at the token level:
fSIM (R`k, R`k?) = |R
`
k ?R`k? |
max(|R`k|, |R`k? |)
This uniqueness potential does not encode any
preference for record values; it simply encourages
each field ` to be distinct across records.
4.3 Term Popularity Factor
The term popularity factor ?POP is the first of two
factors that guide the clustering of messages. Be-
cause speech on Twitter is colloquial, we would like
these clusters to be amenable to many variations of
the canonical record properties that are ultimately
learned. The ?POP factor accomplishes this by rep-
resenting a lenient compatibility score between a
392
message x, its labels y, and some candidate value
v for a record field (e.g., Dave Matthews Band).
This factor decomposes over tokens, and we align
each token xj with the best matching token vk in v
(e.g., Dave). The token level sum is scaled by the
length of the record value being matched to avoid a
preference for long field values.
?POP (x, y,R`A = v) =
?
j
max
k
?POP (xj , yj , R`A = vk)
|v|
This token-level component may be thought of as
a compatibility score between the labeled token xj
and the record field assignment R`A = v. Given that
token xj aligns with the token vk, the token-level
component returns the sum of three parts, subject to
the constraint that yj = `:
? IDF (xj)I[xj = vk], an equality indicator be-
tween tokens xj and vk, scaled by the inverse
document frequency of xj
? ?IDF (xj) (I[xj?1 = vk?1] + I[xj+1 = vk+1]),
a small bonus of ? = 0.3 for matches on adja-
cent tokens, scaled by the IDF of xj
? I[xj = vk and x contains v]/|v|, a bonus for a
complete string match, scaled by the size of the
value. This is equivalent to this token?s contri-
bution to a complete-match bonus.
4.4 Record Consistency Factor
While the uniqueness factor discourages a flood of
messages for a single event from clustering into mul-
tiple event records, we also wish to discourage mes-
sages from multiple events from clustering into the
same record. When such a situation occurs, the
model may either resolve it by changing inconsis-
tent token labelings to the NONE label or by reas-
signing some of the messages to a new cluster. We
encourage the latter solution with a record consis-
tency factor ?CON .
The record consistency factor is an indicator func-
tion on the field values of a record being present and
labeled correctly in a message. While the popular-
ity factor encourages agreement on a per-label basis,
this factor influences the joint behavior of message
labels to agree with the aligned record. For a given
record, message, and labeling, ?CON (x, y,RA) = 1
if ?POP (x, y,R`A) > 0 for all `, and 0 otherwise.
4.5 Parameter Learning
The weights of the CRF component of our model,
?SEQ, are the only weights learned at training time,
using a distant supervision process described in Sec-
tion 6. The weights of the remaining three factors
were hand-tuned4 using our training data set.
5 Inference
Our goal is to predict a set of records R. Ideally we
would like to compute P (R|x), marginalizing out
the nuisance variables A and y. We approximate
this posterior using variational inference.5 Con-
cretely, we approximate the full posterior over latent
variables using a mean-field factorization:
P (R,A,y|x) ? Q(R,A,y)
=
(
K?
k=1
?
`
q(R`k)
)(
n?
i=1
q(Ai)q(yi)
)
where each variational factor q(?) represents an ap-
proximation of that variable?s posterior given ob-
served random variables. The variational distribu-
tion Q(?) makes the (incorrect) assumption that the
posteriors amongst factors are independent. The
goal of variational inference is to set factors q(?) to
optimize the variational objective:
min
Q(?)
KL(Q(R,A,y)?P (R,A,y|x))
We optimize this objective using coordinate descent
on the q(?) factors. For instance, for the case of q(yi)
the update takes the form:
q(yi)? EQ/q(yi) logP (R,A,y|x)
where Q/q(yi) denotes the expectation under all
variables except yi. When computing a mean field
update, we only need to consider the potentials in-
volving that variable. The complete updates for each
of the kinds of variables (y, A, andR`) can be found
in Figure 4. We briefly describe the computations
involved with each update.
q(y) update: The q(y) update for a single mes-
sage yields an implicit expression in terms of pair-
wise cliques in y. We can compute arbitrary
4Their values are: ?UNQ = ?10, ?PhrasePOP = 5, ?
Token
POP = 10,
?CON = 2e8
5See Liang and Klein (2007) for an overview of variational
techniques.
393
Message labeling update:
ln q(y) ?
{
EQ/q(y) ln?SEQ(x, y) + ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
= ln?SEQ(x, y) + EQ/q(y) ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]
= ln?SEQ(x, y) +
?
z,v,`
q(A = z)q(yj = `)q(R`z = v) ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]
Mention record alignment update:
ln q(A = z) ? EQ/q(A)
{
ln?SEQ(x, y) + ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
? EQ/q(A)
{
ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
= ?
z,v,`
q(R`z = v)
{
ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]}
= ?
z,v,`
q(R`z = v)q(yji = `) ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]
Record Field update:
ln q(R`k = v) ? EQ/q(R`k)
{
?
k?
ln?UNQ(R`k? , v) +
?
i
ln [?POP (xi, yi, v)?CON (xi, yi, v)]
}
= ?
k? 6=k,v?
(
q(R`k? = v?) ln?UNQ(v, v?)
+?
i
q(Ai = k)
?
j
q(yji = `) ln
[
?POP (x, y,R`z = v, j)?CON (x, y,R`z = v, j)
])
Figure 4: The variational mean-field updates used during inference (see Section 5). Inference consists of performing
updates for each of the three kinds of latent variables: message labels (y), record alignments (A), and record field
values (R`). All are relatively cheap to compute except for the record field update q(R`k) which requires looping
potentially over all messages. Note that at inference time all parameters are fixed and so we only need to perform
updates for latent variable factors.
marginals for this distribution by using the forwards-
backwards algorithm on the potentials defined in
the update. Therefore computing the q(y) update
amounts to re-running forward backwards on the
message where there is an expected potential term
which involves the belief over other variables. Note
that the popularity and consensus potentials (?POP
and ?CON ) decompose over individual message to-
kens so this can be tractably computed.
q(A) update: The update for individual record
alignment reduces to being log-proportional to the
expected popularity and consensus potentials.
q(R`k) update: The update for the record field
distribution is the most complex factor of the three.
It requires computing expected similarity with other
record field values (the ?UNQ potential) and looping
over all messages to accumulate a contribution from
each, weighted by the probability that it is aligned to
the target record.
5.1 Initializing Factors
Since a uniform initialization of all factors is a
saddle-point of the objective, we opt to initialize
the q(y) factors with the marginals obtained using
just the CRF parameters, accomplished by running
forwards-backwards on all messages using only the
394
?SEQ potentials. The q(R) factors are initialized
randomly and then biased with the output of our
baseline model. The q(A) factor is initialized to uni-
form plus a small amount of noise.
To simplify inference, we pre-compute a finite set
of values that each R`k is allowed to take, condi-
tioned on the corpus. To do so, we run the CRF
component of our model (?SEQ) over the corpus and
extract, for each `, all spans that have a token-level
probability of being labeled ` greater than ? = 0.1.
We further filter this set down to only values that oc-
cur at least twice in the corpus.
This simplification introduces sparsity that we
take advantage of during inference to speed perfor-
mance. Because each term in ?POP and ?CON in-
cludes an indicator function based on a token match
between a field-value and a message, knowing the
possible values v of each R`k enables us to precom-
pute the combinations of (x, `, v) for which nonzero
factor values are possible. For each such tuple, we
can also precompute the best alignment position k
for each token xj .
6 Evaluation Setup
Data We apply our approach to construct a database
of concerts in New York City. We used Twitter?s
public API to collect roughly 4.7 Million tweets
across three weekends that we subsequently filter
down to 5,800 messages. The messages have an av-
erage length of 18 tokens, and the corpus vocabu-
lary comprises 468,000 unique words6. We obtain
labeled gold records using data scraped from the
NYC.com music event guide; totaling 110 extracted
records. Each gold record had two fields of interest:
ARTIST and VENUE.
The first weekend of data (messages and events)
was used for training and the second two weekends
were used for testing.
Preprocessing Only a small fraction of Twitter mes-
sages are relevant to the target extraction task. Di-
rectly processing the raw unfiltered stream would
prohibitively increase computational costs and make
learning more difficult due to the noise inherent in
the data. To focus our efforts on the promising por-
tion of the stream, we perform two types of filter-
6Only considering English tweets and not counting user
names (so-called -mentions.)
ing. First, we only retain tweets whose authors list
some variant of New York as their location in their
profile. Second, we employ a MIRA-based binary
classifier (Ritter et al, 2010) to predict whether a
message mentions a concert event. After training on
2,000 hand-annotated tweets, this classifier achieves
an F1 of 46.9 (precision of 35.0 and recall of 71.0)
when tested on 300 messages. While the two-stage
filtering does not fully eliminate noise in the input
stream, it greatly reduces the presence of irrelevant
messages to a manageable 5,800 messages without
filtering too many ?signal? tweets.
We also filter our gold record set to include only
records in which each field value occurs at least once
somewhere in the corpus, as these are the records
which are possible to learn given the input. This
yields 11 training and 31 testing records.
Training The first weekend of data (2,184 messages
and 11 records after preprocessing) is used for train-
ing. As mentioned in Section 4, the only learned pa-
rameters in our model are those associated with the
sequence labeling factor ?SEQ. While it is possi-
ble to train these parameters via direct annotation of
messages with label sequences, we opted instead to
use a simple approach where message tokens from
the training weekend are labeled via their intersec-
tion with gold records, often called ?distant super-
vision? (Mintz et al, 2009b). Concretely, we auto-
matically label message tokens in the training cor-
pus with either the ARTIST or VENUE label if they
belonged to a sequence that matched a gold record
field, and with NONE otherwise. This is the only use
that is made of the gold records throughout training.
?SEQ parameters are trained using this labeling with
a standard conditional likelihood objective.
Testing The two weekends of data used for test-
ing totaled 3,662 tweets after preprocessing and 31
gold records for evaluation. The two weekends were
tested separately and their results were aggregated
across weekends.
Our model assumes a fixed number of records
K = 130.7 We rank these records according to
a heuristic ranking function that favors the unique-
ness of a record?s field values across the set and the
number of messages in the testing corpus that have
7Chosen based on the training set
395
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0.45	 ?
0.5	 ?
0.55	 ?
0.6	 ?
0.65	 ?
0.7	 ?
1.00	 ? 1.5	 ? 2	 ? 2.5	 ? 3	 ? 3.5	 ? 4	 ? 4.5	 ? 5	 ?
Re
ca
ll	 ?a
ga
ins
t	 ?G
old
	 ?Ev
en
t	 ?R
ec
or
ds
	 ?
k,	 ?as	 ?a	 ?mul?ple	 ?of	 ?the	 ?number	 ?of	 ?gold	 ?records	 ?
Low	 ?Thresh	 ? CRF	 ? List	 ? Our	 ?Work	 ?
Figure 5: Recall against the gold records. The horizontal
axis is the number of records kept from the ranked model
output, as a multiple of the number of golds. The CRF
lines terminate because of low record yield.
token overlap with these values. This ranking func-
tion is intended to push garbage collection records
to the bottom of the list. Finally, we retain the top k
records, throwing away the rest. Results in Section
7 are reported as a function of this k.
Baseline We compare our system against three base-
lines that employ a voting methodology similar to
Mann and Yarowsky (2005). The baselines label
each message and then extract one record for each
combination of labeled phrases. Each extraction is
considered a vote for that record?s existence, and
these votes are aggregated across all messages.
Our List Baseline labels messages by finding
string overlaps against a list of musical artists and
venues scraped from web data (the same lists used as
features in our CRF component). The CRF Baseline
is most similar to Mann and Yarowsky (2005)?s CRF
Voting method and uses the maximum likelihood
CRF labeling of each message. The Low Thresh-
old Baseline generates all possible records from la-
belings with a token-level likelihood greater than
? = 0.1. The output of these baselines is a set of
records ranked by the number of votes cast for each,
and we perform our evaluation against the top k of
these records.
7 Evaluation
The evaluation of record construction is challeng-
ing because many induced music events discussed
in Twitter messages are not in our gold data set; our
gold records are precise but incomplete. Because
of this, we evaluate recall and precision separately.
Both evaluations are performed using hard zero-one
loss at record level. This is a harsh evaluation crite-
rion, but it is realistic for real-world use.
Recall We evaluate recall, shown in Figure 5,
against the gold event records for each weekend.
This shows how well our model could do at replac-
ing the a city event guide, providing Twitter users
chat about events taking place.
We perform our evaluation by taking the top
k records induced, performing a stable marriage
matching against the gold records, and then evalu-
ating the resulting matched pairs. Stable marriage
matching is a widely used approach that finds a bi-
partite matching between two groups such that no
pairing exists in which both participants would pre-
fer some other pairing (Irving et al, 1987). With
our hard loss function and no duplicate gold records,
this amounts to the standard recall calculation. We
choose this bipartite matching technique because it
generalizes nicely to allow for other forms of loss
calculation (such as token-level loss).
Precision To evaluate precision we assembled a list
of the distinct records produced by all models and
then manually determined if each record was cor-
rect. This determination was made blind to which
model produced the record. We then used this ag-
gregate list of correct records to measure precision
for each individual model, shown in Figure 6.
By construction, our baselines incorporate a hard
constraint that each relation learned must be ex-
pressed in entirety in at least one message. Our
model only incorporates a soft version of this con-
straint via the ?CON factor, but this constraint
clearly has the ability to boost precision. To show
it?s effect, we additionally evaluate our model, la-
beled Our Work + Con, with this constraint applied
in hard form as an output filter.
The downward trend in precision that can be seen
in Figure 6 is the effect of our ranking algorithm,
which attempts to push garbage collection records
towards the bottom of the record list. As we incor-
porate these records, precision drops. These lines
trend up for two of the baselines because the rank-
396
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
10	 ? 20	 ? 30	 ? 40	 ? 50	 ?
Pr
ec
isi
on
	 ?(M
an
ua
l	 ?E
ve
lua
?o
n)	 ?
Number	 ?of	 ?Records	 ?Kept	 ?
Low	 ?Thresh	 ? CRF	 ? List	 ? Our	 ?Work	 ? Our	 ?Work	 ?+	 ?Con	 ?
Figure 6: Precision, evaluated manually by cross-
referencing model output with event mentions in the in-
put data. The CRF and hard-constrained consensus lines
terminate because of low record yield.
ing heuristic is not as effective for them.
These graphs confirm our hypothesis that we gain
significant benefit by intertwining constraints on ex-
traction consistency in the learning process, rather
than only using this constraint to filter output.
7.1 Analysis
One persistent problem is a popular phrase appear-
ing in many records, such as the value ?New York?
filling many ARTIST slots. The uniqueness factor
?UNQ helps control this behavior, but it is a rela-
tively blunt instrument. Ideally, our model would
learn, for each field `, the degree to which dupli-
cate values are permitted. It is also possible that by
learning, rather than hand-tuning, the ?CON , ?POP ,
and ?UNQ parameters, our model could find a bal-
ance that permits the proper level of duplication for
a particular domain.
Other errors can be explained by the lack of con-
stituent features in our model, such as the selection
of VENUE values that do not correspond to noun
phrases. Further, semantic features could help avoid
learning syntactically plausible artists like ?Screw
the Rain? because of the message:
Screw the rainArtist! Grab an umbrella and head down to
Webster HallVenue for some American rock and roll.
Our model?s soft string comparison-based clus-
tering can be seen at work when our model uncov-
ers records that would have been impossible without
this approach. One such example is correcting the
misspelling of venue names (e.g. Terminal Five ?
Terminal 5) even when no message about the event
spells the venue correctly.
Still, the clustering can introduce errors by com-
bining messages that provide orthogonal field con-
tributions yet have overlapping tokens (thus escap-
ing the penalty of the consistency factor). An exam-
ple of two messages participating in this scenario is
shown below; the shared term ?holiday? in the sec-
ond message gets relabeled as ARTIST:
Come check out the holiday cheerArtist parkside is bursting..
Pls tune in to TV Guide NetworkVenue TONIGHT at 8 pm
for 25 Most Hilarious Holiday TV Moments...
While our experiments utilized binary relations,
we believe our general approach should be useful for
n-ary relation recovery in the social media domain.
Because short messages are unlikely to express high
arity relations completely, tying extraction and clus-
tering seems an intuitive solution. In such a sce-
nario, the record consistency constraints imposed by
our model would have to be relaxed, perhaps exam-
ining pairwise argument consistency instead.
8 Conclusion
We presented a novel model for record extraction
from social media streams such as Twitter. Our
model operates on a noisy feed of data and extracts
canonical records of events by aggregating informa-
tion across multiple messages. Despite the noise
of irrelevant messages and the relatively colloquial
nature of message language, we are able to extract
records with relatively high accuracy. There is still
much room for improvement using a broader array
of features on factors.
9 Acknowledgements
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-0172. Any opin-
ions, findings, and conclusions expressed in this ma-
terial are those of the author(s) and do not necessar-
ily reflect the views of DARPA, AFRL, or the US
government. Thanks also to Tal Wagner for his de-
velopment assistance and the MIT NLP group for
their helpful comments.
397
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of DL.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In Proceedings of the ACL.
J Eisenstein, B O?Connor, and N Smith. . . . 2010. A
latent variable model for geographic lexical variation.
Proceedings of the 2010 . . . , Jan.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL.
Robert W. Irving, Paul Leather, and Dan Gusfield. 1987.
An efficient algorithm for the optimal stable marriage.
J. ACM, 34:532?543, July.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of International Conference of Machine
Learning (ICML), pages 282?289.
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tutorial).
In Association for Computational Linguistics (ACL).
Gideon S. Mann and David Yarowsky. 2005. Multi-field
information extraction and cross-document fusion. In
Proceeding of the ACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009a. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL/IJCNLP.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009b. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the ACL,
pages 1003?1011.
A Ritter, C Cherry, and B Dolan. 2010. Unsupervised
modeling of twitter conversations. Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?180.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of HLT/NAACL.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition of
domain knowledge for information extraction. In Pro-
ceedings of COLING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010a. Collective cross-document relation extraction
without labelled data. In Proceedings of the EMNLP,
pages 1013?1023.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010b. Cross-document relation extraction without la-
belled data. In Proceedings of EMNLP.
Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW.
398
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1109?1116,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Ordering Prenominal Modifiers with a Reranking Approach
Jenny Liu
MIT CSAIL
jyliu@csail.mit.edu
Aria Haghighi
MIT CSAIL
me@aria42.com
Abstract
In this work, we present a novel approach
to the generation task of ordering prenomi-
nal modifiers. We take a maximum entropy
reranking approach to the problem which ad-
mits arbitrary features on a permutation of
modifiers, exploiting hundreds of thousands of
features in total. We compare our error rates to
the state-of-the-art and to a strong Google n-
gram count baseline. We attain a maximum
error reduction of 69.8% and average error re-
duction across all test sets of 59.1% compared
to the state-of-the-art and a maximum error re-
duction of 68.4% and average error reduction
across all test sets of 41.8% compared to our
Google n-gram count baseline.
1 Introduction
Speakers rarely have difficulty correctly ordering
modifiers such as adjectives, adverbs, or gerunds
when describing some noun. The phrase ?beau-
tiful blue Macedonian vase? sounds very natural,
whereas changing the modifier ordering to ?blue
Macedonian beautiful vase? is awkward (see Table
1 for more examples). In this work, we consider
the task of ordering an unordered set of prenomi-
nal modifiers so that they sound fluent to native lan-
guage speakers. This is an important task for natural
language generation systems.
Much linguistic research has investigated the se-
mantic constraints behind prenominal modifier or-
derings. One common line of research suggests
that modifiers can be organized by the underlying
semantic property they describe and that there is
a. the vegetarian French lawyer
b. the French vegetarian lawyer
a. the beautiful small black purse
b. the beautiful black small purse
c. the small beautiful black purse
d. the small black beautiful purse
Table 1: Examples of restrictions on modifier orderings
from Teodorescu (2006). The most natural sounding or-
dering is in bold, followed by other possibilities that may
only be appropriate in certain situations.
an ordering on semantic properties which in turn
restricts modifier orderings. For instance, Sproat
and Shih (1991) contend that the size property pre-
cedes the color property and thus ?small black cat?
sounds more fluent than ?black small cat?. Using
> to denote precedence of semantic groups, some
commonly proposed orderings are: quality > size
> shape > color > provenance (Sproat and Shih,
1991), age > color > participle > provenance >
noun > denominal (Quirk et al, 1974), and value
> dimension > physical property > speed > human
propensity > age > color (Dixon, 1977). However,
correctly classifying modifiers into these groups can
be difficult and may be domain dependent or con-
strained by the context in which the modifier is being
used. In addition, these methods do not specify how
to order modifiers within the same class or modifiers
that do not fit into any of the specified groups.
There have also been a variety of corpus-based,
computational approaches. Mitchell (2009) uses
1109
a class-based approach in which modifiers are
grouped into classes based on which positions they
prefer in the training corpus, with a predefined or-
dering imposed on these classes. Shaw and Hatzi-
vassiloglou (1999) developed three different ap-
proaches to the problem that use counting methods
and clustering algorithms, and Malouf (2000) ex-
pands upon Shaw and Hatzivassiloglou?s work.
This paper describes a computational solution to
the problem that uses relevant features to model the
modifier ordering process. By mapping a set of
features across the training data and using a maxi-
mum entropy reranking model, we can learn optimal
weights for these features and then order each set of
modifiers in the test data according to our features
and the learned weights. This approach has not been
used before to solve the prenominal modifier order-
ing problem, and as we demonstrate, vastly outper-
forms the state-of-the-art, especially for sequences
of longer lengths.
Section 2 of this paper describes previous compu-
tational approaches. In Section 3 we present the de-
tails of our maximum entropy reranking approach.
Section 4 covers the evaluation methods we used,
and Section 5 presents our results. In Section 6 we
compare our approach to previous methods, and in
Section 7 we discuss future work and improvements
that could be made to our system.
2 Related Work
Mitchell (2009) orders sequences of at most 4 mod-
ifiers and defines nine classes that express the broad
positional preferences of modifiers, where position
1 is closest to the noun phrase (NP) head and posi-
tion 4 is farthest from it. Classes 1 through 4 com-
prise those modifiers that prefer only to be in posi-
tions 1 through 4, respectively. Class 5 through 7
modifiers prefer positions 1-2, 2-3, and 3-4, respec-
tively, while class 8 modifiers prefer positions 1-3,
and finally, class 9 modifiers prefer positions 2-4.
Mitchell counts how often each word type appears in
each of these positions in the training corpus. If any
modifier?s probability of taking a certain position is
greater than a uniform distribution would allow, then
it is said to prefer that position. Each word type is
then assigned a class, with a global ordering defined
over the nine classes.
Given a set of modifiers to order, if the entire
set has been seen at training time, Mitchell?s sys-
tem looks up the class of each modifier and then or-
ders the sequence based on the predefined ordering
for the classes. When two modifiers have the same
class, the system picks between the possibilities ran-
domly. If a modifier was not seen at training time
and thus cannot be said to belong to a specific class,
the system favors orderings where modifiers whose
classes are known are as close to their classes? pre-
ferred positions as possible.
Shaw and Hatzivassiloglou (1999) use corpus-
based counting methods as well. For a corpus with
w word types, they define a w ? w matrix where
Count[A,B] indicates how often modifier A pre-
cedes modifier B. Given two modifiers a and b to
order, they compare Count[a, b] and Count[b, a] in
their training data. Assuming a null hypothesis that
the probability of either ordering is 0.5, they use a
binomial distribution to compute the probability of
seeing the ordering < a, b > for Count[a, b] num-
ber of times. If this probability is above a certain
threshold then they say that a precedes b. Shaw and
Hatzivassiloglou also use a transitivity method to fill
out parts of the Count table where bigrams are not
actually seen in the training data but their counts can
be inferred from other entries in the table, and they
use a clustering method to group together modifiers
with similar positional preferences.
These methods have proven to work well, but they
also suffer from sparsity issues in the training data.
Mitchell reports a prediction accuracy of 78.59%
for NPs of all lengths, but the accuracy of her ap-
proach is greatly reduced when two modifiers fall
into the same class, since the system cannot make
an informed decision in those cases. In addition, if a
modifier is not seen in the training data, the system
is unable to assign it a class, which also limits accu-
racy. Shaw and Hatzivassiloglou report a highest ac-
curacy of 94.93% and a lowest accuracy of 65.93%,
but since their methods depend heavily on bigram
counts in the training corpus, they are also limited in
how informed their decisions can be if modifiers in
the test data are not present at training time.
In this next section, we describe our maximum
entropy reranking approach that tries to develop a
more comprehensive model of the modifier ordering
process to avoid the sparsity issues that previous ap-
1110
proaches have faced.
3 Model
We treat the problem of prenominal modifier or-
dering as a reranking problem. Given a set B of
prenominal modifiers and a noun phrase head H
whichB modifies, we define ?(B) to be the set of all
possible permutations, or orderings, of B. We sup-
pose that for a set B there is some x? ? ?(B) which
represents a ?correct? natural-sounding ordering of
the modifiers in B.
At test time, we choose an ordering x ? ?(B) us-
ing a maximum entropy reranking approach (Collins
and Koo, 2005). Our distribution over orderings
x ? ?(B) is given by:
P (x|H,B,W ) =
exp{W T?(B,H, x)}
?
x
???(B) exp{W T?(B,H, x?)}
where ?(B,H, x) is a feature vector over a particu-
lar ordering of B and W is a learned weight vector
over features. We describe the set of features in sec-
tion 3.1, but note that we are free under this formu-
lation to use arbitrary features on the full ordering x
of B as well as the head noun H , which we implic-
itly condition on throughout. Since the size of the
set of prenominal modifiers B is typically less than
six, enumerating ?(B) is not expensive.
At training time, our data consists of sequences of
prenominal orderings and their corresponding nom-
inal heads. We treat each sequence as a training ex-
ample where the labeled ordering x? ? ?(B) is the
one we observe. This allows us to extract any num-
ber of ?labeled? examples from part-of-speech text.
Concretely, at training time, we select W to maxi-
mize:
L(W ) =
?
?
?
(B,H,x?)
P (x?|H,B,W )
?
??
?W?2
2?2
where the first term represents our observed data
likelihood and the second the ?2 regularization,
where ?2 is a fixed hyperparameter; we fix the value
of ?2 to 0.5 throughout. We optimize this objective
using standard L-BFGS optimization techniques.
The key to the success of our approach is us-
ing the flexibility afforded by having arbitrary fea-
tures ?(B,H, x) to capture all the salient elements
of the prenominal ordering data. These features can
be used to create a richer model of the modifier or-
dering process than previous corpus-based counting
approaches. In addition, we can encapsulate previ-
ous approaches in terms of features in our model.
Mitchell?s class-based approach can be expressed as
a binary feature that tells us whether a given permu-
ation satisfies the class ordering constraints in her
model. Previous counting approaches can be ex-
pressed as a real-valued feature that, given all n-
grams generated by a permutation of modifiers, re-
turns the count of all these n-grams in the original
training data.
3.1 Feature Selection
Our features are of the form ?(B,H, x) as expressed
in the model above, and we include both indica-
tor features and real-valued numeric features in our
model. We attempt to capture aspects of the modifier
permutations that may be significant in the ordering
process. For instance, perhaps the majority of words
that end with -ly are adverbs and should usually be
positioned farthest from the head noun, so we can
define an indicator function that captures this feature
as follows:
?(B,H, x) =
?
?
?
1 if the modifier in position i
of ordering x ends in -ly
0 otherwise
We create a feature of this form for every possible
modifier position i from 1 to 4.
Wemight also expect permutations that contain n-
grams previously seen in the training data to be more
natural sounding than other permutations that gener-
ate n-grams that have not been seen before. We can
express this as a real-valued feature:
?(B,H, x) =
?
count in training data of all
n-grams present in x
See Table 2 for a summary of our features. Many
of the features we use are similar to those in Dunlop
et al (2010), which uses a feature-based multiple se-
quence alignment approach to order modifiers.
1111
Numeric Features
n-gram Count If N is the set of all n-grams present in the permutation, returns
the sum of the counts of each element of N in the training data.
A separate feature is created for 2-gms through 5-gms.
Count of Head Noun and Closest Modifier Returns the count of < M,H > in the training data where H is
the head noun and M is the modifier closest to H .
Length of Modifier? Returns the length of modifier in position i
Indicator Features
Hyphenated? Modifier in position i contains a hyphen.
Is Word w? Modifier in position i is word w ? W , where W is the set of all
word types in the training data.
Ends In e? Modifier in position i ends in suffix e ? E, where E = {-al -ble
-ed -er -est -ic -ing -ive -ly -ian}
Is A Color? Modifier in position i is a color, where we use a list of common
colors
Starts With a Number? Modifier in position i starts with a number
Is a Number? Modifier in position i is a number
Satisfies Mitchell Class Ordering The permutation?s class ordering satisfies the Mitchell class or-
dering constraints
Table 2: Features Used In Our Model. Features with an asterisk (*) are created for all possible modifier positions i
from 1 to 4.
4 Experiments
4.1 Data Preprocessing and Selection
We extracted all noun phrases from four corpora: the
Brown, Switchboard, and Wall Street Journal cor-
pora from the Penn Treebank, and the North Amer-
ican Newswire corpus (NANC). Since there were
very few NPs with more than 5 modifiers, we kept
those with 2-5 modifiers and with tags NN or NNS
for the head noun. We also kept NPs with only 1
modifier to be used for generating <modifier, head
noun> bigram counts at training time. We then fil-
tered all these NPs as follows: If the NP contained
a PRP, IN, CD, or DT tag and the corresponding
modifier was farthest away from the head noun, we
removed this modifier and kept the rest of the NP. If
the modifier was not the farthest away from the head
noun, we discarded the NP. If the NP contained a
POS tag we only kept the part of the phrase up to this
tag. Our final set of NPs had tags from the following
list: JJ, NN, NNP, NNS, JJS, JJR, VBG, VBN, RB,
NNPS, RBS. See Table 3 for a summary of the num-
ber of NPs of lengths 1-5 extracted from the four
corpora.
Our system makes several passes over the data
during the training process. In the first pass,
we collect statistics about the data, to be used
later on when calculating our numeric features.
To collect the statistics, we take each NP in
the training data and consider all possible 2-
gms through 5-gms that are present in the NP?s
modifier sequence, allowing for non-consecutive
n-grams. For example, the NP ?the beautiful
blue Macedonian vase? generates the following bi-
grams: <beautiful blue>, <blue Macedonian>,
and <beautiful Macedonian>, along with the 3-
gram <beautiful blue Macedonian>. We keep a
table mapping each unique n-gram to the number
of times it has been seen in the training data. In
addition, we also store a table that keeps track of
bigram counts for < M,H >, where H is the
head noun of an NP and M is the modifier clos-
est to it. In the example ?the beautiful blue Mace-
donian vase,? we would increment the count of <
Macedonian, vase > in the table. The n-gram and
< M,H > counts are used to compute numeric fea-
1112
Number of Sequences (Token)
1 2 3 4 5 Total
Brown 11,265 1,398 92 8 2 12,765
WSJ 36,313 9,073 1,399 229 156 47,170
Switchboard 10,325 1,170 114 4 1 11,614
NANC 15,456,670 3,399,882 543,894 80,447 14,840 19,495,733
Number of Sequences (Type)
1 2 3 4 5 Total
Brown 4,071 1,336 91 8 2 5,508
WSJ 7,177 6,687 1,205 182 42 15,293
Switchboard 2,122 950 113 4 1 3,190
NANC 241,965 876,144 264,503 48,060 8,451 1,439,123
Table 3: Number of NPs extracted from our data for NP sequences with 1 to 5 modifiers.
ture values.
4.2 Google n-gram Baseline
The Google n-gram corpus is a collection of n-gram
counts drawn from public webpages with a total of
one trillion tokens ? around 1 billion each of unique
3-grams, 4-grams, and 5-grams, and around 300,000
unique bigrams. We created a Google n-gram base-
line that takes a set of modifiers B, determines the
Google n-gram count for each possible permutation
in ?(B), and selects the permutation with the high-
est n-gram count as the winning ordering x?. We
will refer to this baseline as GOOGLE N-GRAM.
4.3 Mitchell?s Class-Based Ordering of
Prenominal Modifiers (2009)
Mitchell?s original system was evaluated using only
three corpora for both training and testing data:
Brown, Switchboard, and WSJ. In addition, the
evaluation presented by Mitchell?s work considers a
prediction to be correct if the ordering of classes in
that prediction is the same as the ordering of classes
in the original test data sequence, where a class
refers to the positional preference groupings defined
in the model. We use a more stringent evaluation as
described in the next section.
We implemented our own version of Mitchell?s
system that duplicates the model and methods but
allows us to scale up to a larger training set and to
apply our own evaluation techniques. We will refer
to this baseline as CLASS BASED.
4.4 Evaluation
To evaluate our system (MAXENT) and our base-
lines, we partitioned the corpora into training and
testing data. For each NP in the test data, we gener-
ated a set of modifiers and looked at the predicted
orderings of the MAXENT, CLASS BASED, and
GOOGLE N-GRAM methods. We considered a pre-
dicted sequence ordering to be correct if it matches
the original ordering of the modifiers in the corpus.
We ran four trials, the first holding out the Brown
corpus and using it as the test set, the second hold-
ing out the WSJ corpus, the third holding out the
Switchboard corpus, and the fourth holding out a
randomly selected tenth of the NANC. For each trial
we used the rest of the data as our training set.
5 Results
The MAXENT model consistently outperforms
CLASS BASED across all test corpora and sequence
lengths for both tokens and types, except when test-
ing on the Brown and Switchboard corpora for mod-
ifier sequences of length 5, for which neither ap-
proach is able to make any correct predictions. How-
ever, there are only 3 sequences total of length 5
in the Brown and Swichboard corpora combined.
1113
Test Corpus Token Accuracy (%) Type Accuracy (%)
2 3 4 5 Total 2 3 4 5 Total
Brown GOOGLE N-GRAM 82.4 35.9 12.5 0 79.1 81.8 36.3 12.5 0 78.4
CLASS BASED 79.3 54.3 25.0 0 77.3 78.9 54.9 25.0 0 77.0
MAXENT 89.4 70.7 87.5 0 88.1 89.1 70.3 87.5 0 87.8
WSJ GOOGLE N-GRAM 84.8 53.5 31.4 71.8 79.4 82.6 49.7 23.1 16.7 76.0
CLASS BASED 85.5 51.6 16.6 0.6 78.5 85.1 50.1 19.2 0 78.0
MAXENT 95.9 84.1 71.2 80.1 93.5 94.7 81.9 70.3 45.2 92.0
Switchboard GOOGLE N-GRAM 92.8 68.4 0 0 90.3 91.7 68.1 0 0 88.8
CLASS BASED 80.1 52.6 0 0 77.3 79.1 53.1 0 0 75.9
MAXENT 91.4 74.6 25.0 0 89.6 90.3 75.2 25.0 0 88.4
One Tenth of GOOGLE N-GRAM 86.8 55.8 27.7 43.0 81.1 79.2 44.6 20.5 12.3 70.4
NANC CLASS BASED 86.1 54.7 20.1 1.9 80.0 80.3 51.0 18.4 3.3 74.5
MAXENT 95.2 83.8 71.6 62.2 93.0 91.6 78.8 63.8 44.4 88.0
Test Corpus Number of Features Used In MaxEnt Model
Brown 655,536
WSJ 654,473
Switchboard 655,791
NANC 565,905
Table 4: Token and type prediction accuracies for the GOOGLE N-GRAM, MAXENT, and CLASS BASED approaches
for modifier sequences of lengths 2-5. Our data consisted of four corpuses: Brown, Switchboard, WSJ, and NANC.
The test data was held out and each approach was trained on the rest of the data. Winning scores are in bold. The
number of features used during training for the MAXENT approach for each test corpus is also listed.
MAXENT also outperforms the GOOGLE N-GRAM
baseline for almost all test corpora and sequence
lengths. For the Switchboard test corpus token
and type accuracies, the GOOGLE N-GRAM base-
line is more accurate than MAXENT for sequences
of length 2 and overall, but the accuracy of MAX-
ENT is competitive with that of GOOGLE N-GRAM.
If we examine the error reduction between MAX-
ENT and CLASS BASED, we attain a maximum error
reduction of 69.8% for the WSJ test corpus across
modifier sequence tokens, and an average error re-
duction of 59.1% across all test corpora for tokens.
MAXENT also attains a maximum error reduction of
68.4% for the WSJ test corpus and an average error
reduction of 41.8% when compared to GOOGLE N-
GRAM.
It should also be noted that on average the MAX-
ENT model takes three hours to train with several
hundred thousand features mapped across the train-
ing data (the exact number used during each test run
is listed in Table 4) ? this tradeoff is well worth the
increase we attain in system performance.
6 Analysis
MAXENT seems to outperform the CLASS BASED
baseline because it learns more from the training
data. The CLASS BASED model classifies each
modifier in the training data into one of nine broad
categories, with each category representing a differ-
ent set of positional preferences. However, many of
the modifiers in the training data get classified to the
same category, and CLASS BASED makes a random
choice when faced with orderings of modifiers all in
the same category. When applying CLASS BASED
1114
0 20 40 60 80 1000
10
20
30
40
50
60
70
80
90
100 Sequences of 2 Modifiers
Portion of NANC Used in Training (%)
Corr
ect P
redic
tions
 (%)
 
 
MaxEntClassBased
(a)
0 20 40 60 80 1000
10
20
30
40
50
60
70
80
90
100 Sequences of 3 Modifiers
Portion of NANC Used in Training (%)
Corr
ect P
redic
tions
 (%)
 
 
MaxEntClassBased
(b)
0 20 40 60 80 1000
10
20
30
40
50
60
70
80
90
100 Sequences of 4 Modifiers
Portion of NANC Used in Training (%)
Corr
ect P
redic
tions
 (%)
 
 
MaxEntClassBased
(c)
0 20 40 60 80 1000
10
20
30
40
50
60
70
80
90
100 Sequences of 5 Modifiers
Portion of NANC Used in Training (%)
Corr
ect P
redic
tions
 (%)
 
 
MaxEntClassBased
(d)
0 20 40 60 80 1000
10
20
30
40
50
60
70
80
90
100 All Modifier Sequences
Portion of NANC Used in Training (%)
Corr
ect P
redic
tions
 (%)
 
 
MaxEntClassBased
(e)
0 20 40 60 80 1000
1
2
3
4
5
6
7 x 105 Features Used by MaxEnt Model
Portion of NANC Used in Training (%)
Num
ber o
f Fea
tures
 Use
d
(f)
Figure 1: Learning curves for the MAXENT and CLASS BASED approaches. We start by training each approach on
just the Brown and Switchboard corpora while testing on WSJ. We incrementally add portions of the NANC corpus.
Graphs (a) through (d) break down the total correct predictions by the number of modifiers in a sequence, while graph
(e) gives accuracies over modifier sequences of all lengths. Prediction percentages are for sequence tokens. Graph (f)
shows the number of features active in the MaxEnt model as the training data scales up.
1115
to WSJ as the test data and training on the other cor-
pora, 74.7% of the incorrect predictions contained
at least 2 modifiers that were of the same positional
preferences class. In contrast, MAXENT allows us
to learn much more from the training data. As a re-
sult, we see much higher numbers when trained and
tested on the same data as CLASS BASED.
The GOOGLE N-GRAM method does better than
the CLASS BASED approach because it contains n-
gram counts for more data than the WSJ, Brown,
Switchboard, and NANC corpora combined. How-
ever, GOOGLE N-GRAM suffers from sparsity issues
as well when testing on less common modifier com-
binations. For example, our data contains rarely
heard sequences such as ?Italian, state-owned, hold-
ing company? or ?armed Namibian nationalist guer-
rillas.? While MAXENT determines the correct or-
dering for both of these examples, none of the per-
mutations of either example show up in the Google
n-gram corpus, so the GOOGLE N-GRAM method is
forced to randomly select from the six possibilities.
In addition, the Google n-gram corpus is composed
of sentence fragments that may not necessarily be
NPs, so we may be overcounting certain modifier
permutations that can function as different parts of a
sentence.
We also compared the effect that increasing the
amount of training data has when using the CLASS
BASED and MAXENT methods by initially train-
ing each system with just the Brown and Switch-
board corpora and testing on WSJ. Then we incre-
mentally added portions of NANC, one tenth at a
time, until the training set included all of it. The re-
sults (see Figure 1) show that we are able to benefit
from the additional data much more than the CLASS
BASED approach can, since we do not have a fixed
set of classes limiting the amount of information the
model can learn. In addition, adding the first tenth
of NANC made the biggest difference in increasing
accuracy for both approaches.
7 Conclusion
The straightforward maximum entropy reranking
approach is able to significantly outperform previous
computational approaches by allowing for a richer
model of the prenominal modifier ordering process.
Future work could include adding more features to
the model and conducting ablation testing. In addi-
tion, while many sets of modifiers have stringent or-
dering requirements, some variations on orderings,
such as ?former famous actor? vs. ?famous former
actor,? are acceptable in both forms and have dif-
ferent meanings. It may be beneficial to extend the
model to discover these ambiguities.
Acknowledgements
Many thanks to Margaret Mitchell, Regina Barzilay, Xiao Chen,
and members of the CSAIL NLP group for their help and sug-
gestions.
References
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
R. M. W. Dixon. 1977. Where Have all the Adjectives
Gone? Studies in Language, 1(1):19?80.
A. Dunlop, M. Mitchell, and B. Roark. 2010. Prenomi-
nal modifier ordering via multiple sequence alignment.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 600?
608. Association for Computational Linguistics.
R. Malouf. 2000. The order of prenominal adjectives
in natural language generation. In Proceedings of
the 38th Annual Meeting on Association for Computa-
tional Linguistics, pages 85?92. Association for Com-
putational Linguistics.
M. Mitchell. 2009. Class-based ordering of prenominal
modifiers. In Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 50?57.
Association for Computational Linguistics.
R. Quirk, S. Greenbaum, R.A. Close, and R. Quirk. 1974.
A university grammar of English, volume 1985. Long-
man London.
J. Shaw and V. Hatzivassiloglou. 1999. Ordering among
premodifiers. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 135?143. Asso-
ciation for Computational Linguistics.
R. Sproat and C. Shih. 1991. The cross-linguistic dis-
tribution of adjective ordering restrictions. Interdisci-
plinary approaches to language, pages 565?593.
A. Teodorescu. 2006. Adjective Ordering Restrictions
Revisited. In Proceedings of the 25th West Coast Con-
ference on Formal Linguistics, pages 399?407. West
Coast Conference on Formal Linguistics.
1116
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1?9,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Modeling Syntactic Context Improves Morphological Segmentation
Yoong Keok Lee Aria Haghighi Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{yklee, aria42, regina}@csail.mit.edu
Abstract
The connection between part-of-speech (POS)
categories and morphological properties is
well-documented in linguistics but underuti-
lized in text processing systems. This pa-
per proposes a novel model for morphologi-
cal segmentation that is driven by this connec-
tion. Our model learns that words with com-
mon affixes are likely to be in the same syn-
tactic category and uses learned syntactic cat-
egories to refine the segmentation boundaries
of words. Our results demonstrate that incor-
porating POS categorization yields substantial
performance gains on morphological segmen-
tation of Arabic. 1
1 Introduction
A tight connection between morphology and syntax
is well-documented in linguistic literature. In many
languages, morphology plays a central role in mark-
ing syntactic structure, while syntactic relations
help to reduce morphological ambiguity (Harley and
Phillips, 1994). Therefore, in an unsupervised lin-
guistic setting which is rife with ambiguity, model-
ing this connection can be particularly beneficial.
However, existing unsupervised morphological
analyzers take little advantage of this linguistic
property. In fact, most of them operate at the vo-
cabulary level, completely ignoring sentence con-
text. This design is not surprising: a typical mor-
phological analyzer does not have access to syntac-
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/morphsyn/.
tic information, because morphological segmenta-
tion precedes other forms of sentence analysis.
In this paper, we demonstrate that morphological
analysis can utilize this connection without assum-
ing access to full-fledged syntactic information. In
particular, we focus on two aspects of the morpho-
syntactic connection:
? Morphological consistency within POS cat-
egories. Words within the same syntactic cat-
egory tend to select similar affixes. This lin-
guistic property significantly reduces the space
of possible morphological analyses, ruling out
assignments that are incompatible with a syn-
tactic category.
? Morphological realization of grammatical
agreement. In many morphologically rich lan-
guages, agreement between syntactic depen-
dents is expressed via correlated morphological
markers. For instance, in Semitic languages,
gender and number agreement between nouns
and adjectives is expressed using matching suf-
fixes. Enforcing mutually consistent segmen-
tations can greatly reduce ambiguity of word-
level analysis.
In both cases, we do not assume that the relevant
syntactic information is provided, but instead jointly
induce it as part of morphological analysis.
We capture morpho-syntactic relations in a
Bayesian model that grounds intra-word decisions
in sentence-level context. Like traditional unsuper-
vised models, we generate morphological structure
from a latent lexicon of prefixes, stems, and suffixes.
1
In addition, morphological analysis is guided by a
latent variable that clusters together words with sim-
ilar affixes, acting as a proxy for POS tags. More-
over, a sequence-level component further refines the
analysis by correlating segmentation decisions be-
tween adjacent words that exhibit morphological
agreement. We encourage this behavior by encoding
a transition distribution over adjacent words, using
string match cues as a proxy for grammatical agree-
ment.
We evaluate our model on the standard Arabic
treebank. Our full model yields 86.2% accuracy,
outperforming the best published results (Poon et
al., 2009) by 8.5%. We also found that modeling
morphological agreement between adjacent words
yields greater improvement than modeling syntac-
tic categories. Overall, our results demonstrate that
incorporating syntactic information is a promising
direction for improving morphological analysis.
2 Related Work
Research in unsupervised morphological segmenta-
tion has gained momentum in recent years bring-
ing about significant developments to the area.
These advances include novel Bayesian formula-
tions (Goldwater et al, 2006; Creutz and Lagus,
2007; Johnson, 2008), methods for incorporat-
ing rich features in unsupervised log-linear models
(Poon et al, 2009) and the development of multilin-
gual morphological segmenters (Snyder and Barzi-
lay, 2008a).
Our work most closely relates to approaches that
aim to incorporate syntactic information into mor-
phological analysis. Surprisingly, the research in
this area is relatively sparse, despite multiple results
that demonstrate the connection between morphol-
ogy and syntax in the context of part-of-speech tag-
ging (Toutanova and Johnson, 2008; Habash and
Rambow, 2005; Dasgupta and Ng, 2007; Adler
and Elhadad, 2006). Toutanova and Cherry (2009)
were the first to systematically study how to in-
corporate part-of-speech information into lemmati-
zation and empirically demonstrate the benefits of
this combination. While our high-level goal is simi-
lar, our respective problem formulations are distinct.
Toutanova and Cherry (2009) have considered a
semi-supervised setting where an initial morpholog-
ical dictionary and tagging lexicon are provided but
the model also has access to unlabeled data. Since a
lemmatizer and tagger trained in isolation may pro-
duce mutually inconsistent assignments, and their
method employs a log-linear reranker to reconcile
these decisions. This reranking method is not suit-
able for the unsupervised scenario considered in our
paper.
Our work is most closely related to the approach
of Can and Manandhar (2009). Their method also
incorporates POS-based clustering into morpholog-
ical analysis. These clusters, however, are learned
as a separate preprocessing step using distributional
similarity. For each of the clusters, the model se-
lects a set of affixes, driven by the frequency of their
occurrences in the cluster. In contrast, we model
morpho-syntactic decisions jointly, thereby enabling
tighter integration between the two. This design
also enables us to capture additional linguistic phe-
nomena such as agreement. While this technique
yields performance improvement in the context of
their system, the final results does not exceed state-
of-the-art systems that do not exploit this informa-
tion (for e.g., (Creutz and Lagus, 2007)).
3 Model
Given a corpus of unannotated and unsegmented
sentences, our goal is to infer the segmentation
boundaries of all words. We represent segmen-
tations and syntactic categories as latent variables
with a directed graphical model, and we perform
Bayesian inference to recover the latent variables of
interest. Apart from learning a compact morpheme
lexicon that explains the corpus well, we also model
morpho-syntactic relations both within each word
and between adjacent words to improve segmenta-
tion performance. In the remaining section, we first
provide the key linguistic intuitions on which our
model is based before describing the complete gen-
erative process.
3.1 Linguistic Intuition
While morpho-syntactic interface spans a range of
linguistic phenomena, we focus on two facets of this
connection. Both of them provide powerful con-
straints on morphological analysis and can be mod-
eled without explicit access to syntactic annotations.
2
Morphological consistency within syntactic cate-
gory. Words that belong to the same syntactic cat-
egory tend to select similar affixes. In fact, the power
of affix-related features has been empirically shown
in the task of POS tag prediction (Habash and Ram-
bow, 2005). We hypothesize that this regularity can
also benefit morphological analyzers by eliminat-
ing assignments with incompatible prefixes and suf-
fixes. For instance, a state-of-the-art segmenter er-
roneously divides the word ?Al{ntxAbAt? into four
morphemes ?Al-{ntxAb-A-t? instead of three ?Al-
{ntxAb-At? (translated as ?the-election-s?.) The af-
fix assignment here is clearly incompatible ? de-
terminer ?Al? is commonly associated with nouns,
while suffix ?A? mostly occurs with verbs.
Since POS information is not available to the
model, we introduce a latent variable that encodes
affix-based clustering. In addition, we consider a
variant of the model that captures dependencies be-
tween latent variables of adjacent words (analogous
to POS transitions).
Morphological realization of grammatical agree-
ment. In morphologically rich languages, agree-
ment is commonly realized using matching suffices.
In many cases, members of a dependent pair such
as adjective and noun have the exact same suf-
fix. A common example in the Arabic Treebank
is the bigram ?Al-Df-p Al-grby-p? (which is trans-
lated word-for-word as ?the-bank the-west?) where
the last morpheme ?p? is a feminine singular noun
suffix.
Fully incorporating agreement constraints in the
model is difficult, since we do not have access to
syntactic dependencies. Therefore, we limit our at-
tention to adjacent words which end with similar
strings ? for e.g., ?p? in the example above. The
model encourages consistent segmentation of such
pairs. While our string-based cue is a simple proxy
for agreement relation, it turns to be highly effective
in practice. On the Penn Arabic treebank corpus, our
cue has a precision of around 94% at the token-level.
3.2 Generative Process
The high-level generative process proceeds in four
phases:
(a) Lexicon Model: We begin by generating mor-
pheme lexicons L using parameters ?. This set
of lexicons consists of separate lexicons for pre-
fixes, stems, and suffixes generated in a hierar-
chical fashion.
(b) Segmentation Model: Conditioned on L, we
draw word types, their segmentations, and also
their syntactic categories (W ,S,T ).
(c) Token-POS Model: Next, we generate the un-
segmented tokens in the corpus and their syn-
tactic classes (w, t) from a standard first-order
HMM which has dependencies between adja-
cent syntactic categories.
(d) Token-Seg Model: Lastly, we generate token
segmentations s from a first-order Markov chain
that has dependencies between adjacent seg-
mentations.
The complete generative story can be summarized
by the following equation:
P (w,s, t,W ,S,T ,L,?,?|?,?,?) =
P (L|?) (a)
P (W ,S,T ,?|L,?,?) (b)
Ppos(w, t,?|W ,S,T ,L,?) (c)
Pseg(s|W ,S,T ,L,?,?) (d)
where ?,?,?,?,? are hyperparameters and pa-
rameters whose roles we shall detail shortly.
Our lexicon model captures the desirability of
compact lexicon representation proposed by prior
work by using parameters ? that favors small lexi-
cons. Furthermore, if we set the number of syntac-
tic categories in the segmentation model to one and
exclude the token-based models, we recover a seg-
menter that is very similar to the unigram Dirichlet
Process model (Goldwater et al, 2006; Snyder and
Barzilay, 2008a; Snyder and Barzilay, 2008b). We
shall elaborate on this point in Section 4.
The segmentation model captures morphological
consistency within syntactic categories (POS tag),
whereas the Token-POS model captures POS tag
dependencies between adjacent tokens. Lastly, the
Token-Seg model encourages consistent segmenta-
tions between adjacent tokens that exhibit morpho-
logical agreement.
3
Lexicon Model The design goal is to encourage
morpheme types to be short and the set of affixes
(i.e. prefixes and suffixes) to be much smaller than
the set of stems. To achieve this, we first draw each
morpheme ? in the master lexicon L? according to a
geometric distribution which assigns monotonically
smaller probability to longer morpheme lengths:
|?| ? Geometric(?l)
The parameter ?l for the geometric distribution is
fixed and specified beforehand. We then draw the
prefix, the stem, and suffix lexicons (denoted by
L?, L0, L+ respectively) from morphemes in L?.
Generating the lexicons in such a hierarchical fash-
ion allows morphemes to be shared among the
lower-level lexicons. For instance, once determiner
?Al? is generated in the master lexicon, it can be
used to generate prefixes or stems later on. To fa-
vor compact lexicons, we again make use of a ge-
ometric distribution that assigns smaller probability
to lexicons that contain more morphemes:
prefix: |L?| ? Geometric(??)
stem: |L0| ? Geometric(?0)
suffix: |L+| ? Geometric(?+)
By separating morphemes into affixes and stems, we
can control the relative sizes of their lexicons with
different parameters.
Segmentation Model The model independently
generates each word type using only morphemes in
the affix and stem lexicons, such that each word
has exactly one stem and is encouraged to have few
morphemes. We fix the number of syntactic cate-
gories (tags) to K and begin the process by generat-
ing multinomial distribution parameters for the POS
tag prior from a Dirichlet prior:
?T ? Dirichlet(?T , {1, . . . ,K})
Next, for each possible value of the tag T ?
{1, . . . ,K}, we generate parameters for a multino-
mial distribution (again from a Dirichlet prior) for
each of the prefix and the suffix lexicons:
??|T ? Dirichlet(??, L?)
?0 ? Dirichlet(?0, L0)
?+|T ? Dirichlet(?+, L+)
By generating parameters in this manner, we allow
the multinomial distributions to generate only mor-
phemes that are present in the lexicon. Also, at infer-
ence time, only morphemes in the lexicons receive
pseudo-counts. Note that the affixes are generated
conditioned on the tag; But the stem are not.2
Now, we are ready to generate each word type
W , its segmentation S, and its syntactic category T .
First, we draw the number of morpheme segments
|S| from a geometric distribution truncated to gener-
ate at most five morphemes:
|S| ? Truncated-Geometric(?|S|)
Next, we pick one of the morphemes to be the stem
uniformly at random, and thus determine the number
of prefixes and suffixes. Then, we draw the syntactic
category T for the word. (Note that T is a latent
variable which we recover during inference.)
T ? Multinomial(?T )
After that, we generate each stem ?0, prefix ??, and
suffix ?+ independently:
?0 ? Multinomial(?0)
??|T ? Multinomial(??|T )
?+|T ? Multinomial(?+|T )
Token-POS Model This model captures the de-
pendencies between the syntactic categories of ad-
jacent tokens with a first-order HMM. Conditioned
on the type-level assignments, we generate (unseg-
mented) tokens w and their POS tags t:
Ppos(w, t|W ,T ,?)
=
?
wi,ti
P (ti?1|ti, ?t|t)P (wi|ti, ?w|t)
where the parameters of the multinomial distribu-
tions are generated by Dirichlet priors:
?t|t ? Dirichlet(?t|t, {1, . . . ,K})
?w|t ? Dirichlet(?w|t,W t)
2We design the model as such since the dependencies be-
tween affixes and the POS tag are much stronger than those be-
tween the stems and tags. In our preliminary experiments, when
stems are also generated conditioned on the tag, spurious stems
are easily created and associated with garbage-collecting tags.
4
Here, W t refers to the set of word types that are
generated by tag t. In other words, conditioned on
tag t, we can only generate word w from the set of
word types inW t which is generated earlier (Lee et
al., 2010).
Token-Seg Model The model captures the mor-
phological agreement between adjacent segmenta-
tions using a first-order Markov chain. The proba-
bility of drawing a sequence of segmentations s is
given by
Pseg(s|W ,S,T ,L,?,?) =
?
(si?1,si)
p(si|si?1)
For each pair of segmentations si?1 and si, we de-
termine: (1) if they should exhibit morpho-syntactic
agreement, and (2) if their morphological segmenta-
tions are consistent. To answer the first question, we
first obtain the final suffix for each of them. Next,
we obtain n, the length of the longer suffix. For
each segmentation, we define the ending to be the
last n characters of the word. We then use matching
endings as a proxy for morpho-syntactic agreement
between the two words. To answer the second ques-
tion, we use matching final suffixes as a cue for con-
sistent morphological segmentations. To encode the
linguistic intuition that words that exhibit morpho-
syntactic agreement are likely to be morphological
consistent, we define the above probability distribu-
tion to be:
p(si|si?1)
=
?
?
?
?1 if same endings and same final suffix
?2 if same endings but different final suffixes
?3 otherwise (e.g. no suffix)
where ?1 + ?2 + ?3 = 1, with ?1 > ?3 > ?2. By
setting ?1 to a high value, we encourage adjacent
tokens that are likely to exhibit morpho-syntactic
agreement to have the same final suffix. And by set-
ting ?3 > ?2, we also discourage adjacent tokens
with the same endings to be segmented differently. 3
4 Inference
Given a corpus of unsegmented and unannotated
word tokens w, the objective is to recover values of
3Although p sums to one, it makes the model deficient since,
conditioned everything already generated, it places some prob-
ability mass on invalid segmentation sequences.
all latent variables, including the segmentations s.
P (s, t,S,T ,L|w,W ,?,?,?)
?
?
P (w, s, t,W ,S,T ,L,?,?|?,?,?)d?d?
We want to sample from the above distribution us-
ing collapsed Gibbs sampling (? and ? integrated
out.) In each iteration, we loop over each word type
Wi and sample the following latent variables: its tag
Ti, its segmentation Si, the segmentations and tags
for all of its token occurrences (si, ti), and also the
morpheme lexicons L:
P (L, Ti, Si, si, ti|
s?i, t?i,S?i,T?i,w?i,W?i,?,?,?) (1)
such that the type and token-level assignments are
consistent, i.e. for all t ? ti we have t = Ti, and for
all s ? si we have s = Si.
4.1 Approximate Inference
Naively sampling the lexicons L is computationally
infeasible since their sizes are unbounded. There-
fore, we employ an approximation which turns is
similar to performing inference with a Dirichlet Pro-
cess segmentation model. In our approximation
scheme, for each possible segmentation and tag hy-
pothesis (Ti, Si, si, ti), we only consider one possi-
ble value for L, which we denote the minimal lexi-
cons. Hence, the total number of hypothesis that we
have to consider is only as large as the number of
possibilities for (Ti, Si, si, ti).
Specifically, we recover the minimal lexicons as
follows: for each segmentation and tag hypothesis,
we determine the set of distinct affix and stem types
in the whole corpus, including the morphemes intro-
duced by segmentation hypothesis under considera-
tion. This set of lexicons, which we call the minimal
lexicons, is the most compact ones that are needed
to generate all morphemes proposed by the current
hypothesis.
Furthermore, we set the number of possible POS
tags K = 5. 4 For each possible value of the tag,
we consider all possible segmentations with at most
five segments. We further restrict the stem to have no
4We find that increasing K to 10 does not yield improve-
ment.
5
more than two prefixes or suffixes and also that the
stem cannot be shorter than the affixes. This further
restricts the space of segmentation and tag hypothe-
ses, and hence makes the inference tractable.
4.2 Sampling equations
Suppose we are considering the hypothesis with seg-
mentation S and POS tag T for word type Wi. Let
L = (L?, L?, L0, L+) be the minimal lexicons for
this hypothesis (S, T ). We sample the hypothesis
(S, T, s = S, t = T,L) proportional to the product
of the following four equations.
Lexicon Model
?
??L?
?l(1? ?l)
|?| ?
??(1? ??)
|L?| ?
?0(1? ?0)
|L0| ?
?+(1? ?+)
|L+| (2)
This is a product of geometric distributions involv-
ing the length of each morpheme ? and the size
of each of the prefix, the stem, and the suffix lexi-
cons (denoted as |L?|, |L0|, |L+| respectively.) Sup-
pose, a new morpheme type ?0 is introduced as a
stem. Relative to a hypothesis that introduces none,
this one incurs an additional cost of (1 ? ?0) and
?l(1 ? ?l)|?0|. In other words, the hypothesis is pe-
nalized for increasing the stem lexicon size and gen-
erating a new morpheme of length |?0|. In this way,
the first and second terms play a role similar to the
concentration parameter and base distribution in a
DP-based model.
Segmentation Model
?|S|(1? ?|S|)
|S|
?5
j=0 ?|S|(1? ?|S|)
j
?
n?iT + ?
N?i + ?K
?
n?i?0 + ?0
N?i0 + ?0|L0|
?
n?i??|T + ??
N?i?|T + ??|L?|
?
n?i?+|T + ?+
N?i+|T + ?+|L+|
(3)
The first factor is the truncated geometric distribu-
tion of the number of segmentations |S|, and the
second factor is the probability of generate the tag
T . The rest are the probabilities of generating the
stem ?0, the prefix ??, and the suffix ?+ (where the
parameters of the multinomial distribution collapsed
out). n?1T is the number of word types with tag T
and N?i is the total number of word types. n?i??|T
refers to the number of times prefix ?? is seen in all
word types that are tagged with T , and N?i?|T is the
total number of prefixes in all word types that has tag
T . All counts exclude the word type Wi whose seg-
mentation we are sampling. If there is another pre-
fix, N?i?|T is incremented (and also n
?i
??|T
if the sec-
ond prefix is the same as the first one.) Integrating
out the parameters introduces dependencies between
prefixes. The rest of the notations read analogously.
Token-POS Model
?w|t
(mi)
(M?it + ?w|t|W t|)(m
i)
?
K?
t=1
K?
t?=1
(m?it?|t + ?t|t)
(mi
t?|t
)
(M?it + ?t|t)
(mi
t?|t
)
(4)
The two terms are the token-level emission and tran-
sition probabilities with parameters integrated out.
The integration induces dependences between all
token occurrences of word type W which results
in ascending factorials defined as ?(m) = ?(? +
1) ? ? ? (? + m ? 1) (Liang et al, 2010). M?it is
the number of tokens that have POS tag t, mi is the
number of tokens wi, and m
?i
t?|t is the number of to-
kens t-to-t? transitions. (Both exclude counts con-
tributed by tokens belong to word type Wi.) |W t| is
the number of word types with tag t.
Token-Seg Model
?
mi?1
1 ?
mi?2
2 ?
mi?3
3 (5)
Here,mi?1 refers to the number of transitions involv-
ing token occurrences of word type Wi that exhibit
morphological agreement. This does not result in
ascending factorials since the parameters of transi-
tion probabilities are fixed and not generated from
Dirichlet priors, and so are not integrated out.
6
4.3 Staged Training
Although the Gibbs sampler mixes regardless of the
initial state in theory, good initialization heuristics
often speed up convergence in practice. We there-
fore train a series of models of increasing complex-
ity (see section 6 for more details), each with 50 iter-
ations of Gibbs sampling, and use the output of the
preceding model to initialize the subsequent model.
The initial model is initialized such that all words are
not segmented. When POS tags are first introduced,
they are initialized uniformly at random.
5 Experimental Setup
Performance metrics To enable comparison with
previous approaches, we adopt the evaluation set-up
of Poon et al (2009). They evaluate segmentation
accuracy on a per token basis, using recall, precision
and F1-score computed on segmentation points. We
also follow a transductive testing scenario where the
same (unlabeled) data is used for both training and
testing the model.
Data set We evaluate segmentation performance
on the Penn Arabic Treebank (ATB).5 It consists of
about 4,500 sentences of modern Arabic obtained
from newswire articles. Following the preprocessing
procedures of Poon et al (2009) that exclude certain
word types (such as abbreviations and digits), we
obtain a corpus of 120,000 tokens and 20,000 word
types. Since our full model operates over sentences,
we train the model on the entire ATB, but evaluate
on the exact portion used by Poon et al (2009).
Pre-defined tunable parameters and testing
regime In all our experiments, we set ?l = 12 (for
length of morpheme types) and ?|S| =
1
2 (for num-
ber of morpheme segments of each word.) To en-
courage a small set of affix types relative to stem
types, we set ?? = ?+ = 11.1 (for sizes of the af-
fix lexicons) and ?0 = 110,000 (for size of the stem
lexicon.) We employ a sparse Dirichlet prior for the
type-level models (for morphemes and POS tag) by
setting ? = 0.1. For the token-level models, we set
hyperparameters for Dirichlet priors ?w|t = 10
?5
5Our evaluation does not include the Hebrew and Arabic
Bible datasets (Snyder and Barzilay, 2008a; Poon et al, 2009)
since these corpora consists of short phrases that omit sentence
context.
Model R P F1 t-test
PCT 09 69.2 88.5 77.7 -
Morfessor 72.6 77.4 74.9 -
BASIC 71.4 86.7 78.3 (2.9) -
+POS 75.4 87.4 81.0 (1.5) +
+TOKEN-POS 75.7 88.5 81.6 (0.7) ?
+TOKEN-SEG 82.1 90.8 86.2 (0.4) ++
Table 1: Results on the Arabic Treebank (ATB) data
set: We compare our models against Poon et al (2009)
(PCT09) and the Morfessor system (Morfessor-CAT).
For our full model (+TOKEN-SEG) and its simplifica-
tions (BASIC, +POS, +TOKEN-POS), we perform five
random restarts and show the mean scores. The sample
standard deviations are shown in brackets. The last col-
umn shows results of a paired t-test against the preceding
model: ++ (significant at 1%), + (significant at 5%), ?
(not significant), - (test not applicable).
(for unsegmented tokens) and ?t|t = 1.0 (for POS
tags transition.) To encourage adjacent words that
exhibit morphological agreement to have the same
final suffix, we set ?1 = 0.6, ?2 = 0.1, ?1 = 0.3.
In all the experiments, we perform five runs us-
ing different random seeds and report the mean score
and the standard deviation.
Baselines Our primary comparison is against the
morphological segmenter of Poon et al (2009)
which yields the best published results on the ATB
corpus. In addition, we compare against the Mor-
fessor Categories-MAP system (Creutz and Lagus,
2007). Similar to our model, their system uses latent
variables to induce clustering over morphemes. The
difference is in the nature of the clustering: the Mor-
fessor algorithm associates a latent variable for each
morpheme, grouping morphemes into four broad
categories (prefix, stem, suffix, and non-morpheme)
but not introducing dependencies between affixes di-
rectly. For both systems, we quote their performance
reported by Poon et al (2009).
6 Results
Comparison with the baselines Table 1 shows that
our full model (denoted +TOKEN-SEG) yields a
mean F1-score of 86.2, compared to 77.7 and 74.9
obtained by the baselines. This performance gap
corresponds to an error reduction of 38.1% over the
best published results.
7
Ablation Analysis To assess relative impact of
various components, we consider several simplified
variants of the model:
? BASIC is the type-based segmentation model
that is solely driven by the lexicon.6
? +POS adds latent variables but does not cap-
ture transitions and agreement constraints.
? +TOKEN-POS is equivalent to the full model,
without agreement constraints.
Our results in Table 1 clearly demonstrate that
modeling morpho-syntactic constraints greatly im-
proves the accuracy of morphological segmentation.
We further examine the performance gains arising
from improvements due to (1) encouraging morpho-
logical consistency within syntactic categories, and
(2) morphological realization of grammatical agree-
ment.
We evaluate our models on a subset of words that
exhibit morphological consistency. Table 2 shows
the accuracies for words that begin with the prefix
?Al? (determiner) and end with a suffix ?At? (plural
noun suffix.) An example is the word ?Al-{ntxAb-
At? which is translated as ?the-election-s?. Such
words make up about 1% of tokens used for eval-
uation, and the two affix boundaries constitute about
3% of the all gold segmentation points. By intro-
ducing a latent variable to capture dependencies be-
tween affixes, +POS is able to improve segmenta-
tion performance over BASIC. When dependencies
between latent variables are introduced, +TOKEN-
POS yields additional improvements.
We also examine the performance gains due to
morphological realization of grammatical agree-
ment. We select the set of tokens that share the
same final suffix as the preceding token, such as
the bigram ?Al-Df-p Al-grby-p? (which is translated
word-for-word as ?the-bank the-west?) where the
last morpheme ?p? is a feminine singular noun suf-
fix. This subset makes up about 4% of the evaluation
set, and the boundaries of the final suffixes take up
about 5% of the total gold segmentation boundaries.
6The resulting model is similar in spirit to the unigram DP-
based segmenter (Goldwater et al, 2006; Snyder and Barzilay,
2008a; Snyder and Barzilay, 2008b).
Model
Token Type
F1 Acc. F1 Acc.
BASIC 68.3 13.9 73.8 24.3
+POS 75.4 26.4 78.5 38.0
+TOKEN-POS 76.5 34.9 82.0 49.6
+TOKEN-SEG 84.0 49.5 85.4 57.7
Table 2: Segmentation performance on words that begin
with prefix ?Al? (determiner) and end with suffix ?At?
(plural noun suffix). The mean F1 scores are computed
using all boundaries of words in this set. For each word,
we also determine if both affixes are recovered while ig-
noring any other boundaries between them. The other
two columns report this accuracy at both the type-level
and the token-level.
Model
Token Type
F1 Acc. F1 Acc.
BASIC 85.6 70.6 79.5 58.6
+POS 87.6 76.4 82.3 66.3
+TOKEN-POS 87.5 75.2 82.2 65.3
+TOKEN-SEG 92.8 91.1 88.9 84.4
Table 3: Segmentation performance on words that have
the same final suffix as their preceding words. The F1
scores are computed based on all boundaries within the
words, but the accuracies are obtained using only the final
suffixes.
Table 3 reveals this category of errors persisted un-
til the final component (+TOKEN-SEG) was intro-
duced.
7 Conclusion
Although the connection between syntactic (POS)
categories and morphological structure is well-
known, this relation is rarely exploited to improve
morphological segmentation performance. The per-
formance gains motivate further investigation into
morpho-syntactic models for unsupervised language
analysis.
Acknowledgements
This material is based upon work supported by
the U.S. Army Research Laboratory and the
U.S. Army Research Office under contract/grant
number W911NF-10-1-0533. Thanks to the MIT
NLP group and the reviewers for their comments.
8
References
Meni Adler and Michael Elhadad. 2006. An un-
supervised morpheme-based hmm for hebrew mor-
phological disambiguation. In Proceedings of the
ACL/CONLL, pages 665?672.
Burcu. Can and Suresh Manandhar. 2009. Unsupervised
learning of morphology by using syntactic categories.
In Working Notes, CLEF 2009 Workshop.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the EMNLP-CoNLL,
pages 218?227.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the ACL, pages
673?680.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Heidi Harley and Colin Phillips, editors. 1994. The
Morphology-Syntax Connection. Number 22 in MIT
Working Papers in Linguistics. MIT Press.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group
on Computational Morphology and Phonology, pages
20?27, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 853?
861, Cambridge, MA, October. Association for Com-
putational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based mcmc. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of HLT-NAACL
2009, pages 209?217, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Crosslin-
gual propagation for morphological analysis. In Pro-
ceedings of the AAAI, pages 848?854.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of ACL-08: HLT, pages 737?
745, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494, Suntec, Singapore,
August. Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 1521?1528. MIT Press,
Cambridge, MA.
9
