Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 765?774,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Integrating sentence- and word-level error identification
for disfluency correction
Erin Fitzgerald
Johns Hopkins University
Baltimore, MD, USA
erinf@jhu.edu
Frederick Jelinek
Johns Hopkins University
Baltimore, MD, USA
jelinek@jhu.edu
Keith Hall
Google Inc.
Z?urich, Switzerland
kbhall@google.com
Abstract
While speaking spontaneously, speakers
often make errors such as self-correction
or false starts which interfere with the
successful application of natural language
processing techniques like summarization
and machine translation to this data. There
is active work on reconstructing this error-
ful data into a clean and fluent transcript
by identifying and removing these simple
errors.
Previous research has approximated the
potential benefit of conducting word-level
reconstruction of simple errors only on
those sentences known to have errors. In
this work, we explore new approaches
for automatically identifying speaker con-
struction errors on the utterance level, and
quantify the impact that this initial step has
on word- and sentence-level reconstruc-
tion accuracy.
1 Introduction
A system would accomplish reconstruction of its
spontaneous speech input if its output were to rep-
resent, in flawless, fluent, and content-preserving
text, the message that the speaker intended to con-
vey. While full speech reconstruction would likely
require a range of string transformations and po-
tentially deep syntactic and semantic analysis of
the errorful text (Fitzgerald, 2009), in this work we
will attempt only to resolve less complex errors,
correctable by deletion alone, in a given manually-
transcribed utterance.
The benefit of conducting word-level recon-
struction of simple errors only on those sen-
tences known to have errors was approximated in
(Fitzgerald et al, 2009). In the current work, we
explore approaches for automatically identifying
speaker-generated errors on the utterance level,
and calculate the gain in accuracy that this initial
step has on word- and sentence-level accuracy.
1.1 Error classes in spontaneous speech
Common simple disfluencies in sentence-like ut-
terances (SUs) include filler words (i.e., ?um?, ?ah?,
and discourse markers like ?you know?), as well as
speaker edits consisting of a reparandum, an inter-
ruption point (IP), an optional interregnum (like ?I
mean?), and a repair region (Shriberg, 1994), as
seen in Figure 1.
[that
?
s]
? ?? ?
reparandum
IP
????
+ {uh}
????
interregnum
that
?
s
? ?? ?
repair
a relief
Figure 1: Typical edit region structure.
These reparanda, or edit regions, can be classified
into three main groups:
1. In a repetition (above), the repair phrase is
approximately identical to the reparandum.
2. In a revision, the repair phrase alters reparan-
dum words to correct the previously stated
thought.
EX1: but [when he] + {i mean} when she put it
that way
EX2: it helps people [that are going to quit] + that
would be quitting anyway
3. In a restart fragment an utterance is aborted
and then restarted with a new train of thought.
EX3: and [i think he?s] + he tells me he?s glad he
has one of those
EX4: [amazon was incorporated by] {uh} well i
only knew two people there
In simple cleanup (a precursor to full speech re-
construction), all detected filler words are deleted,
and the reparanda and interregna are deleted while
the repair region is left intact. This is a strong ini-
tial step for speech reconstruction, though more
765
1 he that ?s uh that ?s a relief
2 E E E FL - - - -
3 NC RC RC FL - - - -
Figure 2: Example of word class and refined word
class labels, where - denotes a non-error, FL de-
notes a filler, E generally denotes reparanda, and
RC and NC indicate rough copy and non-copy
speaker errors, respectively. Line 3 refines the la-
bels of Line 2.
complex and less deterministic changes may be
required for generating fluent and grammatical
speech text in all cases.
1.2 Related Work
Stochastic approaches for simple disfluency de-
tection use features such as lexical form, acous-
tic cues, and rule-based knowledge. State-of-
the-art methods for edit region detection such as
(Johnson and Charniak, 2004; Zhang and Weng,
2005; Kahn et al, 2005; Honal and Schultz, 2005)
model speech disfluencies as a noisy channel
model, though direct classification models have
also shown promise (Fitzgerald et al, 2009; Liu
et al, 2004). The final output is a word-level tag-
ging of the error condition of each word in the se-
quence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach,
referred to in this document as JC04, combines
the noisy channel paradigm with a tree-adjoining
grammar (TAG) to capture approximately re-
peated elements. The TAG approach models the
crossed word dependencies observed when the
reparandum incorporates the same or very simi-
lar words in roughly the same word order, which
JC04 refer to as a rough copy. Line 3 of Figure
2 refines ?edits? (E) into rough copies (RC) and
non-copies (NC).
As expected given the assumptions of the
TAG approach, JC04 identifies repetitions and
most revisions in spontaneous data, but is less
successful in labeling false starts and other
speaker self-interruptions without cross-serial cor-
relations. These non-copy errors hurt the edit de-
tection recall and overall accuracy.
Fitzgerald et al (2009) (referred here as FHJ)
used conditional random fields (CRFs) and the
Spontaneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008) corpus for word-
level error identification, especially targeting im-
provement of these non-copy errors. The CRF was
trained using features based on lexical, language
model, and syntactic observations along with fea-
tures based on JC04 system output.
Alternate experimental setup showed that train-
ing and testing only on SUs known from the la-
beled corpus to contain word-level errors yielded
a notable improvement in accuracy, indicating that
the described system was falsely identifying many
non-error words as errors.
Improved sentence-level identification of error-
ful utterances was shown to help improve word-
level error identification and overall reconstruction
accuracy. This paper describes attempts to extend
these efforts.
2 Approach
2.1 Data
We conducted our experiments on the recently re-
leased Spontaneous Speech Reconstruction (SSR)
corpus (Fitzgerald and Jelinek, 2008), a medium-
sized set of disfluency annotations atop Fisher
conversational telephone speech data (Cieri et al,
2004)
1
. Advantages of the SSR data include
? aligned parallel original and cleaned sen-
tences
? several levels of error annotations, allowing
for a coarse-to-fine reconstruction approach
? multiple annotations per sentence reflecting
the occasional ambiguity of corrections
As reconstructions are sometimes non-
deterministic, the SSR provides two manual
reconstructions for each utterance in the data. We
use these dual annotations to learn complemen-
tary approaches in training and to allow for more
accurate evaluation.
The Spontaneous Speech Reconstruction cor-
pus is partitioned into three subcorpora: 17,162
training sentences (119,693 words), 2,191 sen-
tences (14,861 words) in the development set, and
2,288 sentences (15,382 words) in the test set. Ap-
proximately 17% of the total utterances contain a
reparandum-type error. In constructing the data,
two approaches were combined to filter out the
utterances considered most likely to be errorful
(6,384 in total) and only those SUs were manually
reconstructed. However the entire data set was in-
cluded in the distribution ? and used in training for
this work ? to maintain data balance.
1
The Spontaneous Speech Reconstruction corpus can be
downloaded from http://www.clsp.jhu.edu/PIRE/ssr.
766
The training of the TAG model for JC04, used
as a feature in this work, requires a very specific
data format, and thus is trained not with SSR but
with Switchboard (SWBD) data (Godfrey et al,
1992). Key differences in these corpora, besides
the granularity and form of their annotations, in-
clude:
? SSR aims to correct speech output, while
SWBD edit annotation aims to identify
reparandum structures specifically. SSR only
marks those reparanda which annotators be-
lieve must be deleted to generate a grammat-
ical and content-preserving reconstruction.
? SSR includes more complex error identifi-
cation and correction, not considered in this
work.
While the SWBD corpus has been used in
some previous simple disfluency labeling work
(e.g., Johnson and Charniak, 2004; Kahn et al,
2005), we consider the SSR for its fine-grained er-
ror annotations.
3 Identifying poor constructions
Prior to reconstruction, it is to our advantage to au-
tomatically identify poorly constructed sentences,
defined as being ungrammatical, incomplete, or
missing necessary sentence boundaries. Accu-
rately extracting ill-formed sentences prior to sub-
sentential error correction helps to minimize the
risk of information loss posed by unnecessarily
and incorrectly reconstructing well-formed text.
To evaluate the efforts described below, we
manually label each SU s in the SSR test set S
(including those not originally annotated with re-
constructions but still included in the SSR distri-
bution) as well-formed or poorly-formed, form-
ing the set of poorly constructed SUs P ? S,
|P | = 531 and |S| = 2288 utterances.
To identify speaker errors on the sentence level,
we consider and combine a collection of features
into a single framework using a maximum entropy
model (implemented with the Daum?e III (2004)
MEGA Model toolkit).
3.1 SU-level error features
Six feature types are presented in this section.
? Features #1 and #2 are the two methods in-
cluded in a similar though less exhaustive ef-
fort by (Fitzgerald and Jelinek, 2008) in error
filtering for the creation of the SSR corpus it-
self.
? Feature types #3 and #4 extract features from
automatic parses assigned to the given sen-
tence. It is expected that these parses will
contain some errors and the usefulness of
these features may be parser-specific. The
value of these features though is the con-
sistent, if not always accurate, treatment of
similar construction errores given a particu-
lar state-of-the-art parser.
? Feature type #5 investigates the relationship
between the probability of a SU-internal error
and the number of words it contains.
? Feature type #6 serves to bias the probabil-
ity against assigning a backchannel acknowl-
edgement SU as an error instance.
Feature #1 (JC04): Consider only sentences with
JC04 detected edit regions. This approach takes
advantage of the high precision, low recall JC04
disfluency detection approach described in Section
1.2. We apply the out-of-box JC04 system and
consider any sentence with one or more labeled
reparanda as a ?poor? indicator. Since speakers re-
pairing their speech once are often under a higher
cognitive load and thus more likely to make more
serious speech errors (in other words, there is a
higher probability of making an error given that an
error has already been made (Bard et al, 2001)).
This is a reasonable first order approach for find-
ing deeper problems.
Feature #2 (HPSG): Use deep linguistic parsers
to confirm well-formedness. Statistical context-
free parsers are highly robust and, due to smooth-
ing, can assign a non-zero probability syntac-
tic structure even for text and part-of-speech se-
quences never seen during training. However,
sometimes no output is preferable to highly er-
rorful output. Hand-built rule-based parsers can
produce extremely accurate and context-sensitive
syntactic structures, but are also brittle and do not
adapt well to never before seen input. We use this
inflexibility to our advantage.
Head-driven Phrase Structure Grammar
(HPSG) is a deep-syntax phrase structure gram-
mar which produces rich, non-context-free
syntactic analyses of input sentences based on
a collection of carefully constructed rules and
lexical item structures (Pollard and Sag, 1994;
Wahlster, 2000). Each utterance is parsed using
767
the PET deep parser produced by the inter-
institutional DELPH-IN group
2
. The manually
compiled English Resource Grammar (ERG)
(Flickinger, 2002) rules have previously been
extended for the Verbmobil (Wahlster, 2000)
project to allow for the parsing of basic conversa-
tional elements such as SUs with no verb or basic
backchannel acknowledgements like ?last thursday?
or ?sure?, but still produce strict HPSG parses
based on these rules. We use the binary result of
whether or not each SU is parsable by the HPSG
ERG as binary indicator functions in our models.
There has been some work on producing partial
parses for utterances for which a full HPSG analy-
sis is not deemed possible by the grammar (Zhang
et al, 2007). This work has shown early promise
for identifying coherent substrings within error-
ful SUs given subjective analysis; as this technol-
ogy progresses, HPSG may offer informative sub-
sentential features for word-level error analysis as
well.
Feature #3 (Rules): Mark unseen phrase rule ex-
pansions. Phrase-based parses are composed of
a recursive sequence of non-terminal (NT) rule ex-
pansions, such as those detailed for the example
parse shown in Figure 3. These rules are learned
from training data such as the Switchboard tree-
bank, where telephone conversation transcripts
were manually parsed. In many statistical parsers,
new structures are generated based on the relative
frequencies of such rules in the training treebank,
conditioned on the terminal words and some local
context, and the most probable parse (roughly the
joint probability of its rule expansions) is selected.
Because parsers are often required to produce
output for words and contexts never seen in the
training corpus, smoothing is required. The
Charniak (1999) parser accomplishes this in part
through a Markov grammar which works top-
down, expanding rules to the left and right of an
expansion head M of a given rule. The non-
terminal (NT) M is first predicted from the parent
P , then ? in order ?L
1
throughL
m
(stopping sym-
bol ?#?) and R
1
through R
n
(again ?#?), as shown
in Equation 1.
parent P ? #L
m
. . . L
1
MR
1
. . . R
n
# (1)
In this manner, it is possible to produce rules
never before seen in the training treebank. While
2
The DEep Linguistic Processing with HPSG INitiative
(see http://www.delph-in.net/)
this may be required for parsing grammatical sen-
tences with rare elements, this SU-level error pre-
diction feature indicates whether the automatic
parse for a given SU includes an expansion never
seen in the training treebank. If an expansion rule
in the one-best parse was not seen in training (here
meaning in the SWBD treebank after EDITED
nodes have been removed), the implication is that
new rule generation is an indicator of a speaker
error within a SU.
Feature #4 (C-comm): Mark unseen rule c-
commanding NTs. In X? theory (Chomsky,
1970), lexical categories such as nouns and verbs
are often modified by a specifier (such as the DT ?a?
modifying the NN ?lot? in the NP
3
phrase in Figure
3 or an auxiliary verb for a verb in a verb phrase
(VBZ for VP
3
) and a complement (such as the ob-
ject of a verb NP
3
for VBG in the phrase VP
3
).
In each of these cases, an NT tree node A has
the following relationship with a second NT P :
? Neither does node A dominate P nor node P
dominateA, (i.e., neither is directly above the
other in the parse tree), and
? Node A immediately precedes P in the tree
(precedence is represented graphically in left-
to-right order in the tree).
Given these relationships, we say that A locally
c-commands P and its descendants. We further
extend this definition to say that, if node
?
A is the
only child of nodeA (a unary expansion) andA lo-
cally c-commands P , then
?
A locally c-commands
P (so both [SBAR ? S] and [S ? NP
2
VP
2
] are
c-commanded by VBP). See Figure 3 for other ex-
amples of non-terminal nodes in c-commanding
relationships, and the phrase expansion rule they
c-command.
The c-command relationship is fundamental in
syntactic theory, and has uses such as predicting
the scope of pronoun antecedents. In this case,
however, we use it to describe two nodes which are
in a specifier?category relationship or a category?
complement relationship (e.g., subject?verb and
verb?object, respectively). This is valuable to us
because it takes advantage of a weakness of sta-
tistical parsers: the context used to condition the
probability of a given rule expansion generally
does not reach beyond dominance relationships,
and thus parsers rarely penalize for the juxtapo-
sition of A c-commanding P and its children as
768
a) S
NP
1
PRP
they
VP
1
VBP
are
SBAR
S
NP
2
DT
that
VP
2
VBZ
is
VP
3
VBG
saying
NP
3
DT
a
NN
lot
b) Rules expansions:
S? NP VP
NP
1
? PRP
VP
1
? VBP SBAR
SBAR? S
S? NP
2
VP
2
NP
2
? DT
VP
2
? VBZ VP
VP
3
? VBG NP
NP
3
? DT NN
c) Rule expansions + c-commanding NT:
S? NP VP no local c-command
NP
1
? PRP no local c-command
VP
1
? V SBAR NP
1
SBAR? S VBP
S? NP
2
VP
2
VBP
NP
2
? DT no local c-command
VP
2
? VBZ VP NP
2
VP
3
? VBG NP VBZ
NP
3
? DT NN VBG
Figure 3: The automatically generated parse (a) for an errorful sentence-like unit (SU), with accompa-
nying rule expansions (b) and local c-commands (c). Non-terminal indices such as NP
2
are for reader
clarification only and are not considered in the feature extraction process.
long as they have previously seen NT type A pre-
ceding NT type P . Thus, we can use the children
of a parent node P as a way to enrich a NT type P
and make it more informative.
For example, in Figure 3, the rule [S ? NP
2
VP
2
] is routinely seen in the manual parses of
the SWBD treebank, as is [VP
1
? VBP SBAR].
However, it is highly unusual for VBP to immedi-
ately precede SBAR or S when this rule expands
to NP
2
VP
2
. So, not only does SBAR/S comple-
ment VBP, but a very specific type of [SBAR/S
? NP VP] is the complement of VBP. This con-
ditional infrequency serves as an indication of
deeper structural errors.
Given these category relationship observations,
we include in our maximum entropy model a fea-
ture indicating whether a given parse includes a
c-command relationship not seen in training data.
Feature #5 (Length): Threshold sentences based
on length. Empirical observation indicates that
long sentences are more likely to contain speaker
errors, while very short sentences tend to be
backchannel acknowledgments like ?yeah? or ?I
know? which are not considered errorful. Oviatt
(1995) quantifies this, determining that the dis-
fluency rate in human-computer dialog increases
roughly linearly with the number of words in an
utterance.
The length-based feature value for each sen-
tence therefore is defined to be the number of word
tokens in that sentence.
Feature #6 (Backchannel): Bias backchannel
acknowledgements as non-errors A backchan-
nel acknowledgement is a short sentence-like unit
(SU) which is produced to indicate that the speaker
is still paying attention to the other speaker, with-
out requesting attention or adding new content to
the dialog. These SUs include ?uh-huh?, ?sure?,
or any combination of backchannel acknowledge-
ments with fillers (ex. ?sure uh uh-huh?).
To assign this feature, fifty-two common
backchannel acknowledgement tokens are consid-
ered. The indicator feature is one (1) if the SU in
question is some combination of these backchan-
nel acknowledgements, and zero (0) otherwise.
3.2 SU-level error identification results
We first observe the performance of each feature
type in isolation in our maximum entropy frame-
work (Table 1(a)). The top-performing individual
769
Features included
Setup JC04 HPSG Rules C-comm Length Backchannel F
1
-score
a) Individual features
1
?
? ? ? ? ? 79.9
2 ?
?
? ? ? ? 77.1
5 ? ? ? ?
?
? 59.7
4 ? ? ?
?
? ? 42.2
3 ? ?
?
? ? ? 23.2
6 ? ? ? ? ?
?
0.0
b) All features combined
7
? ? ? ? ? ?
83.3
c) All-but-one
8 ?
? ? ? ? ?
78.4 (-4.9)
9
? ? ?
?
? ?
81.2 (-2.1)
10
?
?
? ? ? ?
81.3 (-2.0)
11
? ?
?
? ? ?
82.1 (-1.2)
12
? ? ? ? ?
? 82.9 (-0.4)
13
? ? ? ?
?
?
83.2 (-0.1)
Table 1: Comparison of poor construction identification features, tested on the SSR test corpus.
feature is the JC04 edit indicator, which is not sur-
prising as this is the one feature whose existence
was designed specifically to predict speaker errors.
Following JC04 in individual performance are the
HPSG parsability feature, length feature, and un-
seen c-command rule presence feature. Backchan-
nel acknowledgements had no predictive power on
their own. This was itself unsurprising as the fea-
ture was primarily meant to reduce the probability
of selecting these SUs as errorful.
Combining all rules together (Table 1(b)), we
note an F
1
-score gain of 3.4 as compared to the top
individual feature JC04. (JC04 has a precision of
97.6, recall of 67.6, and F of 79.9; the combined
feature model has a precision of 93.0, a recall of
75.3, and an F of 83.3, so unsurprisingly our gain
primarily comes from increased error recall).
In order to understand the contribution of an in-
dividual feature, it helps not only to see the pre-
diction results conditioned only on that feature,
but the loss in accuracy seen when only that fea-
ture is removed from the set. We see in Table 1(c)
that, though the c-command prediction feature was
only moderately accurate in predicting SU errors
on its own, it has the second largest impact after
JC04 (an F-score loss of 2.1) when removed from
the set of features. Such a change indicates the
orthogonality of the information within this fea-
ture to the other features studied. Length, on the
other hand, while moderately powerful as a sin-
gle indicator, had negligible impact on classifica-
tion accuracy when removed from the feature set.
This indicates that the relationship between error-
ful sentences and length can be explained away by
the other features in our set.
We also note that the combination of all features
excluding JC04 is competitive with JC04 itself.
Additional complementary features seem likely to
further compete with the JC04 prediction feature.
4 Combining efforts
The FHJ work shows that the predictive power of
a CRF model could greatly improve (given a re-
striction on only altering SUs suspected to contain
errors) from an F-score of 84.7 to as high as 88.7
for rough copy (RC) errors and from an F-score of
47.5 to as high as 73.8 for non-copy (NC) errors.
Now that we have built a model to predict con-
struction errors on the utterance level, we combine
the two approaches to analyze the improvement
possible for word-level identification (measured
again by precision, recall, and F-score) and for
SU-level correction (measured by the SU Match
metric defined in Section 4.2).
4.1 Word-level evaluation of error
identification, post SU filtering
We first evaluate edit detection accuracy on those
test SUs predicted to be errorful on a per-word ba-
sis. To evaluate our progress identifying word-
770
level error classes, we calculate precision, recall
and F-scores for each labeled class c in each exper-
imental scenario. As usual, these metrics are cal-
culated as ratios of correct, false, and missed pre-
dictions. However, to take advantage of the double
reconstruction annotations provided in SSR (and
more importantly, in recognition of the occasional
ambiguities of reconstruction) we modified these
calculations slightly to account for all references.
Analysis of word-level label evaluation, post SU
filtering. Word-level F
1
-score results for error
region identification are shown in Table 2.
By first automatically selecting testing as de-
scribed in Section 3 (with a sentence-level F-score
of 83.3, Table 1(b)), we see in Table 2 some gain in
F-score for all three error classes, though much po-
tential improvement remains based on the oracle
gain (rows indicated as having ?Gold errors? test-
ing data). Note that there are no results from train-
ing only on errorful data but testing on all data, as
this was shown to yield dramatically worse results
due to data mismatch issues.
Unlike in the experiments where all data was
used for testing and training, the best NC and RC
detection performance given the automatically se-
lected testing data was achieved when training a
CRF model to detect each class separately (RC
or NC alone) and not in conjunction with filler
word detection FL. As in FHJ, training RC and NC
models separately instead of in a joint FL+RC+NC
model yielded higher accuracy.
We notice also that the F-score for RC identi-
fication is lower when automatically filtering the
test data. There are two likely causes. The most
likely issue is that the automatic SU-error clas-
sifier filtered out some SUs with true RC errors
which had previously been correctly identified, re-
ducing the overall precision ratio as well as re-
call (i.e., we no longer receive accuracy credit for
some easier errors once caught). A second, related
possibility is that the errorful SUs identified by
the Section 3 method had a higher density of er-
rors that the current CRF word-level classification
model is unable to identify (i.e. the more difficult
errors are now a higher relative percentage of the
errors we need to catch). While the former pos-
sibility seems more likely, both causes should be
investigated in future work.
The F-score gain in NC identification from 42.5
to 54.6 came primarily from a gain in precision (in
the original model, many non-errorful SUs were
mistakenly determined to include errors). Though
capturing approximately 55% of the non-copy NC
errors (for SUs likely to have errors) is an im-
provement, this remains a challenging and un-
solved task which should be investigated further
in the future.
4.2 Sentence-level evaluation of error
identification and region deletion, post
SU identification
Depending on the downstream task of speech re-
construction, it may be imperative not only to
identify many of the errors in a given spoken ut-
terance, but indeed to identify all errors (and only
those errors), yielding the exact cleaned sentence
that a human annotator might provide.
In these experiments we apply simple cleanup
(as described in Section 1.1) to both JC04 out-
put and the predicted output for each experimental
setup, deleting words when their error class is a
filler, rough copy or non-copy.
Taking advantage of the dual annotations pro-
vided for each sentence in the SSR corpus, we
can report double-reference evaluation. Thus, we
judge that if a hypothesized cleaned sentence ex-
actly matches either reference sentence cleaned in
the same manner we count the cleaned utterance as
correct, and otherwise we assign no credit. We re-
port double-reference exact match evaluation be-
tween a given SU s and references r ? R, as de-
fined below.
SU match =
1
S
?
s?S
max
r?R
?(s, r) (2)
Analysis of sentence level evaluation, post SU
identification. Results from this second evalua-
tion of rough copy and non-copy error reconstruc-
tion can be seen in Table 3.
As seen in word-level identification results (Ta-
ble 2), automatically selecting a subset of testing
data upon which to apply simple cleanup recon-
struction does not perform at the accuracy shown
to be possible given an oracle filtering. While
measuring improvement is difficult (here, non-
filtered data is incomparable to filtered test data
results since a majority of these sentences require
no major deletions at all), we note again that our
methods (MaxEnt/FHJ-x) outperform the baseline
of deleting nothing but filled pauses like ?eh? and
?um?, as well as the state-of-the-art baseline JC04.
771
Class labeled Training SUs for Testing FL RC NC
All data All SU data 71.0 80.3 47.4
FL+RC+NC Errorful only Auto ID?d SU errors 87.9 79.9 49.0
Errorful only Gold SU errors 91.6 84.1 52.2
All data All SU data - - 42.5
NC Errorful only Auto ID?d SU errors - - 54.6
Errorful only Gold SU errors - - 73.8
All data All SU data 70.8 - 47.5
NC+FL Errorful only Auto ID?d SU errors 88.8 - 53.3
Errorful only Gold SU errors 90.7 - 69.8
All data All SU data - /84.2/ -
RC Errorful only Auto ID?d SU errors - 81.3 -
Errorful only Gold SU errors - 88.7 -
All data All SU data 67.8 /84.7/ -
RC+FL Errorful only Auto ID?d SU errors 88.1 80.5 -
Errorful only Gold SU errors 92.3 87.4 -
Table 2: Error predictions, post-SU identification: F
1
-score results. Automatically identified ?SUs for
testing? were determined via the maximum entropy classification model described earlier in this paper,
and feature set #7 from Table 1. Filler (FL), rough copy error (RC) and non-copy error (NC) results are
given in terms of word-level F
1
-score. Bold numbers indicate the highest performance post-automatic
filter for each of the three classes. Italicized values indicate experiments where no filtering outperformed
automatic filtering (for RC errors).
# SUs # SUs that %
Setup Classed deleted Testing (filt/unfilt) match ref accuracy
Baseline-1 only filled pauses All data 2288 1800 78.7%
JC04-1 E+FL All data 2288 1858 81.2%
MaxEnt/FHJ-1 FL+RC+NC All data 2288 1922 84.0%
Baseline-2 only filled pauses Auto ID?d 430 84 19.5%
JC04-2 E+FL Auto ID?d 430 187 43.5%
MaxEnt/FHJ-2 FL+RC+NC Auto ID?d 430 223 51.9%
Baseline-3 only filled pauses Gold errors 281 5 1.8%
JC04-3 E+FL Gold errors 281 126 44.8%
MaxEnt/FHJ-3 FL+RC+NC Gold errors 281 156 55.5%
Table 3: Error predictions, post-SU identification: Exact Sentence Match Results.
For the baseline, we delete only filled pause filler words like ?eh? and ?um?. For JC04 output, we deleted
any word assigned the class E or FL. Finally, for the MaxEnt/FHJ models, we used the jointly trained
FL+RC+NC CRF model and deleted all words assigned any of the three classes.
5 Future Work
While some success and improvements for the
automatic detection and deletion of fillers and
reparanda (i.e., ?simple cleanup?) have been
demonstrated in this work, much remains to be
done to adequately address the issues and criteria
considered here for full reconstruction of sponta-
neous speech.
Included features for both the word level and
SU-level error detection have only skimmed the
surface of potentially powerful features for spon-
taneous speech reconstruction. There should be
continued development of complementary parser-
based features (such as those from dependency
parsers or even deep syntax parsers such as im-
plementations of HPSG as well as additional syn-
tactic features based on automatic constituent or
context-free grammar based parsers). Prosodic
772
features, though demonstrated to be unnecessary
for at least moderately successful detection of sim-
ple errors, also hold promise for additional gains.
Future investigators should evaluate the gains pos-
sible by integrating this information into the fea-
tures and ideas presented here.
6 Summary and conclusions
This work was an extension of the results in FHJ,
which showed that automatically determining
which utterances contain errors before attempting
to identify and delete fillers and reparanda has the
potential to increase accuracy significantly.
In Section 3, we built a maximum entropy clas-
sification model to assign binary error classes to
spontaneous speech utterances. Six features ?
JC04, HPSG, unseen rules, unseen c-command re-
lationships, utterance length, and backchannel ac-
knowledgement composition ? were considered.
The combined model achieved a precision of 93.0,
a recall of 75.3, and an F
1
-score of 83.3.
We then, in Section 4, cascaded the sentence-
level error identification system output into the
FHJ word-level error identification and simple
cleanup system. This combination lead to non-
copy error identification with an F
1
-score of 54.6,
up from 47.5 in the experiments conducted on all
data instead of data identified to be errorful, while
maintaining accuracy for rough copy errors and in-
creasing filler detection accuracy as well. Though
the data setup is slightly different, the true errors
are common across both sets of SUs and thus the
results are comparable.
This work demonstrates that automatically se-
lecting a subset of SUs upon which to imple-
ment reconstruction improves the accuracy of non-
copy (restart fragment) reparanda identification
and cleaning, though less improvement results
from doing the same for rough copy identification.
Acknowledgments
The authors thank our anonymous reviewers for
their valuable comments. Support for this work
was provided by NSF PIRE Grant No. OISE-
0530118. Any opinions, findings, conclusions,
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the supporting agency.
References
Ellen G. Bard, Robin J. Lickley, and Matthew P. Aylett.
2001. Is disfluency just difficult? In Disfluencies in
Spontaneous Speech Workshop, pages 97?100.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Compu-
tational Linguistics.
Noam Chomsky, 1970. Remarks on nominalization,
pages 184?221. Waltham: Ginn.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Hal Daum?e III. 2004. Notes on CG and
LM-BFGS optimization of logistic regression.
Paper available at http://pub.hal3.name\
#daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek.
2009. Reconstructing false start errors in sponta-
neous speech text. In Proceedings of the Annual
Meeting of the European Association for Computa-
tional Linguistics.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen,
Dan Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit,
editors, Collaborative Language Engineering, pages
1?17. CSLI Publications, Stanford.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 517?520,
San Francisco.
Matthias Honal and Tanja Schultz. 2005. Au-
tomatic disfluency removal on recognized spon-
taneous speech ? rapid adaptation to speaker-
dependent disfluenices. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
773
Jeremy Kahn, Matthew Lease, Eugene Charniak, Mark
Johnson, and Mari Ostendorf. 2005. Effective use
of prosody in parsing conversational speech. In Pro-
ceedings of the Conference on Human Language
Technology, pages 561?568.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, and Mary Harper. 2004. The ICSI/UW
RT04 structural metadata extraction system. In Rich
Transcription Fall Workshop.
Sharon L. Oviatt. 1995. Predicting and managing
spoken disfluencies during human-computer interac-
tion. Computer Speech and Language, 9:19?35.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chiacgo
Press and CSLI Publications, Chicago and Stanford.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin.
Qi Zhang and Fuliang Weng. 2005. Exploring fea-
tures for identifying edited regions in disfluent sen-
tences. In Proceedings of the International Work-
shop on Parsing Techniques, pages 179?185.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007.
Partial parse selection for robust deep processing. In
Proceedings of ACL Workshop on Deep Linguistic
Processing, pages 128?135.
774
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255?263,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Reconstructing false start errors in spontaneous speech text
Erin Fitzgerald
Johns Hopkins University
Baltimore, MD, USA
erinf@jhu.edu
Keith Hall
Google, Inc.
Zurich, Switzerland
kbhall@google.com
Frederick Jelinek
Johns Hopkins University
Baltimore, MD, USA
jelinek@jhu.edu
Abstract
This paper presents a conditional ran-
dom field-based approach for identifying
speaker-produced disfluencies (i.e. if and
where they occur) in spontaneous speech
transcripts. We emphasize false start re-
gions, which are often missed in cur-
rent disfluency identification approaches
as they lack lexical or structural similar-
ity to the speech immediately following.
We find that combining lexical, syntac-
tic, and language model-related features
with the output of a state-of-the-art disflu-
ency identification system improves over-
all word-level identification of these and
other errors. Improvements are reinforced
under a stricter evaluation metric requiring
exact matches between cleaned sentences
annotator-produced reconstructions, and
altogether show promise for general re-
construction efforts.
1 Introduction
The output of an automatic speech recognition
(ASR) system is often not what is required for sub-
sequent processing, in part because speakers them-
selves often make mistakes (e.g. stuttering, self-
correcting, or using filler words). A cleaner speech
transcript would allow for more accurate language
processing as needed for natural language process-
ing tasks such as machine translation and conver-
sation summarization which often assume a gram-
matical sentence as input.
A system would accomplish reconstruction of
its spontaneous speech input if its output were
to represent, in flawless, fluent, and content-
preserving text, the message that the speaker in-
tended to convey. Such a system could also be ap-
plied not only to spontaneous English speech, but
to correct common mistakes made by non-native
speakers (Lee and Seneff, 2006), and possibly ex-
tended to non-English speaker errors.
A key motivation for this work is the hope that a
cleaner, reconstructed speech transcript will allow
for simpler and more accurate human and natu-
ral language processing, as needed for applications
like machine translation, question answering, text
summarization, and paraphrasing which often as-
sume a grammatical sentence as input. This ben-
efit has been directly demonstrated for statistical
machine translation (SMT). Rao et al (2007) gave
evidence that simple disfluency removal from tran-
scripts can improve BLEU (a standard SMT eval-
uation metric) up to 8% for sentences with disflu-
encies. The presence of disfluencies were found to
hurt SMT in two ways: making utterances longer
without adding semantic content (and sometimes
adding false content) and exacerbating the data
mismatch between the spontaneous input and the
clean text training data.
While full speech reconstruction would likely
require a range of string transformations and po-
tentially deep syntactic and semantic analysis of
the errorful text (Fitzgerald, 2009), in this work
we will first attempt to resolve less complex errors,
corrected by deletion alone, in a given manually-
transcribed utterance.
We build on efforts from (Johnson et al, 2004),
aiming to improve overall recall ? especially of
false start or non-copy errors ? while concurrently
maintaining or improving precision.
1.1 Error classes in spontaneous speech
Common simple disfluencies in sentence-like ut-
terances (SUs) include filler words (i.e. ?um?, ?ah?,
and discourse markers like ?you know?), as well as
speaker edits consisting of a reparandum, an inter-
ruption point (IP), an optional interregnum (like ?I
mean?), and a repair region (Shriberg, 1994), as
seen in Figure 1.
255
[that?s]
? ?? ?
reparandum
IP
????
+ {uh}
????
interregnum
that?s? ?? ?
repair
a relief
Figure 1: Typical edit region structure. In these
and other examples, reparandum regions are in
brackets (?[?, ?]?), interregna are in braces (?{?,
?}?), and interruption points are marked by ?+?.
These reparanda, or edit regions, can be classified
into three main groups:
1. In a repetition (above), the repair phrase is
approximately identical to the reparandum.
2. In a revision, the repair phrase alters reparan-
dum words to correct the previously stated
thought.
EX1: but [when he] + {i mean} when she put it
that way
EX2: it helps people [that are going to quit] + that
would be quitting anyway
3. In a restart fragment (also called a false
start), an utterance is aborted and then
restarted with a new train of thought.
EX3: and [i think he?s] + he tells me he?s glad he
has one of those
EX4: [amazon was incorporated by] {uh} well i
only knew two people there
In simple cleanup (a precursor to full speech re-
construction), all detected filler words are deleted,
and the reparanda and interregna are deleted while
the repair region is left intact. This is a strong ini-
tial step for speech reconstruction, though more
complex and less deterministic changes are of-
ten required for generating fluent and grammatical
speech text.
In some cases, such as the repetitions men-
tioned above, simple cleanup is adequate for re-
construction. However, simply deleting the identi-
fied reparandum regions is not always optimal. We
would like to consider preserving these fragments
(for false starts in particular) if
1. the fragment contains content words, and
2. its information content is distinct from that in
surrounding utterances.
In the first restart fragment example (EX3 in Sec-
tion 1.1), the reparandum introduces no new ac-
tive verbs or new content, and thus can be safely
deleted. The second example (EX4) however
demonstrates a case when the reparandum may be
considered to have unique and preservable con-
tent of its own. Future work should address how
to most appropriately reconstruct speech in this
and similar cases; this initial work will for risk
information loss as we identify and delete these
reparandum regions.
1.2 Related Work
Stochastic approaches for simple disfluency de-
tection use features such as lexical form, acoustic
cues, and rule-based knowledge. Most state-of-
the-art methods for edit region detection such as
(Johnson and Charniak, 2004; Zhang and Weng,
2005; Liu et al, 2004; Honal and Schultz, 2005)
model speech disfluencies as a noisy channel
model. In a noisy channel model we assume that
an unknown but fluent string F has passed through
a disfluency-adding channel to produce the ob-
served disfluent string D, and we then aim to re-
cover the most likely input string F? , defined as
F? = argmaxFP (F |D)
= argmaxFP (D|F )P (F )
where P (F ) represents a language model defin-
ing a probability distribution over fluent ?source?
strings F , and P (D|F ) is the channel model defin-
ing a conditional probability distribution of ob-
served sentences D which may contain the types
of construction errors described in the previous
subsection. The final output is a word-level tag-
ging of the error condition of each word in the se-
quence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach,
referred to in this document as JC04, combines
the noisy channel paradigm with a tree-adjoining
grammar (TAG) to capture approximately re-
peated elements. The TAG approach models the
crossed word dependencies observed when the
reparandum incorporates the same or very similar
words in roughly the same word order, which JC04
refer to as a rough copy. Our version of this sys-
tem does not use external features such as prosodic
classes, as they use in Johnson et al (2004), but
otherwise appears to produce comparable results
to those reported.
While much progress has been made in sim-
ple disfluency detection in the last decade, even
top-performing systems continue to be ineffec-
tive at identifying words in reparanda. To bet-
ter understand these problems and identify areas
256
Label % of words Precision Recall F-score
Fillers 5.6% 64% 59% 61%
Edit (reparandum) 7.8% 85% 68% 75%
Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.
Label % of edits Recall
Rough copy (RC) edits 58.8% 84.8%
Non-copy (NC) edits 41.2% 43.2%
Total edits 100.0% 67.6%
Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.
1 he that ?s uh that ?s a relief
2 E E E FL - - - -
3 NC RC RC FL - - - -
Figure 2: Example of word class and refined word
class labels, where - denotes a non-error, FL de-
notes a filler, E generally denotes reparanda, and
RC and NC indicate rough copy and non-copy
speaker errors, respectively.
for improvement, we used the top-performing1
JC04 noisy channel TAG edit detector to produce
edit detection analyses on the test segment of the
Spontaneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008). Table 1 demon-
strates the performance of this system for detect-
ing filled pause fillers, discourse marker fillers,
and edit words. The results of a more granular
analysis compared to a hand-refined reference (as
shown in line 3 of Figure 2) are shown in Table 2.
The reader will recall that precision P is defined
as P = |correct||correct|+|false| and recall R =
|correct|
|correct|+|miss| .
We denote the harmonic mean of P and R as F-
score F and calculate it F = 21/P+1/R .
As expected given the assumptions of the TAG
approach, JC04 identifies repetitions and most
revisions in the SSR data, but less success-
fully labels false starts and other speaker self-
interruptions which do not have a cross-serial cor-
relations. These non-copy errors (with a recall of
only 43.2%), are hurting the overall edit detection
recall score. Precision (and thus F-score) cannot
be calculated for the experiment in Table 2; since
the JC04 does not explicitly label edits as rough
copies or non-copies, we have no way of knowing
whether words falsely labeled as edits would have
1As determined in the RT04 EARS Metadata Extraction
Task
been considered as false RCs or false NCs. This
will unfortunately hinder us from using JC04 as a
direct baseline comparison in our work targeting
false starts; however, we consider these results to
be further motivation for the work.
Surveying these results, we conclude that there
is still much room for improvement in the
field of simple disfluency identification, espe-
cially the cases of detecting non-copy reparandum
and learning how and where to implement non-
deletion reconstruction changes.
2 Approach
2.1 Data
We conducted our experiments on the recently re-
leased Spontaneous Speech Reconstruction (SSR)
corpus (Fitzgerald and Jelinek, 2008), a medium-
sized set of disfluency annotations atop Fisher
conversational telephone speech (CTS) data (Cieri
et al, 2004). Advantages of the SSR data include
? aligned parallel original and cleaned sen-
tences
? several levels of error annotations, allowing
for a coarse-to-fine reconstruction approach
? multiple annotations per sentence reflecting
the occasional ambiguity of corrections
As reconstructions are sometimes non-
deterministic (illustrated in EX6 in Section
1.1), the SSR provides two manual reconstruc-
tions for each utterance in the data. We use
these dual annotations to learn complementary
approaches in training and to allow for more
accurate evaluation.
The SSR corpus does not explicitly label all
reparandum-like regions, as defined in Section 1.1,
but only those which annotators selected to delete.
257
Thus, for these experiments we must implicitly
attempt to replicate annotator decisions regarding
whether or not to delete reparandum regions when
labeling them as such. Fortunately, we expect this
to have a negligible effect here as we will empha-
size utterances which do not require more complex
reconstructions in this work.
The Spontaneous Speech Reconstruction cor-
pus is partitioned into three subcorpora: 17,162
training sentences (119,693 words), 2,191 sen-
tences (14,861 words) in the development set, and
2,288 sentences (15,382 words) in the test set. Ap-
proximately 17% of the total utterances contain a
reparandum-type error.
The output of the JC04 model ((Johnson and
Charniak, 2004) is included as a feature and used
as an approximate baseline in the following exper-
iments. The training of the TAG model within this
system requires a very specific data format, so this
system is trained not with SSR but with Switch-
board (SWBD) (Godfrey et al, 1992) data as de-
scribed in (Johnson and Charniak, 2004). Key dif-
ferences in these corpora, besides the form of their
annotations, include:
? SSR aims to correct speech output, while
SWBD edit annotation aims to identify
reparandum structures specifically. Thus, as
mentioned, SSR only marks those reparanda
which annotators believe must be deleted
to generate a grammatical and content-
preserving reconstruction.
? SSR considers some phenomena such as
leading conjunctions (?and i did? ? ?i did?) to
be fillers, while SWBD does not.
? SSR includes more complex error identifi-
cation and correction, though these effects
should be negligible in the experimental
setup presented herein.
While we hope to adapt the trained JC04 model
to SSR data in the future, for now these difference
in task, evaluation, and training data will prevent
direct comparison between JC04 and our results.
2.2 Conditional random fields
Conditional random fields (Lafferty et al, 2001),
or CRFs, are undirected graphical models whose
prediction of a hidden variable sequence Y is
globally conditioned on a given observation se-
quence X , as shown in Figure 3. Each observed
Figure 3: Illustration of a conditional random
field. For this work, x represents observable in-
puts for each word as described in Section 3.1 and
y represents the error class of each word (Section
3.2).
state xi ? X is composed of the corresponding
word wi and a set of additional features Fi, de-
tailed in Section 3.1.
The conditional probability of this model can be
represented as
p?(Y |X) =
1
Z?(X)
exp(
?
k
?kFk(X,Y )) (1)
where Z?(X) is a global normalization factor and
? = (?1 . . . ?K) are model parameters related to
each feature function Fk(X,Y ).
CRFs have been widely applied to tasks in
natural language processing, especially those in-
volving tagging words with labels such as part-
of-speech tagging and shallow parsing (Sha and
Pereira, 2003), as well as sentence boundary
detection (Liu et al, 2005; Liu et al, 2004).
These models have the advantage that they model
sequential context (like hidden Markov models
(HMMs)) but are discriminative rather than gen-
erative and have a less restricted feature set. Ad-
ditionally, as compared to HMMs, CRFs offer
conditional (versus joint) likelihood, and directly
maximizes posterior label probabilities P (E|O).
We used the GRMM package (Sutton, 2006) to
implement our CRF models, each using a zero-
mean Gaussian prior to reduce over-fitting our
model. No feature reduction is employed, except
where indicated.
3 Word-Level ID Experiments
3.1 Feature functions
We aim to train our CRF model with sets of
features with orthogonal analyses of the errorful
text, integrating knowledge from multiple sources.
While we anticipate that repetitions and other
rough copies will be identified primarily by lexical
258
and local context features, this will not necessarily
help for false starts with little or no lexical overlap
between reparandum and repair. To catch these er-
rors, we add both language model features (trained
with the SRILM toolkit (Stolcke, 2002) on SWBD
data with EDITED reparandum nodes removed),
and syntactic features to our model. We also in-
cluded the output of the JC04 system ? which had
generally high precision on the SSR data ? in the
hopes of building on these results.
Altogether, the following features F were ex-
tracted for each observation xi.
? Lexical features, including
? the lexical item and part-of-speech
(POS) for tokens ti and ti+1,
? distance from previous token to the next
matching word/POS,
? whether previous token is partial word
and the distance to the next word with
same start, and
? the token?s (normalized) position within
the sentence.
? JC04-edit: whether previous, next, or cur-
rent word is identified by the JC04 system as
an edit and/or a filler (fillers are classified as
described in (Johnson et al, 2004)).
? Language model features: the unigram log
probability of the next word (or POS) token
p(t), the token log probability conditioned on
its multi-token history h (p(t|h))2, and the
log ratio of the two (log p(t|h)p(t) ) to serve as
an approximation for mutual information be-
tween the token and its history, as defined be-
low.
I(t;h) =
?
h,t
p(h, t) log
p(h, t)
p(h)p(t)
=
?
h,t
p(h, t)
[
log
p(t|h)
p(t)
]
This aims to capture unexpected n-grams
produced by the juxtaposition of the reparan-
dum and the repair. The mutual information
feature aims to identify when common words
are seen in uncommon context (or, alterna-
tively, penalize rare n-grams normalized for
rare words).
2In our model, word historys h encompassed the previous
two words (a 3-gram model) and POS history encompassed
the previous four POS labels (a 5-gram model)
? Non-terminal (NT) ancestors: Given an au-
tomatically produced parse of the utterance
(using the Charniak (1999) parser trained on
Switchboard (SWBD) (Godfrey et al, 1992)
CTS data), we determined for each word all
NT phrases just completed (if any), all NT
phrases about to start to its right (if any), and
all NT constituents for which the word is in-
cluded.
(Ferreira and Bailey, 2004) and others have
found that false starts and repeats tend to end
at certain points of phrases, which we also
found to be generally true for the annotated
data.
Note that the syntactic and POS features we
used are extracted from the output of an automatic
parser. While we do not expect the parser to al-
ways be accurate, especially when parsing errorful
text, we hope that the parser will at least be con-
sistent in the types of structures it assigns to par-
ticular error phenomena. We use these features in
the hope of taking advantage of that consistency.
3.2 Experimental setup
In these experiments, we attempt to label the
following word-boundary classes as annotated in
SSR corpus:
? fillers (FL), including filled pauses and dis-
course markers (?5.6% of words)
? rough copy (RC) edit (reparandum incor-
porates the same or very similar words in
roughly the same word order, including repe-
titions and some revisions) (?4.6% of words)
? non-copy (NC) edit (a speaker error where the
reparandum has no lexical or structural re-
lationship to the repair region following, as
seen in restart fragments and some revisions)
(?3.2% of words)
Other labels annotated in the SSR corpus (such
as insertions and word reorderings), have been ig-
nored for these error tagging experiments.
We approach our training of CRFs in several
ways, detailed in Table 3. In half of our exper-
iments (#1, 3, and 4), we trained a single model
to predict all three annotated classes (as defined
at the beginning of Section 3.3), and in the other
half (#2, 5, and 6), we trained the model to predict
NCs only, NCs and FLs, RCs only, or RCs and FLs
(as FLs often serve as interregnum, we predict that
these will be a valuable cue for other edits).
259
Setup Train data Test data Classes trained per model
#1 Full train Full test FL + RC + NC
#2 Full train Full test {RC,NC}, FL+{RC,NC}
#3 Errorful SUs Errorful SUs FL + RC + NC
#4 Errorful SUs Full test FL + RC + NC
#5 Errorful SUs Errorful SUs {RC,NC}, FL+{RC,NC}
#6 Errorful SUs Full test {RC,NC}, FL+{RC,NC}
Table 3: Overview of experimental setups for word-level error predictions.
We varied the subcorpus utterances used in
training. In some experiments (#1 and 2) we
trained with the entire training set3, including sen-
tences without speaker errors, and in others (#3-6)
we trained only on those sentences containing the
relevant deletion errors (and no additionally com-
plex errors) to produce a densely errorful train-
ing set. Likewise, in some experiments we pro-
duced output only for those test sentences which
we knew to contain simple errors (#3 and 5). This
was meant to emulate the ideal condition where
we could perfectly predict which sentences con-
tain errors before identifying where exactly those
errors occurred.
The JC04-edit feature was included to help us
build on previous efforts for error classification.
To confirm that the model is not simply replicating
these results and is indeed learning on its own with
the other features detailed, we also trained models
without this JC04-edit feature.
3.3 Evaluation of word-level experiments
3.3.1 Word class evaluation
We first evaluate edit detection accuracy on a per-
word basis. To evaluate our progress identify-
ing word-level error classes, we calculate preci-
sion, recall and F-scores for each labeled class c in
each experimental scenario. As usual, these met-
rics are calculated as ratios of correct, false, and
missed predictions. However, to take advantage of
the double reconstruction annotations provided in
SSR (and more importantly, in recognition of the
occasional ambiguities of reconstruction) wemod-
3Using both annotated SSR reference reconstructions for
each utterance
ified these calculations slightly as shown below.
corr(c) =
?
i:cwi=c
?(cwi = cg1,i or cwi = cg2,i)
false(c) =
?
i:cwi=c
?(cwi 6= cg1,i and cwi 6= cg2,i)
miss(c) =
?
i:cg1,i=c
?(cwi 6= cg1,i)
where cwi is the hypothesized class forwi and cg1,i
and cg2,i are the two reference classes.
Setup Class labeled FL RC NC
Train and test on all SUs in the subcorpus
#1 FL+RC+NC 71.0 80.3 47.4
#2 NC - - 42.5
#2 NC+FL 70.8 - 47.5
#2 RC - 84.2 -
#2 RC+FL 67.8 84.7 -
Train and test on errorful SUs
#3 FL+RC+NC 91.6 84.1 52.2
#4 FL+RC+NC 44.1 69.3 31.6
#5 NC - - 73.8
#6 w/ full test - - 39.2
#5 NC+FL 90.7 - 69.8
#6 w/ full test 50.1 - 38.5
#5 RC - 88.7 -
#6 w/ full test - 75.0 -
#5 RC+FL 92.3 87.4 -
#6 w/ full test 62.3 73.9 -
Table 4: Word-level error prediction F1-score re-
sults: Data variation. The first column identifies
which data setup was used for each experiment
(Table 3). The highest performing result for each
class in the first set of experiments has been high-
lighted.
Analysis: Experimental results can be seen in
Tables 4 and 5. Table 4 shows the impact of
260
Features FL RC NC
JC04 only 56.6 69.9-81.9 1.6-21.0
lexical only 56.5 72.7 33.4
LM only 0.0 15.0 0.0
NT bounds only 44.1 35.9 11.5
All but JC04 58.5 79.3 33.1
All but lexical 66.9 76.0 19.6
All but LM 67.9 83.1 41.0
All but NT bounds 61.8 79.4 33.6
All 71.0 80.3 47.4
Table 5: Word-level error prediction F-score re-
sults: Feature variation. All models were trained
with experimental setup #1 and with the set of fea-
tures identified.
training models for individual features and of con-
straining training data to contain only those ut-
terances known to contain errors. It also demon-
strates the potential impact on error classification
after prefiltering test data to those SUs with er-
rors. Table 5 demonstrates the contribution of each
group of features to our CRF models.
Our results demonstrate the impact of varying
our training data and the number of label classes
trained for. We see in Table 4 from setup #5 exper-
iments that training and testing on error-containing
utterances led to a dramatic improvement in F1-
score. On the other hand, our results for experi-
ments using setup #6 (where training data was fil-
tered to contain errorful data but test data was fully
preserved) are consistently worse than those of ei-
ther setup #2 (where both train and test data was
untouched) or setup #5 (where both train and test
data were prefiltered). The output appears to suf-
fer from sample bias, as the prior of an error oc-
curring in training is much higher than in testing.
This demonstrates that a densely errorful training
set alne cannot improve our results when testing
data conditions do not match training data condi-
tions. However, efforts to identify errorful sen-
tences before determining where errors occur in
those sentences may be worthwhile in preventing
false positives in error-less utterances.
We next consider the impact of the four feature
groups on our prediction results. The CRF model
appears competitive even without the advantage
of building on JC04 results, as seen in Table 54.
4JC04 results are shown as a range for the reasons given in
Section 1.2: since JC04 does not on its own predict whether
an ?edit? is a rough copy or non-copy, it is impossible to cal-
Interestingly and encouragingly, the NT bounds
features which indicate the linguistic phrase struc-
tures beginning and ending at each word accord-
ing to an automatic parse were also found to be
highly contribututive for both fillers and non-copy
identification. We believe that further pursuit of
syntactic features, especially those which can take
advantage of the context-free weakness of statisti-
cal parsers like (Charniak, 1999) will be promising
in future research.
It was unexpected that NC classification would
be so sensitive to the loss of lexical features while
RC labeling was generally resilient to the drop-
ping of any feature group. We hypothesize that
for rough copies, the information lost from the re-
moval of the lexical items might have been com-
pensated for by the JC04 features as JC04 per-
formed most strongly on this error type. This
should be further investigated in the future.
3.3.2 Strict evaluation: SU matching
Depending on the downstream task of speech re-
construction, it could be imperative not only to
identify many of the errors in a given spoken ut-
terance, but indeed to identify all errors (and only
those errors), yielding the precise cleaned sentence
that a human annotator might provide.
In these experiments we apply simple cleanup
(as described in Section 1.1) to both JC04 out-
put and the predicted output for each experimental
setup in Table 3, deleting words when their right
boundary class is a filled pause, rough copy or
non-copy.
Taking advantage of the dual annotations for
each sentence in the SSR corpus, we can report
both single-reference and double-reference eval-
uation. Thus, we judge that if a hypothesized
cleaned sentence exactly matches either reference
sentence cleaned in the same manner, we count the
cleaned utterance as correct and otherwise assign
no credit.
Analysis: We see the outcome of this set of ex-
periments in Table 6. While the unfiltered test sets
of JC04-1, setup #1 and setup #2 appear to have
much higher sentence-level cleanup accuracy than
the other experiments, we recall that this is natu-
ral also due to the fact that the majority of these
sentences should not be cleaned at all, besides
culate precision and thus F1 score precisely. Instead, here we
show the resultant F1 for the best case and worst case preci-
sion range.
261
Setup Classes deleted # SUs # SUs which match gold % accuracy
Baseline only filled pauses 2288 1800 78.7%
JC04-1 E+FL 2288 1858 81.2%
CRF-#1 RC, NC, and FL 2288 1922 84.0%
CRF-#2
?
{RC,NC} 2288 1901 83.1%
Baseline only filled pauses 281 5 1.8%
JC04-2 E+FL 281 126 44.8%
CRF-#3 RC, NC, and FL 281 156 55.5%
CRF-#5
?
{RC,NC} 281 132 47.0%
Table 6: Word-level error predictions: exact SU match results. JC04-2 was run only on test sentences
known to contain some error to match the conditions of Setup #3 and #5 (from Table 3). For the baselines,
we delete only filled pause filler words like ?eh? and ?um?.
occasional minor filled pause deletions. Look-
ing specifically on cleanup results for sentences
known to contain at least one error, we see, once
again, that our system outperforms our baseline
JC04 system at this task.
4 Discussion
Our first goal in this work was to focus on an area
of disfluency detection currently weak in other
state-of-the-art speaker error detection systems ?
false starts ? while producing comparable classi-
fication on repetition and revision speaker errors.
Secondly, we attempted to quantify how far delet-
ing identified edits (both RC and NC) and filled
pauses could bring us to full reconstruction of
these sentences.
We?ve shown in Section 3 that by training and
testing on data prefiltered to include only utter-
ances with errors, we can dramatically improve
our results, not only by improving identification
of errors but presumably by reducing the risk of
falsely predicting errors. We would like to further
investigate to understand how well we can auto-
matically identify errorful spoken utterances in a
corpus.
5 Future Work
This work has shown both achievable and demon-
strably feasible improvements in the area of iden-
tifying and cleaning simple speaker errors. We be-
lieve that improved sentence-level identification of
errorful utterances will help to improve our word-
level error identification and overall reconstruction
accuracy; we will continue to research these areas
in the future. We intend to build on these efforts,
adding prosodic and other features to our CRF and
maximum entropy models,
In addition, as we improve the word-level clas-
sification of rough copies and non-copies, we will
begin to move forward to better identify more
complex speaker errors such as missing argu-
ments, misordered or redundant phrases. We will
also work to apply these results directly to the out-
put of a speech recognition system instead of to
transcripts alone.
Acknowledgments
The authors thank our anonymous reviewers for
their valuable comments. Support for this work
was provided by NSF PIRE Grant No. OISE-
0530118. Any opinions, findings, conclusions,
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the supporting agency.
References
J. Kathryn Bock. 1982. Toward a cognitive psy-
chology of syntax: Information processing contri-
butions to sentence formulation. Psychological Re-
view, 89(1):1?47, January.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Meeting of the North American
Association for Computational Linguistics.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Fernanda Ferreira and Karl G. D. Bailey. 2004. Disflu-
encies and human language comprehension. Trends
in Cognitive Science, 8(5):231?237, May.
262
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference, May.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 517?520,
San Francisco.
Matthias Honal and Tanja Schultz. 2005. Au-
tomatic disfluency removal on recognized spon-
taneous speech ? rapid adaptation to speaker-
dependent disfluenices. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Rich Transcription
Fall Workshop.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Proceedings of the International Conference on
Spoken Language Processing.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, and Mary Harper. 2004. The ICSI/UW
RT04 structural metadata extraction system. In Rich
Transcription Fall Workshop.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, pages 451?458,
Ann Arbor, MI.
Sharath Rao, Ian Lane, and Tanja Schultz. 2007. Im-
proving spoken language translation by automatic
disfluency removal: Evidence from conversational
speech transcripts. In Machine Translation Summit
XI, Copenhagen, Denmark, October.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In HLT-NAACL.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing, Denver, CO, September.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Qi Zhang and Fuliang Weng. 2005. Exploring fea-
tures for identifying edited regions in disfluent sen-
tences. In Proceedings of the International Work-
shop on Parsing Techniques, pages 179?185.
263
Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
A Study on Richer Syntactic Dependencies for Structured Language
Modeling
Peng Xu
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
xp@clsp.jhu.edu
Ciprian Chelba
Microsoft Research
One Microsoft Way
Redmond, WA 98052
chelba@microsoft.com
Frederick Jelinek
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
jelinek@clsp.jhu.edu
Abstract
We study the impact of richer syntac-
tic dependencies on the performance of
the structured language model (SLM)
along three dimensions: parsing accu-
racy (LP/LR), perplexity (PPL) and word-
error-rate (WER, N-best re-scoring). We
show that our models achieve an im-
provement in LP/LR, PPL and/or WER
over the reported baseline results us-
ing the SLM on the UPenn Treebank
and Wall Street Journal (WSJ) corpora,
respectively. Analysis of parsing per-
formance shows correlation between the
quality of the parser (as measured by pre-
cision/recall) and the language model per-
formance (PPL and WER). A remarkable
fact is that the enriched SLM outperforms
the baseline 3-gram model in terms of
WER by 10% when used in isolation as a
second pass (N-best re-scoring) language
model.
1 Introduction
The structured language model uses hidden parse
trees to assign conditional word-level language
model probabilities. As explained in (Chelba and
Jelinek, 2000), Section 4.4.1, if the final best parse
is used to be the only parse, the reduction in PPL
?relative to a 3-gram baseline? using the SLM?s
headword parametrization for word prediction is
about 40%. The key to achieving this reduction is
a good guess of the final best parse for a given sen-
tence as it is being traversed left-to-right, which is
much harder than finding the final best parse for the
entire sentence, as it is sought by a regular statistical
parser. Nevertheless, it is expected that techniques
developed in the statistical parsing community that
aim at recovering the best parse for an entire sen-
tence, i.e. as judged by a human annotator, should
also be productive in enhancing the performance of
a language model that uses syntactic structure.
The statistical parsing community has used var-
ious ways of enriching the dependency structure
underlying the parametrization of the probabilistic
model used for scoring a given parse tree (Charniak,
2000) (Collins, 1999). Recently, such models (Char-
niak, 2001) (Roark, 2001) have been shown to out-
perform the SLM in terms of both PPL and WER on
the UPenn Treebank and WSJ corpora, respectively.
In (Chelba and Xu, 2001), a simple way of enriching
the probabilistic dependencies in the CONSTRUC-
TOR component of the SLM also showed better
PPL and WER performance; the simple modifica-
tion to the training procedure brought the WER per-
formance of the SLM to the same level with the best
as reported in (Roark, 2001).
In this paper, we present three simple ways of
enriching the syntactic dependency structure in the
SLM, extending the work in (Chelba and Xu, 2001).
The results show that an improved parser (as mea-
sured by LP/LR) is indeed helpful in reducing the
PPL and WER. Another remarkable fact is that for
the first time a language model exploiting elemen-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 191-198.
                         Proceedings of the 40th Annual Meeting of the Association for
tary syntactic dependencies obviates the need for
interpolation with a 3-gram model in N-best re-
scoring.
2 SLM Review
An extensive presentation of the SLM can be found
in (Chelba and Jelinek, 2000). The model assigns a
probability  
	 to every sentence  and ev-
ery possible binary parse  . The terminals of 
are the words of  with POS tags, and the nodes
of

are annotated with phrase headwords and non-
terminal labels. Let  be a sentence of length 
(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>
h_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)
Figure 1: A word-parse  -prefix
words to which we have prepended the sentence be-
ginning marker <s> and appended the sentence end
marker </s> so that  <s> and  </s>.
Let

Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 746?754,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
What lies beneath: Semantic and syntactic analysis
of manually reconstructed spontaneous speech
Erin Fitzgerald
Johns Hopkins University
Baltimore, MD, USA
erinf@jhu.edu
Frederick Jelinek
Johns Hopkins University
Baltimore, MD, USA
jelinek@jhu.edu
Robert Frank
Yale University
New Haven, CT, USA
bob.frank@yale.edu
Abstract
Spontaneously produced speech text often
includes disfluencies which make it diffi-
cult to analyze underlying structure. Suc-
cessful reconstruction of this text would
transform these errorful utterances into
fluent strings and offer an alternate mech-
anism for analysis.
Our investigation of naturally-occurring
spontaneous speaker errors aligned to
corrected text with manual semantico-
syntactic analysis yields new insight into
the syntactic and structural semantic
differences between spoken and recon-
structed language.
1 Introduction
In recent years, natural language processing tasks
such as machine translation, information extrac-
tion, and question answering have been steadily
improving, but relatively little of these systems
besides transcription have been applied to the
most natural form of language input: spontaneous
speech. Moreover, there has historically been lit-
tle consideration of how to analyze the underlying
semantico-syntactic structure of speech.
A system would accomplish reconstruction of
its spontaneous speech input if its output were
to represent, in flawless, fluent, and content-
preserved English, the message that the speaker
intended to convey (Fitzgerald and Jelinek, 2008;
Fitzgerald et al, 2009). Examples of such recon-
structions are seen in the following sentence-like
units (SUs).
EX1: that?s uh that?s a relief
becomes
that?s a relief
EX2: how can you do that without + it?s a catch-22
becomes
how can you do that without <ARG>
it?s a catch-22
EX3: they like video games some kids do
becomes
some kids like video games
In EX1, reconstruction requires only the dele-
tion of a simple filled pause and speaker repetition
(or reparandum (Shriberg, 1994)). The second ex-
ample shows a restart fragment, where an utter-
ance is aborted by the speaker and then restarted
with a new train of thought. Reconstruction here
requires
1. Detection of an interruption point (denoted
+ in the example) between the abandoned
thought and its replacement,
2. Determination that the abandoned portion
contains unique and preservable content and
should be made a new sentence rather than be
deleted (which would alter meaning)
3. Analysis showing that a required argument
must be inserted in order to complete the sen-
tence.
Finally, in the third example EX3, in order to pro-
duce one of the reconstructions given, a system
must
1. Detect the anaphoric relationship between
?they? and ?some kids?
2. Detect the referral of ?do? to ?like video games?
3. Make the necessary word reorderings and
deletion of the less informative lexemes.
These examples show varying degrees of diffi-
culty for the task of automatic reconstruction. In
each case, we also see that semantic analysis of the
reconstruction is more straightforward than of the
746
original string directly. Such analysis not only in-
forms us of what the speaker intended to commu-
nicate, but also reveals insights into the types of er-
rors speakers make when speaking spontaneously
and where these errors occur. The semantic la-
beling of reconstructed sentences, when combined
with the reconstruction alignments, may yield new
quantifiable insights into the structure of disfluent
natural speech text.
In this paper, we will investigate this relation-
ship further. Generally, we seek to answer two
questions:
? What generalizations about the underlying
structure of errorful and reconstructed speech
utterances are possible?
? Are these generalizations sufficiently robust
as to be incorporated into statistical models
in automatic systems?
We begin by reviewing previous work in the au-
tomatic labeling of structural semantics and moti-
vating the analysis not only in terms of discovery
but also regarding its potential application to auto-
matic speech reconstruction research. In Section 2
we describe the Spontaneous Speech Reconstruc-
tion (SSR) corpus and the manual semantic role
labeling it includes. Section 3 analyzes structural
differences between verbatim and reconstructed
text in the SSR as evaluated by a combination of
manual and automatically generated phrasal con-
stituent parses, while Section 4 combines syntactic
structure and semantic label annotations to deter-
mine the consistency of patterns and their compar-
ison to similar patterns in the Wall Street Journal
(WSJ)-based Proposition Bank (PropBank) corpus
(Palmer et al, 2005). We conclude by offering a
high level analysis of discoveries made and sug-
gesting areas for continued analysis in the future.
Expanded analysis of these results is described in
(Fitzgerald, 2009).
1.1 Semantic role labeling
Every verb can be associated with a set of core
and optional argument roles, sometimes called a
roleset. For example, the verb ?say? must have a
sayer and an utterance which is said, along with
an optionally defined hearer and any number of
locative, temporal, manner, etc. adjunctival argu-
ments.
The task of predicate-argument labeling (some-
times called semantic role labeling or SRL) as-
signs a simplewho didwhat towhom when, where,
some kids? ?? ?
ARG0
like????
predicate
video games
? ?? ?
ARG1
Figure 1: Semantic role labeling for the sentence
?some kids like video games?. According to Prop-
Bank specifications, core arguments for each pred-
icate are assigned a corresponding label ARG0-
ARG5 (where ARG0 is a proto-agent, ARG1 is a
proto-patient, etc. (Palmer et al, 2005)).
why, how, etc. structure to sentences (see Figure
1), often for downstream processes such as infor-
mation extraction and question answering. Reli-
ably identifying and assigning these roles to gram-
matical text is an active area of research (Gildea
and Jurafsky, 2002; Pradhan et al, 2004; Prad-
han et al, 2008), using training resources like the
Linguistic Data Consortium?s Proposition Bank
(PropBank) (Palmer et al, 2005), a 300k-word
corpus with semantic role relations labeled for
verbs in the WSJsection of the Penn Treebank.
A common approach for automatic semantic
role labeling is to separate the process into two
steps: argument identification and argument label-
ing. For each task, standard cue features in au-
tomatic systems include verb identification, anal-
ysis of the syntactic path between that verb and
the prospective argument, and the direction (to the
left or to the right) in which the candidate argu-
ment falls in respect to its predicate. In (Gildea
and Palmer, 2002), the effect of parser accuracy
on semantic role labeling is quantified, and con-
sistent quality parses were found to be essential
when automatically identifying semantic roles on
WSJ text.
1.2 Potential benefit of semantic analysis to
speech reconstruction
With an adequate amount of appropriately anno-
tated conversational text, methods such as those
referred to in Section 1.1 may be adapted for
transcriptions of spontaneous speech in future re-
search. Furthermore, given a set of semantic
role labels on an ungrammatical string, and armed
with the knowledge of a set of core semantico-
syntactic principles which constrain the set of pos-
sible grammatical sentences, we hope to discover
and take advantage of new cues for construction
errors in the field of automatic spontaneous speech
reconstruction.
747
2 Data
We conducted our experiments on the Spon-
taneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008), a 6,000 SU set of
reconstruction annotations atop a subset of Fisher
conversational telephone speech data (Cieri et al,
2004), including
? manual word alignments between corre-
sponding original and cleaned sentence-like
units (SUs) which are labeled with transfor-
mation types (Section 2.1), and
? annotated semantic role labels on predicates
and their arguments for all grammatical re-
constructions (Section 2.2).
The fully reconstructed portion of the SSR cor-
pus consists of 6,116 SUs and 82,000 words to-
tal. While far smaller than the 300,000-word Prop-
Bank corpus, we believe that this data will be ad-
equate for an initial investigation to characterize
semantic structure of verbatim and reconstructed
speech.
2.1 Alignments and alteration labels
In the SSR corpus, words in each reconstructed
utterance were deleted, inserted, substituted, or
moved as required to make the SU as grammatical
as possible without altering the original meaning
and without the benefit of extrasentential context.
Alignments between the original words and their
reconstructed ?source? words (i.e. in the noisy
channel paradigm) are explicitly defined, and for
each alteration a corresponding alteration label
has been chosen from the following.
- DELETE words: fillers, repetitions/revisions,
false starts, co-reference, leading conjuga-
tion, and extraneous phrases
- INSERT neutral elements, such as function
words like ?the?, auxiliary verbs like ?is?, or
undefined argument placeholders, as in ?he
wants <ARG>?
- SUBSTITUTE words to change tense or num-
ber, correct transcriber errors, and replace
colloquial phrases (such as: ?he was like...? ?
?he said...?)
- REORDER words (within sentence bound-
aries) and label as adjuncts, arguments, or
other structural reorderings
Unchanged original words are aligned to the cor-
responding word in the reconstruction with an arc
marked BASIC.
2.2 Semantic role labeling in the SSR corpus
One goal of speech reconstruction is to develop
machinery to automatically reduce an utterance to
its underlying meaning and then generate clean
text. To do this, we would like to understand
how semantic structure in spontaneous speech text
varies from that of written text. Here, we can take
advantage of the semantic role labeling included
in the SSR annotation effort.
Rather than attempt to label incomplete ut-
terances or errorful phrases, SSR annotators as-
signed semantic annotation only to those utter-
ances which were well-formed and grammatical
post-reconstruction. Therefore, only these utter-
ances (about 72% of the annotated SSR data) can
be given a semantic analysis in the following sec-
tions. For each well-formed and grammatical sen-
tence, all (non-auxiliary and non-modal) verbs
were identified by annotators and the correspond-
ing predicate-argument structure was labeled ac-
cording to the role-sets defined in the PropBank
annotation effort1.
We believe the transitive bridge between the
aligned original and reconstructed sentences and
the predicate-argument labels for those recon-
structions (described further in Section 4) may
yield insight into the structure of speech errors and
how to extract these verb-argument relationships
in verbatim and errorful speech text.
3 Syntactic variation between original
and reconstructed strings
As we begin our analysis, we first aim to under-
stand the types of syntactic changes which occur
during the course of spontaneous speech recon-
struction. These observations are made empiri-
cally given automatic analysis of the SSR corpus
annotations. Syntactic evaluation of speech and
reconstructed structure is based on the following
resources:
1. the manual parse Pvm for each verbatim orig-
inal SU (from SSR)
2. the automatic parse Pva of each verbatim
original SU
1PropBank roleset definitions for given verbs can be re-
viewed at http://www.cs.rochester.edu/?gildea/Verbs/.
748
3. the automatic parse Pra of each reconstructed
SU
We note that automatic parses (using the state
of the art (Charniak, 1999) parser) of verbatim,
unreconstructed strings are likely to contain many
errors due to the inconsistent structure of ver-
batim spontaneous speech (Harper et al, 2005).
While this limits the reliability of syntactic obser-
vations, it represents the current state of the art for
syntactic analysis of unreconstructed spontaneous
speech text.
On the other hand, automatically obtained
parses for cleaned reconstructed text are more
likely to be accurate given the simplified and more
predictable structure of these SUs. This obser-
vation is unfortunately not evaluable without first
manually parsing all reconstructions in the SSR
corpus, but is assumed in the course of the follow-
ing syntax-dependent analysis.
In reconstructing from errorful and disfluent
text to clean text, a system makes not only surface
changes but also changes in underlying constituent
dependencies and parser interpretation. We can
quantify these changes in part by comparing the
internal context-free structure between the two
sets of parses.
We compare the internal syntactic structure be-
tween sets Pva and Pra of the SSR check set.
Statistics are compiled in Table 1 and analyzed be-
low.
? 64.2% of expansion rules in parses Pva
also occur in reconstruction parses Pra , and
92.4% (86.8%) of reconstruction parse Pra
expansions come directly from the verbatim
parses Pva (from columns one and two of Ta-
ble 1).
? Column three of Table 1 shows the rule types
most often dropped from the verbatim string
parses Pva in the transformation to recon-
struction. The Pva parses select full clause
non-terminals (NTs) for the verbatim parses
which are not in turn selected for automatic
parses of the reconstruction (e.g. [SBAR ?
S] or [S ? VP]). This suggests that these
rules may be used to handle errorful struc-
tures not seen by the trained grammar.
? Rule types in column four of Table 1 are the
most often ?generated? in Pra (as they are
unseen in the automatic parse Pva). Since
rules like [S ? NP VP], [PP ? IN NP],
and [SBAR ? IN S] appear in a recon-
struction parse but not corresponding verba-
tim parse at similar frequencies regardless of
whether Pvm or Pva are being compared, we
are more confident that these patterns are ef-
fects of the verbatim-reconstruction compar-
ison and not the specific parser used in anal-
ysis. The fact that these patterns occur in-
dicates that it is these common rules which
are most often confounded by spontaneous
speaker errors.
? Given a Levenshtein alignment between al-
tered rules, the most common changes within
a given NT phrase are detailed in column five
of Table 1. We see that the most com-
mon aligned rule changes capture the most
basic of errors: a leading coordinator (#1
and 2) and rules proceeded by unnecessary
filler words (#3 and 5). Complementary rules
#7 and 9 (e.g. VP ? [rule]/[rule SBAR] and
VP? [rule SBAR]/[rule]) show that comple-
menting clauses are both added and removed,
possibly in the same SU (i.e. a phrase shift),
during reconstruction.
4 Analysis of semantics for speech
Figure 2: Manual semantic role labeling for the
sentence ?some kids like video games? and SRL
mapped onto its verbatim source string ?they like
video games and stuff some kids do?
To analyze the semantic and syntactic patterns
found in speech data and its corresponding recon-
structions, we project semantic role labels from
strings into automatic parses, and moreover from
their post-reconstruction source to the verbatim
original speech strings via the SSR manual word
alignments, as shown in Figures 2.
The automatic SRL mapping procedure from
the reconstructed string Wr to related parses Pra
and Pva and the verbatim original string Wv is as
follows.
749
Pva rules Pra rules Pva rules most Pra rules most Levenshtein-aligned expansion
in Pra in Pva frequently dropped frequently added changes (Pva/Pra)
1. NP? PRP 1. S? NP VP 1. S? [ CC rule] / [rule]
2. ROOT? S 2. PP? IN NP 2. S? [ CC NP VP] / [ NP VP]
3. S? NP VP 3. ROOT? S 3. S? [ INTJ rule] / [rule]
4. INTJ? UH 4. ADVP? RB 4. S? [ NP rule] / [rule]
64.2% 92.4% 5. PP? IN NP 5. S? NP ADVP VP 5. S? [ INTJ NP VP] / [ NP VP]
6. ADVP? RB 6. SBAR? IN S 6. S? [ NP NP VP] / [ NP VP]
7. SBAR? S 7. SBAR? S 7. VP? [rule] / [rule SBAR]
8. NP? DT NN 8. S? ADVP NP VP 8. S? [ RB rule] / [rule]
9. S? VP 9. S? VP 9. VP? [rule SBAR] / [rule]
10. PRN? S 10. NP? NP SBAR 10. S? [rule] / [ ADVP rule]
Table 1: Internal syntactic structure removed and gained during reconstruction. This table compares
the rule expansions for each verbatim string automatically parsed Pva and the automatic parse of the
corresponding reconstruction in the SSR corpus (Pra).
1. Tag each reconstruction word wr ? string
Wr with the annotated SRL tag twr .
(a) Tag each verbatim word wv ? stringWv
aligned to wr via a BASIC, REORDER,
or SUBSTITUTE alteration label with the
SRL tag twr as well.
(b) Tag each verbatim word wv aligned
to wr via a DELETE REPETITION
or DELETE CO-REFERENCE alignment
with a shadow of that SRL tag twr (see
the lower tags in Figure 2 for an exam-
ple)
Any verbatim original word wv with any
other alignment label is ignored in this se-
mantic analysis as SRL labels for the aligned
reconstruction word wr do not directly trans-
late to them.
2. Overlay tagged words of string Wv and Wr
with the automatic (or manual) parse of the
same string.
3. Propagate labels. For each constituent in
the parse, if all children within a syntactic
constituent expansion (or all but EDITED or
INTJ) has a given SRL tag for a given pred-
icate, we instead tag that NT (and not chil-
dren) with the semantic label information.
4.1 Labeled verbs and their arguments
In the 3,626 well-formed and grammatical SUs la-
beled with semantic roles in the SSR, 895 distinct
verb types were labeled with core and adjunct ar-
guments as defined in Section 1.1. The most fre-
quent of these verbs was the orthographic form ??s?
which was labeled 623 times, or in roughly 5%
of analyzed sentences. Other forms of the verb
?to be?, including ?is?, ?was?, ?be?, ?are?, ?re?, ??m?,
and ?being?, were labeled over 1,500 times, or at
a rate of nearly one in half of all well-formed re-
constructed sentences. The verb type frequencies
roughly follow a Zipfian distribution (Zipf, 1949),
where most verb words appear only once (49.9%)
or twice (16.0%).
On average, 1.86 core arguments (ARG[0-4])
are labeled per verb, but the specific argument
types and typical argument numbers per predicate
are verb-specific. For example, the ditransitive
verb ?give? has an average of 2.61 core arguments
for its 18 occurrences, while the verb ?divorced?
(whose core arguments ?initiator of end of mar-
riage? and ?ex-spouse? are often combined, as in
?we divorced two years ago?) was labeled 11 times
with an average of 1.00 core arguments per occur-
rence.
In the larger PropBank corpus, annotated atop
WSJ news text, the most frequently reported verb
root is ?say?, with over ten thousand labeled ap-
pearances in various tenses (this is primarily ex-
plained by the genre difference between WSJ and
telephone speech)2; again, most verbs occur two
or fewer times.
4.2 Structural semantic statistics in cleaned
speech
A reconstruction of a verbatim spoken utterance
can be considered an underlying form, analogous
2The reported PropBank analysis ignores past and present
participle (passive) usage; we do not do this in our analysis.
750
to that of Chomskian theory or Harris?s concep-
tion of transformation (Harris, 1957). In this view,
the original verbatim string is the surface form of
the sentence, and as in linguistic theory should be
constrained in some manner similar to constraints
between Logical Form (LF) and Surface Structure
(SS).
Most common syntactic
Data SRL Total categories, with rel. frequency
Pva 10110 NP (50%) PP (6%)
Pra ARG1 8341 NP (58%) SBAR (9%)
PB05 Obj-NP (52%) S (22%)
Pva 4319 NP (90%) WHNP (3%)
Pra ARG0 4518 NP (93%) WHNP (3%)
PB05 Subj-NP (97%) NP (2%)
Pva 3836 NP (28%) PP (13%)
Pra ARG2 3179 NP (29%) PP (18%)
PB05 NP (36%) Obj-NP (29%)
Pva 931 ADVP (25%) NP (20%)
Pra TMP 872 ADVP (27%) PP (18%)
PB05 ADVP (26%) PP-in (16%)
Pva 562 MD (58%) TO (18%)
Pra MOD 642 MD (57%) TO (19%)
PB05 MD (99%) ADVP (1%)
Pva 505 PP (47%) ADVP (16%)
Pra LOC 489 PP (54%) ADVP (17%)
PB05 PP-in (59%) PP-on (10.0%)
Table 2: Most frequent phrasal categories for com-
mon arguments in the SSR (mapping SRLs onto
Pva parses). PB05 refers to the PropBank data de-
scribed in (Palmer et al, 2005).
Most common argument
Data NT Total labels, with rel. frequency
Pva 10541 ARG1 (48%) ARG0 (37%)
Pra NP 10218 ARG1 (47%) ARG0 (41%)
PB05 ARG2 (34%) ARG1 (24%)
PB05 Subj-NP ARG0 (79%) ARG1 (17%)
PB05 Obj-NP ARG1 (84%) ARG2 (10%)
Pva PP 1714 ARG1 (34%) ARG2 (30%)
Pra 1777 ARG1 (31%) ARG2 (30%)
PB05 PP-in LOC (48%) TMP (35%)
PB05 PP-at EXT (36%) LOC (27%)
Pva 1519 ARG2 (21%) ARG1 (19%)
Pra ADVP 1444 ARG2 (22%) ADV (20%)
PB05 TMP (30%) MNR (22%)
Pva 930 ARG1 (61%) ARG2 (14%)
Pra SBAR 1241 ARG1 (62%) ARG2 (12%)
PB05 ADV (36%) TMP (30%)
Pva 523 ARG1 (70%) ARG2 (16%)
Pra S 526 ARG1 (72%) ARG2 (17%)
PB05 ARG1 (76%) ADV (9%)
Pva 449 MOD (73%) ARG1 (18%)
Pra MD 427 MOD (86%) ARG1 (11%)
PB05 MOD (97%)Adjuncts (3%)
Table 3: Most frequent argument categories for
common syntactic phrases in the SSR (mapping
SRLs onto Pva parses).
In this section, we identify additional trends
which may help us to better understand these con-
straints, such as the most common phrasal cate-
gory for common arguments in common contexts
? listed in Table 2 ? and the most frequent seman-
tic argument type for NTs in the SSR ? listed in
Table 3.
4.3 Structural semantic differences between
verbatim speech and reconstructed
speech
We now compare the placement of semantic role
labels with reconstruction-type labels assigned in
the SSR annotations.
These analyses were conducted on Pra parses of
reconstructed strings, the strings upon which se-
mantic labels were directly assigned.
Reconstructive deletions
Q: Is there a relationship between speaker er-
ror types requiring deletions and the argument
shadows contained within? Only two deletion
types ? repetitions/revisions and co-references ?
have direct alignments between deleted text and
preserved text and thus can have argument shad-
ows from the reconstruction marked onto the ver-
batim text.
Of 9,082 propagated deleted repetition/ revision
phrase nodes from Pva , we found that 31.0% of ar-
guments within were ARG1, 22.7% of arguments
were ARG0, 8.6% of nodes were predicates la-
beled with semantic roles of their own, and 8.4%
of argument nodes were ARG2. Just 8.4% of
?delete repetition/revision? nodes were modifier
(vs. core) arguments, with TMP and CAU labels
being the most common.
Far fewer (353) nodes from Pva represented
deleted co-reference words. Of these, 57.2% of ar-
gument nodes were ARG1, 26.6% were ARG0 and
13.9% were ARG2. 7.6% of ?argument? nodes
here were SRL-labeled predicates, and 10.2%
were in modifier rather than core arguments, the
most prevalent were TMP and LOC.
These observations indicate to us that redun-
dant co-references are far most likely to occur for
ARG1 roles (most often objects, though also sub-
jects for copular verbs (i.e. ?to be?) and others) and
appear more likely than random to occur in core
argument regions of an utterance rather than in op-
tional modifying material.
Reconstructive insertions
751
Q: When null arguments are inserted into re-
constructions of errorful speech, what seman-
tic role do they typically fill? Three types of
insertions were made by annotators during the re-
construction of the SSR corpus. Inserted function
words, the most common, were also the most var-
ied. Analyzing the automatic parses of the recon-
structions Pra , we find that the most commonly
assigned parts-of-speech (POS) for these elements
was fittingly IN (21.5%, preposition), DT (16.7%,
determiner) and CC (14.3%, conjunction). Inter-
estingly, we found that the next most common
POS assignments were noun labels, which may in-
dicate errors in SSR labeling.
Other inserted word types were auxiliary or oth-
erwise neutral verbs, and, as expected, most POS
labels assigned by the parses were verb types,
mostly VBP (non-third person present singular).
About half of these were labeled as predicates with
corresponding semantic roles; the rest were unla-
beled which makes sense as true auxiliary verbs
were not labeled in the process.
Finally, around 147 insertion types made were
neutral arguments (given the orthographic form
<ARG>). 32.7% were common nouns and 18.4%
of these were labeled personal pronouns PRP. An-
other 11.6% were adjectives JJ. We found that 22
(40.7%) of 54 neutral argument nodes directly as-
signed as semantic roles were ARG1, and another
33.3% were ARG0. Nearly a quarter of inserted
arguments became part of a larger phrase serv-
ing as a modifier argument, the most common of
which were CAU and LOC.
Reconstructive substitutions
Q: How often do substitutions occur in the an-
alyzed data, and is there any semantic con-
sistency in the types of words changed? 230
phrase tense substitutions occurred in the SSR cor-
pus. Only 13 of these were directly labeled as
predicate arguments (as opposed to being part of
a larger argument), 8 of which were ARG1. Mor-
phology changes generally affect verb tense rather
than subject number, and with no real impact on
semantic structure.
Colloquial substitutions of verbs, such as ?he
was like...? ? ?he said...?, yield more unusual seman-
tic analysis on the unreconstructed side as non-
verbs were analyzed as verbs.
Reconstructive word re-orderings
Q: How is the predicate-argument labeling af-
fected? If reorderings occur as a phrase, what
type of phrase? Word reorderings labeled as
argument movements occurred 136 times in the
3,626 semantics-annotated SUs in the SSR corpus.
Of these, 81% were directly labeled as arguments
to some sentence-internal predicate. 52% of those
arguments were ARG1, 17%were ARG0, and 13%
were predicates. 11% were labeled as modifying
arguments rather than core arguments, which may
indicate confusion on the part of the annotators
and possibly necessary cleanup.
More commonly labeled than argument move-
ment was adjunct movement, assigned to 206
phrases. 54% of these reordered adjuncts were not
directly labeled as predicate arguments but were
within other labeled arguments. The most com-
monly labeled adjunct types were TMP (19% of all
arguments), ADV (13%), and LOC (11%).
Syntactically, 25% of reordered adjuncts were
assigned ADVP by the automatic parser, 19% were
assigned NP, 18% were labeled PP, and remaining
common NT assignments included IN, RB, and
SBAR.
Finally, 239 phrases were labeled as being re-
ordered for the general reason of fixing the gram-
mar, the default change assignment given by the
annotation tool when a word was moved. This
category was meant to encompass all movements
not included in the previous two categories (argu-
ments and adjuncts), including moving ?I guess?
from the middle or end of a sentence to the be-
ginning, determiner movement, etc. Semantically,
63% of nodes were directly labeled as predicates
or predicate arguments. 34% of these were PRED,
28% were ARG1, 27% were ARG0, 8% were
ARG2, and 8% were roughly evenly distributed
across the adjunct argument types.
Syntactically, 31% of these changes were NPs,
16% were ADVPs, and 14% were VBPs (24% were
verbs in general). The remaining 30% of changes
were divided amongst 19 syntactic categories from
CC to DT to PP.
4.4 Testing the generalizations required for
automatic SRL for speech
The results described in (Gildea and Palmer, 2002)
show that parsing dramatically helps during the
course of automatic SRL. We hypothesize that
the current state-of-art for parsing speech is ade-
quate to generally identify semantic roles in spon-
752
taneously produced speech text. For this to be true,
features for which SRL is currently dependent on
such as consistent predicate-to-parse paths within
automatic constituent parses must be found to ex-
ist in data such as the SSR corpus.
The predicate-argument path is defined as the
number of steps up and down a parse tree (and
through which NTs) which are taken to traverse
the tree from the predicate (verb) to its argument.
For example, the path from predicate VBP? ?like?
to the argument ARG0 (NP ? ?some kids?) might
be [VBP ? VP ? S ? NP]. As trees grow more
complex, as well as more errorful (as expected
for the automatic parses of verbatim speech text),
the paths seen are more sparsely observed (i.e. the
probability density is less concentrated at the most
common paths than similar paths seen in the Prop-
Bank annotations). We thus consider two path
simplifications as well:
? compressed: only the source, target, and root
nodes are preserved in the path (so the path
above becomes [VBP ? S ? NP])
? POS class clusters: rather than distinguish,
for example, between different tenses of
verbs in a path, we consider only the first let-
ter of each NT. Thus, clustering compressed
output, the new path from predicate to ARG0
becomes [V ? S ? N].
The top paths were similarly consistent regardless
of whether paths are extracted from Pra , Pvm , or
Pva (Pva results shown in Table 4), but we see that
the distributions of paths are much flatter (i.e. a
greater number and total relative frequency of path
types) going from manual to automatic parses and
from parses of verbatim to parses of reconstructed
strings.
5 Discussion
In this work, we sought to find generalizations
about the underlying structure of errorful and re-
constructed speech utterances, in the hopes of de-
termining semantic-based features which can be
incorporated into automatic systems identifying
semantic roles in speech text as well as statisti-
cal models for reconstruction itself. We analyzed
syntactic and semantic variation between original
and reconstructed utterances according to manu-
ally and automatically generated parses and man-
ually labeled semantic roles.
Argument Path from Predicate Freq
VBP ? VP ? S ? NP 4.9%
Predicate- VB ? VP ? VP ? S ? NP 3.9%
Argument VB ? VP ? NP 3.8%
Paths VBD ? VP ? S ? NP 2.8%
944 more path types 84.7%
VB ? S ? NP 7.3%
VB ? VP ? NP 5.8%
Compressed VBP ? S ? NP 5.3%
VBD ? S ? NP 3.5%
333 more path types 77.1%
V ? S ? N 25.8%
V ? V ? N 17.5%
POS class+ V ? V ? A 8.2%
compressed V ? V ? V 7.7%
60 more path types 40.8%
Table 4: Frequent Pva predicate-argument paths
Syntactic paths from predicates to arguments
were similar to those presented for WSJ data
(Palmer et al, 2005), though these patterns de-
graded when considered for automatically parsed
verbatim and errorful data. We believe that auto-
matic models may be trained, but if entirely depen-
dent on automatic parses of verbatim strings, an
SRL-labeled resource much bigger than the SSR
and perhaps even PropBank may be required.
6 Conclusions and future work
This work is an initial proof of concept that au-
tomatic semantic role labeling (SRL) of verbatim
speech text may be produced in the future. This is
supported by the similarity of common predicate-
argument paths between this data and the Prop-
Bank WSJ annotations (Palmer et al, 2005) and
the consistency of other features currently empha-
sized in automatic SRL work on clean text data.
To automatically semantically label speech tran-
scripts, however, is expected to require additional
annotated data beyond the 3k utterances annotated
for SRL included in the SSR corpus, though it may
be adequate for initial adaptation studies.
This new ground work using available corpora
to model speaker errors may lead to new intelli-
gent feature design for automatic systems for shal-
low semantic labeling and speech reconstruction.
Acknowledgments
Support for this work was provided by NSF PIRE
Grant No. OISE-0530118. Any opinions, find-
ings, conclusions, or recommendations expressed
in this material are those of the authors and do
not necessarily reflect the views of the supporting
agency.
753
References
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Compu-
tational Linguistics.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek.
2009. Reconstructing false start errors in sponta-
neous speech text. In Proceedings of the Annual
Meeting of the European Association for Computa-
tional Linguistics.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Mary Harper, Bonnie Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. Structural metadata and parsing
speech. Technical report, JHU Language Engineer-
ing Workshop.
Zellig S. Harris. 1957. Co-occurrence and transforma-
tion in linguistic structure. Language, 33:283?340.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106, March.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of the Human Language Technology Confer-
ence/North American chapter of the Association of
Computational Linguistics (HLT/NAACL), Boston,
MA.
Sameer Pradhan, James Martin, and Wayne Ward.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34(2):289?310.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
George K. Zipf. 1949. Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
754
Training Connectionist Models for the Structured Language Model  
Peng Xu, Ahmad Emami and Frederick Jelinek
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218

xp,emami,jelinek  @jhu.edu
Abstract
We investigate the performance of the
Structured Language Model (SLM) in
terms of perplexity (PPL) when its compo-
nents are modeled by connectionist mod-
els. The connectionist models use a dis-
tributed representation of the items in
the history and make much better use of
contexts than currently used interpolated
or back-off models, not only because of
the inherent capability of the connection-
ist model in fighting the data sparseness
problem, but also because of the sub-
linear growth in the model size when the
context length is increased. The connec-
tionist models can be further trained by an
EM procedure, similar to the previously
used procedure for training the SLM. Our
experiments show that the connectionist
models can significantly improve the PPL
over the interpolated and back-off mod-
els on the UPENN Treebank corpora, after
interpolating with a baseline trigram lan-
guage model. The EM training procedure
can improve the connectionist models fur-
ther, by using hidden events obtained by
the SLM parser.
1 Introduction
In many systems dealing with natural speech or lan-
guage such as Automatic Speech Recognition and

This work was supported by the National Science Founda-
tion under grants No.IIS-9982329 and No.IIS-0085940.
Statistical Machine Translation, a language model
is a crucial component for searching in the often
prohibitively large hypothesis space. Most of the
state-of-the-art systems use n-gram language mod-
els, which are simple and effective most of the
time. Many smoothing techniques that improve lan-
guage model probability estimation have been pro-
posed and studied in the n-gram literature (Chen and
Goodman, 1998).
Recent efforts have studied various ways of us-
ing information from a longer context span than that
usually captured by normal n-gram language mod-
els, as well as ways of using syntactical informa-
tion that is not available to the word-based n-gram
models (Chelba and Jelinek, 2000; Charniak, 2001;
Roark, 2001; Uystel et al, 2001). All these language
models are based on stochastic parsing techniques
that build up parse trees for the input word sequence
and condition the generation of words on syntactical
and lexical information available in the parse trees.
Since these language models capture useful hierar-
chical characteristics of language, they can improve
the PPL significantly for various tasks. Although
more improvement can be achieved by enriching the
syntactical dependencies in the structured language
model (SLM) (Xu et al, 2002), a severe data sparse-
ness problem was observed in (Xu et al, 2002) when
the number of conditioning features was increased.
There has been recent promising work in us-
ing distributional representation of words and neu-
ral networks for language modeling (Bengio et al,
2001) and parsing (Henderson, 2003). One great ad-
vantage of this approach is its ability to fight data
sparseness. The model size grows only sub-linearly
with the number of predicting features used. It has
been shown that this method improves significantly
on regular n-gram models in perplexity (Bengio et
al., 2001). The ability of the method to accommo-
date longer contexts is most appealing, since exper-
iments have shown consistent improvements in PPL
when the context of one of the components of the
SLM is increased in length (Emami et al, 2003).
Moreover, because the SLM provides an EM train-
ing procedure for its components, the connectionist
models can also be improved by the EM training.
In this paper, we will study the impact of neural
network modeling on the SLM, when all of its three
components are modeled with this approach. An EM
training procedure will be outlined and applied to
further training of the neural network models.
2 A Probabilistic Neural Network Model
Recently, a relatively new type of language model
has been introduced where words are represented
by points in a multi-dimensional feature space and
the probability of a sequence of words is computed
by means of a neural network. The neural network,
having the feature vectors of the preceding words as
its input, estimates the probability of the next word
(Bengio et al, 2001). The main idea behind this
model is to fight the curse of dimensionality by inter-
polating the seen sequences in the training data. The
generalization this model aims at is to assign to an
unseen word sequence a probability similar to that
of a seen word sequence whose words are similar to
those of the unseen word sequence. The similarity
is defined as being close in the multi-dimensional
space mentioned above.
In brief, this model can be described as follows.
A feature vector is associated with each token in the
input vocabulary, that is, the vocabulary of all the
items that can be used for conditioning. Then the
conditional probability of the next word is expressed
as a function of the input feature vectors by means
of a neural network. This probability is produced
for every possible next word from the output vocab-
ulary. In general, there does not need to be any rela-
tionship between the input and output vocabularies.
The feature vectors and the parameters of the neural
network are learned simultaneously during training.
The input to the neural network are the feature vec-
tors for all the inputs concatenated, and the output
is the conditional probability distribution over the
output vocabulary. The idea here is that the words
which are close to each other (close in the sense of
their role in predicting words to follow) would have
similar (close) feature vectors and since the proba-
bility function is a smooth function of these feature
values, a small change in the features should only
lead to a small change in the probability.
2.1 The Architecture of the Neural Network
Model
The conditional probability function
  
		
	
where

and  are from the
input and output vocabularies Random Forests in Language Modeling
Peng Xu and Frederick Jelinek
Center for Language and Speech Processing
the Johns Hopkins University
Baltimore, MD 21218, USA
 
xp,jelinek  @jhu.edu
Abstract
In this paper, we explore the use of Random Forests
(RFs) (Amit and Geman, 1997; Breiman, 2001) in
language modeling, the problem of predicting the
next word based on words already seen before. The
goal in this work is to develop a new language mod-
eling approach based on randomly grown Decision
Trees (DTs) and apply it to automatic speech recog-
nition. We study our RF approach in the context
of  -gram type language modeling. Unlike regu-
lar  -gram language models, RF language models
have the potential to generalize well to unseen data,
even when a complicated history is used. We show
that our RF language models are superior to regular
 -gram language models in reducing both the per-
plexity (PPL) and word error rate (WER) in a large
vocabulary speech recognition system.
1 Introduction
In many systems dealing with natural speech or lan-
guage, such as Automatic Speech Recognition and
Statistical Machine Translation, a language model
is a crucial component for searching in the often
prohibitively large hypothesis space. Most state-of-
the-art systems use  -gram language models, which
are simple and effective most of the time. Many
smoothing techniques that improve language model
probability estimation have been proposed and stud-
ied in the  -gram literature (Chen and Goodman,
1998). There has also been work in exploring Deci-
sion Tree (DT) language models (Bahl et al, 1989;
Potamianos and Jelinek, 1998), which attempt to
cluster similar histories together to achieve better
probability estimation. However, the results were
not promising (Potamianos and Jelinek, 1998): in
a fair comparison, decision tree language models
failed to improve upon the baseline  -gram models
with the same order  .
The aim of DT language models is to alleviate
the data sparseness problem encountered in  -gram
language models. However, the cause of the neg-
ative results is exactly the same: data sparseness,
coupled with the fact that the DT construction al-
gorithms decide on tree splits solely on the basis
of seen data (Potamianos and Jelinek, 1998). Al-
though various smoothing techniques were studied
in the context of DT language models, none of them
resulted in significant improvements over  -gram
models.
Recently, a neural network based language mod-
eling approach has been applied to trigram lan-
guage models to deal with the curse of dimension-
ality (Bengio et al, 2001; Schwenk and Gauvain,
2002). Significant improvements in both perplex-
ity (PPL) and word error rate (WER) over backoff
smoothing were reported after interpolating the neu-
ral network models with the baseline backoff mod-
els. However, the neural network models rely on
interpolation with  -gram models, and use  -gram
models exclusively for low frequency words. We
believe improvements in  -gram models should also
improve the performance of neural network models.
We propose a new Random Forest (RF) approach
for language modeling. The idea of using RFs for
language modeling comes from the recent success
of RFs in classification and regression (Amit and
Geman, 1997; Breiman, 2001; Ho, 1998). By defi-
nition, RFs are collections of Decision Trees (DTs)
that have been constructed randomly. Therefore, we
also propose a new DT language model which can
be randomized to construct RFs efficiently. Once
constructed, the RFs function as a randomized his-
tory clustering which can help in dealing with the
data sparseness problem. Although they do not per-
form well on unseen test data individually, the col-
lective contribution of all DTs makes the RFs gen-
eralize well to unseen data. We show that our RF
approach for  -gram language modeling can result
in a significant improvement in both PPL and WER
in a large vocabulary speech recognition system.
The paper is organized as follows: In Section 2,
we review the basics about language modeling and
smoothing. In Section 3, we briefly review DT
based language models and describe our new DT
and RF approach for language modeling. In Sec-
tion 4, we show the performance of our RF based
language models as measured by both PPL and
WER. After some discussion and analysis, we fi-
nally summarize our work and propose some future
directions in Section 5.
2 Basic Language Modeling
The purpose of a language model is to estimate the
probability of a word string. Let   denote a string
of  words, that is,   	
	 . Then,
by the chain rule of probability, we have
ACL Lifetime Achievement Award
The Dawn of Statistical ASR and MT
Frederick Jelinek?
Johns Hopkins University
I am very grateful for the award you have bestowed on me. To understand your
generosity I have to assume that you are honoring the leadership of three innovative
groups that I headed in the last 47 years: at Cornell, IBM, and now at Johns Hopkins.
You know my co-workers in the last two teams. The Cornell group was in Information
Theory and included Toby Berger, Terrence Fine, and Neil J. A. Sloane (earlier my Ph.D.
student), all of whom earned their own laurels.
I was told that I should give an acceptance speech and was furnished with example
texts by previous recipients. They wrote about the development and impact of their
ideas. So I will tell you about my beginnings and motivations and then focus on the
contributions of my IBM team. In this way the text will have some historical value and
may clear up certain widely held misconceptions.
1. Beginnings
Information Theory seemed to be one of the most prestigious disciplines during my
years as a student at MIT (1954?1962). The faculty included the founders of the field?
Shannon, Fano, Elias, and others. Some of my contemporaries were Viterbi, Jacobs,
Kleinrock (founders of Qualcom), Gallagher, Kailath, and Massey. Not daring to ap-
proach Shannon himself, I asked Professor Fano to be my thesis adviser. I was making
slow progress when in 1961, after three years of trying, I succeeded in extricating my
future wife Milena from communist Czechoslovakia (how this was accomplished is
another story) and married her. One problem we needed to solve was how she should
occupy herself during the long hours I was spending in the underground stacks of the
MIT library. At the time the famous linguist Roman Jacobson was simultaneously a
University Professor at Harvard and an Institute Professor at MIT. Russian by origin,
he spent 18 inter-war years (1920?1938) in Czechoslovakia, where he became one of
the founders of the Prague Linguistic Circle. He had a Czech wife, the anthropologist
Svatava Pirkova. He continued to maintain his connections with Czechs, and even
young Czechs. I was invited to dinner at his house several times, once also with the
newly arrived Milena. Jacobson was well known to have an eye for beautiful young
women and he was reputed to enjoy exercising his influence. When my wife asked him
for advice as to what to do, he suggested that she take up a fellowship at MIT which he
would arrange for her to get. As promised, she got the fellowship and enrolled in the
Ph.D. program of the Linguistics department. It should be appreciated that Jacobson
did not interview her in a non-social setting and was aware that her previous schooling
? Center for Language and Speech Processing, Johns Hopkins University, 320 Barton Hall, 3400 N. Charles
Street, Baltimore, MD 21218, USA. E-mail: jelinek@jhu.edu. This article is the text of the talk given on
receipt of the ACL?s Lifetime Achievement Award in 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
in Prague consisted only of one year of Slavic studies followed by two years at the Film
Academy.
So Milena started attending lectures, several of them taught by Noam Chomsky. I
sat in with her and got the crazy notion that I should switch from Information Theory to
Linguistics. I went so far as to explore this notion with Professor Chomsky. Of course,
word got around to my adviser Fano, whom it really upset. He declared that I could
contemplate switching only after I had received my doctorate in Information Theory. I
had no choice other than to obey. Soon thereafter, my thesis almost finished, I started
interviewing at universities for a faculty position. After my job talk at Cornell I was
approached by the eminent linguist Charles Hockett, who said that he hoped that I
would accept the Cornell offer and help develop his ideas on how to apply Information
Theory to Linguistics. That decided me. Surprisingly, when I took up my post in the fall
of 1962, there was no sign of Hockett. After several months I summoned my courage
and went to ask him when he wanted to start working with me. He answered that he
was no longer interested, that he now concentrated on composing operas.
Discouraged a second time, I devoted the next ten years to Information Theory. This
was the golden period of government support for science and technology. It seemed
easy to get grants. Perhaps that was the reason I neglected to make any arrangements
for work during the coming summer of 1972. I phoned Joe Raviv (whom I knew as a
colleague from my sabbatical at IBM in 1968?69) to ask if I could spend three months in
his group in Yorktown Heights. His answer was ?Certainly, the sooner you arrive the
better. We are starting to work on speech recognition.?
By the time I arrived to take up that summer job, Raviv was promoted to manager
of the IBM Scientific Center in Haifa, and IBM was negotiating with Professor Jonathan
Allen of MIT to take over the speech group. These negotiations were not successful, and
several weeks later the job was offered to me. I requested Cornell to grant me a leave of
absence; they did, and I joined IBM (the following year I asked for and got another year,
but when I tried to carry out the same maneuver in 1974, I was turned down).
Why did IBM start research in speech recognition? Believe it or not, IBM was
worried that, with the advance of computing power, there might soon come a time
when all the need for further improvements would disappear, and IBM business would
dry up. Somebody came up with the suggestion that speech recognition would require
lots of computing cycles. A task force was put together in 1971 to study the matter.
The group included John Cocke (inventor of RISC machines), Herman Goldstine (right
hand of von Neumann in research leading to ENIAC) and others. It recommended that
a Continuous Speech Recognition group be established in the Research Division.
So the CSR group was started in early 1972 under the management of Joe Raviv.
At the time IBM had a small speech group in one of its development laboratories in
Raleigh, NC. (Actually, IBM ?had? speech recognition even earlier. At the 1964 World?s
Fair in New York, Ernest Nassimbene demonstrated an isolated digits recognizer ?in
a shoe box.?) Its three main members, Das, Dixon, and Tappert, were transferred from
Raleigh to the Research Division in Yorktown. High management concluded that to get
going the speech group would need the help of linguists. It transferred temporarily
Fred Damerau, Stan Petrick, and Jane Robinson from linguistics to CSR. The staffing of
the group was then completed by volunteers from the Computer Sciences Department:
Lalit Bahl, Raimo Bakis, George Nagy, and others (later Jim and Janet Baker joined as
well). But at the time only Bakis, Das, Dixon, and Tappert knew anything about speech.
Towards the end of the summer I took over the direction of the group and received a gift
from heaven: the freshly graduated physicist Robert Mercer, who in the spring accepted
an IBM job in a group that was abolished before he arrived in September.
484
Jelinek The Dawn of Statistical ASR and MT
Table 1
Sentences from the Resource Management language.
Show locations and C-ratings for all deployed subs that were in their home ports April 5.
List the cruisers in Persian Sea that have casualty reports earlier than Jarrett?s oldest one.
How many ships were in Galveston May 3rd?
Is Puffer?s remaining fuel sufficient to arrive in port at the present speed?
How many long tuns is the average displacement of ships in Bering Strait?
2. The Competition
In 1971, parallel to the work of the IBM task force, ARPA established a project in Speech
Understanding. I don?t know what led to that decision, but the main forces behind it
were Allen Newell and J. C. R. Licklinder. Funds were provided to Carnegie Mellon,
Systems Development Corporation, Bolt Beranek & Newman, and probably SRI,
Sperry-Univac, University of Pennsylvania, UC Berkeley, and UCLA. Not all of these
institutions were to field complete systems. For instance, Ohio, UCLA, and Berkeley
provided consulting by linguists (Peter Ladefoged, Vicky Fromkin, John Ohala, Michael
O?Malley, and June Shoup).
Here are the names of some other researchers who attended the meetings organized
by the new project: Raj Reddy, Dennis Klatt, L. D. Erman, V. Lesser, Bruce Lowerry,
Bonnie Nash-Webber, George White, Fil Alleva, Wayne Ward, Don Walker, Victor Zue,
Stephanie Sennef, Bill Woods, John Makhoul, Wayne Lea, Beatrice Oshika, and Janet
and Jim Baker. IBM was invited to attend the meetings, but we did not compete. ARPA
was a six-year project which was supposed to recognize (and interpret?) sentences from
a ?Resource Management? grammar; for an example of the sentences generated, see
Table 1. At the end of the six-year period the project was declared a success because it
?met its goals.?
Clearly the best of the constructed recognizers was the Dragon System (Baker 1975)
implemented by the Bakers, graduate students at CMU. It used Hidden Markov models
(HMMs) whereas the rest of the ARPA participants based their work on templates,
Dynamic Time Warping (DTW), and hand-written rules. The best of these latter systems
was Harpy, developed by another CMU graduate student, Bruce Lowerry.
3. IBM?s Initial Formulation
For our first task we decided to recognize sentences generated by the so-called New
Raleigh grammar, a finite?state device whose schematic is shown in Figure 1. The
grammar is actually a Hidden Markov Model. State transitions generate words and
are taken with uniform probability. Generation starts in the initial state, a transition is
taken, and a word from the list associated with that transition is selected with uniform
probability; then one of the transitions out of the new state is taken (again with uniform
probability), a word corresponding to that transition is again selected at random, a new
state is reached, and so on. The process continues until one arrives at the final state. The
grammar generates such bizarre sentences as are shown in Table 2.1
1 Note that, from a syntactic point of view, all the sentences generated are structurally English, even
though the vast majority make no sense.
485
Computational Linguistics Volume 35, Number 4
F
ig
u
re
1
T
h
e
N
e
w
R
a
le
ig
h
g
ra
m
m
a
r.
486
Jelinek The Dawn of Statistical ASR and MT
Table 2
Sentences generated by the New Raleigh grammar.
Each distant town disturbed the telephone during the purpose.
Some matters survive never between those systems.
Should important grounds be the radio over the concern?
Some matters rejected the radio during the capitol.
One recognition condition proposes only across those engineers.
Figure 2
Schematic of statistical speech recognition.
The generated sentences were read (as naturally as possible) by native American
speakers and were recorded in a special sound-proofed room. The recorded speech
signal was transformed into a string A of symbols (one symbol per centisecond)
chosen from an ?alphabet? A of size 200. The alphabet A itself was extracted by vector
quantization from a training sample of the speech signal.
Our formulation of the combined generation/recognition process is seen in Fig-
ure 2. The entire operation can be regarded as transmission through a noisy channel,
the basic problem of Information Theory.2 Its corresponding mathematical formula is
?W = arg max
W?S
P(W|A) = arg max
W?S
P(A|W)P(W)
where W denotes a string of words, A denotes the acoustic signal observed by the
recognizer, and S is the set of sentences that can be generated by the grammar. The
formula calls for a statistical approach. The noisy channel of Figure 2 has input W and
output A and is characterized by the probability P(A|W). The designer of the recognizer
must attempt to estimate the channel probabilities P(A|W) and the a priori probability
P(W) that the speaker will utter the word string W. Once the models ?P(A|W) and ?P(W)3
2 No wonder that we immediately hit upon the diagram: Both Lalit Bahl and I wrote Information Theoretic
Ph.D. theses.
3 ?P(A|W) is called the acoustic model and ?P(W) is called the language model. Both these terms, now used
throughout the field, were ?invented? at IBM, as was the term perplexity, denoting an Information
Theoretic measure of the difficulty of the recognition task.
487
Computational Linguistics Volume 35, Number 4
are established, the recognizer when it observes A can conduct a search leading to the
maximizing word string ?W.
In the case of an artificial grammar, such as the New Raleigh grammar, the model
?P(W) is in fact equal to the actual probability P(W). Furthermore, because the set S of
word strings W over which we maximize can be listed, the difficulty of the task can
be measured approximately (because the acoustic similarity of words is not taken into
account) by entropy:
H(W)  ?
?
WS
P(W) log P(W)
However, because the participants in the ARPA project introduced the false measure
?branching factor? (which was the arithmetic mean of the out-of-state branching of
their finite?state grammar), we replaced H(W) as a measure of difficulty by perplexity,
defined by
PP  2H(W)
It turned out that the New Raleigh grammar had approximate perplexity 7 whereas the
Resource Management grammar had 2.
The early IBM approach was described in three papers: Jelinek, Bahl, and Mercer
(1975), Bahl and Jelinek (1975), and Jelinek (1976).
4. The Tangora
In 1978 (or so), we thought it was time to abandon artificial grammars and to start
recognizing ?natural? speech. We settled on a 5,000-word vocabulary and set ourselves
the task of recognizing read sentences from an IBM internal correspondence corpus.
Our ambition was to use a combination of IBM array processors to achieve essentially
real-time performance. We fulfilled a promise to the management to achieve it by 1984.
To make things easier on ourselves we limited the recognizer to the transcription of
discrete speech, where sentences were spoken with pauses between words. We got rid
of the sound room and recorded the readings on close-talking microphones. The system
was to be speaker-sensitive, that is, acoustic models were trained for each individual
reader separately.
It is worth noting that during this time we persuaded the IBM management to hire
Jim and Janet Baker, who were not to receive their Ph.D.s from Carnegie Mellon until
a year later. At the same time, ?our? three linguist helpers returned to their original
group.
When handling natural speech, the main question was how to estimate the language
model ?P(W). There was no simple way of achieving this. We thought that the right
approach ought to be somehow related to English grammar. The linguist Stan Petrick,
while he still was with us, said ?Don?t worry, I will just make a little grammar.? Of
course he never did, and the phrase acquired a mythical status in the manner of ?famous
last words.?
488
Jelinek The Dawn of Statistical ASR and MT
So at John Cocke?s suggestion we decided to model English by trigrams. That is, we
used the approximation
?P(W) = P(w1,w2,w3...wk) ?
?P(w1,w2) ?P(w3|w1,w2) ?P(w4|w2,w3)....?P(wk|wk?2,wk?1)
But this decision did not dispose of the problem. How should we estimate the basic
building blocks P(wk|wk?2,wk?1)? It was clear that the estimate would have to be based
on trigram counts C(wk?2,wk?1,wk) assembled from some appropriate text (the one that
provided us with the read speech). But relative frequencies
f (wk|wk?2,wk?1) 
C(wk?2,wk?1,wk)
C(wk?2,wk?1)
would not suffice. Indeed, the speech would frequently involve trigrams wk?2,wk?1,wk
whose count C(wk?2,wk?1,wk) in the training corpus equalled 0. Then if wk?2,wk?1,wk
were uttered by the speaker (reader), the recognizer would necessarily make an
error. What was required was smoothing, and the type we chose at first was linear
interpolation:
?P(wk|wk?2,wk?1) = ?3 f (wk|wk?2,wk?1) + ?2 f (wk|wk?1) + ?1 f (wk)
where ??j s would be non-negative, would satisfy ?3 + ?2 + ?1 = 1, and would be
optimally chosen (we knew how).
Just as in the initial phase of IBM CSR research, the acoustics A input to the
recognizer were a string of centisecond symbols chosen from an alphabet A which
itself was derived by vector quantization. This discretization of speech allowed for an
easier estimate of parameters defining the acoustic processor model ?P(A|W). The latter
was implemented as a concatenation of HMMs, one for each word of the string W.
An example of such an HMM (itself a concatenation of HMMs as prescribed by the
pronunciation lexicon) is shown in Figure 3.
Having defined the signal processing leading to the string A, and the structures of
the acoustic and language models ?P(A|W) and ?P(W), it remains to discuss the search
for the recognizer output ?W implicit in the formula
?W = arg max
W
?P(A|W) ?P(W)
Figure 3
HMM for the word v realized by phone sequence ?1?2 ? ? ??lv .
489
Computational Linguistics Volume 35, Number 4
We used the appropriately modified Viterbi algorithm version of dynamic program-
ming (Viterbi 1967), a decision we took in spite of the fact that the algorithm would
carry out a sub-optimal search.
We called the system Tangora after Albert Tangora who, during a 1923 business
show in New York, ran off a total of 8,840 correctly spelled words in one hour of
nonstop typing, a rate of 147 words per minute. Incredibly, it was estimated that Tangora
executed an average of twelve-and-a-half strokes per second!
5. Some ASR Firsts
During my time at IBM, my colleagues and I pioneered various techniques that were
later taken over by the entire field. Here are some examples:
 We replaced the discrete signal processing leading to A by directly
modeling the real input as a mixture of Gaussians. Also, the resulting
HMM now generated outputs from states instead of from transitions.
 We introduced pronunciation modeling by triphones, thereby modeling
phones as influenced by context. That is, in our pronunciation lexicon each
word was first transformed into a string of symbols roughly
corresponding to phones. The HMM model of a word then became a
concatenation of triphone models. Here is an example:
table =? T EI B L =? # T EI T EI B EI B L B L #
 The phone alphabet was of the order of 50. That would necessitate
503 = 125, 000 models (one for each triphone), an intolerable number to
use and/or estimate parameters for. To reduce this number meant
categorizing the context into equivalence classes. This was accomplished
by clustering carried out by decision trees.
 We tried to move from the simple trigram language model to a more
sophisticated one. Slava Katz designed a back-off model (Katz 1987)
relying on Good-Turing probability estimation. It was an improvement
which moved the state of the art forward, but it was later superseded by
Kneser-Ney smoothing. Because language modeling is a search for
equivalence classification, we tried several methods based on decision
trees. These were an improvement, but the attendant complication in
implementation was not deemed justified, particularly now that enormous
amounts of text are available from the Internet.
 Aware that a Viterbi search for the maximizing word string ?W is not
optimal, Lalit Bahl developed a multi-stack search algorithm inspired by
the stack algorithm of Information Theory (Jelinek 1969). The procedure
was later formalized and cleaned up by Doug Paul of Lincoln Laboratories
(Paul 1992). The multi-stack algorithm required the invention of a Fast
Acoustic Match which suggested word candidates for detailed
examination on the basis of a parallel, crude acoustic-fit test.
 Many problems of probability estimation involve simultaneous
consideration of influence by disparate phenomena. We found that
Maximum Entropy estimation is a method suitable for treating the
490
Jelinek The Dawn of Statistical ASR and MT
situation. This method facilitated the development by the summer intern
Ronnie Rosenfeld of a Trigger Language Model that allowed the discourse
topic to influence word prediction (Rosenfeld 1996).
 It was hoped that language modeling could be strengthened by including
consideration of the parts-of-speech of words preceding the prediction.
This unfortunately did not turn out to be the case. Nevertheless, Bahl and
Mercer pioneered a method of part-of-speech tagging by HMM (Bahl and
Mercer 1976) that is now widely used in various applications of Natural
Language Processing.
6. IBM Influence on the Speech and Language Field
The success of the statistical formulation of the speech recognition problem led to
invitations to share our methods with a wider audience. We presented several courses
teaching our data-centric approach. In 1980 we gave a course in Udine, Italy, organized
by CISM (International Centre for Mechanical Sciences); in 1983 a two-week course
at MIT; and finally in 1986 a course in Oberlech, Austria, organized by IBM Scientific
Centers. Furthermore, I was invited to give a keynote speech at the 1990 ACL Meeting
in Pittsburgh, PA.
We were not satisfied with the crude n-gram language model we were using and
were ?sure? that an appropriate grammatical approach would be better. Because we
wanted to stick to our data-centric philosophy, we thought that what was needed as
training material was a large collection of parses of English sentences. We found out
that researchers at the University of Lancaster had hand-constructed a ?treebank?
under the guidance of Professors Geoff Leach and Geoff Sampson (Garside, Leech,
and Sampson 1987). Because we wanted more of this annotation, we commissioned
Lancaster in 1987 to create a treebank for us. Our view was that what we needed
above all was quantity, possibly at some expense of quality: We wanted to extract the
grammatical language model statistically and so a large amount of data was required.
Another belief of ours was that the parse annotation should be carried out by intelligent
native speakers of English, not by linguistic experts, and that any inaccuracies would
naturally cancel each other. Indeed, the work was done by Lancaster housewives led
by a high school drop-out.
Meanwhile, Geoff Leech set out to assemble the British National Corpus and we
thought that the United States should have something like that as well. So in 1987
I arranged to visit Jack Schwartz, who was the boss of Charles Wayne at DARPA,
and I explained to him what was needed and why. He immediately entrusted Charles
with the creation of the appropriate organization. One of the problems was where the
eventual corpus should reside. Deep-pocketed IBM would be unsuitable: Possessors of
desirable corpora would charge immoderate sums for the acquisition of rights. I thought
that only a university would do. So I inquired of Aravind Joshi and Mitch Marcus (and
perhaps even Mark Liberman) at the 1988 Conference of Applied Natural Language
Processing in Austin whether the required site could be the University of Pennsylvania.
My colleagues were interested, and Charles Wayne invited appropriate people to a
meeting at the Lake Mohunk Mountain House to discuss the matter. That is how the
Linguistic Data Consortium was born.
Once the commissioned treebank was delivered, we started to experiment with
parsers. Jointly with the University of Pennsylvania we applied for and received an
491
Computational Linguistics Volume 35, Number 4
Figure 4
Schematic of statistical machine translation.
NSF grant for grammatical development. The eventual result was SPATTER, imple-
mented by David Magerman. It used statistics, decision trees, and a history-based
operation. The University of Pennsylvania graduate student Eric Brill developed his
transformation-based learning (Brill 1992) that he applied to part-of-speech tagging
and grammar derivation. Ezra Black of IBM organized a committee whose aim was
the specification of a metric suitable for evaluation of parser performance. The result
was the PARSEVAL measure.
7. Machine Translation
Even though the problem of speech recognition remains unsolved to this day, some
of us started to wonder in the mid 1980s whether our ASR methods could be success-
fully applied to new fields. Bob Mercer and I spent many of our after-lunch ?periphery?
walks discussing possible candidates. We soon came up with two: machine transla-
tion and stock market modeling. It is probably only coincidence that Bob eventually
ended up investigating the possibilities of stock value prediction. Indeed, he and Peter
Brown departed IBM in 1993 to work for the phenomenally successful hedge fund
Renaissance Technologies. Eventually at least 10 former members of the IBM CSR group
were to be employed by that same company. The performance of the Renaissance fund
is legendary, but I have no idea whether any methods we pioneered at IBM have
ever been used. My former colleagues will not tell me: Theirs is a very hush-hush
operation!
On the other hand, we did start working on machine translation (MT) in 1987. As
expected, we formulated the problem statistically. The basic diagram of MT, shown
in Figure 4, is practically identical to that of ASR. In fact, even the basic formulas are
identical except for a change in the letters that designate the variables:
?E = arg max
E
P(F|E)P(E)
This formula assumes translation from the foreign language4 F into English E. The
formulation pretends that the speaker?s mind works in English, the generated text
is then translated by him into the foreign language F, and the task of the machine
translator is to ferret out the speaker?s original thought ?E.
That we wanted to translate into English was a given?not because of the utility
of the task, but because our knowledge of English would allow us to judge the quality
4 Originally F stood for French.
492
Jelinek The Dawn of Statistical ASR and MT
of the translation. We wanted to make the problem real, yet as easy as possible. So we
looked for a language F that was relatively close to English. The answer was French.
Because we wanted the process to be data-centric, we searched for a pair of corpora F
and E that would be translations of each other. Luck was with us: The Canadian par-
liament Hansards (proceedings) were maintained in English and French.5 So statistical
language translation was born (Brown et al 1990), and the descendants of our original
methods are being continually improved.
When we started, none of us spoke French, so we decided to learn it. We uncovered
a small institute whose location was opposite New York Grand Central Station on
42nd Street. The institute advertised that it would teach French to anybody in two (!)
weeks of intensive immersion. We didn?t believe it, of course, but because the costs and
location were convenient, we started on our daily commute. I will not go into the semi-
fraudulent aspects of the operation, but Lalit Bahl, Peter Brown, Bob Mercer, and I had
a lot of fun and did advance considerably our knowledge of French.
When we had our first results we submitted them to Coling 1988. Here is a part of
the rejection review we received:
The validity of a statistical (information theoretic) approach to MT has indeed been
recognized, as the authors mention, by Weaver as early as 1949. And was universally
recognized as mistaken by 1950 (cf. Hutchins, MT ? Past, Present, Future, Ellis
Horwood, 1986, p. 30ff and references therein). The crude force of computers is not
science. The paper is simply beyond the scope of COLING.
Anonymous Coling review, 1 March 1988
8. Conclusion
Research in both ASR and MT continues. The statistical approach is clearly dominant.
The knowledge of linguists is added wherever it fits. And although we have made
significant progress, we are very far from solving the problems. That is a good thing:
We can continue accepting new students into our field without any worry that they will
have to search, in the middle of their careers, for new fields of action.
References
Bahl, L. R. and F. Jelinek. 1975. Decoding
for channels with insertions, deletions,
and substitutions with applications
to speech recognition. IEEE Transactions
on Information Theory, IT-21:404?411.
Bahl, L. R. and R. L. Mercer. 1976. Part
of speech assignment by a statistical
algorithm. In IEEE International Symposium
on Information Theory, pp. 88?89,
Ronneby, June.
Baker, J. K. 1975. The dragon system: an
overview. IEEE Transactions on Acoustics,
Speech, and Signal Processing,
ASSP-23(1):24?29.
Brill, E. 1992. A simple rule-based part
of speech tagger. In Proceedings of the
Third Conference on Applied Natural
Language Processing, pages 152?155,
Trento, Italy. ACL.
Brown, P. F., J. Cocke, S. A. DellaPietra,
V. DellaPietra, F. Jelinek, J. D. Lafferty,
R. L. Mercer, and P. Roosin. 1990.
A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Garside, R., G. Leech, and G. Sampson.
1987. Computational Analysis of English:
A Corpus-Based Approach. Longman,
London.
5 Every sentence spoken by a deputy in one language was faithfully translated into the other.
493
Computational Linguistics Volume 35, Number 4
Jelinek, F. 1969. A fast sequential decoding
algorithm using a stack. IBM Journal of
Research and Development, 13:675?685.
Jelinek, F. 1976. Continuous speech
recognition by statistical methods. IEEE
Proceedings, 64(4):532?556.
Jelinek, F., L. R. Bahl, and R. L. Mercer. 1975.
Design of a linguistic statistical decoder
for the recognition of continuous speech.
IEEE Transactions on Information Theory,
IT-21:250?256.
Katz, S. 1987. Estimation of probabilities
from sparse data for the language model
component of a speech recognizer. IEEE
Transactions on Acoustics, Speech, and Signal
Processing, ASSP-35(3):400?401.
Paul, D. B. 1992. An essential a? stack
decoder algorithm for continuous
speech recognition with a stochastic
language model. In Proceedings of the
1992 International Conference on Acoustics,
Speech, and Signal Processing, pages 25?28,
San Francisco.
Rosenfeld, R. 1996. A maximum entropy
approach to adaptive statistical language
modeling. Computer Speech and Language,
10(3):187?228.
Viterbi, A. J. 1967. Error bounds for
convolutional codes and an
asymmetrically optimum decoding
algorithm. IEEE Transactions on Information
Theory, IT-13:260?267.
494
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190?197,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Model Adaptation using Information-Theoretic Criterion
Ariya Rastrow1, Frederick Jelinek1, Abhinav Sethy2 and Bhuvana Ramabhadran2
1Human Language Technology Center of Excellence, and
Center for Language and Speech Processing, Johns Hopkins University
{ariya, jelinek}@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
{asethy, bhuvana}@us.ibm.com
Abstract
In this paper we propose a novel general
framework for unsupervised model adapta-
tion. Our method is based on entropy which
has been used previously as a regularizer in
semi-supervised learning. This technique in-
cludes another term which measures the sta-
bility of posteriors w.r.t model parameters, in
addition to conditional entropy. The idea is to
use parameters which result in both low con-
ditional entropy and also stable decision rules.
As an application, we demonstrate how this
framework can be used for adjusting language
model interpolation weight for speech recog-
nition task to adapt from Broadcast news data
to MIT lecture data. We show how the new
technique can obtain comparable performance
to completely supervised estimation of inter-
polation parameters.
1 Introduction
All statistical and machine learning techniques for
classification, in principle, work under the assump-
tion that
1. A reasonable amount of training data is avail-
able.
2. Training data and test data are drawn from the
same underlying distribution.
In fact, the success of statistical models is cru-
cially dependent on training data. Unfortunately,
the latter assumption is not fulfilled in many appli-
cations. Therefore, model adaptation is necessary
when training data is not matched (not drawn from
same distribution) with test data. It is often the case
where we have plenty of labeled data for one specific
domain/genre (source domain) and little amount of
labeled data (or no labeled data at all) for the de-
sired domain/genre (target domain). Model adapta-
tion techniques are commonly used to address this
problem. Model adaptation starts with trained mod-
els (trained on source domain with rich amount of la-
beled data) and then modify them using the available
labeled data from target domain (or instead unla-
beled data). A survey on different methods of model
adaptation can be found in (Jiang, 2008).
Information regularization framework has been
previously proposed in literature to control the la-
bel conditional probabilities via input distribution
(Szummer and Jaakkola, 2003). The idea is that la-
bels should not change too much in dense regions
of the input distribution. The authors use the mu-
tual information between input features and labels as
a measure of label complexity. Another framework
previously suggested is to use label entropy (condi-
tional entropy) on unlabeled data as a regularizer to
Maximum Likelihood (ML) training on labeled data
(Grandvalet and Bengio, 2004).
Availability of resources for the target domain cat-
egorizes these techniques into either supervised or
unsupervised. In this paper we propose a general
framework for unsupervised adaptation using Shan-
non entropy and stability of entropy. The assump-
tion is that in-domain and out-of-domain distribu-
tions are not too different such that one can improve
the performance of initial models on in-domain data
by little adjustment of initial decision boundaries
(learned on out-of-domain data).
190
2 Conditional Entropy based Adaptation
In this section, conditional entropy and its relation
to classifier performance are first described. Next,
we introduce our proposed objective function for do-
main adaptation.
2.1 Conditional Entropy
Considering the classification problem whereX and
Y are the input features and the corresponding class
labels respectively, the conditional entropy is a mea-
sure of the class overlap and is calculated as follows
H(Y|X) = EX[H(Y|X = x)] =
?
?
p(x)
(
?
y
p(y|x) log p(y|x)
)
dx (1)
Through Fano?s Inequality theorem, one can see
how conditional entropy is related to classification
performance.
Theorem 1 (Fano?s Inequality) Suppose
Pe = P{Y? 6= Y} where Y? = g(X) are the
assigned labels for the data points, based on the
classification rule. Then
Pe ?
H(Y|X)? 1
log(|Y| ? 1)
where Y is the number of possible classes and
H(Y |X) is the conditional entropy with respect to
true distibution.
The proof to this theorem can be found in (Cover and
Thomas, 2006). This inequality indicates thatY can
be estimated with low probability of error only if the
conditional entropy H(Y|X) is small.
Although the above theorem is useful in a sense
that it connects the classification problem to Shan-
non entropy, the true distributions are almost never
known to us1. In most classification methods, a spe-
cific model structure for the distributions is assumed
and the task is to estimate the model parameters
within the assumed model space. Given the model
1In fact, Theorem 1 shows how relevant the input features
are for the classification task by putting a lower bound on the
best possible classifier performance. As the overlap between
features from different classes increases, conditional entropy in-
creases as well, thus lowering the performance of the best pos-
sible classifier.
structure and parameters, one can modify Fano?s In-
equality as follows,
Corollary 1
Pe(?) = P{Y? 6= Y |?} ? H?(Y|X)? 1log(|Y| ? 1)
(2)
where Pe(?) is the classifier probability of error
given model parameters, ? and
H?(Y|X) =
?
?
p(x)
(
?
y
p?(y|x) log p?(y|x)
)
dx
Here, H?(Y|X) is the conditional entropy imposed
by model parameters.
Eqn. 2 indicates the fact that models with low
conditional entropy are preferable. However, a low
entropy model does not necessarily have good per-
formance (this will be reviewed later on) 2
2.2 Objective Function
Minimization of conditional entropy as a framework
in the classification task is not a new concept and
has been tried by researchers. In fact, (Grandvalet
and Bengio, 2004) use this along with the maxi-
mum likelihood criterion in a semi-supervised set
up such that parameters with both maximum like-
lihood on labeled data and minimum conditional en-
tropy on unlabeled data are chosen. By minimiz-
ing the entropy, the method assumes a prior which
prefers minimal class overlap. Entropy minimiza-
tion is used in (Li et al, 2004) as an unsupervised
non-parametric clustering method and is shown to
result in significant improvement over k-mean, hier-
archical clustering and etc.
These methods are all based on the fact that mod-
els with low conditional entropy have their decision
boundaries passing through low-density regions of
the input distribution, P (X). This is consistent with
the assumption that classes are well separated so that
one can expect to take advantage of unlabeled exam-
ples (Grandvalet and Bengio, 2004).
In many cases shifting from one domain to an-
other domain, initial trained decision boundaries (on
2Imagine a model which classifies any input as class 1.
Clearly for this model H?(Y|X) = 0.
191
out-of-domain data) result in high conditional en-
tropy for the new domain, due to mismatch be-
tween distributions. Therefore, there is a need to
adjust model parameters such that decision bound-
aries goes through low-density regions of the distri-
bution. This motivates the idea of using minimum
conditional entropy criterion for adapting to a new
domain. At the same time, two domains are often
close enough that one would expect that the optimal
parameters for the new domain should not deviate
too much from initial parameters. In order to formu-
late the technique mentioned in the above paragraph,
let us define ?init to be the initial model parame-
ters estimated on out-of-domain data (using labeled
data). Assuming the availability of enough amount
of unlabeled data for in-domain task, we try to min-
imize the following objective function w.r.t the pa-
rameters,
?new = argmin
?
H?(Y|X) + ? ||? ? ?init||p
(3)
where ||? ? ?init||p is an Lp regularizer and tries to
prevent parameters from deviating too much from
their initial values3.
Once again the idea here is to adjust the param-
eters (using unlabeled data) such that low-density
separation between the classes is achieved. In the
following section we will discuss the drawback of
this objective function for adaptation in realistic sce-
narios.
3 Issues with Minimum Entropy Criterion
It is discussed in Section 2.2 that the model param-
eters are adapted such that a minimum conditional
entropy is achieved. It was also discussed how this is
related to finding decision boundaries through low-
density regions of input distribution. However, the
obvious assumption here is that the classes are well
separated and there in fact exists low-density regions
between classes which can be treated as boundaries.
Although this is a suitable/ideal assumption for clas-
sification, in most practical problems this assump-
tion is not satisfied and often classes overlap. There-
fore, we can not expect the conditional entropy to be
3The other reason for using a regularizer is to prevent trivial
solutions of minimum entropy criterion
convex in this situation and to achieve minimization
w.r.t parameters (other than the trivial solutions).
Let us clarify this through an example. Consider
X to be generated by mixture of two 2-D Gaus-
sians (each with a particular mean and covariance
matrix) where each Gaussian corresponds to a par-
ticular class ( binary class situation) . Also in order
to have linear decision boundaries, let the Gaussians
have same covariance matrix and let the parameter
being estimated be the prior for class 1, P (Y = 1).
Fig. 1 shows two different situations with over-
lapping classes and non-overlapping classes. The
left panel shows a distribution in which classes are
well separated whereas the right panel corresponds
to the situation where there is considerable overlap
between classes. Clearly, in the later case there is
no low-density region separating the classes. There-
fore, as we change the parameter (here, the prior on
the class Y = 1), there will not be any well defined
point with minimum entropy. This can be seen from
Fig. 2 where model conditional entropy is plotted
vs. class prior parameter for both cases. In the case
of no-overlap between classes, entropy is a convex
function w.r.t the parameter (excluding trivial solu-
tions which happens at P (Y = 1) = 0, 1) and is
minimum at P (Y = 1) = 0.7 which is the true prior
with which the data was generated.
We summarize issues with minimum entropy cri-
terion and our proposed solutions as follows:
? Trivial solution: this happens when we put de-
cision boundaries such that both classes are
considered as one class (this can be avoided us-
ing the regularizer in Eqn. 3 and the assump-
tion that initial models have a reasonable solu-
tion, e.g. close to the optimal solution for new
domain )
? Overlapped Classes: As it was discussed in
this section, if the overlap is considerable then
the entropy will not be convex w.r.t to model
parameters. We will address this issue in
the next section by introducing the entropy-
stability concept.
4 Entropy-Stability
It was discussed in the previous section that a mini-
mum entropy criterion can not be used (by itself) in
192
?3 ?2 ?1 0 1 2 3 4 5 6 7?4
?2
0
2
4
6
8
10
X1
X 2
?3 ?2 ?1 0 1 2 3 4 5 6 7?3
?2
?1
0
1
2
3
4
5
6
7
X1
X 2
Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap
(right) with class overlap
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0	 ?
0.005	 ?
0.01	 ?
0.015	 ?
0.02	 ?
0.025	 ?
0.03	 ?
0.035	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ? 0.6	 ? 0.7	 ? 0.8	 ? 0.9	 ? 1	 ?
Con
di?n
al	 ?E
ntro
py	 ?
P(Y=1)	 ?
without	 ?overlap	 ?
with	 ?overlap	 ?
Figure 2: Condtional entropy vs. prior parameter, P (Y =
1)
situations where there is a considerable amount of
overlap among classes. Assuming that class bound-
aries happen in the regions close to the tail of class
distributions, we introduce the concept of Entropy-
Stability and show how it can be used to detect
boundary regions. Define Entropy-Stability to be the
reciprocal of the following
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
?
?
p(x)
?
(?
y p?(y|x) log p?(y|x)
)
?? dx
?
?
?
?
?
?
?
?
?
?
?
?
p
(4)
Recall: since ? is a vector of parameters, ?H?(Y|X)??
will be a vector and by using Lp norm Entropy-
stability will be a scalar.
The introduced concept basically measures the
stability of label entropies w.r.t the model parame-
ters. The idea is that we prefer models which not
only have low-conditional entropy but also have sta-
ble decision rules imposed by the model. Next, we
show through the following theorem how Entropy-
Stability measures the stability over posterior prob-
abilities (decision rules) of the model.
Theorem 2
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
p(x)
(
?
y
?p?(y|x)
?? log p?(y|x)
)
dx
?
?
?
?
?
?
?
?
?
?
p
where the term inside the parenthesis is the weighted
sum (by log-likelihood) over the gradient of poste-
rior probabilities of labels for a given sample x
Proof The proof is straight forward and uses the fact
that
? ?p?(y|x)
?? = ?(
P
p?(y|x))
?? = 0 .
Using Theorem 2 and Eqn. 4, it should be clear
how Entropy-Stability measures the expected sta-
bility over the posterior probabilities of the model.
A high value of
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
implies models with
less stable decision rules. In order to explain how
this is used for detecting boundaries (overlapped
193
regions) we once again refer back to our mixture
of Gaussians? example. As the decision boundary
moves from class specific regions to overlapped re-
gions (by changing the parameter which is here class
prior probability) we expect the entropy to continu-
ously decrease (due to the assumption that the over-
laps occur at the tail of class distributions). How-
ever, as we get close to the overlapping regions the
added data points from other class(es) will resist
changes in the entropy. resulting in stability over the
entropy until we enter the regions specific to other
class(es).
In the following subsection we use this idea to
propose a new objective function which can be used
as an unsupervised adaptation method even for the
case of input distribution with overlapping classes.
4.1 Better Objective Function
The idea here is to use the Entropy-Stability con-
cept to accept only regions which are close to the
overlapped parts of the distribution (based on our
assumption, these are valid regions for decision
boundaries) and then using the minimum entropy
criterion we find optimum solutions for our parame-
ters inside these regions. Therefore, we modify Eqn.
3 such that it also includes the Entropy-Stability
term
?new = argmin
?
(
H?(Y|X) + ?
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p?
+ ? ||? ? ?init||p
)
(5)
The parameter ? and ? can be tuned using small
amount of labeled data (Dev set).
5 Speech Recognition Task
In this section we will discuss how the proposed
framework can be used in a speech recognition task.
In the speech recognition task, Y is the sequence
of words and X is the input speech signal. For a
given speech signal, almost every word sequence is
a possible output and therefore there is a need for
a compact representation of output labels (words).
For this, word graphs (Lattices) are generated dur-
ing the recognition process. In fact, each lattice is
an acyclic directed graph whose nodes correspond
to particular instants of time, and arcs (edges con-
necting nodes) represent possible word hypotheses.
Associated with each arc is an acoustic likelihood
and language model likelihood scores. Fig. 3 shows
an example of recognition lattice 4 (for the purpose
of demonstration likelihood scores are not shown).L. Mangu et al: Finding Consensus in Speech Recognition 6
(a) Input lattice (?SIL? marks pauses)
SIL
SIL
SIL
SIL
SIL
SIL
VEAL
VERY
HAVE
MOVE
HAVE
HAVE
IT
MOVE
HAVE IT
VERY
VERY
VERY
VERY
OFTEN
OFTEN
FINE
FINE
FAST
I
I
I
(b) Multiple alignment (?-? marks deletions)
- -
I
MOVE
HAVE IT VEAL 
VERY
FINE
OFTEN
FAST
Figure 1: Sample recognition lattice and corresponding multiple alignment represented as
confusion network.
alignment (which gives rise to the standard string edit distance WE (W,R)) with
a modified, multiple string alignment. The new approach incorporates all lattice
hypotheses7 into a single alignment, and word error between any two hypotheses
is then computed according to that one alignment. The multiple alignment thus
defines a new string edit distance, which we will call MWE (W,R). While the
new alignment may in some cases overestimate the word error between two
hypotheses, as we will show in Section 5 it gives very similar results in practice.
The main benefit of the multiple alignment is that it allows us to extract
the hypothesis with the smallest expected (modified) word error very efficiently.
To see this, consider an example. Figure 1 shows a word lattice and the corre-
sponding hypothesis alignment. Each word hypothesis is mapped to a position
in the alignment (with deletions marked by ?-?). The alignment also supports
the computation of word posterior probabilities. The posterior probability of a
word hypothesis is the sum of the posterior probabilities of all lattice paths of
which the word is a part. Given an alignment and posterior probabilities, it is
easy to see that the hypothesis with the lowest expected word error is obtained
by picking the word with the highest posterior at each position in the alignment.
We call this the consensus hypothesis.
7In practice we apply some pruning of the lattice to remove low probability word hypotheses
(see Section 3.4).
Figure 3: Lattice Example
Since lattices contain all the likely hypotheses
(unlikely hypotheses are pruned during recognition
and will not be included in the lattice), conditional
entropy for any given input speech signal, x, can be
approximated by the conditional entropy of the lat-
tice. That is,
H?(Y|X = xi) = H?(Y|Li)
whereLi is the corresponding decoded lattice (given
speech recognizer parameters) of utterance xi.
For the calculation of entropy we need to
know the distribution of X because H?(Y|X) =
EX [H?(Y|X = x)] and since this distribution is not
known to us, we use Law of Large Numbers to ap-
proximate it by the empirical average
H?(Y|X) ? ? 1N
N?
i=1
?
y?Li
p?(y|Li) log p?(y|Li) (6)
Here N indicates the number of unlabeled utter-
ances for which we calculate the empirical value of
conditional entropy. Similarly, expectation w.r.t in-
put distribution in entropy-stability term is also ap-
proximated by the empirical average of samples.
Since the number of paths (hypotheses) in the lat-
tice is very large, it would be computationally infea-
sibl to c ute the conditi nal entropy y enumer-
ating all possible paths in the lattice and calculating
4The figure is adopted from (Mangu et al, 1999)
194
Element ?p, r?
?p1, r1?? ?p2, r2? ?p1p2, p1r2 + p2r1?
?p1, r1?? ?p2, r2? ?p1 + p2, r1 + r2?
0 ?0, 0?
1 ?1, 0?
Table 1: First-Order (Expectation) semiring: Defining
multiplication and sum operations for first-order semir-
ings.
their corresponding posterior probabilities. Instead
we use Finite-State Transducers (FST) to represent
the hypothesis space (lattice). To calculate entropy
and the gradient of entropy, the weights for the FST
are defined to be First- and Second-Order semirings
(Li and Eisner, 2009). The idea is to use semirings
and their corresponding operations along with the
forward-backward algorithm to calculate first- and
second-order statistics to compute entropy and the
gradient of entropy respectively. Assume we are in-
terested in calculating the entropy of the lattice,
H(p) = ??
d?Li
p(d)
Z log(
p(d)
Z )
= logZ ? 1Z
?
d?Li
p(d) log p(d)
= logZ ? r?Z (7)
where Z is the total probability of all the paths in
the lattice (normalization factor). In order to do so,
we need to compute ?Z, r?? on the lattice. It can
be proved that if we define the first-order semir-
ing ?pe, pe log pe? (pe is the non-normalized score of
each arc in the lattice) as our FST weights and define
semiring operations as in Table. 1, then applying the
forward algorithm will result in the calculation of
?Z, r?? as the weight (semiring weight) for the final
node.
The details for using Second-Order semirings for
calculating the gradient of entropy can be found
in (Li and Eisner, 2009). The same paper de-
scribes how to use the forward-backward algorithm
to speed-up the this procedure.
6 Language Model Adaptation
Language Model Adaptation is crucial when the
training data does not match the test data being de-
coded. This is a frequent scenario for all Automatic
Speech Recognition (ASR) systems. The applica-
tion domain very often contains named entities and
N-gram sequences that are unique to the domain of
interest. For example, conversational speech has
a very different structure than class-room lectures.
Linear Interpolation based methods are most com-
monly used to adapt LMs to a new domain. As
explained in (Bacchiani et al, 2003), linear inter-
polation is a special case of Maximum A Posterior
(MAP) estimation, where an N-gram LM is built on
the adaptation data from the new domain and the two
LMs are combined using:
p(wi|h) = ?pB(wi|h) + (1? ?)pA(wi|h)
0 ? ? ? 1
where pB refers to out-of-domain (background)
models and pA is the adaptation (in-domain) mod-
els. Here ? is the interpolation weight.
Conventionally, ? is calculated by optimizing per-
plexity (PPL) or Word Error Rate (WER) on some
held-out data from target domain. Instead using
our proposed framework, we estimate ? on enough
amount of unlabeled data from target domain. The
idea is that resources on the new domain have al-
ready been used to build domain specific models
and it does not make sense to again use in-domain
resources for estimating the interpolation weight.
Since we are trying to just estimate one parameter
and the performance of the interpolated model is
bound by in-domain/out-of-domain models, there is
no need to include a regularization term in Eqn. 5.
Also
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
= |?H?(Y|X)?? | because we only
have one parameter. Therefore, interpolation weight
will be chosen by the following criterion
?? = argmin
0???1
H?(Y|X) + ?|?H?(Y|X)?? | (8)
For the purpose of estimating one parameter ?, we
use ? = 1 in the above equation
7 Experimental Setup
The large vocabulary continuous speech recognition
(LVCSR) system used throughout this paper is based
on the 2007 IBM Speech transcription system for
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models used in this system
195
are state-of-the-art discriminatively trained models
and are the same ones used for all experiments pre-
sented in this paper.
For LM adaptation experiments, the out-of-
domain LM (pB , Broadcast News LM) training
text consists of 335M words from the follow-
ing broadcast news (BN) data sources (Chen et
al., 2006): 1996 CSR Hub4 Language Model
data, EARS BN03 closed captions, GALE Phase
2 Distillation GNG Evaluation Supplemental Mul-
tilingual data, Hub4 acoustic model training tran-
scripts, TDT4 closed captions, TDT4 newswire, and
GALE Broadcast Conversations and GALE Broad-
cast News. This language model is of order 4-gram
with Kneser-Ney smoothing and contains 4.6M n-
grams based on a lexicon size of 84K.
The second source of data is the MIT lectures data
set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D.
Huynh, and R. Barzilay, 2007) . This serves as the
target domain (in-domain) set for language model
adaptation experiments. This set is split into 8 hours
for in-domain LM building, another 8 hours served
as unlabeled data for interpolation weight estimation
using criterion in Eqn. 8 (we refer to this as unsuper-
vised training data) and finally 2.5 hours Dev set for
estimating the interpolation weight w.r.t WER (su-
pervised tuning) . The lattice entropy and gradient
of entropy w.r.t ? are calculated on the unsupervised
training data set. The results are discussed in the
next section.
8 Results
In order to optimize the interpolation weight ? based
on criterion in Eqn. 8, we devide [0, 1] to 20 differ-
ent points and evaluate the objective function (Eqn.
8) on those points. For this, we need to calculate
entropy and gradient of the entropy on the decoded
lattices of the ASR system on 8 hours of MIT lecture
set which is used as an unlabeled training data. Fig.
4 shows the value of the objective function against
different values of model parameters (interpolation
weight ?). As it can be seen from this figure just
considering the conditional entropy will result in a
non-convex objective function whereas adding the
entropy-stability term will make the objective func-
tion convex. For the purpose of the evaluation, we
show the results for estimating ? directly on the tran-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy
Model Entropy+Entropy-Stability
BN-LM MIT-LM?
Figure 4: Objective function with and without including
Entropy-Stability term vs. interpolation weight ? on 8
hours MIT lecture unlabeled data
scription of the 8 hour MIT lecture data and compare
it to estimated value using our framework. The re-
sults are shown in Fig. 5. Using ? = 0 and ? = 1
the WERs are 24.7% and 21.1% respectively. Us-
ing the new proposed objective function, the optimal
? is estimated to be 0.6 with WER of 20.1% (Red
circle on the figure). Estimating ? w.r.t 8 hour train-
ing data transcription (supervised adaptation) will
result in ? = 0.7 (green circle) andWER of 20.0%.
Instead ? = 0.8 will be chosen by tuning the inter-
polation weight on 2.5 hour Dev set with compara-
ble WER of 20.1%. Also it is clear from the figure
that the new objective function can be used to pre-
dict the WER trend w.r.t the interpolation weight
parameter.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy + Entropy Stability
WER
24.7%
20.0%
21.1%
supervised tuning
?
Figure 5: Estimating ? based on WER vs. the
information-theoretic criterion
Therefore, it can be seen that the new unsuper-
196
vised method results in the same performance as su-
pervised adaptation in speech recognition task.
9 Conclusion and Future Work
In this paper we introduced the notion of entropy
stability and presented a new criterion for unsu-
pervised adaptation which combines conditional en-
tropy minimization with entropy stability. The en-
tropy stability criterion helps in selecting parameter
settings which correspond to stable decision bound-
aries. Entropy minimization on the other hand tends
to push decision boundaries into sparse regions of
the input distributions. We show that combining
the two criterion helps to improve unsupervised pa-
rameter adaptation in real world scenario where
class conditional distributions show significant over-
lap. Although conditional entropy has been previ-
ously proposed as a regularizer, to our knowledge,
the gradient of entropy (entropy-stability) has not
been used previously in the literature. We presented
experimental results where the proposed criterion
clearly outperforms entropy minimization. For the
speech recognition task presented in this paper, the
proposed unsupervised scheme results in the same
performance as the supervised technique.
As a future work, we plan to use the proposed
criterion for adapting log-linear models used in
Machine Translation, Conditional Random Fields
(CRF) and other applications. We also plan to ex-
pand linear interpolation Language Model scheme
to include history specific (context dependent)
weights.
Acknowledgments
The Authors want to thank Markus Dreyer and
Zhifei Li for their insightful discussions and sugges-
tions.
References
M. Bacchiani, B. Roark, and M. Saraclar. 2003. Un-
supervised language model adaptation. In Proc.
ICASSP, pages 224?227.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, 3rd edition.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In
Advances in neural information processing systems
(NIPS), volume 17, pages 529?536.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in MIT spo-
ken lecture processing project. In Proc. Interspeech.
Jing Jiang. 2008. A literature survey on domain adapta-
tion of statistical classifiers, March.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP.
Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Min-
imum entropy clustering and applications to gene ex-
pression analysis. In Proceedings of IEEE Computa-
tional Systems Bioinformatics Conference, pages 142?
151.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999.
Finding consensus among words: Lattice-based word
error minimization. In Sixth European Conference on
Speech Communication and Technology.
M. Szummer and T. Jaakkola. 2003. Information regu-
larization with partially labeled data. In Advances in
Neural Information Processing Systems, pages 1049?
1056.
197
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 216?224,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Contextual Information Improves OOV Detection in Speech
Carolina Parada, Mark Dredze
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
carolinap@jhu.edu
mdredze@cs.jhu.edu
Denis Filimonov
HLTCOE
University of Maryland,
College Park, MD 20742 USA
den@cs.umd.edu
Frederick Jelinek
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
jelinek@jhu.edu
Abstract
Out-of-vocabulary (OOV) words represent an
important source of error in large vocabulary
continuous speech recognition (LVCSR) sys-
tems. These words cause recognition failures,
which propagate through pipeline systems im-
pacting the performance of downstream ap-
plications. The detection of OOV regions in
the output of a LVCSR system is typically ad-
dressed as a binary classification task, where
each region is independently classified using
local information. In this paper, we show that
jointly predicting OOV regions, and includ-
ing contextual information from each region,
leads to substantial improvement in OOV de-
tection. Compared to the state-of-the-art, we
reduce the missed OOV rate from 42.6% to
28.4% at 10% false alarm rate.
1 Introduction
Even with a vocabulary of one hundred thou-
sand words, a large vocabulary continuous speech
recognition (LVCSR) system encounters out-of-
vocabulary (OOV) words, especially in new do-
mains or genres. New words often include named
entities, foreign words, rare and invented words.
Since these words were not seen during training, the
LVCSR system has no way to recognize them.
OOV words are an important source of error in
LVCSR systems for three reasons. First, OOVs can
never be recognized by the LVCSR system, even if
repeated. Second, OOV words contribute to recog-
nition errors in surrounding words, which propagate
into to later processing stages (translation, under-
standing, document retrieval, etc.). Third, OOVs
are often information-rich nouns ? mis-recognized
OOVs can have a greater impact on the understand-
ing of the transcript than other words.
One solution is to simply increase the LVCSR
system?s vocabulary, but there are always new
words. Additionally, increasing the vocabulary size
without limit can sometimes produce higher word
error rates (WER), leading to a tradeoff between
recognition accuracy of frequent and rare words.
A more effective solution is to detect the presence
of OOVs directly. Once identified, OOVs can be
flagged for annotation and addition to the system?s
vocabulary, or OOV segments can be transcribed
with a phone recognizer, creating an open vocabu-
lary LVCSR system. Identified OOVs prevent error
propagation in the application pipeline.
In the literature, there are two basic approaches
to OOV detection: 1) filler models, which explicitly
represent OOVs using a filler, sub-word, or generic
word model (Bazzi, 2002; Schaaf, 2001; Bisani and
Ney, 2005; Klakow et al, 1999; Wang, 2009); and
2) confidence estimation models, which use differ-
ent confidence scores to find unreliable regions and
label them as OOV (Lin et al, 2007; Burget et al,
2008; Sun et al, 2001; Wessel et al, 2001).
Recently, Rastrow et al (2009a) presented an ap-
proach that combined confidence estimation models
and filler models to improve state-of-the-art results
for OOV detection. This approach and other confi-
dence based systems (Hazen and Bazzi, 2001; Lin
et al, 2007), treat OOV detection as a binary clas-
sification task; each region is independently classi-
fied using local information as IV or OOV. This
work moves beyond this independence assumption
216
that considers regions independently for OOV de-
tection. We treat OOV detection as a sequence la-
beling problem and add features based on the local
lexical context of each region as well as global fea-
tures from a language model using the entire utter-
ance. Our results show that such information im-
proves OOV detection and we obtain large reduc-
tions in error compared to the best previously re-
ported results. Furthermore, our approach can be
combined with any confidence based system.
We begin by reviewing the current state-of-the-art
results for OOV detection. After describing our ex-
perimental setup, we generalize the framework to a
sequence labeling problem, which includes features
from the local context, lexical context, and entire ut-
terance. Each stage yields additional improvements
over the baseline system. We conclude with a review
of related work.
2 Maximum Entropy OOV Detection
Our baseline system is the Maximum Entropy model
with features from filler and confidence estimation
models proposed by Rastrow et al (2009a). Based
on filler models, this approach models OOVs by
constructing a hybrid system which combines words
and sub-word units. Sub-word units, or fragments,
are variable length phone sequences selected using
statistical methods (Siohan and Bacchiani, 2005).
The vocabulary contains a word and a fragment lex-
icon; fragments are used to represent OOVs in the
language model text. Language model training text
is obtained by replacing low frequency words (as-
sumed OOVs) by their fragment representation. Pro-
nunciations for OOVs are obtained using grapheme
to phoneme models (Chen, 2003).
This approach also includes properties from con-
fidence estimation systems. Using a hybrid LVCSR
system, they obtain confusion networks (Mangu et
al., 1999), compact representations of the recog-
nizer?s most likely hypotheses. For an utterance,
the confusion network is composed of a sequence
of confused regions, indicating the set of most likely
word/sub-word hypotheses uttered and their poste-
rior probabilities1 in a specific time interval.
1P (wi|A): posterior probability of word i given the acous-
tics, which includes the language model and acoustic model
scores, as described in (Mangu et al, 1999).
Figure 1 depicts a confusion network decoded by
the hybrid system for a section of an utterance in our
test-set. Below the network we present the reference
transcription. In this example, two OOVs were ut-
tered: ?slobodan? and ?milosevic? and decoded as
four and three in-vocabulary words, respectively. A
confused region (also called ?bin?) corresponds to
a set of competing hypothesis between two nodes.
The goal is to correctly label each of the ?bins? as
OOV or IV. Note the presence of both fragments
(e.g. s l ow, l aa s) and words in some of the
hypothesis bins.
For any bin of the confusion network, Rastrow et
al. combine features from that region using a binary
Maximum Entropy classifier (White et al, 2007).
Their most effective features were:
Fragment-Posterior =
?
f?tj
p(f |tj)
Word-Entropy = ?
?
w?tj
p(w|tj) log p(w|tj)
tj is the current bin in the confusion network and f
is a fragment in the hybrid dictionary.
We obtained confusion networks for a standard
word based system and the hybrid system described
above. We re-implemented the above features, ob-
taining nearly identical results to Rastrow et al us-
ing Mallet?s MaxEnt classifier (McCallum, 2002). 2
All real-valued features were normalized and quan-
tized using the uniform-occupancy partitioning de-
scribed in White et al (2007).3 The MaxEnt model
is regularized using a Gaussian prior (?2 = 100),
but we found results generally insensitive to ?.
3 Experimental Setup
Before we introduce and evaluate our context ap-
proach, we establish an experimental setup. We used
the dataset constructed by Can et al (2009) to eval-
uate Spoken Term Detection (STD) of OOVs; we
refer to this corpus as OOVCORP. The corpus con-
tains 100 hours of transcribed Broadcast News En-
glish speech emphasizing OOVs. There are 1290
unique OOVs in the corpus, which were selected
with a minimum of 5 acoustic instances per word.
2Small differences are due to a change in MaxEnt library.
3All experiments use 50 partitions with a minimum of 100
training values per partition.
217
Figure 1: Example confusion network from the hybrid system with OOV regions and BIO encoding. Hypothesis are
ordered by decreasing value of posterior probability. Best hypothesis is the concatenation of the top word/fragments
in each bin. We omit posterior probabilities due to spacing.
Common English words were filtered out to ob-
tain meaningful OOVs: e.g. NATALIE, PUTIN,
QAEDA, HOLLOWAY. Since the corpus was de-
signed for STD, short OOVs (less than 4 phones)
were explicitly excluded. This resulted in roughly
24K (2%) OOV tokens.
For a LVCSR system we used the IBM Speech
Recognition Toolkit (Soltau et al, 2005)4 with
acoustic models trained on 300 hours of HUB4 data
(Fiscus et al, 1998) and excluded utterances con-
taining OOV words as marked in OOVCORP. The lan-
guage model was trained on 400M words from var-
ious text sources with a 83K word vocabulary. The
LVCSR system?s WER on the standard RT04 BN
test set was 19.4%. Excluded utterances were di-
vided into 5 hours of training and 95 hours of test
data for the OOV detector. Both train and test sets
have a 2% OOV rate. We used this split for all exper-
iments. Note that the OOV training set is different
from the LVCSR training set.
In addition to a word-based LVCSR system, we
use a hybrid LVCSR system, combining word and
sub-word (fragments) units. Combined word/sub-
word systems have improved OOV Spoken Term
Detection performance (Mamou et al, 2007; Parada
et al, 2009), better phone error rates, especially in
OOV regions (Rastrow et al, 2009b), and state-of-
the-art performance for OOV detection. Our hybrid
system?s lexicon has 83K words and 20K fragments
derived using Rastrow et al (2009a). The 1290 ex-
cluded words are OOVs to both the word and hybrid
4We use the IBM system with speaker adaptive training
based on maximum likelihood with no discriminative training.
systems.
Note that our experiments use a different dataset
than Rastrow et. al., but we have a larger vocabu-
lary (83K vs 20K), which is closer to most modern
LVCSR system vocabularies; the resulting OOVs
are more challenging but more realistic.
3.1 Evaluation
Confusion networks are obtained from both the
word and hybrid LVCSR systems. In order to eval-
uate the performance of the OOV detector, we align
the reference transcript to the audio. The LVCSR
transcript is compared to the reference transcript at
the confused region level, so each confused region
is tagged as either OOV or IV. The OOV detector
assigns a score/probability for IV/OOV to each of
these regions.
Previous research reported OOV detection accu-
racy on all test data. However, once an OOV word
has been observed in the training data for the OOV
detector, even if it never appeared in the LVCSR
training data, it is no longer truly OOV. The fea-
tures used in previous approaches did not necessar-
ily provide an advantage on observed versus unob-
served OOVs, but our features do yield an advan-
tage. Therefore, in the sections that follow we re-
port unobserved OOV accuracy: OOV words that
do not appear in either the OOV detector?s or the
LVCSR?s training data. While this penalizes our re-
sults, it is a more informative metric of true system
performance.
We present results using standard detection error
tradeoff (DET) curves (Martin et al, 1997). DET
218
curves measure tradeoffs between misses and false
alarms and can be used to determine the optimal op-
erating point of a system. The x-axis varies the false
alarm rate (false positive) and the y-axis varies the
miss (false negative) rate; lower curves are better.
4 From MaxEnt to CRFs
As a classification algorithm, Maximum Entropy as-
signs a label to each region independently. However,
OOV words tend to be recognized as two or more IV
words, hence OOV regions tend to co-occur. In the
example of Figure 1, the OOV word ?slobodan? was
recognized as four IV words: ?slow vote i mean?.
This suggests that sequence models, which jointly
assign all labels in a sequence, may be more appro-
priate. Therefore, we begin incorporating context by
moving from classification to sequence models.
MaxEnt classification models the target label as
p(yi|xi), where yi is a discrete variable representing
the ith label (?IV? or ?OOV?) and xi is a feature
vector representing information for position i. The
conditional distribution for yi takes the form
p(yi|xi) =
1
Z(xi)
exp(
K?
k=1
?kfk(yi,xi)) ,
Z(xi) is a normalization term and f(yi,xi) is a vec-
tor ofK features, such as those defined in Section 2.
The model is trained discriminatively: parameters ?
are chosen to maximize conditional data likelihood.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) generalize MaxEnt models to sequence
tasks. While having the same model structure as
Hidden Markov Models (HMMs), CRFs are trained
discriminatively and can use large numbers of corre-
lated features. Their primary advantage over Max-
Ent models is their ability to find an optimal labeling
for the entire sequence rather than greedy local deci-
sions. CRFs have been used successfully used in nu-
merous text processing tasks and while less popular
in speech, still applied successfully, such as sentence
boundary detection (Liu et al, 2005).
A CRF models the entire label sequence y as:
p(y|x) =
1
Z(x)
exp(?F (y,x)) ,
where F (y,x) is a global feature vector for input
sequence x and label sequence y and Z(x) is a nor-
malization term.5
5 Context for OOV Detection
We begin by including a minimal amount of local
context in making OOV decisions: the predicted la-
bels for adjacent confused regions (bins). This infor-
mation helps when OOV bins occur in close proxim-
ity, such as successive OOV bins. This is indeed the
case: in the OOV detector training data only 48% of
OOV sequences contained a single bin; sequences
were of length 2 (40%), 3 (9%) and 4 (2%). We
found similar results in the test data. Therefore, we
expect that even a minimal amount of context based
on the labels of adjacent bins will help.
A natural way of incorporating contextual infor-
mation is through a CRF, which introduces depen-
dencies between each label and its neighbors. If a
neighboring bin is likely an OOV, it increases the
chance that the current bin is OOV.
In sequence models, another technique for cap-
turing contextual dependence is the label encoding
scheme. In information extraction, where sequences
of adjacent tokens are likely to receive the same
tag, the beginning of each sequence receives a dif-
ferent tag from words that continue the sequence.
For example, the first token in a person name is
labeled B-PER and all subsequent tokens are la-
beled I-PER. This is commonly referred to as BIO
encoding (beginning, inside, outside). We applied
this encoding technique to our task, labeling bins
as either IV (in vocabulary), B-OOV (begin OOV)
and I-OOV (inside OOV), as illustrated in Figure 1.
This encoding allows the algorithm to identify fea-
tures which might be more indicative of the begin-
ning of an OOV sequence. We found that this en-
coding achieved a superior performance to a simple
IV/OOV encoding. We therefore utilize the BIO en-
coding in all CRF experiments.
Another means of introducing context is through
the order of the CRF model. A first order model
(n = 1) adds dependencies only between neighbor-
ing labels, whereas an n order model creates depen-
dencies between labels up to a distance of n posi-
tions. Higher order models capture length of label
5CRF experiments used the CRF++ package
http://crfpp.sourceforge.net/
219
regions (up to length n). We experiment with both
a first order and a second order CRF. Higher order
models did not provide any improvements.
In order to establish a comparative baseline, we
first present results using the same features from
the system described in Section 2 (Word-Entropy
and Fragment-Posterior). All real-valued features
were normalized and quantized using the uniform-
occupancy partitioning described in White et al
(2007).6 Quantization of real valued features is stan-
dard for log-linear models as it allows the model to
take advantage of non-linear characteristics of fea-
ture values and is better handled by the regulariza-
tion term. As in White et. al. we found it improved
performance.
Figure 2 depicts DET curves for OOV detection
for the MaxEnt baseline and first and second order
CRFs with BIO encoding on unobserved OOVs in
the test data. We generated predictions at different
false alarm rates by varying a probability threshold.
For MaxEnt we used the predicted label probability
and for CRFs the marginal probability of each bin?s
label. While the first order CRF achieves nearly
identical performance to the MaxEnt baseline, the
second order CRF shows a clear improvement. The
second order model has a 5% absolute improvement
at 10% false alarm rate, despite using the identi-
cal features as the MaxEnt baseline. Even a small
amount of context as expressed through local label-
ing decisions improves OOV detection.
The quantization of the features yields quan-
tized prediction scores, resulting in the non-smooth
curves for the MaxEnt and 1st order CRF results.
However, when using a second order CRF the OOV
score varies more smoothly since more features
(context labels) are considered in the prediction of
the current label.
6 Local Lexical Context
A popular approach in sequence tagging, such as in-
formation extraction or part of speech tagging, is to
include features based on local lexical content and
context. In detecting a name, both the lexical form
?John? and the preceding lexical context ?Mr.? pro-
vide clues that ?John? is a name. While we do not
6All experiments use 50 partitions with a minimum of 100
training values per partition.
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
MaxEnt (Baseline)CRF (First Order)CRF (Second Order)
Figure 2: DET curves for OOV detection using a Max-
imum Entropy (MaxEnt) classifier and contextual infor-
mation using a 1st order and 2nd order CRF. All models
use the same baseline features (Section 2).
know the actual lexical items in the speech sequence,
the speech recognizer output can be used as a best
guess. In the example of Figure 1, the words ?for-
mer president? are good indicators that the following
word is either the word ?of? or a name, and hence a
potential OOV. Combining this lexical context with
hypothesized words can help label the subsequent
regions as OOVs (note that none of the hypothesized
words in the third bin are ?of?, names, or nouns).
Words from the LVCSR decoding of the sentence
are used in the CRF OOV detector. For each bin in
the confusion network, we select the word with the
highest probability (best hypothesis). We then add
the best hypothesis word as a feature of the form:
current word=X. These features capture how the
LVCSR system incorrectly recognizes OOV words.
However, since detection is measured on unobserved
OOVs, these features alone may not help.
Instead, we turn to lexical context, which includes
correctly recognized IV words. We evaluate the fol-
lowing sets of features derived from lexical context:
? Current bin?s best hypothesis. (Current-Word)
? Unigrams and bigrams from the best hypoth-
esis in a window of 5 words around current
bin. This feature ignores the best hypothesis in
the current bin, i.e., word[-2],word[-1]
is included, but word[-1],word[0] is not.
(Context-Bigrams)
220
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
CRF (Second Order)+Current-Word+Context-Bigrams+Current-Trigrams+All-Words+All-Words-Stemmed
Figure 3: A second order CRF (Section 5) and additional
features including including word identities from current
and neighboring bins (Section 6).
? Unigrams, bigrams, and trigrams in a window
of 5 words around and including current bin.
(Current-Trigrams)
? All of the above features. (All-Words)
? All above features and their stems.7 (All-
Words-Stemmed)
We added these features to the second order CRF
with BIO encoding and baseline features (Figure 3).
As expected, the current words did not improve per-
formance on unobserved OOVs. When the current
words are combined with the lexical context and
their lemmas, they give a significant boost in perfor-
mance: a 4.2% absolute improvement at 10% false
alarm rate over the previous CRF system, and 9.3%
over the MaxEnt baseline. Interestingly, only com-
bining context and current word gives a substantial
gain. This indicates that OOVs tend to occur with
certain distributional characteristics that are inde-
pendent of the OOV word uttered (since we consider
only unobserved OOVs), perhaps because OOVs
tend to be named entities, foreign words, or rare
nouns. The importance of distributional features is
well known for named entity recognition and part
of speech tagging (Pereira et al, 1993). Other fea-
tures such as sub-strings or baseline features (Word-
7To obtain stemmed words, we use the CPAN package:
http://search.cpan.org/~snowhare/Lingua-Stem-0.83.
Entropy, Fragment-Posterior) from neighboring bins
did not provide further improvement.
7 Global Utterance Context
We now include features that incorporate informa-
tion from the entire utterance. The probability of an
utterance as computed by a language model is of-
ten used as a measure of fluency of the utterance.
We also observe that OOV words tend to take very
specific syntactic roles (more than half of them are
proper nouns), which means the surrounding context
will have predictive lexical and syntactic properties.
Therefore, we use a syntactic language model.
7.1 Language Models
We evaluated both a standard trigram language
model and a syntactic language model (Filimonov
and Harper, 2009a). The syntactic model estimates
the joint probability of the word and its syntactic tag
based on the preceding words and tags. The proba-
bility of an utterance wn1 of length n is computed by
summing over all latent syntactic tag assignments:
p(utt) = p(wn1 ) =
?
t1...tn
n?
i?1
p(wi, ti|w
i?1
1 , t
i?1
1 )
(1)
where wi and ti are the word and tag at posi-
tion i, and wi?11 and t
i?1
1 are sequences of words
and tags of length i ? 1 starting a position 1.
The model is restricted to a trigram context, i.e.,
p(wi, ti|w
i?1
i?2, t
i?1
i?2); experiments that increased the
order yielded no improvement.
We trained the language model on 130 million
words from Hub4 CSR 1996 (Garofolo et al, 1996).
The corpus was parsed using a modified Berkeley
parser (Huang and Harper, 2009) and tags extracted
from parse trees incorporated the word?s POS, the
label of its immediate parent, and the relative posi-
tion of the word among its siblings. 8 The parser
required separated contractions and possessives, but
we recombined those words after parsing to match
the LVCSR tokenization, merging their tags. Since
we are considering OOV detection, the language
model was restricted to LVCSR system?s vocabu-
lary.
8The parent tagset of Filimonov and Harper (2009a).
221
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
All-Words-Lemmas+3gram-LM+Syntactic-LM+Syntactic-LM+Tags
Figure 4: Features from a language model added to the
best CRF from Section 6 (All-Words-Stemmed).
We also used the standard trigram LM for refer-
ence. It was trained on the same data and with the
same vocabulary using the SRILM toolkit. We used
interpolated modified KN discounting.
7.2 Language Model Features
We designed features based on the entire utterance
using the language model to measure how the utter-
ance is effected by the current token: whether the
utterance is more likely given the recognized word
or some OOV word.
Likelihood-ratio = log
p(utt)
p(utt|wi = unknown)
Norm-LM-score =
log p(utt)
length(utt)
where p(utt) represents the probability of the ut-
terance using the best path hypothesis word of the
LVCSR system, and p(utt|wi = unknown) is the
probability of the entire utterance with the current
word in the LVCSR output replaced by the token
<unk>, used to represent OOVs. Intuitively, when
an OOV word is recognized as an IV word, the flu-
ency of the utterance is disrupted, especially if the
IV is a function word. The Likelihood-ratio is de-
signed to show whether the utterance is more fluent
(more likely) if the current word is a misrecognized
OOV. 9 The second feature (Norm-LM-score) is the
9Note that in the standard n-gram LM the feature reduces to
log
Qi+n?1
k=i p(wk|w
k?1
k?n+1)
Qi+n?1
k=i p(wk|w
k?1
k?n+1,wi=unknown)
, i.e., only n n-grams actu-
0 5 10 15 20 25 30 35 40P(FA)0
10
20
30
40
50
60
70
80
P(M
iss)
MaxEnt (Baseline)CRF All FeaturesCRF All Features (Unobserved)CRF All Features (Observed)
Figure 5: A CRF with all context features compared to
the state-of-the-art MaxEnt baseline. Results for the CRF
are shown for unobserved, observed and both OOVs.
normalized likelihood of the utterance. An unlikely
utterance biases the system to predicting OOVs.
We evaluated a CRF with these features and
all lexical context features (Section 6) using both
the trigram model and the joint syntactic language
model (Figure 4). Each model improved perfor-
mance, but the syntactic model provided the largest
improvement. At 10% false alarm rate it yields a
4% absolute improvement with respect to the pre-
vious best result (All-Words-Stemmed) and 13.3%
over the MaxEnt baseline. Higher order language
models did not improve.
7.3 Additional Syntactic Features
We explored other syntactic features; the most ef-
fective was the 5-tag window of POS tags of the
best hypothesis.10 The additive improvement of this
feature is depicted in Figure 4 labeled ?+Syntactic-
LM+Tags.? With this feature, we achieve a small ad-
ditional gain. We tried other syntactic features with-
out added benefit, such as the most likely POS tag
for <unk>in the utterance.
ally contribute. However, in the syntactic LM, the entire utter-
ance is affected by the change of one word through the latent
states (tags) (Eq. 1), thus making it a truly global feature.
10The POS tags were generated by the same syntactic LM
(see Section 7.1) as described in (Filimonov and Harper,
2009b). In this case, POS tags include merged tags, i.e., the vo-
cabulary word fred?s may be tagged as NNP-POS or NNP-VBZ.
222
8 Final System
Figure 5 summarizes all of the context features in a
single second order BIO encoded CRF. Results are
shown for state-of-the-art MaxEnt (Rastrow et al,
2009a) as well as for the CRF on unobserved, ob-
served and combined OOVs. For unobserved OOVs
our final system achieves a 14.2% absolute improve-
ment at 10% FA rate. The absolute improvement
on all OOVs was 23.7%. This result includes ob-
served OOVs: words that are OOV for the LVCSR
but are encountered in the OOV detector?s training
data. MaxEnt achieved similar performance for ob-
served and unobserved OOVs so we only include a
single combined result.
Note that the MaxEnt curve flattens at 26% false
alarms, while the CRF continues to decrease. The
elbow in the MaxEnt curve corresponds to the prob-
ability threshold at which no other labeled OOV re-
gion has a non-zero OOV score (regions with zero
entropy and no fragments). In this case, the CRF
model can still rely on the context to predict a non-
zero OOV score. This helps applications where
misses are more heavily penalized than false alarms.
9 Related Work
Most approaches to OOV detection in speech can
be categorized as filler models or confidence esti-
mation models. Filler models vary in three dimen-
sions: 1) The type of filler units used: variable-
length phoneme units (as the baseline system) vs
joint letter sound sub-words; 2) Method used to de-
rive units: data-driven (Bazzi and Glass, 2001) or
linguistically motivated (Choueiter, 2009); 3) The
method for incorporating the LVCSR system: hi-
erarchical (Bazzi, 2002) or flat models (Bisani and
Ney, 2005). Our approach can be integrated with
any of these systems.
We have shown that combining the presence of
sub-word units with other measures of confidence
can provided significant improvements, and other
proposed local confidence measures could be in-
cluded in our system as well. Lin et al (2007)
uses joint word/phone lattice alignments and clas-
sifies high local miss-alignment regions as OOVs.
Hazen and Bazzi (2001) combines filler models with
word confidence scores, such as the minimum nor-
malized log-likelihood acoustic model score for a
word and, the fraction of the N-best utterance hy-
potheses in which a hypothesized word appears.
Limited contextual information has been pre-
viously exploited (although maintaining indepen-
dence assumptions on the labels). Burget et al
(2008) used a neural-network (NN) phone-posterior
estimator as a feature for OOV detection. The
network is fed with posterior probabilities from
weakly-constrained (phonetic-based) and strongly-
constrained (word-based) recognizers. Their sys-
tem estimates frame-based scores, and interestingly,
they report large improvements when using tempo-
ral context in the NN input. This context is quite lim-
ited; it refers to posterior scores from one frame on
each side. Other features are considered and com-
bined using a MaxEnt model. They attribute this
gain to sampling from neighboring phonemes. Sun
et al (2001) combines a filler-based model with a
confidence approach by using several acoustic fea-
tures along with context based features, such as
whether the next word is a filler, acoustic confidence
features for next word, number of fillers, etc.
None of these approaches consider OOV detec-
tion as a sequence labeling problem. The work of
Liu et al (2005) is most similar to the approach pre-
sented here, but applies a CRF to sentence boundary
detection.
10 Conclusion and Future Work
We have presented a novel and effective approach to
improve OOV detection in the output confusion net-
works of a LVCSR system. Local and global con-
textual information is integrated with sub-word pos-
terior probabilities obtained from a hybrid LVCSR
system in a CRF to detect OOV regions effectively.
At a 10% FA rate, we reduce the missed OOV rate
from 42.6% to 28.4%, a 33.3% relative error reduc-
tion. Our future work will focus on additional fea-
tures from the recognizer aside from the single best-
hypothesis, as well as other applications of contex-
tual sequence prediction to speech tasks.
Acknowledgments
The authors thank Ariya Rastrow for providing the
baseline system code, Abhinav Sethy and Bhuvana
Ramabhadran for providing the data used in the ex-
periments and for many insightful discussions.
223
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Eurospeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flag hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
Dogan Can, Erica Cooper, Abhinav Sethy, Chris White,
Bhuvana Ramabhadran, and Murat Saraclar. 2009.
Effect of pronounciations on OOV queries in spoken
term detection. ICASSP.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Denis Filimonov and Mary Harper. 2009a. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Denis Filimonov and Mary Harper. 2009b. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Timothy J. Hazen and Issam Bazzi. 2001. A comparison
and combination of methods for OOV word detection
and word confidence scoring. In Proceedings of the
International Conference on Acoustics.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Interna-
tional Conference on Machine Learning (ICML).
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The DET curve in assessment of
detection task performance. In Eurospeech.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for OOV terms. In ASRU.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhad-
ran. 2009a. A new method for OOV detection using
hybrid word/fragment system. ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary-
independent audio search using path-based graph in-
dexing. In INTERSPEECH.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
224
