Proceedings of the 12th Conference of the European Chapter of the ACL, pages 406?414,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Coordinations
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Erhard Hinrichs
Universita?t Tu?bingen
eh@sfs.uni-tuebingen.de
Wolfgang Maier
Unversita?t Tu?bingen
wo.maier@uni-tuebingen.de
Eva Klett
Universita?t Tu?bingen
eklett@sfs.uni-tuebingen.de
Abstract
The present paper is concerned with sta-
tistical parsing of constituent structures
in German. The paper presents four ex-
periments that aim at improving parsing
performance of coordinate structure: 1)
reranking the n-best parses of a PCFG
parser, 2) enriching the input to a PCFG
parser by gold scopes for any conjunct, 3)
reranking the parser output for all possi-
ble scopes for conjuncts that are permissi-
ble with regard to clause structure. Exper-
iment 4 reranks a combination of parses
from experiments 1 and 3.
The experiments presented show that n-
best parsing combined with reranking im-
proves results by a large margin. Provid-
ing the parser with different scope possi-
bilities and reranking the resulting parses
results in an increase in F-score from
69.76 for the baseline to 74.69. While the
F-score is similar to the one of the first ex-
periment (n-best parsing and reranking),
the first experiment results in higher re-
call (75.48% vs. 73.69%) and the third one
in higher precision (75.43% vs. 73.26%).
Combining the two methods results in the
best result with an F-score of 76.69.
1 Introduction
The present paper is concerned with statistical
parsing of constituent structures in German. Ger-
man is a language with relatively flexible phrasal
ordering, especially of verbal complements and
adjuncts. This makes processing complex cases
of coordination particularly challenging and error-
prone. The paper presents four experiments that
aim at improving parsing performance of coor-
dinate structures: the first experiment involves
reranking of n-best parses produced by a PCFG
parser, the second experiment enriches the input
to a PCFG parser by offering gold pre-bracketings
for any coordinate structures that occur in the sen-
tence. In the third experiment, the reranker is
given all possible pre-bracketed candidate struc-
tures for coordinated constituents that are permis-
sible with regard to clause macro- and microstruc-
ture. The parsed candidates are then reranked.
The final experiment combines the parses from the
first and the third experiment and reranks them.
Improvements in this final experiment corroborate
our hypothesis that forcing the parser to work with
pre-bracketed conjuncts provides parsing alterna-
tives that are not present in the n-best parses.
Coordinate structures have been a central is-
sue in both computational and theoretical linguis-
tics for quite some time. Coordination is one of
those phenomena where the simple cases can be
accounted for by straightforward empirical gen-
eralizations and computational techniques. More
specifically, it is the observation that coordination
involves two or more constituents of the same cat-
egories. However, there are a significant number
of more complex cases of coordination that defy
this generalization and that make the parsing task
of detecting the right scope of individual conjuncts
and correctly delineating the correct scope of the
coordinate structure as a whole difficult. (1) shows
some classical examples of this kind from English.
(1) a. Sandy is a Republican and proud of it.
b. Bob voted, but Sandy did not.
c. Bob supports him and Sandy me.
In (1a), unlike categories (NP and adjective) are
conjoined. (1b) and (1c) are instances of ellipsis
(VP ellipsis and gapping). Yet another difficult set
of examples present cases of non-constituent con-
junction, as in (2), where the direct and indirect
object of a ditransitive verb are conjoined.
(2) Bob gave a book to Sam and a record to Jo.
406
2 Coordination in German
The above phenomena have direct analogues in
German.1 Due to the flexible ordering of phrases,
their variability is even higher. For example, due
to constituent fronting to clause-initial position in
German verb-second main clauses, cases of non-
constituent conjunction can involve any two NPs
(including the subject) of a ditransitive verb to the
exclusion of the third NP complement that appears
in clause-initial position. In addition, German ex-
hibits cases of asymmetric coordination first dis-
cussed by Ho?hle (1983; 1990; 1991) and illus-
trated in (3).2
(3) In
Into
den
the
Wald
woods
ging
went
ein
a
Ja?ger
hunter
und
and
schoss
shot
einen
a
Hasen.
hare.
Such cases of subject gap coordination are fre-
quently found in text corpora (cf. (4) below) and
involve conjunction of a full verb-second clause
with a VP whose subject is identical to the subject
in the first conjunct.
3 Experimental Setup and Baseline
3.1 The Treebank
The data source used for the experiments is the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z) (Telljohann et al, 2005). Tu?Ba-D/Z uses
the newspaper ?die tageszeitung? (taz) as its data
source, version 3 comprises approximately 27 000
sentences. The treebank annotation scheme dis-
tinguishes four levels of syntactic constituency:
the lexical level, the phrasal level, the level of
topological fields, and the clausal level. The pri-
mary ordering principle of a clause is the inventory
of topological fields (VF, LK, MF, VC, and NF),
which characterize the word order regularities
among different clause types of German. Tu?Ba-
D/Z annotation relies on a context-free backbone
(i.e. proper trees without crossing branches) of
phrase structure combined with edge labels that
specify the grammatical function of the phrase in
question. Conjuncts are generally marked with the
1To avoid having to gloss German examples, they were
illustrated for English.
2Yet, another case of such asymmetric coordination dis-
cussed by Ho?hle involves cases of conjunction of different
clause types: [V?final Wenn du nach Hause kommst ] und
[V?2nd da warten Polizeibeamte vor der Tu?r. ?If you come
home and there are policemen waiting in front of the door ] .?
function label KONJ. Figure 1 shows the anno-
tation that sentence (4) received in the treebank.
Syntactic categories are displayed as nodes, gram-
matical functions as edge labels in gray (e.g. OA:
direct object, PRED: predicate). This is an exam-
ple of a subject-gap coordination, in which both
conjuncts (FKONJ) share the subject (ON) that is
realized in the first conjunct.
(4) Damit
So
hat
has
sich
itself
der
the
Bevo?lkerungs-
decline in
ru?ckgang
population
zwar
though
abgeschwa?cht,
lessened,
ist
is
aber
however
noch
still
doppelt
double
so
so
gro?
big
wie
as
1996.
1996.
?For this reason, although the decline in
population has lessened, it is still twice as
big as in 1996.?
The syntactic annotation scheme of the Tu?Ba-
D/Z is described in more detail in Telljohann et al
(2004; 2005).
All experiments reported here are based on a
data split of 90% training data and 10% test data.
3.2 The Parsers and the Reranker
Two parsers were used to investigate the influ-
ence of scope information on parser performance
on coordinate structures: BitPar (Schmid, 2004)
and LoPar (Schmid, 2000). BitPar is an effi-
cient implementation of an Earley style parser that
uses bit vectors. However, BitPar cannot han-
dle pre-bracketed input. For this reason, we used
LoPar for the experiments where such input was
required. LoPar, as it is used here, is a pure
PCFG parser, which allows the input to be par-
tially bracketed. We are aware that the results
that can be obtained by pure PCFG parsers are
not state of the art as reported in the shared task
of the ACL 2008 Workshop on Parsing German
(Ku?bler, 2008). While BitPar reaches an F-score
of 69.76 (see next section), the best performing
parser (Petrov and Klein, 2008) reaches an F-
score of 83.97 on Tu?Ba-D/Z (but with a different
split of training and test data). However, our ex-
periments require certain features in the parsers,
namely the capability to provide n-best analyses
and to parse pre-bracketed input. To our knowl-
edge, the parsers that took part in the shared task
do not provide these features. Should they become
available, the methods presented here could be ap-
plied to such parsers. We see no reason why our
407
Figure 1: A tree with coordination.
methods should not be able to improve the results
of these parsers further.
Since we are interested in parsing coordina-
tions, all experiments are conducted with gold
POS tags, so as to abstract away from POS tag-
ging errors. Although the treebank contains mor-
phological information, this type of information is
not used in the experiments presented here.
The reranking experiments were conducted us-
ing the reranker by Collins and Koo (2005). This
reranker uses a set of candidate parses for a sen-
tence and reranks them based on a set of features
that are extracted from the trees. The reranker uses
a boosting method based on the approach by Fre-
und et al (1998). We used a similar feature set
to the one Collins and Koo used; the following
types of features were included: rules, bigrams,
grandparent rules, grandparent bigrams, lexical
bigrams, two-level rules, two-level bigrams, tri-
grams, head-modifiers, PPs, and distance for head-
modifier relations, as well as all feature types in-
volving rules extended by closed class lexicaliza-
tion. For a more detailed description of the rules,
the interested reader is referred to Collins and
Koo (2005). For coordination, these features give
a wider context than the original parser has and
should thus result in improvements for this phe-
nomenon.
3.3 The Baseline
When trained on 90% of the approximately 27,000
sentences of the Tu?Ba-D/Z treebank, BitPar
reaches an F-Score of 69.73 (precision: 68.63%,
recall: 70.93%) on the full test set of 2611 sen-
tences. These results as well as all further re-
sults presented here are labeled results, including
grammatical functions. Since German has a rela-
tively free word order, it is impossible to deduce
the grammatical function of a noun phrase from
the configuration of the sentence. Consequently,
an evaluation based solely on syntactic constituent
labels would be meaningless (cf. (Ku?bler, 2008)
for a discussion of this point). The inclusion of
grammatical labels in the trees, makes the parsing
process significantly more complex.
Looking at sentences with coordination (i.e.
sentences that contain a conjunction which is not
in sentence-initial position), we find that 34.9%
of the 2611 test sentences contain coordinations.
An evaluation of only sentences with coordina-
tion shows that there is a noticeable difference: the
F-score reaches 67.28 (precision: 66.36%, recall:
68.23%) as compared to 69.73 for the full test set.
The example of a wrong parse shown below il-
lustrates why parsing of complex coordinations is
so hard. Complex coordinations can take up a con-
siderable part of the input string and accordingly
of the overall sentence structure. Such global phe-
nomena are particularly hard for pure PCFG pars-
ing, due to the independence assumption inherent
in the statistical models for PCFGs.
Sentence (4) has the following Viterbi parse:
(VROOT
(SIMPX
(VF
(SIMPX-OS
(VF (PX-MOD (PROP-HD Damit)))
(LK
(VXFIN-HD (VAFIN-HD hat)))
(MF
408
(NX-OA (PRF-HD sich))
(NX-ON (ART der)
(NN-HD Bevo?lkerungsru?ckgang))
(ADVX-MOD (ADV-HD zwar)))
(VC (VXINF-OV
(VVPP-HD abgeschwa?cht)))))
($, ,)
(LK
(VXFIN-HD (VAFIN-HD ist)))
(MF
(ADVX-MOD (ADV-HD aber))
(ADVX-MOD (ADV-HD noch))
(ADJX-PRED
(ADJX-HD (ADVX (ADV-HD mehr))
(ADJX (KOKOM als)
(ADJD-HD doppelt))
(ADVX (ADV-HD so))
(ADJD-HD gro?))
(NX (KOKOM wie)
(CARD-HD 1996)))))
($. .))
The parse shows that the parser did not
recognize the coordination. Instead, the first con-
junct including the fronted constituent, Damit
hat sich der Bevo?lkerungsru?ckgang
zwar abgeschwa?cht, is treated as a fronted
subordinate clause.
4 Experiment 1: n-Best Parsing and
Reranking
The first hypothesis for improving coordination
parsing is based on the assumption that the correct
parse may not be the most probable one in Viterbi
parsing but may be recovered by n-best parsing
and reranking, a technique that has become stan-
dard in the last few years. If this hypothesis holds,
we should find the correct parse among the n-best
parses. In order to test this hypothesis, we con-
ducted an experiment with BitPar (Schmid, 2004).
We parsed the test sentences in a 50-best setting.
A closer look at the 50-best parses shows that of
the 2611 sentences, 195 (7.5%) were assigned the
correct parse as the best parse. For 325 more sen-
tences (12.4%), the correct parse could be found
under the 50 best analyses. What is more, in
90.2% of these 520 sentences, for which the cor-
rect parse was among the 50 best parses, the best
parse was among the first 10 parses. Additionally,
only in 4 cases were the correct analyses among
the 40-best to 50-best parses, an indication that in-
creasing n may not result in improving the results
significantly. These findings resulted in the deci-
sion not to conduct experiments with higher n.
That the 50 best analyses contain valuable infor-
mation can be seen from an evaluation in which an
oracle chooses from the 50 parses. In this case, we
reach an F-score of 80.28. However, this F-score
is also the upper limit for improvement that can be
achieved by reranking the 50-best parses.
For reranking, the features of Collins and
Koo (2005) were extended in the following way:
Since the German treebank used for our exper-
iments includes grammatical function informa-
tion on almost all levels in the tree, all feature
types were also included with grammatical func-
tions attached: All nodes except the root node
of the subtree in question were annotated with
their grammatical information. Thus, for the noun
phrase (NX) rule with grandparent prepositional
phrase (PX) PXGP NX? ART ADJX NN, we add
an additional rule PXGP NX-HD ? ART ADJX
NN-HD.
After pruning all features that occurred in the
training data with a frequency lower than 5, the ex-
tractions produced more than 5 mio. different fea-
tures. The reranker was optimized on the training
data, the 50-best parses were produced in a 5-fold
cross-validation setting. A non-exhaustive search
for the best value for the ? parameter showed that
Collins and Koo?s value of 0.0025 produced the
best results. The row for exp. 1 in Table 1 shows
the results of this experiment. The evaluation of
the full data set shows an improvement of 4.77
points in the F-score, which reached 74.53. This is
a relative reduction in error rate of 18.73%, which
is slightly higher that the error rate reduction re-
ported by Collins and Koo for the Penn Treebank
(13%). However, the results for Collins and Koo?s
original parses were higher, and they did not eval-
uate on grammatical functions.
The evaluation of coordination sentences shows
that such sentences profit from reranking to the
same degree. These results prove that while coor-
dination structures profit from reranking, they do
not profit more than other phenomena. We thus
conclude that reranking is no cure-all for solving
the problem of accurate coordination parsing.
5 Experiment 2: Gold Scope
The results of experiment 1 lead to the conclusion
that reranking the n-best parses can only result
in restricted improvements on coordinations. The
fact that the correct parse often cannot be found
in the 50-best analyses suggests that the different
possible scopes of a coordination are so different
in their probability distribution that not all of the
possible scopes are present in the 50-best analyses.
409
all sentences coord. sentences
precision recall F-score precision recall F-score
baseline: 68.63 70.93 69.76 66.36 68.23 67.28
exp. 1: 50-best reranking: 73.26 75.84 74.53 70.67 72.72 71.68
exp. 2: with gold scope: 76.12 72.87 74.46 75.78 72.22 73.96
exp. 3: automatic scope: 75.43 73.96 74.69 72.88 71.42 72.14
exp. 4: comb. 1 and 3: 76.15 77.23 76.69 73.79 74.73 74.26
Table 1: The results of parsing all sentences and coordinated sentences only
If this hypothesis holds, forcing the parser to con-
sider the different scope readings should increase
the accuracy of coordination parsing. In order to
force the parser to use the different scope readings,
we first extract these scope readings, and then for
each of these scope readings generate a new sen-
tence with partial bracketing that represents the
corresponding scope (see below for an example).
LoPar is equipped to parse partially-bracketed in-
put. Given input sentences with partial brackets,
the parser restricts analyses to such cases that do
not contradict the brackets in the input.
(5) Was
Which
stimmt,
is correct,
weil
because
sie
they
unterhaltsam
entertaining
sind,
are,
aber
but
auch
also
falsche
wrong
Assoziationen
associations
weckt.
wakes.
?Which is correct because they are enter-
taining, but also triggers wrong associa-
tions.?
In order to test the validity of this hypothe-
sis, we conducted an experiment with coordination
scopes extracted from the treebank trees. These
scopes were translated into partial brackets that
were included in the input sentences. For the sen-
tence in (5) from the treebank (sic), the input for
LoPar would be the following:
Was/PWS stimmt/VVFIN ,/$, weil/
KOUS ( sie/PPER unterhalt-
sam/ADJD sind/VAFIN ) ,/$,
aber/KON ( auch/ADV falsche/ADJA
Assoziationen/NN weckt/VVFIN )
The round parentheses delineate the conjuncts.
LoPar was then forced to parse sentences contain-
ing coordination with the correct scope for the co-
ordination. The results for this experiment are
shown in Table 1 as exp. 2.
The introduction of partial brackets that delimit
the scope of the coordination improve overall re-
sults on the full test set by 4.7 percent points, a
rather significant improvement when we consider
that only approximately one third of the test sen-
tences were modified. The evaluation of the set
of sentences that contain coordination shows that
here, the difference is even higher: 6.7 percent
points. It is also worth noticing that provided with
scope information, the parser parses such sen-
tences with the same accuracy as other sentences.
The difference in F-scores between all sentences
and only sentences with coordination in this ex-
periment is much lower (0.5 percent points) than
for all other experiments (2.5?3.0 percent points).
When comparing the results of experiment 1 (n-
best parsing) with the present one, it is evident that
the F-scores are very similar: 74.53 for the 50-best
reranking setting, and 74.46 for the one where we
provided the gold scope. However, a comparison
of precision and recall shows that there are differ-
ences: 50-best reranking results in higher recall,
providing gold scope for coordinations in higher
precision. The lower recall in the latter experiment
indicates that the provided brackets in some cases
are not covered by the grammar. This is corrob-
orated by the fact that in n-best parsing, only 1
sentence could not be parsed; but in parsing with
gold scope, 8 sentences could not be parsed.
6 Experiment 3: Extracting Scope
The previous experiment has shown that providing
the scope of a coordination drastically improves
results for sentences with coordination as well as
for the complete test set (although to a lower de-
gree). The question that remains to be answered is
whether automatically generated possible scopes
can provide enough information for the reranker
to improve results.
The first question that needs to be answered is
how to find the possible scopes for a coordina-
tion. One possibility is to access the parse forest
of a chart parser such as LoPar and extract infor-
410
mation about all the possible scope analyses that
the parser found. If the same parser is used for
this step and for the final parse, we can be cer-
tain that only scopes are extracted that are com-
patible with the grammar of the final parser. How-
ever, parse forests are generally stored in a highly
packed format so that an exhaustive search of the
structures is very inefficient and proved impossi-
ble with present day computing power.
(6) ?Es
?There
gibt
are
zwar
indeed
ein
a
paar
few
Niederflurbusse,
low-floor buses,
aber
but
das
that
reicht
suffices
ja
part.
nicht?,
not?,
sagt
says
er.
he.
??There are indeed a few low-floor buses,
but that isn?t enough?, he says.
Another solution consists of generating all pos-
sible scopes around the coordination. Thus, for
the sentence in (6), the conjunction is aber. The
shortest possible left conjunct is Niederflurbusse,
the next one paar Niederflurbusse, etc. Clearly,
many of these possibilities, such as the last exam-
ple, are nonsensical, especially when the proposed
conjunct crosses into or out of base phrase bound-
aries. Another type of boundary that should not
be crossed is a clause boundary. Since the con-
junction is part of the subordinated clause in the
present example, the right conjunct cannot extend
beyond the end of the clause, i.e. beyond nicht.
For this reason, we used KaRoPars (Mu?ller and
Ule, 2002), a partial parser for German, to parse
the sentences. From the partial parses, we ex-
tracted base phrases and clauses. For (6), the rel-
evant bracketing provided by KaRoPars is the fol-
lowing:
( " Es gibt zwar { ein paar
Niederflurbusse } , ) aber ( das
reicht ja nicht ) " , sagt er .
The round parentheses mark clause boundaries,
the curly braces the one base phrase that is longer
than one word. In the creation of possible con-
juncts, only such conjuncts are listed that do not
cross base phrase or clause boundaries. In order to
avoid unreasonably high numbers of pre-bracketed
versions, we also use higher level phrases, such as
coordinated noun phrases. KaRoPars groups such
higher level phrases only in contexts that allow
a reliable decision. While a small percentage of
such decisions is wrong, the heuristic used turns
out to be reliable and efficient.
For each scope, a partially bracketed version
of the input sentence is created, in which only
the brackets for the suggested conjuncts are in-
serted. Each pre-bracketed version of the sentence
is parsed with LoPar. Then all versions for one
sentence are reranked. The reranker was trained
on the data from experiment 1 (n-best parsing).
The results of the reranker show that our restric-
tions based on the partial parser may have been
too restrictive. Only 375 sentences had more than
one pre-bracketed version, and only 328 sentence
resulted in more than one parse. Only the latter set
could then profit from reranking.
The results of this experiment are shown in Ta-
ble 1 as exp. 3. They show that extracting pos-
sible scopes for conjuncts from a partial parse
is possible. The difference in F-score between
this experiment and the baseline reaches 5.93 per-
cent points. The F-score is also minimally higher
than the F-score for experiment 2 (gold scope),
and recall is increased by approximately 1 per-
cent point (even though only 12.5% of the sen-
tences were reranked). This can be attributed to
two factors: First, we provide different scope pos-
sibilities. This means that if the correct scope is
not covered by the grammar, the parser may still
be able to parse the next closest possibility in-
stead of failing completely. Second, reranking is
not specifically geared towards improving coordi-
nated structures. Thus, it is possible that a parse is
reranked higher because of some other feature. It
is, however, not the case that the improvement re-
sults completely from reranking. This can be de-
duced from two points: First, while the F-score
for experiment 1 (50-best analyses plus reranking)
and the present experiment are very close (74.53
vs. 74.69), there are again differences in precision
and recall: In experiment 1, recall is higher, and in
the present experiment precision. Second, a look
at the evaluation on only sentences with coordi-
nation shows that the F-score for the present ex-
periment is higher than the one for experiment 1
(72.14 vs. 71.68). Additionally, precision for the
present experiment is more than 2 percent points
higher.
7 Experiment 4: Combining n-Best
Parses and Extracted Scope Parses
As described above, the results for reranking the
50-best analyses and for reranking the versions
411
with automatically extracted scope readings are
very close. This raises the question whether the
two methods produce similar improvements in the
parse trees. One indicator that this is not the case
can be found in the differences in precision and re-
call. Another possibility of verifying our assump-
tion that the improvements do not overlap lies in
the combination of the 50-best parses with the
parses resulting from the automatically extracted
scopes. This increases the number of parses be-
tween which the reranker can choose. In effect,
this means a combination of the methods of exper-
iments 1 (n-best) and 3 (automatic scope). Con-
sequently, if the results from this experiment are
very close to the results from experiment 1 (n-
best), we can conclude that adding the parses with
automatic scope readings does not add new infor-
mation. If, however, adding these parses improves
results, we can conclude that new information was
present in the parses with automatic scope that
was not covered in the 50-best parses. Note that
the combination of the two types of input for the
reranker should not be regarded as a parser ensem-
ble but rather as a resampling of the n-best search
space since both parsers use the same grammar,
parsing model, and probability model. The only
difference is that LoPar can accept partially brack-
eted input, and BitPar can list the n-best analyses.
The results of this experiment are shown in Ta-
ble 1 as exp. 4. For all sentences, both precision
and recall are higher than for experiment 1 and 3,
resulting in an F-score of 76.69. This is more than
2 percent points higher than for the 50-best parses.
This is a very clear indication that the parses con-
tributed by the automatically extracted scopes pro-
vide parses that were not present in the 50 best
parses from experiment 1 (n-best). The same trend
can be seen in the evaluation of the sentences con-
taining coordination: Here, the improvement in F-
score is higher than for the whole set, a clear in-
dication that this method is suitable for improving
coordination parsing. A comparison of the results
of the present experiment and experiment 3 (with
automatic scope only) shows that the gain in pre-
cision is rather small, but the combination clearly
improves recall, from 73.96% to 77.23%. We can
conclude that adding the 50 best parses remedies
the lacking coverage that was the problem of ex-
periment 3. More generally, experiment 4 suggests
that for the notoriously difficult problem of pars-
ing coordination structures, a hybrid approach that
combines parse selection of n best analyses with
pre-bracketed scope in the input results in a con-
siderable reduction in error rate compared to each
of these methods used in isolation.
8 Related Work
Parsing of coordinate structures for English has
received considerable attention in computational
linguistics. Collins (1999), among many other au-
thors, reports in the error analysis of his WSJ pars-
ing results that coordination is one of the most fre-
quent cases of incorrect parses, particularly if the
conjuncts involved are complex. He manages to
reduce errors for simple cases of NP coordination
by introducing a special phrasal category of base
NPs. In the experiments presented above, no ex-
plicit distinction is made between simple and com-
plex cases of coordination, and no transformations
are performed on the treebank annotations used for
training.
Our experiment 1, reranking 50-best parses, is
similar to the approaches of Charniak and John-
son (2005) and of Hogan (2007). However, it dif-
fers from their experiments in two crucial ways: 1)
Compared to Charniak and Johnson, who use 1.1
mio. features, our feature set is appr. five times
larger (more than 5 mio. features), with the same
threshold of at least five occurrences in the training
set. 2) Both Hogan and Charniak and Johnson use
special features for coordinate structures, such as a
Boolean feature for marking parallelism (Charniak
and Johnson) or for distinguishing between coor-
dination of base NPs and coordination of complex
conjuncts (Hogan), while our approach refrains
from such special-purpose features.
Our experiments using scope information are
similar to the approaches of Kurohashi and Na-
gao (1994) and Agarwal and Bogges (1992) in that
they try to identify coordinate structure bracket-
ings. However, the techniques used by Agarwal
and Bogges and in the present paper are quite dif-
ferent. Agarwal and Bogges and Kurohashi and
Nagao rely on shallow parsing techniques to de-
tect parallelism of conjuncts while we use a par-
tial parser only for suggesting possible scopes of
conjuncts. Both of these approaches are limited
to coordinate structures with two conjuncts only,
while our approach has no such limitation. More-
over, the goal of Agarwal and Bogges is quite dif-
ferent from ours. Their goal is robust detection of
coordinate structures only (with the intended ap-
412
plication of term extraction), while our goal is to
improve the performance of a parser that assigns a
complete sentence structure to an input sentence.
Finally, our approach at present is restricted to
purely syntactic structural properties. This is in
contrast to approaches that incorporate semantic
information. Hogan (2007) uses bi-lexical head-
head co-occurrences in order to identify nominal
heads of conjuncts more reliably than by syntactic
information alone. Chantree et al (2005) resolve
attachment ambiguities in coordinate structures, as
in (7a) and (7b), by using word frequency informa-
tion obtained from generic corpora as an effective
estimate of the semantic compatibility of a modi-
fier vis-a`-vis the candidate heads.
(7) a. Project managers and designers
b. Old shoes and boots
We view the work by Hogan and by Chantree
et al as largely complementary to, but at the same
time as quite compatible with our approach. We
must leave the integration of structural syntac-
tic and lexical semantic information to future re-
search.
9 Conclusion and Future Work
We have presented a study on improving the treat-
ment of coordinated structures in PCFG parsing.
While we presented experiments for German, the
methods are applicable for any language. We have
chosen German because it is a language with rel-
atively flexible phrasal ordering (cf. Section 2)
which makes parsing coordinations particularly
challenging. The experiments presented show that
n-best parsing combined with reranking improves
results by a large margin. However, the number
of cases in which the correct parse is present in
the n-best parses is rather low. This led us to the
assumption that the n-best analyses often do not
cover the whole range of different scope possibil-
ities but rather present minor variations of parses
with few differences in coordination scope. The
experiments in which the parser was forced to as-
sume predefined scopes show that the scope infor-
mation is important for parsing quality. Provid-
ing the parser with different scope possibilities and
reranking the resulting parses results in an increase
in F-score from 69.76 for the baseline to 74.69.
One of the major challenges for this approach lies
in extracting a list of possible conjuncts. Forc-
ing the parser to parse all possible sequences re-
sults in a prohibitively large number of possibili-
ties, especially for sentences with 3 or more con-
junctions. For this reason, we used chunks above
base phases, such as coordinated noun chunks, to
restrict the space. However, an inspection of the
lists of bracketed versions of the sentences shows
that the definition of base phrases is one of the ar-
eas that must be refined. As mentioned above, the
partial parser groups sequences of ?NP KON NP?
into a single base phrase. This may be correct in
many cases, but there are exceptions such as (8).
(8) Die
The
31ja?hrige
31-year-old
Gewerkschaftsmitarbei-
union staff member
terin und
and
ausgebildete
trained
Industriekauffrau
industrial clerk
aus
from
Oldenburg
Oldenburg
bereitet
is preparing
nun
now
ihre
her
erste
first
eigene
own
CD
CD
vor.
part..
For (8), the partial parser groups Die 31ja?hrige
Gewerkschaftsmitarbeiterin und ausgebildete In-
dustriekauffrau as one noun chunk. Since our
proposed conjuncts cannot cross these boundaries,
the correct second conjunct, ausgebildete Indus-
triekauffrau aus Oldenburg, cannot be suggested.
However, if we remove these chunk boundaries,
the number of possible conjuncts increases dra-
matically, and parsing times become prohibitive.
As a consequence, we will need to find a good bal-
ance between these two needs. Our plan is to in-
crease flexibility very selectively, for example by
enabling the use of wider scopes in cases where
the conjunction is preceded and followed by base
noun phrases. For the future, we are planning to
repeat experiment 3 (automatic scope) with differ-
ent phrasal boundaries extracted from the partial
parser. It will be interesting to see if improvements
in this experiment will still improve results in ex-
periment 4 (combining 50-best parses with exp. 3).
Another area of improvement is the list of fea-
tures used for reranking. At present, we use a fea-
ture set that is similar to the one used by Collins
and Koo (2005). However, this feature set does
not contain any coordination specific features. We
are planning to extend the feature set by features
on structural parallelism as well as on lexical sim-
ilarity of the conjunct heads.
413
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-92), pages
15?21, Newark, DE.
Francis Chantree, Adam Kilgarriff, Anne de Roeck,
and Alistair Willis. 2005. Disambiguating coordi-
nations using word distribution information. In Pro-
ceedings of Recent Advances in NLP (RANLP 2005),
pages 144?151, Borovets, Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180, Ann Arbor, MI.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Anette Frank. 2002. A (discourse) functional analysis
of asymmetric coordination. In Proceedings of the
LFG-02 Conference, Athens, Greece.
Yoav Freund, Ray Iyer, Robert Shapire, and Yoram
Singer. 1998. An efficient boosting algorithm
for combining preferences. In Proceedings of the
15th International Conference on Machine Learn-
ing, Madison, WI.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 680?687, Prague,
Czech Republic.
Tilman Ho?hle. 1983. Subjektlu?cken in Koordinatio-
nen. Universita?t Tu?bingen.
Tilman Ho?hle. 1990. Assumptions about asymmetric
coordination in German. In Joan Mascaro? and Ma-
rina Nespor, editors, Grammar in Progress. Glow
Essays for Henk van Riemsdijk, pages 221?235.
Foris, Dordrecht.
Tilman Ho?hle. 1991. On reconstruction and coor-
dination. In Hubert Haider and Klaus Netter, ed-
itors, Representation and Derivation in the The-
ory of Grammar, volume 22 of Studies in Natural
Language and Linguistic Theory, pages 139?197.
Kluwer, Dordrecht.
Andreas Kathol. 1990. Linearization vs. phrase struc-
ture in German coordination constructions. Cogni-
tive Linguistics, 10(4):303?342.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the ACL Work-
shop on Parsing German, pages 55?63, Columbus,
Ohio.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507?534.
Frank Henrik Mu?ller and Tylman Ule. 2002. Annotat-
ing topological fields and chunks?and revising POS
tags at the same time. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics, COLING?02, pages 695?701, Taipei, Taiwan.
Slav Petrov and Dan Klein. 2008. Parsing German
with latent variable grammars. In Proceedings of
the ACL Workshop on Parsing German, pages 33?
39, Columbus, Ohio.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart.
Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004),
Geneva, Switzerland.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229?
2235, Lisbon, Portugal.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Tu?bingen, Germany.
Dieter Wunderlich. 1988. Some problems of coor-
dination in German. In Uwe Reyle and Christian
Rohrer, editors, Natural Language Parsing and Lin-
guistic Theories, Studies in Linguistics and Philoso-
phy, pages 289?316. Reidel, Dordrecht.
414
Tu?SBL: A Similarity-Based Chunk Parser
for Robust Syntactic Processing
Sandra Ku?bler
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
Wilhelmstr. 113
D-72074 Tu?bingen, Germany
kuebler@sfs.nphil.uni-tuebingen.de
Erhard W. Hinrichs
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
Wilhelmstr. 113
D-72074 Tu?bingen, Germany
eh@sfs.nphil.uni-tuebingen.de
ABSTRACT
Chunk parsing has focused on the recognition of partial constituent
structures at the level of individual chunks. Little attention has been
paid to the question of how such partial analyses can be combined
into larger structures for complete utterances.
The Tu?SBL parser extends current chunk parsing techniques by
a tree-construction component that extends partial chunk parses to
complete tree structures including recursive phrase structure as well
as function-argument structure. Tu?SBL?s tree construction algo-
rithm relies on techniques from memory-based learning that allow
similarity-based classification of a given input structure relative to
a pre-stored set of tree instances from a fully annotated treebank.
A quantitative evaluation of Tu?SBL has been conducted using
a semi-automatically constructed treebank of German that consists
of appr. 67,000 fully annotated sentences. The basic PARSEVAL
measures were used although they were developed for parsers that
have as their main goal a complete analysis that spans the entire in-
put. This runs counter to the basic philosophy underlying Tu?SBL,
which has as its main goal robustness of partially analyzed struc-
tures.
Keywords
robust parsing, chunk parsing, similarity-based learning
1. INTRODUCTION
Current research on natural language parsing tends to gravitate
toward one of two extremes: robust, partial parsing with the goal
of broad data coverage versus more traditional parsers that aim at
complete analysis for a narrowly defined set of data. Chunk pars-
ing [1, 2] offers a particularly promising and by now widely used
example of the former kind. The main insight that underlies the
chunk parsing strategy is to isolate the (finite-state) analysis of non-
recursive, syntactic structure, i.e. chunks, from larger, recursive
structures. This results in a highly-efficient parsing architecture
that is realized as a cascade of finite-state transducers and that pur-
.
sues a longest-match, right-most pattern-matching strategy at each
level of analysis.
Despite the popularity of the chunk parsing approach, there seem
to be two apparent gaps in current research:
1. Chunk parsing research has focused on the recognition of
partial constituent structures at the level of individual chunks.
By comparison, little or no attention has been paid to the
question of how such partial analyses can be combined into
larger structures for complete utterances.
2. Relatively little has been reported on quantitative evaluations
of chunk parsers that measure the correctness of the output
structures obtained by a chunk parser.
The main goal of the present paper is help close those two re-
search gaps.
2. THE T ?USBL ARCHITECTURE
In order to ensure a robust and efficient architecture, Tu?SBL, a
similarity-based chunk parser, is organized in a three-level archi-
tecture, with the output of each level serving as input for the next
higher level. The first level is part-of-speech (POS) tagging of the
input string with the help of the bigram tagger LIKELY [10].1 The
parts of speech serve as pre-terminal elements for the next step,
i.e. the chunk analysis. Chunk parsing is carried out by an adapted
version of Abney?s [2] scol parser, which is realized as a cascade
of finite-state transducers. The chunks, which extend if possible to
the simplex clause level, are then remodeled into complete trees in
the tree construction level.
The tree construction is similar to the DOP approach [3, 4] in that
it uses complete tree structures instead of rules. Contrary to Bod,
we do not make use of probabilities and do not allow tree cuts,
instead we only use the complete trees and minimal tree modifica-
tions. Thus the number of possible combinations of partial trees
is strictly controlled. The resulting parser is highly efficient (3770
English sentences took 106.5 seconds to parse on an Ultra Sparc
10).
3. CHUNK PARSING AND TREE CONSTRUC-
TION
The division of labor between the chunking and tree construction
modules can best be illustrated by an example.
1The inventory of POS tags is based on the Stuttgart-Tu?bingen
Tagset (STTS) [11].
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500501502
503
504505
506
507
508509
510
511
512
513
514
515
516
517
dann
ADV
w"urde
VAFIN
ich
PPER
vielleicht
ADV
noch
ADV
vorschlagen
VVINF
Donnerstag
NN
den
ART
elften
NN
und
KON
Freitag
NN
den
ART
zw"olften
ADJA
August
NN
HDHDHD
VXINF
OV
HDHD
VXFIN
HD
? HD
NX
HD APP
ADVX
MOD
HD
NX ADVX ADVX
ON MOD MOD
HD
ADJX
? ? HD
NX
HD APP
NX
NX
? ? ?
NX
OA
VF LK MF VC
NF
SIMPX
? ? ? ? ?
Figure 2: Sample tree construction output
Input:
dann w?urde ich vielleicht noch vorschlagen Donnerstag den elften
und Freitag den zw?olften August
(then I would suggest maybe Thursday eleventh and Friday twelfth
of August)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[advx [adv vielleicht]]
[advx [advmd noch]]
[vvinf vorschlagen]]
[nx3 [day Donnerstag]
[art den]
[adja elften]]
[kon und]
[nx3 [day Freitag]
[art den]
[adja zw"olften]
[month August]]
Figure 1: Chunk parser output
For complex sentences such as the German input dann w?urde
ich vielleicht noch vorschlagen Donnerstag den elften und Fre-
itag den zw?olften August (then I would suggest maybe Thursday
eleventh and Friday twelfth of August), the chunker produces a
structure in which some constituents remain unattached or partially
annotated in keeping with the chunk-parsing strategy to factor out
recursion and to resolve only unambigous attachments, as shown in
Fig. 1.
In the case at hand, the subconstituents of the extraposed co-
ordinated noun phrase are not attached to the simplex clause that
ends with the non-finite verb that is typically in clause-final posi-
tion in declarative main clauses of German. Moreover, each con-
junct of the coordinated noun phrase forms a completely flat struc-
ture. Tu?SBL?s tree construction module enriches the chunk output
as shown in Fig. 22. Here the internally recursive NP conjuncts
have been coordinated and integrated correctly into the clause as a
whole. In addition, function labels such as mod (for: modifier), hd
(for: head), on (for: subject), oa (for: direct object), and ov (for:
verbal object) have been added that encode the function-argument
structure of the sentence.
4. SIMILARITY-BASED TREE CONSTRUC-
TION
The tree construction algorithm is based on the machine learning
paradigm of memory-based learning [12].3 Memory-based learn-
ing assumes that the classification of a given input should be based
on the similarity to previously seen instances of the same type that
have been stored in memory. This paradigm is an instance of lazy
learning in the sense that these previously encountered instances
are stored ?as is? and are crucially not abstracted over, as is typi-
cally the case in rule-based systems or other learning approaches.
Past applications of memory-based learning to NLP tasks consist
of classification problems in which the set of classes to be learnt
is simple in the sense that the class items do not have any internal
structure and the number of distinct items is small.
The use of a memory-based approach for parsing implies that
parsing needs to be redefined as a classification task. There are two
fundamentally different, possible approaches: the one is to split
parsing up into different subtasks, that is, one needs separate clas-
sifiers for each functional category and for each level in a recur-
sive structure. Since the classifiers for the functional categories as
well as the individual decisions of the classifiers are independent,
multiple or no candidates for a specific grammatical function or
constituents with several possible functions may be found so that
an additional classifier is needed for selecting the most appropriate
assignment (cf. [6]).
The second approach, which we have chosen, is to regard the
complete parse trees as classes so that the task is defined as the
selection of the most similar tree from the instance base. Since in
2All trees in this contribution follow the data format for trees de-
fined by the NEGRA project of the Sonderforschungsbereich 378
at the University of the Saarland, Saarbru?cken. They were printed
by the NEGRA annotation tool [5].
3Memory-based learning has recently been applied to a variety of
NLP classification tasks, including part-of-speech tagging, noun
phrase chunking, grapheme-phoneme conversion, word sense dis-
ambiguation, and pp attachment (see [9], [14], [15] for details).
construct tree(chunk list, treebank):
while (chunk list is not empty) do
remove first chunk from chunk list
process chunk(chunk, treebank)
Figure 3: Pseudo-code for tree construction, main routine.
process chunk(chunk, treebank):
words := string yield(chunk)
tree := complete match(words, treebank)
if (tree is not empty) direct hit,
then output(tree) i.e. complete chunk found in treebank
else
tree := partial match(words, treebank)
if (tree is not empty)
then
if (tree = postfix of chunk)
then
tree1 := attach next chunk(tree, treebank)
if (tree is not empty)
then tree := tree1
if ((chunk - tree) is not empty) if attach next chunk succeeded
then tree := extend tree(chunk - tree, tree, treebank) chunk might consist of both chunks
output(tree)
if ((chunk - tree) is not empty) chunk might consist of both chunks (s.a.)
then process chunk(chunk - tree, treebank) i.e. process remaining chunk
else back off to POS sequence
pos := pos yield(chunk)
tree := complete match(pos, treebank)
if (tree is not empty)
then output(tree)
else back off to subchunks
while (chunk is not empty) do
remove first subchunk c1 from chunk
process chunk(c1, treebank)
Figure 4: Pseudo-code for tree construction, subroutine process chunk.
this case, the internal structure of the item to be classified (i.e. the
input sentence) and of the class item (i.e. the most similar tree in the
instance base) need to be considered, the classification task is much
more complex, and the standard memory-based approach needs to
be adapted to the requirements of the parsing task.
The features Tu?SBL uses for classification are the sequence of
words in the input sentence, their respective POS tags and (to a
lesser degree) the labels in the chunk parse. Rather than choosing a
bag-of-words approach, since word order is important for choosing
the most similar tree, the algorithm needed to be modified in order
to rely more on sequential information.
Another modification was necessitated by the need to generalize
from the limited number of trees in the instance base. The classifi-
cation is simple only in those cases where a direct hit is found, i.e.
where a complete match of the input with a stored instance exists.
In all other cases, the most similar tree from the instance base needs
to be modified to match the chunked input.
If these strategies for matching complete trees fail, Tu?SBL at-
tempts to match smaller subchunks in order to preserve the qual-
ity of the annotations rather than attempt to pursue only complete
parses.
The algorithm used for tree construction is presented in a slightly
simplified form in Figs. 3-6. For readability?s sake, we assume
here that chunks and complete trees share the same data structure
so that subroutines like string yield can operate on both of them
indiscriminately.
The main routine construct tree in Fig. 3 separates the list of in-
put chunks and passes each one to the subroutine process chunk in
Fig. 4 where the chunk is then turned into one or more (partial)
trees. process chunk first checks if a complete match with an in-
stance from the instance base is possible.4 If this is not the case,
a partial match on the lexical level is attempted. If a partial tree
is found, attach next chunk in Fig. 5 and extend tree in Fig. 6 are
used to extend the tree by either attaching one more chunk or by re-
sorting to a comparison of the missing parts of the chunk with tree
extensions on the POS level. attach next chunk is necessary to en-
sure that the best possible tree is found even in the rare case that the
original segmentation into chunks contains mistakes. If no partial
tree is found, the tree construction backs off to finding a complete
match in the POS level or to starting the subroutine for processing
a chunk recursively with all the subchunks of the present chunk.
The application of memory-based techniques is implemented in
the two subroutines complete match and partial match. The pre-
sentation of the two cases as two separate subroutines is for ex-
pository purposes only. In the actual implementation, the search
is carried out only once. The two subroutines exist because of
4string yield returns the sequence of words included in the input
structure, pos yield the sequence of POS tags.
attach next chunk(tree, treebank): attempts to attach the next chunk to the tree
take first chunk chunk2 from chunk list
words2 := string yield(tree, chunk2)
tree2 := complete match(words2, treebank)
if (tree2 is not empty)
then
remove chunk2 from chunk list
return tree2
else return empty
Figure 5: Pseudo-code for tree construction, subroutine attach next chunk.
extend tree(rest chunk, tree, treebank): extends the tree on basis of POS comparison
words := string yield(tree)
rest pos := pos yield(rest chunk)
tree2 := partial match(words + rest pos, treebank)
if ((tree2 is not empty) and (subtree(tree, tree2)))
then return tree2
else return empty
Figure 6: Pseudo-code for tree construction, subroutine extend tree.
the postprocessing of the chosen tree which is necessary for par-
tial matches and which also deviates from standard memory-based
applications. Postprocessing mainly consists of shortening the tree
from the instance base so that it covers only those parts of the chunk
that could be matched. However, if the match is done on the lexical
level, a correction of tagging errors is possible if there is enough ev-
idence in the instance base. Tu?SBL currently uses an overlap met-
ric, the most basic metric for instances with symbolic features, as
its similarity metric. This overlap metric is based on either lexical
or POS features. Instead of applying a more sophisticated metric
like the weighted overlap metric, Tu?SBL uses a backing-off ap-
proach that heavily favors similarity of the input with pre-stored
instances on the basis of substring identity. Splitting up the classi-
fication and adaptation process into different stages allows Tu?SBL
to prefer analyses with a higher likelihood of being correct. This
strategy enables corrections of tagging and segmentation errors that
may occur in the chunked input.
4.1 Example
Input:
dann w?urde ich sagen ist das vereinbart
(then I would say this is arranged)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[vvinf sagen]]
[simpx [vafin ist]
[nx2 [pds das]]
[vvpp vereinbart]]
Figure 7: Chunk parser output
For the input sentence dann w?urde ich sagen ist das vereinbart
(then I would say this is arranged), the chunked output is shown in
Fig. 7. The chunk parser correctly splits the input into two clauses
Table 1: Quantitative evaluation
minimum maximum average
precision 76.82% 77.87% 77.23%
recall 66.90% 67.65% 67.28%
crossing accuracy 93.44% 93.95% 93.70%
dann w?urde ich sagen and ist das vereinbart. A look-up in the
instance base finds a direct hit for the first clause. Therefore, the
correct tree can be output directly. For the second clause, only a
partial match on the level of words can be found. The system finds
the tree for the subsequence of words ist das, as shown in Fig. 8.
By backing off to a comparison on the POS level, it finds a tree for
the sentence hatten die gesagt (they had said) with the same POS
sequence and the same structure for the first two words. Thus the
original tree that covers only two words is extended via the newly
found tree. Tu?SBL?s output for the complete sentence is shown in
Fig. 9.
5. QUANTITATIVE EVALUATION
A quantitative evaluation of Tu?SBL has been conducted using
a semi-automatically constructed treebank of German that consists
of appr. 67,000 fully annotated sentences or sentence fragments.5
The evaluation consisted of a ten-fold cross-validation test, where
the training data provide an instance base of already seen cases for
Tu?SBL?s tree construction module.
The evaluation focused on three PARSEVAL measures: labeled
precision, labeled recall and crossing accuracy, with the results
shown in Table 1.
While these results do not reach the performance reported for
other parsers (cf. [7], [8]), it is important to note that the task carried
out here is more difficult in a number of respects:
1. The set of labels does not only include phrasal categories, but
also functional labels marking grammatical relations such as
subject, direct object, indirect object and modifier. Thus, the
evaluation carried out here is not subject to the justified crit-
icism levelled against the gold standards that are typically
5See [13] for further details.
0 1
500 501
502 503
504
ist
VAFIN
das
PDS
HD HD
VXFIN
HD
NX
ON
LK MF
SIMPX
? ?
Figure 8: A partial tree found be the system
0 1 2 3 4 5 6
500 501 502 503 504 505
506 507 508 509510 511
513
514
515
516
dann
ADV
w"urde
VAFIN
ich
PPER
sagen
VVINF
ist
VAFIN
das
PDS
vereinbart
VVPP
HD HD HD HD HD HD
ADVX
MOD
VXFIN
HD
NX
ON
VXFIN
HD
VXINF
OV
NX
ON
VF LK MF VC
? ? ? ?
SIMPX
HD
VXINF
OV
LK MF VC
SIMPX
? ? ?
Figure 9: Tu?SBL?s output for the complete sentence
in conjunction with the PARSEVAL measures, namely that
the gold standards used typically do not include annotations
of syntactic-semantic dependencies between bracketed con-
stituents.
2. The German treebank consists of transliterated spontaneous
speech data. The fragmentary and partially ill-formed na-
ture of such spoken data makes them harder to analyze than
written data such as the Penn treebank typically used as gold
standard.
It should also be kept in mind that the basic PARSEVAL mea-
sures were developed for parsers that have as their main goal a
complete analysis that spans the entire input. This runs counter to
the basic philosophy underlying an amended chunk parser such as
Tu?SBL, which has as its main goal robustness of partially analyzed
structures: Precision and recall measure the percentage of brackets,
i.e. constituents with the same yield or bracketing scope, which are
identical in the parse tree and the gold standard. If Tu?SBL finds
only a partial grouping on one level, both measures consider this
grouping wrong, as a consequence of the different bracket scopes.
In most cases, the error ?percolates? up to the highest level. Fig.
10 gives an example of a partially matched tree structure for the
sentence ?bei mir ginge es im Februar ab Mittwoch den vierten?
(for me it would work in February after Wednesday the fourth).
The only missing branch is the branch connecting the second noun
phrase (NX) above ?Mittwoch? to the NX ?den vierten?. This re-
sults in precision and recall values of 10 out of 15 because of the
altered bracketing scopes of the noun phrase, the two prepositional
phrases (PX), the field level (MF) and the sentence level (SIMPX).
In order to capture this specific aspect of the parser, a second
evaluation was performed that focused on the quality of the struc-
tures produced by the parser. This evaluation consisted of manually
judging the Tu?SBL output and scoring the accuracy of the recog-
nized constituents. The scoring was performed by the human an-
notator who constructed the treebank and was thus in a privileged
position to judge constituent accuracy with respect to the treebank
annotation standards. This manual evaluation resulted in a score
of 92.4% constituent accuracy; that is: of all constituents that were
recognized by the parser, 92.4% were judged correct by the hu-
man annotator. This seems to indicate that approximately 20% of
the precision errors are due to partial constituents whose yield is
shorter than in the corresponding gold standard. Such discrepan-
cies typically arise when Tu?SBL outputs only partial trees. This
occurs when no complete tree structures can be constructed that
span the entire input.
6. CONCLUSION AND FUTURE RESEARCH
In this paper we have described how the Tu?SBL parser extends
current chunk parsing techniques by a tree-construction compo-
nent that completes partial chunk parses to tree structures including
function-argument structure.
As noted in section 4, Tu?SBL currently uses an overlap metric, i.
e. the most basic metric for instances with symbolic features, as its
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502 503 504 505
506 507 508 509
510 511
512
513
514
bei
APPR
mir
PPER
ginge
VVFIN
es
PPER
im
APPRART
Februar
NN
ab
APPR
Mittwoch
NN
,
$,
den
ART
vierten
NN
.
$.
HD HD HD HD HD ? HD
NX
?
NX
HD
VXFIN
HD ?
NX
HD
NX
HD
PX
FOPP ?
NX
HD
PX
HD
PX
?
NX
ON
PX
V?MOD
VF
?
LK
?
MF
?
SIMPX
Figure 10: A partially grouped tree output of the T ?USBL system
similarity metric. We anticipate that the results reported in Fig. 1
can be further improved by experimenting with more sophisticated
similarity metrics. However, we will have to leave this matter to
future research.6
7. ACKNOWLEDGMENTS
The research reported here was funded both by the German Fed-
eral Ministry of Education, Science, Research, and Technology
(BMBF) in the framework of the VERBMOBIL Project under Grant
01 IV 101 N 0 and by the Deutsche Forschungsgemeinschaft (DFG)
in the framework of the Sonderforschungsbereich 441.
8. REFERENCES
[1] S. Abney. Parsing by chunks. In R. Berwick, S. Abney, and
C. Tenney, editors, Principle-Based Parsing. Kluwer
Academic Publishers, 1991.
[2] S. Abney. Partial parsing via finite-state cascades. In
J. Carroll, editor, Workshop on Robust Parsing (ESSLLI ?96),
1996.
[3] R. Bod. Beyond Grammar: An Experience-Based Theory of
Language. CSLI Publications, Stanford, California, 1998.
[4] R. Bod. Parsing with the shortest derivation. In Proceedings
of COLING 2000, 2000.
[5] T. Brants and W. Skut. Automation of treebank annotation.
In Proceedings of NeMLaP-3/CoNLL98, Sydney, Australia,
1998.
[6] S. Buchholz, J. Veenstra, and W. Daelemans. Cascaded
grammatical relation assignment. In Proceedings of
EMNLP/VLC-99, University of Maryland, USA, June 21-22,
1999, pages 239 ? 246, 1999.
[7] E. Charniak. Statistical parsing with a context-free grammar
and word statistics. In Proceedings of the Fourteenth
National Conference on Artifical Intelligence, Menlo Park,
1997.
6[9] reports that the gain ratio similarity metric has yielded excel-
lent results for the NLP applications considered by these investiga-
tors.
[8] M. Collins. Head-Driven Statistical Models for Natural
Language Parsing. PhD thesis, University of Pennsylvania,
1999.
[9] W. Daelemans, J. Zavrel, and A. van den Bosch. Forgetting
exceptions is harmful in language learning. Machine
Learning: Special Issue on Natural Language Learning, 34,
1999.
[10] H. Feldweg. Stochastische Wortartendisambiguierung fu?r das
Deutsche: Untersuchungen mit dem robusten System
LIKELY. Technical report, Universita?t Tu?bingen, 1993.
SfS-Report-08-93.
[11] A. Schiller, S. Teufel, and C. Thielen. Guidelines fu?r das
Tagging deutscher Textkorpora mit STTS. Technical report,
Universita?t Stuttgart and Universita?t Tu?bingen, 1995. (URL:
http://www.sfs.nphil.uni-tuebingen.de/Elwis/stts/stts.html).
[12] C. Stanfill and D. Waltz. Towards memory-based reasoning.
Communications of the ACM, 29(12), 1986.
[13] R. Stegmann, H. Schulz, and E. W. Hinrichs. Stylebook for
the German Treebank in VERBMOBIL. Technical Report
239, Verbmobil, 2000.
[14] J. Veenstra, A. van den Bosch, S. Buchholz, W. Daelemans,
and J. Zavrel. Memory-based word sense disambiguation.
Computers and the Humanities, Special Issue on Senseval,
Word Sense Disambiguations, 34, 2000.
[15] J. Zavrel, W. Daelemans, and J. Veenstra. Resolving PP
attachment ambiguities with memory-based learning. In
M. Ellison, editor, Proceedings of the Workshop on
Computational Natural Language Learning (CoNLL?97),
Madrid, 1997.
From Chunks to Function-Argument Structure: A Similarity-Based
Approach
Sandra Ku?bler   and Erhard W. Hinrichs  
 
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
D-72074 Tu?bingen, Germany

kuebler,eh  @sfs.nphil.uni-tuebingen.de
Abstract
Chunk parsing has focused on the
recognition of partial constituent struc-
tures at the level of individual chunks.
Little attention has been paid to the
question of how such partial analyses
can be combined into larger structures
for complete utterances. Such larger
structures are not only desirable for a
deeper syntactic analysis. They also
constitute a necessary prerequisite for
assigning function-argument structure.
The present paper offers a similarity-
based algorithm for assigning func-
tional labels such as subject, object,
head, complement, etc. to complete
syntactic structures on the basis of pre-
chunked input.
The evaluation of the algorithm has
concentrated on measuring the quality
of functional labels. It was performed
on a German and an English treebank
using two different annotation schemes
at the level of function-argument struc-
ture. The results of 89.73 % cor-
rect functional labels for German and
90.40 % for English validate the general
approach.
1 Introduction
Current research on natural language parsing
tends to gravitate toward one of two extremes:
robust, partial parsing with the goal of broad
data coverage versus more traditional parsers that
aim at complete analysis for a narrowly defined
set of data. Chunk parsing (Abney, 1991; Ab-
ney, 1996) offers a particularly promising and by
now widely used example of the former kind.
The main insight that underlies the chunk pars-
ing strategy is to isolate the (finite-state) analysis
of non-recursive syntactic structure, i.e. chunks,
from larger, recursive structures. This results
in a highly-efficient parsing architecture that is
realized as a cascade of finite-state transducers
and that pursues a leftmost longest-match pattern-
matching strategy at each level of analysis.
Despite the popularity of the chunk parsing ap-
proach, there seems to be a gap in current re-
search:
Chunk parsing research has focused on the
recognition of partial constituent structures at the
level of individual chunks. By comparison, lit-
tle or no attention has been paid to the ques-
tion of how such partial analyses can be com-
bined into larger structures for complete utter-
ances. Such larger structures are not only de-
sirable for a deeper syntactic analysis; they also
constitute a necessary prerequisite for assigning
function-argument structure.
Automatic assignment of function-argument
structure has long been recognized as a desider-
atum beyond pure syntactic labeling (Marcus et
al., 1994)1. The present paper offers a similarity-
1With the exception of dependency-grammar-based
parsers (Tapanainen and Ja?rvinen, 1997; Bro?ker et al, 1994;
Lesmo and Lombardo, 2000), where functional labels are
treated as first-class citizens as relations between words, and
recent work on a semi-automatic method for treebank con-
struction (Brants et al, 1997), little has been reported on
based algorithm for assigning functional labels
such as subject, object, head, complement, etc.
to complete syntactic structures on the basis of
pre-chunked input. The evaluation of the algo-
rithm has concentrated on measuring the quality
of these functional labels.
2 The Tu?SBL Architecture
In order to ensure a robust and efficient archi-
tecture, Tu?SBL, a similarity-based chunk parser,
is organized in a three-level architecture, with
the output of each level serving as input for the
next higher level. The first level is part-of-speech
(POS) tagging of the input string with the help
of the bigram tagger LIKELY (Feldweg, 1993).2
The parts of speech serve as pre-terminal ele-
ments for the next step, i.e. the chunk analysis.
Chunk parsing is carried out by an adapted ver-
sion of Abney?s (1996) CASS parser, which is
realized as a cascade of finite-state transducers.
The chunks, which extend if possible to the sim-
plex clause level, are then remodeled into com-
plete trees in the tree construction level.
The tree construction level is similar to the
DOP approach (Bod, 1998; Bod, 2000) in that
it uses complete tree structures instead of rules.
Contrary to Bod, we only use the complete trees
and do not allow tree cuts. Thus the number of
possible combinations of partial trees is strictly
controlled. The resulting parser is highly efficient
(3770 English sentences took 106.5 seconds to
parse on an Ultra Sparc 10).
3 Chunking and Tree Construction
The division of labor between the chunking and
tree construction modules can best be illustrated
by an example.
For sentences such as the input shown in Fig.
1, the chunker produces a structure in which some
constituents remain unattached or partially anno-
tated in keeping with the chunk-parsing strategy
to factor out recursion and to resolve only unam-
biguous attachments.
Since chunks are by definition non-recursive
structures, a chunk of a given category cannot
fully automatic recognition of functional labels.
2The inventory of POS tags is based on the STTS
(Schiller et al, 1995) for German and on the Penn Treebank
tagset (Santorini, 1990) for English.
Input: alright and that should get us there about
nine in the evening
Chunk parser output:
[uh alright]
[simpx_ind
[cc and]
[that that]
[vp [md should]
[vb get]]
[pp us]
[adv [rb there]]
[prep_p [about about]
[np [cd nine]]]
[prep_p [in in]
[np [dt the]
[daytime evening]]]]
Figure 1: Chunk parser output.
contain another chunk of the same type. In
the case at hand, the two prepositional phrases
(?prep p?) about nine and in the evening in the
chunk output cannot be combined into a sin-
gle chunk, even though semantically these words
constitute a single constituent. At the level of tree
construction, as shown in Fig. 2, the prohibition
against recursive phrases is suspended. There-
fore, the proper PP attachment becomes possible.
Additionally, the phrase about nine was wrongly
categorized as a ?prep p?. Such miscategoriza-
tions can arise if a given word can be assigned
more than one POS tag. In the case of about
the tags ?in? (for: preposition) or ?rb? (for: ad-
verb) would be appropriate. However, since the
POS tagger cannot resolve this ambiguity from
local context, the underspecified tag ?about? is as-
signed, instead. However, this can in turn lead to
misclassification in the chunker.
The most obvious deficiency of the chunk out-
put shown in Fig. 1 is that the structure does
not contain any information about the function-
argument structure of the chunked phrases. How-
ever, once a (more) complete parse structure is
created, the grammatical function of each ma-
jor constituent needs to be identified. The la-
bels SUBJ (for: subject), HD (for: head), ADJ
(for: adjunct) COMP (for: complement), SPR
(for: specifier), which appear as edge-labels be-
tween tree nodes in Fig. 2, signify the grammati-
cal functions of the constituents in question. E.g.
the label SUBJ encodes that the NP that is the
alright
UH
and
CC
that
DT
should
MD
get
VB
us
PP
there
RB
about
RB
nine
CD
in
IN
the
DT
evening
NN
? ? HD HD HD ? ?
PR?DM
HD
DT?ART
HD
DTP
SPR HD
HD
NP
COMP
ADVP
ADJ
CNUM
HD
PP
ADJ
HD
NP
COMP
ADVP
ADJ
NP
ADJ
NP
SBJ HD
VP
COMP
CNJ
?
S
?
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502 503 504 505 506
507 508
509
510
511
512
513
514
S
Figure 2: Sample tree construction output for the sentence in Fig. 1.
subject of the whole sentence. The label ADJ
above the phrase about nine in the evening signi-
fies that this phrase is an adjunct of the verb get.
Tu?SBL currently uses as its instance base two
semi-automatically constructed treebanks of Ger-
man and English that consist of appr. 67,000 and
35,000 fully annotated sentences, respectively3 .
Each treebank uses a different annotation scheme
at the level of function-argument structure4 . As
shown in Table 1, the English treebank uses a to-
tal of 13 functional labels, while the German tree-
bank has a richer set of 36 function labels.
For German, therefore, the task of tree con-
struction is slightly more complex because of the
larger set of functional labels. Fig. 3 gives an ex-
ample for a German input sentence and its corre-
sponding chunk parser output.
In this case, the subconstituents of the extra-
posed coordinated noun phrase are not attached
to the simplex clause that ends with the non-finite
verb that is typically in clause-final position in
declarative main clauses of German. Moreover,
each conjunct of the coordinated noun phrase
forms a completely flat structure. Tu?SBL?s tree
construction module enriches the chunk output
as shown in Fig. 4. Here the internally recur-
sive NP conjuncts have been coordinated and in-
3See (Stegmann et al, 2000; Kordoni, 2000) for further
details.
4The annotation for German follows the topological-
field-model standardly used in empirical studies of German
syntax. The annotation for English is modeled after the theo-
retical assumptions of Head-Driven Phrase Structure Gram-
mar.
Input:
dann w?urde ich vielleicht noch vorschlagen
Donnerstag den elften und Freitag den zw?olften
August (then I would suggest maybe Thursday eleventh
and Friday twelfth of August)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[advx [adv vielleicht]]
[advx [advmd noch]]
[vvinf vorschlagen]]
[nx3 [day Donnerstag]
[art den]
[adja elften]]
[kon und]
[nx3 [day Freitag]
[art den]
[adja zw"olften]
[month August]]
Figure 3: Chunk parser output for a German sen-
tence.
tegrated correctly into the clause as a whole. In
addition, function labels such as MOD (for: mod-
ifier), HD (for head), ON (for: subject), OA (for:
direct object), OV (for: verbal object), and APP
(for: apposition) have been added that encode the
function-argument structure of the sentence.
4 Similarity-based Tree Construction
The tree construction algorithm is based on the
machine learning paradigm of memory-based
German label description English label description
HD head HD head
- non-head - intentionally empty
ON nominative object COMP complement
OD dative object SPR specifier
OA accusative object SBJ subject
OS sentential object SBQ subject, wh-
OPP prepositional object SBR subject, rel.
OADVP adverbial object ADJ adjunct
OADJP adjectival object ADJ? adjunct ambiguities
PRED predicate FIL filler
OV verbal object FLQ filler, wh-
FOPP optional prepositional object FLR filler, rel.
VPT separable verb prefix MRK marker
APP apposition
MOD ambiguous modifier
x-MOD 8 distinct labels for specific
modifiers, e.g. V-MOD
yK 13 labels for second conjuncts in
split-up coordinations, e.g. ONK
Table 1: The functional label set for the German and the English treebanks.
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500501502
503
504505
506
507
508509
510
511
512
513
514
515
516
517
dann
ADV
w"urde
VAFIN
ich
PPER
vielleicht
ADV
noch
ADV
vorschlagen
VVINF
Donnerstag
NN
den
ART
elften
NN
und	
KON
Freitag
NN
den
ART
zw"olften

ADJA
August
NN
HDHDHD
VXINF
OV
HDHD
VXFIN
HD
? HD
NX
HD APP
ADVX
MOD
HD
NX ADVX ADVX
ON MOD MOD
HD
ADJX
? ? HD
NX
HD APP
NX
NX
? ? ?
NX
OA
VF LK MF VC
NF
SIMPX
? ? ? ? ?
Figure 4: Tree construction output for the German sentence in Fig. 3.
learning (Stanfill and Waltz, 1986).5 Memory-
based learning assumes that the classification of
a given input should be based on the similarity
to previously seen instances of the same type that
have been stored in memory. This paradigm is an
instance of lazy learning in the sense that these
previously encountered instances are stored ?as
is? and are crucially not abstracted over, as is
typically the case in rule-based systems or other
learning approaches. Previous applications of
5Memory-based learning has recently been applied to a
variety of NLP classification tasks, including part-of-speech
tagging, noun phrase chunking, grapheme-phoneme conver-
sion, word sense disambiguation, and PP attachment (see
(Daelemans et al, 1999; Veenstra et al, 2000; Zavrel et al,
1997) for details).
memory-based learning to NLP tasks consisted of
classification problems in which the set of classes
to be learnt was simple in the sense that the class
items did not have any internal structure and the
number of distinct items was small. Since in the
current application, the set of classes are parse
trees, the classification task is much more com-
plex. The classification is simple only in those
cases where a direct hit is found, i.e. where a com-
plete match of the input with a stored instance ex-
ists. In all other cases, the most similar tree from
the instance base needs to be modified to match
the chunked input. This means that the output
tree will group together only those elements from
the chunked input for which there is evidence in
the instance base. If these strategies fail for com-
plete chunks, Tu?SBL attempts to match smaller
subchunks.
The algorithm used for tree construction is pre-
sented in a slightly simplified form in Figs. 5-8.
For readability, we assume here that chunks and
complete trees share the same data structure so
that subroutines like string yield can operate on
both of them indiscriminately.
The main routine construct tree in Fig. 5 sepa-
rates the list of input chunks and passes each one
to the subroutine process chunk in Fig. 6 where
the chunk is then turned into one or more (partial)
trees. process chunk first checks if a complete
match with an instance from the instance base is
possible.6 If this is not the case, a partial match
on the lexical level is attempted. If a partial tree is
found, attach next chunk in Fig. 7 and extend tree
in Fig. 8 are used to extend the tree by either at-
taching one more chunk or by resorting to a com-
parison of the missing parts of the chunk with tree
extensions on the POS level. attach next chunk is
necessary to ensure that the best possible tree is
found even in the rare case that the original seg-
mentation into chunks contains mistakes. If no
partial tree is found, the tree construction backs
off to finding a complete match at the POS level or
to starting the subroutine for processing a chunk
recursively with all the subchunks of the present
chunk.
The application of memory-based techniques
is implemented in the two subroutines com-
plete match and partial match. The presentation
of the two cases as two separate subroutines is for
expository purposes only. In the actual implemen-
tation, the search is carried out only once. The
two subroutines exist because of the postprocess-
ing of the chosen tree, which is necessary for par-
tial matches and which also deviates from stan-
dard memory-based applications. Postprocessing
mainly consists of shortening the tree from the in-
stance base so that it covers only those parts of
the chunk that could be matched. However, if the
match is done on the lexical level, a correction of
tagging errors is possible if there is enough evi-
dence in the instance base. Tu?SBL currently uses
an overlap metric, the most basic metric for in-
6string yield returns the sequence of words included in
the input structure, pos yield the sequence of POS tags.
stances with symbolic features, as its similarity
metric. This overlap metric is based on either
lexical or POS features. Instead of applying a
more sophisticated metric like the weighted over-
lap metric, Tu?SBL uses a backing-off approach
that heavily favors similarity of the input with pre-
stored instances on the basis of substring identity.
Splitting up the classification and adaptation pro-
cess into different stages allows Tu?SBL to prefer
analyses with a higher likelihood of being correct.
This strategy enables corrections of tagging and
segmentation errors that may occur in the chun-
ked input.
5 Quantitative Evaluation
Quantitive evaluations of robust parsers typically
focus on the three PARSEVAL measures: labeled
precision, labeled recall and crossing accuracy. It
has frequently been pointed out that these evalu-
ation parameters provide little or no information
as to whether a parser assigns the correct seman-
tic structure to a given input, if the set of category
labels comprises only syntactic categories in the
narrow sense, i.e. includes only names of lexi-
cal and phrasal categories. This justified criticism
observes that a measure of semantic accuracy can
only be obtained if the gold standard includes an-
notations of syntactic-semantic dependencies be-
tween bracketed constituents. It is to answer this
criticism that the evaluation of the Tu?SBL system
presented here focuses on the correct assignment
of functional labels. For an in-depth evaluation
that focuses on syntactic categories, we refer the
interested reader to (Ku?bler and Hinrichs, 2001).
The quantitative evaluation of Tu?SBL has been
conducted on the treebanks of German and En-
glish described in section 3. Each treebank uses
a different annotation scheme at the level of
function-argument structure. As shown in Table
1, the English treebank uses a total of 13 func-
tional labels, while the German treebank has a
richer set of 36 function labels.
The evaluation consisted of a ten-fold cross-
validation test, where the training data provide an
instance base of already seen cases for Tu?SBL?s
tree construction module. The evaluation was per-
formed for both the German and English data.
For each language, the following parameters were
measured: 1. labeled precision for syntactic cat-
construct tree(chunk list, treebank):
while (chunk list is not empty) do
remove first chunk from chunk list
process chunk(chunk, treebank)
Figure 5: Pseudo-code for tree construction, main routine.
process chunk(chunk, treebank):
words := string yield(chunk)
tree := complete match(words, treebank)
if (tree is not empty) direct hit,
then output(tree) i.e. complete chunk found in treebank
else
tree := partial match(words, treebank)
if (tree is not empty)
then
if (tree = postfix of chunk)
then
tree1 := attach next chunk(tree, treebank)
if (tree is not empty)
then tree := tree1
if ((chunk - tree) is not empty) if attach next chunk succeeded
then tree := extend tree(chunk - tree, tree, treebank) chunk might consist of both chunks
output(tree)
if ((chunk - tree) is not empty) chunk might consist of both chunks (s.a.)
then process chunk(chunk - tree, treebank) i.e. process remaining chunk
else back off to POS sequence
pos := pos yield(chunk)
tree := complete match(pos, treebank)
if (tree is not empty)
then output(tree)
else back off to subchunks
while (chunk is not empty) do
remove first subchunk c1 from chunk
process chunk(c1, treebank)
Figure 6: Pseudo-code for tree construction, subroutine process chunk.
attach next chunk(tree, treebank): attempts to attach the next chunk to the tree
take first chunk chunk2 from chunk list
words2 := string yield(tree, chunk2)
tree2 := complete match(words2, treebank)
if (tree2 is not empty)
then
remove chunk2 from chunk list
return tree2
else return empty
Figure 7: Pseudo-code for tree construction, subroutine attach next chunk.
extend tree(rest chunk, tree, treebank): extends the tree on basis of POS comparison
words := string yield(tree)
rest pos := pos yield(rest chunk)
tree2 := partial match(words + rest pos, treebank)
if ((tree2 is not empty) and (subtree(tree, tree2)))
then return tree2
else return empty
Figure 8: Pseudo-code for tree construction, subroutine extend tree.
egories alone, and 2. labeled precision for func-
tional labels.
The results of the quantitative evaluation are
shown in Tables 2 and 3. The results for labeled
recall underscore the difficulty of applying the
classical PARSEVAL measures to a partial pars-
language parameter minimum maximum average
German true positives 60.38 % 64.23 % 61.45 %
false positives 2.93 % 3.14 % 3.03 %
unattached constituents 15.15 % 19.23 % 18.18 %
unmatched constituents 17.05 % 17.59 % 17.35 %
English true positives 59.11 % 60.18 % 59.78 %
false positives 3.11 % 3.39 % 3.25 %
unattached constituents 9.57 % 10.30 % 9.88 %
unmatched constituents 26.80 % 27.54 % 27.10 %
Table 2: Quantitative evaluation: recall.
language parameter minimum maximum average
German labeled precision for synt. cat. 81.28 % 82.08 % 81.56 %
labeled precision for funct. cat. 89.26 % 90.13 % 89.73 %
English labeled precision for synt. cat. 66.15 % 67.34 % 66.84 %
labeled precision for funct. cat. 90.07 % 90.93 % 90.40 %
Table 3: Quantitative evaluation: precision.
ing approach like ours. We have, therefore di-
vided the incorrectly matched nodes into three
categories: the genuine false positives where a
tree structure is found that matches the gold stan-
dard, but is assigned the wrong label; nodes
which, relative to the gold standard, remain
unattached in the output tree; and nodes contained
in the gold standard for which no match could be
found in the parser output. Our approach follows
a strategy of positing and attaching nodes only if
sufficient evidence can be found in the instance
base. Therefore the latter two categories can-
not really be considered errors in the strict sense.
Nevertheless, in future research we will attempt to
significantly reduce the proportion of unattached
and unmatched nodes by exploring matching al-
gorithms that permit a higher level of generaliza-
tion when matching the input against the instance
base. What is encouraging about the recall results
reported in Table 2 is that the parser produces gen-
uine false positives for an average of only 3.03 %
for German and 3.25 % for English.
For German, labeled precision for syntactic
categories yielded 81.56 % correctness. While
these results do not reach the performance re-
ported for other parsers (cf. (Collins, 1999; Char-
niak, 1997)), it is important to note that the two
treebanks consist of transliterated spontaneous
speech data. The fragmentary and partially ill-
formed nature of such spoken data makes them
harder to analyze than written data such as the
Penn treebank typically used as gold standard.
It should also be kept in mind that the basic
PARSEVAL measures were developed for parsers
that have as their main goal a complete analy-
sis that spans the entire input. This runs counter
to the basic philosophy underlying an amended
chunk parser such as Tu?SBL, which has as its
main goal robustness of partially analyzed struc-
tures.
Labeled precision of functional labels for the
German data resulted in a score of 89.73 % cor-
rectness. For English, precision of functional la-
bels was 90.40 %. The slightly lower correctness
rate for German is a reflection of the larger set of
function labels used by the grammar. This raises
interesting more general issues about trade-offs
in accuracy and granularity of functional annota-
tions.
6 Conclusion and Future Research
The results of 89.73 % (German) and 90.40 %
(English) correctly assigned functional labels val-
idate the general approach. We anticipate fur-
ther improvements by experimenting with more
sophisticated similarity metrics7 and by enrich-
ing the linguistic information in the instance base.
The latter can, for example, be achieved by pre-
serving more structural information contained in
the chunk parse. Yet another dimension for ex-
perimentation concerns the way in which the al-
gorithm generalizes over the instance base. In
the current version of the algorithm, generaliza-
tion heavily relies on lexical and part-of-speech
information. However, a richer set of backing-off
strategies that rely on larger domains of structure
are easy to envisage and are likely to significantly
improve recall performance.
While we intend to pursue all three dimensions
of refining the basic algorithm reported here, we
have to leave an experimentation of which modi-
fications yield improved results to future research.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Caroll Tenney, editors,
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Steven Abney. 1996. Partial parsing via finite-state
cascades. In John Carroll, editor, Workshop on Ro-
bust Parsing (ESSLLI ?96).
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publications,
Stanford, California.
Rens Bod. 2000. Parsing with the shortest derivation.
In Proceedings of COLING 2000, Saarbru?cken,
Germany.
Thorsten Brants, Wojiech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP-2 1997, Providence, RI.
Norbert Bro?ker, Udo Hahn, and Susanne Schacht.
1994. Concurrent lexicalized dependency parsing:
the ParseTalk model. In Proceedings of COLING
94, Kyoto, Japan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artifical Intelligence, Menlo Park.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
7(Daelemans et al, 1999) reports that the gain ratio sim-
ilarity metric has yielded excellent results for the NLP appli-
cations considered by these investigators.
Walter Daelemans, Jakub Zavrel, and Antal van den
Bosch. 1999. Forgetting exceptions is harmful in
language learning. Machine Learning: Special Is-
sue on Natural Language Learning, 34.
Helmut Feldweg. 1993. Stochastische Wortartendis-
ambiguierung fu?r das Deutsche: Untersuchungen
mit dem robusten System LIKELY. Technical re-
port, Universita?t Tu?bingen. SfS-Report-08-93.
Valia Kordoni. 2000. Stylebook for the English
Treebank in VERBMOBIL. Technical Report 241,
Verbmobil.
Sandra Ku?bler and Erhard W. Hinrichs. 2001.
Tu?SBL: A similarity-based chunk parser for robust
syntactic processing. In Proceedings of HLT 2001,
San Diego, Cal.
Leonardo Lesmo and Vincenzo Lombardo. 2000. Au-
tomatic assignment of grammatical relations. In
Proceedings of LREC 2000, Athens, Greece.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Anne Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
HLT 94, Plainsboro, New Jersey.
Beatrice Santorini. 1990. Part-Of-Speech Tagging
Guidelines for the Penn Treebank Project. Univer-
sity of Pennsylvania, 3rd Revision, 2nd Printing.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fu?r das Tagging deutscher Text-
korpora mit STTS. Technical report, Universita?ten
Stuttgart and Tu?bingen. http://www.sfs.nphil.uni-
tuebingen.de/Elwis/stts/stts.html.
Craig Stanfill and David L. Waltz. 1986. Towards
memory-based reasoning. Communications of the
ACM, 29(12).
Rosmary Stegmann, Heike Schulz, and Erhard W.
Hinrichs. 2000. Stylebook for the German Tree-
bank in VERBMOBIL. Technical Report 239,
Verbmobil.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP?97, Washington, D.C.
Jorn Veenstra, Antal van den Bosch, Sabine Buch-
holz, Walter Daelemans, and Jakub Zavrel. 2000.
Memory-based word sense disambiguation. Com-
puters and the Humanities, Special Issue on Sense-
val, Word Sense Disambiguations, 34.
Jakub Zavrel, Walter Daelemans, and Jorn Veen-
stra. 1997. Resolving PP attachment ambiguities
with memory-based learning. In Proceedings of
CoNLL?97, Madrid, Spain.
  	
 
 	Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 13?20,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Unified Representation for Morphological, Syntactic, Semantic, and
Referential Annotations
Erhard W. Hinrichs, Sandra K?bler, Karin Naumann
SfS-CL, University of T?bingen
Wilhelmstr. 19
72074 T?bingen, Germany
{eh,kuebler,knaumann}@sfs.uni-tuebingen.de
Abstract
This paper reports on the SYN-RA
(SYNtax-based Reference Annotation)
project, an on-going project of annotating
German newspaper texts with referential
relations. The project has developed an in-
ventory of anaphoric and coreference rela-
tions for German in the context of a uni-
fied, XML-based annotation scheme for
combining morphological, syntactic, se-
mantic, and anaphoric information. The
paper discusses how this unified annota-
tion scheme relates to other formats cur-
rently discussed in the literature, in par-
ticular the annotation graph model of Bird
and Liberman (2001) and the pie-in-the-
sky scheme for semantic annotation.
1 Introduction
The purpose of this paper is threefold: (i) it dis-
cusses an annotation scheme for referential relations
for German that is significantly broader in scope
than existing schemes for the same task and lan-
guage and that also goes beyond the inventory of
anaphoric relations included in the pie-in-the-sky
sample feature structures1 , (ii) it presents a unified,
XML-based annotation scheme for combining mor-
phological, syntactic, semantic, and anaphoric infor-
mation, and (iii) it discusses how this unified anno-
tation scheme relates to other formats currently dis-
cussed in the literature, in particular the annotation
1See e.g. nlp.cs.nyu.edu/meyers/pie-in-the-sky/
analysis5.
graph model of Bird and Liberman (2001) and the
pie-in-the-sky scheme for semantic annotation2 .
2 Referential Relations
This section introduces the inventory of referential
relations adopted in the SYN-RA project. We define
referential relations as a cover-term for all contex-
tually dependent reference relations. The inventory
of such relations adopted for SYN-RA is inspired by
the annotation scheme first developed in the MATE
project (Davies et al, 1998). However, it takes a
cautious approach in that it only adopts those refer-
ential relations from MATE for which the develop-
ers of MATE report a sufficiently high level of inter-
annotator agreement (Poesio et al, 1999).
SYN-RA currently uses the following subset
of relations: coreferential, anaphoric, cataphoric,
bound, split antecedent, instance, and expletive. The
potential markables are definite NPs, personal pro-
nouns, relative, reflexive, and reciprocal pronouns,
demonstrative, indefinite and possessive pronouns.
There is a second research effort under way at the
European Media Laboratory Heidelberg, which also
annotates German text corpora and dialog data with
referential relations. Since their corpora are not pub-
licly available, it is difficult to verify their inventory
of referential relations. Kouchnir (2003) has used
their data and describes the relations anaphoric,
coreferential, bridging, and none.
Following van Deemter and Kibble (2000), we
define a coreference relation to hold between two
2See nlp.cs.nyu.edu/meyers/pie-in-the-sky/
pie-in-the-sky-descript.html.
13
NPs just in case they refer to the same extra-
linguistic referent in the real world. In the following
example, a coreference relation exists between the
noun phrases [1] and [2], and an anaphoric relation
between the noun phrase [2] and the personal pro-
noun [3]. Since noun phrases [1] and [2] are corefer-
ential, all three NPs belong to the same coreference
chain. In keeping with the MUC-6 annotation stan-
dard3, we establish the anaphoric relations of a pro-
noun only to its most recently mentioned antecedent.
(1) [1 Der
The
neue
new
Vorsitzende
chairman
der
of the
Gewerkschaft
union
Erziehung
Education
und
and
Wissenschaft]
Science
hei?t
is called
[2 Ulli
Ulli
Th?ne].
Th?ne.
[3 Er]
He
wurde
was
gestern
yesterday
mit
with
217
217
von
out of
355
355
Stimmen
votes
gew?hlt.
elected.
?The new chairman of the union of educators
and scholars is called Ulli Th?ne. He was
elected yesterday with 217 of 355 votes.?
Cataphoric relations hold between a preceding
pronoun and a following antecedent within the same
sentence, even if this antecedent has already been
mentioned within the preceding text. An example
for a cataphoric relation is shown in (2).
(2) Vier
Four
Wochen
weeks
sind
are
[sie]
they
nun
now
schon
already
in
in
Berlin,
Berlin,
[die
the
220
220
Albaner
Albanians
aus
from
dem
the
Kosovo].
Kosovo.
?They have already been in Berlin for four
weeks, the 200 Albanians from Kosovo.?
The relation bound holds between anaphoric ex-
pressions and quantified noun phrases as their an-
tecedents (see example (3)).
(3) [Niemandem]
To nobody
f?llt
is
es
it
schwer,
difficult,
das
the
Bild
picture
vor
in front of
[sich]
himself
zu
to
sehen.
see.
?Nobody has trouble imagining the picture.?
3See www.cs.nyu.edu/cs/faculty/grishman/
COtask21.book_1.html.
The split antecedent relation holds between co-
ordinate NPs/plural pronouns and pronouns/definite
NPs referring to one member of the plural expres-
sion. In example (4), the indefinite pronoun beide
enters into two split antecedent relations, with noun
phrases 1 and 2.
(4) Aber
But
pl?tzlich
suddenly
gibt
gives
es
it
da
there
einen
a
v?llig
completely
unglaubw?rdig
implausible
und
and
grotesk
grotesque
wirkenden
seeming
Anruf
phone call
[1 des
of the
Detektiven]
detective
bei
to
[2 der
the
Mutter
mother
des
of the
Opfers]
victim
,
,
[beide]
both
weinen
cry
sich
themselves
minutenlang
for some minutes
etwas
something
vor
verb part
,
,
...
...
?But suddenly, there is a completely implausi-
ble and grotesque phone call from the detective
to the mother of the victim, they both cry at
each other for several minutes, ...?
An instance relation exists between a preced-
ing/following pronoun and its NP antecedent when
the pronoun refers to a particular instantiation of the
class identified by the NP.
(5) Die
The
konservativen
conservative
Kr?fte
powers
warten
wait
ja
just
nur
only
darauf,
for that,
ihm
him
[S?tze]
sentences
um
around
die
the
Ohren
ears
zu
to
hauen
hit
wie
like
[jenen
the one
von
about
den
the
16
16
Mittelstrecklern],
middle-distance runners,
denen
to whom
er
he
in
in
vier
four
Wochen
weeks
die
the
Viererkette
double full-back formation
beibringe.
teaches.
?The conservative powers are just waiting to
bombard him with sentences like the one about
the 16 middle-distance runners who he is teach-
ing the double full-back formation in four
weeks.?
14
In sentence (5), the relation between the two
bracketed NPs is an example of such an instance re-
lation since the second NP is a particular instantia-
tion of the referent denoted by the first NP.
A third person singular neuter pronoun es is
marked as expletive if it has no proper antecedent.
This is the case for presentational es in example (6),
impersonal passive as in example (7), or es as sub-
ject for verbs without an agent as in example (8).
(6) [1 Es]
It
zeichnet sich
emerges
die
the
konkrete
concrete
M?glichkeit
possibility
ab.
verb part.
?The concrete possibility emerges.?
(7) [Es]
There
wird
is
bis zum
until the
Morgen
morning
getanzt.
danced.
?People are dancing until morning.?
(8) [Es]
It
steht
stands
schlecht
bad
um
for
ihn.
him.
?He is in a bad way.?
Apart from expletive uses of es and anaphoric
uses with an NP antecedent, the pronoun es can also
be used in cases of event anaphora as in sentence
(9). Here es refers to the event of Jochen?s win-
ning the lottery. Currently, the annotation in SYN-
RA is restricted to NP anaphora and therefore event
anaphors such as in sentence (9) remain unannotated
for anaphora.
(9) Jochen
Jochen
hat
has
im
in the
Lotto
lottery
gewonnen.
won.
Aber
But
er
he
weiss
knows
es
it
noch
yet
nicht.
not.
?Jochen has won the lottery. But he does not
know it yet.?
The annotation of such relations is performed
manually with the annotation tool MMAX (M?ller
and Strube, 2003). Its graphical user interface al-
lows for easy selection of the relevant markables and
the accompanying relation between the contextually
dependent expression and its antecedent.
3 Automatic Extraction of Markables and
of Semantic Information
Annotation of referential relations involves two
main tasks: the identification of markables, i.e.,
identifying the class of expressions that can enter
into referential relations, and the identification of the
particular referential relations that two or more ex-
pressions enter into. Identification of markables re-
quires at least partial syntactic annotation of the text.
If referential relations need to be annotated from
plain text, then markables must be identified semi-
automatically from the output of a chunker or full
parser, if available, or otherwise completely man-
ually. However, in each of these two scenarios,
identification of markables is a time-consuming pro-
cess. In case of semi-automatic annotation, the ef-
fort required depends on the quality of the parser, but
will require at least some amount of manual post-
correction of the parser output.
Identification of markables is considerably easier
for treebank data since treebanks already provide the
necessary syntactic information. For German, there
are currently two large-scale treebanks available: the
NEGRA/TIGER (Brants et al, 2002) treebank and
the T?bingen treebanks for spoken and written Ger-
man (Stegmann et al, 2000; Telljohann et al, 2003).
All the treebanks were annotated with the help of the
annotation tool Annotate (Plaehn, 1998). The tree-
bank annotations are available in the Annotate ex-
port format (Brants, 1997) and in an XML format.
The SYN-RA project is based on the T?bingen
treebank of written German (T?Ba-D/Z). This tree-
bank uses as its data source a collection of articles of
the German daily newspaper taz (die tageszeitung).
The treebank currently comprises appr. 15 000 sen-
tences, with a new release of 7 000 additional sen-
tences scheduled for June of this year.
Due to its fine grained syntactic annotation, the
T?Ba-D/Z treebank data are ideally suited as a basis
for the identification of markables and for extract-
ing relevant syntactic and semantic properties for
each markable. The T?Ba-D/Z annotation scheme
distinguishes four levels of syntactic constituency:
the lexical level, the phrasal level, the level of topo-
logical fields, and the clausal level. The primary
ordering principle of a clause is the inventory of
topological fields, which characterize the word or-
15
Ihre
PPOSAT
asf
Schulkameradin
NN
asf
Cassie
NE
asf
Bernall
NE
asf
fragten
VVFIN
3pit
sie
PPER
np*3
,
$,
??
ob
KOUS
??
sie
PPER
nsf3
an
APPR
a
Gott
NE
asm
glaube
VVFIN
3sks
.
$.
??
? HD ? ? HD HD ? HD HD HD
NX
?
VXFIN
HD
NX
ON ?
NX
HD
VXFIN
HD
NX
APP
EN?ADD
APP
NX
ON
PX
OPP
NX
OA
C
?
MF
?
VC
?
SIMPX
OS
VF
?
LK
?
MF
?
NF
?
0 1 2 3 4 5 6 7 8 9 10 11 12
500 501 502 503 504 505 506 507
508 509 510 511 512
513 514
515 516
517
518
SIMPX
Figure 1: A sample tree from the T?Ba/D-Z treebank.
der regularities among different clause types of Ger-
man and which are widely accepted among descrip-
tive linguists of German (cf. e.g. (Drach, 1937;
H?hle, 1986)). The T?Ba-D/Z annotation relies
on a context-free backbone (i.e. proper trees with-
out crossing branches) of phrase structure combined
with edge labels that specify the grammatical func-
tion of the phrase in question.
Figure 1 shows an example tree from the T?Ba-
D/Z treebank for sentence (10). The sentence is di-
vided into two clauses (SIMPX), and each clause is
subdivided into topological fields. The main clause
is made up of the following fields: VF (mnemonic
for: Vorfeld ? ?initial field?) contains the sentence-
initial, topicalized constituent. LK (for: linke Satz-
klammer ? ?left sentence bracket?) is occupied by the
finite verb. MF (for: Mittelfeld ? ?middle field?) con-
tains adjuncts and complements of the main verb.
NF (for: Nachfeld ? ?final field?) contains extra-
posed material ? in this case an indirect yes/no ques-
tion. The subordinate clause is again divided into
three topological fields: C (for: Komplementierer ?
?complementizer?), MF, and VC (for: Verbalkomp-
lex ? verbal complex). Edge labels are rendered
in boxes and indicate grammatical functions. The
sentence-initial NX (for: noun phrase) is marked as
OA (for: accusative complement), the pronouns sie
in the main and subordinate clause as ON (for: nom-
inative complement).
(10) Ihre
Their
Schulkameradin
fellow student
Cassie
Cassie
Bernall
Bernall
fragten
asked
sie
they[subj]
,
,
ob
whether
sie
she[subj]
an
in
Gott
God
glaube.
believes.
?They asked their fellow student Cassie Bernall
whether she believes in God.?
Topological field information and grammatical
function information is crucial for anaphora resolu-
tion since binding-theory constraints crucially rely
on sentence-structure (if the binding theory princi-
ples are stated configurationally (Chomsky, 1981))
or on argument-obliqueness (if the binding theory
principles are stated in terms of argument structure,
as in (Pollard and Sag, 1994)). In the case at hand,
the subject pronoun of the main clause, sie, can-
not be anaphorically related to the object NP Ihre
Schulkameradin Cassie Bernall since they are co-
arguments of the same verb. However, the posses-
sive pronoun ihre and the subject pronoun sie of the
subordinate clause, can be and, in fact, are anaphor-
ically related, since they are not co-arguments of the
same verb. This can be directly inferred from the
treebank annotation, specifically from the sentence
structure and the grammatical function information
16
encoded on the edge labels. Most published compu-
tational algorithms of anaphora resolution, including
(Hobbs, 1978; Lappin and Leass, 1994; Ingria and
Stallard, 1989), rely on such binding-constraint fil-
ters to minimize the set of potential antecedents for
pronouns and reflexives.
As already pointed out, the sample sentence con-
tains four markables: one possessive pronoun Ihre,
two occurrences of the pronoun sie and one complex
NP Ihre Schulkameradin Cassie Bernall. The latter
NP is a good example of SYN-RA?s longest-match
principle for identifying markables. In case of com-
plex NPs, the entire NP counts as a markable, but
so do its subconstituents ? in the case at hand, par-
ticularly the possessive pronoun ihre. All of this in-
formation can be directly derived from the treebank
account. Compared to other annotation efforts for
German where markables have to be chosen manu-
ally (M?ller and Strube, 2003), manual annotation
in the SYN-RA project can, thus, be restricted to the
selection of the appropriate referential relations be-
tween referentially dependent expressions and their
nominal antecedents.
4 The Unified, XML-based Annotation
Scheme
The annotation of referential expressions is em-
bedded in a unified format which also contains
morphological, syntactic, and semantic information.
The annotation scheme is represented in XML, the
widely acknowledged standard for exchanging data,
which guarantees portability and re-usability of the
data. Each sentence, as well as all words and
all nodes in the syntactic structure, are assigned a
unique ID. These IDs are used in the annotation of
referential relations. The annotation of the treebank
sentence 11976 (cf. example (10)) is shown in Fig-
ure 2.
The sentence number is encoded as the ID of the
sentence. The first word, Ihre, has an anaphoric rela-
tion to a noun phrase in the previous sentence. This
relation is marked in the element anaphora, which
gives the antecedent as node 517 of sentence 11975,
i.e. the previous sentence. The other two anaphoric
relations are sentence-internal, the first personal pro-
noun sie having Ihre (id: s11976w0) as antecedent,
the second one the noun phrase Ihre Schulfreundin
Cassie Bernall (id: s11976n513). The annotation of
the first personal pronoun is an example for the an-
notation of an anaphoric chain. Ihre and sie belong
to the same chain. However, in order to facilitate the
extraction of direct relations, such chains are repre-
sented in a way that each anaphoric expression refers
to the last occurrence of an antecedent.
The SYN-RA scheme is very similar to the
MUC-6 coreference annotation scheme4 but it is
more powerful in two respects: As described above,
the inventory is not restricted to coreference and
anaphoric relations, it also covers e.g. instance rela-
tions or split antecedent relations. The latter relation
is also the reason for encoding the relational infor-
mation as XML elements, and not as attributes of
a word or a node. If an anaphor enters into a split
antecedent relation, it has more than one distinct an-
tecedent. In this case, the element anaphora has two
(or more) relations. Such an example is graphically
displayed for sentence (4) in Figure 3. The rele-
vant XML representation of the complex entry for
the word beide is shown in Figure 4.
5 Related Work
This section discusses how the unified SYN-RA an-
notation scheme relates to other formats currently
discussed in the literature, in particular the pie-in-
the-sky scheme for semantic annotation5 and the
annotation graph model of (Bird and Liberman,
2001). While these two annotation schemes are by
no means the only contenders for corpus annotation
standards in the literature, they are certainly among
the most ambitious and promising.
While the pie-in-the-sky scheme is clearly still
under development, the following characteristics
and goals can already be gleaned from its web-
page and the annotation examples presented there:
The annotation is feature-structure-based and incor-
porates various levels of linguistic annotation, in
particular a PROPBANK style predicate-argument
structure, dependency style syntactic information,
as well as morpho-syntactic and word class infor-
mation. All this information is rooted in the at-
tributes needed for predicate-argument assignment,
4See www.cs.nyu.edu/cs/faculty/grishman/
COtask21.book_1.html.
5See nlp.cs.nyu.edu/meyers/pie-in-the-sky/
pie-in-the-sky-descript.html.
17
<sentence id="s11976">
<node id="s11976n518" cat="SIMPX" func="--" parent="0">
<node id="s11976n515" cat="VF" func="-">
<node id="s11976n513" cat="NX" func="OA">
<node id="s11976n500" cat="NX" func="APP">
<word id="s11976w0" form="Ihre" pos="PPOSAT" morph="asf" func="-">
< anaphora>
< relation type="ana" antecedent="s11975n517"/>
< /anaphora> </word>
<word id="s11976w1" form="Schulkameradin" pos="NN" morph="asf" func="HD"/>
</node>
<node id="s11976n508" cat="EN-ADD" func="APP">
<node id="s11976n501" cat="NX" func="-">
<word id="s11976w2" form="Cassie" pos="NE" morph="asf" func="-"/>
<word id="s11976w3" form="Bernall" pos="NE" morph="asf" func="-"/>
</node> </node> </node> </node>
<node id="s11976n509" cat="LK" func="-">
<node id="s11976n502" cat="VXFIN" func="HD">
<word id="s11976w4" form="fragten" pos="VVFIN" morph="3pit" func="HD"/>
</node> </node>
<node id="s11976n510" cat="MF" func="-">
<node id="s11976n503" cat="NX" func="ON">
<word id="s11976w5" form="sie" pos="PPER" morph="np*3" func="HD">
< anaphora>
< relation type="ana" antecedent="s11976w1"/>
< /anaphora> </word> </node> </node>
<word id="s11976w6" form="," pos="$," morph="--" func="--" parent="0"/>
<node id="s11976n517" cat="NF" func="-">
<node id="s11976n516" cat="SIMPX" func="OS">
<node id="s11976n504" cat="C" func="-">
<word id="s11976w7" form="ob" pos="KOUS" morph="--" func="-"/>
</node>
<node id="s11976n514" cat="MF" func="-">
<node id="s11976n505" cat="NX" func="ON">
<word id="s11976w8" form="sie" pos="PPER" morph="nsf3" func="HD">
< anaphora>
< relation type="ana" antecedent="s11976n513"/>
< /anophora> </word> </node>
<node id="s11976n511" cat="PX" func="OPP" comment="">
<word id="s11976w9" form="an" pos="APPR" morph="a" func="-"/>
<node id="s11976n506" cat="NX" func="HD">
<word id="s11976w10" form="Gott" pos="NE" morph="asm" func="HD"/>
</node> </node> </node>
<node id="s11976n512" cat="VC" func="-">
<node id="s11976n507" cat="VXFIN" func="HD">
<word id="s11976w11" form="glaube" pos="VVFIN" morph="3sks" func="HD"/>
</node> </node> </node> </node> </node>
<word form="." pos="$." morph="--" func="--" parent="0"/>
</sentence>
Figure 2: The XML format represents information on all levels of annotation. The words of the sentence
and the anaphoric annotation are shown in bold.
18
NP NP
Aber pl?tzlich gibt es da einen ... Anruf des Detektiven bei der Mutter ..., beide weinen sich
minutenlang etwas vor ...
split
split
Figure 3: The annotation of the split antecedent relation in sentence (4). For representational reasons, the
sentence is shortened and only relevant information is displayed. Syntactic boundaries are shown as dotted
lines, anaphoric relations as black lines.
<word id="s3426w20" form="beide" pos="PIS" morph="np*" func="HD">
< anaphora>
<relation type="split" antecedent="s3426n507"/>
<relation type="split" antecedent="s3426n526"/>
< /anaphora>
</word>
Figure 4: The XML representation of the encoding of split antecedents for the word beide in sentence (4).
A graphical representation of the relation is shown in Figure 3. The antecedent "s3426n507" refers to the
first NP, "s3426n526" to the second one in Figure 3.
with syntactic and morpho-syntactic information
distributed among the corresponding elements in
the predicate-argument structure representation. Ac-
cordingly, semantic representations provide the or-
ganizing principle while morpho-syntactic and syn-
tactic information play a subordinated role.
The SYN-RA annotation scheme resembles the
pie-in-the-sky scheme in that it also uses one level
of representation, in this case hierarchical syntac-
tic structure, as the organizing principle and treats
referential relations, grammatical function informa-
tion, and morpho-syntactic annotation as subordi-
nated types of information. More generally, the pie-
in-the-sky and the SYN-RA representations offer a
particular view of the annotation, each with its own
?perspective?: semantics-based (pie-in-the-sky) and
syntax-based (SYN-RA).
By contrast, Bird and Liberman?s (2001) anno-
tation graphs are intended as a graph-based, multi-
layered annotation scheme where each level of lin-
guistic annotation is treated equally, as an indepen-
dent layer. The graph-based annotation model is
powerful enough to also allow groupings of discon-
tinuous constituents and other non-adjacent linguis-
tic phenomena, without having to rearrange the lin-
ear order of the input. In both respects, their annota-
tion model is maximally general.
6 Future Directions
In the previous section we have compared two
perspective-dependent annotation schemes that use
a particular level of linguistic annotation as their pri-
mary organizing principle and have contrasted them
with the perspective-independent annotation-graph
model. We believe that both types of represen-
tation models have their independent justification.
Perspective-based representations, such as SYN-
RA and pie-in-the-sky, are well-justified for partic-
ular application scenarios. For example, for text
summarization and other semantic tasks, the pie-
in-the-sky model seems particularly well-motivated
since the pertinent semantic information can be eas-
ily extracted from its predicate-argument-structure-
rooted feature structures. For other tasks, such as
anaphora resolution, for which syntactic informa-
tion is more relevant, the syntax-based representa-
tion of SYN-RA allows for an easier extraction of
the relevant information for rule-based, statistical,
19
and machine-learning approaches to computational
anaphora resolution. More generally, perspective-
based representations are highly task-dependent. It
would be misguided to consider them as ideal, task-
independent annotation standards. If one wants
to establish a task-independent annotation standard,
then a perspective-independent annotation scheme
such as the annotation graph model looks like a
promising direction for future research. In particu-
lar, such research should focus on techniques that al-
low for easy conversion of perspective-independent
representations to task-dependent views of the rele-
vant linguistic information.
References
Steven Bird and Mark Liberman. 2001. A formal frame-
work for linguistic annotation. Speech Communica-
tion, 33(1,2):23?60.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Thorsten Brants, 1997. The NeGra Export Format for
Annotated Corpora. Universit?t des Saarlandes, Com-
putational Linguistics, Saarbr?cken, Germany.
Noam Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
Sarah Davies, Massimo Poesio, Florence Bruneseaux,
and Laurent Romary, 1998. Annotating Coreference in
Dialogues: Proposal for a Scheme for MATE. MATE.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(2):629?637.
Erich Drach. 1937. Grundgedanken der Deutschen Satz-
lehre. Diesterweg, Frankfurt/M.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", An-
merkungen ?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germanistenkon-
gresses 1985, pages 329?340, G?ttingen, Germany.
Robert J. P. Ingria and David Stallard. 1989. A compu-
tational mechanism for pronominal reference. In Pro-
ceedings of the 27th Conference of the Association for
Computational Linguistics, pages 262?271, Vancou-
ver, Canada.
Beata Kouchnir. 2003. A machine learning approach to
German pronoun resolution. Master?s thesis, School
of Informatics, University of Edinburgh.
Shalom Lappin and Herbert Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4):535?561.
Christoph M?ller and Michael Strube. 2003. Multi-level
annotation in MMAX. In Proceedings of the 4th SIG-
dial Workshop on Discourse and Dialogue, Sapporo,
Japan.
Oliver Plaehn, 1998. Annotate Bedienungsanleitung.
Universit?t des Saarlandes, Sonderforschungsbereich
378, Projekt C3, Saarbr?cken, Germany, April.
Massimo Poesio, Florence Bruneseaux, and Laurent Ro-
mary. 1999. The MATE meta-scheme for coreference
in dialogues in multiple languages. In Proceedings of
the ACL Workshop on Standards for Discourse Tag-
ging, pages 65?74.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago, IL.
Rosmary Stegmann, Heike Telljohann, and Erhard W.
Hinrichs. 2000. Stylebook for the German Treebank
in VERBMOBIL. Technical Report 239, Verbmobil.
Heike Telljohann, Erhard W. Hinrichs, and Sandra
K?bler, 2003. Stylebook for the T?bingen Treebank of
Written German (T?Ba-D/Z). Seminar f?r Sprachwis-
senschaft, Universit?t T?bingen, T?bingen, Germany.
20
Proceedings of the Workshop on Linguistic Distances, pages 1?6,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Linguistic Distances
John Nerbonne
Alfa-informatica
University of Groningen
j.nerbonne@rug.nl
Erhard Hinrichs
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
eh@sfs.uni-tuebingen.de
Abstract
In many theoretical and applied areas of
computational linguistics researchers op-
erate with a notion of linguistic distance
or, conversely, linguistic similarity, which
is the focus of the present workshop.
While many CL areas make frequent use
of such notions, it has received little fo-
cused attention, an honorable exception
being Lebart & Rajman (2000). This
workshop brings a number of these strands
together, highlighting a number of com-
mon issues.
1 Introduction
In many theoretical and applied areas of compu-
tational linguistics researchers operate with a no-
tion of linguistic distance or, conversely, linguistic
similarity, which is the focus of the present work-
shop. While many CL areas make frequent use of
such notions, it has received little focused atten-
tion, an honorable exception being Lebart & Raj-
man (2000).
In information retrieval (IR), also the focus of
Lebart & Rajman?s work, similarity is at heart
of most techniques seeking an optimal match be-
tween query and document. Techniques in vector
space models operationalize this via (weighted)
cosine measures, but older tf/idf models were also
arguably aiming at a notion of similarity.
Word sense disambiguation models often work
with a notion of similarity among the contexts
within which word (senses) appear, and MT iden-
tifies candidate lexical translation equivalents via
a comparable measure of similarity. Many learn-
ing algorithms currently popular in CL, including
not only supervised techniques such as memory-
based learning (k-nn) and support-vector ma-
chines, but also unsupervised techniques such as
Kohonen maps and clustering, rely essentially on
measures of similarity for their processing.
Notions of similarity are often invoked in lin-
guistic areas such as dialectology, historical lin-
guistics, stylometry, second-language learning (as
a measure of learners? proficiency), psycholin-
guistics (accounting for lexical ?neighborhood?
effects, where neighborhoods are defined by simi-
larity) and even in theoretical linguistics (novel ac-
counts of the phonological constraints on semitic
roots).
This volume reports on a workshop aimed at
bringing together researchers employing various
measures of linguistic distance or similarity, in-
cluding novel proposals, especially to demonstrate
the importance of the abstract properties of such
measures (consistency, validity, stability over cor-
pus size, computability, fidelity to the mathemati-
cal distance axioms), but also to exchange infor-
mation on how to analyze distance information
further.
We assume that there is always a ?hidden vari-
able? in the similarity relation, so that we should
always speak of similarity with respect to some
property, and we suspect that there is such a
plethora of measures in part because researchers
are often inexplicit on this point. It is useful to
tease the different notions apart. Finally, it is most
intriguing to try to make a start on understanding
how some of the different notions might construed
as alternative realizations of a single abstract no-
tion.
2 Pronunciation
John Laver, the author of the most widely used
textbook in phonetics, claimed that ?one of the
1
most basic concepts in phonetics, and one of the
least discussed, is that of phonetic similarity
[boldface in original, JN & EH]? (Laver, 1994,
p. 391), justifying the attention the workshop pays
to it. Laver goes on to sketch the work that has
been done on phonetic similarity, or, more ex-
actly, phonetic distance, in particular, the empir-
ical derivation of confusion matrices, which indi-
cate the likelihood with which people or speech
recognition systems confusion one sound for an-
other. Miller & Nicely (1955) founded this ap-
proach with studies of how humans confused some
sounds more readily than others. Although ?con-
fusability? is a reasonable reflection of phonetic
similarity, it is perhaps worth noting that confu-
sion matrices are often asymmetric, suggesting
that something more complex is at play. Clark
& Yallop (1995, p. 319ff) discuss this line of
work further, suggesting more sophisticated anal-
yses which aggregate confusion matrices based on
segments.
In addition to the phonetic interest (above), pho-
nologists have likewise shown interest in the ques-
tion of similarity, especially in recent work. Al-
bright and Hayes (2003) have proposed a model
of phonological learning which relies on ?mini-
mal generalization?. The idea is that children learn
e.g. rules of allomorphy on the basis not merely
of rules and individual lexical exceptions (the ear-
lier standard wisdom), but rather on the basis of
slight but reliable generalizations. An example is
the formation of the past tense of verbs ending in
[IN], ?ing? (fling, sing, sting, spring, string) that
build past tenses as ?ung? [2N]. We omit details
but note that the ?minimal generalization? is min-
imally DISTANT in pronunciation.
Frisch, Pierrehumbert & Broe (2004) have also
kindled an interest in segmental similarity among
phonologists with their claim that syllables in
Semitic languages are constrained to have unlike
consonants in syllable onset and coda. Their work
has not gone unchallenged (Bailey and Hahn,
2005; Hahn and Bailey, 2005), but it has certainly
created further theoretical interest in phonological
similarity.
There has been a great deal of attention in
psycholinguistics to the the problem of word
recognition, and several models appeal explic-
itly to the ?degree of phonetic similarity among
the words? (Luce and Pisoni, 1998, p. 1), but
most of these models employ relatively simple no-
tions of sequence similarity and/or, e.g., the idea
that distance may be operationalized by the num-
ber or replacements needed to derive one word
from another?ignoring the problem of similarity
among words of different lengths (Vitevitch and
Luce, 1999). Perhaps more sophisticated com-
putational models of pronunciation distance could
play a role in these models in the future.
Kessler (1995) showed how to employ edit dis-
tance to operationalize pronunciation difference in
order to investigate dialectology more precisely,
an idea which, particular, Heeringa (2004) pursued
at great length. Kondrak (2002) created a vari-
ant of the dynamic programming algorithm used
to compute edit distance which he used to iden-
tify cognates in historical linguistics. McMahon
& McMahon (2005) include investigations of pro-
nunciation similarity in their recent book on phy-
logenetic techniques in historical linguistics. Sev-
eral of the contributions to this volume build on
these earlier efforts or are relevant to them.
Kondrak and Sherif (this volume) continue the
investigation into techniques for identifying cog-
nates, now comparing several techniques which
rely solely on parameters set by the researcher to
machine learning techniques which automatically
optimize those parameters. They show the the ma-
chine learning techniques to be superior, in partic-
ular, techniques basic on hidden Markov models
and dynamic Bayesian nets.
Heeringa et al (this volume) investigate several
extensions of the fundamental edit distance algo-
rithm for use in dialectology, including sensitivity
to order and context as well syllabicity constraints,
which they argue to be preferable, and length nor-
malization and graded weighting schemes, which
they argue against.
Dinu & Dinu (this volume) investigate metrics
on string distances which attach more importance
to the initial parts of the string. They embed this
insight into a scheme in which n-grams are ranked
(sorted) by frequency, and the difference in the
rankings is used to assay language differences.
Their paper proves that difference in rankings is
a proper mathematical metric.
Singh (this volume) investigates the technical
question of identifying languages and character
encoding systems from limited amounts of text.
He collects about 1, 000 or so of the most fre-
quent n-grams of various sizes and then classifies
next texts based on the similarity between the fre-
2
quency distributions of the known texts with those
of texts to be classified. His empirical results show
?mutual cross entropy? to identify similarity most
reliably, but there are several close competitors.
3 Syntax
Although there is less interest in similarity at the
syntactic level among linguistic theorists, there is
still one important areas of theoretical research in
which it could play an important role and several
interdisciplinary studies in which similarity and/or
distant is absolutely crucial. Syntactic TYPOLOGY
is an area of linguistic theory which seeks to iden-
tify syntactic features which tend to be associated
with one another in all languages (Comrie, 1989;
Croft, 2001). The fundamental vision is that some
sorts of languages may be more similar to one
another?typologically?than would first appear.
Further, there are two interdisciplinary linguis-
tic studies in which similarity and/or distance
plays a great role, including similarity at the syn-
tactic level (without, however, exclusively focus-
ing on syntax). LANGUAGE CONTACT studies
seek to identify the elements of one language
which have been adopted in a second in a situa-
tion in which two or more languages are used in
the same community (Thomason and Kaufmann,
1988; van Coetsem, 1988). Naturally, these may
be non-syntactic, but syntactic CONTAMINATION
is a central concept which is recognized in con-
taminated varieties which have become more sim-
ilar to the languages which are the source of con-
tamination.
Essentially the same phenomena is studied in
SECOND-LANGUAGE LEARNING, in which syn-
tactic patterns from a dominant, usually first, lan-
guage are imposed on a second. Here the focus is
on the psychology of the individual language user
as opposed to the collective habits of the language
community.
Nerbonne and Wiersma (this volume) collect
frequency distributions of part-of-speech (POS)
trigrams and explore simple measures of distance
between these. They approach issues of statisti-
cal significance using permutation tests, which re-
quires attention to tricky issues of normalization
between the frequency distributions.
Homola & Kubon? (this volume) join Nerbonne
and Wiersma in advocating a surface-oriented
measure of syntactic difference, but base their
measure on dependency trees rather than POS
tags, a more abstract level of analysis. From there
they propose an analogue to edit distance to gauge
the degree of difference. The difference between
two tree is the sum of the costs of the tree-editing
operations needed to obtain one tree from another
(Noetzel and Selkow, 1999).
Emms (this volume) concentrates on applica-
tions of the notion ?tree similarity? in particular in
order to identify text which is syntactically sim-
ilar to questions and which may therefore be ex-
pected to constitute an answer to the question. He
is able to show that the tree-distance measure out-
performs sequence distance measures, at least if
lexical information is also emphasized.
Ku?bler (this volume) uses the similarity mea-
sure in memory-based learning to parse. This is
a surprising approach, since memory-based tech-
niques are normally used in classification tasks
where the target is one of a small number of po-
tential classifications. In parsing, the targets may
be arbitrarily complex, so a key step is select an
initial structure in a memory-based way, and then
to adapt it further. In this paper Ku?bler first applies
chunking to the sentence to be parsed and selects
an initial parse based on chunk similarity.
4 Semantics
While similarity as such has not been a prominent
term in theoretical and computational research on
natural language semantics, the study of LEXICAL
SEMANTICS, which attempts to identify regulari-
ties of and systematic relations among word mean-
ings, is more often than not predicated on an im-
plicit notion of ?semantic similarity?. Research
on the lexical semantics of verbs tries to identify
verb classes whose members exhibit similar syn-
tactic and semantic behavior. In logic-based the-
ories of word meaning (e.g., Vendler (1967) and
Dowty (1979)), verb classes are identified by sim-
ilarity patterns of inference, while Levin?s (1993)
study of English verb classes demonstrates that
similarities of word meanings for verbs can be
gleaned from their syntactic behavior, in particu-
lar from their ability or inability to participate in
diatheses, i.e. patterns of argument alternations.
With the increasing availability of large elec-
tronic corpora, recent computational research on
word meaning has focused on capturing the notion
of ?context similarity? of words. Such studies fol-
low the empiricist approach to word meaning sum-
marized best in the famous dictum of the British
3
linguist J.R. Firth: ?You shall know a word by the
company it keeps.? (Firth, 1957, p. 11) Context
similarity has been used as a means of extract-
ing collocations from corpora, e.g. by Church &
Hanks (1990) and by Dunning (1993), of identify-
ing word senses, e.g. by Yarowski (1995) and by
Schu?tze (1998), of clustering verb classes, e.g. by
Schulte im Walde (2003), and of inducing selec-
tional restrictions of verbs, e.g. by Resnik (1993),
by Abe & Li (1996), by Rooth et al (1999) and by
Wagner (2004).
A third approach to lexical semantics, devel-
oped by linguists and by cognitive psychologists,
primarily relies on the intuition of lexicographers
for capturing word meanings, but is also informed
by corpus evidence for determining word usage
and word senses. This type of approach has led to
two highly valued semantic resources: the Prince-
ton WordNet (Fellbaum, 1998) and the Berkeley
Framenet (Baker et al, 1998). While originally
developed for English, both approaches have been
successfully generalized to other languages.
The three approaches to word meaning dis-
cussed above try to capture different aspects of
the notion of semantic similarity, all of which are
highly relevant for current and future research in
computational linguistics. In fact, the five pa-
pers that discuss issues of semantic similarity in
the present volume build on insights from these
three frameworks or address open research ques-
tions posed by these frameworks. Zesch and
Gurevych (this volume) discuss how measures
of semantic similarity?and more generally: se-
mantic relatedness?can be obtained by similarity
judgments of informants who are presented with
word pairs and who, for each pair, are asked to
rate the degree of semantic relatedness on a pre-
defined scale. Such similarity judgments can pro-
vide important empirical evidence for taxonomic
models of word meanings such as wordnets, which
thus far rely mostly on expert knowledge of lexi-
cographers. To this end, Zesch and Gurevych pro-
pose a corpus-based system that supports fast de-
velopment of relevant data sets for large subject
domains.
St-Jacques and Barrie`re (this volume) review
and contrast different philosophical and psycho-
logical models for capturing the notion of seman-
tic similarity and different mathematical models
for measuring semantic distance. They draw at-
tention to the fact that, depending on which un-
derlying models are in use, different notions of se-
mantic similarity emerge and conjecture that dif-
ferent similarity metrics may be needed for differ-
ent NLP tasks. Dagan (this volume) also explores
the idea that different notions of semantic similar-
ity are needed when dealing with semantic disam-
biguation and language modeling tasks on the one
hand and with applications such as information ex-
traction, summarization, and information retrieval
on the other hand.
Dridan and Bond (this volume) and Hachey
(this volume) both consider semantic similarity
from an application-oriented perspective. Dri-
dan and Bond employ the framework of robust
minimal recursion semantics in order to obtain
a more adequate measure of sentence similar-
ity than can be obtained by word-overlap met-
rics for bag-of-words representations of sentences.
They show that such a more fine-grained mea-
sure, which is based on compact representations
of predicate-logic, yields better performance for
paraphrase detection as well as for sentence se-
lection in question-answering tasks than simple
word-overlap metrics. Hachey considers an au-
tomatic content extraction (ACE) task, a particu-
lar subtask of information extraction. He demon-
strates that representations based on term co-
occurrence outperform representations based on
term-by-document matrices for the task of iden-
tifying relationships between named objects in
texts.
Acknowledgments
We are indebted to our program committee and
to the incidental reviewers named in the organi-
zational section of the book, and to others who
remain anonymous. We thank Peter Kleiweg for
managing the production of the book and Therese
Leinonen for discussions about phonetic similar-
ity. We are indebted to the Netherlands Organi-
zation for Scientific Research (NWO), grant 200-
02100, for cooperation between the Center for
Language and Cognition, Groningen, and the Sem-
inar fu?r Sprachwissenschaft, Tu?bingen, for sup-
port of the work which is reported on here. We are
also indebted to the Volkswagen Stiftung for their
support of a joint project ?Measuring Linguistic
Unity and Diversity in Europe? that is carried out
in cooperation with the Bulgarian Academy of
Science, Sofia. The work reported here is directly
related to the research objectives of this project.
4
References
Naoki Abe and Hang Li. 1996. Learning word associ-
ation norms using tree cut pair models. In Proceed-
ings of 13th International Conference on Machine
Learning.
Adam Albright and Bruce Hayes. 2003. Rules
vs. analogy in English past tenses: A computa-
tional/experimental study. Cognition, 90:119?161.
Todd M. Bailey and Ulrike Hahn. 2005. Phoneme
Similarity and Confusability. Journal of Memory
and Language, 52(3):339?362.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, Proceedings of
the Thirty-Sixth Annual Meeting of the Association
for Computational Linguistics and Seventeenth In-
ternational Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Mor-
gan Kaufmann Publishers.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
John Clark and Colin Yallop. 1995. An Introduction to
Phonetics and Phonology. Blackwell, Oxford.
Bernard Comrie. 1989. Language Universals and Lin-
guistic Typology:Syntax and Morphology. Oxford,
Basil Blackwell.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Ox-
ford University Press, Oxford.
David Dowty. 1979. Word Meaning and Montague
Grammar. Reidel, Dordrecht.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
J. R. Firth. 1957. A synopsis of linguistic theory. Ox-
ford: Philological Society. Reprinted in F. Palmer
(ed.)(1968). Studies in Linguistic Analysis 1930-
1955. Selected Papers of J.R. Firth., Harlow: Long-
man.
Stefan A. Frisch, Janet B. Pierrehumbert, and
Michael B. Broe. 2004. Similarity Avoidance and
the OCP. Natural Language & Linguistic Theory,
22(1):179?228.
Ulrike Hahn and Todd M. Bailey. 2005. What Makes
Words Sound Similar? Cognition, 97(3):227?267.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Brett Kessler. 1995. Computational dialectology in
Irish Gaelic. In Proc. of the European ACL, pages
60?67, Dublin.
Grzegorz Kondrak. 2002. Algorithms for Language
Reconstruction. Ph.D. thesis, University of Toronto.
John Laver. 1994. Principles of Phonetics. Cambridge
Univeristy Press, Cambridge.
Ludovic Lebart and Martin Rajman. 2000. Comput-
ing similarity. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing, pages 477?505. Dekker, Basel.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a Preliminary Investigation. University of
Chicago Press, Chicago and London.
Paul A. Luce and David B. Pisoni. 1998. Recognizing
spoken words: The neighborhood activation model.
Ear and Hearing, 19(1):1?36.
April McMahon and Robert McMahon. 2005. Lan-
guage Classification by the Numbers. Oxford Uni-
versity Press, Oxford.
George A. Miller and Patricia E. Nicely. 1955. An
Analysis of Perceptual Confusions Among Some
English Consonants. The Journal of the Acoustical
Society of America, 27:338?352.
Andrew S. Noetzel and Stanley M. Selkow. 1999.
An analysis of the general tree-editing problem. In
David Sankoff and Joseph Kruskal, editors, Time
Warps, String Edits and Macromolecules: The The-
ory and Practice of Sequence Comparison, pages
237?252. CSLI, Stanford. 11983.
Philip Stuart Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing an semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, Maryland.
Sabine Schulte im Walde. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart. Pub-
lished as AIMS Report 9(2).
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Sarah Thomason and Terrence Kaufmann. 1988. Lan-
guage Contact, Creolization, and Genetic Linguis-
tics. University of California Press, Berkeley.
Frans van Coetsem. 1988. Loan Phonology and the
Two Transfer Types in Language Contact. Publica-
tions in Language Sciences. Foris Publications, Dor-
drecht.
5
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, Ithaca, NY.
Michael S. Vitevitch and Paul A. Luce. 1999. Prob-
abilistic Phonotactics and Neighborhood Activation
in Spoken Word Recognition. Journal of Memory
and Language, 40(3):374?408.
Andreas Wagner. 2004. Learning Thematic Role Rela-
tions for Lexical Semantic Nets. Ph.D. thesis, Uni-
versita?t Tu?bingen.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196,
Cambridge, MA.
6
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 111?119,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Is it Really that Difficult to Parse German?
Sandra Ku?bler, Erhard W. Hinrichs, Wolfgang Maier
SfS-CL, SFB 441, University of Tu?bingen
Wilhelmstr. 19
72074 Tu?bingen, Germany
 
kuebler,eh,wmaier@sfs.uni-tuebingen.de
Abstract
This paper presents a comparative study
of probabilistic treebank parsing of Ger-
man, using the Negra and Tu?Ba-D/Z tree-
banks. Experiments with the Stanford
parser, which uses a factored PCFG and
dependency model, show that, contrary to
previous claims for other parsers, lexical-
ization of PCFG models boosts parsing
performance for both treebanks. The ex-
periments also show that there is a big
difference in parsing performance, when
trained on the Negra and on the Tu?Ba-
D/Z treebanks. Parser performance for the
models trained on Tu?Ba-D/Z are compara-
ble to parsing results for English with the
Stanford parser, when trained on the Penn
treebank. This comparison at least sug-
gests that German is not harder to parse
than its West-Germanic neighbor language
English.
1 Introduction
There have been a number of recent studies on
probabilistic treebank parsing of German (Dubey,
2005; Dubey and Keller, 2003; Schiehlen, 2004;
Schulte im Walde, 2003), using the Negra tree-
bank (Skut et al, 1997) as their underlying data
source. A common theme that has emerged from
this research is the claim that lexicalization of
PCFGs, which has been proven highly beneficial
for other languages1 , is detrimental for parsing
accuracy of German. In fact, this assumption
is by now so widely held that Schiehlen (2004)
does not even consider lexicalization as a possible
1For English, see Collins (1999).
parameter and concentrates instead only on tree-
bank transformations of various sorts in his exper-
iments.
Another striking feature of all studies men-
tioned above are the relatively low parsing F-
scores achieved for German by comparison to the
scores reported for English, its West-Germanic
neighbor, using similar parsers. This naturally
raises the question whether German is just harder
to parse or whether it is just hard to parse the Ne-
gra treebank.2
The purpose of this paper is to address pre-
cisely this question by training the Stanford parser
(Klein and Manning, 2003b) and the LoPar parser
(Schmid, 2000) on the two major treebanks
available for German, Negra and Tu?Ba-D/Z, the
Tu?bingen treebank of written German (Telljohann
et al, 2005). A series of comparative parsing
experiments that utilize different parameter set-
tings of the parsers is conducted, including lexi-
calization and markovization. These experiments
show striking differences in performance between
the two treebanks. What makes this comparison
interesting is that the treebanks are of compara-
ble size and are both based on a newspaper cor-
pus. However, both treebanks differ significantly
in their syntactic annotation scheme. Note, how-
ever, that our experiments concentrate on the orig-
inal (context-free) annotations of the treebank.
The structure of this paper is as follows: sec-
tion 2 discusses three characteristic grammatical
features of German that need to be taken into ac-
count in syntactic annotation and in choosing an
appropriate parsing model for German. Section 3
introduces the Negra and Tu?Ba-D/Z treebanks and
2German is not the first language for which this question
has been raised. See Levy and Manning (2003) for a similar
discussion of Chinese and the Penn Chinese Treebank.
111
discusses the main differences between their anno-
tation schemes. Section 4 explains the experimen-
tal setup, sections 5-7 the experiments, and section
8 discusses the results.
2 Grammatical Features of German
There are three distinctive grammatical features
that make syntactic annotation and parsing of Ger-
man particularly challenging: its placement of the
finite verb, its flexible phrasal ordering, and the
presence of discontinuous constituents. These fea-
tures will be discussed in the following subsec-
tions.
2.1 Finite Verb Placement
In German, the placement of finite verbs depends
on the clause type. In non-embedded assertion
clauses, the finite verb occupies the second posi-
tion in the clause, as in (1a). In yes/no questions,
as in (1b), the finite verb appears clause-initially,
whereas in embedded clauses it appears clause fi-
nally, as in (1c).
(1) a. Peter
Peter
wird
will
das
the
Buch
book
gelesen
read
haben.
have
?Peter will have read the book.?
b. Wird
Will
Peter
Peter
das
the
Buch
book
gelesen
have
haben?
read
?Will Peter have read the book??
c. dass
that
Peter
Peter
das
the
Buch
book
gelesen
read
haben
have
wird.
will
?... that Peter will have read the book.?
Regardless of the particular clause type, any
cluster of non-finite verbs, such as gelesen haben
in (1a) and (1b) or gelesen haben wird in (1c), ap-
pears at the right periphery of the clause.
The discontinuous positioning of the verbal el-
ements in verb-first and verb-second clauses is the
traditional reason for structuring German clauses
into so-called topological fields (Drach, 1937;
Erdmann, 1886; Ho?hle, 1986). The positions of
the verbal elements form the Satzklammer (sen-
tence bracket) which divides the sentence into a
Vorfeld (initial field), a Mittelfeld (middle field),
and a Nachfeld (final field). The Vorfeld and the
Mittelfeld are divided by the linke Satzklammer
(left sentence bracket), which is realized by the
finite verb or (in verb-final clauses) by a comple-
mentizer field. The rechte Satzklammer (right sen-
tence bracket) is realized by the verb complex and
consists of verbal particles or sequences of verbs.
This right sentence bracket is positioned between
the Mittelfeld and the Nachfeld. Thus, the theory
of topological fields states the fundamental regu-
larities of German word order.
The topological field structures in (2) for the ex-
amples in (1) illustrate the assignment of topolog-
ical fields for different clause types.
(2) a.       Peter     wird        das
Buch    	   
 gelesen haben.  
b.   Wird        Peter     das Buch  
 	   
 gelesen haben?  
c.    
  dass         Peter     das
Buch    	   
 gelesen haben wird.  
(2a) and (2b) are made up of the following
fields: LK (for: linke Satzklammer) is occupied
by the finite verb. MF (for: Mittelfeld) contains
adjuncts and complements of the main verb. RK
(for: rechte Satzklammer) is realized by the ver-
bal complex (VC). Additionally, (2a) realizes the
topological field VF (for: Vorfeld), which contains
the sentence-initial constituent. The left sentence
bracket (LK) in (2c) is realized by a complemen-
tizer field (CF) and the right sentence bracket (RK)
by a verbal complex (VC) that contains the finite
verb wird.
2.2 Flexible Phrase Ordering
The second noteworthy grammatical feature of
German concerns its flexible phrase ordering. In
(3), any of the three complements and adjuncts
of the main verb (ge)lesen can appear sentence-
initially.
(3) a. Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen.
read
?The man read the novel yesterday.?
b. Gestern hat der Mann den Roman gelesen
c. Den Roman hat der Mann gestern gelesen
In addition, the ordering of the elements that oc-
cur in the Mittelfeld is also free so that there are
two possible linearizations for each of the exam-
ples in (3a) - (3b), yielding a total of six distinct
orderings for the three complements and adjuncts.
Due to this flexible phrase ordering, the gram-
matical functions of constituents in German, un-
like for English, cannot be deduced from the con-
stituents? location in the tree. As a consequence,
parsing approaches to German need to be based on
treebank data which contain a combination of con-
stituent structure and grammatical functions ? for
parsing and evaluation.
112
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
NP
OA
PP
MO HD
HD
NP
SB MO
VP
OC
S
Figure 1: A sample tree from Negra.
2.3 Discontinuous Constituents
A third characteristic feature of German syntax
that is a challenge for syntactic annotation and
for parsing is the treatment of discontinuous con-
stituents.
(4) Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen,
read
den
which
ihm
him
Peter
Peter
empfahl.
recommended
?Yesterday the man read the novel which Peter rec-
ommended to him.?
(5) Peter
Peter
soll
is to
dem
the
Mann
man
empfohlen
recommended
haben,
have
den
the
Roman
novel
zu
to
lesen.
read
?Peter is said to have recommended to the man to
read the novel.?
(4) shows an extraposed relative clause which
is separated from its head noun den Roman by the
non-finite verb gelesen. (5) is an example of an
extraposed non-finite VP complement that forms a
discontinuous constituent with its governing verb
empfohlen because of the intervening non-finite
auxiliary haben. Such discontinuous structures
occur frequently in both treebanks and are handled
differently in the two annotation schemes, as will
be discussed in more detail in the next section.
3 The Negra and the Tu?Ba-D/Z
Treebanks
Both treebanks use German newspapers as their
data source: the Frankfurter Rundschau news-
paper for Negra and the ?die tageszeitung? (taz)
newspaper for Tu?Ba-D/Z. Negra comprises 20 000
sentences, Tu?Ba-D/Z 15 000 sentences. There is
evidence that the complexity of sentences in both
treebanks is comparable: sentence length as well
as the percentage of clause nodes per sentence is
comparable. In Negra, a sentence is 17.2 words
long, in Tu?ba-D/Z, 17.5 words. Negra has an av-
erage of 1.4 clause nodes per sentence, Tu?Ba-D/Z
1.5 clause nodes.
Both treebanks use an annotation framework
that is based on phrase structure grammar and that
is enhanced by a level of predicate-argument struc-
ture. Annotation for both was performed semi-
automatically. Despite all these similarities, the
treebank annotations differ in four important as-
pects: 1) Negra does not allow unary branching
whereas Tu?Ba-D/Z does; 2) in Negra, phrases re-
ceive a flat annotation whereas Tu?Ba-D/Z uses
phrase internal structure; 3) Negra uses crossing
branches to represent long-distance relationships
whereas Tu?Ba-D/Z uses a pure tree structure com-
bined with functional labels to encode this infor-
mation; 4) Negra encodes grammatical functions
in a combination of structural and functional la-
beling whereas Tu?Ba-D/Z uses a combination of
topological fields functional labels, which results
in a flatter structure on the clausal level. The two
treebanks also use different notions of grammat-
ical functions: Tu?Ba-D/Z defines 36 grammati-
cal functions covering head and non-head infor-
mation, as well as subcategorization for comple-
ments and modifiers. Negra utilizes 48 grammat-
ical functions. Apart from commonly accepted
grammatical functions, such as SB (subject) or
OA (accusative object), Negra grammatical func-
tions comprise a more extended notion, e.g. RE
(repeated element) or RC (relative clause).
(6) Diese
This
Metapher
metaphor
kann
can
die
the
Freizeitmalerin
amateur painter
durchaus
by all means
auch
also
auf
to
ihr
her
Leben
life
anwenden.
apply.
?The amateur painter can by all means apply this
metaphor also to her life.?
Figure 1 shows a typical tree from the Negra
treebank for sentence (6). The syntactic categories
are shown in circular nodes, the grammatical func-
tions as edge labels in square boxes. A major
113
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
PP
MO HD
NP
OA HD
NP
SB MO
VP
OC
S
Figure 2: A Negra tree with resolved crossing branches.
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500 501 502 503 504 505
506 507 508 509 510
511 512
513
Den
ART
vorigen
ADJA
Sonntag
NN
h?tte
VAFIN
Frank
NE
Michael
NE
Nehr
NE
am
PTKA
liebsten
ADJD
aus
APPR
dem
ART
Kalender
NN
gestrichen
VVPP
.
$.
HD HD ? ? ? ? HD ? HD HD
?
ADJX
? HD
VXFIN
HD
NX
? ?
NX
HD
VXINF
OV
NX
OA
EN?ADD
ON
ADJX
MOD
PX
FOPP
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 3: A sample tree from Tu?ba-D/Z.
phrasal category that serves to structure the sen-
tence as a whole is the verb phrase (VP). It con-
tains non-finite verbs (here: anwenden) together
with their complements (here: the accusative ob-
ject Diese Metapher) and adjuncts (here: the ad-
verb durchaus and the PP modifier auch auf ihr
Leben). The subject NP (here: die Freizeitma-
lerin) stands outside the VP and, depending on its
linear position, leads to crossing branches with the
VP. This happens in all cases where the subject
follows the finite verb as in Figure 1. Notice also
that the PP is completely flat and does not contain
an internal NP.
Another phenomenon that leads to the introduc-
tion of crossing branches in the Negra treebank are
discontinuous constituents of the kind illustrated
in section 2.3. Extraposed relative clauses, as in
(4), are analyzed in such a way that the relative
clause constituent is a sister of its head noun in the
Negra tree and crosses the branch that dominates
the intervening non-finite verb gelesen.
The crossing branches in the Negra treebank
cannot be processed by most probabilistic parsing
models since such parsers all presuppose a strictly
context-free tree structure. Therefore the Negra
trees must be transformed into proper trees prior
to training such parsers. The standard approach
for this transformation is to re-attach crossing non-
head constituents as sisters of the lowest mother
node that dominates all constituents in question in
the original Negra tree.
Figure 2 shows the result of this transformation
of the tree in Figure 1. Here, the fronted accusative
object Diese Metapher is reattached on the clause
level. Crossing branches do not only arise with re-
spect to the subject at the sentence level but also in
cases of extraposition and fronting of partial con-
stituents. As a result, approximately 30% of all
Negra trees contain at least one crossing branch.
Thus, tree transformations have a major impact
on the type of constituent structures that are used
for training probabilistic parsing models. Previous
work, such as Dubey (2005), Dubey and Keller
(2003), and Schiehlen (2004), uses the version of
Negra in which the standard approach to resolving
crossing branches has been applied.
(7) Den
The
vorigen
previous
Sonntag
Sunday
ha?tte
would have
Frank
Frank
Michael
Michael
Nehr
Nehr
am liebsten
preferably
aus
from
dem
the
Kalender
calendar
gestrichen.
deleted.
?Frank Michael Nehr would rather have deleted the
previous Sunday from the calendar.?
Figure 3 shows the Tu?Ba-D/Z annotation for
sentence (7), a sentence with almost identi-
cal phrasal ordering to sentence (6). Crossing
branches are avoided by the introduction of topo-
114
0 1 2 3 4 5 6 7 8 9
500 501 502 503 504 505
506 507 508 509
510
511
F?r
APPR
diese
PDAT
Behauptung
NN
hat
VAFIN
Beckmeyer
NE
bisher
ADV
keinen
PIAT
Nachweis
NN
geliefert
VVPP
.
$.
? HD HD HD HD ? HD HD
?
NX
HD
VXFIN
HD
NX
ON
ADVX
MOD
NX
OA
VXINF
OV
PX
OA?MOD
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 4: Tu?Ba-D/Z annotation without crossing branches.
logical structures (here: VF, MF and VC) into the
tree. Notice also that compared to the Negra anno-
tation, Tu?Ba-D/Z introduces more internal struc-
ture into NPs and PPs.
(8) Fu?r
For
diese
this
Behauptung
claim
hat
has
Beckmeyer
Beckmeyer
bisher
yet
keinen
no
Nachweis
evidence
geliefert.
provided.
?For this claim, Beckmeyer has not provided evi-
dence yet.?
In Tu?Ba-D/Z, long-distance relationships are
represented by a pure tree structure and specific
functional labels. Figure 4 shows the Tu?Ba-D/Z
annotation for sentence (8). In this sentence,
the prepositional phrase Fu?r diese Behauptung is
fronted. Its functional label (OA-MOD ) provides
the information that it modifies the accusative ob-
ject (OA ) keinen Nachweis.
4 Experimental Setup
The main goals behind our experiments were
twofold: (1) to re-investigate the claim that lex-
icalization is detrimental for treebank parsing of
German, and (2) to compare the parsing results for
the two German treebanks.
To investigate the first issue, the Stanford Parser
(Klein and Manning, 2003b), a state-of-the-art
probabilistic parser, was trained with both lexical-
ized and unlexicalized versions of the two tree-
banks (Experiment I). For lexicalized parsing, the
Stanford Parser provides a factored probabilistic
model that combines a PCFG model with a depen-
dency model.
For the comparison between the two treebanks,
two types of experiments were performed: a
purely constituent-based comparison using both
the Stanford parser and the pure PCFG parser
LoPar (Schmid, 2000) (Experiment II), and an in-
depth evaluation of the three major grammatical
functions subject, accusative object, and dative
object, using the Stanford parser (Experiment III).
All three experiments use gold POS tags ex-
tracted from the treebanks as parser input. All
parsing results shown below are averaged over a
ten-fold cross-validation of the test data. Experi-
ments I and II used versions of the treebanks that
excluded grammatical information, thus only con-
tained constituent labeling. For Experiment III,
all syntactic labels were extended by their gram-
matical function (e.g NX-ON for a subject NP in
Tu?Ba-D/Z or NP-SB for a Negra subject). Experi-
ments I and II included all sentences of a maximal
length of 40 words. Due to memory limitations
(7 GB), Experiment III had to be restricted to sen-
tences of a maximal length of 35 words.
5 Experiment I: Lexicalization
Experiment I investigates the effect of lexicaliza-
tion on parser performance for the Stanford Parser.
The results, summarized in Table 1, show that lex-
icalization improves parser performance for both
the Negra and the Tu?Ba-D/Z treebank in compar-
ison to unlexicalized counterpart models: for la-
beled bracketing, an F-score improvement from
86.48 to 88.88 for Tu?Ba-D/Z and an improve-
ment from 66.92 to 67.13 for Negra. This di-
rectly contradicts the findings reported by Dubey
and Keller (2003) that lexicalization has a nega-
tive effect on probabilistic parsing models for Ger-
man. We therefore conclude that these previous
claims, while valid for particular configurations of
115
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
Stanford PCFG unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford lexicalized unlabeled 71.31 73.12 72.20 91.60 91.21 91.36
labeled 66.30 67.99 67.13 89.12 88.65 88.88
Table 1: The results of lexicalizing German.
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
LoPar unlabeled 70.84 72.51 71.67 92.62 88.58 90.56
labeled 65.86 67.41 66.62 87.39 83.57 85.44
Stanford unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford + markov unlabeled 74.13 74.12 74.12 92.28 90.90 91.58
labeled 69.96 69.95 69.95 89.86 88.51 89.18
Table 2: A comparison of unlexicalized parsing of Negra and Tu?Ba-D/Z.
parsers and parameters, should not be generalized
to claims about probabilistic parsing of German in
general.
Experiment I also shows considerable differ-
ences in the overall scores between the two tree-
banks, with the F-scores for Tu?Ba-D/Z parsing ap-
proximating scores reported for English, but with
Negra scores lagging behind by an average mar-
gin of appr. 20 points. Of course, it is impor-
tant to note that such direct comparisons with En-
glish are hardly possible due to different annota-
tion schemes, different underlying text corpora,
etc. Nevertheless, the striking difference in parser
performance between the two German treebanks
warrants further attention. Experiments II and III
will investigate this matter in more depth.
6 Experiment II: Different Parsers
The purpose of Experiment II is to rule out the pos-
sibility that the differences in parser performance
for the two German treebanks produced by Ex-
periment I may just be due to using a particular
parser ? in this particular case the hybrid PCFG
and dependency model of the Stanford parser. Af-
ter all, Experiment I also yielded different results
concerning the received wisdom about the utility
of lexicalization from previously reported results.
In order to obtain a broader experimental base, un-
lexicalized models of the Stanford parser and the
pure PCFG parser LoPar were trained on both tree-
banks. In addition we experimented with two dif-
ferent parameter settings of the Stanford parser,
one with and one without markovization. The ex-
periment with markovization used parent informa-
tion (v=1) and a second order Markov model for
horizontal markovization (h=2). The results, sum-
marized in Table 2, show that parsing results for all
unlexicalized experiments show roughly the same
20 point difference in F-score that were obtained
for the lexicalized models in Experiment I. We
can therefore conclude that the difference in pars-
ing performance is robust across two parsers with
different parameter settings, such as lexicalization
and markovization.
Experiment II also confirms the finding of Klein
and Manning (2003a) and of Schiehlen (2004) that
horizontal and vertical markovization has a pos-
itive effect on parser performance. Notice also
that markovization with unlexicalized grammars
yields almost the same improvement as lexicaliza-
tion does in Experiment I.
7 Experiment III: Grammatical
Functions
In Experiments I and II, only constituent structure
was evaluated, which is highly annotation depen-
dent. It could simply be the case that the Tu?Ba-
D/Z annotation scheme contains many local struc-
tures that can be easily parsed by a PCFG model
or the hybrid Stanford model. Moreover, such
easy to parse structures may not be of great im-
portance when it comes to determining the cor-
rect macrostructure of a sentence. To empirically
verify such a conjecture, a separate evaluation of
116
0 1 2 3 4
500
Moran
NE
ist
VAFIN
l?ngst
ADV
weiter
ADJD
.
$.
SB HD MO PD
S
Figure 5: Negra annotation without unary nodes.
Negra Tu?Ba-D/Z
lab. prec. lab. rec. lab. F-score lab. prec. lab. rec. lab. F-score
without gramm. functions 69.96 69.95 69.95 89.86 88.51 89.18
all gramm. functions 47.20 56.43 51.41 75.73 74.93 75.33
subjects 52.50 58.02 55.12 66.82 75.93 71.08
accusative objects 35.14 36.30 35.71 43.84 47.31 45.50
dative objects 8.38 3.58 5.00 24.46 9.96 14.07
Table 3: A comparison of unlexicalized, markovized parsing of constituent structure and grammatical
functions in Negra and Tu?Ba-D/Z.
parser performance for different constituent types
would be necessary. However, even such an eval-
uation would only be meaningful if the annotation
schemes agree on the defining characteristics of
such constituent types. Unfortunately, this is not
the case for the two treebanks under considera-
tion. Even for arguably theory-neutral constituents
such as NPs, the two treebanks differ considerably.
In the Negra annotation scheme, single word NPs
directly project from the POS level to the clausal
level, while in Tu?Ba-D/Z, they project by a unary
rule first to an NP. An extreme case of this Negra
annotation is shown in Figure 5 for sentence (9).
Here, all the phrases are one word phrases and are
thus projected directly to the clause level.
(9) Moran
Moran
ist
is
la?ngst
already
weiter.
further
?Moran is already one step ahead.?
There is an even more important motivation
for not focusing on the standard constituent-based
parseval measures ? at least when parsing Ger-
man. As discussed earlier in section 2.2, obtain-
ing the correct constituent structure for a German
sentence will often not be sufficient for determin-
ing its intended meaning. Due to the word order
freeness of phrases, a given NP in any one po-
sition may in principle fulfill different grammat-
ical functions in the sentence as a whole. There-
fore grammatical functions need to be explicitly
marked in the treebank and correctly assigned dur-
ing parsing. Since both treebanks encode gram-
matical functions, this information is available for
parsing and can ultimately lead to a more mean-
ingful comparison of the two treebanks when used
for parsing.
The purpose of Experiment III is to investigate
parser performance on the treebanks when gram-
matical functions are included in the trees. For
these experiments, the unlexicalized, markovized
PCFG version of the Stanford parser was used,
with markovization parameters v=1 and h=2, as
in Experiment II. The results of this experiment
are shown in Table 3. The comparison of the ex-
periments with (line 2) and without grammatical
functions (line 1) confirms the findings of Dubey
and Keller (2003) that the task of assigning cor-
rect grammatical functions is harder than mere
constituent-based parsing. When evaluating on all
grammatical functions, the results for Negra de-
crease from 69.95 to 51.41, and for Tu?Ba-D/Z
from 89.18 to 75.33. Notice however, that the rela-
tive differences between Negra and Tu?Ba-D/Z that
were true for Experiments I and II remain more or
less constant for this experiment as well.
In order to get a clearer picture of the quality
of the parser output for each treebank, it is im-
portant to consider individual grammatical func-
tions. As discussed in section 3, the overall in-
ventory of grammatical functions is different for
the two treebanks. We therefore evaluated those
grammatical functions separately that are crucial
for determining function-argument structure and
117
that are at the same time the most comparable for
the two treebanks. These are the functions of sub-
ject (encoded as SB in Negra and as ON in Tu?Ba-
D/Z), accusative object (OA ), and dative object
(DA in Negra and OD in Tu?Ba-D/Z). Once again,
the results are consistently better for Tu?Ba-D/Z
(cf. lines 3-5 in Table 3), with subjects yielding
the highest results (71.08 vs. 55.12 F-score) and
dative objects the lowest results (14.07 vs. 5.00).
The latter results must be attributed to data sparse-
ness, dative object occur only appr. 1 000 times
in each treebank while subjects occur more than
15 000 times.
8 Discussion
The experiments presented in sections 5-7 show
that there is a difference in results of appr. 20%
between Negra and Tu?Ba-D/Z. This difference is
consistent throughout, i.e. with different parsers,
under lexicalization and markovization. These re-
sults lead to the conjecture that the reasons for
these differences must be sought in the differences
in the annotation schemes of the two treebanks.
In section 3, we showed that one of the ma-
jor differences in annotation is the treatment of
discontinuous constituents. In Negra, such con-
stituents are annotated via crossing branches,
which have to be resolved before parsing. In such
cases, constituents are extracted from their mother
constituents and reattached at higher constituents.
In the case of the discontinuous VP in Figure 1,
it leads to a VP rule with the following daugh-
ters: head (HD ) and modifier (MO ), while the
accusative object is directly attached at the sen-
tence level as a sister of the VP. This conversion
leads to inconsistencies in the training data since
the annotation scheme requires that object NPs are
daughters of the VP rather than of S. The incon-
sistency introduced by tree conversion are con-
siderable since they cover appr. 30% of all Ne-
gra trees (cf. section 3). One possible explana-
tion for the better performance of Tu?ba-D/Z might
be that it has more information about the correct
attachment site of extraposed constituents, which
is completely lacking in the context-free version
of Negra. For this reason, Ku?bler (2005) and
Maier (2006) tested a version of Negra which con-
tained information of the original attachment site
of these discontinuous constituents. In this ver-
sion of Negra, the grammatical function OA in
Figure 2 would be changed to OA VP to show
that it was originally attached to the VP. Experi-
ments with this version showed a decrease in F-
score from 52.30 to 49.75. Consequently, adding
this information in a similar way to the encoding
of discontinuous constituents in Tu?ba-D/Z harms
performance.
By contrast, Tu?Ba-D/Z uses topological fields
as the primary structuring principle, which leads to
a purely context-free annotation of discontinuous
structures. There is evidence that the use of topo-
logical fields is advantageous also for other pars-
ing approaches (Frank et al, 2003; Ku?bler, 2005;
Maier, 2006).
Another difference in the annotation schemes
concerns the treatment of phrases. Negra phrases
are flat, and unary projections are not annotated.
Tu?Ba-D/Z always projects to the phrasal category
and annotates more phrase-internal structure. The
deeper structures in Tu?Ba-D/Z lead to fewer rules
for phrasal categories, which allows the parser a
more consistent treatment of such phrases. For ex-
ample, the direct attachment of one word subjects
on the clausal level in Negra leads to a high num-
ber of different S rules with different POS tags for
the subject phrase. An empirical proof for the as-
sumption that flat phrase structures and the omis-
sion of unary nodes decrease parsing results is pre-
sented by Ku?bler (2005) and Maier (2006).
We want to emphasize that our experiments
concentrate on the original context-free annota-
tions of the treebanks. We did not investigate
the influence of treebank refinement in this study.
However, we would like to note that by a com-
bination of suffix analysis and smoothing, Dubey
(2005) was able to obtain an F-score of 85.2 for
Negra. For other work in the area of treebank re-
finement using the German treebanks see Ku?bler
(2005), Maier (2006), and Ule (2003).
9 Conclusion and Future Work
We have presented a comparative study of proba-
bilistic treebank parsing of German, using the Ne-
gra and Tu?Ba-D/Z treebanks. Experiments with
the Stanford parser, which uses a factored PCFG
and dependency model, show that, contrary to
previous claims for other parsers, lexicalization
of PCFG models boosts parsing performance for
both treebanks. The experiments also show that
there is a big difference in parsing performance,
when trained on the Negra and on the Tu?Ba-D/Z
treebanks. This difference remains constant across
118
lexicalized, unlexicalized (also using the LoPar
parser), and markovized models and also extends
to parsing of major grammatical functions. Parser
performance for the models trained on Tu?Ba-D/Z
are comparable to parsing results for English with
the Stanford parser, when trained on the Penn tree-
bank. This comparison at least suggests that Ger-
man is not harder to parse than its West-Germanic
neighbor language English.
Additional experiments with the Tu?Ba-D/Z
treebank are planned in future work. A new re-
lease of the Tu?Ba-D/Z treebank has become avail-
able that includes appr. 22 000 trees, instead of
the release with 15 000 sentences used for the ex-
periments reported in this paper. This new re-
lease also contains morphological information at
the POS level, including case and number. With
this additional information, we expect consider-
able improvement in grammatical function assign-
ment for the functions subject, accusative object,
and dative object, which are marked by nomina-
tive, accusative, and dative case, respectively.
Acknowledgments
We are grateful to Helmut Schmid and to Chris
Manning and his group for making their parsers
publicly available as well as to Tylman Ule for
providing the evaluation scripts. We are also grate-
ful to the anonymous reviewers for many help-
ful comments. And we are especially grateful to
Roger Levy for all the help he gave us in creating
the language pack for Tu?Ba-D/Z in the Stanford
parser.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Erich Drach. 1937. Grundgedanken der Deutschen
Satzlehre. Diesterweg, Frankfurt/M.
Amit Dubey and Frank Keller. 2003. Probabilistic
parsing for German using sister-head dependencies.
In Proceedings of ACL 2003, pages 96?103, Sap-
poro, Japan.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: Parsing German with suffix analysis and
smoothing. In Proceedings of ACL 2005, Ann Ar-
bor, MI.
Oskar Erdmann. 1886. Grundzu?ge der deutschen
Syntax nach ihrer geschichtlichen Entwicklung
dargestellt. Verlag der Cotta?schen Buchhandlung,
Stuttgart, Germany.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Scha?fer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In
Proceedings of ACL 2003, Sapporo, Japan.
Tilman Ho?hle. 1986. Der Begriff ?Mittel-
feld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Dan Klein and Christopher Manning. 2003a. Accurate
unlexicalized parsing. In Proceedings of ACL 2003,
pages 423?430, Sapporo, Japan.
Dan Klein and Christopher Manning. 2003b. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 3?10,
Vancouver, Canada.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL 2003, pages 439?446, Sapporo,
Japan.
Wolfgang Maier. 2006. Annotation schemes and
their influence on parsing results. In Proceedings of
the ACL-2006 Student Research Workshop, Sydney,
Australia.
Michael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In Proceedings of
COLING 2004, Geneva, Switzerland.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart, Ger-
many.
Sabine Schulte im Walde. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of ANLP
1997, Washington, D.C.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Tylman Ule. 2003. Directed treebank refinement for
PCFG parsing. In Proceedings of TLT 2003, Va?xjo?,
Sweden.
119
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 456?464,
Beijing, August 2010
Standardizing Wordnets in the ISO Standard LMF: Wordnet-LMF for GermaNet 
Verena Henrich Universtiy of T?bingen Department of Linguistics verena.henrich@uni-tuebingen.de 
Erhard Hinrichs Universtiy of T?bingen Department of Linguistics erhard.hinrichs@uni-tuebingen.de  Abstract It has been recognized for quite some time that sustainable data formats play an important role in the development and curation of linguistic resources. The purpose of this paper is to show how GermaNet, the German version of the Princeton WordNet, can be con-verted to the Lexical Markup Frame-work (LMF), a published ISO standard (ISO-24613) for encoding lexical re-sources. The conversion builds on Wordnet-LMF, which has been pro-posed in the context of the EU KYOTO project as an LMF format for wordnets. The present paper proposes a number of crucial modifications and a set of extensions to Wordnet-LMF that are needed for conversion of wordnets in general and for conversion of Ger-maNet in particular. 1 Introduction It has been recognized for quite some time that sustainable data formats play an important role in the development and curation of linguistic resources. As witnessed by the success of the guidelines of the Text Encoding Initiative 1 (TEI) and of published standards issued by the International Standards Organization 2  (ISO), markup languages such as XML3 (short for: Extensible Markup Language) have become lingua francas for encoding linguistic resources of different types, including phonetic transcrip-                                                1 See http://www.tei-c.org 2 See http://www.iso.org 3 See http://www.w3.org/TR/REC-xml/ 
tions, (annotated) text corpora, and dictionar-ies. It is fair to say that it has become common practice among developers of new linguistic resources to consult TEI guidelines and ISO standards in order to develop standard-conformant encoding schemes that serve as an interchange format and that can be docu-mented and validated by Document Type Definitions (DTD) and XML schemata. However, for resources that were developed prior to or largely in parallel with the emerging acceptance of markup languages and of emerg-ing encoding standards, the situation is far more heterogeneous. A wide variety of legacy formats exists, many of which have persisted due to existing user communities and the availability of tools that can process only such idiosyncratic formats. The development of wordnets for a large number of languages is a typical example of a type of linguistic re-source, where legacy formats still persist as a de facto standard. WordNet 1.6 is encoded in the data format of lexicographer files4 that was designed for the English Princeton WordNet (Fellbaum, 1998). It is a plain-text format for storing wordnet data and allows lexicographers to encode lexical and conceptual relations among lexical units and synsets by use of spe-cial-purpose diacritics. There exist numerous tools that can process WordNet 1.6 lexicogra-pher files to extract relevant information or to transform the data into other special-purpose formats such as Prolog-fact databases. Even tough still widely used for the reasons just mentioned, the complexity of the format itself has a number of undesirable consequences. As Henrich and Hinrichs (2010) have pointed out,                                                 4 See http://wordnet.princeton.edu/man/lexnames.5 WN.html 
456
the editing of lexicographer files is highly er-ror-prone and time-consuming in actual lexi-cographic development. Moreover, format validation of the data as well as development of new tools for data visualization and data extraction become increasingly difficult since they cannot be based on generic state-of-the-art tools, that are, for example, available for XML-based encodings. For exactly these reasons, XML-based inter-change formats have been proposed in recent years also for wordnets. One of the first, if not the first, example is the XML format for Ger-maNet5, a wordnet for German (Lemnitzer and Kunze, 2002; Henrich and Hinrichs, 2010). An even more recent development along these lines is the specification of Wordnet-LMF (see Soria et al, 2009), an instantiation of the Lexi-cal Markup Framework6 (LMF, (Francopoulo et al, 2006)) customized for wordnets. Since LMF is an ISO standard (ISO-24613), it is a particularly attractive candidate for en-coding wordnets. Everything else being equal, ISO standards have a high chance of being adopted by a wide user community and of be-ing recognized as an interchange format.7 Such agreed-upon interchange formats are a crucial prerequisite for interoperable linguistic re-sources in the context of web services and of processing pipelines for linguistic resources. The purpose of this paper is threefold: 1. To compare and contrast the GermaNet XML initially proposed by Lemnitzer and Kunze (2002) with the Wordnet-LMF. This comparison is instructive since it reveals two completely differ-ent conceptions of representing seman-tic knowledge at the lexical level. 2. To point out a number of open issues that need to be resolved if Wordnet-LMF is to be adopted widely among 
                                                5 See http://www.sfs.uni-tuebingen.de/GermaNet/ 6 See http://www.lexicalmarkupframework.org 7 An anonymous reviewer raised the question why OWL is not a good candidate for encoding wordnets. On this issue, we agree with the assessment of Soria et al (2009) who point out that ?[?] RDF and OWL are conceptual repositories representation formats that are not designed to represent polysemy and store linguistic properties of words and word meanings.? 
wordnets for a steadily increasing number of languages. 3. To show how these open issues can be resolved in a customized version of Wordnet-LMF suitable for GermaNet. The remainder of this paper is structured as follows: section 2 provides a general introduc-tion to GermaNet. Details about the adapted XML format used for GermaNet up until now are provided in section 3. Section 4 introduces the challenge of how to represent a wordnet in the Lexical Markup Framework. As one possi-bility, Wordnet-LMF is regarded. Issues that arise during the conversion of GermaNet into Wordnet-LMF lead to a modified version of Wordnet-LMF. Finally, section 5 concludes with a comparison of the two representation formats. 2 GermaNet GermaNet is a lexical semantic network that is modeled after the Princeton WordNet for Eng-lish. It partitions the lexical space into a set of concepts that are interlinked by semantic rela-tions. A semantic concept is modeled by a syn-set. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same meaning. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units and a paraphrase (repre-sented as a string). The lexical units in turn have frames (which specify the syntactic va-lence of the lexical unit) and examples. The list of lexical units for a synset is never empty, but any of the other properties may be. There are two types of semantic relations in GermaNet: conceptual and lexical relations. Conceptual relations hold between two seman-tic concepts, i.e. synsets. They include rela-tions such as hyperonymy, part-whole rela-tions, entailment, or causation. Lexical rela-tions hold between two individual lexical units. Antonymy, a pair of opposites, is an example of a lexical relation. GermaNet covers the three word categories of adjectives, nouns, and verbs, each of which is hierarchically structured in terms of the hy-peronymy relation of synsets.  
457
 Figure 1. Structure of the XML synset files.   3 Current GermaNet XML Format The structure of the XML files closely follows the internal structure of GermaNet, which means that the file structure mirrors the under-lying relational organization of the data. There are two DTDs that jointly describe the XML-encoded GermaNet. One DTD represents all synsets with their lexical units and their attrib-utes (see subsection 3.1). The other DTD rep-resents all relations, both conceptual and lexi-cal relations (see subsection 3.2). The GermaNet XML format was initially developed by Kunze and Lemnitzer (2002), but modifications of the GermaNet data itself led to an adopted XML format, which is presented here.8 3.1 XML Synset Files The XML files that represent all synsets and lexical units of GermaNet are organized around the three word categories currently in-cluded in GermaNet: nouns, adjectives, and verbs (altogether 54 synset files since the se-mantic space for each word category is divided into a number of semantic subfields). The structure of each of these files is illus-trated in Figure 19. Each synset represents a set of lexical units (lexUnits) which all express the same meaning. This grouping represents the                                                 8 The interested reader might compare the version at hand with (Lemnitzer and Kunze, 2002) or (Kunze and Lem-nitzer, 2002), which both describe the initial GermaNet XML version. 9 In fact, this figure is not quite complete for the reason of simplicity. 
semantic relation of synonymy. Further prop-erties of a synset (e.g., the word category or a describing paraphrase) and a lexical unit (e.g., a sense number or the orthographical form (orthForm)) are encoded appropriately. Figure 1 describes the underlying XML structure. Each box in the figure stands for an element in the XML files, and the properties in each box (listed underneath the wavy line) rep-resent the attributes of an XML element. This means, for example, that a synset element has the attributes of an id and a category.10 Figure 2 shows an example of a synset with two lexical units (lexUnit elements) and a paraphrase. The lexUnit elements in turn con-tain several attributes and an orthographical form (the orthForm element), e.g., leuchten (German verb for: to shine). The first of the two lexical units even has a frame and an ex-ample.  <synset id="s58377" category="verben">   <lexUnit id="l82207"            sense="1"            namedEntity="no"            artificial="no"            styleMarking="no">     <orthForm>leuchten</orthForm>     <frame>NN</frame>     <example>       <text>         Der Mond leuchtete in der Nacht.       </text>       <exframe>NN</exframe>     </example>   </lexUnit>   <lexUnit id="l82208"                                                 10 Note that XML element or attribute names appear italic if they are referenced in the text. 
458
           sense="2"            namedEntity="no"            artificial="no"            styleMarking="no">     <orthForm>strahlen</orthForm>   </lexUnit>   <paraphrase>     Lichtstrahlen aussenden,     gro?e Helligkeit verbreiten   </paraphrase> </synset> Figure 2. Synset file example.  3.2 XML Relation File This type of XML file represents both kinds of relations: conceptual and lexical relations. All relations are encoded within one XML file, whose structure is illustrated in Figure 3.  
 Figure 3. Structure of the XML relation file.  The boxes in Figure 3 again represent XML elements, which means that there is one rela-tions element that contains all lexical relations (lex_rel elements) and conceptual relations (con_rel elements). Both relation types contain several attributes. Figure 4 illustrates an example for each of the two relation types. The type of the concep-tual relation is hyperonymy (indicated by the name attribute), and it holds between the syn-set with ID s58377 (from attribute) and the synset with ID s58376 (to attribute). The lexi-cal relation is of type antonymy (again indi-cated by the name attribute), and holds be-tween the lexical units with the IDs l2471 (from attribute) and l12470 (to attribute). 
<con_rel name="hyperonymy"          from="s58377" to="s58376"          dir="revert" inv="hyponymy" /> <lex_rel name="antonymy"          from="l2471" to="l2470"          dir="both" /> Figure 4. Example from relation file.  4 Wordnet-LMF The Lexical Markup Framework (ISO-24613) is an ISO standard for encoding natural lan-guage processing lexicons and machine read-able dictionaries (Francopoulo et al, 2006). The intention of LMF is to provide a common model for the creation and use of lexical re-sources, to manage the exchange of data be-tween and among these resources, and to en-able the merging of a large number of individ-ual electronic resources to form extensive global electronic resources. 4.1 The Challenge The core structure of LMF is based on the pro-totypical structuring of a lexicon in terms of lexical entries, each of which enumerates the different senses of the lexical item in question. This word-driven perspective contrasts the synset-driven relational structure of wordnets ? the grouping of word senses (i.e., lexical units) that express the same meaning into synsets. Exactly these two radically different organiz-ing principles (relation-based in the case of wordnets versus lexical-entry-based in the case of LMF) constitute the challenge of encoding wordnets in LMF. We take up this challenge: How can a synset-based wordnet, e.g. Ger-maNet, be represented in a word-driven format like LMF? 4.2 Apply LMF to Wordnets The conversion of GermaNet to LMF will build on Wordnet-LMF (Soria et al, 2009; Lee et al, 2009), an existing Lexical Markup Framework subset11. Wordnet-LMF has been developed in the context of the EU KYOTO                                                11 Wordnet-LMF is a proper subset of LMF since there are specifications in LMF that are not in Wordnet-LMF and since there is nothing in Wordnet-LMF which is not in LMF. Soria et al (2009) themselves refer to Wordnet-LMF as an LMF dialect. 
459
 Figure 5. The Wordnet-LMF structure.   project12 and is especially tailored to encode wordnets in the LMF standard. Wordnet-LMF is specified by a Document Type Definition (see Appendix E in (Soria and Monachini, 2008)) and fully complies with standard LMF. The Wordnet-LMF XML structure is shown in Figure 513 . There is a Lexical Resource which contains at least one Lexicon (in this case a wordnet lexicon).14 A Lexical Entry rep-resents a word entry in a Lexicon, where the word itself is represented by the writtenForm attribute of the Lemma element. Lexical En-tries group different Senses of a particular word. The Senses have a synset attribute that relates them to a Synset element by the corre-sponding ID. If two Senses have the same syn-set attribute, they belong to the same Synset and are thus synonyms. A Synset can have several relations to other Synsets. These relations are encoded in Syn-setRelation elements.                                                 12 See http://www.kyoto-project.eu 13 Note that this figure does not show the whole Wordnet-LMF model. Only the monolingual part that is relevant for this paper is represented. The representation of multi-lingual resources (i.e., the optional SenseAxis element with its children) is not considered in this paper. For a complete picture, see Soria et Monachini (2008). 14 Here, XML element or attribute names again appear italic if they are referenced in the text. 
4.3 Apply Wordnet-LMF to GermaNet The differences between the synset-driven structure of GermaNet (see Figures 1 and 3) and the word-driven format of Wordnet-LMF (see Figure 5) are obvious. But there is also a strong commonality: Both formats have synset elements that cluster synonymous words. In GermaNet, the words are represented by lexi-cal units that are child elements of a synset. In Wordnet-LMF, senses, which correspond to the lexical units in GermaNet, are linked to a synset (by an attribute containing a synset ID). The conversion of GermaNet to Wordnet-LMF proceeds as follows: Each lexical unit of GermaNet is turned into a Sense element in Wordnet-LMF (see Figure 5). The synset at-tribute (containing a Synset ID) of the Sense element links this Sense with the Synset that it is a member of. The different Sense elements are grouped by their orthographical form (the Lemma in Wordnet-LMF) into Lexical Entries. An example of a GermaNet LexicalEntry in Wordnet-LMF is shown in Figure 6. This LexicalEntry represents the word leuchten (German verb for: to shine), as the written-Form attribute of the Lemma element indi-cates. This LexicalEntry has two Senses, which belong to different Synsets (see the different synset attributes of the Sense elements). 
460
Each Sense has a MonolingualExternalRefs element with at least one MonolingualExter-nalRef representing a reference to an external system. In this case, each Sense is linked to the corresponding entry in the GermaNet data-base 15 ; the externalReference attribute of a MonolingualExternalRef specifies the database table name with a database ID.  <LexicalEntry id="deu-52-l4601-v">   <Lemma writtenForm="leuchten"                        partOfSpeech="v" />   <Sense id="deu-52-l4601-v_1"                  synset="deu-52-s58377-v">     <MonolingualExternalRefs>       <MonolingualExternalRef         externalSystem="GermaNet-Database"         externalReference=                "lex_uni_table#id=82207" />     </MonolingualExternalRefs>   </Sense>   <Sense id="deu-52-l4601-v_2"                  synset="deu-52-s58718-v">     <MonolingualExternalRefs>       <MonolingualExternalRef         externalSystem="GermaNet-Database"         externalReference=                "lex_uni_table#id=82677" />     </MonolingualExternalRefs>   </Sense> </LexicalEntry> Figure 6. Example of a LexicalEntry.  In the next conversion step, all synsets of Ger-maNet are listed with their relations to other synsets. The corresponding Synset (with the ID deu-52-s58377-v) of the first Sense in Figure 6 is illustrated in Figure 7. It has, inter alia, a describing gloss and two example sentences. The element SynsetRelations encodes rela-tions to other Synset instances. The relations are simply encoded with a target attribute that contains the ID of the referencing Synset. The Synsets in Wordnet-LMF are logically the ?same? as the synsets in GermaNet XML, i.e. the concept that a synset expresses is exactly the same in both formats. Each Synset has a reference to the Ger-maNet database. Therefore, the Monolin-gualExternalRef element links to the corre-sponding entry in the GermaNet database; the 
                                                15 For efficency reasons, GermaNet is stored in a relational database. 
externalReference attribute specifies the data-base table name with the synsets database ID.  <Synset id="deu-52-s58377-v"                           baseConcept="1">   <Definition gloss="Lichtstrahlen                aussenden, gro?e Helligkeit                verbreiten">     <Statement example="Der Mond leuchtete                           in der Nacht."/>     <Statement example="Die Lichter der            Stadt strahlen in die Nacht."/>   </Definition>   <SynsetRelations>     <SynsetRelation                  target="deu-52-s58376-v"                  relType="has_hyperonym"/>   </SynsetRelations>   <MonolingualExternalRefs>     <MonolingualExternalRef         externalSystem="GermaNet-Database"         externalReference=                  "synset_table#id=58377"/>   </MonolingualExternalRefs> </Synset> Figure 7. Example of a Synset.  These two Figures 6 and 7 represent the same example in Wordnet-LMF that was already shown in the GermaNet XML format in Figure 1. 4.4 Necessary Modifications to Wordnet-LMF As the previous discussion has shown, Word-net-LMF provides a very useful basis for con-verting GermaNet into LMF. However, a number of modifications to Wordnet-LMF are needed if this conversion is to preserve all in-formation present in the original resource. The present section will discuss a number of modi-fications to Wordnet-LMF that are needed for conversion of wordnets in general. In addition, we will also discuss a set of extensions to Wordnet-LMF that are needed for conversion of GermaNet in particular. The most glaring omission in Wordnet-LMF concerns the modeling of lexical relations which hold between lexical units (i.e., Senses in the terminology of Wordnet-LMF). In the current Wordnet-LMF DTD only conceptual relations (i.e., SynsetRelations in the terminol-ogy of Wordnet-LMF), which hold between synsets, are modeled. Thus antonymy, which is a typical example of a lexical relation (see (Fellbaum, 1998) for further details), can cur-
461
rently not be modeled without violating the Wordnet-LMF DTD. Among the synset relations specified in Wordnet-LMF, the entailment relation is miss-ing, which plays a crucial role in the modeling of verbs in the Princeton WordNet and in GermaNet alke. The list of values of attribute relType for SynsetRelation elements (see Ap-pendix A in (Soria and Monachini, 2008)) therefore has to be amended accordingly.16 A third omission in the current Wordnet-LMF DTD concerns syntactic frames used in the Princeton WordNet to indicate the syntac-tic valence of a given word sense. Syntactic frames are also used in GermaNet, albeit using a different encoding17. Syntactic frames to-gether with example sentences, which illustrate the meaning and prototypical usage of a par-ticular word, help to distinguish among word senses. In WordNet both syntactic frames and ex-amples are linked to synsets. However, at least in the case of syntactic frames the linkage to synsets seems problematic since different members of the same synset may well have different valence frames. For example, the German verbs finden and begegnen both mean meet and thus belong to the same synset. Both are transitive verbs, but their object NPs have different cases: accusative case for treffen and dative case for begegnen. As this example shows, syntactic frames need to be associated with lexical units rather than synsets. This is exactly the design choice made in GermaNet, as shown in Figure 1. A related question concerns the anchoring of example sentences which illustrate the mean-ings and prototypical usage of a particular word sense. In both the Princeton WordNet and GermaNet such examples are associated
                                                16 Piek Vossen (personal communication) has pointed out to us that Wordnet-LMF does not impose a list of rela-tions as a standard yet. 17 In WordNet, frames are encoded in a controlled lan-guage using paraphrases such as Somebody ----s some-thing for a transitive verb with an animate subject and an inanimate object. The frames in GermaNet use comple-mentation codes provided with the German version of the CELEX Lexical Database (Baayen et al, 2005) such as NN.AN for transitive verbs with accusative objects. 
with lexical units18. GermaNet correlates ex-amples additionally with particular syntactic frames and treats both examples and syntactic frames as properties of lexical units, i.e. Senses in the terminology of Wordnet-LMF. The above issues lead to a modified version of the Wordnet-LMF DTD as shown in Figure 8. Compared to Figure 5, the Sense element is enriched by three optional subelements: Sen-seRelations, SenseExamples, and Subcategori-zationFrames. It has to be noted, though, that LMF proper contains all necessary elements. The three no-tions SenseRelation, SenseExample, and Sub-categorizationFrame come from LMF proper and these elements can be used to remedy the omissions in Wordnet-LMF. The SenseRelation element in Figure 8 rep-resents relations between different Senses (the lexical units in GermaNet). The SenseExam-ples and SubcategorizationFrames elements both group several SenseExample or Subcate-gorizationFrame instances. A Subcategoriza-tionFrame element represents the syntactic valence of a word sense. A SenseExample shows the prototypical usage of a word sense as an example sentence. The syntactic valence for a concrete example sentence can be speci-fied with the optional frame attribute of a Sen-seExample. 5 Conclusion: Comparing GermaNet XML with Wordnet-LMF XML We would like to conclude with a comparison between the GermaNet native XML format described in section 3 and the modified Word-net-LMF format described in section 4.4. Since the GermaNet native XML format was particu-larly tailored to the structure of GermaNet, it enjoys the usual advantages of such custom-ized solutions: it contains all and only the nec-essary XML elements and attributes to de-scribe the resource. Moreover, the data are dis-tributed over 55 different XML files, which facilitates easy data handling and efficient search by word classes and lexical fields. These properties are in fact exploited by a number of GermaNet-specific tools, including                                                18 In WordNet, the examples are placed at the synset level, but referencing to a word sense at the same time. 
462
 Figure 8. Revised Wordnet-LMF structure.   a GermaNet-Explorer, a tool for data explora-tion and retrieval, and a GermaNet Pathfinder, a tool for the calculation of semantic related-ness, similarity, and distance (Cramer and Finthammer, 2008). All of these tools utilize the Java API that has been developed for the GermaNet native XML format. At the same time the GermaNet native XML format is a proprietary data format that was developed at a time when the only de facto encoding standard for wordnets consisted of the lexicographer files, originally developed for the Princeton WordNet. As such GermaNet XML was never developed with the goal of providing an XML standard for modeling wordnets in general. With Wordnet-LMF a candidate standard has now been proposed that is compliant with the LMF ISO standard for lexical resources and that strives to provide a general encoding standard of wordnets for dif-ferent languages. As the discussion in section 4.4 has shown, the current Wordnet-LMF DTD still needs to be amended to account for the full range of wordnet relations, frames, and examples (see Figure 8). These elements are not in Wordnet-LMF because Wordnet-LMF is a subset, but these elements are defined in the ISO document 24613 where LMF proper is defined. However, Wordnet-LMF appears to be suitably mature to serve as an interchange format for wordnets of different languages as 
well as for linking wordnets of different lan-guages with one another19. Acknowledgements The research reported in this paper was funded by the BW-eSci(T) grant sponsored by the Ministerium f?r Wissenschaft, Forschung und Kunst Baden-W?rttemberg. We would like to thank Piek Vossen and an anonymous reviewer for valuable comments on an earlier version of this paper. References Baayen, R. H., R. Piepenbrock, and L. Gulikers. 2005. The CELEX Lexical Database (Release 2) CD-ROM. Philadelphia, PA: Linguistic Data Consortium, University of Pennsylvania (Distributor). Cramer, Irene, and Marc Finthammer. 2008. Tools for Exploring GermaNet in the Context of CL-Teaching. In: Angelika Storrer, Alexander Geyken, Alexander Siebert, and Kay-Michael W?rzner, (Eds.): Text Resources and Lexical Knowledge. Selected Papers from the 9th Con-ference on Natural Language Processing KON-VENS 2008. Berlin/New York: Mouton de Gruyter, 195-208. 
                                                19 For example, the Interlingual Index, based on the Princeton WordNet, can be used to link different word-nets with one another. 
463
Kunze, Claudia, and Lothar Lemnitzer. 2002. Ger-maNet ? representation, visualization, applica-tion. Proceedings of LREC 2002, main confer-ence, Vol V. pp. 1485-1491. Fellbaum, Christiane (eds.). 1998. WordNet ? An Electronic Lexical Database. The MIT Press. Francopoulo, Gil, Monte George, Nicoletta Calzo-lari, Monica Monachini, Nuria Bel, Mandy Pet, and Claudia Soria. 2006. Lexical markup framework (LMF). Proceedings of the 5th Inter-national Conference on Language Resources and Evaluation (LREC 2006). Genoa, Italy. Henrich, Verena, and Erhard Hinrichs. 2010. Gern-EdiT ? The GermaNet Editing Tool. Proceed-ings of LREC 2010, main conference. Valletta, Malta. Lee, Lung-Hao, Shu-Kai Hsieh, and Chu-Ren Huang. 2009. CWN-LMF: Chinese WordNet in the Lexical Markup Framework. Proceedings of the 7th Workshop on Asian Resources. Suntec, Singapore, August 06 - 07, 2009, pp. 123-130 Lemnitzer, Lothar, and Claudia Kunze. 2002. Adapting GermaNet for the Web. Proceedings of the First Global Wordnet Conference. Central Institute of Indian Languages, Mysore, India, 21.-25.01.2002, pp. 174-181 Soria, Claudia, Monica Monachini, and Piek Vossen. 2009. Wordnet-LMF: Fleshing out a Standardized Format for Wordnet Interoperabil-ity. Proceedings of ACM Workshop on Intercultural Collaboration. Soria, Claudia, and Monica Monachini. 2008. Kyoto-LMF ? Wordnet representation format. KYOTO Working paper: WP02_TR002_V04_Kyoto_LMF. Vossen, Piek, Eneko Agirre, Nicoletta Calzolari, Christiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hitoshi Isahara, Kyoko Kanzaki, Andrea Marchetti, Monica Monachini, Federico Neri, Remo Raffaelli, German Rigau, Maurizio Tescon, and Joop VanGent. 2008. KYOTO: A system for mining, structuring and distributing knowledge across languages and cultures. Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08). Marrakech, Morocco. 
464
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 387?396,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
WebCAGe ? A Web-Harvested Corpus Annotated with GermaNet Senses
Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova
University of Tu?bingen
Department of Linguistics
{firstname.lastname}@uni-tuebingen.de
Abstract
This paper describes an automatic method
for creating a domain-independent sense-
annotated corpus harvested from the web.
As a proof of concept, this method has
been applied to German, a language for
which sense-annotated corpora are still in
short supply. The sense inventory is taken
from the German wordnet GermaNet. The
web-harvesting relies on an existing map-
ping of GermaNet to the German version
of the web-based dictionary Wiktionary.
The data obtained by this method consti-
tute WebCAGe (short for: Web-Harvested
Corpus Annotated with GermaNet Senses),
a resource which currently represents the
largest sense-annotated corpus available for
German. While the present paper focuses
on one particular language, the method as
such is language-independent.
1 Motivation
The availability of large sense-annotated corpora
is a necessary prerequisite for any supervised and
many semi-supervised approaches to word sense
disambiguation (WSD). There has been steady
progress in the development and in the perfor-
mance of WSD algorithms for languages such as
English for which hand-crafted sense-annotated
corpora have been available (Agirre et al 2007;
Erk and Strapparava, 2012; Mihalcea et al 2004),
while WSD research for languages that lack these
corpora has lagged behind considerably or has
been impossible altogether.
Thus far, sense-annotated corpora have typi-
cally been constructed manually, making the cre-
ation of such resources expensive and the com-
pilation of larger data sets difficult, if not com-
pletely infeasible. It is therefore timely and ap-
propriate to explore alternatives to manual anno-
tation and to investigate automatic means of cre-
ating sense-annotated corpora. Ideally, any auto-
matic method should satisfy the following crite-
ria:
(1) The method used should be language inde-
pendent and should be applicable to as many
languages as possible for which the neces-
sary input resources are available.
(2) The quality of the automatically generated
data should be extremely high so as to be us-
able as is or with minimal amount of manual
post-correction.
(3) The resulting sense-annotated materials (i)
should be non-trivial in size and should be
dynamically expandable, (ii) should not be
restricted to a narrow subject domain, but
be as domain-independent as possible, and
(iii) should be freely available for other re-
searchers.
The method presented below satisfies all of
the above criteria and relies on the following re-
sources as input: (i) a sense inventory and (ii) a
mapping between the sense inventory in question
and a web-based resource such as Wiktionary1 or
1http://www.wiktionary.org/
387
Wikipedia2.
As a proof of concept, this automatic method
has been applied to German, a language for which
sense-annotated corpora are still in short supply
and fail to satisfy most if not all of the crite-
ria under (3) above. While the present paper
focuses on one particular language, the method
as such is language-independent. In the case
of German, the sense inventory is taken from
the German wordnet GermaNet3 (Henrich and
Hinrichs, 2010; Kunze and Lemnitzer, 2002).
The web-harvesting relies on an existing map-
ping of GermaNet to the German version of the
web-based dictionary Wiktionary. This mapping
is described in Henrich et al(2011). The
resulting resource consists of a web-harvested
corpus WebCAGe (short for: Web-Harvested
Corpus Annotated with GermaNet Senses),
which is freely available at: http://www.sfs.uni-
tuebingen.de/en/webcage.shtml
The remainder of this paper is structured as
follows: Section 2 provides a brief overview of
the resources GermaNet and Wiktionary. Sec-
tion 3 introduces the mapping of GermaNet to
Wiktionary and how this mapping can be used
to automatically harvest sense-annotated materi-
als from the web. The algorithm for identifying
the target words in the harvested texts is described
in Section 4. In Section 5, the approach of au-
tomatically creating a web-harvested corpus an-
notated with GermaNet senses is evaluated and
compared to existing sense-annotated corpora for
German. Related work is discussed in Section 6,
together with concluding remarks and an outlook
on future work.
2 Resources
2.1 GermaNet
GermaNet (Henrich and Hinrichs, 2010; Kunze
and Lemnitzer, 2002) is a lexical semantic net-
work that is modeled after the Princeton Word-
Net for English (Fellbaum, 1998). It partitions the
2http://www.wikipedia.org/
3Using a wordnet as the gold standard for the sense inven-
tory is fully in line with standard practice for English where
the Princeton WordNet (Fellbaum, 1998) is typically taken
as the gold standard.
lexical space into a set of concepts that are inter-
linked by semantic relations. A semantic concept
is represented as a synset, i.e., as a set of words
whose individual members (referred to as lexical
units) are taken to be (near) synonyms. Thus, a
synset is a set-representation of the semantic rela-
tion of synonymy.
There are two types of semantic relations in
GermaNet. Conceptual relations hold between
two semantic concepts, i.e. synsets. They in-
clude relations such as hypernymy, part-whole re-
lations, entailment, or causation. Lexical rela-
tions hold between two individual lexical units.
Antonymy, a pair of opposites, is an example of a
lexical relation.
GermaNet covers the three word categories of
adjectives, nouns, and verbs, each of which is
hierarchically structured in terms of the hyper-
nymy relation of synsets. The development of
GermaNet started in 1997, and is still in progress.
GermaNet?s version 6.0 (release of April 2011)
contains 93407 lexical units, which are grouped
into 69594 synsets.
2.2 Wiktionary
Wiktionary is a web-based dictionary that is avail-
able for many languages, including German. As
is the case for its sister project Wikipedia, it
is written collaboratively by volunteers and is
freely available4. The dictionary provides infor-
mation such as part-of-speech, hyphenation, pos-
sible translations, inflection, etc. for each word.
It includes, among others, the same three word
classes of adjectives, nouns, and verbs that are
also available in GermaNet. Distinct word senses
are distinguished by sense descriptions and ac-
companied with example sentences illustrating
the sense in question.
Further, Wiktionary provides relations to
other words, e.g., in the form of synonyms,
antonyms, hypernyms, hyponyms, holonyms, and
meronyms. In contrast to GermaNet, the relations
are (mostly) not disambiguated.
For the present project, a dump of the Ger-
man Wiktionary as of February 2, 2011 is uti-
4Wiktionary is available under the Cre-
ative Commons Attribution/Share-Alike license
http://creativecommons.org/licenses/by-sa/3.0/deed.en
388
Figure 1: Sense mapping of GermaNet and Wiktionary using the example of Bogen.
lized, consisting of 46457 German words com-
prising 70339 word senses. The Wiktionary data
was extracted by the freely available Java-based
library JWKTL5.
3 Creation of a Web-Harvested Corpus
The starting point for creating WebCAGe is an
existing mapping of GermaNet senses with Wik-
tionary sense definitions as described in Henrich
et al(2011). This mapping is the result of a
two-stage process: i) an automatic word overlap
alignment algorithm in order to match GermaNet
senses with Wiktionary sense descriptions, and
ii) a manual post-correction step of the automatic
alignment. Manual post-correction can be kept at
a reasonable level of effort due to the high accu-
racy (93.8%) of the automatic alignment.
The original purpose of this mapping was to
automatically add Wiktionary sense descriptions
to GermaNet. However, the alignment of these
two resources opens up a much wider range of
5http://www.ukp.tu-darmstadt.de/software/jwktl
possibilities for data mining community-driven
resources such as Wikipedia and web-generated
content more generally. It is precisely this poten-
tial that is fully exploited for the creation of the
WebCAGe sense-annotated corpus.
Fig. 1 illustrates the existing GermaNet-
Wiktionary mapping using the example word Bo-
gen. The polysemous word Bogen has three dis-
tinct senses in GermaNet which directly corre-
spond to three separate senses in Wiktionary6.
Each Wiktionary sense entry contains a definition
and one or more example sentences illustrating
the sense in question. The examples in turn are
often linked to external references, including sen-
tences contained in the German Gutenberg text
archive7 (see link in the topmost Wiktionary sense
entry in Fig. 1), Wikipedia articles (see link for
the third Wiktionary sense entry in Fig. 1), and
other textual sources (see the second sense en-
try in Fig. 1). It is precisely this collection of
6Note that there are further senses in both resources not
displayed here for reasons of space.
7http://gutenberg.spiegel.de/
389
Figure 2: Sense mapping of GermaNet and Wiktionary using the example of Archiv.
heterogeneous material that can be harvested for
the purpose of compiling a sense-annotated cor-
pus. Since the target word (rendered in Fig. 1
in bold face) in the example sentences for a par-
ticular Wiktionary sense is linked to a GermaNet
sense via the sense mapping of GermaNet with
Wiktionary, the example sentences are automati-
cally sense-annotated and can be included as part
of WebCAGe.
Additional material for WebCAGe is harvested
by following the links to Wikipedia, the Guten-
berg archive, and other web-based materials. The
external webpages and the Gutenberg texts are ob-
tained from the web by a web-crawler that takes
some URLs as input and outputs the texts of the
corresponding web sites. The Wikipedia articles
are obtained by the open-source Java Wikipedia
Library JWPL 8. Since the links to Wikipedia, the
Gutenberg archive, and other web-based materials
also belong to particular Wiktionary sense entries
that in turn are mapped to GermaNet senses, the
target words contained in these materials are au-
tomatically sense-annotated.
Notice that the target word often occurs more
8http://www.ukp.tu-darmstadt.de/software/jwpl/
than once in a given text. In keeping with
the widely used heuristic of ?one sense per dis-
course?, multiple occurrences of a target word in
a given text are all assigned to the same GermaNet
sense. An inspection of the annotated data shows
that this heuristic has proven to be highly reliable
in practice. It is correct in 99.96% of all target
word occurrences in the Wiktionary example sen-
tences, in 96.75% of all occurrences in the exter-
nal webpages, and in 95.62% of the Wikipedia
files.
WebCAGe is developed primarily for the pur-
pose of the word sense disambiguation task.
Therefore, only those target words that are gen-
uinely ambiguous are included in this resource.
Since WebCAGe uses GermaNet as its sense in-
ventory, this means that each target word has at
least two GermaNet senses, i.e., belongs to at least
two distinct synsets.
The GermaNet-Wiktionary mapping is not al-
ways one-to-one. Sometimes one GermaNet
sense is mapped to more than one sense in Wik-
tionary. Fig. 2 illustrates such a case. For
the word Archiv each resource records three dis-
tinct senses. The first sense (?data repository?)
390
in GermaNet corresponds to the first sense in
Wiktionary, and the second sense in GermaNet
(?archive?) corresponds to both the second and
third senses in Wiktionary. The third sense in
GermaNet (?archived file?) does not map onto any
sense in Wiktionary at all. As a result, the word
Archiv is included in the WebCAGe resource with
precisely the sense mappings connected by the
arrows shown in Fig. 2. The fact that the sec-
ond GermaNet sense corresponds to two sense
descriptions in Wiktionary simply means that the
target words in the example are both annotated by
the same sense. Furthermore, note that the word
Archiv is still genuinely ambiguous since there is
a second (one-to-one) mapping between the first
senses recorded in GermaNet and Wiktionary, re-
spectively. However, since the third GermaNet
sense is not mapped onto any Wiktionary sense at
all, WebCAGe will not contain any example sen-
tences for this particular GermaNet sense.
The following section describes how the target
words within these textual materials can be auto-
matically identified.
4 Automatic Detection of Target Words
For highly inflected languages such as German,
target word identification is more complex com-
pared to languages with an impoverished inflec-
tional morphology, such as English, and thus re-
quires automatic lemmatization. Moreover, the
target word in a text to be sense-annotated is
not always a simplex word but can also appear
as subpart of a complex word such as a com-
pound. Since the constituent parts of a compound
are not usually separated by blank spaces or hy-
phens, German compounding poses a particular
challenge for target word identification. Another
challenging case for automatic target word detec-
tion in German concerns particle verbs such as an-
ku?ndigen ?announce?. Here, the difficulty arises
when the verbal stem (e.g., ku?ndigen) is separated
from its particle (e.g., an) in German verb-initial
and verb-second clause types.
As a preprocessing step for target word identi-
fication, the text is split into individual sentences,
tokenized, and lemmatized. For this purpose, the
sentence detector and the tokenizer of the suite
of Apache OpenNLP tools9 and the TreeTagger
(Schmid, 1994) are used. Further, compounds
are split by using BananaSplit10. Since the au-
tomatic lemmatization obtained by the tagger and
the compound splitter are not 100% accurate, tar-
get word identification also utilizes the full set of
inflected forms for a target word whenever such
information is available. As it turns out, Wik-
tionary can often be used for this purpose as well
since the German version of Wiktionary often
contains the full set of word forms in tables11 such
as the one shown in Fig. 3 for the word Bogen.
Figure 3: Wiktionary inflection table for Bogen.
Fig. 4 shows an example of such a sense-
annotated text for the target word Bogen ?vi-
olin bow?. The text is an excerpt from the
Wikipedia article Violine ?violin?, where the target
word (rendered in bold face) appears many times.
Only the second occurrence shown in the figure
(marked with a 2 on the left) exactly matches the
word Bogen as is. All other occurrences are ei-
ther the plural form Bo?gen (4 and 7), the geni-
tive form Bogens (8), part of a compound such
as Bogenstange (3), or the plural form as part
of a compound such as in Fernambukbo?gen and
Schu?lerbo?gen (5 and 6). The first occurrence
of the target word in Fig. 4 is also part of a
compound. Here, the target word occurs in the
singular as part of the adjectival compound bo-
gengestrichenen.
For expository purposes, the data format shown
in Fig. 4 is much simplified compared to the ac-
tual, XML-based format in WebCAGe. The infor-
9http://incubator.apache.org/opennlp/
10http://niels.drni.de/s9y/pages/bananasplit.html
11The inflection table cannot be extracted with the Java
Wikipedia Library JWPL. It is rather extracted from the Wik-
tionary dump file.
391
Figure 4: Excerpt from Wikipedia article Violine ?violin? tagged with target word Bogen ?violin bow?.
mation for each occurrence of a target word con-
sists of the GermaNet sense, i.e., the lexical unit
ID, the lemma of the target word, and the Ger-
maNet word category information, i.e., ADJ for
adjectives, NN for nouns, and VB for verbs.
5 Evaluation
In order to assess the effectiveness of the ap-
proach, we examine the overall size of WebCAGe
and the relative size of the different text col-
lections (see Table 1), compare WebCAGe to
other sense-annotated corpora for German (see
Table 2), and present a precision- and recall-based
evaluation of the algorithm that is used for auto-
matically identifying target words in the harvested
texts (see Table 3).
Table 1 shows that Wiktionary (7644 tagged
word tokens) and Wikipedia (1732) contribute
by far the largest subsets of the total number of
tagged word tokens (10750) compared with the
external webpages (589) and the Gutenberg texts
(785). These tokens belong to 2607 distinct pol-
ysemous words contained in GermaNet, among
which there are 211 adjectives, 1499 nouns, and
897 verbs (see Table 2). On average, these words
have 2.9 senses in GermaNet (2.4 for adjectives,
2.6 for nouns, and 3.6 for verbs).
Table 2 also shows that WebCAGe is consid-
erably larger than the other two sense-annotated
corpora available for German ((Broscheit et al
2010) and (Raileanu et al 2002)). It is impor-
tant to keep in mind, though, that the other two
resources were manually constructed, whereas
WebCAGe is the result of an automatic harvesting
method. Such an automatic method will only con-
stitute a viable alternative to the labor-intensive
manual method if the results are of sufficient qual-
ity so that the harvested data set can be used as is
or can be further improved with a minimal amount
of manual post-editing.
For the purpose of the present evaluation, we
conducted a precision- and recall-based analy-
sis for the text types of Wiktionary examples,
external webpages, and Wikipedia articles sep-
392
Table 1: Current size of WebCAGe.
Wiktionary External Wikipedia Gutenberg All
examples webpages articles texts texts
Number of
tagged
word
tokens
adjectives 575 31 79 28 713
nouns 4103 446 1643 655 6847
verbs 2966 112 10 102 3190
all word classes 7644 589 1732 785 10750
Number of
tagged
sentences
adjectives 565 31 76 26 698
nouns 3965 420 1404 624 6413
verbs 2945 112 10 102 3169
all word classes 7475 563 1490 752 10280
Total
number of
sentences
adjectives 623 1297 430 65030 67380
nouns 4184 9630 6851 376159 396824
verbs 3087 5285 263 146755 155390
all word classes 7894 16212 7544 587944 619594
Table 2: Comparing WebCAGe to other sense-tagged corpora of German.
WebCAGe
Broscheit et Raileanu et
al., 2010 al., 2002
Sense
tagged
words
adjectives 211 6 0
nouns 1499 18 25
verbs 897 16 0
all word classes 2607 40 25
Number of tagged word tokens 10750 approx. 800 2421
Domain independent yes yes
medical
domain
arately for the three word classes of adjectives,
nouns, and verbs. Table 3 shows that precision
and recall for all three word classes that occur
for Wiktionary examples, external webpages, and
Wikipedia articles lies above 92%. The only size-
able deviations are the results for verbs that occur
in the Gutenberg texts. Apart from this one excep-
tion, the results in Table 3 prove the viability of
the proposed method for automatic harvesting of
sense-annotated data. The average precision for
all three word classes is of sufficient quality to be
used as-is if approximately 2-5% noise in the an-
notated data is acceptable. In order to eliminate
such noise, manual post-editing is required. How-
ever, such post-editing is within acceptable lim-
its: it took an experienced research assistant a to-
tal of 25 hours to hand-correct all the occurrences
of sense-annotated target words and to manually
sense-tag any missing target words for the four
text types.
6 Related Work and Future Directions
With relatively few exceptions to be discussed
shortly, the construction of sense-annotated cor-
pora has focussed on purely manual methods.
This is true for SemCor, the WordNet Gloss Cor-
pus, and for the training sets constructed for En-
glish as part of the SensEval and SemEval shared
task competitions (Agirre et al 2007; Erk and
Strapparava, 2012; Mihalcea et al 2004). Purely
manual methods were also used for the German
sense-annotated corpora constructed by Broscheit
et al(2010) and Raileanu et al(2002) as well as
for other languages including the Bulgarian and
393
Table 3: Evaluation of the algorithm of identifying the target words.
Wiktionary External Wikipedia Gutenberg
examples webpages articles texts
Precision
adjectives 97.70% 95.83% 99.34% 100%
nouns 98.17% 98.50% 95.87% 92.19%
verbs 97.38% 92.26% 100% 69.87%
all word classes 97.32% 96.19% 96.26% 87.43%
Recall
adjectives 97.70% 97.22% 98.08% 97.14%
nouns 98.30% 96.03% 92.70.% 97.38%
verbs 97.51% 99.60% 100% 89.20%
all word classes 97.94% 97.32% 93.36% 95.42%
the Chinese sense-tagged corpora (Koeva et al
2006; Wu et al 2006). The only previous at-
tempts of harvesting corpus data for the purpose
of constructing a sense-annotated corpus are the
semi-supervised method developed by Yarowsky
(1995), the knowledge-based approach of Lea-
cock et al(1998), later also used by Agirre and
Lopez de Lacalle (2004), and the automatic asso-
ciation of Web directories (from the Open Direc-
tory Project, ODP) to WordNet senses by Santa-
mar??a et al(2003).
The latter study (Santamar??a et al 2003) is
closest in spirit to the approach presented here.
It also relies on an automatic mapping between
wordnet senses and a second web resource. While
our approach is based on automatic mappings be-
tween GermaNet and Wiktionary, their mapping
algorithm maps WordNet senses to ODP subdi-
rectories. Since these ODP subdirectories contain
natural language descriptions of websites relevant
to the subdirectory in question, this textual mate-
rial can be used for harvesting sense-specific ex-
amples. The ODP project also covers German so
that, in principle, this harvesting method could be
applied to German in order to collect additional
sense-tagged data for WebCAGe.
The approach of Yarowsky (1995) first collects
all example sentences that contain a polysemous
word from a very large corpus. In a second step,
a small number of examples that are representa-
tive for each of the senses of the polysemous tar-
get word is selected from the large corpus from
step 1. These representative examples are manu-
ally sense-annotated and then fed into a decision-
list supervised WSD algorithm as a seed set for it-
eratively disambiguating the remaining examples
collected in step 1. The selection and annotation
of the representative examples in Yarowsky?s ap-
proach is performed completely manually and is
therefore limited to the amount of data that can
reasonably be annotated by hand.
Leacock et al(1998), Agirre and Lopez de La-
calle (2004), and Mihalcea and Moldovan (1999)
propose a set of methods for automatic harvesting
of web data for the purposes of creating sense-
annotated corpora. By focusing on web-based
data, their work resembles the research described
in the present paper. However, the underlying har-
vesting methods differ. While our approach re-
lies on a wordnet to Wiktionary mapping, their
approaches all rely on the monosemous relative
heuristic. Their heuristic works as follows: In or-
der to harvest corpus examples for a polysemous
word, the WordNet relations such as synonymy
and hypernymy are inspected for the presence of
unambiguous words, i.e., words that only appear
in exactly one synset. The examples found for
these monosemous relatives can then be sense-
annotated with the particular sense of its ambigu-
ous word relative. In order to increase coverage
of the monosemous relatives approach, Mihalcea
and Moldovan (1999) have developed a gloss-
based extension, which relies on word overlap of
the gloss and the WordNet sense in question for
all those cases where a monosemous relative is
not contained in the WordNet dataset.
The approaches of Leacock et al Agirre and
Lopez de Lacalle, and Mihalcea and Moldovan as
394
well as Yarowsky?s approach provide interesting
directions for further enhancing the WebCAGe re-
source. It would be worthwhile to use the au-
tomatically harvested sense-annotated examples
as the seed set for Yarowsky?s iterative method
for creating a large sense-annotated corpus. An-
other fruitful direction for further automatic ex-
pansion of WebCAGe is to use the heuristic of
monosemous relatives used by Leacock et al by
Agirre and Lopez de Lacalle, and by Mihalcea
and Moldovan. However, we have to leave these
matters for future research.
In order to validate the language independence
of our approach, we plan to apply our method to
sense inventories for languages other than Ger-
man. A precondition for such an experiment is an
existing mapping between the sense inventory in
question and a web-based resource such as Wik-
tionary or Wikipedia. With BabelNet, Navigli and
Ponzetto (2010) have created a multilingual re-
source that allows the testing of our approach to
languages other than German. As a first step in
this direction, we applied our approach to English
using the mapping between the Princeton Word-
Net and the English version of Wiktionary pro-
vided by Meyer and Gurevych (2011). The re-
sults of these experiments, which are reported in
Henrich et al(2012), confirm the general appli-
cability of our approach.
To conclude: This paper describes an automatic
method for creating a domain-independent sense-
annotated corpus harvested from the web. The
data obtained by this method for German have
resulted in the WebCAGe resource which cur-
rently represents the largest sense-annotated cor-
pus available for this language. The publication of
this paper is accompanied by making WebCAGe
freely available.
Acknowledgements
The research reported in this paper was jointly
funded by the SFB 833 grant of the DFG and by
the CLARIN-D grant of the BMBF. We would
like to thank Christina Hoppermann, Marie Hin-
richs as well as three anonymous EACL 2012 re-
viewers for their helpful comments on earlier ver-
sions of this paper. We are very grateful to Rein-
hild Barkey, Sarah Schulz, and Johannes Wahle
for their help with the evaluation reported in Sec-
tion 5. Special thanks go to Yana Panchenko and
Yannick Versley for their support with the web-
crawler and to Emanuel Dima and Klaus Sut-
tner for helping us to obtain the Gutenberg and
Wikipedia texts.
References
Agirre, E., Lopez de Lacalle, O. 2004. Publicly
available topic signatures for all WordNet nominal
senses. Proceedings of the 4th International Con-
ference on Languages Resources and Evaluations
(LREC?04), Lisbon, Portugal, pp. 1123?1126
Agirre, E., Marquez, L., Wicentowski, R. 2007. Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations. Assoc. for Computational Lin-
guistics, Stroudsburg, PA, USA
Broscheit, S., Frank, A., Jehle, D., Ponzetto, S. P.,
Rehl, D., Summa, A., Suttner, K., Vola, S. 2010.
Rapid bootstrapping of Word Sense Disambigua-
tion resources for German. Proceedings of the 10.
Konferenz zur Verarbeitung Natu?rlicher Sprache,
Saarbru?cken, Germany, pp. 19?27
Erk, K., Strapparava, C. 2010. Proceedings of the 5th
International Workshop on Semantic Evaluation.
Assoc. for Computational Linguistics, Stroudsburg,
PA, USA
Fellbaum, C. (ed.). 1998. WordNet An Electronic
Lexical Database. The MIT Press.
Henrich, V., Hinrichs, E. 2010. GernEdiT ? The Ger-
maNet Editing Tool. Proceedings of the Seventh
Conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, pp.
2228?2235
Henrich, V., Hinrichs, E., Vodolazova, T. 2011. Semi-
Automatic Extension of GermaNet with Sense Def-
initions from Wiktionary. Proceedings of the 5th
Language & Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics (LTC?11), Poznan, Poland,
pp. 126?130
Henrich, V., Hinrichs, E., Vodolazova, T. 2012. An
Automatic Method for Creating a Sense-Annotated
Corpus Harvested from the Web. Poster pre-
sented at 13th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2012), New Delhi, India, March 2012
Koeva, S., Leseva, S., Todorova, M. 2006. Bul-
garian Sense Tagged Corpus. Proceedings of the
5th SALTMIL Workshop on Minority Languages:
395
Strategies for Developing Machine Translation for
Minority Languages, Genoa, Italy, pp. 79?87
Kunze, C., Lemnitzer, L. 2002. GermaNet rep-
resentation, visualization, application. Proceed-
ings of the 3rd International Language Resources
and Evaluation (LREC?02), Las Palmas, Canary Is-
lands, pp. 1485?1491
Leacock, C., Chodorow, M., Miller, G. A. 1998.
Using corpus statistics and wordnet relations for
sense identification. Computational Linguistics,
24(1):147?165
Meyer, C. M., Gurevych, I. 2011. What Psycholin-
guists Know About Chemistry: Aligning Wik-
tionary and WordNet for Increased Domain Cov-
erage. Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), Chiang Mai, Thailand, pp. 883?892
Mihalcea, R., Moldovan, D. 1999. An Auto-
matic Method for Generating Sense Tagged Cor-
pora. Proceedings of the American Association for
Artificial Intelligence (AAAI?99), Orlando, Florida,
pp. 461?466
Mihalcea, R., Chklovski, T., Kilgarriff, A. 2004. Pro-
ceedings of Senseval-3: Third International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text, Barcelona, Spain
Navigli, R., Ponzetto, S. P. 2010. BabelNet: Build-
ing a Very Large Multilingual Semantic Network.
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?10),
Uppsala, Sweden, pp. 216?225
Raileanu, D., Buitelaar, P., Vintar, S., Bay, J. 2002.
Evaluation Corpora for Sense Disambiguation in
the Medical Domain. Proceedings of the 3rd In-
ternational Language Resources and Evaluation
(LREC?02), Las Palmas, Canary Islands, pp. 609?
612
Santamar??a, C., Gonzalo, J., Verdejo, F. 2003. Au-
tomatic Association of Web Directories to Word
Senses. Computational Linguistics 29 (3), MIT
Press, PP. 485?502
Schmid, H. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Proceedings of the In-
ternational Conference on New Methods in Lan-
guage Processing, Manchester, UK
Wu, Y., Jin, P., Zhang, Y., Yu, S. 2006. A Chinese
Corpus with Word Sense Annotation. Proceedings
of 21st International Conference on Computer Pro-
cessing of Oriental Languages (ICCPOL?06), Sin-
gapore, pp. 414?421
Yarowsky, D. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. Proceed-
ings of the 33rd Annual Meeting on Association
for Computational Linguistics (ACL?95), Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, pp. 189?196
396
Proceedings of the ACL 2010 System Demonstrations, pages 19?24,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
GernEdiT: A Graphical Tool for GermaNet Development 
  Verena Henrich University of T?bingen T?bingen, Germany. verena.henrich@uni-tuebingen.de 
Erhard Hinrichs University of T?bingen T?bingen, Germany. erhard.hinrichs@uni-tuebingen.de     Abstract 
GernEdiT (short for: GermaNet Editing Tool) offers a graphical interface for the lexicogra-phers and developers of GermaNet to access and modify the underlying GermaNet re-source. GermaNet is a lexical-semantic word-net that is modeled after the Princeton Word-Net for English. The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. GernEdiT re-places the earlier development by a more user-friendly tool, which facilitates automatic checking of internal consistency and correct-ness of the linguistic resource. This paper pre-sents all these core functionalities of GernEdiT along with details about its usage and usabil-ity. 1 Introduction The main purpose of the GermaNet Editing Tool GernEdiT tool is to support lexicographers in accessing, modifying, and extending the Ger-maNet data (Kunze and Lemnitzer, 2002; Hen-rich and Hinrichs, 2010) in an easy and adaptive way and to aid in the navigation through the GermaNet word class hierarchies, so as to find the appropriate place in the hierarchy for new synsets (short for: synonymy set) and lexical units. GernEdiT replaces the traditional Ger-maNet development based on lexicographer files (Fellbaum, 1998) by a more user-friendly visual tool that supports versioning and collaborative annotation by several lexicographers working in parallel. Furthermore, GernEdiT facilitates internal consistency of the GermaNet data such as appro-priate linking of lexical units with synsets, connectedness of the synset graph, and automatic 
closure among relations and their inverse coun-terparts. All these functionalities along with the main aspects of GernEdiT?s usage and usability are presented in this paper. 2 The Structure of GermaNet GermaNet is a lexical-semantic wordnet that is modeled after the Princeton WordNet for English (Fellbaum, 1998). It covers the three word cate-gories of adjectives, nouns, and verbs and parti-tions the lexical space into a set of concepts that are interlinked by semantic relations. A semantic concept is modeled by a synset. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same mean-ing. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units. There are two types of semantic relations in GermaNet: conceptual and lexical relations. Conceptual relations hold between two semantic concepts, i.e. synsets. They include relations such as hyperonymy, part-whole relations, en-tailment, or causation. GermaNet is hierarchi-cally structured in terms of the hyperonymy rela-tion. Lexical relations hold between two individ-ual lexical units. Antonymy, a pair of opposites, is an example of a lexical relation. 3 The GermaNet Editing Tool The GermaNet Editing Tool GernEdiT provides a graphical user interface, implemented as a Java Swing application, which primarily allows main-taining the GermaNet data in a user-friendly way. The editor represents an interface to a rela-tional database, where all GermaNet data is stored from now on. 
19
 Figure 1. The main view of GernEdiT.  3.1 Motivation The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. This is exactly why GernEdiT was de-veloped: It supports lexicographers who need to access, modify, and extend GermaNet data by providing these functions through simple button-clicks, searches, and form editing. There are sev-eral ways to search data and browse through the GermaNet graph. These functionalities allow lexicographers, among other things, to find the 
appropriate place in the hierarchy for the inser-tion of new synsets and lexical units. Last but not least, GernEdiT facilitates internal consistency and correctness of the linguistic resource and supports versioning and collaborative annotation of GermaNet by several lexicographers working in parallel. 3.2 The Main User Interface Figure 1 illustrates the main user panel of Gern-EdiT. It shows a Search panel above, two panels for Synsets and Lexical Units in the middle, and four tabs below: a Conceptual Relations Editor, a Graph with Hyperonyms and Hyponyms, a Lexi-
20
 Figure 2: Filtered list of lexical units.  cal Relations Editor, and an Examples and Frames tab. In Figure 1, a search for synsets consisting of lexical units with the word Nuss (German noun for: nut) has been executed. Accordingly, the Synsets panel displays the three resulting synsets that match the search item. The Synset Id is the unique database ID that unambiguously identi-fies a synset, and which can also be used to search for exactly that synset. Word Category specifies whether a synset is an adjective (adj), a noun (nomen), or a verb (verben), whereas Word Class classifies the synsets into semantic fields. The word class of the selected synset in Figure 1 is Nahrung (German noun for: food). The Para-phrase column contains a description of a synset, e.g., for the selected synset the paraphrase is: der essbare Kern einer Nuss (German phrase for: the edible kernel of a nut). The column All Orth Forms simply lists all orthographical variants of all its lexical units. Which lexical units are listed in the Lexical Units panel depends on the selected synset in the Synsets panel. Here, Lex Unit Id and Synset Id again reflect the corresponding unique database IDs. Orth Form (short for: orthographic form) represents the correct spelling of a word accord-ing to the rules of the spelling reform Neue Deutsche Rechtschreibung (Rat f?r deutsche Rechtschreibung, 2006), a recently adopted spelling reform. In our example, the main ortho-graphic form is Nuss. Orth Var may contain an 
alternative spelling that is admissible according to the Neue Deutsche Rechtschreibung. 1  Old Orth Form represents the main orthographic form prior to the Neue Deutsche Recht-schreibung. This means that Nu? was the correct spelling instead of Nuss before the German spell-ing reform. Old Orth Var contains any accepted variant prior to the Neue Deutsche Recht-schreibung. The Old Orth Var field is filled only if it is no longer allowed in the new orthography. The Boolean values Named Entity, Artificial, and Style Marking express further properties of a lexical unit, whether the lexical unit is a named entity, an artificial concept node, or a stylistic variant. For both the lexical units and the synsets, there are two buttons Use as From and Use as To, which help to add new relations (see the explana-tion of Figure 3 in section 3.6 below which ex-plains the creation of new relations). 3.3 Search Functionalities It is possible to search for words or synset data-base IDs via the search panel (see Figure 1 at the top). The check box Ignore Case offers the pos-sibility of searching without distinguishing be-tween upper and lower case.                                                  1 An example of this kind is the German word Delfin (Ger-man noun for: dolphin). Apart from the main form Delfin, there is an orthographic variant Delphin. 
21
 Figure 3. Conceptual Relations Editor tab.  Via the file menu, lists of all synsets or lexical units with their properties can be accessed. To these lists, very detailed filters can be applied: e.g., filtering the lexical units or synsets by parts of their orthographical forms. Figure 2 shows a list of lexical units to which a detailed filter has been applied: verbs have been chosen (see the chosen tab) whose orthographical forms start with an a- (see starts with check box and corre-sponding text field) and end with the suffix -ten (see ends with check box and corresponding text field). Only verbs that have a frame that contains NN are chosen (see Frame contains check box and corresponding text field). Furthermore, the resulting filtered list is sorted in descending or-der by their examples (see the little triangle in the Examples header of the result table). The number in the brackets behind the word category in the tab title indicates the count of the filtered lexical units (in this example 193 verbs pass the filter). 3.4 Visualization of the Graph Hierarchy There is the possibility to display a graph with all hyperonyms and hyponyms of a selected synset. This is shown in the bottom half of Figure 1 in the tab Graph with Hyperonyms and Hyponyms. The graph in Figure 1 visualizes a part of the hi-erarchical structure of GermaNet centered around the synset containing Nuss and displays the hyperonyms and hyponyms of this synset up to a certain parameterized depth (in this case depth 2 has been chosen). The Hyperonym Depth chooser allows unfolding the graph to the top up to the preselected depth. As it is not possible to visualize the whole GermaNet contents at once, the graph can be seen as a window to GermaNet. 
A click on any synset node within the graph, navigates to that synset. This functionality sup-ports lexicographers especially in finding the appropriate place in the hierarchy for the inser-tion of new synsets. 3.5 Modifications of Existing Items If the lexicographers? task is to modify existing synsets or lexical units, this is done by selecting a synset or lexical unit displayed in the Synsets and the Lexical Units panels shown in Figure 1. The properties of such selected items can be ed-ited by a click in the corresponding table cell. For example by clicking in the cell Orth Form the spelling of a lexical unit can be corrected in case of an earlier typo was made. If lexicographers want to edit examples, frames, conceptual, or lexical relations this is done by choosing the appropriate tab indicated at the bottom of Figure 1. By clicking one of these tabs, the corresponding panel appears below these tabs. In Figure 1 the panel for Graph with Hyperonyms and Hyponyms is displayed. It is possible to edit the examples and frames associated with a lexical unit via the Examples and Frames tab. Frames specify the syntactic valence of a lexical unit. Each frame can have an associated example that indicates a possible us-age of the lexical unit for that particular frame. The tab Examples and Frames is thus particu-larly geared towards the editing of verb entries. By clicking on the tab all examples and frames of a lexical unit are listed and can then be modi-fied by choosing the appropriate editing buttons. For more information about these editing func-tions see Henrich and Hinrichs (2010). 
22
    Figure 4. Synset Editor (left). Lexical Units Editor (right).  3.6 Editing of Relations If lexicographers want to add new conceptual or lexical relations to a synset or a lexical unit this is done by clicking on the Conceptual Relations Editor or the Lexical Relations Editor shown in Figure 1. Figure 3 shows the panel that appears if the Conceptual Relations Editor has been chosen for the synset containing Nuss. To create a new rela-tion, the lexicographer needs to use the buttons Use as From and Use as To shown in Figure 1. This will insert the ID of the selected synsets from the Synsets panel in the corresponding From or To field in Figure 3. The button Delete ConRel allows deletion of a conceptual relation, if all consistency checks are passed. The Lexical Relations Editor tab supports edit-ing all lexical relations. It is not displayed sepa-rately for reasons of space, but it is analogue to the Conceptual Relations Editor tab for editing conceptual relations. 3.7 Adding Synsets and Lexical Units The buttons Add New Hyponym and Add New LexUnit in the Synsets panel (see Figure 1) can be used to insert a new synset or lexical unit at the selected place in the GermaNet graph, and the buttons Delete Synset and Delete LexUnit remove the selected entry, respectively. The Synset Editor in Figure 4 (on the left) shows the window which appears after a click on Add New Hyponym. When clicking on the button Create Synset, the Lexical Unit Editor (shown in Figure 4, right) pops up. This workflow forces the parallel creation of a lexical unit while creat-ing a synset. 3.8 Consistency Checks GernEdiT facilitates internal consistency of the GermaNet data. This is achieved by the 
workflow-oriented design of the editor. It is not possible to create a synset without creating a lexical unit in parallel (as described in section 3.7). Furthermore, it is not possible to insert a new synset without specifying the place in the GermaNet hierarchy where the new synset should be added. This is achieved by the button Add New Hyponym (see Figure 1) which forces the user to identify the appropriate hyperonym for the new synset to be added. Furthermore, it is not possible to insert a lexical unit without speci-fying the corresponding synset. On deletion of a synset, all corresponding data such as conceptual relations, lexical units with their lexical relations, frames, and examples, are deleted automatically. Consistency checks also take effect for the ta-ble cell editing in the Synsets and Lexical Units panels of the main user interface (see Figure 1), e.g., the main orthographic form of a lexical unit may never be empty. All buttons in GernEdiT are enabled only if the corresponding functionalities meet the con-sistency requirements, e.g., if a synset consists only of one lexical unit, it is not possible to de-lete that lexical unit and thus the button Delete LexUnit is disabled. Also, if the deletion of a synset or a relation would violate the complete connectedness of the GermaNet graph, it is not possible to delete that synset. 3.9 Further Functionalities There are further functionalities available through the file menu. Besides retrieving the up-to-date statistics of GermaNet, an editing history makes it possible to list all modifications on the GermaNet data, with the information about who made the change and how the modified item looked before. GernEdiT supports various export functionali-ties. For example, it is possible to export all GermaNet contents into XML files, which are used as an exchange format of GermaNet, or to 
23
export a list of all verbs with their corresponding frames and examples. 4 Tool Evaluation In order to assess the usefulness of GernEdiT, we conducted in depth interviews with the Germa-Net lexicographers and with the senior researcher who oversees all lexicographic development. At the time of the interview all of these researchers had worked with the tool for about eight months. The present section summarizes the feedback about GernEdiT that was obtained in this way. The initial learning curve for getting familiar with GernEdiT is considerably lower compared to the learning curve required for the traditional development based on lexicographer files. Moreover, the GermaNet development with GernEdiT is both more efficient and accurate compared to the traditional development along the following dimensions: 1. The menu-driven and graphics-based navigation through the GermaNet graph is much easier compared to finding the cor-rect entry point in the purely text-based format of lexicographer files. 2. Lexicographers no longer need to learn the complex specification syntax of the lexi-cographer files. Thereby, syntax errors in the specification language ? a frequent source of errors prior to development with GernEdiT ? are entirely eliminated. 3. GernEdiT facilitates automatic checking of internal consistency and correctness of the GermaNet data such as appropriate linking of lexical units with synsets, con-nectedness of the synset graph, and auto-matic closure among relations and their inverse counterparts. 4. It is now even possible to perform further queries, which were not possible before, e.g., listing all hyponyms of a synset. 5. Especially for the senior researcher who is responsible for coordinating the GermaNet lexicographers, it is now much easier to trace back changes and to verify who was responsible for them. 6. The collaborative annotation by several lexicographers working in parallel is now easily possible and does not cause any management overhead as before. 
In sum, the lexicographers of GermaNet gave very positive feedback about the use of Gern-EdiT and also made smaller suggestions for im-proving its user-friendliness further. This under-scores the utility of GernEdiT from a practical point of view. 5 Conclusion and Future Work In this paper we have described the functionality of GernEdiT. The extremely positive feedback of the GermaNet lexicographers underscores the practical benefits gained by using the GernEdiT tool in practice. At the moment, GernEdiT is customized for maintaining the GermaNet data. In future work, we plan to adapt the tool so that it can be used with wordnets for other languages as well. This would mean that the wordnet data for a given language would have to be stored in a relational database and that the tool itself can handle the language specific data structures of the wordnet in question. Acknowledgements We would like to thank all GermaNet lexicogra-phers for their willingness to experiment with GernEdiT and to be interviewed about their ex-periences with the tool. Special thanks go to Reinhild Barkey for her valuable input on both the features and user-friendliness of GernEdiT and to Alexander Kis-lev for his contributions to the underlying data-base format. References Claudia Kunze and Lothar Lemnitzer. 2002. Ger-maNet ? representation, visualization, appli-cation. Proceedings of LREC 2002, Main Confer-ence, Vol V. pp. 1485-1491, 2002. Christiane Fellbaum (ed.). 1998. WordNet ? An Electronic Lexical Database. Cambridge, MA: MIT Press. Verena Henrich and Erhard Hinrichs. 2010. GernEdiT ? The GermaNet Editing Tool. Proceedings of LREC 2010, Main Conference, Valletta, Malta. Rat f?r deutsche Rechtschreibung (eds.) (2006). Deutsche Rechtschreibung ? Regeln und W?rterverzeichnis: Amtliche Regelung. Gunter Narr Verlag T?bingen. 
24
Proceedings of the ACL 2010 System Demonstrations, pages 25?29,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
WebLicht: Web-based LRT services for German 
 Erhard Hinrichs,  Marie Hinrichs, Thomas Zastrow Seminar f?r Sprachwissenschaft, University of T?bingen firstname.lastname@uni-tuebingen.de    Abstract 
This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a web-based service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. We-bLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services.   WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.)  Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service.  1 Introduction Currently, WebLicht offers LRT services that were developed independently at the Institut f?r Informatik, Abteilung Automa-tische Sprachverarbeitung at the University of Leipzig (tokenizer, lemmatizer, co-occurrence extraction, and frequency analyzer), at the Institut f?r Maschinelle Sprachverarbeitung at the University of Stuttgart (tokenizer, tag-ger/lemmatizer, German morphological analyser SMOR, constituent and dependency parsers), at the Berlin Brandenburgische Akademie der Wissenschaften (conversion of plain text to D-Spin format, tokenizer, taggers, NE recog-                                                1 D-SPIN stands for   Deutsche SPrachressourcen INfrastruktur;   the D-SPIN project is partly financed by the BMBF; it is a national   German complement to the EU-project CLARIN. See the URLs   http://www.d-spin.org and http://www.clarin.eu for   details 
nizer) and at the Seminar f?r Sprachwissen-schaft/Computerlinguistik at the University of T?bingen (conversion of plain text to D-Spin format, GermaNet, Open Thesaurus syno-nym service, and Treebank browser). They cover a wide range of linguistic applications, like tokenization, co-occurrence extraction, POS Tagging, lexical and semantic analysis, and sev-eral laguages (currently German, English, Italian, French, Romanian, Spanish and Finnish). For some of these tasks, more than one web service is available.  As a first external partner, the Uni-versity of Helsinki in Finnland contributed a set of web services to create morphological anno-tated text corpora in the Finnish language.  With the help of the webbased user interface, these individual web services can be combined into a chain of linguistic applications.   2 Service Oriented Architecture 
WebLicht is a so-called Service Oriented Archi-tecture (Binildas et. al., 2008), which means that distributed and independent services (Tanen-baum et al 2002) are combined together to a chain of LRT tools. A centralized database, the repository, stores technical and content-related metadata about each service. With the help of 
Figure 1: The Overall Structure of WebLicht 
25
this repository, the chaining mechanism as de-scribed in section 3 is implemented. The We-bLicht user interface encapsulates this chaining mechanism in an AJAX driven web application. Since web applications can be invoked from any browser, downloading and installation of indi-vidual tools on the user's local computer is avoided. But using WebLicht web services is not restricted to the use of the integrated user inter-face. It is also possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.). Figure 1 depicts the overall structure of We-bLicht. An important part of Service Oriented Architec-tures is ensuring interoperability between the underlying services. Interoperability of web serv-ices, as they are implemented in WebLicht, re-fers to the seamless flow of data between them. To be interoperable, these web services must first agree on protocols defining the interaction be-tween the services (WSDL/SOAP, REST, XML-RPC). They must also use a shared and standard-ized data exchange format, which is preferably based on widely accepted formats already in use (UTF-8, XML). WebLicht uses the RESTstyle API and its own XML-based data exchange for-mat (Text Corpus Format, TCF).   
3 The Service Repository Every tool included in WebLicht is registered in a central repository, located in Leipzig. Also re-alized as a web service, it offers metadata and processing information about each registered tool. For example, the metadata includes infor-mation about the creator, name and the adress of the service. The input and output specifications of each web service are required in order to de-termine which processing chains are possible. Combining the metadata and the processing in-formation, the repository is able to offer func-tions for the chain building process.  Wrappers: TCF, 0.3 / TCF, 0.3 Inputs Outputs lemmas postags      -tagset: stts sem_lex_rels      -source: GermaNet Table 1: Input and Output Specifications of T?bingen's Semantic Annotator  A specialized tool for registering new web serv-ices in the repository is available.   
Figure 2: A Screenshot of the WebLicht Webinterface 
1 
2 
3 4 
26
4 The WebLicht User Interface  Figure 2 shows a screenshot of the WebLicht web interface, developed and hosted in T?bin-gen. Area 1 shows a list of all WebLicht web services along with a subset of metadata (author, URL, description etc.). This list is extracted on-the-fly from a centralized repository located in Leipzig.  This means that after registration in the repository, a web service is immediatley avail-able for inclusion in a processing chain.  The Language Filter selection box allows the selection of any language for which tools are available in WebLicht (currently, German, Eng-lish, Italian, French, Romanian, Spanish or Fin-nish). The majority of the presently integrated web services operates on German input. The platform, however, is language-independent and supports LRT resources for any language.  Plain text input to the service chain can be speci-fied in one of three ways: a) entered by the user in the Input tab, b) file upload from the user's local harddrive or c) selecting one of the sample texts offered by WebLicht (Area 2). Various format converters can be used to convert up-loaded files into the data exchange format (TCF) used by WebLicht.  Input file formats accepted by WebLicht currently include plain text, Micro-soft Word, RTF and PDF. 
In Area 3, one can assemble the service tool chain and execute it on the input text. The Se-lected Tools list displays all web services that have already been entered into the web service chain. The list under Next Tool Choices then of-fers the set of tools that can be entered as next into the chain. This list is generated by inspect-ing the metadata of the tools which are already in the chain. The chaining mechanism ensures that this list only contains tools, that are a valid next step in the chain. For example, a Part-of-Speech 
Tagger can only be added to a chain after a to-kenizer has been added. The metadata of each tool contains information about the annotations which are required in the input data and which annotations are added by that tool. As Figure 3 shows, the user sometimes has a choice of alternative tools - in the example at hand a wide variety of services are offered as candidates.  Figure 3 shows a subset of web service workflows currently available in We-bLicht. Notice that these workflows can combine tools from various institutions and are not re-stricted to predefined combinations of tools. This allows users to compare the results of several tool chains and find the best solution for their individual use case. The final result of running the tool chain as well as each individual step can be visualized in a Ta-ble View (implemented as a seperate frame, Area 4), or downloaded to the user's local harddrive in WebLicht's own data exchange format TCF.  5 The TCF Format The D-SPIN Text Corpus Format TCF (Heid et al 2010) is used by WebLicht as an internal data 
exchange format. The TCF format allows the combination of the different linguistic annota-tions produced by the tool chain.  It supports in-cremental enrichment of linguistic annotations at different levels of analysis in a common XML-based format (see Figure 4).  
Figure 3: A Choice of Alternative Services 
Figure 4: A Short Example of a TCF Document, Containing the Plain Text, Tokens and POS Tags and Lemmas 
27
The Text Corpus Format was designed to effi-ciently enable the seamless flow of data between the individual services of a Service Oriented Architecture.  Figure 4 shows a data sample in the D-SPIN Text Corpus Format.    Lexical tokens are identi-fied via token IDs which serve as unique identifiers in different annotation layers. From an organizational point-of-view, tokens can be seen as the central, atomic elements in TCF to which  other annotation layers refer. For exam-ple, the POS annotations refer to the token IDs in the token annotation layer via the attribute tokID.  The annotation layers are rendered in a stand-off annotation format. TCF stores all linguistic anno-tation layers in one single file. That means that during the chaining process, the file grows (see Figure 5). Each tool is permitted to add an arbi-trary number of layers, but it is not allowed to change or delete any existing layer. Within the D-SPIN project, several other XML based data formats were developed beside the TCF format (for example, an encoding for lexi-con based data). In order to avoid any confusion of element names between these different for-mats, namespaces for the different contextual scopes within each format have been introduced. At the end of the chaining process, converter services will convert the textcorpora from the 
TCF format into other common and standardized data formats, for example MAF/SynAF or TEI. 6 Implementation Details The web services are available in RESTstyle and use the TCF data format for input and output. The concrete implementation can use any com-bination of programming language and server environment. The repository is a relational database, offering its content also as RESTstyle web services. The user interface is a Rich Internet Application (RIA), using an AJAX driven toolkit. It incorpo-rates the Java EE 5 technology and can be de-ployed in any Java application server.   
7 How to Participate in WebLicht Since WebLicht follows the paradigm of a Serv-ice Oriented Architecture, it is easily extendable by adding new services. In order to participate in WebLicht by donating additional tools,  one must implement the tool as as RESTful web service using the TCF data format. You can find further information including a tutorial on the D-SPIN homepage2.   8 Further Work The WebLicht platform in its current form moves the functionality of LRT tools from the users desktop computer into the net (Gray et al 2005). At this point, the user must download the results of the chaining process and deal with them on his local machine again. In the future, an online workspace has to be implemented so that annotated textcorpora created with WebLicht can also be stored in and retrieved from the net. For that purpose, an integration of the eSciDoc re-search environment3 into Weblicht is planned. The eSciDoc infrastructure enables sustainable and reliable long-term preservation of primary research and analysis data. To make the use of WebLicht more convenient to the end user, there will be predefined process-ing chains. These will consist of the most com-monly used processing chains and will relieve the user of having to define the chains manually.  In the last year, WebLicht has proven to be a re-alizable and useful service environment for the humanities. In its current state, WebLicht is still a prototype: due to the restrictions of the under-lying hardware, WebLicht cannot yet be made available to the general public.  9 Scope of the Software Demonstration This demonstration will present the core func-tionalities of WebLicht as well as related mod-ules and applications. The process of building language-specific processing tool chains will be shown. WebLichts capability of offering only appropriate tools at each step in the chain-building process will be demonstrated.                                                 2 http://weblicht.sfs.uni-tuebingen.de/englisch/weblichttutorial.shtml 3 For further information about the eSciDoc platform, see https://www.escidoc.org/ 
Figure 5: Annotation Layers are Added to the TCF Document by Each Service 
28
The selected tool chain can be applied to any arbitrary uploaded text. The resulting annotated text corpus can be downloaded or visualized us-ing an integrated software module. All these functions will be shown live using just a webbrowser during the software demonstra-tion.Demo Preview and Hardware Requirements   The call for papers asks submitters of software demonstrations to provide pointers to demo pre-views and to provide technical details about hardware requirements for the actual demo at the conference.  The WebLicht web application is currently password protected. Access can be granted by requesting an account (weblicht@d-spin.org). If the software demonstration is accepted, inter-net access is necessary at the conference, but no special hardware is required. The authors will bring a laptop of their own and if necessary also a beamer.  Acknowledgments WebLicht is the product of a combined effort within the D-SPIN projects (www.d-spin.org).  Currently, partners include:  Seminar f?r Sprachwissenschaft/Computerlinguistik, Univer-sit?t T?bingen, Abteilung f?r Automatische Sprachverarbeitung, Universit?t Leipzig, Institut f?r Maschinelle Sprachverarbeitung, Universit?t  Stuttgart and Berlin Brandenburgische Akademie der Wissenschaften.   
References   Binildas, C.A., Malhar Barai et.al. (2008). Service Oriented Architectures with Java. PACKT Publish-ing, Birmingham ? Mumbai Gray, J., Liu, D., Nieto-Santisteban, M., Szalay, A., DeWitt, D., Heber, G. (2005). Scientific Data Man-agement in the Coming Decade. Technical Report MSR-TR-2005-10, Microsoft Research.   Heid, U., Schmid, H., Eckart, K., Hinrichs, E. (2010). A Corpus Representation Format for Linguistic Web Services: the D_SPIN Text Corpus Format and its Relationship with ISO Standards. In Pro-ceedings of LREC 2010, Malta.  Tanenbaum, A., van Steen, M. (2002). Distributed Systems, Prentice Hall, Upper Saddle River, NJ, 1st Edition.  
29
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 147?151,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Chunking German: An Unsolved Problem
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Kathrin Beck, Erhard Hinrichs, Heike Telljohann
Universita?t Tu?bingen
Tu?bingen, Germany
{kbeck,eh,telljohann}@sfs.
uni-tuebingen.de
Abstract
This paper describes a CoNLL-style
chunk representation for the Tu?bingen
Treebank of Written German, which as-
sumes a flat chunk structure so that each
word belongs to at most one chunk. For
German, such a chunk definition causes
problems in cases of complex prenominal
modification. We introduce a flat annota-
tion that can handle these structures via a
stranded noun chunk.
1 Introduction
The purpose of this paper is to investigate how the
annotation of noun phrases in the Tu?bingen Tree-
bank of Written German (Tu?Ba-D/Z) can be trans-
formed into chunks with no internal structure, as
proposed in the CoNLL 2000 shared task (Tjong
Kim Sang and Buchholz, 2000). Chunk parsing is
a form of partial parsing, in which non-recursive
phrases are annotated while difficult decisions,
such as prepositional phrase attachment, are left
unsolved. Flat chunk representations are particu-
larly suitable for machine learning approaches to
partial parsing and are inspired by the IOB ap-
proach to NP chunking first proposed by Ramshaw
and Marcus (1995). They are particularly relevant
for approaches that require an efficient analysis but
not necessarily a complete syntactic analysis.
German allows a higher degree of syntactic
complexity in prenominal modification of the syn-
tactic head of an NP compared to English. This
is particularly evident in written texts annotated
in the Tu?Ba-D/Z. The complexity of German
NPs that causes problems in the conversion to
CoNLL-style chunks also affects PCFG parsing
approaches to German.The complexity of NPs is
one of the phenomena that have been addressed in
tree transformation approaches for German pars-
ing (Trushkina, 2004; Ule, 2007; Versley and Reh-
bein, 2009).
2 Defining Chunks
The notion of a chunk is orginally due to Abney
(1991), who considers chunks as non-recursive
phrases which span from the left periphery of a
phrase to the phrasal head. Accordingly, the sen-
tence ?The woman in the lab coat thought you
had bought an expensive book.? is assigned the
chunk structure: ?[S [NP The woman] [PP in [NP
the lab coat] ] [VP thought] ] [S [NP you] [VP
had bought] [NP an [ADJP expensive] book]] .?.
Abney-style chunk parsing is implemented as cas-
caded, finite-state transduction (cf. (Abney, 1996;
Karlsson et al, 1995)).
Notice that cascaded, finite-state transduction
allows for the possibility of chunks containing
other chunks as in the above sentence, where the
prepositional chunk contains a noun chunk within.
The only constraint on such nested chunks is the
prohibition on recursive structures. This rules out
chunks in which, for example, a noun chunk con-
tains another noun chunk. A much stricter con-
straint on the internal structure of chunks was sub-
sequently adopted by the shared task on chunk
parsing as part of the Conference for Natural Lan-
guage Learning (CoNLL) in the year 2000 (Tjong
Kim Sang and Buchholz, 2000). In this shared
task, chunks were defined as non-overlapping,
non-recursive phrases so that each word is part of
at most one chunk. Based on this definition, the
prepositional phrase in the sentence above would
be chunked as ?[Prep in] [NP the lab coat]?. Since
the prepositional chunk cannot have an embedded
noun chunk, the definition of the CoNLL shared
task assumed that the prepositional chunk only
contains the preposition, thus taking the definition
seriously that the chunk ends with the head. The
noun chunk remains separate. Additionally, the
noun phrase ?an expensive book? is annotated as a
noun chunk without internal structure.
The CoNLL shared task definition of chunks is
147
Figure 1: Treebank annotation for the sentence in (2).
useful for machine learning based approaches to
chunking since it only requires one level of anal-
ysis, which can be represented as IOB-chunking
(Tjong Kim Sang and Buchholz, 2000). For En-
glish, this definition of chunks has become stan-
dard in the literature on machine learning.
For German, chunk parsing has been investi-
gated by Kermes and Evert (2002) and by Mu?ller
(2004). Both approaches used an Abney-style
chunk definition. However, there is no corre-
sponding flat chunk representation for German be-
cause of the complexity of pre-head modification
in German noun phrases. Sentence (1) provides a
typical example of this kind.
(1) [NC der
the
[NC seinen
his
Sohn]
son
liebende
loving
Vater]
father
?the father who loves his son?
The structure in (1) violates both the Abney-
style and the CoNLL-style definitions of chunks ?
Abney?s because it is recursive and the CoNLL-
style definition because of the embedding. A
single-level, CoNLL-style chunk analysis will
have to cope with the separation of the determiner
?der? and the head of the outer phrase. We will
discuss an analysis in section 5.
3 The Treebank: Tu?Ba-D/Z
The Tu?bingen Treebank of Written German
(Tu?Ba-D/Z) is a linguistically annotated corpus
based on data of the German newspaper ?die
tageszeitung? (taz). Currently, it comprises ap-
proximately 45 000 sentences. For the syntactic
annotation, a theory-neutral and surface-oriented
annotation scheme has been adopted that is in-
spired by the notion of topological fields and
enriched by a level of predicate-argument struc-
ture. The annotation scheme comprises four lev-
els of syntactic annotation: the lexical level, the
phrasal level, the level of topological fields, and
the clausal level. The primary ordering princi-
ple of a clause is the inventory of topological
fields, which characterize the word order regu-
larities among different clause types of German,
and which are widely accepted among descrip-
tive linguists of German (cf. (Drach, 1937; Ho?hle,
1986)). Below this level of annotation, i.e. strictly
within the bounds of topological fields, a phrase
level of predicate-argument structure is applied
with its own descriptive inventory based on a min-
imal set of assumptions that has to be captured by
any syntactic theory. The context-free backbone of
phrase structure (Telljohann et al, 2004) is com-
bined with edge labels specifying the grammatical
functions and long-distance relations. For more
details on the annotation scheme see Telljohann et
al. (2009).
(2) Der Spitzenreiter in der europa?ischen
Gastgeberliga war bei den bosnischen
Bu?rgerkriegsflu?chtlingen noch weitaus
gro?zu?giger.
?The front-runner in the European league of host
countries was far more generous with the Bosnian
civil war refugees.?
Figure 1 shows the tree for the sentence in (2).
The main clause (SIMPX) is divided into three
topological fields: initial field (VF), left sentence
bracket (LK), and middle field (MF). The finite
148
verb in LK is the head (HD) of the sentence.
The edge labels between the level of topological
fields and the phrasal level constitute the gram-
matical function of the respective phrase: sub-
ject (ON), ambiguous modifier (MOD), and predi-
cate (PRED). The label V-MOD specifies the long-
distance dependency of the prepositional phrase
on the main verb. Below the lexical level, the parts
of speech are annotated. The hierarchical annota-
tion of constituent structure and head (HD) / non-
head (-) labels capture phrase internal dependen-
cies. While premodifiers are attached directly on
the same level, postmodifiers are attached higher
in order to keep their modification scope ambigu-
ous. The PP ?in der europa?ischen Gastgeberliga?
is the postmodifier of the head-NX and therefore
attached on a higher phrase level.
4 General Conversion Strategy
The conversion to CoNLL-style chunks starts
from the syntactic annotation of the Tu?Ba-D/Z.
In general, we directly convert the lowest phrasal
projections with lexical content to chunks. For
the sentence in (2) above, the chunk annotation is
shown in (3). Here, the first noun phrase1, ?Der
Spitzenreiter?, as well as the finite verb phrase and
the adverbial phrase are used as chunks.
(3) [NX Der Spitzenreiter] [PX in
der europa?ischen Gastgeberliga]
[VXFIN war] [PX bei den bosnischen
Bu?rgerkriegsflu?chtlingen] [ADVX noch]
[ADJX weitaus gro?zu?giger].
This sentence also shows exceptions to the
general conversion rule: We follow Tjong
Kim Sang and Buchholz (2000) in including
ADJPs into the NCs, such as in ?den bos-
nischen Bu?rgerkriegsflu?chtlingen?. We also in-
clude premodifying adverbs into ADJCs, such as
in ?weitaus gro?zu?giger?. But we deviate from
Tjong Kim Sang and Buchholz in our definition of
the PCs and include the head NP into this chunk,
such as in ?in der europa?ischen Gastgeberliga?.
(4) a. Allerdings werden wohl Rational-
isierungen mit der Modernisierung
1For the sake of convenience, we will use acronyms in the
remainder of the paper. Since we use the same labels in the
treebank annotation and in the chunk representation (mostly
ending in X), we will use labels ending in P (e.g. NP, PP) to
talk about phrases in the treebank and labels ending in C (e.g.
NC, PC) to talk about chunks.
der Beho?rdenarbeit einhergehen.
?However, rationalizations will accompany
modernization in the workflow of civil service
agencies.?
b. [ADVX Allerdings] [VXFIN wer-
den] [ADVX wohl] [NX Rationalis-
ierungen] [PX mit der Moder-
nisierung] [NX der Beho?rdenarbeit]
[VXINF einhergehen].
In cases of complex, post-modified noun
phrases grouped under the prepositional phrase,
we include the head noun phrase into the preposi-
tional chunk but group the postmodifying phrase
into a separate phrase. The sentence in (4a)
gives an example for such a complex noun phrase.
This sentence is assigned the chunk annotation in
(4b). Here, the head NP ?der Modernisierung? is
grouped in the PC while the post-modifying NP
?der Beho?rdenarbeit? constitutes its own NC.
The only lexical constituent in the treebank that
is exempt from becoming a chunk is the named
entity constituent (EN-ADD). Since these con-
stituents do not play a syntactic role in the tree,
they are elided in the conversion to chunks.
5 Complications in German
While the conversion based on the phrasal anno-
tation of Tu?Ba-D/Z results in the expected chunk
structures, it is incapable of handling a small num-
ber of cases correctly. Most of these cases involve
complex NPs. We will concentrate here on one
case: complex premodified NPs that include the
complement of a participle or an adjective, as dis-
cussed in section 2. This is a non-trivial problem
since the treebank contains 1 497 cases in which
an ADJP within an NP contains a PP and 415
cases, in which an ADJP within an NP contains
another NP. Sentence (5a) with the syntactic an-
notation in Figure 2 gives an example for such an
embedded PP.
(5) a. Die teilweise in die Erde gebaute
Sporthalle wird wegen ihrer futuris-
tischen Architektur auch als ?Sport-
Ei? bezeichnet.
?The partially underground sports complex is
also called the ?sports egg? because of its fu-
turistic architecture.?
b. [sNX Die] [ADVX teilweise] [PX in
die Erde] [NX gebaute Sporthalle]
[VXFIN wird] [PX wegen ihrer futu-
ristischen Architektur] [ADVX auch]
149
Figure 2: Treebank annotation for the sentence in (5a).
[NX als ? Sport-Ei] ? [VXINF be-
zeichnet].
Since we are interested in a flat chunk annota-
tion in which each word belongs to at most one
chunk, the Abney-style embedded chunk defini-
tion shown in sentence (1) is impossible. If we de-
cide to annotate the PP ?in die Erde? as a chunk,
we are left with two parts of the embedding NP:
the determiner ?Die? and the ADVP ?teilweise? to
the left of the PP and the ADJP ?gebaute? and the
noun on the right. The right part of the NP can
be easily grouped into an NC, and the ADVP can
stand on its own. The only remaining problem is
the treatment of the determiner, which in German,
cannot constitute a phrase on its own. We decided
to create a new type of chunk, stranded NC (sNX),
which denotes that this chunk is part of an NC, to
which it is not adjacent. Thus the sentence in (5a)
has the chunk structure shown in (5b).
The type of complex NPs shown in the previ-
ous section can become arbitrarily complex. The
example in (6a) with its syntactic analysis in Fig-
ure 3 shows that the attributively used adjective
?sammelnden? can have all its complements and
adjuncts. Here, we have a reflexive pronoun ?sich?
and a complex PP ?direkt vor ihrem Sezessions-
Standort am Karlsplatz?. The chunk analysis
based on the principles from section 4 gives us the
analysis in (6b). The complex PP is represented as
three different chunks: an ADVC, and two PCs.
(6) a. Sie ?thematisierten? auf Anraten des
jetzigen Staatskurators Wolfgang
Zinggl die sich direkt vor ihrem
Sezessions-Standort am Karlsplatz
sammelnden Fixer.
?On the advice of the current state curator
Wolfgang Zinggl, they ?broach the issue? of
the junkies who gather right in front of their
location of secession at the Karlsplatz .?
b. [NX Sie] ? [VXFIN thematisierten]
? [PX auf Anraten] [NX des jet-
zigen Staatskurators] [NX Wolfgang
Zinggl] [sNX die] [NX sich] [ADVX
direkt] [PX vor ihrem Sezessions-
Standort] [PX am Karlsplatz] [NX
sammelnden Fixer].
6 Conclusion
In this paper, we have shown how a CoNLL-
style chunk representation can be derived from
Tu?Ba-D/Z. For the complications stemming from
complex prenominal modification, we proposed
an analysis in which the stranded determiner is
marked as such. For the future, we are planning
to make this chunk representation available to li-
cense holders of the treebank.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Caroll Tenney, editors,
Principle-Based Parsing, pages 257?278. Kluwer
Academic Publishers, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state
cascades. In John Carroll, editor, ESSLLI Workshop
on Robust Parsing, pages 8?15, Prague, Czech Re-
public.
Erich Drach. 1937. Grundgedanken der Deutschen
Satzlehre. Diesterweg, Frankfurt/M.
150
Figure 3: Treebank annotation for the sentence in (6a).
Tilman Ho?hle. 1986. Der Begriff ?Mit-
telfeld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Fred Karlsson, Atro Voutilainen, J. Heikkila?, and Atro
Anttila, editors. 1995. Constraint Grammar: A
Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter.
Hannah Kermes and Stefan Evert. 2002. YAC ? a
recursive chunker for unrestricted German text. In
Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria.
Frank H. Mu?ller. 2004. Annotating grammatical func-
tions in German using finite-state cascades. In Pro-
ceedings of COLING 2004, Geneva, Switzerland.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the ACL 3rd Workshop on Very
Large Corpora, pages 82?94, Cambridge, MA.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC), pages 2229?2235,
Lisbon, Portugal.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
Heike Zinsmeister, and Kathrin Beck, 2009. Style-
book for the Tu?bingen Treebank of Written German
(Tu?Ba-D/Z). Seminar fu?r Sprachwissenschaft, Uni-
versita?t Tu?bingen, Germany.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL shared task: Chunking. In
Proceedings of The Fourth Conference on Computa-
tional Language Learning, CoNLL?00, and the Sec-
ond Learning Language in Logic Workshop, LLL?00,
pages 127?132, Lisbon, Portugal.
Julia S. Trushkina. 2004. Morpho-Syntactic Annota-
tion andDependency Parsing of German. Ph.D. the-
sis, Eberhard-Karls Universita?t Tu?bingen.
Tylman Ule. 2007. Treebank Refinement: Opti-
mising Representations of Syntactic Analyses for
Probabilistic Context-Free Parsing. Ph.D. thesis,
Eberhard-Karls Universita?t Tu?bingen.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for German. In Proceedings of
the International Conference on Parsing Technology
(IWPT?09), Paris, France.
151
