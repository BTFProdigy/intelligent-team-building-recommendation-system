35
36
37
38
39
40
41
42
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 16?17,
Vancouver, October 2005.
 
Translation Exercise Assistant: 
Automated Generation of Translation Exercises  
for Native-Arabic Speakers Learning English 
 
Jill Burstein 
Educational Testing Service 
Princeton, NJ 08541 
jburstein@ets.org 
Daniel Marcu 
Language Weaver, Inc 
Marina del Rey, CA 90292 
dmarcu@languageweaver.com 
 
1. Introduction 
 
Machine translation has clearly entered into 
the marketplace as a helpful technology. 
Commercial applications are used on the internet 
for automatic translation of web pages and news 
articles. In the business environment, companies 
offer software that performs automatic 
translations of web sites for localization 
purposes, and translations of business 
documents (e.g., memo and e-mails).  With 
regard to education, research using machine 
translation for language learning tools has been 
of interest since the early 1990?s (Anderson, 
1993, Richmond, 1994, and Yasuda, 2004), 
though little has been developed. Very recently, 
Microsoft introduced a product called Writing 
Wizard that uses machine translation to assist 
with business writing for native Chinese 
speakers. To our knowledge, this is currently the 
only deployed education-based tool that uses 
machine translation. 
Currently, all writing-based English 
language learning (ELL) writing-based products 
and services at Educational Testing Service rely 
on e-rater automated essay scoring and the 
Critique writing analysis tool capabilities 
(Burstein, Chodorow, and Leacock, 2004).  In 
trying to build on a portfolio of innovative 
products and services, we have explored using 
machine translation toward the development of 
new ELL-based capabilities. We have developed 
a prototype system for automatically generating 
translation exercises in Arabic --- the 
Translation Exercise Assistant.   
Translation exercises are one kind of task 
that teachers can offer to give students practice 
with specific grammatical structures in English. 
Our hypothesis is that teachers could use such a 
tool to help them create exercises for the 
classroom, homework, or quizzes. The idea 
behind our prototype is a capability that can be 
used either by classroom teachers to help them 
generate sentence-based translation exercises 
from an infinite number of Arabic language texts 
of their choice. The capability might be 
integrated into a larger English language 
learning application. In this latter application, 
these translation exercises could be created by 
classroom teachers for the class or for 
individuals who may need extra help with 
particular grammatical structures in English. 
Another potential use of this system that has 
been discussed is to use it in ESL classrooms in 
the United States, to allow teachers to offer 
exercises in students? native language, especially 
for students who are competent in their own 
language, but only beginners in English. 
We had two primary goals in mind in 
developing our prototype. First, we wanted to 
evaluate how well the machine translation 
capability itself would work with this 
application.  In other words, how useful were the 
system outputs that are based on the machine 
translations? We also wanted to know to what 
extent this kind of tool facilitated the task of 
creating translation exercise items.  So, how 
much time is involved for a teacher to manually 
create these kinds of items versus using the 
exercise assistant tool to create them? Manually 
creating such an item involves searching through 
numerous reference sources (e.g., paper or web-
based version of newspapers), finding sentences 
with the relevant grammatical structure in the 
source language (Arabic), and then manually 
producing an English translation that can be 
used as an answer key.   
To evaluate these aspects, we implemented a 
graphical user interface that offered our two 
users the ability to create sets of translation 
 16
 exercise items for six pre-selected, grammatical 
structures. For each structure the system 
automatically identified and offered a set of 200 
system-selected potential sentences per category. 
For the exercise creation task, we collected 
timing information that told us how long it took 
users to create 3 exercises of 10 sentences each, 
for each category. In addition, users rated a set 
of up to 200 Arabic sentences with regard to if 
they were usable as translation exercise items, so 
that we could gauge the proportion of sentences 
selected by the application. These were the 
sentences that remained in the set of 200 
because they were not selected for an exercise. 
Two teachers participated in the evaluation of 
our prototype. One of the users also did the task 
manually. 
 
2. Translation Exercise Selection 
 
2.1     Data Sets 
 
The source of the data was Arabic English 
Parallel News Part 1 and the Multiple 
Translation Arabic Part 1 corpus from the 
Linguistic Data Consortium.1   Across these data 
sets we had access to about 45,000 Arabic 
sentences from Arabic journalistic texts taken 
from Ummah Press Service, Xinhua News and 
the AFP News Service available for this 
research. We used approximately 10,000 of 
these Arabic sentences for system development, 
and selected sentences from the remaining 
Arabic sentences for use with the interface.2  
 
2.2 System Description 
 
We used Language Weaver?s3 Arabic-to-English 
system to translate the Arabic sentences in the 
data sets. We built a module to find the relevant 
grammatical structures in the English 
translations. This module first passes the English 
                                                 
1 The LDC reference numbers for these corpora are: 
LDC2004T18 and LDC2003T18. 
2 To avoid producing sentences with overly 
complicated structures, we applied two constraints to 
the English translation: 1) it contained 20 words or 
less, and 2) it contained only a single sentence.  
3 See http://www.languageweaver.com. 
 
translation to a part-of-speech tagger that assigns 
a part-of-speech to each word in the sentence. 
Another module identifies regular expressions 
for the relevant part-of-speech sequences in the 
sentences, corresponding to one of these six 
grammatical structures: a) subject-verb 
agreement, b) complex verbs, c) phrasal verbs, 
d) nominal compounds, e) prepositions, and f) 
adjective modifier phrases.  When the 
appropriate pattern was found in the English 
translation, the well-formed Arabic sentence that 
corresponds to that translation is added to the set 
of potential translation exercise sentence 
candidates in the interface.   
 
2.3 Results 
 
The outcome of the evaluation indicated 
that between 98% and 100% of automatically-
generated sentence-based translation items were 
selected by both users as usable for translation 
items.  In addition, the time involved to create 
the exercises using the tool was 2.6 times faster 
than doing the task manually. 
 
References 
 
Anderson, Don D. (1995) ?Machine Translation as a 
Tool in Second Language Learning?, CALICO 
Journal 13.1, 68?97.  
 
Burstein, J., Chodorow, M., & Leacock, C. (2004). 
Automated essay evaluation: The Criterion online 
writing service. AI Magazine, 25(3), 27-36. 
 
Johnson, Rod (1993) ?MT Technology and 
Computer-Aided Language Learning?, in Sergei 
Nirenburg (ed.) Progress in Machine Translation, 
Amsterdam: IOS and Tokyo: Ohmsha, pages 286?
287. 
 
Richmond, Ian M. (1994) ?Doing it backwards: 
Using translation software to teach target-language 
grammaticality?, Computer Assisted Language 
Learning 7, 65?78. 
 
Yasuda, K. Sugaya F., Sumita E, Takezawa T.,  
Kikui G., Yamamoto, S. (2004). Automatic 
Measuring of English Language Proficiency using 
MT Evaluation Technology. Proceedings of e-
Learning workshop, COLING 2004, Geneva, 
Switzerland. 
 17
Evaluating Multiple Aspects of Coherence in Student Essays
Derrick Higgins
Educational
Testing Service
Jill Burstein
Educational
Testing Service
Daniel Marcu
University of Southern
California
/ Information Sciences
Institute
Claudia Gentile
Educational
Testing Service
Abstract
CriterionSM Online Essay Evaluation Service
includes a capability that labels sentences in
student writing with essay-based discourse el-
ements (e.g., thesis statements). We describe
a new system that enhances Criterion?s capa-
bility, by evaluating multiple aspects of co-
herence in essays. This system identifies fea-
tures of sentences based on semantic similarity
measures and discourse structure. A support
vector machine uses these features to capture
breakdowns in coherence due to relatedness
to the essay question and relatedness between
discourse elements. Intra-sentential quality is
evaluated with rule-based heuristics. Results
indicate that the system yields higher perfor-
mance than a baseline on all three aspects.
1 Overview
This work is motivated by a need for advanced discourse
analysis capabilities for writing instruction applications.
CriterionSM Online Essay Evaluation Service is an appli-
cation for writing instruction which includes a capability
to annotate sentences in student essays with discourse el-
ement labels. These labels include the categories Thesis
Statement, Main Idea, Supporting Idea, and Conclusion
(Burstein et al, 2003b). Though it accurately annotates
sentences with essay-based discourse labels, Criterion
does not provide an evaluation of the expressive quality
of the sentences that comprise a discourse segment. The
system might accurately label a student?s essay as hav-
ing all of the typically expected discourse elements: the-
sis statement, 3 main ideas, supporting evidence linked
to each main idea, and a conclusion. As teachers have
pointed out, however, an essay may have all of these or-
ganizational elements, but the quality of individual ele-
ments may need improvement.
In this paper, we present a capability that captures ex-
pressive quality of sentences in the discourse segments
of an essay. For this work, we have defined expressive
quality in terms of four aspects related to global and lo-
cal essay coherence. The first two dimensions capture
global coherence, and the latter two relate to local coher-
ence: a) relatedness to the essay question (topic), b) re-
latedness between discourse elements, c) intra-sentential
quality, and d) sentence-relatedness within a discourse
segment. Each dimension represents a different aspect
of coherence.
Essentially, the goal of the system is to be able to pre-
dict whether a sentence in a discourse segment has high
or low expressive quality with regard to a particular co-
herence dimension. We have deliberately developed an
approach to essay coherence that is comprised of multi-
ple dimensions, so that an instructional application may
provide appropriate feedback to student writers, based on
the system?s prediction of high or low for each dimen-
sion. For instance, sentences in the student?s thesis state-
ment may have a strong relationship to the essay topic,
but may have a number of serious grammatical errors that
make it hard to follow. For this student, we may want to
point out that on the one hand, the sentences in the thesis
address the topic, but the thesis statement as a discourse
segment might be more clearly stated if the grammar er-
rors were fixed. By contrast, the sentences that comprise
the student?s thesis statement may be grammatically cor-
rect, but only loosely related to the essay topic. For this
student, we would also want the system to provide ap-
propriate feedback to, so that the student could revise the
thesis statement text appropriately.
In earlier work, Foltz, Kintsch & Landauer (1998),
and Wiemer-Hastings & Graesser (2000) have devel-
oped systems that also examine coherence in student
writing. Their systems measure lexical relatedness be-
tween text segments by using vector-based similarity
between adjacent sentences. This linear approach to
similarity scoring is in line with the TextTiling scheme
(Hearst and Plaunt, 1993; Hearst, 1997), which may
be used to identify the subtopic structure of a text.
Miltsakaki and Kukich (2000) have also addressed the is-
sue of establishing the coherence of student essays, using
the Rough Shift element of Centering Theory. Again, this
previous work looks at the relatedness of adjacent text
segments, and does not explore global aspects of text co-
herence.
Hierarchical models of discourse have been applied to
the question of coherence (Mann and Thompson, 1986),
but so far these have been more useful in language gen-
eration than in determining how coherent a given text is,
or in identifying the specific problem, such as the break-
down of coherence in a document.
Our approach differs in fundamental ways from this
earlier work that deals with student writing. First, Foltz
et al (1998), Wiemer-Hastings and Graesser (2000),
and Miltsakaki and Kukich (2000) assume that text co-
herence is linear. They calculate the similarity between
adjacent segments of text. By contrast, our approach
considers the discourse structure in the text, following
Burstein et al (2003b). Our method considers sentences
with regard to their discourse segments, and how the sen-
tences relate to other text segments both inside (such as
the essay thesis) and outside (such as the essay topic) of a
document. This allows us to identify cases in which there
may be a breakdown in coherence due to more global as-
pects of essay-based discourse structure. Second, previ-
ous work has used Latent Semantic Analysis as a seman-
tic similarity measure (Landauer and Dumais, 1997). We
have adapted another vector-based method of semantic
representation: Random Indexing (Kanerva et al, 2000;
Sahlgren, 2001). Another difference between our sys-
tem and earlier systems is that we use essays manually
annotated on the four coherence dimensions to train our
system.
The final system employs a hybrid approach to classify
the first two of the four coherence dimensions with a high
or low quality rank. For these dimensions, a support vec-
tor machine is used to model features derived from Ran-
dom Indexing and from essay-based discourse structure
information. A third local coherence dimension compo-
nent is driven by rule-based heuristics. A fourth dimen-
sion related to coherence within a discourse segment can-
not be classified due to a lack of data characterizing low
expressive quality. This is fully explained later in the pa-
per.
2 Protocol Development and Human
Annotation
2.1 Protocol Development
The development of this system required a corpus of hu-
man annotated essay data for modeling purposes. In the
end, the goal is to have the system make judgments sim-
ilar to those made by a human with regard to ranking the
coherence of an essay on four dimensions. Therefore, we
created a detailed protocol for annotating the expressive
quality of essay-based discourse elements in essays with
regard to four aspects related to global and local essay
coherence. This protocol was designed for the following
purposes:
1. To yield annotations that are useful for the purpose
of providing students with feedback about the ex-
pressive relatedness of discourse elements in their
essays, given four relatedness dimensions;
2. To permit human annotators to achieve high levels
of consistency during the annotation process;
3. To produce annotations that have the potential of be-
ing derivable by computer programs through train-
ing on corpora annotated by humans.
2.1.1 Expressive Quality of Discourse Segments:
Protocol Description
According to writing experts who collaborated in this
work, the expressive relatedness of a sentence discourse
element may be characterized in terms of four dimen-
sions: a) relationship to prompt (essay question topic),
b) relationship to other discourse elements, c) relevance
with discourse segment, and d) errors in grammar, us-
age, and mechanics. For the sake of brevity, we refer to
these four dimensions as DimP (relatedness to prompt),
DimT (typically, relatedness to thesis), DimS (related-
ness within a discourse segment), and DimERR.
The two annotators were required to label each sen-
tence of an essay for expressive quality on the four di-
mensions (above). For the 989 essays used in this study,
each sentence had already been manually annotated with
these discourse labels: background material, thesis, main
idea, supporting idea, and conclusion (Burstein et al,
2003b).1 An assignment of high (1) or low (0) was given
to each sentence, on the dimensions relevant to the dis-
course element. Not all dimensions apply to all discourse
elements. The protocol is extremely specific as to how
annotators should label the expressive quality for each
sentence in a discourse element with regard to the four
dimensions. In this paper, we provide a brief description
of the labeling protocol, so that the purpose of each di-
mension is clear.
Figure 1 shows a sample essay and prompt. A hu-
man judge has assigned a label to each sentence in the
essay, resulting in the illustrated division into discourse
segments. In addition, the figure indicates human annota-
tors? ratings for two of our coherence dimensions (DimP
and DimT , discussed below). By and large, the essay
consistently follows up on the ideas of the essay thesis,
and so most sentences get a high relatedness score on
DimT . However, much of the essay fails to directly ad-
dress the question posed in the essay prompt, and so many
sentences are assigned low relatedness on DimP .
Dimension 1: DimP (Relatedness to Prompt)
The text of the discourse element and the prompt (text
of the essay question) must be related. Specifically, the
thesis statement, main ideas, and conclusion statement
should all contain text that is strongly related to the essay
topic. If this relationship does not exist, this is perhaps
evidence that the student has written an off-topic essay.
For this dimension, a high rank is assigned to each sen-
tence from background material, thesis, main idea and
conclusion statement that is related to the prompt text;
otherwise a low rank is assigned.
1The annotated data from the Burstein et al (2003b) study
were used to develop a commercial application that automati-
cally assigns these discourse labels to student essays.
Discourse Sentence DimP DimT
Segment
Prompt Images of beauty?both male and female?are promoted in magazines, in movies, on
billboards, and on television. Explain the extent to which you think these images can
be beneficial or harmful.
Background
A lot of people really care about how they look or how other people look. Low High
A lot of people like reading magazines or watch t.v about how you can fix your looks if
you don?t like the way your looks are. High High
Thesis
People that care about how they look is because they have problems at home, their parents
don?t pay attention to them or even that they have a high self-steem which that is not good. Low N/A
A lot of people get to the extent of killing themselfs just because they?re not happy with
there looks. Low N/A
Support Many people go thru make-overs to experiment how they will look but, some people stilldon?t like themself. N/A High
Main Point
The people that don?t like themselfs need some helps and they probably feel like that be-
cause they have told them oh! your ugly , you look like Blank! or maybe a guy never ask a
her out.
Low Low
Support
In case of a guy probably the same comments but he won?t dare to ask a girl out because
he feels that the girl is going to say no because of the way he looks. N/A High
Things like this make people don?t like each other. N/A High
Conclusion I suggest that a those people out here that are not happy with their looks get some help. Low HighTheirs alot of programs that you can get help. Low Low
Figure 1: Student essay with discourse segments and two coherence dimensions as annotated by human judge
Dimension 2: DimT (Relatedness to Thesis)
The relationship between a discourse element and
other discourse elements in the text governs the global
coherence of the essay text. For a text to hold together,
certain discourse elements must be related or the text will
appear choppy and will be difficult to follow. Specifi-
cally, a high rank is assigned to each sentence in the back-
ground material, main ideas and conclusion that is related
to the thesis, and supporting idea sentences that relate to
the relevant main idea. A conclusion sentence may also
be given a high rank if it is related to a main idea or back-
ground information. Low ranks are assigned to sentences
that do not have these relationships.
Dimension 3: DimS (Relatedness within Segment)
This dimension indicates the cohesiveness of the mul-
tiple sentences in a discourse segment of a text. This
dimension distinguishes a text segment that may go off
task within a discourse segment. For this dimension, a
high rank was assigned to each sentence in a discourse
segment that related to at least one other sentence in the
segment; otherwise the sentence received a low rank. If
the discourse segment contained only one sentence, then
the DimT label was assigned as the default.
Dimension 4: DimERR (Technical Errors)
Dimension 4 measures a sentence?s relatedness of ex-
pression with regard to grammar, mechanics and word
usage. More specifically, a sentence is considered to be
low on this dimension if it contains frequent patterns of
error, defined as follows: (a) contains 2 errors in gram-
mar, word usage or mechanics (i.e., spelling, capitaliza-
tion or punctuation), (b) is an incomplete sentence, or (c)
is a run-on sentence (i.e., 4 or more independent clauses
within a sentence).
2.2 Topics, Human Annotation, and Human
Agreement
2.2.1 Topics & Writing Genre
Essays written to two genres were used: five of the top-
ics were persuasive, and one was expository. Persuasive
writing requires the reader to state an opinion on a par-
ticular topic, support the stated opinion, and convince the
reader that the perspective is valid and well-supported.
An expository topic requires the writer only to state an
opinion on a topic. This typically elicits more personal
and descriptive writing. Four of the five sets of persua-
sive essay responses were written by college freshman,
and the fifth by 12th graders. The set of expository re-
sponses were also written by 12th graders.
2.2.2 Human Annotation
Two human judges participated in this study. The
judges were instructed to assign relevant dimension la-
bels to each sentence. Pre-training of the judges was done
using a set of approximately 50 essays across the six top-
ics in the study. During this phase, the authors and the
judges discussed and labeled the essays together. During
the next training phase, the judges labeled a total of 292
essays across six topics. They labeled the identical set of
essays, and were allowed to discuss their decisions. In the
next annotation phase, the judges did not discuss their an-
notations. In this post-training phase (annotation phase),
each judge labeled an average of about 278 unique es-
says for each of four prompts (556 essays together). Each
judge also labeled an additional set of 141 essays that was
overlapping. So, about 20 percent of the data annotated
by each judge in the annotation phase was overlapping,
Agreement ?
DimP (N=779) 99% .99
DimT (N=1890) 100% .99
DimS (N=2119) 100% .99
DimERR (N=2170) 99% .98
Table 1: Annotator agreement across coherence
dimensions?data from annotation phase
and 80 percent was unique. The 20 percent is used to ob-
tain human agreement.2 During both the training and an-
notation phases, Kappa statistics were run on their judg-
ments regularly, and if the Kappa for any particular cate-
gory fell below 0.8, then the judges were asked to review
the protocol until their agreement was acceptable. At the
end of the annotation phase, we had a total of 989 labeled
essays: 292 (training phase) + 278 ? 2 (unique essays
from annotator 1 + annotator 2, annotation phase) + 141
(overlapping set, annotation phase).
Human Judge Agreement
It is critical that the annotation process yields agree-
ment that is high enough between human judges, such
that it suggests that people can agree on how to categorize
the discourse elements. As is stated in the above section,
during the training of the judges for this study, Kappa
statistics were computed on a regular basis. Kappa be-
tween the judges for each category had to be maintained
at least 0.8, since this is believed to represent strong
agreement (Krippendorff, 1980). In Table 1 we report
human agreement for overlapping data from the four top-
ics on all four dimensions. Clearly, the level of human
agreement is quite high across all four coherence dimen-
sions. In addition, if we look at kappas of sentences based
on discourse category, no kappa falls below 0.9.
3 Method
Our final system uses a hybrid approach to label three of
the four coherence dimensions. For DimP and DimT ,
assigning coherence judgments to sentences in an essay
proceeds in three stages 1) identifying the discourse label
associated with each sentence in an essay, 2) computing
features that quantify the semantic similarity between dif-
ferent discourse segments of the essay, and 3) applying a
classifier to make a coherence judgment on a dimension.
Consistent with the human annotated data, a coherence
judgment on any dimension is either ?high? or ?low.? The
method for DimERR is rule-based, and is discussed later.
3.1 Discourse element feature identification
As noted earlier, the two human judges in this study anno-
tated the four coherence dimensions according to the hu-
2For the annotation phase, we were unable to collect data
for two essay prompts because of our annotators? availability.
This means that we only have inter-annotator agreement statis-
tics on 4 prompts, although some data from all six prompts was
available for training and testing our models (with the extra two
prompts being represented in the training phase of annotation).
man discourse label assignments. Accordingly, we also
used the human assigned discourse labels as features for
predicting coherence judgments. In a deployed system,
however, we would use discourse element labels gener-
ated from Criterion?s discourse analysis system (Burstein
et al, 2003b). Further evaluation is, of course, necessary
in order to determine the effect of using these automat-
ically assigned labels in place of the gold standard dis-
course labels.
3.2 Semantic similarity features
Given the partition of an essay into discourse segments,
we then derive a set of features from the essay in order
to predict how closely related each sentence is to various
important text segments, such as the essay topic, and dis-
course elements, such as thesis statement. As described
in Section 4, the features that are most useful for clas-
sifying sentences according to coherence are semantic
similarity features derived from Random Indexing (Kan-
erva et al, 2000; Sahlgren, 2001). Random Indexing is
a vector-based semantic representation system similar to
Latent Semantic Analysis. Our Random Indexing (RI)
semantic space is trained on about 30 million words of
newswire text.
When we extract a feature such as ?RI similarity to
prompt? for a sentence, this essentially measures to what
extent the sentence contains terms in the same semantic
domain as compared to those found in the prompt. Within
any discourse segment, any semantic information that is
word-order dependent is lost.
3.3 Support vector classification
Finally, for each sentence in the essay we use the fea-
tures derived from the essay to make a determination as
to whether it meets our criteria for coherence in these
dimensions (DimP and DimT ). To make this determi-
nation, we use a support vector machine (SVM) classi-
fier (Vapnik, 1995; Christianini and Shawe-Taylor, 2000).
Specifically, we use an SVM with a radial basis function
kernel, which exhibited good performance on a subset of
about 30 essays from the pre-training data.
4 Results
In each of the experiments below, the results are re-
ported for the entire set of 989 essays annotated for this
project. We performed ten-fold cross-validation, training
our SVM classifier on 910 of the data at a time, and testing
on the remaining 110 . We report the results on the cross-
validation set for all runs combined.
For each dimension, we also report the performance
of a simple baseline measure, which assumes that all of
our essay coherence criteria are satisfied. That is, our
baseline assigns category 1 (high relevance) to every
sentence, on every dimension.
These essays were written in response to six different
prompts, and had an average (human-assigned) score of
Score DimP DimT DimS DimERR
1?2 64.1% 71.2% 94.8% 61.1%
5?6 72.0% 70.9% 97.2% 92.9%
Table 2: Baseline performance on each coherence dimen-
sion, broken down by essay score point
4.0 on a six-point scale. Therefore, a priori, it seems pos-
sible that we could build a better baseline model by con-
ditioning its predictions on the overall score of the essay
(assigning 1?s to sentences from better-scoring essays,
and 0?s to sentences from lower-scoring essays). How-
ever, the coherence requirements of each of our dimen-
sions are usually met even in the lowest-scoring essays,
as shown in Table 2, which lists the percentage of sen-
tences in different essay score ranges which our human
annotators assigned category 1. Looking at the highest
and lowest score points on our six-point scale, it is clear
that higher-scoring essays do tend to have fewer problems
with coherence, but this effect is not overwhelming. (The
largest gap between the highest- and lowest-scoring es-
says is on DimERR, which deals with errors in grammar,
usage, and mechanics.)
4.1 DimP
According to the protocol, there are four discourse ele-
ments for which DimP , the degree of relatedness to the
essay prompt, is relevant: Background, Conclusion, Main
Point, and Thesis. The Supporting Idea category of sen-
tence is not required to be related to the prompt, because
it may express an elaboration of one of the main points of
the essay, and has a more tenuous and mediated logical
connection to the essay prompt text.
The features which we provide to the SVM for predict-
ing a sentence?s relatedness to the prompt are:
1. The RI similarity score of the target sentence with
the entire essay prompt,
2. The maximum RI similarity score of the target sen-
tence with any sentence in the essay prompt,
3. The RI similarity score of the target sentence with
the required task sentence (a designated portion of
the prompt text which contains an explicit directive
to the student to write about a specific topic),
4. The RI similarity score of the target sentence with
the entire thesis of the essay,
5. The maximum RI similarity score of the target sen-
tence with any sentence in the thesis,
6. The maximum RI similarity score of the target sen-
tence with any sentence in the preceding discourse
chunk,
7. The number of sentences in the current chunk,
8. The offset of the target sentence (sentence number)
from the beginning of the current discourse chunk,
9. The number of sentences in the current chunk whose
similarity with the prompt is greater than .2,
10. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .2,
11. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .2,
12. The number of sentences in the current chunk whose
similarity with the prompt is greater than .4,
13. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .4,
14. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .4,
15. The length of the target sentence in words,
16. A Boolean feature indicating whether the target sen-
tence contains a transition word, such as ?however?,
or ?although?,
17. A Boolean feature indicating whether the target sen-
tence contains an anaphoric element, and
18. The category of the current chunk. (This is encoded
as five Boolean features: one bit for each of ?Back-
ground?, ?Conclusion?, ?Main Point?, ?Supporting
Idea?, and ?Thesis?.)
In calculating features 2, 5, and 6, we use the maximum
similarity score of the sentence with any other sentence in
the relevant discourse segment, rather than simply using
the similarity score of the sentence with the entire text
chunk. We add this feature based on the intuition that for
a sentence to be relevant to another discourse segment, it
need only be connected to some part of that segment.
It is perhaps surprising that we include features which
measure the degree of similarity between the sentence
and the thesis, since we are trying to predict its related-
ness to the prompt, rather than the thesis. However, there
are two reasons we believe this is fruitful. First, since we
are dealing with a relatively small amount of text, com-
paring a single sentence to a short essay prompt, looking
at the thesis as well helps to overcome data sparsity is-
sues. Second, it may be that the relevance of the current
sentence to the prompt is mediated by the student?s thesis
statement. For example, the prompt may ask the student
to take a position on some topic. They may state this po-
sition in the thesis, and provide an example to support it
as one of their Main Points. In such a case, the example
would be more clearly linked to the Thesis, but this would
suffice for it to be related to the prompt.
Considering the similarity scores of sentences in the
current discourse segment is also, in part, an attempt to
overcome data sparsity issues, but is also motivated by
the idea that it may be an entire discourse segment which
can properly be said to be (ir)relevant to the essay prompt.
The sentence length and transition word features do
not directly reflect the relatedness of a sentence to the
prompt, but they are likely to be useful correlates.
Finally, the feature (#17) indicating the presence of
a pronoun is to help the system deal with cases in
which a sentence contains very few content words, but
is still linked to other material in the essay by means of
anaphoric elements, such as ?This is shown by my argu-
ment.? In such as case, the sentence would normally get
a low similarity score with the prompt (and other parts of
the essay), but the information that it contains a pronoun
might still allow the system to classify it correctly.
Table 3 shows results using the baseline algorithm to
classify sentences according to their relatedness to the
prompt. Table 4 presents the results using the SVM clas-
sifier. We provide precision, recall, and f-measure for the
assignment of the labels 1 and 0, and an overall accuracy
measure in the far right column. (The accuracy measure
is the value for precision and recall when 1 and 0 ranks
are collapsed. Precision and recall will be the same, since
the number of labels assigned by the model is equal to the
number of labels in the target assignment.)
The SVM model outperforms the baseline on every
subcategory, with the largest gains on Background sen-
tences, most of which are, in fact, unrelated to the prompt
according to our human judges. This low baseline result
on Background sentences could indicate that many stu-
dents have a problem with providing unnecessary and ir-
relevant prefaces to the important points in their essays.
Note that the trained SVM has around .9 recall on the
class of sentences which according to our human annota-
tors have high relevance to the prompt. This means that
our system is less likely to incorrectly assign a low rank
to a sentence that is high. So, the system will tend to err
on the side of the student, which is a preferable trade-off.
In part, this is due to the nature of the semantic similarity
measure we are using, which does not take word order
into account. While RI does allow us to capture a richer
meaning component than simply matching words which
co-occur in the target sentence and prompt, it still does
not encompass all that goes into determining whether a
sentence ?relates? to another chunk of text. Students of-
ten write something which bears a loose topical connec-
tion with the essay prompt, but does not directly address
the question. This sort of problem is hard to address with
a tool such as LSA or RI; the vocabulary of the sentence
on its own will not provide a clue to the sentence?s failure
to address the task.
4.2 DimT
The annotation protocol states that these four discourse
elements come into play for DimT : Background, Con-
clusion, Main Point, and Supporting Idea. Because this
dimension indicates the degree of relatedness to the the-
sis of the essay (and also other discourse segments in the
case of Supporting Idea and Conclusion sentences; see
Section 2.1.1 above), we do not consider thesis sentences
with regard to this aspect of coherence.
The features which we provide to the SVM for pre-
dicting whether or not a given sentence is related to the
thesis are almost the same ones used for DimP . The only
difference is that we omit features #12 and #13 in our
model of DimT . These are the features which evaluate
how many sentences in the current chunk have a simi-
larity score with the prompt and required task sentence
greater than 0.4. While DimP is to some degree sensitive
to the similarity of a sentence to the thesis, and DimT can
likewise benefit from the information about a sentence?s
similarity to the prompt, it seems that the latter link is less
important, so a single cutoff suffices for this model.
Tables 5?6 present the results for our SVM model and
for a baseline which assigns all sentences ?high? rele-
vance. The improvements on DimT are smaller than the
ones reported for DimP , but we still record an overall
gain of four percentage points in accuracy. Only on con-
clusion sentences were we unable to produce an improve-
ment over the baseline; we need to investigate this further.
Again, the system achieves high recall on sentences
with high relatedness. It outperforms the baseline by cor-
rectly identifying a modest percentage of the sentences
labeled as having low relatedness with the thesis.
4.3 DimS
DimS , which concerns whether the target sentence re-
lates to another sentence within the same discourse seg-
ment, seems another good candidate for applying our se-
mantic similarity score to the task of establishing coher-
ence. At present, however we have not made substan-
tial progress on this task. The baselines for DimS are
substantially higher than those for dimensions DimP and
DimT ? 98.1% of all sentences in our data were anno-
tated as ?highly related? with respect to this dimension.
This indicates that it is relatively rare to find a sentence
which is not related to anything in the same discourse
segment. This makes our task, to characterize those sen-
tences which are not related to the discourse segment,
much more difficult, since there are so few examples of
sentences with low-ranking coherence.
4.4 DimERR
DimERR is clearly a different kind of problem. Here, we
are looking for clarity of expression, or coherence within
a sentence. We base this solely on technical correctness.
We are able to automatically assign high and low ranks to
essay sentences using a set of rules based on the number
of grammar, usage and mechanics errors. The rules used
for DimERR are as follows: a) assign a low label if the
sentence is a fragment, if the sentence contains 2 or more
grammar, usage, and mechanics errors, or if the sentence
is a run-on, b) assign a high label if no criteria in (a) apply.
Criterion?s discourse analysis system also provides
an essay score with e-rater?, and qualitative feedback
about grammar, usage, mechanics, and style (Leacock
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.486 1.000 0.654 0.000 0.000 0.000 0.486
Conclusion (N = 1830) 0.757 1.000 0.862 0.000 0.000 0.000 0.757
Main Point (N = 1566) 0.663 1.000 0.797 0.000 0.000 0.000 0.663
Thesis (N = 1899) 0.712 1.000 0.832 0.000 0.000 0.000 0.712
All sentence types (N = 6372) 0.675 1.000 0.806 0.000 0.000 0.000 0.675
Table 3: Baseline performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.714 0.702 0.708 0.723 0.735 0.729 0.719
Conclusion (N = 1830) 0.784 0.959 0.863 0.578 0.175 0.269 0.768
Main Point (N = 1566) 0.729 0.888 0.801 0.616 0.352 0.448 0.708
Thesis (N = 1899) 0.771 0.929 0.843 0.644 0.318 0.426 0.753
All sentence types (N = 6372) 0.759 0.901 0.824 0.665 0.407 0.505 0.740
Table 4: SVM performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.793 1.000 0.885 0.000 0.000 0.000 0.793
Conclusion (N = 1829) 0.834 1.000 0.909 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.742 1.000 0.852 0.000 0.000 0.000 0.742
Support (N = 10332) 0.664 1.000 0.798 0.000 0.000 0.000 0.664
All sentence types (N = 14777) 0.702 1.000 0.825 0.000 0.000 0.000 0.702
Table 5: Baseline performance on DimT
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.856 0.980 0.914 0.827 0.368 0.509 0.853
Conclusion (N = 1829) 0.834 1.000 0.910 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.776 0.997 0.873 0.958 0.172 0.292 0.785
Support (N = 10332) 0.709 0.945 0.810 0.684 0.237 0.352 0.706
All sentence types (N = 14777) 0.744 0.962 0.839 0.709 0.221 0.337 0.741
Table 6: SVM performance on DimT
and Chodorow, 2000; Burstein et al, 2003a). We can
easily use Criterion?s outputs about grammar, usage, and
mechanics errors to assign high and low ranks to essay
sentences, using the rules described in the previous sec-
tion.
The performance of the module that does the DimERR
assignments is in Table 7. We used half of the 292 essays
from the training phase of annotation for development,
and the remaining data from the training and post-training
phases of annotation for cross-validation. Results are re-
ported for the cross-validation set. Text labeled as titles,
or opening or closing salutations, are not included in the
results. The baselines were computed by assigning all
sentences a high rank label. The baseline is high; how-
ever, the algorithm outperforms the baseline.
5 Discussion and Conclusions
There were multiple goals in this work. We wanted to in-
troduce a concept of essay coherence comprising multi-
ple aspects, and investigate what linguistic features drive
each aspect in student essay writing. Further, we wanted
Sentence N Precision Recall F-measure
Baseline
High 11789 0.83 1.00 0.91
Low 2351 0.00 0.00 0.00
Overall 14140 0.83 0.83 0.83
Algorithm
High 11789 0.88 0.96 0.92
Low 2351 0.63 0.34 0.44
Overall 14140 0.86 0.86 0.86
Table 7: Performance on DimERR
to build a system to automatically evaluate these multiple
aspects of coherence, so that appropriate feedback can be
provided through a writing instruction application.
To accomplish these goals, we have worked with writ-
ing experts to develop a comprehensive protocol that de-
tails how coherence in writing can be evaluated, either
manually or automatically. Using this protocol, human
annotators labeled a corpus of student essays, using the
coherence dimensions. These annotations built on a pre-
vious set of annotations for these data, whereby discourse
element labels were assigned. The result is a richly anno-
tated data set with information about discourse elements,
as well as their coherence in the context of the discourse
structure. Using this data set, we were able to learn what
linguistic features can be used to evaluate various aspects
of coherence in student writing. We then developed a
prototype system that ranks global and local aspects of
coherence in an essay. This capability shows promise in
ranking three aspects of coherence in essays: a) relation-
ship to essay topic, b) relationship between discourse ele-
ments, and c) intra-sentential technical quality. More low
ranking data on a fourth dimension, coherence within a
discourse segment, needs to be identified and annotated
before this dimension can be modeled.
The approach used is innovative, since it moves beyond
earlier methods of evaluating coherence in student writ-
ing that capture only local information between adjacent
sentences. Two methods are used to model the aspects
of coherence handled by the system. For the two global
coherence dimensions, DimP and DimT , a support vec-
tor machine provides a coherence ranking of sentences
based on features related to essay-based discourse infor-
mation, and semantic similarity values derived from the
RI algorithm. Using this classification method, we are
able to rank the expressive quality of sentences in essay-
based discourse segments, with regard to relatedness to
the text of the prompt, and also as they relate to the thesis
statement. With regard to the local coherence dimension,
DimERR, we use a rule-based heuristic to rank intra-
sentential quality. This addresses the issue of sentences in
essays that have serious grammatical problems that may
interfere with a reader?s comprehension. We take advan-
tage of Criterion?s identification of grammar, usage, and
mechanics errors to design the rules for ranking this local
coherence dimension.
We hope that in further investigation of this richly an-
notated data set, we will be able to build on the current
prototype and develop a full-scale writing instruction ca-
pability that provides feedback on the coherence dimen-
sions described in this paper.
Acknowledgements
We would like to thank Irma Lorenz and Shauna Cooper
for advice on protocol development and for the annota-
tion work, and Martin Chodorow for discussions about
Random Indexing. We thank the anonymous reviewers
for their helpful feedback.
Any opinions expressed here are those of the authors
and not necessarily of the Educational Testing Service.
References
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003a. CriterionSM: Online essay evaluation: An ap-
plication for automated evaluation of student essays.
In Proceedings of the Fifteenth Annual Conference on
Innovative Applications of Artificial Intelligence, Aca-
pulco, Mexico.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003b.
Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Trans-
actions on Intelligent Systems: Special Issue on Ad-
vances in Natural Language Processing, 181:32?39.
Nello Christianini and John Shawe-Taylor. 2000. Sup-
port Vector Machines and other Kernel-based Learn-
ing Methods. Cambridge University Press, Cam-
bridge, UK.
Peter Foltz, Walter Kintsch, and Thomas K. Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25(2&3):285?307.
Marti A. Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Pro-
ceedings of ACM SIGIR, pages 59?68.
Marti A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Random
indexing of text samples for Latent Semantic Analysis.
In L. R. Gleitman and A. K. Josh, editors, Proc. 22nd
Annual Conference of the Cognitive Science Society.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Claudia Leacock and Martin Chodorow. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL 2000, pages 140?147.
William Mann and Sandra Thompson. 1986. Relational
processes in discourse. Discourse Processes, 9:57?90.
Eleni Miltsakaki and Karen Kukich. 2000. Automated
evaluation of coherence in student essays. In Proceed-
ings of LREC 2000, Athens, Greece.
Magnus Sahlgren. 2001. Vector based semantic analy-
sis: Representing word meanings based on random la-
bels. In Proceedings of the ESSLLI 2001 Workshop on
Semantic Knowledge Acquisition and Categorisation.
Helsinki, Finland.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York.
Peter Wiemer-Hastings and Arthur Graesser. 2000.
Select-a-Kibitzer: A computer tool that gives mean-
ingful feedback on student compositions. Interactive
Learning Environments, 8(2):149?169.
NAACL HLT Demonstration Program, pages 3?4,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
The Automated Text Adaptation Tool 
 
Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee & Matthew Ventura 
Educational Testing Service 
Rosedale Road MS 12R 
Princeton, New Jersey 08541 
{jburstein, jshore, jsabatini , ylee, mventura}@ets.org 
 
 
1. Introduction 
 
Text adaptation is a teacher practice used to help 
with reading comprehension and English 
language skills development for English language 
learners (ELLs) (Carlo, August, McLaughlin, 
Snow, Dressler, Lippman, Lively, & White, 
2004; Echevarria, Vogt and Short, 2004; Yano, 
Long and Ross, 1994). The practice of text 
adaptation involves a teacher?s modification of 
texts to make them more understandable, given a 
student?s reading level.  Teacher adaptations 
include text summaries, vocabulary support (e.g., 
providing synonyms), and translation. It is a time-
consuming, but critical practice for K-12 teachers 
who teach ELLs, since reading-level appropriate 
texts are often hard to find. To this end, we have 
implemented the Automated Text Adaptation 
Tool v.1.0 (ATA v.1.0): an innovative, 
educational tool that automatically generates text 
adaptations similar to those teachers might create. 
We have also completed a teacher pilot study.  
Schwarm and Ostendorf (2005), and Heilman, 
Collins-Thompson, Callan, and Eskenazi (2006) 
describe related research addressing the 
development of NLP-based reading support tools. 
During our interactive demonstration, 
conference participants can (a) login to the 
Internet-accessible tool, (b) import text files, and 
(c) experiment with adaptation features. We are 
currently interested in feedback from the 
computational linguistics community to inform 
tool development related to (a) feature 
enhancement, and (b) ideas for new NLP-based 
features. Until now, our primary source of 
feedback has been from teachers toward tool 
development from an educational perspective. 
 
2. The Automated Text Adaptation Tool  
 
NLP-based text adaptation capabilities in the tool 
are described in this section (also see Figure 1.) 
These adaptation features were selected for 
implementation since they resemble teacher-
based adaptation methods.  
 
2.1 English and Spanish Marginal Notes 
 
Pedagogically, marginal notes are a kind of text 
summary. The Rhext automatic summarization 
tool (Marcu, 2000) is used to produce marginal 
notes in English. The amount of marginal notes 
generated can be increased or decreased based on 
students? needs. Using Language Weaver?s1 
English-to-Spanish machine translation system, 
English marginal notes can be translated into 
Spanish. 
 
2.2 Vocabulary Support 
 
Synonyms for lower frequency (more difficult) 
words are output using a statistically-generated 
word similarity matrix (Lin, 1998). ATA v.1.0 
generates antonyms for vocabulary in the text 
using WordNet?.2   Cognates are words which 
have the same spelling and meaning in two 
languages (e.g., animal in English and Spanish). 
The tool generates these using an ETS 
English/Spanish cognate lexicon. 
 
2.3 English and Spanish Text-to-Speech  
 
The tool offers English and Spanish text-to-
speech (TTS)3. English TTS may be useful for  
pronunciation support, while Spanish TTS 
provides access to the Spanish texts for Spanish-
speaking ELLs who are not literate in Spanish.   
                                                 
1 See http://www.languageweaver.com 
2 See http://wordnet.princeton.edu/
3 See http://www.cstr.ed.ac.uk/projects/festival/  & 
http://cslu.cse.ogi.edu/tts/download/.  
3
 
Figure 1.  Example Main Interface Screen showing English Marginal Notes  
in the right column and Synonyms for ?enjoyable? (entertaining, enjoyable, pleasant.)  
3. Pilot Study with Teachers 
The survey feedback indicated that the 12 teachers 
were positive about the tool?s potential. Overall, 
the vocabulary and English marginal notes were 
the most favorite features, while the text-to-speech 
was the least favorite.  Teachers commented that 
they would like to see an editing capability added 
that would allow them to make changes to the 
automatically generated outputs (i.e., vocabulary 
support, and English and Spanish marginal notes.) 
Teachers viewed the tool either as lesson planning 
support, or as a student tool for independent work.   
 
4. Future Research 
 
ATA v.1.0 is a young application that uses NLP 
methods to create text adaptations. The teacher pilot 
evaluation suggested that it produces adaptations 
with potentially effective support for ELLs. It could 
also save teachers lesson planning time. We are 
currently implementing teacher-suggested 
modifications, and planning a larger, school-based 
pilot. The pilot will evaluate the tool?s effectiveness 
in terms of measurable learning gains in reading 
comprehension and English language skills.  
 
References  
 
Carlo, M.S., August, D., McLaughlin, B., Snow, C.E., 
Dressler, C., Lippman, D., Lively, T. & White, C.  
 
 
(2004). Closing the gap: Addressing the vocabulary 
needs of English language learners in bilingual and 
mainstream classrooms. Reading Research Quarterly, 
39(2), 188-215. 
 
Echevarria, J., Vogt, M., and Short, D. (2004). Making 
Content Comprehensible for English Language 
Learners: the SIOP model.  New York: Pearson 
Education, Inc. 
 
Heilman, M., Collins-Thompson, K., Callan, J., 
Eskenazi, M. (2006) Classroom Success of an 
Intelligent Tutoring System for Lexical Practice and 
Reading Comprehension. In Proceedings of the Ninth 
International Conference on Spoken Language 
Processing.  Pittsburgh. 
 
Lin, D. (1998). Automatic Retrieval and Clustering of 
Similar Words.  In Proceedings of the 35th Annual 
Meeting of the Association for Computational 
Linguistics, Montreal, 898-904. 
 
Marcu, D. (2000) The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press, 
Cambridge, Massachusetts. 
 
Schwarm, S. and Ostendorf, M. Reading Level 
Assessment Using Support Vector Machines and 
Statistical Language Models. In Proceedings of the 
Association for Computational Linguistics, Ann 
Arbor, MI, 523-530. 
 
Yano, Y., Long, M. & Ross, S. (1994). The effects of 
simplified and elaborated texts on foreign language 
reading comprehension. Language Learning, 44, 189-
219.  
4
Towards Automatic Classification of Discourse Elements in Essays 
 
Jill Burstein 
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
Jburstein@ 
etstechnologies.com 
Daniel Marcu 
ISI/USC 
4676 Admiralty 
Way 
Marina del Rey, 
CA, USA 
Marcu@isi.edu 
Slava Andreyev  
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
sandreyev@ 
etstechnologies.com 
Martin Chodorow 
Hunter College, The 
City University of 
 New York 
New York, NY USA 
Martin.chodorow@ 
hunter.cuny.edu 
 
 
Abstract 
Educators are interested in essay 
evaluation systems that include 
feedback about writing features that 
can facilitate the essay revision 
process. For instance, if the thesis 
statement of a student?s essay could be 
automatically identified, the student 
could then use this information to 
reflect on the thesis statement with 
regard to its quality, and its relationship 
to other discourse elements in the 
essay. Using a relatively small corpus 
of manually annotated data, we use 
Bayesian classification to identify 
thesis statements.  This method yields 
results that are much closer to human 
performance than the results produced 
by two baseline systems.  
 
1 Introduction 
 
Automated essay scoring technology can 
achieve agreement with a single human judge 
that is comparable to agreement between two 
single human judges (Burstein, et al1998; Foltz, 
et al1998; Larkey, 1998; and Page and 
Peterson, 1995). Unfortunately, providing 
students with just a score (grade) is insufficient 
for instruction. To help students improve their 
writing skills, writing evaluation systems need 
to provide feedback that is specific to each 
individual?s writing and that is applicable to 
essay revision. 
The factors that contribute to improvement 
of student writing include refined sentence 
structure, variety of appropriate word usage, and 
organizational structure. The improvement of  
organizational structure is believed to be critical 
in the essay revision process toward overall 
improvement of essay quality.  Therefore, it 
would be desirable to have a system that could 
indicate as feedback to students, the discourse 
elements in their essays. Such a system could 
present to students a guided list of questions to 
consider about the quality of the discourse.  
For instance, it has been suggested by writing 
experts that if the thesis statement1 of a student?s 
essay could be automatically provided, the 
student could then use this information to reflect 
on the thesis statement and its quality. In 
addition, such an instructional application could 
utilize the thesis statement to discuss other types 
of discourse elements in the essay, such as the 
relationship between the thesis statement and the 
conclusion, and the connection between the 
thesis statement and the main points in the 
essay.  In the teaching of writing, in order to 
facilitate the revision process, students are often 
presented with ?Revision Checklists.? A revision 
checklist is a list of questions posed to the 
student to help the student reflect on the quality 
of his or her writing. Such a list might pose 
questions such as: 
a) Is the intention of my thesis statement 
clear? 
                                                           
1
 A thesis statement is generally defined as the 
sentence that explicitly identifies the purpose of the 
paper or previews its main ideas. See the Literacy 
Education On-line (LEO) site at 
http://leo.stcloudstate.edu. 
 (Annotator 1) ?In my opinion student should do what they want to do because they feel everything 
and they can't have anythig they feel because they probably feel to do just because other people do it not they 
want it. 
(Annotator 2) I think doing what students want is good for them. I sure they want to achieve in the 
highest place but most of the student give up. They they don?t get what they want. To get what they want, they 
have to be so strong and take the lesson from their parents Even take a risk, go to the library, and study hard by 
doing different thing. 
Some student they do not get what they want because of their family. Their family might be careless 
about their children so this kind of student who does not get support, loving from their family might not get 
what he wants. He just going to do what he feels right away. 
So student need a support from their family they has to learn from them and from their background. I 
learn from my background I will be the first generation who is going to gradguate from university that is what I 
want.? 
 
Figure 1: Sample student essay with human annotations of thesis statements. 
 
b) Does my thesis statement respond 
directly to the essay question?  
c) Are the main points in my essay 
clearly stated? 
d) Do the main points in my essay relate 
to my original thesis statement?  
If these questions are expressed in general 
terms, they are of little help; to be useful, they 
need to be grounded and need to refer 
explicitly to the essays students write 
(Scardamalia and Bereiter, 1985; White 1994). 
The ability to automatically identify and 
present to students the discourse elements in 
their essays can help them focus and reflect on 
the critical discourse structure of the essays.  
In addition, the ability for the application to 
indicate to the student that a discourse element 
could not be located, perhaps due to the ?lack 
of clarity? of this element, could also be 
helpful. Assuming that such a capability was 
reliable, this would force the writer to think 
about the clarity of an intended discourse 
element, such as a thesis statement. 
Using a relatively small corpus of essay 
data where thesis statements have been 
manually annotated, we built a Bayesian 
classifier using the following features:  
sentence position; words commonly used in 
thesis statements; and discourse features, 
based on Rhetorical Structure Theory (RST) 
parses (Mann and Thompson, 1988 and 
Marcu, 2000).  Our results indicate that this 
classification technique may be used toward 
automatic identification of thesis statements in 
essays.  Furthermore, we show that this 
method generalizes across essay topics. 
 
2 What Are Thesis Statements? 
 
A thesis statement is defined as the sentence that 
explicitly identifies the purpose of the paper or 
previews its main ideas (see footnote 1). This 
definition seems straightforward enough, and 
would lead one to believe that even for people to 
identify the thesis statement in an essay would be 
clear-cut.  However, the essay in Figure 1 is a 
common example of the kind of first-draft writing 
that our system has to handle. Figure 1 shows a 
student response to the essay question:  
Often in life we experience a conflict in 
choosing between something we "want" to do 
and something we feel we "should" do.  In your 
opinion, are there any circumstances in which 
it is better for people to do what they  "want" to 
do rather than what they feel they "should" do?  
Support your position with evidence from your 
own experience or your observations of other 
people.  
The writing in Figure 1 illustrates one kind of 
challenge in automatic identification of discourse 
elements, such as thesis statements.  In this case, 
the two human annotators independently chose 
different text as the thesis statement (the two texts 
highlighted in bold and italics in Figure 1).  In this 
kind of first-draft writing, it is not uncommon for 
writers to repeat ideas, or express more than one 
general opinion about the topic, resulting in text 
that seems to contain multiple thesis statements. 
Before building a system that automatically 
identifies thesis statements in essays, we wanted to 
determine whether the task was well-defined. In 
collaboration with two writing experts, a simple 
discourse-based annotation protocol was 
developed to manually annotate discourse 
elements in essays for a single essay topic.  
This was the initial attempt to annotate essay 
data using discourse elements generally 
associated with essay structure, such as thesis 
statement, concluding statement, and topic 
sentences of the essay?s main ideas. The 
writing experts defined the characteristics of 
the discourse labels.  These experts then 
annotated 100 essay responses to one English 
Proficiency Test (EPT) question, called Topic 
B, using a PC-based interface implemented in 
Java. 
We computed the agreement between the 
two human annotators using the kappa 
coefficient (Siegel and Castellan, 1988), a 
statistic used extensively in previous empirical 
studies of discourse.  The kappa statistic 
measures pairwise agreement among a set of 
coders who make categorial judgments, 
correcting for chance expected agreement. 
The kappa agreement between the two 
annotators with respect to the thesis statement 
labels was 0.733 (N=2391, where 2391 
represents the total number of sentences 
across all annotated essay responses).  This 
shows high agreement based on research in 
content analysis (Krippendorff, 1980) that 
suggests that values of kappa higher than 0.8 
reflect very high agreement and values higher 
than 0.6 reflect good agreement.  The 
corresponding z statistic was 27.1, which 
reflects a confidence level that is much higher 
than 0.01, for which the corresponding z value 
is 2.32 (Siegel and Castellan, 1988). 
 In the early stages of our project, it was 
suggested to us that thesis statements reflect 
the most important sentences in essays.  In 
terms of summarization, these sentences 
would represent indicative, generic summaries 
(Mani and Maybury, 1999; Marcu, 2000). To 
test this hypothesis (and estimate the adequacy 
of using summarization technology for 
identifying thesis statements), we carried out 
an additional experiment. The same 
annotation tool was used with two different 
human judges, who were asked this time to 
identify the most important sentence of each 
essay. The agreement between human judges 
on the task of identifying summary sentences 
was significantly lower: the kappa was 0.603 
(N=2391). Tables 1a and 1b summarize the results 
of the annotation experiments. 
Table 1a shows the degree of agreement 
between human judges on the task of identifying 
thesis statements and generic summary sentences. 
The agreement figures are given using the kappa 
statistic and the relative precision (P), recall (R), 
and F-values (F), which reflect the ability of one 
judge to identify the sentences labeled as thesis 
statements or summary sentences by the other 
judge. The results in Table 1a show that the task of 
thesis statement identification is much better 
defined than the task of identifying important 
summary sentences. In addition, Table 1b indicates 
that there is very little overlap between thesis and 
generic summary sentences: just 6% of the 
summary sentences were labeled by human judges 
as thesis statement sentences. This strongly 
suggests that there are critical differences between 
thesis statements and summary sentences, at least 
in first-draft essay writing. It is possible that thesis 
statements reflect an intentional facet (Grosz and 
Sidner, 1986) of language, while summary 
sentences reflect a semantic one (Martin, 1992). 
More detailed experiments need to be carried out 
though before proper conclusions can be derived.  
Table 1a: Agreement between human judges on 
thesis and summary sentence identification. 
Metric Thesis 
Statements 
Summary 
Sentences 
Kappa 0.733 0.603 
P (1 vs. 2) 0.73 0.44 
R (1 vs. 2) 0.69 0.60 
F (1 vs. 2) 0.71 0.51 
 
Table 1b: Percent overlap between human labeled 
thesis statements and summary sentences. 
 Thesis statements  vs. 
Summary sentences 
Percent Overlap 0.06 
 
The results in Table 1a provide an estimate for 
an upper bound of a thesis statement identification 
algorithm. If one can build an automatic classifier 
that identifies thesis statements at recall and 
precision levels as high as 70%, the performance 
of such a classifier will be indistinguishable from 
the performance of humans. 
 
3 A Bayesian Classifier for 
Identifying Thesis Statements 
 
3.1 Description of the Approach 
 
We initially built a Bayesian classifier for 
thesis statements using essay responses to one 
English Proficiency Test (EPT) test question: 
Topic B.  
McCallum and Nigam (1998) discuss two 
probabilistic models for text classification that 
can be used to train Bayesian independence 
classifiers. They describe the multinominal 
model as being the more traditional approach 
for statistical language modeling (especially in 
speech recognition applications), where a 
document is represented by a set of word 
occurrences, and where probability estimates 
reflect the number of word occurrences in a 
document. In using the alternative, 
multivariate Bernoulli model, a document is 
represented by both the absence and presence 
of features. On a text classification task, 
McCallum and Nigam (1998) show that the 
multivariate Bernoulli model performs well 
with small vocabularies, as opposed to the 
multinominal model which performs better 
when larger vocabularies are involved.  
Larkey (1998) uses the multivariate Bernoulli 
approach for an essay scoring task, and her 
results are consistent with the results of 
McCallum and Nigam (1998) (see also Larkey 
and Croft (1996) for descriptions of additional 
applications). In Larkey (1998), sets of essays 
used for training scoring models typically 
contain fewer than 300 documents.  
Furthermore, the vocabulary used across these 
documents tends to be restricted.   
Based on the success of Larkey?s 
experiments, and McCallum and Nigam?s 
findings that the multivariate Bernoulli model 
performs better on texts with small 
vocabularies, this approach would seem to be 
the likely choice when dealing with data sets 
of essay responses. Therefore, we have 
adopted this approach in order to build a thesis 
statement classifier that can select from an 
essay the sentence that is the most likely 
candidate to be labeled as thesis statement.2   
                                                           
2
 In our research, we trained classifiers using a 
classical Bayes approach too, where two classifiers 
were built: a thesis classifier and a non-thesis 
In our experiments, we used three general 
feature types to build the classifier: sentence 
position; words commonly occurring in thesis 
statements; and RST labels from outputs generated 
by an existing rhetorical structure parser (Marcu, 
2000).  
We trained the classifier to predict thesis 
statements in an essay. Using the multivariate 
Bernoulli formula, below, this gives us the log 
probability that a sentence (S) in an essay belongs 
to the class (T) of sentences that are thesis 
statements.  We found that it helped performance 
to use a Laplace estimator to deal with cases where 
the probability estimates were equal to zero. 
 
i i
i ii
log(P(T | S)) =
log(P(T)) +
log(P(A | T) /P(A)),
log(P(A | T) /P(A )),
i
i
if S contains A
if S does not contain A
?????
?
 
 
In this formula, P(T) is the prior probability that a 
sentence is in class T, P(Ai|T) is the conditional 
probability of a sentence having feature Ai , given 
that the sentence is in T, and P(Ai) is the prior 
probability that a sentence contains feature Ai, 
P( iA |T) is the conditional probability that a 
sentence does not have feature Ai, given that it is 
in T, and P( iA ) is the prior probability that a 
sentence does not contain feature Ai.  
 
3.2 Features Used to Classify Thesis 
Statements 
3.2.1 Positional Feature 
We found that the likelihood of a thesis statement 
occurring at the beginning of essays was quite high 
in the human annotated data. To account for this, 
we used one feature that reflected the position of 
each sentence in an essay. 
                                                                                           
classifier. In the classical Bayes implementation, each 
classifier was trained only on positive feature evidence, 
in contrast to the multivariate Bernoulli approach that 
trains classifiers both on the absence and presence of 
features. Since the performance of the classical Bayes 
classifiers was lower than the performance of the 
Bernoulli classifier, we report here only the 
performance of the latter. 
 3.2.2 Lexical Features 
All words from human annotated thesis 
statements were used to build the Bayesian 
classifier. We will refer to these words as the 
thesis word list.  From the training data, a 
vocabulary list was created that included one 
occurrence of each word used in all resolved 
human annotations of thesis statements.  All 
words in this list were used as independent 
lexical features. We found that the use of 
various lists of stop words decreased the 
performance of our classifier, so we did not 
use them. 
3.2.3 Rhetorical Structure Theory 
Features 
According to RST (Mann and Thompson, 
1988), one can associate a rhetorical structure 
tree to any text. The leaves of the tree 
correspond to elementary discourse units and 
the internal nodes correspond to contiguous 
text spans. Each node in a tree is characterized 
by a status (nucleus or satellite) and a 
rhetorical relation, which is a relation that 
holds between two non-overlapping text 
spans.  The distinction between nuclei and 
satellites comes from the empirical 
observation that the nucleus expresses what is 
more essential to the writer?s intention than the 
satellite; and that the nucleus of a rhetorical 
relation is comprehensible independent of the 
satellite, but not vice versa.  When spans are 
equally important, the relation is multinuclear. 
Rhetorical relations reflect semantic, 
intentional, and textual relations that hold 
between text spans as is illustrated in Figure 2. 
For example, one text span may elaborate on 
another text span; the information in two text 
spans may be in contrast; and the information 
in one text span may provide background for 
the information presented in another text span. 
Figure 2 displays in the style of Mann and 
Thompson (1988) the rhetorical structure tree 
of a text fragment. In Figure 2, nuclei are 
represented using straight lines; satellites 
using arcs. Internal nodes are labeled with 
rhetorical relation names.  
We built RST trees automatically for each 
essay using the cue-phrase-based discourse parser 
of Marcu (2000). We then associated with each 
sentence in an essay a feature that reflected the 
status of its parent node (nucleus or satellite), and 
another feature that reflected its rhetorical relation. 
For example, for the last sentence in Figure 2 we 
associated the status satellite and the relation 
elaboration because that sentence is the satellite 
of an elaboration relation.  For sentence 2, we 
associated the status nucleus and the relation 
elaboration because that sentence is the nucleus 
of an elaboration relation.  
We found that some rhetorical relations 
occurred more frequently in sentences annotated as 
thesis statements. Therefore, the conditional 
probabilities for such relations were higher and 
provided evidence that certain sentences were 
thesis statements.  The Contrast relation shown in 
Figure 2, for example, was a rhetorical relation 
that occurred more often in thesis statements.  
Arguably, there may be some overlap between 
words in thesis statements, and rhetorical relations 
used to build the classifier. The RST relations, 
however, capture long distance relations between 
text spans, which are not accounted by the words 
in our thesis word list.  
  
3.3 Evaluation of the Bayesian classifier 
 
We estimated the performance of our system using 
a six-fold cross validation procedure. We 
partitioned the 93 essays that were labeled by both 
human annotators with a thesis statement into six 
groups. (The judges agreed that 7 of the 100 essays 
they annotated had no thesis statement.) We 
trained six times on 5/6 of the labeled data and 
evaluated the performance on the other 1/6 of the 
data. 
The evaluation results in Table 2 show the average 
performance of our classifier with respect to the 
resolved annotation (Alg. wrt. Resolved), using 
traditional recall (R), precision (P), and F-value (F) 
metrics. For purposes of comparison, Table 2 also 
shows the performance of two baselines: the 
random baseline    classifies    the     thesis   
statements  
 Figure 2:  Example of RST tree.
randomly; while the position baseline assumes 
that the thesis statement is given by the first 
sentence in each essay. 
Table 2: Performance of the thesis statement 
classifier.  
System vs. system P R F 
Random baseline 
wrt. Resolved 
0.06 0.05 0.06 
Position baseline wrt. 
Resolved 
0.26 0.22 0.24  
Alg. wrt. Resolved 0.55 0.46 0.50  
1 wrt. 2 0.73 0.69 0.71  
1 wrt. Resolved 0.77 0.78 0.78  
2 wrt. Resolved 0.68 0.74 0.71  
 
4 Generality of the Thesis Statement 
Identifier 
In commercial settings, it is crucial that a 
classifier such as the one discussed in Section 3 
generalizes across different test questions. New 
test questions are introduced on a regular basis; 
so it is important that a classifier that works well 
for a given data set works well for other data 
sets as well, without requiring additional 
annotations and training.  
For the thesis statement classifier it was 
important to determine whether the positional, 
lexical, and RST-specific features are topic 
independent, and thus generalizable to new test 
questions.  If so, this would indicate that we 
could annotate thesis statements across a number 
of topics, and re-use the algorithm on additional 
topics, without further annotation. We asked a 
writing expert to manually annotate the thesis 
statement in approximately 45 essays for 4 
additional test questions: Topics A, C, D and E.  
The annotator completed this task using the 
same interface that was used by the two 
annotators in Experiment 1.  
To test generalizability for each of the five 
EPT questions, the thesis sentences selected by a 
writing expert were used for building the 
classifier.  Five combinations of 4 prompts were 
used to build the classifier in each case, and the 
resulting classifier was then cross-validated on 
the fifth topic, which was treated as test data.  
To evaluate the performance of each of the 
classifiers, agreement was calculated for each 
?cross-validation? sample (single topic) by 
comparing the algorithm selection to our writing 
expert?s thesis statement selections.  For 
example, we trained on Topics A, C, D, and E, 
using the thesis statements selected manually.  
This classifier was then used to select, 
automatically, thesis statements for Topic B.  In 
the evaluation, the algorithm?s selection was 
compared to the manually selected set of thesis 
statements for Topic B, and agreement was 
calculated. Table 3 illustrates that in all but one 
case, agreement exceeds both baselines from 
Table 2.  In this set of manual annotations, the 
human judge almost always selected one 
sentence as the thesis statement.  This is why 
Precision, Recall, and the F-value are often 
equal in Table 3. 
Table 3: Cross-topic generalizability of the thesis 
statement classifier. 
Training 
Topics 
CV Topic P R  F  
ABCD   E 0.36 0.36 0.36 
ABCE   D 0.49 0.49 0.49 
ABDE   C 0.45 0.45 0.45 
ACDE   B 0.60 0.59 0.59 
BCDE   A 0.25 0.24 0.25 
Mean  0.43 0.43 0.43 
 5 Discussion and Conclusions 
 
The results of our experimental work indicate 
that the task of identifying thesis statements in 
essays is well defined. The empirical evaluation 
of our algorithm indicates that with a relatively 
small corpus of manually annotated essay data, 
one can build a Bayes classifier that identifies 
thesis statements with good accuracy. The 
evaluations also provide evidence that this 
method for automated thesis selection in essays 
is generalizable.  That is, once trained on a few 
human annotated prompts, it can be applied to 
other prompts given a similar population of 
writers, in this case, writers at the college 
freshman level.  The larger implication is that 
we begin to see that there are underlying 
discourse elements in essays that can be 
identified, independent of the topic of the test 
question. For essay evaluation applications this 
is critical since new test questions are 
continuously being introduced into on-line essay 
evaluation applications.  
Our results compare favorably with results 
reported by Teufel and Moens (1999) who also 
use Bayes classification techniques to identify 
rhetorical arguments such as aim and 
background in scientific texts, although the texts 
we are working with are extremely noisy. 
Because EPT essays are often produced for 
high-stake exams, under severe time constraints, 
they are often ungrammatical, repetitive, and 
poorly organized at the discourse level. 
Current investigations indicate that this 
technique can be used to reliably identify other 
essay-specific discourse elements, such as, 
concluding statements, main points of 
arguments, and supporting ideas.  In addition, 
we are exploring how we can use estimated 
probabilities as confidence measures of the 
decisions made by the system. If the confidence 
level associated with the identification of a 
thesis statement is low, the system would 
instruct the student that no explicit thesis 
statement has been found in the essay. 
 
Acknowledgements 
 
We would like to thank our annotation 
experts, Marisa Farnum, Hilary Persky, Todd 
Farley, and Andrea King. 
 
References 
 
Burstein, J., Kukich, K. Wolff, S. Lu, C. 
Chodorow, M, Braden-Harder, L. and Harris 
M.D. (1998). Automated Scoring Using A 
Hybrid Feature Identification Technique. 
Proceedings of ACL, 206-210. 
Foltz, P. W., Kintsch, W., and Landauer, T.. 
(1998). The Measurement of Textual Coherence 
with Latent Semantic Analysis. Discourse 
Processes, 25(2&3), 285-307. 
Grosz B. and Sidner, C. (1986). Attention, 
Intention, and the Structure of Discourse. 
Computational Linguistics, 12 (3), 175-204. 
Krippendorff K. (1980). Content Analysis: 
An Introduction to Its Methodology. Sage Publ. 
Larkey, L. and Croft, W. B. (1996).  
Combining Classifiers in Text Categorization. 
Proceedings of  SIGIR,  289-298. 
Larkey, L. (1998). Automatic Essay Grading 
Using Text Categorization Techniques.  
Proceedings of SIGIR, pages 90-95. 
Mani, I. and Maybury, M. (1999). Advances 
in Automatic Text Summarization. The MIT 
Press. 
Mann, W.C. and Thompson, S.A.(1988). 
Rhetorical Structure Theory: Toward a 
Functional Theory of Text Organization. Text 
8(3), 243?281. 
Martin, J. (1992). English Text. System and 
Structure. John Benjamin Publishers.  
 Marcu, D. (2000). The Theory and Practice 
of Discourse Parsing and Summarization. The 
MIT Press.  
McCallum, A. and Nigam, K. (1998). A 
Comparison of Event Models for Naive Bayes 
Text Classification. The AAAI-98 Workshop on 
"Learning for Text Categorization".  
Page, E.B. and Peterson, N. (1995). The 
computer moves into essay grading: updating 
the ancient test. Phi Delta Kappa, March, 561-
565. 
Scardamalia, M. and Bereiter, C. (1985). 
Development of Dialectical Processes in 
Composition.  In Olson, D. R., Torrance, N. and 
Hildyard, A. (eds), Literacy, Language, and 
Learning: The nature of consequences of 
reading and writing.  Cambridge University 
Press. 
Siegel S. and Castellan, N.J. (1988). 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill. 
Teufel , S. and Moens, M. (1999). Discourse-
level argumentation in scientific articles. 
Proceedings of the ACL99 Workshop on 
Standards and Tools for Discourse Tagging. 
White E.M. (1994). Teaching and Assessing 
Writing. Jossey-Bass Publishers, 103-108. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 950?961, Dublin, Ireland, August 23-29 2014.
Lexical Chaining for Measuring Discourse Coherence Quality in
Test-taker Essays
Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
ssomasundaran@ets.org
Jill Burstein
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
jburstein@ets.org
Martin Chodorow
Hunter College, CUNY
695 Park Avenue
New York, NY 10065
martin.chodorow@hunter.cuny.edu
Abstract
This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring
discourse coherence quality in test-taker essays. We hypothesize that attributes of lexical chains,
as well as interactions between lexical chains and explicit discourse elements, can be harnessed
for representing coherence. Our experiments reveal that performance achieved by our new lexical
chain features is better than that of previous discourse features used for this task, and that the best
system performance is achieved when combining lexical chaining features with complementary
discourse features, such as those provided by a discourse parser based on rhetorical structure
theory, and features that reflect errors in grammar, word usage, and mechanics.
1 Introduction
Coherence, the reader?s ability to construct meaning from a document, is greatly influenced by the pres-
ence and organization of cohesive elements in the text (Halliday and Hasan, 1976; Moe, 1979). The
lexical chain (Morris and Hirst, 1991) is one such element. It consists of a sequence of related words that
contribute to the continuity of meaning based on word repetition, synonymy and similarity. In this paper
we explore how lexical chains can be employed to measure coherence in essays. Specifically, our goal
is to investigate how attributes of lexical chains can encode discourse coherence quality, such as adher-
ence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and
ideas. To do this, we build lexical chains and extract linguistically-motivated features from them. The
number of chains and their properties, such as length, density and link strength, can potentially reveal
discourse qualities related to focus and elaboration. In addition, features that capture the interactions
between chains and explicit discourse cues, such as transition words, can show if the cohesive elements
in text have been organized in a coherent fashion.
The main contributions of this paper are as follows: We use lexical chaining features to train a dis-
course coherence classifier on annotated essays from six different essay-writing tasks which differ in
essay genre and/or test-taker population. We then perform experiments to measure the effect of the fea-
tures when they are used alone and when they are combined with state-of-the-art features to classify the
coherence quality of essays. Our results indicate that lexical chaining features yield better results than
discourse features previously explored for this task and that the best performing feature combinations
contain lexical chaining features. We also show that lexical chaining features can improve system per-
formance across multiple genres of writing and populations. Our efforts result in the creation of a higher
performing state-of-the-art feature set for measuring coherence in test-taker writing.
The rest of the paper is organized as follows: In Section 2, we describe our intuitions about lexical
chains and how they can be used for measuring discourse coherence quality in essays. Section 3 describes
our data, and Section 4 describes our experiments in predicting discourse coherence quality. We discuss
related work in Section 5 and conclude in Section 6.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
950
2 Lexical Chains and Discourse Coherence Quality
According to Morris and Hirst (1991), lexical cohesion is the result of chains of related words that con-
tribute to the continuity of lexical meaning. These sequences are characterized by the relations between
the words, as well as by their distance and density within a given span. Lexical chains do not stop at
sentence boundaries ? they can connect a pair of adjacent words or range over an entire text. Morris
and Hirst also observe that lexical chains tend to delineate portions of text that have a strong unity of
meaning. In this paper, we use this underlying principle of cohesion to detect the quality of coherence
in a discourse. Specifically, we employ lexical chains to quantify and represent expectations for coher-
ent discourse in test-taker essays. Presumably, violations of these expectations would indicate lack of
(or poor) coherence. We believe lexical chains have the potential to reveal the following characteristics
about discourse coherence in essays:
Text unity: Textual continuity is vital for the reader?s ability to construct meaning from the text (Halli-
day and Hasan, 1976). Coherent essays generally maintain focus over the main theme, so lexical chains
constructed over such essays will have chains representing the central topic running through most of the
length of the essay. These types of chains would presumably represent the main claim or position in
persuasive texts, the main object or person in descriptive texts, and the main story-line in narrative texts.
On the other hand, incoherent texts that jump from one topic to another, or do not adhere to a central
idea, will exhibit no chains or chains with very few member words.
Elaboration and Detailing: A function of elaboration in discourse is to overcome misunderstanding or
lack of understanding, and to enrich the understanding of the reader by expressing the same thought from
a different perspective (Hobbs, 1979). Good writers usually initiate topics, ideas or claims and provide
clear elaborations or reasons. That is, a sequence of many related words and phrases will be evoked to
explain an idea or provide an account of the writer?s reasoning. This development and detailing will be
exhibited by lexical chains with a good number of member words.
Variety: While cohesiveness is vital for coherence, too much repetition of the same word can, in fact,
harm the discourse quality (Witte and Faigley, 1981). Using a variety of words to express an idea or
elaborate on a topic is generally a characteristic of skillful writing. Lexical chains corresponding to such
writing will have a variety of similar words within the same chain.
Organization: In addition to cohesion (as represented by lexical chains in our case), one other factor
must be present for text to have coherence ? organization (Moe, 1979; Perfetti and Lesgold, 1977). Thus,
it is important to organize ideas using clear discourse transitions. Transitions from one topic to another,
or from a topic to its subtopic, should be clearly cued in order to assist the reader?s understanding of
the discourse. Consequently, in coherent writing, we would expect lexical chain patterns to synchronize
with discourse cues. For example, we would expect some chains to start after a new (sub) topic initiation
cue, such as ?Secondly? or ?Finally?, and at least some chains (corresponding to the previous topic) to
end immediately before the cue. Similarly, we would expect at least some chains to cross over discourse
cues indicating elaboration or reason (e.g. ?because?) due to topic continuity.
2.1 Features for Measuring Discourse Coherence
In order to measure discourse coherence quality, we create features based on attributes of lexical chains
extracted from essays. These features are then used to train a machine learning model, using essays
manually labeled for overall discourse coherence quality.
2.1.1 Lexical Chain Construction
Lexical chains in a text are composed of words and terms that are related. Based on Hirst and St-
Onge (1995), these relations can be exact repetitions, called extra-strong relations, close synonyms,
called strong relations, or words with weaker semantic relations, called medium-strong relations. We
implement the lexical chaining program described in Hirst and St. Onge (1995), where if a word or
phrase is potentially chainable, it is considered a candidate node for existing chains. First, an extra-
strong relation is sought throughout all existing chains, and if one is found, the word is added to it. If
not, strong relations are sought, but for these, the search scope is limited to the words in any chain that is
951
no farther away than the previous six sentences in the text; the search ends as soon as a strong relation is
found. Finally, if no relationship has yet been found, medium-strong relations are sought with the search
scope limited to words in chains that are no farther away than the previous three sentences. If the node
cannot be added to any existing chains, it forms its own single-node chain.
In this work, nouns are the focus of the lexical chains. Nouns, adjective-noun and noun-noun structures
are identified as potential chain participants. Lin?s thesaurus (Lin, 1998) is used to measure similarity
between words and phrases. Candidate pairs receiving similarity scores greater than 0.8 are considered
to have an extra-strong relationship (word repetition receives a similarity score of 1), pairs with similarity
greater than 0.172 are considered to have a strong relation, and pairs with similarity scores greater than
0.099 are considered to have a medium-strong relation. These thresholds were chosen after qualitative
inspection of a separate development data set of essays, and are also based on a previous finding (Burstein
et al., 2012) that 0.172 is the mean similarity value across different parts of speech in the Lin thesaurus.
We created two feature sets to capture the intuitions described above. The first set, LEX-1, encodes
the characteristics of text unity, elaboration and variety, while the second, LEX-2, encodes organization.
2.1.2 LEX-1 feature set
In order to capture text unity and detailing, we create features such as: total number of chains in the
essay, average chain size, number (and percentage) of large chains (chains having more than four nodes
are considered to be large chains
1
). As discussed previously, essays that show ample chaining might do
so because they adhere to themes and their development, while the presence of large, dense chains might
be an indicator that a topic is being discussed in detail. To represent variety, we employ features such
as number (and percentage) of chains that have a variety of words (chains containing more than one
word/phrase type are considered to have variety), as well as number (and percentage) of large chains
with a variety of words. To encode the characteristics of cohesive relationships, we look at the nature
of the links. Examples of these features are: number and percentage of each link type, number (and
percentage) of links of each type in large chains as well as in small chains. Corresponding to each
feature that uses counts (e.g. total number of chains) we also created normalized versions of the numbers
to account for the essay length. LEX-1 has a total of 38 features.
2.1.3 LEX-2 feature set
LEX-2 features capture the interactions between discourse transitions, indicated by explicit discourse
cues, and lexical chaining patterns. For this, we use a discourse cue tagger described in Burstein et al.
(1998) that was specifically developed for tagging discourse cues in the essay genre. Using patterns and
syntactic rules, the tagger automatically identifies words and phrases used as discourse cues, and assigns
them a discourse tag. Each tag has a primary component, indicating whether an argument (or topic) is
being initialized (arg-init) or developed (arg-dev), and a secondary component indicating the specific
type of discourse initialization (e.g. CLAIM, SUMMARY), or development (e.g. CLAIM, CONTRAST).
Examples of the discourse tags and their cues are: arg-init:SUMMARY (e.g. all in all, in conclusion,
in summary, overall), arg-init:TRANSITION (e.g. let us), arg-init:PARALLEL (e.g. firstly, similarly,
finally), arg-dev:CONTRAST (e.g. nonetheless, however, on the contrary, rather than, even if ), arg-
dev:EVIDENCE (e.g. because of, since), arg-dev:INFERENCE (e.g. as a result, consequently, there-
fore), arg-dev:DETAIL (e.g. as well as, in this case, in addition, such as), arg-dev:REFORMULATION
(e.g. in other words, that is).
For each discourse cue tagged in the text, we replace the cue with its tag and measure the number of
chains that (1) start after it, (2) end before it, and (3) continue over it (chains having nodes before and
after the tag). We create such features for the tags in the original form (e.g. arg-dev:DETAIL), as well
as for the primary component alone (e.g. arg-dev) and the secondary component alone (e.g. DETAIL).
This alleviates the data sparseness that we see with certain tags, and results in a total of 138 tags for the
LEX-2 feature set.
1
This number was chosen after inspecting chains in a separate development data set.
952
3 Data
We use essays from different essay-writing tasks, representing different genres, writing proficiency and
populations. Specifically, our essays consist of the following six subsets:
1. PE-G-N: Persuasive/Expository essays written by graduate school applicants who are a mix of
native and non-native speakers. (e.g. ?As people rely more and more on technology to solve prob-
lems, the ability of humans to think for themselves will surely deteriorate. Discuss the extent to
which you agree or disagree ... ? ) [n= 145 essays]
2. AC-G-N: Argumentation critique essays written by graduate school applicants who are a mix of
native and non-native speakers. (?Examine the stated and/or unstated assumptions of the argument.
Be sure to explain how the argument depends on the assumptions and what the implications are if
the assumptions prove unwarranted ...?) [n= 138 essays]
3. PE-UG-NN: Persuasive/Expository essays written by undergraduate and graduate school appli-
cants, who are non-native speakers. [n= 146 essays]
4. CS-UG-NN: Contrastive summary essays written by undergraduate and graduate school applicants
who are non-native speakers. Here, the prompt focuses on a specific type of summarization, where
ideas from an audio lecture are to be contrasted with ideas from a written passage. [n= 147 essays]
5. S-G-N: Subject matter essays written by graduates in a professional licensure exam who are a mix
of native and non-native speakers. [n= 150 essays]
6. M-K12-N: A Mix of expository, persuasive, descriptive and narrative essays written by K-12 school
students who are a mix of native and non-native speakers. [n= 150 essays]
Of the total of 876 essays, 40 essays were used for system development, and the rest were used for
cross-validation experiments. Each essay in the data set was manually annotated for overall discourse
coherence quality by annotators not involved in this research. The discourse coherence score was as-
signed using a 4-point scale (with score point 4 for excellent discourse coherence). Twenty percent of
the essays were double annotated and the rest were annotated by one of the annotators. Inter-annotator
agreement over the doubly annotated essays, calculated using quadratic weighted kappa (QWK), was
0.61 (substantial agreement). The data distribution for each score point was: 1% for score 1, 9% for
score 2, 27% for score 3, 63% for score 4.
4 Experiments
4.1 Baseline Features
A review by Burstein et al. (2013a) describes the several systems that measure discourse coherence
quality across various text genres including test-taker essays. Features used to evaluate the discourse
coherence quality systems in this study include those previously discussed in Burstein et al. (described
below). In addition to comparing our features with previously explored features, our goal is to see if the
state-of-the-art feature set can be extended with the use of lexical chaining features.
Entity-grid transition probabilities (entity). Entity-grid transition probabilities (Barzilay and Lap-
ata, 2008) are intended to address unity, progression and coherence by tracking nouns and pronouns in
text. An entity grid is constructed in which all entities (nouns and pronouns) are represented by their
syntactic roles in a sentence (i.e., Subject, Object, Other). Entity grid transitions track how the same
word appears in a syntactic role across adjacent sentences.
Type/Token Ratios for Entities (type/token). These are modified entity-grid transition probabilities.
While the entity grid only captures, for example ?Subject-Subject? transitions, type/token ratios capture
the proportion of unique words that make such transitions. Higher ratios indicate that more concepts are
being introduced in a given syntactic role, and lower ratios indicate fewer concepts.
953
RST-derived features (RST). Rhetorical relations (Mann and Thompson, 1988) derived from an RST
parser (Marcu, 2000) are used to evaluate if and how certain rhetorical relations, combinations of rhetor-
ical relations, or rhetorical relation tree structures contribute to discourse coherence quality. These in-
clude: (a) relative frequencies of n-gram rhetorical relations in the context of the RST parse tree structure
(unigrams, or occurrences of a single relation (e.g., ThemeShift); bigrams, (e.g., ?ThemeShift -> Elab-
oration?); and trigrams, (e.g., ?ThemeShift -> Elaboration -> Circumstance?)); (b) relative proportions
of leaf-parent relation types in the tree structure; and (c) counts of root node relation types in the trees.
Maximum LSA Value for Distant Sentence Pairs (maxLSA). This feature set is the maximum
Latent Semantic Analysis (LSA) similarity score found between pairs of sentences that are separated
by at least 5 intervening sentences in the essay. It captures reintroduction of topics later in an essay,
consistent with a backward inference strategy (Van den Broek et al., 1993; Van den Broek, 2012). LSA
has also been employed to measure semantic relatedness between texts for discourse coherence (Foltz et
al., 1998).
Grammatical error features (gramErr). These features address errors in grammar that could inter-
fere with a reader?s ability to construct meaning and have been used in previous studies (e.g. (Attali and
Burstein, 2006; Burstein et al., 2013b)). Specifically, they are based on more than 30 kinds of errors in
grammar, such as subject-verb agreement errors, in word usage, such as missing article errors, and in
spelling. We use e-rater
r
, an essay scoring engine developed by Educational Testing Service (ETS), to
detect the grammar errors. Aggregate counts of these errors are used as features for predicting discourse
coherence.
Program Features (program). This is a single feature for identifying the data type listed in Section
3. Genre and population play an important role with respect to discourse coherence ? essays written
by more advanced writers, such as those at the graduate level, are typically more coherent than essays
written by populations where English is a second language, or by K-12 school students. Note that the
program feature is not linguistically motivated ? it does not capture the writing construct or a writing
skill. However, it is a strong feature as it can reliably bias the system to change its expectations about the
discourse quality based on population and task.
4.2 Principal Components Analysis
To reduce the number of lexical chain features, a Principal Components Analysis (PCA) was calculated
on an independent set of 6000 essays randomly sampled from the six task types. For 38 LEX-1 features,
a 4-component solution accounted for about 0.70 of the feature variance. An 8-component solution
explained about 0.30 of the feature variance for the 138 LEX-2 features. (While the variance was lower
for this PCA solution, the components were fairly clean.) The component scores were then computed
for the 876 essays in our annotated data set. The 4-component scores were used as LEX-1 features, and
the 8-component scores were used as LEX-2 features. PCA was used for lexical chaining features in
order to reduce the number of features used to build the models rather than using a much larger number
of correlated features. PCA was not applied to features from previous work, as we wanted to reproduce
their performance.
4.3 Results
A 10-fold cross-validation was run with an unscaled, gradient boosting regressor
2
tuned using quadratic
weighted kappa
3
. Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn
toolkit
4
(Pedregosa et al., 2011). The learner was trained to assign 4-point coherence quality scores
using different combinations of the feature sets described in sections 2.1 and 4.1. In addition to each of
the individual features in Section 4.1, we tested two baseline feature combinations: Baseline-1, a system
using all discourse-based features from Section 4.1, and Baseline-2, a system using all features described
in Section 4.1.
2
We experimented with SVMs and Random Forest learners too, but the best results were obtained with the regressor.
3
The software for the regressor can be found at https://github.com/EducationalTestingService/skll/
4
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble
954
Performance was calculated using Quadratic Weighted Kappa (QWK) (Cohen, 1968), which measures
the agreement between the system score and the human-annotated coherence score. QWK corrects for
chance agreement, and it penalizes large disagreements more than small disagreements. The formula for
QWK is as follows:
? = 1?
k
?
i=1
k
?
j=1
w
ij
o
ij
k
?
i=1
k
?
j=1
w
ij
e
ij
where k is the total number of categories (4 in our case), o
ij
is the observed value in cell i, j of the
confusion matrix between system predictions and human scores, e
ij
is the expected value for cell i, j,
and w
ij
is the weight given to the discrepancy between category
i
and category
j
. The expected value
e
ij
is calculated as:
e
ij
=
k
?
j=1
o
ij
k
?
i=1
o
ij
k
?
i=1
k
?
j=1
o
ij
For quadratic weighted kappa, w
ij
is calculated as:
w
ij
= 1?
(i? j)
2
(k ? 1)
2
where i and j are categories, and k is the total number of categories. We use QWK as it is the standard
evaluation metric used in automated essay scoring, and it also helps us to compare our results with
previous work.
Table 1 reports the results for our proposed features and for each individual feature set investigated in
previous work. Here, feature sets explicitly targeting discourse phenomena are grouped under Discourse-
based Features. The features grouped under Non-Discourse Features also capture coherence quality;
however they are based on grammatical errors or data type information. The best performing system in
each group is shown in bold. We see that the full set of lexical chaining features (LEX-1 + LEX-2) is the
best performing discourse-based feature set. It performs better than each of the other discourse-based
features used alone, and also better than Baseline-1, which uses a combination of all discourse-based
features from previous work. Notice that the performance of each discourse-based system is below the
performance of both gramErr and program, indicating that they can play an important role in predicting
text coherence.
While grammar (gramErr) and data type (program) are powerful features, it is also important to incor-
porate capabilities for detecting and evaluating discourse-specific phenomena to ensure construct rele-
vance, as the grading guidelines for essays specify the need for proper organization of ideas (e.g.?sustains
a well-focused, well-organized analysis, connecting ideas logically?). Lack of construct relevance has
been a major criticism of automated scoring methods (Deane, 2013; Shermis and Burstein, 2013). Ad-
ditionally, discourse-relevant features will allow for interpretable, useful, explicit feedback to students
regarding discourse coherence and its breakdown.
In Table 1 we also see that no individual discourse-based system outperforms Baseline-2, compris-
ing all features from the state-of-the-art (Section 4.1). In fact, the human-system agreement obtained
by Baseline-2 surpasses the human-human agreement (QWK of 0.61) reported in Section 3. This phe-
nomenon is not uncommon in essay scoring. For example, Bridgeman et al. (2012) performed detailed
analyses and found that across all test populations, human-automated system score correlations surpassed
human-human score correlations.
Because the gramErr and program features contain information that is complementary to discourse-
based features, we combined the discourse features, first with gramErr features, and then with
gramErr+program features. Table 2 reports the results from these experiments. The best performing sys-
tem for each column is in bold, and all features with QWK higher than Baseline-2 are in italics. Here,
955
Feature set QWK
Discourse-based features
LEX-1+ LEX-2 0.316
LEX-1 0.176
LEX-2 0.246
entity 0.249
type/token 0.178
RST 0.295
maxLSA 0.171
Baseline-1 0.302
Non-Discourse Features
gramErr 0.592
program 0.387
Baseline-2 0.631
Table 1: Performance of individual feature sets.
Feature set +gramErr +gramErr
+program
LEX-1+ LEX-2 0.608 0.646
LEX-1 0.611 0.650
LEX-2 0.577 0.654
entity 0.621 0.609
type/token 0.600 0.623
RST 0.612 0.649
maxLSA 0.592 0.650
gramErr+program 0.644
Table 2: Performance (QWK), of individ-
ual discourse-based features when gramErr is
added (column 2) and gramErr and program
are added (column 3)
we see that, when combined with gramErr+program, the full set of lexical chaining features (LEX-
1+LEX-2), as well as LEX-1 and LEX-2 individually, perform above Baseline-2. Surprisingly, we find
that when some individual discourse features from previous work are combined with gramErr+program,
they achieve better performance than Baseline-2 indicating that using the full combination of discourse
features may not result in the best system. In the last row, we see that the combination of gramErr
and program features alone (gramErr+program) is more competitive that Baseline-2, underscoring their
usefulness for detecting coherence quality.
Finally, we performed full ablation studies to see which feature set combination produces the best
system for identifying discourse coherence quality. Different combinations of the 8 feature sets resulted
in 255 different systems, which we ranked based on their performance. Table 3 lists some of the systems,
with their respective ranks and QWK values.
First, we observe that the best performing system contains the full set of lexical chaining features
and achieves a QWK of 0.673. In fact, all of the top-5 performing systems contain either LEX-1 or
LEX-2. The best performance produced by a system not containing any lexical chaining features ranks
eighth (gramErr+ maxLSA+ program+ RST). Notice that gramErr+program is at rank 31, Baseline-2
is at rank 61, and Baseline-1 is at rank 235. Interestingly, RST features are also seen in all of the top-5
systems, indicating that RST features and lexical chaining features capture complementary information
about discourse quality. Surprisingly, maxLSA features, which have the same underlying principle of
cohesion in text as lexical chains, are in some of the top-performing feature combinations (at ranks 4
and 5), indicating that, in addition to how ideas and themes are presented throughout the essay, the
re-introduction of topics is also important.
We tested the statistical significance of the performance differences between our best system (gramErr
+ LEX-2+ LEX-1+ maxLSA+ program+ RST, at rank 1 in Table 3) and three other systems (Baseline-1,
Baseline-2 and gramErr+program) by drawing 10,000 bootstrap samples (Berg-Kirkpatrick et al., 2012)
from our manually scored essays. For each sample, QWKs were calcuated between the human scores and
the predictions of our best system, and between the human scores and each of the other three systems?
predictions. For each sample, the differences in QWKs were recorded, and the distributions of differences
were used for significance testing. Results show that our best performing system is significantly better
than Baseline-1 (p < 0.001) and Baseline-2 (p < 0.01), and it marginally outperformed the system with
gramErr+program features (p < 0.06).
These results show that lexical chaining information is a reliable indicator of discourse quality, and
that it can be combined synergistically with other complementary features to extend the state-of-the-art
for measuring discourse coherence quality.
956
Feature set QWK Rank
gramErr + LEX-2+ LEX-1+ maxLSA+ program+ RST 0.673 1
gramErr+ LEX-1+ program+ RST 0.661 2
gramErr+ LEX-2+ program+ RST 0.661 3
gramErr+ LEX-2+ maxLSA+ program+ RST 0.660 4
gramErr+ LEX-1+ maxLSA+ program+ RST 0.659 5
gramErr+ maxLSA+ program+ RST 0.656 8
gramErr+ program 0.644 31
Baseline-2: entity+ gramErr+ RST+ maxLSA+ program+ type/token 0.631 61
Baseline-1: entity+ RST+ maxLSA+ type/token 0.302 235
Table 3: Performance (QWK), and ranks of systems using different feature combinations
4.4 Analysis by Data Type
In the previous section we saw that features based on lexical chaining are able to successfully encode and
predict the quality of discourse coherence. We now examine if this impact is uniform across all essay
genres and populations of writers. Table 4 shows the performance of gramErr +program (in column
2), the best performing features and their respective performance (Best system, columns 3 and 4), and
the best feature set when lexical chaining features are removed, with their respective performance (Best
Minus LEX-1 and LEX-2, columns 5 and 6). Here we use gramErr +program as an additional baseline,
as it was found to be more competitive than both Baseline-1 and Baseline-2.
Program gramErr Best system Best Minus LEX-1 and LEX-2
+prog Features QWK Features QWK
CS-UG-NN 0.418 gramErr+ maxLSA+ RST 0.523 gramErr+ maxLSA+ RST 0.523
PE-UG-NN 0.406 gramErr + LEX-2 + maxLSA + RST 0.468 gramErr 0.406
PE-G-N 0.614 gramErr + LEX-1 + maxLSA 0.676 gramErr + maxLSA 0.650
AC-G-N 0.744 gramErr + LEX-2 + maxLSA 0.839 gramErr + maxLSA + type/token 0.766
S-G-N 0.414 entity + gramErr+ LEX-1+ RST+
type/token
0.532 gramErr+ RST+ type/token 0.487
M-K12-N 0.635 gramErr + LEX-2 + maxLSA 0.656 gramErr + maxLSA + RST +
type/token
0.649
Table 4: Performance of feature sets by data type. Best performance is shown in bold.
In general, for all data types, addition of discourse features produces improvement over just using a
combination of gramErr and program features. Also, the addition of lexical chaining features produces
performance improvement for most data types. Specifically, there is substantial improvement in perfor-
mance for persuasive writing (PE-UG-NN and PE-G-N), expository subject writing (S-G-N) and writing
involving critical argumentation (AC-G-N). M-K12-N, which is composed of a mix of genres and writing
proficiency, shows a minor improvement. Interestingly, for contrastive summarization (CS-UG-NN), the
best system for predicting discourse coherence does not employ any lexical chaining features. For this
type of writing, the best feature set using lexical chaining features achieved a QWK of 0.465, which im-
proves over gramErr+program but is lower than the best performing feature set. This is perhaps because
the discourse phenomena targeted by our lexical chaining features (topical detailing, variety and organi-
zation) are already provided for the writer in the source document and the audio lecture, i.e., the materials
that are to be referred to in writing this type of essay. Thus, other features play a more prominent role,
such as the RST features that capture local discourse organization which is needed, for example, when
drawing a contrast between two sources of conflicting information.
957
5 Related Work
5.1 Discourse coherence quality
A number of models for measuring the quality of discourse coherence have been based on Centering
Theory (Grosz et al., 1995). For example, Barzilay and Lapata (2008) construct entity grids based
on syntactic subjects and objects. Their algorithm keeps track of the distribution of entity transitions
between adjacent sentences and computes a value for all transition types based on their proportion of
occurrence in a text. The algorithm has been evaluated with three tasks using well-formed newspaper
corpora: text ordering, summary coherence evaluation, and readability assessment. Along similar lines,
Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Milt-
sakaki and Kukich (2000) use manually marked centering information and find that higher numbers of
Rough Shifts within paragraphs are indicative of a lack of coherence. Using well-formed texts, Pitler
and Nenkova (2008) show that a text coherence detection system yields the best performance when it
includes features using the Barzilay and Lapata (2008) entity grids, syntactic features, discourse rela-
tions from the Penn Discourse Treebank (Prasad et al., 2008), and vocabulary and length features. Wang,
Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki
and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach
and the approaches based on Centering Theory is that we do not use syntactically prominent items or try
to establish a center. Instead, multiple concurrent thematic chains can ?flow? through the paragraph, and
their length, density, and interaction with discourse markers are used to model coherence.
In other related work, Lin et al. (2011) use discourse relations from Discourse Lexicalized Tree
Adjoining Grammar (D-LTAG) and compile sub-sequences of discourse role transitions to see how the
discourse role of a term varies through the progression of the text. Our work, in contrast, traces how
chains or thematic threads are organized with respect to the discourse. Our approach also differs from
models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains
can run though the length of the entire text, and hence the features derived from them are able to capture
aggregate thematic properties of the entire text such as number, distribution and elaboration of topics.
Discourse coherence models have been previously employed for the task of information-ordering in
well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)).
In our tasks, discourse coherence quality is influenced by many factors including, but not limited to,
ordering of information, such as text unity, detailing and organization.
Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality
in essays. Their approach, however, was reliant on organizational structures particular to expository and
persuasive essays, such as thesis statement and conclusion.
5.2 Lexical Chaining and Cohesion
Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003),
question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), de-
tection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000),
topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007).
In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain
features are employed to indicate the number of entities/concepts that a reader must keep in mind while
reading a document, and two of their features (number of chains in the document and average length of
chains) overlap with our LEX-1 features. Our work also differs from systems using cohesion to measure
writing quality (e.g., (Witte and Faigley, 1981; Flor et al., 2013)), in that we focus on predicting the
quality of discourse coherence.
6 Conclusion
In this paper, we investigated the use of lexical chaining for measuring discourse coherence quality.
Based on intuitions about what makes a text coherent, we extracted two sets of features from lexical
chains, one encoding how topical themes and cohesive elements are addressed in the text, and another
958
encoding how the topical themes interact with explicit discourse organizational cues. We performed
detailed experiments which showed that lexical chaining features are useful for predicting discourse
coherence quality. Specifically, when compared to other previously explored discourse-based features,
we found that our lexical chaining features are best performers when used alone. We then experimented
with various feature combinations and showed that top performing systems contain lexical chaining
features. Our analyses also indicated that lexical chaining features can improve performance on various
genres of writing by different populations of writers. Our future work on measuring discourse coherence
quality involves extending chains by using verb information and by exploring finer distinctions within
the chains themselves (e.g., topical and sub-topical chains).
References
Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v. 2.0. Journal of Technology, Learning,
and Assessment, 4:3.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
ACL workshop on intelligent scalable text summarization, volume 17, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical signif-
icance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 995?1005. Association for Computational
Linguistics.
Brent Bridgeman, Catherine Trapani, and Yigal Attali. 2012. Comparison of human and machine scoring of
essays: Differences by gender, ethnicity, and country. Applied Measurement in Education, 25(1):27?40.
Jill Burstein, Karen Kukich, Susanne Wolff, Ji Lu, and Martin Chodorow. 1998. Enriching automated essay
scoring using discourse marking. In Workshop on Discourse Relations and Discourse Marking. ERIC Clear-
inghouse.
Jill Burstein, Jane Shore, John Sabatini, Brad Moulder, Steven Holtzman, and Ted Pedersen. 2012. The language
musesm system: Linguistically focused instructional authoring. Technical report, Educational Testing Services
(ETS).
Jill Burstein, Joel Tetreault, and Martin Chodorow. 2013a. Holistic discourse coherence annotation for noisy essay
writing. Dialogue and Discourse, 4(2):34?52.
Jill Burstein, Joel Tetreault, and Nitin Madnani, 2013b. Handbook of Automated Essay Evaluation: Current
Applications and New Directions, chapter The E-rater Automated Essay Scoring System. Routledge.
Joseph Carthy and Michael Sherwood-Smith. 2002. Lexical chains for topic tracking. In 2002 IEEE International
Conference on Systems, Man and Cybernetics, volume 7. IEEE.
Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.
Psychological Bulletin, 70(4):213.
Paul Deane. 2013. On the relation between automated essay scoring and modern views of the writing construct.
Assessing Writing, 18(1):7 ? 24. Automated Assessment of Writing.
Micha Elsner and Eugene Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of the 46th
Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short
Papers, pages 41?44. Association for Computational Linguistics.
Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical chains for keyword extraction. Information Processing &
Management, 43(6):1705?1714.
Lijun Feng, No?emie Elhadad, and Matt Huenerfauth. 2009. Cognitively motivated features for readability assess-
ment. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, pages 229?237. Association for Computational Linguistics.
959
Michael Flor, Beata Beigman Klebanov, and Kathleen M. Sheehan. 2013. Lexical tightness and text complexity.
In Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility, pages
29?38, Atlanta, Georgia, June. Association for Computational Linguistics.
Peter W Foltz, Walter Kintsch, and Thomas K Landauer. 1998. The measurement of textual coherence with latent
semantic analysis. Discourse processes, 25(2-3):285?307.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995. Centering: A framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Michael AK Halliday and Ruqaiya Hasan. 1976. Cohesion in english. English Language Series. Longman Group
Ltd.
Paula Hatch, Nicola Stokes, and Joe Carthy. 2000. Topic detection, a new application for lexical chaining. In the
proceedings of BCS-IRSG, pages 94?103.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. 2004. Evaluating multiple aspects of coherence
in student essays. In HLT-NAACL, pages 185?192.
Graeme Hirst and David St-Onge. 1995. Lexical chains as representations of context for the detection and correc-
tion of malapropisms. WordNet: An electronic lexical database, 305:305?332.
Jerry R Hobbs. 1979. Coherence and coreference. Cognitive Science, 3(1):67?90.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 997?1006. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press.
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In Proceedings
of LREC 2000.
Alden J Moe. 1979. Cohesion, coherence, and the comprehension of text. Journal of Reading, 23(1):16?20.
Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of the 19th
international conference on Computational linguistics-Volume 1, pages 1?7. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the
structure of text. Computational linguistics, 17(1):21?48.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Charles A Perfetti and Alan M Lesgold. 1977. Discourse Comprehension and Sources of Individual Differences.
ERIC.
Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.
Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber.
2008. The Penn Discourse TreeBank 2.0. In LREC.
Vasile Rus and Nobal Niraula. 2012. Automated detection of local coherence in short argumentative essays based
on centering theory. In Computational Linguistics and Intelligent Text Processing, pages 450?461. Springer.
Mark D Shermis and Jill Burstein. 2013. Handbook of automated essay evaluation: Current applications and new
directions. Routledge.
960
Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceed-
ings of the COLING/ACL on Main conference poster sessions, pages 803?810. Association for Computational
Linguistics.
Nicola Stokes. 2003. Spoken and written news story segmentation using lexical chains. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: Proceedings of the HLT-NAACL 2003 student research workshop-Volume 3, pages 49?
54. Association for Computational Linguistics.
Paul Van den Broek, Charles R Fletcher, and Kirsten Risden. 1993. Investigations of inferential processes in
reading: A theoretical and methodological integration. Taylor & Francis.
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing
cognitive processes and outcomes. Measuring up: Advances in how we assess reading ability, page 39.
Y Wang, M Harrington, and P White. 2012. Detecting breakdowns in local coherence in the writing of Chinese
English learners. Journal of Computer Assisted Learning, 28(4):396?410.
Stephen P Witte and Lester Faigley. 1981. Coherence, cohesion, and writing quality. College composition and
communication, pages 189?204.
961
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 681?684,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Entity-Based Features to Model Coherence in Student Essays  
 
 
Jill Burstein 
Educational Testing Service 
Princeton, NJ 08541 
Joel Tetreault 
Educational Testing Service 
Princeton, NJ 08541 
Slava Andreyev 
Educational Testing Service 
Princeton, NJ 08541 
jburstein@ets.org  jtetreault@ets.org     sandreyev@ets.org 
 
 
Abstract 
 
We show how the Barzilay and Lapata entity-
based coherence algorithm (2008) can be 
applied to a new, noisy data domain ? student 
essays. We demonstrate that by combining 
Barzilay and Lapata?s entity-based features 
with novel features related to grammar errors 
and word usage, one can greatly improve the 
performance of automated coherence prediction 
for student essays for different populations.  
 
1 Introduction 
 
There is a small body of work that has investigated 
using NLP for the problem of identifying 
coherence in student essays.  For example, Foltz, 
Kintsch & Landauer (1998), and Higgins, Burstein, 
Marcu & Gentile (2004) have developed systems 
that examine coherence in student writing.  Foltz, 
et al (1998) systems measure lexical relatedness 
between text segments by using vector-based 
similarity between adjacent sentences; Higgins et 
al?s (2004) system computes similarity across text 
segments.  Foltz et al?s (1998) approach is in line 
with the earlier TextTiling method that identifies 
subtopic structure in text (Hearst, 1997). 
Miltsakaki and Kukich (2000) addressed essay 
coherence using Centering Theory (Grosz, Joshi & 
Weinstein, 1995).  More recently, Barzilay and 
Lapata?s (2008) approach (henceforth, BL08) used 
an entity-based representation to evaluate 
coherence. In BL08, entities (nouns and pronouns) 
are represented by their sentence roles in a text. 
The algorithm keeps track of the distribution of 
entity transitions between adjacent sentences, and 
computes a value for all transition types based on 
their proportion of occurrence in a text.  BL08 
apply their algorithm to three tasks, using well-
formed newspaper corpora: text ordering, summary 
coherence evaluation, and readability assessment.  
For each task, their system outperforms a Latent 
Semantic Analysis baseline.  In addition, best 
performance on each task is achieved using 
different system and feature configurations. Pitler 
& Nenkova (2008) applied BL08 to detect text 
coherence in well-formed texts.   
 Coherence quality is typically present in scoring 
criteria for evaluating a student?s essay. This paper 
focuses on the development of models to predict 
low-and high-coherence ratings for essays.  
Student essay data, unlike newspaper text, is 
typically noisy, especially when students are non-
native English speakers (NNES).  Here, we 
evaluate how BL08 algorithm features can be used 
to model coherence in a new, noisy data domain -- 
student essays.  We found that coherence can be 
best modeled by combining BL08 entity-based 
features with novel writing quality features. 
Further, our use of data sets from three different 
test-taker populations also shows that coherence 
models will differ across populations. Different 
populations might use language differently which 
could affect how coherence is presented. We 
expect to incorporate coherence ratings into e-
rater?, ETS?s automated essay scoring system 
(Attali & Burstein, 2006).   
 
2 Corpus and Annotation 
 
We collected approximately 800 essays (in total) 
across three data sets1: 1) adult, NNES test essays 
(TOEFL); 2) adult, native and NNES test essays; 
(GRE) 3) U.S. middle- and high-school native and 
NNES student essay submissions to Criterion?, 
ETS?s instructional writing application. 
Two annotators were trained to rate coherence 
quality based on how easily they could read an 
essay without stumbling on a coherence barrier 
(i.e., a confusing sentence(s)).  Annotators rated 
                                                 
1 TOEFL? is the Test of English as a Foreign Language, 
and GRE? is the Graduate Record Admissions Test. 
681
essays on a 3-point scale: 1) low coherence, 2) 
somewhat coherent, and 3) high coherence.  They 
were instructed to ignore grammar and spelling 
errors, unless they affected essay comprehension. 
During training, Kappa agreement statistics 
indicated that annotators had difficulty agreeing on 
the middle, somewhat coherent category. The 
annotation scale was therefore collapsed into a 2-
point scale: somewhat coherent and high 
coherence categories were collapsed into the high 
coherence class (H), and low-coherence (L) 
remained unchanged.  Two annotators labeled an 
overlapping set of about 100 essays to calculate 
inter-rater agreement; weighted Kappa was 0.677. 
 
3 System  
 
3.1 BL08 Algorithm 
 
We implemented BL08?s entity-based algorithm to 
build and evaluate coherence models for the essay 
data. In short, the algorithm generates a vector of 
entity transition probabilities for documents 
(essays, here). Vectors are used to build coherence 
models.  The first step in the algorithm is to 
construct an entity grid in which all entities (nouns 
and pronouns) are represented by their roles (i.e., 
Subject (S), Object (O), Other (X)). Entity roles 
are then used to generate entity transitions ? the 
role transitions across adjacent sentences (e.g., 
Subject-to-Object, Object-to-Object). Entity 
transition probabilities are the proportions of 
different entity transition types within a text. The 
probability values are used then used as features to 
build a coherence model.  
Entity roles can be represented in the following 
ways. In this study, consistent with BL08, different 
combinations are applied and reported (see Tables 
2-4).  Entities can be represented in grids with 
specified roles (Syntax+) (S,O,X). Alternatively, 
roles can be reduced to show only the presence and 
absence of an entity (Syntax-) (i.e., Entity Present 
(P) or Not (N). Co-referential entities can be 
resolved (Coreference+) or not (Coreference-).  
Finally, the Salience option reflects the frequency 
with which an entity appears in the discourse: if 
the entity is mentioned two or more times, it is 
salient (Salient+), otherwise, not (Salient-).  
Consistent with BL08, we systematically 
completed runs using various configurations of 
entity representations (see Section 4).  
Given the combination, the entity transition 
probabilities were computed for all labeled essays 
in each data set. We used n-fold cross-validation 
for evaluation. Feature vectors were input to C5.0, 
a decision-tree machine learning application.  
 
3.2 Additional Features 
 
In BL08, augmenting the core coherence features 
with additional features improved the power of the 
algorithm.  We extended the feature set with 
writing quality features (Table 1).  GUMS features 
describe the technical quality of the essay.  The 
motivation for type/token features (*_TT) is to 
measure word variety.  For example, a high 
probability for a ?Subject-to-Subject? transition 
indicates that the writer is repeating an entity in 
Subject position across adjacent sentences. 
However, this does not take into account whether 
the same word is repeated or a variety of words are 
used.  The {S,O,X,SOX}_TT (type/token) features 
uncover the actual words collapsed into the entity 
transition probabilities. Shell nouns (Atkas & 
Cortes, 2008), common in essay writing, might 
also affect coherence. 
NNES essays can contain many spelling errors.  
We evaluated the impact of a context-sensitive 
spell checker (SPCR+), as spelling variation will 
affect the transition probabilities in the entity grid.  
Finally, we experimented with a majority vote 
method that combined the best performing feature 
combinations.  
 
4 Evaluation 
 
For all experiments, we used a series of n-fold 
cross-validation runs with C5.0 to evaluate 
performance for numerous feature configurations.  
In Tables 2, 3 and 4, we report: baselines, results 
on our data with BL08?s best system configuration 
from the summary coherence evaluation task 
(closest to our task), and our best systems. In the 
Tables, ?best systems? combined feature sets and 
outperformed baselines.   Rows in bold indicate 
final independent best systems that contribute to 
best performance in the majority vote method.  
Agreement is reported as Weighted Kappa (WK), 
Precision (P), Recall (R) and F-measure (F).  
Baselines. We implemented three non-trivial 
baseline systems. E-rater indicates use of the full  
682
feature set from e-rater. The GUMS (GUMS+) 
feature baseline, uses the Grammar (G+), Usage 
 
Feature Descriptor Feature Description 
GUMS  Grammar, usage, and 
mechanics errors, and style 
features from an AES system 
S_TT 
O_TT 
X_TT 
SOX_TT2 
P_TT 
Type/token ratios for actual 
words recovered from the 
entity grid, using the entity 
roles.  
S_TT_Shellnouns 
O_TT_Shellnouns 
X_TT_Shellnouns 
Type/token ratio of non-topic  
content, shell nouns (e.g., 
approach, aspect, challenge) 
Table 1: New feature category description 
 
 (U+), Mechanics (M+), and Style (ST+) flags 
(subset of e-rater features) to evaluate a coherence 
model.  The third baseline represents the best run 
using type/token features ({S,O,X,SOX}_TT), and 
{S,O,X}_TT_Shellnouns feature sets (Table 1).  
The baseline majority voting system includes e-
rater, GUMS, and the best performing type/token 
baseline (see Tables 2-4).  
Extended System.   We combined our writing 
quality features with the core BL08 feature set. 
The combination improved performance over the 
three baselines, and over the best performing BL08 
feature.  Type/token features added to BL08 entity 
transitions probabilities improved performance of 
all single systems. This supports the need to 
recover actual word use.  In Table 2, for TOEFL 
data, spell correction improved performance with 
the Mechanics error feature (where Spelling is 
evaluated). This would suggest that annotators 
were trying to ignore spelling errors when labeling 
coherence. In Table 3, for GRE data, spell 
correction improved performance with the 
Grammar error feature. Spell correction did 
change grammar errors detected: annotators may 
have self-corrected for grammar. Finally, the 
majority vote outperformed all systems. In Tables 
3 and 4, Kappa was comparable to human 
agreement (K=0.677). 
 
5 Conclusions and Future Work 
We have evaluated how the BL08 algorithm 
features can be used to model coherence for 
                                                 
2 Indicates an aggregate feature that computes the type/token 
ratio for entities that appear in any of S,O,X role. 
student essays across three different populations.   
We found that the best coherence models for 
essays are built by combining BL08 entity-based 
features with writing quality features. BL08?s 
outcomes showed that optimal performance was 
obtained by using different feature sets for 
different tasks. Our task was most similar to 
BL08?s summary coherence task, but we used 
noisy essay data. The difference in the data types 
might also explain the need for our systems to 
include additional writing quality features. 
Our majority vote method outperformed three 
baselines (and a baseline majority vote). For two of 
the populations, Weighted Kappa between system 
and human agreement was comparable. These 
results show promise toward development of an 
entity-based method that produces reliable 
coherence ratings for noisy essay data. We plan to 
evaluate this method on additional data sets, and in 
the context of automated essay scoring. 
 
References 
 
Aktas, R. N., & Cortes, V. (2008). Shell nouns as 
cohesive devices in published and ESL  student 
writing. Journal of English for Academic Purposes, 
7(1), 3?14. 
Attali, Y., & Burstein, J. (2006). Automated essay 
scoring with e-rater v.2.0 . Journal of Technology, 
Learning, and Assessment, 4(3). 
Barzilay, R. and Lapata, M. (2008). Modeling local 
coherence: An entity-based approach. 
Computational Linguistics, 34(1), 1-34. 
Foltz, P., Kintsch, W., and Landauer, T. K. (1998). The 
measurement of textual coherence with Latent 
Semantic Analysis. Discourse Processes, 
25(2&3):285?307. 
Higgins, D., Burstein, J., Marcu, D., & Gentile, C. 
(2004).  Evaluating multiple aspects of coherence in 
student essays . In Proceedings of HLT-NAACL 
2004, Boston, MA. 
Grosz, B., Joshi, A., and Weinstein, S.  1995, Centering: 
A framework for modeling the local coherence of 
discourse.  Computational Linguistics, 21(2): 203-
226. 
Hearst, M. A. (1997). TextTiling: Segmenting text into 
multi-paragraph subtopic passages. Computational 
Linguistics, 23(1):33?6 
Miltsakaki, E. and Kukich, K. (2000). Automated 
evaluation of coherence in student essays. In 
Proceedings of LREC 2000, Athens, Greece 
Pitler, E.,and Nenkova, A (2008). Revisiting 
Readability: A Unified Framework for Predicting 
683
Text Quality. In Proceedings of EMNLP 2008, 
Honolulu, Hawaii. 
 
 
 
 
  L (n=64) H  (n=196) L+H (n=260) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.472 56 69 62 89 82 86 79 79 79 
(b) GUMS 0.455 55 66 60 88 83 85 79 79 79 
(c)  SOX_TT3  0.484 66 55 60 86 91 88 82 82 82 
SYSTEMS: Includes BL08  FEATURES  
Coreference-Syntax+Salient+ (B&L08 
summary task configuration) 
0.253 49 34 40 81 88 84 75 75 75 
(d) Coreference-Syntax-Salient-SPCR+M+ 0.472 76 45 57 84 95 90 83 83 83 
(e) Coreference+Syntax+Salient-GUMS+ 0.590 68 70 69 90 89 90 85 85 85 
(f) Coreference+Syntax+Salient-
GUMS+O_TT_Shellnouns+ 
0.595 68 72 70 91 89 90 85 85 85 
Baseline Majority vote: (a),(b), (c) 0.450 55 64 59 88 83 85 79 79 79 
Majority vote:  (d), (e), (f) 0.598 69 70 70 90 90 90 85 85 85 
 
Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement  
 
  L (n=48) H (n=210) L+H (n=258) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.383 79 31 45 86 98 92 86 86 86 
(b) GUMS 0.316 68 27 39 85 97 91 84 84 84 
(c)  e-rater+SOX_TT4  0.359 78 29 42 86 98 92 85 85 85 
SYSTEMS: INCLUDES BL08  FEATURES  
Coreference-Syntax+Salient+ (BL08 summary 
task configuration) 
0.120 35 17 23 83 93 88 79 79 79 
(d) Coreference+Syntax+Salient-SPCR+G+ 0.547 1.0 43 60 89 1.0 94 90 90 90 
(e) Coreference+Syntax-Salient-P_TT+ 0.462 70 44 54 88 96 92 86 86 86 
(f) Coreference+Syntax+Salient+GUMS+ 
SOX_TT+ 
0.580 71 60 65 91 94 93 88 88 88 
Baseline Majority vote: (a),(b), (c) 0.383 79 31 45 86 98 92 86 86 86 
Majority vote: (d), (e), (f) 0.610 1.0 49 66 90 1.0 95 91 91 91 
 
Table 3: Native and Non-Native English Speaker Test-taker Data (GRE): Annotator/System Agreement  
 
  L (n=37) H  (n=226) L+H (n=263) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.315 39 46 42 91 88 89 82 82 82 
(b) GUMS 0.350 47 41 43 90 92 91 85 85 85 
(c)  SOX_TT 0.263 78 19 30 88 99 93 88 88 88 
SYSTEMS: INCLUDES BL08  FEATURES  
(d) Coreference-Syntax+Salient+ (BL08 
summary task configuration) 
0.383 79 30 43 90 99 94 89 89 89 
(e) Coreference-Syntax-Salient-SPCR+ 0.424 67 38 48 90 97 94 89 89 89 
(f) Coreference+Syntax+Salient+S_TT+ 0.439 65 41 50 91 96 94 89 89 89 
Baseline Majority vote: (a),(b), (c) 0.324 43 41 42 90 91 91 84 84 84 
Majority vote: (d), (e), (f) 0.471 82 38 52 91 99 94 90 90 90 
Table 4:  Criterion Essay Data: Annotator/System Agreement   
                                                 
3 Type/token ratios from all roles using a Coreference+Syntax+Salient+ grid. 
4 Type/token ratios from all roles using Coreference+Syntax+Salient- grid. 
684
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247?252,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Content Importance Models for Scoring Writing From Sources
Beata Beigman Klebanov Nitin Madnani Jill Burstein Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,nmadnani,jburstein,ssomasundaran}@ets.org
Abstract
Selection of information from external
sources is an important skill assessed in
educational measurement. We address an
integrative summarization task used in an
assessment of English proficiency for non-
native speakers applying to higher educa-
tion institutions in the USA. We evaluate a
variety of content importance models that
help predict which parts of the source ma-
terial should be selected by the test-taker
in order to succeed on this task.
1 Introduction
Selection and integration of information from ex-
ternal sources is an important academic and life
skill, mentioned as a critical competency in the
Common Core State Standards for English Lan-
guage Arts/Literacy: College-ready students will
be able to ?gather relevant information from mul-
tiple print and digital sources, assess the credibi-
lity and accuracy of each source, and integrate the
information while avoiding plagiarism.?
1
Accordingly, large-scale assessments of writing
incorporate tasks that test this skill. One such test
requires test-takers to read a passage, then to lis-
ten to a lecture discussing the same topic from
a different point of view, and to summarize the
points made in the lecture, explaining how they
cast doubt on points made in the reading. The qua-
lity of the information selected from the lecture is
emphasized in excerpts from the scoring rubric for
this test (below); essays are scored on a 1-5 scale:
Score 5 successfully selects the important infor-
mation from the lecture and coherently and
accurately presents this information in rela-
tion to the relevant information presented in
the reading.
1
http://www.corestandards.org/
ELA-Literacy/CCRA/W.
Score 4 is generally good in selecting the impor-
tant information from the lecture ..., but it
may have a minor omission.
Score 3 contains some important information
from the lecture ..., but it may omit one major
key point.
Score 2 contains some relevant information from
the lecture ... The response significantly
omits or misrepresents important points.
Score 1 provides little or no meaningful or rele-
vant coherent content from the lecture.
The ultimate goal of our project is to improve
automated scoring of such essays by taking into
account the extent to which a response integrates
important information from the lecture. This pa-
per reports on the first step aimed at automatically
assigning importance scores to parts of the lecture.
The next step ? developing an essay scoring sys-
tem using content importance models along with
other features of writing quality, will be addressed
in future work. A simple essay scoring mechanism
will be used for evaluation purposes in this paper,
as described in the next section.
2 Design of Experiment
In evaluations of summarization algorithms, it is
common practice to derive the gold standard con-
tent importance scores from human summaries, as
done, for example, in the pyramid method, where
the importance of a content element corresponds
to the number of reference human summaries that
make use of it (Nenkova and Passonneau, 2004).
Selection of the appropriate content plays a cru-
cial role in attaining a high score for the essays
we consider here, as suggested by the quotes from
the scoring rubric in ?1, as well as by a corpus
study by Plakans and Gebril (2013). We therefore
observe that high-scoring essays can be thought
247
of as high-quality human summaries of the lec-
ture, albeit containing, in addition, references to
the reading material and language that contrasts
the different viewpoints, making them a somewhat
noisy gold standard. On the other hand, since low-
scoring essays contain deficient summaries of the
lecture, our setup allows for a richer evaluation
than typical in studies using gold standard human
data only, in that a good model should not only
agree with the gold standard human summaries
but should also disagree with sub-standard human
summaries. We therefore use correlation with es-
say score to evaluate content importance models.
The evaluation will proceed as follows. Every
essay E is responding to a test prompt that con-
tains a lecture L and a reading R. We identify the
essay?s overlap with the lecture:
O(E,L) = {x|x ? L, x ? E} (1)
where the exact definition of x, that is, what is
taken to be a single unit of information, will be
one of the parameters to be studied. The essay is
then assigned the following score by the content
importance model M :
S
M
(E) =
?
x?O(E,L)
w
M
(x)? C(x,E)
n
E
(2)
where w
M
(x) is the importance weight as-
signed by model M to item x in the lecture,
C(x,E) is the count of tokens in E that realize
the information unit x, and n
E
is the number of
tokens in the essay. In this paper, the distinction
between x and C is that between type and token
count of instances of that type.
2
This simple sco-
ring mechanism quantifies the rate of usage of im-
portant information per token in the essay. Finally,
we calculate the correlation of scores assigned to
essays by model M with scores assigned to the
same essays by human graders.
This design ensures that once x is fixed, all the
content importance models are evaluated within
the same scoring scheme, so any differences in the
correlations can be attributed to the differences in
the weights assigned by the importance models.
2
In the future, we intend to explore more complex rea-
lization functions, allowing paraphrase, skip n-grams (as in
ROUGE (Lin, 2004)), and other approximate matches, such
as misspellings and inflectional variants.
3 Content Importance Models
Our setting can be thought of as a special kind
of summarization task. Test-takers are required
to summarize the lecture while referencing the
reading, making this a hybrid of single- and multi-
document summarization, where one source is
treated as primary and the other as secondary.
We therefore consider models of content impor-
tance that had been found useful in the summariza-
tion literature, as well as additional models that
utilize a special feature of our scenario: We have
hundreds of essays of varying quality responding
to any given prompt, as opposed to a typical news
summarization scenario where a small number of
high quality human summaries are available for a
given article. A sample of these essays can be used
when developing a content importance model.
We define the following importance models.
For all definitions, x is a unit of information
in the lecture; C(x, t) is the number of tokens in
text t that realize x; n
L
and n
R
are the number of
tokens in the lecture and the reading, respectively.
3
Na??ve: w(x) = 1. This is a simple overlap model.
Prob: w(x) =
C(x,L)
n
L
, an MLE estimate of
the probability that x appears in the lecture.
Those x that appear more are more important.
Position: w(x) =
FP (x)
n
L
, where FP (x) is the
offset of the first occurrence of x in the lec-
ture. The offset corresponds to the token?s
serial number in the text, 1 through n
L
.
LectVsRead: w(x) =
C(x,L)
n
L
?
C(x,R)
n
R
, that is, the
difference in the probabilities of occurrence
of x in the lecture and in the reading passage
that accompanies the lecture. This model at-
tempts to capture the contrastive aspect of
importance ? the content that is unique to
the lecture is more important than the content
that is shared by the lecture and the reading.
The following two models capitalize on evi-
dence of use of information in better and worse es-
says. For estimating these models, we sample, for
each prompt, a development set of 750 essays re-
sponding to the prompt (that is, addressing a given
pair of lecture and reading stimuli). Out of these,
we take, for each prompt, all essays at score points
3
Prob, Position, and LectVsRead models normalize by
n
R
and n
L
to enable comparison of essays responding to dif-
ferent lecture + reading stimuli (prompts).
248
4 and 5 (EGood) and all essays at score points 1
and 2 (EBad). These data do not overlap with the
experimental data described in section 4. In both
definitions below, e is an essay.
Good: w(x) =
|{e?EGood|x?e}|
|EGood|
. An x is more im-
portant if more good essays use it. Hong and
Nenkova (2014) showed that a variant of this
measure used on pairs of articles and their ab-
stracts from the New York Times effectively
identified words that typically go into sum-
maries, across topics. In contrast, our mea-
surements are prompt-specific.
GoodVsBad: w(x) =
|{e?EGood|x?e}|
|EGood|
?
|{e?EBad|x?e}|
|EBad|
. An x is more important if
good essays use it more than bad essays.
To our knowledge, this measure has not
been used in the summarization literature,
probably because a large sample of human
summaries of varying quality is typically not
available.
4 Data
We use 116 prompts drawn from an assessment of
English proficiency for non-native speakers. Each
prompt contains a lecture and a reading passage.
For each prompt, we sample about 750 essays.
Each essay has an operational score provided by
a human grader. Table 1 shows the distribution of
essay scores; mean score is 3. Text transcripts of
the lectures were used.
Score 1 2 3 4 5
Proportion 0.13 0.18 0.35 0.25 0.09
Table 1: Distribution of essay scores.
5 Results
Independently from the content importance
models, we address the effect of the granularity of
the unit of information. Intuitively, since all the
materials for a given prompt deal with the same
topic, we expect large unigram overlaps between
lecture and reading, and between good and bad
essays, whereas n-grams with larger n can be
more distinctive. On the other hand, larger n lead
to misses, where an information unit would fail
to be identified in an essay due to a paraphrase,
thus impairing the ability of the scoring function
to use the content importance model effectively.
We therefore evaluate each content importance
model for different granularities of the content
unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows
the correlations with essay scores.
Content Pearson?s r
Importance
Model n=1 n=2 n=3 n=4
Na??ve 0.24 0.27* 0.24 0.20
Prob 0.04 0.14 0.17 0.14
Position 0.22 0.30* 0.26* 0.20
LectVsRead 0.09 0.25* 0.31* 0.26*
Good 0.07 0.15 0.10 0.07
GoodVsBad 0.54* 0.42* 0.32* 0.21
Table 2: Correlations with essay scores attained by
content models, for various definitions of informa-
tion unit (n-grams with n = 1, 2, 3, 4). Five top
scores are boldfaced. The baseline performance
is shown in underlined italics. Correlations that
are significantly better (p < 0.05) than the na??ve
n = 1 model are marked with an asterisk. We
use McNemar (1955, p. 148) test for significance
of difference between same-sample correlations.
N = 85, 252 for all correlations.
6 Discussion
The Na??ve model with n = 1 can be considered a
baseline, corresponding to unweighted word over-
lap between the lecture and the essay. This model
attains a significant positive correlation with essay
score (r = 0.24), suggesting that, in general, bet-
ter writers use more material from the lecture.
Our next observation is that the Prob and Good
models do not improve over the baseline, that is,
their weighting schemes generally assign higher
weights to the wrong units. We believe the rea-
son for this is that the most highly used n-grams,
in the lecture and in the essays, correspond to ge-
neral topical and functional elements. The impor-
tance of these elements is discounted in the more
effective Position, LectVsRead, and GoodVsBad
models, highlighting subtler aspects of the lecture.
Next, let us consider the granularity of the units
of information. We observe that 4-grams are in-
ferior to trigrams for all models, suggesting that
data sparsity is becoming a problem for matching
4-word sequences. For models that assign weight
based on one or two sources (lecture, or lecture
and reading) ? Na??ve, Position, LectVsRead ? un-
igram models are generally ineffective, while bi-
249
gram and trigram models significantly outperform
the baseline. We interpret this as suggesting that
it is certain particular, detailed aspects of the top-
ical concepts that constitute the important nuggets
in the lecture; these are usually realized by multi-
word sequences.
The GoodVsBad models show a different pat-
tern, obtaining the best performance with a uni-
gram version. These models are sensitive to data
sparsity not only when matching essays to the
lecture (this problem is common to all models)
but also during model building. Recall that the
weights in a GoodVsBad model are estimated
based on differential use in samples of good and
bad essays. The estimation of use-in-a-corpus is
more accurate for smaller n, because longer n-
grams are more susceptible to paraphrasing, which
leads to under-estimation of use. Assuming that
paraphrasing behavior of good and bad writers is
not the same ? in fact, there is corpus evidence
that better writers paraphrase more (Burstein et
al., 2012) ? the resulting inaccuracies might im-
pact the estimation of differential use in a sys-
tematic manner, making the n > 1 models less
effective than the unigrams. Given that (a) the
GoodVsBad bigram model is the second best over-
all in spite of the shortcomings of the estimation
process, and (b) that the bigram models worked
better than unigram models for all the other con-
tent importance models, the GoodVsBad bigram
model could probably be improved significantly
by using a more flexible information realization
mechanism.
To illustrate the information assigned high im-
portance by different models, consider a lec-
ture discussing advantages of fish farming. The
top-scoring Good bigrams are topical expressions
(fish farming), functional bigrams around fish and
farming,
4
aspects of content dealt with at length
in the lecture (wild fish, commercial fishing), bi-
grams referencing some of the claims ? fish con-
taining less fat and being used for fish meal. In
addition, this model picks out some sequences of
function words and punctuation (of the, are not,
?, and?, ?, the?) that suggest that better essays
tend to give more detail (hence have more com-
plex noun phrases and coordinated constructions)
and to draw contrast.
For the bigram GoodVsBad model, the topi-
cal bigram fish farming is not in the top 20 bi-
4
such as that fish, of fish, farming is, ?, fish?
grams. Although some bigrams are shared with
the Good model, the GoodVsBad model selects
additional details about the claims, such as the
contrast between inedible fish and edible fish that
is eaten by humans, as well as reference to chemi-
cals used in farming and to the claim that wild fish
are already endangered by other practices.
The most important bigrams according to the
LectVsRead model include functional bigrams
around fish and farming, functional sequences
(that the, is a), as well as commercial fishing and
edible fish. Also selected are functional bigrams
around consumption and species, hinting, indi-
rectly, at the edibility differences between species.
Finally, this model selects almost all bigrams in
the reading passage makes, the reading makes
claims that and the reading says. While distin-
guishing the lecture from the reading, these do not
capture topic-relevant content of the lecture.
The GoodVsBad unigram model selects poul-
try, endangered, edible, chemicals among its top 6
unigrams,
5
effectively touching upon the connec-
tion with other farm-raised foods (poultry, chemi-
cals), with wild fish (endangered) and with human
benefit (edible) that are made in the lecture.
7 Related work
Modern essay scoring systems are complex and
cover various aspects of the writing construct,
such as grammar, organization, vocabulary (Sher-
mis and Burstein, 2013). The quality of content
is often addressed by features that quantify the
similarity between the vocabulary used in an es-
say and reference essays from given score points
(Attali and Burstein, 2006; Foltz et al, 2013; At-
tali, 2011). For example, Attali (2011) proposed a
measure of differential use of words in higher and
lower scoring essays defined similarly to Good-
VsBad, without, however, considering the source
text at all. Such features can be thought of as con-
tent quality features, as they implicitly assume that
writers of better essays use better content. How-
ever, there are various kinds of better content, only
one of them being selection of important informa-
tion from the source; other elements of content
originate with the writer, such as examples, dis-
course markers, evaluations, introduction and con-
clusion, etc. Our approach allows focusing on a
particular aspect of content quality, namely, selec-
tion of appropriate materials from the source.
5
the other two being fishing and used.
250
Our results are related to the findings of Gure-
vich and Deane (2007) who studied the difference
between the reading and the lecture in their im-
pact on essay scores for this test. Using data from
a single prompt, they showed that the difference
between the essay?s average cosine similarity to
the reading and its average cosine similarity to the
lecture is predictive of the score for non-native
speakers of English, thus using a model similar
to LectVsRead, although they took all lecture,
reading, and essay words into account, in contrast
to our model that looks only at n-grams that ap-
pear in the lecture. Our study shows that the ef-
fectiveness of lecture-reading contrast models for
essay scoring generalizes to a large set of prompts.
Similarly, Evanini et al (2013) found that over-
lap with material that is unique to the lecture (not
shared with the reading) was predictive of scores
in a spoken source-based question answering task.
In the vast literature on summarization, our
work is closest to Hong and Nenkova (2014) who
studied models of word importance for multi-
document summarization of news. The Prob, Po-
sition, and Good models are inspired by their
findings of the effectiveness of similar models in
their setting. We found that, in our setting, Prob
and Good models performed worse than assigning
a uniform weight to all words. We note, however,
that models from Hong and Nenkova (2014) are
not strictly comparable, since their word proba-
bility models were calculated after stopword ex-
clusion, and their model that inspired our Good
model was defined somewhat differently and val-
idated using content words only. The defini-
tion of our Position model and its use in the es-
say scoring function S (equation 2) correspond to
Hong and Nenkova (2014) average first location
model for scoring summaries. Differently from
their findings, this model is not effective for sin-
gle words in our setting. Position models over n-
grams with n > 1 are effective, but their predic-
tion is in the opposite direction of that found for
the news data ? the more important materials tend
to appear later in the lecture, as indicated by the
positive r between average first position and essay
score. These findings underscore the importance
of paying attention to the genre of the source ma-
terial when developing summarization systems.
Our summarization task incorporates elements
of contrastive opinion summarization (Paul et al,
2010; Kim and Zhai, 2009), since the lecture and
the reading sometimes interpret the same facts in
a positive or negative light (for example, the fact
that chemicals are used in fish farms is negative
if compared to wild fish, but not so if compared
to other farm-raised foods like poultry). Relation-
ships between aspect and sentiment (Brody and
Elhadad, 2010; Lazaridou et al, 2013) are also
relevant, since aspects of the same fact are em-
phasized with different evaluations (the quantity
vs the variety of species that go into fish meal for
farmed fish). We hypothesize that units participat-
ing in sentiment and aspect contrasts are of higher
importance; this is a direction for future work.
8 Conclusion
In this paper, we addressed the task of automati-
cally assigning importance scores to parts of a lec-
ture that is to be summarized as part of an English
language proficiency test. We investigated the op-
timal units of information to which importance
should be assigned, as well as a variety of impor-
tance scoring models, drawing on the news sum-
marization and essay scoring literature.
We found that bigrams and trigrams were ge-
nerally more effective than unigrams and 4-grams
across importance models, with some exceptions.
We also found that the most effective impor-
tance models are those that equate importance
of an n-gram with its preferential use in higher-
scoring essays than in lower-scoring ones, above
and beyond merely looking at the n-grams used in
good essays. This demonstrates the utility of using
not only gold, high-quality human summaries, but
also sub-standard ones when developing content
importance models.
Additional importance criteria that are intrinsic
to the lecture, as well as those that capture contrast
with a different source discussing the same topic,
were also found to be reasonably effective. Since
different importance models often select different
items as most important, we intend to investigate
complementarity of the different models.
Finally, our results highlight that the effective-
ness of an importance model depends on the genre
of the source text. Thus, while a first sentence
baseline is very competitive in news summariza-
tion, we found that important information tends
not to be located in the opening sentences in our
data (these tend to provide general, introductory
information), but appears later on, when more de-
tailed, specific claims are put forward.
251
References
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater
R
?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali. 2011. A Differential Word Use Measure
for Content Analysis in Automated Essay Scoring.
ETS Research Report, RR-11-36.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 804?812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jill Burstein, Michael Flor, Joel Tetreault, Nitin Mad-
nani, and Steven Holtzman. 2012. Examining Lin-
guistic Characteristics of Paraphrase in Test-Taker
Summaries. ETS Research Report, RR-12-18.
Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based content scoring for automated spoken
language assessment. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 157?162, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Peter Foltz, Lynn Streeter, Karen Lochbaum, and
Thomas Landauer. 2013. Implementation and Ap-
plication of the Intelligent Essay Assessor. In Mark
Shermis and Jill Burstein, editors, Handbook of au-
tomated essay evaluation: Current applications and
new directions, pages 68?88. New York: Routh-
ledge.
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
49?52, Rochester, New York, April. Association for
Computational Linguistics.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In The Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Gottenberg, Sweden, April. As-
sociation for Computational Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of the 18th ACM Confer-
ence on Information and Knowledge Management,
CIKM ?09, pages 385?394, New York, NY, USA.
ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
ACL workshop: Text summarization branches out,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Human Language Technologies
2004: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 145?152, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguis-
tics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lia Plakans and Atta Gebril. 2013. Using multiple
texts in an integrated writing assessment: Source
text use as a predictor of score. Journal of Second
Language Writing, 22:217?230.
Mark Shermis and Jill Burstein, editors. 2013. Hand-
book of Automated Essay Evaluation: Current Ap-
plications and Future Directions. New York: Rout-
ledge.
252
Transactions of the Association for Computational Linguistics, 1 (2013) 99?110. Action Editor: Chris Callison-Burch.
Submitted 12/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a
Subjectivity Lexicon for Essay Data
Beata Beigman Klebanov, Nitin Madnani, Jill Burstein
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{bbeigmanklebanov,nmadnani,jburstein@ets.org}
Abstract
We demonstrate a method of improving a seed
sentiment lexicon developed on essay data by
using a pivot-based paraphrasing system for
lexical expansion coupled with sentiment pro-
file enrichment using crowdsourcing. Profile
enrichment alone yields up to 15% improve-
ment in the accuracy of the seed lexicon on 3-
way sentence-level sentiment polarity classifi-
cation of essay data. Using lexical expansion
in addition to sentiment profiles provides a
further 7% improvement in performance. Ad-
ditional experiments show that the proposed
method is also effective with other subjectivity
lexicons and in a different domain of applica-
tion (product reviews).
1 Introduction
In almost any sub-field of computational linguistics,
creation of working systems starts with an invest-
ment in manually-generated or manually-annotated
data for computational exploration. In subjectivity
and sentiment analysis, annotation of training and
testing data and construction of subjectivity lexicons
have been the loci of costly labor investment.
Many subjectivity lexicons are mentioned in the
literature. The two large manually-built lexicons
for English ? the General Inquirer (Stone et al,
1966) and the lexicon provided with the Opinion-
Finder distribution (Wiebe and Riloff, 2005) ? are
available for research and education only1 and un-
der GNU GPL license that disallows their incor-
poration into proprietary materials,2 respectively.
1http://www.wjh.harvard.edu/ inquirer/j1 1/manual/
2http://www.gnu.org/copyleft/gpl.html
Those wishing to integrate sentiment analysis into
products, along with those studying subjectivity in
languages other than English, or for specific do-
mains such as finance, or for particular genres
such as MySpace comments, reported construction
of lexicons (Taboada et al, 2011; Loughran and
McDonald, 2011; Thelwall et al, 2010; Rao and
Ravichandran, 2009; Jijkoun and Hofmann, 2009;
Pitel and Grefenstette, 2008; Mihalcea et al, 2007).
In this paper, we address the step of expanding
a small-scale, manually-built subjectivity lexicon (a
seed lexicon, typically for a domain or language
in question) into a much larger but noisier lexi-
con using an automatic procedure. We present
a novel expansion method using a state-of-the-art
paraphrasing system. The expansion yields a 4-fold
increase in lexicon size; yet, the expansion alone
is insufficient in order to improve performance on
sentence-level sentiment polarity classification.
In this paper we test the following hypothesis.
We suggest that the effectiveness of the expansion
is hampered by (1) introduction of opposite-polarity
items, such as introducing resolute as an expansion
of forceful, or remarkable as an expansion of pecu-
liar; (2) introduction of weakly polar, neutral, or am-
biguous words as expansions of polar seed words,
such as generating concern as an expansion of anx-
iety or future as an expansion of aftermath;3 (3) in-
ability to distinguish between stronger or clear-cut
versus weaker or ambiguous sentiment and to make
a differential use of those.
We address items (1) and (2) by enriching the lexi-
con with sentiment profiles (section 3), and propose
3Table 2 and Figure 1 provide support to these assessments.
99
a way of effectively utilizing this information for
the sentence-level sentiment polarity classification
task (sections 5 and 6). Profile-enrichment alone
yields up to 15% increase in performance for the
seed lexicon when using different machine learning
algorithms; paraphraser-based expansion with sen-
timent profiles improves performance by an addi-
tional 7%. Overall, we observe an improvement of
up to 25% in classification accuracy over the seed
lexicon without profiles.
In section 7, we present comparative evaluations,
demonstrating the competitiveness of the expanded
and profile-enriched lexicon, as well as the effective-
ness of the expansion and enrichment paradigm pre-
sented here for different subjectivity lexicons, dif-
ferent lexical expansion methods, and in a different
domain of application (product reviews).
2 Building Subjectivity Lexicons
The goal of our sentiment analysis project is to allow
for the identification of sentiment in sentences that
appear in essay responses to a variety of tasks de-
signed to test English proficiency in both native- and
non-native-speaker populations in a standardized as-
sessment as well as in an instructional settings. In
order to allow for the future use of the sentiment
analyzer in a proprietory product and to ensure its fit
to the test-taker essay domain, we began our work
with the construction of a seed lexicon relying on
our materials (section 2.1). We then used a statisti-
cal paraphrasing system to expand the seed lexicon
(section 2.2).
2.1 Seed Lexicon
In order to inform the process of lexicon construc-
tion, we randomly sampled 5,000 essays from a cor-
pus of about 100,000 essays containing writing sam-
ples across many topics. Essays were responses
to several different writing assignments, including
graduate school entrance exams, non-native English
speaker proficiency exams, and professional licen-
sure exams. Our seed lexicon is a combination of
(1) positive and negative sentiment words manually
selected from a full list of word types in these data,
and (2) words marked in a small-scale annotation of
a sample of sentences from these data for all posi-
tive and negative words. A more detailed descrip-
tion of the construction of seed lexicon can be found
in Beigman Klebanov et al(2012). The seed lexi-
con contains 749 single words, 406 positive and 343
negative.
2.2 Expanded Lexicon
We used a pivot-based lexical and phrasal para-
phrase generation system (Madnani and Dorr, 2013).
The paraphraser implements the pivot-based method
as described by Bannard and Callison-Burch (2005)
with several additional filtering mechanisms to in-
crease the precision of the extracted pairs. The
pivot-based method utilizes the inherent monolin-
gual semantic knowledge from bilingual corpora:
We first identify phrasal correspondences between
English and a given foreign language F , then map
from English to English by following translation
units from English to the other language and back.
For example, if the two English phrases e1 and e2
both correspond to the same foreign phrase f , then
they may be considered to be paraphrases of each
other with the following probability:
p(e1|e2) ? p(e1|f)p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?)p(f ?|e2)
Seed Expansion Seed Expansion
abuse exploitation costly onerous
accuse reproach dangerous unsafe
anxiety disquiet improve reinforce
conflict crisis invaluable precious
Table 1: Examples of paraphraser expansions.
Some examples of expansions generated by the
paraphraser are shown in Table 1. More details
about this kind of approach can be found in Ban-
nard and Callison-Burch (2005). We use the French-
English parallel corpus (approximately 1.2 million
sentences) from the corpus of European parliamen-
tary proceedings (Koehn, 2005) as the data on which
pivoting is performed to extract the paraphrases.
However, the base paraphrase system is susceptible
100
to large amounts of noise due to the imperfect bilin-
gual word alignments. Therefore, we implement ad-
ditional heuristics in order to minimize the num-
ber of noisy paraphrase pairs (Madnani and Dorr,
2013). For example, one such heuristic filters out
pairs where a function word may have been inferred
as a paraphrase of a content word. For the lexicon
expansion experiment reported here, we use the top
15 single-word paraphrases for every word from the
seed lexicon, excluding morphological variants of
the seed word. This process results in an expanded
lexicon of 2,994 different words, 1,666 positive and
1,761 negative (433 words are in both the positive
and the negative lists). The expanded lexicon in-
cludes the seed lexicon.
3 Inducing sentiment profiles
Let ?w be the sentiment profile of the word w.
?w = (pposw , pnegw , pneuw ) (1)
where ?i?{pos,neg,neu} piw = 1. Thus, a sentiment
profile of a word is essentially a 3-sided coin, cor-
responding to its probability of coming out positive,
negative, and neutral, respectively.
3.1 Estimating sentiment profiles
Our goal is to estimate the profile using outcomes of
multiple trials as follows. For every word, a person
is shown the word and asked whether it is positive,
negative, or neutral. A person?s decision is modeled
as flipping the coin corresponding to the word, and
recording the outcome ? positive, negative, or neu-
tral. We run N=20 such trials for every word in the
expanded lexicon using the CrowdFlower crowd-
sourcing site,4 for a total cost of $800. We use maxi-
mum likelihood estimate of sentiment profile:
p?iw = niw (2)
where niw is the proportion ofN trials on the wordw
that fell in cell i ? {pos, neg, neu}. Table 2 shows
some estimated profiles.
Following Goodman (1965) and Quesenberry and
Hurst (1964), we calculate confidence intervals for
the parameters piw:
(p?iw)? = (B + 2niw ? T )/(2(N +B)) (3)
4www.crowdflower.com
Word p?posw p?neuw p?negw
forceful 0 0.15 0.85
resolute 0.8 0.15 0.05
peculiar 0.05 0.15 0.8
remarkable 1 0 0
anxiety 0 0 1
concern 0.25 0.4 0.35
absurd 0 0 1
laughable 0.5 0.05 0.45
deadly 0 0 1
fateful 0.25 0.45 0.3
consequence 0.05 0.15 0.8
outcome 0.15 0.85 0
Table 2: Examples of estimated sentiment profiles.
Words in gray are expansions generated from words in
the preceding row; note the difference in the profiles.
(p?iw)+ = (B + 2niw + T )/(2(N +B)) (4)
where
T =
?
B[B + 4niw(N ? niw)/N ]) (5)
For confidence ? that all piw, i ? {pos, neg, neu}
are simultaneously within their respective intervals,
the value of B is determined as the upper?/3?100th
percentile of the ?2 distribution with one degree of
freedom. We use ?=0.1, resulting in B=4.55. The
resulting interval is about 0.2 around the estimated
value when p?iw is close to 0.5, and somewhat nar-
rower for p?iw closer to 0 or 1. We will use this infor-
mation when inducing features from the profiles.
3.2 Sentiment distributions of the lexicons
The estimated sentiment profiles per word allow us
to visualize the distributions of the two lexicons. In
Figure 1, we plot the number of entries in the lexi-
con as a function of the difference in positive and
negative parts of the profile, in 0.2-wide bins. Thus,
a word w would be in the second-leftmost bin if
?0.8 < (p?posw ? p?negw ) < ?0.6.
While the expansion process more than doubles
the number of words in the highest bins for both
the positive and the negative polarity, it clearly
introduces a large number of words in the low-
and medium bins into the lexicon. It is in this
sense that the expansion process is noisy; appa-
rently, seed words with clear and strong polarity
101
10
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 1: Sentiment distributions for the seed (left) and
the expanded (right) lexicons.
are often expanded into low intensity, neutral, or
ambiguous ones, as in pairs like absurd/laughable,
deadly/fateful, anxiety/concern shown in Table 2.
4 Related Work
The most popular seed expansion methods discussed
in the literature are based on WordNet (Miller,
1995) or another lexicographic resource, on dis-
tributional similarity with the seeds, or on a mix-
ture thereof (Cruz et al, 2011; Baccianella et al,
2010; Velikovich et al, 2010; Qiu et al, 2009; Mo-
hammad et al, 2009; Esuli and Sebastiani, 2006;
Kim and Hovy, 2004; Andreevskaia and Bergler,
2006; Hu and Liu, 2004; Kanayama and Nasukawa,
2006; Strapparava and Valitutti, 2004; Kamps et al,
2004; Takamura et al, 2005; Turney and Littman,
2003; Hatzivassiloglou and McKeown, 1997). The
paraphrase-based expansion method is in the dis-
tributional similarity camp; we also experimented
with WordNet-based expansion as descibed in sec-
tion 7.2.
The task of assigning sentiment profiles to words
in a sentiment lexicon has been addressed in the lite-
rature. SentiWordNet assigns profiles to all words in
WordNet based on a propagation algorithm from a
small seed set manually annotated by a small num-
ber of judges (Baccianella et al, 2010; Cerini et al,
2007). Andreevskaia and Bergler (2006) use graph
propagation algorithms on WordNet to assign cen-
trality scores in positive and negative categories; a
similar approach based on web-scale co-occurrence
graphs is discussed in Velikovich et al(2010). Thel-
wall et al(2010) manually annotated a set of words
for strength of sentiment and used machine learning
to fine-tune it. Taboada et al(2011) produced an
expert annotation of their lexicon with strength of
sentiment. Subasic and Huettner (2001) manually
built an affect lexicon with intensities. Wiebe and
Riloff (2005) classifed lexicon entries into weakly
and strongly subjective, based on their relative fre-
quency of appearance in subjective versus objective
contexts in a large annotated dataset.
Our sentiment profiles are best thought of as
relatively fine-grained priors for the sentiment ex-
pressed by a given word out-of-context. These re-
flect a mixture of strength of sentiment (p?posgood >
p?posdecent), contextual ambiguity (concern can be in-terpreted as similar to worry or to care, as in ?Her
condition was causing concern? versus ?He showed
genuine concern for her?), and dominance of a po-
lar connotation (abandon is p?neg=1; it has a negative
overtone even if the actual sense is not that of desert
but of vacate, as in ?You must abandon your office?).
To the best of our knowledge, this paper presents
the first attempt to integrate judgements obtained
through crowdsourcing on a large scale into a sen-
timent lexicon, showing the effectiveness of this
lexicon-enrichment procedure for a sentiment clas-
sification task.
5 Using profiles for sentence-level
sentiment polarity classification
To evaluate the usefulness of the lexicons, we use
them to generate features for machine learning sys-
tems, and compare performance on 3-way sentence-
level sentiment polarity classification. To ensure ro-
bustness of the observed trends, we experiment with
a number of machine learning algorithms: SVM
Linear and RBF, Na??ve Bayes, Logistic Regression
(using WEKA (Hall et al, 2009)), and c5.0 Decision
Trees (Quinlan, 1993).5
5.1 Data
We generated the data for training and testing the
machine learning systems as follows. We used our
5available from http://rulequest.com/
102
pool of 100,000 essays to sample a second, non-
overlapping set of 5,000 essays, so that no essay
used for lexicon development appears in this set.
From these essays, we randomly sampled 550 sen-
tences, and submitted them to sentiment polarity an-
notation by two experienced research assistants; 50
double-annotated sentenced showed ?=0.8. TEST
set contains the 43 agreed double-annotated sen-
tences, and additional 238 sampled from the 500
single-annotated sentences, 281 sentence in total.
The category distribution in the TEST set is 46.6%
neutral, 32.4% positive, and 21% negative.
The TRAIN set contains the remaining sentences,
plus positive, negative, and neutral sentences anno-
tated during lexicon development, for the total of
1,631 sentences. The category distribution in TRAIN
is 39% neutral, 35% positive, 26% negative.
5.2 From lexicons to features
Our goal is to evaluate the impact of sentiment pro-
files on sentence-level sentiment polarity classifica-
tion for the seed and the expanded lexicons, while
also looking for the most effective ways to represent
this information for machine learners.
We implement two baseline systems. One pro-
vides the machine learner with the most detailed in-
formation contained in a lexicon: BL-full has 2 fea-
tures for every lexicon word, taking the values (1,0)
for positive match in a sentence, (0,1) ? for negative,
(1,1) for a word in both positive and negative parts
of the lexicon, and (0,0) otherwise.
The second baseline provides the machine learner
with only summary information about the overall
sentiment of the sentence. BL-sum uses only 2 fea-
tures: (1) the total count of positive words in the
sentence; (2) the total count of negative words in the
sentence, according to the given lexicon.
For the sentiment-enriched runs, we construct a
number of representations: Int-full, Int-sum, Int-
bin, and Int-c. Int-full and Int-sum are parallel to
the respective baseline systems. Int-full represents
each lexicon word as 2 features corresponding to the
word?s estimated p?posw and p?negw , providing the most
detailed information to the machine learner. In the
Int-sum condition, we use p?posw and p?negw for every
word to induce 2 features: (1) the sum of positive
probabilities of all words in the sentence; (2) the
sum of negative probabilities for all words in the
sentence, according to the given lexicon.
For Int-bin runs, we use bins of the size of 0.2 ?
half of the maximal confidence interval ? to group
together words with close estimates. We produce
10 features. For positive bins, the 5 features count
the number of words in the sentence that fall in
bini, 1 ? i ? 5, respectively, that is, words with
0.2(i? 1) < p?posw ? 0.2i. Bin 1 also includes words
with p?posw = 0, since these cannot be distinguished
with high confidence from p?posw =0.1. Note that we
do not provide a scale, we merely represent different
ranges with different features. This should allow the
machine learners the flexibility to weight the diffe-
rent bins differently when inducing classifiers.
The Int-c condition represents a coarse-grained
setting. We produce 4 features, two for each pola-
rity: (1) the number of words such that 0 ? p?posw <
0.4; (2) the number of words such that 0.4 ? p?posw ?
1; similarity for the negative polarity.
Table 3 summarizes conditions and features.
Cond. #F Feature Description
BL-full 2|L| (1Lpos?S(w),1Lneg?S(w))
BL-sum 2 f1=|{w : w ? Lpos ? S}|
f2=|{w : w ? Lneg ? S}|
Int-full 2|L| (p?posw , p?negw ) ?w ? A
Int-sum 2 (?w?A p?posw , ?w?A p?negw )
Int-bin 10 f1=|{w ? A : 0 ? p?posw ? 0.2}|
...
f10=|{w ? A : 0.8 < p?negw ? 1}|
Int-c 4 f1=|{w ? A : 0 ? p?posw < 0.4}|
...
f4=|{w ? A : 0.4 ? p?negw ? 1}|
Table 3: Description of conditions. Column 2 shows the
number of features. In column 3: 1 is an indicator func-
tion; L is a lexicon; Lpos is the part of the lexicon con-
taining positive words (same with negatives); S is a sen-
tence for which a feature vector is built; A = L ? S. For
all w ? L ? S in the -full conditions, w is represented
with (0,0).
6 Results
Table 4 shows classification accuracies for 5 ma-
chine learning systems across 6 conditions, for the
seed and the expanded lexicons.
Let BL denote the best-performing baseline (BL-
103
Machine Condition Seed Expanded
Learner
? Majority 0.466 0.466
c5.0 BL-full 0.441 0.498
BL-sum 0.512 0.480
Int-full 0.441 0.498
Int-sum 0.566 0.616
Int-bin 0.587 0.641
Int-c 0.530 0.577
SVM BL-full 0.466 0.466
RBF BL-sum 0.527 0.495
Int-full 0.466 0.466
Int-sum 0.548 0.601
Int-bin 0.573 0.644
Int-c 0.530 0.562
SVM BL-full 0.584 0.566
Linear BL-sum 0.509 0.502
Int-full 0.580 0.609
Int-sum 0.601 0.580
Int-bin 0.573 0.630
Int-c 0.569 0.569
Logistic BL-full 0.545 0.509
Regression BL-sum 0.545 0.509
Int-full 0.534 0.502
Int-sum 0.555 0.584
Int-bin 0.584 0.616
Int-c 0.545 0.577
Na??ve BL-full 0.598 0.584
Bayes BL-sum 0.509 0.473
Int-full 0.598 0.580
Int-sum 0.545 0.605
Int-bin 0.559 0.626
Int-c 0.537 0.601
Table 4: Classification accuracies on TEST set. Majo-
rity baseline corresponds to classifying all sentences as
neutral. The best performance is boldfaced. Let BL
stand for the best-performing baseline (BL-full or BL-
sum) for a combination of machine learner and lexicon.
We use Wilcoxon Signed-Rank test, reporting the num-
ber of signed ranks (N) and the sum of signed ranks (W).
Statistically significant results at p=0.05 are: Int-sum >
BL (N=10, W=43); Int-bin > BL (N=10, W=48); Int-
bin > Int-sum (N=10, W=43); Int-bin > Int-full (N=10,
W=47); Int-sum > Int-full (N=10, W=37); Int-bin > Int-
c (N=10, W=55); Int-sum > Int-c (N=10, W=55); Ex-
panded> Seed under Int condition (includes Int-full, Int-
sum, Int-bin, Int-c) (N=18, W=152, z=3.3). Differences
between Int-full, Int-c, and BL are not significant.
full or BL-sum) for a combination of machine
learner and lexicon. The results show that (1) Int-
bin > Int-sum > BL = Int-c = Int-full; (2) Ex-
panded > Seed under Int condition. All inequalities
are statistically significant at p=0.05 (see caption of
Table 4 for details).
First, both the seed and the expanded lexicons
benefit from profile enrichment, although, as pre-
dicted, the expanded lexicon yields larger gains due
to its more varied profiles: The seed lexicon gains up
to 15% in accuracy (c5.0 BL-sum vs Int-bin), while
the expanded lexicon gains up to 30%, as SVM RBF
scores go up from 0.495 to 0.644.
Second, observe that profiling allows the ex-
panded lexicon to leverage its improved coverage:
While it is inferior to the best baseline run with the
seed lexicon for all systems, it succeeds in impro-
ving the seed lexicon accuracies by 5%-12% across
the different systems for the Int-bin runs. The best
run of the expanded lexicon (Int-bin for SVM RBF)
improves upon the best run of the seed lexicon (Int-
sum for SVM-linear) by 7%, demonstrating the suc-
cess of the paraphraser-based expansion once pro-
files are taken into account. Overall, comparing the
best baseline for the seed lexicon with Int-bin con-
dition of the expanded lexicon, we observe an im-
provement between 5% (0.598 to 0.626 for Na??ve
Bayes) and 25% (0.512 to 0.641 for c5.0), proving
the effectiviness of the paraphrase-based expansion
with profile enrichment paradigm.
Third, representing profiles using 10 bins (Int-bin)
provides a small but consistent improvement over
the summary representation (Int-sum) that sums
positivity and negativity of the sentiment-bearing
words in a sentence, over a coarse-grained represen-
tation (Int-c), as well as over the full-information
representation (Int-full). Even Na??ve Bayes and
SVM linear, known to work well with large feature
sets, show better performance in the Int-bin con-
dition for the expanded lexicon. The results indi-
cate that an intermediate degree of detail ? between
summary-only and coarse-grained representation on
the one hand and full-information representation on
the other ? is the best choice in our setting.
104
7 Comparative Evaluations
In this section, we present comparative evaluations
of the work presented in this paper with respect to
related work. This section shows that the paraphrase
expansion+profile enrichment solution proposed in
this paper is effective for our task beyond off-the-
shelf solutions, and that its effectiveness generalizes
to sentiment analysis in a different domain. We also
show that profile enrichment can be effectively cou-
pled with other methods of lexical expansion, al-
though the paraphraser-based expansion receives a
larger boost in performance from profile enrichment
than the alternative expansion methods we consider.
In section 7.1, we demonstrate that the
paraphrase-based expansion and profile enrich-
ment yield superior performance on our data
relative to state-of-art subjectivity lexicons ? Opin-
ionFinder, General Inquirer, and SentiWordNet.
In section 7.2, we show that profile enrichment
can be effectively coupled with other methods
of lexical expansion, such as a WordNet-based
expansion and an expansion that utilizes Lin?s
distributional thesaurus. However, we find that the
paraphraser-based expansion benefits the most from
profile enrichment, and attains better performance
on our data than the alterantive expansion methods.
In section 7.3, we show that the paraphrase-based
expansion and profile enrichment paradigm is
effective for other subjecitivy lexicons on other
data. We use a dataset of product reviews annotated
for sentence-level positivity and negativity as
new data for evaluation (Hu and Liu, 2004). We
use subsets of OpinionFinder, General Inquirer,
and sentiment lexicon from Hu and Liu (2004).
We demonstrate that paraphrase-based expansion
and profile enrichment improve the accuracy of
sentiment classification of product reviews for
every lexicon and machine learner combination; the
magnitude of improvement is 5% on average.
7.1 Competitiveness of the Expanded Lexicon
Had we been able to use the OpinionFinder or
the General Inquirer lexicons (OFL and GIL) as-
is, how would the results have compared to those
attained using our lexicons? We performed the
baseline runs with both lexicons; OFL accuracies
were 0.544-0.594 across machine learning systems,
GIL?s ? 0.491-0.584 (see GIL column in Table 5).
We also experimented with using the weaksubj
and strongsubj labels in OFL as somewhat parallel
distinctions to the ones presented here (see sec-
tion 4 ? Related Work ? for a more detailed discus-
sion). We used (1,0,0) profile for strong positives,
(0.3,0,0.7) for weak positives, (0,1,0) for strong neg-
atives, and (0,0.3,0.7) for weak negatives, and ran all
the feature representations discussed in section 5.2.
Table 5 column OFL shows the best run for every
machine learning system, across the different feature
representations, and choosing the better performing
run between vanilla OFL and the version enriched
with weak/strong distinctions.
Machine Seed OFL GIL SWN Exp.
Learner BL
c5.0 0.512 0.598 0.491 0.516 0.641
SVM-RBF 0.527 0.594 0.495 0.520 0.644
SVM-lin. 0.584 0.594 0.580 0.569 0.630
Log. Reg. 0.545 0.598 0.541 0.537 0.616
Na??ve B. 0.598 0.573 0.584 0.587 0.626
Table 5: Performance of different lexicons on essay data
using various machine learning systems. For each sys-
tem and lexicon, the best performance across the applica-
ble feature representations from section 5.2 and the vari-
ants (see text) is shown. Seed BL column shows the best
baseline performance of our seed lexicon ? before para-
phraser expansion and profile enrichment were applied.
Exp. column shows the performance of Int-bin feature
representation for the expanded lexicon after profile en-
richment.
Additionally, we experimented with SentiWord-
Net (Baccianella et al, 2010). SentiWordNet is a
resource for opinion mining built on top of Word-
Net, which assigns each synset in WordNet a score
triplet (positive, negative, and objective), indicating
the strength of each of these three properties for the
words in the synset. The SentiWordNet annotations
were automatically generated, starting with a set of
manually labeled synsets. Currently, SentiWordNet
includes an automatic annotation for all the synsets
in WordNet, totaling more than 100,000 words. It
is therefore the largest-scale lexicon with intensity
information that is currently available.
Since SentiWordNet assigns scores to synsets and
since our data is not sense-tagged, we induced Sen-
105
tiWordNet scores in the following ways. We part-
of-speech tagged our train and test data using Stan-
ford tagger (Toutanova et al, 2003). Then, we took
the SentiWordNet scores for the top sense for the
given part-of-speech (SWN-1). In a different vari-
ant, we took a weighted average of the scores for the
different senses, using the weighting algorithm pro-
vided on SentiWordNet website6 (SWN-2). Table 5
column SWN shows the best performance figures
between SWN-1 and SWN-2, across the feature rep-
resentations in section 5.2.
The comparative results in Table 5 clearly show
that while our vanilla seed lexicon performs com-
parably to off-the-shelf lexicons on our data, the
paraphraser-expanded lexicon with sentitment pro-
files outperforms OpinionFinder, General Inquirer,
and SentiWordNet.
7.2 Sentiment Profile Enrichment with Other
Lexical Expansion Methods
We presented a novel lexicon expansion method
using a paraphrasing system. We also experimented
with more standard methods, using WordNet and
distributional similarity (Beigman Klebanov et al,
2012; Esuli and Sebastiani, 2006; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006; Hu and Liu,
2004; Kanayama and Nasukawa, 2006; Strapparava
and Valitutti, 2004; Kamps et al, 2004; Takamura
et al, 2005; Turney and Littman, 2003; Hatzivas-
siloglou and McKeown, 1997). Specifically, we im-
plemented a WordNet (Miller, 1995) based expan-
sion that uses the 3 most frequent synonyms of the
top sense of the seed word (WN-e). We also imple-
mented a method based on distributional similarity:
Using Lin?s proximity-based thesaurus (Lin, 1998)
trained on our in-house essay data as well as on well-
formed newswire texts, we took all words with the
proximity score > 1.80 to any of the seed lexicon
words (Lin-e). Just like the paraphraser lexicon,
both perform worse than the seed lexicon in 9 out
of 10 baseline runs (BL-sum and Bl-full conditions
for the 5 machine learners).
To test the effect of profile enrichment, all words
in WN-e and Lin-e underwent profile estimation as
described in section 3.1, yielding lexicons WN-e-p
and Lin-e-p, respectively. Figure 2 shows the distri-
6http://sentiwordnet.isti.cnr.it/, under ?Sample code.?
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 2: Sentiment profile distributions for Lin-e-p (left)
and WN-e-p (right) lexicons.
butions. WN-e-p and Lin-e-p exhibit similar trends
to those of the paraphraser. Substituting WN-e-p
for Expanded data in Table 4, we find the same re-
lationships between the different feature sets: Int-
bin>Int-sum>Int-full=BL. For Lin-e-p, Int-sum de-
teriorates: Int-bin>Int-sum=Int-full=BL. For the
20 runs in the Int condition, Paraphraser>WN-e-
p>Lin-e-p.7 Note that this is also the order of lexi-
con sizes: Lin-e is the most conservative expan-
sion (1,907 words), WN-e is the second with 2,527
words, and the lexicon expanded using paraphrasing
is the largest with 2,994 words. Table 6 shows the
performance of Lin-e-p, WN-e-p, and of the Ex-
panded lexicon from Table 4 using the Int-bin fea-
ture representation. The average relative improve-
ments over the best baseline range between 6.6% to
14.6% for the different expansion methods.
Profile induction appears to be a powerful lexicon
clean-up procedure that works especially well with
more aggressive and thus potentially noisier expan-
sions: The machine learners depress low-intensity
and ambiguous expansions, thereby allowing the
effective utilization of the improved coverage of
sentiment-bearing vocabulary.
7.3 Effectiveness of the Paraphrase Expansion
with Profile Enrichment Paradigm in a
Different Domain
In order to check whether the paraphrase-based ex-
pasion and profile enrichment paradigm discussed in
this paper generalizes to other subjectivity lexicons
7All > are signficant at p=0.05 using Wilcoxon test.
106
Machine Seed Lin-e-p WN-e-p Exp.
Learner BL
c5.0 0.512 0.584 0.616 0.641
SVM-RBF 0.527 0.598 0.601 0.644
SVM-lin. 0.584 0.577 0.569 0.630
Log. Reg. 0.545 0.587 0.580 0.616
Na??ve B. 0.598 0.591 0.623 0.626
Av. Gain 0.066 0.085 0.146
Table 6: Performance of WordNet-based, Lin-based, and
Paraphraser-based expansions with profile enrichment in
the Int-bin condition. Seed BL column shows the best
baseline performance of the seed lexicon ? before expan-
sion and profile enrichment were applied. The last line
shows the average relative gain over the best baseline
calculated as AGlex = ?m?M Lexm?SeedBLmSeedBLm , where
M = {c5.0, SVM-RBF, SVM-linear, Logistic Regres-
sion, Na??ve Bayes}, and lex ? {Lin-e-p, Wn-e-p, Exp}.
and domains of application, we experimented with
a product reviews dataset (Hu and Liu, 2004) and
additional lexicons as follows.
7.3.1 Lexicons
We use the OpinionFinder and General Inquirer
lexicons (OFL and GIL) as before, as well as
the lexicon of positive and negative sentiment and
opinion words available along with (Hu and Liu,
2004) product reviews dataset ? HL.8
Since each of these lexicons contains more than
3,000 words, enrichment of the full lexicons with
profiles is beyond the financial scope of our project.
We therefore restrict each of the lexicons to the size
of their overlap with our seed lexicon (see 2.1); the
overlaps have between 415 and 467 words. These re-
stricted lexicons are our initial lexicons for the new
experiment that parallel the role of the seed lexicon
in the experiments on essay data.
For each of the 3 initial lexicons L, L?{OFL,
GIL, HL}, we follow the paraphrase-based expan-
sion as described in section 2.2. This results in about
4.5-fold expansion of each lexicon, the new lexi-
cons L-e, L?{OFL, GIL, HL}, numbering between
2,015 and 2,167 words. Both the initial and the ex-
panded lexicons now undergo profile enrichment as
described in section 3.1, producing lexicons L-p and
8http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#lexicon
L-e-p, L?{OFL, GIL, HL}.
7.3.2 Data
We use the dataset from Hu and Liu (2004)9 that
contains reviews of 5 products from amazon.com:
two digital cameras, a DVD player, an MP3 player,
and a cellular phone. The reviews are annotated at
sentence level with a label that desrcibes the par-
ticular feature that is the subject of the positive or
negative evaluation and the polarity and extent of
the evaluation. For example, the sentence ?The
phone book is very user-friendly and the speaker-
phone is excellent? is labeled as PHONE BOOK[+2],
SPEAKERPHONE[+2], while the sentence ?I am
bored with the silver look? is labeled LOOK[?1]. We
used all sentences that were labeled with a numeri-
cal score for at least one feature, removing a small
number of sentences labeled with both positive and
negative scores for different features.10 We used the
sign of the numerical score to label the sentences as
positive or negative. The resulting dataset consists
of 1,695 sentences, 1,061 positive and 634 nega-
tive; accuracy for a majority baseline on this dataset
is 0.626. Our experiments on this dataset are done
using 5-fold cross-validation.
7.3.3 Results
Table 7 shows classification accuracies for the
product review data using different lexicons and ma-
chine learners. We observe that the combination of
paraphrase-based expansion and profile enrichment
(L-e-p column in the table) resulted in an improved
performance over the initial lexicon (L column in
the table) in all cases, with the average gain of 5%
in accuracy.
Furthermore, the contributions of the expansion
and the profile enrichment are complementary, since
their combination performs better than each in iso-
lation. We note that profile enrichment alone for the
initial lexicon did not yield an improvement. This
can be explained by the fact that the initial lexicons
are highly polar, so profiles provide little additional
information: The percentage of words with p?pos ?
0.8 or p?neg ? 0.8 is 84%, 86% and 91% for GIL,
9http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#datasets, the link under ?Customer Review
Datasets (5 products)?
10such as ?The headset that comes with the phone has good
sound volume but it hurts the ears like you cannot imagine!?
107
Machine Lexicon Variant
Learner L L-p L-e L-e-p
L = OFL?Seed, |L|=467, |L-e|=2,167
c5.0 0.663 0.670 0.691 0.704
SVM-RBF 0.668 0.676 0.693 0.714
SVM-lin. 0.675 0.670 0.688 0.696
Log. Reg. 0.666 0.658 0.693 0.698
Na??ve B. 0.668 0.668 0.686 0.695
L = GIL?Seed, |L|=415,|L-e|=2,015
c5.0 0.644 0.658 0.663 0.686
SVM-RBF 0.650 0.665 0.653 0.683
SVM-lin. 0.665 0.665 0.677 0.681
Log. Reg. 0.664 0.658 0.678 0.694
Na??ve B. 0.669 0.666 0.678 0.703
L = HL?Seed, |L|=434, |L-e|=2,054
c5.0 0.676 0.675 0.689 0.706
SVM-RBF 0.673 0.674 0.700 0.713
SVM-lin. 0.676 0.664 0.703 0.710
Log. Reg. 0.668 0.661 0.703 0.699
Na??ve B. 0.668 0.672 0.697 0.697
Table 7: Accuracies on product review data. For each ma-
chine learner and lexicon, the best baseline performance
is shown as L for the initial lexicon and as L-e for the
paraphrase-expanded lexicon. L-p and L-e-p show the
performance of Int-bin feature set on the profile-enriched
initial and paraphrase-expanded lexicons, respectively.
The three initial lexicons L are OpinionFinder (OFL),
General Inquirer (GIL), and (Hu and Liu, 2004) (HL),
each intersected with our seed lexicon. Sizes of the intial
and expanded lexicons are provided.
OFL, and HL-derived lexicons, respectively. In con-
trast, for the expanded lexicons, these percentages
are 51%, 53%, and 56%; these lexicons benefit from
profile enrichment.
8 Conclusions
We demonstrated a method of improving a seed sen-
timent lexicon by using a pivot-based paraphrasing
system for lexical expansion and sentiment profile
enrichment using crowdsourcing. Profile enrich-
ment alone yielded up to 15% improvement in the
performance of the seed lexicon on the task of 3-
way sentence-level sentiment polarity classification
of test-taker essay data. While the lexical expansion
on its own failed to improve upon the performance
of the seed lexicon, it became much more effective
on top of sentiment profiles, generating a 7% perfor-
mance boost over the best profile-enriched run with
the seed lexicon. Overall, paraphrase-based expan-
sion coupled with profile enrichment yields an up to
25% improvement in accuracy.
Additionally, we showed that our paraphrase-
expanded and profile-enriched lexicon performs
significantly better on our data than off-the-shelf
subjectivity lexicons, namely, Opinion Finder, Gen-
eral Inquirer, and SentiWordNet. Furthermore, our
results suggest that paraphrase-based expansion de-
rives more benefit from profiles than two competing
expansion mechanisms based on WordNet and on
Lin?s distributional thesaurus.
Finally, we demonstrated the effectiveness of the
paraphraser-based expansion with profile enrich-
ment paradigm on a different dataset. We used Hu
and Liu (2004) product review data with sentence-
level sentiment polarity labels. Paraphrase-based
expansion with profile enrichment yielded an im-
proved performance across all lexicons and machine
learning algorithms we tried, with an average im-
provement rate of 5% in classification accuracy.
Recent literature argues that sentiment polarity
is a property of word senses, rather than of words
(Gyamfi et al, 2009; Su and Markert, 2008; Wiebe
and Mihalcea, 2006), although Dragut et al(2012)
successfully operate with ?mostly negative? and
?mostly positive? words based on the polarity distri-
butions of word senses. We plan to address in future
work sense disambiguation for words that have mul-
tiple senses with very different sentiment, such as
stress, as either anxiety (negative) or emphasis (neu-
tral).
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion of WordNet glosses. In Proceedings of EACL,
pages 209?216, Trento, Italy.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An Enhanced
Lexical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of LREC, pages 2200?2204,
Malta.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604, Ann Arbor, MI.
108
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Build-
ing sentiment lexicon(s) from scratch for essay data.
In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), New Delhi, India, March.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lexi-
cal resources for opinion mining. In Andrea Sanso,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, pages 200?210.
Franco Angeli Editore, Milano, IT.
Ferm??n L. Cruz, Jose? A. Troyano, F. Javier Ortega, and
Fernando Enr??quez. 2011. Automatic expansion
of feature-level opinion lexicons. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis, pages 125?131,
Portland, Oregon, June.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency check-
ing for sentiment dictionaries. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
997?1005, Jeju Island, Korea, July. Association for
Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing term subjectivity and term orientation for opinion
mining. In Proceedings of EACL, pages 193?200,
Trento, Italy.
Leo A. Goodman. 1965. On Simultaneous Confidence
Intervals for Multinomial Proportions. Technometrics,
7(2):247?254.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectivity
sense labeling. In Proceedings of NAACL, pages 10?
18, Boulder, CO.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181, Madrid,
Spain.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177,
Seattle, WA.
Valentin Jijkoun and Katja Hofmann. 2009. Gener-
ating a Non-English Subjectivity Lexicon: Relations
That Matter. In Proceedings of EACL, pages 398?405,
Athens, Greece.
Jaap Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to measure
semantic orientation of adjectives. In Proceedings of
LREC, pages 1115?1118, Lisbon, Portugal.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In Proceedings of EMNLP, pages
355?363, Syndey, Australia.
Soo-Min Kim and Edward Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING,
pages 1367?1373, Geneva, Switzerland.
Philip Koehn. 2005. EUROPARL: A Parallel corpus for
Statistical Machine Translation. In Proceedings of the
Machine Translation Summit.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL, pages 768?774,
Montreal, Canada.
Tim Loughran and Bill McDonald. 2011. When is a Li-
ability not a Liability? Textual Analysis, Dictionaries,
and 10-Ks. Journal of Finance, 66:35?65.
Nitin Madnani and Bonnie Dorr. 2013. Generating Tar-
geted Paraphrases for Improved Translation. ACM
Transactions on Intelligent Systems and Technology, to
appear.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL, pages
976?983, Prague, Czech Republic.
George Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38:39?41.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of EMNLP, pages 599?608, Singapore,
August.
Guillaume Pitel and Gregory Grefenstette. 2008. Semi-
automatic building method for a multidimensional af-
fect dictionary for a new language. In Proceedings of
LREC, Marrakech, Morocco.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204.
C. Quesenberry and D. Hurst. 1964. Large sample si-
multaneous confidence intervals for multinomial pro-
portions. Technometrics, 6:191?195.
J. R. Quinlan. 1993. C4.5: Programs for machine lear-
ning. Morgan Kaufmann Publishers.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of EACL, pages 675?682, Athens.
109
Philip Stone, Dexter Dunphy, Marshall Smith, and Daniel
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-affect: an affective extension of WordNet.
In Proceedings of LREC, pages 1083?1086, Lisbon,
Portugal.
Fangzhong Su and Katja Markert. 2008. Eliciting
Subjectivity and Polarity Judgements on Word Senses.
In Proceedings of COLING, pages 825?832, Manch-
ester, UK.
P. Subasic and A. Huettner. 2001. Affect analysis of text
using fuzzy semantic typing. IEEE Transactions on
Fuzzy Systems, 9(4).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-Based
Method for Sentiment Analysis. Computational Lin-
guistics, 37(2):267?307.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
spin model. In Proceedings of ACL, pages 133?140,
Ann Arbor, MI.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252?259.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21(4):315346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of
Web-derived polarity lexicons. In Proceedings of
NAACL, pages 777?785, Los Angeles, CA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of ACL, pages 1065?
1072, Sydney, Australia.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLING (invited pa-
per), pages 486?497, Mexico City.
110
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), page 138,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
The Far Reach of Multiword Expressions in Educational Technology
Jill Burstein
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541 USA
JBurstein@ets.org
Abstract
Multiword expressions as they appear as nominal
compounds, collocational forms, and idioms are
now leveraged in educational technology in assess-
ment and instruction contexts. The talk will focus on
how multiword expression identification is used in
different kinds of educational applications, includ-
ing automated essay evaluation, and teacher pro-
fessional development in curriculum development
for English language learners. Recent approaches
developed to resolve polarity for noun-noun com-
pounds in a sentiment system being designed to han-
dle evaluation of argumentation (sentiment) in test-
taker writing (Beigman-Klebanov, Burstein, and
Madnani, to appear) will also be described.
About the Speaker
Jill Burstein is a managing principal research scien-
tist in the Research & Development division at Ed-
ucational Testing Service in Princeton, New Jersey.
Her background and expertise is in computational
linguistics with a focus on educational applications
for writing, reading, and teacher professional devel-
opment. She holds 13 patents for educational tech-
nology inventions. Jills inventions include e-rater,
an automated essay scoring and evaluation system.
And, in more recent work, she has leveraged natural
language processing to develop Language MuseSM,
a teacher professional development application that
supports teachers in the development of language-
based instruction that aids English learner content
understanding and language skills development. She
received her B.A. in Linguistics and Spanish from
New York University, and her M.A. and Ph.D. in
Linguistics from the Graduate Center, City Univer-
sity of New York.
References
Beigman Klebanov, B., Burstein, J., and Madnani, N. (to
appear) Sentiment Profiles of Multi-Word Expressions
in Test-Taker Essays: The Case of Noun-Noun Com-
pounds. In V. Kardoni, C. Ramisch, and A. Villavi-
cencio (eds.) ACM Transactions for Speech and Lan-
guage Processing, Special Issue on Multiword Expres-
sions: From Theory to Practice.
138
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 1?10,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
 
A User Study: Technology to Increase Teachers? Linguistic Awareness  
to Improve Instructional Language Support  
for English Language Learners 
 
 
Jill Burstein, John Sabatini, Jane Shore, Brad Moulder, and Jennifer Lentini 
 
Educational Testing Service 
666 Rosedale Road, Princeton, New Jersey 08541 
{jburstein, jsabatini, jshore, bmoulder, jlentini}@ets.org 
 
 
 
Abstract 
This paper discusses user study outcomes with 
teachers who used Language MuseSM a web-
based teacher professional development (TPD) 
application designed to enhance teachers? lin-
guistic awareness, and support teachers in the 
development of language-based instructional 
scaffolding (support) for their English language 
learners (ELL). System development was 
grounded in literature that supports the notion 
that instruction incorporating language support 
for ELLs can improve their accessibility to 
content-area classroom texts ?in terms of ac-
cess to content, and improvement of language 
skills. Measurement outcomes of user piloting 
with teachers in a TPD setting indicated that 
application use increased teachers' linguistic 
knowledge and awareness, and their ability to 
develop appropriate language-based instruction 
for ELLs. Instruction developed during the pi-
lot was informed by the application?s linguistic 
analysis feedback, provided by natural lan-
guage processing capabilities in Language 
Muse. 
1 Introduction 
Statistics show that between 1997 and 2009 the 
number of ELLs enrolled in U.S. public schools 
has increased by 51% (National Clearinghouse for 
Language Acquisition, 2011). ELLs who have 
lower literacy skills, and who are reading below 
grade level may be mainstreamed into regular con-
tent-area classrooms, and may not receive supple-
mental English language instruction. 
Unfortunately, K-12 content-area teachers1 are less 
likely to be trained to adapt their instructional ap-
proaches to accommodate the diverse cultural and 
linguistic backgrounds of students with varying 
levels of English proficiency (Adger, Snow, & 
Christian, 2002; Calder?n, August, Slavin, Cheun, 
Dur?n, & Madden, 2005; Rivera, Moughamian, 
Lesaux, & Francis, 2008; Walqui & Heritage, 
2012). This situation motivated the development 
of Language MuseSM, a web-based application de-
signed to offer teacher professional development 
(TPD) for content-area teachers to support their 
understanding of potential sources of linguistic 
unfamiliarity that may obscure text content for 
ELLs, and their ability to develop relevant lan-
guage-based instructional scaffolding. We rea-
soned that prerequisite to effectively planning or 
implementing instructional supports for ELLs, 
teachers first needed to be able to recognize poten-
tial sources of linguistic difficulty. Further, teach-
ers might need training about the specific 
linguistic structures that might be unfamiliar to 
learners, and which might lead to learners? inac-
cessibility to core content in text.  
    The motivation for Language Muse, thus, grew 
from the need to provide teachers with training 
about linguistic features in texts that may be un-
familiar to learners. In complement to training 
videos and reading resources, Language Muse 
contains a module that provides automated and 
explicit linguistic feedback for texts, and is intend-
                                                          
1 These are Kindergarten-12th grade teachers of subject areas, 
including math, science, social studies, and English language 
arts. 
1
 
 
ed to support teachers in the development of les-
son plans with language-based instructional activi-
ties and assessments to support reading and 
content comprehension of texts. The linguistic 
feedback module uses various natural language 
processing methods to provide feedback at the vo-
cabulary, phrasal, sentential, and discourse levels. 
Another motivation of application was efficiency. 
Even with a strong linguistic awareness, manual 
identification of linguistic features would be a 
very time-consuming process. 
   Outcomes from pre-post teacher assessments 
delivered through user piloting with teachers indi-
cated that teachers who used Language Muse 
showed gains in linguistic knowledge. Outcomes 
also indicated that Language Muse use supported 
teachers in the ability to develop appropriate lan-
guage-based instruction for ELLs, informed by the 
application?s linguistic analysis feedback.  
2 Related Work 
In a brief literature review, we address the lan-
guage demands for ELLs in reading content-area 
texts, and the need for relevant teacher training for 
content-area teachers (Section 2.1).  We also dis-
cuss NLP-related applications that support the lin-
guistic analysis of texts -- typically in the context 
of developing readability measures -- which con-
tinues to be a prominent area of research; other 
research supports student tools allowing direct 
interaction with language forms (Section 2.2).  
 
2.1 Language Demands on ELLs, and 
Teacher Training 
 
Language Demands on ELLs. The English Lan-
guage Arts Common Core State Standards2 
(Standards) (NGA Center & CCSSO, 2010) has 
now been adopted by 46 states and is a trend-setter 
in U.S. education. The Standards emphasize the 
need for all learners (including ELLs3) to read 
progressively more complex texts across multiple 
genres in the content areas, preparing learners for 
college and careers. To accomplish this, learners 
must have familiarity with numerous linguistic 
features related to vocabulary, English language 
                                                          
2 http://www.corestandards.org/ 
3 For details and about Standards and ELLs, see: 
http://ell.stanford.edu/. 
structures, and a variety of text structures (dis-
course).  
    In terms of vocabulary demands, research re-
ports on investigations of academic vocabulary 
and the Tier word system (Beck, McKeown, & 
Kucan, 2008; Calder?n, 2007). Specifically, Tier 1 
words are those used in everyday conversation; 
Tier 2 words are general academic words; and Tier 
3 words are found in specific domains (Beck et al 
2008; Coleman & Pimental, 2011a).  All three Ti-
ers are necessary to academic content learning.  
Key content-area terms in any text would include 
the vocabulary that students are expected to learn 
regardless of the Tier. However, there are many 
other vocabulary terms in the same text that may 
or may not be key content, but may still pose diffi-
culties for an ELL reader.  For instance, the phrase 
?rock star? is a figurative term whose meaning is 
not obvious from knowing the various meanings 
of ?rock? or ?star?.  A deficit in morphological 
awareness can be a source of reading comprehen-
sion difficulties among native speakers of English, 
(Berninger, Abbott, Nagy, & Carlisle, 2009; Nagy, 
Berninger, & Abbot, 2006), but even more so 
among ELLs (Carlo, August, McLaughlin, Snow, 
Dressler, Lippmann, & White, 2004; Kieffer & 
Lesaux, 2008). Teaching morphological structure 
has been shown to be effective with ELLs 
(Lesaux, Kieffer, Faller, & Kelley, 2010; Proctor, 
Dalton, Uccelli, Biancarosa, Snow, & Neugebauer, 
2011). Native language support can also aid stu-
dents in learning text-based content (Francis, Au-
gust, Goldenberg, & Shanahan, 2004). 
Specifically, lessons that incorporate cognates 
(e.g., individual (English) and individuo (Spanish)) 
have been found to be effective in expanding Eng-
lish vocabulary development and aiding in com-
prehension (August, 2003; Proctor, Dalton, & 
Grisham, 2007).  Polysemous words can contribute 
to overall text difficulty.  Papamihiel, Lake & Rice 
(2005) specifically discuss difficulties of content-
specific, polysemous words, where the more 
common meaning may lead to a misconception 
when using that meaning to infer the more specific 
content meaning (e.g., prime in prime numbers). 
Unfamiliar cultural references (e.g., He?s a mem-
ber of the Senate.), when reading an unfamiliar 
language to learn unfamiliar content, imposes a 
triple cognitive load for ELLs (Goldenberg, 2008). 
    With regard to sentence-level demands, long, 
multi-clause sentences can present frustrating 
2
 
 
complexities. Readers need to analyze sentence 
clauses to understand and encode key information 
in working memory as they build a coherent men-
tal model of the meaning of a text (Kintsch, 1998).  
Different subject areas often have sentential and 
phrasal structures that are unique to that subject, 
resulting in comprehension breakdowns, e.g., the 
noun phrases in math texts ?a number which can 
be divided by itself ?? (Schleppegrell, 2007; 
Schleppegrell & de Oliveira, 2006).   
    Regarding discourse structure demands, con-
tent-area texts may represent varying discourse 
relationships. Discourse relations such as, com-
pare-contrast, cause-effect can all be intermingled 
within a single passage (Goldman & Rakestraw, 
2000; Meyer, 2003). Teachers need to learn how 
to identify discourse-level information and devel-
op scaffolding to support students? ability to navi-
gate discourse elements in texts. Students may also 
be challenged in keeping track of and resolving 
referential (anaphoric) relationships. Pronomial 
reference can be a challenge for ELLs in texts 
with multiple characters or agents (Kral, 2004). 
An equal challenge concerns the resolution of ref-
erential relations among nouns, phrases, or ideas - 
a common occurrence in expository texts- whether 
the category of reference is pronominal, synony-
my, paraphrase, or determiner, e.g., this, that, or 
those (Pretorius, 2005). Also critical to learning 
new content is understanding connector words 
functions (e.g., because, therefore) for building 
text cohesion (Goldman & Murray, 1992; 
Graesser, McNamara, & Louwerse, 2003).   
    Teacher Training. Teachers need to become lin-
guistically aware of aspects of the English lan-
guage that present potential obstacles to content 
access for ELLs. Yet, teachers often lack training 
in the identification of features of English that may 
challenge diverse groups of ELLs (Adger et al, 
2002; Calder?n et al, 2005; Rivera et al, 2008; 
Walqui & Heritage, 2012), and in the implementa-
tion of strategies to help ELLs academic language 
and vocabulary acquisition (Flinspach, Scott, Mil-
ler, Samway, & Vevea, 2008).  Further, the num-
ber of teachers trained in effective instructional 
strategies to meet the range of needs of ELLs has 
not increased consistently with the rate of the ELL 
population (G?ndara, Maxwell-Jolly, & Driscoll, 
2005; Green, Foote, Walker & Shuman, 2010). 
Studies suggest that teachers with specialized 
training have a positive impact on student perfor-
mance (Darling-Hammond, 2000; Peske & Hay-
cock, 2006). 
 
2.2 Text Accessibility and NLP 
 
Considerable research in NLP and text 
accessibility has focussed on linguistic properties 
of text that render a text relatively more or less 
accessible (comprehensible). This research stream 
has often fed into applications offering readability 
measures ? specifically, measures that predict the 
grade level, or grade range of a text (e.g., 
elementary, middle or high-school). Foundational 
research in this area examined the effect of  
morphological and syntactic text properties. Flesch 
(1948) reported that text features such as syllable 
counts of words, and sentence length were 
predictors of text difficulty.  Newer research in 
this area has included increasingly more NLP-
based investigations (Collins-Thompson & Callan, 
2004; Schwarm & Ostendorf, 2005; Miltsakaki, 
2009). Some research examines text quality in 
terms of discourse coherence of  well-formed texts 
(Barzilay & Lapata, 2008; Pitler & Nenkova, 
2008; Graesser, McNamara, & Kulikowich, 
2011).   
    Human evaluation of text complexity in curricu-
lum materials development (i.e., adaptation and 
scaffolding of reading texts, and the creation of 
activities and assessments) is a time-consuming, 
and typically intuitive process. Determining text 
complexity is also not a clear and objective meas-
ure. For example, what is complex for a native 
English speaker reading on grade level may vary 
from what is complex (or unfamiliar) for an ELL 
reading below grade level. This area of research 
continues to grow as is evidenced by NLP shared 
tasks (Mihalcea, Sinha & McCarthy, 2010), in the 
research and educational measurement communi-
ties (Burstein, Sabatini, and Shore, in press; Nel-
son, Perfetti, Liben & Liben, 2012).  
    The REAP system uses statistical language 
modeling to assign readability measures to Web 
documents (Collins-Thompson & Callan, 2004). 
This system is used in college-level ESL class-
rooms for higher level ESL students. It is designed 
to support automatic selection and delivery of ap-
propriate and authentic texts to students in an in-
structional setting (Heilman, Zhao, Pino, & 
Eskenazi, 2008). Teacher users can set a number 
of constraints (e.g., reading level, text length, and 
3
 
 
target vocabulary) to direct the text search.  The 
system then automatically performs the text selec-
tion.  The system also has tools that allow English 
learners to work with the text, including dictionary 
definition access and vocabulary practice exercis-
es. In pilot studies with high-intermediate learners 
in a university setting, a post-test showed promis-
ing learning outcomes (Heilman et al 2008). 
    WERTi (Working with English Real Texts in-
teractively) (Meurers et al, 2010) is an innovative 
Computer-Assisted Language Learning (CALL) 
tool that allows learners to interact directly with 
NLP outputs related to specific linguistic forms. In 
the context of a standard search environment, 
learners can select texts from the web. NLP meth-
ods are applied to identify linguistic forms that are 
often problematic for ELLs, including, use of de-
terminers and prepositions, wh-question formation, 
and phrasal verbs in the texts. Meurers et al point 
out that this CALL method is intended to draw 
learners? attention to specific properties of a lan-
guage (Rutherford and Sharwood Smith , 1985). 
ELLs? direct interaction with different linguistic 
forms could support them in language skills de-
velopment, and content accessibility.  
    To our knowledge, Language Muse is unique 
from other NLP applications in that it is designed 
as a teacher professional development (TPD) ap-
plication intended to enhance teachers? linguistic 
awareness, and as a result, aid teachers in the de-
velopment of language-based scaffolding to sup-
port learners? content accessibility, and language 
skills development. Key text complexity drivers 
cannot be communicated to teachers through nu-
merical aggregate readability measures which ap-
pear to be the predominant approach to analysis of 
text difficulty described in the literature. Lan-
guage Muse fills a critical TPD gap.  The appli-
cation is an innovative resource designed to help 
teachers understand the specific linguistic features 
that may contribute to text difficulty and ELLs? 
inaccessibility to text content; linguistic feedback 
features in SYSTEM are grounded in the literature 
about ELL language demands (Section 2.1). 
3 Language Muse  
Language Muse is a web-based application for 
enhancing teachers? linguistic awareness and sup-
porting the development of language-based in-
struction for ELLs. It uses NLP methods to 
provide explicit linguistic feedback that is ground-
ed in the literature discussing ELL language de-
mands and needs (Section 2.1).      
  We will discuss (a) the system?s specific lesson 
planning components, and (b) a text exploration 
tool that provides automated linguistic feedback. 
    The lesson planning component has three mod-
ules that support the creation of lesson plans, and 
related activities and assessments. To create a les-
son plan, teachers complete a lesson plan template 
(provided by the system) with five sections com-
monly found in lesson plans: (a) standards and 
objectives, (b) formative and summative assess-
ments, (c) engaging student interest/connecting to 
student background  knowledge, (d) modeling and 
guided practice, and (e) independent practice. 
Teachers use system functionality to link specific 
texts to a lesson plan. Texts have typically been 
analyzed, first, using the feedback tool. Feedback 
is then used to inform lesson plan development. 
Activities and assessments may also be created for 
a specific lesson plan and will also be linked to the 
plan.  Teachers are instructed to use linguistic 
feedback from the tool to develop language-
focused activities and assessments that can be used 
to    support the language objectives proposed in 
the lesson plan.      The Text Explorer & Adapter 
(TEA-Tool) feedback module uses NLP methods 
for automatic summarization (Marcu, 1999); Eng-
lish-to-Spanish machine translation (SDL n.d.); 
and, linguistic feedback. A text4, or a webpage 
with the relevant text is uploaded, or accessed, 
respectively, into the TEA-Tool module. The 
summarization capability may be used to reduce 
the amount of text that learners are exposed to re-
duce cognitive load. The machine translation ca-
pability can be used to offer native language 
support to learners with little English proficiency.   
The primary focus in this section, however, will 
center around the linguistic feedback that supports 
the core goal of building teachers? awareness of 
specific linguistic features in texts. The linguistic 
feedback includes specific information about vo-
cabulary, phrasal and sentence complexity, and 
discourse relations.  For vocabulary5, categories of 
feedback include: academic words, cognates, col-
locations and figurative words and terms, cultural 
                                                          
4 Microsoft Word, PDF, and Plain text files may be used. 
5 For academic words, cognates, cultural references, and 
homonyms, customized word lists are used. No NLP is used 
in these cases. 
4
 
 
references, morphological analysis, homonyms 
(e.g., their, there, and they?re), key content words, 
and similes6. For phrasal and sentential complexi-
ty, complex verb and noun phrases, sentences with 
one or more dependent clauses, and passive sen-
tences. For discourse, cause-effect, compare-
contrast, evidence and details, opinion, persuasion, 
and summary relations.  
     The remainder of this section describes features 
in the TEA-Tool module that use NLP to generate 
linguistic feedback. Providing individual evalua-
tion descriptions for each NLP feature is beyond 
the scope of this paper7, intended to focus on user 
study outcomes associated with Language Muse 
use (Section 4).  
    The specific vocabulary (lexical) features that 
use NLP methods or resources include these op-
tions8: basic and challenge synonyms, complex and 
irregular word forms, variant word forms, and 
multiple word expressions.  As discussed earlier, 
unfamiliar vocabulary is recognized as a big con-
tributor to text inaccessibility. The Basic Synonym 
and Challenge Synonym features support the vo-
cabulary comprehension and vocabulary building 
aspects, respectively. To generate the greatest 
breadth of synonyms, the tool uses a distributional 
thesaurus (Lin, 1998), WordNet (Miller, 1995) and 
a paraphrase generation tool (Dorr and Madnani, 
to appear). Previous research has evaluated using 
these combined resources with relevant constraints 
to prevent too many false positives (Burstein and 
Pedersen, 2010).  An additional slider feature al-
lows users to adjust the number of words for 
which the tool will return synonyms for existing 
words in the text. Outputs are based on word fre-
quency. Frequencies are determined using a stand-
ard frequency index (Breland, Jones, and Jenkins, 
1994). If users want synonyms for a larger number 
of words across a broader frequency range that 
includes lower (more rare words) and higher 
(more common words) frequency words, then they 
move the slider further to the right. To retrieve 
synonyms for fewer and rarer words, the slider is 
moved to the left. For all words in the text that are 
within the range of word frequencies at the partic-
ular point on the slider, the tool returns synonyms.  
If users select Basic Synonyms, the tool returns all 
                                                          
6 This new feature was not available during the pilot study. 
7 For details, see Burstein, Sabatini, Shore, Moulder, 
Holtzman & Pedersen (2012). 
8 These reflect the feature names in TEA-Tool. 
words with equivalent or higher frequencies than 
the word in the text. In theory, these words should 
be more common words that support basic com-
prehension. If users select Challenge Synonyms, 
then the tool returns all words with equivalent or 
lower frequencies than the word in the text. In this 
case, the teacher might want to work on vocabu-
lary building skills to help the learner with new 
vocabulary. If the user  selects both the Basic Syn-
onyms and Challenge Synonyms features, then the 
tool will output the  full list of basic (more famil-
iar), and challenge (less familiar) synonyms for 
words in the text.  The teacher can use these syno-
nyms to modify the text directly, or to develop 
instructional activities to support word learning.   
The Complex and Irregular Word Forms and Var-
iant Word Forms feature offers feedback related to 
morphological form. A morphological analyzer 
originally evaluated for an automated short-answer 
scoring system (Leacock & Chodorow, 2003) is 
used. This analyzer handles derivational and in-
flectional morphology. Feedback can be used for 
instructional scaffolding that includes discussion 
and activities related to morphological structure is 
an effective method to build ELLs? vocabulary. 
There are two features that identify words with 
morphological complexity, specifically, words 
with prefixes or suffixes: (1) Complex and Irregu-
lar Word Forms and (2) Variant Word Forms. For 
(1), the morphological analyzer identifies words 
that are morphologically complex. A rollover is 
available for these words. Users can place their 
cursor over the highlighted word, and the word 
stem is shown (e.g., lost ? stem: lose). For (2), the 
system underlines words with the same stem that 
have different parts of speech, such as poles and 
polar. Teachers can build instruction related to this 
kind of morphological variation and teach students 
about variation and relationships to parts of 
speech.   
  Multiple word expressions (MWE) may include 
idioms (e.g., body and soul), phrasal verbs (e.g., 
reach into), and MWEs that are not necessarily 
idiomatic, but typically appear together (colloca-
tions) to express a single meaningful concept (e.g., 
heart disease). All of these MWE types may be 
unfamiliar terms to ELLs, and so they may inter-
fere with content comprehension. Teachers can get 
feedback identifying MWEs to design relevant 
scaffolding for a text. To identify MWEs, two re-
sources are used.  The WordNet 3.0 compounds 
5
 
 
list of approximately 65,000 collocational terms is 
used in combination with a collocation tool that 
was designed to identify collocations in test-taker 
essays (Futagi, Deane, Chodorow, & Tetreault, 
2008). Some terms in the WordNet list are com-
plementary to what is found by the collocation 
tool.  We have found that both outputs are useful. 
Futagi et al?s collocation tool identifies colloca-
tions in a text that occur in seven syntactic struc-
tures that are the most common structures for 
collocations in English based on The BBI Combi-
natory Dictionary of English (Benson, Benson, & 
Ilson, 1997). For instance, these include Noun of 
Noun (e.g., swarm of bees), and Adjective + Noun 
(e.g., strong tea), and Noun + Noun (e.g., house 
arrest). See Futagi et al (2008) for further details.   
    Complex phrasal or sentential features can in-
troduce potential difficulty in a text. A rule-based 
NLP module is used to identify all of these fea-
tures using a shallow parser that had been previ-
ously evaluated for prepositional phrase and noun 
phrase detection (Leacock & Chodorow, 2003). 
The module to identify passive sentence construc-
tion had been previously evaluated for commercial 
use (Burstein, Chodorow, & Leacock, 2004). The 
following feedback features can be selected: Long 
Prepositional Phrases, which identifies sequences 
of two or more consecutive prepositional phrases 
(e.g., He moved the dishes from the table to the 
sink in the kitchen.); Complex Noun Phrases, 
which shows noun compounds composed of two 
or more nouns (e.g., emergency management 
agency) and noun phrases (e.g., shark-infested wa-
ters); Passives, which indicate passive sentence 
constructions (e.g., The book was bought by the 
boy.); 1+Clauses, which highlights sentences with 
at least one dependent clause (e.g., The newspaper 
indicated that there are no weather advisories.); 
and Complex Verbs, which identifies verbs with 
multiple verbal constituents (e.g., would have 
gone, will be leaving, had not eaten). 
       With regard to discourse transition features, 
discourse-relevant cue words and terms are  
highlighted when the following discourse transi-
tions features are identified, including: Evidence 
& Details, Compare-Contrast, Summary, Opinion, 
Persuasion, and Cause-Effect.  A discourse ana-
lyzer previously evaluated for a commercial auto-
mated scoring application is used (Burstein, 
Kukich, Wolff, Lu, Chodorow, Braden-Harder, & 
Harris, 1998). The system identifies cue words and 
phrases in text that are being used as specific dis-
course (or rhetorical) contexts. For instance, ?be-
cause? is typically associated with a cause-effect 
relation. However, some words need to appear in a 
specific syntactic construction to function as a dis-
course term. For instance, the word first functions 
as an adjective modifier and not a discourse term 
in a phrase, e.g., ?the first piece of cake.? When 
first is sentence-initial, as in, ?First, she sliced a 
piece of cake,? then it is more likely to be used as 
a discourse marker, indicating a sequence of 
events.  
4 TPD Pilot 
We report on Language Muse use as it was inte-
grated into a Stanford University TPD program for 
in-service9  teachers.  The site agreed to integrate 
the application into their coursework to support 
coursework instruction, and instructional goals. 
This section describes a pilot study and outcomes 
with in-service teachers enrolled in the program. 
4.1 Study Design 
4.1.1 Site Description 
Stanford University?s courses are offered entirely 
online to teachers as part of a professional devel-
opment program that awards the California State 
Cross-Cultural Language and Academic Devel-
opment (CLAD) certificate through its California 
Teachers of English Learners (CTEL) certification 
process. By state law, all California teachers of 
ELLs must obtain a CLAD/CTEL or equivalent 
certification.  
4.1.2 Teacher Participants 
 
Responses to a background survey administered to 
teachers indicated a range of teaching experience 
from less than a year of teaching experience to as 
much as 37 years of teaching experience.  Teach-
ers taught across a broad range of content areas, 
including Art, Computers, Health, Language Arts, 
Math, Music, Physical Education, Science, and 
Social Studies, and grade levels from Kindergarten 
through 12th grade. 
 
                                                          
9 This refers to teachers who have teaching credentials, and 
can be employed as a classroom teachers. 
6
 
 
4.1.3 Pilot Instructional Activities10, 
 
After responding to the background survey, and 
the two pre-tests (Section 4.1.4), teachers com-
pleted the following TPD activities before moving 
on to post-tests (Section 4.1.4.) First, teachers read 
an article written by a teacher training expert on 
the team. The article describes best practices for 
developing language-based scaffolding for ELLs. 
The article also offers strategy descriptions as to 
how to use Language Muse to complete the lesson 
plan assignment (Section 4.1.4), in particular.  
Teachers then viewed three instructional videos 
that provided instruction about how to use the tool. 
Videos were created by a research team member, 
and included additional instruction about scaffold-
ing strategies. Finally, teachers completed two 
practice activities with Language Muse which 
gave them an opportunity to use the different tool 
modules (TEA-Tool and lesson planning) before 
developing the final lesson plan assignment.  
 
4.1.4 Measurement Instruments11 
 
Teachers completed two surveys, one pre-survey, 
responding to questions about their professional 
background and school context, and a second post-
survey responding to questions related to percep-
tions about Language Muse use.  To evaluate 
teacher knowledge gains, pre- and post-test in-
struments were developed by the project team, and 
included: (a) a multiple-choice (MC) test that 
evaluated teachers? knowledge of linguistic struc-
tures at the Vocabulary, Sentence, and Discourse 
levels, and (b) a constructed- response12 (CR) test t 
measured teachers? ability to identify linguistic 
features in a text13 that were likely to interfere with 
content comprehension,  and to suggest language-
based instructional scaffolding to support compre-
hension. The pretests were administered prior to 
exposure to Language Muse (through the instruc-
tional activities (Section 4.1.3)), and the posttest 
                                                          
10 Instructional activities are available on the Language Muse 
homepage. Teachers save all of their work in Language Muse 
so it can be viewed by course instructors and the research 
team, and accessed by users.  
11 For measurement instruments details, see Burstein et al 
(2012). 
12 Constructed-response tasks require extended written re-
sponses. 
13 An 300-word, 8th grade Social Studies text about U.S. colo-
nization was used. 
after exposure. The same test was administered at 
pre- and post-.14 The CR task was scored by two 
human raters on a 6-point scale (0 to 5, where 
5=highest quality response). Inter-rater reliabili-
ties15 were 0.72 for Vocabulary; 0.75 for Sentenc-
es; and 0.71 for Discourse CR items.  At post-test 
only, teachers developed a lesson plan using the 
lesson planning and TEA-Tool16 modules in Lan-
guage Muse. This occurred after teachers had 
completed the instructional activities included as 
part of Language Muse integration in the Stanford 
program. Lesson plans were evaluated by two hu-
man raters using two distinct rubrics: a) quality of 
Language Skill objectives or b) ELL-specific Skills 
objectives, i.e., unique challenges to ELLs such as, 
idioms or cultural references. Inter-rater reliabili-
ties were 0.61 and 0.71 respectively.   In addition, 
raters reviewed the linguistic feedback features 
that teachers had used to explore the lesson plan 
text, using TEA-Tool. The raters then examined 
the lesson plan and recorded the number of fea-
tures explored that ended up informing the lesson 
plan. Inter-rater reliabilities were 0.69. 
 
4.2 Study Results 
 
    Pre-Posttests, MC and CR. Analyses were con-
ducted for 107 teacher participants for pre- and 
post-MC; 103 pre- and post-CR17.  Paired-samples 
t-test showed statistically significant (p=0.02) in-
crease in the MC Discourse score from pre-test (M 
=13.71, SD =2.22) to post- (M=14.20, SD =2.35; 
(p=0.02) increase in CR Vocabulary pre (M=2.79, 
SD=0.88) to post- (M=2.99, SD=0.86); in the CR 
Sentences score (p=0.02) from pre- (M=1.51, 
SD=1.23) to post- (M=1.91, SD=1.24); in the CR 
Total score (p=0.00) pre- (M=5.96, SD=2.35) to 
post- (M=6.76, SD=2.08).  There were no statisti-
cally significant increases in the MC Vocabulary, 
Sentences, and Total scores, nor CR Discourse.      
    Lesson Plans. Of the 112 teachers who com-
pleted the Lesson Plan assignment, a significant 
                                                          
14 There was a lapse of approximately 8 weeks between the 
pre- and the post-test. 
15 Inter-rater reliabilities in this study reflect Pearson correla-
tions. 
16 The TEA-Tool module is used to explore the linguistic 
features in the text; feedback features are then used to inform 
lesson plan development with regard to the creation of lan-
guage-based scaffolding. 
17 Analyses are reported only for participants who responded 
to the pre- and post-. 
7
 
 
correlation of 0.205 was found between the Lan-
guage Skills Score and the number of feedback 
features used to inform the lesson plan.  
5 Discussion and Conclusions 
This paper discusses how Language Muse, an 
NLP-driven TPD application, supported K-12 
teachers in understanding linguistic features in text 
that may be obstacles to content understanding 
during reading. Through the development of 
teachers? linguistic awareness, our original hy-
pothesis was that teachers would become more 
knowledgeable about linguistic structures, and in 
turn, this would support them in the practice of 
creating lesson plans with greater coverage of text 
language and language objectives that would facil-
itate students? text and content understanding.  
   Study outcomes indicated that the teacher pro-
fessional development package can be successful-
ly implemented in the context of in-service, post-
secondary course work. Through a study with a 
TPD program at Stanford University, results of the 
pre-post assessments administered in the study 
indicated at statistically-significant levels that 
teachers did improve their linguistic knowledge 
about vocabulary, sentences relations, and dis-
course relations, and that they also demonstrated 
and increased ability to offer language-based scaf-
folding strategies as evidenced by an gains pre-
post total score on the CR.  In the context of lesson 
plan development, as a secondary post-test evalua-
tion, teachers who productively used the linguistic 
feedback to inform their lesson plans designed 
higher-quality plans (i.e., addressed language ob-
jectives that target development of new language 
skills), than those who did not.   
   The Language Muse TPD package is now being 
evaluated with nine middle-school teachers with 
high populations of ELLs in California, New Jer-
sey, and Texas. After completion of the TPD, 
teachers will develop lesson units using Language 
Muse, and administer the lessons in their class-
rooms. Pre- and post-tests will be administered to 
students to evaluate the effectiveness of the lesson 
plans vis-?-vis language-based instruction.  
 
Acknowledgments 
 
Research presented in this paper was supported by 
the Institute of Education Science, U.S. Depart-
ment of Education, Award No. R305A100105. 
Any opinions, findings, and conclusions or rec-
ommendations are those of the authors and do not 
necessarily reflect the IES?s views. We are grate-
ful to Steven Holtzman and Jennifer Minsky for 
statistical analysis support. We would like to thank 
Dr. Kenji Hakuta for supporting this work through 
his TPD program at Stanford University. 
 
References 
 
Adger, C. T., Snow, C., & Christian D. (2002). What 
teachers need to know about language. Washington, 
DC: Center for Applied Linguistics. 
August, D. (2003). Supporting the development of Eng-
lish literacy in English language learners: Key issues 
and promising practices (Report No. 61). Baltimore, 
MD: Johns  Hopkins University Center for Re-
search on the Education of Students Placed at Risk.  
Barzilay, Regina and Mirella Lapata (2008). ?Modeling 
Local Coherence: An Entity-Based Approach.? Com-
putational Linguistics, 43(1): 1-34. 
Beck, I. L., McKeown, M. G., & Kucan, L. (2008). 
Creating robust vocabulary: Frequently asked ques-
tions and extended examples. New York, NY: Guil-
ford Press. 
Benson, M., Benson, E., & Ilson, R. (Eds.). (1997).  
The BBI Combinatory Dictionary of English: A 
Guide to Word Combinations. Amsterdam & Phila-
delphia: John Benjamins Publishing Company. 
Berninger, V., Abbot, R., Nagy, W., & Carlisle, J. 
(2009). Growth in phonological, orthographic, and 
morphological awareness in grades 1-6. Journal of 
Psycholinguistic Research, 39, 141-163. 
Breland, H.  Jones, R., and  Jenkins, L (1994). The col-
lege board vocabulary study. Technical Report Col-
lege 
Burstein, J., Sabatini, J., & Shore, J. (in press). In 
Ruslan Mitkov (Ed.), Developing NLP Applications 
for Educational Problem Spaces, Oxford Handbook 
of Computational Linguistics. New York: Oxford 
University Press. 
Burstein, J., Shore, J., Sabatini, J., Moulder, B., 
Holtzman, S., & Pedersen, T. (2012). The Language 
Muse system: Linguistically focused instructional au-
thoring ETS RR-12-21. Princeton, NJ: ETS. 
Burstein, J., and Pedersen, T. (2010). Towards Improv-
ing Synonym Options in a Text Modification Appli-
cation. University of Minnesota Supercomputing 
Institute Research Report Series, UMSI 2010/165, 
November 2010. 
Burstein, J., Chodorow, M., and Leacock, C. (2004). 
Automated Essay Evaluation: The Criterion Online 
Service, AI Magazine, 25(3), 27-36.  
Burstein, J., Kukich, K., Wolff, S., Lu, C.,  Chodorow, 
8
 
 
M., Braden-Harder, L., and Harris, M. D.  (1998). 
Automated Scoring Using A Hybrid Feature Identifi-
cation Technique.  In the Proceedings of the Annual 
Meeting of the Association of Computational Lin-
guistics, August, 1998. Montreal, Canada. 
Calder?n, M. (2007). Teaching reading to English lan-
guage learners, grades 6-12: A framework for im-
proving achievement in the content areas. Thousand 
Oaks, CA: Corwin Press. 
 Calder?n, M., August, D., Slavin, R., Cheung, A., 
Dur?n, D., & Madden, N. (2005). Bringing words to 
life in classrooms with English language learners. In 
A. Hiebert & M. Kamil (Eds.), Research and devel-
opment on vocabulary. Mahwah, NJ: Lawrence Erl-
baum Associates. 
Carlo, M. S., August, D., McLaughlin, B., Snow, C. E., 
Dressler, C., Lippman, D. N., & White, C. E. (2004). 
Closing the gap: Addressing the vocabulary needs of 
English language learners in bilingual and main-
stream classrooms. Reading Research Quarterly, 39, 
188-215. 
Coleman, D., & Pimentel, S. (2011a). Publishers? crite-
ria for the Common Core State Standards in English 
Language Arts and Literacy, grades 3-12. Washing-
ton, DC: National Governors Association Center for 
Best Practices and Council of Chief State School Of-
ficers. 
Collins-Thompson, Kevyn and Jamie Callan (2004). ?A 
Language Modeling Approach to Predicting Reading 
Difficulty.? In Proceedings of the Human Language 
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics. 
Boston, MA: Association for Computational Linguis-
tics, 193-200. 
Darling-Hammond, L. (2000). Teacher quality and stu-
dent achievement: A review of state policy evidence.  
Education Policy Analysis Archives, 8. 
Flesch, R.. (1948). A new readability yardstick. 
Journal of Applied Psychology, 32, 221-233.  
Flinspach, S. L., Scott, J. A., Samway, K. D., & Miller, 
T. (2008, March). Developing cognate awareness to 
enhance literacy: Importante y necesario. Paper pre-
sented at the Annual Meeting of the American Edu-
cational Research Association, New York, NY..  
Francis, D., August, D. Goldenberg, C., & Shanahan, T. 
(2004). Developing literacy skills in English lan-
guage learners: Key issues and promising practices. 
Retrieved June 11, 2007, from:  
www.cal.org/natl-lit-
panel/reports/Executive_Summary.pdf 
Futagi, Y., Deane, P., Chodorow, M., & Tetreault, J.  
(2008). A Computational Approach to Detecting Col-
location Errors in the Writing of Non-native Speakers 
of English, Computer Assisted Language Learning, 
Vol. 21, pp. 353?367. 
G?ndara, P., Maxwell-Jolly, J., & Driscoll, A. (2005). 
Listening to teachers of English language learners: A 
survey of California teachers? challenges, experienc-
es, and professional development needs. Sacramento, 
CA: The Regents of the University of California. Re-
trieved from 
http://www.cftl.org/documents/2005/listeningforweb.
pdf.  
Goldenberg, C. (2008). Teaching English language 
learners: What the research does?and does not?
say. American Educator, 32, 8-21. 
Goldman, S. R., & Rakestraw Jr., J. A. (2000).  Struc-
tural aspects of constructing meaning from text.  In 
M. L. Kamil, P. B. Mosenthal, P. D. Pearson, & R. 
Barr (Eds.), Handbook of reading research (Vol. III, 
pp. 311-335).  Mahwah, NJ: Lawrence Erlbaum As-
sociates. 
Graesser, Arthur C., Danielle S. McNamara, and Jonna 
M. Kulikowich (2011). ?Coh-Metrix: Providing Mul-
tilevel Analyses of Text Characteristics.? Educational 
Researcher, 40(5): 223-234. 
Green, C., Foote, M., Walker, C., & Shuman, C. 
(2010). From questions to answers: Education faculty 
members learn about English learners. In S. Szabo, 
M. B. Sampson, M. M. Foote, & F. Falk-Ross (Eds.), 
Mentoring literacy professionals: Continuing the 
spirit of CRA/ALER after 50 years (pp. 113-125). 
Commerce, TX: Texas A&M University Press. 
Heilman, Michael, Lee Zhao, Juan Pinto, and Maxine 
Eskenazi (2008). ?Retrieval of Reading Materials for 
Vocabulary and Reading Practice.? In Proceedings of 
the Third Workshop on Innovative Use of NLP for 
Building Educational Applications. Columbus, OH: 
Association for Computational Linguistics, 80-88. 
Kieffer, M. J. & Lesaux, N. K. (2008). The role of deri-
vational morphology in the reading comprehension of 
Spanish-speaking English language learners. Reading 
and Writing, 21, 783-804. 
Kintsch, W. (1998). Comprehension: A paradigm for 
comprehension. Cambridge, UK: Cambridge Univer-
sity Press. 
Leacock, C.  & Chodorow, M.  (2003). C-rater: Scoring 
of Short-Answer Questions. Computers and the Hu-
manities, Vol. 37, pp. 389?405. 
Lesaux, N. K., Kieffer, M. J., Faller, S. E., & Kelley, J. 
G. (2010). The effectiveness and ease of implementa-
tion of an academic vocabulary intervention for lin-
guistically diverse students in urban middle schools. 
Reading Research Quarterly, 45, 196-228. 
Lin, Dekang (1998). ?Automatic Retrieval and Cluster-
ing of Similar Words.? In ?Proceedings of the 17th 
International Conference on Computational Linguis-
tics and the 36th Annual Meeting of the Association 
for Computational Linguistics. Montreal, Canada: 
768-774. 
Madnani, Nitin and Bonnie J. Dorr (in press). ?Generat-
ing Targeted Paraphrases for Improved Translation.? 
9
 
 
ACM Transactions on Intelligent Language Muses 
and Technology: Special Issue on Paraphrasing.  
Marcu, Daniel (1999). ?Discourse Trees Are Good In-
dicators of Importance in Text. In Advances in Auto-
matic Text Summarization, eds. Inderjeet Mani and 
Mark T. Maybury. Cambridge, MA: MIT Press, 123-
136. 
Meurers, W. Detmar, Ramon Ziai, Luiz Amaral, Adri-
ane Boyd, Aleksandar Dimitrov, Vanessa Metcalf, 
and Niels Ott (2010). ?Enhancing Authentic Web 
Pages for Language Learners.? In Proceedings of the 
NAACL HLT 2010 Fifth International Workshop on 
Innovative Use of NLP for Building Educational Ap-
plications, eds. Joel Tetreault, Jill Burstein, and 
Claudia Leacock. Los Angeles, CA: Association for 
Computational Linguistics, 10-18. 
Meyer, B. J. F. (2003). Text coherence and readability. 
Topics in Language Disorders, 23, 204-221. 
Mihalcea, Rada, Ravi Sinha, and Diana McCarthy 
(2010). ?SemEval-2010 Task 2: Cross-Lingual Lexi-
cal Substitution.? In Proceedings of SemEval-2010: 
Fifth International Workshop on Semantic Evalua-
tions. Uppsala, Sweden: Association for Computa-
tional Linguistics, 9-14. 
Miller, George A. (1990). ?An On-line Lexical Data-
base.? International Journal of Lexicography 3(4): 
235-312. 
Miltsakaki, Eleni (2009). ?Matching Readers? Prefer-
ences and Reading Skills with Appropriate Web 
Texts.? In Proceedings of the European Association 
for Computational Linguistics. Athens, Greece: As-
sociation for Computational Linguistics, 49-52. 
Nagy, W., Beringer, V., & Abbott, R. (2006). Contribu-
tions of morphology beyond phonology to literacy 
outcomes of upper elementary and middle school 
students. Journal of Educational Psychology, 98, 
134-147. 
National Clearinghouse for English Language Acquisi-
tion (2011). The growing numbers of English learner 
students. Washington, DC: Author. Retrieved from 
http://www.ncela.gwu.edu/files/uploads/9/growingLE
P_0809.pdf.  
National Governors Association Center for Best Prac-
tices and Council of Chief State School Officers 
(2010). Common Core State Standards for English 
language Arts & Literacy in History/Social Studies, 
Science, and Technical Subjects. Appendix A: Re-
search supporting key elements of the Standards. 
Washington, DC: Author. 
Nelson, Jessica, Charles Perfetti, David Liben, and 
Meredith Liben (2012). Measures of Text Difficulty: 
Testing Their Predictive Value for Grade Levels and 
Student Performance. Washington, DC: The Council 
of Chief State School Officers. Retrieved from 
http://www.ccsso.org/Documents/2012/Measures%2
0ofText%20Difficulty_final.2012.pdf.  
Pappamihiel, N. E., Lake, V., & Rice, D. (2005).  
Adapting a Social Studies lesson to include English 
language learners.  Social Studies and the Young 
Learner, 17, 4-7. 
Peske, H. G., & Haycock, K. (2006). Teaching inequal-
ity: How poor and minority students are 
shortchanged on teacher quality. Washington, DC: 
The Education Trust. Retrieved from 
http://www.edtrust.org/sites/edtrust.org/files/publicati
ons/files/TQReportJune2006.pdf.  
Pitler, Emily  and Ani Nenkova (2008). ?Revisiting 
Readability: A Unified Framework for Predicting 
Text Quality.? In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing. Honolulu, HI: Association for Computa-
tional Linguistics, 186-195. 
Proctor, C. P., Dalton, D., Uccelli, P., Biancarosa, G., 
Mo, E., Snow, C. E., & Neugebauer, S. (2011).  Im-
proving comprehension online (ICON): Effects of 
deep vocabulary instruction with bilingual and mono-
lingual fifth graders.  Reading and Writing: An Inter-
disciplinary Journal, 24, 517-544. 
Proctor, C. P., Dalton, B., & Grisham, D. (2007).  Scaf-
folding English language learners and struggling 
readers in a multimedia hypertext environment with 
embedded strategy instruction and vocabulary sup-
port.  Journal of Literacy Research, 39, 71-93. 
Rivera, M. O., Moughamian, A. C., Lesaux, N. K., & 
Francis, D. J. (2008). Language and reading inter-
ventions for English language learners and English 
language learners with disabilities. Portsmouth, NJ: 
Research Corporation, Center on Instruction. 
Rutherford William E. and Michael Sharwood Smith 
(1985). ?Consciousness-Raising and Universal 
Grammar.? Applied Linguistics 6(3): 274-282. 
Schwarm, Sarah E.  and Mari Ostendorf (2005). ?Read-
ing Level Assessment Using Support Vector Ma-
chines and Statistical Language Models.? In 
Proceedings of the Annual Meeting of the Association 
for Computational Linguistics. Ann Arbor, MI: As-
sociation for Computational Linguistics, 523-530. 
Schleppegrell, M. J. (2007). The linguistic challenges 
of mathematics teaching and learning: A research re-
view. Reading and Writing Quarterly, 23, 139-159.  
Schleppegrell, M. J., & de Oliveira, L. C. (2006). An 
integrated language and content approach for history 
teachers. Journal of English for Academic Purposes, 
5, 254-268. 
SDL. (n.d.). Automated translation. Retrieved from 
http://www.sdl.com/en/languagetechnology/products/
automated-translation/ 
Walqui, A., & Heritage, M. (2012, January). Instruction 
for diverse groups of ELLs. Paper presented at the 
Understanding Language Conference, Stanford, CA. 
10
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 163?168,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Automated Scoring of a Summary Writing Task
Designed to Measure Reading Comprehension
Nitin Madnani, Jill Burstein, John Sabatini and Tenaha O?Reilly
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{nmadnani,jburstein,jsabatini,toreilly}@ets.org
Abstract
We introduce a cognitive framework for mea-
suring reading comprehension that includes
the use of novel summary writing tasks. We
derive NLP features from the holistic rubric
used to score the summaries written by stu-
dents for such tasks and use them to design a
preliminary, automated scoring system. Our
results show that the automated approach per-
forms well on summaries written by students
for two different passages.
1 Introduction
In this paper, we present our preliminary work on
automatic scoring of a summarization task that is de-
signed to measure the reading comprehension skills
of students from grades 6 through 9. We first intro-
duce our underlying reading comprehension assess-
ment framework (Sabatini and O?Reilly, In Press;
Sabatini et al, In Press) that motivates the task of
writing summaries as a key component of such as-
sessments in ?2. We then describe the summariza-
tion task in more detail in ?3. In ?4, we describe our
approach to automatically scoring summaries writ-
ten by students for this task and compare the results
we obtain using our system to those obtained by hu-
man scoring. Finally, we conclude in ?6 with a brief
discussion and possible future work.
2 Reading for Understanding (RfU)
Framework
We claim that to read for understanding, readers
should acquire the knowledge, skills, strategies, and
dispositions that will enable them to:
? learn and process the visual and typographical
elements and conventions of printed texts and
print world of literacy;
? learn and process the verbal elements of lan-
guage including grammatical structures and
word meanings;
? form coherent mental representations of texts,
consistent with discourse, text structures, and
genres of print;
? model and reason about conceptual content;
? model and reason about social content.
We also claim that the ability to form a coher-
ent mental model of the text that is consistent with
text discourse is a key element of skilled reading.
This mental model should be concise but also reflect
the most likely intended meaning of the source. We
make this claim since acquiring this ability:
1. requires the reader to have knowledge of
rhetorical text structures and genres;
2. requires the reader to model the propositional
content of a text within that rhetorical frame,
both from an author?s or reader?s perspective;
and
3. is dependent on a skilled reader having ac-
quired mental models for a wide variety of
genres, each embodying specific strategies for
modeling the meaning of the text sources to
achieve reading goals.
In support of the framework, research has shown
that the ability to form a coherent mental model
163
is important for reading comprehension. Kintsch
(1998) showed that it is a key aspect in the process of
construction integration and essential to understand-
ing the structure and organization of the text. Sim-
ilarly, Gernsbacher (1997) considers mental models
essential to structure mapping and in bridging and
making knowledge-based inferences.
2.1 Assessing Mental Models
Given the importance of mental models for reading
comprehension, the natural question is how does one
assess whether a student has been able to build such
models after reading a text. We believe that such
an assessment must encompass asking a reader to
(a) sample big ideas by asking them to describe the
main idea or theme of a text, (b) find specific details
in the text using locate/retrieve types of questions,
and (c) bridging gaps between different points in the
text using inference questions. Although these ques-
tions can be multiple-choice, existing research indi-
cates that it is better to ask the reader to write a brief
summary of the text instead. Yu (2003) states that
a good summary can prove useful for assessment of
reading comprehension since it contains the relevant
important ideas, distinguishes accurate information
from opinions, and reflects the structure of the text
itself. More specifically, having readers write sum-
maries is a promising solution since:
? there is considerable empirical support that it
both measures and encourages reading compre-
hension and is an effective instructional strat-
egy to help students improve reading skills
(Armbruster et al, 1989; Bean and Steenwyk,
1984; Duke and Pearson, 2002; Friend, 2001;
Hill, 1991; Theide and Anderson, 2003);
? it is a promising technique for engaging stu-
dents in building mental models of text; and
? it aligns with our framework and cognitive the-
ory described earlier in this section.
However, asking students to write summaries in-
stead of answering multiple choice questions entails
that the summaries must be scored. Asking human
raters to score these summaries, however, can be
time consuming as well as costly. A more cost-
effective and efficient solution would be to use an
automated scoring technique using machine learn-
ing and natural language processing. We describe
such a technique in the subsequent sections.
During the Neolithic Age, humans developed agriculture-what we 
think of as farming.  Agriculture meant that people stayed in one 
place to grow their crops.  They stopped moving from place to 
place to follow herds of animals or to find new wild plants to eat. 
And because they were settling down, people built permanent 
shelters.  The caves they had found and lived in before could be 
replaced by houses they built themselves.
To build their houses, the people of this Age often stacked mud 
bricks together to make rectangular or round buildings.  At first, 
these houses had one big room.  Gradually, they changed to 
include several rooms that could be used for different purposes. 
People dug pits for cooking inside the houses.  They may have 
filled the pits with water and dropped in hot stones to boil it.  You 
can think of these as the first kitchens.
The emergence of permanent shelters had a dramatic effect on 
humans.  They gave people more protection from the weather and 
from wild animals.  Along with the crops that provided more food 
than hunting and gathering, permanent housing allowed people to 
live together in larger communities.
Please write a summary. The first sentence of your summary 
should be about the whole passage.  Then write 3 more 
sentences. Each sentence should be about one of the 
paragraphs.
Passage
Directions
Figure 1: An example passage for which students are
asked to write a summary, and the summary-writing di-
rections shown to the students.
3 Summary Writing Task
Before describing the automated scoring approach,
we describe the details of the summary writing task
itself. The summarization task is embedded within
a larger reading comprehension assessment. As part
of the assessment, students read each passage and
answer a set of multiple choice questions and, in ad-
dition, write a summary for one of the passages. An
example passage and the instructions can be seen in
Figure 1. Note the structured format of summary
that is asked for in the directions: the first sentence
of the summary must be about the whole passage
and the next three should correspond to each of the
paragraphs in the passage. All summary tasks are
structured similarly in that the first sentence should
identify the ?global concept? of the passage and the
164
next three sentences should identify ?local concepts?
corresponding to main points of each subsequent
paragraph.
Each summary written by a student is scored ac-
cording to a holistic rubric, i.e., based on holistic
criteria rather than criteria based on specific dimen-
sions of summary writing. The scores are assigned
on a 5-point scale which are defined as:
Grade 4: summary demonstrates excellent global
understanding and understanding of all 3 lo-
cal concepts from the passage; does not include
verbatim text (3+ words) copied from the pas-
sage; contains no inaccuracies.
Grade 3: summary demonstrates good global un-
derstanding and demonstrates understanding of
at least 2 local concepts; may or may not in-
clude some verbatim text, contains no more
than 1 inaccuracy.
Grade 2: summary demonstrates moderate local
understanding only (2-3 local concepts but no
global); with or without verbatim text, contains
no more than 1 inaccuracy; OR good global un-
derstanding only with no local concepts
Grade 1: summary demonstrates minimal local
understanding (1 local concept only), with or
without verbatim text; OR contains only verba-
tim text
Grade 0: summary is off topic, garbage, or demon-
strates no understanding of the text; OR re-
sponse is ?I don?t know? or ?IDK?.
Note that students had the passage in front of them
when writing the summaries and were not penalized
for poor spelling or grammar in their summaries. In
the next section, we describe a system to automati-
cally score these summaries.
4 Automated Scoring of Student
Summaries
We used a machine learning approach to build an
automated system for scoring summaries of the type
described in ?3. To train and test our system, we
used summaries written by more than 2600 students
from the 6th, 7th and 9th grades about two differ-
ent passages. Specifically, there were a total of 2695
summaries ? 1016 written about a passage describ-
ing the evolution of permanent housing through his-
tory (the passage shown in Figure 1) and 1679 writ-
ten about a passage describing living conditions at
the South Pole. The distribution of the grades for
the students who wrote the summaries for each pas-
sage is shown in Table 1.
Passage Grade Count
South Pole
6 574
7 521
9 584
Perm. Housing
6 387
7 305
9 324
Table 1: The grade distribution of the students who wrote
summaries for each of the two passages.
All summaries were also scored by an experi-
enced human rater in accordance with the 5-point
holistic rubric described previously. Figure 2 shows
the distribution of the human scores for both sets of
summaries.
South Pole (N=1679)
Permanent Housing (N=1016)
0100
200300
400500
600700
800900
Score0 1 2 3 4 Score0 1 2 3 4
Figure 2: A histogram illustrating the human score distri-
bution of the summaries written for the two passages.
Our approach to automatically scoring these sum-
maries is driven by features based on the rubric.
Specifically, we use the following features:
1. BLEU: BLEU (BiLingual Evaluation Under-
study) (Papineni et al, 2002) is an automated
metric used extensively in automatically scor-
ing the output of machine translation systems.
165
It is a precision-based metric that computes n-
gram overlap (n=1 . . . 4) between the summary
(treated as a single sentence) against the pas-
sage (treated as a single sentence). We chose to
use BLEU since it measures how many of the
words and phrases are borrowed directly from
the passage. Note that some amount of borrow-
ing from the passage is essential for writing a
good summary.
2. ROUGE: ROUGE (Recall-Oriented Under-
study for Gisting Evaluation) (Lin and Hovy,
2003) is an automated metric used for scoring
summaries produced by automated document
summarization systems. It is a recall-based
metric that measures the lexical and phrasal
overlap between the summary under consider-
ation and a set of ?model? (or reference) sum-
maries. We used a single model summary for
the two passages by randomly selecting each
from the set of student summaries assigned a
score of 4 by the human rater.
3. CopiedSumm: Ratio of the sum of lengths of
all 3-word (or longer) sequences that are copied
from the passage to the length of the summary.
4. CopiedPassage: Same as CopiedSumm but
with the denominator being the length of the
passage.
5. MaxCopy: Length of the longest word se-
quence in the summary copied from the pas-
sage.
6. FirstSent: Number of passage sentences that
the first sentence of the summary borrows 2-
word (or longer) sequences from.
7. Length: Number of sentences in the summary.
8. Coherence: Token counts of commonly used
discourse connector words in the summary.
ROUGE computes the similarity between the
summary S under consideration and a high-scoring
summary - a high value of this similarity indicates
that S should also receive a high score. Copied-
Summ, CopiedPassage, BLEU, and MaxCopy
capture verbatim copying from the passage. First-
Sent directly captures the ?global understanding?
concept for the first sentence, i.e., a large value for
this feature means that the first sentence captures
more of the passage as expected. Length captures
the correspondence between the number of para-
graphs in the passage and the number of sentences
in the summary. Finally, Coherence captures how
well the student is able to connect the different ?lo-
cal concepts? present in the passage. Note that:
? Although the rubric states that students not be
penalized for spelling errors, we did not spell-
correct the summaries before scoring them. We
plan to do this for future experiments.
? The students were not explicitly told to refrain
from verbatim copying since the summary-
writing instructions indicated this implicitly
(?. . . about the whole passage? and ?. . . about
one of the paragraphs?). However, for future
experiments, we plan to include explicit in-
structions regarding copying.
All features were combined in a logistic regres-
sion classifier that output a prediction on the same
5-point scale as the holistic rubric. We trained a sep-
arate classifier for each of the two passage types.1
The 5-fold cross-validation performance of this clas-
sifier on our data is shown in Table 2. We compute
exact as well as adjacent agreement of our predic-
tions against the human scores using the confusion
matrices from the two classifiers. The exact agree-
ment shows the rate at which the system and the
human rater awarded the same score to a summary.
Adjacent agreement shows the rate at which scores
given by the system and the human rater were no
more than one score point apart (e.g., the system as-
signed a score of 4 and the human rater assigned a
score of 5 or 3). For holistic scoring using 5-point
rubrics, typical exact agreement rates are in the same
range as our scores (Burstein, 2012; Burstein et al,
2013). Therefore, our system performed reasonably
well on the summary scoring task. For comparison,
we also show the exact and adjacent agreement of
the most-frequent-score baseline.
It is important to investigate whether the various
features correlated in an expected manner with the
score in order to ensure that the summary-writing
construct is covered accurately. We examined the
weights assigned to the various features in the clas-
sifier and found that this was indeed the case. As ex-
pected, the CopiedSumm, CopiedPassage, BLEU,
1We used the Weka Toolkit (Hall et al, 2009).
166
Method Passage Exact Adjacent
Baseline
South Pole .51 .90
Perm. Housing .32 .77
Logistic
South Pole .65 .97
Perm. Housing .52 .93
Table 2: Exact and adjacent agreements of the most-
frequent-score baseline and of the 5-fold cross-validation
predictions from the logistic regression classifier, for both
passages.
and MaxCopy features all correlate negatively with
score, and ROUGE, FirstSent and Coherence cor-
relate positively.
In addition to overall performance, we also exam-
ined which features were most useful to the classi-
fier in predicting summary scores. Table 3 shows the
various features ranked using the information-gain
metric for both logistic regression models. These
rankings show that the features performed consis-
tently for both models.
South Pole Perm. Housing
BLEU (.375) BLEU (.450)
CopiedSumm (.290) ROUGE (.400)
ROUGE (.264) CopiedSumm (.347)
Length (.257) Length (.340)
CopiedPassage (.246) MaxCopy(.253)
MaxCopy (.231) CopiedPassage (.206)
FirstSent (.120) Coherence (.155)
Coherence (.103) FirstSent (.058)
Table 3: Classifier features for both passages ranked by
average merit values obtained using information-gain.
5 Related Work
There has been previous work on scoring summaries
as part of the automated document summarization
task (Nenkova and McKeown, 2011). In that task,
automated systems produce summaries of multiple
documents on the same topic and those machine-
generated summaries are then scored by either hu-
man raters or by using automated metrics such as
ROUGE. In our scenario, however, the summaries
are produced by students?not automated systems?
and the goal is to develop an automated system to
assign scores to these human-generated summaries.
Although work on automatically scoring student
essays (Burstein, 2012) and short answers (Lea-
cock and Chodorow, 2003; Mohler et al, 2011) is
marginally relevant to the work done here, we be-
lieve it is different in significant aspects based on
the scoring rubric and on the basis of the underlying
RfU framework. We believe that the work most di-
rectly related to ours is the Summary Street system
(Franzke et al, 2005; Kintsch et al, 2007) which
attempts to score summaries written for tasks not
based on the RfU framework and uses latent seman-
tic analysis (LSA) rather than a feature-based classi-
fication approach.
6 Conclusion & Future Work
We briefly introduced the Reading for Understand-
ing cognitive framework and how it motivates the
use of a summary writing task in a reading compre-
hension assessment. Our motivation is that such a
task is theoretically suitable for capturing the abil-
ity of a reader to form coherent mental representa-
tions of the text being read. We then described a
preliminary, feature-driven approach to scoring such
summaries and showed that it performed quite well
for scoring the summaries about two different pas-
sages. Obvious directions for future work include:
(a) getting summaries double-scored to be able to
compare system-human agreement against human-
human agreement (b) examining whether a single
model trained on all the data can perform as well as
passage-specific models, and (c) using more sophis-
ticated features such as TERp (Snover et al, 2010)
which can capture and reward paraphrasing in ad-
dition to exact matches, and features that can better
model the ?local concepts? part of the scoring rubric.
Acknowledgments
The research reported here was supported by the Institute
of Education Sciences, U.S. Department of Education,
through Grant R305F100005 to the Educational Testing
Service as part of the Reading for Understanding Re-
search Initiative. The opinions expressed are those of the
authors and do not represent views of the Institute or the
U.S. Department of Education. We would also like to
thank Kelly Bruce, Kietha Biggers and the Strategic Ed-
ucational Research Partnership.
167
References
B. B. Armbruster, T. H. Anderson, and J. Ostertag. 1989.
Teaching Text Structure to Improve Reading and Writ-
ing. Educational Leadership, 46:26?28.
T. W. Bean and F. L. Steenwyk. 1984. The Effect of
Three Forms of Summarization Instruction on Sixth-
graders? Summary Writing and Comprehension. Jour-
nal of Reading Behavior, 16(4):297?306.
J. Burstein, J. Tetreault, and N. Madnani. 2013. The E-
rater Automated Essay Scoring System. In M.D. Sher-
mis and J. Burstein, editors, Handbook for Automated
Essay Scoring. Routledge.
J. Burstein. 2012. Automated Essay Scoring and Evalu-
ation. In Carol Chapelle, editor, The Encyclopedia of
Applied Linguistics. Wiley-Blackwell.
N. K. Duke and P. D. Pearson. 2002. Effective Practices
for Developing Reading Comprehension. In A. E.
Farstrup and S. J. Samuels, editors, What Research has
to Say about Reading Instruction, pages 205?242. In-
ternational Reading Association.
M. Franzke, E. Kintsch, D. Caccamise, N. Johnson, and
S. Dooley. 2005. Summary Street: Computer sup-
port for comprehension and writing. Journal of Edu-
cational Computing Research, 33:53?80.
R. Friend. 2001. Effects of Strategy Instruction on Sum-
mary Writing of College Students. Contemporary Ed-
ucational Psychology, 26(1):3?24.
M. A. Gernsbacher. 1997. Two Decades of Structure
Building. Discourse Processes, 23:265?304.
P. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
M. Hill. 1991. Writing Summaries Promotes Think-
ing and Learning Across the Curriculum ? But Why
are They So Difficult to Write? Journal of Reading,
34(7):536?639.
E. Kintsch, D. Caccamise, M. Franzke, N. Johnson, and
S. Dooley. 2007. Summary Street: Computer-guided
summary writing. In T. K. Landauer, D. S. McNa-
mara, S. Dennis, and W. Kintsch, editors, Handbook
of latent semantic analysis. Lawrence Erlbaum Asso-
ciates Publishers.
W. Kintsch. 1998. Comprehension: A Paradigm for
Cognition. Cambridge University Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
C.-Y. Lin and E. H. Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of HLT-NAACL, pages 71?78.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to Grade Short Answer Questions using Seman-
tic Similarity Measures and Dependency Graph Align-
ments. In Proceedings of ACL, pages 752?762.
A. Nenkova and K. McKeown. 2011. Automatic Sum-
marization. Foundations and Trends in Information
Retrieval, 5(2?3):103?233.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL, pages 311?
318.
J. Sabatini and T. O?Reilly. In Press. Rationale For a
New Generation of Reading Comprehension Assess-
ments. In B. Miller, L. Cutting, and P. McCardle,
editors, Unraveling the Behavioral, Neurobiological,
and Genetic Components of Reading Comprehension.
Brookes Publishing, Inc.
J. Sabatini, T. O?Reilly, and P. Deane. In Press. Prelimi-
nary Reading Literacy Assessment Framework: Foun-
dation and Rationale for Assessment and System De-
sign.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2010.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23:117?127.
K. W. Theide and M. C. M. Anderson. 2003. Summariz-
ing Can Improve Metacomprehension Accuracy. Edu-
cational Psychology, 28(2):129?160.
G. Yu. 2003. Reading for Summarization as Reading
Comprehension Test Method: Promises and Problems.
Language Testing Update, 32:44?47.
168
LAW VIII - The 8th Linguistic Annotation Workshop, pages 48?53,
Dublin, Ireland, August 23-24 2014.
Finding your ?inner-annotator?: An experiment in annotator 
independence for rating discourse coherence quality in essays  
 
  Jill Burstein              Swapna Somasundaran Martin Chodorow 
Educational Testing Service Educational Testing Service Hunter College, CUNY 
666 Rosedale Road  666 Rosedale Road  695 Park Avenue 
Princeton, NJ 08541  Princeton, NJ 08541  New York, NY  
 
jburstein@ets.org   ssomasundaran@ets.org   martin.chodorow@hunter.cuny.edu 
 
Abstract 
An experimental annotation method is described, showing promise for a subjective labeling task ? 
discourse coherence quality of essays.   Annotators developed personal protocols, reducing front-end 
resources: protocol development and annotator training.  Substantial inter-annotator agreement was 
achieved for a 4-point scale.  Correlational analyses revealed how unique linguistic phenomena were 
considered in annotation. Systems trained with the annotator data demonstrated utility of the data. 
 
1 Introduction1 
  
Systems designed to evaluate discourse coherence quality often use supervised methods, relying on 
human annotation that requires significant front-end resources (time and cost) for protocol 
development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon 
Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional 
means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis & Nenkova, 
2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., 
the need for automated writing evaluation  systems to score proprietary test-taker data is likely to 
increase when Common Core2 assessments are administered to school-age students beginning in 2015 
(Shermis, in press), increasing the need for data annotation. This paper describes an experimental 
method for capturing discourse coherence quality judgments for test-taker essays. Annotators 
developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard 
front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 
3), system evaluations (Section 4), and conclusions (Section 5). 
 
2 Related Work 
 
Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein & 
Wolska, 2003; Reidsma & op den Akker, 2008; Burstein et al., 2013).  Front-end annotation activities 
may require significant resources (protocol development and annotator training) (Miltsakaki and 
Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013).  Burstein et al (2013) 
reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading 
research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text 
coherence is highly personal , relying on a variety of features, including adherence to standard writing 
conventions (e.g., grammar),  and patterns of rhetorical structure and vocabulary usage.  They describe 
an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) 
applied by 2 annotators to label 1,500 test-taker essays from 6 task types (Table 1).  Protocol 
development took several weeks, and offered extensive descriptions of the 3-point scale, including 
illustrative test-taker responses; rigorous annotator training was also conducted. Burstein et al, 2013 
collapsing the 3-point scale to a 2-point scale (i.e., high (3), low (1,2)). Results for a binary discourse 
coherence quality system (high and low coherence) for essays achieved only borderline modest 
                                                          
1 This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings 
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
2
 See http://www.corestandards.org/. 
48
Essay-Writing Item Type Test-Taker Population 
1. K-12 expository  Students3, ages 11-16 
2. Expository  NNES-Univ 
3. Source-based, integrated (reading and listening)  NNES-Univ 
4. Expository  Graduate school applicants 
5. Critical argument  Graduate school applicants 
6. Professional licensing, content/expository  Certification for a business-related profession 
 
Table 1. Six item types & populations in the experimental annotation task. NNES-Univ = non-native 
English speakers, university applicants 
 
performance (?=0.41)4.  Outcomes reported in Burstein et al are consistent with discussions that text 
coherence is a complex and individual process (Graesser et al, 2004; Van den Broek, 2012), motivating 
our experimental method.  In contrast to training annotators to follow an annotation scheme pre-
determined by others, annotators devised their own scoring protocols, capturing their independent 
impressions ? finding their ?inner-annotator.?  The practical outcomes of success of the method would 
be reduced front-end resources in terms of time required to (a) develop the annotation protocol and (b) 
train annotators. As a practical end-goal, another success criterion would be to achieve inter-annotator 
agreement such that classifiers could be trained, yielding substantial annotator-system agreement. 
 
3 Experimental Annotation Study 
 
Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described. 
 
3.1 Human Annotators 
 
Two high school English teachers (employed by a company specializing in annotation) performed the 
annotation.  Annotators never met each other, did not know about each other?s activities, and only 
communicated about the annotation with a facilitator from the company. 
 
3.2 Data 
 
A random sample of 250 essays for 6 different item types (n=1500) and test-taker populations (Table 1) 
was selected. The sample was selected across 20 different prompts (test questions) for each item type in 
order to ensure topic generalizability in the resulting systems. Forty essays were randomly selected for a 
small pilot study; the remaining data (1460 essays) were used for the full annotation study. For the full 
study, 20% of the essays (n=292) had been randomly selected for double annotation to measure inter-
annotator agreement; the remaining 1168 essays were evenly divided, and each annotator labeled half 
(n=584 per annotator). Each annotator labeled a total of 876 essays across the 6 task types. 
 
3.3 Experimental Method Description 
 
A one-week pilot study was conducted. To provide some initial grounding, annotators received a 1-page 
task description that offered a high-level explanation of ?coherence? describing the end-points of a 
potential protocol.  (This description was written in about an hour.) It indicated that high coherence is 
associated with an essay that can be easily understood, and low coherence is associated with an 
incomprehensible essay.  Each annotator developed her own protocol: for each score point she wrote 
descriptive text illustrating a set of defining characteristics for each score point of coherence quality 
(e.g., ?The writer?s point is difficult to understand.?). Annotator 1 (A1) developed a 4-point scale; 
 
                                                          
3 Note that this task type was administered in an instructional setting; all other tasks were completed in high-stakes 
assessment settings. 
4
 Kappa was not reported in the paper, but was accessed through personal communication. 
49
Feature Type A1 (r) A2 (r) 
Grammar errors (e.g., subject verb agreement) 0.42 0.35 
Word usage errors (e.g., determiner errors) 0.46 0.44 
Mechanics errors (e.g.,  spelling, punctuation) 0.58 0.52 
EGT -- best 3 features (out of 112 features): F1, F2, F3 F1. -0.30 
F2. -0.28 
F3.  0.27 
F1. -0.14 
F2. -0.15 
F3.  0.11 
RST features--   best 3 features (out of 100 features): F1, F2, F3 F1. -0.27 
F2.  0.15 
F3.  0.19 
F1. -0.19 
F2.  0.08 
F3.  0.06 
LDSP 0.19 0.06 
Table 2. Pearson r between annotator discourse coherence scores and features. All correlations are 
significant at p < .0001, except for A2?s long-distance sentence-pair similarity at p < .05. 
 
Annotator 2 (A2) developed a 5-point scale. Because the two scales were different, ? could not be used 
to measure agreement, so a Spearman rank-order correlation (rS) was used, yielding a promising value 
(rS=0.82). Annotator protocols were completed at the end of the pilot study. 
 A full experiment was conducted. Each annotator used her protocol to assign a coherence quality 
score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from 
the protocol). Comments provided a score supplement that could be used to support analyses beyond 
quantitative measures (Reidsma & Carletta, 2008).  The data were annotated in 12 batches (by task) 
composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was 
computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which 
was infrequent), one of the authors reviewed the annotator scores and comments to look for 
inconsistencies.  Agreement was re-computed when annotator revisions were completed  to ensure inter-
rater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate 
annotator schedules.  While a time log was not strictly maintained, we estimate the total time for 
communication to resolve inconsistency issues was about 4-6 hours. One author communicated score-
comment inconsistencies (e.g., high score with critical comments) to the company?s facilitator (through a 
brief e-mail); the facilitator then relayed the inconsistency information to the annotator(s).  The author?s 
data review and communication e-mail took no longer than 45 minutes for the few rounds where 
agreement fell below 0.70. Communication between the facilitator and the annotator(s) involved a brief 
discussion, essentially reviewing the points made in the e-mail.  
 
3.4 Results: Inter-annotator agreement 
 
Using the Spearman rank-order correlation, inter-rater agreement on the double-annotated data was 
rS=0.71. In order to calculate Kappa statistic, A2?s 5-point scale assignments were then mapped to a 4-
point scale by collapsing the two lowest  categories (1,2) into one (1), since there were very few cases of 
1?s; this is consistent with low frequencies of very low-scoring essays. Using quadratic weighted kappa 
(QWK), post-mapping indicated substantial agreement between the two annotators (?=0.61).  
 
3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider? 
 
A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation 
drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.). 
Both annotators included descriptions such as ?word patterns,? ?logical sequencing,? and ?clarity of 
ideas?; however, A2 appeared to have more comments related to grammar and spelling.   Burstein et al., 
(2013)  describe the following features in their binary classification system: (1) grammar, word usage, 
and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid 
transition probabilities to capture local ?topic distribution? (Barzilay & Lapata, 2008) (EGT), and (4) a 
long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture 
?long distance, topical distribution. (LDSP).  Annotated data from this study were processed with the 
Burstein et al (2013) system to extract the features above in (1) ? (4).  To quantify the observed 
50
differences in the annotators? comments and potential effects for system score assignment (Section 4), 
we computed Pearson (r) correlations between the system features (on our annotated data set), and the 
discourse coherence scores of A1 and A2 (using the 4-point scale mapping for A2). There are 112 entity-
transition probability features and 100 Rhetorical Structure Theory (RST) features. In Table 2, the 
correlations of the three best predictors from the EGT and RST sets, and the GUM features and the 
LDSP feature are shown. Correlations in Table 2 are significantly correlated between the feature sets and 
annotator coherence scores. However, we observed that the EGT, RST, and LDSP feature correlation 
values for A2 are notably smaller than A1?s. This suggests that A2 may have had a strong reliance on 
GUM features, or that the system feature set did not capture all linguistic phenomena that A2 considered. 
 
4 System Evaluation5 
 
To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with 
all system features (Sys_All), and a second with the GUM features (Sys_GUM). Using 10-fold cross-
validation with a gradient boosting regression learner, four classifiers were trained to predict coherence 
quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1 
and A2 Sys_GUM systems. 
  
4.1 Results 
 
Sys-All trained with A1 data consistently outperformed Sys-All trained with A2 data. Results are 
reported for averages across the 10-folds, and  showed substantial system-human agreement for A1 (? = 
0.68) and modest system-human agreement for A2 (? = 0.55). When Sys_GUM was trained with A1 
data, system-human agreement dropped to a modest  range (? = 0.60); when Sys_GUM was trained with 
A2 data, however, human agreement was essentially unchanged, staying in the modest  agreement range 
(? = 0.50).  Consistent with the correlational analysis, this finding suggests that A2 has strong reliance 
on GUM features, or the system may have been less successful in capturing A2 features beyond GUM. 
 
5  Discussion and Conclusions 
Our experimental annotation method significantly reduced front-end resources for protocol development 
and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time 
was required from the authors or the facilitator (about two hours) for protocol development; the 
annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this 
process was report to take about one month. Approximately 4-6 hours of additional discussion from one 
author and the facilitator was required during the task; Burstein et al (2013) required two researchers and 
two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time 
across the 4 participants (personal communication). In addition to its efficiency, the experimental 
method was successful per criteria in Section 2. The method captures annotators? subjective judgments 
about coherence quality, yielding substantial inter-annotator agreement (?=0.61) across a 4-point scale.  
Second, classifiers trained with annotator data showed that the systems showed substantial and modest 
agreement (A1 and A2, respectively) ? demonstrating annotation utility, especially for A1. Correlational 
analyses were used to analyze effects of features that annotators may have considered in making their 
decisions. Comment patterns and results from the correlation analysis suggested that A2?s decisions 
were either based on narrower considerations (GUM errors), or not captured by our feature set.  
  The experimental task facilitated the successful collection of subjective coherence judgments with 
substantial inter-annotator agreement on test-taker essays. Consistent with conclusions from Reidsma & 
Carletta (2008), outcomes show that quantitative measures of inter-annotator agreement should not be 
used exclusively.  Descriptive comments were useful for monitoring during annotation, interpreting 
annotator considerations and system evaluations during and after annotation, and informing system 
development. In the future, we would explore strategies to evaluate intra-annotator reliability (Beigman-
Klebanov, Beigman, & Diermeier, 2008) which may have contributed to  lower system performance 
with A2 data. 
                                                          
5
 Many thanks to Binod Gywali for engineering support. 
51
References 
 
Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013.  Using Pivot-Based Paraphrasing and 
Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data, Transactions of the Association for 
Computational Linguistics, Vol.1: 99-110. 
 
Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In 
Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7. 
 
Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality 
in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges 
of annotating pragmatic and discourse phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber. 
Discourse & Dialogue 42, 34-52. 
 
Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use. 
In Proceedings of the 11th Conference of the European Chapter of the Association for Computational 
Linguistics. Budapest, Hungary. 
 
Jacob Cohen.  1960. "A coefficient of agreement for nominal scales". Educational and Psychological 
Measurement 20 1: 37?46.  
 
Joseph Fleiss and Jacob Cohen 1973. "The equivalence of weighted kappa and the intraclass correlation 
coefficient as measures of reliability" in Educational and Psychological Measurement, Vol. 33:613?619. 
 
Peter Foltz, Walter Kintsch, & Thomas Landuaer.  1998. Textual coherence using latent semantic analysis. 
Discourse Processes, 252&3: 285?307.  
 
Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang  Cai, Z. 2004. Coh-metrix: Analysis of text 
on cohesion and language. Behavior Research Methods, Instruments, & Computers, 36(2), 193-202. 
 
Derrick Higgins, Jill Burstein,  Daniel Marcu &. Claudia Gentile. 2004. Evaluating Multiple Aspects of 
Coherence in Student Essays. In Proceedings of 4th Annual Meeting of the Human Language Technology and 
North American Association for Computation Linguistics:185?192, Boston, MA 
 
 J. Richard Landis,.  & G. Koch. 1977. "The measurement of observer agreement for categorical 
data". Biometrics 33 1: 159?174. 
 
Annie Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Special Issue 
of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse 
phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber, 42: 87-117. 
 
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In 
Proceedings of the Language Resources and Evaluation Conference, Athens, Greece. 
 
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The 
MIT Press. 
 
Dennis Reidsma,  and  Rieks op den Akker. 2008. Exploiting `Subjective' Annotations.  In Proceedings of the 
Workshop on Human Judgments in Computational Linguistics, Coling 2008, 23 August 2008, Manchester, 
UK. 
 
Dennis Reidsma,  and  Jean Carletta. 2008. Reliability measurements without limits. Computational Linguistics, 
343: 319-336. 
 
Mark Shermis. to appear. State-of-the-art automated essay scoring: Competition, results, and future directions 
from a United States demonstration.  Assessing Writing. 
 
Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing  of 
Chinese English Speakers. The Journal of Computer Assisted Learning. 28: 396?410. 
 
52
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing 
cognitive processes and outcomes. In: Sabatini, J.P., Albro, E.R., O'Reilly, T. (Eds.), Measuring up: 
Advances in how we assess reading ability., pp. 39-58. Lanham: Rowman & Littlefield Education. 
 
 
 
53
