Coling 2010: Poster Volume, pages 1373?1381,
Beijing, August 2010
Syntax-Driven Machine Translation as a Model of ESL
Revision
Huichao Xue and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{hux10,hwa}@cs.pitt.edu
Abstract
In this work, we model the writing re-
vision process of English as a Second
Language (ESL) students with syntax-
driven machine translation methods.
We compare two approaches: tree-to-
string transformations (Yamada and
Knight, 2001) and tree-to-tree trans-
formations (Smith and Eisner, 2006).
Results suggest that while the tree-to-
tree model provides a greater cover-
age, the tree-to-string approach offers
a more plausible model of ESL learn-
ers? revision writing process.
1 Introduction
When learning a second language, students
make mistakes along the way. While some
mistakes are idiosyncratic and individual,
many are systematic and common to people
who share the same primary language. There
has been extensive research on grammar error
detection. Most previous efforts focus on iden-
tifying specific types of problems commonly
encountered by English as a Second Language
(ESL) learners. Some examples include the
proper usage of determiners (Yi et al, 2008;
Gamon et al, 2008), prepositions (Chodorow
et al, 2007; Gamon et al, 2008; Hermet et al,
2008), and mass versus count nouns (Nagata
et al, 2006). However, previous work suggests
that grammar error correction is considerably
more challenging than detection (Han et al,
2010). Furthermore, an ESL learner?s writing
may contain multiple interacting errors that
are difficult to detect and correct in isolation.
A promising research direction is to tackle
automatic grammar error correction as a ma-
chine translation (MT) problem. The dis-
fluent sentences produced by an ESL learner
can be seen as the input source language,
and the corrected revision is the result of the
translation. Brockett et al (2006) showed
that phrase-based statistical MT can help to
correct mistakes made on mass nouns. To
our knowledge, phrase-based MT techniques
have not been applied for rewriting entire sen-
tences. One major challenge is the lack of ap-
propriate training data such as a sizable par-
allel corpus. Another concern is that phrase-
based MT may not be similar enough to the
problem of correcting ESL learner mistakes.
While MT rewrites an entire source sentence
into the target language, not every word writ-
ten by an ESL learner needs to be modified.
Another alternative that may afford a more
general model of ESL error corrections is to
consider syntax-driven MT approaches. We
argue that syntax-based approaches can over-
come the expected challenges in applying MT
to this domain. First, it can be less data-
intensive because the mapping is formed at a
structural level rather than the surface word
level. While it does require a robust parser,
a syntax-driven MT model may not need to
train on a very large parallel corpus. Second,
syntactic transformations provide an intuitive
description of how second language learners
revise their writings: they are transforming
structures in their primary language to those
in the new language.
In this paper, we conduct a first inquiry into
the applicability of syntax-driven MT meth-
ods to automatic grammar error correction.
In particular, we investigate whether a syntax-
driven model can capture ESL students? pro-
cess of writing revisions. We compare two ap-
proaches: a tree-to-string mapping proposed
by Yamada & Knight (2001) and a tree-to-
tree mapping using the Quasi-Synchronous
1373
Grammar (QG) formalism (Smith and Eisner,
2006). We train both models on a parallel cor-
pus consisting of multiple drafts of essays by
ESL students. The approaches are evaluated
on how well they model the revision pairs in an
unseen test corpus. Experimental results sug-
gest that 1) the QG model has more flexibility
and is able to describe more types of transfor-
mations; but 2) the YK model is better at cap-
turing the incremental improvements in the
ESL learners? revision writing process.
2 Problem Description
This paper explores the research question: can
ESL learners? process of revising their writ-
ings be described by a computational model?
A successful model of the revision process has
several potential applications. In addition to
automatic grammar error detection and cor-
rection, it may also be useful as an auto-
matic metric in an intelligent tutoring system
to evaluate how well the students are learning
to make their own revisions.
Revising an ESL student?s writing bears
some resemblance to translating. The stu-
dent?s first draft is likely to contain disfluent
expressions that arose from translation diver-
gences between English and the student?s pri-
mary language. In the revised draft, the diver-
gences should be resolved so that the text be-
comes fluent English. We investigate to what
extent are formalisms used for machine trans-
lation applicable to model writing revision.
We hypothesize that ESL students typically
modify sentences to make them sound more
fluent rather than to drastically change the
meanings of what they are trying to convey.
Thus, our work focuses on syntax-driven MT
models.
One challenge of applying MT methods to
model grammar error correction is the lack of
appropriate training data. The equivalence
to the bilingual parallel corpus used for de-
veloping MT systems would be a corpus in
which each student sentence is paired with a
fluent version re-written by an instructor. Un-
like bilingual text, however, there is not much
data of this type in practice because there
are typically too many students for the teach-
ers to provide detailed manual inspection and
correction at a large scale. More commonly,
students are asked to revise their previously
written essays as they learn more about the
English language. Here is an example of a
student sentence from a first-draft essay:
The problem here is that they come
to the US like illegal.
In a later draft, it has been revised into:
The problem here is that they come
to the US illegally.
Although the students are not able to cre-
ate ?gold standard revisions? due to their still
imperfect understanding of English, a corpus
that pairs the students? earlier and later drafts
still offers us an opportunity to model how
ESL speakers make mistakes.
More formally, the corpus C consists of a
set of sentence pairs (O,R), where O repre-
sents the student?s original draft and R rep-
resents the revised draft. Note that while R
is assumed to be an improvement upon O,
its quality may fall short of the gold stan-
dard revision, G. To train the syntax-driven
MT models, we optimize the joint probabil-
ity of observing the sentence pair, Pr(O,R),
through some form of mapping between their
parse trees, ?O and ?R.
An added wrinkle to our problem is that it
might not always be possible to assign a sen-
sible syntactic structure to an ungrammati-
cal sentence. It is well-known that an English
parser trained on the Penn Treebank is bad
at handling disfluent sentences (Charniak et
al., 2003; Foster et al, 2008). In our domain,
since O (and perhaps also R) might be disflu-
ent, an important question that a translation
model must address is: how should the map-
ping between the trees ?O and ?R be handled?
3 Syntax-Driven Models for Essay
Revisions
There is extensive literature on syntax-driven
approaches to MT (cf. a recent survey by
1374
Lopez (2008)); we focus on two particular for-
malisms that reflects different perspectives on
the role of syntax. Our goal is to assess which
formalism is a better fit with the domain of
essay revision modeling, in which the data
largely consist of imperfect sentences that may
not support a plausible syntactic interpreta-
tion.
3.1 Tree-to-String Model
The Yamada & Knight (henceforth, YK) tree-
to-string model is an instance of noisy channel
translation systems, which assumes that the
observed source sentence is the result of trans-
formation performed on the parse tree of the
intended target sentence due to a noisy com-
munication channel. Given a parallel corpus,
and a parser for the the target side, the pa-
rameters of this model can be estimated using
EM(Expectation Maximization). The trained
model?s job is to recover the target sentence
(and tree) through decoding.
While the noisy channel generation story
may sound somewhat counter-intuitive for
translation, it gives a plausible account of ESL
learner?s writing process. The student really
wants to convey a fluent English sentence with
a well-formed structure, but due to an im-
perfect understanding of the language, writes
down an ungrammatical sentence, O, as a first
draft. The student serves as the noisy channel.
The YK model describes this as a stochastic
process that performs three operations on ?G,
the parse of the intended sentence, G:
1. Each node in ?G may have its children
reordered with some probability.
2. Each node in ?G may have a child node
inserted to its left or right with some
probability.
3. Each leaf node (i.e., surface word) in ?G
is replaced by some (possibly empty)
string according to its lexical translation
distribution.
The resulting sentence, O, is the concatena-
tion of the leaf nodes of the transformed ?G.
Common mistakes made by ESL learners,
such as misuses of determiners and preposi-
tions, word choice errors, and incorrect con-
stituency orderings, can be modeled by a com-
bination of the insert, replace, and reorder
operators. The YK model allows us to per-
form transformations on a higher syntactic
level. Another potential benefit is that the
model does not attempt to assign syntactic
interpretations over the source sentences (i.e.,
the less fluent original draft).
3.2 Tree-to-Tree Model
The Quasi-Synchronous Grammar formalism
(Smith and Eisner, 2006) is a generative model
that aims to produce the most likely target
tree for a given source tree. It differs from the
more strict synchronous grammar formalisms
(Wu, 1995; Melamed et al, 2004) because it
does not try to perform simultaneous pars-
ing on parallel grammars; instead, the model
learns an augmented target-language gram-
mar whose rules make ?soft alignments? with
a given source tree.
QG has been applied to some NLP tasks
other than MT, including answer selection for
question-answering (Wang et al, 2007), para-
phrase identification (Das and Smith, 2009),
and parser adaptation and projection (Smith
and Eisner, 2009). In this work we use
an instantiation of QG that largely follows
the model described by Smith and Eisner
(2006). The model is trained on a parallel
corpus in which both the first-draft and re-
vised sentences have been parsed. Using EM
to estimate its parameters, it learns an aug-
mented target PCFG grammar1 whose pro-
duction rules form associations with the given
source trees.
Consider the scenario in Figure 1. Given a
source tree ?O, the trained model generates a
target tree by expanding the production rules
in the augmented target PCFG. To apply a
1For expository purposes, we illustrate the model
using a PCFG production rule. In the experiment, a
statistical English dependency parser (Klein and Man-
ning, 2004) was used.
1375
Figure 1: An example of QG?s soft alignments
between a given source tree and a possible tar-
get rule expansion.
target-side production rule such as
A? BC,
the model considers which source tree nodes
might be associated with each target-side non-
terminals:
(?,A)? (?,B)(?,C)
where ?, ?, ? are nodes in ?O. Thus, as-
suming that the target symbol A has already
been aligned to source node ? from an ear-
lier derivation step, the likelihood of expand-
ing (?,A) with the above production rule de-
pends on three factors:
1. the likelihood of the monolingual tar-
get rule, Pr(A? BC)
2. the likelihood of alignments between B
and ? as well as C and ?.
3. the likelihood that the source nodes form
some expected configuration (i.e., be-
tween ? and ? as well as between ? and
?). In this work, we distinguish between
two configuration types: parent-child and
other. This restriction doesn?t reduce the
explanatory power of the resulting QG
model, though it may not be as fine-tuned
as some models in (Smith and Eisner,
2006).
Under QG, the ESL students? first drafts
are seen as text in a different language that
has its own syntactic constructions. QG ex-
plains the grammar rules that govern the re-
vised text in terms of how different compo-
nents map to structures in the original draft.
It makes explicit the representation of diver-
gences between the students? original mental
model and the expected structure.
3.3 Method of Model Comparison
Cross entropy can be used as a metric that
measures the distance between the learned
probabilistic model and the real data. It can
be interpreted as measuring the amount of in-
formation that is needed in addition to the
model to accurately recover the observed data.
In language modeling, cross entropy is widely
used in showing a given model?s prediction
power.
To determine how well the two syntax-
driven MT models capture the ESL student
revision generation process, we measure the
cross entropy of each trained model on an un-
seen test corpus. This quantity measures how
surprised a model is about relating an initial
sentence, O, to its corresponding revision, R.
Specifically, the cross entropy for some model
M on a test corpus C of original and revised
sentence pairs (O,R) is:
? 1|C|
?
(O,R)?C
log Pr
M
(O,R)
Because neither model computes the joint
probability of the sentence pair, we need to
make additional computations so that the
models can be compared directly.
The YK model computes the likelihood
of the first-draft sentence O given an as-
sumed gold parse ?R of the revised sentence:
PrY K(O | ?R). To determine the joint proba-
bility, we would need to compute:
Pr
Y K
(O,R) =
?
?R??R
Pr
Y K
(O, ?R)
=
?
?R??R
Pr
Y K
(O | ?R) Pr(?R)
where ?R represents the set of possible parse
trees for sentence R. Practically, perform-
ing tree-to-string mapping over the entire set
of trees in ?R is computationally intractable.
Moreover, the motivation behind the YK
1376
mean stdev
percentage of O = R 54.11% N/A
O?s length 12.95 4.87
R?s length 12.74 4.20
edit distance 1.88 3.58
Table 1: This table summarizes some statis-
tics of the dataset.
model is to trust the given ?R. Thus, we made
a Viterbi approximation:
Pr
Y K
(O,R) =
?
?R??R
Pr
Y K
(O | ?R) Pr(?R)
? Pr
Y K
(O | ??R) Pr(??R)
where Pr(??R) is the probability of the single
best parse tree according to a standard En-
glish parser.
Similarly, to compute the joint sentence pair
probability under the QG model would require
summing over both sets of trees because the
model computes PrQG(?R | ?O). Here, we
make the Viterbi approximation on both trees.
Pr
QG
(O,R) =
?
?R??R
?
?O??O
Pr
QG
(?O, ?R)
=
?
?R??R
?
?O??O
Pr
QG
(?R | ?O) Pr(?O)
? Pr
QG
(??R | ??O) Pr(??O)
where ??O and ??R are the best parses for sen-
tences O and R according to the underlying
English dependency parser, respectively.
4 Experiments
4.1 Data
Our experiments are conducted using a collec-
tion of ESL students? writing samples2. These
are short essays of approximately 30 sentences
on topics such as ?a letter to your parents.?
The students are asked to revise their essays
at least once. From the dataset, we extracted
358 article pairs.
2The dataset is made available by the Pittsburgh
Science of Learning Center English as a Second Lan-
guage Course Committee, supported by NSF Award
SBE-0354420.
Typically, the changes between the drafts
are incremental. Approximately half of the
sentences are not changed at all. These sen-
tences are considered useful because this phe-
nomenon strongly implies that the original
version is good enough to the best of the au-
thor?s knowledge. In a few rare cases, stu-
dents may write an entirely different essay.
We applied TF-IDF to automatically align the
sentences between essay drafts. Any sentence
pair with a cosine similarity score of less than
0.3 is filtered. This resulted in a parallel cor-
pus of 7580 sentence pairs.
Because both models are computational in-
tensive, we further restricted our experiments
to sentence pairs for which the revised sen-
tence has no more than 20 words. This re-
duces our corpus to 4666 sentence pairs. Some
statistics of the sentence pairs are shown in
Table 1.
4.2 Experimental Setup
We randomly split the resulting dataset into
a training corpus of 4566 sentence pairs and a
test corpus of 100 pairs.
The training of both models involve an EM
algorithm. We initialize the model parameters
with some reasonable values. Then, in each it-
eration of training, the model parameters are
re-estimated by collecting the expected counts
across possible alignments between each sen-
tence pair in the training corpus. In out ex-
periments, both models had two iterations of
training. Below, we highlight our initializa-
tion procedure for each model.
In the YK model, the initial reordering
probability distribution is set to prefer no
change 50% of the time. The remaining prob-
ability mass is distributed evenly over all of
the other permutations. For the insertion
operation, for each node, the YK model first
chooses whether to insert a new string to its
left, to its right, or not at all, conditioned on
the node?s label and its parent?s label. These
distributions are initialized uniformly (13). If
a new string should be inserted, the model
then makes that choice with some probability.
The insertion probability of each string in the
1377
dictionary is assigned evenly with 1N , where
N is the number of words in the dictionary.
Finally, the replace probability distribution
is initialized uniformly with the same value
( 1N+1) across all words in the dictionary, in-
cluding the empty string.
For the QG model, the initial parameters
are determined as follows: For the monolin-
gual target parsing model parameters,
we first parse the target side of the corpus
(i.e., the revised sentences) with the Stanford
parser; we then use the maximum likelihood
estimates based on these parse trees to ini-
tialize the parameters of the target parser,
Dependency Model with Valence (DMV). We
uniformly initialized the configuration pa-
rameters; the parent-child configuration and
other configuration each has 0.5 probability.
For the alignment parameters, we ran the
GIZA++ implementation of the IBM word
alignment model (Och and Ney, 2003) on the
sentence pairs, and used the resulting transla-
tion table as our initial estimation. There may
be better initialization setups, but the differ-
ence between those setups will become small
after a few rounds of EM.
Once trained, the two models compute the
joint probability of every sentence pair in the
test corpus as described in Section 3.3.
4.3 Experiment I
To evaluate how well the models describe the
ESL revision domain, we want to see which
model is less ?surprised? by the test data. We
expected that the better model should be able
to transform more sentence pair in the test
corpus; we also expect that the better model
should have a lower cross entropy with respect
to the test corpus.
Applying both YK and QG to the test cor-
pus, we find that neither model is able to
transform all the test sentence pairs. Of the
two, QG had the better coverage; it success-
fully modeled 59 pairs out of 100 (we denote
this subset as DQG). In contrast, YK modeled
36 pairs (this subset is denoted as DY K).
To determine whether there were some
characteristics of the data that made one
model better at performing transformations
for certain sentence pairs, we compare corpus
statistics for different test subsets. Based on
the results summarized in Table 2, we make a
few observations.
First, the sentence pairs that neither model
could transform seem, as a whole, more diffi-
cult. Their average lengths are longer, and the
average per word Levenshtein edit distance is
bigger. The differences between Neither and
the other subsets are statistically significant
with 90% confidence. For the length differ-
ence, we applied standard two-sample t-test.
For the edit distance difference, we applied hy-
pothesis testing with the null-hypothesis that
?longer sentence pairs are as likely to be cov-
ered by our model as shorter ones.?
Second, both models sometimes have trou-
ble with sentence pairs that require no change.
This may be due to out-of-vocabulary words
in the test corpus. A more aggressive smooth-
ing strategy could improve the coverage for
both models.
Third, comparing the subset of sentence
pairs that only QG could transform (DQG ?
DY K) against the subset of sentences that
both models could transform (DQG ? DY K),
the former has slightly higher average edit dis-
tance and length, but the difference is not
statistically significant. Although QG could
transform more sentence pairs, the cross en-
tropy of DQG ?DY K is higher than QG?s es-
timate for the DQG ?DY K subset. QG?s soft
alignment property allows it to model more
complex transformations with greater flexibil-
ity.
Finally, while the YK model has a more lim-
ited coverage, it models those transformations
with a greater certainty. For the common sub-
set of sentence pairs that both models could
transform, YK has a much lower cross entropy
than QG. Table 3 further breaks down the
common subset. It is not surprising that both
models have low entropy for identical sentence
pairs. For modeling sentence pairs that con-
tain revisions, YK is more efficient than QG.
1378
Neither DQG ?DY K DQG ?DY K DY K ?DQG
number of instances 38 33 26 3
average edit distance 2.42 1.88 2.08 1
% of identical pairs 53% 48% 58% 67%
average O length 14.63 12.36 12.58 6.67
average R length 13.87 12.06 12.62 6.67
QG cross entropy N/A 127.95 138.9 N/A
YK cross entropy N/A 78.76 N/A 43.84
Table 2: A comparison of the two models based on their coverage of the test corpus. Some
relevant statistics on the sentence subsets are also summarized in the table.
YK QG
overall entropy 78.76 127.95
on identical pairs 52.59 85.40
on non-identical pairs 103.99 168.00
Table 3: A further comparison of the two mod-
els on DQG ?DY K , the sentence pairs in the
test corpus that both could transform.
4.4 Experiment II
The results of the previous experiment raises
the possibility that QG might have a greater
coverage because it is too flexible. However,
an appropriate model should not only assign
large probability mass to positive examples,
but it should also have a low chance of choos-
ing negative examples. In this next experi-
ment, we construct a ?negative? test corpus
to see how it affects the models.
To construct a negative scenario, we still
use the same test corpus as before, but we re-
verse the sentence pairs. That is, we use the
revised sentences as ?originals? and the origi-
nal sentences as ?revisions.? We would expect
a good model to have a raised cross entropy
values along with a drop in coverage on the
new dataset because the ?revisions? should be
more disfluent than the ?original? sentences.
Table 4 summarizes the results. We ob-
serve that the number of instances that can
be transformed has dropped for both models:
from 59 to 49 pairs for QG, and from 36 to
20 pairs for YK; also, the proportion of iden-
tical instances in each set has raised. This
means that both models are more surprised
by the reverse test corpus, suggesting that
both models have, to some extent, succeeded
in modeling the ESL revision domain. How-
ever, QG still allows for many more transfor-
mations. Moreover, 16 out of the 49 instances
are non-identical pairs. In contrast, YK mod-
eled only 1 non-identical sentence pair. The
results from these two experiments suggest
that YK is more suited for modeling the ESL
revision domain than QG. One possible expla-
nation is that QG allows more flexibility and
would require more training. Another possi-
ble explanation is that because YK assumes
well-formed syntax structure for only the tar-
get side, the philosophy behind its design is a
better fit with the ESL revision problem.
5 Related Work
There are many research directions in the field
of ESL error correction. A great deal of the
work focuses on the lexical or shallow syn-
tactic level. Typically, local features such
as word identity and POS tagging informa-
tion are combined to deal with some specific
kind of error. Among them, (Burstein et al,
2004) developed a tool called Critique that
detects collocation errors and word choice er-
rors. Nagata et al (2006) uses a rule-based
approach in distinguishing mass and count
nouns. Knight and Chander (1994) and Han
et al (2006) both addressed the misuse of ar-
ticles. Chodorow et al (2007), Gamon et al
(2008), Hermet et al (2008) proposed several
techniques in detecting and correcting propo-
sition errors. In detecting errors and giving
suggestions, Liu et al (2000), Gamon et al
(2008) and Hermet et al (2008) make use of
1379
Neither DQG ?DY K DQG ?DY K DY K ?DQG
number of instances 50 19 30 1
average edit distance 2.88 0.05 2.17 1
percentage of identical pairs 0.40 0.95 0.5 0
average O length 14.18 9.00 12.53 17
average R length 14.98 9.05 12.47 16
QG cross entropy N/A 81.85 139.36 N/A
YK cross entropy N/A 51.2 N/A 103.75
Table 4: This table compares the two models on a ?trick? test corpus in which the earlier and
later drafts are reversed. If a model is trained to prefer more fluent English sentences are the
revision, it should be perplexed on this corpus.
information retrieval techniques. Chodorow
et al (2007) instead treat it as a classification
problem and employed a maximum entropy
classifier. Similar to our approach, Brockett
et al (2006) view error correction as a Ma-
chine Translation problem. But their transla-
tion system is built on phrase level, with the
purpose of correcting local errors such as mass
noun errors.
The problem of error correction at a syn-
tactic level is less explored. Lee and Seneff
(2008) examined the task of correcting verb
form misuse by applying tree template match-
ing rules. The parse tree transformation rules
are learned from synthesized training data.
6 Conclusion
This paper investigates the suitability of
syntax-driven MT approaches for modeling
the revision writing process of ESL learn-
ers. We have considered both the Yamada &
Knight tree-to-string model, which only con-
siders syntactic information from the typically
more fluent revised text, as well as Quasi-
Synchronous Grammar, a tree-to-tree model
that attempts to learn syntactic transforma-
tion patterns between the students? original
and revised texts. Our results suggests that
while QG offers a greater degree of freedom,
thus allowing for a better coverage of the
transformations, YK has a lower entropy on
the test corpus. Moreover, when presented
with an alternative ?trick? corpus in which the
?revision? is in fact the earlier draft, YK was
more perplexed than QG. These results sug-
gest that the YK model may be a promising
approach for automatic grammar error correc-
tion.
Acknowledgments
This work has been supported by NSF Grant
IIS-0745914. We thank Joel Tetreault and the
anonymous reviewers for their helpful com-
ments and suggestions.
References
Brockett, Chris, William B. Dolan, and Michael
Gamon. 2006. Correcting esl errors using
phrasal smt techniques. In Proceedings of
COLING-ACL 2006, Sydney, Australia, July.
Burstein, Jill, Martin Chodorow, and Claudia Lea-
cock. 2004. Automated essay evaluation: The
criterion online writing service. AI Magazine,
25(3).
Charniak, Eugene, Kevin Knight, and Kenji Ya-
mada. 2003. Syntax-based language models for
machine translation. In Proc. MT Summit IX,
New Orleans, Louisiana, USA.
Chodorow, Martin, Joel Tetreault, and Na-Rae
Han. 2007. Detection of grammatical errors
involving prepositions. In Proceedings of the
4th ACL-SIGSEM Workshop on Prepositions,
Prague, Czech Republic.
Das, Dipanjan and Noah A. Smith. 2009.
Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proceedings of
ACL-IJCNLP 2009, Suntec, Singapore, August.
Foster, Jennifer, Joachim Wagner, and Josef van
Genabith. 2008. Adapting a WSJ-trained
1380
parser to grammatically noisy text. In Proceed-
ings of the 46th ACL on Human Language Tech-
nologies: Short Papers, Columbus, Ohio.
Gamon, Michael, Jianfeng Gao, Chris Brock-
ett, Alexandre Klementiev, William B. Dolan,
Dmitriy Belenko, and Lucy Vanderwende. 2008.
Using contextual speller techniques and lan-
guage modeling for ESL error correction. In
Proceedings of IJCNLP, Hyderabad, India.
Han, Na-Rae, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in English article
usage by non-native speakers. Natural Language
Engineering, 12(02).
Han, Na-Rae, Joel Tetreault, Soo-Hwa Lee, and
Jin-Young Han. 2010. Using an error-annotated
learner corpus to develop and ESL/EFL er-
ror correction system. In Proceedings of LREC
2010, Valletta, Malta.
Hermet, Matthieu, Alain De?silets, and Stan Sz-
pakowicz. 2008. Using the web as a linguis-
tic resource to automatically correct Lexico-
Syntactic errors. In Proceedings of the LREC,
volume 8.
Klein, Dan and Christopher Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Pro-
ceedings of ACL 2004, Barcelona, Spain.
Knight, Kevin and Ishwar Chander. 1994. Auto-
mated postediting of documents. In Proceedings
of AAAI-94, Seattle, Washington.
Lee, John and Stephanie Seneff. 2008. Correcting
misuse of verb forms. Proceedings of the 46th
ACL, Columbus.
Liu, Ting, Ming Zhou, Jianfeng Gao, Endong
Xun, and Changning Huang. 2000. PENS: a
machine-aided english writing system for chi-
nese users. In Proceedings of the 38th ACL,
Hong Kong, China.
Lopez, Adam. 2008. Statistical machine transla-
tion. ACM Computing Surveys, 40(3), Septem-
ber.
Melamed, I. Dan, Giorgio Satta, and Ben Welling-
ton. 2004. Generalized multitext grammars. In
Proceedings of the 42nd ACL, Barcelona, Spain.
Nagata, Ryo, Atsuo Kawai, Koichiro Morihiro,
and Naoki Isu. 2006. A feedback-augmented
method for detecting errors in the writing of
learners of english. In Proceedings of COLING-
ACL 2006, Sydney, Australia, July.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical align-
ment models. Computational Linguistics, 29(1).
Smith, David A. and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft pro-
jection of syntactic dependencies. In Proceed-
ings on the Workshop on Statistical Machine
Translation, New York City, June.
Smith, David A. and Jason Eisner. 2009.
Parser adaptation and projection with quasi-
synchronous grammar features. In Proceedings
of EMNLP 2009, Singapore, August.
Wang, Mengqiu, Noah A. Smith, and Teruko Mi-
tamura. 2007. What is the Jeopardy model?
a quasi-synchronous grammar for QA. In
Proceedings of EMNLP-CoNLL 2007, Prague,
Czech Republic, June.
Wu, Dekai. 1995. Stochastic inversion transduc-
tion grammars, with application to segmenta-
tion, bracketing, and alignment of parallel cor-
pora. In Proc. of the 14th Intl. Joint Conf. on
Artificial Intelligence, Montreal, Aug.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model. In
Proceedings of the 39th ACL, Toulouse, France.
Yi, Xing, Jianfeng Gao, and William B Dolan.
2008. A web-based english proofing system for
english as a second language users. In Proceed-
ings of IJCNLP, Hyderabad, India.
1381
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 683?691,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Redundancy Detection in ESL Writings
Huichao Xue and Rebecca Hwa
Department of Computer Science,
University of Pittsburgh,
210 S Bouquet St, Pittsburgh, PA 15260, USA
{hux10,hwa}@cs.pitt.edu
Abstract
This paper investigates redundancy detec-
tion in ESL writings. We propose a mea-
sure that assigns high scores to words and
phrases that are likely to be redundant
within a given sentence. The measure is
composed of two components: one cap-
tures fluency with a language model; the
other captures meaning preservation based
on analyzing alignments between words
and their translations. Experiments show
that the proposed measure is five times
more accurate than the random baseline.
1 Introduction
Writing concisely is challenging. It is especially
the case when writing in a foreign language that
one is still learning. As a non-native speaker, it
is more difficult to judge whether a word or a
phrase is redundant. This study focuses on auto-
matically detecting redundancies in English as a
Second Language learners? writings.
Redundancies occur when the writer includes
some extraneous word or phrase that do not add
to the meaning of the sentence but possibly make
the sentence more awkward to read. Upon re-
moval of the unnecessary words or phrases, the
sentence should improve in its fluency while main-
taining the original meaning. In the NUCLE
corpus (Dahlmeier and Ng, 2011), an annotated
learner corpus comprised of essays written by pri-
marily Singaporean students, 13.71% errors are
tagged as ?local redundancy errors?, making re-
dundancy error the second most frequent prob-
lem.
1
Although redundancies occur frequently, it has
not been studied as widely as other ESL errors. A
1
The most frequent error type is Wrong collocation/idiom
preposition, which comprises 15.69% of the total errors.
major challenge is that, unlike mistakes that vio-
late the grammaticality of a sentence, redundan-
cies do not necessarily ?break? the sentence. De-
termining which word or phrase is redundant is
more of a stylistic question; it is more subjective,
and sometimes difficult even for a native speaker.
To the best of our knowledge, this paper reports
a first study on redundancy detection. In particu-
lar, we focus on the task of defining a redundancy
measure that estimates the likelihood that a given
word or phrase within a sentence might be extrane-
ous. We propose a measure that takes into account
each word?s contribution to fluency and meaning.
The fluency component computes the language
model score of the sentence after the deletion of a
word or a phrase. The meaning preservation com-
ponent makes use of the sentence?s translation into
another language as pivot, then it applies a statis-
tical machine translation (SMT) alignment model
to infer the contribution of each word/phrase to the
meaning of the sentence. As a first experiment, we
evaluate our measures on their abilities in picking
the most redundant phrase of a given length. We
show that our measure is five times more accurate
than a random baseline.
2 Redundancies in ESL Writings
According to The Elements of Style (Strunk,
1918): concise writing requires that ?every word
tell.? In that sense, words that ?do not tell?
are redundant. Determining whether a certain
word/phrase is redundant is a stylistic question,
which is difficult to quantify. As a result, most an-
notation resources do not explicitly identify redun-
dancies. One exception is the NUCLE corpus. Be-
low are some examples from the NUCLE corpus,
where the bold-faced words/phrases are marked as
redundant.
Ex
1
: First of all , there should be a careful con-
sideration about what are the things that gov-
ernments should pay for.
683
Ex
2
: GM wishes to reposition itself as an inno-
vative company to the public.
Ex
3
: These findings are often unpredictable and
uncertain.
Ex
4
: . . . the cost incurred is not only just large
sum of money . . .
These words/phrases are considered redundant be-
cause they are unnecessary (e.g. Ex
1
, Ex
2
) or
repetitive (e.g. Ex
3
, Ex
4
).
However, in NUCLE?s annotation scheme,
some words that were marked redundant are re-
ally words that carry undesirable meanings. For
example:
Ex
5
: . . . through which they can insert a special
. . .
Ex
6
: . . . the analysis and therefore selection of
a single solution for adaptation. . .
Note that unlike redundancies, these undesirable
words/phrases change the sentences? meanings.
Despite the difference in definitions, our exper-
imental work uses the NUCLE corpus because
it provides many real world examples of redun-
dancy.
While redundancy detection has not yet been
widely studied, it is related to several areas of ac-
tive research, such as grammatical error correction
(GEC), sentence simplification and sentence com-
pression.
Work in GEC attempts to build automatic sys-
tems to detect/correct grammatical errors (Lea-
cock et al., 2010; Liu et al., 2010; Tetreault et al.,
2010; Dahlmeier and Ng, 2011; Rozovskaya and
Roth, 2010). Both redundancy detection and GEC
aim to improve students? writings. However, be-
cause redundancies do not necessarily break gram-
maticality, they have received little attention in
GEC.
Sentence compression and sentence simplifica-
tion also consider deleting words from input sen-
tences. However, these tasks have different goals.
Automated sentence simplification (Coster and
Kauchak, 2011) systems aim at reducing the gram-
matical complexity of an input sentence. To il-
lustrate the difference, consider the phrase ?crit-
ical reception.? A sentence simplification sys-
tem might rewrite it into ?reviews?; but a sys-
tem that removes redundancy should leave it un-
changed because neither ?critical? nor ?reception?
is extraneous. Moreover, consider the redundant
phrase ?had once before? in Ex
4
. A simplification
system does not need to change it because these
words do not add complexity to the sentence.
?	 ?and	 ?the	 ?cost	 ?incurred	 ?is	 ?not	 ?only	 ?just	 ?large	 ?sum	 ?of	 ?money	 ?? 
Figure 1: Among the three circled words, ?just?
is more redundant because deleting it hurts nether
fluency nor meaning.
Sentence compression systems (Jing, 2000;
Knight and Marcu, 2000; McDonald, 2006;
Clarke and Lapata, 2007) aim at shortening a
sentence while retaining the most important in-
formation and keeping it grammatically correct.
This goal distinguishes these systems from ours
in two major aspects. First, sentence compres-
sion systems assume that the original sentence is
well-written; therefore retaining words specific to
the sentence (e.g. ?uncertain? in Ex
3
) can be a
good strategy (Clarke and Lapata, 2007). In the
ESL context, however, even specific words could
still be redundant. For example, although ?un-
certain? is specific to Ex
3
, it is redundant, be-
cause its meaning is already implied by ?unpre-
dictable?. Second, sentence compression systems
try to shorten a sentence as much as possible, but
an ESL redundancy detector should leave as much
of the input sentences unchanged, if possible.
One challenge involved in redundancy detection
is that it often involves open class words (Ex
3
), as
well as multi-word expressions (Ex
1
, Ex
4
). Cur-
rent GEC systems dealing with such error types
are mostly MT based. MT systems tend to ei-
ther require large training corpora (Brockett et al.,
2006; Liu et al., 2010), or provide whole sentence
rewritings (Madnani et al., 2012). Hermet and
D?esilets (2009) attempted to extract single prepo-
sition corrections from whole sentence rewritings.
Our work incorporates alignments information to
handle complex changes on both word and phrase
levels.
In our approximation, we consider MT out-
put as an approximation of word/phrase mean-
ings. Using words in other languages to repre-
sent meanings has been explored in Carpuat and
Wu (2007), where the focus is the aligned words?
identities. Our work instead focuses more on how
many words each word is aligned to.
3 A Probabilistic Model of Redundancy
We consider a word or a phrase to be redundant
if deleting it results in a fluent English sentence
that conveys the same meaning as before. For
example, ?not? and ?the? are not considered re-
684
(a) Unaligned English
words are considered
redundant.
(b) Multiple English words
aligned to the same meaning
unit. These words are con-
sidered redundant.
Figure 2: Configurations our system consider as
redundant. In each figure, the shaded squares are
the words considered to be more redundant than
other words in the same figure.
dundant in Figure 1. This is because discarding
?not? would flip the sentence?s meaning; discard-
ing ?the? would lose a necessary determiner be-
fore a noun. In contrast, discarding ?just? would
hurt neither fluency nor meaning. It is thus con-
sidered to be more redundant.
Therefore, our computational model needs to
consider words? contributions to both fluency and
meaning. Figure 2 illustrates words? contribution
to meaning. In those two examples, each sub-
graph visualizes a sentence: English words corre-
spond to squares in the top row, while their mean-
ings correspond to circles in the bottom row. The
knowledge of which word represents what mean-
ing helps in evaluating its contribution. In partic-
ular, if a word does not connote any significant
meaning, deleting it would not affect the overall
sentence; if several words express the same mean-
ing, then deleting some of them might not affect
the overall sentence either. Also, deleting a more
semantically meaningful word (or phrase) is more
likely to cause a loss of meaning of the overall sen-
tence (e.g. uncertain v.s. the).
Our model computes a single probabilistic value
for both fluency judgment and meaning preserva-
tion ? the log-likelihood that after deleting a cer-
tain word or phrase of a sentence, the new sen-
tence is still fluent and conveys the same meaning
as before. This value reflects our definition of re-
dundancy ? the higher this probability, the more
redundant the given word/phrase is.
More formally, suppose an English sentence e
contains l
e
words: e = e
1
e
2
. . . e
l
e
; after some
sub-string e
s,t
= e
s
. . . e
t
(1 ? s ? t ? l
e
) is
deleted from e, we obtain a shorter sentence, de-
noted as e
s,t
?
. We wish to compute the quantity
R(s, t; e), the chance that the sub-string e
s,t
is re-
dundant in sentence e. We propose a probabilistic
model to formalize this notion.
LetM be a random variable over some meaning
representation; Pr(M |e) is the likelihood that M
carries the meaning of e. If the sub-string e
s,t
is
redundant, then the new sentence e
s,t
?
should still
express the same meaning; Pr(e
s,t
?
|M) computes
the likelihood that the after-deletion sentence can
be generated from meaning M .
R(s, t; e)
= log
?
M=m
Pr(m|e) Pr(e
s,t
?
|m)
= log
?
M=m
Pr(m|e) Pr(e
s,t
?
) Pr(m|e
s,t
?
)
Pr(m)
= log Pr(e
s,t
?
) + log
?
M=m
Pr(m|e
s,t
?
) Pr(m|e)
Pr(m)
= LM(e
s,t
?
) + AGR(M |e
s,t
?
, e) (1)
The first term LM(e
s,t
?
) is the after-deletion sen-
tence?s log-likelihood, which reflects its fluency.
We calculate the first term with a trigram language
model (LM).
The second term AGR(M |e
s,t
?
, e) can be inter-
preted as the chance that e and e
s,t
?
carry the same
meaning, discounted by ?chance agreement?. This
term captures meaning preservation.
The two terms above are complementary to each
other. Intuitively, LM prefers keeping common
words in e
s,t
?
(e.g. the, to) while AGR prefers
keeping words specific to e (e.g. disease, hyper-
tension).
To make the calculation of the second term
practical, we make two simplifying assumptions.
Assumption 1 A sentence?s meaning can be rep-
resented by its translations in another language; its
words? contributions to the meaning of the sen-
tence can be represented by the mapping between
the words in the original sentence and its transla-
tions (Figure 3).
Note that the choice of translation language may
impact the interpretation of words? contributions.
We will discuss about this issue in our experiments
(Section 5).
Assumption 2 Instead of considering all possi-
ble translations f for e, our computation will make
use of the most likely translation, f
?
.
685
?? ?? ? ???? ?? ????
??
?? ????
a new idea is created through results from rigorous process of research .
Figure 3: Illustration of Assumption 1 and Approximation 1. An English sentence?s meaning is presented
as a Chinese translation. Meanwhile, each (English) word?s contribution to the sentence meaning is
realized as a word alignment. For Approximation 1, note that sentence alignments normally won?t be
affected before/after deleting words (e.g. ?results from?) from the source sentence.
With the two approximations:
AGR(M |e
s,t
?
, e) ? log
Pr(f
?
|e
s,t
?
) Pr(f
?
|e)
Pr(f
?
)
= log Pr(f
?
|e
s,t
?
) + C
1
(e)
(We use C
i
(e) to denote constant numbers within
sentence e throughout the paper.)
We now rely on a statistical machine translation
model to approximate the translation probability
log Pr(f
?
|e
s,t
?
).
One naive way of calculating this probabil-
ity measure is to consult the MT system. This
method, however, is too computationally expen-
sive for one single input sentence. For a sentence
of length n, calculating the redundancy measure
for all chunks in it would require issuing O(n
2
)
translation queries. We propose an approximation
that instead calculates the difference of translation
probability caused by discarding e
s,t
, based on an
analysis on the alignment structure between e and
f
?
. We show the measure boils down to sum-
ming the expected number of aligned words for
each e
i
(s ? i ? t), and possibly weighting these
numbers by e
i
?s unigram probability. This method
requires one translation query, and O(n
2
) queries
into a language model, which is much more suit-
able for practical applications. Our method also
sheds light on the role of alignment structures in
the redundancy detection context.
3.1 Alignments Approximation
One key insight in our approximation is that the
alignment structure a between e
s,t
?
and f
?
would
be largely similar with the alignment structure be-
tween e and f
?
. We illustrate this notion in Fig-
ure 3. Note that after deleting two words ?results
from? from the source sentence in Figure 3, the
alignment structure remains unchanged elsewhere.
Also, ????, the word once connected with ?re-
sults?, can now be seen as connected to blanks.
We hence approximate log Pr(f
?
|e
s,t
?
) by
reusing the alignment structure between e
and f
?
. To make the alignment structures
compatible, we start with redefining e
s,t
?
as
e
1
, e
2
, . . . , e
s?1
,, . . . ,, e
t+1
, . . . , e
l
e
, where
the deleted words are left blank.
Let Pr(a|f, e) be the posterior distribution of
alignment structure between sentence pair (f, e).
Approximation 1 We formalize the similarity
between the alignment structures by assuming the
KL-divergence between their alignment distribu-
tions to be small.
D
KL
(a|f
?
, e; a|f
?
, e
s,t
?
) ? 0
This allows using Pr(a|f
?
, e) to help approximate
log Pr(f
?
|e
s,t
?
):
log Pr(f
?
|e
s,t
?
)
= log
?
a
Pr(a|f
?
, e)
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
=
?
a
Pr(a|f
?
, e) log
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
+
?
a
Pr(a|f
?
, e) log
(
Pr(f
?
|e
s,t
?
)/
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
)
? ?? ?
D
KL
(a|f
?
,e;a|f
?
,e
s,t
?
)?0
?
?
a
Pr(a|f
?
, e) log Pr(f
?
|e
s,t
?
, a) + C
2
(e)
We then use an SMT model to calculate
log Pr(f
?
|e
s,t
?
, a), the translation probability under
a given alignment structure.
3.2 The Translation Model
Approximation 2 We will use IBM
Model 1 (Brown et al., 1993) to calculate
log Pr(f
?
|e
s,t
?
, a)
IBM Model 1 is one of the earliest statisti-
cal translation models. It helps us to compute
686
log Pr(f
?
|e
s,t
?
, a) by making explicit how each
word contributes to words it aligns with. In partic-
ular, to compute the probability that f is a transla-
tion of e, Pr(f |e), IBM Model 1 defined a gener-
ative alignment model where every word f
i
in f is
aligned with exactly one word e
a
i
in e, so that f
i
and e
a
i
are word level translations of each other.
?
a
Pr(a|f
?
, e) log Pr(f
?
|e
s,t
?
, a)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log Pr(f
i
?
|e
s,t
?a
i
)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
+ C
3
(e)
Note that
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
=
{
0 , for a
i
/? {s . . . t}
log
Pr(f
i
?
|)
Pr(f
i
?
|e
a
i
)
, otherwise
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
?
s?j?t
I
a
i
=j
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
=
?
s?j?t
?
1?i?l
f
?
Pr(a
i
= j|f
?
, e)
? ?? ?
A
i,j
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
? ?? ?
DIFF(e
s,t
?
,e)
Here A
i,j
= Pr(a
i
= j | f
?
, e), which is the
probability of the i-th word in the translation being
aligned to the j-th word in the original sentence.
3.3 Per-word Contribution
Through deductions,
R(s, t; e) = LM(e
s,t
?
) + DIFF(e
s,t
?
, e)
+C
1
(e) + C
2
(e) + C
3
(e)
the redundancy measure boils down to how we
define Pr(f
i
?
|
j
), which is: when we discard e
j
,
how do we generate the word it aligns f
i
?
with in
its translation. This value reflects e
j
?s contribution
in generating f
i
?
.
We approximate Pr(f
i
?
|
j
) in two ways.
1. Suppose that all words in the translation
are of equal importance. We assume
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
= ?C
c
, where C
c
is a constant
number. A larger C
c
value indicates a higher
importance of e
j
during the translation.
DIFF(e
s,t
?
, e) = ?C
c
?
s?j?t
?
1?i?l
f
?
A
i,j
= ?C
c
?
s?j?t
A(j) (2)
Here A(j) is the expected number of align-
ments to e
j
. This metric demonstrates the in-
tuition that words aligned to more words in
the translation are less redundant.
2. We note that rare words are often more im-
portant, and therefore harder to be generated.
We assume Pr(f
i
?
|) = Pr(e
j
|) Pr(f
i
?
|e
j
).
DIFF(e
s,t
?
, e)
=
?
s?j?t
?
1?i?l
f
?
A
i,j
log
Pr(e
j
|) Pr(f
i
?
|e
j
)
Pr(f
i
?
|e
j
)
=
?
s?j?t
A(j) log Pr(e
j
|) (3)
This gives us counts on how likely each
word is aligned with Chinese words ac-
cording to Pr(a|f
?
, e), where each word is
weighted by its importance log Pr(e
j
|). We
use e
j
?s unigram probability to approximate
log Pr(e
j
|).
When estimating the alignment probabilities
A
i,j
, we smooth the alignment result from Google
translation using Dirichlet-smoothing, where we
set ? = 0.1 empirically based on experiments in
the development dataset.
4 Experimental Setup
A fully automated redundancy detector has to de-
cide (1) whether a given sentence contains any re-
dundancy errors; (2) how many words constitute
the redundant part; and (3) which exact words are
redundant. In this paper, we focus on the third part
while assuming the first two are given. Thus, our
experimental task is: given a sentence known to
contain a redundant phrase of a particular length,
can that redundant phrase be accurately identified?
For most sentences in our study, this results in
choosing one from around 20 words/phrases.
While the task has a somewhat limited scope, it
allows us to see how we could formally measure
the difference between redundant words/phrases
687
and non-redundant ones. For each measure, we
observe whether it has assigned the highest score
to the redundant part of the sentence. We compare
the proposed redundancy model described in Sec-
tion 3 against a set of baselines and other potential
redundancy measures (to be described shortly).
To better understand different measures? perfor-
mance on function words vs. content words, we
also calculate the percentage of redundant func-
tion/content words that are detected successfully ?
accuracy in both categories. In our experiments,
we consider prepositions and determiners as func-
tion words; and we consider other words/phrases
as content words/phrases.
4.1 Redundancy Measures
To gain insight into redundancy error detection?s
difficulty, we first consider a random baseline.
random The random baseline assigns a random
score to each word/phrase. The resulting system
will pick one word/phrase of the given length at
random.
We consider relying on large scale language
models to decide redundancy.
trigram We use a trigram language model to
capture fluency, by calculating the log-likelihood
of the whole sentence after discarding the given
word/phrase. A higher probability indicates a
higher fluency.
round-trip Inspired by Madnani et al. (2012;
Hermet and D?esilets (2009), an MT system may
eliminate grammar errors with the help of large
scale language models. In this method, we analyze
which parts are considered redundant by an MT
system by comparing the original sentence with
its round-trip translation. We use Google trans-
late to first translate one sentence into a pivot lan-
guage, and then back to English. We measure one
phrase?s redundancy by the number of words that
disappeared after the round-trip. We determine if
one word disappeared in two ways:
extract word match: one word is considered
disappeared if the same word does not occur
in the round-trip.
aligned word: we use the Berkeley
aligner (DeNero and Klein, 2007) to
align original sentences with their round-trip
translations. Unaligned words are considered
to have disappeared.
We consider measures for words/phrases? con-
tributions to sentence meaning.
sig-score This measure accounts for whether
one word w
i
is capturing the gist of a sen-
tence (Clarke and Lapata, 2007)
2
. It was shown to
help decide whether one part should be discarded
during sentence compression.
I(w
i
) = ?
l
N
? f
i
log
F
a
F
i
f
i
and F
i
are the frequencies of w
i
in the current
document and a large corpus respectively; F
a
is
the number of all word occurrences in the corpus;
l is the number of clause constituents above w
i
;
N is the deepest level of clause embeddings. This
measure assigns low scores to document specific
words occurring at deep syntax levels.
align # We use the number of alignments that a
word/phrase has in the translation to measure its
redundancy, as deducted in Equation 2.
contrib We compute the word/phrase?s contri-
bution to meaning, according to Equation 3.
We consider the combinations of measures.
trigram + C
c
align # We use a linear combina-
tion between language model and align # (Equa-
tion 2). We tune C
c
on development data.
trigram+contrib This measure (as we proposed
in Section 3) is the sum of the trigram lan-
guage model component and the contrib compo-
nent which represents the phrase?s contribution to
meaning.
trigram+? round-trip/sig-score We combine
language model with round-trip and sig-score
linearly (McDonald, 2006; Clarke and Lapata,
2007). To obtain baselines that are as strong as
possible, we tune the weight ? on evaluation data
for best accuracy.
4.2 Pivot Languages
Our proposed model uses machine translation out-
puts from different pivot languages. To see which
language helps measuring redundancy, we com-
pare 52 pivot languages available at Google trans-
late
3
for meaning representation
4
.
2
We extend this measure, which was only defined for con-
tent words in Clarke and Lapata (2007), to include all English
words.
3
http://translate.google.com
4
These languages include Albanian (sq), Arabic (ar),
Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu),
688
length count percentage
1 356 67.55%
2 80 15.18%
3 40 7.59%
4 18 3.42%
other 33 6.26%
Table 1: Length distribution of redundant chunks?
lengths in the evaluation data.
4.3 Data and Tools
We extract instances from the NUCLE cor-
pus (Dahlmeier and Ng, 2011), an error annotated
corpus mainly written by Singaporean students,
to conduct this study. The corpus is composed
of 1,414 student essays on various topics. An-
notations in NUCLE include error locations, er-
ror types, and suggested corrections. Redundancy
errors are marked by annotators as Rloc. In this
study, we only consider the cases where the sug-
gested correction is to delete the redundant part
(97.09% among all Rloc errors).
To construct our evaluation dataset, we
pick sentences with exactly one redundant
word/phrase. This is the most common case
(81.18%) among sentences containing redundant
words/phrases. We use 10% of the essays (336
sentences) for development purposes, and an-
other 200 essays as the evaluation corpus (527
sentences). A distribution of redundant chunks?
lengths in evaluation corpus is shown in Table 1.
We train a trigram language model using the
SRILM toolkit (Stolcke, 2002) on the Agence
France-Presse (afp) portion of the English Giga-
words corpus.
5 Experiments
The experiment aims to address the following
questions: (1) Does a sentence?s translation serve
as a reasonable approximation for its meaning? (2)
Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl),
Persian (fa), Boolean (language ((Afrikaans) (af), Danish
(da), German (de), Russian (ru), French (fr), Tagalog (tl),
Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu),
Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl),
Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin
(la), Latvian (lv), Lao (lo), Lithuanian (lt), Romanian (ro),
Maltese (mt), Malay (ms), Macedonian (mk), Bengali (bn),
Norwegian (no), Portuguese (pt), Japanese (ja), Swedish (sv),
Serbian (sr), Esperanto (eo), Slovak (sk), Slovenian (sl),
Swahili (sw), Telugu (te), Tamil (ta), Thai (th), Turkish (tr),
Welsh (cy), Urdu (ur), Ukrainian (uk), Hebrew (iw), Greek
(el), Spanish (es), Hungarian (hu), Armenian (hy), Italian (it),
Yiddish (yi), Hindi (hi), Indonesian (id), English (en), Viet-
namese (vi), Simplified Chinese (zh-CN), Traditional Chi-
nese (zh-TW).
Metrics overall
function
words
content
words
random 4.44% 4.62% 4.36%
trigram 8.06% 3.95% 9.73%
sig-score 10.71% 22.16% 6.07%
round-trip (aligned word) 10.69% 12.72% 9.87%
round-trip (exact word
match)
5.75% 4.27% 6.35%
trigram + ? round-trip
(aligned word)
14.80% 11.84% 16.00%
trigram + ? round-trip
(exact word match)
9.49% 4.61% 11.47%
trigram + ? sig-score 11.01% 22.68% 6.28%
align # 5.04% 3.36% 5.72%
trigram + C
c
?align # 9.58% 4.61% 11.60%
contrib 8.59% 20.23% 3.87%
trigram + contrib 21.63% 38.16% 14.93%
Table 2: Redundancy part identification accuracies
for different redundancy metrics on NUCLE cor-
pus, using French as the pivot language.
If so, does the choice of the pivot language matter?
(3) How do the potentially conflicting goals of pre-
serving fluency versus preserving meaning impact
the definition of a redundancy measure?
Our experimental results are presented in Fig-
ure 4 and Table 2. In Figure 4 we compare using
different pivot languages in our proposed model;
in Table 2 we compare using different redundancy
metrics for the same pivot language ? French.
Figure 4: Using different pivot languages for re-
dundancy measurement.
First, compared to other measures, our proposed
model best captures redundancy. In particular, our
model picks the correct redundant chunk 21.63%
of the time, which is five times higher than the ran-
dom baseline. This suggests that using translation
to approximate sentence meanings is a plausible
option. Note that one partial reason for the low
figures is the limitation of data resources. During
error analysis, we found linkers/connectors (e.g.
moreover, however) and modal auxiliaries (e.g.
689
can, had) are often marked redundant when they
actually carry undesirable meanings (Ex
6
, Ex
5
).
These cases comprise a 16% portion among our
model?s failures. Despite this limitation, the evalu-
ation still suggests that current approaches are not
ready for a full redundancy detection pipeline.
Second, we find that the choice of pivot lan-
guage does make a difference. Experimental result
suggests that the system tends to achieve higher
redundancy detection accuracy when using trans-
lations of a language more similar to English. In
particular, when using European languages (e.g.
German (de), French (fr), Hungarian (hu) etc.) as
pivot, the system performs much better than using
Asian languages (e.g. Chinese (zh-CN), Japanese
(ja), Thai (th) etc.). One reason for this phe-
nomenon is that the default Google translation out-
put in Asian languages (as well as the alignment
between English and these languages) are orga-
nized into characters, while characters are not the
minimum meaning component. For example, in
Chinese, ???? is the translation of ?explana-
tion?, but the two characters ??? and ??? mean
?to solve? and ?to release? respectively. In the
alignment output, this will cause certain words be-
ing associated with more or less alignments than
others. In this case, the number of alignments no
longer directly reflect how many meaning units
a certain word helps to express. To confirm this
phenomenon, we tried improving the system using
Simplified Chinese as the pivot language by merg-
ing characters together. In particular, we applied
Chinese tokenization (Chang et al., 2008), and
then merged alignments accordingly. This raised
the system?s accuracy from 17.74% to 20.11%.
Third, to better understand the salient features
of a successful redundancy measure, we experi-
mented with using different components in isola-
tion. We find that the language model component
is better at detecting redundant content words,
while the alignment analysis component is better
at detecting redundant function words. The lan-
guage model detects the function word redundan-
cies with a worse accuracy than the random base-
line; the alignment analysis component also has a
worse accuracy than the random baseline on con-
tent words. However, the English language model
and the alignment analysis result can build on top
of each other when we analyze the redundancies.
We also found that alignments help us to better
account for each word?s contribution to the ?mean-
ing? of the sentence. A linear combination of a
language model score and our proposed measure
based on analysis of alignments best captures re-
dundancy. However, as our experimental results
suggest, it is necessary both to use alignments in
translation outputs, and to use them in a good
way. Alignments help isolating fluency from the
meaning component ? making them easy to inte-
grate. As our experiments demonstrated, although
methods comparing Google round-trip transla-
tion?s output with the original sentence could lead
to a 10.69% prediction accuracy, it is harder to
combine it with the English language model. This
is partly because of the non-orthogonality of these
two measures ? the English language model has
already been used in the round-trip translation re-
sult. Also, an information theoretical interpreta-
tion of alignments is essential for the model?s suc-
cess. For example, a more naive way of using
alignment results, align #, which counts the num-
ber of alignments, leads to a much lower accuracy.
6 Conclusions
Despite the prevalence of redundant phrases in
ESL writings, there has not been much work in the
automatic detection of these problems. We con-
duct a first study on developing a computational
model of redundancies. We propose to account for
words/phrases redundancies by comparing an ESL
sentence with outputs from off-the-shelf machine
translation systems. We propose a redundancy
measure based on this comparison. We show that
by interpreting the translation outputs with IBM
Models, redundancies can be measured by a lin-
ear combination of a language model score and
the words? contribution to the sentence?s mean-
ing. This measure accounts for both the fluency
and completeness of a sentence after removing one
chunk. The proposed measure outperforms the di-
rect round-trip translation and a random baseline
by a large margin.
Acknowledgements
This work is supported by U.S. National Science
Foundation Grant IIS-0745914. We thank the
anonymous reviewers for their suggestions; we
also thank Joel Tetreault, Janyce Wiebe, Wencan
Luo, Fan Zhang, Lingjia Deng, Jiahe Qian, Nitin
Madnani and Yafei Wei for helpful discussions.
690
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
smt techniques. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 249?
256, Sydney, Australia. Association for Computa-
tional Linguistics.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11.
Will Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9, Portland, Oregon, June. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 915?923, Portland, Oregon, USA.
Association for Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthieu Hermet and Alain D?esilets. 2009. Using first
and second language models to correct preposition
errors in second language authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 64?72.
Association for Computational Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315. Association for Computa-
tional Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence and Twelfth
Conference on Innovative Applications of Artificial
Intelligence, pages 703?710. AAAI Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated grammatical error detection for
language learners. Synthesis lectures on human lan-
guage technologies, 3(1):1?134.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 1068?1076, Cam-
bridge, Massachusetts. Association for Computa-
tional Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182?
190, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
volume 6, pages 297?304. Association for Compu-
tational Linguistics.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 961?970, Cambridge, Massachusetts. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the interna-
tional conference on spoken language processing,
volume 2, pages 901?904.
William Strunk. 1918. The elements of style / by
William Strunk, Jr.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selec-
tion and error detection. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ?10,
pages 353?358, Uppsala, Sweden. Association for
Computational Linguistics.
691
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1305?1315,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Shallow Analysis Based Assessment of Syntactic Complexity for
Automated Speech Scoring
Suma Bhat
Beckman Institute,
University of Illinois,
Urbana, IL
spbhat2@illinois.edu
Huichao Xue
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA
hux10@cs.pitt.edu
Su-Youn Yoon
Educational Testing Service
Princeton, NJ
syoon@ets.org
Abstract
Designing measures that capture various
aspects of language ability is a central
task in the design of systems for auto-
matic scoring of spontaneous speech. In
this study, we address a key aspect of lan-
guage proficiency assessment ? syntactic
complexity. We propose a novel measure
of syntactic complexity for spontaneous
speech that shows optimum empirical per-
formance on real world data in multiple
ways. First, it is both robust and reliable,
producing automatic scores that agree well
with human rating compared to the state-
of-the-art. Second, the measure makes
sense theoretically, both from algorithmic
and native language acquisition points of
view.
1 Introduction
Assessment of a speaker?s proficiency in a second
language is the main task in the domain of au-
tomatic evaluation of spontaneous speech (Zech-
ner et al, 2009). Prior studies in language ac-
quisition and second language research have con-
clusively shown that proficiency in a second lan-
guage is characterized by several factors, some of
which are, fluency in language production, pro-
nunciation accuracy, choice of vocabulary, gram-
matical sophistication and accuracy. The design of
automated scoring systems for non-native speaker
speaking proficiency is guided by these studies in
the choice of pertinent objective measures of these
key aspects of language proficiency.
The focus of this study is the design and per-
formance analysis of a measure of the syntactic
complexity of non-native English responses for
use in automatic scoring systems. The state-of-
the art automated scoring system for spontaneous
speech (Zechner et al, 2009; Higgins et al, 2011)
currently uses measures of fluency and pronuncia-
tion (acoustic aspects) to produce scores that are in
reasonable agreement with human-rated scores of
proficiency. Despite its good performance, there
is a need to extend its coverage to higher order as-
pects of language ability. Fluency and pronunci-
ation may, by themselves, already be good indi-
cators of proficiency in non-native speakers, but
from a construct validity perspective
1
, it is neces-
sary that an automatic assessment model measure
higher-order aspects of language proficiency. Syn-
tactic complexity is one such aspect of proficiency.
By ?syntactic complexity?, we mean a learner?s
ability to use a wide range of sophisticated gram-
matical structures.
This study is different from studies that fo-
cus on capturing grammatical errors in non-native
speakers (Foster and Skehan, 1996; Iwashita et al,
2008). Instead of focusing on grammatical errors
that are found to be highly representative of lan-
guage proficiency, our interest is in capturing the
range of forms that surface in language production
and the degree of sophistication of such forms,
collectively referred to as syntactic complexity in
(Ortega, 2003).
The choice and design of objective measures of
language proficiency is governed by two crucial
constraints:
1. Validity: a measure should show high dis-
criminative ability between various levels of
language proficiency, and the scores pro-
duced by the use of this measure should show
high agreement with human-assigned scores.
2. Robustness: a measures should be derived
automatically and should be robust to errors
in the measure generation process.
A critical impediment to the robustness con-
straint in the state-of-the-art is the multi-stage au-
1
Construct validity is the degree to which a test measures
what it claims, or purports, to be measuring and an important
criterion in the development and use of assessments or tests.
1305
tomated process, where errors in the speech recog-
nition stage (the very first stage) affect subsequent
stages. Guided by studies in second language de-
velopment, we design a measure of syntactic com-
plexity that captures patterns indicative of profi-
cient and non-proficient grammatical structures by
a shallow-analysis of spoken language, as opposed
to a deep syntactic analysis, and analyze the per-
formance of the automatic scoring model with its
inclusion. We compare and contrast the proposed
measure with that found to be optimum in Yoon
and Bhat (2012).
Our primary contributions in this study are:
? We show that the measure of syntactic com-
plexity derived from a shallow-analysis of
spoken utterances satisfies the design con-
straint of high discriminative ability between
proficiency levels. In addition, including our
proposed measure of syntactic complexity in
an automatic scoring model results in a statis-
tically significant performance gain over the
state-of-the-art.
? The proposed measure, derived through a
completely automated process, satisfies the
robustness criterion reasonably well.
? In the domain of native language acquisition,
the presence or absence of a grammatical
structure indicates grammatical development.
We observe that the proposed approach ele-
gantly and effectively captures this presence-
based criterion of grammatical development,
since the feature indicative of presence or ab-
sence of a grammatical structure is optimal
from an algorithmic point of view.
2 Related Work
Speaking in a non-native language requires diverse
abilities, including fluency, pronunciation, into-
nation, grammar, vocabulary, and discourse. In-
formed by studies in second language acquisition
and language testing that regard these factors as
key determiners of spoken language proficiency,
some researchers have focused on the objective
measurement of these aspects of spoken language
in the context of automatic assessment of language
ability. Notable are studies that have focused on
assessment of fluency (Cucchiarini et al, 2000;
Cucchiarini et al, 2002), pronunciation (Witt and
Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner
et al, 2009). The relative success of these studies
has yielded objective measures of acoustic aspects
of speaking ability, resulting in a shift in focus
to more complex aspects of assessment of gram-
mar (Bernstein et al, 2010; Chen and Yoon, 2011;
Chen and Zechner, 2011), topic development (Xie
et al, 2012), and coherence (Wang et al, 2013).
In an effort to assess grammar and usage in a
second language learning environment, numerous
studies have focused on identifying relevant quan-
titative measures. These measures have been used
to estimate proficiency levels in English as a sec-
ond language (ESL) writing with reasonable suc-
cess. Wolf-Quintero et al (1998), Ortega (2003),
and Lu (2010) found that measures such as mean
length of T-unit
2
and dependent clauses per clause
(henceforth termed as length-based measures) are
well correlated with holistic proficiency scores
suggesting that these quantitative measures can be
used as objective indices of grammatical develop-
ment.
In the context of spoken ESL, these measures
have been studied as well but the results have been
inconclusive. The measures could only broadly
discriminate between students? proficiency levels,
rated on a scale with moderate to weak correla-
tions, and strong data dependencies on the par-
ticipant groups were observed (Halleck, 1995;
Iwashita et al, 2008; Iwashita, 2010).
With the recent interest in the area of auto-
matic assessment of speech, there is a concur-
rent need to assess the grammatical development
of ESL students automatically. Studies that ex-
plored the applicability of length-based measures
in an automated scoring system (Chen and Zech-
ner, 2011; Chen and Yoon, 2011) observed another
important drawback of these measures in that set-
ting. Length-based measures do not meet the con-
straints of the design, that, in order for measures
to be effectively incorporated in the automated
speech scoring system, they must be generated in
a fully automated manner, via a multi-stage au-
tomated process that includes speech recognition,
part of speech (POS) tagging, and parsing.
A major bottleneck in the multi-stage process
of an automated speech scoring system for second
language is the stage of automated speech recog-
nition (ASR). Automatic recognition of non-native
speakers? spontaneous speech is a challenging task
as evidenced by the error rate of the state-of-the-
2
T-units are defined as ?the shortest grammatically allow-
able sentences into which writing can be split.? (Hunt, 1965)
1306
art speech recognizer. For instance, Chen and
Zechner (2011) reported a 50.5% word error rate
(WER) and Yoon and Bhat (2012) reported a 30%
WER in the recognition of ESL students? spoken
responses. These high error rates at the recogni-
tion stage negatively affect the subsequent stages
of the speech scoring system in general, and in
particular, during a deep syntactic analysis, which
operates on a long sequence of words as its con-
text. As a result, measures of grammatical com-
plexity that are closely tied to a correct syntac-
tic analysis are rendered unreliable. Not surpris-
ingly, Chen and Zechner (2011) studied measures
of grammatical complexity via syntactic parsing
and found that a Pearson?s correlation coefficient
of 0.49 between syntactic complexity measures
(derived from manual transcriptions) and profi-
ciency scores, was drastically reduced to near non-
existence when the measures were applied to ASR
word hypotheses. This suggests that measures
that rely on deep syntactic analysis are unreliable
in current ASR-based scoring systems for sponta-
neous speech.
In order to avoid the problems encountered
with deep analysis-based measures, Yoon and
Bhat (2012) explored a shallow analysis-based ap-
proach, based on the assumption that the level of
grammar sophistication at each proficiency level
is reflected in the distribution of part-of-speech
(POS) tag bigrams. The idea of capturing dif-
ferences in POS tag distributions for classification
has been explored in several previous studies. In
the area of text-genre classification, POS tag dis-
tributions have been found to capture genre differ-
ences in text (Feldman et al, 2009; Marin et al,
2009); in a language testing context, it has been
used in grammatical error detection and essay
scoring (Chodorow and Leacock, 2000; Tetreault
and Chodorow, 2008). We will see next what as-
pects of syntactic complexity are captured by such
a shallow-analysis.
3 Shallow-analysis approach to
measuring syntactic complexity
The measures of syntactic complexity in this ap-
proach are POS bigrams and are not obtained by a
deep analysis (syntactic parsing) of the structure of
the sentence. Hence we will refer to this approach
as ?shallow analysis?. In a shallow-analysis ap-
proach to measuring syntactic complexity, we rely
on the distribution of POS bigrams at every profi-
ciency level to be representative of the range and
sophistication of grammatical constructions at that
level. At the outset, POS-bigrams may seem too
simplistic to represent any aspect of true syntactic
complexity. We illustrate to the contrary, that they
are indeed able to capture certain grammatical er-
rors and sophisticated constructions by means of
the following instances. Consider the two sentence
fragments below taken from actual responses (the
bigrams of interest and their associated POS tags
are bold-faced).
1. They can/MD to/TO survive . . .
2. They created the culture/NN that/WDT
now/RB is common in the US.
We notice that Example 1 is not only less gram-
matically sophisticated than Example 2 but also
has a grammatical error. The error stems from the
fact that it has a modal verb followed by the word
?to?. On the other hand, Example 2 contains a
relative clause composed of a noun introduced by
?that?. Notice how these grammatical expressions
(one erroneous and the other sophisticated) can be
detected by the POS bigrams ?MD-TO? and ?NN-
WDT?, respectively.
The idea that the level of syntactic complex-
ity (in terms of its range and sophistication) can
be assessed based on the distribution of POS-tags
is informed by prior studies in second language
acquisition. It has been shown that the usage of
certain grammatical constructions (such as that of
the embedded relative clause in the second sen-
tence above) are indicators of specific milestones
in grammar development (Covington et al, 2006).
In addition, studies such as Foster and Skehan
(1996) have successfully explored the utility of
frequency of grammatical errors as objective mea-
sures of grammatical development.
Based on this idea, Yoon and Bhat (2012) de-
veloped a set of features of syntactic complex-
ity based on POS sequences extracted from a
large corpus of ESL learners? spoken responses,
grouped by human-assigned scores of proficiency
level. Unlike previous studies, it did not rely
on the occurrence of normative grammatical con-
structions. The main assumption was that each
score level is characterized by different types of
prominent grammatical structures. These repre-
sentative constructions are gathered from a collec-
tion of ESL learners? spoken responses rated for
overall proficiency. The syntactic complexity of
a test spoken response was estimated based on its
1307
similarity to the proficiency groups in the refer-
ence corpus with respect to the score-specific con-
structions. A score was assigned to the response
based on how similar it was to the high score
group. In Section 4.1, we go over the approach
in further detail.
Our current work is inspired by the shallow
analysis-based approach of Yoon and Bhat (2012)
and operates under the same assumptions of cap-
turing the range and sophistication of grammati-
cal constructions at each score level. However,
the approaches differ in the way in which a spo-
ken response is assigned to a score group. We
first analyze the limitations of the model studied in
(Yoon and Bhat, 2012) and then describe how our
model can address those limitations. The result is
a new measure based on POS bigrams to assess
ESL learners? mastery of syntactic complexity.
4 Models for Measuring Grammatical
Competence
We mentioned that the measure proposed in this
study is derived from assumptions similar to those
studied in (Yoon and Bhat, 2012). Accordingly,
we will summarize the previously studied model,
outline its limitations, show how our proposed
measure addresses those limitations and compare
the two measures for the task of automatic scoring
of speech.
4.1 Vector-Space Model based approach
Yoon and Bhat (2012) explored an approach in-
spired by information retrieval. They treat the con-
catenated collection of responses from a particular
score-class as a ?super? document. Then, regard-
ing POS bigrams as terms, they construct POS-
based vector space models for each score-class
(there are four score classes denoting levels of pro-
ficiency as will be explained in Section 5.2), thus
yielding four score-specific vector-space models
(VSMs). The terms of the VSM are weighted by
the term frequency-inverse document frequency
(tf -idf ) weighting scheme (Salton et al, 1975).
The intuition behind the approach is that responses
in the same proficiency level often share similar
grammar and usage patterns. The similarity be-
tween a test response and a score-specific vector is
then calculated by a cosine similarity metric. Al-
though a total of 4 cosine similarity scores (one
per score group) were generated, only cos
4
from
among the four similarity scores, and cosmax,
were selected as features.
? cos
4
: the cosine similarity score between the
test response and the vector of POS bigrams
for the highest score class (level 4); and,
? cosmax: the score level of the VSM with
which the given response shows maximum
similarity.
Of these, cos
4
was selected based on its empir-
ical performance (it showed the strongest corre-
lation with human-assigned scores of proficiency
among the distance-based measures). In addition,
an intuitive justification for the choice is that the
score-4 vector is a grammatical ?norm? represent-
ing the average grammar usage distribution of the
most proficient ESL students. The measure of syn-
tactic complexity of a response, cos
4
, is its simi-
larity to the highest score class.
The study found that the measures showed rea-
sonable discriminative ability across proficiency
levels. Despite its encouraging empirical perfor-
mance, the VSM method of capturing grammati-
cal sophistication has the following limitations.
First, the VSM-based method is likely to over-
estimate the contribution of the POS bigrams
when highly correlated bigrams occur as terms in
the VSM. Consider the presence of a grammar pat-
tern represented by more than one POS bigram.
For example, both ?NN-WDT? and ?WDT-RB? in
Sentence 2 reflect the learner?s usage of a relative
clause. However, we note that the two bigrams are
correlated and including them both results in an
over-estimation of their contribution. The VSM
set-up has no mechanism to handle correlated fea-
tures.
Second, the tf -idf weighting scheme for rela-
tively rare POS bigrams does not adequately cap-
ture their underlying distribution with respect to
score groups. Grammatical expressions that occur
frequently in one score level but rarely in other
levels can be assumed to be characteristic of a
specific score level. Therefore, the more uneven
the distribution of a grammatical expression across
score classes, the more important that grammatical
expression should be as an indicator of a particular
score class. However, the simple idf scheme can-
not capture this uneven distribution. A pattern that
occurs rarely but uniformly across different score
groups can get the same weight as a pattern which
is unevenly distributed to one score group. Mar-
tineau and Finin (2009) observed this weakness of
the tf -idf weighting in the domain of sentiment
1308
analysis. When using tf -idf weighting to extract
words that were strongly associated with positive
sentiment in a movie review corpus (they consid-
ered each review as a document and a word as a
term), it was found that a substantial proportion
of words with the highest tf -idf were rare words
(e.g., proper nouns) which were not directly asso-
ciated with the sentiment.
We propose to address these important limita-
tions of the VSM approach by the use of a method
that accounts for each of the deficiencies. This is
done by resorting to a maximum entropy model
based approach, to which we turn next.
4.2 Maximum Entropy-Based model
In order to address the limitations discussed in 4.1,
we propose a classification-based approach. Tak-
ing an approach different from previous studies,
we formulate the task of assigning a score of syn-
tactic complexity to a spoken response as a classi-
fication problem: given a spoken response, assign
the response to a proficiency class. A classifier is
trained in an inductive fashion, using a large cor-
pus of learner responses that is divided into pro-
ficiency scores as the training data and then used
to test data that is similar to the training data. A
distinguishing feature of the current study is that
the measure is based on a comparison of charac-
teristics of the test response to models trained on
large amounts of data from each score point, as op-
posed to measures that are simply characteristics
of the responses themselves (which is how mea-
sures have been considered in prior studies).
The inductive classifier we use here is the
maximum-entropy model (MaxEnt) which has
been used to solve several statistical natural lan-
guage processing problems with much success
(Berger et al, 1996; Borthwick et al, 1998; Borth-
wick, 1999; Pang et al, 2002; Klein et al, 2003;
Rosenfeld, 2005). The productive feature en-
gineering aspects of incorporating features into
the discriminative MaxEnt classifier motivate the
model choice for the problem at hand. In partic-
ular, the ability of the MaxEnt model?s estimation
routine to handle overlapping (correlated) features
makes it directly applicable to address the first lim-
itation of the VSM model. The second limitation,
related to the ineffective weighting of terms via
the the tf -idf scheme, seems to be addressed by
the fact that the MaxEnt model assigns a weight
to each feature (in our case, POS bigrams) on a
per-class basis (in our case, score group), by tak-
ing every instance into consideration. Therefore,
a MaxEnt model has an advantage over the model
described in 4.1 in that it uses four different weight
schemes (one per score level) and each scheme is
optimized for each score level. This is beneficial
in situations where the features are not evenly im-
portant across all score levels.
5 Experimental Setup
Our experiments seek answers to the following
questions.
1. To what extent does a MaxEnt-score of syn-
tactic complexity discriminate between levels
of proficiency?
2. What is the effect of including the proposed
measure of syntactic complexity in the state-
of-the-art automatic scoring model?
3. How robust is the measure to errors in the var-
ious stages of automatic generation?
5.1 Tasks
In order to answer the motivating questions of the
study, we set-up two tasks. In the first task, we
compare the extent to which the VSM-based mea-
sure and the MaxEnt-based measure (outlined in
4.1 and 4.2 above) discriminate between levels of
syntactic complexity. Additionally, we compare
the performance of an automatic scoring model of
overall proficiency that includes the measures of
syntactic complexity from each of the two mod-
els being compared and analyze the gains with re-
spect to the state-of-the-art. In the second task, we
study the measures? robustness to errors incurred
by ASR.
5.2 Data
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment consisted of questions to
which speakers were prompted to provide sponta-
neous spoken responses lasting approximately 45-
60 seconds per question. Test takers read and/or
listened to stimulus materials and then responded
to questions based on the stimuli. All questions so-
licited spontaneous, unconstrained natural speech.
A small portion of the available data with inad-
equate audio quality and lack of student response
was excluded from the study. The remaining re-
sponses were partitioned into two datasets: the
ASR set and the scoring model training/test (SM)
1309
set. The ASR set, with 47,227 responses, was
used for ASR training and POS similarity model
training. The SM set, with 2,950 responses, was
used for feature evaluation and automated scoring
model evaluation. There was no overlap in speak-
ers between the ASR set and the SM set.
Each response was rated for overall proficiency
by trained human scorers using a 4-point scoring
scale, where 1 indicates low speaking proficiency
and 4 indicated high speaking proficiency. The
distribution of proficiency scores, along with other
details of the data sets, are presented in Table 1.
As seen in Table 1, there is a strong bias towards
the middle scores (score 2 and 3) with approxi-
mately 84-85% of the responses belonging to these
two score levels. Although the skewed distribution
limits the number of score-specific instances for
the highest and lowest scores available for model
training, we used the data without modifying the
distribution since it is representative of responses
in a large-scale language assessment scenario.
Human raters? extent of agreement in the sub-
jective task of rating responses for language pro-
ficiency constrains the extent to which we can ex-
pect a machine?s score to agree with that of hu-
mans. An estimate of the extent to which human
raters agree on the subjective task of proficiency
assessment, is obtained by two raters scoring ap-
proximately 5% of data (2,388 responses from
ASR set and 140 responses from SM set). Pear-
son correlation r between the scores assigned by
the two raters was 0.62 in ASR set and 0.58 in SM
set. This level of agreement will guide the evalua-
tion of the human-machine agreement on scores.
5.3 Stages of Automatic Grammatical
Competence Assessment
Here we outline the multiple stages involved in the
automatic syntactic complexity assessment. The
first stage, ASR, yields an automatic transcription,
which is followed by the POS tagging stage. Sub-
sequently, the feature extraction stage (a VSM or
a MaxEnt model as the case may be) generates the
syntactic complexity feature which is then incor-
porated in a multiple linear regression model to
generate a score.
The steps for automatic assessment of overall
proficiency follow an analogous process (either in-
cluding the POS tagger or not), depending on the
objective measure being evaluated. The various
objective measures are then combined in the mul-
tiple regression scoring model to generate an over-
all score of proficiency.
5.3.1 Automatic Speech Recognizer
An HMM recognizer was trained using ASR set
(approximately 733 hours of non-native speech
collected from 7,872 speakers). A gender inde-
pendent triphone acoustic model and combination
of bigram, trigram, and four-gram language mod-
els were used. A word error rate (WER) of 31%
on the SM dataset was observed.
5.3.2 POS tagger
POS tags were generated using the POS tagger
implemented in the Open-NLP toolkit
3
. It was
trained on the Switchboard (SWBD) corpus. This
POS tagger was trained on about 528K word/tag
pairs. A combination of 36 tags from the Penn
Treebank tag set and 6 tags generated for spoken
languages were used in the tagger.
The tagger achieved a tagging accuracy of
96.3% on a Switchboard evaluation set composed
of 379K words, suggesting high accuracy of the
tagger. However, due to substantial amount of
speech recognition errors in our data, the POS
error rate (resulting from the combined errors of
ASR and automated POS tagger) is expected to be
higher.
5.3.3 VSM-based Model
We used the ASR data set to train a POS-bigram
VSM for the highest score class and generated
cos
4
and cosmax reported in Yoon and Bhat
(2012), for the SM data set as outlined in Sec-
tion 4.1.
5.3.4 Maximum Entropy Model Classifier
The input to the classifier is a set of POS bi-
grams (1366 bigrams in all) obtained from the
POS-tagged output of the data. We considered
binary-valued features (whether a POS bigram oc-
curred or not), occurrence frequency, and relative
frequency as input for the purpose of experimen-
tation. We used the maximum entropy classifier
implementation in the MaxEnt toolkit
4
. The clas-
sifier was trained using the LBFGS algorithm for
parameter estimation and used equal-scale gaus-
sian priors for smoothing. The results that fol-
low are based on MaxEnt classifier?s parameter
settings initialized to zero. Since a preliminary
3
http://opennlp.apache.org
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
1310
Data set No. of No. of Score Score distribution
responses speakers Mean SD 1 2 3 4
ASR 47,227 7,872 2.67 0.73 1,953 16,834 23,106 5,334
4% 36% 49% 11%
SM 2,950 500 2.61 0.74 166 1,103 1,385 296
6% 37% 47% 10%
Table 1: Data size and score distribution
analysis of the effect of varying the feature (bi-
nary or frequency) revealed that the binary-valued
feature was optimal (in terms of yielding the best
agreement between human and machine scores),
we only report our results for this case. The ASR
data set was used to train the MaxEnt classifier and
the features generated from the SM data set were
used for evaluation.
One straightforward way of using the maximum
entropy classifier?s prediction for our case is to
directly use its predicted score-level ? 1, 2, 3 or
4. However, this forces the classifier to make a
coarse-grained choice and may over-penalize the
classifier?s scoring errors. To illustrate this, con-
sider a scenario where the classifier assigns two
responses A and B to score level 2 (based on the
maximum a posteriori condition). Suppose that,
for response A, the score class with the second
highest probability corresponds to score level 1
and that, for response B, it corresponds to score
level 3. It is apparent that the classifier has an
overall tendency to assign a higher score to B, but
looking at its top preference alone (2 for both re-
sponses), masks this tendency.
We thus capture the classifier?s finer-grained
scoring tendency by calculating the expected value
of the classifier output. For a given response, the
MaxEnt classifier calculates the conditional prob-
ability of a score-class given the response, in turn
yielding conditional probabilities of each score
group given the observation ? p
i
for score group
i ? {1, 2, 3, 4}. In our case, we consider the pre-
dicted score of syntactic complexity to be the ex-
pected value of the class label given the observa-
tion as, mescore = 1?p
1
+2?p
2
+3?p
3
+4?p
4
.
This permits us to better represent the score as-
signed by the MaxEnt classifier as a relative pref-
erence over score assignments.
5.3.5 Automatic Scoring System
We consider a multiple regression automatic scor-
ing model as studied in Zechner et al (2009; Chen
and Zechner (2011; Higgins et al (2011). In its
state-of-the-art set-up, the following model uses
the features ? HMM acoustic model score (global
normalized), speaking rate, word types per sec-
ond, average chunk length in words and language
model score (global normalized). We use these
features by themselves (Base), and also in con-
junction with the VSM-based feature (cva4) and
the MaxEnt-based feature (mescore).
5.4 Evaluation Metric
We evaluate the measures using the metrics cho-
sen in previous studies (Zechner et al, 2009; Chen
and Zechner, 2011; Yoon and Bhat, 2012). A
measure?s utility has been evaluated according to
its ability to discriminate between levels of pro-
ficiency assigned by human raters. This is done
by considering the Pearson correlation coefficient
between the feature and the human scores. In an
ideal situation, we would have compared machine
score with scores of grammatical skill assigned by
human raters. In our case, however, with only
access to the overall proficiency scores, we use
scores of language proficiency as those of gram-
matical skill.
A criterion for evaluating the performance of
the scoring model is the extent to which the au-
tomatic scores of overall proficiency agree with
the human scores. As in prior studies, here too
the level of agreement is evaluated by means of
the weighted kappa measure as well as unrounded
and rounded Pearson?s correlations between ma-
chine and human scores (since the output of the re-
gression model can either be rounded or regarded
as is). The feature that maximizes this degree of
agreement will be preferred.
6 Experimental Results
First, we compare the discriminative ability of
measures of syntactic complexity (VSM-model
based measure with that of the MaxEnt-based
measure) across proficiency levels. Table 2 sum-
marizes our experimental results for this task. We
1311
Features Manual Transcriptions ASR
mescore 0.57 0.52
cos
4
0.48 0.43
cosmax - 0.31
Table 2: Pearson correlation coefficients between measures and holistic proficiency scores. All values
are significant at level 0.01. Only the measures cos
4
and mescore were compared for robustness using
manual and ASR transcriptions.
notice that of the measures compared, mescore
shows the highest correlation with scores of syn-
tactic complexity. The correlation was approxi-
mately 0.1 higher in absolute value than that of
cos
4
, which was the best performing feature in the
VSM-based model and the difference is statisti-
cally significant.
Seeking to study the robustness of the mea-
sures derived using a shallow analysis, we next
compare the two measures studied here, with re-
spect to the impact of speech recognition errors on
their correlation with scores of syntactic complex-
ity. Towards this end, we compare mescore and
cos
4
when POS bigrams are extracted from man-
ual transcriptions (ideal ASR) and ASR transcrip-
tions.
In Table 2, noticing that the correlations de-
crease going along a row, we can say that the er-
rors in the ASR system caused both mescore and
cos
4
to under-perform. However, the performance
drop (around 0.05) resulting from a shallow anal-
ysis is relatively small compared to the drop ob-
served while employing a deep syntactic analysis.
Chen and Zechner (2011) found that while using
measures of syntactic complexity obtained from
transcriptions, errors in ASR transcripts caused
over 0.40 drop in correlation from that found with
manual transcriptions
5
. This comparison suggests
that the current POS-based shallow analysis ap-
proach is more robust to ASR errors compared to
a syntactic analysis-based approach.
The effect of the measure of syntactic complex-
ity is best studied by including it in an automatic
scoring model of overall proficiency. We com-
pare the performance gains over the state-of-the-
art with the inclusion of additional features (VSM-
based and MaxEnt-based, in turn). Table 3 shows
the system performance with different grammar
sophistication measures. The results reported are
averaged over a 5-fold cross validation of the mul-
tiple regression model, where 80% of the SM data
5
Due to differences in the dataset and ASR system, a di-
rect comparison between the current study and the cited prior
study was not possible.
set is used to train the model and the evaluation is
done using 20% of the data in every fold.
As seen in Table 3, using the proposed measure,
mescore, leads to an improved agreement be-
tween human and machine scores of proficiency.
Comparing the unrounded correlation results in
Table 3 we notice that the model Base+mescore
shows the highest correlation of predicted scores
with human scores. In addition, we test the sig-
nificance of the difference between two depen-
dent correlations using Steiger?s Z-test (via the
paired.r function in the R statistical package
(Revelle, 2012)). We note that the performance
gain of Base+mescore over Base as well as over
Base + cos4 is statistically significant at level =
0.01. The performance gain of Base+cos4 over
Base, however, is not statistically significant at
level = 0.01. Thus, the inclusion of the MaxEnt-
based measure of syntactic complexity results in
improved agreement between machine and hu-
man scores compared to the state-of-the-art model
(here, Base).
7 Discussions
We now discuss some of the observations and re-
sults of our study with respect to the following
items.
Improved performance: We sought to verify
empirically that the MaxEnt model really outper-
forms the VSM in the case of correlated POS
bigrams. To see this, we separate the test set
into three subsets A,B,C. Set A contains re-
sponses where MaxEnt outperforms VSM; set B
contains responses where VSM outperforms Max-
Ent; set C contains responses where their predic-
tions are comparable. For each group of responses
s ? {A,B,C}, we calculate the percentage of re-
sponses P
s
where two highly correlated POS bi-
grams occur
6
. We found that the percentages fol-
low the order: P
A
= 12.93% > P
C
= 7.29% >
6
We consider two POS bigrams to be highly correlated,
when the their pointwise-mutual information is higher than
4.
1312
Evaluation method Base Base+cos4 Base+mescore
Weighted kappa 0.503 0.524 0.546
Correlation (unrounded) 0.548 0.562 0.592
Correlation (rounded) 0.482 0.492 0.519
Table 3: Comparison of scoring model performances using features of syntactic complexity studied in
this paper along with those available in the state-of-the-art. Here, Base is the scoring model without the
measures of syntactic complexity. All correlations are significant at level 0.01.
P
B
= 4.41%. This suggests that when correlated
POS bigrams occur, MaxEnt is more likely to pro-
vide better score predictions than VSM does.
Feature design: In the case of MaxEnt,
the observation that binary-valued features (pres-
ence/absence of POS bigrams) yield better perfor-
mance than features indicative of the occurrence
frequency of the bigram has interesting implica-
tions. This was also observed in Pang et al (2002)
where it was interpreted to mean that overall senti-
ment is indicated by the presence/absence of key-
words, as opposed to topic of a text, which is in-
dicated by the repeated use of the same or simi-
lar terms. An analogous explanation is applicable
here.
At first glance, the use of the presence/absence
of grammatical structures may raise concerns
about a potential loss of information (e.g. the dis-
tinction between an expression that is used once
and another that is used multiple times is lost).
However, when considered in the context of lan-
guage acquisition studies, this approach seems to
be justified. Studies in native language acquisi-
tion, have considered multiple grammatical devel-
opmental indices that represent the grammatical
levels reached at various stages of language acqui-
sition. For instance, Covington et al (2006) pro-
posed the revised D-level scale which was origi-
nally studied by Rosenberg and Abbeduto (1987).
The D-Level Scale categorizes grammatical de-
velopment into 8 levels according to the pres-
ence of a set of diverse grammatical expressions
varying in difficulty (for example, level 0 con-
sists of simple sentences, while level 5 consists
of sentences joined by a subordinating conjunc-
tion). Similarly, Scarborough (1990) proposed
the Index of Productive Syntax (IPSyn), accord-
ing to which, the presence of particular grammati-
cal structures, from a list of 60 structures (ranging
from simple ones such as including only subjects
and verbs, to more complex constructions such as
conjoined sentences) is evidence of language ac-
quisition milestones.
Despite the functional differences between the
indices, there is a fundamental operational simi-
larity - that they both use the presence or absence
of grammatical structures, rather than their oc-
currence count, as evidence of acquisition of cer-
tain grammatical levels. The assumption that a
presence-based view of grammatical level acquisi-
tion is also applicable to second language assess-
ment helps validate our observation that binary-
valued features yield a better performance when
compared with frequency-valued features.
Generalizability: The training and test sets
used in this study had similar underlying distribu-
tions ? they both sought unconstrained responses
to a set of items with some minor differences in
item type. Looking ahead, an important question
is the extent to which our measure is sensitive to a
mismatch between training and test data.
8 Conclusions
Seeking alternatives to measuring syntactic com-
plexity of spoken responses via syntactic parsers,
we study a shallow-analysis based approach for
use in automatic scoring.
Empirically, we show that the proposed mea-
sure, based on a maximum entropy classification,
satisfied the constraints of the design of an objec-
tive measure to a high degree. In addition, the pro-
posed measure was found to be relatively robust to
ASR errors. The measure outperformed a related
measure of syntactic complexity (also based on
shallow-analysis of spoken response) previously
found to be well-suited for automatic scoring. In-
cluding the measure of syntactic complexity in
an automatic scoring model resulted in statisti-
cally significant performance gains over the state-
of-the-art. We also make an interesting observa-
tion that the impressionistic evaluation of syntactic
complexity is better approximated by the presence
or absence of grammar and usage patterns (and
not by their frequency of occurrence), an idea sup-
ported by studies in native language acquisition.
1313
References
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
Speech, pages 1241?1244.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse
knowledge sources via maximum entropy in named
entity recognition. In Proc. of the Sixth Workshop
on Very Large Corpora.
Andrew Borthwick. 1999. A maximum entropy ap-
proach to named entity recognition. Ph.D. thesis,
New York University.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
IUNLPBEA ?11, pages 38?45, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of NAACL, pages 140?147.
Michael A Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? a proposed revision of the rosen-
berg and abbeduto d-level scale. ReVision. Wash-
ington, DC http://www. ai. uga. edu/caspr/2006-01-
Covington. pdf.(Accessed May 10, 2010.).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. The Journal of the Acoustical Soci-
ety of America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: comparisons between read and sponta-
neous speech. The Journal of the Acoustical Society
of America, 111(6):2862?2873.
Sergey Feldman, M.A. Marin, Mari Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms
for genre classification of text. In Proceedings of
ICASSP, pages 4781 ?4784.
Pauline Foster and Peter Skehan. 1996. The influence
of planning and task type on second language per-
formance. Studies in Second Language Acquisition,
18:299?324.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scor-
ing for language instruction. In Proceedings of
ICASSP, pages 1471?1474.
Gene B Halleck. 1995. Assessing oral proficiency: a
comparison of holistic and objective measures. The
Modern Language Journal, 79(2):223?234.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech & Language, 25(2):282?
306.
Kellogg W Hunt. 1965. Grammatical structures writ-
ten at three grade levels. ncte research report no. 3.
Noriko Iwashita, Annie Brown, Tim McNamara, and
Sally O?Hagan. 2008. Assessed levels of second
language speaking proficiency: How distinct? Ap-
plied Linguistics, 29(1):24?49.
Noriko Iwashita. 2010. Features of oral proficiency in
task performance by efl and jfl learners. In Selected
proceedings of the Second Language Research Fo-
rum, pages 32?47.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 180?183. Asso-
ciation for Computational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
M.A Marin, Sergey Feldman, Mari Ostendorf, and
Maya R. Gupta. 2009. Filtering web text to match
target genres. In Proceedings of ICASSP, pages
3705?3708.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In
ICWSM.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college?level L2 writing. Applied Lin-
guistics, 24(4):492?518.
1314
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
William Revelle, 2012. psych: Procedures for Psycho-
logical, Psychometric, and Personality Research.
Northwestern University, Evanston, Illinois. R
package version 1.2.1.
Sheldon Rosenberg and Leonard Abbeduto. 1987. In-
dicators of linguistic competence in the peer group
conversational behavior of mildly retarded adults.
Applied Psycholinguistics, 8:19?32.
Ronald Rosenfeld. 2005. Adaptive statistical language
modeling: a maximum entropy approach. Ph.D. the-
sis, IBM.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Hollis S Scarborough. 1990. Index of productive syn-
tax. Applied Psycholinguistics, 11(1):1?22.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING, pages 865?
872.
Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of NAACL-HLT, pages 814?819.
Silke Witt and Steve Young. 1997. Performance
measures for phone-level pronunciation teaching in
CALL. In Proceedings of STiLL, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Kate Wolf-Quintero, Shunji Inagaki, and Hae-Young
Kim. 1998. Second language development in writ-
ing: Measures of fluency, accuracy, and complexity.
Technical Report 17, Second Language Teaching
and curriculum Center, The University of Hawai?i,
Honolulu, HI.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the NAACL-HLT, pages
103?111.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
esl learners? syntactic competence based on similar-
ity measures. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 600?608. Association for Compu-
tational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883?895.
1315
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599?604,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Improved Correction Detection in Revised ESL Sentences
Huichao Xue and Rebecca Hwa
Department of Computer Science,
University of Pittsburgh,
210 S Bouquet St, Pittsburgh, PA 15260, USA
{hux10,hwa}@cs.pitt.edu
Abstract
This work explores methods of automat-
ically detecting corrections of individual
mistakes in sentence revisions for ESL
students. We have trained a classifier
that specializes in determining whether
consecutive basic-edits (word insertions,
deletions, substitutions) address the same
mistake. Experimental result shows that
the proposed system achieves an F
1
-score
of 81% on correction detection and 66%
for the overall system, out-performing the
baseline by a large margin.
1 Introduction
Quality feedback from language tutors can
help English-as-a-Second-Language (ESL) stu-
dents improve their writing skills. One of the tu-
tors? tasks is to isolate writing mistakes within
sentences, and point out (1) why each case is
considered a mistake, and (2) how each mistake
should be corrected. Because this is time consum-
ing, tutors often just rewrite the sentences with-
out giving any explanations (Fregeau, 1999). Due
to the effort involved in comparing revisions with
the original texts, students often fail to learn from
these revisions (Williams, 2003).
Computer aided language learning tools offer
a solution for providing more detailed feedback.
Programs can be developed to compare the stu-
dent?s original sentences with the tutor-revised
sentences. Swanson and Yamangil (2012) have
proposed a promising framework for this purpose.
Their approach has two components: one to de-
tect individual corrections within a revision, which
they termed correction detection; another to deter-
mine what the correction fixes, which they termed
error type selection. Although they reported a
high accuracy for the error type selection classifier
alone, the bottleneck of their system is the other
component ? correction detection. An analysis of
their system shows that approximately 70% of the
system?s mistakes are caused by mis-detections
in the first place. Their correction detection al-
gorithm relies on a set of heuristics developed
from one single data collection (the FCE corpus
(Yannakoudakis et al, 2011)). When determining
whether a set of basic-edits (word insertions, dele-
tions, substitutions) contributes to the same cor-
rection, these heuristics lack the flexibility to adapt
to a specific context. Furthermore, it is not clear if
the heuristics will work as well for tutors trained
to mark up revisions under different guidelines.
We propose to improve upon the correction de-
tection component by training a classifier that de-
termines which edits in a revised sentence address
the same error in the original sentence. The classi-
fier can make more accurate decisions adjusted to
contexts. Because the classifier were trained on re-
visions where corrections are explicitly marked by
English experts, it is also possible to build systems
adjusted to different annotation standards.
The contributions of this paper are: (1) We show
empirically that a major challenge in correction
detection is to determine the number of edits that
address the same error. (2) We have developed a
merging model that reduces mis-detection by 1/3,
leading to significant improvement in the accu-
racies of combined correction detection and er-
ror type selection. (3) We have conducted experi-
ments across multiple corpora, indicating that the
proposed merging model is generalizable.
2 Correction Detection
Comparing a student-written sentence with its re-
vision, we observe that each correction can be de-
composed into a set of more basic edits such as
word insertions, word deletions and word substi-
tutions. In the example shown in Figure 1, the
correction ?to change ? changing? is composed
of a deletion of to and a substitution from change
599
Figure 1: Detecting corrections from revisions. Our system detects individual corrections by comparing the original sentence
with its revision, so that each correction addresses one error. Each polygon corresponds to one correction; the labels are codes
of the error types. The codes follow the annotation standard in FCE corpus (Nicholls, 2003). In this example, W is incorrect
Word order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error.
Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process. The basic edits are
indicated by black polygons. The corrections are shown in red polygons.
(a) (b)
Figure 3: Basic edits extracted by the edit-distance algo-
rithm (Levenshtein, 1966) do not necessarily match our lin-
guistic intuition. The ideal basic-edits are shown in Figure
3a, but since the algorithm only cares about minimizing the
number of edits, it may end up extracting basic-edits shown
in Figure 3b.
to changing; the correction ?moment ? minute?
is itself a single word substitution. Thus, we can
build systems to detect corrections which operates
in two steps: (1) detecting the basic edits that took
place during the revision, and (2) merging those
basic edits that address the same error. Figure 2 il-
lustrates the process for a fragment of the example
sentence from Figure 1.
In practice, however, this two-step approach
may result in mis-detections due to ambiguities.
Mis-detections may be introduced from either
steps. While detecting basic edits, Figures 3 gives
an example of problems that might arise. Because
the Levenshtein algorithm only tries to minimize
the number of edits, it does not care whether the
edits make any linguistic sense. For merging basic
edits, Swanson and Yamangil applied a distance
heuristic ? basic-edits that are close to each other
(e.g. basic edits with at most one word lying in
between) are merged. Figure 4 shows cases for
which the heuristic results in the wrong scope.
These errors caused their system to mis-detect
30% of the corrections. Since mis-detected cor-
rections cannot be analyzed down the pipeline,
(a) The basic edits are addressing the same problem. But
these basic edits are non-adjacent, and therefore not merged by
S&Y?s algorithm.
(b) The basic edits in the above two cases address different
problems though they are adjacent. S&Y?s merging algorithm
incorrectly merges them.
Figure 4: Merging mistakes by the algorithm proposed in
Swanson and Yamangil (2012) (S&Y), which merges adja-
cent basic edits.
the correction detection component became the
bottle-neck of their overall system. Out of the
42% corrections that are incorrectly analyzed
1
,
30%/42%?70% are caused by mis-detections in
the first place. An improvement in correction de-
tection may increase the system accuracy overall.
We conducted an error analysis to attribute er-
rors to either step when the system detects a wrong
set of corrections for a sentence. We examine
the first step?s output. If the resulting basic ed-
its do not match with those that compose the ac-
tual corrections, we attribute the error to the first
step. Otherwise, we attribute the error to the sec-
ond step. Our analysis confirms that the merging
step is the bottleneck in the current correction de-
tection system ? it accounts for 75% of the mis-
detections. Therefore, to effectively reduce the
algorithm?s mis-detection errors, we propose to
1
Swanson and Yamangil reported an overall system with
58% F-score.
600
build a classifier to merge with better accuracies.
Other previous tasks also involve comparing
two sentences. Unlike evaluating grammar er-
ror correction systems (Dahlmeier and Ng, 2012),
correction detection cannot refer to a gold stan-
dard. Our error analysis above also highlights our
task?s difference with previous work that identify
corresponding phrases between two sentences, in-
cluding phrase extraction (Koehn et al, 2003) and
paraphrase extraction (Cohn et al, 2008). They
are fundamentally different in that the granularity
of the extracted phrase pairs is a major concern
in our work ? we need to guarantee each detected
phrase pair to address exactly one writing prob-
lem. In comparison, phrase extraction systems
aim to improve the end-to-end MT or paraphrasing
systems. A bigger concern is to guarantee the ex-
tracted phrase pairs are indeed translations or para-
phrases. Recent work therefore focuses on identi-
fying the alignment/edits between two sentences
(Snover et al, 2009; Heilman and Smith, 2010).
3 A Classifier for Merging Basic-Edits
Figures 4 highlights the problems with indiscrimi-
nantly merging basic-edits that are adjacent. Intu-
itively, it seems that the decision should be more
context dependent. Certain patterns may indicate
that two adjacent basic-edits are a part of the same
correction while others may indicate that they each
address a different problem. For example, in Fig-
ure 5a, when the insertion of one word is followed
by the deletion of the same word, the insertion
and deletion are likely addressing one single error.
This is because these two edits would combine to-
gether as a word-order change. On the other hand,
in Figure 5b, if one edit includes a substitution be-
tween words with the same POS?s, then it is likely
fixing a word choice error by itself. In this case, it
should not be merged with other edits.
To predict whether two basic-edits address the
same writing problem more discriminatively, we
train a Maximum Entropy binary classifier based
on features extracted from relevant contexts for
the basic edits. We use features in Table 1 in the
proposed classifier. We design the features to in-
dicate: (A) whether merging the two basic-edits
matches the pattern for a common correction. (B)
whether one basic-edit addresses one single error.
We train the classifier using samples extracted
from revisions where individual corrections are
explicitly annotated. We first extract the basic-
(a) The pattern indicates that
the two edits address the
same problem
(b) The pattern indicates that
the two edits do not address
the same problem
Figure 5: Patterns indicating whether two edits address the
same writing mistake.
Figure 6: Extracting training instances for the merger. Our
goal is to train classifiers to tell if two basic edits should
be merged (True or False). We break each correction (outer
polygons, also colored in red) in the training corpus into a set
of basic edits (black polygons). We construct an instance for
each consecutive pair of basic edits. If two basic edits were
extracted from the same correction, we will mark the outcome
as True, otherwise we will mark the outcome as False.
edits that compose each correction. We then create
a training instance for each pair of two consecutive
basic edits: if two consecutive basic edits need to
be merged, we will mark the outcome as True, oth-
erwise it is False. We illustrate this in Figure 6.
4 Experimental Setup
We combine Levenshtein algorithm with different
merging algorithms for correction detection.
4.1 Dataset
An ideal data resource would be a real-world col-
lection of student essays and their revisions (Tajiri
et al, 2012). However, existing revision corpora
do not have the fine-grained annotations necessary
for our experimental gold standard. We instead
use error annotated data, in which the corrections
were provided by human experts. We simulate the
revisions by applying corrections onto the original
sentence. The teachers? annotations are treated as
gold standard for the detailed corrections.
We considered four corpora with different ESL
populations and annotation standards, including
FCE corpus (Yannakoudakis et al, 2011), NU-
CLE corpus (Dahlmeier et al, 2013), UIUC cor-
pus
2
(Rozovskaya and Roth, 2010) and HOO2011
corpus (Dale and Kilgarriff, 2011). These corpora
all provide experts? corrections along with error
2
UIUC corpus contains annotations of essays collected
from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003).
601
Type name description
A
gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits?
original words, as well as the revised words. Note that Swanson and Yamangil?s approach is a
special case that only considers if the basic-edits have zero gap in both sentences.
tense-change We detect patterns such as: if the original-revision pair matches the pattern ?V-ing?to V?.
word-order-error Whether the basic-edits? original word set and the revised word set are the same (one or zero).
same-word-set If the original sentence and the revised sentence have the same word set, then it?s likely that all
the edits are fixing the word order error.
revised-to The phrase comprised of the two revised words.
B
editdistance=1 If one basic-edit is a substitution, and the original/revised word only has 1 edit distance, it
indicates that the basic-edit is fixing a misspelling error.
not-in-dict If the original word does not have a valid dictionary entry, then it indicates a misspelling error.
word-choice If the original and the revised words have the same POS, then it is likely fixing a word choice
error.
preposition-error Whether the original and the revised words are both prepositions.
Table 1: Features used in our proposed classifier.
corpus sentences
sentences with? 2 corrections
revised sentences
FCE 33,900 53.45%
NUCLE 61,625 48.74%
UIUC 883 61.32%
HOO2011 966 42.05%
Table 2: Basic statistics of the corpora that we consider.
type mark-ups. The basic statistics of the corpora
are shown in Table 2. In these corpora, around half
of revised sentences contains multiple corrections.
We have split each corpus into 11 equal parts. One
part is used as the development dataset; the rest are
used for 10-fold cross validation.
4.2 Evaluation Metrics
In addition to evaluating the merging algorithms
on the stand-alone task of correction detection, we
have also plugged in the merging algorithms into
an end-to-end system in which every automati-
cally detected correction is further classified into
an error type. We replicated the error type selector
described in Swanson and Yamangil (2012). The
error type selector?s accuracies are shown in Table
3
3
. We compare two merging algorithms, com-
bined with Levenshtein algorithm:
S&Y The merging heuristic proposed by Swan-
son and Yamangil, which merges the adjacent ba-
sic edits into single corrections.
MaxEntMerger We use the Maximum Entropy
classifier to predict whether we should merge the
two edits, as described in Section 3
4
.
We evaluate extrinsically the merging compo-
nents? effect on overall system performance by
3
Our replication has a slightly lower error type selection
accuracy on FCE (80.02%) than the figure reported by Swan-
son and Yamangil (82.5%). This small difference on error
type selection does not affect our conclusions about correc-
Corpus Error Types Accuracy
FCE 73 80.02%
NUCLE 27 67.36%
UIUC 8 80.23%
HOO2011 38 64.88%
Table 3: Error type selection accuracies on different cor-
pora. We use a Maximum Entropy classifier along with fea-
tures suggested by Swanson and Yamangil for this task. The
reported figures come from 10-fold cross validations on dif-
ferent corpora.
comparing the boundaries of system?s detected
corrections with the gold standard. We evaluate
both (1) the F-score in detecting corrections (2)
the F-score in correctly detecting both the correc-
tions? and the error types they address.
5 Experiments
We design experiments to answer two questions:
1. Do the additional contextual information
about correction patterns help guide the merging
decisions? How much does a classifier trained for
this task improve the system?s overall accuracy?
2. How well does our method generalize over re-
visions from different sources?
Our major experimental results are presented in
Table 4 and Table 6. Table 4 compares the over-
all educational system?s accuracies with different
merging algorithms. Table 6 shows the system?s
F
1
score when trained and tested on different cor-
pora. We make the following observations:
First, Table 4 shows that by incorporating cor-
rection patterns into the merging algorithm, the
tion detection.
4
We use the implementation at http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.
html.
602
errors in correction detection step were reduced.
This led to a significant improvement on the over-
all system?s F
1
-score on all corpora. The improve-
ment is most noticeable on FCE corpus, where
the error in correction detection step was reduced
by 9%. That is, one third of the correction mis-
detections were eliminated. Table 5 shows that the
number of merging errors are significantly reduced
by the new merging algorithm. In particular, the
number of false positives (system proposes merges
when it should not) is significantly reduced.
Second, our proposed model is able to gener-
alize over different corpora. As shown in Table
6. The models built on corpora can generally im-
prove the correction detection accuracy
5
. Mod-
els built on the same corpus generally perform
the best. Also, as suggested by the experimental
result, among the four corpora, FCE corpus is a
comparably good resource for training correction
detection models with our current feature set. One
reason is that FCE corpus has many more training
instances, which benefits model training. We tried
varying the training dataset size, and test it on dif-
ferent corpora. Figure 7 suggests that the model?s
accuracies increase with the training corpus size.
6 Conclusions
A revision often contains multiple corrections that
address different writing mistakes. We explore
building computer programs to accurately detect
individual corrections in one single revision. One
major challenge lies in determining whether con-
secutive basic-edits address the same mistake. We
propose a classifier specialized in this task. Our
experiments suggest that: (1) the proposed classi-
fier reduces correction mis-detections in previous
systems by 1/3, leading to significant overall sys-
tem performance. (2) our method is generalizable
over different data collections.
Acknowledgements
This work is supported by U.S. National Sci-
ence Foundation Grant IIS-0745914. We thank
the anonymous reviewers for their suggestions;
we also thank Homa Hashemi, Wencan Luo, Fan
Zhang, Lingjia Deng, Wenting Xiong and Yafei
Wei for helpful discussions.
5
We currently do not evaluate the end-to-end system over
different corpora. This is because different corpora employ
different error type categorization standards.
Method Corpus Correction
Detection F
1
Overall
F
1
-score
S&Y FCE 70.40% 57.10%
MaxEntMerger FCE 80.96% 66.36%
S&Y NUCLE 61.18% 39.32%
MaxEntMerger NUCLE 63.88% 41.00%
S&Y UIUC 76.57% 65.08%
MaxEntMerger UIUC 82.81% 70.55%
S&Y HOO2011 68.73% 50.95%
MaxEntMerger HOO2011 75.71% 56.14%
Table 4: Extrinsic evaluation, where we plugged the two
merging models into an end-to-end feedback detection sys-
tem by Swanson and Yamangil.
Merging algorithm TP FP FN TN
S&Y 33.73% 13.46% 5.71% 47.10%
MaxEntMerger 36.04% 3.26% 3.41% 57.30%
Table 5: Intrinsic evaluation, where we evaluate the pro-
posed merging model?s prediction accuracy on FCE corpus.
This table shows a breakdown of true-positives (TP), false-
positives (FP), false-negatives (FN) and true-negatives (TN)
for the system built on FCE corpus.
training
testing
FCE NUCLE UIUC HOO2011
S&Y 70.44 61.18% 76.57% 68.73%
FCE 80.96% 61.26% 83.07% 75.43%
NUCLE 74.53% 63.88% 78.57% 74.73%
UIUC 77.25% 58.21% 82.81% 70.83%
HOO2011 71.94% 54.99% 71.19% 75.71%
Table 6: Correction detection experiments by building the
model on one corpus, and applying it onto another. We
evaluate the correction detection performance with F
1
score.
When training and testing on the same corpus, we run a 10-
fold cross validation.
101 102 103 104 105Number of sentences in the training corpus0.40
0.450.50
0.550.60
0.650.70
0.750.80
F 1 sco
re
HOO2011UIUCFCENUCLE
Figure 7: We illustrate the performance of correction detec-
tion systems trained on subsets of FCE corpus. Each curve in
this figure represents the F
1
-scores for correction detection
of the model trained on a subset of FCE and tested on differ-
ent corpora. When testing on FCE, we used
1
11
of the FCE
corpus, which we kept as development data.
603
References
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4):597?614.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS corpus of learner english. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242?249. Association for
Computational Linguistics.
Laureen A Fregeau. 1999. Preparing ESL students
for college writing: Two case studies. The Internet
TESL Journal, 5(10).
Sylviane Granger. 2003. The International Corpus of
Learner English: a new resource for foreign lan-
guage learning and teaching and second language
acquisition research. Tesol Quarterly, 37(3):538?
546.
Shicun Gui and Huizhong Yang. 2003. Zhong-
guo xuexizhe yingyu yuliaohu.(chinese learner en-
glish corpus). Shanghai: Shanghai Waiyu Jiaoyu
Chubanshe.
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011?1019.
Association for Computational Linguistics.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707710.
D. Nicholls. 2003. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics 2003 con-
ference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL errors: Challenges and rewards. In Proceed-
ings of the NAACL HLT 2010 fifth workshop on inno-
vative use of NLP for building educational applica-
tions, pages 28?36. Association for Computational
Linguistics.
Matthew G Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an ESL educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357?361, Montr?eal, Canada, June.
Association for Computational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume 2,
pages 198?202. Association for Computational Lin-
guistics.
Jason Gordon Williams. 2003. Providing feedback
on ESL students written assignments. The Internet
TESL Journal, 4(10).
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 180?189. Association for Computational
Linguistics.
604
