Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
WebAnno: A Flexible, Web-based and Visually Supported
System for Distributed Annotations
Seid Muhie Yimam1,3 Iryna Gurevych2,3 Richard Eckart de Castilho2 Chris Biemann1
(1) FG Language Technology, Dept. of Computer Science, Technische Universita?t Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universita?t Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
We present WebAnno, a general pur-
pose web-based annotation tool for a wide
range of linguistic annotations. Web-
Anno offers annotation project manage-
ment, freely configurable tagsets and the
management of users in different roles.
WebAnno uses modern web technology
for visualizing and editing annotations in
a web browser. It supports arbitrarily
large documents, pluggable import/export
filters, the curation of annotations across
various users, and an interface to farming
out annotations to a crowdsourcing plat-
form. Currently WebAnno allows part-of-
speech, named entity, dependency parsing
and co-reference chain annotations. The
architecture design allows adding addi-
tional modes of visualization and editing,
when new kinds of annotations are to be
supported.
1 Introduction
The creation of training data precedes any sta-
tistical approach to natural language processing
(NLP). Linguistic annotation is a process whereby
linguistic information is added to a document,
such as part-of-speech, lemmata, named entities,
or dependency relations. In the past, platforms
for linguistic annotations were mostly developed
ad-hoc for the given annotation task at hand, used
proprietary formats for data exchange, or required
local installation effort. We present WebAnno, a
browser-based tool that is immediately usable by
any annotator with internet access. It supports an-
notation on a variety of linguistic levels (called an-
notation layers in the remainder), is interoperable
with a variety of data formats, supports annotation
project management such as user management, of-
fers an adjudication interface, and provides qual-
ity management using inter-annotator agreement.
Furthermore, an interface to crowdsourcing plat-
forms enables scaling out simple annotation tasks
to a large numbers of micro-workers. The added
value of WebAnno, as compared to previous an-
notation tools, is on the one hand its web-based
interface targeted at skilled as well as unskilled
annotators, which unlocks a potentially very large
workforce. On the other hand, it is the support for
quality control, annotator management, and adju-
dication/curation, which lowers the entrance bar-
rier for new annotation projects. We created Web-
Anno to fulfill the following requirements:
? Web-based: Distributed work, no installation
effort, increased availability.
? Interface to crowdsourcing: unlocking a very
large distributed workforce.
? Quality and user management: Integrated
different user roles support (administra-
tor, annotator, and curator), inter-annotator
agreement measurement, data curation, and
progress monitoring.
? Flexibility: Support of multiple annotation
layers, pluggable import and export formats,
and extensibility to other front ends.
? Pre-annotated and un-annotated documents:
supporting new annotations, as well as man-
ual corrections of existing, possibly auto-
matic annotations.
? Permissive open source: Usability of our tool
in future projects without restrictions, under
the Apache 2.0 license.
In the following section, we revisit related work
on annotation tools, which only partially fulfill the
aforementioned requirements. In Section 3, the ar-
chitecture as well as usage aspects of our tool are
lined out. The scope and functionality summary
1
of WebAnno is presented in Section 4. Section 5
elaborates on several use cases of WebAnno, and
Section 6 concludes and gives an outlook to fur-
ther directions.
2 Related Work
GATE Teamware (Bontcheva et al, 2010) is prob-
ably the tool that closely matches our requirements
regarding quality management, annotator manage-
ment, and support of a large set of annotation lay-
ers and formats. It is mostly web-based, but the
annotation is carried out with locally downloaded
software. An interface to crowdsourcing platforms
is missing. The GATE Teamware system is heav-
ily targeted towards template-based information
extraction. It sets a focus on the integration of au-
tomatic annotation components rather than on the
interface for manual annotation. Besides, the over-
all application is rather complex for average users,
requires considerable training and does not offer
an alternative simplified interface as it would be
required for crowdsourcing.
General-purpose annotation tools like MMAX2
(Mu?ller and Strube, 2006) or WordFreak (Morton
and LaCivita, 2003) are not web-based and do not
provide annotation project management. They are
also not sufficiently flexible regarding different an-
notation layers. The same holds for specialized
tools for single annotation layers, which we can-
not list here for the sake of brevity.
With the brat rapid annotation tool (Stenetorp
et al, 2012), for the first time a web-based open-
source annotation tool was introduced, which sup-
ports collaborative annotation for multiple anno-
tation layers simultaneously on a single copy of
the document, and is based on a client-server ar-
chitecture. However, the current version of brat
has limitations such as: (i) slowness for docu-
ments of more than 100 sentences, (ii) limits re-
garding file formats, (iii) web-based configuration
of tagsets/tags is not possible and (iv) configuring
the display of multiple layers is not yet supported.
While we use brat?s excellent visualization front
end in WebAnno, we decided to replace the server
layer to support the user and quality management,
and monitoring tools as well as to add the interface
to crowdsourcing.
3 System Architecture of WebAnno
The overall architecture of WebAnno is depicted
in Figure 1. The modularity of the architecture,
Figure 1: System architecture, organized in user,
front end, back end and persistent data storage.
which is mirrored in its open-source implementa-
tion1, makes it possible to easily extend the tool or
add alternative user interfaces for annotation lay-
ers that brat is less suited for, e.g. for constituent
structure. In Section 3.1, we illustrate how differ-
ent user roles are provided with different graphical
user interfaces, and show the expressiveness of the
annotation model. Section 3.2 elaborates on the
functionality of the back end, and describes how
data is imported and exported, as well as our im-
plementation of the persistent data storage.
3.1 Front End
All functionality of WebAnno is accessible via
a web browser. For annotation and visualiza-
tion of annotated documents, we adapted the brat
rapid annotation tool. Changes had to be made to
make brat interoperate with the Apache Wicket,
on which WebAnno is built, and to better integrate
into the WebAnno experience.
3.1.1 Project Definition
The definition and the monitoring of an annota-
tion project is conducted by a project manager (cf.
Figure 1) in a project definition form. It supports
creating a project, loading un-annotated or pre-
annotated documents in different formats2, adding
annotator and curator users, defining tagsets, and
configuring the annotation layers. Only a project
manager can administer a project. Figure 2 illus-
trates the project definition page with the tagset
editor highlighted.
1Available for download at (this paper is based on v0.3.0):
webanno.googlecode.com/
2Formats: plain text, CoNLL (Nivre et al, 2007), TCF
(Heid et al, 2010), UIMA XMI (Ferrucci and Lally, 2004)
2
Figure 2: The tagset editor on the project definition page
3.1.2 Annotation
Annotation is carried out with an adapted ver-
sion of the brat editor, which communicates with
the server via Ajax (Wang et al, 2008) using the
JSON (Lin et al, 2012) format. Annotators only
see projects they are assigned to. The annotation
page presents the annotator different options to set
up the annotation environment, for customization:
? Paging: For heavily annotated documents or
very large documents, the original brat vi-
sualization is very slow, both for displaying
and annotating the document. We use a pag-
ing mechanism that limits the number of sen-
tences displayed at a time to make the perfor-
mance independent of the document size.
? Annotation layers: Annotators usually work
on one or two annotations layers, such as
part-of-speech and dependency or named en-
tity annotation. Overloading the annota-
tion page by displaying all annotation layers
makes the annotation and visualization pro-
cess slower. WebAnno provides an option to
configure visible/editable annotation layers.
? Immediate persistence: Every annotation is
sent to the back end immediately and per-
sisted there. An explicit interaction by the
user to save changes is not required.
3.1.3 Workflow
WebAnno implements a simple workflow to track
the state of a project. Every annotator works on a
separate version of the document, which is set to
the state in progress the first time a document is
opened by the annotator. The annotator can then
mark it as complete at the end of annotation at
which point it is locked for further annotation and
can be used for curation. Such a document cannot
be changed anymore by an annotator, but can be
used by a curator. A curator can mark a document
as adjudicated.
3.1.4 Curation
The curation interface allows the curator to open a
document and compare annotations made by the
annotators that already marked the document as
complete. The curator reconciles the annotation
with disagreements. The curator can either decide
on one of the presented alternatives, or freely re-
annotate. Figure 3 illustrates how the curation in-
terface detects sentences with annotation disagree-
ment (left side of Figure 3) which can be used to
navigate to the sentences for curation.
3.1.5 Monitoring
WebAnno provides a monitoring component, to
track the progress of a project. The project man-
ager can check the progress and compute agree-
ment with Kappa and Tau (Carletta, 1996) mea-
sures. The progress is visualized using a matrix of
annotators and documents displaying which docu-
ments the annotators have marked as complete and
which documents the curator adjudicated. Fig-
ure 4 shows the project progress, progress of in-
dividual annotator and completion statistics.
3
Figure 3: Curation user interface (left: sentences
with disagreement; right: merging editor)
3.1.6 Crowdsourcing
Crowdsourcing is a way to quickly scale annota-
tion projects. Distributing a task that otherwise
will be performed by a controlled user group has
become much easier. Hence, if quality can be en-
sured, it is an alternative to high quality annotation
using a large number of arbitrary redundant anno-
tations (Wang et al, 2013). For WebAnno, we
have designed an approach where a source doc-
ument is split into small parts that get presented
to micro-workers in the CrowdFlower platform3.
The crowdsourcing component is a separate mod-
ule that handles the communication via Crowd-
Flower?s API, the definition of test items and job
parameters, and the aggregation of results. The
crowdsourced annotation appears as a virtual an-
notator in the tool.
Since it is not trivial to express complex anno-
tation tasks in comparably simple templates suit-
able for crowdsourcing (Biemann, 2013), we pro-
ceed by working out crowdsourcing templates and
strategies per annotation layer. We currently only
support named entity annotation with predefined
templates. However, the open and modular archi-
tecture allows to add more crowdsourced annota-
tion layers.
3.2 Back End
WebAnno is a Java-based web application that
may run on any modern servlet container. In mem-
ory and on the file system, annotations are stored
3www.crowdflower.com
Figure 4: Project monitoring
as UIMA CAS objects (Ferrucci and Lally, 2004).
All other data is persisted in an SQL database.
3.2.1 Data Conversion
WebAnno supports different data models that re-
flect the different communication of data between
the front end, back end, and the persistent data
storage. The brat data model serves exchanging
data between the front end and the back end.
The documents are stored in their original for-
mats. For annotations, we use the type system
from the DKPro Core collection of UIMA compo-
nents (Eckart de Castilho and Gurevych, 2009)4.
This is converted to the brat model for visualiza-
tion. Importing documents and exporting anno-
tations is implemented using UIMA reader and
writer components from DKPro Core as plug-ins.
Thus, support for new formats can easily be added.
To provide quick reaction times in the user inter-
face, WebAnno internally stores annotations in a
binary format, using the SerializedCasReader and
SerializedCasWriter components.
3.2.2 Persistent Data Storage
Project definitions including project name and de-
scriptions, tagsets and tags, and user details are
kept in a database, whereas the documents and an-
notations are stored in the file system. WebAnno
supports limited versioning of annotations, to pro-
tect against the unforeseen loss of data. Figure 5
shows the database entity relation diagram.
4code.google.com/p/dkpro-core-asl/
4
Figure 5: WebAnno database scheme
4 Scope and Functionality Summary
WebAnno supports the production of linguistically
annotated corpora for different natural language
processing applications. WebAnno implements
ease of usage and simplicity for untrained users,
and provides:
? Annotation via a fast, and easy-to-use web-
based user interface.
? Project and user management.
? Progress and quality monitoring.
? Interactive curation by adjudicating disagree-
ing annotations from multiple users.
? Crowdsourcing of annotation tasks.
? Configurable annotation types and tag sets.
5 Use Cases
WebAnno currently allows to configure different
span and arc annotations. It comes pre-configured
with the following annotation layers from the
DKPro Core type system:
Span annotations
? Part-of-Speech (POS) tags: an annotation
task on tokens. Currently, POS can be added
to a token, if not already present, and can be
modified. POS annotation is a prerequisite of
dependency annotation (Figure 6).
Figure 6: Parts-of-speech & dependency relations
Figure 7: Co-reference & named entites
? Named entities: a multiple span annotation
task. Spans can cover multiple adjacent to-
kens, nest and overlap (Figure 7), but cannot
cross sentence boundaries.
Arc Annotations
? Dependency relations: This is an arc annota-
tion which connects two POS tag annotations
with a directed relation (Figure 6).
? Co-reference chains: The co-reference chain
is realized as a set of typed mention spans
linked by typed co-reference relation arcs.
The co-reference relation annotation can
cross multiple sentences and is represented in
co-reference chains (Figure 7).
The brat front end supports tokens and sub-
tokens as a span annotation. However, tokens are
currently the minimal annotation units in Web-
Anno, due to a requirement of supporting the TCF
file format (Heid et al, 2010). Part-of-speech an-
notation is limited to singles token, while named
entity and co-reference chain annotations may
span multiple tokens. Dependency relations are
implemented in such a way that the arc is drawn
from the governor to the dependent (or the other
way around, configurable), while co-reference
chains are unidirectional and a chain is formed by
referents that are transitively connected by arcs.
Based on common practice in manual annota-
tion, every user works on their own copy of the
same document so that no concurrent editing oc-
curs. We also found that displaying all annotation
layers at the same time is inconvenient for anno-
tators. This is why WebAnno supports showing
5
and hiding of individual annotation layers. The
WebAnno curation component displays all anno-
tation documents from all users for a given source
document, enabling the curator to visualize all of
the annotations with differences at a time. Unlike
most of the annotation tools which rely on config-
uration files, WebAnno enables to freely configure
all parameters directly in the browser.
6 Conclusion and Outlook
WebAnno is a new web-based linguistic annota-
tion tool. The brat annotation and GUI front end
have been enhanced to support rapidly process-
ing large annotation documents, configuring the
annotation tag and tagsets in the browser, speci-
fying visible annotation layers, separating anno-
tation documents per user, just to name the most
important distinctions. Besides, WebAnno sup-
ports project definition, import/export of tag and
tagsets. Flexible support for importing and ex-
porting different data formats is handled through
UIMA components from the DKPro Core project.
The monitoring component of WebAnno helps the
administrator to control the progress of annotators.
The crowdsourcing component of WebAnno pro-
vides a unique functionality to distribute the an-
notation to a large workforce and automatically
integrate the results back into the tool via the
crowdsourcing server. The WebAnno annotation
tool supports curation of different annotation doc-
uments, displaying annotation documents created
by users in a given project with annotation dis-
agreements. In future work, WebAnno will be en-
hanced to support several other front ends to han-
dle even more annotation layers, and to provide
more crowdsourcing templates. Another planned
extension is a more seamless integration of lan-
guage processing tools for pre-annotation.
Acknowledgments
We would like to thank Benjamin Milde and Andreas
Straninger, who assisted in implementing WebAnno, as well
as Marc Reznicek, Nils Reiter and the whole CLARIN-D F-
AG 7 for testing and providing valuable feedback. The work
presented in this paper was funded by a German BMBF grant
to the CLARIN-D project, the Hessian LOEWE research ex-
cellence program as part of the research center ?Digital Hu-
manities? and by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No. I/82806.
References
Chris Biemann. 2013. Creating a system for lexical substi-
tutions from scratch using crowdsourcing. Lang. Resour.
Eval., 47(1):97?122, March.
Kalina Bontcheva, Hamish Cunningham, Ian Roberts, and
Valentin Tablan. 2010. Web-based collaborative corpus
annotation: Requirements and a framework implementa-
tion. In New Challenges for NLP Frameworks workshop
at LREC-2010, Malta.
Jean Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. In Computational Linguistics,
Volume 22 Issue 2, pages 249?254.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach to
Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting around
the framework Apache UIMA, LINA CNRS UMR 6241 -
University of Nantes, France.
David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. In Journal of Natu-
ral Language Engineering 2004, pages 327?348.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Erhard
Hinrichs. 2010. A Corpus Representation Format for
Linguistic Web Services: the D-SPIN Text Corpus Format
and its Relationship with ISO Standards. In Proceedings
of LREC 2010, Malta.
Boci Lin, Yan Chen, Xu Chen, and Yingying Yu. 2012.
Comparison between JSON and XML in Applications
Based on AJAX. In Computer Science & Service System
(CSSS), 2012, Nanjing, China.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak:
an open tool for linguistic annotation. In Proceedings of
NAACL-2003, NAACL-Demonstrations ?03, pages 17?18,
Edmonton, Canada.
Christoph Mu?ller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In S. Braun,
K. Kohn, and J. Mukherjee, editors, Corpus Technology
and Language Pedagogy: New Resources, New Tools,
NewMethods, pages 197?214. Peter Lang, Frankfurt a.M.,
Germany.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-
public.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proceedings of the Demonstrations at EACL-2012, Avi-
gnon, France.
Qingling Wang, Qin Liu, Na Li, and Yan Liu. 2008. An
Automatic Approach to Reengineering Common Website
with AJAX. In 4th International Conference on Next Gen-
eration Web Services Practices, pages 185?190, Seoul,
South Korea.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013.
Perspectives on Crowdsourcing Annotations for Natural
Language Processing. In Language Resources And Eval-
uation, pages 9?31. Springer Netherlands.
6
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 91?96,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Automatic Annotation Suggestions and
Custom Annotation Layers in WebAnno
Seid Muhie Yimam
1
Richard Eckart de Castilho
2
Iryna Gurevych
2,3
Chris Biemann
1
(1) FG Language Technology, Dept. of Computer Science, Technische Universit?at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.{lt,ukp}.tu-darmstadt.de
Abstract
In this paper, we present a flexible ap-
proach to the efficient and exhaustive man-
ual annotation of text documents. For this
purpose, we extend WebAnno (Yimam et
al., 2013) an open-source web-based an-
notation tool.
1
While it was previously
limited to specific annotation layers, our
extension allows adding and configuring
an arbitrary number of layers through a
web-based UI. These layers can be an-
notated separately or simultaneously, and
support most types of linguistic annota-
tions such as spans, semantic classes, de-
pendency relations, lexical chains, and
morphology. Further, we tightly inte-
grate a generic machine learning compo-
nent for automatic annotation suggestions
of span annotations. In two case studies,
we show that automatic annotation sug-
gestions, combined with our split-pane UI
concept, significantly reduces annotation
time.
1 Introduction
The annotation of full text documents is a costly
and time-consuming task. Thus, it is important to
design annotation tools in such a way that the an-
notation process can happen as swiftly as possible.
To this end, we extend WebAnno with the capabil-
ity of suggesting annotations to the annotator.
A general-purpose web-based annotation tool
can greatly lower the entrance barrier for linguistic
annotation projects, as tool development costs and
preparatory work are greatly reduced. WebAnno
1.0 only partially fulfilled desires regarding gen-
erality: Although it covered already more kinds
1
WebAnno is open-source software under the terms of the
Apache Software License 2.0. This paper describes v1.2:
http://webanno.googlecode.com
of annotations than most other tools, it supported
only a fixed set of customizable annotation lay-
ers (named entities, part-of-speech, lemmata, co-
reference, dependencies). Thus, we also remove a
limitation of the tool, which was previously bound
to specific, hardcoded annotation layers.
We have generalized the architecture to support
three configurable generic structures: spans, rela-
tions, and chains. These support all of the original
layers and allow the user to define arbitrary custom
annotation layers based on either of these struc-
tures. Additionally, our approach allows maintain-
ing multiple properties on annotations, e.g. to sup-
port morphological annotations, while previously
only one property per annotation was supported.
Automatic suggestion of annotations is based
on machine learning, which is common practice
in annotation tools. However, most of existing
web-based annotation tools, such as GATE (Cun-
ningham et al., 2011) or brat (Stenetorp et al.,
2012), depend on external preprocessing and post-
processing plugins or on web services. These tools
have limitations regarding adaptability (difficulty
to adapt to other annotation tasks), reconfigurabil-
ity (generating a classifier when new features and
training documents are available is complicated),
and reusability (requires manual intervention to
add newly annotated documents into the iteration).
For our approach, we assume that an annota-
tor actually does manually verify all annotations
to produce a completely labeled dataset. This task
can be sped up by automatically suggesting anno-
tations that the annotator may then either accept
or correct. Note that this setup and its goal differs
from an active learning scenario, where a system
actively determines the most informative yet unan-
notated example to be labeled, in order to quickly
arrive at a high-quality classifier that is then to be
applied to large amounts of unseen data.
Our contribution is the integration of machine
learning into the tool to support exhaustive an-
91
notation of documents providing a shorter loop
than comparable tools (Cunningham et al., 2011;
Stenetorp et al., 2012), because new documents
are added to the training set as soon as they are
completed by the annotators. The machine learn-
ing support currently applies to sequence classifi-
cation tasks only. It is complemented by our ex-
tension allowing to define custom annotation lay-
ers, making it applicable to a wide range of anno-
tation tasks with only little configuration effort.
Section 2 reviews related work about the uti-
lization of automatic supports and customiza-
tion of annotation schemes in existing annotation
tools. The integration of automatic suggestions
into WebAnno, the design principles followed, and
two case studies are explained in Section 3. Sec-
tion 4 presents the implementation of customiz-
able annotation layers into the tool. Finally, Sec-
tion 5 summarizes the main contributions and fu-
ture directions of our work.
2 Related Work
Automatic annotation support The impact of
using lexical and statistical resources to produce
pre-annotation automatically to increase the anno-
tation speed has been studied widely for various
annotation tasks. For the task of medical named
entity labeling, Lingren et al. (2013) investigate
the impact of automatic suggestions on annotation
speed and potential biases using dictionary-based
annotations. This technique results in 13.83% to
21.5% time saving and in an inter-annotator agree-
ment (IAA) increase by several percentage points.
WordFreak (Morton and LaCivita, 2003) in-
cludes an automation component, where instances
with a low machine learning confidence are pre-
sented for annotation in an active learning setup.
Beck et al. (2013) demonstrate that the use of ac-
tive learning for machine translation reduces the
annotation effort and show a reduced annotation
load on three out of four datasets.
The GoldenGATE editor (Sautter et al., 2007)
integrates NLP tools and assistance features for
manual XML editing. The tool is used in correct-
ing/editing an automatically annotated document
with an editor where both text and XML markups
are modified. GoldenGATE is merely used to fa-
cilitate the correction of an annotation while pre-
annotation is conducted outside of the tool.
Automatic annotation support in brat (Stenetorp
et al., 2012) was carried out for a semantic class
disambiguation task to investigate how such au-
tomation facilitates the annotators? progress. They
report a 15.4% reduction in total annotation time.
However, the automation process in brat 1) de-
pends on bulk annotation imports and web service
configurations, which is labor intensive, 2) is task
specific so that it requires a lot of effort to adapt it
to different annotation tasks, 3) there is no way of
using the corrected result for the next iteration of
training the automatic tool.
The GATE Teamware (Bontcheva et al., 2013)
automation component is most similar to our
work. It is based either on plugins and externally
trained classification models, or uses web services.
Thus, it is highly task specific and requires exten-
sive configuration. The automatic annotation sug-
gestion component in our tool, in contrast, is easily
configurable and adaptable to different annotation
tasks and allows the use of annotations from the
current annotation project.
Custom annotation layers Generic annotation
data models are typically directed graph models
(e.g. GATE, UIMA CAS (G?otz and Suhre, 2004),
GrAF (Ide and Suderman, 2007)). In addition, an
annotation schema defines possible kinds of anno-
tations, their properties and relations. While these
models offer great expressiveness and flexibility, it
is difficult to adequately transfer their power into
a convenient annotation editor. For example, one
schema may prescribe that the part-of-speech tag
is a property on a Token annotation, another one
may prescribe that the tag is a separate annotation,
which is linked to the token. An annotator should
not be exposed to these details in the UI and should
be able to just edit a part-of-speech tag, ignorant of
the internal representation.
This problem is typically addressed in two
ways. Either, the full complexity of the annota-
tion model is exposed to the annotator, or the an-
notation editor uses a simplified model. The first
approach can easily lead to an unintuitive UI and
make the annotation an inconvenient task. The
second approach (e.g. as advocated by brat) re-
quires the implementation of specific import and
export filters to transform between the editor data
model and the generic annotation data models.
We propose a third approach integrating a con-
figurable mapping between a generic annotation
model (UIMA CAS) and a simplified editing
model (brat) directly into the annotation tool.
Thus, we avoid exposing the full complexity of
92
the generic model to the user and also avoid the
necessity for implementing import/export filters.
Similar approaches have already been used to map
annotation models to visualization modules (cf.
(Zeldes et al., 2009)), but have, to our knowledge,
not been used in an annotation editor. Our ap-
proach is different from schema-based annotation
editors (e.g. GATE), which employ a schema as
a template of properties and controlled vocabular-
ies that can be used to annotate documents, but
which do not allow to map structures inherent in
annotations, like relations or chains, to respective
concepts in the UI.
3 Automatic Annotation Suggestions
It is the purpose of the automatic annotation sug-
gestion component to increase the annotation ef-
ficiency, while maintaining the quality of annota-
tions. The key design principle of our approach is
a split-pane (Figure 1) that displays automatic an-
notation suggestions in the suggestion pane (lower
part) and only verified or manual ones in the anno-
tation pane (upper part). In this way, we force the
annotators to review each automatic suggestion as
to avoid overlooking wrong suggestions.
Figure 1: Split-pane UI. Upper: the annotation
pane, which should be completed by the annotator.
Lower: the suggestion pane, displaying predic-
tions or automatic suggestions, and coding their
status in color. This examples shows automatic
suggestions for parts-of-speech. Unattended anno-
tations are rendered in blue, accepted annotations
in grey and rejected annotations in red. Here, the
last five POS annotations have been attended, four
have been accepted by clicking on the suggestion,
and one was rejected by annotating it in the anno-
tation pane.
3.1 Suggestion modes
We distinguish three modes of automatic annota-
tion suggestion:
Correction mode In this mode, we import doc-
uments annotated by arbitrary external tools and
present them to the user in the suggestion pane
of the annotation page. This mode is specifi-
cally appropriate for annotation tasks where a pre-
annotated document contains several possibilities
for annotations in parallel, and the user?s task is
to select the correct annotation. This allows to
leverage specialized external automatic annotation
components, thus the tool is not limited to the in-
tegrated automation mechanism.
Repetition mode In this mode, further occur-
rences of a word annotated by the user are high-
lighted in the suggestion pane. To accept sugges-
tions, the user can simply click on them in the sug-
gestion pane. This basic ? yet effective ? sugges-
tion is realized using simple string matching.
Learning mode For this mode, we have inte-
grated MIRA (Crammer and Singer, 2003), an ex-
tension of the perceptron algorithm for online ma-
chine learning which allows for the automatic sug-
gestions of span annotations. MIRA was selected
because of its relatively lenient licensing, its good
performance even on small amounts of data, and
its capability of allowing incremental classifier up-
dates. Results of automatic tagging are displayed
in the suggestion pane. Our architecture is flexible
to integrate further machine learning tools.
3.2 Suggestion Process
The workflow to set up an automatically supported
annotation project consists of the following steps.
Importing annotation documents We can im-
port documents with existing annotations (manual
or automatic). The annotation pane of the automa-
tion page allows users to annotate documents and
the suggestion pane is used for the automatic sug-
gestion as shown in Figure 1. The suggestion pane
facilitates accepting correct pre-annotations with
minimal effort.
Configuring features For the machine learning
tool, it is required to define classification features
to train a classifier. We have designed a UI where
a range of standard classification features for se-
quence tagging can be configured. The features
include morphological features (prefixes, suffixes,
and capitalization), n-grams, and other layers as a
feature (for example POS annotation as a feature
93
Figure 2: Configuring an annotation suggestion: 1) layers for automation, 2) different features, 3) training
documents, 4) start training classifier.
for named entity recognition). While these stan-
dard features do not lead to state-of-the-art per-
formance on arbitrary tasks, we have found them
to perform very well for POS tagging, named en-
tity recognition, and chunking. Figure 2 shows the
feature configuration in the project settings.
Importing training documents We offer two
ways of providing training documents: importing
an annotated document in one of the supported file
formats, such as CoNLL, TCF, or UIMA XMI; or
using existing annotation documents in the same
project that already have been annotated.
Starting the annotation suggestion Once fea-
tures for a training layer are configured and train-
ing documents are available, automatic annotation
is possible. The process can be started manually
by the administrator from the automation settings
page, and it will be automatically re-initiated when
additional documents for training become avail-
able in the project. While the automatic annotation
is running in the background, users still can work
on the annotation front end without being affected.
Training and creating a classifier will be repeated
only when the feature configuration is changed or
when a new training document is available.
Display results on the monitoring page Af-
ter the training and automatic annotation are com-
pleted, detailed information about the training data
such as the number of documents (sentence, to-
kens), features used for each layer, F-score on
held-out data, and classification errors are dis-
played on the monitoring page, allowing an esti-
mation whether the automatic suggestion is use-
ful. The UI also shows the status of the training
process (not started, running, or finished).
3.3 Case Studies
We describe two case studies that demonstrate lan-
guage independence and flexibility with respect to
sequence label types of our automatic annotation
suggestions. In the first case study, we address the
task of POS tagging for Amharic as an example of
an under-resourced language. Second, we explore
German named entity recognition.
3.3.1 Amharic POS tagging
Amharic is an under-resourced language in the
Semitic family, mainly spoken in Ethiopia. POS
tagging research for Amharic is mostly conducted
as an academic exercise. The latest result re-
ported by Gebre (2009) was about 90% accuracy
using the Walta Information Center (WIC) corpus
of about 210,000 tokens (1065 news documents).
We intentionally do not use the corpus as training
data because of the reported inconsistencies in the
tagging process (Gebre, 2009). Instead, we man-
ually annotate Amharic documents for POS tag-
ging both to test the performance of the automa-
tion module and to produce POS-tagged corpora
for Amharic. Based upon the work by Petrov et al.
(2012) and Ethiopian Languages Research Cen-
ter (ELRC) tagset, we have designed 11 POS tags
equivalent to the Universal POS tags. The tag DET
is not included as Amharic denotes definiteness as
noun suffixes.
We collected some Amharic documents from an
online news portal.
2
Preprocessing of Amharic
documents includes the normalization of charac-
ters and tokenization (sentence and word bound-
2
http://www.ethiopianreporter.com/
94
Figure 3: Example Amharic document. The red
tags in the suggestion pane have not been con-
firmed by the annotator.
ary detection). Initially, we manually annotated 21
sentences. Using these, an iterative automatic an-
notation suggestion process was started until 300
sentences were fully annotated. We obtained an
F-score of 0.89 with the final model. Hence the
automatic annotation suggestion helps in decreas-
ing the total annotation time, since the user has
to manually annotate only one out of ten words,
while being able to accept most automatic sugges-
tions. Figure 3 shows such an Amharic document
in WebAnno.
3.3.2 German Named Entity Recognition
A pilot Named Entity Recognition (NER) project
for German was conducted by Benikova et al.
(2014). We have used the dataset ? about 31,000
sentences, over 41,000 NE annotations ? for train-
ing NER. Using this dataset, an F-score of about
0.8 by means of automatic suggestions was ob-
tained, which leads to an increase in annotation
speed of about 21% with automatic suggestion.
4 Customs Annotation Layers
The tasks in which an annotation editor can be em-
ployed depends on the expressiveness of the un-
derlying annotation model. However, fully expos-
ing the expressive power in the UI can make the
editor inconvenient to use.
We propose an approach that allows the user
to configure a mapping of an annotation model to
concepts well-supported in a web-based UI. In this
way, we can avoid to expose all details of the an-
notation model in the UI, and remove the need to
implement custom import/export filters.
WebAnno 1.0 employs a variant of the annota-
tion UI provided by brat, which offers the concepts
of spans and arcs. Based on these, WebAnno 1.2
implements five annotation layers: named entity,
part-of-speech, lemmata, co-reference, and depen-
dencies. In the new WebAnno version, we gener-
alized the support for these five layers into three
Figure 4: UI for custom annotation layers.
structural categories: span, relation (arc), and
chain. Each of these categories is handled by a
generic adapter which can be configured to sim-
ulate any of the original five layers. Based on
this generalization, the user can now define cus-
tom layers (Figure 4).
Additionally, we introduced a new concept of
constraints. For example, NER spans should not
cross sentence boundaries and attach to whole to-
kens (not substrings of tokens). Such constraints
not only help preventing the user from making in-
valid annotations, but can also offer extra conve-
nience. We currently support four hard-coded con-
straints:
Lock to token offsets Defines if annotation
boundaries must coincide with token boundaries,
e.g. named entities, lemmata, part-of-speech, etc.
For the user?s convenience, the annotation is auto-
matically expanded to include the full token, even
if only a part of a token is selected during annota-
tion (span/chain layers only).
Allow multiple tokens Some kinds of annota-
tions may only cover a single token, e.g. part-of-
speech, while others may cover multiple tokens,
e.g. named entities (span/chain layers only).
Allow stacking Controls if multiple annotations
of the same kind can be at the same location, e.g.
if multiple lemma annotations are allowed per to-
ken. For the user?s convenience, an existing an-
notation is replaced if a new annotation is created
when stacking is not allowed.
Allow crossing sentence boundaries Certain
annotations, e.g. named entities or dependency de-
lations, may not cross sentence boundaries, while
others need to, e.g. coreference chains.
Finally, we added the ability to define multiple
properties for annotations to WebAnno. For exam-
ple, this can be use to define a custom span-based
morphology layer with multiple annotation prop-
erties such as gender, number, case, etc.
95
5 Conclusion and Outlook
We discussed two extensions of WebAnno: the
tight and generic integration of automatic annota-
tion suggestions for reducing the annotation time,
and the web-based addition and configuration of
custom annotation layers.
While we also support the common practice
of using of external tools to automatically pre-
annotate documents, we go one step further by
tightly integrating a generic sequence classifier
into the tool that can make use of completed an-
notation documents from the same project. In two
case studies, we have shown quick convergence
for Amharic POS tagging and a substantial reduc-
tion in annotation time for German NER. The key
concept here is the split-pane UI that allows to dis-
play automatic suggestions, while forcing the an-
notator to review all of them.
Allowing the definition of custom annotation
layers in a web-based UI is greatly increasing
the number of annotation projects that potentially
could use our tool. While it is mainly an engineer-
ing challenge to allow this amount of flexibility
and to hide its complexity from the user, it is a ma-
jor contribution in the transition from specialized
tools towards general-purpose tools.
The combination of both ? custom layers and
automatic suggestions ? gives rise to the rapid
setup of efficient annotation projects. Adding to
existing capabilities in WebAnno, such as cura-
tion, agreement computation, monitoring and fine-
grained annotation project definition, our contri-
butions significantly extend the scope of annota-
tion tasks in which the tool can be employed.
In future work, we plan to support annota-
tion suggestions for non-span structures (arcs and
chains), and to include further machine learning
algorithms.
Acknowledgments
The work presented in this paper was funded by a German
BMBF grant to the CLARIN-D project, the Hessian LOEWE
research excellence program as part of the research center
?Digital Humanities? and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant
No. I/82806.
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013. Re-
ducing annotation effort for quality estimation via active
learning. In Proc. ACL 2013 System Demonstrations,
Sofia, Bulgaria.
Darina Benikova, Chris Biemann, and Marc Reznicek. 2014.
NoSta-D Named Entity Annotation for German: Guide-
lines and Dataset. In Proc. LREC 2014, Reykjavik, Ice-
land.
Kalina Bontcheva, H. Cunningham, I. Roberts, A. Roberts,
V. Tablan, N. Aswani, and G. Gorrell. 2013. GATE
Teamware: a web-based, collaborative text annota-
tion framework. Language Resources and Evaluation,
47(4):1007?1029.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. In Journal of
Machine Learning Research 3, pages 951 ? 991.
Hamish Cunningham, D. Maynard, K. Bontcheva, V. Tablan,
N. Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,
D. Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion,
J. Petrak, Y. Li, and W. Peters. 2011. Text Processing with
GATE (Version 6). University of Sheffield Department of
Computer Science, ISBN 978-0956599315.
Binyam Gebrekidan Gebre. 2009. Part-of-speech tagging for
Amharic. In ISMTCL Proceedings, International Review
Bulag, PUFC.
T. G?otz and O. Suhre. 2004. Design and implementation
of the UIMA Common Analysis System. IBM Systems
Journal, 43(3):476 ?489.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-based
format for linguistic annotations. In Proc. Linguistic An-
notation Workshop, pages 1?8, Prague, Czech Republic.
Todd Lingren, L. Deleger, K. Molnar, H. Zhai, J. Meinzen-
Derr, M. Kaiser, L. Stoutenborough, Q. Li, and I. Solti.
2013. Evaluating the impact of pre-annotation on anno-
tation speed and potential bias: natural language process-
ing gold standard development for clinical named entity
recognition in clinical trial announcements. In Journal of
the American Medical Informatics Association, pages 951
? 991.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak: an
open tool for linguistic annotation. In Proc. NAACL 2003,
demonstrations, pages 17?18, Edmonton, Canada.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A
universal part-of-speech tagset. In Proc LREC 2012, Is-
tanbul, Turkey.
Guido Sautter, Klemens B?ohm, Frank Padberg, and Walter
Tichy. 2007. Empirical Evaluation of Semi-automated
XML Annotation of Text Documents with the GoldenGATE
Editor. Budapest, Hungary.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proc. EACL 2012 Demo Session, Avignon, France.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart
de Castilho, and Chris Biemann. 2013. WebAnno: A
flexible,web-based and visually supported system for dis-
tributed annotations. In Proc. ACL 2013 System Demon-
strations, pages 1?6, Sofia, Bulgaria.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian Chiar-
cos. 2009. ANNIS: A search tool for multi-layer anno-
tated corpora. In Proc. Corpus Linguistics 2009, Liver-
pool, UK.
96
