Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723?730,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Continuous Space Language Models for Statistical Machine Translation
Holger Schwenk and Daniel Dchelotte and Jean-Luc Gauvain
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{schwenk,dechelot,gauvain}@limsi.fr
Abstract
Statistical machine translation systems are
based on one or more translation mod-
els and a language model of the target
language. While many different trans-
lation models and phrase extraction al-
gorithms have been proposed, a standard
word n-gram back-off language model is
used in most systems.
In this work, we propose to use a new sta-
tistical language model that is based on a
continuous representation of the words in
the vocabulary. A neural network is used
to perform the projection and the proba-
bility estimation. We consider the trans-
lation of European Parliament Speeches.
This task is part of an international evalua-
tion organized by the TC-STAR project in
2006. The proposed method achieves con-
sistent improvements in the BLEU score
on the development and test data.
We also present algorithms to improve the
estimation of the language model proba-
bilities when splitting long sentences into
shorter chunks.
1 Introduction
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f . Among all possible target sentences the
one with maximal probability is chosen. The clas-
sical Bayes relation is used to introduce a target
language model (Brown et al, 1993):
e? = argmaxe Pr(e|f) = argmaxe Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation.
Since the introduction of this basic model, many
improvements have been made, but it seems that
research is mainly focused on better translation
and alignment models or phrase extraction algo-
rithms as demonstrated by numerous publications
on these topics. On the other hand, we are aware
of only a small amount of papers investigating
new approaches to language modeling for statis-
tical machine translation. Traditionally, statistical
machine translation systems use a simple 3-gram
back-off language model (LM) during decoding to
generate n-best lists. These n-best lists are then
rescored using a log-linear combination of feature
functions (Och and Ney, 2002):
e? ? argmaxe Pr(e)
?1 Pr(f |e)?2 (1)
where the coefficients ?i are optimized on a devel-
opment set, usually maximizing the BLEU score.
In addition to the standard feature functions, many
others have been proposed, in particular several
ones that aim at improving the modeling of the tar-
get language. In most SMT systems the use of a
4-gram back-off language model usually achieves
improvements in the BLEU score in comparison
to the 3-gram LM used during decoding. It seems
however difficult to improve upon the 4-gram LM.
Many different feature functions were explored in
(Och et al, 2004). In that work, the incorporation
of part-of-speech (POS) information gave only a
small improvement compared to a 3-gram back-
off LM. In another study, a factored LM using
POS information achieved the same results as the
4-gram LM (Kirchhoff and Yang, 2005). Syntax-
based LMs were investigated in (Charniak et al,
723
2003), and reranking of translation hypothesis us-
ing structural properties in (Hasan et al, 2006).
An interesting experiment was reported at the
NIST 2005 MT evaluation workshop (Och, 2005):
starting with a 5-gram LM trained on 75 million
words of Broadcast News data, a gain of about
0.5 point BLEU was observed each time when the
amount of LM training data was doubled, using at
the end 237 billion words of texts. Most of this
additional data was collected by Google on the In-
ternet. We believe that this kind of approach is dif-
ficult to apply to other tasks than Broadcast News
and other target languages than English. There are
many areas where automatic machine translation
could be deployed and for which considerably less
appropriate in-domain training data is available.
We could for instance mention automatic trans-
lation of medical records, translation systems for
tourism related tasks or even any task for which
Broadcast news and Web texts is of limited help.
In this work, we consider the translation of Eu-
ropean Parliament Speeches from Spanish to En-
glish, in the framework of an international evalua-
tion organized by the European TC-STAR project
in February 2006. The training data consists of
about 35M words of aligned texts that are also
used to train the target LM. In our experiments,
adding more than 580M words of Broadcast News
data had no impact on the BLEU score, despite
a notable decrease of the perplexity of the target
LM. Therefore, we suggest to use more complex
statistical LMs that are expected to take better ad-
vantage of the limited amount of appropriate train-
ing data. Promising candidates are random forest
LMs (Xu and Jelinek, 2004), random cluster LMs
(Emami and Jelinek, 2005) and the neural network
LM (Bengio et al, 2003). In this paper, we inves-
tigate whether the latter approach can be used in a
statistical machine translation system.
The basic idea of the neural network LM, also
called continuous space LM, is to project the word
indices onto a continuous space and to use a prob-
ability estimator operating on this space. Since the
resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neu-
ral network can be used to simultaneously learn
the projection of the words onto the continuous
space and to estimate the n-gram probabilities.
This is still a n-gram approach, but the LM pos-
terior probabilities are ?interpolated? for any pos-
sible context of length n-1 instead of backing-off
to shorter contexts. This approach was success-
fully used in large vocabulary speech recognition
(Schwenk and Gauvain, 2005), and we are inter-
ested here if similar ideas can be applied to statis-
tical machine translation.
This paper is organized as follows. In the next
section we first describe the baseline statistical
machine translation system. Section 3 presents
the architecture of the continuous space LM and
section 4 summarizes the experimental evaluation.
The paper concludes with a discussion of future
research directions.
2 Statistical Translation Engine
A word-based translation engine is used based on
the so-called IBM-4 model (Brown et al, 1993).
A brief description of this model is given below
along with the decoding algorithm.
The search algorithm aims at finding what tar-
get sentence e is most likely to have produced the
observed source sentence f . The translation model
Pr(f |e) is decomposed into four components:
1. a fertility model;
2. a lexical model of the form t(f |e), which
gives the probability that the target word e
translates into the source word f ;
3. a distortion model, that characterizes how
words are reordered when translated;
4. and probabilities to model the insertion of
source words that are not aligned to any tar-
get words.
An A* search was implemented to find the best
translation as predicted by the model, when given
enough time and memory, i.e., provided pruning
did not eliminate it. The decoder manages par-
tial hypotheses, each of which translates a subset
of source words into a sequence of target words.
Expanding a partial hypothesis consists of cover-
ing one extra source position (in random order)
and, by doing so, appending one, several or possi-
bly zero target words to its target word sequence.
For details about the implemented algorithm, the
reader is referred to (De?chelotte et al, 2006).
Decoding uses a 3-gram back-off target lan-
guage model. Equivalent hypotheses are merged,
and only the best scoring one is further expanded.
The decoder generates a lattice representing the
724
we
I
we
should
should
must
remember
remind
remember
that
,
that
that
,
that
you
,
,
,
becausebecause
because
it
I
they
that
can
can
can be
say
be
, because
can
itthey
we
that
can
can
can
be
be
have
be
have
be
have
it
it
has
forgotten
has forgotten
has
has
forgotten
forgotten
been
forgotten
been
forgotten
forgotten
.
.
forgotten
.
.
.
.
.
.
Figure 1: Example of a translation lattice. Source
sentence: ?conviene recordarlo , porque puede
que se haya olvidado .?, Reference 1: ?it is ap-
propriate to remember this , because it may have
been forgotten .? Reference 2: ?it is good to re-
member this , because maybe we forgot it .?
explored search space. Figure 1 shows an example
of such a search space, here heavily pruned for the
sake of clarity.
2.1 Sentence Splitting
The execution complexity of our SMT decoder in-
creases non-linearly with the length of the sen-
tence to be translated. Therefore, the source text
is split into smaller chunks, each one being trans-
lated separately. The chunks are then concatenated
together. Several algorithms have been proposed
in the literature that try to find the best splits, see
for instance (Berger et al, 1996). In this work, we
first split long sentences at punctuation marks, the
remaining segments that still exceed the allowed
length being split linearly. In a second pass, ad-
joining very short chunks are merged together.
During decoding, target LM probabilities of the
type Pr(w1|<s>) and Pr(</s>|wn?1wn) will be
requested at the beginning and at the end of the
hypothesized target sentence respectively.1 This is
correct when a whole sentence is translated, but
leads to wrong LM probabilities when processing
smaller chunks. Therefore, we define a sentence
break symbol, <b>, that is used at the beginning
and at the end of a chunk. During decoding a 3-
gram back-off LM is used that was trained on text
where sentence break symbols have been added.
Each chunk is translated and a lattice is gen-
1The symbols <s> and </s> denote the begin and end of
sentence marker respectively.
erated. The individual lattices are then joined,
omitting the sentence break symbols. Finally, the
resulting lattice is rescored with a LM that was
trained on text without sentence breaks. In that
way we find the best junction of the chunks. Sec-
tion 4.1 provides comparative results of the differ-
ent algorithms to split and join sentences.
2.2 Parameter Tuning
It is nowadays common practice to optimize the
coefficients of the log-linear combination of fea-
ture functions by maximizing the BLEU score on
the development data (Och and Ney, 2002). This
is usually done by first creating n-best lists that
are then reranked using an iterative optimization
algorithm.
In this work, a slightly different procedure was
used that operates directly on the translation lat-
tices. We believe that this is more efficient than
reranking n-best lists since it guarantees that al-
ways all possible hypotheses are considered. The
decoder first generates large lattices using the cur-
rent set of parameters. These lattices are then
processed by a separate tool that extracts the best
path, given the coefficients of six feature functions
(translations, distortion, fertility, spontaneous in-
sertion, target language model probability, and a
sentence length penalty). Then, the BLEU score
of the extracted solution is calculated. This tool is
called in a loop by the public numerical optimiza-
tion tool Condor (Berghen and Bersini, 2005). The
solution vector was usually found after about 100
iterations. In our experiments, only two cycles
of lattice generation and parameter optimization
were necessary (with a very small difference in the
BLEU score).
In all our experiments, the 4-gram back-off and
the neural network LM are used to calculate lan-
guage model probabilities that replace those of the
default 3-gram LM. An alternative would be to de-
fine each LM as a feature function and to combine
them under the log-linear model framework, us-
ing maximum BLEU training. We believe that this
would not make a notable difference in our experi-
ments since we do interpolate the individual LMs,
the coefficients being optimized to minimize per-
plexity on the development data. However, this
raises the interesting question whether the two cri-
teria lead to equivalent performance. The result
section provides some experimental evidence on
this topic.
725
3 Continuous Space Language Models
The architecture of the neural network LM is
shown in Figure 2. A standard fully-connected
multi-layer perceptron is used. The inputs to
the neural network are the indices of the n?1
previous words in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the posterior
probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (2)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word
of the vocabulary is coded by setting the ith ele-
ment of the vector to 1 and all the other elements
to 0. The ith line of the N ? P dimensional pro-
jection matrix corresponds to the continuous rep-
resentation of the ith word. Let us denote cl these
projections, dj the hidden layer activities, oi the
outputs, pi their softmax normalization, and mjl,
bj , vij and ki the hidden and output layer weights
and the corresponding biases. Using these nota-
tions, the neural network performs the following
operations:
dj = tanh
(?
l
mjl cl + bj
)
(3)
oi =
?
j
vij dj + ki (4)
pi = eoi /
N?
r=1
eor (5)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj). Training is
performed with the standard back-propagation al-
gorithm minimizing the following error function:
E =
N?
i=1
ti log pi + ?
?
??
jl
m2jl +
?
ij
v2ij
?
? (6)
where ti denotes the desired output, i.e., the prob-
ability should be 1.0 for the next word in the train-
ing sentence and 0.0 for all the other ones. The
first part of this equation is the cross-entropy be-
tween the output and the target probability dis-
tributions, and the second part is a regulariza-
tion term that aims to prevent the neural network
from overfitting the training data (weight decay).
The parameter ? has to be determined experimen-
tally. Training is done using a resampling algo-
rithm (Schwenk and Gauvain, 2005).
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space
LM. hj denotes the context wj?n+1, . . . , wj?1. P
is the size of one projection and H ,N is the size
of the hidden and output layer respectively. When
short-lists are used the size of the output layer is
much smaller then the size of the vocabulary.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the poste-
rior probabilities. Therefore, the neural network
directly minimizes the perplexity on the train-
ing data. Note also that the gradient is back-
propagated through the projection-layer, which
means that the neural network learns the projec-
tion of the words onto the continuous space that is
best for the probability estimation task.
The complexity to calculate one probability
with this basic version of the neural network LM is
quite high due to the large output layer. To speed
up the processing several improvements were used
(Schwenk, 2004):
1. Lattice rescoring: the statistical machine
translation decoder generates a lattice using
a 3-gram back-off LM. The neural network
LM is then used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Efficient implementation: collection of all
LM probability requests with the same con-
text ht in one lattice, propagation of several
examples at once through the neural network
and utilization of libraries with CPU opti-
mized matrix-operations.
The idea behind short-lists is to use the neural
726
network only to predict the s most frequent words,
s being much smaller than the size of the vocab-
ulary. All words in the vocabulary are still con-
sidered at the input of the neural network. The
LM probabilities of words in the short-list (P?N )
are calculated by the neural network and the LM
probabilities of the remaining words (P?B) are ob-
tained from a standard 4-gram back-off LM:
P? (wt|ht) =
{
P?N (wt|ht)PS(ht) if wt ? short-list
P?B(wt|ht) else
(7)
PS(ht) =
?
w?short?list(ht)
P?B(w|ht) (8)
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
short-list. This probability mass is precalculated
and stored in the data structures of the back-off
LM. A back-off technique is used if the probability
mass for a input context is not directly available.
It was not envisaged to use the neural network
LM directly during decoding. First, this would
probably lead to slow translation times due to the
higher complexity of the proposed LM. Second, it
is quite difficult to incorporate n-gram language
models into decoding, for n>3. Finally, we be-
lieve that the lattice framework can give the same
performances than direct decoding, under the con-
dition that the alternative hypotheses in the lattices
are rich enough. Estimates of the lattice oracle
BLEU score are given in the result section.
4 Experimental Evaluation
The experimental results provided here were ob-
tained in the framework of an international evalua-
tion organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
long-term effort to advance research in all core
technologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the min-
utes edited by the European Parliament in sev-
eral languages, also known as the Final Text Edi-
tions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used
to train the statistical translation models (see Ta-
ble 1 for some statistics). In addition, about 100h
of Parliament plenary sessions were recorded and
transcribed. This data is mainly used to train
2http://www.tc-star.org/
Spanish English
Sentence Pairs 1.2M
Total # Words 37.7M 33.8M
Vocabulary size 129k 74k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
the speech recognizers, but the transcriptions were
also used for the target LM of the translation sys-
tem (about 740k words).
Three different conditions are considered in
the TC-STAR evaluation: translation of the Fi-
nal Text Edition (text), translation of the tran-
scriptions of the acoustic development data (ver-
batim) and translation of speech recognizer output
(ASR). Here we only consider the verbatim condi-
tion, translating from Spanish to English. For this
task, the development data consists of 792 sen-
tences (25k words) and the evaluation data of 1597
sentences (61k words). Parts of the test data ori-
gins from the Spanish parliament which results in
a (small) mismatch between the development and
test data. Two reference translations are provided.
The scoring is case sensitive and includes punctu-
ation symbols.
The translation model was trained on 1.2M sen-
tences of parallel text using the Giza++ tool. All
back-off LMs were built using modified Kneser-
Ney smoothing and the SRI LM-toolkit (Stolcke,
2002). Separate LMs were first trained on the
English EPPS texts (33.8M words) and the tran-
scriptions of the acoustic training material (740k
words) respectively. These two LMs were then in-
terpolated together. Interpolation usually results in
lower perplexities than training directly one LM
on the pooled data, in particular if the corpora
come from different sources. An EM procedure
was used to find the interpolation coefficients that
minimize the perplexity on the development data.
The optimal coefficients are 0.78 for the Final Text
edition and 0.22 for the transcriptions.
4.1 Performance of the sentence splitting
algorithm
In this section, we first analyze the performance of
the sentence split algorithm. Table 2 compares the
results for different ways to translate the individ-
ual chunks (using a standard 3-gram LM versus
an LM trained on texts with sentence breaks in-
serted), and to extracted the global solution (con-
727
LM used Concatenate Lattice
during decoding 1-best join
Without
sentence breaks 40.20 41.63
With
sentence breaks 41.45 42.35
Table 2: BLEU scores for different ways to trans-
late sentence chunks and to extract the global so-
lution (see text for details).
catenating the 1-best solutions versus joining the
lattices followed by LM rescoring). It can be
clearly seen that joining the lattices and recalculat-
ing the LM probabilities gives better results than
just concatenating the 1-best solutions of the in-
dividual chunks (first line: BLEU score of 41.63
compared to 40.20). Using a LM trained on texts
with sentence breaks during decoding gives an ad-
ditional improvement of about 0.7 points BLEU
(42.35 compared to 41.63).
In our current implementation, the selection of
the sentence splits is based on punctuation marks
in the source text, but our procedure is compatible
with other methods. We just need to apply the sen-
tence splitting algorithm on the training data used
to build the LM during decoding.
4.2 Using the continuous space language
model
The continuous space language model was trained
on exactly the same data than the back-off refer-
ence language model, using the resampling algo-
rithm described in (Schwenk and Gauvain, 2005).
In this work, we use only 4-gram LMs, but the
complexity of the neural network LM increases
only slightly with the order of the LM. For each
experiment, the parameters of the log-linear com-
bination were optimized on the development data.
Perplexity on the development data set is a pop-
ular and easy to calculate measure to evaluate the
quality of a language model. However, it is not
clear if perplexity is a good criterion to predict
the improvements when the language model will
be used in a SMT system. For information, and
comparison with the back-off LM, Figure 3 shows
the perplexities for different configurations of the
continuous space LM. The perplexity clearly de-
creases with increasing size of the short-list and a
value of 8192 was used. In this case, 99% of the
requested LM probabilities are calculated by the
neural network when rescoring a lattice.
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 0  5  10  15  20  25  30  35
Pe
rp
le
xit
y
Number of training epochs
4-gram back-off LM
short-list of 2k
short-list of 4k
short-list of 8k
Figure 3: Perplexity of different configurations of
the continuous space LM.
Although the neural network LM could be used
alone, better results are obtained when interpolat-
ing it with the 4-gram back-off LM. It has even
turned out that it was advantageous to train several
neural network LMs with different context sizes3
and to interpolate them altogether. In that way,
a perplexity decrease from 79.6 to 65.0 was ob-
tained. For the sake of simplicity we will still call
this interpolation the neural network LM.
Back-off LM Neural LM
3-gram 4-gram 4-gram
Perplexity 85.5 79.6 65.0
Dev data:
BLEU 42.35 43.36 44.42
WER 45.9% 45.1% 44.4%
PER 31.8% 31.3% 30.8%
Eval data:
BLEU 39.77 40.62 41.45
WER 48.2% 47.4% 46.7%
PER 33.6% 33.1% 32.8%
Table 3: Result comparison for the different LMs.
BLEU uses 2 reference translations. WER=word
error rate, PER=position independent WER.
Table 3 summarizes the results on the devel-
opment and evaluation data. The coefficients of
the feature functions are always those optimized
on the development data. The joined translation
lattices were rescored with a 4-gram back-off and
the neural network LM. Using a 4-gram back-
off LM gives an improvement of 1 point BLEU
3The values are in the range 150. . .400. The other param-
eters are: H=500, ?=0.00003 and the initial learning rate was
0.005 with an exponential decay. The networks were trained
for 20 epochs through the training data.
728
Spanish: es el nico premio Sajarov que no ha podido recibir su premio despus de ms de tres
mil quinientos das de cautiverio .
Backoff LM: it is only the Sakharov Prize has not been able to receive the prize after three thousand
, five days of detention .
CSLM : it is the only Sakharov Prize has not been able to receive the prize after three thousand
five days of detention .
Reference 1: she is the only Sakharov laureate who has not been able to receive her prize after
more than three thousand five hundred days in captivity .
Reference 2: she is the only Sacharov prizewinner who couldn?t yet pick up her prize after more
than three thousand five hundred days of imprisonment .
Figure 4: Example translation using the back-off and the continuous space language model (CSLM).
on the Dev data (+0.8 on Test set) compared to
the 3-gram back-off LM. The neural network LM
achieves an additional improvement of 1 point
BLEU (+0.8 on Test data), on top of the 4-gram
back-off LM. Small improvements of the word er-
ror rate (WER) and the position independent word
error rate (PER) were also observed.
As usually observed in SMT, the improvements
on the test data are smaller than those on the de-
velopment data which was used to tune the param-
eters. As a rule of thumb, the gain on the test data
is often half as large as on the Dev-data. The 4-
gram back-off and neural network LM show both
a good generalization behavior.
 42.8
 43
 43.2
 43.4
 43.6
 43.8
 44
 44.2
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 64
 66
 68
 70
 72
 74
 76
 78
BL
EU
 s
co
re
Pe
rp
le
xit
y
Interpolation coefficient
4-gram back-off LM
BLEU score
Perplexity
Figure 5: BLEU score and perplexity in function
of the interpolation coefficient of the back-off 4-
gram LM.
Figure 5 shows the perplexity and the BLEU
score for different interpolation coefficients of the
4-gram back-off LM. For a value of 1.0 the back-
off LM is used alone, while only the neural net-
work LMs are used for a value of 0.0. Using an
EM procedure to minimize perplexity of the inter-
polated model gives a value of 0.189. This value
also seems to correspond to the best BLEU score.
This is a surprising result, and has the advan-
tage that we do not need to tune the interpola-
tion coefficient in the framework of the log-linear
feature function combination. The weights of the
other feature functions were optimized separately
for each experiment. We noticed a tendency to
a slightly higher weight for the continuous space
LM and a lower sentence length penalty.
In a contrastive experiment, the LM training
data was substantially increased by adding 352M
words of commercial Broadcast News data and
232M words of CNN news collected on the Inter-
net. Although the perplexity of the 4-gram back-
off LM decreased by 5 points to 74.1, we observed
no change in the BLEU score. In order to estimate
the oracle BLEU score of the lattices we build a 4-
gram back-off LM on the development data. Lat-
tice rescoring achieved a BLEU score of 59.10.
There are many discussions about the BLEU
score being or not a meaningful measure to as-
sess the quality of an automatic translation sys-
tem. It would be interesting to verify if the contin-
uous space LM has an impact when human judg-
ments of the translation quality are used, in partic-
ular with respect to fluency. Unfortunately, this is
not planed in the TC-STAR evaluation campaign,
and we give instead an example translation (see
Figure 4). In this case, two errors were corrected
(insertion of the word ?the? and deletion of the
comma).
5 Conclusion and Future Work
Some SMT decoders have an execution complex-
ity that increases rapidly with the length of the
sentences to be translated, which are usually split
729
into smaller chunks and translated separately. This
can lead to translation errors and bad modeling
of the LM probabilities of the words at both ends
of the chunks. We have presented a lattice join-
ing and rescoring approach that obtained signifi-
cant improvements in the BLEU score compared
to simply concatenating the 1-best solutions of
the individual chunks. The task considered is the
translation of European Parliament Speeches in
the framework of the TC-STAR project.
We have also presented a neural network LM
that performs probability estimation in a contin-
uous space. Since the resulting probability func-
tions are smooth functions of the word represen-
tation, better generalization to unknown n-grams
can be expected. This is particularly interesting
for tasks where only limited amounts of appropri-
ate LM training material are available, but the pro-
posed LM can be also trained on several hundred
millions words. The continuous space LM is used
to rescore the translation lattices. We obtained
an improvement of 0.8 points BLEU on the test
data compared to a 4-gram back-off LM, which it-
self had already achieved the same improvement
in comparison to a 3-gram LM.
The results reported in this paper have been ob-
tained with a word based SMT system, but the
continuous space LM can also be used with a
phrase-based system. One could expect that the
target language model plays a different role in
a phrase-based system since the phrases induce
some local coherency on the target sentence. This
will be studied in the future. Another promis-
ing direction that we have not yet explored, is to
build long-span LM, i.e. with n much greater than
4. The complexity of our approach increases only
slightly with n. Long-span LM could possibly im-
prove the word-ordering of the generated sentence
if the translation lattices include the correct paths.
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(2):1137?1155.
A. Berger, S. Della Pietra, and Vincent J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22:39?71.
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Machine Translation Summit.
Daniel De?chelotte, Holger Schwenk, and Jean-Luc
Gauvain. 2006. The 2006 LIMSI statistical ma-
chine translation system for TC-STAR. In TC-STAR
Speech to Speech Translation Workshop, Barcelona.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In ICASSP,
pages I:581?584.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and
H. Ney. 2005. Cross domain automatic transcrip-
tion on the TC-STAR EPPS corpus. In ICASSP.
Sasa Hasan, Olivier Bender, and Hermann Ney. 2006.
Reranking translation hypothesis using structural
properties. In LREC.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In ACL?05 workshop on Building and Using Paral-
lel Text, pages 125?128.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302,
University of Pennsylvania.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In NAACL, pages 161?168.
Franz-Joseph Och. 2005. The Google statistical ma-
chine translation system for the 2005 Nist MT eval-
uation, Oral presentation at the 2005 Nist MT Eval-
uation workshop, June 20.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In EMNLP, pages 201?208.
Holger Schwenk. 2004. Efficient training of large
neural networks for language modeling. In IJCNN,
pages 3059?3062.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
Peng Xu and Frederick Jelinek. 2004. Random forest
in language modeling. In EMNLP, pages 325?332.
730
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65?71,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Combining Morphosyntactic Enriched Representation with
n-best Reranking in Statistical Translation
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte and H. Schwenk
Spoken Language Processing Group
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{maynard,allauzen,dechelot,schwenk}@limsi.fr
Abstract
The purpose of this work is to explore
the integration of morphosyntactic infor-
mation into the translation model itself, by
enriching words with their morphosyntac-
tic categories. We investigate word dis-
ambiguation using morphosyntactic cate-
gories, n-best hypotheses reranking, and
the combination of both methods with
word or morphosyntactic n-gram lan-
guage model reranking. Experiments
are carried out on the English-to-Spanish
translation task. Using the morphosyn-
tactic language model alone does not
results in any improvement in perfor-
mance. However, combining morphosyn-
tactic word disambiguation with a word
based 4-gram language model results in a
relative improvement in the BLEU score
of 2.3% on the development set and 1.9%
on the test set.
1 Introduction
Recent works in statistical machine translation
(SMT) shows how phrase-based modeling (Och and
Ney, 2000a; Koehn et al, 2003) significantly out-
perform the historical word-based modeling (Brown
et al, 1993). Using phrases, i.e. sequences of
words, as translation units allows the system to pre-
serve local word order constraints and to improve
the consistency of phrases during the translation pro-
cess. Phrase-based models provide some sort of
context information as opposed to word-based mod-
els. Training a phrase-based model typically re-
quires aligning a parallel corpus, extracting phrases
and scoring them using word and phrase counts. The
derived statistics capture the structure of natural lan-
guage to some extent, including implicit syntactic
and semantic relations.
The output of a SMT system may be difficult to
understand by humans, requiring re-ordering words
to recover its syntactic structure. Modeling language
generation as a word-based Markovian source (an n-
gram language model) discards linguistic properties
such as long term word dependency and word-order
or phrase-order syntactic constraints. Therefore, ex-
plicit introduction of structure in the language mod-
els becomes a major and promising focus of atten-
tion.
However, as of today, it seems difficult to outper-
form a 4-gram word language model. Several stud-
ies have attempted to use morphosyntactic informa-
tion (also known as part-of-speech or POS informa-
tion) to improve translation. (Och et al, 2004) have
explored many different feature functions. Rerank-
ing n-best lists using POS has also been explored by
(Hasan et al, 2006). In (Kirchhoff and Yang, 2005),
a factored language model using POS information
showed similar performance to a 4-gram word lan-
guage model. Syntax-based language models have
also been investigated in (Charniak et al, 2003). All
these studies use word phrases as translation units
and POS information in just a post-processing step.
This paper explores the integration of morphosyn-
tactic information into the translation model itself
by enriching words with their morphosyntactic cat-
65
egories. The same idea has already been applied
in (Hwang et al, 2007) to the Basic Travel Ex-
pression Corpus (BTEC). To our knowledge, this
approach has not been evaluated on a large real-
word translation problem. We report results on
the TC-STAR task (public European Parliament Ple-
nary Sessions translation). Furthermore, we pro-
pose to combine this approach with classical n-best
list reranking. Experiments are carried out on the
English-to-Spanish task using a system based on the
publicly available Moses decoder.
This paper is organized as follows: In Section
2 we first describe the baseline statistical machine
translation systems. Section 3 presents the consid-
ered task and the processing of the corpora. The
experimental evaluation is summarized in section 4.
The paper concludes with a discussion of future re-
search directions.
2 System Description
The goal of statistical machine translation is to pro-
duce a target sentence e from a source sentence f .
Among all possible target language sentences the
one with the highest probability is chosen. The use
of a maximum entropy approach simplifies the intro-
duction of several additional models explaining the
translation process:
e? = argmaxPr(e|f)
= argmaxe {exp(
?
i
?ihi(e, f))} (1)
where the feature functions hi are the system
models characterizing the translation process, and
the coefficients ?i act as weights.
2.1 Moses decoder
Moses1 is an open-source, state-of-the-art phrase-
based decoder. It implements an efficient beam-
search algorithm. Scripts are also provided to train a
phrase-based model. The popular Giza++ (Och and
Ney, 2000b) tool is used to align the parallel corpora.
The baseline system uses 8 feature functions hi,
namely phrase translation probabilities in both di-
rections, lexical translation probabilities in both di-
rections, a distortion feature, a word and a phrase
1http://www.statmt.org/moses/
penalty and a trigram target language model. Ad-
ditional features can be added, as described in the
following sections. The weights ?i are typically op-
timized so as to maximize a scoring function on a
development set (Och and Ney, 2002).
The moses decoder can output n-best lists, pro-
ducing either distinct target sentences or not (as
different segmentations may lead to the same sen-
tence). In this work, distinct sentences were always
used.
These n-best lists can be rescored using higher
order language models (word- or syntactic-based).
There are two ways to carry out the rescoring: one,
by replacing the language model score or by adding
a new feature function; two, by performing a log-
linear interpolation of the language model used for
decoding and the new language model. This latter
approach was used in all the experiments described
in this paper. The set of weights is systematically
re-optimized using the algorithm presented below.
2.2 Weight optimization
A common criterion to optimize the coefficients of
the log-linear combination of feature functions is to
maximize the BLEU score (Papineni et al, 2002)
on a development set (Och and Ney, 2002). For
this purpose, the public numerical optimization tool
Condor (Berghen and Bersini, 2005) is integrated in
the following iterative algorithm:
0. Using good general purpose weights, the
Moses decoder is used to generate 1000-best
lists.
1. The 1000-best lists are reranked using the cur-
rent set of weights.
2. The current hypothesis is extracted and scored.
3. This BLEU score is passed to Condor, which
either computes a new set of weights (the al-
gorithm then proceeds to step 1) or detects that
a local maxima has been reached and the algo-
rithm stops iterating.
The solution is usually found after about 100 itera-
tions. It is stressed that the n-best lists are generated
only once and that the whole tuning operates only
on the n-best lists.
66
English: IPP declareV V P resumedV V D theDT sessionNN ofIN theDT EuropeanNP ParliamentNP
Spanish: declaroV Lfin reanudadoV Ladj elART perodoNC dePREP sesionesNC
delPDEL ParlamentoNC EuropeoADJ
Figure 1: Example of POS-tag enriched bi-text used to train the translation models
2.3 POS disambiguation
It is well-known that syntactic structures vary
greatly across languages. Spanish, for example,
can be considered as a highly inflectional language,
whereas inflection plays only a marginal role in En-
glish.
POS language models can be used to rerank the
translation hypothesis, but this requires tagging the
n-best lists generated by the SMT system. This can
be difficult since POS taggers are not well suited for
ill-formed or incorrect sentences. Finding a method
in which morphosyntactic information is used di-
rectly in the translation model could help overcome
this drawback but also takes account for the syntac-
tic specificities of both source and target languages.
It seems likely that the morphosyntactic informa-
tion of each word will be useful to encode linguis-
tic characteristics, resulting in a sort of word disam-
biguation by considering its morphosyntactic cate-
gory. Therefore, in this work we investigate a trans-
lation model which enriches every word with its syn-
tactic category. The enriched translation units are a
combination of the original word and the POS tag, as
shown in Figure 1. The translation system takes a se-
quence of enriched units as inputs and outputs. This
implies that the test data must be POS tagged before
translation. Likewise, the POS tags in the enriched
output are removed at the end of the process to pro-
vide the final translation hypothesis which contain
only a word sequence. This approach also allows
to carry out a n-best reranking step using either a
word-based or a POS-based language model.
3 Task, corpus and tools
The experimental results reported in this article were
obtained in the framework of an international evalu-
ation organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
2http://www.tc-star.org/
long-term effort to advance research in all core tech-
nologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the sum-
mary edited by the European Parliament in several
languages, which is also known as the Final Text
Editions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used to
train the statistical translation models (see Table 1
for some statistics).
Spanish English
Whole parallel corpus
Sentence Pairs 1.2M
Total # Words 34.1M 32.7M
Vocabulary size 129k 74k
Sentence length ? 40
Sentence Pairs 0.91M
Total # Words 18.5M 18.0M
Word vocabulary 104k 71k
POS vocabulary 69 59
Enriched units vocab. 115k 77.6k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
Three different conditions are considered in the
TC-STAR evaluation: translation of the Final Text
Edition (text), translation of the transcriptions of the
acoustic development data (verbatim) and transla-
tion of speech recognizer output (ASR). Here we
only consider the verbatim condition, translating
from English to Spanish. For this task, the develop-
ment and test data consists of about 30k words. The
test data is partially collected in the Spanish parlia-
ment. This results in a small mismatch between de-
velopment and test data. Two reference translations
are provided. The scoring is case sensitive and in-
cludes punctuation symbols.
67
3.1 Text normalization
The training data used for normalization differs sig-
nificantly from the development and test data. The
Final Text Edition corpus follows common ortho-
graphic rules (for instance, the first letter of the word
following a full stop or a column is capitalized) and
represents most of the dates, quantities, article refer-
ences and other numbers in digits. Thus the text had
to be ?true-cased? and all numbers were verbalized
using in-house language-specific tools. Numbers are
not tagged as such at this stage; this is entirely left
to the POS tagger.
3.2 Translation model training corpus
Long sentences (more than 40 words) greatly slow
down the training process, especially at the align-
ment step with Giza++. As shown in Figure 2, the
histogram of the length of Spanish sentences in the
training corpus decreases steadily after a length of
20 to 25 words, and English sentences exhibit a sim-
ilar behavior. Suppressing long sentences from the
corpus reduces the number of aligned sentences by
roughly 25% (see Table 1) but speeds the whole
training procedure by a factor of 3. The impact on
performance is discussed in the next section.
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 0  10  20  30  40  50  60  70  80  90  100
Histogram of Spanish sentences? lengths (training set)
Figure 2: Histogram of the sentence length (Spanish
part of the parallel corpus).
3.3 Language model training corpus
In the experiments reported below, a trigram word
language model is used during decoding. This
model is trained on the Spanish part of the parallel
corpus using only sentences shorter than 40 words
(total of 18.5M of language model training data).
Second pass language models were trained on all
available monolingual data (34.1M words).
3.4 Tools
POS tagging was performed with the TreeTagger
(Schmid, 1994). This software provides resources
for both of the considered languages and it is freely
available. TreeTagger is a Markovian tagger that
uses decision trees to estimate trigram transition
probabilities. The English version is trained on the
PENN treebank corpus3 and the Spanish version on
the CRATER corpus.4
Language models are built using the SRI-LM
toolkit (Stolcke, 2002). Modified Knesser-Ney dis-
counting was used for all models. In (Goodman,
2001), a systematic description and comparison of
the usual smoothing methods is reported. Modified
Knesser-Ney discounting appears to be the most ef-
ficient method.
4 Experiments and Results
Two baseline English-to-Spanish translation mod-
els were created with Moses. The first model was
trained on the whole parallel text ? note that sen-
tences with more than 100 words are excluded by
Giza++. The second model was trained on the cor-
pus using only sentences with at most 40 words. The
BLEU score on the development set using good gen-
eral purpose weights is 48.0 for the first model and
47.0 for the second. Because training on the whole
bi-text is much slower, we decided to perform our
experiments on the bi-texts restricted to the ?short?
sentences.
4.1 Language model generation
The reranking experiments presented below use the
following language models trained on the Spanish
part of the whole training corpus:
? word language models,
? POS language model,
? POS language model, with a stop list used to
remove the 100 most frequent words (POS-
stop100 LM),
? language model of enriched units.
3http://www.cis.upenn.edu/ treebank
4http://www.comp.lancs.ac.uk/linguistics/crater/corpus.html
68
English : you will be aware President that over the last few sessions in Strasbourg. ..
Baseline: usted sabe que el Presidente durante los u?ltimos sesiones en Estrasburgo ...
Enriched units: usted sabe que el Presidente en los u?ltimos per??odos de sesiones en Estrasburgo ...
English : ... in this house there might be some recognition ...
Baseline: ... en esta asamblea no puede ser un cierto reconocimiento ...
Enriched units: ... en esta asamblea existe un cierto reconocimiento ...
Figure 3: Comparative translations using the baseline word system and the enriched unit system.
For each of these four models, various orders
were tested (n = 3, 4, 5), but in this paper we only
report those orders that yielded the greatest improve-
ments. POS language models were obtained by first
extracting POS sequences from the previously POS-
tagged training corpus and then by estimating stan-
dard back-off language models.
As shown in Table 1, the vocabulary size of the
word language model is 104k for Spanish and 74k
for English. The number of POS is small: 69 for
Spanish and 59 for English. We emphasize that
the tagset provided by TreeTagger does include nei-
ther gender nor number distinction. The vocabulary
size of the enriched-unit language model is 115k for
Spanish and 77.6k for English. The syntactical am-
biguity of words is low: the mean ambiguity ratio is
1.14 for Spanish and 1.12 for English.
4.2 Reranking the word n-best lists
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on words as units are summarized in Table 2. The
baseline result, with trigram word LM reranking,
gives a BLEU score of 47.0 (1rst row). From the
n-best lists provided by this translation model, we
compared reranking performances with different tar-
get language models. As observed in the literature,
an improvement can be obtained by reranking with
a 4-gram word language model (47.0 ? 47.5, 2d
row). By post-tagging this n-best list, a POS lan-
guage model reranking can be performed. However,
reranking with a 5-gram POS language model alone
does not give any improvement from the baseline
(BLEU score of 46.9, 3rd row). This result corre-
sponds to known work in the literature (Kirchhoff
and Yang, 2005; Hasan et al, 2006), when using
POS only as a post-processing step during rerank-
ing. As suggested in section 2.3, this lack of per-
formance can be due to the fact that the tagger is
not able to provide a usefull tagging of sentences
included in the n-best lists. This observation is
also available when reranking of the word n-best is
done with a language model based on enriched units
(BLEU score of 47.6, not reported in Table 2).
4.3 POS disambiguation and reranking
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on enriched units are summarized in Table 3. Us-
ing a trigram language model of enriched transla-
tion units leads to a BLEU score of 47.4, a 0.4 in-
crease over the baseline presented in section 4.2.
Figure 3 shows comparative translation examples
from the baseline and the enriched translation sys-
tems. In the first example, the baseline system out-
puts ?durante los u?ltimos sesiones? where the en-
riched translation system produces ?en los u?ltimos
per??odos de sesiones?, a better translation that may
be attributed to the introduction of the masculine
word ?per??odos?, allowing the system to build a
syntactically correct sentence. In the second exam-
ple, the syntactical error ?no puede ser un cierto re-
conocimiento? produced by the baseline system in-
duces an incorrect meaning of the sentence, whereas
the enriched translation system hypothesis ?existe un
cierto reconocimiento? is both syntactically and se-
mantically correct.
Reranking the enriched n-best with POS language
models (either with or without a stop list) does not
seem to be efficient (0.3 BLEU increasing with the
POS-stop100 language model).
A better improvement is obtained when reranking
is performed with the 4-gram word language model.
This results in a BLEU score of 47.9, correspond-
ing to a 0.9 improvement over the word baseline. It
is interesting to observe that reranking a n-best list
69
Dev. Test
3g word LM baseline 47.0 46.0
4g word LM reranking 47.5 46.5
5g POS reranking 46.9 46.1
Table 2: BLEU scores using words as translation
units.
obtained with a translation model based on enriched
units with a word LM results in better performances
than a enriched units LM reranking of a n-best list
obtained with a translation model based on words.
The last two rows of Table 3 give results when
combining word and POS language models to rerank
the enriched n-best lists. In both cases, 10 features
are used for reranking (8 Moses features + word
language model probability + POS language model
probability). The best result is obtained by com-
bining the 5-gram word language model with the 5-
gram POS-stop100 language model. In that case,
the best BLEU score is observed (48.1), with a 2.3%
relative increase over the trigram word baseline.
4.4 Results on the test set
The results on the test set are given in the second
column of Tables 2 and 3. Although the enriched
translation system is only 0.1 BLEU over the base-
line system (46.0 ? 46.1) when using a trigram lan-
guage model, the best condition observed on the de-
velopment set (word and POS-stop100 LMs rerank-
ing) results in a 46.8 BLEU score, corresponding to
a 0.8 increasing.
It can be observed that rescoring with a 4-gram
word language model leads to same score resulting
in a 1.9% relative increase over the trigram word
baseline.
5 Conclusion and future work
Combining word language model reranking of n-
best lists based on syntactically enriched units seems
to produce more consistent hypotheses. Using en-
riched translation units results in a relative 2.3%
improvement in BLEU on the development set and
1.9% on the test over the trigram baseline. Over a
standard translation model with 4-gram rescoring,
the enriched unit translation model leads to an abso-
lute increase in BLEU score of 0.4 both on the devel-
opment and the test sets. These first results are en-
Dev. Test
3g enriched units LM baseline 47.4 46.1
4g enriched units LM reranking 47.8 46.8
4g word LM reranking 47.9 46.9
5g POS LM reranking 47.5 46.2
5g POS-stop100 LM reranking 47.7 46.3
word + POS LMs reranking 47.9 46.9
word + POS-stop100 LMs rerank. 48.1 46.8
Table 3: BLEU scores using enriched translation
units.
couraging enough to further investigate the integra-
tion of syntactic information in the translation model
itself, rather than to restrict it to the post-processing
pass. As follow-up experiments, it is planned to in-
clude gender and number information in the tagset,
as well as the word stems to the enriched units.
This work should be considered as preliminary
experiments for the investigation of factored trans-
lation models, which Moses is able to handle. POS
factorization is indeed a way to add some general-
ization capability to the enriched translation models.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), and by the French Government
under the project INSTAR (ANR JCJC06 143038).
We would like to thanks Marc Ferras for his help
concerning the Spanish language.
References
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proceedings of MT Summit IX.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and ?H.
Ney. 2005. Cross domain automatic transcription on
70
the TC-STAR epps corpus. In Proceedings of ICASSP
2005.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434, October.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypothesis using structural properties. In Pro-
ceedings of EACL 2006.
Y.S. Hwang, A. Finch, and Y. Sasaki. 2007. Improving
statistical machine translation using shallow linguistic
knoledge. to be published in Computer, Speech and
Language.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation. In
Proceedings of ACL ?05 workshop on Building and Us-
ing Parallel Text, pages 125?128.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
2003 (HLT-NAACL 2003), Edmonton, Canada, May.
Franz Josef Och and Hermann Ney. 2000a. Improved
statistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 440?447, Hongkong, China, Octo-
ber.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440?447, Hong Kong, China,
October.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In NAACL,
pages 161?168.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, University of Pennsylva-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, September.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages II:
901?904.
71
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
