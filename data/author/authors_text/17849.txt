Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Word Association Profiles and their Use for Automated Scoring of Essays
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,mflor}@ets.org
Abstract
We describe a new representation of the
content vocabulary of a text we call word
association profile that captures the pro-
portions of highly associated, mildly asso-
ciated, unassociated, and dis-associated
pairs of words that co-exist in the given
text. We illustrate the shape of the dis-
tirbution and observe variation with genre
and target audience. We present a study
of the relationship between quality of writ-
ing and word association profiles. For a
set of essays written by college graduates
on a number of general topics, we show
that the higher scoring essays tend to have
higher percentages of both highly asso-
ciated and dis-associated pairs, and lower
percentages of mildly associated pairs of
words. Finally, we use word association
profiles to improve a system for automated
scoring of essays.
1 Introduction
The vast majority of contemporary research that
investigates statistical properties of language deals
with characterizing words by extracting infor-
mation about their behavior from large corpora.
Thus, co-occurrence of words in n-word windows,
syntactic structures, sentences, paragraphs, and
even whole documents is captured in vector-space
models built from text corpora (Turney and Pan-
tel, 2010; Basili and Pennacchiotti, 2010; Erk and
Pado?, 2008; Mitchell and Lapata, 2008; Bullinaria
and Levy, 2007; Jones and Mewhort, 2007; Pado
and Lapata, 2007; Lin, 1998; Landauer and Du-
mais, 1997; Lund and Burgess, 1996; Salton et al,
1975). However, little is known about typical pro-
files of texts in terms of co-occurrence behavior
of their words. Some information can be inferred
from the success of statistical techniques in pre-
dicting certain structures in text. For example, the
fact that a text segmentation algorithm that uses
information about patterns of word co-occurrences
can detect sub-topic shifts in a text (Riedl and Bie-
mann, 2012; Misra et al, 2009; Eisenstein and
Barzilay, 2008) tells us that texts contain some
proportion of more highly associated word pairs
(those in subsequent sentences within the same
topical unit) and of less highly associated pairs
(those in sentences from different topical units).1
Yet, does each text have a different distribution
of highly associated, mildly associated, unassoci-
ated, and dis-associated pairs of words, or do texts
tend to strike a similar balance of these? What
are the proportions of the different levels of asso-
ciation, how much variation there exists, and are
there systematic differences between various kinds
of texts? We present research that makes a first
step in addressing these questions.
From the applied perspective, our interest is in
quantifying differences between well-written and
poorly written essays, for the purposes of auto-
mated scoring of essays. We therefore concentrate
on essay data for the main experiments reported in
this paper, although some additional corpora will
be used for illustration purposes.
The paper is organized as follows. Section 2
presents our methodology for building word as-
sociation profiles for texts. Section 3 illustrates
the profiles for three corpora from different gen-
res. Section 4.2 presents our study of the relation-
ship between writing quality and patterns of word
associations, with section 4.5 showing the results
of adding a feature based on word association pro-
file to a state-of-art essay scoring system. Related
work is reviewed is section 5.
1Note that the classical approach to topical segmentation
of texts, TextTiling (Hearst, 1997), uses only word repeti-
tions. The cited approaches use topic models that are in turn
estimated using word co-occurrence.
1148
2 Methodology
In order to describe the word association profile
of a text, three decisions need to be made. The
first decision is how to quantify the extent of co-
occurrence between two words; we will use point-
wise mutual information (PMI) estimated from a
large and diverse corpus of texts. The second is
which pairs of words in a text to consider when
building a profile for the text; we opted for all pairs
of content word types occurring in a text, irrespec-
tive of the distance between them. We consider
word types, not tokens; no lemmatization is per-
formed. The third decision is how to represent the
co-occurrence profiles; we use a histogram where
each bin represents the proportion of word pairs in
the given interval of PMI values. The rest of the
section gives more detail about these decisions.
To obtain comprehensive information about
typical co-occurrence behavior of words of
English, we build a first-order co-occurrence
word-space model (Turney and Pantel, 2010; Ba-
roni and Lenci, 2010). The model was generated
from a corpus of texts of about 2.5 billion words,
counting co-occurrence in a paragraph,2 using no
distance coefficients (Bullinaria and Levy, 2007).
About 2 billion words come from the Gigaword
2003 corpus (Graff and Cieri, 2003). Additional
500 million words come from an in-house corpus
containing popular science and fiction texts. Oc-
currence counts of 2.1 million word types and of
1,279 million word type pairs are efficiently com-
pressed using the TrendStream technology (Flor,
2013), resulting in a database file of 4.7GB. Trend-
Stream is a trie-based architecture for storage, re-
trieval, and updating of very large word n-gram
datasets. We store pairwise word associations as
bigrams; since associations are unordered, only
one of the orders in actually stored in the database.
There is an extensive literature on the use of
word-association measures for NLP, especially for
detection of collocations (Pecina, 2010; Evert,
2008; Futagi et al, 2008). The use of point-
wise mutual information with word-space models
is noted in (Zhang et al, 2012; Baroni and Lenci,
2010; Mitchell and Lapata, 2008; Turney, 2001).
Point-wise mutual information is defined as fol-
lows (Church and Hanks, 1990):
2In all texts, we use human-marked paragraphs, indicated
either by a new line or by an xml markup.
PMI(x, y) = log2
P (x, y)
P (x)P (y) (1)
Differently from Church and Hanks (1990), we
disregard word order when computing P (x, y).
All probabilities are estimated using frequencies.
We define WAPT ? a word association pro-
file of a text T ? as the distribution of PMI(x, y)
for all pairs of content3 word types (x, y) ?T.
All pairs of word types for which the associations
database returned a null value (the pair has never
been observed in the same paragraph) are ex-
cluded from the calculation. For our main dataset
(described later as setA, section 4.1), the average
percentage of non-null values per text is 92%.
To represent the WAP of a text, we use a 60-bin
histogram spanning all PMI values. The lowest
bin (shown in Figures 1 and 2 as PMI = ?5) con-
tains pairs with PMI??5; the topmost bin (shown
in Figures 1 and 2 as PMI = 4.83) contains pairs
with PMI> 4.67, while the rest of the bins contain
word pairs (x, y) with ?5 <PMI(x, y) ? 4.67.
Each bin in the histogram (apart from the top and
the bottom ones) corresponds to a PMI interval
of 0.167. We chose a relatively fine-grained bin-
ning and performed no optimization for grid selec-
tion; for more sophisticated gridding approaches
to study non-linear relationships in the data, see
Reshef et al (2011).
We will say that a text A is tighter than text
B if the WAP of A is shifted towards the higher
end of PMI values relative to text B. The intuition
behind the terminology is that texts with higher
proportions of highly associated pairs are likelier
to be more focused, dealing with a small num-
ber of topics at greater length, as opposed to texts
that bring various different themes into the text to
various extents. Thus, the text ?The dog barked
and wagged its tail? is much tighter than the text
?Green ideas sleep furiously?, with all the six con-
tent word pairs scoring above PMI=5.5 in the first
and below PMI=2.2 in the second.4
3 Illustration: The shape of the
distribution
For a first illustration, we use a corpus of 5,904
essays written as part of a standardized graduate
3We part-of-speech tag a text using OpenNLP tagger
(http://opennlp.apache.org) and only take into account com-
mon and proper nouns, verbs, adjectives, and adverbs.
4We omitted colorless from the second example, as color-
less is actually highly associated with green (PMI=4.36).
1149
school admission test (a full descrption of these
data is given in section 4.1, under setA p1-p6). For
each essay, we compute the WAP and represent it
using the 60-bin histogram. For each bin in the
histogram, we compute its average value over the
5,904 essays; additionally, we compute the 15th
and 85th percentiles for each bin, so that the band
between them contains values observed for 70%
of the texts. The series with the solid thick (blue)
line in Figure 1 shows the distribution of the ave-
rage percentage of word type pairs per bin (essays-
av); the dotted lines above and below show the
band capturing the middle 70% of the distribution
(essays-15 and essays-85).
We observe that the shape of the WAP is very
stable across essays, and the variation around the
average is quite limited.
Next, consider the thin solid (green) line with
asterisk-shaped markers in Figure 1 that plots a
similarly-binned histogram for the normal distri-
bution with ?=0.90 and ?=0.66. We note that
for values below PMI=2.17, the normal curve is
within or almost within the 70% band for the essay
data. The divergence occurs at the right tail with
PMI>2.17, that covers, on average, about 8% of
the pairs (5.6% and 10.4% for the 15th and 85th
percentiles, respectively).
To get an idea about possible variation in the
distribution, we consider two additional corpora
from different genres. We use a corpus of Wall
Street Journal 1987 articles from the TIPSTER
collection.5 We picked articles of 250 to 700
words in length, in order to keep the length of texts
comparable to the essay data, while varying the
genre; 770 such articles were found. The dashed
(orange) line in Figure 1 shows the distribution of
average values for the WSJ collection (wsj-av).
We observe that the shape of the distribution is
similar to that of essay data, although WSJ articles
tend to be less tight, on average, since the distribu-
tion in PMI<2.17 area in the WSJ data is shifted
to the left relative to essays. Yet, the picture at the
right tail is remarkably similar to that of the es-
says, with 9% of word pairs, on average, having
PMI>2.17.
The second additional corpus contains 140 lite-
rary texts written or adapted for readers in grades
3 and 4 in US schools (Sheehan et al, 2008).
In terms of length, these texts fall into the same
range as the other corpora, averaging 507 words.
5LDC93T3A in LDC catalogue
The average WAP for these texts is shown with
a thin solid (purple) line with circular markers
in Figure 1 (Grades 3-4). These texts are much
tighter than texts in the other two collections, as
the distribution is shifted to the right. The right
tail, with PMI>2.17, holds 19% of all word pairs
in these texts ? more than twice the proportion
in essays written by college graduates or in texts
from the WSJ.
It is instructive to check whether the over-use
of highly associated pairs is felt during reading.
These texts strike an adult reader as overly ex-
plicit, taking the space to state things that an adult
reader would readily infer or assume. For exam-
ple, consider the following opening paragraph:
?Grandma Rose gave Daniel a recorder.
A recorder is a musical instrument.
Daniel learned to play by blowing on the
recorder. It didn?t take lots of air. It
didn?t take big hands to hold since it was
pocket-sized. His fingers covered the
toneholes just fine. Soon Daniel played
entire songs. His mother loved to lis-
ten. Sometimes she hummed along with
Daniel?s recorder.?
The second and the third sentences state things
that for an adult reader would be too obvious
to need mention. In fact, these sentences al-
most seem like training sentences ? the kind of
sentences from which the associations between
recorder and musical instrument, play, blowing
can be learned. According to Hoey?s theory of
lexical priming (Hoey, 2005), one of the main
functions of schooling is to imbue children with
the societally sanctioned word associations.
To conclude the illustration, we observe that
there are some broad similarities between the dif-
ferent copora in terms of the distribution of pairs
of word types. Thus, texts seem to be mainly made
of pairs of weakly associated words ? about half
the pairs of word types lie between PMI of 0.5
and 1.5, in all the examined collections (52% for
essays, 44% for each of WSJ and young reader
corpora). The percentages of pairs at the low and
the high ends of PMI differ with genre ? writing
for children favors the higher end, while typical
Wall Street Journal writing favors the low end,
relatively to a corpus of essays on general topics
written by college graduates.
These observations are necessarily very tenta-
tive, as only a few corpora were examined. Still,
1150
681012
e?of?pairs?of?word?types
es
sa
ys
?av
es
sa
ys
?1
5
es
sa
ys
?8
5
ws
j?a
v
N(
0.9
0,0
.66
)
Gr
ad
es
?3?
4
024
?5
?4
?3
?2
?1
0
1
2
3
4
5
Percentag
PM
I
Figure 1: WAP histograms for three corpora, shown with smooth lines instead of bars for readability.
Average for essays (a thick solid blue line), average for WSJ articles (a dashed orange line); average for
Grades 3-4 corpus (a thin solid purple line with round markers). Normal distribution is shown with a thin
solid green line with asterisk markers. Middle 70% of essays fall between the dotted lines.
we believe the illustration is suggestive, in that
there is both constancy in writing for a similar pur-
pose (observe the limited variation around the ave-
rage that captures 70% of the essays) and variation
with genre and target audience. In what follows,
we will explore more thoroughly the information
provided by word association profiles regarding
the quality of writing.
4 Application to Essay Scoring
Texts written for a test and scored by relevant pro-
fessionals is a setting where variation in text qua-
lity is expected. In this section, we report our ex-
periments with using WAPs to explore the varia-
tion in quality as quantified by essay scores. We
first describe the data (section 4.1), then show the
patterns of relationships between essay scores and
word association profiles (section 4.2). Finally,
we report on an experiment where we significantly
improve the performance of a very competitive,
state-of-art system for automated scoring of es-
says, using a feature derived from WAP.
4.1 Data
We consider two collections of essays written as
responses in an analytical writing section of a
high-stakes standardized test for graduate school
admission; the time limit for essay composition
was 45 minutes. Essays were written in response
to a prompt (essay question). A prompt is usually a
general statement, and the test-taker is asked to de-
velop an argument supporting or refuting the state-
ment. Example prompts are: ?High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication? and ?In the age of
television, reading books is not as important as it
once was. People can learn as much by watching
television as they can by reading books.?
The first collection (henceforth, setA) contains
8,899 essays written in response to nine different
prompts, about 1,000 per prompt;6 the per-prompt
subsets will be termed setA-p1 through setA-p9.
Each essay in setA was scored by 1 to 4 human
raters on a scale of 1 to 6; the majority of essays re-
ceived 2 human scores. We use the average of the
available human scores as the gold-standard score
for the essay. Most essays thereby receive an inte-
ger score,7 so the ranking of the essays is coarse.
From this set, p1-p6 were used for feature selec-
tion, data visualization, and estimation of the re-
gression models (training), while sets p7-p9 were
reserved for a blind test.
The second collection (henceforth, setB) con-
6While we sampled exactly 1,000 essays per prompt, we
removed empty responses, resulting in 975 to 1,000 essays
per sample.
7as the two raters agree most of the time
1151
tains 400 essays, with 200 essays written on each
of two prompts given as examples above (setB-p1
and setB-p2). In an experimental study by Attali
et al (2013), each essay was scored by 16 profes-
sional raters on a scale of 1 to 6, allowing plus and
minus scores as well, quantified as 0.33 ? thus, a
score of 4- is rendered as 3.67. This fine-grained
scale resulted in higher mean pairwise inter-rater
correlations than the traditional integer-only scale
(r=0.79 vs around r=0.70 for the operational sco-
ring). We use the average of 16 raters as the final
grade for each essay. This dataset provides a very
fine-grained ranking of the essays, with almost no
two essays getting exactly the same score.
Rounded setA p1-p9 setB
Score av min max p1 p2
1 .01 .00 .01 ? ?
2 .05 .04 .06 .03 .03
3 .25 .20 .29 .30 .28
4 .44 .42 .47 .54 .55
5 .21 .16 .24 .13 .14
6 .04 .02 .07 .01 .02
Table 1: Score distribution in the essay data. For
the sake of presentation in this table, all scores
were rounded to integer scores, so a score of 3.33
was counted as 3, and a score of 3.5 was counted
as 4. A cell with the value of .13 (row titled 5
and column titled SetB p1) means that 13% of
the essays in setB-p1 received scores that round
to 5. For setA, average, minimum, and maximum
values across the nine prompts are shown.
Table 1 shows the distribution of rounded scores
in both collections. Average essay scores are be-
tween 3.74 to 3.98 across the different prompts
from both collections. The use of 16 raters seems
to have moved the rounded scores towards the
middle; however, the relative ranking of the essays
is much more delicate in setB than in setA.
4.2 Essay Score vs WAP
We calculated correlations between essay score
and the proportion of word pairs in each of the 60
bins of the WAP histogram, separately for each of
the prompts p1-p6 in setA. For a sample of 1,000
instances, a correlation of r=0.065 is significant at
p = 0.05. Figure 2 plots the correlations.
First, we observe that, perhaps contrary to ex-
pectation, the proportion of the highest values of
PMI (the area to the right of PMI=4 in Figure 2)
does not yield a consistent correlation with essay
scores. Thus, inasmuch as highest PMI values
tend to capture multi-word expressions (South and
Africa; Merill and Lynch), morphological vari-
ants (bids and bidding), or synonyms (mergers
and takeovers), their proportion in word type pairs
does not seem to give a clear signal regarding the
quality of writing.8
In contrast, the area of moderately high PMI
values (from PMI=2.5 to PMI=3.67 in Figure 2)
produces a very consistent picture, with only two
points out of 48 in that interval9 lacking signif-
icant positive correlation with essay score (p2 at
PMI=3.17 and p5 at PMI=3).
Next, observe the consistent negative correla-
tions between essay score and the proportion of
word pairs in bins PMI=0.833 through PMI=1.5.
Here again, out of the 30 data points correspond-
ing to these values, only 3 failed to reach statistical
significance, although the trend there is still nega-
tive.
Finally, there is a trend towards a positive cor-
relation between essay scores and the proportion
of mildly negative PMI values (-2<PMI<0), that
is, better essays tend to use more pairs of dis-
associated words, although this trend is not as
clear-cut as the one on the right-hand side of the
distribution.
Assuming that a higher proportion of high PMI
pairs corresponds to more topic development and
that a higher proportion of negative PMIs corre-
ponds to more creative use of language (in that
pairs are chosen that do not generally tend to ap-
pear together), it seems that the better essays are
both more topical and more creative than the lower
scoring ones. In what follows, we check whether
the information about essay quality provided by
WAP can be used to improve essay scoring.
8It is also possible that some of the instances with very
high PMI are pairs that contain low frequency words for
which the database predicts a spuriously high PMI based on a
single (and a-typical) co-occurrence that happens to repeat in
an essay ? similar to the Schwartz eschews example in (Man-
ning and Schu?tze, 1999, Table 5.16, p. 181). On the one
hand, we do not expect such pairs to occur in any systematic
pattern, so they could obscure an otherwise more systematic
pattern in the high PMI bins. On the other hand, we do not
expect to see many such pairs, simply because a repetition
of an a-typical event is likely to be very rare. We thank an
anonymous reviewer for suggesting this direction, and leave
a more detailed examination of the pairs in the highest-PMI
bins to future work.
9There are 8 bins of width of 0.167 in the given interval,
with 6 datapoints per bin.
1152
?0
.100.10.20.3
?5
?4
?3
?2
?1
0
1
2
3
4
5
relation?with?Essay?Score
p1 p2 p3 p4 p5 p6
?0
.4
?0
.3
?0
.2
Pearson?Cor
PM
I
Figure 2: Correlations with essay score for various bins of the WAP histogram. P1 to P6 correspond to
the first 6 prompts in SetA.
4.3 Baseline
As a baseline, we use e-rater (Attali and Burstein,
2006), a state-of-art essay scoring system deve-
loped at Educational Testing Service.10 E-rater
computes more than 100 micro-features, which are
aggregated into macro-features aligned with spe-
cific aspects of the writing construct. The system
incorporates macro-features measuring grammar,
usage, mechanics, style, organization and develop-
ment, lexical complexity, and vocabulary usage.
Table 2 gives examples of micro-features covered
by the different macro-features.
E-rater models are built using linear regression
on large samples of test-taker essays. We use a
generic e-rater model built at Educational Testing
Service using essays across a variety of writing
prompts, with no connection to the current project
and its authors. This model obtains Pearson corre-
lations of r=0.8324-0.8721 with the human scores
on setA, and the staggering r=0.9191 and r=0.9146
with the human scores on setB-p1 and setB-p2,
respectively. This is a very competitive baseline,
as e-rater features explain more than 70% of the
variation in essay scores on a relatively coarse
scale (setA) and more than 80% of the variation
in scores on a fine-grained scale (setB).
10http://www.ets.org/erater/about/
Macro- Example Micro-Features
Feature
Grammar, agreement errors
Usage, and verb formation errors
Mechanics missing punctuation
Style passive
very long or short sentences
excessive repetition
Organization use of discourse elements:
and thesis, support, conclusion
Development
Lexical average word frequency
Complexity average word length
Vocabulary similarity to vocabulary in
high- vs low-scoring essays
Table 2: Features used in e-rater (Attali and
Burstein, 2006).
4.4 Adding WAP
We define HAT ? high associative tight-
ness ? as the percentage of word type pairs
with 2.33<PMI?3.67 (bins PMI=2.5 through
PMI=3.67). This range correponds to the longest
sequence of adjacent bins in the PMI>0 area that
had a positive correlation with essay score in the
setA-p1 set. The HAT feature attains significant
1153
(at p = 0.05) correlations with essay scores,
r=0.11 to r=0.27 for the prompts in setA, and
r=0.22 and r=0.21 for the two prompts in setB. We
note that the HAT feature is not correlated with es-
say length. Essay length is not used as a feature in
e-rater models, but it typically correlates strongly
with the human essay score (at about r=0.70 in our
data), as well as with the score provided by e-rater
(at about r=0.80).
We also explored a feature that captured the
area with the negative correlations identified in
section 4.2. This feature did not succeed in im-
proving the performance over the baseline on setA
p1-p6; we tentatively conclude that information
contained in that feature, i.e. the proprotion of
mildly associated vocabulary in an essay, is indi-
rectly captured by another feature or group of fea-
tures already present in e-rater. Likewise, a feature
that calculates the average PMI for all pairs of con-
tent word types in the text failed to produce an im-
provement over the baseline for setA p1-p6. The
reason for this can be observed in Figure 2: The
higher-scoring essays having more of both the low
and the high PMI pairs leads to about the same
average PMI as for the lower-scoring essays that
have a higher concentration of values closer to the
average PMI.
4.5 Evaluation
To evaluate the usefulness of WAP in improving
automated scoring of essays, we estimate a lin-
ear regression model using the human score as a
dependent variable (label) and e-rater score and
the HAT as the two independent variables (fea-
tures). The correlations between the two inde-
pendent variables (e-rater and HAT) are between
r=0.11 and r=0.24 on the prompts in setA and
setB.
We estimate a regression model on each of
setA-pi, i ? {1, .., 6}, and evaluate them on each
of setA-pj, j ? {7, .., 9}, and compare the perfor-
mance with that of e-rater alone on setA-pj. Note
that e-rater itself is not trained on any of the data
in setA and setB; we use the same e-rater model
for all evaluations, a generic model that was pre-
trained on a large number of essays across diffe-
rent prompts. For setB, we estimate the regression
model on setB-p1 and test on setB-p2, and vice
versa.
Table 3 shows the evaluation results. The HAT
feature leads to a statistically significant improve-
Train Test E-rater E-rater+HAT t
on Test on Test
setA
p1 p7 0.84043 0.84021 -0.371
p2 p7 0.84043 0.84045 0.408
p3 p7 0.84043 0.83999 -0.597
p4 p7 0.84043 0.84044 0.411
p5 p7 0.84043 0.84028 -0.280
p6 p7 0.84043 0.83926 -1.080
p1 p8 0.83244 0.83316 1.688
p2 p8 0.83244 0.83250 2.234
p3 p8 0.83244 0.83327 1.530
p4 p8 0.83244 0.83250 2.237
p5 p8 0.83244 0.83311 1.752
p6 p8 0.83244 0.83339 1.191
p1 p9 0.86370 0.86612 4.282
p2 p9 0.86370 0.86389 5.205
p3 p9 0.86370 0.86659 4.016
p4 p9 0.86370 0.86388 5.209
p5 p9 0.86370 0.86591 4.390
p6 p9 0.86370 0.86730 3.448
setB
p1 p2 0.9146 0.9178 0.983
p2 p1 0.9191 0.9242 2.690
Table 3: Performance of baseline model (e-rater)
and models where e-rater was augmented with
HAT, a feature based on the word association
profile. Performance is measured using Pearson
correlation with essay score. We use Wilcoxon
Signed-Ranked test for matched pairs, and report
the sum of signed ranks (W), the number of ranks
(n), and the p value. E-rater+HAT is significantly
better than e-rater alone, W=138, n=20, p<0.05.
We also measure significance of the improvement
for each row individually, using McNemar?s test
for significance of difference in same-sample cor-
relations (McNemar, 1955, p.148); we report the
t value for each test. For values of t > 1.645,
we can reject the hypothesis that e-rater+HAT is
not better than e-rater alone with 95% confidence.
Significant improvements are underlined.
1154
ment in the performance of automated scoring.
An improvement is observed for 14 out of the 18
evaluations for setA, as well as for both evalua-
tions for setB.11 Moreover, the largest relative im-
provement of 0.55%, from 0.9191 to 0.9242, was
observed for the setting with the highest baseline
performance, suggesting that the HAT feature is
still effective even after the delicate ranking of
the essays revealed an exceptionally strong perfor-
mance of e-rater.
5 Related Work
Most of the attention in the computational linguis-
tics research that deals with analysis of the lexis
of texts has so far been paid to what in our terms
would be the very high end of the word associa-
tion profile. Thus, following Halliday and Hasan
(1976), Hoey (1991), and Morris and Hirst (1991),
the notion of lexical cohesion has been used to
capture repetitions of words and occurrence of
words with related meanings in a text. Lexically
cohesive words are traced through the text, for-
ming lexical chains or graphs, and these repre-
sentations are used in a variety of applications,
such as segmentation, keyword extraction, sum-
marization, sentiment analysis, temporal indexing,
hypelink generation, error correction (Guinaudeau
et al, 2012; Marathe and Hirst, 2010; Ercan and
Cicekli, 2007; Devitt and Ahmad, 2007; Hirst
and Budanitsky, 2005; Inkpen and De?silets, 2005;
Gurevych and Strube, 2004; Stokes et al, 2004;
Silber and McCoy, 2002; Green, 1998; Al-Halimi
and Kazman, 1998; Barzilay and Elhadad, 1997).
To our knowledge, lexical cohesion has not so far
been used for automated scoring of essays. Our
results suggest that this direction is promising, as
merely the proportion of highly associated word
pairs is already contributing a clear signal regar-
ding essay quality; it is possible that additional
information can be derived from richer represen-
tations common in the lexical cohesion literature.
Aspects related to the distribution of words in
essays have been studied in relation to essay sco-
ring. One line of work focuses on assessing co-
herence of essays. Foltz et al (1998) use Latent
11We also performed a cross-validation test on setA p1-
p6, where we estimated a regression model on setA-pi and
evaluate it on setA-pj, for all i, j ? {1, .., 6}, i 6= j, and
compared the performance with that of e-rater alone on setA-
pj, yielding 30 different train-test combinations. The results
were similar to those of the blind test presented here, with e-
rater+HAT significantly improving upon e-rater alone, using
Wilcoxon test, W=374, n=29, p<0.05.
Semantic Analysis to model the smoothness of
transitions between adjacent segments of an essay.
Higgins et al (2004) compare sentences from cer-
tain discourse segments in an essay to determine
their semantic similarity, such as comparing the-
sis statements to conclusions or thesis statements
to essay prompts. Additional approaches include
evaluation of coherence based on repeated refe-
rence to entities (Burstein et al, 2010; Barzilay
and Lapata, 2008; Miltsakaki and Kukich, 2004).
Our approach is different in that it does not mea-
sure the flow of the text, that is, the sequencing
and repetition of the words, but rather assesses the
choice of vocabulary as a whole.
Topic models have been proposed as a tech-
nique for capturing clusters of related words that
tend to occur in the same documents in a given
collection. A text is modeled as being composed
of a small number of topics, and words in the text
are generated conditioned on the selected topics
(Gruber et al, 2007; Blei et al, 2003). Since
(a) topics encapsulate clusters of highly associated
words, and (b) topics for a given text are modeled
as being chosen independently from each other,
we expect a negative correlation between the num-
ber of topics in a document and the tightness of the
word association profile of the text.
An alternative representation of word associ-
ation profile would be a weighted graph, where
the weights correspond to pairwise associations
between words. Thus, for longer texts, graph
analysis techniques would be applicable. Steyvers
and Tenenbaum (2005) analyze the graphs in-
duced from large repositories like WordNet or
databases of free associations, and find them to be
scale-free and small-world; it is an open question
whether word association graphs induced from
book-length texts would exhibit similar properties.
In the theoretical tradition, our work is closest in
spirit to Michael Hoey?s theory of lexical priming
(Hoey, 2005), positing that users of language inter-
nalize patterns of occurrence and non-occurrence
of words not only with other words, but also in cer-
tain positions in a text, in certain syntactic environ-
ments, and in certain evaluative contexts, and use
these when creating their own texts. We believe
that word association profiles reflect the artwork
that goes into using those internalized associations
between words when creating a text, achieving the
right mix of strong and weak, positive and nega-
tive associations.
1155
6 Conclusion
In this paper, we described a new representation
of the content vocabulary of a text we call word
association profile that captures the proportions
of highly associated, mildly associated, unassoci-
ated, and dis-associated pairs of words selected to
co-exist in the given text by its author. We ob-
served that the shape of the distribution is quite
stable across various texts, with about half the
pairs having a mild association; the allocation of
pairs to the higher and the lower levels of associa-
tion does vary across genres and target audiences.
We further presented a study of the relationship
between quality of writing and word association
profiles. For a dataset of essays written by college
graduates on a number of general topics in a stan-
dardized test for graduate school admission and
scored by professional raters, we showed that the
higher scoring essays tend to have higher percen-
tages of both highly associated and dis-associated
pairs, and lower percentagese of mildly associated
pairs of words. We hypothesize that this pattern
is consistent with the better essays demonstrating
both a better topic development (hence the higher
percentage of highly related pairs) and a more cre-
ative use of language resources, as manifested in a
higher percentage of word pairs that generally do
not tend to appear together.
Finally, we demonstrated that the information
provided by word association profiles leads to a
significant improvement in a highly competitive,
state-of-art essay scoring system that already mea-
sures various aspects of writing quality.
In future work, we intend to investigate in more
detail the contribution of various kinds of words to
word association profiles, as well as pursue appli-
cation to evaluation of text complexity.
References
Reem Al-Halimi and Rick Kazman. 1998. Temporal
indexing through lexical chaining. In C. Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 333?351. Cambridge, MA: MIT Press.
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali, Will Lewis, and Michael Steier. 2013.
Scoring with the computer: Alternative procedures
for improving reliability of holistic essay scoring.
Language Testing, 30(1):125?141.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of ACL Intelligent Scalable Text Summarization
Workshop.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Roberto Basili and Marco Pennacchiotti. 2010. Dis-
tributional lexical semantics: Toward uniform rep-
resentation paradigms for advanced acquisition and
processing tasks. Natural Language Engineering,
16(4):347?358.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 681?684, Los Angeles, California,
June. Association for Computational Linguistics.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984?991, Prague, Czech Republic,
June. Association for Computational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?08, pages 334?
343, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical
chains for keyword extraction. Information Process-
ing & Management, 43(6):1705?1714.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
1156
Stefan Evert. 2008. Corpora and collocations. In
A. Lu?deling and M. Kyto?, editors, Corpus Linguis-
tics: An International Handbook. Berlin: Mouton de
Gruyter.
Michael Flor. 2013. A fast and flexible architecture for
very large word n-gram datasets. Natural Language
Engineering, 19(1):61?93.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2):285?307.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21(4):353?367.
David Graff and Christopher Cieri. 2003. English Gi-
gaword LDC2003T05. Linguistic Data Consortium,
Philadelphia.
Stephen Green. 1998. Automated link generation: Can
we do better than term repetition? Computer Net-
works, 30:75?84.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. Journal of
Machine Learning Research - Proceedings Track,
2:163?170.
Camille Guinaudeau, Guillaume Gravier, and Pascale
Se?billot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and
language model interpolation for multimedia spoken
content topic segmentation. Computer Speech and
Language, 26(2):90?104.
Iryna Gurevych and Michael Strube. 2004. Seman-
tic similarity applied to spoken dialogue summariza-
tion. In Proceedings of Coling 2004, pages 764?
770, Geneva, Switzerland, August. COLING.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of
coherence in student essays. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 185?192, Boston,
Massachusetts, USA, May. Association for Compu-
tational Linguistics.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexi-
cal cohesion. Natural Language Engineering,
11(1):87?111.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Michael Hoey. 2005. Lexical Priming. Routledge.
Diana Inkpen and Alain De?silets. 2005. Semantic
similarity for detecting recognition errors in auto-
matic speech transcripts. In Proceedings of Empir-
ical Methods in Natural Language Processing Con-
ference, pages 49?56, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Michael Jones and Douglas Mewhort. 2007. Repre-
senting word meaning and order information in a
composite holographic lexicon. Psychological Re-
view, 114(1):1?37.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of ACL, pages
768?774, Montreal, Canada.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments & Computers, 28:203?208.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Meghana Marathe and Graeme Hirst. 2010. Lexical
Chains Using Distributional Measures of Concept
Distance. In Proceedings of 11th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLING), pages 291?302, Iasi,
Romania, March.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Hemant Misra, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proceedings of
the 18th ACM conference on Information and know-
ledge management, CIKM ?09, pages 1553?1556,
New York, NY, USA. ACM.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, pages 236?244, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational linguistics, 17(1):21?48.
1157
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language Resources and
Evaluation, 44:137?158.
David Reshef, Yakir Reshef, Hilary Finucane, Sharon
Grossman, Gilean McVean, Peter Turnbaugh, Eric
Lander, Michael Mitzenmacher, and Pardis Sabeti.
2011. Detecting novel associations in large data
sets. Science, 334(6062):1518?1524.
Martin Riedl and Chris Biemann. 2012. How text seg-
mentation algorithms gain from topic models. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 553?557, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Kathy Sheehan, Irene Kostin, and Yoko Futagi. 2008.
When do standard approaches for measuring vo-
cabulary difficulty, syntactic complexity and refer-
ential cohesion yield biased estimates of text diffi-
culty? In Proceedings of the Cognitive Science So-
ciety, pages 1978?1983, Washington, DC, July.
Gregory Silber and Kathleen McCoy. 2002. Efficiently
computed lexical chains as an intermediate represen-
tation for automatic text summarization. Computa-
tional Linguistics, 28(4):487?496.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
Large-Scale Structure of Semantic Networks: Sta-
tistical Analyses and a Model of Semantic Growth.
Cognitive Science, 29:41?78.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
Select: A lexical cohesion based news story seg-
mentation system. Journal of AI Communications,
17(1):3?12.
Peter Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Articial Intelligence Research,
37:141?188.
Peter D. Turney. 2001. Mining the Web for Syno-
nyms: PMI-IR versus LSA on TOEFL. In European
Conference on Machine Learning, pages 491?502,
Freiburg, Germany, September.
Ziqi Zhang, Anna Gentile, and Fabio Ciravegna. 2012.
Recent advances in methods of lexical semantic re-
latedness ? a survey. Natural Language Engineer-
ing, FirstView:1?69.
1158
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 105?115,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
On using context for automatic correction  
of non-word misspellings in student essays 
 
Michael Flor Yoko Futagi 
Educational Testing Service Educational Testing Service 
Rosedale Road Rosedale Road 
 Princeton, NJ, 08541, USA  Princeton, NJ, 08541, USA 
mflor@ets.org yfutagi@ets.org 
 
  
 
 
Abstract 
In this paper we present a new spell-checking 
system that utilizes contextual information for 
automatic correction of non-word misspel-
lings. The system is evaluated with a large 
corpus of essays written by native and non-
native speakers of English to the writing 
prompts of high-stakes standardized tests 
(TOEFL? and GRE?). We also present com-
parative evaluations with Aspell and the spel-
ler from Microsoft Office 2007. Using 
context-informed re-ranking of candidate sug-
gestions, our system exhibits superior error-
correction results overall and also corrects er-
rors generated by non-native English writers 
with almost same rate of success as it does for 
writers who are native English speakers. 
1 Introduction 
Misspellings are ubiquitous in student writing. 
Connors and Lunsford (1988) have found that spel-
ling errors accounted for about one quarter of all 
errors found in a random sample of 300 student 
essays. Desmet and Balthazor (2006) found that 
spelling errors are among the five most frequent 
errors in first-year college composition of US stu-
dents. Lunsford and Lunsford (2008) found that 
spelling errors constituted about 6.5% of all errors 
found in a US national sample of 3000 college 
composition essays, despite the fact that writers 
had access to spellcheckers. 
Misspellings are even more ubiquitous in texts 
written by non-native speakers of English, espe-
cially English Language Learners (ELL). The 
types of misspellings produced by L2 writers are 
typically different from errors produced by native 
speakers (Hovermale, 2010; Al-Jarf, 2010; Okada, 
2005). 
In the area of automatic assessment of writing, 
detection of misspellings is utilized in computer-
aided language learning applications and in some 
automatic scoring systems, especially when feed-
back to users is involved (Dikli, 2006; Warschauer 
and Ware, 2006). Yet spelling errors may have a 
deeper influence on automated text assessment. As 
noted by Nagata, et al (2011), sub-optimal auto-
matic detection of grammar and mechanics errors 
may be attributed to poor performance of NLP 
tools over noisy text. 
Presence of spelling errors also hinders systems 
that require only lexical analysis of text (Landauer, 
et al , 2003; P?rez, et al, 2004). Granger and 
Wynne (1999) have shown that spelling errors can 
affect automated estimates of lexical variation, 
which in turn are used as predictors of text quality 
(Crossley, et al, 2008; Yu, 2010). In the context of 
automated preposition and determiner error correc-
tion in L2 English, De Felice and Pulman (2008) 
noted that the process is often disrupted by miss-
pellings. Futagi (2010) described how misspellings 
pose problems in development of a tool for detec-
tion of phraseological collocation errors. 
Given this state of affairs, it is only natural for 
automatic text assessment systems to utilize auto-
matic spellchecking components. However, gener-
ic spellcheckers are typically oriented for errors 
produced by writers who are native speakers of a 
language. Rimrott and Heift (2008, 2005) have 
demonstrated that a generic speller has poor per-
formance on data from German language learners. 
105
Bestgen and Granger (2011) and Hovermale 
(2010) have demonstrated similar results on data 
from ELL. 
Many researchers have suggested that spell-
checkers for L2 users need to be adapted for the 
particular patterns of errors that characterize each 
native language (L1), by studying patterns of inter-
ference and influence from L1 to L2 (Mitton and 
Okada, 2007; Mitton, 1996; Rimrott and Heift, 
2008, 2005; Bestgen and Granger, 2011; Hover-
male, 2010). We have set up to explore a different 
path, in the context of automated text assessment. 
Our goal in the present study is to examine to what 
extent detection and automatic correction of non-
word misspellings can be improved by utilizing 
essay context, for data from both native and non-
native English speakers. 
The rest of this paper is organized as follows. 
Section 2 provides a description of the corpus of 
texts and misspellings that was used in this study. 
Section 3 describes the ConSpel automatic spell-
checking system. Section 4 presents results from a 
comparative evaluation of our system, ConSpel, 
the popular Aspell speller and the Microsoft Office 
2007 speller. Section 5 compares our findings with 
some recent studies and discusses implications for 
further development of automatic spell-checking 
systems. 
2 Corpus 
The corpus used in this study is a collection of es-
says, annotated for misspellings by trained annota-
tors. It is developed for evaluation of automatic 
spellcheckers, and for research on patterns of 
misspellings produced by both native English 
speakers and ELL. 
2.1 Texts 
The corpus comprises essays written by exami-
nees on the writing sections of GRE? (Graduate 
Record Examinations) and TOEFL? (Test of Eng-
lish as a Foreign Language) (ETS, 2011a,b). The 
TOEFL test includes two different writing tasks: a 
short opinion essay, on a pre-assigned topic, and a 
summary essay that compares arguments from two 
different sources (both supplied during the test). 
GRE also includes two different writing tasks: one 
is a short argumentative essay taking a position on 
an assigned topic, the other is an essay evaluating 
the soundness of arguments presented in prompt. 
Both tests are delivered on computer (at test cen-
ters around the world and via Internet), always us-
ing the standard English language computer 
keyboard (QWERTY). Editing tools such as a 
spellchecker are not provided in the test-delivery 
software (ETS, 2011a). All writing tasks have time 
constraints. 
In the current phase of the project, the corpus 
includes 3000 essays, for a total of 963,428 words. 
The essays were selected equally from the two 
tests (4 tasks, 10 prompts per task, 75 essays per 
prompt), also covering full range of scores (as a 
proxy for English proficiency levels) for each task. 
The majority of essays in this collection were writ-
ten by examinees for whom English is not the first 
language (98.73% of TOEFL essays, 57.86% of 
GRE essays). 
2.2 Annotation 
Each text was independently reviewed by two 
annotators, who are native English speakers expe-
rienced in linguistic annotation. Annotators were 
asked to identify all non-word misspellings and 
provide the adequate correction for each one. Inter-
annotator agreement was quite high - annotators 
agreed in 82.6% of the cases (Cohen?s Kappa=0.8, 
p<.001). All disagreements were resolved by a 
third annotator (adjudicator). For details of the an-
notation procedure, see Flor and Futagi (2011).  
The Annotation Scheme for this project provides 
three classes of misspellings, as summarized in 
Table 1. Classification of annotated misspellings 
was automatic. 
 
Type Description Count in corpus 
1 single token non-word  
(e.g. ?businees?, ?inthe?) 
21,160
2 single token non-word for which no 
plausible correction was found 
52
3 multi-token non-word misspelling  
(e.g. ?mor efun? for ?more fun?) 
383
 Total 21,595
 
Table 1. Classification of misspellings  
annotated in the study corpus. 
 
106
The annotation effort focused specifically on 
misspellings, rather than on a wider category of 
orthographic errors in general. The annotation ig-
nored repeated words, missing spaces1 and impro-
per capitalization. Many of the essays have 
inconsistent capitalization and essays written fully 
in capital letters are not uncommon (not only in 
our corpus). In addition, different spelling variants 
were acceptable. This consideration stems from the 
international nature of the two tests ? the exami-
nees come from all around the world, being accus-
tomed to either British, American, or some other 
English spelling standard; so, it is only fair to ac-
cept all of them. 
Overall, the annotated corpus of 3,000 essays 
has the following statistics. Average essay length is 
321 words (the range is 28-798 words). 148 essays 
turned out to have no misspellings at all. Total 
spelling error counts are given in Table 1; 2.24% 
of the words in the corpus are non-word misspel-
lings. 
3 Spelling correction systems 
3.1 Background 
Classic approaches to the problem of spelling cor-
rection of non-word errors were reviewed by Ku-
kich (1992). The typical approach for error 
detection is using good spelling dictionaries. The 
typical approach for correction of non-word errors 
is to include modules for computing edit distance 
(Damerau, 1964; Levenshtein, 1966) and phonetic 
similarity. These are used for ranking suggestions 
by their orthographic and phonetic similarity to the 
misspelled word. A more recent feature utilizes 
word frequency data for candidate ranking. Mitton 
(2009) and Deorowicz and Ciura (2005) describe 
state of the art approaches to non-word correction 
without contextual information.  
The use of context for spelling correction was 
initially proposed by Mayes, et al (1991) only for 
?contextual spelling? ? correcting real-word errors 
(e.g. writing ?fig? instead of ?fog?). A common 
strategy for this task is using pre-defined confusion 
sets, which makes it more amenable to classifier-
based approaches (Golding and Roth, 1999). Sev-
                                                          
1 Annotation ignored missing spaces around punctuation (e.g. 
?chairs,tables?, but all cases where missing spaces result in 
fused words were marked in annotation (e.g. ?inthe?). 
eral recent studies used a web-scale language mod-
el (Google Web1T n-gram corpus ? Brants and 
Franz, 2006) for ?context-sensitive? (i.e. real-
words) spelling correction (Bergsma, et al, 2009; 
Islam and Inkpen, 2009; Carlson and Fette, 2007). 
Chen, et al (2007) used a LM for pruning candi-
date corrections for non-words in web queries. 
Whitelaw, et al (2009) used a LM for correcting 
non-word and real-word errors without a dictionary 
and using a statistically trained error model. Our 
study extends the use of language models to auto-
matic correction of non-word errors, with a dictio-
nary, but without any explicit error model. 
3.2 ConSpel system 
The ConSpel system was designed and imple-
mented as a fully automatic system for detection 
and correction of spelling errors. The current ver-
sion is focused on non-word misspellings. The sys-
tem has two intended uses. One is to serve as a 
component in NLP systems for automatic evalua-
tion of student essays. The other use is to facilitate 
automation for research on patterns of misspellings 
in ELL essays. 
In ConSpel, detection policy is quite simple. A 
token in a text is potentially a misspelling if the 
string is not in the system dictionaries. A text may 
include some non-dictionary tokens that systemati-
cally are not misspellings. ConSpel has several 
parameterized options to handle such cases. By 
default, the system will ignore numbers, dates, web 
and email addresses, and mixed alpha-numeric 
strings (e.g. ?RV400?). The system can be in-
structed to ignore capitalized words (e.g. ?Lon-
don?) and/or words in all uppercase (e.g. ?ROME?). 
ConSpel spelling dictionaries include about 
360,000 entries. The core set includes 245,000 en-
tries, providing a comprehensive coverage of mod-
ern English vocabulary. This lexicon includes all 
inflectional variants for a given word (e.g. ?love?, 
?loved?, ?loves?, ?loving?), and international spel-
ling variants (e.g. American and British English). 
Additional dictionaries include about 120,000 en-
tries for international surnames and first names, 
and names for geographical places. 
Dictionaries are also the source of suggested 
corrections. Candidate suggestions for each de-
tected misspelling are generated by returning all 
dictionary words that have an edit distance up to a 
given threshold. With the default threshold of 5, a 
107
misspelling can easily get hundreds of correction 
candidates. Since ConSpel is intended to work on 
ELL data, and ELL misspellings can be quite dis-
similar from the intended words, starting with a 
large number of candidates is a deliberate strategy 
to ensure that the adequate correction will be in-
cluded in the candidate set. Candidates are pruned 
during the re-ranking process, so that only a few 
candidates from the initial set survive to the final 
decision making stage. 
Candidate suggestions for each detected miss-
pelling are ranked using a set of algorithms. An 
edit distance module is used to compute ortho-
graphic similarity between each candidate and the 
original misspelling. Phonetic similarity is com-
puted using the Double Metaphone algorithm (Phi-
lips, 2000). Word frequency is computed for each 
candidate using a very large word-frequency data 
source. 
The main thrust of our new spelling correction 
system is the conjecture that non-word misspel-
lings can be corrected better when their context is 
taken into account.  
Local context (several words around the miss-
pelled word in the text) provides lots of informa-
tion for choosing the adequate correction. For each 
candidate, we check the frequency of its co-
occurrence (in a language model) with the adjacent 
words in the text. This approach borrows from the 
family of noisy-channel error-correction models 
(Zhang, et al, 2006; Cucerzan and Brill, 2004; 
Kernigham, et al, 1990). With the advent of very 
large word n-gram language models, we can utilize 
large contexts (about 4 words on each side of a 
misspelling). Our current language model uses a 
filtered version of the Google Web1T collection, 
containing 1,881,244,352 n-gram types of size 1-5, 
with punctuation included.2 Notably, ConSpel does 
not use any statistical error model. 
A second context-sensitive algorithm utilizes 
non-local context in the essay. The idea is quite 
simple ? given a misspelled token in a text and a 
set of correction-candidates for that word, for each 
candidate we check whether that candidate string 
occurs elsewhere in the text. Since content words 
have some tendency of recurrence in same text, the 
                                                          
2 ConSpel system uses the TrendStream n-gram compression 
software library (Flor, 2012) for fast and memory efficient 
retrieval of n-gram data. As a result, the ConSpel system runs 
even on modest hardware (e.g. a 4GB RAM laptop), concur-
rently with other applications. 
misspelled token might be such a case, and the 
candidate should be strengthened. The idea is 
somewhat similar to cache-based language model 
adaptation (Kuhn and De Mori, 1990), though 
there are considerable differences. First, our sys-
tem looks not only in preceding context, but over 
the whole essay text. Second, and unique to our 
system, ConSpel looks not only in the text, but also 
into the k-best candidate correction lists of the oth-
er misspelled words. Thus, if a word is systemati-
cally misspelled in a document, ConSpel will 
strengthen a candidate correction that appears as a 
candidate for multiple misspelled instances.3  
For each misspelling found in a text, each algo-
rithm produces ranking scores for each candidate. 
We use a linear-weighted ensemble method to 
combine scores from different algorithms. First, 
scores for all candidates of a given misspelling are 
normalized into a 0-1 range, separately for each 
ranker. Normalized scores are then summed using 
a set of constant weights.4  
The ConSpel system is implemented as a flexi-
ble configurable system. Configuration settings 
include choice of dictionaries, choice of algorithms 
and weights for computing the final ranking, and 
choice of the output formats. 
4 Comparative evaluation 
In this section we report the results of evaluation 
on data from our gold-standard corpus of 3,000 
essays described in section 2. This evaluation fo-
cuses on detection and correction of the 21,212 
single-token non-word misspellings (types 1 and 2 
in Table 1) as well as false alarms raised by spell-
checkers.  
Evaluation included three systems. In addition to 
ConSpel, we tested Aspell (version 0.60.6), a pop-
ular open-source spell checking library (Atkinson, 
2011). The third system is spellchecker included in 
Microsoft Office 2007 (hereafter ?MS Word?). 
All evaluations were performed ?in full context? 
(rather than word-by-word) ? each essay in the 
corpus was submitted to each system separately, as 
a simple text file. All evaluations used standard 
                                                          
3 A detailed comparative study of different context utilization 
methods is under way.  
4 The current weights were found experimentally, prior to the 
annotation effort described in this article. We intend to use 
machine learning methods in future research, using the anno-
tated corpus for this purpose. 
108
measures of recall, precision and F-score (Leacock, 
et al, 2010).  
Evaluations for Aspell and MS Word were con-
ducted twice ? once with their original dictiona-
ries5 and once with the ConSpel spelling dictionary 
of about 360,000 word forms. Evaluations where 
Aspell and MS Word were bundled with ConSpel 
dictionary are marked below as Aspell+ and MS 
Word+. 
4.1 Error Detection 
Detection results for non-word misspellings are 
presented in Table 2. All systems show very strong 
recall rates, above 99%. There is more variability 
when precision of error detection is concerned. 
Both MS Word and Aspell benefit from using the 
larger dictionary ? they raise much less false 
alarms than with original dictionaries (Aspell im-
proves precision by about 4% and MS Word by 
about 6%). ConSpel shows best precision, the dif-
ference with second-best (MS Word+) is statisti-
cally significant at p<.01. 
 
System Recall Precision F-score 
Aspell 99.45 86.66 92.62
Aspell+ 99.14 90.92 94.85
ConSpel 99.40 98.43 98.91
MS Word 99.55 90.26 94.68
MS Word+ 99.32 96.16 97.71
 
Table 2. Evaluation results: non-word error detection 
4.2 Error Correction 
For evaluating spelling correction, we again use 
the measures of recall, precision and F-score. Note 
that precision of error correction is defined as pro-
portion of adequately corrected misspellings out of 
total number of misspellings that a system tried to 
correct (this excludes cases missed in detection). 
We conducted error-correction evaluations with 
ConSpel in two variants. The baseline variant, 
ConSpel-A, ranks candidate suggestions using edit 
distance, phonetic similarity and word-frequency. 
                                                          
5 Notably, both Aspell and MS Word in this evaluation came 
with respective default dictionaries for US spelling, and gen-
erated many false alarms when flagging words that are British 
and other international spelling variants. Such false alarm 
cases were discounted from the evaluation statistics. 
The contextual variant, ConSpel-B, adds contex-
tual information in the ranking process. 
Results of error-correction evaluations are 
shown in Table 3. While MS Word speller pro-
vided the adequate correction (top ranked sugges-
tion) in about 73% of annotated cases, its precision 
is only about 67-69%, due to large number of false 
alarms. Aspell has markedly lower accuracy ? both 
in recall and precision. ConSpel-A has approx-
imately same recall as MS-Word, but better preci-
sion (due to low rate of false alarms). ConSpel-B, 
which uses contextual information in ranking can-
didate suggestions, shows markedly better recall 
and precision than either ConSpel-A or MS Word 
(statistically significant at p<.01). 
For Aspell, use of the larger spelling dictionary 
improved detection precision (fewer false alarms ? 
see Table 2), but it has led to degradation in error 
correction ? as shown in Table 3 (possibly ranking 
of candidates is affected by larger dictionaries). 
 
System Recall Precision F-score 
Aspell 61.53 53.62 57.30
Aspell+ 54.17 49.68 51.83
ConSpel-A 72.65 71.94 72.29
ConSpel-B 78.32 77.55 77.93
MS Word 73.34 66.49 69.74
MS Word+ 71.71 69.44 70.56
 
Table 3. Evaluation results: non-word error correction  
(top ranked candidates only) 
 
An additional way to evaluate automatic spel-
ling correction is to consider how often the ade-
quate target correction is found among the k-best 
of the candidate suggestions (Mitton, 2009; Brill 
and Moore, 2000). Figure 1 shows error-correction 
recall and precision results for four systems6 using 
k-best values 1-5 and 10.  
When two-or-more best-ranked candidates are 
considered for each misspelling, the baseline Con-
Spel-A system shows better performance than MS 
Word. Aspell results lag significantly below the 
other systems, although it catches up with MS-
Word beyond k=5. ConSpel-B system outperforms 
all other systems, in both recall and precision. It 
                                                          
6 ?MS Word? and ?MS Word+? overlap for all values of k, 
except for k=1, thus only ?MS Word? is shown in Figure 1. 
109
places the target correction among the top two 
candidates in 88% of cases, and among top three or 
more candidates in beyond 90% of cases. 
 
  
Figure 1. Error correction recall and precision  
for four systems, with different k-best cutoffs. 
 
4.3 Evaluation with data from native and 
non-native English speakers 
In this section we report the results of spell-check 
evaluation with data breakdown by native and non-
native English speakers. Out of 21,212 single-
token non-word misspellings in our corpus, 2,859 
came from 570 essays written by native English 
speakers (NS) and 18,353 misspellings came from 
2,282 essays written by test-takers who are not na-
tive speakers of English (NNS). 
Comparison of error-detection for five systems 
is presented in Table 4. All systems show very 
strong recall results for both types of populations 
(all values are above 99%). The results are a bit 
different for error-detection precision. ConSpel 
achieves best results in both populations (the dif-
ferences with second-best, MS Word+, are statisti-
cally significant at p<.01). MS Word has precision 
around 91%, approximately same in both popula-
tions. Compared to MS Word, MS Word+ has bet-
ter recall rates, in both populations ? due to a 
larger dictionary, it raises much less false alarms. 
Aspell lags behind in this comparison. Using a 
larger dictionary helps, as Aspell+ precision is bet-
ter than that of Aspell in both populations; im-
provement is manifest for NNS data and only 2% 
for NS data. Aspell detection precision on NS data 
(77%) is lower than its precision on NNS data 
(88%). This may be due to Aspell having a prob-
lem with possessive forms (80% of the false alarms 
on NS data are possessives, but only 70% for NNS 
data).7 
 
System  Recall Precision F-score 
Aspell ns:nns:
99.7 
99.4 
76.7
88.5
86.7
93.6
Aspell+ ns:nns:
99.6 
99.3 
78.7
93.3
87.9
96.3
ConSpel ns:nns:
99.5 
99.4 
96.2
98.8
97.9
99.1
MS Word ns:nns:
99.6 
99.6 
91.1
90.1
95.1
94.6
MS Word+ ns:nns:
99.2 
99.3 
94.4
96.5
96.7
97.9
 
Table 4. Evaluation results: percent correct for 
non-word error detection, with breakdown for data from 
native (ns) and non-native (nns) English speakers 
 
Results of error-correction recall, with k-best le-
vels 1-5 and 10, are presented in Figure 2. In com-
parisons of recall, with k=1, on NS data (right 
panel), MS Word (81.3%) and ConSpel-B (80.7%) 
show best results (the difference is not significant). 
For larger k-values, MS Word correction rate8 im-
proves to a ceiling of about 88.5% and both Con-
Spel variants have better improvement than MS 
Word. The context-informed ConSpel-B system 
has error-correction recall above 90% for k?2 and 
reaches 94.2% at k=5. 
On NNS data, ConSpel-B has a clear advantage 
over all other systems. At k=1, ConSpel-A and MS 
Word show equal correction performance (72%). 
For k ?2, ConSpel-A shows constant improvement, 
while MS Word improves to a ceiling of about 
85%. For both NS and NNS populations, Aspell 
error-correction performance lags considerably 
behind the other systems, although it catches up 
with and even outperforms MS Word for k?3. Inte-
restingly, Aspell+ performs consistently worse 
than Aspell; the larger dictionary has detrimental 
effect on error-correction for Aspell, but not for 
MS Word. 
                                                          
7 ConSpel dictionary does not contain possessive forms. 
8 Results for ?MS Word? and ?MS Word+? on this data overlap 
for all values of k, in both populations.  
110
 
 
Figure 2. Error-correction recall for five systems, data 
from native (ns) and non-native (nns) English speakers. 
 
Error-correction precision results are shown in 
Figure 3. Overall, ConSpel-B outperforms all other 
systems, for both NS and NNS populations. On NS 
data, for k=1, MS Word+ (77%), ConSpel-A 
(76%) and MS Word (75%) are very close. For 
k?2, ConSpel-A shows better improvement, reach-
ing 89.4% at k=5, while MS Word+ reaches a ceil-
ing of about 85% (81% for MS Word). Aspell 
performance lags clearly behind the other systems, 
although it also improves considerably with larger 
k-values. For NNS data, the separation between 
systems is even clearer. Aspell lags behind, al-
though it catches up to MS Word at k?5. 
Except for ConSpel-B, all systems have mani-
festly better error-correction precision on NS data 
than on NNS data ? misspellings made by non-
native English speakers are harder to correct. Con-
Spel-B, with context-informed ranking of spelling 
suggestions, performs almost equally well for both 
populations. For k=1, its error-correction precision 
is 77.5% for NNS data and 78% for NS data. For 
k=2, precision is 87.9% for NNS and 88.2% for NS 
data. These differences are not statistically signifi-
cant. For both populations, precision rises beyond 
90% for k?3. ConSpel-B also shows remarkably 
close error-correction recall in both populations: at 
k=1, recall is 77.9% for NNS and 80.7% for NS; at 
k=2, recall is 88.4% for NNS, 91.4% for NS (the 
differences are statistically significant). For k?3, 
recall is beyond 90% for both populations, with 
about 2% advantage for NS population. 
 
 
Figure 3. Error-correction precision for six systems, for 
native (ns) and non-native (nns) English speakers. 
 
Table 5 presents F-scores for error-correction 
evaluation results, for six systems, for k-best val-
ues 1-5 and 10, for NS and NNS data. For each 
value of k, the ConSpel-B system has best values 
for both NS and NNS data. For each cell in Table 
5, we calculated the absolute difference between 
the NS and NNS F-scores. The results are shown in 
Figure 4. Except for ConSpel-B, all systems have 
marked differences in performance on NS and 
NNS data. The differences tend to diminish for 
larger k-values. ConSpel-B is the only system for 
which the differences in error-correction between 
NS and NNS data are consistently below 2%, even 
for k=1. 
 
K-best: 1 2 3 4 5 10 
Aspell 60.654.9 
65.9
62.3 
75.6 
72.8 
77.0 
76.5 
78.9
78.9 
80.3
81.8 
Aspell+ 56.949.6 
61.8
55.6 
74.0 
69.8 
76.4 
74.0 
78.5
77.3 
82.4
82.9 
MS Word 78.268.4 
83.2
76.0 
84.4 
78.5 
84.9 
79.5 
85.1
80.1 
85.2
80.6 
MS Word+ 78.769.3 
84.8
78.7 
86.9 
81.4 
87.3 
82.4 
87.5
83.0 
87.9
83.9 
ConSpel-A 77.271.5 
85.1
81.2 
87.7 
85.0 
89.7 
87.0 
90.9
88.7 
92.4
91.6 
ConSpel-B 79.377.7 
89.8
88.1 
91.6 
90.5 
92.1 
91.3 
92.6
91.7 
93.2
92.5 
 
Table 5. Error-correction evaluation results:  
 F-scores for six systems, data from native (upper value 
in each cell) and non-native English speakers 
 
111
  
 Figure 4. Error-correction F-scores absolute differences 
 
A question we need to address is whether there 
are any real differences in misspellings produced 
by NS and NNS writers in our corpus. Our initial 
analyses show that there are some distinguishing 
characteristics. 
One characterization is obtained when we look 
at the ?complexity? of the error, defining it as the 
edit distance between misspelling and correct 
word. The data is presented in Table 6. Native 
English speakers make significantly more simple 
errors (edit distance 1) than non-native speakers, 
while the latter make more complex errors (edit 
distance 4+). 
 
Edit distance between  
misspelling and correct form NS NNS 
1 83.3% *79.9%
2 13.0% 14.0%
3 3.1% 3.9%
4+ 0.6% *2.1%
 
Table 6. Percent of non-word misspellings (tokens)  
by edit distance to correct word,  
for native and non-native populations. 
* difference significant at p<.01 
 
Another difference we found in our data is the 
length (number of characters) of the correct word 
that was misspelled, for each population (Figure 
5). For words of length 2 to 7, non-native speakers 
produce relatively more misspellings than native 
speakers. For words of length 8 and longer, native 
speakers produce relatively more misspellings than 
non-native speakers. 
ConSpel-B performs about the same on NS and 
NNS data, and better than the other systems. Given 
the above differences of NS and NNS misspellings 
in our corpus, and given that all evaluated systems, 
except ConSpel-B, show better correction on NS 
data, we conclude that ConSpel-B shows this real 
advantage due to utilization of contextual data. 
 
  
 Figure 5. Percent of non-word misspellings (tokens)  
by length of the ?intended? correct word,  
for native and non-native populations. 
5 Discussion 
Large scale comparative studies of spellchecker 
performance on data from non-native language 
speakers are scarce, possibly due to large amount 
of effort required for expert annotation of data. 
Hovermale (2010) analyzed 500 spelling errors 
from a corpus of essays produced by ELL in Japan. 
In that study, MS Word 2007 successfully cor-
rected 72% of non-word errors, while Aspell had a 
success rate of 81% (presumably at k=1). In our 
study, with data from an international sample of 
non-native English speakers, Aspell error-
correction precision rate is only 52% at k=1, and 
rises to 78% for k=5. MS Word and ConSpel-A 
(no-context) begin with precision of about 75-77% 
at k=1. At k=5, MS Word improves to about 85%, 
and ConSpel-A to above 89%.  
Bestgen and Granger (2011) analyzed 222 ar-
gumentative essays from the ICLE corpus (Gran-
ger et al, 2009), written by European EFL students 
across different levels of English proficiency. 
Their sample included about 150,000 words and 
had 1,549 spelling errors. This amounts to spel-
112
ling-error rate of about 1%, compared to 2.2% in 
our data. In that study, MS Word 2007 had detec-
tion recall of 80.43%, and detection precision of 
82.35%. In our study, MS Word had 99.6% recall 
and 90.1% precision in error detection. The differ-
ence may be attributed to the fact that we focus on 
single-token non-word misspellings, while Bestgen 
and Granger included other categories, specifically 
multi-token errors. Error-correction recall was 71% 
and precision 59% (at k=1). In our study, at k=1, 
MS Word achieved 72% recall and 65% precision, 
which is quite close to the above figures. 
Given that our context-informed system has er-
ror-correction F-score of 77.9% at k=1, and 91.8% 
at k=5, it is obvious that the system picks up the 
right corrections. There is a potential for improve-
ment, possibly by better ranking. Why doesn?t the 
context help even more? Could the system perform 
with 90% at k=1? We have tentatively identified 
three major types of influences that detract the sys-
tem from better performance. Those are a) local 
error density; b) poor grammar; and c) competition 
among inflectional variants. Local error density 
means simply that adjacent words are misspelled 
so there is not enough reliable context to use n-
grams in such cases. 
Poor grammar is also problematic for n-gram-
based approach. In a fragment ?If docter want to 
operate, he...?, the intended word was ?doctor?, but 
?doctor want? is a subject-verb agreement error, 
which is not frequent in the normative n-gram data. 
Thus, under n-gram frequency influence, the sys-
tem prefers ?doctors? as top ranked candidate. 
There is competition of inflectional variants in 
presence of grammatical errors. 
We have observed that even in absence of 
grammatical errors, sometimes an inadequate top 
ranked candidate is an inflectional variant of the 
adequate correction. For example: ?They received 
fresh air, interacte with other youth their age, solved 
problems...?. The adequate correction is 'interacted', 
but ConSpel ranks it third, while 'interacts' comes 
second and 'interact' is ranked first. Notably, non-
local context is not always beneficial ? for a exam-
ple, the presence of word 'interact' elsewhere in the 
essay will strengthen the wrong candidate. Possi-
bly, additional linguistic information could help 
improve ranking in such cases, e.g. by observing 
that all verbs in this sentence come in past tense. 
Mitton (1996) suggested that it should be possi-
ble to adapt a spellchecker to cope specifically 
with L1-characteristic errors of English learners. 
Granger and Wynne (1999) analyzed misspellings 
produced by students with several different L1 
backgrounds and have also suggested that it might 
be ?useful to adapt tools such as spellcheckers to 
the needs of non-native users.? Mitton and Okada 
(2007) have demonstrated a successful adaptation 
of a spellchecker (oriented for native English 
speakers) to Japanese learners of English.9 Howev-
er, adaptation to each specific L1 would require 
considerable resources. As noted by Hovermale 
(2010), it is not clear whether it is worthwhile to 
customize spellchecker heuristics for each learner 
population or better to just have one ELL spell-
checker. Results from our study indicate that it is at 
least feasible to produce a general-purpose spell-
checker that can successfully correct misspellings 
produced by non-native English speakers, almost 
as well as it does for native English speakers. A 
key for such development is utilization of essay 
context for re-ranking of spelling suggestions.  
6 Conclusions 
In this paper we presented a method for context-
informed correction of single-token non-word spel-
ling errors. Our results with ConSpel system dem-
onstrate that utilizing contextual information helps 
improve automatic correction of non-word miss-
pellings, for both native and non-native speakers of 
English, at least for essays written by test takers on 
standardized English proficiency tests. In future 
work we intend to produce a detailed study of the 
different ways of context utilization. We also in-
tend to expand the system to handle multi-word 
spelling errors. 
Acknowledgments 
Many thanks to Chong Min Lee and Daniel 
Blanchard for assisting in evaluation with Aspell 
and Microsoft Office 2007; to our annotators, Ni-
cole DiCrecchio, Julia Farnum, Melissa Lopez, 
Susanne Miller, Matthew Mulholland, Sarah Ohls, 
and Waverely VanWinkle. The manuscript has 
also benefited from the comments of three ano-
nymous reviewers. 
                                                          
9 Boyd (2009) used non-native (Japanese ELL) pronunciation 
modeling to improve a speller that uses just an orthographic 
error-model. Her combined system achieved 65% correction 
precision at k=1, and 82.6% at k=5. Our context-informed 
system achieves 77.5% and 91.4% respectively. 
113
References  
Reima Al-Jarf. 2010. Spelling error corpora in EFL. 
Sino-US English Teaching, 7(1):6-15. 
Kevin Atkinson. 2011. GNU Aspell. Software available 
at http://aspell.net. 
Shane Bergsma, Dekang Lin and Randy Goebel. 2009. 
Web-Scale N-gram Models for Lexical Disambigua-
tion. In Proceedings of International Joint Confe-
rence on Artificial Intelligence (IJCAI-2009), pages 
1507-1512. 
Yves Bestgen and Sylviane Granger. 2011. Categorising 
spelling errors to assess L2 writing. International 
Journal of Continuing Engineering Education and 
Life-Long Learning, 21(2/3):235-252. 
Adriane Boyd. 2009. Pronunciation Modeling in Spel-
ling Correction for Writers of English as a Foreign 
Language. In Proceedings of the NAACL HLT 2009 
Student Research Workshop and Doctoral Consor-
tium, pages 31?36.  
Torsten Brants and Alex Franz. 2006. Web 1T 5-gram 
Version 1. LDC2006T13. Philadelphia, PA, USA: 
Linguistic Data Consortium. 
Eric Brill and Robert C. Moore. 2000. An improved 
error model for noisy channel spelling correction. In 
Proceedings of the 38th Annual Meeting of ACL, 
pages 286-293 
Andrew Carlson and Ian Fette. 2007. Memory-Based 
Context-Sensitive Spelling Correction at Web Scale. 
In Proceedings of the Sixth International Conference 
on Machine Learning and Applications, pages 166-
171. 
Qing Chen, Mu Li and Ming Zhou. 2007. Improving 
query spelling correction using web search results. In 
Proceedings of the 2007 Conference on Empirical 
Methods in Natural Language (EMNLP 2007), pages 
181-189.  
Robert Connors and Andrea A. Lunsford. 1988. Fre-
quency of Formal Error in Current College Writing, 
or Ma and Pa Kettle Do Research. College Composi-
tion and Communication, 39(4):395?409. 
Scott A. Crossley, Tom Salsbury, Philip McCarthy and 
Danielle S. McNamara. 2008. Using latent semantic 
analysis to explore second language lexical develop-
ment. In Wilson, D. and Chad Lane, H. (Eds.): Pro-
ceedings of the 21st International Florida Artificial 
Intelligence Research Society Conference, pages 
136?141.  
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collective 
knowledge of web users. In Proceedings of the Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP-2004), pages 293?300. 
Frederick Damerau. 1964. A technique for computer 
detection and correction of spelling errors. Commu-
nications of the ACM, 7(3): 659-664. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
Classifier-Based Approach to Preposition and De-
terminer Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics (COLING 2008), pages 
69-176. 
Sebastian Deorowicz and Marcin G. Ciura. 2005. Cor-
recting spelling errors by modelling their causes. In-
ternational Journal of Applied Mathematics and 
Computer Science, 15(2), pages 275?285. 
Christy Desmet and Ron Balthazor. 2006. Finding Pat-
terns in Textual Corpora: Data Mining, Research, and 
Assessment in First-year Composition. Paper pre-
sented at Computers and Writing 2006, Lubbock, 
Texas, May 25?29, 2006.  
Semire Dikli. 2006. An overview of automated scoring 
of essays. Journal of Technology, Learning, and As-
sessment, 5(1):4-35. ejournals.bc.edu/ojs/index.php/jtla 
(last accessed on February 22, 2012). 
ETS. 2011a. GRE?: Introduction to the Analytical 
Writing Measure. Educational Testing Service. 
www.ets.org/gre/revised_general/prepare/analytical_writin
g  (last accessed on March 9, 2012). 
ETS. 2011b. TOEFL? iBT? Test Content. Educational 
Testing Service. www.ets.org/toefl/ibt/about/content 
(last accessed on March 9, 2012). 
Michael Flor. 2012. A fast and flexible architecture for 
very large word n-gram datasets. Natural Language 
Engineering. Available on CJO 2012 
doi:10.1017/S1351324911000349 
journals.cambridge.org/action/displayJournal?jid=NLE  
Michael Flor and Yoko Futagi. 2011. Producing an an-
notated corpus with automatic spelling correction. 
Presented at the Learner Corpus Research 2011 Con-
ference, 15-17 September 2011, Louvain-la-Neuve, 
Belgium. Submitted for publication.  
Yoko Futagi. 2010. The effects of learner errors on the 
development of a collocation detection tool. In Pro-
ceedings of the Fourth Workshop on Analytics for 
Noisy Unstructured Text Data (AND '10), pages 27-
34. 
Sylviane Granger, Estelle Dagneaux, Fanny Meunier 
and Magali Paquot. 2009. The International Corpus 
of Learner English. Handbook and CD-ROM (Ver-
sion 2), Presses Universitaires de Louvain, Louvain-
la-Neuve.  
Sylviane Granger and Martin Wynne. 1999. Optimising 
measures of lexical variation in EFL learner corpora. 
in Kirk, J. (Ed.): Corpora Galore, pages 249?257, 
Rodopi, Amsterdam.  
Andrew Golding and Dan Roth. 1999. A Winnow based 
approach to Context-Sensitive Spelling Correction. 
Machine Learning, 34(1-3):107-130. 
114
DJ Hovermale. 2010. An analysis of the spelling errors 
of L2 English learners. Presented at CALICO 2010 
Conference, Amherst, MA, USA, June 10-12, 2010. 
Available electronically from http://www.ling.ohio-
state.edu/~djh/presentations/djh_CALICO2010.pptx  
Aminul Islam and Diana Inkpen. 2009. Real-word spel-
ling correction using Google Web 1T n-gram with 
backoff. In Proceedings of the IEEE International 
Conference on Natural Language Processing and 
Knowledge Engineering (IEEE NLP-KE?09), pages 
1-8. 
Mark Kernighan, Kenneth Church and William Gale. 
1990. A spelling correction program based on a noisy 
channel model. In Proceedings of the 13th Confe-
rence on Computational Linguistics (COLING ?90), 
pages 205-210. 
Roland Kuhn and Renato De Mori. 1990. A cache-based 
natural language model for speech recognition. IEEE 
Transactions on Pattern Analysis and Machine Intel-
ligence, 12(6):570?583. 
Karen Kukich, 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, 
24:377-439. 
Thomas K. Landauer, Darrell Laham and Peter Foltz. 
2003. Automatic essay assessment. Assessment in 
Education, 10(3):295?308. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault, 2010. Automated grammatical er-
ror detection for language learners. Synthesis Lec-
tures on Human Language Technologies, No. 9, 
Morgan & Claypool, Princeton, USA. 
Vladimir Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. Soviet 
Physics Doklady, 10:707-710. 
Andrea A. Lunsford and Karen J. Lunsford. 2008. Mis-
takes Are a Fact of Life: A National Comparative 
Study. College Composition and Communication, 
59(4):781-806.  
Eric Mays, Fred J. Damerau and Robert L. Mercer. 
1991. Context based spelling correction. Information 
Processing & Management, 27(5):517?522.  
Roger Mitton. 1996. English spelling and the computer. 
Harlow, Essex: Longman Group. Available electron-
ically from http://eprints.bbk.ac.uk/469 
Roger Mitton. 2009. Ordering the suggestions of a 
spellchecker without using context. Natural Lan-
guage Engineering, 15(2):173?192. 
Roger Mitton and Takeshi Okada. 2007. The adaptation 
of an English spellchecker for Japanese writers. Pre-
sented at: Symposium on Second Language Writing, 
15-17 Sept 2007, Nagoya, Japan. Available electron-
ically from http://eprints.bbk.ac.uk/592 
Ryo Nagata, Edward Whittaker and Vera Sheinman. 
2011. Creating a manually error-tagged and shallow-
parsed learner corpus. In Proceedings of the 49th An-
nual Meeting of the Association for Computational 
Linguistics, pages 1210?1219, Portland, Oregon: As-
sociation for Computational Linguistics.  
Takeshi Okada. 2005. Spelling errors made by Japanese 
EFL writers: with reference to errors occurring at the 
word-initial and the word-final position. In V. Cook 
and B. Bassetti (Ed.), Second language writing sys-
tems, pages 164-183. Clevedon: Multilingual Mat-
ters.  
Diana P?rez, Enrique Alfonseca and Pilar Rodr?guez. 
2004. Application of the Bleu method for evaluating 
free-text answers in an e-learning environment. Pro-
ceedings of the Language Resources and Evaluation 
Conference (LREC-2004), pages 1351-1354. 
Lawrence Philips. 2000. The Double-metaphone Search 
Algorithm. C/C++ User's Journal, June, 2000. 
Anne Rimrott and Trude Heift. 2005. Language learners 
and generic spell checkers in CALL. CALICO Jour-
nal, 23(1):17-48. 
Anne Rimrott and Trude Heift. 2008. Evaluating auto-
matic detection of misspellings in German. Language 
Learning & Technology, 12(3):73-92. 
Casey Whitelaw, Ben Hutchinson, Grace Y Chung and 
Gerard Ellis. 2009. Using the Web for language in-
dependent spellchecking and autocorrection. In Pro-
ceedings of the Conference on Empirical Methods in 
Natural Language Processing (EMNLP 2009), pages 
890-899. 
Mark Warschauer and Paige Ware. 2006. Automated 
writing evaluation: defining the classroom research 
agenda. Language Teaching Research, 10(2):157?
180. 
Guoxing Yu. 2010. Lexical diversity in writing and 
speaking task performances. Applied Linguistics, 
31(2):236?259.  
Yang Zhang, Pilian He, Wei Xiang and Mu Li. 2006. 
Discriminative reranking for spelling correction. In 
Proceedings of the 20th Pacific Asia Conference on 
Language, Information and Computation, pages 64-
71. 
 
115
Proceedings of the First Workshop on Metaphor in NLP, pages 11?20,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Argumentation-Relevant Metaphors in Test-Taker Essays
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
{bbeigmanklebanov,mflor}@ets.org
Abstract
This article discusses metaphor annotation in
a corpus of argumentative essays written by
test-takers during a standardized examination
for graduate school admission. The quality of
argumentation being the focus of the project,
we developed a metaphor annotation proto-
col that targets metaphors that are relevant
for the writer?s arguments. The reliability of
the protocol is ?=0.58, on a set of 116 es-
says (the total of about 30K content-word to-
kens). We found a moderate-to-strong correla-
tion (r=0.51-0.57) between the percentage of
metaphorically used words in an essay and the
writing quality score. We also describe en-
couraging findings regarding the potential of
metaphor identification to contribute to auto-
mated scoring of essays.
1 Introduction
The goal of our project is to automatically score
the quality of argumentation in essays written for
a standardized graduate school admission exam.
Metaphors being important argumentative devices,
we report on annotating data for potential training
and testing of metaphor detection software that
would eventually be used for automated scoring of
essays.
Metaphors of various kinds can be relevant to ar-
gumentation. Some metaphors create vivid images
and function as examples or as organizing ideas be-
hind a series of examples. These are akin to pictures
that are worth a thousand words, and are highly po-
tent rhetorical devices. Metaphors of a less artistic
crafting ? more conventionalized ones, metaphors
that we ?live by? according to Lakoff and John-
son?s (1980) famous tenet ? subtly organize our
thinking and language production in culturally co-
herent ways.
For an example of a vivid metaphor that helps or-
ganize the essay, consider an essay on the relation-
ship between arts and government funding thereof
(see example 1). The author?s image of a piece of
art as a slippery object that escapes its captor?s grip
as a parallel to the relationship between an artist and
his or her patron/financier is a powerful image that
provides a framework for the author?s examples (in
the preceding paragraph, Chaucer is discussed as
a clever and subversive writer for his patron) and
elaborations (means of ?slippage?, like veiled ima-
gery, multiple meanings, etc).
(1) Great artistic productions, thus, tend to
rise above the money that bought them, to
bite, as it were, the hand that fed them.
This is not always so, of course. But
the point is that great art is too slippery
to be held in the grip of a governing
power. Through veiled imagery, multiple
meanings, and carefully guarded language,
a poem can both powerfully criticize a ruler
and not blow its cover.
For an example of a conventional metaphor, con-
sider the metaphor of construction/building. The
connotation of foundations is something essential,
old, solid, and lying deep, something that, once laid,
remains available for new construction for a long pe-
riod of time. It is often used to explain emergence
11
of things ? the existence of foundations (or support,
or basis) is contrasted with the (presumed) idea of
appearance out of nothing. Certain topics of discus-
sion are particularly amenable for arguments from
construction-upon-foundation. For example, con-
sider an essay question ?Originality does not mean
thinking something that was never thought before;
it means putting old ideas together in new ways,?
where an explanation of the emergence of something
is required. Examples 2-6 show excerpts from es-
says answering this prompt that employ the founda-
tion metaphor.
(2) The foundation of the United States was
also based on a series of older ideas into
which the fathers of our nation breathed
new life.
(3) History is a progressive passing on of ideas,
a process of building on the foundations laid
by the previous generations. New ideas can-
not stand if they are without support from
the past.
(4) New discoveries and ideas are also original
for some time, but eventually they become
the older, accepted pieces that are the build-
ing blocks for originality.
(5) Original thinking can include old ideas
which almost always are a basis for
continued thought leading to new ideas.
(6) Humans are born of their ancestors, thrive
from their intelligence, and are free to build
on the intellectual foundations laid.
The two types of metaphors exemplified above
have different argumentative roles. The first orga-
nizes a segment of an essay around it, firstly by
imposing itself on the reader?s mind (a property
rhetoricians call presence (Perelman and Olbrechts-
Tyteca, 1969; Gross and Dearin, 2003; Atkinson
et al, 2008)), secondly by helping select support-
ing ideas or examples that are congruent with the
parts of the target domain that are highlighted by the
metaphor (this property is termed framing (Lakoff,
1991; Entman, 2003)), such as the idea of evasive-
ness purported by the ART AS A SLIPPERY OB-
JECT metaphor that is taken up both in the preceding
Chaucer example and in an elaboration.
By contrast, metaphors ?we live by? without even
noticing, such as TIME IS MONEY or IDEAS ARE
BUILDINGS, are not usually accorded much reader
attention; they are processed by using the conven-
tional connotation of the word as if it were an
additional sense of that word, without invoking a
comparison between two domains (for processing
by categorization see (Bowdle and Gentner, 2005;
Glucksbeg and Haught, 2006)). Thus, the word
foundation is unlikely to elicit an image of a con-
struction site, but rather will directly invoke the con-
cept of something essential and primary. It is un-
clear to what extent such highly conventionalized
metaphors that are not deliberately construed as
metaphors have the framing property beyond fram-
ing induced by any lexical choice ? that of stress-
ing the chosen over the un-chosen alternative (Bil-
lig, 1996). Therefore, the fact that an essay writer
used a conventional metaphor is not in itself a mark
of rhetorical sophistication; it is possible, however,
that, if certain metaphorical source domains are par-
ticularly apt for the given target domain (as the do-
main of construction to discuss emergence), using
the metaphor is akin to choosing a solid though not
particularly original argument.
Our interest being in metaphors that play a role
in argumentation, we attempted to devise an annota-
tion protocol that would be specifically geared to-
wards identification of such metaphors. In what
follows, we review the literature on approaches to
annotating metaphors in a given discourse (sec-
tion 2), we describe the protocol and the annotation
procedure (section 3), report inter-annotator agree-
ment (section 4), quantify the relationship between
metaphorical density (percentage of metaphorically
used words in an essay) and essay quality as mea-
sured by essay score, as well as estimate the poten-
tial usefulness of metaphor detection for automated
scoring of essays (section 5.2).
2 Related Work
Much of the contemporary work on metaphor in psy-
chological and computational veins is inspired by
Lakoff and Johnson?s (1980) research on concep-
tual metaphor. Early work in this tradition concen-
trated on mapping the various conceptual metaphors
in use in a particular culture (Lakoff and Johnson,
12
1980; Lakoff and Ko?vecses, 1987; Ko?vecses, 2002).
Examples for various conceptual mappings are col-
lected, resulting in the Master Metaphor List (Lakoff
et al, 1991), showing common metaphorical map-
pings and their instances of use. For example, the
LIFE IS A JOURNEY conceptual metaphor that maps
the source domain of JOURNEY to the target domain
of LIFE is used in expressions such as:
? He just sails through life.
? He is headed for great things.
? If this doesn?t work, I?ll just try a different
route.
? She?ll cross that bridge when she comes to it.
? We?ve come a long way.
While exemplifying the extent of metaphoricity of
everyday English, such a list is not directly appli-
cable to annotating metaphors in discourse, due to
the limited coverage of the expressions pertaining to
each conceptual metaphor, as well as of the concep-
tual metaphors themselves.
Studies of discourse metaphor conducted in
the Critical Discourse Analysis tradition (Musolff,
2000; Charteris-Black, 2005) analyze a particular
discourse for its employment of metaphors. For
example, an extensive database of metaphors in
British and German newspaper discourse on Euro-
pean integration in the 1990s was compiled by Mu-
solff (2000); the author did not make it clear how
materials for annotation were selected.
A systematic but not comprehensive approach to
creating a metaphor-rich dataset is to pre-select ma-
terials using linguistic clues (Goatly, 1997) for the
presence of metaphor, such as utterly or so to speak.
Shutova and Teufel (2010) report precision statis-
tics for using different clues to detect metaphoric
sentences; expressions such as literally, utterly, and
completely indicate a metaphorical context in more
than 25% of cases of their use in the British National
Corpus. Such cues can aid in pre-selecting data for
annotation so as to increase the proportion of mate-
rials with metaphors beyond a random sample.
Another approach is to decide on the source do-
mains of interest in advance, use a dictionary or
thesaurus to detect words belonging to the domain,
and annotate them for metaphoricity (Stefanowitsch,
2006; Martin, 2006; Gedigan et al, 2006). Gedi-
gan et al (2006) found that more than 90% of
verbs belonging to MOTION and CURE domains in
a Wall Street Journal corpus were used metaphori-
cally. Fixing the source domain is potentially appro-
priate if common metaphorically used domains in a
given discourse have already been identified, as in
(Koller et al, 2008; Beigman Klebanov et al, 2008).
A complementary approach is to fix the target
domain, and do metaphor ?harvesting? in a win-
dow around words belonging to the target domain.
For example, Reining and Lo?neker-Rodman (2007)
chose the lemma Europe to represent the target do-
main in the discourse on European integration. They
extracted small windows around each occurrence of
Europe in the corpus, and manually annotated them
for metaphoricity. This is potentially applicable to
analyzing essays, because the main target domain of
the discourse is usually given in the prompt, such
as art, originality. The strength of this method is
its ability to focus on metaphors with argumentative
potential, because the target domain, which is the
topic of the essay, is directly involved. The weak-
ness is the possibility of missing metaphors because
they are not immediately adjacent to a string from
the target domain.
The Metaphor Identification Procedure (MIP) is
a protocol for exhaustive metaphoricity annota-
tion proposed by the Pragglejaz group (Pragglejaz,
2007). The annotator classifies every word in a
document (including prepositions) as metaphorical
if it has ?a more basic contemporary meaning? in
other contexts than the one it has in the current con-
text. Basic meanings are explained to be ?more con-
crete, related to bodily action, more precise, and his-
torically older.? The authors ? all highly qualified
linguists who have a long history of research collab-
oration on the subject of metaphor ? attained a kappa
of 0.72 for 6 annotators for one text of 668 words
and 0.62 for another text of 676 words. Shutova and
Teufel (2010) used the protocol to annotate content
verbs only, yielding kappa of 0.64 for 3 volunteer
annotators with some linguistic background, on a set
of sentences containing 142 verbs sampled from the
British National Corpus. It is an open question how
well educated lay people can agree on an exhaustive
metaphor annotation of a text.
13
We note that the procedure is geared towards con-
ceptual metaphors at large, not necessarily argumen-
tative ones, in that the protocol does not consider the
writer?s purpose in using the metaphor. For example,
the noun forms in ?All one needs to use high-speed
forms of communication is a computer or television
and an internet cable? is a metaphor according to
the MIP procedure, because the basic meaning ?a
shape of something? is more concrete/physical than
the contextual meaning ?a type of something,? so a
physical categorization by shape stands for a more
abstract categorization into types. This metaphor
could have an argumentative purport; for instance,
if the types in question were actually very blurred
and difficult to tell apart, by calling them forms (and,
by implications, shapes), they are framed as being
more clearly and easily separable than they actually
are. However, since the ease of categorization of
high-speed electronic communication into types is
not part of the author?s argument, the argumentative
relevance of this metaphor is doubtful.
3 Annotation Protocol
In the present study, annotators were given the fol-
lowing guidelines:
Generally speaking, a metaphor is a lin-
guistic expression whereby something is
compared to something else that it is
clearly literally not, in order to make a
point. Thus, in Tony Blair?s famous ?I
haven?t got a reverse gear,? Tony Blair is
compared to a car in order to stress his
unwillingness/inability to retract his state-
ments or actions. We would say in this
case that a metaphor from a vehicle do-
main is used.
. . . [more examples] . . .
The first task in our study of metaphor
in essays is to read essays and underline
words you think are used metaphorically.
Think about the point that is being made
by the metaphor, and write it down. Note
that a metaphor might be expressed by the
author or attributed to someone else. Note
also that the same metaphor can be taken
up in multiple places in a text.
During training, two annotators were instructed
to apply the guidelines to 6 top-scoring essays an-
swering a prompt about the role of art in society.
After they finished, sessions were held where the
annotators and one of the authors of this paper dis-
cussed the annotations, including explication of the
role played by the metaphor in the essay. A sum-
mary document that presents a detailed consensus
annotation of 3 of the essays was circulated to the
annotators. An example of an annotation is shown
below (metaphors are boldfaced in the text and ex-
plained underneath):
F. Scott Fitzgerald wrote, ?There is a dark
night in every man?s soul where it is
always 2 o?clock in the morning.? His
words are a profound statement of human
nature. Within society, we operate under a
variety of social disguises. Some of these
masks become so second nature that we
find ourselves unable to take them off.
(1) Dark night, 2 o?clock in the morning:
True emotions are not accessible (at 2
o?clock a person is usually asleep and un-
aware of what is going on) and frighten-
ing to handle on one?s own (scary to walk
at night alone); people need mediation to
help accessibility, and also company to al-
leviate the fear. Art provides both. This
metaphor puts forward the two main argu-
ments: accessibility and sharing.
(2) Masks, take off, disguises: could
be referring to the domain of the-
ater/performance. Makes the point that
what people do in real life to themselves
is superficially similar to what art (the-
ater) does to performers ? hiding their true
identity. In the theater, the hiding is tem-
porary and completely reversible at will,
there is really no such thing as inability to
take off the mask. The socially-inflicted
hiding is not necessarily under the per-
son?s control, differently from a theatrical
mask. Supports and extends the accessi-
bility argument: not just lack of courage
or will, but lack of control to access the
true selves.
14
The actual annotation then commenced, on a sam-
ple of essays answering a different question (the
data will be described in section 3.1). Annotators
were instructed to mark metaphors in the text using a
graphical interface that was specially developed for
the project. The guidelines for the actual annotation
are shown below:
During training, you practiced careful
reading while paying attention to non-
literal language and saw how metaphors
work in their context. At the annota-
tion stage, you are not asked to expli-
citly interpret the metaphor and identify
its argumentative contribution (or rather,
its attempted argumentative contribution),
only to mark metaphors, trusting your in-
tuition that you could try to interpret the
metaphor in context if needed.
Note that we have not provided formal defini-
tions of what a literal sense is in order to not inter-
fere with intuitive judgments of metaphoricity (dif-
ferently from Pragglejaz (2007), for example, who
provide definition of a basic sense). Neither have
we set up an explicit classification task, whereby an-
notators are required to classify every single word in
the text as a metaphor or a non-metaphor (again, dif-
ferently from Pragglejaz (2007)); in our task, anno-
tators were instructed to mark metaphors while they
read. This is in the spirit of Steen?s (2008) notion of
deliberate metaphors ? words and phrases that the
writer actually meant to produce as a metaphor, as
opposed to cases where the writer did not have a
choice, such as using in for an expression like in
time, due to the pervasiveness of the time-as-space
metaphor. Note, however, that Steen?s notion is
writer-based; since we have no access to the writers
of the essays, we side with an educated lay reader
and his or her perception of a metaphorical use.
The annotators were instructed to give the author
the benefit of the doubt and *not* to assume that a
common metaphor is necessarily unintenional:
When deciding whether to attribute to the
author the intention of making a point
using a metaphor, please be as liberal as
you can and give the author the benefit
of the doubt. Specifically, if something is
a rather common metaphor that still hap-
pens to fit nicely into the argument the au-
thor is making, we assume that the author
intended it that way.
To clarify what kinds of metaphors are excluded
by our guidelines, we explained as follows:
In contrast, consider cases where an ex-
pression might be perhaps formally clas-
sified as a metaphor, but the literal sense
cannot be seen as relevant to the author?s
argument. For example, consider the fol-
lowing sentence from Essay 2 from our
training material: ?Seeing the beauty of
nature or hearing a moving piece of music
may drive one to perhaps try to replicate
that beauty in a style of one?s own.? Look
at the italicized word ? the preposition in.
According to some theories of metaphor,
that would constitute a metaphorical use:
Literally, in means inside some container;
since style is not literally a container, the
use of in here is non-literal. Suppose now
that the non-literal interpretation invites
the reader to see style as a container. A
container might have more or less room,
can be full or empty, can be rigid or flex-
ible, can contain items of the same or dif-
ferent sorts ? these are some potential im-
ages that go with viewing something as a
container, yet none of them seems to be
relevant to whatever the author is saying
about style, that is, that it is unique (one?s
own) and yet the result is not quite original
(replication).
The two annotators who participated in the task
hold BA degrees in Linguistics, but have no back-
ground in metaphor theory. They were surprised and
bemused by an example like in style, commenting
that it would never have occurred to them to mark it
as a metaphor. In general, the thrust of this proto-
col is to identify metaphorical expressions that are
noticeable and support the author?s argumentative
moves; yet, we targeted a reasonable timeline for
completing the task, with about 30 minutes per text,
therefore we did not require a detailed analysis of
the marked metaphors as done during training.
15
3.1 Data
Annotation was performed on 116 essays written on
the following topic: ?High-speed electronic commu-
nications media, such as electronic mail and tele-
vision, tend to prevent meaningful and thought-
ful communication.? Test-takers are instructed to
present their perspective on the issue, using rele-
vant reasons and/or examples to support their views.
Test-takers are given 45 minutes to compose an es-
say. The essays were sampled from the dataset an-
alyzed in Attali et al (2013), with oversampling
of longer essays. In the Attali et al (2013) study,
each essay was scored for the overall quality of En-
glish argumentative composition; thus, to receive the
maximum score, an essay should present a cogent,
well-articulated analysis of the complexities of the
issue and convey meaning skillfully. Each essay was
scored by 16 professional raters on a scale of 1 to 6,
allowing plus and minus scores as well, quantified
as 0.33 ? thus, a score of 4- is rendered as 3.67. This
fine-grained scale resulted in a high mean pairwise
inter-rater correlation (r=0.79). We use the average
of 16 raters as the final score for each essay. This
dataset provides a fine-grained ranking of the essays,
with almost no two essays getting exactly the same
score.
For the 116 essays, the mean length was 478
words (min: 159, max: 793, std: 142); mean score:
3.82 (min: 1.81, max: 5.77, std: 0.73). Table 1
shows the distribution of essay scores.
Score Number Proportion
of Essays of Essays
2 4 0.034
3 33 0.284
4 59 0.509
5 19 0.164
6 1 0.009
Table 1: Score distribution in the essay data. The first
column shows the rounded score. For the sake of pre-
sentation in this table, all scores were rounded to integer
scores, so a score of 3.33 was counted as 3, and a score
of 3.5 was counted as 4.
4 Inter-Annotator Agreement and Parts of
Speech
The inter-annotator agreement on the total of 55,473
word tokens was ?=0.575. In this section, we inves-
tigate the relationship between part of speech and
metaphor use, as well as part of speech and inter-
annotator agreement.
For this discussion, words that appear in the
prompt (essay topic) are excluded from all sets. Fur-
thermore, we concentrate on content words only (as
identified by the OpenNLP tagger1). Table 2 shows
the split of the content-word annotations by part
of speech, as well as the reliability figures. We
report information for each of the two annotators
separately, as well as for the union of their anno-
tations. We report the union as we hypothesize that
a substantial proportion of apparent disagreements
between annotators are attention slips rather than
substantive disagreements; this phenomenon was at-
tested in a previous study (Beigman Klebanov et al,
2008).
POS Count A1 A2 A1
?
A2 ?
All 55,473 2,802 2,591 3,788 0.575
Cont. 29,207 2,380 2,251 3,211 0.580
Noun 12,119 1,033 869 1,305 0.596
Adj 4,181 253 239 356 0.525
Verb 9,561 1,007 1,039 1,422 0.563
Adv 3,346 87 104 128 0.650
Table 2: Reliability by part of speech. The column Count
shows the total number of words in the given part of
speech across the 116 essays. Columns A1 and A2 show
the number of items marked as metaphors by annotators
1 and 2, respectively, while Column A1
?
A2 shows num-
bers of items in the union of the two annotations. The
second row presents the overall figure for content words.
Nouns constitute 41.5% of all content words; they
are 43.4% of all content-word metaphors for anno-
tator 1, 38.6% for annotator 2, and 40.6% for the
union of the two annotations. Nouns are therefore
represented in the metaphor annotated data in their
general distribution proportions. Of all nouns, 7%-
8.5% are identified as metaphors by a single annota-
tor, while 10.8% of the nouns are metaphors in the
union annotation.
1http://opennlp.apache.org/index.html
16
Verbs are 32.7% of all content words; they are
42.3% of all content-word metaphors for annotator
1, 46.2% for annotator 2, and 44.3% in the union.
Verbs are therefore over-represented in the metaphor
annotated data relative to their general distribution
proportions. Of all verbs, 10.5%-10.9% are identi-
fied as metaphors by a single annotator, while 14.9%
are metaphors in the union annotation.
Adjectives are 14.3% of all content words; they
are 10.6% of all content-word metaphors for anno-
tator 1, 10.6% for annotator 2, and 11.1% in the
union. Adjectives are therefore somewhat under-
represented in the metaphor annotated data with re-
spect to their general distribution. About 6% of ad-
jectives are identified as metaphors in individual an-
notations, and 8.5% in the union annotation.
Adverbs are 11.5% of all content words; they are
3.7% of all content-word metaphors for annotator 1
and 4.6% for annotator 2, and 4% in the union. Ad-
verbs are heavily under-represented in the metaphor
annotated data with respect to their general distri-
bution. Of all non-prompt adverbs, about 3-4% are
identified as metaphors.
The data clearly points towards the propensity of
verbs towards metaphoricity, relative to words from
other parts of speech. This is in line with reports in
the literature that identify verbs as central carriers of
metaphorical vehicles: Cameron (2003) found that
about 50% of metaphors in educational discourse are
realized by verbs, beyond their distributional propor-
tion; this finding prompted Shutova et al (2013) to
concentrate exclusively on verbs.
According to Goatly (1997), parts of speech dif-
fer in the kinds of metaphors they realize in terms of
the recognizability of the metaphorical use as such.
Nouns are more recognizable as metaphors than
other word classes for the following two reasons:
(1) Since nouns are referring expressions, they re-
veal very strongly the clashes between conventional
and unconventional reference; (2) Since nouns of-
ten refer to vivid, imaginable entities, they are more
easliy recognized than metaphors of other parts of
speech. Moreover, morphological derivation away
from nouns ? for example, by affixation ? leads to
more lexicalized and less noticeable metaphors than
the original nouns.
Goatly?s predictions seem to be reflected in inter-
annotator agreement figures for nouns versus adjec-
tives and verbs, with nouns yielding higher reliabi-
lity of identification than verbs and adjectives, with
the latter two categories having more cases where
only one but not both of the annotators noticed a
metaphorical use. Since adverbs are the most distant
from nouns in terms of processes of morphological
derivation, one would expect them to be less eas-
ily noticeable, yet in our annotation adverbs are the
most reliably classified category.
Inspecting the metaphorically used adverbs, we
find that a small number of adverbs cover the bulk
of the volume: together (11), closer (11), away (10),
back (8) account for 46% of the adverbs marked by
annotator 1 in our dataset. Almost all cases of to-
gether come from a use in the phrasal verb bring
together (8 cases), in expressions like ?bringing the
world together into one cyberspace without borders?
or ?electronic mail could bring people closer to-
gether? or ?bringing society together.? In fact, 6 of
the 11 cases of closer are part of the construction
bring closer together, and the other cases have simi-
lar uses like ?our conversations are more meaningful
because we are closer through the internet.?
Interestingly, the metaphorical uses of away also
come from phrasal constructions that are used for
arguing precisely the opposite point ? that cyber-
communications drive people away from each other:
?email, instant messaging, and television support a
shift away from throughful communication,? ?mass
media and communications drive people away from
one another,? ?by typing a message ... you can easily
get away from the conversation.?
It seems that the adverbs marked for meta-
phoricity in our data tend to be (a) part of phrasal
constructions, and (b) part of a commonly made ar-
gument for or against electronic communication ?
that it (metaphorically) brings people together, or
(metaphorically) drives them apart by making the
actual togetherness (co-location) unnecessary for
communication. The adverbs are therefore not of the
derivationally complex kind Goatly has in mind, and
their noticeability might be enhanced by being part
of a common argumentative move in the examined
materials, especially since the annotators were in-
structed to look out for metaphors that support the
writer?s argument.
17
5 Metaphor and Content Scoring
In order to assess the potential of metaphor detec-
tion to contribute to essay scoring, we performed
two tests: correlation with essay scores and a regres-
sion analysis in order to check whether metaphor use
contributes information that is beyond what is cap-
tured by a state-of-art essay scoring system.
As a metaphor-derived feature, we calculated
metaphorical density, that is, the percentage of
metaphorically used words in an essay: All words
marked as metaphors in an essay were counted (con-
tent or other), and the total was divided by essay
length.
5.1 E-rater
As a reference system, we use e-rater (Attali and
Burstein, 2006), a state-of-art essay scoring system
developed at Educational Testing Service.2 E-rater
computes more than 100 micro-features, which are
aggregated into macro-features aligned with specific
aspects of the writing construct. The system in-
corporates macro-features measuring grammar, us-
age, mechanics, style, organization and develop-
ment, lexical complexity, and vocabulary usage. Ta-
ble 3 gives examples of micro-features covered by
the different macro-features.
Macro-Feature Example Micro-Features
Grammar, agreement errors
Usage, and verb formation errors
Mechanics missing punctuation
Style passive, very long or very short
sentences, excessive repetition
Organization use of discourse elements:
and thesis, support, conclusion
Development
Lexical average word frequency
Complexity average word length
Vocabulary similarity to vocabulary in
high- vs low-scoring essays
Table 3: Features used in e-rater (Attali and Burstein,
2006).
E-rater models are built using linear regression on
large samples of test-taker essays. We use an e-rater
model built at Educational Testing Service using
2http://www.ets.org/erater/about/
a large number of essays across different prompts,
with no connection to the current project and its
authors. This model obtains Pearson correlations
of r=0.935 with the human scores. The excellent
performance of the system leaves little room for
improvement; yet, none of the features in e-rater
specifically targets the use of figurative language, so
it is interesting to see the extent to which metaphor
use could help explain additional variance.
5.2 Results
We found that metaphorical density attains correla-
tion of r=0.507 with essay score using annotations
of annotator 1, r=0.556 for annotator 2, and r=0.570
using the union of the two annotators. It is clearly
the case that better essays tend to have higher pro-
portions of metaphors.
We ran a regression analysis with essay score as
the dependent variable and e-rater raw score and
metaphor density in the union annotation as two
independent variables. The correlation with essay
score improved from 0.935 using e-rater alone to
0.937 using the regression equation (the adjusted R2
of the model improved from 0.874 to 0.876). While
the contribution of metaphor feature is not statisti-
cally significant for the size of our dataset (n=116,
p=0.07), we are cautiously optimistic that metaphor
detection can make a contribution to essay scoring
when the process is automated and a larger-scale
evaluation can be performed.
6 Conclusion
This article discusses annotation of metaphors in
a corpus of argumentative essays written by test-
takers during a standardized examination for grad-
uate school admission. The quality of argumenta-
tion being the focus of the project, we developed a
metaphor annotation protocol that targets metaphors
that are relevant for the writer?s arguments. The
reliability of the protocol is ?=0.58, on a set of 116
essays (a total of about 30K content word tokens).
We found a moderate-to-strong correlation
(r=0.51-0.57) between the density of metaphors
in an essay (percentage of metaphorically used
words) and the writing quality score as provided by
professional essay raters.
As the annotation protocol is operationally effi-
18
cient (30 minutes per essay of about 500 words),
moderately reliable (?=0.58), and uses annotators
that do not possess specialized knowledge and
training in metaphor theory, we believe it is fea-
sible to annotate a large set of essays for the pur-
pose of building a supervised machine learning sys-
tem for detection of metaphors in test-taker essays.
The observed correlations of metaphor use with es-
say score, as well as the fact that metaphor use is
not captured by state-of-art essay scoring systems,
point towards the potential usefulness of a metaphor
detection system for essay scoring.
References
Nathan Atkinson, David Kaufer, and Suguru Ishizaki.
2008. Presence and Global Presence in Genres of Self-
Presentation: A Framework for Comparative Analysis.
Rhetoric Society Quarterly, 38(3):1?27.
Yigal Attali and Jill Burstein. 2006. Automated Es-
say Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali, Will Lewis, and Michael Steier. 2013. Scor-
ing with the computer: Alternative procedures for im-
proving reliability of holistic essay scoring. Language
Testing, 30(1):125?141.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In COL-
ING 2008 workshop on Human Judgments in Compu-
tational Linguistics, pages 2?7, Manchester, UK.
Michael Billig. 1996. Arguing and Thinking: A Rhetor-
ical Approach to Social Psychology. Cambridge Uni-
versity Press, Cambridge.
Brian Bowdle and Dedre Gentner. 2005. The career of
metaphor. Psychological Review, 112(1):193?216.
Lynne Cameron. 2003. Metaphor in Educational Dis-
course. Continuum, London.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Robert Entman. 2003. Cascading activation: Contesting
the white houses frame after 9/11. Political Communi-
cation, 20:415?432.
Matt Gedigan, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In PProceed-
ings of the 3rd Workshop on Scalable Natural Lan-
guage Understanding, pages 41?48, New York.
Sam Glucksbeg and Catrinel Haught. 2006. On the rela-
tion between metaphor and simile: When comparison
fails. Mind and Language, 21(3):360?378.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Alan Gross and Ray Dearin. 2003. Chaim Perelman.
Albany: SUNY Press.
Zoltan Ko?vecses. 2002. Metaphor: A Practical Intro-
duction. Oxford University Press.
Veronika Koller, Andrew Hardie, Paul Rayson, and Elena
Semino. 2008. Using a semantic annotation tool for
the analysis of metaphor in discourse. Metaphorik.de,
15:141?160.
George Lakoff and Mark Johnson. 1980. Metaphors we
live by. University of Chicago Press, Chicago.
George Lakoff and Zoltan Ko?vecses. 1987. Metaphors
of anger in japanese. In D. Holland and N. Quinn, edi-
tors, Cultural Models in Language and Thought. Cam-
bridge: Cambridge University Press.
George Lakoff, Jane Espenson, Adele Goldberg,
and Alan Schwartz. 1991. Master Metaphor
List, Second Draft Copy. Cognitive Linguisics
Group, Univeristy of California, Berkeley:
http://araw.mede.uic.edu/?alansz/metaphor/
METAPHORLIST.pdf.
George Lakoff. 1991. Metaphor and war: The metaphor
system used to justify war in the gulf. Peace Research,
23:25?32.
James Martin. 2006. A corpus-based analysis of context
effects on metaphor comprehension. In Anatol Ste-
fanowitsch and Stefan Gries, editors, Corpus-Based
Approaches to Metaphor and Metonymy. Berlin: Mou-
ton de Gruyter.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Cha??m Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Notre
Dame, Indiana: University of Notre Dame Press.
Translated by John Wilkinson and Purcell Weaver
from French original published in 1958.
Group Pragglejaz. 2007. MIP: A Method for Identifying
Metaphorically Used Words in Discourse. Metaphor
and Symbol, 22(1):1?39.
Astrid Reining and Birte Lo?neker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings of
the Workshop on Computational Approaches to Figu-
rative Language, pages 5?12, Rochester, New York.
Ekaterina Shutova and Simone Teufel. 2010. Metaphor
Corpus Annotated for Source-Target Domain Map-
pings. In Proceedings of LREC, Valetta, Malta.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2013. Statistical metaphor processing. Computational
Linguistics, 39(1).
Gerard Steen. 2008. The Paradox of Metaphor: Why
We Need a Three-Dimensional Model of Metaphor.
Metaphor and Symbol, 23(4):213?241.
19
Anatol Stefanowitsch. 2006. Corpus-based approaches
to metaphor and metonymy. In Anatol Stefanow-
itsch and Stefan Gries, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy. Berlin: Mouton
de Gruyter.
20
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 29?38,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Lexical Tightness and Text Complexity
Michael Flor Beata Beigman Klebanov Kathleen M. Sheehan
Educational Testing Service
Princeton, NJ, 08541, USA
{mflor,bbeigmanklebanov,ksheehan}@ets.org
Abstract
We present a computational notion of Lexical 
Tightness that measures global cohesion of con-
tent words in a text. Lexical tightness represents 
the degree to which a text tends to use words 
that are highly inter-associated in the language. 
We demonstrate the utility of this measure for 
estimating text complexity as measured by US 
school grade level designations of texts. Lexical 
tightness strongly correlates with grade level in 
a collection of expertly rated reading materials. 
Lexical  tightness  captures  aspects  of  prose 
complexity that are not covered by classic read-
ability indexes, especially for literary texts. We 
also present initial findings on the utility of this 
measure for automated estimation of complex-
ity for poetry.
1 Introduction
Adequate estimation of text complexity has a long 
and rich history.  Various readability metrics have 
been designed in the last 100 years (DuBay, 2004). 
Recent work on computational  estimation of text 
complexity for school- and college-level texts in-
cludes (Vajjala and Meurers 2012; Graesser et al, 
2011;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009; Heilman et al, 2006). Several commer-
cial  systems were recently evaluated in the Race 
To The Top competition (Nelson et al,  2012) in 
relation to the US Common Core State Standards 
for instruction (CCSSI, 2010). 
A variety of factors influence text  complexity, 
including vocabulary, sentence structure, academic 
orientation,  narrativity,  cohesion,  etc.  (Hiebert, 
2011)  and  corresponding  features  are  utilized  in 
automated  systems  of  complexity  evaluation
(Vajjala and Meurers, 2012; Graesser et al, 2011; 
Sheehan et al, 2010).
We focus on text complexity levels expressed as 
US school grade level equivalents1. Our interest is 
in  quantifying  the  differences  among  texts  (es-
say-length  reading  passages)  at  different  grade 
levels, for the purposes of automatically evaluating 
text complexity.  The work described in this paper 
is part of an ongoing project that investigates novel 
features indicative of text complexity.
The paper is organized as follows. Section 2.1 
presents our methodology for building word asso-
ciation profiles  for  texts.  Section 2.2 defines  the 
measure of lexical tightness (LT). Section 2.3 de-
scribes the datasets used in this study. Sections 3.1 
and  3.2  present  our  study  of  the  relationship 
between LT and text complexity.  Section 3.3 de-
scribes application to poetry. Section 3.4 evaluates 
an improved measure (LTR). Section 4 reviews re-
lated work.
2 Methodology
2.1 Word-Association Profile
We define WAPT ? a word association profile of a 
text T ? as the distribution of association values for 
all pairs of content words of text T, where the asso-
ciation values are estimated from a very large cor-
pus of texts. In this work, WAP is purely illustrat-
ive, and sets the stage for lexical tightness.
1 For age equivalents of grade levels see 
http://en.wikipedia.org/wiki/Educational_stage 
29
There exists an extensive literature on the use of 
word-association measures for NLP, especially for 
detection  of  collocations  (Pecina,  2010;  Evert, 
2008).  The  use  of  pointwise  mutual  information 
(PMI) with word-space models is noted in (Zhang 
et al, 2012; Baroni and Lenci, 2010; Mitchell and 
Lapata, 2008; Turney, 2001). We begin with PMI, 
and provide a modified measure in later sections.
To obtain comprehensive information about co-
occurrence behavior of words in English, we build 
a  first-order  co-occurrence  word-space  model 
(Turney  and  Pantel,  2010;  Baroni  and  Lenci, 
2010). The model was generated from a corpus of 
texts  of  about  2.5  billion  word  tokens,  counting 
non-directed co-occurrence in  a  paragraph,  using 
no  distance  coefficients  (Bullinaria  and  Levy, 
2007). About 2 billion word tokens come from the 
Gigaword  2003  corpus  (Graff  and  Cieri,  2003). 
Additional 500 million word tokens come from an 
in-house corpus containing texts from the genres of 
fiction and popular science. The matrix of 2.1x2.1 
million  word  types  and  their  co-occurrence  fre-
quencies, as well as single-word frequencies, is ef-
ficiently compressed using the TrendStream tech-
nology (Flor, 2013), resulting in a database file of 
4.7GB.  The  same  toolkit  allows  fast  retrieval  of 
word  probabilities  and  statistical  associations  for 
pairs of words.2 
In this study we use all content word tokens of a 
text.  We use the OpenNLP tagger3 to POS-tag a 
text and only take into account nouns, verbs, ad-
jective and adverbs.  We further  apply a stop-list 
(see Appendix A) to filter out auxiliary verbs.
To illustrate why WAP is an interesting notion, 
consider  this  toy  example:  The  texts  ?The  dog 
barked and wagged its tail? vs. ?Green ideas sleep  
furiously?. Their matrices of pairwise word associ-
ations are presented in Table 1. For the first text, 
all  the  six  content  word  pairs  score  above 
PMI=5.5.  On  the  other  hand,  for  ?Green  ideas 
sleep  furiously?,  all  the  six  content  word  pairs 
score below PMI=2.2. The first text puts together 
words that often go together in English, and this 
might be one of the reasons it seems easier to un-
derstand than the second text.
We use histograms to illustrate word-association 
profiles  for  real  texts,  containing  hundreds  of 
2 The distributional word-space model includes counts for 2.1 
million words and 1279 million word pairs (types). Associ-
ation measures are computed on the fly. 
3 http://opennlp.apache.org  
words.  For  a 60-bin histrogram spanning all  ob-
tained PMI values,  the  lowest  bin contains  pairs 
with PMI??5, the highest bin contains pairs with 
PMI>4.83, while the rest of the bins contain word 
pairs  (a,b)  with  -5<PMI(a,b)?4.83.  Figure  1 
presents  WAP  histograms  for  two  real  text 
samples, one for grade level 3 (age 8-9) and one 
for grade level 11 (age 16-17). We observe that the 
shape of distribution is normal-like. The distribu-
tion of GL3 text is shifted to the right ? it contains 
more highly associated word-pairs than the text of 
GL11.  In  a  separate  study  we  investigated  the 
properties of WAP distribution (Beigman-Kleban-
ov and Flor,  2013).  The normal-like  shape turns 
out to be stable across a variety of texts.
The dog barked and wagged its tail:
dog barked wagged tail
dog 7.02 7.64 5.57
barked 9.18 5.95
wagged 9.45
tail
Green ideas sleep furiously:
green ideas sleep furiously
green 0.44 1.47 2.05
ideas 1.01 0.94
sleep 2.18
furiously
Table 1. Word association matrices (PMI values) for 
two illustrative examples.
-5 -4 -3 -2 -1 0 1 2 3 4 5
0
1
2
3
4
5
6
7
8
9
10
TextGL11 TextGL3 PMI
Pe
rce
nta
ge
 of
 pa
irs
 of
 wo
rd 
tok
en
s
Figure  1.  Word  Association  Profiles  for  two  sample 
texts,  showing 60-bin histograms with smoothed lines 
instead of bars. The last bin of the histogram contains 
all pairs with PMI>4.83, hence the uptick at PMI=5.
30
2.2 Lexical Tightness
In this section we consider how to derive a single 
measure to represent each text for further analyses. 
Given the stable  normal-like  shape of  WAP,  we 
use average (mean) value per text for further in-
vestigations. We experimented with several associ-
ation measures.
Point-wise mutual information is defined as fol-
lows (Church and Hanks, 1990): 
PMI = log2 p ?a ,b ?p ?a? p ?b?
Normalized PMI (Bouma, 2009):
NPMI = 2 2( , )log log ( , )( ) ( )
p a b p a bp a p b
? ?
?? ?? ?
Unlike the standard PMI (Manning and Sch?tze, 
1999), NPMI has the property that its values are 
mostly constrained in the range {-1,1}, it is less in-
fluenced by rare extreme values, which is conveni-
ent  for  summing  values  over  multiple  pairs  of 
words.  Additional  experiments  on  our  data  have 
shown that ignoring negative NPMI values4.  works 
best.  Thus,  we  define  Positive  Normalized  PMI 
(PNPMI) for a pair of words  a and b as follows:
PNPMI(a,b) 
=  NPMI(a,b)  if NPMI(a,b)>0
=  0  if NPMI(a,b)?0
or if database has no data for 
co-occurrence of a and b.5
We define Lexical Tightness (LT) of a text as 
the mean value of PNPMI for all pairs of content-
word tokens in a text. Thus, if a text has N words, 
and after filtering we remain with K content words, 
the total number of pairs is K*(K-1)/2. 
Lexical tightness represents the degree to which 
a text tends to use words that are highly inter-asso-
ciated in the language. We conjecture that lexically 
tight texts (with higher values of LT) are easier to 
read  and  would  thus  correspond  to  lower  grade 
levels.
4 Ignoring negative values is described by Bullinaria and Levy 
(2007), also Mohammad and Hirst (2006).
5In our text collection, the average percentage of word-pairs 
not found in database is 5.5% per text.
2.3 Datasets
Our data consists of two sets of passages. The first 
set consists of 1012 passages (636K words) ? read-
ing materials that were used in various tests in state 
and national assessment  frameworks in the USA. 
Part of this set is taken from Sheehan et al (2007) 
(from testing programs and US state departments 
of education), and part was taken from the Standar-
dized State Test Passages set of the Race To The 
Top (RTT)  competition  (Nelson et  al.,  2012).  A 
distinguishing feature of this dataset is that the ex-
act grade level specification was available for each 
text. Table 2 provides the breakdown by grade and 
genre.  Text length in this set ranged between 27 
and 2848 words, with average 629 words. Average 
text length in the literary subset was 689 words and 
in the informational subset 560 words.
Grade
Level
Genre TotalInf Lit Other
1 2 4 1 7
2 2 4 3 9
3 49 63 10 122
4 54 77 8 139
5 47 48 15 110
6 44 43 6 93
7 39 61 6 106
8 73 66 19 158
9 25 25 3 53
10 29 52 2 83
11 18 25 0 43
12 47 20 22 89
Total 429 488 95 1012
Table 2. Counts of texts by grade level and genre, set #1 
Grade
Band GL
Genre TotalInf Lit Other
2?3 2.5 6 10 4 20
4?5 4.5 16 10 4 30
6?8 7 12 16 13 41
9?10 9.5 12 10 17 39
11+ ' 11.5 8 10 20 38
Total 54 56 58 168
Table  3. Counts of texts by grade band and genre, for 
dataset #2. GL specifies our grade level designation.
The second dataset comprises 168 texts (80.8K 
word  tokens)  from Appendix  B of  the  Common 
Core State Standards (CCSSI, 2010)6, not includ-
6 www.corestandards.org/assets/Appendix_B.pdf 
31
ing  poetry  items.  Exact  grade  level  designations 
are not  available for this set,  rather the texts are 
classified into grade bands, as established by ex-
pert  instructors  (Nelson  et  al.,  2012).  Table  3 
provides the breakdown by grade and genre. Text 
length  in  this  set  ranged  between  99  and  2073 
words,  with  average  481  words.  Average  text 
length in the literary subset was 455 words and in 
the informational subset 373 words.
Our  collection  is  not  very  large  in  terms  of 
typical datasets used in NLP research. However, it 
has two unique facets: grading and genres. Rather 
than having grade-ranges, set #1 has exact grade 
designations  for each text.  Moreover,  these  were 
rated by educational experts and used in state and 
nationwide testing programs. 
Previous research has emphasized the importan-
ce of genre effects for predicting readability and 
complexity (Sheehan et al, 2008) and for text ad-
aptation (Fountas and Pinnell, 2001). For all texts 
in our collection, genre designations (information-
al, literary, or 'other') were provided by expert hu-
man  judges  (we  used  the  designations  that  were 
prepared for the RTT competition,  Nelson et  al., 
2012). The 'other' category included texts that were 
somewhere in between literary and informational 
(e.g. biographies), as well as speeches, schedules, 
and manuals.
3 Results 
3.1 Lexical Tightness and Grade Level
Correlations of lexical tightness with grade level 
are shown in Table 4, for sets 1 and 2, the com-
bined set and for literary and informational subsets.
Our first finding is that lexical tightness has con-
siderable  and  statistically  significant  correlation 
with grade level, in each dataset, in the combined 
dataset  and  for  the  specific  subsets.  Notably the 
correlation  between  lexical  tightness  and  grade 
level is negative. Texts of higher grade levels are 
lexically less tight, as predicted.  
Although in these datasets grade level is mode-
rately correlated with text length, lexical tightness 
remains  considerably and significantly correlated 
with grade level even after removing the influence 
of correlations with text length.
Our second finding is that lexical tightness has a 
stronger correlation with grade level for the subset 
of literary texts (r=-0.610) than for informational 
texts (r=-0.499) in set #1. A similar pattern exists 
for set #2.
Figure 2 shows the average LT for each grade 
level,  for  texts  of  set  #1.  As the grade level  in-
creases,  average lexical tightness values decrease 
consistently, especially for informational and liter-
ary  texts.  There  are  two  'outliers'.  Informational 
texts for grade 12 show a sudden increase in lexic-
al tightness. Also, for genre 'other', grades 9,10,11 
are underepresented (see Table 2).
Subset N Correlation GL&length
Correlation 
GL&LT
Partial 
Correlation 
GL&LT
  Set #1
All 1012 0.362 -0.546 -0.472
Inf 429 0.396 -0.499 -0.404
Lit 488 0.408 -0.610 -0.549
  Set #2 (Common Core)
All 168 0.360 -0.441 -0.373
Inf 54 0.406 -0.313 -0.347
Lit 56 0.251 -0.546 -0.505
  Combined set
All 1180 0.339 -0.528 -0.462
Inf 483 0.386 -0.472 -0.369
Lit 544 0.374 -0.601 -0.545
Table  4.  Correlations  of  grade  level  (GL)  with  text 
length  and  lexical  tightness  (LT).  Partial  correlation 
GL&LT  controls  for  text  length.  All  correlations  are 
significant with p<0.04.
Figure 3 shows the average LT for each grade 
band, for texts of set #2. Here as well, decrease of 
lexical tightness is evident with increase of grade 
3 4 5 6 7 8 9 10 11 12
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 2. Lexical tighness by grade level and genre, 
for texts of grades 3-12 in dataset #1.
32
level. In this small set, informational texts show a 
relatively  smooth  decrease  of  LT,  while  literary 
texts  show a  sharp  decrease  of  LT in  transition 
from grade band 4-5 (4.5) to grade band 6-8 (7). 
Texts labelled as 'other' genre in set #2 are gener-
ally less 'tight' than literary or informational. Also 
for 'other' genre, bands 7-8, 9-10 and 11-12 have 
equal lexical tighness.
3.2 Grade Level and Readability Indexes
We have also calculated readability indexes for 
each passage in sets 1 and 2. We used well known 
readability formulae: Flesch-Kincaid Grade Level 
(FKGL: Kincaid et al, 1975), Flesch Reading Ease 
(FRE:  Flesch,  1948),  Gunning-Fog  Index  (GFI: 
Gunning, 19527), Coleman Liau Index (CLI: Cole-
man and Liau, 1975) and Automated Readability 
Index (ARI: Senter and Smith, 1967). All of them 
are based on measuring the length of words (in let-
ters  or  syllables)  and  length  of  sentences  (mean 
number  of  words).  For  our  collection,  we  also 
computed the average sentence length (avgSL, as 
word count),  average word frequency8 (avgWF ? 
over all  words),  and average word frequency for 
only  content  words  (avgWFCW).  Results  are 
shown in Table 5. 
Word frequency has quite low correlation with 
grade  level  in  both  datasets.  Readability  indexes 
7 Using the modern formula, as referenced at http://en.wikipe-
dia.org/wiki/Fog_Index 
8 For word frequency we use the unigrams data from the 
Google Web1T collection (Brants and Franz, 2006).
have a strong and consistent correlation with grade 
level.  For  dataset  #1,  readability  indexes  have 
much stronger correlation with grade level for in-
formational  texts  (|r| between  0.7  and  0.81)  as 
compared  to  literary  texts  (|r| between 0.53  and 
0.68), and a similar pattern is seen for dataset #2, 
with overall lower correlation.
The correlation of Flesch-Kincaid (FKGL) val-
ues with LT are  r=-0.444 for set #1,  r=-0.499 for 
the informational subset and  r=-0.541 for literary 
subset. The correlation is r=-0.182 in set #2. 
All Inf Lit
                  Set #1
N (texts): 1012 429 488
FKGL 0.705 0.807 0.673
FRE -0.658 -0.797 -0.629
GFI 0.701 0.810 0.673
CLI 0.537 0.722 0.537
ARI 0.670 0.784 0.653
avgSL 0.667 0.705 0.630
avgWF 0.205 0.128 0.249
avgWFCW 0.039 -0.039 0.095
                    Set #2 (Common Core)
N (texts): 168 54 56
FKGL 0.487 0.670 0.312
FRE9 -0.503 -0.586 -0.398
GFI 0.493 0.622 0.356
CLI 0.430 0.457 0.440
ARI 0.458 0.658 0.298
avgSL 0.407 0.701 0.203
avgWF 0.100 0.234 -0.109
avgWFCW 0.156 -0.053 -0.038
Table 5. Correlations of grade level with readability 
formulae and word frequency. All correlations apart 
from the italicized ones are significant with p<0.05. 
Abbreviations are explained in the text.
3.3 Lexical Tightness and Readability Indexes
To  evaluate  the  usefulness  of  LT  in  predicting 
grade level of passages, we estimate, using dataset 
#1, a linear regression model where the grade level 
is a dependent variable and Flesch-Kincaid score 
and lexical tightness are the two independent vari-
ables (features). First, we checked whether regres-
sion model improves over FKGL in the training set 
(#1). Then, we tested the regression model estim-
ated on 1012 texts of set #1, on 168 texts of set #2.
The  results  of  the  regression  model  on  1012 
texts  of  set  #1  (R2=0.565,  F(2,1009)=655.85, 
9 Flesch Reading Ease formula is inversely related to grade 
level, hence the negative correlations.
2.5 4.5 7 9.5 11.5
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 3. Lexical tighness by grade band and genre, 
for texts in dataset #2 (CommonCore).
33
p<0.0001)  indicate  that  the  amount  of  explained 
variance in the grade levels, as measured by the ad-
justed R2 of the model, improved from 0.497 (with 
FKGL alone,  multiple  r=0.705)  to  0.564 (FKGL 
with LT, r=0.752), that is an absolute improvement 
of 6.7%, and a relative improvement of 13.5%.
A separate regression model  was estimated on 
the  informational  texts  of  dataset  #1.  The  result 
(R2=0.664, F(2,426)=420.3, p<0.0001) reveals that 
adjusted  R2 of  the  model  improved  from  0.651 
(with FKGL alone, r=0.807) to 0.663 (FKGL with 
LT,  r=0.815).  Similarly,  a  regression  model  was 
estimated on the literary texts of set #1. The result 
(R2=0.522, F(2,485)=264.6, p<0.0001) reveals that 
adjusted R2 of the model improved from .453 (with 
FKGL alone,  r=0.673) to 0.520 (FKGL with LT, 
r=0.722). We observe that Flesch-Kincaid formula 
works well on informational texts, better than on 
literary  texts;  while  lexical  tightness  correlates 
with grade level in the literary texts better than it 
does in the informational texts. Thus, for informa-
tional texts, adding LT to FKGL provides a small 
(1.2%) but statistically significant improvement for 
predicting  GL.  For  literary  texts,  LT  provides  a 
considerable  improvement  (explaining  additional 
6.3% in the variance).
We use the regression model (FKGL & LT) es-
timated on the 1012 texts of set #1 and test it on 
168 texts of set #2. In dataset #2, FKGL alone cor-
relates with grade level with  r=0.487, and the es-
timated regression equation achieves correlation of 
r=0.574 (the difference between correlation coeffi-
cients  is  statistically  significant10,  p<0.001).  The 
amount of explained variance rises from 23.7% to 
33%,  an  almost  10%  improvement  in  absolute 
scores, and 39% relative improvement over FKGL 
readability index alone.
3.4 Analyzing Poetry
Since poetry is often included in school curricula, 
automated estimation of poem complexity can be 
useful. Poetry is notoriously hard to analyze com-
putationally. Many poems do not adhere to stand-
ard  punctuation  conventions,  have  peculiar  sen-
tence structure  (if  sentence boundaries are  indic-
ated at all). However, poems can be tackled with 
bag-of-words approaches. 
We have collected 66 poems from Appendix B 
of  the  Common  Core  State  Standards  (CCSSI, 
10Non-independent correlations test, McNemar (1955), p.148.
2010). Just as other materials from that source, the 
poems  are  classified  into  grade  bands,  as  estab-
lished by expert instructors. Table 6 provides the 
breakdown by grade band. Text length in this set 
ranges between 21 and 1100 words, the average is 
182, total word count is 12,030.
Grade Band GL N (texts)
K-1 1 12
2?3 2.5 15
4?5 4.5 9
6?8 7 11
9?10 9.5 7
11+ ' 11.5 12
Total 66
Table 6. Counts of poems by grade band, 
from Common Core Appendix B. 
GL specifies our grade level designation.
We computed lexical tightness for all 66 poems 
using the same procedure as for the two larger text 
collections. For computing correlations, texts from 
each grade band where assigned grade level as lis-
ted in Table 6. For the poetry dataset, LT has rather 
low  correlation  with  grade  level,  r=-0.271 
(p<0.002).  Text  length  correlation  with  GL  is 
r=0.218  (p<0.04).  Correlation  of  LT  and  text 
length is  r=-0.261 (p<0.02). Partial correlation of 
LT and GL, controlling for text length, is r=-0.227 
and only almost significant (p=0.069). In this data-
set,  the  correlation  of  Flesch-Kincaid  index 
(FKGL) with GL is r=0.291 (p<0.003) and Flesch 
Reading Ease (FRE)  has  a  stronger  correlation,  
r=-0.335 (p<0.003).
On examining some of the poems, we noted that 
the LT measure does not assign enough importance 
to recurrence of words within a text. For example, 
PNPMI(voice,  voice)  is  0.208,  while  the  ceiling 
value is 1.0. We modify the LT measure in the fol-
lowing way. Revised Association Score (RAS) for 
two words a and b:
=1.0   if a=b (token repetition)
RAS(a,b) =0.9  if a and b are inflectional variants of same lemma
= PNPMI(a,b)  otherwise
Revised Lexical Tightness (LTR) for  a text  is 
average of RAS scores for all accepted word pairs 
in the text (same filtering as before).
34
For the set of 66 poems, LTR moderately correl-
ates with grade level r=-0.353 (p<0.002). LTR cor-
relates  with  text  length  r=0.28  (p<0.02).  Partial 
correlation  of  LTR and  GL,  controlling  for  text 
length,  is  r=-0.312 (p<0.012).  This  suggests  that 
the revised measure captures some aspect of com-
plexity of the poems. 
We  re-estimated  the  regression  model,  using 
FRE readability and LTR, on all 1012 texts of set 
#1. We then applied this model  for prediction of 
grade levels  in  the  set  of  66  poems.  The model 
achieves  a  solid  correlation  with  grade  level, 
r=0.447 (p<0.0001). 
3.5 Revisiting Prose
We revisit the analysis of our two main datasets, 
set #1 and #2, using the revised lexical tightness 
measure  LTR.  Table  7  presents  correlations  of 
grade level with LT and LTR measures. Evidently, 
in each case LTR achieves better correlations. 
Subset N Correlation GL&LT
Correlation 
GL&LTR
  Set #1
All 1012 -0.546 -0.605
Inf 429 -0.499 -0.561
Lit 488 -0.610 -0.659
  Set #2 (Common Core)
All 168 -0.441 -0.492
Inf 54 -0.310 -0.336
Lit 56 -0.546 -0.662
  Combined set
All 1180 -0.528 -0.587
Inf 483 -0.472 -0.531
Lit 544 -0.601 -0.655
Table 7. Pearson correlations of grade level (GL) with 
lexical tightness (LT) and revised lexical tightness 
(LTR). All correlations are significant with p<0.04.
We re-estimated a linear regression model using 
the grade level as a dependent variable and Flesch-
Kincaid score (FKGL) and LTR as the two inde-
pendent variables. The results of regression model 
on  1012  texts  of  dataset  #1,  R2=0.583, 
F(2,1009)=706.07,  p<0.0001,  indicate  that  the 
amount of explained variance in the grade levels, 
as measured by the adjusted R2 of the model, im-
proved from 0.497 (with FKGL alone, r=0.705) to 
0.582 (FKGL with LTR, r=0.764), that is absolute 
improvement of 8.5%. For comparison, the regres-
sion model  with LT explained 0.564 of the vari-
ance, with 6.7% improvement over FKGL alone.
We re-estimated separate regression models for 
informational and literary subsets of set #1. For in-
formational  texts,  the  model  has  R2=0.667, 
F(2,426)=426.8,  p<0.0001,  R2 improving  from 
0.651 (with FKGL alone,  r=0.807) to adjusted R2 
0.666  (FKGL  with  LTR,  r=0.817).  Regression 
model with LT brought an improvement of 1.2%, 
the model with LTR provides 1.5%.
A regression model was estimated on the literary 
texts  of  dataset  #1.  The  result  (R2=0.560, 
F(2,485)=308.5, p<0.0001) reveals that adjusted R2 
of the  model  rose from .453 (with FKGL alone, 
r=0.673) to 0.558 (FKGL with LT,  r=0.748), that 
is 10.5% absolute improvement.  For comparison, 
LT brought 6.3% improvement. As with the origin-
al LT measure, LTR provides the bulk of improve-
ment for evaluation of literary texts.
The  regression  model  (FKGL  with  LTR), 
estimated on all 1012 texts of set #1, is tested on 
168  texts  of  set  #2.  In  set  #2,  FKGL  alone 
correlates with grade level with  r=0.487, and the 
prediction formula achieves correlation of r=0.585 
(the difference between correlation coefficients is 
statistically significant,  p<0.001).  The amount  of 
explained variance rises from 23.7% to 34.3%, that 
is 10.6% absolute improvement. Even better result 
of predicting grade level in set #2 is achieved using 
a  regression  model  of  Flesch  Readability  Ease 
(FRE) and LTR, estimated on all 1012 texts of set 
#1.  This  model  achieves  correlation  of r=0.616 
(p<0.0001) on the 168 texts of set #2, explaining 
37.9% of the variance. 
For  complexity  estimation,  in  both  proze  and 
poetry, LTR is more effective than simple LT.
4 Related Work 
Traditional readability formulae use a small num-
ber of surface features,  such as the average sen-
tence length (a proxy for syntactic complexity) and 
the average word length in syllables or characters 
(a  proxy to  vocabulary difficulty).  Such features 
are considered linguistically shallow, but they are 
surprisingly  effective  and  are  still  widely  used 
(DuBay, 2004;  ?tajner et al, 2012). The formulae 
or their features are incorporated in modern read-
ability classification systems (Vajjala and Meurers, 
2012;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009).
Developments  in  computational  linguistics  en-
abled inclusion of multiple features for capturing 
35
various  manifestations  of  text-related  readability. 
Peterson and Ostendorf (2009) compute a variety 
of features: vocabulary/lexical (including the clas-
sic 'syllables per word'), parse features, including 
average parse-tree height, noun-phrase count, verb-
phrase  count  and  average  count  of  subordinated 
clauses. They use machine learning to train classi-
fiers  for  direct  prediction of  grade level.  Vajjala 
and  Meurers  (2012)  also  use  machine  learning, 
with a wide variety of features, including classic 
features,  parse  features,  and  features  motivated 
from studies on second language acquisition, such 
as Lexical  Density and Type-Token Ratio.  Word 
frequency and its derivations, such as proportion of 
rare words, are utilized in many models of com-
plexity (Graesser et al, 2011; Sheehan et al 2010; 
Stenner et al, 2006; Collins-Thompson and Callan, 
2004).
Inspired by psycholinguistic research, two sys-
tems have explicitly set to measure textual cohe-
sion for estimations of readability and complexity: 
Coh-Metrix  (Graesser  et  al.,  2011)  and  Sour-
ceRater (Sheehan et al, 2010). One notion of cohe-
sion involved in those two systems is lexical cohe-
sion ? the amount of lexically/semantically related 
words in a text. Some amount of local lexical cohe-
sion can be measured via stem overlap of adjacent 
sentences, with averaging of such metric per text 
(McNamara et al, 2010). However, Sheehan et al 
(submitted) demonstrated that such measure is not 
well correlated with grade levels.
Perhaps closest to our present study is work re-
ported in Foltz et al (1998) and McNamara et al 
(2010). These studies used Latent Semantic Ana-
lysis,  which  reflects  second  order  co-occurrence 
associative relations, to characterize levels of lex-
ical similarity for pairs of adjacent sentences with-
in  paragraphs,  and  for  all  possible  pairs  of  sen-
tences  within  paragraphs.  McNamara  et  al.  have 
shown success in distinguishing lower and higher 
cohesion versions of the same text,  but  have not 
shown  whether  that  approach  systematically  ap-
plies for different texts and across grade levels.
Our study is a first demonstration that a measure 
of  lexical  cohesion  based  on  word-associations, 
and computed globally for the whole text, is an in-
dicative  feature  that  varies  systematically  across 
grade levels.
In the theoretical tradition, our work is closest in 
spirit to Michael Hoey?s theory of lexical priming 
(Hoey, 2005, 1991), positing that users of language 
internalize patterns of word co-occurrence and use 
them in reading, as well as when creating their own 
texts. We suggest that such patterns become richer 
with age and education, beginning with the most 
tight patterns at early age.
5 Conclusions 
In  this  paper  we  defined  a  novel  computational 
measure, lexical tightness. It represents the degree 
to which a text tends to use words that are highly 
inter-associated  in  the  language.  We  interpret 
lexical tightness as a measure of intra-text global 
cohesion.
This  study  presented  the  relationship  between 
lexical  tightness  and  text  complexity,  using  two 
datasets of reading materials (1180 texts in total), 
with  expert-assigned  grade  levels.  Lexical  tight-
ness has a significant correlation with grade levels: 
about  -0.6  overall.  The  correlation  is  negative: 
texts for lower grades are lexically tight, they use a 
higher  proportion  of  mildly  and  strongly  inter-
associated words; texts for higher grades are less 
tight, they use a lesser amount of inter-associated 
words.  The  correlation  of  lexical  tightness  with 
grade level is stronger for texts of the literary genre 
(fiction and stories) than for text belonging to in-
formational genre (expositional).
While lexical tightness is moderately correlated 
with  readability  indexes,  it  also  captures  some 
aspects of prose complexity that are not covered by 
classic  readability  indexes,  especially for  literary 
texts.  Regression analyses  on a  training set  have 
shown  that  lexical  tightness  adds  between  6.7% 
and 8.5% of explained grade level variance on top 
of  the  best  readability  formula.  The  utility  of 
lexical  tightness  was  confirmed  by  testing  the 
regression formula on a held out set of texts. 
Lexical  tightness  is  also moderately correlated 
with grade level (-0.353) in a small set of poems. 
In the same set,  Flesch Reading Ease readability 
formula  correlates  with  grade  level  at  -0.335.  A 
regression  model  using  that  formula  and  lexical 
tightness achieves correlation of  0.447 with grade 
level.  Thus we have shown that  lexical  tightness 
has good potential for analysis of poetry.
In future work, we intend to a) evaluate on lar-
ger datasets, and b) integrate lexical tightness with 
other  features  used  for  estimation  of  readability. 
We also intend to use this or a related measure for 
evaluation of writing quality.
36
References 
Baroni M. and Lenci A. 2010. Distributional Memory: 
A General Framework for Corpus-Based Semantics. 
Computational Linguistics, 36(4):673-721.
Beigman-Klebanov  B.  and  Flor  M.  2013.  Word 
Association  Profiles  and  their  Use  for  Automated 
Scoring of Essays. To appear in  Proceedings of the  
51th  Annual  Meeting  of  the  Association  for  
Computational Linguistics, ACL 2013.
Bouma  G.  2009.  Normalized  (Pointwise)  Mutual 
Information in Collocation Extraction. In:  Chiarcos, 
Eckart  de  Castilho  &  Stede  (eds), From  Form  to  
Meaning:  Processing  Texts  Automatically,  
Proceedings of the Biennial GSCL Conference 2009, 
31?40, Gunter Narr Verlag: T?bingen.
Brants T. and Franz A. 2006. ?Web 1T 5-gram Version 
1?.  LDC2006T13.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Bullinaria  J.  and  Levy  J.  2007.  Extracting  semantic 
representations from word co-occurrence statistics: A 
computational  study.  Behavior  Research  Methods, 
39:510?526.
Church K. and Hanks P. 1990. Word association norms, 
mutual information and lexicography. Computational  
Linguistics, 16(1):22?29.
Coleman,  M.  and  Liau,  T.  L.  1975.  A  computer 
readability  formula  designed  for  machine  scoring. 
Journal of Applied Psychology, 60:283-284.
Collins-Thompson K. and Callan J. 2004. A language 
modeling approach  to  predicting reading  difficulty. 
Proceedings of HLT / NAACL 2004, Boston, USA.
Common Core State Standards Initiative (CCSSI) 2010. 
Common core state standards for English language 
arts & literacy in history/social studies, science and 
technical subjects. Washington, DC: CCSSO & 
National Governors Association. 
http://www.corestandards.org/ELA-Literacy    
DuBay W.H. 2004. The principles of readability. Impact 
Information:  Costa Mesa,  CA.  http://www.impact-
information.com/impactinfo/readability02.pdf    
Evert S. 2008. Corpora and collocations. In A. L?deling 
and  M.  Kyt?  (eds.),  Corpus  Linguistics:  An  
International  Handbook,  article  58.  Mouton  de 
Gruyter: Berlin.
Flesch R. 1948. A new readability yardstick. Journal of  
Applied Psychology, 32:221-233.
Flor M. 2013. A fast and flexible architecture for very 
large word n-gram datasets. Natural Language 
Engineering, 19(1):61-93.
Foltz P.W., Kintsch W., and Landauer T.K. 1998. The 
measurement of textual coherence with Latent 
Semantic Analysis. Discourse Processes, 25:285-
307.
Fountas I. and Pinnell G.S. 2001. Guiding Readers and 
Writers, Grades 3?6. Heinemann, Portsmouth, NH.
Graesser, A.C., McNamara, D.S., and Kulikowich, J.M. 
Coh-Metrix: Providing Multilevel Analyses of Text 
Characteristics.  Educational  Researcher,  40(5): 
223?234.
Graff,  D.  and  Cieri,  C.  2003.  English  Gigaword.  
LDC2003T05.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Gunning  R.  1952.  The  technique  of  clear  writing. 
McGraw-Hill: New York.
Heilman,  M.,  Collins-Thompson,  K.,  Callan,  J.  and 
Eskenazi,  M.  2006.  Classroom  success  of  an 
intelligent  tutoring  system  for  lexical  practice  and 
reading comprehension. In  Proceedings of the Ninth  
International  Conference  on  Spoken  Language  
Processing, Pittsburgh, PA.
Hiebert,  E.H.  2011.  Using  multiple  sources  of  
information in establishing text complexity. Reading 
Research Report 11.03. TextProject Inc., Santa Cruz, 
CA.
Hoey  M.  1991.  Patterns  of  Lexis  in  Text.  Oxford 
University Press.
Hoey M. 2005. Lexical Priming: A new theory of words  
and language. Routledge, London.
Kincaid  J.P.,  Fishburne  R.P.  Jr,  Rogers  R.L.,  and 
Chissom B.S.  1975.  Derivation  of  new readability  
formulas   for  Navy  enlisted  personnel.  Research 
Branch  Report  8-75,  Millington,  TN:  Naval 
Technical  Training,  U.S.  Naval  Air  Station, 
Memphis, TN.
Manning,  C.  and  Sch?tze  H.  1999.  Foundations  of  
Statistical Natural Language Processing. MIT Press, 
Cambridge, MA.
McNamara,  D.S.,  Louwerse,  M.M.,  McCarthy,  P.M. 
and  Graesser  A.C.  2010.  Coh-metrix:  Capturing 
linguistic features of cohesion.  Discourse Processes, 
47:292-330.
McNemar,  Q.  1955.  Psychological  Statistics.  New 
York, John Wiley & Sons.
Mitchell J. and Lapata M.  2008. Vector-based models 
of semantic composition. In Proceedings of the 46th 
Annual  Meeting  of  the  Association  for  
Computational Linguistics, 236?244, Columbus, OH.
Mohammad  S.  and  Hirst  G.  2006.  Distributional 
Measures  of  Concept-Distance:  A  Task-oriented 
Evaluation. In  Proceedings of the 2006 Conference  
on  Empirical  Methods  in  Natural  Language  
Processing (EMNLP 2006), 35?43.
Nelson  J.,  Perfetti  C.,  Liben  D.,  and Liben  M. 2012. 
Measures of Text Difficulty: Testing their Predictive 
Value  for  Grade  Levels  and  Student  Performance. 
Student  Achievement  Partners.  Available  from 
http://www.ccsso.org/Documents/2012/Measures
%20ofText%20Difficulty_final.2012.pd  f   
Pecina  P.  2010.  Lexical  association  measures  and 
collocation  extraction.  Language  Resources  & 
Evaluation, 44:137?158.
37
Petersen  S.E.  and  Ostendorf  M.  2009.  A  machine 
learning  approach  to  reading  level  assessment. 
Computer Speech and Language, 23: 89?109.
Senter  R.J.  and  Smith  E.A.  1967.  Automated 
Readability Index. Report AMRL-TR-6620. Wright-
Patterson Air Force Base, USA.
Sheehan K.M.,  Kostin I.,  Napolitano D.,  and Flor  M. 
TextEvaluator:  Helping  Teachers  and  Test 
Developers  Select  Texts for  Use in Instruction and 
Assessment.  Submitted  to  The  Elementary  School  
Journal (Special Issue: Text Complexity).  
Sheehan K.M., Kostin I., Futagi Y., and Flor M. 2010. 
Generating automated text complexity classifications 
that  are  aligned  with  targeted  text  complexity 
standards. (ETS RR-10-28). ETS, Princeton, NJ.
Sheehan K.M., Kostin I., and Futagi Y. 2008. When do 
standard  approaches  for  measuring  vocabulary 
difficulty,  syntactic  complexity  and  referential 
cohesion yield biased estimates of text difficulty? In 
B.C.  Love,  K.  McRae,  &  V.M.  Sloutsky  (eds.), 
Proceedings  of  the 30th Annual  Conference  of  the  
Cognitive Science Society, Washington DC.
Sheehan  K.M.,  Kostin  I.,  and  Futagi  Y.  2007. 
SourceFinder:  A  construct-driven  approach  for 
locating  appropriately  targeted  reading 
comprehension  source  texts.  In  Proceedings  of  the  
2007  workshop  of  the  International  Speech  
Communication Association,  Special  Interest  Group 
on Speech and Language Technology in Education, 
Farmington, PA.
?tajner S., Evans R., Or?san C., and Mitkov R. 2012. 
What  Can  Readability  Measures  Really  Tell  Us 
About Text Complexity? In proceedings of workshop 
on   Natural  Language  Processing  for  Improving  
Textual Accessibility (NLP4ITA 2012), 14-22.
Stenner A.J.,  Burdick H., Sanford E., and Burdick D. 
2006.  How  accurate  are  Lexile  text  measures? 
Journal of Applied Measurement, 7(3):307-322.
Turney  P.D.  2001.  Mining  the  Web  for  Synonyms: 
PMI-IR versus  LSA on TOEFL.  In  proceedings  of 
European  Conference  on  Machine  Learning,  491?
502, Freiburg, Germany.
Turney  P.D.  and  Pantel  P.  2010.  From Frequency  to 
Meaning:  Vector  Space  Models  of  Semantics. 
Journal  of  Artificial  Intelligence  Research,  37:141-
188.
Vajjala  S.  and  Meurers  D.  2012.  On  Improving  the 
Accuracy of Readability Classification using Insights 
from Second Language Acquisition. In  proceedings 
of  The 7th Workshop on the Innovative Use of NLP  
for  Building  Educational  Applications,  (BEA-7), 
163?173, ACL.
Zhang  Z.,  Gentile  A.L.,  Ciravegna  F.  2012.  Recent 
advances in methods of lexical semantic relatedness 
?  a  survey.  Natural  Language  Engineering,  DOI: 
http://dx.doi.org/10.1017/S1351324912000125   
Appendix A
The list of stopwords utilized in this study:
a, an, the, at, as, by, for, from, in, on, of, off, up,  
to, out, over, if, then, than, with, have, had, has,  
can,  could,  do,  did,  does,  be,  am,  are,  is,  was,  
were, would, will,  it,  this,  that,  no, not,  yes, but,  
all,  and,  or,  any,  so,  every,  we,  us,  you,  also,  s
Note that most of these words would be excluded 
by POS filtering. However, the full  stop list  was 
applied anyway.
38
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 49?58,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
A Two-Stage Approach for Generating Unbiased                                                   
Estimates of Text Complexity 
 
 
Kathleen M. Sheehan Michael Flor Diane Napolitano 
   
Educational Testing Service 
Princeton, NJ, USA 
{ksheehan, mflor, dnapolitano}@ets.org 
 
 
 
 
 
 
Abstract 
Many existing approaches for measuring text 
complexity tend to overestimate the complexi-
ty levels of informational texts while simulta-
neously underestimating the complexity levels 
of literary texts. We present a two-stage esti-
mation technique that successfully addresses 
this problem.  At Stage 1, each text is classi-
fied into one or another of three possible ge-
nres:  informational, literary or mixed.  Next, 
at Stage 2, a complexity score is generated for 
each text by applying one or another of three 
possible prediction models:  one optimized for 
application to informational texts, one opti-
mized for application to literary texts, and one 
optimized for application to mixed texts.  
Each model combines lexical, syntactic and 
discourse features, as appropriate, to best rep-
licate human complexity judgments. We dem-
onstrate that resulting text complexity 
predictions are both unbiased, and highly cor-
related with classifications provided by expe-
rienced educators. 
1 Introduction 
Automated text analysis systems, such as reada-
bility metrics, are frequently used to assess the 
probability that texts with varying combinations of 
linguistic features will be more or less accessible to 
readers with varying levels of reading comprehen-
sion skill (Stajner, Evans, Orasan and Mitkov, 
2012).  This paper introduces TextEvaluator, a ful-
ly-automated text analysis system designed to faci-
litate such work.1
 Our approach for addressing these differences 
can be summarized as follows.  First, a large set of 
lexical, syntactic and discourse features is ex-
tracted from each text.  Next, either human raters, 
or an automated genre classifier is used to classify 
each text into one or another of three possible ge-
nre categories: informational, literary, or mixed.  
Finally, a complexity score is generated for each 
text by applying one or another of three possible 
prediction models: one optimized for application to 
informational texts, one optimized for application 
to literary texts, and one optimized for application 
to mixed texts. We demonstrate that resulting 
complexity measures are both unbiased, and highly 
correlated with text grade level (GL) classifications 
provided by experienced educators.  
 TextEvaluator successfully 
addresses an important limitation of many existing 
readability metrics:  the tendency to over-predict 
the complexity levels of informational texts, while 
simultaneously under-predicting the complexity 
levels of literary texts (Sheehan, Kostin & Futagi, 
2008; Sheehan, Kostin, Futagi & Flor, 2010). We 
illustrate this phenomenon, and argue that it results 
from two fundamental differences between infor-
mational and literary texts:  (a) differences in the 
way that common every-day words are used and 
combined; and (b) differences in the rate at which 
rare words are repeated.  
                                                          
1 TextEvaluator was previously called SourceRater. 
49
Our paper is organized as follows.  Section 2 
summarizes related work on readability assess-
ment. Section 3 describes the two corpora assem-
bled for use in this study, and outlines how genre 
and GL classifications were assigned.  Section 4 
illustrates the problem of genre bias by considering 
the specific biases detected in two widely-used 
readability metrics.  Section 5 describes the Text- 
Evaluator features, methods and results. Section 6 
presents a summary and discussion. 
2    Related Work  
Despite the large numbers of text features that may 
potentially contribute to the ease or difficulty of 
comprehending complex text, many widely-used 
readability metrics are based on extremely limited 
feature sets.  For example, the Flesch-Kincaid GL 
score (Kincaid, et al, 1975), the FOG Index (Gun-
ning, 1952), and the Lexile Framework (Stenner, et 
al., 2006) each consider just two features: a single 
measure of syntactic complexity (average sentence 
length) and a single measure of lexical difficulty 
(either average word length in syllables, average 
frequency of multi-syllable words, or average word 
familiarity estimated via a word frequency, WF, 
index).  
Recently, more computationally sophisticated 
modeling techniques such as Statistical Language 
Models (Si and Callan, 2001; Collins-Thompson 
and Callan, 2004, Heilman, et al, 2007, Pitler and 
Nenkova, 2008), Support Vector Machines 
(Schwarm and Ostendorf, 2005), Principal Com-
ponents Analyses (Sheehan, et al, 2010) and Mul-
ti-Layer Perceptron classifiers (Vajjala and 
Meurers, 2012) have enabled researchers to inves-
tigate a broader range of potentially useful fea-
tures.  For example: Schwarm and Ostendorf 
(2005) demonstrated that vocabulary measures 
based on trigrams were effective at distinguishing 
articles targeted at younger and older readers; Pit-
ler and Nenkova (2008) reported improved validity 
for measures based on the likelihood of vocabulary 
and the likelihood of discourse relations; and Vaj-
jala and Meurers (2012) demonstrated that features 
inspired by Second Language Acquisition research 
also contributed to validity improvements.  Impor-
tantly, however, while this research has contributed 
to our understanding of the types of text features 
that may cause texts to be more or less compre-
hensible, evaluations focused on the presence and 
degree of genre bias have not been reported. 
3   Corpora   
Two text collections are considered in this re-
search.  Our training corpus includes 934 passages 
selected from a set of previously administered 
standardized assessments constructed to provide 
valid and reliable feedback about the types of ver-
bal reasoning skills described in U.S. state and na-
tional assessment frameworks. Human judgments 
of genre (informational, literary or mixed) and GL 
(grades 3-12) were available for all texts.  Genre 
classifications were based on established guide-
lines which place texts structured to inform or per-
suade (e.g., newspaper text, excerpts from science 
or social studies textbooks) in the informational 
category, and texts structured to provide a reward-
ing literary experience (e.g., folk tales, short sto-
ries, excerpts from novels) in the literary category 
(see American Institutes for Research, 2008). We 
added a Mixed category to accommodate texts 
classified as incorporating both informational and 
literary elements.  Nelson, Perfetti, Liben and Li-
ben (2012) describe an earlier, somewhat smaller 
version of this dataset.  We added additional pas-
sages downloaded from State Department of Edu-
cation web sites, and from the National 
Assessment of Educational Progress (NAEP).  In 
each case, GL classifications reflected the GLs at 
which passages were administered to students.  
Thus, all passages classified at Grade 3 appeared 
on high-stakes assessments constructed to provide 
evidence of student performance relative to Grade 
3 reading standards.  
Two important characteristics of this dataset 
should be noted.  First, unlike many previous cor-
pora, (e.g., Stenner, et al, 2006; Zeno, et al, 2005) 
accurate paragraph markings are included for all 
texts. Second, while many of the datasets consi-
dered in previous readability research were com-
prised entirely of informational text (e.g., Pitler 
and Nenkova, 2008; Schwarm and Ostendorf, 
2005;  Vajjala and Meurers, 2012) the current da-
taset covers the full range of text types considered 
by teachers and students in U.S. classrooms.   
Table 1 shows the numbers of informational, li-
terary and mixed training passages at each targeted 
GL.  Passage lengths ranged from 112 words at 
Grade 3, to more than 2000 words at Grade 12. 
50
Average passage lengths were 569 words and 695 
words in the informational and literary subsets, 
respectively.  
 
Grade 
Level 
Genre  
Total Inf. Lit. Mixed 
3 46 60 8 114 
4 51 74 7 132 
5 44 46 12 102 
6 41 40 6 87 
7 36 58 6 100 
8 70 63 18 151 
9 23 23 2 48 
10 26 49 2 77 
11 15 24 0 39 
12 47 15 22 84 
Total 399 452 83 934 
 
Table 1.  Numbers of passages in the model develop-
ment/training dataset, by grade level and genre.  
 
A validation dataset was also constructed.  It in-
cludes the 168 texts that were published as Appen-
dix B of the new Common Core State Standards 
(CCSSI, 2010), a new standards document that has 
now been adopted in 46 U.S. states. Individual 
texts were contributed by teachers, librarians, cur-
riculum experts, and reading researchers.  GL clas-
sifications are designed to illustrate the ?staircase 
of increasing complexity? that teachers and test 
developers are being encouraged to replicate when 
selecting texts for use in K-12 instruction and as-
sessment in the U.S.  The staircase is specified in 
terms of five grade bands:  Grades 2-3, Grades 4-5, 
Grades 6-8, Grades 9-10 or Grades 11+.  Table 2 
shows the numbers of informational, literary and 
?Other? texts (includes both Mixed and speeches) 
included at each grade band.   
 
Grade 
Band 
Genre  
Total Inf. Lit. Other 
2-3 6 10 4 20 
4-5 16 10 4 30 
6-8 12 16 13 41 
9-10 12 10 17 39 
11+ 8 10 20 38 
Total 54 56 58 168 
 
Table 2.  Numbers of passages in the validation dataset, 
by grade band and genre. 
4   Genre Bias 
   
This section examines the root causes of genre bi-
as. We focus on two fundamental differences be-
tween informational and literary texts: differences 
in the types of vocabularies employed, and differ-
ences in the rate at which rare words are repeated.  
These differences have been examined in several 
previous studies.  For example, Lee (2001) docu-
mented differences in the use of ?core? vocabulary 
within a corpus of informational and literary texts 
that included over one million words downloaded 
from the British National Corpus.  Core vocabulary 
was defined in terms of a list of 2000 common 
words classified as appropriate for use in the dic-
tionary definitions presented in the Longman Dic-
tionary of Contemporary English.  The analyses 
demonstrated that core vocabulary usage was high-
er in literary texts than in informational texts.  For 
example, when literary texts such as fiction, poetry 
and drama were considered, the percent of total 
words classified as ?core? vocabulary ranged from 
81% to 84%.  By contrast, when informational 
texts such as science and social studies texts were 
considered, the percent of total words classified as 
?core? vocabulary ranged from 66% to 71%.  In 
interpreting these results Lee suggested that the 
creativity and imaginativeness typically associated 
with literary writing may be less closely tied to the 
type or level of vocabulary employed and more 
closely tied to the way that core words are used 
and combined.  Note that this implies that an indi-
vidual word detected in a literary text may not be 
indicative of the same level of processing chal-
lenge as that same word detected in an informa-
tional text. 
Differences in the vocabularies employed within 
informational and literary texts, and subsequent 
impacts on readability metrics, are also discussed 
in Appendix A of the Common Core State Stan-
dards (CCSSI, 2010).  The tendency of many exist-
ing readability metrics to underestimate the 
complexity levels of literary texts is described as 
follows: ?The Lexile Framework, like traditional 
formulas, may underestimate the difficulty of texts 
that use simple, familiar language to convey so-
phisticated ideas, as is true of much high-quality 
fiction written for adults and appropriate for older 
students? (p. 7).  
Genre bias may also result from genre-specific 
differences in word repetition rates.  Hiebert and 
51
Mesmer (2013, p.46) describe this phenomenon as 
follows:  ?Content area texts often receive inflated 
readability scores since key concept words that are 
rare (e.g., photosynthesis, inflation) are often re-
peated which increases vocabulary load, even 
though repetition of content words can support 
student learning (Cohen & Steinberg, 1983)?.  
Table 3 provides empirical evidence of these 
trends.  The table presents mean GL classifications 
estimated conditional on mean WF scores, for the 
informational (n = 399) and literary (n = 452) pas-
sages in our training dataset.  WF scores were gen-
erated via an in-house WF index constructed from 
a corpus of more than 400 million word tokens.  
The corpus includes more than 17,000 complete 
books, including both fiction and nonfiction titles.   
 
 
Avg. WF 
Informational Literary 
N GL SD N GL SD 
51.0?52.5 2 12.0 0.0 0 -- -- 
52.5?54.0 16 10.8 1.9 0 -- -- 
54.0?55.5 68 9.6 2.0 1 10.0 -- 
55.5?57.0 89 7.8 2.7 18 9.9 1.9 
57.0?58.5 96 6.6 2.3 46 9.2 2.0 
58.5?60.0 78 5.3 1.8 92 7.6 2.4 
60.0?61.5 44 4.6 1.8 142 6.2 2.4 
61.5?63.0 6 3.7 0.8 119 5.5 2.1 
63.0?64.5 0 -- -- 31 4.5 1.9 
64.5?66.0 0 -- -- 3 4.0 1.7 
Total 399 57.4 2.1 452 60.6 1.9 
 
Table 3.  Mean GL classifications, by Average WF 
score, for informational and literary passages targeted at 
readers in grades 3 through 12.   
 
The results in Table 3 confirm that, consistent 
with expectations, texts with lower average WF 
scores are more likely to appear on assessments 
targeted at older readers, while texts with higher 
average WF scores are more likely to appear on 
assessments targeted at younger readers.  But note 
that large genre differences are also present. Figure 
1 provides a graphical representation of these 
trends.  Results for informational texts are plotted 
with a solid line; those for literary texts are plotted 
with a dashed line. Note that the literary curve ap-
pears above the informational curve throughout the 
entire observed range of the data. This suggests 
that a given value of the Average WF measure is 
indicative of a higher GL classification if the text 
in question is a literary text, and a lower GL classi-
fication if the text in question is an informational 
text. Since a readability measure that includes this 
feature (or a feature similar to this feature) without 
also accounting for genre effects will tend to yield 
predictions that fall between the two curves, result-
ing GL predictions will tend to be too high for in-
formational texts (positive bias) and too low for 
literary texts (negative bias).   
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Figure 1.  Mean text GL plotted conditional on average 
WF score. (One literary mean score based on evidence 
from a single text is not plotted.) 
 
Figure 2 confirms that this evidence-based pre-
diction holds true for two widely-used readability 
metrics: the Flesch-Kincaid GL score and the Lex-
ile Framework2
                                                          
2 All Lexile scores were obtained via the Lexile Analyzer 
available at www.lexile.com. Scores are only available for a 
subset of texts since our training corpus included just 548 
passages at the time that these data were collected. Corres-
ponding human GL classifications were approximately evenly 
distributed across grades 3 through 12. 
. Each individual plot compares 
Flesch-Kincaid GL scores (top row), or Lexile 
scores (bottom row) to the human GL classifica-
tions stored in our training dataset, i.e., classifica-
tions that were developed and reviewed by 
experienced educators, and were subsequently used 
to make high-stakes decisions about students and 
teachers, e.g., requiring students to repeat a grade 
rather than advancing to the next GL. The plots 
confirm that, in each case, the predicted pattern of 
over- and under-estimation is present. That is, on 
average, both Flesch-Kincaid scores and Lexile 
scores tend to be slightly too high for informational 
texts, and slightly too low for literary texts, thereby 
calling into doubt any cross-genre comparisons. 
Average ETS Word Frequency
M
ea
n 
G
ra
de
 L
ev
el
52 54 56 58 60 62 64 66
4
6
8
10
12 Literary
Informational
 
52
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Informational (n = 243)
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Literary (n = 305)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Passage complexity scores generated via the 
Flesch-Kincaid GL score (top) and the Lexile Frame-
work (bottom) compared to GL classifications provided 
by experienced educators. 
5  Features, Components and Results 
5.1 Features 
 
The TextEvaluator feature set is designed to 
measure the ease or difficulty of implementing 
four types of processes believed to be critically 
involved in comprehending complex text: (1) 
processes involved in word recognition and decod-
ing, (2) processes associated with using relevant 
syntactic knowledge to assemble words into mea-
ningful propositions, (3) processes associated with 
inferring connections across propositions or larger 
sections of text, and (4) processes associated with 
using relevant prior knowledge and experience to 
develop a more complete, more integrated mental 
representation of a text. (See Kintsch, 1998). 
A total of 43 candidate features were developed. 
Since many of these were expected to be moderate-
ly inter-correlated, a Principal Components Analy-
sis (PCA) was used to locate clusters of features 
that exhibited high within-cluster correlation and 
low between-cluster correlation.  Linear combina-
tions defined in terms of the resulting feature clus-
ters provided the independent variables considered 
in subsequent investigations.  Biber and his col-
leagues (2004) justify this approach by noting that, 
because many important aspects of text variation 
are not well captured by individual linguistic fea-
tures, investigation of such characteristics requires 
a focus on ?constellations of co-occurring linguis-
tic features? as opposed to individual features (p. 
45). 
The PCA suggested that more than 60% of the 
variation captured by the full set of 43 features 
could be accounted for via a set of eight compo-
nent scores, where each component is estimated as 
a linear combination of multiple correlated fea-
tures, and only 3 of the 43 features had moderately 
high loadings on more than one component, and 
most loadings exceeded 0.70.  The individual fea-
tures comprising each component are described 
below.  
Component #1:  Academic Vocabulary.  Ten 
features loaded heavily on this component.  Two 
are based on the Academic Word List described in 
Coxhead (2000). These include:  the frequency per 
thousand words of all words on the Academic 
Word List, and the ratio of listed words to total 
words.  In a previous study, Vajjala and Meurers 
(2012)  demonstrated that the ratio of listed words 
to total wards was very effective at distinguishing 
texts at lower and higher levels in the Weekly 
Reader corpus. Two additional features focus on 
the frequency of nominalizations, including one 
estimated from token counts and one estimated 
from type counts. Four additional features are 
based on word lists developed by Biber and his 
colleagues.  These include the frequency per thou-
sand words of academic verbs, abstract nouns, top-
ical adjectives and cognitive process nouns (see 
Biber, 1986, 1988; and Biber, et al, 2004). Two 
measures of word length also loaded on this di-
mension:  average word length measured in syl-
lables, and the frequency per thousand words of 
words containing more than 8 characters.  
Component #2:  Syntactic Complexity. Seven 
features loaded heavily on this component. These 
include features determined from the output of the 
Stanford  Parser (Klein and Manning, 2003), as 
well as more easily computed measures such as 
average sentence length, average frequency of long 
sentences (>= 25 words), and average number of 
53
words between punctuation marks (commas, semi-
colons, etc.).  Parse-based features include average 
number of dependent clauses, and an automated 
version of the word ?depth? measure introduced by 
Yngve (1960). This last feature, called Average 
Maximum Yngve Depth, is designed to capture 
variation in the memory load imposed by sentences 
with varying syntactic structures. It is estimated by 
first assigning a depth classification to each word 
in the text, then determining the maximum depth 
represented within each sentence, and then averag-
ing over resulting sentence-level estimates to ob-
tain a passage-level estimate.  Several studies of 
this word depth measure have been reported. For 
example, Bormuth (1964) reported a correlation of 
-0.78 between mean word depth scores and cloze 
fill-in rates provided by Japanese EFL learners.  
Component #3:  Concreteness. Words that are 
more concrete are more likely to evoke meaningful 
mental images, a response that has been shown to 
facilitate comprehension (Coltheart, 1981). Alder-
son (2000) argued that the level of concreteness 
present in a text is a useful feature to consider 
when evaluating passages for use on reading as-
sessments targeted at L2 readers. A total of five 
concreteness and imageability measures loaded 
heavily on this dimension.  All five measures are 
based on concreteness and imageability ratings 
downloaded from the MRC psycholinguistic data-
base (Coltheart, 1981).  Ratings are expressed on a 
7 point scale with 1 indicating least concrete, or 
least imageable, and 7 indicating most concrete or 
most imageable.   
Component #4:  Word Unfamiliarity. This com-
ponent summarizes variation detected via six dif-
ferent features.  Two features are measures of 
average word familiarity: one estimated via our in-
house WF Index, and one estimated via the TASA 
WF Index (see Zeno, et al, 1995).  Both features 
have negative loadings, suggesting that the com-
ponent is measuring vocabulary difficulty as op-
posed to vocabulary easiness. The other features 
with high loadings on this component are all meas-
ures of rare word frequency. These all have posi-
tive loadings since texts with large numbers of rare 
words are expected to be more difficult. Two types 
of rare word indices are included: indices based on 
token counts and indices based on type counts. 
Vocabulary measures based on token counts view 
each new word as an independent comprehension 
challenge, even when the same word occurs re-
peatedly throughout the text. By contrast, vocabu-
lary measures based on type counts assume that a 
passage containing five different unfamiliar words 
may be more challenging than a passage contain-
ing the same unfamiliar word repeated five times. 
This difference is consistent with the notion that 
each repetition of an unknown word provides an 
additional opportunity to connect to prior know-
ledge (Cohen & Steinberg, 1983).  
Component #5:  Interactive/Conversational 
Style.  This component includes the frequency per 
thousand words of:  conversation verbs, fiction 
verbs, communication verbs, 1st person plural pro-
nouns, contractions, and words enclosed in quotes.  
Verb types were determined from one or more of 
the following studies: Biber (1986),  Biber (1988), 
and Biber, et al (2004).   
Component #6:  Degree of Narrativity. Three 
features had high positive loadings on this dimen-
sion:  Frequency of past perfect aspect verbs, fre-
quency of past tense verbs and frequency of 3rd 
person singular pronouns.  All three features have 
previously been classified as providing positive 
evidence of the degree of narrativity exhibited in a 
text (see Biber, 1986 and Biber, 1988). 
Component #7:  Cohesion. Cohesion is that 
property of a text that enables it to be interpreted as 
a ?coherent message? rather than a collection of 
unrelated clauses and sentences.  Halliday and Ha-
san (1976) argued that readers are more likely to 
interpret a text as a ?coherent message?  when cer-
tain observable features are present.  These include 
repeated content words and explicit connectives.  
The seventh component extracted in the PCA in-
cludes three different types of cohesion features.  
The first two features measure the frequency of 
content word repetition across adjacent sentences 
within paragraphs. These measures differ from the 
cohesion measures discussed in Graesser et al 
(2004) and in Pitler and Nenkova (2008) in that a 
psychometric linking procedure is used to ensure 
that results for different texts are reported on com-
parable scales (See Sheehan, in press).  The fre-
quency of causal conjuncts (therefore, 
consequently, etc.) also loads on this dimension. 
Component #8:  Argumentation.  Two features 
have high loadings on this dimension:  the fre-
quency of concessive and adversative conjuncts 
(although, though, alternatively, in contrast, etc.), 
and the frequency of negations (no, neither, etc.), 
Just and Carpenter, (1987).  
54
5.2  An Automated Genre Classifier 
 
A preliminary automated genre classifier was 
developed by training a logistic regression model 
to predict the probability that a text is classified as 
informational  as opposed to literary.  A signifi-
cant positive coefficient was obtained for the Aca-
demic Vocabulary component defined above, 
suggesting that a high score on this component 
may be interpreted as an indication that the text is 
more likely to be informational.  Significant nega-
tive coefficients were obtained for Narrativity, In-
teractive/Conversational Style, and Syntactic 
Complexity, indicating that a high score on any of 
these components may be interpreted as an indica-
tion that the text is more likely to be literary.  Two 
individual features that were not included in the 
PCA were also significant:  the proportion of adja-
cent sentences containing at least one overlapping 
stemmed content word, and the frequency of 1st 
person singular pronouns.  These features were not 
included in the PCA because they are not reliably 
indicative of differences in text complexity (See 
Sheehan, in press; Pitler and Nenkova, 2008.) Re-
sults confirmed, however, that these features are 
useful for predicting a text?s genre classification.  
Alternative decision rules based on this model 
were investigated. Table 4 summarizes the levels 
of precision (P), recall (R) and F1 = 2RP/(R+P) 
obtained for the selected decision rule which was 
defined as follows: Classify as informational if 
P(Inf) >= 0.52, classify as literary if P(inf) < 0.48, 
else classify as mixed. This decision rule is defined 
such that few texts are classified into the mixed 
category since, at present, the training dataset in-
cludes very few mixed texts. The table shows de-
creased precision in the Validation dataset since 
many more mixed texts are included, and the ma-
jority of these were classified as informational. 
 
Dataset Genre N R P F1 
Training Inf 399 .84 .79 .81 
Training Lit 452 .88 .79 .83 
Training Mixed 83 .01 .09 .01 
Validation Inf 67 .91 .56 .69 
Validation Lit 56 .80 .80 .80 
Validation Mixed 45 .07 1.0 .13 
 
Table 4.  Levels of Precision, Recall and F1 obtained for 
1, 089 texts in the training and validation datasets.  
Speeches are not included in this summary. 
5.3  Prediction Equations 
 
We use separate genre-specific regression mod-
els to generate GL predictions for texts classified 
as informational, literary, or mixed. The coeffi-
cients estimated for informational and literary texts 
are shown in Table 5. Note that each component is 
significant in one or both models.  The table also 
highlights key genre differences. For example, note 
that the Interactive/Conv. Style score is significant 
in the Inf. model but not in the Literary model.  
This reflects the fact that, while literary texts at all 
GLs tend to exhibit relatively high interactivity, 
similarly high interactivity among inf. texts tends 
to only be present at the lowest GLs.  Thus, a high 
Interactivity is an indication of low complexity if 
the text in question is an informational text, but 
provides no statistically significant evidence about 
complexity if the text in question is a literary text.  
 
Component Informational Literary 
Academic Voc. 1.126* .824* 
Word Unfamiliarity .802* .793* 
Word Concreteness -.610* -.483* 
Syn. Complexity .983* 1.404* 
Lexical Cohesion -.266* -.440* 
Interactive/Conv. Style -.518* ns 
Degree of Narrativity ns -.361* 
Argumentation .431* ns 
 
Table 5. Regression coefficients estimated from training 
texts.  *p < .01, ns = not significant. 
 
5.4  Validity Evidence 
 
Two aspects of system validity are of interest: 
(a) whether genre bias is present, and (b) whether 
complexity scores correlate well with judgments 
provided by professional educators, i.e., the educa-
tors involved in selecting texts for use on high-
stakes state reading assessments. The issue of ge-
nre bias is addressed in Figure 3. Each plot com-
pares GL predictions generated via TextEvaluator 
to GL predictions provided by experienced educa-
tors.  Note that no evidence of a systematic tenden-
cy to under-predict the complexity levels of 
literary texts is present. This suggests that our 
strategy of developing distinct prediction models 
for informational and literary texts has succeeded 
in overcoming the genre biases present among 
many key features.  
55
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3.  TextEvaluator GL predictions compared to 
human GL classifications for informational and literary 
texts. 
 
TestEvaluator performance relative to the goal of 
predicting the human grade band classifications in 
the validation dataset was also examined. Results 
are summarized in Table 6 along with correspond-
ing results for the Lexile Framework (Stenner, et 
al., 2006) and the REAP system (Heilman, et al, 
2007).  All results are reprinted, with permission, 
from Nelson, et al, (2012).  In each case, perfor-
mance is summarized in terms of the Spearman 
rank order correlation between the readability 
scores generated for each text, and corresponding 
human grade band classifications.  95% confidence 
limits estimated via the Fisher r to z transformation 
are also listed.   
 
 
 
System 
Lower 
95% 
Bound 
 
Correlation 
Coefficient 
Upper 
95% 
Bound 
TextEvaluator 0.683 0.76 0.814 
REAP 0.427 0.54 0.641 
Lexile 0.380 0.50 0.607 
 
Table 6. Correlation  between readability scores and 
human grade band classifications for the 168 Common 
Core texts in the validation dataset.   
 
The comparison suggests that, relative to the task 
of  predicting the human grade band classifications 
assigned to the informational, literary and mixed 
texts in Appendix B of the new Common Core 
State Standards, TextEvaluator is significantly 
more effective than both the Lexile Framework 
and the REAP system. 
6  Summary and Discussion 
 
In many recent studies, proposed readability me-
trics have been trained and validated on text collec-
tions composed entirely of informational text, e.g., 
Wall Street Journal articles (Pitler and Nenkova, 
2008), Encyclopedia Britannica articles (Schwarm 
and Ostendorf, 2005) and Weekly Reader articles 
(Vajjala and Meurers, 2012). This paper considers 
the more challenging task of predicting human-
assigned GL classifications in a corpus of texts 
constructed to be representative of the broad range 
of reading materials considered by teachers and 
students in U.S. classrooms.   
Two approaches for modeling the complexity 
characteristics of these passages were compared.  
In Approach #1, a single, non-genre specific pre-
diction equation is estimated, and that equation is 
then applied to texts in all genres.  Two measures 
developed via this approach were evaluated:  the 
Lexile Framework and the REAP system.    
Approach #2 differs from Approach #1 in that 
genre-specific prediction equations are used, there-
by ensuring that important genre effects are ac-
commodated.  This approach is currently only 
available via the TextEvaluator system.  
Measures developed via each approach were 
evaluated on a held-out sample.  Results confirmed 
that complexity classifications obtained via                       
TextEvaluator are significantly more highly corre-
lated with the human grade band classifications in 
the held-out sample than are classifications ob-
tained via the Lexile Framework or REAP system.  
This study also demonstrated that, when genre 
effects are ignored, readability scores for informa-
tional texts tend to be overestimated, while those 
for literary texts tend to be underestimated. Note 
that this finding significantly complicates the 
process of using readability metrics to generate 
valid cross-genre comparisons. For example, 
Stajner, et al (2012) conclude that SimpleWiki 
may not serve as a ?gold standard? of high acces-
sibility because comparisons based on readability 
metrics suggest that it is more complex than Fic-
tion. We intend to further investigate this finding 
using TextEvaluator since conclusions that are not 
impacted by genre bias can then be reported. Addi-
tional planned work involves investigating addi-
tional measures of genre, and incorporating these 
into our genre classifier.    
56
References  
 
Alderson, J. C. (2000). Assessing reading. Cam-
bridge: Cambridge University Press. 
American Institutes for Research (2008). Reading 
framework for the 2009 National Assessment of 
Educational Progress.  Washington, DC: Na-
tional Assessment Governing Board. 
Biber, D. (1986).  Spoken and written textual di-
mension in English: Resolving the contradictory 
findings.  Language, 62: 394-414. 
Biber, D. (1988).  Variation across Speech and 
Writing.  Cambridge: Cambridge University 
Press. 
Biber, D., Conrad, S., Reppen, R., Byrd, P., Helt, 
M., Clark, V., et al, (2004).  Representing lan-
guage use in the university:  Analysis of the 
TOEFL 2000 Spoken and Written Academic 
Language Corpus.  TOEFL Monograph Series, 
MS-25, January 2004.  Princeton, NJ: Educa-
tional Testing Service.  
Bormuth, J.R. (1964).  Mean word depth as a pre-
dictor of comprehension difficulty.  California 
Journal of Educational Research, 15, 226-231. 
Cohen, S. A. & Steinberg, J. E. (1983). Effects of 
three types of vocabulary on readability of in-
termediate grade science textbooks:  An applica-
tion of Finn?s transfer feature theory.  Reading 
Research Quarterly, 19(1), 86-101. 
Collins-Thompson, K. and Callan, J. (2004). A 
language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 
2004, Boston, USA. 
Coltheart, M. (1981).  The MRC psycholinguistic 
database, Quarterly Journal of Experimental 
Psychology, 33A, 497-505. 
Common Core State Standards Initiative (2010).  
Common core state standards for English lan-
guage arts & literacy in history/social studies, 
science and technical subjects.  Washington, 
DC: CCSSO & National Governors Association. 
Coxhead, A. (2000)  A new academic word list.  
TESOL Quarterly, 34(2), 213-238.  
Gunning, R. (1952).  The technique of clear writ-
ing. McGraw-Hill: New York. 
Graesser, A.C., McNamara, D. S., Louwerse, 
M.W. and Cai, Z. (2004).  Coh-Metrix:  Analy-
sis of text on cohesion and language. Behavior 
Research Methods, Instruments & Computers, 
36(2), 193-202.  
Halliday, M. A.K. & Hasan, R. (1976) Cohesion in 
English. Longman, London. 
Hiebert, E. H. & Mesmer, H. A. E. (2013).  Upping 
the ante of text complexity in the Common Core 
State Standards: Examining its potential impact 
on young readers. Educational Researcher, 
42(1), 44-51. 
Heilman, M., Collins-Thompson, K., Callan, J. & 
Eskenazi, M. (2007). Combining lexical and 
grammatical features to improve readability 
measures for first and second language texts. In 
Human Language Technologies 2007: The Con-
ference of the North American Chapter of the 
Association for Computational Linguistics 
(HLT-NAACL?07), 460-467. 
Just, M. A. & Carpenter, P. A. (1987). The psy-
chology of reading and language comprehen-
sion. Boston: Allyn & Bacon. 
Kincaid, J.P., Fishburne, R.P, Rogers, R.L. & 
Chissom, B.S. (1975). Derivation of new reada-
bility formulas (automated readability index, 
Fog count and Flesch reading ease formula) for 
navy enlisted personnel. Research Branch Re-
port 8-75. Naval Air Station, Memphis, TN. 
Kintsch, W. (1998). Comprehension: A paradigm 
for cognition. Cambridge, UK: Cambridge Uni-
versity Press. 
Klein, D. & Manning, C. D. (2003). Accurate Un-
lexicalized Parsing. In Proceedings of the 41st 
Meeting of the Association for Computational 
Linguistics, 423-430. 
Lee, D. Y. W. (2001)  Defining core vocabulary 
and tracking its distribution across spoken and 
written genres.  Journal of English Linguistics.  
29, 250-278. 
Nelson, J., Perfetti, C., Liben, D. and Liben, M. 
(2012). Measures of text difficulty: Testing their 
predictive value for grade levels and student 
performance. Technical Report, The Council of 
Chief State School Officers. 
57
Pitler, E. & Nenkova, A (2008). Revisiting reada-
bility:  A unified framework for predicting text 
quality. In Proceedings of the 2008 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational Lin-
guistics, 186-195. 
Schwarm, S. & Ostendorf, M. (2005). Reading 
level assessment using support vector machines 
and statistical language models.  In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL?05), 523-530. 
Sheehan, K.M. (in press).  Measuring cohesion: An 
approach that accounts for differences in the de-
gree of integration challenge presented by dif-
ferent types of sentences.  Educational 
Measurement: Issues and Practice. 
Sheehan, K.M., Kostin, I & Futagi, Y. (2008). 
When do standard approaches for measuring 
vocabulary difficulty, syntactic complexity and 
referential cohesion yield biased estimates of 
text difficulty?  In B.C. Love, K. McRae, & 
V.M. Sloutsky (Eds.), Proceedings of the 30th 
Annual Conference of the Cognitive Science 
Society, Washington D.C. 
 
Sheehan, K.M., Kostin, I., Futagi, Y. & Flor, M.  
(2010). Generating automated text complexity 
classifications that are aligned with targeted 
text complexity standards. (ETS RR-10-28). 
Princeton, NJ: ETS. 
 
Si, L. & Callan, J. (2001). A statistical model for 
scientific readability. In Proceedings of the 10th 
International Conference on Information and 
Knowledge Management (CIKM), 574-576. 
?tajner, S., Evans, R., Orasan, C., & Mitkov, R. 
(2012). What Can Readability Measures Really 
Tell Us About Text Complexity?. In Natural 
Language Processing for Improving Textual Ac-
cessibility (NLP4ITA) Workshop Programme                   
(p. 14). 
 
Stenner, A. J., Burdick, H., Sanford, E. & Burdick, 
D. (2006). How accurate are Lexile text meas-
ures?  Journal of Applied Measurement, 7(3), 
307-322. 
 
Vajjala, S. & Meurers, D. (2012). On improving 
the accuracy of readability classification using 
insights from second language acquisition. In 
Proceedings of the 7th Workshop on the Innova-
tive Use of NLP for Building Educational Appli-
cations, 163-173. 
Yngve, V.H. (1960).  A model and an hypothesis 
for language structure.  Proceedings of the 
American Philosophical Society, 104, 444-466. 
Zeno, S. M., Ivens, S. H., Millard, R. T., Duvvuri, 
R. (1995). The educator?s word frequency 
guide. Brewster, NY: Touchstone Applied 
Science Associates.  
  
58
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Associative Texture is Lost in Translation
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,mflor}@ets.org
Abstract
We present a suggestive finding regarding
the loss of associative texture in the pro-
cess of machine translation, using com-
parisons between (a) original and back-
translated texts, (b) reference and system
translations, and (c) better and worse MT
systems. We represent the amount of as-
sociation in a text using word association
profile ? a distribution of pointwise mu-
tual information between all pairs of con-
tent word types in a text. We use the av-
erage of the distribution, which we term
lexical tightness, as a single measure of
the amount of association in a text. We
show that the lexical tightness of human-
composed texts is higher than that of the
machine translated materials; human ref-
erences are tighter than machine trans-
lations, and better MT systems produce
lexically tighter translations. While the
phenomenon of the loss of associative tex-
ture has been theoretically predicted by
translation scholars, we present a measure
capable of quantifying the extent of this
phenomenon.
1 Introduction
While most current approaches to machine trans-
lation concentrate on single sentences, there is
emerging interest in phenomena that go beyond a
single sentence and pertain to the whole text being
translated. For example, Wong and Kit (2012)
demonstrated that repetition of content words is
a predictor of translation quality, with poorer
translations failing to repeat words appropriately.
Gong et al (2011) and Tiedemann (2010) present
caching of translations from earlier sections of a
document to facilitate the translation of its later
sections.
In scholarship that deals with properties of hu-
man translation of literary texts, translation is of-
ten rendered as a process that tends to deform
the original, and a number of particular aspects
of deformation have been identified. Specifically,
Berman (2000) discusses the problem of quantita-
tive impoverishment thus:
This refers to a lexical loss. Every
work in prose presents a certain pro-
liferation of signifiers and signifying
chains. Great novelist prose is ?abun-
dant.? These signifiers can be described
as unfixed, especially as a signified may
have a multiplicity of signifiers. For
the signified visage (face) Arlt employs
semblante, rosto and cara without jus-
tifying a particular choice in a particu-
lar sentence. The essential thing is that
visage is marked as an important real-
ity in his work by the use of three sig-
nifiers. The translation that does not re-
spect this multiplicity renders the ?vis-
age? of an unrecognizable work. There
is a loss, then, since the translation con-
tains fewer signifiers than the original.?1
While Berman?s remarks refer to literary trans-
lation, recent work demonstrates its relevance for
machine translation, showing that MT systems
tend to under-use linguistic devices that are com-
monly used for repeated reference, such as super-
ordinates or meronyms, although the pattern with
synonyms and near-synonyms was not clear cut
(Wong and Kit, 2012). Studying a complemen-
tary phenomenon of translation of same-lemma
lexical items in the source document into a target
language, Carpuat and Simard (2012) found that
when MT systems produce different target lan-
guage translations, they are stylistically, syntac-
tically, or semantically inadequate in most cases
1italics in the original
27
(see upper panel of Table 5 therein), that is, diver-
sifying the signifiers appropriately is a challeng-
ing task. For recent work on biasing SMT systems
towards consistent translations of repeated words,
see Ture et al (2012) and Xiao et al (2011).
Moving beyond single signifieds, or concepts,
Berman faults translations for ?the destruction of
underlying networks of signification?, whereby
groups of related words are translated without
preserving the relatedness in the target language.
While these might be unavoidable in any trans-
lation, we show below that machine translation
specifically indeed suffers from such a loss (sec-
tion 3) and that machine translation suffers from it
more than the human translations (section 4).
2 Methodology
We define WAPT ? a word association profile
of a text T ? as the distribution of PMI(x, y) for
all pairs of content2 word types (x, y) ?T.3 We es-
timate PMIs using same-paragraph co-occurrence
counts from a large and diverse corpus of about 2.5
billion words: 2 billion words come from the Gi-
gaword 2003 corpus (Graff and Cieri, 2003); an
additional 500 million words come from an in-
house corpus containing popular science and fic-
tion texts. We further define LTT ? the lexical
tightness of a text T ? as the average value of the
word association profile. All pairs of words in T
for which the corpus had no co-occurrence data
are excluded from the calculations. We note that
the database has very good coverage with respect
to the datasets in sections 3-5, with 94%-96%
of pairs on average having co-occurrence counts
in the database. A more detailed exposition of
the notion of a word association profile, includ-
ing measurements on a number of corpora, can be
found in Beigman Klebanov and Flor (2013).
Our prediction is that translated texts would be
less lexically tight than originals, and that better
translations ? either human or machine ? would be
tighter than worse translations, incurring a smaller
amount of association loss.
3 Experiment 1: Back-translation
For the experiment, we selected 20 editorials on
the topic of baseball from the New York Times
2We part-of-speech tag a text using OpenNLP tagger
(http://opennlp.apache.org) and only take into account com-
mon and proper nouns, verbs, adjectives, and adverbs.
3PMI = Pointwise Mutual Information
Annotated Corpus.4 The selected articles had
baseball annotated as their sole topic, and ranged
from 250 to 750 words in length. We expect
these articles to contain a large group of words
that reflects vocabulary that is commonly used in
discussing baseball and no other systematic sub-
topics. All articles were translated into French,
Spanish, Arabic, and Swedish, and then translated
back to English, using the Google automatic trans-
lation service. Our goal is to observe the effect of
the two layers of translation (out of English and
back) on the lexical tightness of the resulting texts.
Since baseball is not a topic that is commonly
discussed in the European languages or in Ara-
bic, this is a case where culturally foreign material
needs to be rendered in a host (or target) language.
This is exactly the kind of situation where we ex-
pect deformation to occur ? the material is either
altered so that is feels more ?native? in the host
language (domestication) or its foreigness is pre-
served (foreignization) in that the material lacks
associative support in the host language (Venuti,
1995). In the first case, the translation might be
associatively adequate in the host language, but,
being altered, it would produce less culturally pre-
cise result when translated back into English. In
the second case, the result of translating out of En-
glish might already be associatively impoverished
by the standards of the host language.
The italicized phrases in the previous paragraph
underscore the theoretical and practical difficulty
in diagnozing domestication or foreignization in
translating out of English ? an associative model
for each of the host languages will be needed,
as well as some benchmark of the lexical tight-
ness of native texts written on the given topic
against which translations from English could be
judged. While the technique of back-translation
cannot identify the exact path of association loss
? through domestication or foreignization ? it can
help establish that association loss has occurred
in at least one or both of the translation processes
involved, since the original native English version
provides a natural benchmark against which the
resulting back-translations can be measured.
To make the phenomenon of association loss
more concrete, consider the following sentence:
Original Dave Magadan, the hard-hitting rookie
third baseman groomed to replace Knight,
has been hospitalized.
4LDC2008T19 in LDC catalogue
28
Arabic Dave Magadan, the stern rookie 3 base-
man groomed to replace Knight, is in the hos-
pital.5
Spanish Dave Magadan, the strong rookie third
baseman who managed to replace Knight,
has been hospitalized.
French Dave Magadan, the hitting third rookie
player prepared to replace Knight, was hos-
pitalized.
Swedish Dave Magadan, powerful rookie third
baseman groomed to replace Knight, has
been hospitalized.
Observe the translations of the phrase ?hard-
hitting rookie third baseman.? While substituting
strong and powerful for hard-hitting might seem
acceptable semantically, these terms are not asso-
ciated with the other baseball terms in the text,
whereas hitting is highly associated with them:6
Table 1 shows PMI scores for each of hitting,
stern, strong, powerful with the baseball terms
rookie and baseman. The French translation got
the hitting, but substituted the more generic term
player instead of the baseball-specific baseman.
As the bottom panel of Table 1 makes clear, while
player is associated with other baseball terms, the
associations are lower than those of baseman.
rookie baseman hitting
hitting 3.54 5.29
stern 0.35 -1.60
strong 0.54 -0.08
powerful -0.62 -0.63
player 3.95 2.73
baseman 5.11 5.29
Table 1: PMI associations of words introduced in
back-translations with baseball terms rookie, base-
man, and hitting.
Table 2 shows the average lexical tightness
values across 20 texts for the original version as
well as for the back translated versions. The origi-
nal version is statistically significantly tighter than
each of the back translated versions, using 4 ap-
plications of t-test for correlated samples, n=20,
p<0.05 in each case.
5We corrected the syntax of all back-translations while
preserving the content-word vocabulary choices.
6Our tokenizer splits words on hyphens, therefore exam-
ples are shown for hitting rather than for hard-hitting. The
point still holds, since hitting is a baseball term on its own.
Version Av. Std. Min. Max.
LT LT LT LT
Original .953 .092 .832 1.144
Via Arabic .875 .093 .747 1.104
Via Spanish .909 .081 .801 1.069
Via French .912 .087 .786 1.123
Via Swedish .931 .099 .796 1.131
Table 2: Average lexical tightness (Av. LT) for the
original vs back translated versions, on 20 base-
ball texts from the New York Times. Standard de-
viation, minimum, and maximum values are also
shown.
4 Experiment 2: Reference vs Machine
Translation
We use a part of the dataset used in the NIST Open
MT 2008 Evaluation.7 Our set contains transla-
tions of 120 news and web articles from Arabic to
English. For each document, there are 4 human
reference translations and 17 machine translations
by various systems that participated in the bench-
mark. Table 3 shows the average and standard de-
viation of lexical tightness values across the 120
texts for each of the four reference translations,
each of the 17 MT systems, as well as an average
across the four reference translations, and an aver-
age across the 17 MT systems. Each of the 17 MT
systems is statistically significantly less tight than
the average reference human translation (17 appli-
cations of the t-test for correlated samples, n=120,
p<0.05); 12 of the 17 MT systems are statistically
significantly less tight than the least tight human
reference (reference translation #3) at p<0.05; the
average system translation is statistically signifi-
cantly less tight that the average human translation
at p<0.05.
To exemplify a large gap in associative texture
between reference and machine translations, con-
sider the following extracts.8 As the raw MT ver-
sion (MT-raw) is barely readable, we provide a
version where words are re-arranged for readabil-
ity (MT-read), preserving most of the vocabulary.
Since lexical tightness operates on content word
types, adding or removing repetitions and function
words does not impact the calculation, so we re-
moved or inserted those for the sake of readability
7LDC2010T01
8The first paragraph of arb-WL-1-154489-
7725312#Arabic#system21#c.xml vs arb-WL-1-154489-
7725312#Arabic#reference 1#r.xml.
29
Translation Av. Std. Min. Max.
LT LT LT LT
Ref. 1 .873 .140 .590 1.447
Ref. 2 .851 .124 .636 1.256
Ref. 3 .838 .121 .657 1.177
Ref. 4 .865 .131 .639 1.429
Av. Ref. .857 .124 .641 1.317
MT 1 .814 .110 .670 1.113
MT 2 .824 .109 .565 1.089
MT 3 .818 .113 .607 1.137
MT 4 .836 .116 .615 1.144
MT 5 .803 .097 .590 1.067
MT 6 .824 .116 .574 1.173
MT 7 .819 .115 .576 1.162
MT 8 .810 .104 .606 1.157
MT 9 .827 .114 .546 1.181
MT 10 .827 .122 .569 1.169
MT 11 .814 .116 .606 1.131
MT 12 .826 .112 .607 1.119
MT 13 .823 .115 .619 1.116
MT 14 .826 .115 .630 1.147
MT 15 .820 .107 .655 1.124
MT 16 .827 .112 .593 1.147
MT 17 .835 .117 .642 1.169
Av. MT .822 .107 .623 1.106
Table 3: Average lexical tightness (Av. LT) for
the reference vs machine translations, on the NIST
Open MT 2008 Evaluation Arabic to English cor-
pus. Standard deviation, minimum, and maximum
values across the 120 texts are also shown.
in the MT-read version.
MT-raw vision came to me on dream in view of
her dream: Arab state to travel to and group
of friends on my mission and travel quickly
I was with one of the girls seem close to the
remaining more than I was happy and you?re
raised ended === known now
MT-read A vision came to me in a dream. I was
to travel quickly to an Arab state with a group
of friends on a mission. I was with one of
the girls who seemed close to the remaining
ones. I was happy and you are raised. It
ended. It is known now.
Ref A Dream. My sister came to tell me about a
dream she had while she slept. She was say-
ing: I saw you preparing to travel to an Arab
country, myself and a group of girlfriends.
You were sent on a scholarship abroad, and
you were preparing to travel quickly. You
were with one of the girls, who appeared to
be closer to you than the others, and I was
happy and excited because you were travel-
ing. The end. I now know !
The use of vision instead of dream, state in-
stead of country, friends instead of girlfriends,
mission instead of scholarship, raised instead of
excited, along with the complete disapperance
of slept, sister, preparing, abroad, all contribute
to a dramatic loss of associative texture in the
MT version. Highly associated pairs like dream-
slept, tell-saying, girlfriends-girls, travel-abroad,
sister-girls, happy-excited, travel-traveling are all
missed in the machine translation, while the newly
introduced word raised is quite unrelated to the
rest of the vocabulary in the extract.
5 Experiment 3: Quality of Machine
Translation
5.1 System-Level Comparison
In this experiment, we address the following ques-
tion: Is it the case that when a worse MT system A
and a better MT system B translate the same set of
materials, B tends to provide more lexically tight
translations?
To address this question, we use the Metrics-
MATR 2008 development set (Przybocki et al,
2009) from NIST Open MT 2006 evaluation.
Eight MT systems were used to translate 25 news
articles from Arabic to English, and humans pro-
vided scores for translation adequacy on a 1-7
scale. We calculated the average lexical tightness
over 25 texts for each of the eigth MT systems, as
well as the average translation score for each of the
systems. We note that human scores are available
per text segments (roughly equivalent to a sen-
tence, 249 segments in total for 25 texts), rather
than for whole texts. We first derive a human score
for the whole text for a given system by averaging
the scores of the system?s translations of the differ-
ent segments of the text. We then derive a human
score for an MT system by averaging the scores of
its translations of the 25 texts. We found that the
average adequacy score of a system is statistically
significantly positively correlated with the average
lexical tightness that the system?s translations ex-
hibit: r=0.630, n=8, df = 6, p<0.05.
30
5.2 Translation-Level Comparison
The same data could be used to answer the ques-
tion: Is it the case that better translations are
lexically tighter? Experiment 2 demonstrated that
human reference translations are tighter than ma-
chine translations; does the same relationship hold
for better vs worse machine translations? To ad-
dress this question, 25 x 8 = 200 instances of (sys-
tem, text) pairs can be used, where each has a
human score for translation adequacy and a lexi-
cal tightness value. Human scores and lexical
tightness of a translated text are significantly pos-
itively correlated, r=0.178, n=200, p<0.05. Note,
however, that this analysis is counfounded by the
variation in lexical tightness that exists between
texts: As standard deviations and ranges in Ta-
bles 2 and 3 make clear, original human texts, as
well as reference human translation for different
texts, vary in their lexical tightness. Therefore, a
lower lexical tightness value can be expected for
certain texts even for adequate translations, while
for other texts low values of lexical tightness sig-
nal a low quality translation. System-level anal-
ysis as presented in section 5.1 avoids this con-
founding, since all systems translated the same set
of texts, therefore average tightness values per sys-
tem are directly comparable.
6 Discussion and Conclusion
We presented a suggestive finding regarding the
loss of associative texture in the process of ma-
chine translation, using comparisons between (a)
original and back-translated texts, (b) reference
and system translations, (c) better and worse ma-
chine translations. We represented the amount of
association in a text using word association pro-
file ? a distribution of point wise mutual infor-
mation between all pairs of content word types
in a text. We used the average of the distribu-
tion, which we term lexical tightness ? as a sin-
gle measure of the amount of association in a text.
We showed that the lexical tightness of human-
composed texts is higher than that of the machine
translated materials. While the phenomenon of the
loss of associative texture has been theoretically
predicted by translation scholars, lexical tightness
is a computational measure capable of quantifying
the extent of this phenomenon.
Our work complements that of Wong and
Kit (2012) in demonstrating the potential utility
of discourse-level phenomena to assess machine
translations. First, we note that our findings are
orthogonal to the main finding in Wong and Kit
(2012) regarding loss of cohesion through insuffi-
cient word repetition, since our measure looks at
pairs of word types, hence disregards repetitions.
Second, the notion of pairwise word association
generalizes the notion of lexical cohesive devices
by looking not only at repeated reference with dif-
ferent lexical items or at words standing in cer-
tain semantic relations to each other, but at the
whole of the lexical network of the text. Third, dif-
ferently from the cohesion measure proposed by
Wong and Kit (2012), the lexical tightness mea-
sure does not depend on lexicographic resources
such as WordNet that do not exist in many lan-
guages.
References
Beata Beigman Klebanov and Michael Flor. 2013.
Word Association Profiles and their Use for Auto-
mated Scoring of Essays. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, Sofia, Bulgaria, August.
Antoine Berman. 2000. Translation and the Trials of
the Foreign (translated from 1985 French original by
L. Venuti). In Lawrence Venuti, editor, The Trans-
lation Studies Reader, pages 276?289. New York:
Routledge.
Marine Carpuat and Michel Simard. 2012. The Trou-
ble with SMT Consistency. In Proceedings of the
7th Workshop on Statistical Machine Translation,
pages 442?449, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
David Graff and Christopher Cieri. 2003. English Gi-
gaword LDC2003T05. Linguistic Data Consortium,
Philadelphia.
Mark Przybocki, Kay Peterson, and Sebastien Bron-
sart. 2009. 2008 NIST metrics for machine transla-
tion (MetricsMATR08) development data.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15, Uppsala, Sweden,
July. Association for Computational Linguistics.
31
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Lawrence Venuti. 1995. The Translator?s Invisibi-
ilty: A History of Translation. London & New York:
Routledge.
Billy Tak-Ming Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In EMNLP-CoNLL,
pages 1060?1068.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of the Machine
Translation Summit XIII.
32
Proceedings of the Second Workshop on Metaphor in NLP, pages 11?17,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Different Texts, Same Metaphors: Unigrams and Beyond
Beata Beigman Klebanov, Chee Wee Leong, Michael Heilman, Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,cleong,mheilman,mflor}@ets.org
Abstract
Current approaches to supervised learning
of metaphor tend to use sophisticated fea-
tures and restrict their attention to con-
structions and contexts where these fea-
tures apply. In this paper, we describe the
development of a supervised learning sys-
tem to classify all content words in a run-
ning text as either being used metaphori-
cally or not. We start by examining the
performance of a simple unigram baseline
that achieves surprisingly good results for
some of the datasets. We then show how
the recall of the system can be improved
over this strong baseline.
1 Introduction
Current approaches to supervised learning of
metaphor tend to (a) use sophisticated features
based on theories of metaphor, (b) apply to cer-
tain selected constructions, like adj-noun or verb-
object pairs, and (c) concentrate on metaphors
of certain kind, such as metaphors about gover-
nance or about the mind. In this paper, we de-
scribe the development of a supervised machine
learning system to classify all content words in a
running text as either being used metaphorically
or not ? a task not yet addressed in the literature,
to our knowledge. This approach would enable,
for example, quantification of the extent to which
a given text uses metaphor, or the extent to which
two different texts use similar metaphors. Both of
these questions are important in our target appli-
cation ? scoring texts (in our case, essays written
for a test) for various aspects of effective use of
language, one of them being the use of metaphor.
We start by examining the performance of a
simple unigram baseline that achieves surprisingly
good results for some of the datasets. We then
show how the recall of the system can be improved
over this strong baseline.
2 Data
We use two datasets that feature full text anno-
tations of metaphors: A set of essays written for
a large-scale assessment of college graduates and
the VUAmsterdam corpus (Steen et al., 2010),
1
containing articles from four genres sampled from
the BNC. Table 1 shows the sizes of the six sets,
as well as the proportion of metaphors in them; the
following sections explain their composition.
Data #Texts #NVAR #metaphors
tokens (%)
News 49 18,519 3,405 (18%)
Fiction 11 17,836 2,497 (14%)
Academic 12 29,469 3,689 (13%)
Conversation 18 15,667 1,149 ( 7%)
Essay Set A 85 21,838 2,368 (11%)
Essay Set B 79 22,662 2,745 (12%)
Table 1: Datasets used in this study. NVAR =
Nouns, Verbs, Adjectives, Adverbs, as tagged by
the Stanford POS tagger (Toutanova et al., 2003).
2.1 VUAmsterdam Data
The dataset consists of 117 fragments sampled
across four genres: Academic, News, Conversa-
tion, and Fiction. Each genre is represented by ap-
proximately the same number of tokens, although
the number of texts differs greatly, where the news
archive has the largest number of texts.
We randomly sampled 23% of the texts from
each genre to set aside for a blind test to be carried
out at a later date with a more advanced system;
the current experiments are performed using cross-
validation on the remaining 90 fragments: 10-fold
on News, 9-fold on Conversation, 11 on Fiction,
and 12 on Academic. All instances from the same
text were always placed in the same fold.
1
http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html
11
The data is annotated using MIP-VU proce-
dure. It is based on the MIP procedure (Prag-
glejaz, 2007), extending it to handle metaphori-
city through reference (such as marking did as a
metaphor in As the weather broke up, so did their
friendship) and allow for explicit coding of diffi-
cult cases where a group of annotators could not
arrive at a consensus. The tagset is rich and is
organized hierarchically, detecting various types
of metaphors, words that flag the presense of
metaphors, etc. In this paper, we consider only the
top-level partition, labeling all content words with
the tag ?function=mrw? (metaphor-related word)
as metaphors, while all other content words are la-
beled as non-metaphors.
2
2.2 Essay Data
The dataset consists of 224 essays written for a
high-stakes large-scale assessment of analytical
writing taken by college graduates aspiring to en-
ter a graduate school in the United States. Out of
these, 80 were set aside for future experiments and
not used for this paper. Of the remaining essays,
85 essays discuss the statement ?High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication? (Set A), and 79
discuss the statement ?In the age of television,
reading books is not as important as it once was.
People can learn as much by watching television
as they can by reading books.? (Set B). Multiple
essays on the same topic is a unique feature of this
dataset, allowing the examination of the effect of
topic on performance, by comparing performance
in within-topic and across-topic settings.
The essays were annotated using a protocol
that prefers a reader?s intuition over a formal de-
finition, and emphasizes the connection between
metaphor and the arguments that are put forward
by the writer. The protocol is presented in detail
in Beigman Klebanov and Flor (2013). All essays
were doubly annotated. The reliability is ? = 0.58
for Set A and ? = 0.56 for Set B. We merge the two
annotations (union), following the observation in
a previous study Beigman Klebanov et al. (2008)
that attention slips play a large role in accounting
for observed disagreements.
We will report results for 10-fold cross-
validation on each of sets A and B, as well as
2
We note that this top-level partition was used for many
of the analyses discussed in (Steen et al., 2010).
across prompts, where the machine learner would
be trained on Set A and tested on Set B and vice
versa.
3 Supervised Learning of Metaphor
For this study, we consider each content-word to-
ken in a text as an instance to be classified as a
metaphor or non-metaphor. We use the logistic
regression classifier in the SKLL package (Blan-
chard et al., 2013), which is based on scikit-learn
(Pedregosa et al., 2011), optimizing for F
1
score
(class ?metaphor?). We consider the following
features for metaphor detection.
? Unigrams (U): All content words from the
relevant training data are used as features,
without stemming or lemmatization.
? Part-of-Speech (P): We use Stanford POS
tagger 3.3.0 and the full Penn Treebank tagset
for content words (tags starting with A, N, V,
and J), removing the auxiliaries have, be, do.
? Concreteness (C): We use Brysbaert et al.
(2013) database of concreteness ratings for
about 40,000 English words. The mean ra-
tings, ranging 1-5, are binned in 0.25 incre-
ments; each bin is used as a binary feature.
? Topic models (T): We use Latent Dirich-
let Allocation (Blei et al., 2003) to derive
a 100-topic model from the NYT corpus
years 2003?2007 (Sandhaus, 2008) to rep-
resent common topics of public discussion.
The NYT data was lemmatized using NLTK
(Bird, 2006). We used the gensim toolkit
(
?
Reh?u?rek and Sojka, 2010) for building the
models, with default parameters. The score
assigned to an instance w on a topic t is
log
P (w|t)
P (w)
where P (w) were estimated from
the Gigaword corpus (Parker et al., 2009).
These features are based on the hypothesis
that certain topics are likelier to be used as
source domains for metaphors than others.
4 Results
For each dataset, we present the results for the
unigram model (baseline) and the results for the
full model containing all the features. For cross-
validation results, all words from the same text
were always placed in the same fold, to ensure that
we are evaluating generalization across texts.
12
M Unigram UPCT
Data F P R F P R F
Set A .20 .72 .43 .53 .70 .47 .56
Set B .22 .79 .54 .64 .76 .60 .67
B-A .20 .58 .45 .50 .56 .50 .53
A-B .22 .71 .28 .40 .72 .35 .47
News .31 .62 .38 .47 .61 .43 .51
Fiction .25 .54 .23 .32 .54 .24 .33
Acad. .23 .51 .20 .27 .50 .22 .28
Conv. .14 .39 .14 .21 .36 .15 .21
Table 2: Summary of performance, in terms of
precision, recall, and F
1
. Set A, B, and VUAm-
sterdam: cross-validation. B-A and A-B: Training
on B and testing on A, and vice versa, respectively.
Column M: F
1
of a pseudo-system that classifies
all words as metaphors.
4.1 Performance of the Baseline Model
First, we observe the strong performance of the
unigram baseline for the cross-validation within
sets A and B (rows 1 and 2 in Table 2). For a
new essay, about half its metaphors will have been
observed in a sample of a few dozen essays on the
same topic; these words are also consistently used
as metaphors, as precision is above 70%. Once the
same-topic assumption is relaxed down to related
topics, the sharing of metaphor is reduced (com-
pare rows 1 vs 3 and 2 vs 4), but still substantial.
Moving to VUAmsterdam data, we observe that
the performance of the unigram model on the
News partition is comparable to its performance in
the cross-prompt scenario in the essay data (com-
pare row 5 to rows 3-4 in Table 2), suggesting that
the News fragments tend to discuss a set of related
topics and exhibit substantial sharing of metaphors
across texts.
The performance of the unigram model is much
lower for the other VUAmsterdam partitions, al-
though it is still non-trivial, as evidenced by its
consistent improvement over a pseudo-baseline
that classifies all words as metaphor, attaining
100% recall (shown in column M in Table 2). The
weaker performance could be due to highly diver-
gent topics between texts in each of the partitions.
It is also possible that the number of different
texts in these partitions is insufficient for covering
the metaphors that are common in these kinds of
texts ? recall that these partitions have small num-
bers of long texts, whereas the News partition has
a larger number of short texts (see Table 1).
4.2 Beyond Baseline
The addition of topic model, POS, and concrete-
ness features produces a significant increase in
recall across all evaluations (p < 0.01), using
McNemar?s test of the significance of differ-
ences between correlated proportions (McNemar,
1947). Even for Conversations, where recall
improvement is the smallest and F
1
score does
not improve, the UPCT model recovers all 161
metaphors found by the unigrams plus 14 addi-
tional metaphors, yielding a significant result on
the correlated test.
We next investigate the relative contribution of
the different types of features in the UPCT model
by ablating each type and observing the effect on
performance. Table 3 shows ablation results for
essay and News data, where substantial improve-
ments over the unigram baseline were produced.
We observe, as expected, that the unigram fea-
tures contributed the most, as removing them re-
sults in the most dramatic drop in performance,
although the combination of concreteness, POS,
and topic models recovers about one-fourth of
metaphors with over 50% precision, showing non-
trivial performance on essay data.
The second most effective feature set for essay
data are the topic models ? they are responsible for
most of the recall gain obtained by the UPCT mo-
del. For example, one of the topics with a positive
weight in essays in set B deals with visual ima-
gery, its top 5 most likely words in the NYT being
picture, image, photograph, camera, photo. This
topic is often used metaphorically, with words
like superficial, picture, framed, reflective, mirror,
capture, vivid, distorted, exposure, scenes, face,
background that were all observed as metaphors in
Set B. In the News data, a topic that deals with hur-
ricane Katrina received a positive weight, as words
of suffering and recovery from distaster are often
used metaphorically when discussing other things:
starved, severed, awash, damaged, relief, victim,
distress, hits, swept, bounce, response, recovering,
suffering.
The part-of-speech features help improve recall
across all datasets in Table 3, while concreteness
features are effective only for some of the sets.
5 Discussion: Metaphor & Word Sense
The classical ?one sense per discourse? finding of
Gale et al. (1992) that words keep their senses
within the same text 98% of the time suggests that
13
Set A cross-val. Set B cross-val. Train B : Test A Train A : Test B News
P R F P R F P R F P R F P R F
M .11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31
U .72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47
UPCT .70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51
? U .58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32
? P .71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49
? C .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50
? T .71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49
Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors.
if a word is used as a metaphor once in a text, it is
very likely to be a metaphor if it is used again in
the same text. Indeed, this is the reason for putting
all words from the same text in the same fold in
cross-validations, as training and testing on diffe-
rent parts of the same text would produce inflated
estimates of metaphor classification performance.
Koeling et al. (2005) extend the notion of dis-
course beyond a single text to a domain, such as
articles on Finance, Sports, and a general BNC
domain. For a set of words that each have at
least one Finance and one Sports sense and not
more than 12 senses in total, guessing the pre-
dominant sense in Finance and Sports yielded 77%
and 76% precision, respectively. Our results with
the unigram model show that guessing ?metaphor?
based on a sufficient proportion of previously ob-
served metaphorical uses in the given domain
yields about 76% precision for essays on the same
topic. Thus, metaphoricity distinctions in same-
topic essays behave similarly to sense distinctions
for polysemous words with a predominant sense
in the Finance and Sports articles, keeping to their
domain-specific predominant sense
3
4
of the time.
Note that a domain-specific predominant sense
may or may not be the same as the most frequent
sense overall; similarly, a word?s tendency to be
used metaphorically might be domain specific or
general. The results for the BNC at large are likely
to reflect general rather than domain-specific sense
distributions. According to Koeling et al. (2005),
guessing the predominant sense in the BNC yields
51% precision; our finding for BNC News is 62%
precision for the unigram model. The difference
could be due to the mixing of the BNC genres in
Koeling et al. (2005), given the lower precision of
metaphoricity prediction in non-news (Table 2).
In all, our results suggest that the pattern of
metaphorical and non-metaphorical use is in line
with that of dominant word-sense for more and
less topically restricted domains.
6 Related Work
The extent to which different texts use similar
metaphors was addressed by Pasanek and Scul-
ley (2008) for corpora written by the same author.
They studied metaphors of mind in the oeuvre
of 7 authors, including John Milton and William
Shakespeare. They created a set of metaphori-
cal and non-metaphorical references to the mind
using excerpts from various texts written by these
authors. Using cross-validation with unigram
features for each of the authors separately, they
present very high accuracies (85%-94%), suggest-
ing that authors are highly self-consistent in the
metaphors of mind they select. They also find
good generalizations between some pairs of au-
thors, due to borrowing or literary allusion.
Studies using political texts, such as speeches
by politicians or news articles discussing politi-
cally important events, documented repeated use
of words from certain source domains, such as
rejuvenation in Tony Blair?s speeches (Charteris-
Black, 2005) or railroad metaphors in articles dis-
cussing political integration of Europe (Musolff,
2000). Our results regarding settings with substan-
tial topical consistency second these observations.
According to the Conceptual Metaphor theory
(Lakoff and Johnson, 1980), we expect certain ba-
sic metaphors to be highly ubiquitous in any cor-
pus of texts, such as TIME IS SPACE or UP IS
GOOD. To the extent that these metaphors are
realized through frequent content words, we ex-
pect some cross-text generalization power for a
unigram model. Perhaps the share of these basic
metaphors in all metaphors in a text is reflected
most faithfully in the peformance of the unigram
model on the non-News partitions of the VUAms-
14
terdam data, where topical sharing is minimal.
Approaches to metaphor detection are often ei-
ther rule-based or unsupervised (Martin, 1990;
Fass, 1991; Shutova et al., 2010; Shutova and
Sun, 2013; Li et al., 2013), although supervised
approaches have recently been attempted with the
advent of relatively large collections of metaphor-
annotated materials (Mohler et al., 2013; Hovy et
al., 2013; Pasanek and Sculley, 2008; Gedigan
et al., 2006). These approaches are difficult to
compare to our results, as these typically are not
whole texts but excerpts, and only certain kinds of
metaphors are annotated, such as metaphors about
governance or about the mind, or only words be-
longing to certain syntactic or semantic class are
annotated, such as verbs
3
or motion words only.
Concreteness as a predictor of metaphoricity
was discussed in Turney et al. (2011) in the context
of concrete adjectives modifying abstract nouns.
The POS features are inspired by the discussion
of the preference and aversion of various POS
towards metaphoricity in Goatly (1997). Heintz
et al. (2013) use LDA topics built on Wikipedia
along with manually constructed seed lists for po-
tential source and target topics in the broad tar-
get domain of governance, in order to identify
sentences using lexica from both source and tar-
get domains as potentially containing metaphors.
Bethard et al. (2009) use LDA topics built on BNC
as features for classifying metaphorical and non-
metaphorical uses of 9 words in 450 sentences that
use these words, modeling metaphorical vs non-
metaphorical contexts for these words. In both
cases, LDA is used to capture the topical compo-
sition of a sentence; in contrast, we use LDA to
capture the tendency of words belonging to a topic
to be used metaphorically in a given discourse.
Dunn (2013) compared algorithms based on
various theories of metaphor on VUAmsterdam
data. The evaluations were done at sentence level,
where a sentence is metaphorical if it contains at
least one metaphorically used word. In this ac-
counting, the distribution is almost a mirror-image
of our setting, as 84% of sentences in News were
labeled as metaphorical, whereas 18% of content
words are tagged as such. The News partition was
very difficult for the systems examined in Dunn
(2013) ? three of the four systems failed to pre-
dict any non-metaphorical sentences, and the one
system that did so suffered from a low recall of
3
as in Shutova and Teufel (2010)
metaphors, 20%. Dunn (2013) shows that the
different systems he compared had relatively low
agreement (? < 0.3); he interprets this finding as
suggesting that the different theories underlying
the models capture different aspects of metapho-
ricity and therefore detect different metaphors. It
is therefore likely that features derived from the
various models would fruitfully complement each
other in a supervised learning setting; our findings
suggest that the simplest building block ? that of
a unigram model ? should not be ignored in such
experiments.
7 Conclusions
We address supervised learning of metaphoricity
of words of any content part of speech in a running
text. To our knowledge, this task has not yet been
studied in the literature. We experimented with a
simple unigram model that was surprisingly suc-
cessful for some of the datasets, and showed how
its recall can be further improved using topic mo-
dels, POS, and concreteness features.
The generally solid performance of the unigram
features suggests that these features should not be
neglected when trying to predict metaphors in a
supervised learning paradigm. Inasmuch as me-
taphoricity classification is similar to a coarse-
grained word sense disambiguation, a unigram
model can be thought of as a crude predominant
sense model for WSD, and is the more effective
the more topically homogeneous the data.
By evaluating models with LDA-based topic
features in addition to unigrams, we showed that
topical homogeneity can be exploited beyond uni-
grams. In topically homogeneous data, certain
topics commonly discussed in the public sphere
might not be addressed, yet their general fa-
miliarity avails them as sources for metaphors.
For essays on communication, topics like sports
and architecture are unlikely to be discussed; yet
metaphors from these domains can be used, such
as leveling of the playing field through cheap and
fast communications or buildling bridges across
cultures through the internet.
In future work, we intend to add features that
capture the relationship between the current word
and its immediate context, as well as add essays
from additional prompts to build a more topically
diverse set for exploration of cross-topic generali-
zation of our models for essay data.
15
References
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. In Proceedings of the First Workshop on
Metaphor in NLP, pages 11?20, Atlanta, Georgia,
June. Association for Computational Linguistics.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
COLING 2008 workshop on Human Judgments in
Computational Linguistics, pages 2?7, Manchester,
UK.
Steven Bethard, Vicky Tzuyin Lai, and James Martin.
2009. Topic model analysis of metaphor frequency
for psycholinguistic stimuli. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, CALC ?09, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the ACL, Interactive Pre-
sentations, pages 69?72.
Daniel Blanchard, Michael Heilman,
and Nitin Madnani. 2013. SciKit-
Learn Laboratory. GitHub repository,
https://github.com/EducationalTestingService/skll.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1?8.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Jonathan Dunn. 2013. What metaphor identification
systems can tell us about metaphor-in-language. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 1?10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of
the Speech and Natural Language Workshop, pages
233?237.
Matt Gedigan, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48, New York.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, Dave
Barner, Donald Black, Majorie Friedman, and Ralph
Weischedel. 2013. Automatic Extraction of Lin-
guistic Metaphors with LDA Topic Modeling. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 58?66, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the First Workshop on Metaphor in NLP,
pages 52?57, Atlanta, GA. Association for Compu-
tational Linguistics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426, Vancouver, Canada. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press, Chicago.
Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-driven metaphor recognition and explanation.
Transactions of the ACL, 1:379?390.
James Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Michael Mohler, David Bracewell, Marc Tomlinson,
and David Hinote. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 27?35, Atlanta, GA. Association for
Computational Linguistics.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition LDC2009T13. Linguistic Data Consortium,
Philadelphia.
Bradley Pasanek and D. Sculley. 2008. Mining mil-
lions of metaphors. Literary and Linguistic Com-
puting, 23(3):345?360.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
16
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. LDC Catalog No: LDC2008T19.
Ekaterina Shutova and Lin Sun. 2013. Unsu-
pervised metaphor identification using hierarchical
graph factorization clustering. In Proceedings of
HLT-NAACL, pages 978?988.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source - target do-
main mappings. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), pages 3255?
3261, Valletta, Malta, May. European Language Re-
sources Association (ELRA).
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING), pages 1002?1010.
Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna
Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A
Method for Linguistic Metaphor Identification. Am-
sterdam: John Benjamins.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of NAACL, pages 252?259.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identi-
fication through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 680?
690, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
17
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 35?45,
Dublin, Ireland, August 23, 2014.
ETS Lexical Associations System for the COGALEX-4 Shared Task
Michael Flor
Educational Testing Service
Rosedale Road
Princeton, NJ, 08541, USA
mflor@ets.org
Beata Beigman Klebanov
Educational Testing Service
Rosedale Road
Princeton, NJ, 08541, USA
bbeigmanklebanov@ets.org
Abstract
We  present  an  automated  system  that  computes  multi-cue  associations  and  generates 
associated-word suggestions, using lexical co-occurrence data from a large corpus of English 
texts.  The system performs expansion of cue words to  their  inflectional  variants,  retrieves 
candidate words from corpus data, finds maximal associations between candidates and cues, 
computes an aggregate score for each candidate, and outputs an n-best list of candidates. We 
present experiments using several measures of statistical association, two methods of score 
aggregation, ablation of resources and applying additional filters on retrieved candidates. The 
system  achieves  18.6%  precision  on  the  COGALEX-4  shared  task  data.  Results  with 
additional evaluation methods are presented. We also describe an annotation experiment which 
suggests  that  the  shared  task  may  underestimate  the  appropriateness  of  candidate  words 
produced by the corpus-based system.
1 Introduction
The COGALEX-4 shared task is a multi-cue association task: finding a target word that is associated  
with a set of cue words. The task is motivated, for example, by a tip-of-the-tongue search application,  
as described by the organizers: ?Suppose, we were looking for a word expressing the following ideas: 
'superior dark coffee made of beans from Arabia', but could not remember the intended word 'mocha'. 
Since people always remember something concerning the elusive word, it would be nice to have a 
system accepting this kind of input, to propose then a number of candidates for the target word. Given  
the above example,  we might  enter  'dark',  'coffee',  'beans',  and 'Arabia',  and the system would be  
supposed  to  come  up  with  one  or  several  associated  words  such  as  'mocha',  'espresso',  or  
'cappuccino'.?
The data for  the  shared task were sampled  from the Edinburgh Associative Thesaurus (EAT - 
http://www.eat.rl.ac.uk).  For  each  of  about  8,000  stimulus  words,  the  EAT lists  the  associations 
(words) provided by human respondents, sorted according to the number of respondents who provided 
the  respective  word.  Generally,  when  more  people  provided  the  same  response,  the  underlying  
association is considered to be stronger (Kiss et al., 1973). For the COGALEX-4 shared task, the cues 
were the five strongest responses to an unknown stimulus word, and the task was to recover (guess)  
the stimulus word (henceforth, target word). The data for the task consisted of a training set of 2000 
items (for which target words were provided), and a test set of 2000 items. The origin of the data was 
not  disclosed  before  or  during  the system development  and evaluation phases  of  the  shared  task 
competition.
The ETS entry consisted of a system that uses corpus-based distributional information about pairs  
of words in English. No use was made of human association data (EAT or other), nor of any other  
information such as the order of importance of the cue words, or any special preference for the British 
spelling often used in the EAT.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
35
2 The ETS system for computing multi-cue association
Our system is defined by the following components.
1. Corpus from which the distributional information about word pairs is learned, 
along with preprocessing steps (database generation).
2. The kind of distributional information collected from the corpus (collocation & co-occurrence).
3. A measure of association between two words.
4. An algorithm for generating candidate associates using the resources above.
5. An algorithm for scoring candidate associates.
2.1 Corpus
Our corpus is composed of two sources. One part is the English Gigaword 2003 corpus (Graff and 
Cieri, 2003), with 1.7 billion tokens. The second part is an ETS in-house corpus containing texts from 
the genres of fiction and popular science (Sheehan et al., 2006), with about 430 million tokens.
2.2 Types of distributional information
From this combined corpus we have built two specific lexical resources. One resource is a bigram 
repository,  which  stores  counts  for  sequences  of  two  words.  The  other  resource  is  a  first-order
co-occurrence word-space model (Turney and Pantel, 2010), also known as a Distributional Semantic 
Model (DSM) (Baroni and Lenci, 2010). In our implementation of DSM, we counted non-directed co-
occurrence  of  tokens  in  a  paragraph,  using  no  distance  coefficients  (Bullinaria  and Levy,  2007). 
Counts for 2.1 million word-form types, and the sparse matrix of their co-occurrences, are efficiently  
compressed using the TrendStream toolkit (Flor, 2013), resulting in a database file of 4.7GB. 
The same toolkit supports both n-grams and DSM repositories, and allows fast retrieval of word 
probabilities and statistical associations for pairs of words.1 It also supports retrieval of co-occurrence 
vectors. When generating these two resources, we used no lemmatization and no stoplist. All tokens  
were converted to lowercase. All punctuation was retained and counted as tokens. The only significant  
filtering was applied to numbers: all digit-based numbers (e.g. 5, 2.1) were converted to the symbol '#'  
and counted as such. Tokenization was performed by an internal module of the TrendStream toolkit.
The lexical resources described above were not generated for the COGALEX-4 shared task. Rather, 
those are general-purpose large-scale lexical resources that we have used in previous research, for a  
variety of NLP tasks. This is an important aspect, as our intention was to find out how well those 
general resources would perform on this novel task. Our bigrams repository is actually part of a 5-
gram language  model  that  is  used  for  context-aware  spelling  correction.  The  algorithms  for  that 
application are described by Flor (2012). The DSM has been used for spelling correction (Flor, 2012),  
for essay scoring (Beigman Klebanov and Flor, 2013a), for readability estimation (Flor and Beigman  
Klebanov,  in  press;  Flor  et  al.,  2013),  as  well  as  for  a  study on  quality  of  machine  translation  
(Beigman Klebanov and Flor, 2013b).
2.3 Measures of association
For the shared task, we used three measures of word association.
Pointwise Mutual Information (Church & Hanks, 1990):
PMI ?a ,b?=log 2 P ?a ,b?P ?a ?P ?b?
Normalized Pointwise Mutual Information (Bouma, 2009):
NPMI ?a , b?=?log2 P ?a ,b?P ?a?P ?b ? ?/ ??log2 P ?a ,b??
1 The TrendStream toolkit provides compression and storage for large-scale n-gram models, and for large-scale co-occurrence 
matrices. In all cases, actual counts are stored and values for statistical association measures are computed on the fly during 
data retrieval.
36
Simplified log-Likelihood (Evert, 2008):
SLL?a ,b?=2?P ?a ,b??log P ?a ,b ?P ?a ?P ?b??P ?a ,b??P ?a ?P ?b?
P(a,b) signifies probability of joint  co-occurrence.  For bigrams,  that  is  joint  co-occurrence in a 
specific sequential order (e.g. AB vs. BA) ; for DSM data the co-occurrence is order-independent.
2.4 Procedure for generating candidate multi-cue associates
Our general procedure for generating target candidates is as follows. For each of the five cue words,  
candidate targets are generated separately, from the corpus-based resources:
1. From the DSM (generally associated words)
2. Left words from bigrams (words that, in the corpus, appeared immediately to the left of the cue)
3. Right words from bigrams (words that appeared immediately to the right of the cue)
Retrieved lists of candidates can be quite large, with hundreds and even thousands of different 
neighbors. One specific filter implemented at this stage was that only word-forms (alphabetic strings) 
were allowed, and any punctuation or '#' strings were filtered out.
Since  our  resources  are  not  lemmatized,  we  extended  the  candidate  retrieval  procedure  by 
expanding the cue words to their inflectional variants. This provides richer information about semantic 
association. We used an in-house morphological analyzer/generator. Inflectional expansions were not  
constrained for part of speech or word sense. For example, given the cue set {1:letters 2:meaning 
3:sentences 4:book 5:speech} (from the training set of the shared task, target: 'words'), after expansion 
the set  of  cues is   {1:letters,  lettered,  letter,  lettering 2:meaning,  means,  mean,  meant,  meanings  
3:sentences,  sentence,  sentenced,  sentencing 4:book,  books,  booking,  booked 5:speech,  speeches}. 
The vector of right neighbors for the cue 'letters', brings such words as {sent, from, between, written,  
came,  addressed,  ...}.  The vector  of  left  neighbors  for  same  cue word brings  such candidates  as 
{write, send, love, capital, review, ...}. From the DSM, the vector of co-occurrence may bring some of 
the same words (but with different values of association), as well as words that do not generally occur  
immediately before or after the cue word, e.g. {time, people, word, now,?}. 
Next,  we  apply  filtering  that  ensures  the  minimal  requirement  for  multi-word  association  ?  a 
candidate must be related to all cues. The candidate must appear (at least once) on the list of words 
generated from each cue family. A candidate word that does not meet this requirement is filtered out.2
2.5 Scoring of candidate associates
Scoring of candidate associate-words is a two-stage process. First, for each candidate, we look for the 
strongest association value it has with each of the five cue families. Then, the five strongest values are 
combined into an aggregated score.
For a given cue family, several instances of the same candidate associate might be retrieved, with 
various values of association score (from DSM and n-grams, and also for each specific inflectional  
form of the cue). We pick the highest score, siding with the source that provides the strongest evidence 
of connection between the cue and the candidate associate. The maximal association value is stored as 
the best score for this candidate with the given cue family. We note that since the same measure of  
association is used, the scores from the different sources are numerically comparable. 3 For example, 
when  PMI is  used  as  the  association  measure,  the  following values  were  obtained  for  candidate  
'capital'  with  cue  family  'letters,  lettered,  letter,  lettering'  (expanded  from 'letters').  General  co-
occurrence (DSM): capital & letters: 0.477, capital & letter: 0.074, etc.; left bigrams: capital letters: 
5.268, capital letter: 2.474, etc. The strongest association here is the bigram 'capital letters', and the 
value 5.268 is the best association of the candidate 'capital' with this cue family. 
Next, for each candidate we compute an aggregate score that represents its overall association with  
all five cues. In current study, we experimented with two forms of aggregation: 1) sum of best scores  
2 This is 'baseline' filtering, applied in all experiments. Experiments with additional filtering are described in section 4.2.
3 In any single experimental run we consistently use the same measure of association (no mixing of different formulae).
37
(SBS), and 2) product (multiplication) of ranks (MR). Sum of best scores is simply the sum of best 
association scores that a candidate has with each of the five cues (families). To produce a final ranked  
list of candidate targets, candidates are sorted by their aggregate sum value (better candidates have  
higher values). Multiplication of ranks has been proposed as an aggregation procedure by Rapp (2014,  
2008). In this procedure, all candidates are sorted by their association scores with each of the five cues  
(families) separately, and five rank values are registered for each candidate. The five rank values are  
then multiplied to produce the final aggregate score. All candidates are then sorted by the aggregate  
score, and in such ranking better candidates have lower aggregate scores. Multiplication of ranks is  
computationally more intensive than sum of scores ? for a given set of candidate words from five cues, 
multiplication  of  ranks  requires  six  calls  for  sorting,  while  aggregation  via  sum-of-best-scores 
performs sorting only once.
Finally, all candidates are sorted by their aggregate score and top N are outputted for the calculation 
of precision@N, to be described below.
3 Results
Our system ran with several different configuration settings, using various association measures and 
score aggregation procedures. Under any given configuration, the system produces, for each item (i.e.  
a set of five cue words), a ranked list of candidates. According to the rules of the shared task, official  
results are computed by selecting the single best candidate for the item as the suggested target word. If  
the  suggested  word  strictly  matches  the  gold-standard  word  (ignoring  upper/lower  case),  it  is 
considered a match. If the two strings differ even slightly, it is considered a mismatch. The reported 
result is precision (percent matches) over the test set of 2000 items. 
With strict-matching, our best result for the test-set was precision of 18.6% (372 correctly suggested 
targets). This was obtained by using NPMI as the association measure, product of ranks as the score  
aggregation procedure, and with filtering of candidates using a stoplist and a frequency filter.4
The shared task was described as multi-cue association for finding a sought-after 'missing' word, a  
situation  not  unlike  a  tip-of-the-tongue  phenomenon.  In  such  situation,  a  person  looking  for  an 
associated word,  might  find it  useful  if the system returns not  just  one highest-ranked suggestion 
(which would often be a miss), but a list of several top-ranked suggestions ? the target word might be  
somewhere on such list5. Thus, we also present our results in terms of precision for n-best suggestions 
? i.e. in how many cases the target word was among the top n returned by the system, with n ranging 
from 1 up to 25. 
A similar consideration applies to inflectional variants. A person looking for a word associated with 
a set of cue words, might be satisfied when a system returns either a base-form or an inflected variant  
of the target word. Thus, we report our results both in terms of strict matches to gold-standard targets  
and under a condition of 'inflections-allowed'.6 On the test set, our best result for precision@1, with 
inflections allowed, is 24.35% (487 matching suggestions).
First, we present our baseline results. Figure 1 presents the results of our system for the training set 
of  2000  items,  using  the  NPMI  association  measure.  Panel  1A  presents  data  obtained  using 
aggregation via  sum-of-best-scores  (SBS).  Panel  1B presents  data  obtained  using  aggregation  via 
multiplication of ranks (MR). Figure 2 presents similar breakdown for results of the test set. Both sets  
of results are quite similar. Thus, we restrict our attention to just the results of the test set. 7
4 We initially submitted a result of 14.95% strict-match precision@1 (see Figure 2A). This was improved to 16.1% (Figure 
2B), and with additional filters ? to 18.6% (see section 4.2).
5 A list of n-best suggestions is standard approach for presenting candidate corrections for misspellings (Flor, 2013; Mitton, 
2008). Also, precision ?at n documents? is a well known evaluation approach in information retrieval (Manning et al., 2008). 
A recent use of n-best suggestions in an interactive NLP system is illustrated by Madnani and Cahill (2014).
6 Each target word form, both in the training set and the test set, was automatically expanded to all its inflectional variants,  
using our morphological analyzer/generator. In our evaluations, a candidate target is considered a 'hit' if it matches the 
gold-standard target or one of its inflectional variants.
7 We did not use the training set for any training or parameter tuning. We used it to select the optimal association measures 
for this task ? we also experimented with t-score, weighted PMI and conditional probability, but PMI and NPMI performed 
much better than others. 
38
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Training-set:  NPMI with SBS aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
A  
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Training-set:  NPMI with MR aggregation
+Inflections
Strict n-bes t
Pre
cis
ion
 %
B
Figure 1. System performance on the training-set (percent correct out of 2000 items), for various 
values of n. Panel A: using sum-of-best-scores aggregation; Panel B: using multiplication-of-ranks 
aggregation. 'Strict': evaluation uses strict matching to gold-standard target, '+Inflections': inflectional 
variants are allowed in matching to gold-standard target.
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Test-set:  NPMI with SBS aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
A  
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Test-set:  NPMI with MR  aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
B
Figure 2. System performance on the test-set (percent correct out of 2000 items). 
We found,  as expected,  that performance improves when the target is sought among the  n-best 
candidates produced by the system. With NPMI and MR aggregation, strict-match precision improves 
from 16.1% for  precision@1 to  30.3% for  precision@5,  37% for  precision@10,  and  46.9% for  
precision@25 (Figure 2B).
Another expected result is that performance is better when matching of targets allows inflectional  
variants. This is clearly seen on the charts, as the difference between the two lines. With NPMI and  
MR aggregation, precision@1 improves from 16.1% to 21.45%, precision@5 improves from 30.3% to 
36.3%, and precision@25 improves from 46.9% to 54%, Similar improvement is observed when using 
aggregation via sum-of-best-scores.
Our third finding is that multiplication of ranks achieves slightly better results than sum-of-best-
scores  (Figure  2,  panel  B  vs.  panel  A).  For  precision@1 with  strict  matches,  using  NPMI,  MR 
achieves  16.1% and  with  inflectional  variants  21.45%,  while  SBS  achieves  14.95% and  20.25% 
respectively.  For  precision@10,  MR  achieves  37%  (43.55%),  while  SBS  achieves  36%  (42%). 
Notably, MR is consistently superior to SBS for all values of n-best, from 1 to 25, under both strict or 
inflections-allowed matching, with both NPMI and PMI (see Figure 3). However, the advantage is 
consistently rather small ? about 1-1.5%. Since MR is computationally more intensive, SBS emerges 
as a viable alternative. 
We have  also  conducted  experiments  with  three  different  measures  of  association.  Results  are 
presented in Figure 3. With MR aggregation, NPMI achieves better results than the PMI measure.  
Both measures clearly outperform the Simplified log-Likelihood. Similar  results are obtained with 
SBS aggregation. For each association measure, allowing inflections provides better results than strict 
matching to gold-standard targets.
39
1 3 5 7 9 11 13 15 17 19 21 23 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set:  SBS aggregation
NPMI+inf NPMI+strict PMI+inf
PMI+strict SLL+inf SLL+strict
n-bes t
Pre
cis
ion
 %
A
 
1 3 5 7 9 11 13 15 17 19 21 23 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set:  MR aggregation
NPMI+inf NPMI+strict PMI+inf
PMI+strict SLL+inf SLL+strict
n-bes t
Pre
cis
ion
 %
B
Figure 3. System performance on the test-set (2000 items) with three different association measures. 
Panel A: using sum-of-best-scores aggregation; Panel B: using multiplication-of-ranks aggregation. 
Legend: PMI: pointwise mutual information, NPMI: Normalized PMI, SLL: simplified log-likelihood, 
'Strict': evaluation uses strict matching to gold-standard target, '+Inf': inflectional variants are allowed 
in matching to gold-standard target.
4 Additional studies
In  several  additional  experiments  we  looked  at  the  contribution  of  different  factors  to  overall 
performance.  We  tried  several  variations  of  resource  combination  and  also  tested  filtering  of 
candidates by frequency and by using a list of stopwords. 
4.1 Ablation experiments
We investigated how the restriction of resources impacts the performance on this task. Specifically we 
restricted  the  resources  as  follows.  In  one  condition  we  used  only  the  bigrams  data,  retrieving 
candidates only from the vectors of left co-occurring words (immediate preceding words) of each cue  
word (condition NL ? n-grams left). A similar restriction is when candidates are retrieved only from 
right (immediate successor) words (condition NR ? n-grams right). A third condition still uses only 
bigrams, but admits candidates from both left and right vectors (condition NL+NR). Under the fourth 
condition (DSM), n-grams data is not used at all, only the DSM resource is used. In the fifth and sixth 
conditions we combine candidates from DSM with n-gram candidates (left or right vectors only ? 
respectively). The seventh condition is our standard ? candidates from DSM and both left and right 
neighbors from bigrams are admitted. For those experiments, we used NPMI association measure with 
MR aggregation, and included inflections in evaluation. The results are presented in Figure 4.
Using  only  right-hand  associates  (typical  textual  successors  of  cue  words)  provides  very  low 
performance (precision@1 is 2.95%). Using only left-hand associates (typical textual predecessors of 
cue words) provides slightly better performance (precision@1 is 4.5%). However, it is notable that  
there are some items in the EAT data where all cues are strong bigrams with the target, e.g. {orange,  
fruit, lemon, apple, tomato} with target 'juice'.  Combining these two resources (condition NL+NR) 
provides much better performance: precision@1 is 8.5%. Using just the DSM, the system achieves  
10.5% precision@1, which may seem rather close to the combined NL+NR 8.5%. However, with 
DSM, for   n-best  lists  precision rises  quite  sharply (e.g.  24.35% for  precision@5),  while  for  the 
NL+NR setting precision tends to be under 17% for all values of n up to 25. 
Since our DSM and bigrams resources are built on the same corpus of text, for any given set of cues  
the DSM produces all the candidates that the bigrams resource does (but with different association 
values) and a lot of other candidates. However, results for DSM+NR and DSM+NL settings (which 
are better than DSM alone) indicate that association values from bigrams contribute substantially to 
overall  performance.  The  best  result  in  this  experiment  is  achieved  by  a  setting  that  combines  
candidates (and association values) from all three resources, indicating further that associations from 
sequential word combinations (bigrams) provide a substantial contribution to performance in this task.
40
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set: studies with resource variation
ALL
DSM+NL
DSM+NR
DSM
NL+NR
NL
NR
n-bes t
Pre
cis
ion
 %
Figure 4. System performance on the test-set (2000 items), with various resource restrictions. 
All runs used NPMI association measure and MR aggregation. Evaluation allowed inflections.
NL/NR ? left/right neighbors from bigrams.
4.2 Applying filters on retrieved candidates
We also experimented with applying some filters on the retrieved candidates for each item. One of the 
obvious filters to use is to filter out stopwords. For general tip-of-the-tongue search cases, common  
stopwords are  rarely useful  as  target  words;  thus  presenting stopwords as  candidates  makes  little  
sense.  We used a list  of  87 very common English stopwords,  including the articles  {the,  a,  an}, 
common prepositions, pronouns, wh-question words, etc. However, since the data of the shared task 
comes from EAT, common stopwords are actually targets in some cases in that collection. Therefore, 
we used the following strategy. For a given item, if at least one of the five cue words is a stopword,  
then we assume that the target might also be a stopword, and so we do not use the stoplist to filter  
candidates for this item. However, if none of the cues is a stopword, we do apply filtering ? any  
retrieved candidate word is filtered out if it is on the stoplist. An additional filter, applied with the  
stoplist, was defined as follows: if a candidate word is strictly identical to one of the cue words, the  
candidate is filtered out (to allow for potentially more suitable candidates).8 
The other  filter  considers frequency of  words.  The PMI measure  is  known to overestimate  the 
strength of pair association when one of the words is a low-frequency word (Manning & Sch?tze,  
1999).  Normalized  PMI is  also  sensitive  to  this  aspect,  although less  than PMI.  Thus,  we  use  a 
frequency filter to drop some candidate words. For technical reasons, it was easier for us to apply a  
cutoff on the joint frequency of a candidate and a cue word. We used a cutoff value of 10 ? a candidate 
is dropped if corpus data indicates it co-occurs with the cue words fewer than 10 times in the corpus 
data.
We applied the stoplist filter, the frequency filter and a combination of those two filters, always  
using NPMI as our association measure, aggregating scores via multiplication-of-ranks, and allowing 
inflections in evaluation. No ablation of resources was applied. The results are presented in Figure 5.  
The baseline condition is when neither of the two filters is applied. The frequency filter with cutoff=10 
provides a very small improvement for precision@1, and for higher values of best-n it actually hurts 
performance.  Application  of  a  stoplist  provides  a  very  slight  improvement  of  performance.  The 
combination of a stoplist and frequency cutoff=10 provides a sizable improvement of performance 
(precision@1 is  24.35% vs.  baseline 21.45%, and precision@10 is  44.55% vs.  baseline 43.55%). 
However, for n-best lists of size 15 and above, performance without filters is slightly better than with 
those filters.  For the shared task (using strict matching ? no inflections),  our best result  is 18.6% 
precision@1 with two filters (16.1% without filters).
8 Cases when a candidate word is identical to one of the cues do occur when associate candidates are harvested from corpus 
data. Such candidates have little utility for a missing-word-search task. Notably, however, the training-set for the shared 
task did have one item where the target word was identical to one of the cues: Yeah ~ Yeah no Yes Beatles Oh.
41
Given that the gold-standard targets in the shared task are original stimulus words form the EAT 
collection, we can use a special restriction ? restrict the candidates to just the EAT stimuli word-list  
(Rapp,  2014).  Notably,  this  is  a  very  specific  restriction,  suited  to  the  specific  dataset,  and  not  
applicable to the general case of multi-cue associations or tip-of-the-tongue word searches. We used 
the list of 7913 single-word stimuli from EAT as a filter in our system ? generated candidates that  
were  not  on  this  list  were  dropped  from consideration.  The  results  (Figure  5)  indicate  that  this  
restriction  (EATvocab)  provides  a  substantial  improvement  over  the  baseline  condition.  For 
precison@1,  using  EATvocab  (24.55%)  is  comparable  to  using  a  stoplist+cutoff10  (24.35%).  
However, for larger n-best lists, EATvocab filter provides substantially better performance. 
1 3 5 7 9 11 13 15 17 19 21 23 25
20
25
30
35
40
45
50
55
60
65
Test-set: filtering experiments 
EAT vocab
Stoplist+C10
Stoplist
C10
Baseline
n-bes t
Pre
cis
ion
 %
Condition Precision@1 Precision@10
EAT Vocabulary 24.55% 52.00%
Stoplist & Cutoff10 24.35% 44.55%
Stoplist 22.15% 43.85%
Cutoff10 21.70% 42.50%
Baseline (no filters) 21.45% 43.55%
Figure 5. System performance on the test set with different filtering conditions. All runs use NPMI as?
sociation and MR aggregation. Inflections allowed in evaluation. C10: frequency cutoff=10.
5 Small-scale evaluation using direct human judgments
Inspecting results from training-set data, we observed a number of cases where the system produced 
very plausible targets which however were struck down as incorrect (not matching the gold-standard).  
For example, for the cue set {music, piano, play, player, instrument} the gold-standard target was 
'accordion'. But why not 'violin' or 'trombone'? To provide a more in-depth evaluation of the results, 
we sampled 180 items at random from the test set, along with the candidate targets produced by our 
system,9 and submitted those to evaluation by two research assistants. For each item, evaluators were  
given the five cue words and the best candidate target generated by the system. They were told that the 
word is supposed to be a common associate of the five cues, and asked to indicate, for each item,  
whether the candidate was (a) Just Right; or (b) OK; or (c) Inadequate; (a,b,c are on ordinal scale).
Out of the 180 items, 80 were judged by both annotators. Table 1 presents the agreement matrix  
between the two annotators. Agreement on the 3 classes was kappa=0.49. If  Just Right and  OK are 
collapsed, the agreement is kappa=0.60. The discrepancy is largely due to a substantial number of  
instances that one annotator judged OK and the other ? Just Right.
Inadequate OK Just Right TOTAL
Inadequate 17 6 1 24
OK 6 25 10 41
Just Right 0 3 12 15
TOTAL 23 34 23 80
Table 1. Inter-annotator agreement matrix for a subset of items from the test-set.
9 Using all resources, NPMI association measure, MR aggregation, and with the general stoplist filter.
42
We note that one annotator commented on a difficulty making a decision in a number of cases  
where the cues are a list of mostly adjectives or possessives, and the target produced by the system is  
an  adverb.  For  example,  the  cue  set  {busy,  house,  vacant,  engaged,  empty}  with  the  proposed 
candidate  target  'currently';  the  cue  set  {food,  thirsty,  tired,  empty,  starving}  with  the  proposed 
candidate  'perpetually';  the  cue  set  {fat,  short,  build,  thick,  built}  with  the  proposed  candidate 
'slightly'; the cue set {mine, yours, his, is, theirs} with the proposed target 'rightfully'. This annotator 
felt that these responses were OK, while the other annotator rejected them. 
We merged the two annotations to provide a single annotation for the full set of 180 items by taking 
one annotator's judgment on single-annotated cases and taking the lower of the two judgments for the 
double annotated disagreed cases (thus, OK and Inadequate are merged to Inadequate; Just Right and 
OK are merged to OK). We next compare these annotations to the EAT gold standard. Table 2 shows 
the confusion matrix between the ?gold label? from EAT and our annotation. We observe that the 
totals for Just Right and EAT-match are almost identical (43 vs 42); however, only 17 items were both 
Just Right and EAT-matches. There were 24 EAT matches that were judged as OK by the annotators 
(presumably,  these  did  not  quite  create  the  ?just  right?  impression  for  at  least  one  annotator).  
Examples include: the cue set {beer, tea, storm, ale, bear} with the proposed correct target 'brewing' 
(one annotator commented that the relationship with ?bear? was unclear); the cue set {exam, match,  
tube, try, cricket} with the proposed correct target 'test' (one annotator commented that the relationship 
with 'cricket' was unclear); the cue set {school, secondary, first, education, alcohol} with the proposed 
correct target 'primary' (one annotator commented that the relationship with 'alcohol'  was unclear). 
These  results  might  reflect  cultural  differences  between  original  EAT  respondents  (British 
undergraduates circa year 1970) and present-day American young adults who, e.g. might not know 
much  about  cricket.  Another  possibility  is  that  in  the  EAT  collection,  the  5 th cue  sometimes 
corresponds to a very weak associate provided by just a single respondent out of 100, as in brewing-
bear and  primary-alcohol cases.  Interestingly,  the  weak  cues  did  not  confuse  the  system,  but 
replicability of the human judgments for such cases is doubtful.
Just Right OK Inadequate Total
EAT match 17 24 1 42
EAT mismatch 26 58 54 138
Total 43 82 55 180
Table 2. Annotated data vs. gold-standard matches for a set of 180 items.
There were also 26 instances that were judged as Just Right yet were not EAT-matches. Three of 
these were derivationally related, like 'build' (EAT target) vs 'buildings'  (proposed) for the cue set 
{house, up, construct, destroy, bricks}, the others were 'dwell' vs 'dwellings', 'collector' vs 'collecting'. 
In the rest of the cases, the generated candidates seemed as good as, or better, than the EAT words.  
For example, the cue set {ships, boat, sea, ship, ocean} had 'liners' as the EAT target, whereas the 
system proposed 'cruise'. For the cue set {natural, animal, nature, birds, fear}, the gold-standard EAT 
target is 'instinct', whereas the system proposed 'predatory'. For the cue set {sound, speak, sing, noise,  
speech} the gold-standard EAT target is 'voice', while the system produced 'louder'. For the cue set 
{music, band, noise, club, folk} the target was 'jazz', whereas the system proposed 'dance'. For the cue 
set  {violin,  music,  orchestra,  bow,  instrument}  the  target  was  'cello',  while  the  system produced 
'stringed'. Furthermore, in as many as 58 cases (32%) the response produced by the system did not  
match the target from EAT, but was OK-ed by the annotators. Some examples include: the cue set  
{fool, loaf, idiot, lout, lazy} with proposed candidate 'ignorant'; the cue set {hard, problems, work,  
hardship,  trouble}  with  proposed  candidate  'economic';  {interesting,  intriguing,  amazing,  book,  
exciting}  with  proposed  candidate  'discoveries';  {lazy,  chair,  about,  lying,  sitting}  with  proposed 
candidate 'motionless'. In all, if the system were evaluated by counting Just Right and OK annotations 
as correct, the precison@1 would have been (43+82)/180 = 69%. The estimation of performance based 
on gold-standard EAT data for this set is 42/180 = 23%, exactly one-third of what annotators found to  
be reasonable responses. This suggests that evaluation of multi-cued retrieval on targets from EAT 
rejects many good semantic associates, and thus might be considered too harsh.
43
6 Conclusions
This  paper  presented  an  automated  system  that  computes  multi-cue  associations  and  generates  
associated-word suggestions, using lexical co-occurrence data from a large corpus of English texts.  
The system uses pre-existing resources ? a large  n-ngram database and a large word-co-occurrence 
database, which have been previously used for a range of different NLP tasks. The system performs 
expansion of cue words to their inflectional variants, retrieves candidate words from corpus data, finds  
maximal associations between candidates and cues, and then computes an aggregate score for each 
candidate. The collection of candidates is then sorted and an n-best list is presented as output. In the 
paper we presented experiments using various measures of statistical association and two methods of  
score  aggregation.  We  also  experimented  with  limiting  the  lexical  resources,  and  with  applying 
additional filters on retrieved candidates. 
For test-set evaluation, the shared task requires strict-matches to gold-standard targets. Our system,  
in optimal configuration, was correct in 372 of 2000 cases, that is precision of 18.6%. We have also 
suggested a more lenient evaluation, where a candidate target is also considered correct if it is an 
inflectional  variant  of  the gold-standard word.  When inflections are allowed,  our system achieves 
precision of 24.35%. Performance improves dramatically when evaluation considers in how many 
cases the gold-standard target (or its inflectional variants) are found among the  n-best suggestions 
provided by the system. For example, with a list of 10-best suggestions, precision rises to 45%, and to 
54% with a list of 25-best. Using an n-best list of suggestions makes sense for applications like tip-of-
the-tongue situation. 
We note that the specific data set used in COGALEX-4 shared task, i.e. the Edinburgh Associative 
Thesaurus, might be sub-optimal for evaluation of multi-cue associative search. With the EAT dataset, 
the gold-standard words were the original stimuli from EAT, and the cue words were the associated  
words that were most frequently produced by respondents in the original EAT experiment (Kiss et al., 
1973). Rapp (2014) has argued that corpus-based computation of reverse-associations is a reasonable 
test  case  for  multi-cued word  search.  However,  Rapp also  notes  that  in  many cases,  suggestions  
provided by a corpus-based system are quite reasonable, but are not correct for the EAT dataset. We  
have conducted pilot human annotation on a small subset of the test-set ? judging how reasonable the 
top suggestion of our system is in general, and not whether it matched EAT targets. In this experiment,  
69% of the system's  first  responses were judged acceptable by humans,  while only 23% matched  
targets.  This  provides  a  quantitative  confirmation  that  EAT-based  evaluation  underestimates  the 
quality of results produced by a corpus-based multi-cue association system. 
The use of data from EAT hints at the following direction for future research. In the original EAT 
data, the first cue is actually the strongest associate of the target word (original stimulus), while other 
cues  are  much  weaker  associates.  In  our  current  implementation,  we  treated  all  cues  as  equally 
important.  Future  research  may  include  consideration  for  relative  importance  or  relevance  of  the 
different cues. In potential applications, like the tip-of-the-tongue word search, a user may be able to 
specify which cues are more relevant than others.   
Acknowledgments
Special thanks to Melissa Lopez and Matthew Mulholland for their help with the evaluation study. We 
also thank Mo Zhang, Paul Deane and Keelan Evanini at ETS, and three anonymous COGALEX re?
viewers, for comments on earlier drafts of this paper.
References
Marko Baroni and Allesandro Lenci. 2010. Distributional Memory: A General Framework for Corpus-Based 
Semantics. Computational Linguistics, 36(4), 673-721
Beata Beigman Klebanov and Michael Flor.  2013a.  Word Association Profiles and their Use for Automated 
Scoring  of  Essays. In  Proceedings  of  the  51st  Annual  Meeting  of  the  Association  for  Computational 
Linguistics, pages 1148?1158, Sofia, Bulgaria. 
Beata Beigman Klebanov and Michael Flor. 2013b. Associative Texture Is Lost In Translation. In Proceedings 
of the Workshop on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, 
Sofia, Bulgaria.
44
Gerlof  Bouma.  2009.  Normalized  (Pointwise)  Mutual  Information  in  Collocation  Extraction. In:  Chiarcos, 
Eckart de Castilho & Stede (eds), From Form to Meaning: Processing Texts Automatically, Proceedings of  
the Biennial GSCL Conference 2009, T?bingen, Gunter Narr Verlag, p. 31?40.
John A. Bullinaria  and Joseph P. Levy.  2007. Extracting semantic representations from word co-occurrence 
statistics: A computational study. Behavior Research Methods, 39:510?526.
Kenneth  Church  and  Patrick  Hanks.  1990.  Word  association  norms,  mutual  information  and  lexicography.  
Computational Linguistics, 16(1), 22?29.
David Graff and Christopher Cieri. 2003. English Gigaword. LDC2003T05. Philadelphia, PA, USA: Linguistic 
Data Consortium.
Stefan  Evert.  2008.  Corpora  and  collocations.  In  A.  L?deling  and  M. Kyt?  (eds.),  Corpus  Linguistics:  An 
International Handbook, Mouton de Gruyter: Berlin.
Michael Flor. 2013. A fast and flexible architecture for very large word n-gram datasets.  Natural Language 
Engineering, 19(1), 61-93.
Michael  Flor.  2012.  Four  types  of  context  for  automatic  spelling  correction.  Traitement  Automatique  des  
Langues (TAL),  53:3  (Special  Issue:  Managing  noise  in  the  signal:  error  handling  in  natural  language 
processing), 61-99. 
Michael  Flor  and  Beata  Beigman  Klebanov.  (in  press)  Associative  Lexical  Cohesion  as  a  factor  in  Text  
Complexity. Accepted for publication in the International Journal of Applied Linguistics.
Michael  Flor,  Beata  Beigman  Klebanov  and  Kathleen  M.  Sheehan.  2013.  Lexical  Tightness  and  Text 
Complexity.  In  Proceedings of the 2th Workshop of Natural  Language Processing for Improving Textual 
Accessibility (NLP4ITA), p.29?38. NAACL 2013 Conference, Atlanta, Georgia.
G.R. Kiss, C. Armstrong, R. Milroy  and J. Piper. 1973. An associative thesaurus of English and its computer  
analysis.  In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies.  
Edinburgh: University Press.
Nitin Madnani and Aoife Cahill. 2014. An Explicit Feedback System for Preposition Errors based on Wikipedia  
Revisions. To appear in Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational 
applications (BEA-9). ACL 2014 Conference, Baltimore, MD.
Christopher  D.  Manning,  Prabhakar  Raghavan,  and  Hinrich  Sch?tze.  2008.  Introduction  to  Information 
Retrieval. Cambridge University Press.
Christopher D. Manning, and Hinrich Sch?tze. 1999. Foundations of Statistical Natural Language Processing, 
1999, Cambridge, Massachusetts, USA: MIT Press.
Roger  Mitton.  2008.  Ordering  the  suggestions  of  a  spellchecker  without  using  context.  Natural  Language 
Engineering, 15(2), 173?192.
Reinhard Rapp. 2014. Corpus-Based Computation of Reverse-Associations. Proceedings of LREC.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In  Proceedings of the 
Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, UK
Kathleen  M.  Sheehan,  Irene  Kostin,  Yoko  Futagi,  Ramin  Hemat  and  Daniel  Zuckerman.  2006.  Inside 
SourceFinder: Predicting the Acceptability Status of Candidate Reading-Comprehension Source Documents. 
ETS research report RR-06-24. Educational Testing Service: Princeton, NJ.
Peter  Turney  and  Patrick  Pantel.  2010.  From Frequency  to  Meaning:  Vector  Space  Models  of  Semantics.  
Journal of Artificial Intelligence Research, 37, 141-188.
45
