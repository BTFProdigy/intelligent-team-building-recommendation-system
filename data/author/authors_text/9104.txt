Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 72?78,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Hybrid Models 
for Chinese Named Entity Recognition 
 
Lishuang Li, Tingting Mao, Degen Huang, Yuansheng Yang 
Department of Computer Science and Engineering 
Dalian University of Technology 
116023 Dalian, China 
{computer, huangdg, yangys}@dlut.edu.cn  
maotingting1007@sohu.com 
 
Abstract 
This paper describes a hybrid model and 
the corresponding algorithm combining 
support vector machines (SVMs) with 
statistical methods to improve the per-
formance of SVMs for the task of Chi-
nese Named Entity Recognition (NER). 
In this algorithm, a threshold of the dis-
tance from the test sample to the hyper-
plane of SVMs in feature space is used to 
separate SVMs region and statistical 
method region. If the distance is greater 
than the given threshold, the test sample 
is classified using SVMs; otherwise, the 
statistical model is used. By integrating 
the advantages of two methods, the hy-
brid model achieves 93.18% F-measure 
for Chinese person names and 91.49% F-
measure for Chinese location names. 
1 Introduction 
Named entity (NE) recognition is a fundamental 
step to many language processing tasks such as 
information extraction (IE), question answering 
(QA) and machine translation (MT). On its own, 
NE recognition can also provide users who are 
looking for person or location names with quick 
information. Palma and Day (1997) reported that 
person (PER), location (LOC) and organization 
(ORG) names are the most difficult sub-tasks as 
compared to other entities as defined in Message 
Understanding Conference (MUC). So we focus 
on the recognition of PER, LOC and ORG enti-
ties. 
Recently, machine learning approaches are 
widely used in NER, including the hidden 
Markov model (Zhou and Su, 2000; Miller and 
Crystal, 1998), maximum entropy model 
(Borthwick, 1999), decision tree (Qin and Yuan, 
2004), transformation-based learning (Black and 
Vasilakopoulos, 2002), boosting (Collins, 2002; 
Carreras et al, 2002), support vector machine 
(Takeuchi and Collier, 2002; Yu et al, 2004; 
Goh et al, 2003), memory-based learning (Sang, 
2002). SVM has given high performance in vari-
ous classification tasks (Joachims, 1998; Kudo 
and Matsumoto, 2001). Goh et al (2003) pre-
sented a SVM-based chunker to extract Chinese 
unknown words. It obtained higher F-measure 
for person names and organization names. 
Like other classifiers, the misclassified testing 
samples by SVM are mostly near the decision 
plane (i.e., the hyperplane of SVM in feature 
space). In order to increase the accuracy of SVM, 
we propose a hybrid model combining SVM 
with a statistical approach for Chinese NER, that 
is, in the region near the decision plane, statisti-
cal method is used to classify the samples instead 
of SVM, and in the region far away from the de-
cision plane, SVM is used. In this way, the mis-
classification by SVM near the decision plane 
can be decreased significantly. A higher F-
measure for Chinese NE recognition can be 
achieved. 
In the following sections, we shall describe 
our approach in details. 
2 Recognition of Chinese Named Entity 
Using SVM 
Firstly, we segment and assign part-of-speech 
(POS) tags to words in the texts using a Chinese 
lexical analyzer. Secondly, we break segmented 
words into characters and assign each character 
its features. Lastly, a model based on SVM to 
identify Chinese named entities is set up by 
choosing a proper kernel function. 
In the following, we will exemplify the person 
names and location names to illustrate the identi-
fication process. 
72
2.1 Support Vector Machines 
Support Vector Machines first introduced by 
Vapnik (1996) are learning systems that use a 
hypothesis space of linear functions in a high 
dimensional feature space, trained with a learn-
ing algorithm from optimization theory that im-
plements a learning bias derived from statistical 
theory. SVMs are based on the principle of struc-
tural risk minimization. Viewing the data as 
points in a high-dimensional feature space, the 
goal is to fit a hyperplane between the positive 
and negative examples so as to maximize the 
distance between the data points and the hyper-
plane. 
Given training examples: 
},1,1{,)},,(),...,,(),,{( 2211 +???= iill ynRxyxyxyxS  (1) 
ix is a feature vector (n dimension) of the i-th 
sample.  is the class (positive(+1) or nega-
tive(-1) class) label of the i-th sample. l  is the 
number of the given training samples. SVMs find 
an ?optimal? hyperplane:  to sepa-
rate the training data into two classes. The opti-
mal hyperplane can be found by solving the fol-
lowing quadratic programming problem (we 
leave the details to Vapnik (1998)): 
iy
0)( =+ bwx
.,...,2,1,0    ,0           tosubject
),(
2
1
               max
1 11
licy
Kyy   
ii
l
1i
i
l
i
l
j
jjii
l
i
i
=???=?
????
?
? ??
=
= ==
ji xx    (2) 
The function  is called 
kernel function, is the mapping from pri-
mary input space to feature space. Given a test 
example, its label y is decided by the following 
function: 
)()()( jiji xxx,x ???=K
)(x?
.]),(sgn[)( ?
?
+?=
sv
ii bKyf
ix
i xxx            (3) 
Basically, SVMs are binary classifiers, and 
can be extended to multi-class classifiers in order 
to solve multi-class discrimination problems. 
There are two popular methods to extend a bi-
nary classification task to that of K classes: one 
class vs. all others and pairwise. Here, we em-
ploy the simple pairwise method. This idea is to 
build  classifiers considering all 
pairs of classes, and final decision is given by 
their voting. 
2/)1( ?? KK
2.2 Recognition of Chinese Person Names 
Based on SVM 
We use a SVM-based chunker, YamCha (Kudo 
and Masumoto, 2001), to extract Chinese person 
names from the Chinese lexical analyzer. 
1) Chinese Person Names Chunk Tags 
We use the Inside/Outside representation for 
proper chunks: 
I    Current token is inside of a chunk. 
O   Current token is outside of any chunk. 
B   Current token is the beginning of a chunk. 
A chunk is considered as a Chinese person 
name in this case. Every character in the training 
set is given a tag classification of B, I or O, that 
is, },,{ OIByi ? . Here, the multi-class decision 
method pairwise is selected. 
2) Features Extraction for Chinese Person 
Names 
Since Chinese person names are identified 
from the segmented texts, the mistakes of word 
segmentation can result in error identification of 
person names. So we must break words into 
characters and extract features for every charac-
ter. Table 1 summarizes types of features and 
their values. 
 
Type of feature Value 
POS tag n-B, v-I, p-S 
Whether a character 
is a surname  Y or N 
Character surface form of the  character itself 
The frequency of a 
character in person 
names table 
Y or N 
Previous BIO tag B-character, I-character, O-character 
Table 1. Summary of Features and Their Values 
The POS tag from the output of lexical analy-
sis is subcategorized to include the position of 
the character in the word. The list of POS tags is 
shown in Table 2. 
 
POS tag Description of the position of the character in a word 
<POS>-S One-character word 
<POS>-B first character in a multi-character word 
<POS>-I intermediate character in a multi-character word 
<POS>-E last character in a multi-character word 
Table 2.  POS Tags in A Word 
If the character is a surname, the value is as-
signed to Y, otherwise assigned to N. 
The ?character? is surface form of the charac-
ter in the word. 
We extract all person names in January 1998 
of the People?s Daily to set up person names ta-
ble and calculate the frequency of every charac-
73
ter (F) of person names table in the training cor-
pus. The frequency of F is defined as 
,)(
  F of number  total  the
names person of character a as F of number the
FP = (4) 
if P(F) is greater than the given threshold, the 
value is assigned to Y, otherwise assigned to N.  
We also use previous BIO-tags as features. 
Whether a character is inside a person name or 
not, it depends on the context of the character. 
Therefore, we use contextual information of two 
previous and two successive characters of the 
current character as features. 
Figure 1 shows an example of features extrac-
tion for the i-th character. When training, the fea-
tures of the character ?Min? contains all the fea-
tures surrounded in the frames. If the same sen-
tence is used as testing, the same features are 
used. 
 
Position 
Character 
POS tags 
The frequency of a character in 
the person names table 
Previous BIO tags 
-2    -1    0   +1   +2
Jiang  Ze   Min  zhu  xi
n-S  n-B  n-E  n-B  n-E
Y     Y    Y    N    N
B     I    I    O    O
  i 
Whether the character 
is a surname 
Y     N    N    N    Y
 Figure 1.  An example of features extraction
 
3) Choosing Kernel Functions 
Here, we choose polynomial kernel functions: 
to build an optimal 
separating hyperplane. 
d
ii xxxxK ]1)[(),( +?=
2.3 Recognition of Chinese Location Names 
Based on SVM 
The identification process of location names is 
the same as that of person names except for the 
features extraction. Table 3 summarizes types of 
features and their values of location names ex-
traction. 
Type of feature Value 
POS tag n-B, v-I, p-S 
Whether a character 
appears in location names 
characteristic table 
Y or N 
Character surface form of the character itself 
Previous BIO tag 
B-character, I-
character, O-
character 
Table 3. Summary of Features and Their Values 
The location names characteristic table is set 
up in advance, and it includes the characters or 
words expressing the characteristics of location 
names such as ?sheng (province)?, ?shi (city)?, 
?xian (county)?etc. If the character is in the loca-
tion names characteristic table, the value is as-
signed to Y, otherwise assigned to N. 
3 Statistical Models 
Many statistical models for NER have been pre-
sented (Zhang et al, 1992; Huang et al, 2003 
etc). In this section, we proposed our statistical 
models for Chinese person names recognition 
and Chinese location names recognition.     
3.1 Chinese Person Names 
We define a function to evaluate the person name 
candidate PN. The evaluated function Total-
Probability(PN) is composed of two parts: the 
lexical probability LP(PN) and contextual prob-
ability CP(PN) based on POS tags. 
),()1()()( PNCPPNLPPNbilityTotalProba ??+?=  (5) 
where PN is the evaluated person name and ? is 
the balance cofficient. 
1) lexical probability LP(PN)    
We establish the surname table (SurName) and 
the first name table (FirstName) from the 
students of year 1999 in a university (containing 
9986 person names). 
Suppose PN=LF1F2, where L is the surname 
of the evaluated person name PN, Fi (i=1,2) is the 
i-th first name of the evaluated person name PN. 
The probability of the surname Pl(L) is defined 
as 
,
)(
)(
)(
0
0?
?
=
SurNamey
l
l
l yP
LP
LP                                (6) 
where ,  is the 
number of L as the single or multiple surname of 
person names in the SurName. 
)2)((log)( 20 += LNLPl )(LN
The probability of the first name Pf(F) is 
defined as 
,
)(
)(
)(
0
0
?
?
=
FirstNamey
f
f
f yP
FP
FP                                 (7) 
where ,  is the 
number of F in the FirstName. 
)2)((log)( 20 += FNFPf )(FN
The lexical probability of the person name PN 
is defined as 
 ,)FLFif(PN       FP   FPCLPPNLP
)LFif(PN                                FPLPPNLP
ffbl
fl
2121
11
))()(()()(
)()()(
=+??=
=?=  (8) 
74
where Cb is the balance cofficient between the 
single name and the double name. Here, 
Cb=0.844 (Huang et al, 2001). 
2) contextual probability based on POS tags 
CP(PN) 
Chinese person names have characteristic 
contexual POS tags in real Chinese texts, for 
example, in the phrase ?dui Zhangshuai shuo 
(say to Zhangshuai)?, the POS tag before the 
person name ?Zhangshuai? is prepnoun and verb 
occurs after the person name. We define the 
bigram contextual probability CP(PN) of the 
person name PN as the following equation: 
CP(PN)= ,),,(
TotalPOS
rposPNlposPersonPOS ><            (9) 
where lpos is the POS tag of the character before 
PN (called POS forward), rpos is the POS tag of 
the character after PN (called POS backward), 
and is the number 
of PN as a pereson name whose POS forward is 
lpos and POS backward is rpos in training corpus. 
 is the total number of the contexual 
POS tags of every person name in the whole 
training corpus. 
),,( >< rposPNlposPersonPOS
TotalPOS
3.2 Chinese Location Names 
We also define a function to evaluate the location 
name candidate LN. The evaluated function To-
talProbability(LN) is composed of two parts: the 
lexical probability LP (LN) and contextual prob-
ability CP (LN) based on POS tags. 
),()1()()( LNCPLNLPLNbilityTotalProba ??+?= (10) 
where LN is the evaluated location name and?  is 
the balance cofficient. 
1) lexical probability LP (LN) 
Suppose LN=F0F+S, F+=F1?Fn, (i=1,?,n), 
where F0 is the first character of the evaluated 
location name LN, F+ is the middle characters of 
the evaluated location name LN, S is the last 
character of the evaluated location name LN. 
The probability of the first character of the 
evaluated location name is defined as )( 0FPh
,
)(
)(
)(
00
00
0 FP
FP
FP
h
h
h ?=                                      (11) 
where ,  is the 
number of F
)2)((log)( 0200 += FCFPh )( 0FC
0 as the first character of location 
names in the Chinese Location Names Record.  
)2)((log)( 0200 +?=? FCFPh ,  is the total 
number of F
)( 0FC ?
0 in the Chinese Location Names 
Record. 
The probability of the middle character of the 
evaluated location name is defined as )( +FPf
,
)(
)(
)(
1
?
=
+
?=
n
i if
if
f FP
FP
FP                                  (12) 
where ,  is the 
number of F
)2)((log)( 2 += iif FCFP )( iFC
i as the i-th middle character of loca-
tion names in the Chinese Location Names Re-
cord. 
)2)((log)( 2 +?=? iif FCFP ,  is the total 
number of F
)( iFC ?
i in the Chinese Location Names 
Record. 
 The probability of the last character of the 
evaluated location name is defined as )(SPl
,
)(
)(
)(
SP
SP
SP
l
l
l ?=                                          (13) 
where ,  is the 
number of  S as the last character of location 
names in the Chinese Location Names Record. 
)2)((log)( 2 +?= SCSPl )(SC
)2)((log)( 2 +?=? SCSPl , )(SC ?  is the total number 
of S in the Chinese Location Names Record. 
The lexical probability of the location name 
LN is defined as                           
),(/))()()(( 0 LNLenSPFPFPLN lfh ++= +    (14) 
where Len(LN) is the length of the evaluated lo-
cation name LN.                                 
2) contextual probability based on POS tags CP 
(LN) 
Location names also have characteristic 
contexual POS tags in real Chinese texts, for 
example, in the phrase ?zai Chongqing shi 
junxing (to be held in Chongqing)?, the POS tag 
before the location name ?Chongqing?is 
prepnoun and verb occurs after the location name. 
We define the bigram contextual probability 
CP(LN) of the location name LN similar to that 
of the person name PN in equation (9), where PN 
is replaced with LN. 
4 Recognition of Chinese Named Entity 
Using Hybrid Model 
Analyzing the classification results (obtained by 
sole SVMs described in section 2) between B 
and I, B and O, I and O respectively, we find that 
the error is mainly caused by the second classifi-
cation. The samples which attribute to B class 
are misclassified to O class, which leads to B 
class vote?s diminishing and the corresponding 
named entities are lost. Therefore the Recall is 
lower. In the meantime, the number of the mis-
classified samples whose function distances to 
the hyperplane of SVM in feature space are less 
than 1 can reach over 83% of the number of total 
misclassified samples. That means the misclassi-
75
fication of a classifier is occurred in the region of 
two overlapping classes. Considering this fact, 
we can expect to improve SVM using the follow-
ing hybrid model. 
The hybrid model includes the following 
procedure: 
1) compute the distance from the test sample 
to the hyperplane of SVM in feature space.  
2) compare the distance with given threshold. 
The algorithm of hybrid model can be de-
scribed as follows: 
Suppose T is the testing set, 
(1) if  ??T , select Tx? , else stop; 
(2) compute  ?
=
+?= l
i
ii bxxKyxg
1
),()(
(3) if ?>)(xg , output 
, else use the statistic 
models and output the returned results. 
[ ]1,0??
))(sgn()( xgxf =
(4) , repeat(1) { }xTT ??
5 Experiments 
Our experimental results are all based on the 
corpus of Peking University. 
5.1 Extracting Chinese Person Names 
We use 180 thousand characters corpus of year 
1998 from the People?s Daily as the training cor-
pus and extract other sentences (containing 1526 
Chinese person names) as testing corpus to con-
duct an open test experiment. The results are ob-
tained as follows based on different models. 
1) Based on Sole SVM 
An experiment is carried out to recognize Chi-
nese person names based on sole SVM by the 
method as described in Section 2. The Recall, 
Precision and F-measure using different number 
of degree of polynomial kernel function are 
given in Table 4. The best result is obtained 
when d=2. 
 
 Recall Precision F-measure 
d=1 87.22% 94.26% 90.61% 
d=2 87.16% 96.10% 91.41% 
d=3 84.67% 95.14% 89.60% 
Table 4. Results for Person Names Extraction 
Based on Sole SVM 
 
2) Using Hybrid Model 
As mentioned in section 4, the test samples 
which attribute to B class are misclassified to O 
class and therefore the Recall for person names 
extraction from sole SVM is lower. So we only 
deal with the test samples (B class and O class) 
whose function distances to the hyperplane of 
SVM in feature space (i.e. g(x)) is between 0 and 
? . We move class-boundary learned by SVM 
towards the O class, that is, the O class samples 
are considered as B class in that area. 93.64% of 
the Chinese person names in testing corpus are 
recalled when ? =0.9 (Here, ?  also represents 
how much the boundary is moved). However, a 
number of non-person names are also identified 
as person names wrongly and the Precision is 
decreased correspondingly. Table 5 shows the 
Recall and Precision of person names extraction 
with different ? .  
 
 Recall Precision F-measure
? =1 93.05% 75.17% 83.16% 
? =0.9 93.64% 81.75% 87.29% 
? =0.8 93.51% 85.91% 89.55% 
? =0.7 93.05% 88.31% 90.62% 
? =0.6 92.39% 90.21% 91.29% 
? =0.5 91.81% 91.87% 91.84% 
? =0.4 91.02% 93.28% 92.13% 
? =0.3 90.56% 95.05% 92.75% 
? =0.2 90.03% 95.48% 92.68% 
? =0.1 88.66% 95.82% 92.10% 
Table 5. Results for Person Names Extraction 
with Different ?  
 
We use the evaluated function TotalProbabil-
ity(PN) as described in section 3 to filter the 
wrongly recalled person names using SVM. We 
tune?  in equation (5) to obtain the best results. 
The results based on the hybrid model with dif-
ferent ?  are listed in Table 6 (when d=2). We 
can observe that the result is best when ? =0.4. 
Table 7 shows the results based on the hybrid 
model with different ?  when =0.4. We can 
observe that the Recall rises and the Precision 
drops on the whole when 
?
?  increases. The syn-
thetic index F-measures are improved when ?  is 
between 0.1 and 0.8 compared with sole SVM. 
The best result is obtained when ? =0.3. The Re-
call and the F-measure increases 3.27% and 
1.77% respectively. 
 
 Recall Precision F-measure
? =0.1 90.37% 95.76% 92.99% 
? =0.2 90.37% 96.03% 93.11% 
? =0.3 90.43% 96.03% 93.15% 
? =0.4 90.43% 96.10% 93.18% 
? =0.5 90.63% 95.76% 93.13% 
? =0.6 90.43% 95.97% 93.12% 
76
? =0.7 90.43% 95.90% 93.09% 
? =0.8 90.43% 95.90% 93.09% 
? =0.9 90.37% 95.90% 93.05% 
Table 6. Results for Person Names Extraction 
Based on The Hybrid Model with Different?  
 
 Recall Precision F-measure
? =1 92.53% 84.96% 88.58% 
? =0.9 93.05% 88.81% 90.88% 
? =0.8 92.86% 90.95% 91.89% 
? =0.7 92.46% 92.04% 92.25% 
? =0.6 91.93% 93.22% 92.58% 
? =0.5 91.48% 94.26% 92.85% 
? =0.4 90.76% 95.25% 92.95% 
? =0.3 90.43% 96.10% 93.18% 
? =0.2 90.04% 96.15% 92.99% 
? =0.1 88.73% 96.23% 92.32% 
Table 7. Results for Person Names Extraction 
Based on The Hybrid Model ( =0.4) ?
5.2 Extracting Chinese Location Names 
We use 1.5M characters corpus of year 1998 
from the People?s Daily as the training corpus 
and extract sentences of year 2000 from the Peo-
ple?s Daily (containing 2919 Chinese location 
names) as testing corpus to conduct an open test 
experiment. The results are obtained as follows 
based on different models. 
1) Based on Sole SVM 
The Recall, Precision and F-measure using 
different number of degree of polynomial kernel 
function are given in Table 8. The best result is 
obtained when d=2. 
 
 Recall Precision F-measure 
d=1 84.66% 91.95% 88.16% 
d=2 86.69% 93.82% 90.12% 
d=3 86.27% 94.23% 90.07% 
Table 8. Results for Location Names Extraction 
Based on Sole SVM 
 
2) Using Hybrid Model 
The results for Chinese location names extrac-
tion based on the hybrid model are listed in Ta-
ble 9 (when d=2; ? =0.2 in equation (10)). We 
can observe that the Recall rises and the Preci-
sion drops on the whole when ?  increases. The 
synthetic index F-measures are improved when 
?  is between 0.1 and 0.7 compared with sole 
SVM. The best result is obtained when ? =0.3. 
The Recall increases 3.55%, the Precision de-
creases 1.05% and the F-measure increases 
1.37%. 
 Recall Precision F-measure
? =1 90.75% 83.00% 86.71% 
? =0.9 90.85% 85.33% 88.01% 
? =0.8 91.42% 87.42% 89.37% 
? =0.7 91.65% 89.05% 90.33% 
? =0.6 91.75% 90.38% 91.06% 
? =0.5 91.32% 90.98% 91.15% 
? =0.4 90.66% 91.87% 91.26% 
? =0.3 90.24% 92.77% 91.49% 
? =0.2 89.10% 93.28% 91.15% 
? =0.1 87.83% 93.38% 90.52% 
Table 9. Results for Location Names Extraction 
Based on The Hybrid Model (?=0.2) 
6 Comparison with other work 
The same corpus was also tested using statistics-
based approach to identify Chinese person names 
(Huang et al 2001) and location names (Huang 
and Yue, 2003). In their systems, lexical reliabil-
ity and contextual reliability were used to iden-
tify person names and location names calculated 
from statistical information drawn from a train-
ing corpus. The results of our models and the 
statistics-based methods (Huang 2001; Huang 
2003) are shown in Table 10 for comparison. We 
can see that the Recall and F-measure in our 
method all increase a lot.  
 
 Recall Precision F-measure
Our 
models 90.10% 96.15% 93.03%Person 
names Huang
(2001) 88.62% 92.37% 90.46%
Our 
models 90.24% 92.77% 91.49%Location 
names Huang
(2003) 86.86% 91.48% 89.11%
Table 10. Results of Our Method and Huang 
(2001; 2003) for Comparison 
7 Conclusions and Future work 
We recognize Chinese named entities using a 
hybrid model combining support vector ma-
chines with statistical methods. The model inte-
grates the advantages of two methods and the 
experimental results show that it can achieve 
higher F-measure than the sole SVM and indi-
vidual statistical approach. 
  Future work includes optimizing statistical 
models, for example, we can add the probability 
information of Chinese named entities in real 
texts to compute lexical probability, and we can 
77
also use trigram models to compute contextual 
probability. 
The hybrid model is expected to extend to for-
eign names in transliteration to obtain improved 
results by sole SVMs. The identification of trans-
literated names by SVMs has been completed (Li 
et al, 2004). The future work includes: set up 
statistical models for transliterated names and 
combine statistical models with SVMs to identify 
transliterated names. 
References 
William J. Black and Argyrios Vasilakopoulos. 2002. 
Language Independent Named Entity Classifica-
tion by Modified Transformation-based Learning 
and by Decision Tree Induction. The 6th Confer-
ence on Natural Language Learning, Taipei. 
Andrew Eliot Borthwick. 1999. A Maximum Entropy 
Approach to Named Entity Recognition. PhD Dis-
sertation. New York University. 
Xavier Carreras, Lluis Marquez, and Lluis Padro. 
2002. Named Entity Extraction Using AdaBoost. 
The 6th Conference on Natural Language Learning, 
Taipei. 
Michael Collins. 2002. Ranking Algorithms for 
Named-entity Extraction: Boosting and the Voted 
Perceptron. Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2002), Philadelphia, 489-496. 
Chooi-Ling Goh, Masayuki Asahara and Yuji Ma-
tsumoto. 2003. Chinese Unknown Word Identifica-
tion Based on Morphological Analysis and Chunk-
ing. The Companion Volume to the Proceedings of 
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), Sapporo, 197-200. 
De-Gen Huang, Yuan-Sheng Yang, and Xing Wang. 
2001. Identification of Chinese Names Based on 
Statistics. Journal of Chinese Information Process-
ing, 15(2): 31-37. 
De-Gen Huang and Guang-Ling Yue. 2003. Identifi-
cation of Chinese Place Names Based on Statistics. 
Journal of Chinese Information Processing, 17(2): 
46-52. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
Relevant Features. In Proceedings of the European 
Conference on Machine Learning, 1398:137-142. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with Support Vector Machines. In Proceedings of 
NAACL 2001. 
Li-Shuang Li, Chun-Rong Chen, De-Gen Huang and 
Yuan-Sheng Yang. 2004. Identifying Pronuncia-
tion-Translated Names from Chinese Texts Based 
on Support Vector Machines. Advances in Neural 
Networks-ISNN 2004, Lecture Notes in Computer 
Science, Berlin Heidelberg, 3173: 983-988. 
Scott Miller and Michael Crystal. 1998. BBN: De-
scription of the SIFT System as Used for MUC-7. 
Proceedings of 7th Message Understanding Con-
ference, Washington. 
David D. Palmer. 1997. A Trainable Rule-Based Al-
gorithm for Word Segmentation. In Proc of 35th of 
ACL & 8th conf. of EACL, 321-328. 
Wen Qin and Chun-Fa Yuan. 2004. Identification of 
Chinese Unknown Word Based on Decision Tree. 
Journal of Chinese Information Processing, 18(1): 
14-19. 
Erik Tjong Kim Sang. 2002. Memory-based Named 
Entity Recognition. The 6th Conference on Natural 
Language Learning, Taipei. 
Koichi Takeuchi and Nigel Collier. 2002. Use of 
Support Vector Machines in Extended Named En-
tity Recognition. The 6th Conference on Natural 
Language Learning, Taipei. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, Berlin. 
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley & Sons, New York. 
Ying Yu, Xiao-Long Wang, Bing-Quan Liu, and Hui 
Wang. 2004. Efficient SVM-based Recognition of 
Chinese Personal Names. High Technology Letters, 
10(3): 15-18. 
Jun-Sheng Zhang, Shun-De Chen, Ying Zheng, Xian-
Zhong Liu and Shu-Jin Ke. 1992. Large-Corpus-
Based Methods for Chinese Personal Name. Jour-
nal of Chinese Information Processing, 6(3): 7-15. 
Guo-Dong Zhou and Jian Su. 2002. Named Entity 
Recognition Using an HMM-based Chunk Tagger. 
Proceedings of the 40th Annual Meeting of the 
ACL, Philadelphia, 473-480. 
 
 
78
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 106?113,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
 
 
Exploiting Multi-Features to Detect Hedges and Their Scope in 
Biomedical Texts
 
Huiwei Zhou1, Xiaoyan Li2, Degen Huang3, Zezhong Li4, Yuansheng Yang5 
Dalian University of Technology 
Dalian, Liaoning, China 
{1zhouhuiwei, 3huangdg, 5yangys}@dlut.edu.cn 
2lixiaoyan@mail.dlut.edu.cn 
4lizezhonglaile@163.com 
Abstract 
In this paper, we present a machine learning 
approach that detects hedge cues and their 
scope in biomedical texts. Identifying hedged 
information in texts is a kind of semantic 
filtering of texts and it is important since it 
could extract speculative information from 
factual information. In order to deal with the 
semantic analysis problem, various evidential 
features are proposed and integrated through a 
Conditional Random Fields (CRFs) model. 
Hedge cues that appear in the training dataset 
are regarded as keywords and employed as an 
important feature in hedge cue identification 
system. For the scope finding, we construct a 
CRF-based system and a syntactic 
pattern-based system, and compare their 
performances. Experiments using test data 
from CoNLL-2010 shared task show that our 
proposed method is robust. F-score of the 
biological hedge detection task and scope 
finding task achieves 86.32% and 54.18% in 
in-domain evaluations respectively. 
1. Introduction 
Identifying sentences in natural language texts 
which contain unreliable or uncertain information 
is an increasingly important task of information 
extraction since the extracted information that 
falls in the scope of hedge cues cannot be 
presented as factual information. Szarvas et al 
(2008) report that 17.69% of the sentences in the 
abstracts section of the BioScope corpus and 
22.29% of the sentences in the full papers section 
contain hedge cues. Light et al (2004) estimate 
that 11% of sentences in MEDLINE abstracts 
contain speculative fragments. Szarvas (2008) 
reports that 32.41% of gene names mentioned in 
the hedge classification dataset described in 
Medlock and Briscoe (2007) appear in a 
speculative sentence. Many Wikipedia articles 
contain a specific weasel tag which mark 
sentences as non-factual (Ganter and Strube, 
2009). 
There are some Natural Language Processing 
(NLP) researches that demonstrate the benefit of 
hedge detection experimentally in several 
subjects, such as the ICD-9-CM coding of 
radiology reports and gene named Entity 
Extraction (Szarvas, 2008), question answering 
systems (Riloff et al, 2003), information 
extraction from biomedical texts (Medlock and 
Briscoe, 2007). 
The CoNLL-2010 Shared Task (Farkas et al, 
2010) ?Learning to detect hedges and their scope 
in natural language text? proposed two tasks 
related to speculation research. Task 1 aimed to 
identify sentences containing uncertainty and 
Task 2 aimed to resolve the in-sentence scope of 
hedge cues. We participated in both tasks. 
In this paper, a machine learning system is 
constructed to detect sentences in texts which 
contain uncertain or unreliable information and to 
find the scope of hedge cues. The system works 
in two phases: in the first phase uncertain 
sentences are detected, and in the second phase 
in-sentence scopes of hedge cues are found. In the 
uncertain information detecting phase, hedge 
cues play an important role. The sentences that 
contain at least one hedge cue are considered as 
uncertain, while sentences without cues are 
considered as factual. Therefore, the task of 
uncertain information detection can be converted 
into the task of hedge cue identification. Hedge 
cues that appear in the training dataset are 
collected and used as keywords to find hedges. 
Furthermore, the detected keywords are 
employed as an important feature in hedge cue 
identification system. In addition to keywords, 
various evidential features are proposed and 
integrated through a machine learning model. 
Finding the scope of a hedge cue is to determine 
at sentence level which words are affected by the 
106
  
hedge cue. In the scope finding phase, we 
construct a machine learning-based system and a 
syntactic pattern-based system, and compare their 
performances. 
For the learning algorithm, Conditional random 
fields (CRFs) is adopted relying on its flexible 
feature designs and good performance in 
sequence labeling problems as described in 
Lafferty et al (2001). The main idea of CRFs is 
to estimate a conditional probability distribution 
over label sequences, rather than over local 
directed label sequences as with Hidden Markov 
Models (Baum and Petrie, 1966) and Maximum 
Entropy Markov Models (McCallum et al, 
2000). 
Evaluation is carried out on the CoNLL-2010 
shared task (Farkas et al, 2010) dataset in which 
sentences containing uncertain information are 
annotated. For the task of detecting uncertain 
information, uncertain cues are annotated. And 
for the task of finding scopes of hedge cues, 
hedge cues and their scope are annotated as 
shown in sentence (a): hedge cue indicate that, 
and its scope indicate that dhtt is widely 
expressed at low levels during all stages of 
Drosophila development are annotated. 
 
(a)Together, these data <xcope 
id="X8.74.1"><cue ref="X8.74.1" 
type="speculation">indicate that</cue> dhtt 
is widely expressed at low levels during all 
stages of Drosophila development</xcope>. 
2. Related Work 
In the past few years, a number of studies on 
hedge detection from NLP perspective have been 
proposed. Elkin et al (2005) exploited 
handcrafted rule-based negation/uncertainty 
detection modules to detect the negation or 
uncertainty information. However, their detection 
modules were hard to develop due to the lack of 
standard corpora that used for evaluating the 
automatic detection and scope resolution. Szarvas 
et al (2008) constructed a corpus annotated for 
negations, speculations and their linguistic scopes. 
It provides a common resource for the training, 
testing and comparison of biomedical NLP 
systems. 
Medlock and Briscoe (2007) proposed an 
automatic classification of hedging in biomedical 
texts using weakly supervised machine learning. 
They started with a very limited amount of 
annotator-labeled seed data. Then they iterated 
and acquired more training seeds without much 
manual intervention. The best classifier using 
their model achieved 0.76 precision/recall 
break-even-point (BEP). Further, Medlock 
(2008) illuminated the hedge identification task 
including annotation guidelines, theoretical 
analysis and discussion. He argued for separation 
of the acquisition and classification phases in 
semi-supervised machine learning method and 
presented a probabilistic acquisition model. In 
probabilistic model he assumed bigrams and 
single terms as features based on the intuition that 
many hedge cues are bigrams and single terms 
and achieves a peak performance of around 0.82 
BEP.  
Morante and Daelemans (2009) presented a 
meta-learning system that finds the scope of 
hedge cues in biomedical texts. The system 
worked in two phases: in the first phase hedge 
cues are identified, and in the second phase the 
full scopes of these hedge cues are found. The 
performance of the system is tested on three 
subcorpora of the BioScope corpus. In the hedge 
finding phase, the system achieves an F-score of 
84.77% in the abstracts subcorpus. In the scope 
finding phase, the system with predicted hedge 
cues achieves an F-score of 78.54% in the 
abstracts subcorpus. 
The research on detecting uncertain 
information is not restricted to analyze 
biomedical documents. Ganter and Strube (2009) 
investigated Wikipedia as a source of training 
data for the automatic hedge detection using word 
frequency measures and syntactic patterns. They 
showed that the syntactic patterns worked better 
when using the manually annotated test data, 
word frequency and distance to the weasel tag 
was sufficient when using Wikipedia weasel tags 
themselves. 
3. Identifying Hedge Cues 
Previous studies (Light et al, 2004) showed that 
the detection of hedging could be solved 
effectively by looking for specific keywords 
which were useful for deciding whether a 
sentence was speculative. Szarvas (2008) reduces 
the number of keyword candidates without 
excluding helpful keywords for hedge 
classification. Here we also use a simple 
keyword-based hedge cue detection method. 
3.1 Keyword-based Hedge Cue Detection 
In order to recall as many hedge cues as possible, 
107
  
all hedge cues that appear in the training dataset 
are used as keywords. Hedge cues are represented 
by one or more tokens. The list of all hedge cues 
in the training dataset is comprised of 143 cues. 
90 hedge cues are unigrams, 24 hedge cues are 
bigrams, and the others are trigrams, four-grams 
and five-grams. Besides, hedge cues that appear 
in the training dataset and their synonyms in 
WordNet 1  are also selected as keywords for 
hedge cue detection. The complete list of them 
contains 438 keywords, 359 of which are 
unigrams. Many tokens appear in different grams 
cues, such as possibility appears in five-grams 
cue cannot rule out the possibility, four-gram cue 
cannot exclude the possibility, trigrams cue raise 
the possibility and unigram cue possibility. To 
find the complete cues, keywords are matched 
through a maximum matching method (MM) (Liu 
et al, 1994). For example, though indicate and 
indicate that are both in keywords list, indicate 
that is extracted as a keyword in sentence (a) 
through MM. 
3.2 CRF-based Hedge Cue Detection 
Candidate cues are extracted based on keywords 
list in keyword-based hedge cue detection stage. 
But the hedge cue is extremely ambiguous, so 
CRFs are applied to correct the false 
identification results that occurred in the 
keyword-based hedge cue detection stage. The 
extracted hedge cues are used as one feature for 
CRFs-based hedge cue detection. 
A CRF identifying model is generated by 
applying a CRF tool to hedge cue labeled 
sequences. Firstly, hedge cue labeled sentences 
are transformed into a set of tokenized word 
sequences with IOB2 labels: 
 
B-cue Current token is the beginning of a 
hedge cue 
I-cue Current token is inside of  a hedge cue 
O Current token is outside of any hedge 
cue  
 
For sentence (a) the system assigns the B-cue 
tag to indicate, the I-cue tag to that and the O tag 
to the rest of tokens as shown in Figure1. 
The hedge cues that are found by 
keyword-based method is also given IOB2 labels 
feature as shown in Figure1. 
                                                          
1
 Available at http://wordnet.princeton.edu/ 
 
 
 
 
 
 
 
 
 
 
Text 
? 
these 
data 
indicate 
that 
dhtt 
is 
... 
Keyword Labels Feature 
...       
O   
O 
B 
I 
O 
O 
...                            
Cue Labels  
...       
O   
O 
B-cue 
I-cue 
O 
O 
...                          
 
Figure 1: Example of Cues labels and Keywords 
labels Feature 
 
Diverse features including keyword feature are 
employed to our CRF-based hedge cue detection 
system. 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
Where Word (0) is the current word, Word (-1) 
is the first word to the left, Word (1) is the first 
word to the right, etc. 
 
(2) Stem Features 
The motivation for stemming in hedge 
identification is that distinct morphological forms 
of hedge cues are used to convey the same 
semantics (Medlock, 2008). In our method, 
GENIA Tagger2 (Tsuruoka et al, 2005) is applied 
to get stem features. 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where Stem (0) is the stem for the current word, 
Stem(-1) is the first stem to the left, Stem (1) is the 
first stem to the right, etc. 
 
(3) Part-Of-Speech Features  
Since most of hedge cues in the training dataset 
are verbs, auxiliaries, adjectives and adverbs. 
Therefore, Part-of-Speech (POS) may provide 
useful evidence about the hedge cues and their 
boundaries. GENIA Tagger is also used to 
generate this feature.  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where POS (0) is the current POS, POS (-1) is 
the first POS to the left, POS (1) is the first POS 
to the right, etc. 
 
(4) Chunk Features 
Some hedge cues are chunks consisting of more 
than one token. Chunk features may contribute to 
the hedge cue boundaries. We use GENIA 
Tagger to get chunk features for each token. The 
                                                          
2
 Available at 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 
108
  
chunk features include unigram, bigram, and 
trigram types, listed as follows: 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
where Chunk (0) is the chunk label for the 
current word, Chunk (?1) is the chunk label for 
the first word to the left , Chunk (1) is the chunk 
label for the first word to the right, etc. 
 
(5) Keyword Features 
Keyword labels feature is an important feature. 
? Keyword (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Keyword (0) is the current keyword label, 
Keywords (-1) is the keyword label for the first 
keyword to the left, Keywords (1) is the keyword 
label for the first keyword to the right, etc. 
Feature sets can be easily redefined by 
changing the window size n. The relationship of 
the window size and the F-score observed in our 
experiments will be reported in Section 5. 
4. Hedge Scope Finding 
In this task, a CRFs classifier is applied to predict 
for all the tokens in the sentence whether a token 
is the first token of the scope sequence (F-scope), 
the last token of the scope sequence (L-scope), or 
neither (None). For sentence (a) in Section 1, the 
classifier assigns F-scope to indicate, L-scope to 
benchmarks, and None to the rest of the tokens. 
Only sentences that assigned cues in the first 
phase are selected for hedge scope finding. 
Besides, a syntactic pattern-based system is 
constructed, and compared with the CRF-based 
system. 
4.1 CRF-based System 
The features that used in CRF-based hedge cue 
detection systems are also used for scope finding 
except for the keyword features. The features are: 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(2) Stem Features 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
(3) Part-Of-Speech Features  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(4) Chunk Features 
The chunk features include unigram, bigram, 
and trigram types, listed as follows: 
 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
 
(5) Hedge cues Features 
Hedge cues labels that are doped out in Task 1 
are selected as an important feature. 
 
? Hedge cues (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Hedge cues (0) is the cue label for the 
current word, Hedge cues (?1) is the cue label for 
the first word to the left , Hedge cues (1) is the 
cue label for the first word to the right, etc. 
The scope of the sequence must be consistent 
with the hedge cues. That means that the number 
of the F-scope and L-scope must be the same with 
the hedge cues. However, sometimes their 
number predicted by classifier is not same. 
Therefore, we need to process the output of the 
classifier to get the complete sequence of the 
scope. The following post processing rules are 
adapted. 
 
? If the number of F-scope, L-scope and hedge 
cue is the same, the sequence will start at the 
token predicted as F-scope, and end at the 
token predicted as L-scope. 
? If one token has been predicted as F-scope 
and none has been predicted as L-scope, the 
sequence will start at the token predicted as 
F-scope and end at the end of the sentence. 
Since when marking the scopes of keywords, 
linguists always extend the scope to the biggest 
syntactic unit possible. 
? If one token has been predicted as L-scope 
and none has been predicted as F-scope, the 
sequence will start at the hedge cue and end at 
the token predicted as L-scope. Since scopes 
must contain their cues. 
? If one token has been predicted as F-scope 
and more than one has been predicted as 
L-scope, the sequence will end at the first token 
predicted as L-scope. Statistics from prediction 
on CoNLL-2010 Shared Task evaluation data 
show that 20 sentences are in this case. And the 
scope of 6 sentences extends to the first 
L-scope, and the scope of 3 sentences end at 
the last L-scope, the others are predicted 
mistakenly. Our system prediction and 
gold-standard annotation are shown in sentence 
(b1) and (b2) respectively. 
109
  
 
(b1) our system annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" type="speculation">may</cue> 
be more potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling</xcope> [16]. 
 
(b2) gold-standard annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" 
type="speculation">may</cue> be more 
potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling [16]. 
 
? If one token has been predicted as L-scope 
and more than one has been predicted as 
F-scope, the sequence will start at the first 
token predicted as F-scope. 
? If an L-scope is predicted before an F-scope, 
the sequence will start at the token predicted as 
F-scope, and finished at the end of the sentence.  
4.2 Syntactic Pattern-based System 
Hedge scopes usually can be determined on the 
basis of syntactic patterns dependent on the cue. 
Therefore, a syntactic pattern-based system is 
also implemented for hedge scope finding. When 
the sentence is predicted as uncertain, the toolkit 
of Stanford Parser3 (Klein and Manning, 2003) is 
utilized to parse the sentence into a syntactic tree, 
which can release a lot of information about the 
grammatical structure of sentences that is 
beneficial for the finding of hedge scope. For 
sentence (c) the Stanford Parser gives the 
syntactic tree as showed in Figure 2. 
 
(c) This <xcope id="X*.*.*"><cue ref="X*.*.*" 
type="speculation"> may </cue> represent a 
viral illness</xcope>. 
It is obvious to see from the syntactic tree, all 
the words of the parsed sentence concentrate at 
the places of leaves. We use the following rules to 
find the scope. 
? If the tag of the word is ?B-cue?, it is predicted 
as F-scope. 
? If the POS of the hedge cue is verbs and 
auxiliaries, the L-scope is signed at the end of the 
clause. 
? If the POS of the hedge cue is attributive 
                                                          
3
 Available at 
http://nlp.stanford.edu/software/lex-parser.shtml  
adjectives, the L-scope is signed at the following 
noun phrase.  
? If the POS of the hedge cue is prepositions, the 
L-scope is signed at the following noun phrase. 
? If none of the above rules apply, the scope of a 
hedge cue starts with the hedge cue and ends at 
the following clause. 
 
 
 
Figure 2: Syntactic tree parsed by Stanford 
Parser 
5. Experiments and Discussion 
We evaluate our method using CoNLL-2010 
shared task dataset. The evaluation of uncertain 
information detection task is carried out using the 
sentence-level F-score of the uncertainty class. 
As mentioned in Section 1, Task 1 is converted 
into the task of hedge cues identification. 
Sentences can be classified as certain or uncertain 
according to the presence or absence of a few 
hedge cues within the sentences. In task of 
finding in-sentence scopes of hedge cues, a scope 
is correct if all the tokens in the sentence have 
been assigned the correct scope class for a 
specific hedge signal. 
5.1 Detecting Uncertain Information 
In the CoNLL-2010 Shared Task 1, our 
in-domain system obtained the F-score of 85.77%. 
Sentence-level results of in-domain systems 
under the condition n=3 (window size) are 
summarized in Table 1.  
 
System Prec. Recall F-score 
Keyword-based 41.15 99.24 58.18 
CRF-based system 
(without keyword 
features) 
88.66 80.13 84.18 
CRF-based system 
+ keyword features 
86.21 84.68 85.44 
CRF-based system 86.49 85.06 85.77 
110
  
+ keyword features 
+ MM 
 
Table 1: Official in-domain results for Task 1 
(n=3) 
 
The keyword-based system extracts hedge cues 
through maximum matching method (MM). As 
can be seen in Table 1, the system achieves a high 
recall (99.24%). This can be explained that 
almost all of the hedge cues in the test dataset are 
in the keywords list. However, it also brings 
about the low precision since not all potential 
speculative keywords convey real speculation. So 
the keyword-based method can be combined with 
our CRF-based method to get better performance. 
All the CRF-based systems in Table 1 
significantly outperform the keyword-based 
system, since the multi-features achieve a high 
precision. And the result with keyword features is 
better than the result without it. The keyword 
features improve the performance by recalling 39 
true positives. In addition, further improvement is 
achieved by using Maximum Matching method 
(MM). 
In the test dataset, there should be a few hedge 
cues not in the training dataset. And the 
additional resources besides the manually labeled 
data are allowed for in-domain predictions. 
Therefore, the synonyms of the keywords can be 
used for in-domain systems. The synonyms of the 
keywords are added to the keywords list, and are 
expected to improve detecting performance. The 
synonyms are obtained from WordNet. 
Table 2 shows the relationship between the 
window size and the sentence-level results. This 
table shows the results with and without 
synonyms. Generally, the results with synonyms 
are better than the results without them. With 
respect to window size, the wider the window 
size, the better precision can be achieved. 
However, large window size leads to low recall 
which is probably because of data sparse. The 
best F-score 86.32 is obtained when the window 
size is +/-4. 
 
Window 
size 
Synonym
s 
 
Prec. Recall F-score 
without 
synonyms 
85.27 86.46 85.86 1 
with 
synonyms 
85.66 86.20 85.93 
without 
synonyms 
86.35 85.70 86.02 2 
with 86.14 84.94 85.53 
without 
synonyms 
86.49 85.06 85.77 3 
with 
synonyms 
86.69 84.94 85.81 
without 
synonyms 
86.34 84.81 85.57 4 
with 
synonyms 
87.21 85.44 86.32 
 
Table 2: Sentence-level results relative to 
synonyms and window size for speculation 
detection 
5.2 Finding Hedge Scope 
In the CoNLL-2010 Shared Task 2, our 
in-domain system obtained the F-score of 44.42%. 
Table 3 shows the scope finding results. For 
in-domain scope finding system, we use the 
hedge cues extracted by the submitted CRF-based 
in-domain system (the best result 85.77 in Table 
1). The result of the syntactic pattern-based 
system is not ideal probably due to the syntactic 
parsing errors and limited annotation rules. 
 
System Prec. Recall F-score 
syntactic pattern-based 44.31 42.59 43.45 
CRF-based 45.32 43.56 44.42 
 
Table 3: Official in-domain results for Task 2 
 
Through analyzing the false of our scope 
finding system, we found that many of our false 
scope were caused by such scope as sentence (d1) 
shows. Our CRF-based system signed the 
L-scope to the end of sentence mistakenly. The 
incorrectly annotation of our system and 
gold-standard annotation are shown in sentence 
(d1) and (d2) respectively. So an additional rule is 
added to our CRF-based system to correct the 
L-scope. The rule is: 
? If one token has been predicted as L-scope, 
and if the previous token is ?)?, or ?]?, the 
L-scope will be modified just before the 
paired token ?(? or ?[?. 
 
(d1) The incorrectly predicted version: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic</xcope> (85).  
(d2) Gold-standard annotation: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic (85) </xcope>. 
 
111
  
F-score is reached to 51.83 by combining this 
additional rule with the submitted CRF-based 
in-domain system as shown in Table 4. 
 
TP FP FN Prec. Recall F-score 
525 468 508 52.87 50.82 51.83 
 
Table 4: Official in-domain results for Task 2 
 
Several best results of Task 1 are exploited to 
investigate the relationship between the window 
size and the scope finding results. From the 
results of Table 5, we can see that the case of n=4 
gives the best precision, recall and F-score. And 
the case of n=2 and the case of n=3 based on the 
same task 1 system have a very similar score. 
With respect to the different systems of Task 1, in 
principle, the higher the F-score of Task 1, the 
better the performance of Task 2 can be expected. 
However, the result is somewhat different from 
the expectation. The best F-score of Task 2 is 
obtained under the case F-score (task 1) =86.02. 
This indicates that it is not certain that Task 2 
system based on the best Task 1 result gives the 
best scope finding performance.  
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.32 
52.59 
52.90 
51.69 
50.05 
50.34 
52.98 
51.29 
51.59 
86.02 4 
3 
2 
54.85 
53.13 
53.13 
52.57 
50.92 
50.92 
53.68 
52.00 
52.00 
85.86 4 
3 
2 
54.19 
52.50 
52.50 
52.57 
50.92 
50.92 
53.37 
51.70 
51.70 
 
Table 5: Scope finding results relative to the 
results of task 1 and window size 
 
In the case that scopes longer than n (window 
size) words, the relevant cue will thus not fall into 
the +/-n word window of the L-scope and all 
hedge cue features will be O tag. The hedge cue 
features will be useless for detecting L-scopes. 
Taking into account the importance of hedge cue 
features, the following additional features are 
also incorporated to capture hedge cue features. 
 
? Distance to the closest preceding hedge cue 
? Distance to the closest following hedge cue 
? Stem of the closest preceding hedge cue 
? Stem of the closest following hedge cue  
? POS of the closest preceding hedge cue 
? POS of the closest following hedge cue 
 
Table 6 shows the results when the additional 
hedge cue features are used. The results with 
additional hedge cue feature set are constantly 
better than the results without them. In most of 
cases, the improvement is significant. The best 
F-score 54.18% is achieved under the case 
F-score (task 1) =86.02 and n=4. 
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.73 
54.22 
53.41 
52.08 
51.60 
50.82 
53.37 
52.88 
52.08 
86.02 4 
3 
2 
55.35 
54.75 
53.94 
53.05 
52.47 
51.69 
54.18 
53.58 
52.79 
85.86 4 
3 
2 
54.49 
53.79 
53.09 
52.86 
52.18 
51.50 
53.66 
52.97 
52.29 
 
Table 6: Scope finding results relative to the 
results of Task 1 and window size with additional 
cue features 
 
The upper-bound results of CRF-based system 
assuming gold-standard annotation of hedge cues 
are show in Table 7. 
 
TP FP FN Prec. Recall F-score 
618 427 415 59.14 59.83 59.48 
 
Table 7: Scope finding result with gold-standard 
hedge signals 
 
A comparative character analysis of syntactic 
pattern-based method and CRF-based method 
will be interesting, which can provide insights 
leading to better methods in the future. 
6. Conclusion 
In this paper, we have exploited various useful 
features evident to detect hedge cues and their 
scope in biomedical texts. For hedge detection 
task, keyword-based system is integrated with 
CRF-based system by introducing keyword 
features to CRF-based system. Our experimental 
results show that the proposed method improves 
the performance of CRF-based system by the 
additional keyword features. Our system has 
achieved a state of the art F-score 86.32% on the 
sentence-level evaluation. For scope finding task, 
112
  
two different systems are established: CRF-based 
and syntactic pattern-based system. CRF-based 
system outperforms syntactic pattern-based 
system due to its evidential features. 
In the near future, we will improve the hedge 
cue detection performance by investigating more 
implicit information of potential keywords. On 
the other hand, we will study on how to improve 
scope finding performance by integrating 
CRF-based and syntactic pattern-based scope 
finding systems. 
References 
Leonard E. Baum, and Ted Petrie. 1966. Statistical 
inference for probabilistic functions of finite state 
Markov chains. Annals of Mathematical 
Statistics, 37(6):1554?1563. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, 
Casey S. Husser, William Carruth, Larry R. 
Bergstrom, and Dietlind L. Wahner-Roedler. 2005. 
A controlled trial of automated classification of 
negation from clinical notes. BMC Medical 
Informatics and Decision Making, 5(13). 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of CoNLL-2010: Shared Task, 
2010, pages 1?12. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
wikipedia tags and shallow linguistic features. In 
Proceedings of the ACL-IJCNLP 2009 
Conference Short Papers, pages 173?176. 
Dan Klein, and Christopher D. Manning. 2003. 
Accurate unlexicalized parsing. In Proceedings of 
the 41st Meeting of the Association for 
Computational Linguistics, pages 423?430. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data. In Proceedings of the Eighteenth 
International Conference on Machine 
Learning, pages 282?289. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The language of bioscience: facts, 
speculations, and statements in between. In 
HLT-NAACL 2004 Workshop: BioLINK 2004, 
Linking Biological Literature, Ontologies and 
Databases, pages 17?24. 
Yuan Liu, Qiang Tan, and Kunxu Shen. 1994. The 
word segmentation rules and automatic word 
segmentation methods for Chinese information 
processing. QingHua University Press and 
GuangXi Science and Technology Press. 
Andrew McCallum, Dayne Freitag, and Fernando 
Pereira. 2000. Maximum entropy Markov models 
for information extraction and segmentation. In 
Proceedings of ICML 2000, pages 591?598. 
Ben Medlock. 2008. Exploring hedge identification in 
biomedical literature. Journal of Biomedical 
Informatics, 41(4):636?654. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL-07, 
pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP, ACL 2009, pages 28?36. 
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the 7th 
Conference on Computational Natural 
Language Learning, pages 25?32. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised selection 
of keywords. In Proceedings of ACL: HLT, pages 
281?289. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
biomedical texts annotated for uncertainty, negation 
and their scopes. In Proceedings of BioNLP 2008: 
Current Trends in Biomedical Natural 
Language, pages 38?45. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics 2005, pages 382?392.
 
113
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 66?70,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Combining Syntactic and Semantic Features by SVM for Unrestricted
Coreference Resolution
Huiwei Zhou1, Yao Li2, Degen Huang3, Yan Zhang4, Chunlong Wu5, Yuansheng Yang6
Dalian University of Technology
Dalian, Liaoning, China
{1zhouhuiwei,3huangdg,6yangys}@dlut.edu.cn
2tianshanyao@mail.dlut.edu.cn
4zhangyan zyzy@yeah.net
5wuchunlong@gmail.com
Abstract
The paper presents a system for the CoNLL-
2011 share task of coreference resolution. The
system composes of two components: one for
mentions detection and another one for their
coreference resolution. For mentions detec-
tion, we adopted a number of heuristic rules
from syntactic parse tree perspective. For
coreference resolution, we apply SVM by ex-
ploiting multiple syntactic and semantic fea-
tures. The experiments on the CoNLL-2011
corpus show that our rule-based mention iden-
tification system obtains a recall of 87.69%,
and the best result of the SVM-based corefer-
ence resolution system is an average F-score
50.92% of the MUC, B-CUBED and CEAFE
metrics.
1 Introduction
Coreference resolution, defined as finding the dif-
ferent mentions in a document which refer to the
same entity in reality, is an important subject in Nat-
ural Language Processing. In particular, coreference
resolution is a critical component of information ex-
traction systems (Chinchor and Nancy, 1998; Sund-
heim and Beth, 1995) and a series of coreference
resolution tasks have been introduced and evaluated
from MUC (MUC-6, 1995). Some machine learning
approaches have been applied to coreference resolu-
tion (Soon et al, 2001; Ng and Cardie, 2002; Bengt-
son and Roth, 2008; Stoyanov et al, 2009). Soon
et al(2001) use a decision tree classifier to decide
whether two mentions in a document are coreferen-
t. Bergsma and Lin (2006) exploit an effective fea-
ture of gender and number to a pronoun resolution
system and improve the performance significantly,
which is also appeared in our feature set. Howev-
er, automatic coreference resolution is a hard task
since it needs both syntactic and semantic knowl-
edge and some intra-document knowledge. To im-
prove the performance further, many deep knowl-
edge resources like shallow syntactic and seman-
tic knowledge are exploited for coreference resolu-
tion (Harabagiu et al, 2001; McCallum and Well-
ner, 2004; Denis and Baldridge, 2007; Ponzetto and
Strube, 2005; Versley, 2007; Ng, 2007). In order to
make use of more syntactic information, Kong et al
(2010) employ a tree kernel to anaphoricity determi-
nation for coreference resolution and show that ap-
plying proper tree structure in corefernce resolution
can achieve a good performance.
The CoNLL-2011 Share Task (Pradhan et
al., 2011) ?Modeling Unrestricted Coreference in
OntoNotes? proposes a task about unrestricted
coreference resolution, which aims to recognize
mentions and find coreference chains in one docu-
ment. We participate in the closed test.
In this paper, we exploit multi-features to a
coreference resolution system for the CONLL-2011
Share Task, including flat features and a tree struc-
ture feature. The task is divided into two steps in
our system. In the first step, we adopt some heuristic
rules to recognize mentions which may be in a coref-
erence chain; in the second step, we exploit a num-
ber of features to a support vector machine (SVM)
classifier to resolute unrestricted coreference. The
experiments show that our system gets a reasonable
result.
The rest of the paper is organized as follows. In
66
Section 2, we describe in detail how our system does
the work of coreference resolution, including how
we recognize mentions and how we mark the coref-
erence chains. The experimental results are dis-
cussed in Section 3. Finally in Section 4, we give
some conclusion.
2 The Coreference Resolution System
The task of coreference resolution is divided into
two steps in our system: mentions detection and
coreference resolution. In the first step, we use some
heuristic rules to extract mentions which may re-
fer to an entity. In the second step, we make up
mention-pairs with the mentions extracted in the
first step, and then classify the mention-pairs in-
to two groups with an SVM model: Coreferent or
NotCoreferent. Finally we get several coreference
chains in a document according to the result of clas-
sification. Each coreference chain stands for one en-
tity.
2.1 Rule-based Identification of Mentions
The first step for coreference resolution is to identify
mentions from a sequence of words. We have tried
the machine-learning method detecting the bound-
ary of a mention. But the recall cannot reach a high
level, which will lead to bad performance of coref-
erence resolution. So we replace it with a rule-based
method. After a comprehensive study, we find that
mentions are always relating to pronouns, named en-
tities, definite noun phrases or demonstrative noun
phrases. So we adopt the following 5 heuristic rules
to extract predicted mentions:
1. If a word is a pronoun, then it is a mention.
2. If a word is a possessive pronoun or a posses-
sive, then the smallest noun phrase containing
this word is a mention.
3. If a word string is a named entity, then it is a
mention.
4. If a word string is a named entity, then the s-
mallest noun phrase containing it is a mention.
5. If a word is a determiner (a, an, the, this, these,
that, etc.), then all the noun phrase beginning
with this word is a mention.
2.2 Coreference Resolution with
Multi-Features
The second step is to mark the coreference chain us-
ing the model trained by an SVM classifier. We ex-
tract the marked mentions from the training data and
take mention-pairs in one document as instances to
train the SVM classifier like Soon et al(2001) . The
mentions with the same coreference id form the pos-
itive instances while those between the nearest posi-
tive mention-pair form the negative instance with the
second mention of the mention-pair.
The following features are commonly used in
NLP processes, which are also used in our system:
? i-NamedEntity/j-NamedEntity: the named en-
tity the mention i/j belongs to
? i-SemanticRole/j-SemanticRole: the semantic
role the mention i/j belongs to which
? i-POSChain/j-POSChain: the POS chain of the
mention i/j
? i-Verb/j-Verb: the verb of the mention i/j
? i-VerbFramesetID/j-VerbFramesetID: the verb
frameset ID of the mention i/j, which works to-
gether with i/j-Verb
All the 5 kinds of features above belong to a sin-
gle mention. For mention-pairs, there are another 4
kinds of features as below:
? StringMatch: after cutting the articles, 1 if the
two mentions can match completely, 2 if one is
a substring of the other, 3 if they partly match,
4 else.
? IsAlias: after cutting the articles, 1 if one men-
tion is the name alias or the abbreviation of the
other one, 0 else
? Distance: it is the number of sentences between
two mentions, 0 if the two mentions are from
one sentenci-Verb/j-Verb: the verb of the men-
tion i/j
? SpeakerAgreement: 1 if both the speakers of
the two mentions are unknown, 2 if both the
two mentions come from the same speaker, 3 if
the mentions comes from different speakers.
67
All of the 14 simple and effective features above
are applied in the baseline system, which use the
same method with our system. But coreference res-
olution needs more features to make full use of the
intra-documental knowledge, so we employ the fol-
lowing 3 kinds of features to our system to catch
more information about the context.
? i-GenderNumber/j-GenderNumber (GN): 7
values: masculine, feminine, neutral, plu-
ral, ?rst-person singular, ?rst-person plural,
second-person.
? SemanticRelation (SR): the semantic relation
in WordNet between the head words of the t-
wo mentions: synonym, hyponym, no relation,
unknown.
? MinimumTree (MT): a parse tree represents the
syntactic structure of a sentence, but corefer-
ence resolution needs the overall context in a
document. So we add a super root to the forest
of all the parse trees in one document, and then
we get a super parse tree. The minimum tree
(MT) of a mention-pair in a super parse tree is
the minimum sub-tree from the common par-
ent mention to the two mentions, just like the
method uesd by Zhou(2009). And the similari-
ty of two trees is calculated using a convolution
tree kernel (Collins and Duffy, 2001), which
counts the number of common sub-trees.
We try all the features in our system, and get some
interesting results which is given in Experiments and
Results Section.
3 Experiments and Results
Our experiments are all carried out on CONLL-2011
share task data set (Pradhan et al, 2007).
The result of mention identification in the first
step is evaluated through mention recall. And the
performance of coreference resolution in the second
step is measured using the average F1-measures of
MUC, B-CUBED and CEAFE metrics (Recasens et
al., 2010). All the evaluations are implemented us-
ing the scorer downloaded from the CONLL-2011
share task website 1 .
1http://conll.bbn.com/index.php/software.html
3.1 Rule-based Identification of Mentions
The mention recall of our system in the mention i-
dentification step reaches 87.69%, which can result
in a good performance of the coreference resolution
step. We also do comparative experiments to inves-
tigate the effect of our rule-based mention identifica-
tion. The result is shown in Table 1. The CRF-based
method in Table 1 is to train a conditional random
field (CRF) model with 6 basic features, including
Word, Pos, Word ID, Syntactic parse label, Named
entity, Semantic role.
Method Recall Precision F-score
Rule-based 87.69 32.16 47.06
CRF-based 59.66 50.06 54.44
Table 1: comparative experiments of CRF-based and
rule-based methods of mention identification(%)
Table 1 only shows one kind of basic machine-
learning methods performs not so well as our rule-
based method in recall measure in mention iden-
tification, but the F1-measure of the CRF-based
method is higher than that of the rule-based method.
In our system, the mention identification step should
provide as many anaphoricities as possible to the
coreference resolution step to avoid losing corefer-
ent mentions, which means that the higher the recal-
l of mention identification is, the better the system
performs.
3.2 Coreference Resolution with
Multi-Features
In the second step of our system, SVM-LIGHT-
TK1.2 implementation is employed to coreference
resolution. We apply the polynomial kernel for
the flat features and the convolution tree kernel for
the minimum tree feature to the SVM classifier, in
which the parameter d of the polynomial kernel is
set to 3 (polynomial (a ? b + c)d) and the combin-
ing parameter r is set to 0.2 (K = tree? forest ?
kernel ? r + vector ? kernel). All the other pa-
rameters are set to the default value. All the exper-
iments are done on the broadcast conversations part
of CoNLL-2011 corpus as the calculating time of
SVM-LIGHT-TK1.2 is so long.
Experimental result using the baseline method
with the GenderNumber feature added is shown in
68
d=? MUC B3 CEAFE AVE
2 47.49 61.14 36.15 48.26
3 51.37 62.82 38.26 50.82
Table 2: parameter d in polynomial kernel in coreference
resolution using the baseline method with the GN fea-
ture(%)
Talbe 2. The result shows that the parameter d in
polynomial kernel plays an important role in our
coreference resolution system. The score when d is
3 is 2.56% higher than when d is 2, but the running
time becomes longer, too.
r=? MUC B3 CEAFE AVE
1 31.41 45.08 22.72 33.07
0.25 34.15 46.87 23.63 34.88
0 51.37 62.82 38.26 50.82
Table 3: combining parameter r (K = tree ? forest ?
kernel ? r + vector? kernel) in coreference resolution
using the baseline with the GN and MT features(%)
In Table 3, we can find that the lower the combin-
ing parameter r is, the better the system performs,
which indicates that the MT feature plays a negative
role in our system. There are 2 possible reasons for
that: the MT structure is not proper for our coref-
erence resolution system, or the simple method of
adding a super root to the parse forest of a document
is not effective.
Method MUC B3 CEAFE AVE
baseline 42.19 58.12 33.6 44.64
+GN 51.37 62.82 38.26 50.82
+GN+SR 49.61 64.18 38.13 50.64
+GN 50.97 62.53 37.96 50.49
+SEMCLASS
Table 4: effect of GN and SR features in coreference res-
olution using no MT feature (%)
Table 4 shows the effect of GenderNumber fea-
ture and SemanticRelation feature, and the last item
is the method using the SemanticClassAgreement-
Feature (SEMCLASS) used by (Soon et al, 2001)
instead of the SR feature of our system. The GN fea-
ture significantly improves the performance of our
system by 6.18% of the average score, which may
be greater if we break up the gender and number
feature into two features. As the time limits, we
haven?t separated them until the deadline of the pa-
per. The effect of the SR feature is not as good as
we think. The score is lower than the method with-
out SR feature, but is higher than the method using
SEMCLASS feature. The decreasing caused by S-
R feature may be due to that the searching depth in
WordNet is limited to one to shorten running time.
To investigate the performance of the second step,
we do an experiment for the SVM-based corefer-
ence resolution using just all the anaphoricities as
the mention collection input. The result is shown in
Table 5. As the mention collection includes no in-
correct anaphoricity, any mistake in coreference res-
olution step has double effect, which may lead to a
relatively lower result than we expect.
MUC B3 CEAFE AVE
65.55 58.77 39.96 54.76
Table 5: using just all the anaphoricities as the mention
collection input in coreference resolution step (%)
In the three additional features, only the GN fea-
ture significantly improves the performance of the
coreference resolution system, the result we finally
submitted is to use the baseline method with GN fea-
ture added. The official result is shown in Table 6.
The average score achieves 50.92%.
MUC B3 CEAFE AVE
48.96 64.07 39.74 50.92
Table 6: official result in CoNLL-2011 Share Task using
baseline method with GN feature added (%)
4 Conclusion
This paper proposes a system using multi-features
for the CONLL-2011 share task. Some syntactic and
semantic information is used in our SVM-based sys-
tem. The best result (also the official result) achieves
an average score of 50.92%. As the MT and S-
R features play negative roles in the system, future
work will focus on finding a proper tree structure
for the intra-documental coreference resolution and
combining the parse forest of a document into a tree
to make good use of the convolution tree kernel.
69
References
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Advances in Neural Information Processing
Systems (NIPS), 2004.
Chinchor, Nancy A. 1998. Overview of MUC-7/MET-2.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
Eric Bengtson, Dan Roth. 2008. Understanding the Val-
ue of Features for Coreference Resolution Proceed-
ings of the 2008 Conferenceon Empirical Methods in
Natural Language Processing, pages294C303.
Fang Kong, Guodong Zhou, Longhua Qian, Qiaoming
Zhu. 2010. Dependency-driven Anaphoricity Deter-
mination for Coreference Resolution Proceedings of
the 23rd International Conferenceon Computational
Linguistics (Coling2010), pages599C607.
Guodong Zhou, Fang Kong. 2009. Global Learning of
Noun Phrase Anaphoricity in Coreference Resolution
via Label Propagation. In Proceedings of the 2009
Coreference on Empirical Methods in Natural Lan-
guage Processing, pages 978-986, 2009.
M. Collins, N.Duffy. 2001. Convolution Kernels for Nat-
ural Language Resolution NIPS? 2001.
Marta Recasens, Llu?s Mrquez, Emili Sapena, M. Antnia
Mart?, Mariona Taul, Vronique Hoste, Massimo Poe-
sio, Yannick Versley 2010. SemEval-2010 Task 1:
Coreference Resolutionin Multiple Languages In Pro-
ceeding SemEval 2010 Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, 2010.
MUC-6. 1995. Coreference task definition (v2.3, 8 Sep
95) In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 335-344.
P.Denis, J.Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT/NAACL, 2007.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of ACL, 2002.
V. Ng. 2007. Shallow semantics for coreference resolu-
tion. In Proceedings of IJCAI, 2007.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff. 2009. Conundrums in Noun Phrase Corefer-
ence Resolution: Making Sense of the State-of-the-Art
Proceeding ACL ?09 Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL.
W.Soon,H.Ng,and D.Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrase. Com-
putational Linguistics, 27(4):521-544,2001.
S. M.Harabagiu,R.C.Bunescu,and S.J. Maiorano. 2001.
Text and knowledge mining for coreference resolution.
In Proceedings of NAACL, 2001.
S.Ponzetto, M.Strube. 2005. Semantic role labeling for
coreference resolution. In Proceedings of EACL, Italy,
April 2005.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing, 2007.
Shane Bergsma, Dekang Lin. 2006. Bootstrapping Path-
Based Pronoun Resolution. In Proceedings of the 21st
International Conference on Computational Linguis-
tics, 2006.
Sundheim, Beth M. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 13-
31.
Y.Versley. 2007. Antecedent selection techniques for
high-recall coreference resolution. In Proceedings of
EMNLP/CoNLL, 2007.
70
