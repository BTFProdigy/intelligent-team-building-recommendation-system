Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393?398,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tightly-coupled Unsupervised Clustering and
Bilingual Alignment Model for Transliteration
Tingting Li1, Tiejun Zhao1, Andrew Finch2, Chunyue Zhang1
1Harbin Institute of Technology, Harbin, China
2NICT, Japan
1{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn
2andrew.finch@nict.go.jp
Abstract
Machine Transliteration is an essential
task for many NLP applications. Howev-
er, names and loan words typically orig-
inate from various languages, obey dif-
ferent transliteration rules, and therefore
may benefit from being modeled inde-
pendently. Recently, transliteration mod-
els based on Bayesian learning have over-
come issues with over-fitting allowing for
many-to-many alignment in the training of
transliteration models. We propose a nov-
el coupled Dirichlet process mixture mod-
el (cDPMM) that simultaneously clusters
and bilingually aligns transliteration data
within a single unified model. The un-
ified model decomposes into two class-
es of non-parametric Bayesian component
models: a Dirichlet process mixture mod-
el for clustering, and a set of multino-
mial Dirichlet process models that perf-
orm bilingual alignment independently for
each cluster. The experimental results
show that our method considerably outper-
forms conventional alignment models.
1 Introduction
Machine transliteration methods can be catego-
rized into phonetic-based models (Knight et al,
1998), spelling-based models (Brill et al, 2000),
and hybrid models which utilize both phonetic
and spelling information (Oh et al, 2005; Oh et
al., 2006). Among them, statistical spelling-based
models which directly align characters in the train-
ing corpus have become popular because they
are language-independent, do not require phonet-
ic knowledge, and are capable of achieving state-
of-the-art performance (Zhang et al, 2012b). A
major problem with real-word transliteration cor-
pora is that they are usually not clean, may con-
tain name pairs with various linguistic origins and
this can hinder the performance of spelling-based
models because names from different origins obey
different pronunciation rules, for example:
?Kim Jong-il/???? (Korea),
?Kana Gaski/??? (Japan),
?Haw King/??? (England),
?Jin yong/??? (China).
The same Chinese character ??? should be
aligned to different romanized character se-
quences: ?Kim?, ?Kana?, ?King?, ?Jin?. To ad-
dress this issue, many name classification metho-
ds have been proposed, such as the supervised lan-
guage model-based approach of (Li et al, 2007),
and the unsupervised approach of (Huang et al,
2005) that used a bottom-up clustering algorithm.
(Li et al, 2007) proposed a supervised translitera-
tion model which classifies names based on their
origins and genders using a language model; it
switches between transliteration models based on
the input. (Hagiwara et al, 2011) tackled the is-
sue by using an unsupervised method based on the
EM algorithm to perform a soft classification.
Recently, non-parametric Bayesian
models (Finch et al, 2010; Huang et al,
2011; Hagiwara et al, 2012) have attracted
much attention in the transliteration field. In
comparison to many of the previous alignment
models (Li et al, 2004; Jiampojamarn et al,
2007; Berg-Kirkpatrick et al, 2011), the non-
parametric Bayesian models allow unconstrained
monotonic many-to-many alignment and are able
to overcome the inherent over-fitting problem.
Until now most of the previous work (Li et al,
2007; Hagiwara et al, 2011) is either affected by
the multi-origins factor, or has issues with over-
fitting. (Hagiwara et al, 2012) took these two fac-
tors into consideration, but their approach still op-
erates within an EM framework and model order
selection by hand is necessary prior to training.
393
We propose a simple, elegant, fully-
unsupervised solution based on a single generative
model able to both cluster and align simultaneous-
ly. The coupled Dirichlet Process Mixture Model
(cDPMM) integrates a Dirichlet process mixture
model (DPMM) (Antoniak, 1974) and a Bayesian
Bilingual Alignment Model (BBAM) (Finch et
al., 2010). The two component models work
synergistically to support one another: the clus-
tering model sorts the data into classes so that
self-consistent alignment models can be built
using data of the same type, and at the same time
the alignment probabilities from the alignment
models drive the clustering process.
In summary, the key advantages of our model
are as follows:
? it is based on a single, unified generative
model;
? it is fully unsupervised;
? it is an infinite mixture model, and does not
require model order selection ? it is effec-
tively capable of discovering an appropriate
number of clusters from the data;
? it is able to handle data from multiple origins;
? it can perform many-to-many alignment
without over-fitting.
2 Model Description
In this section we describe the methodology and
realization of the proposed cDPMM in detail.
2.1 Terminology
In this paper, we concentrate on the alignment
process for transliteration. The proposed cDP-
MM segments a bilingual corpus of transliteration
pairs into bilingual character sequence-pairs. We
will call these sequence-pairs Transliteration U-
nits (TUs). We denote the source and target of
a TU as sm1 = ?s1, ..., sm? and tn1 = ?t1, ..., tn?
respectively, where si (ti) is a single character in
source (target) language. We use the same no-
tation (s, t) = (?s1, ..., sm?, ?t1, ..., tn?) to de-
note a transliteration pair, which we can write as
x = (sm1 , tn1 ) for simplicity. Finally, we express
the training set itself as a set of sequence pairs:
D = {xi}Ii=1. Our aim is to obtain a bilingual
alignment ?(s1, t1), ..., (sl, tl)? for each transliter-
ation pair xi, where each (sj , tj) is a segment of
the whole pair (a TU) and l is the number of seg-
ments used to segment xi.
2.2 Methodology
Our cDPMM integrates two Dirichlet process
models: the DPMM clustering model, and the
BBAM alignment model which is a multinomial
Dirichlet process.
A Dirichlet process mixture model, models the
data as a mixture of distributions ? one for each
cluster. It is an infinite mixture model, and the
number of components is not fixed prior to train-
ing. Equation 1 expresses the DPMM hierarchi-
cally.
Gc|?c, G0c ? DP (?c, G0c)
?k|Gc ? Gc
xi|?k ? f(xi|?k) (1)
where G0c is the base measure and ?c > 0 is the
concentration parameter for the distribution Gc.
xi is a name pair in training data, and ?k repre-
sents the parameters of a candidate cluster k for
xi. Specifically ?k contains the probabilities of all
the TUs in cluster k. f(xi|?k) (defined in Equa-
tion 7) is the probability that mixture component
k parameterized by ?k will generate xi.
The alignment component of our cDPMM is
a multinomial Dirichlet process and is defined as
follows:
Ga|?a, G0a ? DP (?a, G0a)
(sj , tj)|Ga ? Ga (2)
The subscripts ?c? and ?a? in Equations 1 and 2
indicate whether the terms belong to the clustering
or alignment model respectively.
The generative story for the cDPMM is sim-
ple: first generate an infinite number of clusters,
choose one, then generate a transliteration pair us-
ing the parameters that describe the cluster. The
basic sampling unit of the cDPMM for the cluster-
ing process is a transliteration pair, but the basic
sampling unit for BBAM is a TU. In order to inte-
grate the two processes in a single model we treat
a transliteration pair as a sequence of TUs gener-
ated by a BBAM model. The BBAM generates a
sequence (a transliteration pair) based on the joint
source-channel model (Li et al, 2004). We use a
blocked version of a Gibbs sampler to train each
BBAM (see (Mochihashi et al, 2009) for details
of this process).
2.3 The Alignment Model
This model is a multinomial DP model. Under the
Chinese restaurant process (CRP) (Aldous, 1985)
394
interpretation, each unique TU corresponds to a
dish served at a table, and the number of customers
in each table represents the count of a particular
TU in the model.
The probability of generating the jth TU (sj , tj)
is,
P
(
(sj , tj)|(s?j , t?j)
)
=
N
(
(sj , tj)
)
+ ?aG0a
(
(sj , tj)
)
N + ?a (3)
where N is the total number of TUs generated
so far, and N
(
(sj , tj)
)
is the count of (sj , tj).
(s?j , t?j) are all the TUs generated so far except
(sj , tj). The base measure G0a is a joint spelling
model:
G0a
(
(s, t)
)
= P (|s|)P (s||s|)P (|t|)P (t||t|)
= ?
|s|
s
|s|! e
??sv?|s|s ?
?|t|t
|t|! e
??tv?|t|t
(4)
where |s| (|t|) is the length of the source (target)
sequence, vs (vt) is the vocabulary (alphabet) size
of the source (target) language, and ?s (?t) is the
expected length of source (target) side.
2.4 The Clustering Model
This model is a DPMM. Under the CRP interpre-
tation, a transliteration pair corresponds to a cus-
tomer, the dish served on each table corresponds
to an origin of names.
We use z = (z1, ..., zI), zi ? {1, ...,K} to in-
dicate the cluster of each transliteration pair xi in
the training set and ? = (?1, ..., ?K) to represent
the parameters of the component associated with
each cluster.
In our model, each mixture component is a
multinomial DP model, and since ?k contains the
probabilities of all the TUs in cluster k, the num-
ber of parameters in each ?k is uncertain and
changes with the transliteration pairs that belong
to the cluster. For a new cluster (the K + 1th clus-
ter), we use Equation 4 to calculate the probability
of each TU. The cluster membership probability
of a transliteration pair xi is calculated as follows,
P (zi = k|D, ?, z?i) ? nkn? 1 + ?c
P (xi|z, ?k) (5)
P (zi = K + 1|D, ?, z?i) ? ?cn? 1 + ?c
P (xi|z, ?K+1)
(6)
where nk is the number of transliteration pairs in
the existing cluster k ? {1, ...,K} (cluster K + 1
is a newly created cluster), zi is the cluster indi-
cator for xi, and z?i is the sequence of observed
clusters up to xi. As mentioned earlier, basic sam-
pling units are inconsistent for the clustering and
alignment model, therefore to couple the models
the BBAM generates transliteration pairs as a se-
quence of TUs, these pairs are then used directly
in the DPMM.
Let ? = ?(s1, t1), ..., (sl, tl)? be a derivation of
a transliteration pair xi. To make the model inte-
gration process explicit, we use function f to cal-
culate the probability P (xi|z, ?k), where f is de-
fined as follows,
f(xi|?k) =
{ ?
??R
?
(s,t)?? P (s, t|?k) k ? {1, ...,K}?
??R
?
(s,t)?? G0c(s, t) k = K + 1
(7)
where R denotes the set of all derivations of xi,
G0c is the same as Equation 4.
The cluster membership zi is sampled together
with the derivation ? in a single step according to
P (zi = k|D, ?, z?i) and f(xi|?k). Following the
method of (Mochihashi et al, 2009), first f(xi|?k)
is calculated by forward filtering, and then a sam-
ple ? is taken by backward sampling.
3 Experiments
3.1 Corpora
To empirically validate our approach, we investi-
gate the effectiveness of our model by conduct-
ing English-Chinese name transliteration genera-
tion on three corpora containing name pairs of
varying degrees of mixed origin. The first two cor-
pora were drawn from the ?Names of The World?s
Peoples? dictionary published by Xin Hua Pub-
lishing House. The first corpus was construct-
ed with names only originating from English lan-
guage (EO), and the second with names originat-
ing from English, Chinese, Japanese evenly (ECJ-
O). The third corpus was created by extracting
name pairs from LDC (Linguistic Data Consor-
tium) Named Entity List, which contains names
from all over the world (Multi-O). We divided the
datasets into training, development and test sets
for each corpus with a ratio of 10:1:1. The details
of the division are displayed in Table 2.
395
cDPMM Alignment BBAM Alignment
mun|? din|? ger|?(0, English) mun|? din|? ger|?
ding|? guo|?(2, Chinese) din|? g| guo|?
tei|? be|?(3, Japanese) t| |? e| ibe|?
fan|? chun|? yi|?(2, Chinese) fan|? chun|? y| i|?
hong|? il|? sik|?(5, Korea) hong|? i|? l| si|? k|
sei|? ichi|? ro|?(4, Japanese) seii|? ch| i|? ro|?
dom|? b|? ro|? w|? s|? ki|?(0, Russian) do|? mb|? ro|? w|? s|? ki|?
he|? dong|? chang|?(2, Chinese) he|? don|? gchang|?
b|? ran|? don|?(0, English) b|? ran|? don|?
Table 1: Typical alignments from the BBAM and cDPMM.
3.2 Baselines
We compare our alignment model with
GIZA++ (Och et al, 2003) and the Bayesian
bilingual alignment model (BBAM). We employ
two decoding models: a phrase-based machine
translation decoder (specifically Moses (Koehn
et al, 2007)), and the DirecTL decoder (Jiampo-
jamarn et al, 2009). They are based on different
decoding strategies and optimization targets, and
therefore make the comparison more compre-
hensive. For the Moses decoder, we applied the
grow-diag-final-and heuristic algorithm to extract
the phrase table, and tuned the parameters using
the BLEU metric.
Corpora Corpus ScaleTraining Development Testing
EO 32,681 3,267 3,267
ECJ-O 32,500 3,250 3,250
Multi-O 33,291 3,328 3,328
Table 2: Statistics of the experimental corpora.
To evaluate the experimental results, we uti-
lized 3 metrics from the Named Entities Workshop
(NEWS) (Zhang et al, 2012a): word accuracy in
top-1 (ACC), fuzziness in top-1 (Mean F-score)
and mean reciprocal rank (MRR).
3.3 Parameter Setting
In our model, there are several important parame-
ters: 1) max s, the maximum length of the source
sequences of the alignment tokens; 2) max t, the
maximum length of the target sequences of the
alignment tokens; and 3) nc, the initial number of
classes for the training data. We set max s = 6,
max t = 1 and nc = 5 empirically based on a
small pilot experiment. The Moses decoder was
used with default settings except for the distortion-
limit which was set to 0 to ensure monotonic de-
coding. For the DirecTL decoder the following
settings were used: cs = 4, ng = 9 and nBest =
5. cs denotes the size of context window for fea-
tures, ng indicates the size of n-gram features and
nBest is the size of transliteration candidate list
for updating the model in each iteration. The con-
centration parameter ?c, ?a of the clustering mod-
el and the BBAM was learned by sampling its val-
ue. Following (Blunsom et al, 2009) we used
a vague gamma prior ?(10?4, 104), and sampled
new values from a log-normal distribution whose
mean was the value of the parameter, and variance
was 0.3. We used the Metropolis-Hastings algo-
rithm to determine whether this new sample would
be accepted. The parameters ?s and ?t in Equa-
tion 4 were set to ?s = 4 and ?t = 1.
Model EO ECJ-O Multi-O
#(Clusters) cDPMM 5.8 9.5 14.3
#(Targets)
GIZA++ 14.43 5.35 6.62
BBAM 6.06 2.45 2.91
cDPMM 9.32 3.45 4.28
Table 3: Alignment statistics.
3.4 Experimental Results
Table 3 shows some details of the alignment re-
sults. The #(Clusters) represents the average num-
ber of clusters from the cDPMM. It is averaged
over the final 50 iterations, and the classes which
contain less than 10 name pairs are excluded. The
#(Targets) represents the average number of En-
glish character sequences that are aligned to each
Chinese sequence. From the results we can see
that in terms of the number of alignment targe-
ts: GIZA++ > cDPMM > BBAM. GIZA++ has
considerably more targets than the other approach-
es, and this is likely to be a symptom of it over-
fitting the data. cDPMM can alleviate the over-
fitting through its BBAM component, and at the
same time effectively model the diversity in Chi-
nese character sequences caused by multi-origin.
Table 1 shows some typical TUs from the align-
ments produced by BBAM and cDPMM on cor-
pus Multi-O. The information in brackets in Ta-
ble 1, represents the ID of the class and origin of
396
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.7241 0.8881 0.8061
BBAM 0.7286 0.8920 0.8043
cDPMM 0.7398 0.8983 0.8126
ECJ-O
GIZA 0.5471 0.7278 0.6268
BBAM 0.5522 0.7370 0.6344
cDPMM 0.5643 0.7420 0.6446
Multi-O
GIZA 0.4993 0.7587 0.5986
BBAM 0.5163 0.7769 0.6123
cDPMM 0.5237 0.7796 0.6188
Table 4: Comparison of different methods using
the Moses phrase-based decoder.
the name pair; the symbol ? ? indicates a ?NUL-
L? alignment. We can see the Chinese characters
??(ding) ?(yi) ?(dong)? have different align-
ments in different origins, and that the cDPMM
has provided the correct alignments for them.
We used the sampled alignment from running
the BBAM and cDPMMmodels for 100 iterations,
and combined the alignment tables of each class
together. The experiments are therefore investigat-
ing whether the alignment has been meaningfully
improved by the clustering process. We would ex-
pect further gains from exploiting the class infor-
mation in the decoding process (as in (Li et al,
2007)), but this remains future research. The top-
10 transliteration candidates were used for testing.
The detailed experimental results are shown in Ta-
bles 4 and 5.
Our proposed model obtained the highest per-
formance on all three datasets for all evaluation
metrics by a considerable margin. Surprisingly,
for dataset EO although there is no multi-origin
factor, we still observed a respectable improve-
ment in every metric. This shows that although
names may have monolingual origin, there are hid-
den factors which can allow our model to succeed,
possibly related to gender or convention. Other
models based on supervised classification or clus-
tering with fixed classes may fail to capture these
characteristics.
To guarantee the reliability of the compara-
tive results, we performed significance testing
based on paired bootstrap resampling (Efron et al,
1993). We found all differences to be significant
(p < 0.05).
4 Conclusion
In this paper we propose an elegant unsupervised
technique for monotonic sequence alignment
based on a single generative model. The key ben-
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.6950 0.8812 0.7632
BBAM 0.7152 0.8899 0.7839
cDPMM 0.7231 0.8933 0.7941
ECJ-O
GIZA 0.3325 0.6208 0.4064
BBAM 0.3427 0.6259 0.4192
cDPMM 0.3521 0.6302 0.4316
Multi-O
GIZA 0.3815 0.7053 0.4592
BBAM 0.3934 0.7146 0.4799
cDPMM 0.3970 0.7179 0.4833
Table 5: Comparison of different methods using
the DirecTL decoder.
efits of our model are that it can handle data from
multiple origins, and model using many-to-many
alignment without over-fitting. The model oper-
ates by clustering the data into classes while si-
multaneously aligning it, and is able to discover
an appropriate number of classes from the data.
Our results show that our alignment model can im-
prove the performance of a transliteration gener-
ation system relative to two other state-of-the-art
aligners. Furthermore, the system produced gains
even on data of monolingual origin, where no ob-
vious clusters in the data were expected.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments and helpful suggestions.We also
thank Chonghui Zhu, Mo Yu, and Wenwen Zhang
for insightful discussions. This work was support-
ed by National Natural Science Foundation of Chi-
na (61173073), and the Key Project of the Nation-
al High Technology Research and Development
Program of China (2011AA01A207).
References
D.J. Aldous. 1985. Exchangeability and Related Top-
ics. E?cole d?E?te? St Flour 1983. Springer, 1985,
1117:1?198.
C.E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics. 2:1152, 174.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proc. of EMNLP, pages 313?321.
P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. of ACL, pages 782?790.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
In Proc. of ACL, pages 286?293.
397
B. Efron and R. J. Tibshirani 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York, NY.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Translitera-
tion. In Proc. of the 7th International Workshop on
Spoken Language Translation, pages 259?266.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
Class Transliteration based on Source Language O-
rigin. In Proc. of ACL (Short Papers), pages 53-57.
Masato Hagiwara and Satoshi Sekine. 2012. Latent
semantic transliteration using dirichlet mixture. In
Proc. of the 4th Named Entity Workshop, pages 30?
37.
Fei Huang, Stephan Vogel, and Alex Waibel. 2005.
Clustering and Classifying Person Names by Origin.
In Proc. of AAAI, pages 1056?1061.
Yun Huang, Min Zhang and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration
with Synchronous Adaptor Grammars. In Proc. of
ACL, pages 534?539.
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek
Sherif. 2007. Applying Many-to-Many Alignments
and Hidden Markov Models to Letter-to-Phoneme
Conversion. In Proc. of NAACL, pages 372?379.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer and Grzegorz Kondrak. 2009.
DirecTL: a Language Independent Approach to
Transliteration. In Proc. of the 2009 Named Entities
Workshop: Shared Task on Transliteration (NEWS
2009), pages 1056?1061.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Journal of Computational Linguis-
tics, pages 28?31.
Philipp Koehn and Hieu Hoang and Alexandra Birch
and Chris Callison-Burch and Marcello Federico
and Nicola Bertoldi and Brooke Cowan and Wade
Shen and Christine Moran and Richard Zens and
Chris Dyer and Ondrej Bojar and Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL.
Haizou Li, Min Zhang, and Jian Su 2004. A join-
t source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, Morris-
town, NJ, USA, 159.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic Transliteration of Personal
Names. In Proc. of ACL, pages 120?127.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ue-
da. 2009. Bayesian Unsupervised Word Segmen-
tation with Nested Pitman-Yor Language Modeling.
In Proc. of ACL/IJCNLP, pages 100?108.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Journal of Comput. Linguist., 29(1):19-51.
Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine
Learning Based English-to-Korean Transliteration
Using Grapheme and Phoneme Information. Jour-
nal of IEICE Transactions, 88-D(7):1737-1748.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A machine transliteration model based on
correspondence between graphemes and phonemes.
Journal of ACM Trans. Asian Lang. Inf. Process.,
5(3):185-208.
Min Zhang, Haizhou Li, Ming Liu and A Kumaran.
2012a. Whitepaper of NEWS 2012 shared task on
machine transliteration. In Proc. of the 4th Named
Entity Workshop (NEWS 2012), pages 1?9.
Min Zhang, Haizhou Li, A Kumaran and Ming Liu.
2012b. Report of NEWS 2012 Machine Translitera-
tion Shared Task. In Proc. of the 4th Named Entity
Workshop (NEWS 2012), pages 10?20.
398
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 52?56,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syllable-based Machine Transliteration with Extra Phrase Features  
 
 
Chunyue Zhang, Tingting Li, Tiejun Zhao 
MOE-MS Key Laboratory of Natural Language Processing and Speech 
Harbin Institute of Technology 
Harbin,China 
{cyzhang,ttli,tjzhao}@mtlab.hit.edu.cn 
  
 
 
Abstract 
This paper describes our syllable-based phrase 
transliteration system for the NEWS 2012 
shared task on English-Chinese track and its 
back. Grapheme-based Transliteration maps the 
character(s) in the source side to the target 
character(s) directly. However, character-based 
segmentation on English side will cause 
ambiguity in alignment step. In this paper we 
utilize Phrase-based model to solve machine 
transliteration with the mapping between 
Chinese characters and English syllables rather 
than English characters. Two heuristic rule-
based syllable segmentation algorithms are 
applied. This transliteration model also 
incorporates three phonetic features to enhance 
discriminative ability for phrase. The primary 
system achieved 0.330 on Chinese-English and 
0.177 on English-Chinese in terms of top-1 
accuracy. 
1 Introduction  
Machine transliteration, based on the pronunciation, 
transforms the script of a word from a source 
language to a target language automatically.  
  With a continuous growth of out-of-vocabulary 
names to be transliterated, the traditional 
dictionary-based methods are no longer suitable. 
So data-driven method is gradually prevailing now, 
and many new approaches are explored. 
  Knight(1998) proposes a phoneme-based 
approach to solve the transliteration between 
English names and Japanese katakana. It makes 
use of a common phonetic representation as a pivot.  
  The phoneme-based approach needs a 
pronunciation dictionary for one or two languages. 
These dictionaries usually do not exist or can't 
cover all the names. So grapheme-based(Li et al, 
2004) approach has gained lots of attention 
recently. Huang(2011) proposes a novel 
nonparametric Bayesian using synchronous 
adaptor grammars to model the grapheme-based 
transliteration. Zhang(2010) builds the pivot 
transliteration model with grapheme-based method. 
  The hybrid approach tries to utilize both phoneme 
and grapheme information, and usually integrates 
the output of multiple engines to improve 
transliteration. Oh and Choi(2006) integrate both 
phoneme and grapheme features into a single 
leaning framework.  
  As an instance of grapheme-based approach, 
Jia(2009) views machine transliteration as a special 
example of machine translation and uses the 
phrase-based machine translation model to solve it. 
The approach is simple and effective. Our paper 
follows this way. However, using the English 
letters and Chinese characters as basic mapping 
units will make ambiguity in the alignment and 
translation step. One Chinese character usually 
maps one syllable, so syllabifying English words 
can be more discriminative. 
  We present a solution to this ambiguity by 
replacing the English character with an English 
syllable which is consecutive characters and can 
keep some phonetic properties. For this purpose, 
two heuristic and simple syllable segmentation 
algorithms are used to syllabify English side into 
syllables sequence. Besides two above, three extra 
phrase features for transliteration are used to 
enhance the model. 
  The rest of this paper is organized as follows. 
Section 2 introduces the phrase-based model 
briefly. Section 3 describes two rule-based syllable 
52
segmentation methods and three new special 
features for transliteration in detail. Experiments 
and analyses are discussed in section 4. 
Conclusions and future work are addressed in 
section 5. 
2 Phrase-based Machine Transliteration 
Model 
Machine transliteration can be regarded as a 
special instance of machine translation. Jia(2009) 
solves transliteration with phrase-based model 
firstly. There an English character is treated as a 
word in machine translation. On the contrast, 
character is replaced by syllable in this paper. Then 
transliteration can be viewed as a pure translation 
task. The phrase-based machine transliteration can 
be formulated by equation 1. 
?
?
??
n
i
iie
xhxpe
1
~ )(exp)(maxarg ?
           (1) 
? n is the number of features 
? 
i?  is the weight of feature i  
  In our phrase-based transliteration system, the 
following features are used by default: 
? the bidirectional probability between 
source phrase and the target phrase 
? The bidirectional lexical probability 
between source phrase and target phrase  
? the fluency of the output, namely language 
model 
? the length penalty 
3 Syllable Segmentation and Extra 
Phrase Features 
This section describes two rule-based syllable 
segmentation algorithms and three extra phrase 
features added to machine transliteration model. 
3.1 Syllable Segmentation Algorithm  
In (Jia et al, 2009), the basic alignment units are 
English character and Chinese character(called 
c2c). This setup is the simplest format to 
implement the model. However, transliteration 
from English to Chinese usually maps an English 
syllable to a single Chinese character. As one 
Chinese character usually corresponds to many 
English characters, the c2c method has only a 
modest discriminative ability. Obviously 
syllabifying English is more suitable for this 
situation. Yang(2010) utilizes a CRF-based 
segmentor to syllabify English and Kwong(2011) 
syllabifies English with the Onset First Principle. 
Alternatively, inspired by (Jiang, 2007), two 
heuristic rule-based methods are addressed to 
syllabify the English names in this paper.  
  Given an English name E, it can be syllabified 
into a syllable sequence SE = {e1,e2,...,en} with  
one of the following two linguistic methods. 
 
Simple Segmentation Algorithm(SSA): 
1.  {'a', 'o' , 'e', 'i', 'u'} are defined as vowels. 'y' is 
defined as a vowel when it is not followed by a 
vowel; 'r' is defined as a vowel when it follows a 
vowel and is followed by a consonant1. All other 
characters are defined as consonants; this forms the 
basic vowel set; 
2.  A consecutive vowels sequence, formed by the 
basic vowel set, is treated as a new vowel symbol; 
Step 1 and 2 form the new vowel set; 
3.  A consonant and its following vowel are treated 
as a syllable; 
4.  Consecutive consonants are separated; a vowel 
symbol(in the new vowel set) followed by a 
consonant is separated; 
5. The rest isolated characters sequences are 
regarded as individual syllables in each word. 
 
  SSA treats all the consecutive vowels as a single 
new vowel simply. In fact, many consecutive 
vowels like "io" often align two or more Chinese 
characters, such as " zio ?  ?". It is better to 
separate it as two syllables rather than one syllable 
in alignment step. So we present another segment 
algorithm which takes more details into 
consideration.  
 
Fine-grained Segment Algorithm(FSA): 
1.  Replace 'x' in English names with 'k s' firstly; 
2. {'a','o','e','i','u'} are defined as vowels. 'y' is 
defined as a vowel when it is not followed by a 
vowel; 
3.  When 'w' follows 'a','e','o' and isn't followed by 
'h', treat 'w' and the preceding vowel as a new 
vowel symbol; Step 2 and 3 form the basic vowel 
set; 
4. A consecutive vowels sequence which is formed 
by the basic vowel set is treated as a new vowel 
                                                          
1 A review points the SSA lacking of ability to deal with 'h'. 
We leave it for the future work. 
53
symbol, excepting 'iu', 'eo', 'io', 'oi', 'ia', 'ui', 'ua', 
'uo' ; Step 2, 3 and 4 form the new vowel set; 
5. Consecutive consonants are separated; a vowel 
symbol(in the new vowel set) followed by a 
consonant sequence is separated; 
6. A consonant and its following vowel are treated 
as a syllable; the rest of the isolated consonants 
and vowels are regarded as individual syllables in 
each word. 
   
  After segmenting the English characters sequence, 
the new transliteration units, syllables, will be 
more discriminative. 
3.2 Extra phrase features 
The default features of phrase can't express the 
special characteristic of transliteration. We propose 
three features trying to explore the transliteration 
property. 
  Begin and End Feature(BE) 
  When a Chinese character is chosen as the 
corresponding transliteration, its position in the 
transliteration result is important. Such as a 
syllable "zu" that can be transliterate into "?" or "
?" in Chinese while "?" will be preferred if it 
appears at the beginning position.  
  To explore this kind of information, the pseudo 
characters "B" and "E" are added to the train and 
test data. So in the extracted phrase table, "B" 
always precedes the Chinese character that prefers 
at the first position, and "E" always follows the 
Chinese character that appears at the last position. 
  Phrase Length Feature  
  Chinese character can be pronounced according 
to its pinyin format which is written like English 
word. And the longer English syllable is, the 
longer pinyin format it often has. So the length 
information of Chinese character and its pinyin can 
be used to disambiguate the phrase itself. Here we 
definite two new features to address it. Suppose 
<e,c> as a phrase pair, e= {e1,e2,...,em},c = 
{c1,c2,...,cn},ei stands for an English syllable and 
ci stands for a Chinese character. p(ci) is the pinyin 
format of ci. #(ei) is equal to the number of 
characters in a syllable. #p(cj) is equal to the 
number of characters in a pinyin sequence. And 
then, 
L1 = Sum(#(ei)) / Sum(#(p(cj)) 
L2 = m / n 
4 Experiments 
This section describes the data sets, experimental 
setup, experimental results and analyses. 
4.1 Data Sets 
The training set of English-Chinese transliteration 
track contains 37753 pairs of names. We pick up 
3000 pairs from the training data randomly as the 
closed test set and the rest 34753 pairs as our 
training data set. In the official dev set some 
semantic translation pairs are found, such as 
"REPUBLIC OF CUBA ?????", and some 
many-to-one cases like "SHELL BEACH ???
?" also appear. We modify or delete these cases 
from the original dev set. At last, 3223 pairs are 
treated as the final dev set to tune the weights of 
system features. 
 
Language Segmentation Algorithm Number 
 
English 
Character-based 6.82 
SSA  4.24 
FSA 4.48 
Chinese Character-based 3.17 
Table 1: Average syllables of names based on 
different segmentation methods 
 
Language Segmentation Algorithm Number 
 
English 
Character-based 26 
SSA  922 
FSA 463 
Chinese Character-based 368 
Table 2 :Total number of unique units 
 
  For the Chinese-English back transliteration track, 
the final training and test sets are formed in the 
same way; the original dev set is used directly. 
  Here we use Character-based which treats single 
character as a "syllable", Simple and Fine-grained 
segmentation algorithms to deal with English 
names. Table 1 and table 2 show some syllabic 
statistics information. Table 1 shows the average 
syllables of the three segmentation approaches in 
training data. Table 2 shows the total number of 
unique units. 
4.2 Experimental Setup 
The Moses (Koehn et al, 2007) is used to 
implement the model in this paper. The 
Srilm(Stolcke et al, 2002) toolkit is used to count 
54
n-gram on the target of the training set. Here we 
use a 3-gram language model. In the transliteration 
model training step, the Giza++(Och et al, 2003) 
generates the alignment with the grow-diag-and-
final heuristic, while other setup is default. In order 
to guarantee monotone decoding, the distortion 
distance is limited to 0. The MERT is used to tune 
model's weights. The method of (Jia et al, 2009) is 
the baseline setup.   
4.3 Evaluation Metrics 
The following 4 metrics are used to measure the 
quality of the transliteration results (Li et al, 
2009a): Word Accuracy in Top-1 (ACC), 
Fuzziness in Top-1 (Mean F-score), Mean 
Reciprocal Rank (MRR), MAPref. 
4.4 Results 
Table 3 shows the performance of our system 
corresponding to baseline, SSA and FSA on the 
closed test set of EnCh track. BE, L1,L2 and 
BE+L1+L2 are implemented on the basis of FSA.  
   
 ACC Mean 
F-score 
MRR MAPre
f 
Baseline 0.628 0.847 0.731 0.628 
SSA  0.639 0.850 0.738 0.639 
FSA 0.661 0.861 0.756 0.661 
BE  0.648 0.856 0.751 0.648 
L1 0.661 0.864 0.756 0.661 
L2 0.619 0.844 0.727 0.619 
BE+L1+L2 0.665 0.863 0.762 0.665 
Table 3:The held-in results of EnCh 
 
  Table 3 shows that the forward transliteration 
performance gets consistent improvement from 
baseline to FSA. None of new three features can 
improve by self, while combining three features 
can gain a little.  
 
 ACC Mean 
F-score 
MRR MAPre
f 
EnCh_Pri 0.330 0.676 0.408 0.319 
EnCh_2 0.317 0.667 0.399 0.308 
ChEn_pri 0.177 0.702 0.257 0.173 
Table 4:  The final official results of EnCh and 
ChEn 
 
  According to the performance of closed test, the 
transliteration results of EnCh and ChEn based on 
BE+L1+L2 are chosen as the primary 
submissions(EnCh_Pri and ChEn_Pri). And the 
result of FSA is the contrastive 
submission(EnCh_2). The table 4 shows the final 
official results of EnCh and ChEn. 
5 Conclusions and future work  
This paper uses the phrase-based machine 
translation to model the transliteration task and the 
state-of-the-art translation system Moses is used to 
implement it. We participate in the NEWS 2012 
Machine Transliteration Shared Task English-
Chinese and Chinese-English tracks. 
  To improve the capability of the basic phrase-
based machine transliteration, two heuristic and 
rule-based English syllable segmentation methods 
are addressed. System can also be more robust 
with combination of three new special features for 
transliteration. The experimental results show that 
the Fine-grained Segmentation can improve the 
performance remarkably in English-Chinese 
transliteration track. 
  In the future, extensive error analyses will be 
made and methods will be proposed according to 
the specific error type. More syllable segmentation 
methods such as statistical-based will be tried. 
Acknowledgments  
The authors would like to thank all the reviews for 
their help about correcting grammatical errors of 
this paper and invaluable suggestions. This work is 
supported by the project of National High 
Technology Research and Development Program 
of China (863 Program) (No. 2011AA01A207) and 
the project of National Natural Science Foundation 
of China (No. 61100093). 
References  
Andreas Stolcke. 2002. SRILM - an Extensible 
Language Modeling Toolkit. In Proc. of ICSLP, 
Denver, USA. 
Dong Yang, Paul Dixon and Sadaoki Furui. 2010. 
Jointly optimizing a two-step conditional random 
field model for machine transliteration and its fast 
decoding algorithm. In Proceedings of the ACL 2010 
Conference Short Papers. pp. 275--280 Uppsala, 
Sweden. 
55
Franz Josef Och, Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Comput.Linguistics 29, 1, 19?51. 
Haizhou Li , Min Zhang, Jian Su. 2004. A Joint Source 
Channel Model for Machine Transliteration. In 
Proceedings of the 42nd ACL, pp. 159-166. 
Kevin Knight, Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol. 24, 
No. 4, pp. 599-612. 
Long Jiang , Ming Zhou , Leefeng Chien and Cheng 
Niu. Named entity translation with web mining and 
transliteration, Proceedings of the 20th international 
joint conference on Artifical intelligence, p.1629-
1634, January 06-12, 2007, Hyderabad, India 
Min Zhang, Xiangyu Duan, Vladimir Pervouchine, and 
Haizhou Li. 2010. Machine transliteration: 
Leveraging on third languages. In Coling 2010: 
Posters, pages 1444?1452, Beijing, China, August. 
Coling 2010 Organizing Committee. 
Oi Yee Kwong. 2011. English-Chinese Personal 
Name Transliteration by Syllable-Based Maximum 
Matching. In the Proceedings of the 2011 Named 
Entities Workshop,2011,pp.96-100. 
Philipp Koehn, Hieu Hoang, Marcello Federico Nicola 
Bertoldi , Brooke Cowan and Wade Shen . 2007. 
Moses: Open Source Toolkit for Statistical Machine 
Translation. In Proceedings of the 45th ACL 
Companion Volume of the Demo and Poster Sessions, 
pp. 177-180. 
Yun Huang, Min Zhang and Chewlim Tan. 2011. 
Nonparametric Bayesian Machine Transliteration 
with Synchronous Adaptor Grammars. In 
Proceedings of ACL-HLT 2011: Short 
Papers,Portland, Oregon, pp.534-539. 
Yuxiang Jia, Danqing Zhu, and Shiwen Y. 2009. A 
Noisy Channel Model for Grapheme-based Machine 
Transliteration, In the Proceedings of the 2009 
Named Entities Workshop, 2009, pp. 88-91. 
 
56
