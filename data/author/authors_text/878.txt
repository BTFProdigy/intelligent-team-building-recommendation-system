Proceedings of NAACL HLT 2009: Tutorials, pages 13?14,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
VerbNet overview, extensions, mappings and applications 
(Karin Kipper Schuler, Anna Korhonen, Susan Brown) 
 
Abstract: 
The goal of this tutorial is to introduce and discuss VerbNet, a broad coverage verb lexicon 
freely available on-line. VerbNet contains explicit syntactic and semantic information for classes 
of verbs and has mappings to several other widely-used lexical resources, including WordNet, 
PropBank, and FrameNet.  Since its first release in 2005 VerbNet is being used by a large 
number of researchers as a means of characterizing verbs and verb classes.  
The first part of the tutorial will include an overview of the original Levin verb classification; 
introduce the main VerbNet components, such as thematic roles and syntactic and semantic 
representations, and present a comparison with other available lexical resources.  
During the second part of the tutorial, we will explore VerbNet extensions (how new classes 
were derived and created through manual and semi-automatic processes), and we will present 
on-going work on automatic acquisition of Levin-style classes in corpora. The latter is useful for 
domain-adaptation and tuning of VerbNet for real-world applications which require this.  
The last part of the tutorial will be devoted to discussing the current status of VerbNet; 
including recent work mapping to other lexical resources, such as PropBank, FrameNet, 
WordNet, OntoNotes sense groupings, and the Omega ontology.  We will also present changes 
designed to regularize the syntactic frames and to make the naming conventions more 
transparent and user friendly.  Finally, we will describe some applications in which VerbNet has 
been used. 
Tutorial Outline:  
VerbNet overview: 
- Original Levin classes 
- VerbNet components (roles, syntactic and semantic descriptions) 
- Related work in lexical resources 
VerbNet extensions: 
- Manual and semi-automatic extension of VerbNet with new classes  
- On-going work on automatic acquisition of Levin-style classes in corpora 
13
VerbNet mappings and applications: 
- Mappings to other resources (PropBank, FrameNet, WordNet, OntoNotes sense 
groupings, Omega ontology) 
- Current status of VerbNet 
- Ongoing improvements 
- VerbNet applications 
Short biographical description of the presenters: 
Karin Kipper Schuler  
University of Colorado 
kipper@verbs.colorado.edu 
 
Karin Kipper Schuler received her PhD from the Computer Science Department of the 
University of Pennsylvania. Her primary research is aimed at the development and 
evaluation of large-scale computational lexical resources. During the past couple of 
years she worked for the Mayo Clinic where she was involved in applications related to 
bio/medical informatics including automatic extraction of named entities and relations 
from clinical data and development of knowledge models to guide this data extraction.  
 
Anna Korhonen 
University of Cambridge 
alk23@cam.ac.uk 
 
Anna Korhonen received her PhD from the Computer Laboratory of the University of 
Cambridge in the UK. Her research focuses mainly on automatic acquisition of lexical 
information from texts. She has developed techniques and tools for automatic lexical 
acquisition for English and other languages, and has used them to acquire large lexical 
resources , extend manually built resources, and help NLP application tasks (e.g. parsing, 
word sense disambiguation, information extraction) . She has applied the techniques to 
different domains (e.g. biomedical) and has also used them to advance on research in 
related fields (e.g. cognitive sciences).  
 
Susan Brown 
University of Colorado 
susan.brown@colorado.edu 
 
Susan Brown is currently a PhD student in Linguistics and Cognitive Science at the 
University of Colorado under Dr. Martha Palmer?s supervision. Susan?s research focuses 
on lexical ambiguity, especially as it pertains to natural language processing tasks.  Her 
research methodologies include psycholinguistic experimentation, corpus study, and 
annotation analysis.  She has also been involved in the development of large-scale 
lexical resources and the design and construction of a lexically based ontology. 
14
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 249?252,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Choosing Sense Distinctions for WSD: Psycholinguistic Evidence 
 
Susan Windisch Brown 
Department of Linguistics 
Institute of Cognitive Science 
University of Colorado 
Hellems 295 UCB 
Boulder, CO 80309 
susan.brown@colorado.edu 
 
 
 
Abstract 
Supervised word sense disambiguation re-
quires training corpora that have been tagged 
with word senses, which begs the question of 
which word senses to tag with. The default 
choice has been WordNet, with its broad cov-
erage and easy accessibility. However, con-
cerns have been raised about the 
appropriateness of its fine-grained word 
senses for WSD. WSD systems have been far 
more successful in distinguishing coarse-
grained senses than fine-grained ones (Navig-
li, 2006), but does that approach neglect ne-
cessary meaning differences? Recent 
psycholinguistic evidence seems to indicate 
that closely related word senses may be 
represented in the mental lexicon much like a 
single sense, whereas distantly related senses 
may be represented more like discrete entities. 
These results suggest that, for the purposes of 
WSD, closely related word senses can be clus-
tered together into a more general sense with 
little meaning loss. The current paper will de-
scribe this psycholinguistic research and its 
implications for automatic word sense disam-
biguation. 
1 Introduction* 
The problem of creating a successful word sense 
disambiguation system begins, or should begin, 
well before methods or algorithms are considered. 
The first question should be, ?Which senses do we 
want to be able to distinguish??  Dictionaries en-
                                                          
* I gratefully acknowledge the support of the National Science 
Foundation Grant NSF-0415923, Word Sense Disambigua-
tion. 
courage us to consider words as having a discrete 
set of senses, yet any comparison between dictio-
naries quickly reveals how differently a word?s 
meaning can be divided into separate  senses.    
Rather than having a finite list of senses, many 
words seem to have senses that shade from one 
into another.  
One could assume that dictionaries make broad-
ly similar divisions and the exact point of division 
is only a minor detail. Simply picking one resource 
and sticking with it should solve the problem. In 
fact, WordNet, with its broad coverage and easy 
accessibility, has become the resource of choice for 
WSD. However, some have questioned whether 
WordNet?s fine-grained sense distinctions are ap-
propriate for the task (Ide & Wilks, 2007; Palmer 
et al, 2007). Some are concerned about feasibility: 
Is WSD at this level an unattainable goal? Others 
with practicality: Is this level of detail really 
needed for most NLP tasks, such as machine trans-
lation or question-answering? Finally, some won-
der whether such fine-grained distinctions even 
reflect how human beings represent word meaning. 
Human annotators have trouble distinguishing 
such fine-grained senses reliably.  Interannotator 
agreement with WordNet senses is around 70% 
(Snyder & Palmer, 2004; Chklovski & Mihalcea, 
2002), and it?s understandable that WSD systems 
would have difficulty surpassing this upper bound. 
Researchers have responded to these concerns 
by developing various ways to cluster WordNet 
senses.  Mihalcea & Moldovan (2001) created an 
unsupervised approach that uses rules to cluster 
senses.  Navigli (2006) has induced clusters by 
mapping WordNet senses to a more coarse-grained 
lexical resource.  OntoNotes (Hovy et al, 2006) is 
manually grouping WordNet senses and creating a 
corpus tagged with these sense groups.  Using On-
249
toNotes and another set of manually tagged data, 
Snow et al (2007) have developed a supervised 
method of clustering WordNet senses. 
Although ITA rates and system performance 
both significantly improve with coarse-grained 
senses (Duffield et al, 2007; Navigli, 2006), the 
question about what level of granularity is needed 
remains. Palmer et al (2007) state, ?If too much 
information is being lost by failing to make the 
more fine-grained distinctions, the [sense] groups 
will avail us little.? 
Ides and Wilks (2007) drew on psycholinguistic 
research to help establish an appropriate level of 
sense granularity. However, there is no consensus 
in the psycholinguistics field on how lexical mean-
ing is represented in the mind (Klein & Murphy, 
2001; Pylkk?nen et al, 2006; Rodd et al, 2002), 
and, as the Ide and Wilks (2007) state, ?research in 
this area has been focused on developing psycho-
logical models of language processing and has not 
directly addressed the problem of identifying 
senses that are distinct enough to warrant, in psy-
chological terms, a separate representation in the 
mental lexicon.? 
Our experiment looked directly at sense distinc-
tions of varying degrees of meaning relatedness 
and found indications that the mental lexicon does 
not consist of separate representations of discrete 
senses for each word. Rather, word senses may 
share a greater or smaller portion of a semantic 
representation depending on the how closely re-
lated the senses are. Because closely related senses 
may share a large portion of their semantic repre-
sentation, clustering such senses together would 
result in very little meaning loss. The remainder of 
this paper will describe the experiment and its im-
plications for WSD in more detail.  
2 Experiment 
The goal of this experiment was to determine 
whether each sense of a word has a completely 
separate mental representation or not. If so, we also 
hoped to discover what types of sense distinctions 
seem to have separate mental representations. 
2.1 Materials 
Four groups of materials were prepared using the 
fine-grained sense distinctions found in WordNet 
2.1. Each group consisted of 11 pairs of phrases. 
The groups comprised (1) homonymy, (2) distantly 
related senses, (3) closely related senses, and (4) 
same senses (see Table 1 for examples). Placement 
in these groups depended both on the classification 
of the usages by WordNet and the Oxford English 
Dictionary and on the ratings given to pairs of 
phrases by a group of undergraduates. They rated 
the relatedness of the verb in each pair on a scale 
of 0 to 3, with 0 being completely unrelated and 3 
being the same sense. 
A pair was considered to represent the same 
sense if the usage of the verb in both phrases was 
categorized by WordNet as the same and if the pair 
received a rating greater than 2.7. Closely related 
senses were listed as separate senses by WordNet 
and received a rating between 1.8 and 2.5. Distant-
ly related senses were listed as separate senses by 
WordNet and received ratings between 0.7 and 1.3. 
Because WordNet makes no distinction between 
related and unrelated senses, the Oxford English 
Dictionary was used to classify homonyms. Ho-
monyms were listed as such by the OED and re-
ceived ratings under 0.3. 
 
 
 Prime Target 
Unrelated banked the plane banked the money 
Distantly related ran the track ran the shop 
Closely related broke the glass broke the radio 
Same sense cleaned the shirt cleaned the cup 
 
Table 1. Stimuli. 
2.2 Method 
The experiment used a semantic decision task 
(Klein & Murphy, 2001; Pylkk?nen et al, 2006), in 
which people were asked to judge whether short 
phrases ?made sense? or not. Subjects saw a 
phrase, such as ?posted the guard,? and would de-
cide whether the phrase made sense as quickly and 
as accurately as possible. They would then see 
another phrase with the same verb, such as ?posted 
the letter,? and respond to that phrase as well. The 
response time and accuracy were recorded for the 
second phrase of each pair. 
2.3 Results and Discussion 
When comparing response times between same 
sense pairs and different sense pairs (a combina-
250
tion of closely related, distantly related, and unre-
lated senses), we found a reliable difference (same 
sense mean: 1056ms, different sense mean: 
1272ms; t32 =6.33; p<.0001). We also found better 
accuracy for same sense pairs (same sense: 95.6% 
correct vs. different sense: 78% correct; t32=7.49; 
p<.0001). When moving from one phrase to another 
with the same meaning, subjects were faster and 
more accurate than when moving to a phrase with 
a different sense of the verb. 
By itself, this result would fit with the theory that 
every sense of a word has a separate semantic re-
presentation. One would expect people to access 
the meaning of a verb quickly if they had just seen 
the verb used with that same meaning. One could 
think of the meaning as already having been ?acti-
vated? by the first phrase. Accessing a completely 
different semantic representation when moving 
from one sense to another should be slower.  
If all senses have separate representations, access 
to meaning should proceed in the same way for all. 
For example, if one is primed with the phrase 
?fixed the radio,? response time and accuracy 
should be the same whether the target is ?fixed the 
vase? or ?fixed the date.? Instead, we found a sig-
nificant difference between these two groups, with 
closely related pairs accessed, on average, 173ms 
more quickly than the mean of the distantly and 
unrelated pairs (t32=5.85; p<.0005), and accuracy was 
higher (91% vs. 72%; t32=8.65; p<.0001). 
A distinction between distantly related pairs and 
homonyms was found as well. Response times for 
distantly related pairs was faster than for homo-
nyms (distantly related mean: 1253ms, homonym 
mean: 1406ms; t32=2.38; p<.0001). Accuracy was en-
hanced as well for this group (distantly related 
mean: 81%, unrelated mean: 62%; t32=5.66; p<.0001). 
Related meanings, even distantly related, seem to 
be easier to access than unrelated meanings. 
500
700
900
1100
1300
1500
Same Close Distant Unrelated
 Figure 1. Mean response time (ms).
 
40
50
60
70
80
90
100
Same Close Distant Unrelated
 Figure 2. Mean accuracy (% correct).  
 
A final planned comparison tested for a linear 
progression through the test conditions. Although 
somewhat redundant with the other comparisons, 
this test did reveal a highly significant linear pro-
gression for response time (F1,32=95.8; p<.0001) 
and for accuracy (F1,32=100.1; p<.0001). 
People have an increasingly difficult time ac-
cessing the meaning of a word as the relatedness of 
the meaning in the first phrase grows more distant. 
They respond more slowly and their accuracy de-
clines. However, closely related senses are almost 
as easy to access as same sense phrases. These re-
sults suggest that closely related word senses may 
be represented in the mental lexicon much like a 
single sense, perhaps sharing a core semantic re-
presentation. 
The linear progression through meaning related-
ness is also compatible with a theory in which the 
semantic representations of related senses overlap. 
Rather than being discrete entities attached to a 
main ?entry?, they could share a general semantic 
space. Various portions of the space could be acti-
vated depending on the context in which the word 
occurs. This structure allows for more coarse-
grained or more fine-grained distinctions to be 
made, depending on the needs of the moment. 
A structure in which the semantic representations 
overlap allows for the apparently smooth progres-
sion from same sense usages to more and more 
distantly related usages. It also provides a simple 
explanation for semantically underdetermined 
usages of a word. Although separate senses of a 
word can be identified in different contexts, in 
some contexts, both senses (or a vague meaning 
indeterminate between the two) seem to be 
represented by the same word. For example, 
?newspaper? can refer to a physical object: ?He 
tore the newspaper in half?, or to the content of a 
publication: ?The newspaper made me mad today, 
suggesting that our committee is corrupt.? The sen-
251
tence ?I really like this newspaper? makes no 
commitment to either sense.  
. 
3 Conclusions 
What does this mean for WSD? Most would 
agree that NLP applications would benefit from the 
ability to distinguish homonym-level meaning dif-
ferences.  Similarly, most would agree that it is not 
necessary to make very fine distinctions, even if 
we can describe them. For example, the process of 
cleaning a cup is discernibly different from the 
process of cleaning a shirt, yet we would not want 
to have a WSD system try to distinguish between 
every minor variation on cleaning. The problem 
comes with deciding when meanings can be consi-
dered the same sense, and when they should be 
considered different. 
The results of this study suggest that some word 
usages considered different by WordNet provoke 
similar responses as those to same sense usages. If 
these usages activate the same or largely overlap-
ping meaning representations, it seems safe to as-
sume that little meaning loss would result from 
clustering these closely related senses into one 
more general sense. Conversely, people reacted to 
distantly related senses much as they did to homo-
nyms, suggesting that making distinctions between 
these usages would be useful in a WSD system.  
A closer analysis of the study materials reveals 
differences between the types of distinctions made 
in the closely related senses and the types made in 
the distantly related senses. Most of the closely 
related senses distinguished between different con-
crete usages, whereas the distantly related senses 
distinguished between a concrete usage and a fi-
gurative or metaphorical usage. This suggests that 
grouping concrete usages together may result in 
little, if any, meaning loss. It may be more impor-
tant to keep concrete senses distinct from figura-
tive or metaphorical senses. The present study, 
however, divided senses only on degree of related-
ness rather than type of relatedness. It would be 
useful in future studies to address more directly the 
question of distinctions based on concreteness, 
animacy, agency, and so on.  
 
 
References  
Chklovski, Tim, and Rada Mihalcea. 2002. Building a 
sense tagged corpus with open mind word expert. 
Proc. of ACL 2002 Workshop on WSD: Recent Suc-
cesses and Future Directions. Philadelphia, PA. 
Duffield, Cecily Jill, Jena D. Hwang, Susan Windisch 
Brown, Dmitriy Dligach, Sarah E.Vieweg, Jenny 
Davis, Martha Palmer. 2007. Criteria for the manual 
grouping of verb senses. Linguistics Annotation 
Workshop, ACL-2007. Prague, Czech Republic.  
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance 
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: 
The 90% solution. Proc. of HLT-NAACL 2006. New 
York, NY.  
Ide, Nancy, and Yorick Wilks. 2007. Making sense 
about sense. In Word Sense Disambiguation: Algo-
rithms and Applications, E. Agirre and P. Edmonds 
(eds.). Dordrecht, The Netherlands: Springer. 
Klein, D., and Murphy, G. (2001). The representation of 
polysemous words. J of Memory and Language 45, 
259-282. 
Mihalcea, Rada, and Dan I. Moldovan. 2001. Automatic 
generation of a coarse-grained WordNet. In Proc. of 
NAACL Workshop on WordNet and Other Lexical Re-
sources. Pittsburg, PA. 
Navigli, Roberto. 2006. Meaningful clustering of word 
senses helps boost word sense disambiguation per-
formance. Proc. of the 21st International Conference 
on Computational Linguistics. Sydney, Australia. 
Palmer, Martha, Hwee Tou Ng, and Hoa Trang Dang. 
2007. Evaluation of WSD systems. In Word Sense 
Disambiguation: Algorithms and Applications, E. 
Agirre and P. Edmonds (eds.). Dordrecht, The Neth-
erlands: Springer. 
Pylkk?nen, L., Llin?s, R., and Murphy, G. L. (2006). 
The representation of polysemy: MEG evidence. J of 
Cognitive Neuroscience 18, 97-109. 
Rodd, J., Gaskell, G., and Marslen-Wilson, W. (2002). 
Making sense of semantic ambiguity: Semantic com-
petition in lexical access. J. of Memory and Lan-
guage, 46, 245-266.  
Snow, Rion, Sushant Prakash, Dan Jurafsky and And-
rew Y. Ng. 2007. Learning to merge word senses. 
Proc. of EMNLP 2007. Prague, Czech Republic. 
Snyder, Benjamin, and Martha Palmer. 2004. The Eng-
lish all-words task. Proc. of ACL 2004 SENSEVAL-3 
Workshop. Barcelona, Spain. 
252
Proceedings of the Linguistic Annotation Workshop, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Criteria for the Manual Grouping of Verb Senses  Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,  Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach,  sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu  
Abstract  In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a case-by-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.  1  Credits  We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.  2  Introduction  Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various 
lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on fine-grained senses, but the results are more than 16% lower. We begin by describing the overall process.  3  The Grouping and Annotation Process  The process for building our database with the appropriate level of verb sense distinctions                                                 1 Task 17,  http://nlp.cs.swarthmore.edu/semeval/.  
49
  
involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, ?groupers?) cluster fine-grained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin?s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al, 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged.  Completed verb sense groupings are sent through sample-annotation and tagged by two annotators. Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actual-annotation. Groupings that receive less than 90% ITA scores are regrouped (Hovy et al, 2006). Revisions are made based on a second grouper?s evaluation of the original grouping, as well as patterns of annotator disagreement. Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation. Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team. It is sometimes impossible to get ITA scores over 85% for high   
frequency verbs that also have high entropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sample-annotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al, 2005).  Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actual-annotation and adjudication. The next 660 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping will proceed according to these semantic domains rather than by verb frequency. As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted. This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.  
 Figure 1:  The grouping and annotation process.  
50
  
4  Grouping Methodology  Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et. al. 2006). Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings. Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one possible clustering as a middle ground between PropBank and WordNet senses for each verb. Our highest priority is to then create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high ITA scores. Initial clustering is based on groupers? intuitions of the most salient categories. Many verb groupings, such as that for the verb kill, provide little detailed syntactic or semantic analysis and yet have received high ITA scores. The success of these intuitive sense groupings is not due to lack of polysemy; kill has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.  While annotators have little trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continuum between two distinct lexical usages. In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text. 
Syntactic criteria: Annotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings. For example, split was originally grouped with consideration for the units resulting from a splitting event (i.e. whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units.)  This grouping proved difficult for annotators to distinguish, with an ITA of 42%. Using the causative/inchoative alternation for verbs in the ?break-45.1? class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%. Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continuum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them. Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/-locative]. It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms. Semantic features of the verb that are considered include aspectual features, as illustrated by the use of [+/-punctual] in sense groupings for make (Figure 2). However, it may be argued that this feature is unnecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators. Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others. However, these features proved less useful than those Sense group Description and Commentary WordNet 2.1 senses Examples 8 Attain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has been met. Includes: MAKE IT 
make 13, 22, 38 - He made the basketball team. - We barely made the plane. - I made the opening act in plenty of time. - Can you believe it? We made it!  
9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) PP/INFP  
make 30, 37 make off 1 make way 1  
- As the enemy approached our town, we made for the hills. - He made his way carefully across the icy parking lot. - They made off with the jewels. Figure 2: Sense groupings 8 and 9 for ?make.? Senses are distinguished in part by aspectual features marked on the verb.  
51
 described above, and annotators not familiar with linguistic theory found them to be confusing. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators. Certain compositional features of verbs have also proven to be confusing for annotators. In several cases, attempts to distinguish sense groupings based on manner and path have resulted in increased annotator disagreement. In the first attempt at grouping roll, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (?The baby rolled over,? vs ?She rolled over to the wall,?) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings.  5  Conclusion  Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al, 2004; Palmer et al, 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al, 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verbs in similar semantic domains as defined by VerbNet. By using this approach when creating sense groupings, we are 
able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.  References Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY. Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA. Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy. Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL. OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLT-NAACL 2006. New York, NY. Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.  Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007). Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.  
52
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, page 1
Manchester, August 2008
The Relevance of a Cognitive Model of the Mental
Lexicon to Automatic Word Sense Disambiguation
Martha Palmer and Susan Brown
University of Colorado at Boulder
Department of Linguistics
Hellems 290, 295 UCB
Boulder, CO 80309-0295, USA
Supervised word sense disambiguation requires training corpora that have been tagged with word senses,
and these word senses typically come from a pre-existing sense inventory. Space limitations imposed by
dictionary publishers have biased the field towards lists of discrete senses for an individual lexeme.
Although some dictionaries use hierarchical entries to emphasize relations between senses, many do not.
WordNet, which has been the default choice of NLP researchers for sense tagging because of its broad
coverage and easy accibility, does not have hierarchical entries. Could the relations between senses that
are captured by a hierarchy be useful to NLP systems? Concerns have also been raised about whether or
not WordNet?s word senses are unnecessarily fine-grained. WSD systems are obviously more successful
in distinguishing coarse-grained senses than fine-grained ones (Navigli, 2006), but important information
could be lost if fine-grained distinctions are ignored. Recent psycholinguistic evidence seems to indicate
that closely related word senses may be represented in the mental lexicon much like a single sense,
whereas distantly related senses may be represented more like discrete entities (Brown, 2008). These
results suggest that, for the purposes of WSD, closely related word senses can be clustered together into
a more general sense with little meaning loss. This talk will describe this psycholinguistic research and
its current implications for automatic word sense disambiguation, as well as plans for future research and
its possible impact.
c
? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
1
	ABBCBBDEFFABAAB

BAFDFDBProceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
