Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756?764,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Stream-based Randomised Language Models for SMT
Abby Levenberg
School of Informatics
University of Edinburgh
a.levenberg@sms.ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
Randomised techniques allow very big
language models to be represented suc-
cinctly. However, being batch-based
they are unsuitable for modelling an un-
bounded stream of language whilst main-
taining a constant error rate. We present a
novel randomised language model which
uses an online perfect hash function
to efficiently deal with unbounded text
streams. Translation experiments over
a text stream show that our online ran-
domised model matches the performance
of batch-based LMs without incurring the
computational overhead associated with
full retraining. This opens up the possibil-
ity of randomised language models which
continuously adapt to the massive volumes
of texts published on the Web each day.
1 Introduction
Language models (LM) are an integral feature
of statistical machine translation (SMT) systems.
They assign probabilities to generated hypothe-
ses in the target language informing lexical selec-
tion. The most common form of LMs in SMT
systems are smoothed n-gram models which pre-
dict a word based on a contextual history of n? 1
words. For some languages (such as English) tril-
lions of words are available for training purposes.
This fact, along with the observation that ma-
chine translation quality improves as the amount
of monolingual training material increases, has
lead to the introduction of randomised techniques
for representing large LMs in small space (Talbot
and Osborne, 2007; Talbot and Brants, 2008).
Randomised LMs (RLMs) solve the problem of
representing large, static LMs but they are batch
oriented and cannot incorporate new data with-
out fully retraining from scratch. This property
makes current RLMs ill-suited for modelling the
massive volume of textual material published daily
on the Web. We present a novel RLM which is
capable of incremental (re)training. We use ran-
dom hash functions coupled with an online perfect
hashing algorithm to represent n-grams in small
space. This makes it well-suited for dealing with
an unbounded stream of training material. To our
knowledge this is the first stream-based RLM re-
ported in the machine translation literature. As
well as introducing the basic stream-based RLM,
we also consider adaptation strategies. Perplex-
ity and translation results show that populating
the language model with material chronologically
close to test points yields good results. As with
previous randomised language models, our experi-
ments focus on machine translation but we also ex-
pect that our findings are general and should help
inform the design of other stream-based models.
Section 2 introduces the incrementally retrain-
able randomised LM and section 3 considers re-
lated work; Section 4 then considers the question
of how unbounded text streams should be mod-
elled. Sections 5 and 6 show stream-based trans-
lation results and properties of our novel data-
structure. Section 7 concludes the paper.
2 Online Bloomier Filter LM
Our online randomised LM (O-RLM) is based
on the dynamic Bloomier filter (Mortensen et al,
2005). It is a variant of the batch-based Bloomier
filter LM of Talbot and Brants (2008) which we
refer to as the TB-LM henceforth. As with the
TB-LM, the O-RLM uses random hash functions
to represent n-grams as fingerprints which is the
main source of space savings for the model.
2.1 Online Perfect Hashing
The key difference in our model as compared to
the TB-LM is we use an online perfect hashing
756
Figure 1: Inserting an n-gram into the dynamic Bloomier filter. Above: an n-gram is hashed to its target
bucket. Below: the n-gram is transformed into a fingerprint and the same target bucket is scanned. If a
collision occurs that n-gram is diverted to the overflow dictionary; otherwise the fingerprint is stored in
the bucket.
function instead of having to precompute the per-
fect hash offline prior to data insertion.
The online perfect hash function uses two data
structures: A and D. A is the main, randomised
data structure and is an array of b dictionaries
A
0
, . . . , A
b?1
. D is a lossless data structure which
handles collisions in A. Each of the dictionaries in
A is referred to as a ?bucket?. In our implementa-
tion the buckets are equally sized arrays of w-bit
cells. These cells hold the fingerprints and values
of n-grams (one n-gram-value pair per cell).
To insert an n-gram x and associated value
v(x) into the model, we select a bucket A
i
by
hashing x into the range i ? [0, . . . , b ? 1].
Each bucket has an associated random hash func-
tion, h
A
i
, drawn from a universal hash func-
tion (UHF) family h (Carter and Wegman, 1977),
which is then used to generate the n-gram finger-
print: f(x) = h
A
i
(x).
If the bucket A
i
is not full we conduct a scan of
its cells. If the fingerprint f(x) is not already en-
coded in the bucket A
i
we add the fingerprint and
value to the first empty cell available. We allocate
a preset number of the least significant bits of each
w-bit cell to hold v(x) and the remaining most sig-
nificant bits for f(x) but this is arbitrary. Any en-
coding scheme, such as the packed representation
of Talbot and Brants (2008), is viable here.
However, if f(x) ? A
i
already (there is a colli-
sion) we store the n-gram x and associated value
v(x) in the lossless overflow dictionary D instead.
D also holds the n-grams that were hashed to any
buckets that are already full.
To query for the value of an n-gram, we first
check if the gram is in the overflow dictionary D.
If it is, we return the associated value. Otherwise
we query A using the same hash functions and
procedure as insertion. If we find a matching fin-
gerprint in the appropriate bucket A
i
we have a
hit with high probability. Deletions and updates
are symmetric to querying except we reset the cell
to the null value or update its value respectively.
As with other randomised models we construct
queries with the appropriate sanity checks to lower
the error rate efficiently (Talbot and Brants, 2008).
2.2 Data Insertion
Initially we seed the language model with a large
corpus S in the usual manner associated with
batch LMs. Then, when processing the stream,
we aggregate n-gram counts for some consecu-
tive portion, or epoch, of the input stream. We
can vary the size of stream window. For example
we might batch-up a day or week?s worth of mate-
rial. Intuitively, smaller windows produce results
that are sensitive to small variation in the stream,
while longer windows (corresponding to data over
a longer time period) average out local spikes. The
exact window size is a matter of experimentation.
In our MT experiments (section 5) we can com-
pute counts within the streaming window exactly
but randomised approaches (such as the approxi-
mate counting schemes from section 3) can easily
be employed instead.
757
These n-grams and counts are then considered
for insertion into the online model. If we decide
to insert an n-gram, we either update the count of
that n-gram if we previously inserted it or else we
insert it as a new entry. Note that there is some
probability we may encounter a false positive and
update some other n-gram in the model.
2.3 Properties
The online perfect hash succeeds by associating
each n-gram with only one cell in A rather than
having it depend on cells (or bits) which may be
shared by other n-grams as with the TB-LM. Since
each n-gram?s encoding in the model uses distinct
bits and is independent of all other events it can
not corrupt other n-grams when deleted.
Adding the overflow dictionary D means that
we use more space than the TB-LM for the same
support. It is shown in Mortensen et al (2005) that
the expected size of D is a small fraction of the to-
tal number of events and its space usage comprises
less than O(|S|) bits with high probability.
There is a nonzero probability for false posi-
tives. Since the overflow dictionary D has no er-
rors, the expected error rate for our dynamic struc-
ture is the probability of a random collision in the
hash range of each h
A
i
for each bucket cell com-
pared. In our setup we have
Pr(falsepos) =
|A
i
|
2
|f(x)|
where |f(x)| is the number of bits of each w-bit
cell used for the fingerprint f(x). w also primar-
ily governs space used in the model. The O-RLM
assumes only valid updates and deletions are per-
formed (i.e. we do not remove or update entries
that were never inserted prior).
The O-RLM takes time linear to the input size
for training and uses worst-case constant time for
querying and deletions where the constant is de-
pendent on the number of cells per bucket in A.
The number of bucket cells also effects the overall
error rate significantly since smaller ranges reduce
the probability of a collision. However, too few
cells per bucket will result in many full buckets
when the bucket hash function is not highly IID.
2.4 Basic RLM Comparisons
Table 1 compares expected versus observed false
positive rates for the Bloom filter, TB-LM, and O-
RLM obtained by querying a model of approxi-
mately 280M events with 100K unseen n-grams.
LM Expected Observed RAM
Lossless 0 0 7450MB
Bloom 0.0039 0.0038 390MB
TB-LM 0.0039 0.0033 640MB
O-RLM 0.0039 0.0031 705MB
Table 1: Example false postive rates and corre-
sponding memory usage for all randomised LMs.
We see the bit-based Bloom filter uses signifi-
cantly less memory than the cell-based alternatives
and the O-RLM consumes more memory than the
TB-LM for the same expected error rate.
3 Related Work
3.1 Randomised Language Models
Talbot and Osborne (2007) used a Bloom filter
(Bloom, 1970) to encode a smoothed LM. A
Bloom filter (BF) represents a set S from arbitrary
domain U and supports membership queries such
as?Is x ? S??. The BF uses an array of m bits and
k independent UHFs each with range 0, . . . ,m?1.
For insertion, each item is hashed through the k
hash functions and the resulting target bits are set
to one. During testing, an event x ? U is passed
through the same k hash functions and if any bit
tested is zero then x was not in the support S.
The Bloomier filter directly represents key-
value pairs by using a table of cells and a family of
k associated hash functions (Chazelle et al, 2004).
Each key-value pair is associated with k cells in
the table via a perfect hash function. Talbot and
Brants (2008) used a Bloomier filter to encode a
LM. Before data can be added to the Bloomier fil-
ter, a greedy perfect hashing of all entries needs to
be computed in advance; this attempts to associate
each event in the support with one unique table cell
so no other entry collides with it. The procedure
can fail and might need to be repeated many times.
Neither of these two randomised language mod-
els are suitable for modelling a stream. Given the
fact that the stream is of unbounded size, we are
forced to delete items if we wish to maintain a
constant error rate and account for novel n-grams.
However, the Bloom filter LM nor the Bloomier
Filter LM support deletions. The bit sharing of the
Bloom filter (BF) LM (Talbot and Osborne, 2007)
means deletions may corrupt shared stored events.
The Bloomier filter LM (Talbot and Brants, 2008)
has a precomputed matching of keys shared be-
tween a constant number of cells in the filter array.
758
Deleting items from a Bloomier Filter without re-
computing the perfect hash will corrupt it.
3.2 Probabilistic Counting
Concurrent work has used approximate counting
schemes based on Morris (1978) to estimate in
small space frequencies over a high volume in-
put text stream (Van Durme and Lall, 2009; Goyal
et al, 2009). The space savings are due to com-
pact storage of counts and retention of only a
small subset of the available n-grams in the data
stream. Since the final LMs are still lossless (mod-
ulo counts), the resulting LM needs significant
space. It is trivial to use probabilistic counting
within our framework.
3.3 Compact Exact Language Models
Randomised algorithms are not the only com-
pact representation schemes. Church et al (2007)
looked at Golomb Coding and Brants et al (2007)
used tries in a distributed setting. These methods
are less succinct than randomised approaches.
3.4 Adaptive Language Models
There is a large literature on adaptive LMs from
the speech processing domain (Bellegarda, 2004).
The primary difference between the O-RLM and
other adaptive LMs is that we add and remove n-
grams from the model instead of adapting only the
parameters of the current support set.
3.5 Domain adaptation in Machine
Translation
Within MT there has been a variety of approaches
dealing with domain adaption (for example (Wu
et al, 2008; Koehn and Schroeder, 2007). Typi-
cally LMs are interpolated with one another, yield-
ing good results. These models are usually stat-
ically trained, exact and unable to deal with an
unbounded stream of monolingual data. Domain
adaptation has similarities with streaming, in that
our stream may be non-stationary. A crucial dif-
ference however is that the stream is of unbounded
length, whereas domain adaptation usually as-
sumes some finite and fixed training set.
4 Stream-based translation
Streaming algorithms have numerous applications
in mainstream computer science (Muthukrishnan,
2003) but to date there has been very little aware-
ness of this field within computational linguistics.
Figure 2: Stream-based translation. The online
RLM uses data from the target stream and the last
test point in the source stream for adaptation.
A text stream can be thought of as a unbounded
sequence of documents that are time-stamped and
we have access to them in strict chronological or-
der. The volume of the stream is so large we can
afford only a limited number of passes over the
data (typically one).
Text streams naturally arise on the Web when
millions of new documents are published each day
in many languages. For instance, 18 thousand
websites continuously publish news stories in 40
languages and there are millions of multilingual
blog postings per day. There are over 30 billion
e-mails sent daily and social networking sites, in-
cluding services such as Twitter, generate an adun-
dance of textual data in real time. Web crawlers
that spidered all these new documents would pro-
duce an unbounded input stream.
The stream-based translation scenario is as fol-
lows: we assume that each day we see a source
stream of many new newswire stories that need
translation. We also assume a stream of newswire
stories in the target language. Intuitively, since the
concurrent streams are from the same domain, we
can use the contexts provided in the target stream
to help with the translation of the source stream
(Figure 2). From a theoretical perspective, since
we cannot represent the entirety of the stream and
wish to maintain a constant error rate, we are
forced to throw some information away.
Given that the incoming text stream contains far
too much data to store in its entirety an immediate
question we would like to answer is: within our
LM, which subset of the target text stream should
759
 180
 200
 220
 240
 260
 280
 300
 20  25  30  35  40  45  50
pe
rp
le
xi
ty
weeks
Reuters 96-97 LM subsets
51-week baseline
20-week subset test 1
20-week subset test 2
Figure 3: Perplexity results using streamed data.
Perplexity decreases as we retrain LMs using data
chronologically closer to the (two) test dates.
we represent in our model?
Using perplexity, we investigated this question
using a text stream based on Reuter?s RCV1 text
collection (Rose et al, 2002). This contains 800k
time-stamped newswire stories from a full calen-
der year (8.20.1996 - 8.19.1997). We used the
SRILM (Stolcke, 2002) to construct an exact tri-
gram model built using all the RCV1 data with the
exception of the final week which we held out as
test data. This served as an oracle since we store
all of the stream.
We then trained multiple exact LMs of much
smaller sizes, coined subset LMs, to simulate
memory constraints. For a given date in the RCV1
stream, these subset LMs were trained using a
fixed window of previously seen documents up to
that data. Then we obtained perplexity results for
each subset LM against our test set.
Figure 3 shows an example. For this experiment
subset LMs were trained using a sliding window
of 20 weeks with the window advancing over a
period of three weeks each time. The two arcs
correspond to two different test sets drawn from
different days. The arcs show that recency has a
clear effect: populating LMs using material closer
to the test data date produces improved perplexity
performance. The LM chronologically closest to
a given test set has perplexity closest to the results
of the significantly larger baseline LM which uses
all the stream. As expected, using all of the data
yields the lowest perplexity.
We note that this is a robust finding, since we
also observe it in other domains. For example, we
Epoch Stream Window
1 08.20.1996 to 01.01.1997
2 01.02.1997 to 04.23.1997
3 04.24.1997 to 08.18.1997
Table 2: The stream timeline is divided into win-
dowed epochs for our recency experiments.
conducted the same tests over a stream of 18 bil-
lion tokens drawn from 80 million time-stamped
blog posts downloaded from the web with match-
ing results. The effect of recency on perplexity has
also been observed elsewhere (see, for example,
Rosenfeld (1995) and Whittaker (2001)).
Our experiments show that a possible way to
tackle stream-based translation is to always focus
the attention of the LM on the most recent part
of the stream. This means we remove data from
the model that came from the receding parts of the
stream and replace it with the present.
5 SMT Experiments
5.1 Experimental Setup
We used publicly available resources for all our
tests: for decoding we used Moses (Koehn and
Hoang, 2007) and our parallel data was taken from
the Spanish-English section of Europarl. For test
material, we translated 63 documents (800 sen-
tences) from three randomly selected dates spaced
throughout the RCV1 year (January 2nd, April
24, and August 19).1 This effectively divided the
stream into three epochs between the test dates (
table 2). We held out 300 sentences for minimum
error rate training (MERT) (Och, 2003) and opti-
mised the parameters of the feature functions of
the decoder for each experimental run.
The RCV1 is not a large corpus when compared
to the entire web but it is multilingual, chronologi-
cal, and large enough to enable us to test the effect
of recency in a translation setting.
5.2 Adaption
We looked at a number of ways of adapting the
O-RLM:
1. (Random) Randomly sample the stream and
for each new n-gram encountered, insert
1As RCV1 is not a parallel corpus we translated the ref-
erence documents ourselves. This parallel corpus is available
from the authors.
760
Order Full Epoch 1 Epoch 3
1 1.25M 0.6M 0.7M
2 14.6 M 6.8M 7.0M
3 50.6 M 21.3M 21.7M
4 90.3 M 34.8M 35.4M
5 114.7M 41.8M 42.6M
Total 271.5M 105M 107.5M
Table 3: Distinct n-grams (in millions) encoun-
tered in the full stream and example epochs.
it and remove some previously inserted n-
gram, irrespective of whether it was ever re-
quested by the decoder or is a prefix.
2. (Conservative) For each new n-gram en-
countered in the stream, insert it in the filter
and remove one previously inserted n-gram
which was never requested by the decoder.
To preserve consistency we do not remove
lower-order grams that are needed to estimate
backoff probability for higher-order smooth-
ing. Counts are updated for n-grams already
in the model if the new count observed is
larger than the current one.
3. (Severe) Differs from the conservative ap-
proach only in that we delete all unused n-
grams (i.e. all those not requested by the de-
coder in the previous translation task) from
the O-RLM before adapting with data from
the stream. This means the data structure is
sparsely populated for all runs.
All the TB-LMs and O-RLMs were unpruned 5-
gram models and used Stupid-backoff smoothing
(Brants et al, 2007) 2 with the backoff parameter
set to 0.4 as suggested. The number of distinct n-
grams encountered in the stream for two epochs is
shown in Table 3.
Table 6 shows translation results using these
adaption strategies. In practice, the random ap-
proach does not work while the conservative and
severe adaption techniques produce equivalent re-
sults due to the small proportion of data in the
model that is queried during decoding. All the MT
experiments that follow use the severe method and
the overflow dictionary always holds less than 1%
of the total elements in the model.
2Smoothing text input data streams poses an interesting
problem we hope to investigate in the future.
Date Lossless TB-LM O-RLM
Jan 37.83 37.12 37.17
Apr 34.88 34.21 34.79
Aug 29.05 28.52 28.44
Avg 33.92 33.28 33.46
Table 4: Baseline translation results in BLEU us-
ing data from the first stream epoch with a lossless
LM (4.5GB RAM), the TB-LM and the O-RLM
(300MB RAM). All LMs are static.
5.3 Training Regimes
We now consider stream-based translation. Our
first naive approach is to continually add new data
from the stream to the training set without delet-
ing anything. Given a constant memory bound this
strategy only increases the error rate over time as
discussed. Our second, computationally demand-
ing approach is, before each test point, to rebuild
the TB-LM from scratch using the stream data
from the most recent epoch as the training set.
This is batch retraining. The final approach in-
crementally retrains online. This utilizes the same
training data as above (the stream data from the
last epoch) but instead of full retraining it replaces
n-grams currently in the model with unseen n-
grams and counts encountered in the data stream.
5.4 Streaming Translation Results
Each table shows translation results for the three
different test times in the stream. All results re-
ported use the case-sensitive BLEU score.
For our baselines we use static LMs trained on
the first epoch?s data to test all three translation
points in the source stream. This is the tradi-
tional approach. We trained an exact, modified
Kneser-Ney smoothed LM (here we do not en-
force a memory constraint) and also used the TB-
LM and O-RLM to verify our structures adequecy.
Results are shown in table 4. The exact model
gives better performance overall due to the more
sophisticated smoothing used.
Table 5 shows results for a set of stream-based
LMs using the TB-LM and the O-RLM with mem-
ory bounds of 200MB and 300MB. As expected,
the naive models performance degrades over time
as we funnel more data into the TB-LM and the
error rises. The batch retrained TB-LMs and O-
RLMs have constant error rates of 1
2
8
and 1
2
12
and
so outperform the naive approach. Since the train-
ing data is identical we see (approximately) equal
761
Naive TB-LM Batch Retrained TB-LM O-RLM
Date 200MB 300MB 200MB 300MB 200MB 300MB
Jan 35.94 37.12 35.94 37.12 36.44 37.17
Apr 33.55 35.79 36.01 35.99 35.87 36.10
Aug 22.44 26.07 28.97 29.38 29.00 29.18
Avg 30.64 32.99 33.64 34.16 33.77 34.15
Table 5: Translation results for stream-based LMs in BLEU. Performance degrades with time using the
Naive approach. The batch retrained TB-LM and stream-based O-RLM use constant error rates of 1
2
8
and 1
2
12
.
performance from the batch retrained and online
models. We also see some improvement compared
to the static baselines when the LMs use the most
recent data from the target language stream with
respect to the current translation point.
The key difference is that each time we batch
retrain the TB-LM, we must compute a perfect
hashing of the new training set. This is computa-
tionally demanding since the perfect hashing algo-
rithm uses Monte Carlo randomisation which fails
routinely and must be repeated. To make the al-
gorithm tractable the training data set must be di-
vided into lexically sorted subsets as well. This
requires extra passes over the data which may not
be trivial in a streaming environment.
In contrast, the O-RLM is incrementally re-
trained online. This makes it more resource ef-
ficient since we find bits in the model for the n-
grams dynamically without using more memory
than we intially set. Note that even though the O-
RLM is theoretically less space efficient than the
TB-LM, when using the same amount of memory
translation performance is comparable.
6 O-RLM Properties
The previous experiments confirm that the O-
RLM can be employed as a LM in an SMT setting
but it is useful to get insight into the intrinsic prop-
erties of the data structure. Many of the properties
of the model, such as the number of bits per fin-
gerprint, follow directly from the TB-LM but the
relationship between the overflow dictionary and
the randomised buckets is novel.
Figures 4 and 5 shows properties of the O-RLM
while varying only the number of cells in each
bucket and keeping all other model parameters
constant. We test membership of n-grams in an
unseen corpus against those stored in the table.
Our tests were conducted over a larger stream of
1.25B n-grams from the Gigaword corpus(Graff,
Date Severe Random Conservative
Jan 36.44 36.44 36.44
Apr 35.87 31.08 35.51
Aug 29.00 19.31 29.14
Avg 33.77 29.11 33.70
Table 6: Adaptation results measured in BLEU.
Random deletions degrade performance when
adapting a 200MB O-RLM.
2003). We set our space usage to match the 3.08
bytes per n-gram reported in Talbot and Brants
(2008) and held out just over 1M unseen n-grams
to test the error rates of our models.
In Figure 4 we see a direct correlation between
model error and cells per buckets. As the num-
ber of cells decreases the false positive rate drops
as well since fewer cells to compare against per
bucket means a lower chance of producing colli-
sions. If the range is decreased too much though
more data is diverted to the overflow dictionary
due to many buckets reaching capacity when in-
serting and adapting. Clearly this is less space ef-
ficient. Figure 5 shows the relationship between
the percent of data in the overflow dictionary and
the total cells per bucket.
7 Conclusions
Our experiments have shown that for stream-based
translation, using recent data can benefit perfor-
mance but simply adding entries to a randomised
representation will only reduce translation perfor-
mance over time. We have presented a novel ran-
domised language model based on dynamic per-
fect hashing that supports online insertions and
deletions. As a consequence, it is considerably
faster and more efficient than batch retraining.
While not advocating the idea that only small
amounts of data are needed for language mod-
762
 0
 0.001
 0.002
 0.003
 0.004
 0.005
 0.006
 0.007
 50  100  150  200  250
fa
ls
e 
po
si
tiv
e 
ra
te
s
cells per bucket
O-RLM Error rate
Figure 4: The O-RLM error rises in correlation
with the number of cells per bucket.
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 0.016
 0.018
 50  100  150  200  250
%
 o
f d
at
a 
in
 o
ve
rf
lo
w
 d
ic
tio
na
ry
cells per bucket
Overflow Dictionary Size
Figure 5: Too few cells per bucket causes a higher
percentage of the data to be stored in the overflow
dictionary due to full buckets.
elling, within a bounded amount of space our re-
sults show that it is better to have a low error rate
and store a wisely chosen fraction of the data than
having a high error rate and storing more of it.
Clearly tradeoffs will vary between applications.
This is the first stream-based randomised lan-
guage model and associated machine translation
system reported in the literature. Clearly there are
many interesting open questions for future work.
For example, can we use small randomised repre-
sentations called sketches to compactly represent
side-information on the stream telling us which as-
pects of it we should insert into our data? How
can we efficiently deal with smoothing in this set-
ting? Our adaptation scheme is simple and our
data stream is tractable. Currently we are con-
ducting tests over much larger, higher variance
text streams from crawled blog data. In the fu-
ture we will also consider randomised representa-
tions of other adaptive LMs in the literature using
a static background LM in conjunction with our
online one. We ultimately hope to deploy large-
scale LMs which continuously adapt to the vast
amount of material published on the Web without
incurring significant computational overhead.
Acknowledgements
The authors would like to thank David Talbot,
Adam Lopez and Phil Blunsom for their valu-
able comments and insight. This work was sup-
ported in part under the GALE program of the De-
fense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022.
References
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93?108.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422?426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867.
J. Lawrence Carter and Mark N. Wegman. 1977. Uni-
versal classes of hash functions (extended abstract).
In STOC ?77: Proceedings of the ninth annual ACM
symposium on Theory of computing, pages 106?112,
New York, NY, USA. ACM Press.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: an ef-
ficient data structure for static support lookup ta-
bles. In SODA ?04: Proceedings of the fifteenth an-
nual ACM-SIAM symposium on Discrete algorithms,
pages 30?39, Philadelphia, PA, USA. Society for In-
dustrial and Applied Mathematics.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 199?207,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
763
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boulder, CO.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium (LDC-2003T05).
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Robert Morris. 1978. Counting large numbers
of events in small registers. Commun. ACM,
21(10):840?842.
Christian Worm Mortensen, Rasmus Pagh, and Mihai
Pa?trac?cu. 2005. On dynamic range reporting in one
dimension. In STOC ?05: Proceedings of the thirty-
seventh annual ACM symposium on Theory of com-
puting, pages 104?111, New York, NY, USA. ACM.
S. Muthukrishnan. 2003. Data streams: algorithms
and applications. In SODA ?03: Proceedings of the
fourteenth annual ACM-SIAM symposium on Dis-
crete algorithms, pages 413?413, Philadelphia, PA,
USA. Society for Industrial and Applied Mathemat-
ics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?
31.
Ronald Rosenfeld. 1995. Optimizing lexical and n-
gram coverage via judicious use of linguistic data.
In In Proc. European Conf. on Speech Technology,
pages 1763?1766.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, 2002.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468?476.
Benjamin Van Durme and Ashwin Lall. 2009. Prob-
abilistic counting with randomized storage. In
Twenty-First International Joint Conference on Ar-
tificial Intelligence (IJCAI-09), Pasadena, CA, July.
E. W. D. Whittaker. 2001. Temporal adaptation of lan-
guage models. In In Adaptation Methods for Speech
Recognition, ISCA Tutorial and Research Workshop
(ITRW), pages 203?206.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 993?1000. Coling 2008 Organizing
Committee, August.
764
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 223?232, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Bayesian Model for Learning SCFGs with Discontiguous Rules
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Chris Dyer
School of Computer Science
Carnegie Mellon Univeristy
cdyer@cs.cmu.edu
Phil Blunsom
Dept. of Computer Science
University of Oxford
pblunsom@cs.ox.ac.uk
Abstract
We describe a nonparametric model
and corresponding inference algorithm
for learning Synchronous Context Free
Grammar derivations for parallel text. The
model employs a Pitman-Yor Process prior
which uses a novel base distribution over
synchronous grammar rules. Through both
synthetic grammar induction and statistical
machine translation experiments, we show
that our model learns complex translational
correspondences? including discontiguous,
many-to-many alignments?and produces
competitive translation results. Further,
inference is efficient and we present results on
significantly larger corpora than prior work.
1 Introduction
In the twenty years since Brown et al1992) pio-
neered the first word-based statistical machine trans-
lation (SMT) models substantially more expressive
models of translational equivalence have been devel-
oped. The prevalence of complex phrasal, discon-
tiguous, and non-monotonic translation phenomena
in real-world applications of machine translation has
driven the development of hierarchical and syntac-
tic models based on synchronous context-free gram-
mars (SCFGs). Such models are now widely used in
translation and represent the state-of-the-art in most
language pairs (Galley et al2004; Chiang, 2007).
However, while the models used for translation have
evolved, the way in which they are learnt has not:
na??ve word-based models are still used to infer trans-
lational correspondences from parallel corpora.
In this work we bring the learning of the minimal
units of translation in step with the representational
power of modern translation models. We present a
nonparametric Bayesian model of translation based
on SCFGs, and we use its posterior distribution to
infer synchronous derivations for a parallel corpus
using a novel Gibbs sampler. Our model is able
to: 1) directly model many-to-many alignments,
thereby capturing non-compositional and idiomatic
translations; 2) align discontiguous phrases in both
the source and target languages; 3) have no restric-
tions on the length of a rule, the number of nonter-
minal symbols per rule, or their configuration.
Learning synchronous grammars is hard due to
the high polynomial complexity of dynamic pro-
gramming and the exponential space of possible
rules. As such most prior work for learning SCFGs
has relied on inference algorithms that were heuristi-
cally constrained or biased by word-based alignment
models and small experiments (Wu, 1997; Zhang et
al., 2008; Blunsom et al2009; Neubig et al2011).
In contrast to these previous attempts, our SCFG
model scales to large datasets (over 1.3M sentence
pairs) without imposing restrictions on the form of
the grammar rules or otherwise constraining the set
of learnable rules (e.g., with a word alignment).
We validate our sampler by demonstrating its
ability to recover grammars used to generate
synthetic datasets. We then evaluate our model by
inducing word alignments for SMT experiments
in several typologically diverse language pairs and
across a range of corpora sizes. Our results attest to
our model?s ability to learn synchronous grammars
encoding complex translation phenomena.
223
2 Prior Work
The goal of directly inducing phrasal translation
models from parallel corpora has received a lot of
attention in the NLP and SMT literature. Marcu
and Wong (2002) presented an ambitious maximum
likelihood model and EM inference algorithm for
learning phrasal translation representations. The
first issue this model faced was a massive parameter
space and intractable inference. However a more
subtle issue is that likelihood based models of this
form suffer from a degenerate solution, resulting
in the model learning whole sentences as phrases
rather than minimal units of translation. DeNero
et al2008) recognised this problem and proposed
a nonparametric Bayesian prior for contiguous
phrases. This had the dual benefits of biasing the
model towards learning minimal translation units,
and integrating out the parameters such that a much
smaller set of statistics would suffice for inference
with a Gibbs sampler. However this work fell short
by not evaluating the model independently, instead
only presenting results in which it was combined
with a standard word-alignment initialisation, thus
leaving open the question of its efficacy.
The fact that flat phrasal models lack a structured
approach to reordering has led many researchers to
pursue SCFG induction instead (Wu, 1997; Cherry
and Lin, 2007; Zhang et al2008; Blunsom et
al., 2009). The asymptotic time complexity of
the inside algorithm for even the simplest SCFG
models is O(|s|3|t|3), too high to be practical for
most real translation data. A popular solution to
this problem is to heuristically restrict inference
to derivations which agree with an independent
alignment model (Cherry and Lin, 2007; Zhang et
al., 2008). However this may have the unintended
effect of biasing the model back towards the initial
alignments that they attempt to improve upon.
More recently Neubig et al2011) reported a
novel Bayesian model for phrasal alignment and
extraction that was able to model phrases of multiple
granularities via a synchronous Adaptor Grammar.
However this model suffered from the common
problem of intractable inference and results were
presented for a very small number of samples from
a heuristically pruned beam, making interpreting
the results difficult.
Blunsom et al2009) presented an approach
similar to ours that implemented a Gibbs sampler
for a nonparametric Bayesian model of ITG. While
that work managed to scale to a non-trivially sized
corpus, like other works it relied on a state-of-the-art
word alignment model for initialisation. Our model
goes further by allowing discontiguous phrasal
translation units. Surprisingly, the freedom
that this extra power affords allows the Gibbs
sampler we propose to mix more quickly, allowing
state-of-the-art results from a simple initialiser.
3 Model
We use a nonparametric generative model based on
the 2-parameter Pitman-Yor process (PYP) (Pitman
and Yor, 1997), a generalisation of the Dirichlet Pro-
cess, which has been used for various NLP modeling
tasks with state-of-the-art results such as language
modeling, word segmentation, text compression and
part of speech induction (Teh, 2006; Goldwater et
al., 2006; Wood et al2011; Blunsom and Cohn,
2011). In this section we first provide a brief defi-
nition of the SCFG formalism and then describe our
PYP prior for them.
3.1 Synchronous Context-Free Grammar
An synchronous context-free grammar (SCFG) is a
5-tuple ??,?, V, S,R? that generalises context-free
grammar to generate strings concurrently in two lan-
guages (Lewis and Stearns, 1968). ? is a finite set of
source language terminal symbols, ? is a finite set
of target language terminal symbols, V is a set of
nonterminal symbols, with a designated start sym-
bol S, and R is a set of synchronous rewrite rules.
A string pair is generated by starting with the pair
?S1 | S1? and recursively applying rewrite rules of
the form X ? ?s, t, a? where the left hand side
(LHS) X is a nonterminal in V , s is a string in
(? ? V )?, t is a string in (? ? V )? and a specifies
a one-to-one mapping (bijection) between nontermi-
nal symbols in s and t. The following are examples:1
VP ? ? schlage NP1 NP2 vor | suggest NP2 to NP1 ?
NP ? ? die Kommission | the commission ?
1The nonterminal alignment a is indicated through sub-
scripts on the nonterminals.
224
In a probabilistic SCFG, rules are associated with
probabilities such that the probabilities of all
rewrites of a particular LHS category sum to 1.
Translation with SCFGs is carried out by parsing
the source language with the monolingual source
language projection of the grammar (using standard
monolingual parsing algorithms), which induces
a parallel tree structure and translation in the
target language (Chiang, 2007). Alignment or
synchronous parsing is the process of concurrently
parsing both the source and target sentences,
uncovering the derivation or derivations that give
rise to a string pair (Wu, 1997; Dyer, 2010).
Our goal is to infer the most probable SCFG
derivations that explain a corpus of parallel sen-
tences, given a nonparametric prior over probabilis-
tic SCFGs. In this work we will consider grammars
with a single nonterminal category X.
3.2 Pitman-Yor Process SCFG
Before training we have no way of knowing how
many rules will be needed in our grammar to ade-
quately represent the data. By using the Pitman-
Yor process as a prior on the parameters of a syn-
chronous grammar we can formulate a model which
prefers smaller numbers of rules that are reused
often, thereby avoiding degenerate grammars con-
sisting of large, overly specific rules. However, as
the data being fit grows, the model can become more
complex. The PYP is parameterised by a discount
parameter d, a strength parameter ?, and the base
distribution G0, which gives the prior probability
of an event (in our case, events are rules) before
any observations have occurred. The discount is
subtracted from each positive rule count and damp-
ens the rich get richer effect where frequent rules
are given higher probability compared to infrequent
ones. The strength parameter controls the variance,
or concentration, about the base distribution.
In our model, a draw from a PYP is a distribution
over SCFG rules with a particular LHS (in fact, it is
a distribution over all well-formed rules). From this
distribution we can in turn draw individual rules:
GX ? PY(d, ?,G0),
X ? ?s, t, a? ? GX .
Although the PYP has no known analytical form,
we can marginalise out the GX ?s and reason about
Step 1: Generate source side length.
Step 2: Generate source side configuration of 
terminals (and non-terminal placeholders).
Step 3: Generate target length.
Step 4. Generate target side configuration of 
terminals (and non-terminal placeholders).
Step 5. Generate the words.
X < _ _ _  ||| ? >
X < X1 _ X2 ||| ? >
X < X1 _ X2 ||| _ _ _  >
X < X1 _ X2 ||| _ X1 X2  >
X < X1 ? X2 ||| you X1 X2  >
Figure 1: Example generation of a synchronous
grammar rule in our G0.
individual rules directly using the process described
by Teh (2006). In this process, at time n a rule rn
is generated by stochastically deciding whether to
make another copy of a previously generated rule
or to draw a new one from the base distribution, G0.
Let ? = (?1, ?2, . . .) be the sequence of draws from
G0; thus |?| is the total number of draws from G0. A
rule rn corresponds to a selection of a ?k. Let ck
be a counter indicating the number of times ?k has
been selected. In particular, we set rn to ?k with
probability
ck ? d
? + n ,
and increment ck, or with probability
? + d ? |?|
? + n ,
we draw a new rule from G0, append it to ?, and use
it for rn.
3.3 Base Distribution
The base distribution G0 for the PYP assigns prob-
ability to a rule based our belief about what consti-
tutes a good rule independent of observing any of
225
the data. We describe a novel generative process for
all rules X ? ?s, t, a? that encodes these beliefs.
We describe the generative process generally here
in text, and readers may refer to the example in Fig-
ure 1. The process begins by generating the source
length (total number of terminal and nonterminal
symbols, written |s|) by drawing from a Poisson dis-
tribution with mean 1:
|s| ? Poisson(1) .
This assigns high probability to shorter rules,
but arbitrarily long rules are possible with a low
probability. Then, for every position in s, we decide
whether it will contain a terminal or nonterminal
symbol by repeated, independent draws from a
Bernoulli distribution. Since we believe that shorter
rules should be relatively more likely to contain
terminal symbols than longer rules, we define the
probability of a terminal symbol to be ?|s| where
0 < ? < 1 is a hyperparameter.
si ? Bernoulli(?|s|) ? i ? [1, |s|] .
We next generate the length of the target side of
the rule. Let #NT(s) denote the number of nonter-
minal symbols we generated in s, i.e., the arity of
the rule. Our intuition here is that source and target
lengths should be similar. However, to ensure that
the rule is well-formed, t must contain exactly as
many nonterminal symbols as the source does. We
therefore draw the number of target terminal sym-
bols from a Poisson whose mean is the number of
terminal symbols in the source, plus a small constant
?0 to ensure that it is greater than zero:
|t| ? #NT(s) ? Poisson (|s| ? #NT(s) + ?0) .
We then determine whether each position in t is
a terminal or nonterminal symbol by drawing uni-
formly from the bag of #NT(s) source nontermi-
nals and |t| ? #NT(s) terminal indicators, with-
out replacement. At this point we have created a
rule template which indicates how large the rule is,
whether each position contains a terminal or non-
terminal symbol, and the reordering of the source
nonterminals a. To conclude the process we must
select the terminal types from the source and target
vocabularies. To do so, we use the following distri-
bution:
Pterminals(s, t) =
PM1?(s, t) + PM1?(s, t)
2
where PM1?(s, t) (PM1?(s, t)) first generates the
source (target) terminals from uniform draws from
the vocabulary, then generates the string in the other
language according to IBM MODEL 1, marginaliz-
ing over the alignments (Brown et al1993).
4 Gibbs Sampler
In this section we introduce a Gibbs sampler that
enables us to perform posterior inference given a
corpus of sentence pairs. Our innovation is to repre-
sent the synchronous derivation of a sentence pair in
a hierarchical 4-dimensional binary alignment grid,
with elements z[s,t,u,v] ? {0, 1}.
The settings of the grid variables completely
determine the SCFG rules in the current derivation.
A setting of a binary variable z[s,t,u,v] = 1 represents
a constituent linking the source span [s, t] and the
target span [u, v] in the current derivation; variables
with a value of 0 indicate no link between spans
[s, t] and [u, v].2 This relationship from our grid
representation is illustrated in Figure 2a.
Our Gibbs sampler operates over the space of all
the random variables z[s,t,u,v], resampling one at a
time. Changes to a single variable imply that at most
two additional rules must be generated, as illustrated
in Figure 2b. The probability of choosing a binary
setting of 0 or 1 for a variable is proportional to the
probability of generating the two derivations under
the model described in the previous section. Note
that for a given sentence, most of the bispan vari-
ables must be set to 0 otherwise they would violate
the strict nesting constraint required for valid SCFG
derivations. We discuss below how to exploit this
fact to limit the number of binary variables that must
be resampled for each sentence.
To be valid, a Gibbs sampler must be ergodic and
satisfy detailed balance. Ergodicity requires that
there is non-zero probability that any state in the
sampler be reachable from any other state. Clearly
2Our grid representation is the synchronous generalisation
of the well-known correspondence between CFG derivations
and Boolean matrices; see Lee (2002) for an overview.
226
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(a) An example grid representation of a syn-
chronous derivation. The SCFG rules (annotated
with their bispans) that correspond to this setting
of the grid are:
X[0,4,0,3] ?
? X[0,1,0,1] X[1,4,1,3] | X[0,1,0,1] X[1,4,1,3] ?
X[0,1,0,1] ? ? {mna | Amna ?
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(b) The toggle operator resamples a bispan vari-
able (here, z[1,3,2,3], shown in blue) to determine
whether it should be subtracted from the immedi-
ately dominating rule (bispan in red) and made into
a child rule in the derivation. This would require
the addition of the following two rules:
X[1,4,1,3] ? ? X[1,3,2,3] gy | will X[1,3,2,3]?
X[1,3,2,3] ? ? kAmyAb hw | succeed ?
Alternatively, the active bispan variable can be set
so it is not a constituent, which would require the
single rule:
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Figure 2: A single operation of the Gibbs sampler for a binary alignment grid.
our operator satisfies this since given any configu-
ration of the alignment grid we can use the toggle
operator to flatten the derivation to a single rule and
then break it back down to reach any derivation.
Detailed balance requires that the probability of
transitioning between two possible adjacent sampler
states respects their joint probabilities in the station-
ary distribution. One way to ensure this is to make
the order in which bispan variables are visited deter-
ministic and independent of the variables? current
settings. Then, the probability of the sampler tar-
geting any bispan in the grid is equal regardless of
the current configuration of the alignment grid.
A naive instantiation of this strategy is to visit all
|s|2|t|2 bispans in some order. However, since we
wish to be able to draw many samples, this is not
computationally feasible. A much more efficient
approach avoids resampling variables that would
result in violations without visiting each of them
individually. However, to ensure detailed balanced
is maintained, the order that we resample bispans
has to match the order we would sample them using
any exhaustive approach. We achieve this by always
checking a derivation top-down, from largest to
smallest bispan. Under this ordering, whether or not
a smaller bispan is visited will be independent of
how the larger ones were resampled. Furthermore,
the set of variables that may be resampled is fixed
given this ordering. Therefore, the probability of
sampling any possible bispan in the sentence pair is
still uniform (ensuring detailed balance), while our
sampler remains fast.
5 Evaluation
The preceding sections have introduced a model,
and accompanying inference technique, designed to
induce a posterior distribution over SCFG deriva-
tions containing discontiguous and phrasal transla-
tion rules. The evaluation that follows aims to deter-
mine our models ability to meet these design goals,
and to do so in a range of translation scenarios.
In order to validate both the model and the sam-
pler?s ability to learn an SCFG we first conduct a
synthetic experiment in which the true grammar is
227
known. Subsequently we conduct a series of experi-
ments on real parallel corpora of increasing sizes to
explore the empirical properties of our model.
5.1 Synthetic Data Experiments
Prior work on SCFG induction for SMT has val-
idated modeling claims by reporting BLEU scores
on real translation tasks. However, the combination
of noisy data and the complexity of SMT pipelines
conspire to obscure whether models actually achieve
their design goals, normally stated in terms of an
ability to induce SCFGs with particular properties.
Here we include a small synthetic data experiment
to clearly validate our models ability to learn an
SCFG that includes discontiguous and phrasal trans-
lation rules with non-monotonic word order.
Using the probabilistic SCFG shown in the top
half of Table 1 we stochastically generated three
thousand parallel sentence pairs as training data for
our model. We then ran the Gibbs sampler for fifty
iterations through the data.
The bottom half of Table 1 lists the five rules
with the highest marginal probability estimated by
the sampler. Encouragingly our model was able to
recover a grammar very close to the original. Even
for such a small grammar the space of derivations
is enormous and the task of recovering it from a
data sample is non-trivial. The divergence from the
true probabilities is due to the effect of the prior
assigning shorter rules higher probability. With a
larger data sample we would expect the influence of
the prior in the posterior to diminish.
5.2 Machine Translation Evaluation
Ultimately the efficacy of a model for SCFG induc-
tion will be judged on its ability to underpin a state-
of-the-art SMT system. Here we evaluate our model
by applying it to learning word alignments for par-
allel corpora from which SMT systems are induced.
We train models across a range of corpora sizes and
for language pairs that exhibit the type of complex
alignment phenomena that we are interested in mod-
eling: Chinese ? English (ZH-EN), Urdu ? English
(UR-EN) and German ? English (DE-EN).
Data and Baselines
The UR-EN corpus is the smallest of those used in
our experiments and is taken from the NIST 2009
GRAMMAR RULE TRUE PROBABILITY
X? ? X1 a X2 |X1 X2 1 ? 0.2
X? ? b c d | 3 2 ? 0.2
X? ? b d | 3 ? 0.2
X? ? d | 3 ? 0.2
X? ? c d | 3 1 ? 0.2
SAMPLED RULE SAMPLED PROBABILITY
X? ? d | 3 ? 0.25
X? ? b d | 3 ? 0.24
X? ? c d | 3 1 ? 0.24
X? ? b c d | 3 2 ? 0.211
X? ? X1 a X2 |X1 X2 1 ? 0.012
Table 1: Manually created SCFG used to generate
synthetic data, and the five most probable inferred
rules by our model.
ZH-EN
NIST
UR-EN
NIST
DE-EN
EUROPARL
TRAIN (SRC) 8.6M 1.2M 34M
TRAIN (TRG) 9.5M 1.0M 36M
DEV (SRC) 22K 18K 26K
DEV (TRG) 27K 16K 28K
Table 2: Corpora statistics (in words).
translation evaluation.3 The ZH-EN data is of a
medium scale and comes from the FBIS corpus.
The DE-EN pair constitutes the largest corpus and
is taken from Europarl, the proceedings of the Euro-
pean Parliament (Koehn, 2003). Statistics for the
data are shown in Table 2. We measure translation
quality via the BLEU score (Papineni et al2001).
All translation systems employ a Hiero
translation model during decoding. Baseline
word alignments were obtained by running
GIZA++ in both directions and symmetrizing using
the grow-diag-final-and heuristic (Och and
Ney, 2003; Koehn et al2003). Decoding was
performed with the cdec decoder (Dyer et al
2010) with the synchronous grammar extracted
using the techniques developed by Lopez (2008).
All translation systems include a 5-gram language
model built from a five hundred million token subset
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/
228
LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG
PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT.
UR-EN MT09 23.1 18.5 23.7 24.0
ZH-EN MT03-08 29.4 19.8 28.3 29.8
DE-EN EUROPARL 28.4 25.5 27.8 29.2
Table 3: Results for the SMT experiments in BLEU . The baseline is produced using a full GIZA++ run. The
MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.
The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations.
of all the English data made available for the NIST
2009 shared task (Graff, 2003).
Experimental Setup
To obtain the PYP-SCFG word alignments we
ran the sampler for five hundred iterations for each
of the language pairs and experimental conditions
described below. We used the approach of Newman
et al2007) to distribute the sampler across multi-
ple threads. The strength ? and discount d hyper-
parameters of the Pitman-Yor Processes, and the ter-
minal penalty ? (Section 3.3), were inferred using
slice sampling (Neal, 2000).
The Gibbs sampler requires an initial set of
derivations from which to commence sampling. In
our experiments we investigated both weak and
a strong initialisations, the former based on word
alignments from IBM Model 1 and the latter on
alignments from an HMM model (Vogel et al
1996). For decoding we used the word alignments
implied by the derivations in the final sample to
extract a Hiero grammar with the same standard set
of relative frequency, length, and language model
features used for the baseline.
Weak Initialisation
Our first translation experiments ascertain the
degree to which our proposed Gibbs sampling
inference algorithm is able to learn good
synchronous derivations for the PYP-SCFG model.
A number of prior works on alignment with Gibbs
samplers have only evaluated models initialised
with the more complex GIZA++ alignment models
(Blunsom et al2009; DeNero et al2008), as a
result it can be difficult to separate the performance
of the sampler from that of the initialisation.
In order to do this, we initialise the sampler
PYP-SCFG
LANGUAGE PAIR MODEL 1 INIT. HMM INIT.
UR-EN 1.93/2.08 1.45/1.58
ZH-EN 3.47/4.28 1.69/2.37
DE-EN 4.05/4.77 1.50/2.04
Table 4: Average source/target rule lengths in the
PYP-SCFG models after the 500th sample for the
different initialisations.
using just the MODEL 1 distribution used in the
PYP-SCFG model?s base distribution. We denote
this a weak initialisation as no alignment models
outside of those included in the PYP-SCFG model
influence the resulting word alignments. The
BLEU scores for translation systems built from the
five hundredth sample are show in the WEAK M1
INIT. column of Table 3. Additionally we build a
translation system from the MODEL 1 alignment
used to initialise the sampler without using using our
PYP-SCFG model or sampling. BLEU scores are
shown in the MODEL 1 INITIALISATION column
of Table 3. Firstly it is clear MODEL 1 is indeed a
weak initialiser as the resulting translation systems
achieve uniformly low BLEU scores. In contrast, the
models built from the output of the Gibbs sampler
for the PYP-SCFG model achieve BLEU scores
comparable to those of the MODEL 4 BASELINE.
Thus the sampler has moved a good distance from
its initialisation, and done so in a direction that
results in better synchronous derivations.
Strong Initialisation
Given we have established that the sampler can
produce state-of-the-art translation results from a
229
weak initialisation, it is instructive to investigate
whether initialising the model with a strong
alignment system, the GIZA++ HMM (Vogel et
al., 1996), leads to further improvements. Column
HMM INIT. of Table 3 shows the results for
initialising with the HMM word alignments and
sampling for 500 iterations. Starting with a stronger
initial sample results in both quicker mixing and
better translation quality for the same number of
sampling iterations.
Table 4 compares the average lengths of the rules
produced by the sampler with both the strong and
weak initialisers. As the size of the training corpora
increases (UR-EN ? ZH-EN ? DE-EN) we see that
the average size of the rules produced by the weakly
initialised sampler also increases, while that of the
strongly initialised model stays relatively uniform.
Initially both samplers start out with a large num-
ber of long rules and as the sampling progresses
the rules are broken down into smaller, more gen-
eralisable, pieces. As such we conclude from these
metrics that after five hundred samples the strongly
initialised model has converged to sampling from a
mode of the distribution while the weakly initialised
model converges more slowly and on the longer cor-
pora is still travelling towards a mode. This sug-
gests that longer sampling runs, and Gibbs operators
that make simultaneous updates to multiple parts
of a derivation, would enable the weakly initialised
model to obtain better translation results.
Grammar Analysis
The BLEU scores are informative as a measure of
translation quality but we also explored some of the
differences in the grammars obtained from the PYP-
SCFG model compared to the standard approach. In
Figures 3 and 4 we show some basic statistics of
the grammars our model produces. From Figure 3
we see that the number of unique rules in the PYP-
SCFG grammar decreases steadily as the sampler
iterates through the data, so the model is finding an
increasingly sparser distribution with fewer but bet-
ter quality rules as sampling progresses. Note that
the gradient of the curves appears to be a function of
the size of the corpus and suggests that the model
built from the large DE-EN corpus would benefit
from a longer sampling run. Figure 4 shows the dis-
tribution of rules with a given arity as a percentage
 140
 160
 180
 200
 220
 240
 260
 280
 300
 320
 340
 0  20  40  60  80  100
u
n
iq
ue
 g
ra
m
m
ar
 ru
le
s i
n 
PY
P
samples
ur-en (* 1k)
zh-en (* 3k)
de-en (* 10k)
Figure 3: Unique grammar rules for each language
pair as a function of the number of samples. The
number of rule types decreases monotonically as
sampling continues. Rule counts are displayed by
normalised corpus size (see Table 2).
X? ?? | end of ?
X? ??? | ninth ?*
X? ??? X | charter X ?
X? ??? | confidence in ?
X? ????? X | the chinese government X ?
X? ??? | are ?
X? ?????? X | beijing , X ?*
X? ????? | departments concerned ?
X? ??????? X | washington , X ?*
X? ???? X1? X2 , | he X1 X2 , ?*
Table 5: The five highest ZH-EN probability rules in
the Hiero grammar built from the PYP-SCFG that
are not in the baseline Hiero grammar (top), and the
top five rules in the baseline Hiero grammar that
are not in the PYP-SCFG grammar (bottom). An
* indicates a bad translation rule.
of the full grammar after the final sampling iteration.
The model prior biases the results to shorter rules as
the vast majority of the model probability mass is on
rules with zero, one or two nonterminals.
Tables 5 and 6 show the most probable rules in the
Hiero translation system obtained using the PYP-
SCFG alignments that are not present in the TM
from the GIZA++ alignments and visa versa. For
both language pairs, four of the top five rules in
230
X? ? yh | it is ?
X? ? zmyn | the earth ?
X? ? yhy X | the same X ?
X? ? X1 nhyN X2 gy | X2 not be X1 ?
X? ? X1 gY kh X2 | recommend that X2 X1 ?*
X? ? hwN gY | will ?
X? ? Gyr mlky | international ?*
X? ? X1 *rAye kY X2 | X2 to X1 sources ?*
X? ? nY X1 nhyN kyA X2 | did not X1 X2 ?*
X? ? xAtwn X1 ky X2 | woman X2 the X1?
Table 6: Five of the top scoring rules in the UR-EN
Hiero grammar from sampled PYP-SCFG align-
ments (top) versus the baseline UR-EN Hiero gram-
mar rules not in the sampled grammar (bottom). An
* indicates a bad translation rule.
 0
 0.1
 0.2
 0.3
 0.4
 0  1  2 3+
%
 o
f r
ul
es
arity
zh-en
ur-en
de-en
Figure 4: The percentage of rules with a given arity
in the final grammar of the PYP-SCFG model.
the PYP-SCFG grammar that are not in the heuris-
tically extracted grammar are correct and minimal
phrasal units of translation, whereas only two of the
top probability rules in the GIZA++ grammar are of
good translation quality.
6 Conclusion and Further Work
In this paper we have presented a nonparametric
Bayesian model for learning SCFGs directly
from parallel corpora. We have also introduced
a novel Gibbs sampller that allows for efficient
posterior inference. We show state-of-the-art
results and learn complex translation phenomena,
including discontiguous and many-to-many
phrasal alignments, without applying any heuristic
restrictions on the model to make learning tractable.
Our evaluation shows that we can use a principled
approach to induce SCFGs designed specifically
to utilize the full power of grammar based SMT
instead of relying on complex word alignment
heuristics with inherent bias.
Future work includes the obvious extension to
learning SCFGs that contain multiple nonterminals
instead of a single nonterminal grammar. We also
expect that expanding our sampler beyond strict
binary sampling may allow us to explore the space
of hierarchical word alignments more quickly
allowing for faster mixing. We expect with these
extensions our model of grammar induction may
further improve translation output.
Acknowledgements
This work was supported by a grant from Google,
Inc. and EPRSRC grant no. EP/I010858/1 (Leven-
berg and Blunsom), the U. S. Army Research Lab-
oratory and U. S. Army Research Office under con-
tract/grant no. W911NF-10-1-0533 (Dyer).
References
P. Blunsom and T. Cohn. 2011. A hierarchical pitman-
yor process hmm for unsupervised part of speech
induction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 865?874, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009.
A gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 782?790, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra,
and J. C. Lai. 1992. An estimate of an upper bound
for the entropy of english. Computational Linguistics,
18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
C. Cherry and D. Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
231
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April.
Association for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 314?323, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, ACLDemos ?10, pages 7?12.
C. Dyer. 2010. Two monolingual parses are better than
one (synchronous parse). In Proc. of NAACL.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In D. M. Susan Dumais
and S. Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 273?280, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, Syndney, Australia.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium (LDC-2003T05).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceedings
of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
L. Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1?15.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15:465?488, July.
A. Lopez. 2008. Machine Translation by Pattern Match-
ing. Ph.D. thesis, University of Maryland.
D. Marcu and D. Wong. 2002. A phrase-based,joint
probability model for statistical machine translation.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 133?
139. Association for Computational Linguistics, July.
R. Neal. 2000. Slice sampling. Annals of Statistics,
31:705?767.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. 2011. An unsupervised model for joint
phrase alignment and extraction. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 632?641, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet al
cation. In NIPS. MIT Press.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In ACL ?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Computational Linguistics.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Ann. Probab., 25:855?900.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of the 16th conference on Computational linguis-
tics, pages 836?841, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. Commu-
nications of the Association for Computing Machines,
54(2):91?98.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23:377?403, September.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97?105, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
232
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394?402,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Stream-based Translation Models for Statistical Machine Translation
Abby Levenberg
School of Informatics
University of Edinburgh
a.levenberg@ed.ac.uk
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
ccb@cs.jhu.edu
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
Typical statistical machine translation sys-
tems are trained with static parallel corpora.
Here we account for scenarios with a continu-
ous incoming stream of parallel training data.
Such scenarios include daily governmental
proceedings, sustained output from transla-
tion agencies, or crowd-sourced translations.
We show incorporating recent sentence pairs
from the stream improves performance com-
pared with a static baseline. Since frequent
batch retraining is computationally demand-
ing we introduce a fast incremental alternative
using an online version of the EM algorithm.
To bound our memory requirements we use
a novel data-structure and associated training
regime. When compared to frequent batch re-
training, our online time and space-bounded
model achieves the same performance with
significantly less computational overhead.
1 Introduction
There is more parallel training data available to-
day than there has ever been and it keeps increas-
ing. For example, the European Parliament1 releases
new parallel data in 22 languages on a regular basis.
Project Syndicate2 translates editorials into seven
languages (including Arabic, Chinese and Russian)
every day. Existing translation systems often get
?crowd-sourced? improvements such as the option
to contribute a better translation to GoogleTrans-
late3. In these and many other instances, the data can
be viewed as an incoming unbounded stream since
1http://www.europarl.europa.eu
2http://www.project-syndicate.org
3http://www.translate.google.com
the corpus grows continually with time. Dealing
with such unbounded streams of parallel sentences
presents two challenges: making retraining efficient
and operating within a bounded amount of space.
Statistical Machine Translation (SMT) systems
are typically batch trained, often taking many CPU-
days of computation when using large volumes of
training material. Incorporating new data into these
models forces us to retrain from scratch. Clearly,
this makes rapidly adding newly translated sen-
tences into our models a daunting engineering chal-
lenge. We introduce an adaptive training regime us-
ing an online variant of EM that is capable of in-
crementally adding new parallel sentences without
incurring the burdens of full retraining.
For situations with large volumes of incoming
parallel sentences we are also forced to consider
placing space-bounds on our SMT system. We in-
troduce a dynamic suffix array which allows us to
add and delete parallel sentences, thereby maintain-
ing bounded space despite processing a potentially
high-rate input stream of unbounded length.
Taken as a whole we show that online translation
models operating within bounded space can perform
as well as systems which are batch-based and have
no space constraints thereby making our approach
suitable for stream-based translation.
2 Stepwise Online EM
The EM algorithm is a common way of inducing
latent structure from unlabeled data in an unsuper-
vised manner (Dempster et al, 1977). Given a set
of unlabeled examples and an initial, often uniform
guess at a probability distribution over the latent
variables, the EM algorithm maximizes the marginal
394
log-likelihood of the examples by repeatedly com-
puting the expectation of the conditional probability
of the latent data with respect to the current distri-
bution, and then maximizing the expectations over
the observations into a new distribution used in the
next iteration. EM (and related variants such as vari-
ational or sampling approaches) form the basis of
how SMT systems learn their translation models.
2.1 Batch vs. Online EM
Computing an expectation for the conditional prob-
abilities requires collecting the sufficient statistics S
over the set of n unlabeled examples. In the case
of a multinomial distribution, S is comprised of the
counts over each conditional observation occurring
in the n examples. In traditional batch EM, we col-
lect the counts over the entire dataset of n unlabeled
training examples via the current ?best-guess? proba-
bility model ??t at iteration t (E-step) before normal-
izing the counts into probabilities ??(S) (M-step)4.
After each iteration all the counts in the sufficient
statistics vector S are cleared and the count collec-
tion begins anew using the new distribution ??t+1.
When we move to processing an incoming data
stream, however, the batch EM algorithm?s require-
ment that all data be available for each iteration be-
comes impractical since we do not have access to all
n examples at once. Instead we receive examples
from the input stream incrementally. For this reason
online EM algorithms have been developed to up-
date the probability model ?? incrementally without
needing to store and iterate through all the unlabeled
training data repeatedly.
Various online EM algorithms have been investi-
gated (see Liang and Klein (2009) for an overview)
but our focus is on the stepwise online EM (sOEM)
algorithm (Cappe and Moulines, 2009). Instead
of iterating over the full set of training examples,
sOEM stochastically approximates the batch E-step
and incorporates the information from the newly
available streaming observations in steps. Each step
is called a mini-batch and is comprised of one or
more new examples encountered in the stream.
Unlike in batch EM, in sOEM the expected counts
are retained between EM iterations and not cleared.
4As the M-step can be computed in closed form we desig-
nate it in this work as ??(S).
Algorithm 1: Batch EM for Word Alignments
Input: {F (source),E (target)} sentence-pairs
Output: MLE ??T over alignments a
??0 ?MLE initialization;
for iteration k = 0, . . . , T do
S ? 0; // reset counts
foreach (f, e) ? {F,E} do // E-step
S ? S +
?
a??a
Pr(f, a?|e; ??t);
end
??t+1 ? ??t(S) ; // M-step
end
That is, for each new example we interpolate its ex-
pected count with the existing set of sufficient statis-
tics. For each step we use a stepsize parameter ?
which mixes the information from the current ex-
ample with information gathered from all previous
examples. Over time the sOEM model probabilities
begin to stabilize and are guaranteed to converge to
a local maximum (Cappe and Moulines, 2009).
Note that the stepsize ? has a dependence on the
current mini-batch. As we observe more incoming
data the model?s current probability distribution is
closer to the true distribution so the new observa-
tions receive less weight. From Liang and Klein
(2009), if we set the stepsize as ?t = (t + 2)??,
with 0.5 < ? ? 1, we can guarantee convergence in
the limit as n ? ?. If we set ? low, ? weighs the
newly observed statistics heavily whereas if ? is low
new observations are down-weighted.
2.2 Batch EM for Word Alignments
Batch EM is used in statistical machine translation
to estimate word alignment probabilities between
parallel sentences. From these alignments, bilingual
rules or phrase pairs can be extracted. Given a set
of parallel sentence examples, {F,E}, with F the
set of source sentences and E the corresponding tar-
get sentences, we want to find the latent alignments
a for a sentence pair (f , e) ? {F,E} that defines
the most probable correspondence between words fj
and ei such that aj = i. We can induce these align-
ments using an HMM-based alignment model where
the probability of alignment aj is dependent only on
the previous alignment at aj?1 (Vogel et al, 1996).
395
We can write
Pr(f ,a | e) =
?
a??a
|f |
?
j=1
p(aj | aj?1, |e|) ? p(fj | eaj )
where we assume a first-order dependence on previ-
ously aligned positions.
To find the most likely parameter weights for
the translation and alignment probabilities for the
HMM-based alignments, we employ the EM algo-
rithm via dynamic programming. Since HMMs have
multiple local minima, we seed the HMM-based
model probabilities with a better than random guess
using IBM Model 1 (Brown et al, 1993) as is stan-
dard. IBM Model 1 is of the same form as the
HMM-based model except it uses a uniform distri-
bution instead of a first-order dependency. Although
a series of more complex models are defined, IBM
Models 2 to Model 6 (Brown et al, 1993; Och and
Ney, 2003), researchers typically find that extract-
ing phrase pairs or translation grammar rules using
Model 1 and the HMM-based alignments results in
equivalently high translation quality. Nevertheless,
there is nothing in our approach which limits us to
using just Model 1 and the HMM model.
A high-level overview of the standard, batch EM
algorithm applied to HMM-based word alignment
model is shown in Algorithm 1.
2.3 Stepwise EM for Word Alignments
Application of sOEM to HMM and Model 1 based
word aligning is straightforward. The process of
collecting the counts over the expected conditional
probabilities inside each iteration loop remains the
same as in the batch case. However, instead of clear-
ing the sufficient statistics between the iterations we
retain them and interpolate them with the batch of
counts gathered in the next iteration.
Algorithm 2 shows high level pseudocode of our
sOEM framework as applied to HMM-based word
alignments. Here we have an unbounded input
stream of source and target sentences {F,E} which
we do not have access to in its entirety at once.
Instead we observe mini-batches {M} comprised
of chronologically ordered strict subsets of the full
stream. To word align the sentences for each mini-
batch m ? M, we use the probability assigned by
the current model parameters and then interpolate
Algorithm 2: sOEM Algorithm for Word Align-
ments
Input: mini-batches of sentence pairs
{M : M ? {F (source), E(target)}}
Input: stepsize weight ?
Output: MLE ??T over alignments a
??0 ?MLE initialization;
S ? 0; k = 0;
foreach mini-batch {m : m ?M} do
for iteration t = 0, . . . , T do
foreach (f, e) ? {m} do // E-step
s??
?
a??a
Pr(f, a?|e; ??t);
end
? = (k + 2)??; k = k + 1; // stepsize
S ? ?s? + (1? ?)S; // interpolate
??t+1 ? ??t(S) ; // M-step
end
end
the newest sufficient statistics s? with our full count
vector S using an interpolation parameter ?. The in-
terpolation parameter ? has a dependency on how
far along the input stream we are processing.
3 Dynamic Suffix Arrays
So far we have shown how to incrementally retrain
translation models. We now consider how we might
bound the space we use for them when processing
(potentially) unbounded streams of parallel data.
Suffix arrays are space-efficient data structures for
fast searching over large text strings (Manber and
Myers, 1990). Treating the entire corpus as a sin-
gle string, a suffix array holds in lexicographical or-
der (only) the starting index of each suffix of the
string. After construction, since the corpus is now
ordered, we can query the suffix array quickly us-
ing binary search to efficiently find all occurrences
of a particular token or sequence of tokens. Then we
can easily compute, on-the-fly, the statistics required
such as translation probabilities for a given source
phrase. Suffix arrays can also be compressed, which
make them highly attractive structures for represent-
ing massive translation models (Callison-Burch et
al., 2005; Lopez, 2008).
We need to delete items if we wish to maintain
396
                                                                                         epoch 2           
epoch 1 epoch 2 model coverage
model coverage
input stream
Test Points
input stream
Test Points
Static 
Unbounded
input stream
Test Points
Bounded
model coverage
sliding windows
Figure 1: Streaming coverage conditions. In traditional
batch based modeling the coverage of a trained model
never changes. Unbounded coverage operates without
any memory constraints so the model is able to contin-
ually add data from the input stream. Bounded coverage
uses just a fixed window.
constant space when processing unbounded streams.
Standard suffix arrays are static, store a fixed corpus
and do not support deletions. Nevertheless, a dy-
namic variant of the suffix array does support dele-
tions as well as insertions and therefore can be used
in our stream-based approach (Salson et al, 2009).
Using a dynamic suffix array, we can compactly
represent the set of parallel sentences from which
we eventually extract grammar rules. Furthermore,
when incorporating new parallel sentences, we sim-
ply insert them into the array and, to maintain con-
stant space usage, we delete an equivalent number.
4 Experiments
In this section we describe the experiments con-
ducted comparing various batch trained translation
models (TMs) versus online incrementally retrained
TMs in a full SMT setting with different conditions
set on model coverage. We used publicly available
resources for all our tests. We start by showing that
recency motivates incremental retraining.
4.1 Effects of Recency on SMT
For language modeling, it is known that perfor-
mance can be improved using the criterion of re-
cency where training data is drawn from times
chronologically closer to the test data (Rosenfeld,
 0
 0.5
 1
 1.5
 2
 2.5
 5  10  15  20  25  30  35
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
Figure 2: Recency effects to SMT performance. De-
picted are the differences in BLEU scores for multiple
test points decoded by a static baseline system and a sys-
tem batched retrained on a fixed sized window prior to
the test point in question. The results are accentuated at
the end of the timeline when more time has passed con-
firming that recent data impacts translation performance.
1995). Given an incoming stream of parallel text,
we gauged the extent to which incorporating recent
data into a TM affects translation quality.
We used the Europarl corpus5 with the Fr-En lan-
guage pair using French as source and English as tar-
get. Europarl is released in the format of a daily par-
liamentary session per time-stamped file. The actual
dates of the full corpus are interspersed unevenly
(they do not convene daily) over a continuous time-
line corresponding to the parliament sessions from
April,1996 through October, 2006, but for concep-
tual simplicity we treated the corpus as a continual
input stream over consecutive days.
As a baseline we aligned the first 500k sentence
pairs from the beginning of the corpus timeline. We
extracted a grammar for and translated 36 held out
test documents that were evenly spaced along the re-
mainder of the Europarl timeline. These test docu-
ments effectively divided the remaining training data
into epochs and we used a sliding window over the
timeline to build 36 distinct, overlapping training
sets of 500k sentences each.
We then translated all 36 test points again using
a new grammar for each document extracted from
only the sentences contained in the epoch that was
before it. To explicitly test the effect of recency
5Available at http://www.statmt.org/europarl
397
on the TM all other factors of the SMT pipeline re-
mained constant including the language model and
the feature weights. Hence, the only change from
the static baseline to the epochs performance was the
TM data which was based on recency. Note that at
this stage we did not use any incremental retraining.
Results are shown in Figure 2 as the differences
in BLEU score (Papineni et al, 2001) between the
baseline TM versus the translation models trained
on material chronologically closer to the given test
point. The consistently positive deltas in BLEU
scores between the model that is never retrained and
the models that are retrained show that we achieve a
higher translation performance when using more up-
to-date TMs that incorporate recent sentence pairs.
As the chronological distance between the initial,
static model and the retrained models increases, we
see ever-increasing differences in translation perfor-
mance. This underlines the need to retrain transla-
tion models with timely material.
4.2 Unbounded and Bounded Translation
Model Retraining
Here we consider how to process a stream along two
main axes: by bounding time (batch versus incre-
mental retraining) and by bounding space (either us-
ing all the stream seen so far, or only using a fixed
sized sample of it).
To ensure the recency results reported above were
not limited to French-English, this time our paral-
lel input stream was generated from the German-
English language pair of Europarl with German as
source and English again as target. For testing we
held out a total of 22k sentences from 10 evenly
spaced intervals in the input stream which divided
the input stream into 10 epochs. Stream statistics for
three example epochs are shown in Table 1. We held
out 4.5k sentence pairs as development data to opti-
mize the feature function weights using minimum
error rate training (Och, 2003) and these weights
were used by all models. We used Joshua (Li et
al., 2009), a syntax-based decoder with a suffix array
implementation, and rule induction via the standard
Hiero grammar extraction heuristics (Chiang, 2007)
for the TMs. Note that nothing hinges on whether
we used a syntax or a phrase-based system.
We used a 5-gram, Kneser-Ney smoothed lan-
guage model (LM) trained on the initial segment of
Ep From?To Sent Pairs Source/Target
00 04/1996?12/2000 600k 15.0M/16.0M
03 02/2002?09/2002 70k 1.9M/2.0M
06 10/2003?03/2004 60k 1.6M/1.7M
10 03/2006?09/2006 73k 1.9M/2.0M
Table 1: Date ranges, total sentence pairs, and source and
target word counts encountered in the input stream for
example epochs. Epoch 00 is baseline data that is also
used as a seed corpus for the online models.
the target side parallel data used in the first base-
line as described further in the next subsection. As
our initial experiments aim to isolate the effect of
changes to the TM on overall translation system per-
formance, our in-domain LM remains static for ev-
ery decoding run reported below until indicated.
We used the open-source toolkit GIZA++ (Och
and Ney, 2003) for all word alignments. For the
online adaptation experiments we modified Model
1 and the HMM model in GIZA++ to use the sOEM
algorithm. Batch baselines were aligned using the
standard version of GIZA++. We ran the batch and
incremental versions of Model 1 and HMM for the
same number of iterations each in both directions.
4.3 Time and Space Bounds
For both batch and sOEM we ran a number of ex-
periments listed below corresponding to the differ-
ent training scenarios diagrammed in Figure 1.
1. Static: We used the first half of the in-
put stream, approximately 600k sentences and
15/16 million source/target words, as parallel
training data. We then translated each of the 10
test sets using the static model. This is the tradi-
tional approach and the coverage of the model
never changes.
2. Unbounded Space: Batch or incremental re-
training with no memory constraint. For each
epoch in the stream, we retrained the TM us-
ing all the data from the beginning of the in-
put stream until just before the present with re-
spect to a given test point. As more time passes
our training data set grows so each batch run
of GIZA++ takes more time. Overall this is the
most computationally expensive approach.
398
Baseline Unbounded Bounded
Epoch Test Date Test Sent. Train Sent. Rules Train Sent. Rules Train Sent. Rules
03 09/23/2002 1.0k 580k 4.0M 800k 5.0M 580k 4.2M
06 03/29/2004 1.5k 580k 5.0M 1.0M 7.0M 580k 5.5M
10 09/26/2006 3.5k 580k 8.5M 1.3M 14.0M 580k 10.0M
Table 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.
Test and Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
Hiero grammar rules extracted for the corresponding test set.
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1  2  3  4  5  6  7  8  9  10
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
unbounded
bounded
Figure 3: Static vs. online TM performance. Gains in
translation performance measured by BLEU are achieved
when recent German-English sentence pairs are auto-
matically incorporated into the TM. Shown are relative
BLEU improvements for the online models against the
static baseline.
3. Bounded Space: Batch and incremental re-
training with an enforced memory constraint.
Here we batch or incrementally retrain using
a sliding window approach where the training
set size (the number of sentence pairs) remains
constant. In particular, we ensured that we
used the same number of sentences as the base-
line. Each batch run of GIZA++ takes approxi-
mately the same time.
The time for aligning in the sOEM model is unaf-
fected by the bounded/unbounded conditions since
we always only align the mini-batch of sentences
encountered in the last epoch. In contrast, for batch
EM we must realign all the sentences in our training
set from scratch to incorporate the new training data.
Similarly space usage for the batch training grows
with the training set size. For sOEM, in theory mem-
ory used is with respect to vocabulary size (which
grows slowly with the stream size) since we retain
count history for the entire stream. To make space
usage truly constant, we filter for just the needed
word pairs in the current epoch being aligned. This
effectively means that online EM is more mem-
ory efficient than the batch version. As our exper-
iments will show, the sufficient statistics kept be-
tween epochs by sOEM benefits performance com-
pared to the batch models which can only use infor-
mation present within the batch itself.
4.4 Incremental Retraining Procedure
Our incremental adaptation procedure was as fol-
lows: after the latest mini-batch of sentences had
been aligned using sOEM we added all newly
aligned sentence pairs to the dynamic suffix ar-
rays. For the experiments where our memory was
bounded, we also deleted an equal number of sen-
tences from the suffix arrays before extracting the
Hiero grammar for the next test point. For the un-
bounded coverage experiments we deleted nothing
prior to grammar extraction. Table 2 presents statis-
tics for the number of training sentence pairs and
grammar rules extracted for each coverage condition
for various test points.
4.5 Results
Figure 3 shows the results of the static baseline
against both the unbounded and bounded online EM
models. We can see that both the online models
outperform the static baseline. On average the un-
constrained model that contains more sentence pairs
for rule extraction slightly outperforms the bounded
condition which uses less data per epoch. However,
the static baseline and the bounded models both use
the same number of sentence-pairs for TM training.
We see there is a clear gain by incorporating recent
sentence-pairs made available by the stream.
399
Static Baseline Retrained (Unbounded) Retrained (Bounded)
Test Date Batch Batch Online Batch Online
09/23/2002 26.10 26.60 26.43 26.19 26.40
03/29/2004 27.40 28.33 28.42 28.06 28.38
09/26/2006 28.56 29.74 29.75 29.73 29.80
Table 3: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional
model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream
but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded
online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so
quickly adopt the new data (best results are italicized).
Table 3 gives results of the online models com-
pared to the batch retrained models. For presentation
clarity we show only a sample of the full set of ten
test points though all results follow the pattern that
using more aligned sentences to derive our gram-
mar set resulted in slightly better performance ver-
sus a restricted training set. However, for the same
coverage constraints not only do we achieve com-
parable performance to batch retrained models us-
ing the sOEM method of incremental adaptation, we
are able to align and adopt new data from the input
stream orders of magnitude quicker since we only
align the mini-batch of sentences collected from the
last epoch. In the bounded condition, not only do
we benefit from quicker adaptation, we also see that
sOEM models slightly outperform the batch based
models due to the online algorithm employing a
longer history of count-based evidence to draw on
when aligning new sentence pairs.
Figure 4 shows two example test sentences that
benefited from the online TM adaptation. Trans-
lations from the online model produce more and
longer matching phrases for both sentences (e.g.,
?creation of such a?, ?of the occupying forces?)
leading to more fluent output as well as the improve-
ments achieved in BLEU scores.
We experimented with a variety of interpolation
parameters (see Algorithm 2) but found no signifi-
cant difference between them (the biggest improve-
ment gained over all test points for all parameter set-
tings was less than 0.1% BLEU).
4.6 Increasing LM Coverage
A natural and interesting extension to the experi-
ments above is to use the target side of the incoming
stream to extend the LM coverage alongside the TM.
Test Date Static Unbounded Bounded
09/23/2002 26.46 27.11 26.96
03/29/2004 28.11 29.53 29.20
09/26/2006 29.53 30.94 30.88
Table 4: Unbounded LM coverage improvements. Shown
are the BLEU scores for each experimental conditional
when we allow the LM coverage to increase.
It is well known that more LM coverage (via larger
training data sets) is beneficial to SMT performance
(Brants et al, 2007) so we investigated whether re-
cency gains for the TM were additive with recency
gains afforded by a LM.
To test this we added all the target side data from
the beginning of the stream to the most recent epoch
into the LM training set before each test point. We
then batch retrained6 and used the new LM with
greater coverage for the next decoding run. Experi-
ments were for the static baseline and online models.
Results are reported in Table 4. We can see that
increasing LM coverage is complimentary to adapt-
ing the TM with recent data. Comparing Tables
3 and 4, for the bounded condition, adapting only
the TM achieved an absolute improvement of +1.24
BLEU over the static baseline for the final test point.
We get another absolute gain of +1.08 BLEU by al-
lowing the LM coverage to adapt as well. Using an
online, adaptive model gives a total gain of +2.32
BLEU over a static baseline that does not adapt.
6Although we batch retrain the LMs we could use an online
LM that incorporates new vocabulary from the input stream as
in Levenberg and Osborne (2009).
400
Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them.
Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles.
Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles.
Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen 
              Prinzipien mitzuwirken.
Static:  Our position is clear and we all know: we are against the war and the occupation of Iraq by the United States and the United    
             Kingdom, and we are calling for the immediate withdrawal of the besatzungsm?chte from this country.
Online: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the United
             Kingdom, and we demand the immediate withdrawal of the occupying forces from this country .
Reference: Our position is clear and well known: we are against the war and the US-British occupation in Iraq and we demand the
                   immediate withdrawal of the occupying forces from that country.
Source: Unser Standpunkt ist klar und allseits bekannt: Wir sind gegen den Krieg und die Besetzung des Irak durch die USA und das   
              Vereinigte K?nigreich, und wir verlangen den unverz?glichen Abzug der Besatzungsm?chte aus diesem Land.
Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent
sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
5 Related Work
5.1 Translation Model Domain Adaptation
Our work is related to domain adaptation for transla-
tion models. See, for example, Koehn and Schroeder
(2007) or Bertoldi and Federico (2009). Most tech-
niques center around using mixtures of translation
models. Once trained, these models generally never
change. They therefore fall under the batch training
regime. The focus of this work instead is on incre-
mental retraining and also on supporting bounded
memory consumption. Our experiments examine
updating model parameters in a single domain over
different periods in time. Naturally, we could also
use domain adaptation techniques to further improve
how we incorporate new samples.
5.2 Online EM for SMT
For stepwise online EM for SMT models, the only
prior work we are aware of is Liang and Klein
(2009), where variations of online EM were exper-
imented with on various NLP tasks including word
alignments. They showed application of sOEM can
produce quicker convergence compared to the batch
EM algorithm. However, the model presented does
not incorporate any unseen data, instead iterating
over a static data set multiple times using sOEM.
For Liang and Klein (2009) incremental retraining
is simply an alternative way to use a fixed training
set.
5.3 Streaming Language Models
Recent work in Levenberg and Osborne (2009) pre-
sented a streaming LM that was capable of adapt-
ing to an unbounded monolingual input stream in
constant space and time. The LM has the ability to
add or delete n-grams (and their counts) based on
feedback from the decoder after translation points.
The model was tested in an SMT setting and results
showed recent data benefited performance. How-
ever, adaptation was only to the LM and no tests
were conducted on the TM.
6 Conclusion and Future Work
We have presented an online EM approach for word
alignments. We have shown that, for a SMT system,
incorporating recent parallel data into a TM from an
input stream is beneficial to translation performance
compared to a traditional, static baseline.
Our strategy for populating the suffix array was
simply to use a first-in, first-out stack. For future
work we will investigate whether information pro-
vided by the incoming stream coupled with the feed-
back from the decoder allows for more sophisti-
cated adaptation strategies that reinforce useful word
alignments and delete bad or unused ones.
In the near future we also hope to test the online
EM setup in an application setting such as a com-
puter aided translation or crowdsourced generated
streams via Amazon?s Mechanical Turk.
401
Acknowledgements
Research supported by EuroMatrixPlus funded by
the European Commission, by the DARPA GALE
program under Contract Nos. HR0011-06-2-0001
and HR0011-06-C-0022, and the NSF under grant
IIS-0713448.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09: Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 182?189, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Olivier Cappe and Eric Moulines. 2009. Online EM al-
gorithm for latent data models. Journal Of The Royal
Statistical Society Series B, 71:593.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39:1?38.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In WMT09: Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, pages 135?139, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang and Dan Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Ronald Rosenfeld. 1995. Optimizing lexical and n-gram
coverage via judicious use of linguistic data. In In
Proc. European Conf. on Speech Technology, pages
1763?1766.
Mikae?l Salson, Thierry Lecroq, Martine Le?onard, and
Laurent Mouchard. 2009. Dynamic extended suffix
arrays. Journal of Discrete Algorithms, March.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841, Morristown,
NJ, USA. Association for Computational Linguistics.
402
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177?186,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Multiple-stream Language Models for Statistical Machine Translation
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
David Matthews
School of Informatics
University of Edinburgh
dave.matthews@ed.ac.uk
Abstract
We consider using online language models for
translating multiple streams which naturally
arise on the Web. After establishing that us-
ing just one stream can degrade translations
on different domains, we present a series of
simple approaches which tackle the problem
of maintaining translation performance on all
streams in small space. By exploiting the dif-
fering throughputs of each stream and how
the decoder translates prior test points from
each stream, we show how translation perfor-
mance can equal specialised, per-stream lan-
guage models, but do this in a single language
model using far less space. Our results hold
even when adding three billion tokens of addi-
tional text as a background language model.
1 Introduction
There is more natural language data available today
than there has ever been and the scale of its produc-
tion is increasing quickly. While this phenomenon
provides the Statistic Machine Translation (SMT)
community with a potentially extremely useful re-
source to learn from, it also brings with it nontrivial
computational challenges of scalability.
Text streams arise naturally on the Web where
millions of new documents are published each day in
many different languages. Examples in the stream-
ing domain include the thousands of multilingual
websites that continuously publish newswire stories,
the official proceedings of governments and other
bureaucratic organisations, as well as the millions
of ?bloggers? and host of users on social network
services such as Facebook and Twitter.
Recent work has shown good results using an in-
coming text stream as training data for either a static
or online language model (LM) in an SMT setting
(Goyal et al, 2009; Levenberg and Osborne, 2009).
A drawback of prior work is the oversimplified sce-
nario that all training and test data is drawn from the
same distribution using a single, in-domain stream.
In a real world scenario multiple incoming streams
are readily available and test sets from dissimilar do-
mains will be translated continuously. As we show,
using stream data from one domain to translate an-
other results in poor average performance for both
streams. However, combining streams naively to-
gether hurts performance further still.
In this paper we consider this problem of multiple
stream translation. Since monolingual data is very
abundant, we focus on the subtask of updating an on-
line LM using multiple incoming streams. The chal-
lenges in multiple stream translation include dealing
with domain differences, variable throughput rates
(the size of each stream per epoch), and the need
to maintain constant space. Importantly, we impose
the key requirement that our model match transla-
tion performance reached using the single stream ap-
proach on all test domains.
We accomplish this using the n-gram history of
prior translations plus subsampling to maintain a
constant bound on memory required for language
modelling throughout all stream adaptation. In par-
ticular, when considering two test streams, we are
able to improve performance on both streams from
an average (per stream) BLEU score of 39.71 and
37.09 using a single stream approach (Tables 2 and
3) to an average BLEU score of 41.28 and 42.73 us-
ing multiple streams within a single LM using equal
memory (Tables 6 and 7). We also show additive im-
177
provements using this approach when using a large
background LM consisting of over one billion n-
grams. To our knowledge our approach is the first
in the literature to deal with adapting an online LM
to multiple streams in small space.
2 Previous Work
2.1 Randomised LMs
Randomised techniques for LMs from Talbot and
Osborne (2007) and Talbot and Brants (2008) are
currently industry state-of-the-art for fitting very
large datasets into much smaller amounts of mem-
ory than lossless representations for the data. Instead
of representing the n-grams exactly, the randomised
representation exchanges a small, one-sided error of
false positives for massive space savings.
2.2 Stream-based LMs
An unbounded text stream is an input source of natu-
ral language documents that is received sequentially
and so has an implicit timeline attached. In Leven-
berg and Osborne (2009) a text stream was used to
initially train and subsequently adapt an online, ran-
domised LM (ORLM) with good results. However,
a weakness of Levenberg and Osborne (2009) is that
the experiments were all conducted over a single in-
put stream. It is an oversimplification to assume that
all test material for a SMT system will be from a sin-
gle domain. No work was done on the multi-stream
case where we have more than one incoming stream
from arbitrary domains.
2.3 Domain Adaptation for SMT
Within MT there has been a variety of approaches
dealing with domain adaptation (for example (Wu et
al., 2008; Koehn and Schroeder, 2007)). Our work
is related to domain adaptation but differs in that we
are not skewing the distribution of an out-of-domain
LM to accommodate some test data for which we
have little or no training data for. Rather, we have
varying amounts of training data from all the do-
mains via the incoming streams and the LM must
account for each domain appropriately. However,
known domain adaptation techniques are potentially
applicable to multi-stream translation as well.
3 Multiple Streams and their Properties
Any source that provides a continuous sequence
of natural language documents over time can be
thought of as an unbounded stream which is time-
stamped and access to it is given in strict chronolog-
ical order. The ubiquity of technology and the In-
ternet means there are many such text streams avail-
able already and their number is increasing quickly.
For SMT, multiple text streams provide a potentially
abundant source of new training data that may be
useful for combating model sparsity.
Of primary concern is building models whose
space complexity is independent of the size of the
incoming stream. Allowing unbounded memory to
handle unbounded streams is unsatisfactory. When
dealing with more than one stream we must also
consider how the properties of single streams inter-
act in a multiple stream setting.
Every text stream is associated with a particular
domain. For example, we may draw a stream from
a newswire source, a daily web crawl of new blogs,
or the output of a company or organisation. Obvi-
ously the distribution over the text contained in these
streams will be very different from each other. As
is well-known from the work on domain adaptation
throughout the SMT literature, using a model from
one domain to translate a test document from an-
other domain would likely produce poor results.
Each stream source will also have a different
rate of production, or throughput, which may vary
greatly between sources. Blog data may be received
in abundance but the newswire data may have a sig-
nificantly lower throughput. This means that the text
stream with higher throughput may dominate and
overwhelm the more nuanced translation options of
the stream with less data in the LM during decod-
ing. This is bad if we want to translate well for all
domains in small space using a single model.
4 Multi-Stream Retraining
In a stream-based translation setting we can expect
to translate test points from various domains on any
number of incoming streams. Our goal is a single
unified LM that obtains equal performance in less
space than when using a separate LM per stream.
The underlying LMs could be exact, but here we use
randomised versions based on the ORLM.
178
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
pumi
pumK
Naive Combination Approach
tLwmLroch tLwmLroch
Figure 1: In the naive approach all K streams are simply
combined into a single LM for each new epoch encoun-
tered.
Given an incoming number K of unbounded
streams over a potentially infinite timeline T , with
t ? T an epoch or windowed subset of the timeline,
the full set of n-grams in all K streams over all T
is denoted with S. By St we denote n-grams from
all K streams and Skt, k ? [1,K], as the n-grams
in the kth stream over epoch t. Since the streams
are unbounded, we do not have access to all the n-
grams in S at once. Instead we select n-grams from
each stream Skt ? S. We define the collection of
n-grams encoded in the LM at time t over all K
streams as Ct. Initially, at time t = 0 the LM is
composed of the n-grams in the stream so C0 = S0.
Since it is unsatisfactory to allow unbounded
memory usage for the model and more bits are
needed as we see more novel n-grams from the
streams, we enforce a memory constraint and use
an adaptation scheme to delete n-grams from the
LM Ct?1 before adding any new n-grams from the
streams to get the current n-gram set Ct. Below
we describe various approaches of updating the LM
with data from the streams.
4.1 Naive Combinations
Approach The first obvious approach for an online
LM using multiple input streams is to simply store
all the streams in one LM. That is, n-grams from
all the streams are only inserted into the LM once
and their stream specific counts are combined into a
single value in the composite LM.
Modelling the Stream In the naive case we retrain
the LM Ct in full at epoch t using all the new data
from the streams. We have simply
Ct =
K
?
k=1
Skt (1)
stream 1 LM 1
stream 1 LM 2
stream 1 LM 3
input stream 1
stream 2 LM 1
stream 2 LM 2
stream 2 LM 3
input stream 2
?
stream K LM 1
stream K LM 2
stream K LM 3
input stream K
Multiple LM Approach
new epoch new epoch
Figure 2: Each stream 1 . . . K gets its own stream-based
LM using the multiple LM approach.
where each of the K streams is combined into a sin-
gle model and the n-grams counts are merged lin-
early. Here we carry no n-grams over from the LM
Ct?1 from the previous epoch. The space needed is
the number of unique n-grams present in the com-
bined streams for each epoch.
Resulting LM To query the resulting LM Ct dur-
ing decoding with a test n-gram wni = (wi, . . . , wn)
we use a simple smoothing algorithm called Stupid
Backoff (Brants et al, 2007). This returns the
probability of an n-gram as
P (wi|wi?1i?n+1) :=
?
?
?
Ct(wii?n+1)
Ct(wi?1i?n+1)
if Ct(wii?n+1) > 0
?P (wi|wi?1i?n+2) otherwise
(2)
where Ct(.) denotes the frequency count returned by
the LM for an n-gram and ? is a backoff parameter.
The recursion ends once the unigram is reached in
which case the probability is P (wi) := wi/N where
N is the size of the current training corpus.
Each stream provides a distribution over the n-
grams contained in it and, for SMT, if a separate
LM was constructed for each domain it would most
likely cause the decoder to derive different 1-best
hypotheses than using a LM built from all the stream
data. Using the naive approach blurs the distribution
distinctions between streams and negates any stream
specific differences when the decoder produces a 1-
best hypothesis. It has been shown that doing lin-
ear combinations of this type produces poor perfor-
mance in theory (Mansour et al, 2008).
179
4.2 Weighted Interpolation
Approach An improved approach to using multi-
ple streams is to build a separate LM for each stream
and using a weighted combination of each during
decoding. Each stream is stored in isolation and we
interpolate the information contained within each
during decoding using a weighting on each stream.
Modelling the Stream Here we model the streams
by simply storing each stream at time t in its own
LM so Ckt = Skt for each stream Sk. Then the LM
after epoch t is
Ct = {C1t, . . . , CKt}.
We use more space here than all other approaches
since we must store each n-gram/count occurring in
each stream separately as well as the overhead in-
curred for each separate LM in memory.
Resulting LM During decoding, the probability of
a test n-gram wni is a weighted combination of all
the individual stream LMs. We can write
P (wni ) :=
K
?
k=1
fkPCkt(wni ) (3)
where we query each of the individual LMs Ckt to
get a score from each LM using Equation 2 and
combine them together using a weighting fk spe-
cific to each LM. Here we impose the restriction on
the weights that
?K
k=1 fk = 1. (We discuss specific
weight selections in the next section.)
By maintaining multiple stream specific LMs we
maintain the particular distribution of the individual
streams. This keeps the more nuanced translations
from the lower throughput streams available during
decoding without translations being dominated by a
stream with higher throughput. However using mul-
tiple distinct LMs is wasteful of memory.
4.3 Combining Models via History
Approach We want to combine the streams into
a single LM using less memory than when storing
each stream separately but still achieve at least as
good a translation for each test point. Naively com-
bining the streams removes stream specific transla-
tions but using the history of n-grams selected by the
decoder during the previous test point in the stream
was done in Levenberg and Osborne (2009) for the
single stream case with good results. This is appli-
cable to the multi-stream case as well.
Modelling the Stream For multiple streams and
epoch t > 0 we model the stream combination as
Ct = fT (Ct?1) ?
K
?
k=1
(Skt). (4)
where for each epoch a selected subset of the previ-
ous n-grams in the LM Ct?1 is merged with all the
newly arrived stream data to create the new LM set
Ct. The parameter fT denotes a function that filters
over the previous set of n-grams in the model. It
represents the specific adaptation scheme employed
and stays constant throughout the timeline T . In this
work we consider any n-grams queried by the de-
coder in the last test point as potentially useful to
the next point. Since all of the n-grams St in the
stream at time t are used the space required is of the
same order of complexity as the naive approach.
Resulting LM Since all the n-grams from the
streams are now encoded in a single LM Ct we can
query it using Equation 2 during decoding. The goal
of retraining using decoding history is to keep use-
ful n-grams in the current model so a better model
is obtained and performance for the next transla-
tion point is improved. Note that making use of the
history for hypothesis combination is theoretically
well-founded and is the same approach used here for
history based combination. (Mansour et al, 2008)
4.4 Subsampling
Approach The problem of multiple streams with
highly varying throughput rates can be seen as a type
of class imbalance problem in the machine learning
literature. Given a binary prediction problem with
two classes, for instance, the imbalance problem oc-
curs when the bulk of the examples in the training
data are instances of one class and only a much
smaller proportion of examples are available from
the other class. A frequently used approach to bal-
ancing the distribution for the statistical model is to
use random under sampling and select only a sub-
set of the dominant class examples during training
(Japkowicz and Stephen, 2002).
This approach is applicable to the multiple stream
translation problem with imbalanced throughput
rates between streams. Instead of storing the n-
grams from each stream separately, we can apply a
180
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
LM 2 + (subset of LM 1)
LM 3 + (subset of LM 2)
Naive ComebnatAvaetoprr eAch
new epoch new epoch
SMT Decoder
Figure 3: Using decoding history all the streams are com-
bined into a unified LM.
subsampling selection scheme directly to the incom-
ing streams to balance each stream?s contribution in
the final LM. Note that subsampling is also related
to weighting interpolation. Since all returned LM
scores are based on frequency counts of the n-grams
and their prefixes, taking a weighting on a full prob-
ability of an n-gram is akin to having fewer counts
of the n-grams in the LM to begin with.
Modelling the Stream To this end we use the
weighted function parameter fk from Equation 3 to
serve as the sampling probability rate for accepting
an n-gram from a given stream k. The sampling
rate serves to limit the amount of stream data from
a stream that ends up in the model. For K > 1 we
have
Ct = fT (Ct?1) ?
K
?
k=1
fk(Skt) (5)
where fk is the probability a particular n-gram from
stream Sk at epoch t will be included in Ct. The
adaptation function fT remains the same as in Equa-
tion 4. The space used in this approach is now de-
pendent on the rate fk used for each stream.
Resulting LM Again, since we obtain a single LM
from all the streams, we use Equation 2 to get the
probability of an n-gram during decoding.
The subsampling method is applicable to all of the
approaches discussed in this section. However, since
we are essentially limiting the amount of data that
we store in the final LM we can expect to take a per-
formance hit based on the rate of acceptance given
by the parameters fk. By using subsampling with
the history combination approach we obtain good
performance for all streams in small space.
Stream 1-grams 3-grams 5-grams
EP 19K 520K 760K
GW (xie) 120K 3M 5M
RCV1 630K 21M 42M
Table 1: Sample statistics of unique n-gram counts from
the streams from epoch 2 of our timeline. The throughput
rate varies a lot between streams.
5 Experiments
Here we report on our SMT experiments with multi-
ple streams for translation using the approaches out-
lined in the previous section.
5.1 Experimental Setup
The SMT setup we employ is standard and all re-
sources used are publicly available. We translate
from Spanish into English using phrase-based de-
coding with Moses (Koehn and Hoang, 2007) as our
decoder. Our parallel data came from Europarl.
We use three streams (all are timestamped):
RCV1 (Rose et al, 2002), Europarl (EP) (Koehn,
2003), and Gigaword (GW) (Graff et al, 2007). GW
is taken from six distinct newswire sources but in
our initial experiments we limit the incoming stream
from Gigaword to one of the sources (xie). GW and
RCV1 are both newswire domain streams with high
rates of incoming data whereas EP is a more nu-
anced, smaller throughput domain of spoken tran-
scripts taken from sessions of the European Parlia-
ment. The RCV1 corpus only spans one calender
year from October, 1996 through September, 1997
so we selected only data in this time frame from
the other two streams so our timeline consists of the
same full calendar year for all streams.
For this work we use the ORLM. The crux of the
ORLM is an online perfect hash function that pro-
vides the ability to insert and delete from the data
structure. Consequently the ORLM has the abil-
ity to adapt to an unbounded input stream whilst
maintaining both constant memory usage and error
rate. All the ORLMs were 5-gram models built with
training data from the streams discussed above and
used Stupid Backoff smoothing for n-gram scoring
(Brants et al, 2007). All results are reported using
the BLEU metric (Papineni et al, 2001).
For testing we held-out three random test points
181
LM Type Test 1 Test 2 Test 3
RCV1 (Static) 39.30 38.28 33.06
RCV1 (Online) 39.30 40.64 39.19
EP (Online) 30.22 30.31 26.66
RCV1+EP (Online) 39.00 40.15 39.46
RCV1+EP+GW (Online) 41.29 41.73 40.41
Table 2: Results for the RCV1 test points. RCV1 and GW
streams are in-domain and EP is out-of-domain. Transla-
tion results are improved using more stream data since
most n-grams are in-domain to the test points.
from both the RCV1 and EP stream?s timeline for
a total of six test points. This divided the streams
into three epochs, and we updated the online LM
using the data encountered in the epoch prior to each
translation point. The n-grams and their counts from
the streams are combined in the LM using one of the
approaches from the previous section.
Using the notation from Section 4 we have the
RCV1, EP, and GW streams described above and
K = 3 as the number of incoming streams from two
distinct domains (newswire and spoken dialogue).
Our timeline T is one year?s worth of data split into
three epochs, t ? {1, 2, 3}, with test points at the
end of each epoch t. Since we have no test points
from the GW stream it acts as a background stream
for these experiments. 1
5.2 Baselines and Naive Combinations
In this section we report on our translation exper-
iments using a single stream and the naive linear
combination approach with multiple incoming data
streams from Section 4.1.
Using the RCV1 corpus as our input stream we
tested single stream translation first. Here we are
replicating the experiments from Levenberg and Os-
borne (2009) so both training and test data comes
from a single in-domain stream. Results are in Table
2 where each row represents a different LM type.
RCV1 (Static) is the traditional baseline with no
adaptation where we use the training data for the first
epoch of the stream. RCV1 (Online) is the online
LM adapted with data from the in-domain stream.
Confirming the previous work we get improvements
1A background stream is one that only serves as training
data for all other test domains.
LM Type Test 1 Test 2 Test 3
EP (Static) 42.09 44.15 36.42
EP (Online) 42.09 45.94 37.22
RCV1 (Online) 36.46 42.10 32.73
EP+RCV1 (Online) 40.82 44.07 35.01
EP+RCV1+GW (Online) 40.91 44.05 35.56
Table 3: EP results using in and out-of-domain streams.
The last two rows show that naive combination gets poor
results compared to single stream approaches.
when using an online LM that incorporates recent
data against a static baseline.
We then ran the same experiments using a stream
generated from the EP corpus. EP consists of the
proceedings of the European Parliament and is a sig-
nificantly different domain than the RCV1 newswire
stream. We updated the online LM using n-grams
from the latest stream epoch before translating each
in-domain EP test set. Results are in Table 3 and fol-
low the same naming convention as Table 2 (except
now in-domain is EP and out-of-domain is RCV1).
Using a single stream we also cross tested and
translated each test point using the online LM
adapted on the out-of-domain stream. As expected,
translation performance decreases (sometimes dras-
tically) in this case since the data of the out-of-
domain stream are not suited to the domain of the
current test point being translated.
We then tested the naive approach and combined
both streams into a single LM by taking the union of
the n-grams and adding their counts together. This
is the RCV1+EP (Online) row in Tables 2 and 3 and
clearly, though it contains more data compared to
each single stream LM, the naively combined LM
does not help the RCV1 test points much and de-
grades the performance of the EP translation results.
This translation hit occurs as the throughput of each
stream is significantly different. The EP stream con-
tains far less data per epoch than the RCV1 counter-
part (see Table 1) hence using a naive combination
means that the more abundant newswire data from
the RCV1 stream overrides the probabilities of the
more domain specific EP n-grams during decoding.
When we added a third newswire stream from a
portion of GW, shown in the last row of Tables 2
and 3, improvements are obtained for the RCV1 test
182
Weighting Test 1 Test 2 Test 3
.33R + .33E + .33G 38.97 39.78 35.66
.50R + .25E + .25G 39.59 40.40 37.22
.25R + .50E + .25G 36.57 38.03 34.23
.70R + 0.0E + .30G 40.54 41.46 39.23
Table 4: Weighted LM interpolation results for the RCV1
test points where E = Europarl, R = RCV1, and G =
Gigaword (xie).
points due to the addition of in-domain data but the
EP test performance still suffers.
This highlights why naive combination is unsat-
isfactory. While using more in-domain data aids
in the translation of the newswire tests, for the EP
test sets, naively combining the n-grams from all
streams means the hypotheses the decoder selects
are weighted heavily in favor of the out-of-domain
data. As the out-of-domain stream?s throughput is
significantly larger it swamps the model.
5.3 Interpolating Weighted Streams
Straightforward linear stream combination into a
single LM results in degradation of translations for
test points whose in-domain training data is drawn
from a stream with lower throughput than the other
data streams. We could maintain a separate MT sys-
tem for each streaming domain but intuitively some
combination of the streams may benefit average per-
formance since using all the data available should
benefit test points from streams with low through-
put. To test this we used an alternative approach de-
scribed in Section 4.2 and used a weighted combi-
nation of the single stream LMs during decoding.
We tested this approach using our three streams:
RCV1, EP and GW (xie). We used a separate
ORLM for each stream and then, during testing, the
result returned for an n-gram queried by the decoder
was a value obtained from some weighted interpola-
tion of each individual LM?s score for that n-gram.
To get the interpolation weights for each streaming
LM we minimised the perplexity of all the mod-
els on held-out development data from the streams.
2 Then we used the corresponding stream specific
2Due to the lossy nature of the encoding of the ORLM
means that the perplexity measures were approximations.
Nonetheless the weighting from this approach had the best per-
formance.
Weighting Test 1 Test 2 Test 3
.33E + .33R + .33G 40.75 45.65 35.77
.50E + .25R + .25G 41.46 46.37 36.94
.25E + .50R + .25G 40.57 44.90 35.77
.70E + .20R + .10G 42.47 46.83 38.08
Table 5: EP results in BLEU for the interpolated LMs.
weights to decode the test points from that domain.
Results are shown in Tables 4 and 5 using the
weighting scheme described above plus a selec-
tion of random parameter settings for comparison.
Using the notation from Section 4.2, a caption of
?.5R+ .25E+ .25G?, for example, denotes a weight-
ing of fRCV 1 = 0.5 for the scores returned from the
RCV1 stream LM while fEP and fGW = 0.25 for
the EP and GW stream LMs.
The weighted interpolation results suggest that
while naive combination of the streams may be mis-
guided, average translation performance can be im-
proved upon when using more than a single in-
domain stream. Comparing the best results in Tables
2 and 3 to the single stream baselines in Tables 4 and
5 we achieve comparable, if not improved, transla-
tion performance for both domains. This is espe-
cially true for test domains such as EP which have
low training data throughput from the stream. Here
adding some information from the out-of-domain
stream that contains a lot more data aids signifi-
cantly in the translation of in-domain test points.
However, the optimal weighting differs between
each test domain. For instance, the weighting that
gives the best results for the EP tests results in much
poorer translation performance for the RCV1 test
points requiring us to track which stream we are
decoding and then select the appropriate weighting.
This adds unnecessary complexity to the SMT sys-
tem. And, since we store each stream separately,
memory usage is not optimal using this scheme.
5.4 History and Subsampling
For space efficiency we want to represent multi-
ple streams non-redundantly instead of storing each
stream/domain in its own LM. Here we report on
experiments using both the history combination and
subsampling approaches from Sections 4.3 and 4.4.
Results are in Tables 6 and 7 for the RCV1 and
183
LM Type Test 1 Test 2 Test 3
Multi-fk 41.19 41.73 39.23
Multi-fT 41.29 42.23 40.51
Multi-fk + fT 41.19 42.52 40.12
Table 6: RCV1 test results using history and subsampling
approaches.
LM Type Test 1 Test 2 Test 3
Multi-fk 40.91 43.50 36.11
Multi-fT 40.91 47.84 39.29
Multi-fk + fT 40.91 48.05 39.23
Table 7: Europarl test results with history and subsam-
pling approaches.
EP test sets respectively with the column headers
denoting the test point. The row Multi-fk shows
results using only the random subsampling param-
eter fk and the rows Multi-fT show results with just
the time-based adaptation parameter without sub-
sampling. The final row Multi-fk + fT uses both
the f parameters with random subsampling as well
as taking decoding history into account.
Multi-fk uses the random subsampling parame-
ter fk to filter out higher order n-grams from the
streams. All n-grams that are sampled from the
streams are then combined into the joint LM. The
counts of n-grams sampled from more than one
stream are added together in the composite LM. The
parameter fk is set dependent on a stream?s through-
put rate, we only subsample from the streams with
high throughput, and the rate was chosen based on
the weighted interpolation results described previ-
ously. In Tables 6 and 7 the subsampling rate fk =
0.3 for the combined newswire streams RCV1 and
GW and we kept all of the EP data. We experi-
mented with various other values for the fk sampling
rates and found translation results only minorly im-
pacted. Note that the subsampling is truly random
so two adaptation runs with equal subsampling rates
may produce different final translations. Nonethe-
less, in our experiments we saw expected perfor-
mance, observing slight variation in performance for
all test points that correlated to the percentage of in-
domain data residing in the model.
The next row, Multi-fT , uses recency criteria to
keep potentially useful n-grams but uses no subsam-
pling and accepts all n-grams from all streams into
the LM. Here we get better results than naive combi-
nation or plain subsampling at the expense of more
memory for the same error rate for the ORLM.
The final row, Multi-fk + fT uses both the sub-
sampling function fk and fT so maintains a history
of the n-grams queried by the decoder for the prior
test points. This approach achieves significantly bet-
ter results than naive adaptation and compares to us-
ing all the data in the stream. Combining translation
history as well as doing random subsampling over
the stream means we match the performance of but
use far less memory than when using multiple online
LMs whilst maintaining the same error rate.
5.5 Experiments Summary
We have shown that using data from multiple
streams benefits SMT performance. Our best ap-
proach, using history based combination along with
subsampling, combines all incoming streams into a
single, succinct LM and obtains translation perfor-
mance equal to single stream, domain specific LMs
on all test domains. Crucially we do this in bounded
space, require less memory than storing each stream
separately, and do not incur translation degradations
on any single domain.
A note on memory usage. The multiple LM ap-
proach uses the most memory since this requires
all overlapping n-grams in the streams to be stored
separately. The naive and history combination ap-
proaches use less memory since they store all n-
grams from all the streams in a unified LM. For the
sampling the exact amount of memory is of course
dependent on the sampling rate used. For the results
in Tables 6 and 7 we used significantly less memory
(300MB) but still achieved comparable performance
to approaches that used more memory by storing the
full streams (600MB).
6 Scaling Up
The experiments described in the preceding section
used combinations of relatively small (compared to
current industry standards) input streams. The ques-
tion remains if using such approaches aids in the per-
formance of translation if used in conjunction with
large static LMs trained on large corpora. In this
section we describe scaling up the previous stream-
184
Order Count
1-grams 3.7M
2-grams 46.6M
3-grams 195.5M
4-grams 366.8M
5-grams 454.2M
Total 1067M
Table 8: Singleton-pruned n-gram counts (in millions)
for the GW3 background LM.
LM Type Test 1 Test 2 Test 3
GW (static) 41.69 42.40 35.48
+ RCV1 (online) 42.44 43.83 40.55
+ EP (online) 42.80 43.94 38.82
Table 9: Test results for the RCV1 stream using the large
background LM. Using stream data benefits translation.
based translation experiments using a large back-
ground LM trained on a billion n-grams.
We used the same setup described in Section 5.1.
However, instead of using only a subset of the GW
corpus as one of our incoming streams, we trained
a static LM using the full GW3 corpus of over three
billion tokens and used it as a background LM. As
the n-gram statistics for this background LM show
in Table 8, it contains far more data than each of the
stream specific LMs (Table 1). We tested whether
using streams atop this large background LM had a
positive effect on translation for a given domain.
Baseline results for all test points using only the
GW background LM are shown in the top row in
Tables 9 and 10. We then interpolated the ORLMs
with this LM. For each stream test point we interpo-
lated with the big GW LM an online LM built with
the most recent epoch?s data. Here we used sepa-
rate models per stream so the RCV1 test points used
the GW LM along with a RCV1 specific ORLM. We
used the same mechanism to obtain the interpolation
weights as described in Section 5.3 and minimised
the perplexity of the static LM along with the stream
specific ORLM. Interestingly, the tuned weights re-
turned gave approximately a 50-50 weighting be-
tween LMs and we found that simply using a 50-50
weighting for all test points resulted had no negative
effect on BLEU. In the third row of the Tables 9 and
10 we show the results of interpolating the big back-
LM Type Test 1 Test 2 Test 3
GW (static) 40.78 44.26 34.36
+ EP (online) 43.94 47.82 38.71
+ RCV1 (online) 43.07 47.72 39.15
Table 10: EP test results using the background GW LM.
ground LM with ORLMs built using the approach
described in Section 4.4. In this case all streams
were combined into a single LM using a subsam-
pling rate for higher order n-grams. As before our
sampling rate for the newswire streams was 30%
chosen by the perplexity reduction weights.
The results show that even with a large amount
of static data adding small amounts of stream spe-
cific data relevant to a given test point has an im-
pact on translation quality. Compared to only us-
ing the large background model we obtain signifi-
cantly better results when using a streaming ORLM
to compliment it for all test domains. However the
large amount of data available to the decoder in
the background LM positively impacts translation
performance compared to single-stream approaches
(Tables 2 and 3). Further, when we combine the
streams into a single LM using the subsampling ap-
proach we get, on average, comparable scores for all
test points. Thus we see that the patterns for multi-
ple stream adaptation seen in previous sections hold
in spite of big amounts of static data.
7 Conclusions and Future Work
We have shown how multiple streams can be effi-
ciently incorporated into a translation system. Per-
formance need not degrade on any of the streams.
As well, these results can be additive. Even when
using large amounts of additional background data,
adding stream specific data continues to improve
translation. Further, we achieve all results in
bounded space. Future work includes investigating
more sophisticated adaptation for multiple streams.
We also plan to explore alternative ways of sampling
the stream when incorporating data.
Acknowledgements
Special thanks to Adam Lopez and Conrad Hughes
and Phil Blunosm for helpful discussion and advice.
This work was sponsored in part by the GALE pro-
185
gram, DARPA Contract No. HR0011-06-C-0022
and by ESPRC Grant No. EP/I010858/1bb.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In North American Chapter of the
Association for Computational Linguistics (NAACL),
Boulder, CO.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium (LDC-2007T07).
Nathalie Japkowicz and Shaju Stephen. 2002. The class
imbalance problem: A systematic study. Intell. Data
Anal., 6:429?449, October.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2003. Europarl: A multilingual corpus
for evaluation of machine translation. Available at:
http://www.statmt.org/europarl/.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Domain adaptation with multiple
sources. In NIPS, pages 1041?1048.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference on
Language Resources and Evaluation, pages 29?31.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 993?
1000. Coling 2008 Organizing Committee, August.
186
