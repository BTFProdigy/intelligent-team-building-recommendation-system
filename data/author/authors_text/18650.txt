2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742?751,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Mind the Gap: Learning to Choose Gaps for Question Generation 
Lee Becker 
Department of Computer Science 
University of Colorado Boulder 
Boulder, CO 80309 
lee.becker@colorado.edu 
 
 
Sumit Basu and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{sumitb,lucyv}@microsoft.com 
 
 
Abstract 
Not all learning takes place in an educational 
setting: more and more self-motivated learners 
are turning to on-line text to learn about new 
topics. Our goal is to provide such learners 
with the well-known benefits of testing by au-
tomatically generating quiz questions for on-
line text. Prior work on question generation 
has focused on the grammaticality of generat-
ed questions and generating effective multi-
ple-choice distractors for individual question 
targets, both key parts of this problem. Our 
work focuses on the complementary aspect of 
determining what part of a sentence we should 
be asking about in the first place; we call this 
?gap selection.? We address this problem by 
asking human judges about the quality of 
questions generated from a Wikipedia-based 
corpus, and then training a model to effective-
ly replicate these judgments. Our data shows 
that good gaps are of variable length and span 
all semantic roles, i.e., nouns as well as verbs, 
and that a majority of good questions do not 
focus on named entities. Our resulting system 
can generate fill-in-the-blank (cloze) ques-
tions from generic source materials. 
1 Introduction 
Assessment is a fundamental part of teaching, both 
to measure a student?s mastery of the material and 
to identify areas where she may need reinforce-
ment or additional instruction. Assessment has also 
been shown an important part of learning, as test-
ing assists retention and can be used to guide learn-
ing (Anderson & Biddle, 1975). Thus, as learners 
move on from an educational setting to unstruc-
tured self-learning settings, they would still benefit 
from having the means for assessment available. 
Even in traditional educational settings, there is a 
need for automated test generation, as teachers 
want multiple tests for topics to give to different 
students, and students want different tests with 
which to study and practice the material. 
One possible solution to providing quizzes for 
new source material is the automatic generation of 
questions. This is a task the NLP community has 
already embraced, and significant progress has 
been made in recent years with the introduction of 
a shared task (Rus et al, 2010). However, thus far 
the research community has focused on the prob-
lem of generating grammatical questions (as in 
Heilman and Smith (2010a)) or generating effec-
tive distractors for multiple-choice questions 
(Agarwal and Mannem, 2011). 
While both of these research threads are of crit-
ical importance, there is another key issue that 
must be addressed ? which questions should we be 
asking in the first place? We have highlighted this 
aspect of the problem in the past (see 
Vanderwende (2008)) and begin to address it in 
this work, postulating that we can both collect hu-
man judgments on what makes a good question 
and train a machine learning model that can repli-
cate these judgments. The resulting learned model 
can then be applied to new material for automated 
question generation. We see this effort as comple-
mentary to the earlier progress.    
In our approach, we factor the problem of gen-
erating good questions into two parts: first, the se-
lection of sentences to ask about, and second, the 
identification of which part of the resulting sen-
tences the question should address. Because we 
want to focus on these aspects of the problem and 
not the surface form of the questions, we have cho-
sen to generate simple gap-fill (cloze) questions, 
742
though our results can also be used to trigger Wh-
questions or multiple-choice questions by leverag-
ing prior work. For sentence selection, we turn to 
methods in summarization and use the simple but 
effective SumBasic (Nenkova et al, 2006) algo-
rithm to prioritize and choose important sentences 
from the article. We cast the second part, gap se-
lection, as a learning problem. To do this, we first 
select a corpus of sentences from a very general 
body of instructional material (a range of popular 
topics from Wikipedia). We then generate a con-
strained subset of all possible gaps via NLP heuris-
tics, and pair each gap with a broad variety of 
features pertaining to how it was generated. We 
then solicit a large number of human judgments via 
crowdsourcing to help us rate the quality of various 
gaps. With that data in hand, we train a machine 
learning model to replicate these judgments. The 
results are promising, with one possible operating 
point producing a true positive rate of 83% with a 
corresponding false positive rate of 19% (83% of 
the possible Good gaps are kept, and 19% of the 
not-Good gaps are incorrectly marked); see Figure 
6 for the full ROC curve and Section 4 for an ex-
planation of the labels. As the final model has only 
minimal dependence on Wikipedia-specific fea-
tures, we expect that it can be applied to an even 
wider variety of material (blogs, news articles, 
health sites, etc.).   
2 Background and Related Work 
There already exists a large body of work in auto-
matic question generation (QG) for educational 
purposes dating back to the Autoquest system 
(Wolfe, 1976), which used an entirely syntactic 
approach to generate Wh-Questions from individu-
al sentences. In addition to Autoquest, several oth-
ers have created systems for Wh-question 
generation using approaches including transfor-
mation rules (Mitkov and Ha, 2003), template-
based generation (Chen et al, 2009; Curto et al, 
2011), and overgenerate-and-rank (Heilman and 
Smith, 2010a). The work in this area has largely 
focused on the surface form of the questions, with 
an emphasis on grammaticality.   
Alternatively, generation of gap-fill style ques-
tions (a.k.a. cloze questions) avoids these issues of 
grammaticality by blanking out words or spans in a 
known good sentence. There is a large body of ex-
isting work that has focused on generation of this 
type of question, most of which has focused on 
vocabulary and language learning. The recent work 
of Agarwal and Mannem (2011) is closer to our 
purposes; they generated fill-in-the-blank questions 
and distractor answers for reading comprehension 
tests using heuristic scoring measures and a small 
evaluation set. Our work has similar aims but em-
ploys a data-driven approach. 
The Question-Generation Shared Task and 
Evaluation Challenge (QG-STEC) (Rus et al, 
2010) marks a first attempt at creating a common 
task and corpus for empirical evaluation of ques-
tion generation components. However, evaluation 
in this task was manual and the number of instanc-
es in both the development and training set were 
small. As there exists no other dataset for question 
generation, we created a new corpus using Amazon 
Mechanical Turk by soliciting judgments from 
non-experts. Snow et al (2008) have validated 
AMT as a valid data source by comparing non-
expert with gold-standard expert judgments. Cor-
pus creation using AMT has numerous precedents 
now; see i.e. Callison-Burch and Dredze (2010) 
and Heilman and Smith (2010b). We have made 
our corpus (see Section 4) available online to ena-
ble others to continue research on the gap-selection 
problem we address here.  
3 Question Generation  
To achieve our goal of selecting better gap-fill 
questions, we have broken down the task into stag-
es similar to those proposed by Nielsen (2008): 1) 
sentence selection, 2) question construction, and 3) 
classification/scoring. Specifically, we utilize sum-
marization to identify key sentences from a pas-
sage. We then apply semantic and syntactic 
constraints to construct multiple candidate ques-
tion/answer pairs from a given source sentence.  
Lastly we extract features from these hypotheses 
for use with a question quality classification mod-
el. To train this final question-scoring component, 
we made use of crowdsourcing to collect ratings 
for a corpus of candidate questions. While this 
pipeline currently produces gap-fill questions, we 
envision these components can be used as input for 
more complex surface generation such as Wh- 
forms or distractor selection. 
 
 
743
3.1 Sentence Selection 
When learning about a new subject, a student will 
most likely want to learn about key concepts be-
fore moving onto more obscure details. As such, it 
is necessary to order target sentences in terms of 
their importance. This is fortunately very similar to 
the goals of automatic summarization, in which the 
selected sentences should be ordered by how cen-
tral they are to the article. 
As a result, we make use of our own implemen-
tation of SumBasic (Nenkova et al, 2006), a sim-
ple but competitive document summarization al-
gorithm motivated by the assumption that 
sentences containing the article?s most frequently 
occurring words are the most important. We thus 
use the SumBasic score for each sentence to order 
them as candidates for question construction.    
3.2 Question Construction 
We seek to empirically determine how to choose 
questions instead of relying on heuristics and rules 
for evaluating candidate surface forms. To do this, 
we cast question construction as a generate-and-
filter problem: we overgenerate potential ques-
tion/answer pairs from each sentence and train a 
discriminative classifier on human judgments of 
quality for those pairs. With the trained classifier, 
we can then apply this approach on unseen sen-
tences to return the highest-scoring ques-
tion/answer, all question/answer pairs scoring 
above a threshold, and so on. 
Generation 
Although it would be possible to select every word 
or phrase as a candidate gap, this tactic would pro-
duce a skewed dataset composed mostly of unusa-
ble questions, which would subsequently require 
much more annotation to discriminate good ques-
tions from bad ones. Instead we rely on syntactic 
and semantic constraints to reduce the number of 
questions that need annotation. 
To generate questions we first run the source 
sentence through a constituency parser and a se-
mantic role labeler (components of a state-of-the-
art natural language toolkit from (Quirk et al, 
2012)), with the rationale that important parts of 
the sentence will occur within a semantic role. 
Each verb predicate found within the roles then 
automatically becomes a candidate gap. From eve-
ry argument to the predicate, we extract all child 
noun phrases (NP) and adjectival phrases (ADJP) 
as candidate gaps as well. Figure 1 illustrates this 
generation process.  
Classification 
To train the classifier for question quality, we ag-
gregated per-question ratings into a single label 
(see Section 4 for details). Questions with an aver-
age rating of 0.67 or greater were considered as 
positive examples. This outcome was then paired 
with a vector of features (see Section 5) extracted 
from the source sentence and the generated ques-
tion. 
Because our goal is to score each candidate 
question in a meaningful way, we use a calibrated 
learner, namely L2-regularized logistic regression 
(Bishop 2006). This model?s output is 
(|	
	); in our case this is the posteri-
or probability of a candidate receiving a positive 
label based on its features. 
4 Corpus Construction 
We downloaded 105 articles from Wikipedia?s 
listing of vital articles/popular pages.1 These arti-
cles represent a cross section of historical, social, 
                                                        
1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular
_pages 
In 1874 R?ntgen              a lecturer at the University of Strassburg. 
In          R?ntgen became a lecturer at the University of Strassburg. 
In 1874               became a lecturer at the University of Strassburg. 
In 1874 R?ntgen became a              at the University of Strassburg. 
In 1874 R?ntgen became a lecturer at                         of Strassburg. 
In 1874 R?ntgen became a lecturer at the University of                  . 
In 1874 R?ntgen became a lecturer at                                              . 
Figure 1 An example of the question generation process. 
744
and scientific topics. From each article we sampled 
10 sentences using the sentence selection algorithm 
described in Section 3.1 for 50% of the sentences; 
the other 50% were chosen at random to prevent 
any possible overfitting to the selection method. 
These sentences were then processed using the 
candidate generation method from Section 3.2. 
To collect training data outcomes for our ques-
tion classifier, we used Amazon?s Mechanical 
Turk (AMT) service to obtain human judgments of 
quality for each question. We initially considered 
asking about the quality of individual ques-
tion/answer pairs in isolation, but in pilot studies 
we found poor agreement in this case; we noticed 
that the inability to compare with other possible 
questions actually made the task seem difficult and 
arbitrary. We thus instead presented raters with a 
source sentence and a list of up to ten candidate 
questions along with their corresponding answers 
(see Figure 2). Raters were asked to rate questions? 
quality as Good, Okay, or Bad. The task instruc-
tions defined a Good question as ?one that tests 
key concepts from the sentence and would be rea-
sonable to answer.? An Okay question was defined 
as ?one that tests key concepts but might be diffi-
cult to answer (the answer is too lengthy, the an-
swer is ambiguous, etc.).? A Bad question was 
?one that asks about an unimportant aspect of the 
sentence or has an uninteresting answer that can be 
figured out from the context of the sentence.? The-
se ratings were binarized into a score of one for 
Good and zero for not-Good (Okay or Bad), as our 
goal was to find the probability of a question being 
truly Good (and not just Okay).2  
                                                        
2
 Heilman and Smith (2010a and b) asked raters to identify 
question deficiencies, including vague or obvious, but raters 
were not asked to differentiate between Good and Okay. Thus 
questions considered Good in their study would include Okay. 
Thus far we have run 300 HITs with 4 judges 
per HIT. Each HIT consisted of up to 10 candidate 
questions generated from a single sentence. In total 
this yielded 2252 candidate questions with 4 rat-
ings per question from 85 unique judges. We then 
wished to eliminate judges who were gaming the 
system or otherwise performing poorly on the task. 
It is common to do such filtering when using 
crowdsourced data by using the majority or median 
vote as the final judgment or to calibrate judges 
using expert judgments (Snow et al 2008). Other 
approaches to annotator quality control include 
using EM-based algorithms for estimating annota-
tor bias (Wiebe et al 1999, Ipeirotis et al 2010).  
In our case, we computed the distance for each 
judge from the median judgment (from all judges) 
on each question, then took the mean of this dis-
tance over all questions they rated. We removed 
judges with a mean distance two standard devia-
tions above the mean distance, which eliminated 
the five judges who disagreed most with others. 
In addition to filtering judges, we wanted to fur-
ther constrain the data to those questions on which 
the human annotators had reasonable agreement, as 
it would not make sense to attempt to train a model 
to replicate judgments on which the annotators 
themselves could not agree. To do this, we com-
puted the variance of the judgments for each ques-
tion. By limiting the variance to 0.3, we kept ques-
tions on which up to 1 judge (out of 4) disagreed; 
this eliminated 431 questions and retained the 1821 
with the highest agreement. Of these filtered ques-
tions, 700 were judged to be Good (38%). 
To formally assess inter-rater reliability we 
computed Krippendorff?s alpha (Krippendorff, 
2004), a statistical measure of agreement applica-
ble for situations with multiple raters and incom-
plete data (in our case not all raters provided 
ratings for all items). An alpha value of 1.0 indi-
cates perfect agreement, and an alpha value of 0.0 
Source Sentence:  
    The large scale production of chemicals was an important development during the Industrial Revolution. 
 
Question Answer Ratings 
The _ _ _ _ _ _ _ _ _ _ _ _  of chemicals was an important 
development during the Industrial Revolution. 
large scale production ? Good   ?  Okay   ? Bad 
The large scale production of _ _ _ _ _ _was an important 
development during the Industrial Revolution. 
chemicals ? Good   ?  Okay   ? Bad 
The large scale production of chemicals was an important  
development during the _ _ _ _ _ _ _ _ _ _ _ . 
Industrial Revolution ?  Good   ? Okay   ? Bad 
 Figure 2: Example question rating HIT 
745
indicates no agreement. Our original data yielded 
an alpha of 0.34, whereas after filtering judges and 
questions the alpha was 0.51. It should be noted 
that because Krippendorff?s Alpha accounts for 
variability due to multiple raters and sample size, 
its values tend to be more pessimistic than many 
Kappa values commonly used to measure inter-
rater reliability. 
For others interested in working with this data, 
we have made our corpus of questions and ratings 
available for download at the following location: 
http://research.microsoft.com/~sumitb/questiongen
eration.   
5 Model Features 
While intuition would suggest that selecting high-
quality gaps for cloze questions should be a 
straightforward task, analysis of our features im-
plies that identifying important knowledge depends 
on more complex interactions between syntax, se-
mantics, and other constraints. In designing fea-
tures, we focused on using commonly extracted 
NLP information to profile the answer (gap), the 
source sentence, and the relation between the two. 
To enable extraction of these features, we used 
the MSR Statistical Parsing and Linguistic Analy-
sis Toolkit (MSR SPLAT)3, a state-of-the-art, web-
based service for natural language processing 
(Quirk et al, 2012). Table 1 shows a breakdown of 
our feature categories and their relative proportion 
of the feature space. In the subsections below, we 
describe the intuitions behind our choice of fea-
tures and highlight example features from each of 
these categories. An exhaustive list of the features 
can be found at the corpus URL listed in Section 4. 
5.1 Token Count Features 
A good question gives the user sufficient context to 
answer correctly without making the answer obvi-
ous. At the same time, gaps with too many words 
may be impossible to answer. Figure 3 shows the 
distributions of number of tokens in the answer 
(i.e., the gap) for Good and not-Good questions. As 
intuition would predict, the not-Good class has 
higher likelihoods for the longer answer lengths. In 
addition to the number and percentage of tokens in 
the answer features, we also included other token 
                                                        
3
 http://research.microsoft.com/projects/msrsplat  
count features such as the number of tokens in the 
sentence, and the number of overlapping tokens 
between the answer and the remainder of the sen-
tence. 
 
Feature Category Number of Features 
Token Count 5 
Lexical 11 
Syntactic 112 
Semantic 40 
Named Entity 11 
Wikipedia link 3 
Total 182 
Table 1: Breakdown of features by category 
5.2 Lexical features 
Although lexical features play an important role in 
system performance for several NLP tasks like 
parsing, and semantic role labeling, they require a 
large number of examples to provide practical ben-
efit. Furthermore, because most sentences in Wik-
ipedia articles feature numerous domain-specific 
terms and names, we cannot rely on lexical fea-
tures to generalize across the variety of possible 
articles in our corpus. Instead we approximate lex-
icalization by computing densities of word catego-
ries found within the answer. The intuition behind 
these features is that an answer composed primari-
ly of pronouns and stopwords will make for a bad 
question while an answer consisting of specific 
entities may make for a better question. Examples 
of our semi-lexical features include answer pro-
noun density, answer abbreviation density, answer 
capitalized word density, and answer stopword 
density. 
5.3 Syntactic Features 
The answer?s structure relative to the sentence?s 
structure provides information as to where better 
spans for the gap may exist. Similarly, part-of-
speech (POS) tags give a topic-independent repre-
sentation of the composition and makeup of the 
questions and answers. The collection of syntactic 
features includes the answer?s depth with the sen-
tence?s constituent parse, the answer?s location 
relative to head verb (before/after), the POS tag 
before the answer, the POS tag after the answer, 
and the answer bag-of-POS tags. 
 
746
 Figure 3: Distribution of number of tokens in the answer 
for Good and not-Good questions. 
5.4 Semantic Role Label Features 
Beyond syntactic constraints, semantics can yield 
additional cues in identifying the important spans 
for questioning. Shallow-semantic parses like those 
found in Propbank (Palmer et al, 2005) provide a 
concise representation for linking predicates 
(verbs) to their arguments. Because these semantic 
role labels (SRLs) often correspond to the ?who, 
what, where, and when? of a sentence, they natu-
rally lend themselves for use as features for rating 
question quality. To compute SRL features, we 
used the MSR SPLAT?s semantic role labeler to 
find the SRLs whose spans cover the question?s 
answer, the SRLs whose spans are contained with-
in the answer, and the answer?s constituent parse 
depth within the closest covering SRL node. 
To investigate whether judges keyed in on spe-
cific roles or modifiers when rating questions, we 
plotted the distribution of the answer-covering 
SRLs (Figure 4). This graph indicates that good 
answers are not associated with only a single label 
but are actually spread across all SRL classes. 
While the bulk of questions came from the argu-
ments often corresponding to subjects and objects 
(ARG0-2, shown as A0-A2), we see that good and 
bad questions have mostly similar distributions 
over SRL classes. However, a notable exception 
are answers covered by verb predicates (shown as 
?predicate?), which were highly skewed with 190 
of the 216 (88.0%) question/answer pairs exhibit-
ing this feature labeled as Bad. Together these dis-
tributions may suggest that judges are more likely 
to rate gap-fill questions as Good if they corre-
spond   to  questions  of   ?who,  what,  where,  and  
 
Figure 4: Distribution of semantic role labels for 
Good and not-Good questions.  
 
when? over questions pertaining to ?why and 
how.? 
5.5 Named Entity Features 
For many topics, especially in the social sciences, 
knowing the relevant people and places marks the 
first step toward comprehending new material. To 
reflect these concerns we use the named-entity 
tagger in the toolkit to identify the spans of text 
that refer to persons, locations, or organizations, 
which are then used to derive additional features 
for distinguishing between candidate questions. 
Example named-entity features include: answer 
named entity density, answer named entity type 
frequency (LOC, ORG, PER), and sentence named 
entity frequency. 
Figure 5 shows the distribution of named entity 
types found within the answers for Good and not-
Good questions. From this graph, we see that Good 
questions have a higher class-conditional probabil-
ity of containing a named entity. Furthermore, we 
see that Good questions are not confined to a sin-
gle named entity type, but are spread across all 
types. Together, these distributions indicate that 
while named entities can help to identify important 
gaps, the majority of questions labeled Good do 
not contain any named entity (515/700, i.e. 74%). 
This provides substantial evidence for generating 
questions for more than only named entities. 
 
747
 Figure 5: Distribution of answer named entity type for 
Good and not-Good questions. 
5.6 Wikipedia Link Features 
Wikipedia?s markup language allows spans of text 
to link to other articles. This annotation inherently 
indicates a span of text as noteworthy, and can 
serve as evidence of an answer?s importance. We 
use the presence of this markup to compute fea-
tures such as answer link density, sentence link 
density, and the ratio of the number of linked 
words in the answer to the ratio of linked words in 
the sentence. 
6 Model and Training 
We chose logistic regression as our classifier be-
cause of its calibrated output of the class posterior; 
we combined it with an L2 regularizer to prevent 
overfitting. As the data likelihood is convex in the 
model parameters, we trained the model to maxim-
ize this quantity along with the regularization term 
using the L-BFGS algorithm for Quasi-Newton 
optimization (Nocedal, 1980). Evaluation was 
conducted with 10-fold cross validation, taking 
care to stratify folds so that all questions generated 
from the same source sentence are placed in the 
same fold. Results are shown in Section 7 below. 
To ensure that we were not overly narrow in 
this model choice, we tested two other more pow-
erful classifiers that do not have calibrated outputs, 
a linear SVM and a boosted mixture of decision 
trees (Caruana and Niculescu-Mizil, 2006); both 
produced accuracies within a percentage point of 
our model at the equal error rate. 
7 Results and Discussion 
Figure 6 shows ROC curves for our question quali-
ty classifier produced by sweeping the threshold on 
the output probability, using the raw collected data, 
our filtered version as described above, and a fur-
ther filtered version keeping only those questions 
where judges agreed perfectly; the benefits of fil-
tering can be seen in the improved performance. In 
this context, the true positive rate refers to the frac-
tion of Good questions that were correctly identi-
fied, and the false positive rate refers to the 
fraction of not-Good questions that were incorrect-
ly marked. At the equal error rate, the true positive 
rate was 0.83 and the false positive rate was 0.19.  
Figure 6: ROC for our model using unfiltered data 
(green dots), our filtered version (red dashes), and fil-
tered for perfect agreement (blue line). 
 
Choosing the appropriate operating point depends 
on the final application. By tuning the classifier?s 
true positive and false positive rates, we can cus-
tomize the system for several uses. For example, in 
a relatively structured scenario like compliance 
training, it may be better to reduce any possibility 
of confusion by eliminating false positives. On the 
other hand, a self-motivated learner attempting to 
explore a new topic may tolerate a higher false 
positive rate in exchange for a broader diversity of 
questions. The balance is subtle, though, as ill-
formed and irrelevant questions could leave the 
learner bored or frustrated, but alternatively, overly 
conservative question classification could poten-
tially eliminate all but the most trivial questions. 
 
T
ru
e 
P
os
iti
ve
 R
at
e
748
 Figure 7: ROC for our model with (red dash) and with-
out (blue line) Wikipedia-specific features. 
 
 
Figure 8: Classifier learning curve; each point repre-
sents mean accuracy over 40 folds. 
 
We next wanted to get a sense of how well the 
model would generalize to other text, and as such 
ran an analysis of training the classifier without the 
benefit of the Wikipedia-specific features (Figure 
7). The resulting model performs about the same as 
the original on average over the ROC, slightly bet-
ter in some places and slightly worse in others. We 
hypothesize the effect is small because these fea-
tures relate only to Wikipedia entities, and the oth-
er named entity features likely make them 
redundant. 
Finally, to understand the sensitivity of our 
model to the amount of training data, we plot a 
learning curve of the question classifier?s accuracy 
by training it against fractions of the available data 
(Figure 8). While the curve starts to level out 
around 1200 data points, the accuracy is still rising 
slightly, which suggests the system could achieve 
some small benefits in accuracy from more data. 
7.1 Error Analysis 
To explore the nature of our system?s misclassifi-
cations we examine the errors that occur at the 
equal error rate operating point. For our system, 
false positive errors occur when the system labels a 
question as Good when the raters considered it not-
Good. Table 2 lists three examples of this type of 
error. The incorrect high score in example 1 
(?Greeks declared ___?) suggests that system per-
formance can be improved via language modeling, 
as such features would penalize questions with an-
swers that could be predicted mostly by word tran-
sition probabilities. Similarly, when classifying 
questions like example 2 (?such as ____ for a 
mathematical function?), the system could benefit 
from some measure of word frequency or answer 
novelty. While our model included a feature for the 
number of overlapping words between the question 
and the answer, the high classifier score for exam-
ple 3 (?atop ______, the volcano?), suggests that 
this can be solved by explicitly filtering out such 
questions at generation time.  
With false negative errors the judges rated the 
question as Good, whereas the system classified it 
as Bad. The question and answer pairs listed in 
Table 3 demonstrate some of these errors. In ex-
ample 1 (?where Pompey was soon ___?), the sys-
tem was likely incorrect because a majority of 
questions with verb-predicate answers had Bad 
ratings (only 12% are Good). Conversely, classifi-
cation of example 2 (?Over the course of dec-
ades??) could be improved with a feature 
indicating the novelty of the words in the answers. 
Example 3 (?About 7.5% of the...?) appears to 
come from rater error or rater confusion as the 
question does little to test the understanding of the 
material. 
While the raters considered the answer to ex-
ample 4 as Good, the low classifier score argues 
for different handling of answers derived from 
long coordinated phrases. One alternative approach 
would be to generate questions that use multiple 
gaps. Conversely, one may argue that a learner 
may be better off answering any one of the noun 
phrases like palm oil or cocoa in isolation.  
 
 
ac
cu
ra
cy
749
 Question Answer Confidence 
1 In 1821 the Greeks 
declared ___ on the 
sultan. 
war 0.732 
2 He also introduced 
much of the modern 
mathematical terminol-
ogy and notation, par-
ticularly for 
mathematical analysis, 
such as _________ of a 
mathematical function. 
the notion 0.527 
3 Not only is there much 
ice atop ________, the 
volcano is also being 
weakened by hydro-
thermal activity. 
the volcano 0.790 
Table 2: Example false positives (human judges rated 
these as not-Good) 
 
 Question Answer Confidence 
1 Caesar then pursued 
Pompey to Egypt, 
where Pompey was 
soon  ____.  
murdered 0.471 
2 Over the course of dec-
ades, individual wells 
draw down local tem-
peratures and water 
levels until _______ is 
reached with natural 
flows. 
a new  
equilibrium 
0.306 
3 About 7.5% of world 
sea trade is carried via 
the canal ____.  
today 0.119 
4 Asante and Dahomey 
concentrated on the 
development of ?legiti-
mate commerce? in 
__________, forming 
the bedrock of West 
Africa?s modern export 
trade,  
the form of 
palm oil, 
cocoa, tim-
ber, and 
gold. 
0.029 
Table 3: Example false negatives (human judges rated 
these Good) 
 
7.2 Feature Analysis 
To ensure that all of the gain of the classifier was 
not coming from only a handful of isolated fea-
tures, we examined the mean values for each fea-
ture?s learned weight in the model over the course 
of 10 cross-validation folds, and then sorted the 
means for greater clarity (Figure 8). The weights 
indeed seem to be well distributed across many 
features. 
 
Figure 8: Feature weight means and standard deviations. 
8 Discussion and Future Work 
We have presented a method that determines 
which gaps in a sentence to ask questions about by 
training a classifier that largely agrees with human 
judgments on question quality. We feel this effort 
is complementary to the past work on question 
generation, and represents another step towards 
helping self-motivated learners with automatically 
generated tests. 
In our future work, we hope to expand the set of 
features as described in Section 7. We additionally 
intend to cast the sentence selection problem as a 
separate learning problem that can also be trained 
from human judgments. 
References 
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic Gap-fill Question Generation from Text 
Books. In Proceedings of the 6th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications. Portland, OR, USA. pages 56-64. 
Richard C. Anderson and W. Barry Biddle. 1975. On 
asking people questions about what they are read-
ing. In G. Bower (Ed.) Psychology of Learning and 
Motivation, 9:90-132. 
Christopher M. Bishop. 2006. Pattern Recognition and 
Machine Learning. New York: Springer, 2006. 
Jonathan C. Brown, Gwen A. Frishkoff, and Maxine 
Eskenazi. 2005. Automatic Question Generation for 
Vocabulary Assessment. In Proceedings of 
HLT/EMNLP 2005. Vancouver, Canada: Associa-
tion for Computational Linguistics. pages 819-826. 
Rich Caruana and Alexandru Niculescu-Mizil. 2006. An 
Empirical Comparison of Supervised Learning Al-
gorithms. In Proceedings of ICML 2006. 
m
ea
n 
an
d 
st
d.
 d
ev
. o
f f
ea
tu
re
 w
ei
gh
t
750
Chris Callison-Burch and Mark Dredze. 2010. Creating 
Speech and Language Data with Amazon's Me-
chanical Turk. In Proceedings of NAACL 2010 
Workshop on Creating Speech and Language Data 
with Amazon's Mechanical Turk. Los Angeles, CA. 
pages 1-12. 
Wei Chen, Gregory Aist, and Jack Mostow. 2009. Gen-
erating Questions Automatically from Information-
al Text. In S. Craig & S. Dicheva (Ed.), 
Proceedings of the 2nd Workshop on Question 
Generation. 
S?rgio Curto, Ana Cristina Mendes, and Lu?sa Coheur. 
2011. Exploring linguistically-rich patterns for 
question generation. In Proceedings of the UCNLG 
+ eval: Language Generation and Evaluation 
Workshop. Edinburgh, Scotland: Association for 
Computational Linguistics. Pages 33-38. 
Michael Heilman and Noah A. Smith. 2010a. Good 
Question! Statistical Ranking for Question Genera-
tion. In Proceedings of NAACL/HLT 2010. pages 
609-617. 
Michael Heilman and Noah A. Smith. 2010b. Rating 
Computer-Generated Questions with Mechanical 
Turk. In Proceedings of NAACL 2010 Workshop on 
Creating Speech and Language Data with Ama-
zon's Mechanical Turk. Los Angeles, CA. pages 35-
40. 
Ayako Hoshino and Hiroshi Nakagawa. 2005. A real-
time multiple-choice question generation for lan-
guage testing - a preliminary study -. In Proceed-
ings of the 2nd Workshop on Building Educational 
Applications Using NLP. Ann Arbor, MI, USA: 
Association for Computational Linguistics. pages 
17-20. 
Panagiotis G. Ipeirotis, Foster Provost, Jing Wang . 
2010. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation (HCOMP?10). 
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology. Thousand Oaks, CA: 
Sage. 
Ruslan Mitkov and Le An Ha. 2003. Computer-Aided 
Generation of Multiple-Choice Tests. Proceedings 
of the HLT-NAACL 2003 Workshop on Building 
Educational Applications Using Natural Language 
Processing, pages 17-22. 
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 
2006. A computer-aided environment for generat-
ing multiple choice test items. Natural Language 
Engineering, 12(2): 177-194. 
Ani Nenkova, Lucy Vanderwende, and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multidocument Summarizer. In Proceedings of 
SIGIR 2006. pages 573-580. 
Rodney D. Nielsen. 2008. Question Generation: Pro-
posed Challenge Tasks and Their Evaluation. In V. 
Rus, & A. Graesser (Ed.), In Proceedings of the 
Workshop on the Question Generation Shared Task 
and Evaluation Challenge. Arlington, VA. 
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices 
with Limited Storage. Mathematics of Computa-
tion, 35:773-782. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. 
The Proposition Bank: An Annotated Corpus of 
Semantic Roles. Computational Linguistics, vol. 
31, no. 1, pp. 71--106. 
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami 
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, and Lucy Vanderwende. 2012. MSR 
SPLAT, a language analysis toolkit. In Proceedings 
of NAACL HLT 2012 Demonstration Session. 
http://research.microsoft.com/projects/msrsplat . 
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, 
Svetlana Stoyanchev and Cristian Moldovan. 2010. 
Overview of The First Question Generation Shared 
Task Evaluation Challenge. In Proceedings of the 
Third Workshop on Question Generation. Pitts-
burgh, PA, USA. pages 45-57. 
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and 
Andrew Y. Ng. 2008. Cheap and Fast?but is it 
Good? Evaluating non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of 
EMNLP?08. pages 254-263. 
Lucy Vanderwende. 2008. The Importance of Being 
Important: Question Generation. In Proceedings of 
the 1st Workshop on the Question Generation 
Shared Task Evaluation Challenge, Arlington, VA. 
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In 
Proceedings of ACL 1999. 
John H. Wolfe. 1976. Automatic question generation 
from text - an aid to independent study. In Proceed-
ings of the ACM SIGCSE-SIGCUE technical sym-
posium on Computer science and education. New 
York, NY, USA: ACM. pages 104-112. 
 
 
751
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 333?340, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
AVAYA: Sentiment Analysis on Twitter with Self-Training
and Polarity Lexicon Expansion
Lee Becker, George Erhart, David Skiba and Valentine Matula
Avaya Labs Research
1300 West 120th Avenue
Westminster, CO 80234, USA
{beckerl,gerhart,dskiba,matula}@avaya.com
Abstract
This paper describes the systems submitted by
Avaya Labs (AVAYA) to SemEval-2013 Task
2 - Sentiment Analysis in Twitter. For the
constrained conditions of both the message
polarity classification and contextual polarity
disambiguation subtasks, our approach cen-
ters on training high-dimensional, linear clas-
sifiers with a combination of lexical and syn-
tactic features. The constrained message po-
larity model is then used to tag nearly half
a million unlabeled tweets. These automati-
cally labeled data are used for two purposes:
1) to discover prior polarities of words and
2) to provide additional training examples for
self-training. Our systems performed compet-
itively, placing in the top five for all subtasks
and data conditions. More importantly, these
results show that expanding the polarity lexi-
con and augmenting the training data with un-
labeled tweets can yield improvements in pre-
cision and recall in classifying the polarity of
non-neutral messages and contexts.
1 Introduction
The past decade has witnessed a massive expansion
in communication from long-form delivery such
as e-mail to short-form mechanisms such as mi-
croblogging and short messaging service (SMS) text
messages. Simultaneously businesses, media out-
lets, and investors are increasingly relying on these
messages as sources of real-time information and
are increasingly turning to sentiment analysis to dis-
cover product trends, identify customer preferences,
and categorize users. While a variety of corpora ex-
ist for developing and evaluating sentiment classi-
fiers for long-form texts such as product reviews,
there are few such resources for evaluating senti-
ment algorithms on microblogs and SMS texts.
The organizers of SemEval-2013 task 2, have be-
gun to address this resource deficiency by coordi-
nating a shared evaluation task for Twitter sentiment
analysis. In doing so they have assembled corpora
in support of the following two subtasks:
Task A - Contextual Polarity Disambiguation
?Given a message containing a marked in-
stance of a word or phrase, determine whether
that instance is positive, negative or neutral in
that context.?
Task B - Message Polarity Classification ?Given
a message, classify whether the message is
of positive, negative, or neutral sentiment.
For messages conveying both a positive and
negative sentiment, whichever is the stronger
sentiment should be chosen.?
This paper describes the systems submitted by
Avaya Labs for participation in subtasks A and B.
Our goal for this evaluation was to investigate the
usefulness of dependency parses, polarity lexicons,
and unlabeled tweets for sentiment classification on
short messages. In total we built four systems for
SemEval-2013 task 2. For task B we developed a
constrained model using supervised learning, and
an unconstrained model that used semi-supervised
learning in the form of self-training and polarity lex-
icon expansion. For task A the constrained sys-
tem utilized supervised learning, while the uncon-
strained model made use of the expanded lexicon
333
from task B. Output from these systems were sub-
mitted to all eight evaluation conditions. For a com-
plete description of the data, tasks, and conditions,
please refer to Wilson et al (2013). The remainder
of this paper details the approaches, experiments and
results associated with each of these models.
2 Related Work
Over the past few years sentiment analysis has
grown from a nascent topic in natural language pro-
cessing to a broad research area targeting a wide
range of text genres and applications. There is
now a significant body of work that spans topics
as diverse as document level sentiment classifica-
tion (Pang and Lee, 2008), induction of word polar-
ity lexicons (Hatzivassiloglou and McKeown, 1997;
Turney, 2002; Esuli and Sebastiani, 2006; Moham-
mad and Turney, 2011) and even election prediction
(Tumasjan et al, 2010).
Efforts to train sentiment classifiers for Twitter
messages have largely relied on using emoticons
and hashtags as proxies of the true polarity (Bar-
bosa and Feng, 2010; Davidov et al, 2010b; Pak and
Paroubek, 2010; Agarwal et al, 2011; Kouloumpis
et al, 2011; Mohammad, 2012). Classification of
word and phrase sentiment with respect to surround-
ing context (Wilson et al, 2005) has yet to be ex-
plored for the less formal language often found in
microblog and SMS text. Semi-supervised learn-
ing has been applied to polarity lexicon induction
(Rao and Ravichandran, 2009), and sentiment clas-
sification at the sentence level (Ta?ckstro?m and Mc-
Donald, 2011) and document level (Sindhwani and
Melville, 2008; He and Zhou, 2011); however to
the best of our knowledge self-training and other
semi-supervised learning has seen only minimal use
in classifying Twitter texts (Davidov et al, 2010a;
Zhang et al, 2012).
3 System Overview
Given our overarching goal of combining polarity
lexicons, syntactic information and unlabeled data,
our approach centered on first building strong con-
strained models and then improving performance
by adding additional data and resources. For
both tasks, our data-constrained approach com-
bined standard features for document classification
conj ? conj ?conjunction?
pobj ? prep ?preposition?
pcomp? prepc ?preposition?
prep|punct|cc? ?
Table 1: Collapsed Dependency Transformation Rules
with dependency parse and word polarity features
into a weighted linear classifier. For our data-
unconstrained models we used pointwise mutual in-
formation for lexicon expansion in conjunction with
self-training to increase the size of the feature space.
4 Preprocessing and Text Normalization
Our systems were built with ClearTK (Ogren et
al., 2008) a framework for developing NLP com-
ponents built on top of Apache UIMA. Our pre-
processing pipeline utilized ClearTK?s wrappers for
ClearNLP?s (Choi and McCallum, 2013) tokenizer,
lemmatizer, part-of-speech (POS) tagger, and de-
pendency parser. ClearNLP?s ability to retain emoti-
cons and emoji as individual tokens made it espe-
cially attractive for sentiment analysis. POS tags
were mapped from Penn Treebank-style tags to the
simplified, Twitter-oriented tags introduced by Gim-
pel et al (2011). Dependency graphs output by
ClearNLP were also transformed to the Stanford
Collapsed dependencies representation (de Marneffe
and Manning, 2012) using our own transformation
rules (table 1). Input normalization consisted solely
of replacing all usernames and URLs with common
placeholders.
5 Sentiment Resources
A variety of our classifier features rely on manually
tagged sentiment lexicons and word lists. In partic-
ular we make use of the MPQA Subjectivity Lexi-
con (Wiebe et al, 2005) as well as manually-created
negation and emoticon dictionaries1. The negation
word list consisting of negation words such as no
and not. Because tokenization splits contractions,
the list includes the sub-word token n?t as well as
the apostrophe-less version of 12 contractions (e.g.
cant, wont, etc . . . ). To support emoticon-specific
features we created a dictionary, which paired 183
emoticons with either a positive or negative polarity.
1http://leebecker.com/resources/semeval-2013
334
6 Message Polarity Classification
6.1 Features
Polarized Bag-of-Words Features: Instead of ex-
tracting raw bag-of words (BOW), we opted to in-
tegrate negation directly into the word representa-
tions following the approaches used by Das and
Chen (2001) and Pang et al (2002). All words
between a negation word and the first punctuation
mark after the negation word were suffixed with
a NOT tag ? essentially doubling the number of
BOW features. We extended this polarized BOW
paradigm to include not only the raw word forms
but all of the following combinations: raw word, raw
word+PTB POS tag, raw word+simplified POS tag,
lemma+simplified POS tag.
Word Polarity Features: Using a subjectivity lex-
icon, we extracted features for the number of posi-
tive, negative, and neutral words as well as the net
polarity based on these counts. Individual word po-
larities were inverted if the word had a child depen-
dency relation with a negation (neg) label. Con-
strained models use the MPQA lexicon, while un-
constrained models use an expanded lexicon that is
described in section 6.2.
Emoticon Features: Similar to the word polarity
features, we computed features for the number of
positive, negative, and neutral emoticons, and the
net emoticon polarity score.
Microblogging Features: As noted by Kouloumpis
et al (2011), the emotional intensity of words in so-
cial media messages is often emphasized by changes
to the word form such as capitalization, charac-
ter repetition, and emphasis characters (asterisks,
dashes). To capture this intuition we compute fea-
tures for the number of fully-capitalized words,
words with characters repeated more than 3 times
(e.g. booooo), and words surround by asterisks or
dashes (e.g. *yay*). We also created a binary fea-
ture to indicate the presence of a winning score or
winning record within the target span (e.g. Oh yeah
#Nuggets 15-0).
Part-of-Speech Tag Features: Counts of the Penn
Treebank POS tags provide a rough measure of the
content of the message.
Syntactic Dependency Features: We extracted
dependency pair features using both standard and
collapsed dependency parse graphs. Extracted
head/child relations include: raw word/raw word,
lemma/lemma, lemma/simplified POS tag, simpli-
fied POS tag/lemma. If the head node of the relation
has a child negation dependency, the pair?s relation
label is prefixed with a NEG tag.
6.2 Expanding the Polarity Lexicon
Unseen words pose a recurring challenge for both
machine learning and dictionary-based approaches
to sentiment analysis. This problem is even more
prevalent in social media and SMS messages where
text lengths are often limited to 140 characters or
less. To expand our word polarity lexicon we adopt
a framework similar to the one introduced by Turney
(2002). Turney?s unsupervised approach centered on
computing pointwise mutual information (PMI) be-
tween highly polar seed words and bigram phrases
extracted from a corpus of product reviews.
Instead of relying solely on seed words for po-
larity, we use the constrained version of the mes-
sage polarity classifier to tag a corpus of approxi-
mately 475,000 unlabeled, English language tweets.
These tweets were collected over the period from
November 2012 to February 2013. To reduce the
number of noisy instances and to obtain a more bal-
anced distribution of sentiment labels, we eliminated
all tweets with classifier confidence scores below
0.9, 0.7, and 0.8 for positive, negative and neutral
instances respectively. Applying the threshold, re-
duced the tweet count to 180,419 tweets (50,789
positive, 59,029 negative, 70,601 neutral). This fil-
tered set of automatically labeled tweets was used
to accumulate co-occurrence statistics between the
words in the tweets and their corresponding senti-
ment labels. These statistics are then used to com-
pute word-sentiment PMI (equation 1), which is
the joint probability of a word and sentiment co-
occurring divided by the probability of each of the
events occurring independently. A word?s net po-
larity is computed as the signum (sgn) of the differ-
ence between a its positive and negative PMI values
(equation 2). It should be noted that polarities were
deliberately limited to values of {-1, 0, +1} to ensure
consistency with the existing MPQA lexicon, and to
dampen the bias of any single word.
335
PMI(word, sentiment) = log2
p(word, sentiment)
p(word)p(sentiment)
(1)
polarity(word) = sgn(PMI(word, positive)?
PMI(word, negative))
(2)
Words with fewer than 10 occurrences, words
with neutral polarities, numbers, single characters,
and punctuation were then removed from this PMI-
derived polarity dictionary. Lastly, this dictionary
was merged with the dictionary created from the
MPQA lexicon yielding a final polarity dictionary
with 11,740 entries. In cases where an entry existed
in both dictionaries, the MPQA polarity value was
retained. This final polarity dictionary was used by
the unconstrained models for task A and B.
6.3 Model Parameters and Training
Constrained Model: Models were trained us-
ing the LIBLINEAR classification library (Fan et
al., 2008). L2 regularized logistic regression was
chosen over other LIBLINEAR loss functions be-
cause it not only gave improved performance on
the development set but also produced calibrated
outcomes for confidence thresholding. Training
data for the constrained model consisted of all
9829 examples from the training (8175 exam-
ples) and development (1654 examples) set re-
leased for SemEval 2013. Cost and label-specific
cost weight parameters were selected via exper-
imentation on the development set to maximize
the average positive and negative F1 values. The
c values ranged over {0.1, 0.5, 1, 2, 5, 10, 20, 100}
and the label weights wpolarity ranged over
{0.1, 1, 2, 5, 10, 20, 25, 50, 100}. Final parameters
for the constrained model were cost c = 1 and
weights wpositive = 1, wnegative = 25, and
wneutral = 1.
Unconstrained Model: In addition to using the ex-
panded polarity dictionary described in 6.2 for fea-
ture extraction, the unconstrained model also makes
use of automatically labeled tweets for self-training
(Scudder, 1965). In contrast to preparation of the ex-
panded polarity dictionary, the self-training placed
no threshold on the examples. Combining the self-
labeled tweets, with the official training and devel-
opment set yielded a new training set consisting
of 485,112 examples. Because the self-labeled in-
stances were predominantly tagged neutral, the LI-
BLINEAR cost parameters were adjusted to heav-
ily discount neutral while emphasizing positive and
neutral instances. The size and cost of training
this model prevented extensive parameter tuning and
instead were chosen based on experience with the
constrained model and to maximize recall on pos-
itive and negative items. Final parameters for the
unconstrained model were cost c = 1 and cate-
gory weights wpositive = 2, wnegative = 5, and
wneutral = 0.1.
7 Contextual Polarity Disambiguation
7.1 Features
The same base set of features used for message po-
larity classification were used for the contextual po-
larity classification, with the exception of the syn-
tactic dependency features. To better express the in-
context and out-of-context relation these additional
feature classes were added:
Scoped Dependency Features: Because this task
focuses on a smaller context within the message,
collapsed dependencies are less useful as the com-
pression may cross over context boundaries. In-
stead the standard syntactic dependency features de-
scribed above were modified to account for their re-
lation to the context. All governing relations for the
words contained within the contact were extracted.
Relations wholly contained within the boundaries of
the context were prefixed with an IN tag, whereas
those that crossed outside of the context were pre-
fixed with an OUT tag. Additionally counts of IN
and OUT relations were included as features.
Dependency Path Features: Like the single de-
pendency arcs, a dependency path can provide addi-
tional information about the syntactic and semantic
role of the context in the sentence. Our path fea-
tures consisted of two varieties: 1) POS-path and
2) Sentiment-POS-path. The POS-path consisted of
the PTB POS tags and dependency relation labels
for all nodes between the head of the context and the
root node of the parent sentence. The Sentiment-
POS-path follows the same path but omits the de-
pendency relation labels, uses the simplified POS
tags and appends word polarities (POS/NEG/NTR)
to the POS tags along the path.
336
System
Positive Negative Neutral Favg Rank
P R F P R F P R F +/-
Tw
ee
t NRC-Canada (top) 0.814 0.667 0.733 0.697 0.604 0.647 0.677 0.826 0.744 0.690 1
AVAYA-Unconstrained 0.751 0.655 0.700 0.608 0.557 0.582 0.665 0.768 0.713 0.641 5
AVAYA-Constrained 0.791 0.580 0.669 0.593 0.509 0.548 0.636 0.832 0.721 0.608 12
Mean of submissions 0.687 0.591 0.626 0.491 0.456 0.450 0.612 0.663 0.615 0.538 -
S
M
S
NRC-Canada (top) 0.731 0.730 0.730 0.554 0.754 0.639 0.852 0.753 0.799 0.685 1
AVAYA-Constrained 0.630 0.667 0.648 0.526 0.581 0.553 0.802 0.756 0.778 0.600 4
AVAYA-Unconstrained 0.609 0.659 0.633 0.494 0.637 0.557 0.814 0.710 0.759 0.595 5
Mean of submissions 0.512 0.620 0.546 0.462 0.518 0.456 0.754 0.578 0.627 0.501 -
Table 2: Message Polarity Classification (Task B) Results
System
Positive Negative Neutral Favg Rank
P R F P R F P R F +/-
Tw
ee
t NRC-Canada (top) 0.889 0.932 0.910 0.866 0.871 0.869 0.455 0.063 0.110 0.889 1
AVAYA-Unconstrained 0.892 0.905 0.898 0.834 0.865 0.849 0.539 0.219 0.311 0.874 2
AVAYA-Constrained 0.882 0.911 0.896 0.844 0.843 0.843 0.493 0.225 0.309 0.870 3
Mean of submissions 0.837 0.745 0.773 0.745 0.656 0.677 0.159 0.240 0.115 0.725 -
S
M
S
GUMLTLT (top) 0.814 0.924 0.865 0.908 0.896 0.902 0.286 0.050 0.086 0.884 1
AVAYA-Unconstrained 0.815 0.871 0.842 0.853 0.896 0.874 0.448 0.082 0.138 0.858 3
AVAYA-Constrained 0.777 0.875 0.823 0.859 0.852 0.856 0.364 0.076 0.125 0.839 4
Mean of submissions 0.734 0.722 0.710 0.807 0.663 0.698 0.144 0.184 0.099 0.704 -
Table 3: Contextual Polarity Disambiguation (Task A) Results
For example given the bold-faced context in the
sentence:
@User Criminals killed Sadat, and in the
process they killed Egypt. . . they destroyed
the future of young & old Egyptians..
the extracted POS-path feature would be:
{NNP} dobj <{VBD} conj <{VBD}
ccomp <{VBD} root <{TOP}
while the Sentiment-POS path would be:
{?/pos}{V/neg}{V/neg}{V/neg}{TOP}.
Paths with depth greater than 4 dependency rela-
tions were truncated to reduce feature sparsity. In
addition to these detailed path features, we include
two binary features to indicate if any part of the path
contains subject or object relations.
7.2 Model Parameters and Training
Like with message polarity classification, the con-
textual polarity disambiguation systems rely on LI-
BLINEAR?s L2 regularized logistic regression for
model training. Both constrained and unconstrained
models use identical parameters of cost c = 1
and weights wpositive = 1, wnegative = 2, and
wneutral = 1. They vary only in the choice of polar-
ity lexicon. The constrained model uses the MPQA
subjectivity lexicon, while the unconstrained model
uses the expanded dictionary derived via computa-
tion of PMI, which ultimately differentiates these
models through the variation in the sentiment path
and word polarity features.
8 Experiments and Results
In this section we report results for the series of Sen-
timent Analysis in Twitter tasks at SemEval 2013.
Please refer to refer to Wilson et al (2013) for the
exact details about the corpora, evaluation condi-
tions, and methodology.
We submitted polarity output for the Message Po-
larity Classification (task B) and the Contextual Po-
larity Disambiguation (task A). For each task we
submitted system output from our constrained and
unconstrained models. As stated above, the con-
strained models made use of only the training data
released for the task, whereas the unconstrained
models trained on additional tweets. Each subtask
had two test sets one comprised of tweets and the
other comprised of SMS messages. Final task 2
337
S G Message / Context
1 + / Going to Helsinki tomorrow or on the day after tomorrow,yay!
2 / + Eric Decker catches his second TD pass from Manning. This puts Broncos up 31-7 with 14:54 left in the 4th.
3 - / So, crashed a wedding reception and Andy Lee?s bro was in the bridal party. How?d you spend your Saturday
night? #longstory
4 - + Aiyo... Dun worry la, they?ll let u change one... Anyway, sleep early, nite nite...
5 + - Sori I haven?t done anything for today?s meeting.. pls pardon me. Cya guys later at 10am.
6 + - these PSSA?s are just gonna be the icing to another horrible monday. #fmlll #ihateschool
Table 4: Example Classification Errors: S=System, G=Gold, +=positive, ?=negative, /=neutral. Bold-faced text
indicates the span for contextual polarities.
evaluation is based on the average positive and neg-
ative F-score. Task B results are listed in table 2,
and task A results are shown in table 3. For compar-
ison these tables also include the top-ranked system
in each category as well as the mean scores across
all submissions.
9 Error Analysis
To better understand our systems? limitations we
manually inspected misclassified output. Table 4
lists errors representative of the common issues un-
covered in our error analysis.
Though some degree of noise is expected in senti-
ment analysis, we found several instances of annota-
tion error or ambiguity where it could be argued that
the system was actually correct. The message in #1
was annotated as neutral, whereas the presence of
the word ?yay? suggests an overall positive polarity.
The text in #2 could be interpreted as positive, nega-
tive or neutral depending on the author?s disposition.
Unseen vocabulary and unexpected usages were
the largest category of error. For example in #3
?crashed? means to attend without an invitation in-
stead of the more negative meaning associated with
car accidents and airplane failures. Although POS
features can disambiguate word senses, in this case
more sophisticated features for word sense disam-
biguation could help. While the degradation in
performance between the Tweet and SMS test sets
might be explained by differences in medium, er-
rors like those found in #4 and #5 suggest that this
may have more to do with the dialectal differences
between the predominantly American and British
English found in the Tweet test set and the Collo-
quial Singaporean English (aka Singlish) found in
the SMS test set. Error #6 illustrates both how hash-
tags composed of common words can easily become
a problem when assigning a polarity to a short con-
text. Hashtag segmentation presents one possible
path to reducing this source of error.
10 Conclusions and Future Work
The results and rankings reported in section 8 sug-
gest that our systems were competitive in assign-
ing sentiment across the varied tasks and data con-
ditions. We performed particularly well in dis-
ambiguating contextual polarities finishing second
overall on the Tweet test set. We hypothesize this
performance is largely due to the expanded vocabu-
lary obtained via unlabeled data and the richer syn-
tactic context captured with dependency path repre-
sentations.
Looking forward, we expect that term recall and
unseen vocabulary will continue to be key chal-
lenges for sentiment analysis on social media. While
larger amounts of data should assist in that pursuit,
we would like to explore how a more iterative ap-
proach to self-training and lexicon expansion may
provide a less noisy path to attaining such recall.
11 Acknowledgments
We would like to thank the organizers of SemEval
2013 and the Sentiment Analysis in Twitter task for
their time and energy. We also would like to ex-
press our appreciation to the anonymous reviewers
for their helpful feedback and suggestions.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
338
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
36?44, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jinho D. Choi and Andrew McCallum. 2013. Transition-
based dependency parsing with selectional branching.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL?13).
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: extracting market sentiment from stock message
boards. In Proceedings of the 8th Asia Pacific Finance
Association Annual Conference.
Dmitry Davidov, Oren Tsur, and Ari Rappaport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Coling 2010, pages 241?249.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2012. Stanford typed dependencies manual.
Stanford University, v2.0.4 edition, November.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC?06).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classication. Journal of Ma-
chine Learning Research, 9:1871?1874.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies ACL:HLT 2011.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics (ACL
1997).
Yulan He and Deyu Zhou. 2011. Self-training from
labeled features for sentiment analysis. Information
Processing and Management, 47(4):606?616.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM 2011).
Saif M. Mohammad and Peter D. Turney. 2011. Crowd-
sourcing a word-emotion association lexicon. Compu-
tational Intelligence, 59(000).
Saif M. Mohammad. 2012. #emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM).
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC ?08), 5.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification Using
Machine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002).
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009).
H. J. Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machine. IEEE Transactions
on Information Theory, 11:363?371.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In Proceedings of the 2008 Eighth IEEE In-
ternational Conference on Data Mining, ICDM ?08,
pages 1025?1030.
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2011).
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with twitter what 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM 2010).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
339
Meeting of the Association for Computational Linguis-
tics (ACL 2002).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39:165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the 7th International Workshop on Se-
mantic Evaluation. Association for Computation Lin-
guistics.
Xiuzhen Zhang, Yun Zhou, James Bailey, and Kota-
giri Ramamohanarao. 2012. Sentiment analysis
by augmenting expectation maximisation with lexi-
cal knowledge. Proceedings of the 13th International
Conference on Web Information Systems Engineering
(WISE2012).
340
DISCUSS: A dialogue move taxonomy layered over semantic
representations
Lee Becker1 Wayne H. Ward1,2 Sarel van Vuuren1 Martha Palmer1
{lee.becker, martha.palmer, sarel.vanvuuren}@colorado.edu,
wward@bltek.com
1University of Colorado at Boulder, 2Boulder Language Technologies
Abstract
In this paper we describe DISCUSS, a dialogue move taxonomy layered over semantic represen-
tations. We designed this scheme to enable development of computational models of tutorial dia-
logues and to provide an intermediate representation suitable for question and tutorial act generation.
As such, DISCUSS captures semantic and pragmatic elements across four dimensions: Dialogue Act,
Rhetorical Form, Predicate Type, Semantic Roles. Together these dimensions provide a summary of
an utterance?s propositional content and how it may change the underlying information state of the
conversation. This taxonomy builds on previous work in both general dialogue act taxonomies as
well as work in tutorial act and tutorial question categorization. The types and values found within
our taxonomy are based on preliminary observations and on-going annotation from our corpus of
multimodal tutorial dialogues for elementary school science education.
1 Introduction
Past successes with conversational Intelligent Tutoring Systems (ITS) (Graesser et al, 2001), have helped
to demonstrate the efficacy of computer-led, tutorial dialogue. However, ITS will not reach their full
potential until they can overcome current limitations in spoken dialogue technologies. Producing systems
capable of leading open-ended, Socratic-style tutorials will likely require more sophisticated models to
automate analysis and generation of dialogue. A well defined tutorial dialogue annotation scheme can
serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring
style and question scaffolding techniques and should capture the subtle distinctions between different
question types. To do this, requires a representation that connects a turn?s communicative and rhetorical
functions to its underlying semantic content.
While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make
dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific
initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue
acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences
in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely,
question type categories (Graesser and Person, 1994; Nielsen et al, 2008) have been designed with
education in mind, but they largely ignore how the student and tutor may work together to construct
meaning. The DISCOUNT scheme?s (Pilkington, 1999) combination of dialogue acts and rhetorical
functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it
from capturing how content influences behavior.
Our long-term goals of automatic dialogue characterization, tutorial move prediction and question
generation led us to design our own dialogue representation called DISCUSS (Dialogue Scheme for
Unifying Speech and Semantics). Design of this dialogue move taxonomy was based on preliminary
observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
We hope that undertaking this ambitious endeavor to capture not only a turn?s pragmatic interpretation,
310
but also its rhetorical and semantic functions will enable us to better model the complexity of open-ended,
tutorial dialogue.
The remainder of the this paper is organized as follows. In the next section we describe our tutorial
dialogue setting and our data. Section 3 discusses the organization of the DISCUSS annotation scheme.
Section 4 briefly explains the current status of our annotation. Lastly section 5 outlines our future plans
and conclusions.
2 Tutorial Dialogue Setting and Data
My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve
science learning and understanding for students in grades 3-5. Students using MyST investigate and
discuss science through natural spoken dialogues and multimedia interactions with a virtual tutor named
Marni. The MyST dialogue design and tutoring style is based on a pedagogy called Questioning the
Author (QtA) (Beck et al, 1996), wherein the teacher facilitates discovery by challenging students with
open-ended questions and by directly keying in on ideas expressed in the student?s language.
To gather data for MyST system coverage and dialogue analysis, we ran Wizard-of-Oz (WoZ) exper-
iments that allowed a human tutor to be inserted into the interaction loop. Project tutors trained in QtA
served as Wizards and were responsible for accepting and overriding system actions. Over the past three
years we have accumulated over five-hundred, 15-minute WoZ sessions across four modules Magnetism
and Electricity, Measurement, Variables, and Water, each with 16 lessons. Student speech from these
sessions was professionally transcribed at the word level.
3 The DISCUSS Annotation Scheme
The Dialogue Scheme for Unifying Speech and Semantics (DISCUSS) is a multifaceted dialogue move
taxonomy intended to capture both the pragmatic and semantic interpretations of an utterance. A DIS-
CUSS move is a tuple composed of values from four dimensions: Dialogue Act, Rhetorical Form, Pred-
icate Type, and Semantic Roles. Together these dimensions convey the communicative action, surface
form, and meaning of an utterance independent of the original utterance text.
We designed DISCUSS to serve as an intermediate representation that will enable future work in
dialogue session characterization, dialogue strategy optimization, and automatic question generation. To
facilitate these goals, we have endeavored to create a taxonomy that is both descriptive and curriculum-
independent while allowing for expansion as necessary. A complete listing of all the DISCUSS moves
and dimensions can be found in our forthcoming technical report.
In the following subsection we will describe the different DISCUSS move categories. Descriptions
of the Semantic Role and Predicate Type are found in the subsection about semantic dimensions, while
discussion about the dialogue act and rhetorical form has been placed in the pragmatic dimensions
subsection. Throughout the rest of this paper we denote DISCUSS tuples using the following notation:
Dialogue Act/Rhetorical Form/Predicate Type ?Semantic Role?.
3.1 Move Categories
DISCUSS moves are dictated by the dialogue act dimension and may belong to one of three broad cate-
gories: Dialogue Control, Information Exchange, and Attention Management. Dialogue Control moves
are largely concerned with maintaining and enabling the flow of information. This includes dialogue
acts such as Acknowledge, Open, Close, Repeat, and RequestRepeat. The Information Exchange moves
relay content (often lesson-specific) between speakers using moves such as Assert, Ask, Answer, Mark,
Revoice. For tutorial dialogue the bulk of student-tutor interactions reside in this category. Lastly, At-
tention Management moves indicate how a speaker exercises initiative over other speakers or topics.
Dialogue acts found in the attention category are Focus, Defer, Elicit, and Direct.
311
3.2 Semantic Dimensions
The semantic dimensions define the objects, events, properties and relations contained within an utter-
ance. The semantic roles at the lowest level of the DISCUSS hierarchy directly capture the propositional
entities. Predicate Types summarize the interactions between all of the semantic roles found within an
utterance.
Semantic Roles: The MyST system models a lesson?s key concepts as propositions which are real-
ized as semantic frames. For MyST natural language understanding, these frames serve as the top-level
nodes for a manually written semantic grammar used by the Phoenix parser (Ward, 1994). Two example
concepts/frames and Phoenix parses are shown below. Although these semantic frames form the basis
of MyST dialogues, for DISCUSS annotation we sought a more domain-independent representation that
would generalize across a wide range of subjects. We began with VerbNet (Schuler, 2005) for defining
our set of semantic roles because of its intuitive balance between descriptiveness and portability. While
we used a majority of the labels as is, we found that the definition of some roles needed to be modified
or extended to properly cover our set of concepts. For example, many concepts that express proportion-
ality relationships can not be easily represented using predicate argument structure, and are more easily
decomposed into cause and effect roles. We also added the catch-all keyword label to reflect terms that
may relate to the proposition, but are not part of the core representation.
For our annotation project, rather than manually tagging all of the utterances with VerbNet labels, we
created a mapping layer between the Phoenix frame roles and the VerbNet roles. The table below shows
two frames along with their role mappings. We envision that in future projects, the hand-tuned semantic
grammars could be replaced with a statistically trained semantic role labeler.
Frame: BatteryFunction Frame: MagnetsAttract
Description: The DCell is the source of elec-
tricity.
Description: Magnets attract to certain ob-
jects.
?Instrument?: [Battery] ?Instrument?: [Magnet]
?Predicate?: [Source] ?Predicate?: [Attract]
?Theme?: [Electricity] ?Theme?: [Object]
Predicate Type: Simply knowing an utterance?s propositional content is insufficient for inferring
what was stated. Consider the two exchanges shown in the table below. The mixture of semantic roles
in both students? responses are identical. Additionally, we can not differentiate between the exchanges
based solely on dialogue act or rhetorical form. We need additional information to know the first scenario
seeks to elicit discussion about observations while the second scenario focuses on procedures. One can
also imagine such information would be useful for identifying communication breakdowns. For example,
responding with a description of a procedure to a request about a process may indicate that the student
did not understand the question or that the student is unwilling or unable to address the question.
T12: Tell me about what?s going on here in this picture.
Ask/Describe/Observation
S13: The wires connect the battery and the light bulb and then then light bulb lights up.
Answer/Describe/Observation
?Instrument?.wires ?Predicate?.connect ?Theme1?.battery ?Theme2?.light bulb ?Effect?.bulb
lights up
T7: Tell me about how you got the bulb to light up.
Ask/Describe/Procedure
S8: To make the light go we connected the wires to the battery and the bulb.
Answer/Describe/Procedure
?Effect?.light go ?Predicate?.connected ?Instrument?.wires ?Theme1?.battery ?Theme2?.bulb
To address this need, we created the Predicate Type based partly on the rhetorical predicates used in
the DISCOUNT (Pilkington, 1999) scheme. While DISCOUNT included discourse relations in the set
of predicate types, we restrict predicate types to those that encapsulate or summarize the collection of
semantic roles in an utterance. Example predicate types include procedure, observation and purpose. A
complete list of predicate types can be found in our forthcoming technical report.
312
3.3 Pragmatic Dimensions
The pragmatic dimensions are composed of the dialogue act dimension and the rhetorical form dimen-
sion. The dialogue act expresses the communicative function of a move and is the most general dimen-
sion in DISCUSS. The rhetorical form expresses attributes of the utterance?s surface realization and can
be thought of as refining the intent of the coarser dialogue act.
Dialogue Act: The dialogue act dimension is the top-level dimension in DISCUSS with the values
of all other dimensions depending on the value of this dimension. Like with the majority of dialogue
act taxonomies, DISCUSS dialogue acts have a grounding in speech act theory with a focus on what
action the utterance performs. While most of the dialogue acts in the Dialogue Control and Informa-
tion Exchange move categories have direct corollaries to those found in other taxonomies like DIT++ or
DAMSL, we needed to supplement them with two frequently used Questioning the Author discussion
moves: marking and revoicing. In marking, the tutor highlights parts of the student?s language to em-
phasize important points and to steer the conversation towards key concepts. Revoicing serves a similar
purpose, but instead of highlighting, the tutor rephrases student speech to clarify ideas they may have
been struggling with. Examples of these acts are shown below.
S5: that when you stick a magnet to a rusty nail and then you stick it to a paper clip it sticks
Answer/Describe/Process
T6: I think I heard you say something about magnets sticking or attracting. Tell me more about that.
Mark/None/None, Ask/Elaborate/Process
S33: well when you scrub the the paperclip to the magnet the paperclip is starting to be a magnet
Answer/Describe/Process
T34: very good, so if the magnet gets close to the paperclip it picks it up
Feedback/Positive/None, Revoice/None/None
Dialogue acts in the Attention Management move category also reflect many of the actions regularly
seen in tutorial dialogue. Focus and Defer acts are often used to move to or away from lesson-specific
topics. In our corpus Direct is typically used to give instructions related to the multimedia (e.g. ?Click
on the box? or ?Look at this animation.?).
Rhetorical Form: The DISCUSS Rhetorical Form dimension provides another mechanism for dif-
ferentiating between utterances with identical semantic content. While the dialogue act dimension is
useful for providing an utterance?s pragmatic interpretation and for determining what sequences are li-
censed, by itself it provides no indication of how a speaker is advancing the topic under discussion.
Additional information is needed to create an utterance?s surface form. Consider the two transactions
in the table below. The semantic parses in both scenarios would be identical, however the tutor?s ques-
tions and the resulting student response serve very different functions. In the first, the tutor is asking
for a description and in the second, identification. Selection of the DISCUSS rhetorical forms found in
the Information Exchange move category were inspired by the sixteen top-level tags used in Rhetori-
cal Structure Theory (RST) (Mann and Thompson, 1988). While RST uses a rhetorical relation to link
clauses and to show the development of an argument, DISCUSS uses the rhetorical form to refine the
dialogue act. A sequence of dialogue acts paired with rhetorical forms can show progressions in the
dialogue and tutoring process such as a shift from open-ended to directed questioning.
T1: Can you tell which one is the battery? T1: Can you describe what is going on with the battery?
Ask/Describe/Visual Ask/Identify/None
S2: The battery is putting out electricity. S2: The battery is the one putting out the electricity.
Answer/Describe/Process Answer/Identify/None
4 Annotation Status
We are still in the early stages of this ambitious annotation project. We currently have approximately
60 transcripts singly-annotated with DISCUSS moves. Each of these transcripts represents roughly 15
minutes of conversation and 50 turns on average. The DISCUSS taxonomy is a work in progress. Though
313
we have created the tags for each dimension based on a wide body of prior research and on preliminary
studies of our transcripts, we expect that future analysis of our annotation reliability and consistency will
likely lead us to add, modify, and combine tags. We anticipate that DISCUSS?s multidimensional nature
will likely raise issues for inter-annotator reliability, and the ability to add multiple tags per turn will
further complicate the process of evaluating agreement.
5 Future Work and Conclusions
We plan to use our corpus of DISCUSS annotated tutorial dialogues to build dialogue models for a variety
of applications including assessment of tutorial quality and dialogue move prediction. This annotation
will allow us to investigate what features of tutorial dialogue correlate with increased learning gains and
what types of questions encourage greater student interaction. Data-driven dialogue characterization will
also allow us to explore how tutorial tactics vary across domains and tutors. We envision this work as an
important first step towards automatic question generation.
In this paper we introduced the DISCUSS dialogue move taxonomy. This scheme overlays dialogue
act and rhetorical annotation over semantic representations. We believe this combination of pragmatic
interpretations and semantic representations provide an intermediate representation rich enough to an-
alyze the interactions in a complex task-oriented domain like tutorial dialogue. Furthermore, we think
DISCUSS moves can succinctly summarize the actions of a speaker?s turn, while still providing suffi-
cient information for natural language generation of dialogue moves.
Acknowledgments This work was supported by grants from the NSF (DRL-0733322, DRL-0733323) and the IES (R3053070434).
Any findings, recommendations, or conclusions are those of the author and do not necessarily represent the views of NSF or
IES.
References
Beck, I. L., M. G. McKeown, J. Worthy, C. A. Sandora, and L. Kucan (1996). Questioning the author: A year-long classroom
implementation to engage students with text. The Elementary School Journal 96(4), 387?416.
Buckley, M. and M. Wolska (2008). A classification of dialogue actions in tutorial dialogue. In Proc. COLING, pp. 73?80.
ACL.
Bunt, H. (2009). The dit++ taxonomy for functional dialogue markup. In Proc. EDAML 2009.
Core, M. and J. Allen (1997). Coding dialogs with the damsl annotation scheme. In AAAI Fall Symposium on Comm. Action in
Humans and Machines, pp. 28?35.
Graesser, A., X. Hu, S. Susarla, D. Harter, N. Person, M. Louwerse, B. Olde, and the Tutoring Research Group (2001).
Autotutor: An intelligent tutor and conversational tutoring scaffold. In Proc. AIED?01, pp. 47?49.
Graesser, A. and N. Person (1994). Question asking during tutoring. American Educational Research Journal 31, 104?137.
Mann, W. C. and S. A. Thompson (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text 8(3),
243?281.
Nielsen, R. D., J. Buckingham, G. Knoll, B. Marsh, and L. Palen (2008, September). A taxonomy of questions for question
generation. In Proc. WS on the Question Generation STEC.
Pilkington, R. M. (1999). Analysing educational discourse: The discount scheme. Technical Report 99/2, Computer Based
Learning Unit, University of Leeds.
Schuler, K. K. (2005). VerbNet: A broad-coverage, comprehensive verb lexicon. Ph. D. thesis, University of Pennsylvania.
Tsovaltzi, D. and E. Karagjosova (2004). A view on dialogue move taxonomies for tutorial dialogues. In Proc. SIGDial, pp.
35?38. ACL.
Ward, W. (1994). Extracting information from spontaneous speech. In Proc. ICSLP.
Ward, W., R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. Van Vuuren, T. Weston, J. Zheng, and L. Becker (2010).
My science tutor: A conversational multi-media virtual tutor for elementary school science. ACM TSLP: Special Issue on
Speech and Language Processing of Children?s Speech for Child-machine Interaction Applications.
314
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1?11,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Question Ranking and Selection in Tutorial Dialogues
Lee Beckera and Martha Palmer 2a and Sarel van Vuuren 3a and Wayne Ward 4a,b
aThe Center for Computational Language and Education Research (CLEAR)
University of Colorado Boulder
bBoulder Language Technologies
{lee.becker,martha.palmer,sarel.vanvuuren}@colorado.edu
wward@bltek.com
Abstract
A key challenge for dialogue-based intelligent
tutoring systems lies in selecting follow-up
questions that are not only context relevant
but also encourage self-expression and stimu-
late learning. This paper presents an approach
to ranking candidate questions for a given di-
alogue context and introduces an evaluation
framework for this task. We learn to rank us-
ing judgments collected from expert human
tutors, and we show that adding features de-
rived from a rich, multi-layer dialogue act
representation improves system performance
over baseline lexical and syntactic features to
a level in agreement with the judges. The ex-
perimental results highlight the important fac-
tors in modeling the questioning process. This
work provides a framework for future work
in automatic question generation and it rep-
resents a step toward the larger goal of di-
rectly learning tutorial dialogue policies di-
rectly from human examples.
1 Introduction
Socratic tutoring styles place an emphasis on elicit-
ing information from the learner to help them build
their own connections to the material. The role of a
tutor in a Socratic dialogue is to scaffold the material
and present questions that ultimately lead the student
to an ?A-ha!? moment. Numerous studies have il-
lustrated the effectiveness of Socratic-style tutoring
(VanLehn et al, 2007; Rose et al, 2001; Collins and
Stevens, 1982); consequently recreating the behav-
ior on a computer has long been a goal of research
in Intelligent Tutoring Systems (ITS). Recent suc-
cesses have shown the efficacy of conversational ITS
(Graesser et al, 2005; Litman and Silliman, 2004;
Ward et al, 2011b), however these systems are still
not as effective as human tutors, and much improve-
ment is needed before they can truly claim to be So-
cratic. Furthermore, development and tuning of tu-
torial dialogue behavior requires significant human
effort.
While our overarching goal is to improve ITS
by automatically learning tutorial dialogue strategies
directly from expert tutor behavior, we focus on the
crucial subtask of selecting follow-up questions. Al-
though asking questions is only a subset of the over-
all tutoring process, it is still a complex process that
requires understanding of the dialogue state, the stu-
dent?s ability, and the learning goals.
This work frames question selection as a task of
scoring and ranking candidate questions for a spe-
cific point in the tutorial dialogue. Since dialogue
is a dynamic process with multiple correct possibil-
ities, we do not restrict ourselves only to the moves
and questions found in a corpus of transcripts. In-
stead we posit ?What if we had a fully automatic
question generation system?? and subsequently use
candidate questions hand-authored for each dialogue
context. To explore the mechanisms involved in
ranking follow-up questions against one other, we
pair these questions with judgments of quality from
expert human tutors and extract surface form and
dialogue-based features to train machine learning
classification models to rank the appropriateness of
questions for specific points in a dialogue.
Our results show promise with our best question
1
ranking models exhibiting performance on par with
expert human tutors. Furthermore these experiments
demonstrate the utility and importance of rich dia-
logue move annotation for modeling decision mak-
ing in conversation and tutoring.
2 Background and Related Works
Learning tutorial dialogue policies from corpora is
a growing area of research in natural language pro-
cessing and intelligent tutoring systems. Past studies
have made use of hidden Markov models (Boyer et
al., 2009a) and reinforcement learning (Chi et al,
2010; Chi et al, 2009; Chi et al, 2008) to discover
tutoring strategies. However, these approaches are
typically optimized to maximize learning gains, and
are not necessarily focused on replicating human tu-
tor behavior. Other work has explored specific fac-
tors in questioning such as when to ask ?why? ques-
tions (Rose et al, 2003), provide hints (Tsovaltzi
and Matheson, 2001), or insert discourse markers
(Kim et al, 2000).
There is also an expanding body of work that ap-
plies ranking algorithms toward the task of ques-
tion generation (QG) using approaches such as over-
generation-and-ranking (Heilman and Smith, 2010),
language model ranking (Yao, 2010), and heuristics-
based ranking (Agarwal and Mannem, 2011). While
the focus of these efforts centers on issues of gram-
maticality, fluency, and content selection for auto-
matic creation of standalone questions, we move to
the higher level task of choosing context appropri-
ate questions. Our work merges aspects of these
QG approaches with the sentence planning tradi-
tion from natural language generation (Walker et al,
2001; Rambow et al, 2001). In sentence planning
the goal is to select lexico-structural resources that
encode communicative action. Rather than select-
ing representations, we use them directly as part of
the feature space for learning functions to rank the
questions? actual surface form realization. To our
knowledge there has been no research in ranking the
quality and suitability of questions within a tutorial
dialogue context.
Because questioning tactics depend heavily on the
curriculum and choice of pedagogy, we ground our
investigations within the context of the My Science
Tutor (MyST) intelligent tutoring system (Ward et
al., 2011b), a conversational virtual tutor designed
to improve science learning and understanding for
students in grades 3-5 (ages 8-11). Students using
MyST investigate and discuss science through nat-
ural spoken dialogues and multimedia interactions
with a virtual tutor named Marni. The MyST dia-
logue design and tutoring style is based on a ped-
agogy called Questioning the Author (QtA) (Beck
et al, 1996) which emphasizes open-ended ques-
tions and keying in on student language to promote
self-explanation of concepts, and its curriculum is
based on the Full Option Science System (FOSS) 1
a proven system for inquiry based learning.
3 Data Collection
3.1 MyST Logfiles and Transcripts
For these experiments, we use MyST transcripts col-
lected in a Wizard-of-Oz (WoZ) condition with a hu-
man tutor inserted into the interaction loop. Project
tutors trained in both QtA and in the tutorial sub-
ject matter served as the wizards. During a ses-
sion tutors were responsible for accepting, overrid-
ing, and/or authoring system actions. Tutor wizards
were also responsible for setting the current dialogue
frame to indicate which of the learning goals was
currently in focus. Students talked to MyST via mi-
crophone while MyST communicates using Text-to-
Speech (TTS) in the WoZ setting. A typical MyST
session revolves around a single FOSS lesson and
lasts approximately 15 minutes. To obtain a dia-
logue transcript, tutor moves are taken directly from
the system logfile, while student speech is manu-
ally transcribed from audio. In addition to the di-
alogue text, MyST records additional information
such as timestamps and the current dialogue frame
(i.e. learning goal). In total we make use of tran-
scripts from 122 WoZ dialogues covering 10 units
on magnetism and electricity and 2 in measurement
and standards.
3.2 Dialogue Annotation
Lesson-independent analysis of dialogue requires
a level of abstraction that reduces a dialogue to
its underlying actions and intentions. To address
this need we use the Dialogue Schema Unifying
Speech and Semantics (DISCUSS) (Becker et al,
1http://www.fossweb.com
2
2011), a multidimensional dialogue move taxon-
omy that captures both the pragmatic and seman-
tic interpretation of an utterance. Instead of us-
ing one label, a DISCUSS move is a tuple com-
posed of three dimensions: Dialogue Act, Rhetor-
ical Form, Predicate Type. Together these labels
account for the action, function, and content of an
utterance. This scheme draws from past work in
task-oriented dialogue acts (Bunt, 2009; Core and
Allen, 1997), tutorial act taxonomies (Pilkington,
1999; Tsovaltzi and Karagjosova, 2004; Buckley
and Wolska, 2008; Boyer et al, 2009b) discourse
relations (Mann and Thompson, 1986) and question
taxonomies (Graesser and Person, 1994; Nielsen et
al., 2008).
Dialogue Act (22 tags): The dialogue act dimen-
sion is the top-level dimension in DISCUSS, and its
values govern the possible values for the other di-
mensions. Though the DISCUSS dialogue act layer
seeks to replicate the learnings from other well-
established taxonomies like DIT++ (Bunt, 2009) or
DAMSL (Core and Allen, 1997) wherever possible,
the QtA style of pedagogy driving our tutoring ses-
sions dictated the addition of two tutorial specific
acts: marking and revoicing. A mark act highlights
key words from the student?s speech to draw atten-
tion to a particular term or concept. Like with mark-
ing, revoicing keys in on student language, but in-
stead of highlighting specific words, a revoice act
will summarize or refine the student?s language to
bring clarity to a concept.
Rhetorical Form (22 tags): Although the dia-
logue act is useful for identifying the speaker?s in-
tent, it gives no indication of how the speaker is ad-
vancing the conversation. The rhetorical form re-
fines the dialogue act by providing a link to its sur-
face form realization. Consider the questions ?What
is the battery doing?? and ?Which one is the bat-
tery??. They would both be labeled with Ask dia-
logue acts, but they elicit two very different kinds
of responses. The former, which elicits some form
of description, would be labeled with a Describe
rhetorical form, while the latter is seeking to Iden-
tify an object. Similarly an Assert act from a tutor
could be coupled with a Describe rhetorical form to
introduce new information or with a Recap to recon-
vey a major point.
Predicate Type (19 tags): Beyond knowing the
Reliability Metric DA RF PT
Cohen?s Kappa 0.75 0.72 0.63
Exact Agreement 0.80 0.66 0.56
Partial Agreement 0.89 0.77 0.68
Table 1: Inter-annotator agreement for DISCUSS types
(DA=Dialogue Act, RF=Rhetorical Form, PT=Predicate
Type)
propositional content of an utterance, it is useful to
know how the entities and predicates in a response
relate to one another. A student may mention several
keywords that are semantically similar to the learn-
ing goals, but it is important for a tutor to recognize
whether the student?s language provides a deeper de-
scription of some phenomena or if it is simply a su-
perficial observation. The Predicate Type aims to
categorize the semantic relationships a student may
talk about; whether it is a Procedure, a Function, a
Causal Relation, or some other predicate type.
3.2.1 Annotation
All transcripts used in this experiment have been
annotated with DISCUSS labels at the turn level. A
reliability study using 15% of the transcripts was
conducted to assess inter-rater agreement of DIS-
CUSS tagging. This consisted of 18 doubly anno-
tated transcripts comprised of 828 dialogue utter-
ances.
To assess inter-rater reliability we use Cohen?s
Kappa (?) (Carletta, 1996). Because DISCUSS per-
mits multiple labels per instance, we compute a ?
value for each label and provide a mean for each
DISCUSS dimension. To get an additional sense of
agreement, we use two other metrics: exact agree-
ment and partial agreement. For each of these met-
rics, we treat each annotators? annotations as a per
class bag-of-labels. For exact agreement, each an-
notators? set of labels must match exactly to receive
credit. Partial agreement is defined as the number
of intersecting labels divided by the total number
of unique labels. Together these statistics help to
bound the reliability of the DISCUSS annotation.
Table 1 lists all three metrics broken down by DIS-
CUSS dimension. The ? values show fair agreement
for the dialogue act and rhetorical form dimensions,
whereas the predicate type shows more moderate
agreement. This difference reflects the relative diffi-
3
culty in labeling each dimension, and the agreement
as a whole illustrates the open-endedness of the task.
3.3 Question Authoring
While the long-term plan for this work is to inte-
grate fully automatic question generation into a tu-
toring system, for this study we opted to use manu-
ally authored questions. This allows us to remain
focused on learning to identify context appropri-
ate questions rather than confounding our experi-
ments with issues of question grammaticality and
well-formedness. Even though using multiple au-
thors would provide greater diversity of questions,
to avoid repeated effort and to maintain consistency
in authoring we trained a single question author
in both the FOSS material and MyST QtA tech-
niques. Although he was free to author any ques-
tion he found appropriate, our guidelines primar-
ily emphasized authoring by making permutations
aligned with DISCUSS dimensions while also per-
mitting the author to incorporate changes in word-
ing, learning-goal content, and tutoring tactics. For
example, we taught him to consider how QtA moves
such as Revoicing, Marking, or Recapping could al-
ter otherwise similar questions. To minimize the risk
of rater bias, we explicitly told our author to avoid
using positive feedback expressions such as ?Good
job!? or ?Great!?. Table 2 illustrates how the com-
binations of DISCUSS labels, QtA tactics, and dia-
logue context drives the question generation process.
To simulate the conditions available to both the
human WoZ and computer MyST tutors, the author
was presented with the entire dialogue history pre-
ceding the decision point, the current dialogue frame
(learning goal), and any visuals that may be on-
screen. Question authoring contexts were manually
selected to capture points where students provided
responses to tutor questions. This eliminated the
need to account for other dialogue behavior such as
greetings, closings, or meta-behavior, and allowed
us to focus on follow-up style questions. Because
these question authoring contexts came from actual
tutorial dialogues, we also extracted the original turn
provided by the tutor, and we filtered out turns that
did not contain questions related to the lesson con-
tent. Our corpus has 205 question authoring contexts
comprised of 1025 manually authored questions and
131 questions extracted from the original transcript
yielding 1156 questions in total.
3.4 Ratings Collection
To rate questions, we enlisted the help of four tu-
tors who had previously served as project tutors and
wizards. The raters were presented with much of
the same information used during question author-
ing. The interface included the entire dialogue his-
tory preceding the question decision point and a list
of up to 6 candidate questions (5 manually authored,
1 taken from the original transcript if applicable). To
give a more complete tutoring context, raters also
had access to the lessons? learning goals and the in-
teractive visuals used by MyST.
Previous studies in rating questions (Becker et al,
2009) have found poor inter-rater agreement when
rating questions in isolation. To decrease the task?s
difficulty we instead ask raters to simultaneously
score all candidate questions. Because we did not
want to bias raters, we did not specify specific cri-
teria for question quality. Instead we instructed the
raters to consider the question?s role in assisting stu-
dent understanding of the learning goals and to think
about factors such as tutorial pacing, context appro-
priateness, and content. Scores were collected us-
ing an ordinal 10-point scale ranging from 1 (low-
est/worst) to 10 (highest/best).
Each set of questions was rated by at least three
tutors, and rater assignments were selected to ensure
raters never score questions from sessions they tu-
tored themselves. In total we collected ratings for
1156 question representing a total of 205 question
contexts distributed across 30 transcripts.
3.4.1 Rater Agreement
Because these judgments are subjective, a key
challenge in this work centers on understanding to
what degree the tutors agree with one another. Since
our goal is to rank questions and not to score ques-
tions, we convert each tutors scores for a given con-
text into a rank-ordered list. To compute inter-
rater agreement in ranking, we use Kendall?s-Tau
(? ) rank correlation coefficient. This measure is a
non-parametric statistic that quantifies the similarity
in orderings of data, and it is closely tied to AUC,
the area under the receiver operating characteristics
(ROC) curve. Though Kendall?s-? can vary from -1
to 1, its value is highly task dependent, and it is typ-
4
. . .
T: Tell me more about what is happening with the electricity in a complete circuit.
S: Well the battery sends all the electricity in a circuit to the motor so the motor starts to go.
Candidate Question Frame Element DISCUSS
Q1 Roll over the switch and then in your own
words, tell me again what a complete or
closed circuit is all about.
Same Same Direct/Task/Visual
Ask/Describe/Configuration
Q2 How is this circuit setup? Is it open or closed? Same Same Ask/Select/Configuration
Q3 To summarize, a closed circuit allows the
electricity to flow and the motor to spin. Now
in this circuit, we have a new component. The
switch. What is the switch all about?
Diff Diff Assert/Recap/Proposition
Direct/Task/Visual
Ask/Describe/Function
Q4 You said something about the motor spinning
in a complete circuit. Tell me more about that.
Same Same Revoice/None/None
Ask/Elaborate/CausalRelation
Table 2: Example dialogue context snippet and a collection of candidate questions. The frame, element, and DISCUSS
columns show how the questions vary from one another.
ically lower when the range of possible choices is
narrow as it is in this task. To get a single score we
average ? values across all sets of questions (con-
texts) and all pairs of raters. The mean value for all
pairs of raters and contexts is ? = 0.1478. The inter-
rater statistics are shown in table 3. While inter-rater
agreement is fairly modest, we do see lots of vari-
ation between different pairs of tutors. Addition-
ally, we found that a pair of raters agreed on the top
rated question 33% of the time. This suggests that
despite their common training and experience, the
raters may be using different criteria in rating.
To assess the tutors? internal consistency, we had
each tutor re-rate 60 sets of questions approximately
two months after their first trial, and we computed
self-agreement Kendall?s-? values using the method
above. These statistics are listed in the bottom row
of table 3. In contrast with the inter-rater agreement,
self-agreement is much more consistent giving fur-
ther evidence for a difference in criteria. Together
self and inter-rater agreement help bound expected
system performance in ranking.
4 Automatic Ranking
Because we are more interested in learning to pre-
dict which questions are more suitable for a given
tutoring scenario than we are in assigning specific
scores to questions, we approach the task of ques-
tion selection as a ranking task. To create a gold-
rater A rater B rater C rater D
rater A X 0.2590 0.1418 0.0075
rater B 0.2590 X 0.1217 0.2370
rater C 0.1418 0.1217 X 0.0540
rater D 0.0075 0.2370 0.0540 X
mean 0.1361 0.2059 0.1058 0.0995
self 0.4802 0.4022 0.2327 0.3531
Table 3: Inter-rater rank agreement (Kendall?s-? ). The
bottom row is the self-agreement for contexts they rated
in two separate trials.
standard for training and evaluation we first need to
convert the collective ratings for a set of questions
into a rank-ordered list. While the most straight-
forward way to make this conversion is to average
the ratings for each item, this approach assumes all
raters operate on the same scale. Furthermore, a sin-
gle score does not account for how a question re-
lates to other candidate questions. Instead we create
a single rank-order by tabulating pairwise wins for
all pairs of questions qi, qj , (i 6= j) within a given
dialogue context C. If rating(qi) > rating(qj),
questions qi receives a win. This is summed across
all raters for the context. The question(s) with the
most wins has rank 1. Questions with an equal num-
ber of wins are considered tied and are given the av-
erage ranking of their ordinal positions. For exam-
ple if two questions are tied for second place, they
5
are each assigned a ranking of 2.5.
Using this rank-ordering we then train a pairwise
classifier to learn a preferences function (Cohen et
al., 1998) that determines if one question has a bet-
ter rank than another. For each question qi within a
contextC, we construct a vector of features ?i. For a
pair of questions qi and qj , we then create a new vec-
tor using the difference of features: ?(qi, qj , C) =
?i ? ?j . For training, if rank(qi) < rank(qj), the
classification is positive otherwise it is negative. To
account for the possibility of ties, and to make the
difference measure appear symmetric, we train both
combinations (qi, qj) and (qj , qi). During decoding,
we run the trained classifier on all pairs and tabulate
wins using the approach described above.
For our experiments we train pairwise classi-
fiers using Mallet?s Maximum Entropy (McCallum,
2002) and SVMLight?s Support Vector Machines
models (Joachims, 1999). We also use SVMRank
(Joachims, 1999), which performs the same max-
imum margin separation as SVMLight, but uses
Kendall?s-? as a loss function to optimize for rank
ordering. We run SVMRank with a linear kernel
and model parameters of c = 2.0 and  = 0.0156.
For MaxEnt, we use Mallet?s default model param-
eters. Training and evaluation are carried out us-
ing 10-fold cross validation (3 transcripts per fold,
approximately 7 dialogue contexts per transcript).
Folds are partitioned by FOSS unit, to ensure train-
ing and evaluation are on different lessons. To ex-
plore the impact of DISCUSS representations on this
question ranking task, we train and evaluate models
by incrementally adding additional information ex-
tracted from the DISCUSS annotation.
4.1 Features
When designing features for this task, we wanted to
capture the factors that may play a role in the tutor?s
decision making process during question selection.
When rating, scorers may consider factors such as
the question?s surface form, lesson relevance, con-
textual relevance. The subsections below detail the
motivations and intuitions behind these factors.
4.1.1 Surface Form Features
When presented with a list of questions, a rater
likely bases the decision on his or her initial reaction
to the questions? wording. In some cases, wording
may supercede any other decisions regarding edu-
cational value or dialogue cohesiveness. Question
verbosity is captured by the number of words in the
question feature. Analysis of rater comments also
suggested that preferences are often tied to the ques-
tion?s form and structure. A rough measure of form
comes from the Wh-word features to mark the pres-
ence of the following question words: who, what,
why, where, when, which, and how. Additionally we
use the bag-of-part-of-speech-tags (POS) features to
provide another aspect of the question?s structure.
4.1.2 Lexical Similarity Features
Past work (Ward et al, 2011a) has shown that en-
trainment, the process of automatic alignment be-
tween dialogue partners, is a useful predictor of
learning and is a key factor in facilitating a success-
ful conversation. For question selection, we hypoth-
esize that successful tutors ask questions that dis-
play some degree of semantic entrainment with stu-
dent utterances. In MyST-based tutoring, dialogue
actions are driven by the goal of eliciting student re-
sponses that address the learning goals for the les-
son. Consequently, choosing an appropriate ques-
tion may depend on how closely student responses
align with the learning goals. To model both en-
trainment and lexical similarity we extract features
for unigram and bigram overlap of words, word-
lemmas, and part-of-speech tags between the pairs
below.
? The candidate question and the student?s last
utterance
? The candidate question and the last tutor?s ut-
terance
? The candidate question and the text of the cur-
rent learning goal
? The candidate question and the text of the other
learning goals
Example learning goals for a lesson on circuits are
provided in table 4. The current learning goal is sim-
ply the learning goal in focus at the point of question
asking according to the MyST logfile. Other learn-
ing goals are all other goals for the lesson. Using
the example from the table, if goal 2 is the current
learning goal, then goals 1 and 3 are the other goals.
6
Goal 1: Wires carry electricity and can connect
components
Goal 2: Bulb receives electricity and transforms
electricity into heat
Goal 3: A circuit provides a pathway for energy
to flow
Table 4: Example learning goals
4.1.3 DISCUSS Features
The lexical and surface form features provide
some cues about the content of the question, but
they do not account for the action or intent in tutor-
ing. The DISCUSS annotation allows us to bridge
between the question?s semantics and pragmatically
and focus on what differentiates one question from
another. Basic DISCUSS features include bags of
Dialogue Acts (DA), Rhetorical Forms (RF), and
Predicate types (PT) found in the question?s DIS-
CUSS annotation. We capture the question?s dia-
logue cohesiveness with binary features indicating
whether or not the question?s RF and PT match those
found in the previous student and tutor turns.
4.1.4 Contextualized DISCUSS Features
In tutoring, follow-up questions are licensed by
the questions that precede them. For example a tutor
may be less likely to ask how an object functions un-
til after the object has first been identified by the stu-
dent. Along a different dimension, a tutor?s line of
questioning may change to match a student?s under-
standing of the material. Struggling students may re-
quire additional opportunities to explain themselves,
while advanced students may benefit more from a
more rapid pace of instruction.
We model the conditional relevance of moves
by computing dialogue act transition probabilities
from our corpus of DISCUSS annotated tutorial di-
alogues. Although DISCUSS allows multiple tags
per dialogue turn, we simplify probability calcula-
tions by treating each DISCUSS tuple as a separate
event, and tallying all pairs of turn-turn labels. A
DISCUSS tuple consists of a Dialogue Act (DA),
Rhetorical Form (RF), and Predicate Type (PT),
and we use different subsets of the tuple to com-
pute the transition probabilities listed in equations 1-
3. All probabilities are computed using Laplace-
smoothing. When extracting features, we sum the
log of the probabilities for each DISCUSS label
present in the question.
MyST models dialogue as a sequence of seman-
tic frames which correspond to specific learning
goals. For natural language understanding, MyST
uses Phoenix semantic grammars (Ward, 1994) to
identify which elements within these frames have
been filled. To account for student progress in ques-
tion asking, we compute the conditional probabil-
ity of a DISCUSS label given the percentage of el-
ements filled in the current dialogue frame (equa-
tion 4). This progress percentage is discretized into
bins of 0-25%, 25-50%, 50-75%, and 75-100%.
p(DA,RF, PTquestion|DA,RF, PTstud. turn) (1)
p(DA,RFquestion|DA,RFstudent turn) (2)
p(PTquestion|PTstudent turn) (3)
p(DA,RF, PTques.|% elements filled) (4)
4.2 Evaluation
To evaluate our systems? performance in ranking,
we use two measures commonly used in information
retrieval: the Mean Kendall?s-? measure described
in section 3.4.1 and Mean Reciprocal Rank (MRR).
MRR is the average of the multiplicative inverse of
the rank of the highest ranking question across all
contexts. To account for ties we use the Tau-b vari-
ant of Kendall?s-? , and for MRR we compute re-
ciprocal rank by averaging the system rankings for
all of the questions tied for first. To obtain a gold-
standard ranking for comparison, we combine indi-
vidual raters? ratings using the approached described
in section 4.
5 Results and Discussion
We trained several models to investigate how differ-
ent feature classes influence overall performance in
ranking. The results for these experiments are listed
in Table 5. Because we found comparable perfor-
mance between MaxEnt and SVMLight, we only
report results for MaxEnt and SVMRank models.
In addition to MRR and Kendall?s-? , we list the
number of concordances and discordances in pair-
wise classification to give the reader another sense
of the accuracy associated with rank agreement.
Random Baseline: On average, assigning ran-
dom ranks will yield mean ?=0 and MRR=0.408.
7
Model Features Mean Num. Num. Pairwise MRR
Kendall?s-? Concord. Discord. Accuracy
MaxEnt CONTEXT+DA+PT+MATCH+POS- 0.211 1560 974 0.616 0.516
SVMRank CONTEXT+DA+PT+MATCH+POS- 0.190 1725 1154 0.599 0.555
MaxEnt CONTEXT+DA+RF+PT+MATCH+POS- 0.185 1529 1014 0.601 0.512
MaxEnt DA+RF+PT+MATCH+POS- 0.179 1510 1009 0.599 0.503
MaxEnt DA+RF+PT+MATCH+ 0.163 1506 1044 0.591 0.485
MaxEnt DA+RF+PT+ 0.147 1500 1075 0.583 0.480
MaxEnt DA+RF+ 0.130 1458 1082 0.574 0.476
MaxEnt DA+ 0.120 1417 1076 0.568 0.458
SVMRank Baseline 0.108 1601 1278 0.556 0.473
MaxEnt Baseline 0.105 1410 1115 0.558 0.448
Table 5: System scores by feature set and and machine learning model. Presence or absence of specific features is
denoted with a ?+? or ?-? otherwise the label refers to a set of features. The Baseline features consist of the Surface Form
and Lexical Similarity features described in sections 4.1.1 and 4.1.2. POS are the bag-of-POS surface form features.
DA, RF, and PT refer to the DISCUSS presence features for the Dialogue Act, Rhetorical Form, and Predicate Type
dimensions described in section 4.1.3. MATCH refers specifically to the RF and PT match features. CONTEXT
refers to the Contextualized DISCUSS features described in section 4.1.4. The best scores for each column appear in
boldface.
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.00
10
20
30
40
50
Freq
uen
cy
?mean=0.211
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.0Kendall's Tau(?) Range
0
10
20
30
40
50
Freq
uen
cy
?mean=0.105
Figure 1: Distribution of per-context Kendall?s-? values
for the top-scoring system (top), and the baseline system
(bottom).
Baseline System: Our baseline system used all
of the surface form and lexical similarity features
described above. This set of features achieves the
highest rank agreement (? = 0.105) using max-
imum entropy and the highest MRR (0.473) with
SVMRank . This improvement over the random
baseline suggests there is a correlation between a
question?s ranking and its surface form.
DISCUSS System: Table 5 shows system per-
formance steadily improves as additional DISCUSS
features are included in the model. When us-
1 2 3 4 5 6 70.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=1.80
1 2 3 4 5 6 7Mean System Rank
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=2.11
Figure 2: Distribution of per-context system ranks for the
highest rated question for the top-scoring system (top),
and the baseline system (bottom). These ranks are the
inverse of the reciprocal rank used to calculate MRR.
ing DISCUSS features, removing the part-of-speech
features gives an additional bump in performance
suggesting that there is an overlap in information
between DISCUSS representations and POS tags.
Finally, adding contextualized DISCUSS features
pushes our ranking models to their highest level
of agreement with ? = 0.211 using MaxEnt and
MRR=0.555 using SVMRank . Inspection of the
MRR values shows that without taking into account
the possibility of ties the baseline system selects
8
the top-ranked question in 44/205 (21.4%) contexts.
While the system with the best MRR score, correctly
chooses the top-ranked question in 71/205 (34.6%)
contexts ? a rate comparable to how often a pair of
raters agreed on the number-one item (33.4%).
Application of the Wilcoxon signed-rank test
shows the DISCUSS system exhibits statistically
significant improvement over the baseline system in
its distribution of Kendall?s-? values (n = 205, z =
7350, p < 0.001) and distribution of reciprocal
ranks (n = 205, z = 3739, p < 0.001). Figures 1
and 2 give visual confirmation of this improvement,
and highlight the overall reduction in negative ? val-
ues as well as the greater-than-50% increase in like-
lihood of selecting the best question first.
To get another perspective on system perfor-
mance, we evaluated our human raters on the gold-
standard rankings from the subset of questions used
for assessing internal agreement. This yielded a
mean ? between 0.2589 and 0.3619. If we remove
ratings so that the gold-standard does not include the
rater under evaluation, tutor performance drops to
a range of 0.1523 to 0.2432, which is roughly cen-
tered around the agreement exhibited by our best-
performing system.
Looking at the impact of learning algorithms
we see that SVMRank tends to perform better on
MRR while the pairwise maximum entropy mod-
els yield higher ? ?s. One possible explanation for
this discrepancy may stem from the ranking algo-
rithms? different treatment of ties. The pairwise
model permits ties, whereas the scores produced by
SVMRank produce a strict order. Without ties, it is
difficult to exactly match the raters? orderings which
had numerous ties, which can in turn produce an
overall higher number of concordances and discor-
dances than the pairwise classification model.
6 Conclusions and Future Work
We have introduced a framework for learning and
evaluating models for ranking and selecting ques-
tions for a given point in a tutorial dialogue. Fur-
thermore these experiments show that it is feasible
to learn this behavior by coupling predefined ques-
tions with ratings from trained tutors. Supplement-
ing our baseline surface form and lexical similarity
features with additional features extracted from the
dialogue context and DISCUSS dialogue act anno-
tation improves system performance in ranking to a
level on par with expert human tutors. These results
illustrate how question asking depends not only on
the form of the question but also on the underlying
dialogue action, function and content.
In the near future we plan to train models on indi-
vidual tutors to investigate which factors drive in-
dividual preferences in question asking. We also
plan to characterize system performance using auto-
matically labeled DISCUSS annotation. Lastly, we
feel these results provide a natural starting point to
explore automatic generation of questions from the
DISCUSS dialogue move representation.
Acknowledgments
This work was supported by grants from the
NSF (DRL-0733322, DRL-0733323), the IES
(R3053070434) and the DARPA GALE program
(Contract No. HR0011-06-C-0022, a supplement
for VerbNet attached to the subcontract from the
BBN-AGILE Team). Any findings, recommenda-
tions, or conclusions are those of the author and do
not necessarily represent the views of NSF, IES, or
DARPA.
References
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic gap-fill question generation from text books au-
tomatic gap-fill question generation from text books
automatic gap-fill questions from text books. In Pro-
ceedings of the Sixth Workshop on Innovative Use of
NLP for Building Educational Applications.
I. L. Beck, M. G. McKeown, J. Worthy, C. A. San-
dora, and L. Kucan. 1996. Questioning the au-
thor: A year-long classroom implementation to engage
students with text. The Elementary School Journal,
96(4):387?416.
L. Becker, R. D. Nielsen, and W. Ward. 2009. What a
pilot study says about running a question generation
challenge. In Proceedings of the Second Workshop on
Question Generation, Brighton, England, July.
L. Becker, W. Ward, S. van Vuuren, and M. Palmer. 2011.
Discuss: A dialogue move taxonomy layered over se-
mantic representations. In In Proceedings of the In-
ternational Conference on Computational Semantics
(IWCS) 2011, Oxford, England, January 12-14.
K.E. Boyer, E.Y. Ha, M. Wallis, R. Phillips, M.A. Vouk,
and J.C. Lester. 2009a. Discovering tutorial dialogue
9
strategies with hidden markov models. In Proceed-
ings of the 14th International Conference on Artificial
Intelligence in Education (AIED ?09), pages 141?148,
Brighton, U.K.
K.E. Boyer, W.J. Lahti, R. Phillips, M. D. Wallis, M. A.
Vouk, and J. C. Lester. 2009b. An empirically derived
question taxonomy for task-oriented tutorial dialogue.
In Proceedings of the Second Workshop on Question
Generation, pages 9?16, Brighton, U.K.
M. Buckley and M. Wolska. 2008. A classification of
dialogue actions in tutorial dialogue. In Proceedings
of COLING 2008, pages 73?80. ACL.
H. C. Bunt. 2009. The DIT++ taxonomy for functional
dialogue markup. In Proc. EDAML 2009.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):pp. 249?254.
M. Chi, P. Jordan, K. VanLehn, and M. Hall. 2008. Re-
inforcement learning-based feature selection for devel-
oping pedagogically effective tutorial dialogue tactics.
In Ryan S. Baker, Tiffany Barnes, and Joseph Becker,
editors, Proceedings of the 1st International Confer-
ence on Educational Data Mining, pages pp258?265.
M. Chi, P. W. Jordan, K. VanLehn, and D. J. Litman.
2009. To elicit or to tell: Does it matter? In Artifi-
cial Intelligence in Education, pages 197?204.
M. Chi, K. VanLehn, and D. Litman. 2010. Do micro-
level tutorial decisions matter: Applying reinforce-
ment learning to induce do micro-level tutorial deci-
sions matter. In Vincent Aleven, Judy Kay, and Jack
Mostow, editors, Preceedings of the 10th Internation
Confernce on Intelligent Tutoring Systems (ITS 2010).
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1998. Learning to order things. In Advances
in Neural Information Processing Systems 10 (NIPS
1998).
A. Collins and A. Stevens. 1982. Goals and methods for
inquiry teachers. Advances in Instructional Psychol-
ogy, 2.
M. G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In AAAI Fall Symposium,
pages 28?35.
A.C. Graesser and N.K. Person. 1994. Question ask-
ing during tutoring. American Educational Research
Journal, 31:104?137.
A.C. Graesser, P. Chipman, B.C Haynes, and A. Olney.
2005. Autotutor: An intelligent tutoring system with
mixed-initiative dialogue. IEEE Transactions in Edu-
cation, 48:612?618.
M. Heilman and N. A. Smith. 2010. Good question! sta-
tistical ranking for question generation. In Proceed-
ings of NAACL/HLT 2010.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
J.H. Kim, M. Glass, R. Freedman, and M.W. Evens.
2000. Learning the use of discourse markers in tuto-
rial dialogue learning the use of discourse markers in
tutorial dialogue. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society.
D. Litman and S. Silliman. 2004. Itspoke: An intel-
ligent tutoring spoken dialogue system. In Compan-
ion Proceedings of the Human Language Technology
Conference: 4th Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
W.C. Mann and S.A Thompson. 1986. Rhetorical struc-
ture theory: Description and construction of text struc-
tures. In In Proceedings of the Third International
Workshop on Text Generation, August.
A. K. McCallum, 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
R. D. Nielsen, J. Buckingham, G. Knoll, B. Marsh, and
L. Palen. 2008. A taxonomy of questions for ques-
tion generation. In Proceedings of the Workshop on
the Question Generation Shared Task and Evaluation
Challenge, September.
R.M. Pilkington. 1999. Analysing educational dis-
course: The discount scheme. Technical Report 99/2,
Computer Based Learning Unit, University of Leeds.
Owen Rambow, Monica Rogati, and Marilyn A. Walker.
2001. Evaluating a trainable sentence planner for a
spoken dialogue system evaluating a trainable sen-
tence planner for a spoken dialogue system. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL 2001).
C.P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. A comparative evalu-
ation of socratic versus didactic tutoring. In Proceed-
ings of Cognitive Sciences Society.
C.P. Rose, D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of Artificial
Intelligence in Education (AIED 2003).
D. Tsovaltzi and E. Karagjosova. 2004. A view on dia-
logue move taxonomies for tutorial dialogues. In Pro-
ceedings of SIGDIAL 2004, pages 35?38. ACL.
D. Tsovaltzi and C. Matheson. 2001. Formalising hint-
ing in tutorial dialogues. In In EDILOG: 6th workshop
on the semantics and pragmatics of dialogue, pages
185?192.
K. VanLehn, A.C. Graesser, G.T. Jackson, P. Jordan,
A. Olney, and C.P. Rose. 2007. When are tutorial
dialogues more effective than reading? Cognitive Sci-
ence, 31(1):3?62.
10
Marilyn A. Walker, Owen Rambow, and Monica Rogati.
2001. SPOT: A trainable sentence planner. In Pro-
ceedings of the North American Meeting of the Asso-
ciation for Computational Linguistics (NAACL).
A. Ward, D. Litman, and M. Eskenazi. 2011a. Predict-
ing change in student motivation by measuring cohe-
sion between predicting change in student motivation
by measuring cohesion between tutor and student. In
Proceedings of the Sixth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
136?141.
W. Ward, R. Cole, D. Bolan?os, C. Buchenroth-Martin,
E. Svirsky, S. van Vuuren, T. Weston, J. Zheng, and
L. Becker. 2011b. My science tutor: A conversa-
tional multi-media virtual tutor for elementary school
science. ACM Transactions on Speech and Language
Processing (TSLP), 7(4), August.
W. Ward. 1994. Extracting information from sponta-
neous speech. In Proceedings of the International
Conference on Speech and Language Processing (IC-
SLP).
Xuchen Yao. 2010. Question generation with minimal
recursion semantics. Master?s thesis, Saarland Uni-
versity.
11
