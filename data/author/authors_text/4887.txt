BioNLP 2007: Biological, translational, and clinical language processing, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Unsupervised Method for Extracting Domain-specific Affixes in
Biological Literature
Haibin Liu Christian Blouin Vlado Kes?elj
Faculty of Computer Science, Dalhousie University, Canada, {haibin,cblouin,vlado}@cs.dal.ca
Abstract
We propose an unsupervised method to au-
tomatically extract domain-specific prefixes
and suffixes from biological corpora based
on the use of PATRICIA tree. The method is
evaluated by integrating the extracted affixes
into an existing learning-based biological
term annotation system. The system based
on our method achieves comparable experi-
mental results to the original system in locat-
ing biological terms and exact term match-
ing annotation. However, our method im-
proves the system efficiency by significantly
reducing the feature set size. Additionally,
the method achieves a better performance
with a small training data set. Since the af-
fix extraction process is unsupervised, it is
assumed that the method can be generalized
to extract domain-specific affixes from other
domains, thus assisting in domain-specific
concept recognition.
1 Introduction
Biological term annotation is a preparatory step in
information retrieval in biological science. A bi-
ological term is generally defined as any technical
term related to the biological domain. Consider-
ing term structure, there are two types of biologi-
cal terms: single word terms and multi-word terms.
Many systems (Fukuda et al, 1998; Franzn et al,
2002) have been proposed to annotate biological
terms based on different methodologies in which de-
termining term boundaries is usually the first task. It
has been demonstrated (Jiampojamarn et al, 2005a),
however, that accurately locating term boundaries
is difficult. This is so because of the ambiguity of
terms, and the peculiarity of the language used in
biological literature.
(Jiampojamarn et al, 2005b) proposed an auto-
matic biological term annotation system (ABTA)
which applies supervised learning methods to an-
notate biological terms in the biological litera-
ture. Given unstructured texts in biological research,
the annotation system first locates biological terms
based on five word position classes, ?Start?, ?Mid-
dle?, ?End?, ?Single? and ?Non-relevant?. There-
fore, multi-word biological terms should be in a con-
sistent sequence of classes ?Start (Middle)* End?
while single word terms will be indicated by the
class ?Single?. Word n-grams (Cavnar and Tren-
kle, 1994) are used to define each input sentence
into classification instances. For each element in
an n-gram, the system extracts feature attributes as
input for creating the classification model. The ex-
tracted feature attributes include word feature pat-
terns(e.g., Greek letters, uppercase letters, digits and
other symbols), part-of-speech (POS) tag informa-
tion, prefix and suffix characters. Without using
other specific domain resources, the system achieves
comparable results to some other state-of-the-art
systems (Finkel et al, 2004; Settles, 2004) which
resort to external knowledge, such as protein dictio-
naries. It has been demonstrated (Jiampojamarn et
al., 2005b) that the part-of-speech tag information
is the most effective attribute in aiding the system
to annotate biological terms because most biologi-
cal terms are partial noun phrases.
The ABTA system learns the affix feature by
recording only the first and the last n characters (e.g.,
n = 3) of each word in classification instances, and
the authors claimed that the n characters could pro-
vide enough affix information for the term annota-
tion task. Instead of using a certain number of char-
acters to provide affix information, however, it is
more likely that a specific list of typically used pre-
fixes and suffixes of biological words would provide
more accurate information to classifying some bio-
logical terms and boundaries. We hypothesize that
33
a more flexible affix definition will improve the per-
formance of the taks of biological term annotation.
Inspired by (Jiampojamarn et al, 2005b), we
propose a method to automatically extract domain-
specific prefixes and suffixes from biological cor-
pora. We evaluate the effectiveness of the extracted
affixes by integrating them into the parametrization
of an existing biological term annotation system,
ABTA (Jiampojamarn et al, 2005b), to evaluate the
impact on performance of term annotation. The pro-
posed method is completely unsupervised. For this
reason, we suggest that our method can be gener-
alized for extracting domain-specific affixes from
many domains.
The rest of the paper is organized as follows: In
section 2, we review recent research advances in bi-
ological term annotation. Section 3 describes the
methodology proposed for affix extraction in detail.
The experiment results are presented and evaluated
in section 4. Finally, section 5 summarizes the paper
and introduces future work.
2 Related Work
Biological term annotation denotes a set of proce-
dures that are used to systematically recognize per-
tinent terms in biological literature, that is, to differ-
entiate between biological terms and non-biological
terms and to highlight lexical units that are related to
relevant biology concepts (Nenadic and Ananiadou,
2006).
Recognizing biological entities from texts allows
for text mining to capture their underlying meaning
and further extraction of semantic relationships and
other useful information. Because of the importance
and complexity of the problem, biological term an-
notation has attracted intensive research and there is
a large number of published work on this topic (Co-
hen and Hersh, 2005; Franzn et al, 2003).
Current approaches in biological term annota-
tion can be generalized into three main categories:
lexicon-based, rule-based and learning-based (Co-
hen and Hersh, 2005). Lexicon-based approaches
use existing terminological resources, such as dic-
tionaries or databases, in order to locate term oc-
currences in texts. Given the pace of biology re-
search, however, it is not realistic to assume that a
dictionary can be maintained up-to-date. A draw-
back of lexicon-based approaches is thus that they
are not able to annotate recently coined biological
terms. Rule-based approaches attempt to recover
terms by developing rules that describe associated
term formation patterns. However, rules are often
time-consuming to develop while specific rules are
difficult to adjust to other types of terms. Thus, rule-
based approaches are considered to lack scalability
and generalization.
Systems developed based on learning-based ap-
proaches use training data to learn features useful for
biological term annotation. Compared to the other
two methods, learning-based approaches are theo-
retically more capable to identify unseen or multi-
word terms, and even terms with various writing
styles by different authors. However, a main chal-
lenge for learning-based approaches is to select a set
of discriminating feature attributes that can be used
for accurate annotation of biological terms. The fea-
tures generally fall into four classes: (1) simple de-
terministic features which capture use of uppercase
letters and digits, and other formation patterns of
words, (2) morphological features such as prefix and
suffix, (3) part-of-speech features that provide word
syntactic information, and (4) semantic trigger fea-
tures which capture the evidence by collecting the
semantic information of key words, for instances,
head nouns or special verbs.
As introduced earlier, the learning-based biologi-
cal term annotation system ABTA obtained an 0.705
F-score in exact term matching on Genia corpus
(v3.02)1 which contains 2,000 abstracts of biolog-
ical literature. In fact, the morphological features
in ABTA are learned by recording only the first and
the last n characters of each word in classification
instances. This potentially leads to inaccurate affix
information for the term annotation task.
(Shen et al, 2003) explored an adaptation of a
general Hidden Markov Model-based term recog-
nizer to biological domain. They experimented with
POS tags, prefix and suffix information and noun
heads as features and reported an 0.661 F-score in
overall term annotation on Genia corpus. 100 most
frequent prefixes and suffixes are extracted as can-
didates, and evaluated based on difference in likeli-
hood of part of a biological term versus not. Their
method results in a modest positive improvement in
recognizing biological terms. Two limitations of this
method are: (1) use of only a biological corpus, so
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
34
that the general domain-independent affixes are not
removed, and (2) a supervised process of choosing a
score threshold that is used in affix selection.
(Lee et al, 2003) used prefix and suffix fea-
tures coupled with a dictionary-based refinement of
boundaries of the selected candidates in their exper-
iments for term annotation. They extracted affix fea-
tures in a similar way with (Shen et al, 2003). They
also reported that affix features made a positive ef-
fect on improving term annotation accuracy.
In this project, we consider the quality of domain-
specific affix features extracted via an unsupervised
method. Successful demonstration of the quality of
this extraction method implies that domain-specific
affixes can be identified for arbitrary corpora without
the need to manually generate training sets.
3 PATRICIA-Tree-based Affix Extraction
3.1 PATRICIA Tree
The method we propose to extract affixes from bio-
logical words is based on the use of PATRICIA tree.
?PATRICIA? stands for ?Practical Algorithm To Re-
trieve Information Coded In Alphanumeric?. It was
first proposed by (Morrison, 1968) as an algorithm
to provide a flexible means of storing, indexing, and
retrieving information in a large file. PATRICIA
tree uses path compression by grouping common se-
quences into nodes. This structure provides an ef-
ficient way of storing values while maintaining the
lookup time for a key of O(N) in the worst case,
where N is the length of the longest key. Meanwhile,
PATRICIA tree has little restriction on the format of
text and keys. Also it does not require rearrange-
ment of text or index when new material is added.
Because of its outstanding flexibility and efficiency,
PATRICIA tree has been applied to many large in-
formation retrieval problems (Morrison, 1968).
In our project, all biological words are inserted
and stored in a PATRICIA tree, using which we can
efficiently look up specific biological word or extract
biological words that share specified affixes and cal-
culated required statistics.
3.2 Experiment Design
In this work, we have designed the experiments to
extract domain-specific prefixes and suffixes of bio-
logical words from a biological corpus, and investi-
gate whether the extracted affix information could
facilitate better biological term annotation. The
overall design of our experiments consists of three
major processes: affix extraction, affix refining and
evaluation of experimental results. It is seen that
every node in PATRICIA tree contains exactly one
string of 1 or more characters, which is the preced-
ing substring of its descendant nodes. Meanwhile,
every word is a path of substrings from the root node
to a leaf. Therefore, we propose that every substring
that can be formed from traversing the internal nodes
of the tree is a potential affix.
In the affix extraction process, we first populate a
PATRICIA tree using all words in the combined cor-
pus(CC) of a Biological Corpus (BC) and a General
English Corpus (GEC). GEC is used against BC in
order to extract more accurate biological affix infor-
mation. Two PATRICIA trees are populated sepa-
rately for extracting prefixes and suffixes. The suffix
tree is based on strings derived by reversing all the
input words from the combined corpus. All the po-
tential prefixes and suffixes are then extracted from
the populated PATRICIA trees.
In the affix refining process, for each extracted
potential affix, we compute its joint probability of
being both an English affix and a biological affix,
P (D = Biology, A = Yes|PA), where D stands
for Domain, A stands for Affix and PA represents
Potential Affix. This joint probability can be fur-
ther decomposed as shown in Eq.(1). In the for-
mula, P (A = Yes|PA) denotes the probability that
a given potential affix is a true English affix while
P (D = Biology|A = Yes,PA) refers to the proba-
bility that a given English affix is actually a biologi-
cal affix.
P (D = Biology, A = Yes|PA) =
P (D=Biology|A=Yes,PA)? P (A=Yes|PA) (1)
To calculate P (A = Yes|PA), the probabilities of
prefixes and suffixes are measured separately. In
linguistics, a prefix is described as a type of affix
that precedes the morphemes to which it can attach
(Soanes and Stevenson, 2004). Simply speaking, a
prefix is a substring that can be found at the begin-
ning of a word. Our functional definition of a prefix
is a substring which precedes words existing in the
English language. This can be done by enumerating,
for each node, all descendant substring and assess-
ing their existence as stand-alone words. For exam-
ple, ?radioimmunoassay?, ?radioiodine? and ?radio-
35
labeled? are three words and have a common start-
ing string ?radio?. If we take out the remaining part
of each word, three new strings are obtained, ?im-
munoassay?, ?iodine? and ?labeled?. Since all the
input words are already stored in PATRICIA tree,
we lookup these three strings in PATRICIA tree and
find that ?immunoassay?, ?iodine? and ?labeled? are
also meaningful words in the tree. This indicates
that ?radio? is a prefix among the input words. On
the other hand, it is obvious that ?radioimmunoas-
say? and ?radioiodine? share another string ?radioi?.
However, ?mmunoassay? and ?odine? are not mean-
ingful words due to their absence in the PATRICIA
tree. This suggests that ?radioi? is not a prefix.
For each extracted potential prefix,
P (A = Yes|PA) is computed as the proportion of
strings formed by traversing all descendant nodes
that are meaningful terms. In our experiments,
the measure of determining a string meaningful
is to look up whether the string is an existing
word present in the built prefix PATRICIA tree.
Algorithm 1 shows the procedure of populating a
PATRICIA tree and calculating P (A = Yes|PA)
for each potential prefix.
Algorithm 1 P (A = Yes|PA) for Prefix
Input: words (w) ? Combined Corpus (CC)
Output: P (A = Yes|PA) for each potential prefix
PT = ? //PT : Patricia Trie
for all words w ? CC do
PT ? Insert(w) //Populating Patricia Trie
for all nodes ni ? PT do
PA? String(ni) //Concatenate strings
// in nodes from root to ni,
// which is a potential prefix
TPA ? PrefixSearch(PA)
//TPA : all words w ? CC beginning with PA
score ? 0
for all words w ? TPA do
if Extrstr(PA,w) in PT then
//Extrstr() returns the remaining string
// of w without PA
score ++
P (A = Yes|PA) ? score/|TPA|
//|TPA| is the number of words in TPA
Likewise, in linguistics a suffix is an affix that
follows the morphemes to which it can attach
(Soanes and Stevenson, 2004). Simply speaking,
a suffix of a word is a substring exactly match-
ing the last part of the word. Similar to the idea
of calculating P (A = Yes|PA) for potential pre-
fix, we conjecture that the extracted potential suf-
fix could be a reasonable English suffix if the in-
verted strings formed from traversing the descen-
dant nodes of the potential suffix in the suffix PA-
TRICIA tree are meaningful words. For instance,
?Calcium-dependent?, ?Erythropoietin-dependent?
and ?Ligand-dependent? share a common ending
string ?-dependent?. Since the remaining strings of
each word, ?Calcium?, ?Erythropoietin? and ?Lig-
and? can be found in the ?forward? PATRICIA tree,
?-dependent? is a potentially useful suffix.
However, it is often observable that some English
words do not begin with another meaningful word
but a typical prefix, for example, ?alpha-bound? and
?pro-glutathione?. It is known that ?-bound?and
?-glutathione? are good suffixes in biology. ?al-
pha? and ?pro?, however, are not meaningful words
but typical prefixes, and in fact have been extracted
when calculating P (A = Yes|PA) for potential pre-
fix. Therefore, in order to detect and capture such
potential suffixes, we further assume that if a word
begins with a recognized prefix instead of another
meaningful word, the remaining part of the word
still has the potential to be an informative suffix.
Therefore, strings ?-bound? and ?-glutathione? can
be successfully extracted as potential suffixes. In our
experiments, an extracted potential prefix is consid-
ered a recognized prefix if its P (A = Yes|PA) is
greater than 0.5.
To calculate P (D = Biology|A = Yes, PA), it
is necessary to first determine true English affixes
from extracted potential affixes. In our experiments,
we consider that an extracted potential prefix or suf-
fix is a recognized affix only if its P (A = Yes|PA)
is greater than 0.5. It is also necessary to consider
the biological corpus BC and the general English
corpus GEC separately. It is assumed that a biol-
ogy related affix tends to occur more frequently in
words of BC than GEC. Eq.(2) is used to estimate
P (D = Biology|A = Yes, PA).
P (D = Biology|A = Yes, PA) =
(#Words with PA in BC/Size (BC))/
(#Words with PA in BC/Size (BC) +
#Words with PA in GEC/Size (GEC)), (2)
36
where only PA with P (A = Yes|PA) greater than
0.5 are used, and the number of words with a certain
PA is further normalized by the size of each corpus.
Finally, the joint probability of each potential af-
fix, P (D = Biology, A = Yes|PA), can be used to
parametrize a word beginning or ending with PA.
In the evaluation process of our experiments, the
prefix-suffix pair with maximum joint probability
values is used to parametrize a word. Therefore,
each word in BC has exactly two values as affix fea-
ture: a joint probability value for its potential prefix
and a joint probability value for its potential suffix.
We then replace the original affix feature of ABTA
system with our obtained joint probability values,
and investigate whether these new affix information
leads to equivalent or better term annotation on BC.
4 Results and Evaluation
4.1 Dataset and Environment
For our experiments, it is necessary to use a corpus
that includes widely used biological terms and com-
mon English words. This dataset, therefore, will al-
low us to accurately extract the information of bi-
ology related affixes. As a proof-of-concept proto-
type, our experiments are conducted on two widely
used corpora: Genia corpus (v3.02) and Brown cor-
pus2.The Genia version 3.02 corpus is used as the
biological corpus BC in our experiments. It contains
2,000 biological research paper abstracts. They were
selected from the search results in the MEDLINE
database3, and each biological term has been an-
notated into different terminal classes based on the
opinions of experts in biology. Used as the general
English corpus GEC, Brown corpus includes 500
samples of common English words, totalling about
a million words drawn from 15 different text cate-
gories.
All the experiments were executed on a Sun So-
laris server Sun-Fire-880. Our experiments were
mainly implemented using Perl and Python.
4.2 Experimental Results
We extracted 15,718 potential prefixes and 21,282
potential suffixes from the combined corpus of Ge-
nia and Brown. Among them, there are 2,306 poten-
tial prefixes and 1,913 potential suffixes with joint
2http://clwww.essex.ac.uk/w3c/corpus ling/
3http://www.ncbi.nlm.nih.gov/PubMed/
probability value P (D = Biology, A = Yes|PA)
greater than 0.5. Table 1 shows a few examples
of extracted potential affixes whose joint probabil-
ity value is equal to 1.0. It is seen that most of
these potential affixes are understandable biological
affixes which directly carry specific semantic mean-
ings about certain biological terms. However, some
substrings are also captured as potential affixes al-
though they may not be recognized as ?affixes? in
linguistics, for example ?adenomyo? in prefixes, and
?mopoiesis? in suffixes. In Genia corpus, ?adeno-
myo? is the common beginning substring of biologi-
cal terms ?adenomyoma?, ?adenomyosis? and ?ade-
nomyotic? , while ?plasias? is the common ending
substring of biological terms ?neoplasias? and ?hy-
perplasias?. The whole list of extracted potential af-
fixes is available upon request.
In order to investigate whether the extracted af-
fixes improves the performance of biological term
annotation, it is necessary to obtain the experimen-
tal results of both original ABTA system and the
ABTA system using our extracted affix information.
In ABTA, the extraction of feature attributes is per-
formed on the whole 2000 abstracts of Genia cor-
pus, and then 1800 abstracts are used as training
set while the rest 200 abstracts are used as testing
set. The evaluation measures are precision, recall
and F-score. C4.5 decision tree classifier (Alpay-
din, 2004) is reported as the most efficient classi-
fier which leads to the best performance among all
the classifiers experimented in (Jiampojamarn et al,
2005b). Therefore, C4.5 is used as the main clas-
sifier in our experiments. The experimental results
of ABTA system with 10 fold cross-validation based
on different combinations of the original features are
presented in Table 2 in which feature ?WFP? is short
for Word Feature Patterns, feature ?AC? denotes Af-
fix Characters, and feature ?POS? refers to POS tag
information. The setting of parameters in the exper-
iments with ABTA is: the word n-gram size is 3, the
number of word feature patterns is 3, and the number
of affix characters is 4. We have reported the F-score
and the classification accuracy of the experiments in
the table. It is seen that there is a tendency with the
experimental performance that for a multi-word bi-
ological term, the middle position is most difficult
to detect while the ending position is generally eas-
ier to be identified than the starting position. The
assumed reason for this tendency is that for multi-
37
Potential Prefixes Potential Suffixes
13-acetate
B-cell
endotoxin
I-kappaB
macrophage
adenomyo
Rel/NF-kappaB
anti-CD28
VitD3
cytokine
3-kinase
CD28
HSV-1
ligand
N-alpha-tosyl-L
platelet
pharmaco
adenovirus
chromatin
hemoglobin
-T-cell
-coated
-expressed
-inducer
plasias
-alpha-activated
mopoiesis
-nonresponsive
coagulant
-soluble
cytoid
-bearing
-kappaB-mediated
-globin-encoding
-immortalized
-methyl
lyse
-receptor
glycemia
racrine
Table 1: Examples of Extracted Potential Affixes with Joint Probability Value 1.0
word biological terms, many middle words of are
seemingly unrelated to biology domain while many
ending words directly indicate their identity, for in-
stances, ?receptor?, ?virus? or ?expression?.
Table 3 shows the experimental results of ABTA
system after replacing the original affix feature with
our obtained joint probability values for each word
in Genia corpus. ?JPV? is used to denote Joint Prob-
ability Values. It is seen that based on all three
features the system achieves a classification accu-
racy of 87.5%, which is comparable to the results
of the original ABTA system. However, the size of
the feature set of the system is significantly reduced,
and the classification accuracy of 87.5% is achieved
based on only 18 parameters, which is 1/2 of the size
of the original feature set. Meanwhle, the execution
time of the experiments generally reduces to nearly
half of the original ABTA system (e.g., reduces from
4 hours to 1.7 hours). Furthermore, when the feature
set contains only our extracted affix information, the
system reaches a classification accuracy of 81.46%
based on only 6 parameters. It is comparable with
the classification accuracy achieved by using only
POS information in the system. In addition, Table 3
also presents the experimental results when our ex-
tracted affix information is used as an addtional fea-
ture to the original feature set. It is expected that the
system performance is further improved when the
four features are applied together. However, the size
of the feature set increases to 42 parameters, which
increases the data redundancy. This proves that the
extracted affix information has a positive impact on
locating biological terms, and it could be a good re-
placement of the original affix feature.
Moreover, we also evaluated the performance of
the exact matching biological term annotation based
on the obtained experimental results of ABTA sys-
tem. The exact matching annotation in ABTA sys-
tem is to accurately identify every biological term,
including both multi-word terms and single word
terms, therefore, all the word position classes of
a term have to be classified correctly at the same
time. An error occurring in any one of ?Start? ?Mid-
dle? and ?End? classes leads the system to annotate
multi-word terms incorrectly. Consequently, the ac-
cumulated errors will influence the exact matching
annotation performance. Table 4 presents the exact
matching annotation results of different combination
of features based on 10 fold cross-validation over
Genia corpus. It is seen that after replacing the orig-
inal affix feature of ABTA system with our obtained
joint probability values for each word in Genia cor-
pus, the system achieves an 0.664 F-score on exact
matching of biological term annotation, compara-
ble to the exact matching performance of the orig-
inal ABTA system. In addition, when the feature
set contains only our extracted affix information, the
system reaches an 0.536 F-score on exact matching.
Although it is a little lower than the exact matching
performance achieved by using only the original af-
fix features in the system, the feature set size of the
system is significantly reduced from 24 to 6.
In order to further compare our method with the
original ABTA system, we attempted eleven differ-
ent sizes of training data set to run the experiments
separately based on our method and the original
ABTA system. They can then be evaluated in terms
of their performance on each training set size. These
eleven different training set sizes are: 0.25%, 0.5%,
1%, 2.5%, 5%, 7.5%, 10%, 25%, 50%, 75% and
90%. For instance, 0.25% denotes that the train-
ing data set is 0.25% of Genia corpus while the
rest 99.75% becomes the testing data set for exper-
iments. It is observed that there are about 21 paper
abstracts in training set when its size is 1% , and 52
abstracts when its size is 2.5%. It is expected that
larger training set size leads to better classification
accuracy of experiments.
For each training set size, we randomly extracted
10 different training sets from Genia corpus to run
the experiments. We then computed the mean clas-
sification accuracy (MCA) of 10 obtained classifi-
cation accuracies. Figure 1 was drawn to illustrate
the distribution of MCA of each training set size
38
Feature F-Measure Classification #
sets Start Middle End Single Non Accuracy (%) Parameters
WFP 0.467 0.279 0.495 0.491 0.864 74.59 9
AC 0.709 0.663 0.758 0.719 0.932 85.67 24
POS 0.69 0.702 0.775 0.67 0.908 83.96 3
WFP+AC 0.717 0.674 0.762 0.730 0.933 86.02 33
WFP+POS 0.726 0.721 0.793 0.716 0.923 85.96 12
AC+POS 0.755 0.741 0.809 0.732 0.930 87.14 27
WFP+AC+POS 0.764 0.745 0.811 0.749 0.933 87.59 36
Table 2: Experimental Results of Original ABTA System
Feature F-Measure Classification #
sets Start Middle End Single Non Accuracy (%) Parameters
JPV 0.652 0.605 0.713 0.602 0.898 81.46 6
WFP+JPV 0.708 0.680 0.756 0.699 0.919 84.84 15
JPV+POS 0.753 0.740 0.805 0.722 0.928 86.92 9
WFP+JPV+POS 0.758 0.749 0.809 0.74 0.933 87.50 18
WFP+AC+POS+JPV 0.767 0.746 0.816 0.751 0.934 87.77 42
Table 3: Experimental Results of ABTA System with Extracted Affix Information
for both methods, with the incremental proportion of
training data. It is noted in Figure 1 that the change
patterns of MCA obtained by our method and the
original ABTA system are similar. It is also seen
that our method achieves marginally better classifi-
cation performance when the proportion of training
data is under 2.5%.
Figure 1: MCA Distribution
In order to determine if the classification perfor-
mance difference between our method and the origi-
nal ABTA system is statistically significant, we per-
formed one-tailed t-Test (Alpaydin, 2004) on the
classification results with our hypothesis that MCA
of our proposed method is higher than MCA of orig-
inal ABTA system. The significance level ? is set
to be the conventional value 0.05. As a result, the
classification performance difference between two
methods is statistically significant when the propor-
tion of training data is 0.25%, 0.5%, 1% or 2.5%.
Table 5 shows the P values of t-Test results for the
various training set sizes. This demonstrates that
the ABTA system adopting our method outperforms
the original ABTA system in classification accuracy
when the proportion of training data is lower than
2.5% of Genia corpus, and achieves comparable
classification performance with the original ABTA
system when the proportion continuously increases.
One-tailed Training set size
t-Test 0.25% 0.5% 1% 2.5%
P value 0.0298 0.0006 0.0002 0.0229
Table 5: One-tailed t-Test Results
5 Conclusions
In this paper, we have presented an unsupervised
method to extract domain-specific prefixes and suf-
fixes from the biological corpus based on the use
of PATRICIA tree. The ABTA system (Jiampoja-
marn et al, 2005b) adopting our method achieves
an overall classification accuracy of 87.5% in locat-
ing biological terms, and derives an 0.664 F-score in
exact term matching annotation, which are all com-
parable to the experimental results obtained by the
original ABTA system. However, our method helps
the system significantly reduce the size of feature set
and thus improves the system efficiency. The sys-
tem also obtains a classification accuracy of 81.46%
based only on our extracted affix information. This
39
Feature Exact Matching Annotation #
sets Precision Recall F-score Parameters
AC 0.548 0.571 0.559 24
WFP+AC+POS 0.661 0.673 0.667 36
JPV 0.527 0.545 0.536 6
WFP+JPV+POS 0.658 0.669 0.664 18
Table 4: Exact Matching Annotation Performance
demonstates that the affix information acheived by
the proposed method is important to accurately lo-
cating biological terms.
We further explored the reliability of our method
by gradually increasing the proportion of training
data from 0.25% to 90% of Genia corpus. One-tailed
t-Test results confirm that the ABTA system adopt-
ing our method achieves more reliable performance
than the original ABTA system when the training
corpus is small. The main result of this work is thus
that affix features can be parametrized from small
corpora at no cost in performance.
There are some aspects in which the proposed
method can be improved in our future work. We
are interested in investigating whether there exists
a certain threshold value for the joint probability
which might improve the classification accuracy of
ABTA system to some extent. However, this could
import supervised elements into our method. More-
over, we would like to incorporate our method into
other published learning-based biological term an-
notation systems to see if better system performance
will be achieved. However, superior parametriza-
tion will improve the annotation performance only
if the affix information is not redundant with other
features such as POS.
References
Ethem Alpaydin. 2004. Introduction to Machine Learning.
MIT Press.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based
text categorization. In Proc. SDAIR-94, 3rd Ann. Symposium
on Doc. Analysis and Inf. Retr., pages 161?175, Las Vegas,
USA.
Aaron Michael Cohen and William R. Hersh. 2005. A sur-
vey of current work in biomedical text mining. Briefings in
Bioinformatics, 5(1):57?71.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nissim,
Gail Sinclair, and Christopher Manning. 2004. Exploiting
context for biomedical entity recognition: From syntax to
the web. In Joint wsh. on NLP in Biomedicine and its Appli-
cations (JNLPBA-2004).
Kristofer Franzn, Gunnar Eriksson, Fredrik Olsson, Lars
Asker Per Lidn, and Joakim Cster. 2002. Protein names
and how to find them. International Journal of Medical In-
formatics special issue on NLP in Biomedical Applications,
pages 49?61.
Kristofer Franzn, Gunnar Eriksson, Fredrik Olsson, Lars
Asker Per Lidn, and Joakim Cster. 2003. Mining the Bio-
medical Literature in the Genomic Era: An Overview. J.
Comp. Biol., 10(6):821?855.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998. To-
ward information extraction: Identifying protein names from
biological papers. In the Pacific Symposium on Biocomput-
ing, pages 707?718.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005a. Automatic Biological Term Annotation Using N-
gram and Classification Models. Master?s thesis, Faculty of
Comp.Sci., Dalhousie University.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005b. Biological Named Entity Recognition using N-
grams and Classification Methods. In Conf. of the Pacific
Assoc. for Computational Linguistics, PACLING?05, Tokyo,
Japan.
Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang Rim.
2003. Two-phase biomedical NE recognition based on
SVMs. In Proc. of the ACL 2003 workshop on Natural lan-
guage processing in biomedicine, pages 33?40, Morristown,
NJ, USA. ACL.
Donald R. Morrison. 1968. Patricia - Practical Algorithm To
Retrieve Information Coded in Alphanumeric. Journal of
the ACM, 15(4):514?534.
Goran Nenadic and Sophia Ananiadou. 2006. Mining semanti-
cally related terms from biomedical literature. ACM Trans-
actions on Asian Language Information Processing (TALIP),
5(1):22 ? 43.
Burr Settles. 2004. Biomedical named entity recognition using
conditional random fields and novel feature sets. In Joint
wsh. on NLP in Biomedicine and its Applications (JNLPBA-
2004).
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2003. Effective adaptation of a Hidden Markov Model-
based named entity recognizer for biomedical domain. In
Proc. of the ACL 2003 wsh. on NLP in Biomedicine, pages
49?56, Morristown, NJ, USA.
Catherine Soanes and Angus Stevenson. 2004. Oxford Dictio-
nary of English. Oxford University Press.
40
Proceedings of the Workshop on BioNLP, pages 133?141,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Identifying Interaction Sentences from Biological Literature Using
Automatically Extracted Patterns
Haibin Liu
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
haibin@cs.dal.ca
Christian Blouin
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
cblouin@cs.dal.ca
Vlado Kes?elj
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
vlado@cs.dal.ca
Abstract
An important task in information retrieval is to
identify sentences that contain important relation-
ships between key concepts. In this work, we
propose a novel approach to automatically extract
sentence patterns that contain interactions involv-
ing concepts of molecular biology. A pattern is
defined in this work as a sequence of specialized
Part-of-Speech (POS) tags that capture the struc-
ture of key sentences in the scientific literature.
Each candidate sentence for the classification task
is encoded as a POS array and then aligned to
a collection of pre-extracted patterns. The qual-
ity of the alignment is expressed as a pairwise
alignment score. The most innovative component
of this work is the use of a Genetic Algorithm
(GA) to maximize the classification performance
of the alignment scoring scheme. The system
achieves an F-score of 0.834 in identifying sen-
tences which describe interactions between bio-
logical entities. This performance is mostly af-
fected by the quality of the preprocessing steps
such as term identification and POS tagging.
1 Introduction
Recent research in information extraction (IE) in bio-
logical science has focused on extracting information
about interactions between biological entities from re-
search communications. The type of interaction of in-
terest includes protein-protein, protein-DNA, gene reg-
ulations and other interactions between macromole-
cules. This work broadens the definition of the term
?interaction? to include other types of concepts that
are semantically related to cellular components and
processes. This contrasts with the past efforts focus-
ing strictly on molecular interactions (Blaschke et al,
1999; Ono et al, 2001). We anticipate that identifying
the relationships between concepts of molecular biol-
ogy will facilitate the building of knowledge models,
improve the sensitivity of IE tasks and ultimately facil-
itate the formulation of new hypothesis by experimen-
talists.
The extraction of interactions is based on the heuris-
tic premise that interacting concepts co-occur within
a given section of text. The challenge is that co-
occurrence certainly does not guarantee that a passage
contains an interaction(Jang et al, 2006; Skusa et al,
2005). Co-occurrence is highly dependent on the de-
finition of the section of text within which the target
terms are expected to be found. A thorough compari-
son on the prediction of protein-protein interaction be-
tween abstract-level co-occurrence and sentence-level
co-occurrence was undertaken (Raychaudhuri, 2006).
It is demonstrated that abstract co-occurrence is more
sensitive but less specific for interactions. At the cost
of wide coverage, sentence co-occurrence increases the
accuracy of interaction prediction. Since the ultimate
goal of IE is to extract knowledge and accuracy is the
most important aspect in evaluating the performance
of such systems, it makes sense to focus the effort
in seeking interaction sentences rather than passages
or abstracts. Not every co-occurrence in sentences
implies a relationship that expresses a fact. In the
2005 Genomics Track dataset, 50% of all sentence co-
occurrences of entities correspond to definite relation-
ships while the rest of the co-occurrences only convey
some possible relationships or contain no relationship
of interest (Li et al, 2005). Therefore, more sophisti-
cated text mining strategies are required to classify sen-
tences that describe interactions between co-occurring
concepts.
In the BioCreative II challenge 1, teams were asked
to determine whether a given passage of text contained
information about the interaction between two proteins.
This classification task worked at the abstract level and
the interacting protein pairs were not required to be ex-
tracted. The task for the Learning Language in Logic
1http://biocreative.sourceforge.net/
133
(LLL?05) challenge 2 was to build systems that ex-
tract interactions between genes or proteins from bio-
logical literature. From individual sentences annotated
with agent-target relations, patterns or models had to be
learned to extract these interactions. The task focused
on extracting only the interacting partners. The context
of an interaction may also be critical to the validity of
the extracted knowledge since not all statements found
in the literature are always true.
In this work, we propose an approach to automati-
cally extract patterns containing relevant interaction be-
tween biological concepts. This extraction is based on
the assumption that biological interactions are articu-
lated by a limited number of POS patterns embedded
in sentences where entities/concepts are co-occurring.
The extracted patterns are then applied to identify inter-
action sentences which describe interactions between
biological entities. Our work aims to identify precise
sentences rather than passages. Because of the nature
of the patterns, we hope that some of the contextual in-
formation present in interaction sentences also play a
role in the classification task.
The rest of the paper is organized as follows: In Sec-
tion 2, we review recent research advances in extracting
biological interactions. Section 3 describes an experi-
mental system designed for our work. Sections 4, 5
and 6 elaborate the approaches and algorithms. Per-
formance is evaluated in Section 7. Finally, Section 8
summarizes the paper and introduces future work.
2 Related work
Early on, Blaschke (Blaschke et al, 1999) employed
patterns to predict the presence of a protein-protein in-
teraction. A series of patterns was developed manu-
ally to cover the most obvious descriptions of protein
functions. This process was based on a set of key-
words, including interaction verbs, that are commonly
used to describe this type of interaction. A sentence ex-
traction system BioIE (Divoli and Attwood, 2005) also
uses patterns to extract entire sentences related to pro-
tein families, protein structures, functions and diseases.
The patterns were manually defined and consisted of
single words, word pairs, and small phrases.
Although systems relying on hand-coded patterns
have achieved some success in extracting biological in-
teractions, the strict requirement of dedicated expert
work is problematic. Moreover, each type of interac-
tion may require a definition of many different patterns
including different arrangements and different variants
2http://genome.jouy.inra.fr/texte/LLLchallenge/
of the same keyword. Manually encoding all patterns
encountered in a corpus is time-consuming and poten-
tially impractical in real applications. Thus, automati-
cally learning such patterns is an attractive solution.
An approach which combines dynamic program-
ming and sequence alignment algorithms as normally
used for the comparison between nucleotide sequences
was introduced by Huang et al (Huang et al, 2004).
This approach is designed to generate patterns useful
for extracting protein-protein interactions. The main
problem with this approach is that the scoring scheme
that is required to implement the alignment algorithm is
difficult to define and contains a potentially large num-
ber of free parameters. We propose a method based
on Genetic Algorithm (GA) heuristics to maximize the
alignment procedure for the purpose of classification.
GAs were also used as a learning strategy to train finite
state automata for finding biological relation patterns
in texts(Plake et al, 2005). It was reported (Bunescu et
al., 2005; Hakenberg et al, 2005) that automatically
learned patterns identify biological interactions even
more accurately than hand-coded patterns.
3 Overview of system design
In this work, we have designed an experimental sys-
tem to facilitate the automatic extraction of biological
interaction patterns and the identification of interaction
sentences. It consists of three major modules: biolog-
ical text preprocessing, interaction pattern extraction,
and interaction sentence identification.
Biological text preprocessing reformats the original
biological texts into candidate sentences. A pattern
learning method is then proposed to automatically ex-
tract the representative patterns of biological interac-
tions. The obtained patterns are further used to iden-
tify instances that evidently describe biological inter-
actions. Poor performance during preprocessing will
have detrimental effects on later stages. In the follow-
ing sections, we will describe each component.
4 Biological text preprocessing
4.1 Sentence preparation
A heuristic method is implemented to detect sentence
boundaries (Mikheev, 2002) based on the assumption
that sentences are usually demarcated by some indica-
tive delimiting punctuation marks in order to segment
the biological texts into sentence units. Captions and
headings that are not grammatically valid sentences are
therefore detected and further eliminated for our work.
134
4.2 Part-of-Speech tagging
POS tagging is then performed to associate each word
in a sentence with its most likely POS tag. Because
subsequent processing steps typically depend on the
tagger?s output, high performance at this level is cru-
cial for success in later stages. A statistical tagger Lin-
gua::EN::Tagger 3 is used to perform this task.
4.3 Biological term annotation
A learning-based biological term annotation system,
ABTA (Jiampojamarn et al, 2005), is embedded in our
system. The type of terms includes molecules, such
as genes, proteins and cell lines, and also biological
processes. Examples of biological processes as entities
are: ?T cell activation? and ?IL-2 gene transcription?.
We consider that a broader definition of biological term
will include more facts from literature, thus leading to
more general use of interaction patterns for IE tasks.
ABTA considers the longest expression and ignores
embedded entities. Further, instead of distinguishing
terms from their relevant biology concepts, a unified
tag ?BIO? is assigned to all the identified terms. We
aim to discover patterns of the general interactions be-
tween biological concepts, not only the interactions be-
tween molecules, e.g., protein-protein interaction.
Tags like NN (noun) and VB (verb) are typically used
to define entities and the action type of interactions,
and thus they are indispensable. However, tags such
as JJ (adjective) and RB (adverb) could occur at differ-
ent positions in a sentence. We decided to remove these
tags to prevent the combinatorial effect that these would
induce within the set of extracted patterns.
4.4 Text chunking
Next, a rule-based text chunker (Ramshaw and Mar-
cus, 1995) is applied on the tagged sentences to fur-
ther identify phrasal units, such as base noun phrases
NP and verbal units VB. This allows us to focus on the
holistic structure of each sentence. Text chunking is not
applied on the identified biological terms. In order to
achieve more generalized interaction patterns, a unified
tag ?VB? is used to represent every verbal unit instead
of employing different tags for various tenses of verbs.
As a result of preprocessing, every sentence is rep-
resented by its generalized form as a sequence of cor-
responding tags consisting of POS tags and predefined
tags. Table 1 summarizes the main tags in the system.
A biological interaction tends to involve at least three
objects: a pair of co-occurring biological entities con-
3http://search.cpan.org/?acoburn
Tag name Tag description Tag type
BIO Biological entity Predefined
NP Base noun phrase Predefined
VB Verbal unit Predefined
IN Preposition POS
CC Coordinating conjunction POS
TO to POS
PPC Punctuation comma POS
PRP Possessive 2nd determiner POS
DET Determiner POS
POS Possessive POS
Table 1: Main tags used in the system
nected by a verb which specifies the action type of the
interaction. Thus, a constraint is applied that only sen-
tences satisfying form ?BioEntity A ? Verb ? BioEn-
tity B? will be preserved as candidate sentences to be
further processed in the system. It is possible that the
presence of two entities in different sentence structures
implies a relationship. However, this work assumes the
underlying co-occurrence of two concepts and a verb in
the interest of improving the classification accuracy.
The obtained candidate sentences are split into train-
ing and testing sets. The training set is used to ex-
tract the representative patterns of biological interac-
tions. The testing set is prepared for identifying sen-
tences that evidently describe biological interactions.
5 Interaction pattern extraction
5.1 PATRICIA trees
The method we propose to extract interaction patterns
from candidate sentences is based on the use of PATRI-
CIA trees (Morrison, 1968). A PATRICIA tree uses
path compression by grouping common sequences into
nodes. This structure provides an efficient way of stor-
ing values while maintaining the lookup time for a key
of O(N). It has been applied to many large information
retrieval problems (Chien, 1997; Chen et al, 1998).
In our work, a PATRICIA tree is used for the first
time to facilitate the automatic extraction of interaction
patterns. All training sentences are inserted and stored
in a generic PATRICIA tree from which the common
patterns of POS tags can be efficiently stored and the
tree structure used to compute relevant usage statistics.
5.2 Potential pattern extraction
Patterns of straightforward biological interactions are
frequently encountered in a range of actual sentences.
Conversely, vague relationships or complex interac-
tions patterns are seldom repeated. Therefore, the
135
premise of this work is that there is a set of frequently
occurring interaction patterns that matches a majority
of stated facts about molecular biology. In this work, a
biological interaction pattern is defined as follows:
Definition 5.1. A biological interaction pattern bip
is a sequence of tags defined in Table 1 that captures an
aggregate view of the description of certain types of bi-
ological interactions based on the consistently repeated
occurrences of this sequence of tags in different inter-
action sentences. BIP = {bip1, bip2, ? ? ? , bipk} repre-
sents the set of biological interaction patterns.
We first extract potential interaction patterns by
populating a PATRICIA tree using training sentences.
Every node in the tree contains one or more system
tags, which is the preceding tag sequence of its descen-
dant nodes in each sentence. Every sentence is com-
posed of a path of system tags from the root to a leaf.
Hence, we propose that the sequence of system tags
that can be formed from traversing the nodes of the tree
is a potential pattern of biological interactions. At the
same time, the occurrence frequency of each pattern is
also retrieved from the traversal of tree nodes.
A predefined frequency threshold fmin is used as
a constraint to filter out patterns that occur less than
fmin times. It has been demonstrated that if an interac-
tion is well recognized, it will be consistently repeated
(Blaschke et al, 1999; Ono et al, 2001). The general-
ization and the usability of patterns can be controlled by
tuning fmin. Further, some filtering rules are adapted
to control the form of a pattern and enhance the quality
of the discovered patterns, such as if a pattern ends with
a tag IN, VB, CC or TO, the pattern will be rejected.
Flexibility in setting this threshold can be applied to
meet special demands. Algorithm 1 shows our pattern
learning method which has a time complexity of O(n)
in the size of candidate sentences, n.
Algorithm 1 Patricia-Tree-based Extraction of Biolog-
ical Interaction Patterns
Input: Candidate Sentences CS ? Biological text; a prede-
fined threshold fmin; a set of filtering rules FR
Output: BIP : Set of biological interaction patterns
BIP ? ?; PT ? ? //PT : Patricia Trie
for all sentences s ? CS do
PT ? Insert(s) //Populating Patricia Tree
for all nodes ni ? PT do
bipi ? Pattern(ni) //Concatenating tags in nodes
from root to ni, which is a potential pattern
if Count(bipi) ? fmin and bipi does not meet FR
then
//Count(bipi) returns No. of occurrences of bipi;
BIP ? bipi
5.3 Interaction verb mining
Although the obtained patterns are derived from the
candidate sentences possessing the form ?BioEntity A
? Verb ? BioEntity B?, some of them may not contain
facts about biological interactions. This is possible if
the action verbs do not describe an interaction. Quite a
few verbs, such as ?report?, ?believe?, and ?discover?,
only serve a narrative discourse purpose. Therefore,
mining the correct interaction verbs becomes an impor-
tant step in the automatic discovery of patterns. We de-
cided to perform the method applied in (Huang et al,
2004) to mine a list of interaction verbs. This will be
used to further improve the relevance of achieved pat-
terns by filtering out patterns formed by the sentences
in which the action verbs are not on the list.
6 Interaction sentence identification
Once the biological interaction patterns are obtained,
we perform interaction sentence identification on test-
ing sentences. For our work, they are partitioned into
two sets: interaction sentences which explicitly discuss
interactions between entities, and non-interaction sen-
tences which do not describe interactions, or merely
imply some vague relationships between entities. The
task of interaction sentence identification is treated as a
classification problem to differentiate between interac-
tion sentences and non-interaction sentences.
6.1 Pattern matching scoring
We first perform pattern matching by iteratively apply-
ing the interaction patterns to each testing sentence.
This is done using sequence alignment which calculates
the degree of the similarity of a sentence to an inter-
action pattern. Since patterns capture various ways of
expressing interactions among sentences, a high simi-
larity between an interaction sentence and a pattern is
expected. Therefore, we conjecture that the alignment
scores can be used to discriminate some type of inter-
action sentences from other types of sentences.
The scoring scheme involved in the pattern match-
ing consists of penalties for introducing gaps, match re-
wards and mismatch penalties for different system tag
pairs. Table 2 presents an example scoring scheme for
main tags. Penalties and rewards are denoted respec-
tively by negative and positive values.
As a variation of global alignment, an end-space free
alignment algorithm is implemented to facilitate the
alignment between patterns and testing sentences. The
shortest pattern is always preferred for a sentence in
case that same alignment score is achieved by multiple
136
Tag Gap Match Mismatch
BIO -10 +8 -3
NP -8 +6 -3
VB -7 +7 -3
IN -6 +5 -1
CC -6 +5 -1
TO -1 +5 -1
PPC -1 +3 -1
PRP -1 +3 -1
DET -1 +3 -1
POS -1 +3 -1
Table 2: An alignment scoring scheme for system tags
patterns. As a result, each sentence is assigned to its
most appropriate pattern along with a maximum align-
ment score. Therefore, an interaction sentence will be
highlighted with a high alignment score by its most
similar interaction pattern, while a non-interaction sen-
tence will be characterized by a low alignment score
indicating rejections by all patterns. Essentially, this
procedure can be seen as a variation of the well-known
k Nearest Neighbors classification method, with k = 1.
6.2 Performance evaluation
We then evaluate whether the alignment scores can be
used to classify the testing sentences. We have pro-
posed two independent evaluation measures: statistical
analysis (SA) and classification accuracy (AC).
SA measures whether the scoring difference be-
tween the mean of interaction sentences and the mean
of non-interaction sentences is statistically significant.
If the difference is significant, there will be a tendency
that interaction sentences outscore non-interaction sen-
tences in alignment. Hence, it would be reliable to
use alignment scores to classify testing sentences. Al-
though non-interaction sentences could come from the
same documents as interaction sentences and discuss
concepts that are associated with the target interac-
tions, we assume that interaction sentences and non-
interaction sentences are two independent samples.
The statistical two-sample z test (Freund and Per-
les, 2006) is performed with the null hypothesis that
there is no scoring difference between the means of
interaction and non-interaction sentences. A compar-
atively large z will lead to the rejection of the null
hypothesis. Naturally, the increase of z value will in-
crease the difference between the means and therefore
conceptually keep pushing the overall scoring distrib-
utions of two samples further away from each other.
Consequently, interaction sentences can be separated
from non-interaction sentences according to alignment
scores. In reality, the distinction between interaction
and non-interaction sentences is not absolute. Thus,
the scoring distributions of two samples can only be
distanced by a certain maximum value of z depending
on the scoring scheme applied in pattern matching.
Conversely, AC measures the proportion of correctly
classified testing sentences, including both interaction
and non-interaction sentences, to the total testing sen-
tences. An appropriate threshold T is determined for
obtained alignment scores to differentiate between in-
teraction and non-interaction sentences, and to facili-
tate the calculation of classification accuracy.
It is not possible to evaluate the performance without
correctly pre-labeled testing sentences. We decided to
manually classify the testing sentences in advance by
assigning each sentence an appropriate label of inter-
action or non-interaction. This work was done by two
independent experts, both with Ph.D. degrees in mole-
cular biology or a related discipline.
6.3 Scoring scheme optimization
The scoring scheme applied in pattern matching has a
crucial impact on the performance of interaction sen-
tence identification. An interesting problem is whether
there exists an optimal scoring scheme covering the
costs of gap, match and mismatch for different sys-
tem tags in the pattern matching alignment, which is
destined to achieve the best performance on classify-
ing testing sentences. To the best of our knowledge,
no efforts have been made to investigate this problem.
Instead, an empirical or arbitrary scoring scheme was
adopted in previous research for the pairwise align-
ments (Huang et al, 2004; Hakenberg et al, 2005). We
have proved that the problem is NP-hard by reducing a
well-known NP-hard problem 3-SAT to this problem.
The proof is not presented in this work.
A genetic algorithm (GA) is used as a heuristic
method to optimize parameters of the scoring scheme
for sentence classification. The costs of penalties and
rewards for different system tags are encoded by inte-
ger values within two predefined ranges: [-50, 0) and
(0, 50], and assembled as a potential solution of scor-
ing scheme, which consists of 30 parameters covering
the costs for tags in the alignment as listed in Table 2.
The two evaluation measures SA and AC are used as
the fitness function for GA respectively with the goal
of maximizing z value or classification accuracy.
GA is set up to evolve for 100 generations, each of
which consists of a population of 100 potential solu-
tions of scoring scheme. GA starts with a randomly
137
generated population of 100 potential solutions and
proceeds until 100 generations are reached. The num-
ber of generations and the population size are decided
with consideration of the runtime cost of evaluating the
fitness function, which requires running the scoring al-
gorithm with each sentence. A large number of gener-
ations or a large population size would incur an expen-
sive runtime cost of evaluation.
In addition, we further divide the labeled set of can-
didate sentences into two subsets: The first dataset is
used to optimize parameters of the scoring scheme,
while the second dataset, testing set, is used to test the
achieved scheme on the task of sentence classification.
7 Results and evaluation
7.1 Dataset
Our experiments have been conducted on Genia cor-
pus (v3.02) 4, the largest, publicly available corpus in
molecular biology domain. It consists of 2,000 biolog-
ical research paper abstracts and is intended to cover
biological reactions concerning transcription factors in
human blood cells. The information of sentence seg-
mentation, word tokenization, POS tagging and biolog-
ical term annotation is also encoded in the corpus.
7.2 Biological text preprocessing results
Evaluated using the inherently equipped annotation in-
formation, our system achieves nearly 99% accuracy
on segmenting sentences. Further, it obtains an overall
POS tagging accuracy of 91.0% on 364,208 individ-
ual words. We noticed that the tagging information en-
coded in Genia corpus is not always consistent through-
out the whole corpus, thus introducing detrimental ef-
fects on the tagging performance. Also, considering
that the tagger is parameterized according to the gen-
eral English domain, porting this tagger to the biology
domain is accompanied by some loss in performance.
The system reaches an F-score of 0.705 on annotat-
ing all biological terms including both multi-word and
single word terms. After performing text chunking, the
system produces a set of candidate sentences. We fur-
ther perform text chunking on Genia corpus based on
its encoded annotations and use the resulting set of sen-
tences for the subsequent experiments to provide a gold
standard to which results produced based on our system
annotations can be compared. Table 3 presents some
statistics of the preprocessed dataset. For each type of
annotations, we randomized the candidate sentence set
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
and chose 12,525 candidate sentences as the training
set to extract biological interaction patterns. The rest
of candidate sentences are prepared as the testing set.
Attributes Genia Our system
Total preprocessed sentences 18,545 18,355
Candidate sentences 16,272 17,525
Training set sentences 12,525 12,525
Testing set sentences 6,020 5,000
Table 3: Statistics of experimental dataset
7.3 Interaction pattern extraction results
fmin = 5 is used to filter out the potential patterns
that appear less than 5 times in the training set. Eval-
uated by domain experts, lists of 300 interaction verbs
and 700 non-interaction verbs are obtained from 12,525
training sentences with Genia annotations. Inflectional
variants of the verbs are also added into the lists.
Refined by the filtering rules and the interaction
verbs, a final set of representative patterns of biological
interactions are obtained from Algorithm 1. We per-
formed our proposed pattern learning method on train-
ing sentences of both the GENIA and our own anno-
tations. There are respectively 241 and 329 potential
patterns. Of these, 209 and 302 were extracted. Inter-
estingly, only 97 extracted patterns are common to both
annotation schemes.
Table 4 lists the 10 most frequent interaction patterns
based on Genia annotations. For instance, a training
sentence conforming to the second pattern is ?The ex-
pression of the QR gene is regulated by the transcrip-
tion factor AP-1.? (MEDLINE: 96146856).
Pattern count Pattern
264 BIO VB BIO IN BIO
261 NP IN BIO VB IN BIO
182 NP IN BIO VB BIO
162 BIO IN BIO VB IN BIO
160 BIO VB IN BIO IN BIO
143 NP IN BIO VB IN NP IN BIO
142 NP VB IN BIO VB BIO
138 PRP VB IN BIO VB BIO
126 BIO VB NP IN BIO IN BIO
121 NP IN BIO VB NP IN BIO
Table 4: Extracted Biological Interaction Patterns
7.4 Interaction sentence identification results
Since the total testing sentence set is large, we decided
to randomly extract 400 sentences from it as the sam-
ple set for our task. The 400 sentences were manu-
138
Figure 1: AC comparison between two measures
ally pre-labeled into two classes: interaction and non-
interaction. Further, a subset of 300 testing sentences
was used by GA to optimize parameters of the scor-
ing scheme, while the remaining 100 sentences were
prepared to test the achieved scheme on sentence clas-
sification. The distribution of class labels of the sample
sentences is shown in Table 5.
Class label 300 sentences 100 sentences
No. % No. %
Interaction 158 52.67 53 53
Non-interaction 142 47.33 47 47
Table 5: Class distribution of sample sentences
7.4.1 Comparison between two measures
We applied the evaluation measures, SA and AC,
respectively to the subset of 300 testing sentences as
the fitness function for GA, and recorded the scoring
scheme of every generation resulted from GA. Figure 1
presents the distribution of achieved classification ac-
curacy in terms of each scoring scheme optimized by
GA. This comparison is done with respect to the gener-
ation and evaluated on 300 testing sentences using the
annotations from the Genia corpus.
The achieved classification accuracy for AC gen-
erally outperforms the classification accuracy derived
by SA. It reaches its highest classification accuracy
80.33% from the 91th generation. Therefore, AC is
considered more efficient with the system and becomes
our final choice of fitness function for GA.
7.4.2 Results of sentence identification
GA results in an optimized performance on the 300
sentences. It also results in an optimized scoring
scheme along with its associated scoring threshold T ,
which are then applied together to the other 100 test-
ing sentences. Table 6 and 7 present the system perfor-
mance on the two sets respectively to both annotations.
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.757 0.887 0.704 0.702
Recall 0.928 0.665 0.761 0.640
F-score 0.834 0.750 0.731 0.670
Overall AC(%) 80.33 70.33
Table 6: Performance on 300 testing sentences
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.739 0.762 0.676 0.697
Recall 0.792 0.723 0.755 0.638
F-score 0.765 0.742 0.713 0.666
Overall AC(%) 75.96 70.00
Table 7: Performance on 100 testing sentences
Table 6 shows that when using the Genia annota-
tions the system achieves an 0.834 F-score in identify-
ing interaction sentences and an overall AC of 80.33%,
which is much higher than the proportion of either in-
teraction or non-interaction sentences in the 300 sen-
tence subset. This indicates that the system performs
well on both classes. In 100 generations GA is not able
to evolve a scoring scheme that leads to an AC above
80.33%. Moreover, our system annotations achieve
a lower performance than Genia annotations. We at-
tribute the difference to the accuracy loss of our system
annotations in the preprocessing steps as inaccurate an-
notations will lead to inappropriate patterns, thus harm-
ing the performance of sentence identification. For Ge-
nia annotations, the performance on the 100 testing sen-
tences suggests an overfitting problem.
There are a number of preprocessing steps that affect
the final classification performance. However, even as-
suming an ideal preprocessing of the unstructured text,
our method relies on the assumption that all interac-
tion sentences are articulated by a set of POS patterns
that are distinct to all other types of sentences. The
manual annotation of the training/testing set was a dif-
ficult task, so it is reasonable to assume that this will
also be difficult for the classifier. The use of passive
voice and the common use of comma splicing within
patterns makes sentence-level classification an espe-
cially difficult task. Another source of interactions that
our system cannot identify are implied and assume a
deeper semantic understanding of the concepts them-
139
selves. Other sentences are long enough that the inter-
action itself is merely a secondary purpose to another
idea. All of these factors pose interesting challenges
for future development of this work.
Moreover, we also experimented with 10 empirical
scoring schemes derived from previous experiments on
the 300 sentences respectively, including the scheme in
the Table 2. Several fixed thresholds were attempted for
obtained alignment scores to differentiate between in-
teraction and non-interaction sentences. Without using
GA to optimize parameters of the scoring scheme, the
best performance of 10 empirical schemes is an overall
AC of 65.67%, which is outperformed at the 3rd gen-
eration of the GA optimization with Genia annotations.
7.5 System performance comparison
Within the framework of our system, we further con-
ducted experiments on the same dataset for sentence
identification using interaction patterns generated by
another pattern generating algorithm (PGA) (Huang et
al., 2004) in order to compare with the performance of
patterns obtained by our pattern learning method.
In our implementation, PGA iterates over all pairs
of candidate sentences in the training set and calculates
the best alignment for each pair in terms of the cost
scheme of gap penalties proposed (Huang et al, 2004).
Each consensus sequence from the optimal alignment
of each pair forms a pattern. The filter rules proposed
are also applied. PGA has a time complexity of O(n2)
in the size of candidate sentences, n. Hence, our pro-
posed pattern learning method is much more efficient
when dealing with large collections of biological texts.
PGA produces a large number of patterns, even with
fmin = 5 and other filtering criteria. There are 37,319
common patterns between two types of annotations.
Attributes Genia Our system
Potential patterns (fmin = 5) 476,600 387,302
Extracted patterns (fmin = 5) 176,082 88,800
Table 8: Pattern extraction results of PGA
In order to make a direct comparison, we decided to
experiment with the same number of interaction pat-
terns. For Genia annotations, we chose the most fre-
quent 209 patterns generated by PGA to compare with
the 209 patterns by our method. For our system annota-
tions, two sets of 302 patterns are employed. Further, it
is found that there are 96 common patterns between the
two sets of 209 patterns for Genia annotations, and 153
common patterns between the two sets of 302 patterns
for our system annotations. Table 9 and 10 present the
results of sentence identification of PGA. The results
show that patterns generated by PGA do not perform
as well as patterns obtained by our method.
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.721 0.869 0.663 0.699
Recall 0.918 0.606 0.785 0.556
F-score 0.808 0.714 0.719 0.619
Overall AC(%) 77.00 67.67
Table 9: Performance of PGA on 300 testing sentences
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.664 0.796 0.698 0.635
Recall 0.849 0.574 0.566 0.766
F-score 0.745 0.667 0.625 0.694
Overall AC(%) 71.98 66.00
Table 10: Performance of PGA on 100 testing sentences
8 Conclusion and future work
In this paper, a novel approach is presented to auto-
matically extract the representative patterns of biologi-
cal interactions, which are used to detect sentences that
describe biological interactions. We conducted the ex-
periments on our designed system based on the Ge-
nia corpus. By means of a genetic algorithm, the sys-
tem achieves an 0.834 F-score using Genia annotations
and an 0.731 F-score using our system annotations in
identifying interaction sentences by evaluating 300 sen-
tences. By applying the optimized scoring scheme to
another set of 100 sentences, the system achieves com-
parable results for both types of annotations. Further-
more, by comparing with another pattern generating al-
gorithm, we infer that our proposed method is more ef-
ficient in producing patterns to identify interaction sen-
tences.
In our future work, we would like to employ the ob-
tained interaction patterns to guide the extraction of
specific interactions. The matching between patterns
and sentences will be performed and the matched parts
of each sentence will be extracted as candidate interac-
tions. Further reasoning processes can be performed
by means of available biological ontologies, such as
UMLS Semantic Network (Mccray and Bodenreider,
2002) and Gene Ontology (Consortium, 2001), to in-
fer new relations from the initial interactions. Such
processes can be employed to derive additional biolog-
ical knowledge from existing knowledge, or test for bi-
ological consistency of the newly entered data.
140
References
Christian Blaschke, Miguel A. Andrade, Christos Ouzounis,
and Alfonso Valencia. 1999. Automatic extraction of bi-
ological information from scientific text: Protein-protein
interactions. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology,
pages 60?67. AAAI Press.
Razvan Bunescu, Ruifang Ge, Rohit J Kate, Edward M Mar-
cotte, Raymond J Mooney, Arun K Ramani, and Yuk W
Wong. 2005. Comparative experiments on learning infor-
mation extractors for proteins and their interactions. Arti-
ficial Intelligence in Medicine, 33(2):139?155.
Keh-Jiann Chen, Wen Tsuei, and Lee-Feng Chien. 1998.
Pat-trees with the deletion function as the learning device
for linguistic patterns. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
244?250, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Lee-Feng Chien. 1997. Pat-tree-based keyword extrac-
tion for chinese information retrieval. SIGIR Forum,
31(SI):50?58.
Gene Ontology Consortium. 2001. Creating the gene ontol-
ogy resource: design and implementation. Genome Re-
search, 11(8):1425?1433.
Anna Divoli and Teresa K. Attwood. 2005. Bioie: extract-
ing informative sentences from the biomedical literature.
Bioinformatics, 21(9):2138?2139.
John E. Freund and Benjamin M. Perles. 2006. Modern
Elementary Statistics. Prentice Hall.
Jorg Hakenberg, Conrad Plake, Ulf Leser, Harald Kirsch,
and Dietrich Rebholz-Schuhmann. 2005. Lll?05 chal-
lenge: Genic interaction extraction with alignments and
finite state automata. In Proceedings of Learning Lan-
guage in Logic Workshop (LLL?05) at ICML, page 38C45,
Bonn, Germany.
Minlie Huang, Xiaoyan Zhu, Yu Hao, Donald G. Payan,
Kunbin Qu, and Ming Li. 2004. Discovering patterns to
extract protein-protein interactions from full texts. Bioin-
formatics, 20:3604?3612.
Hyunchul Jang, Jaesoo Lim, Joon-Ho Lim, Soo-Jun Park,
Kyu-Chul Lee, and Seon-Hee Park. 2006. Finding the ev-
idence for protein-protein interactions from pubmed ab-
stracts. Bioinformatics, 22(14):e220?e226.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005. Biological Named Entity Recognition using N-
grams and Classification Methods. In Proceedings of the
Conference Pacific Association for Computational Lin-
guistics, PACLING?05, Tokyo, Japan.
Jiao Li, Xian Zhang, Yu Hao, Minlie Huang, and Xiaoyan
Zhu. 2005. Learning domain-specific knowledge from
context?thuir at trec2005 genomics track. In Proceed-
ings of 14th Text Retrireval Conference (TREC2005),
Gaithersburg, USA.
Alexa T. Mccray and Olivier Bodenreider. 2002. A concep-
tual framework for the biomedical domain. In Semantics
of Relationships, Kluwer, pages 181?198. Kluwer Acad-
emic Publishers.
Andrei Mikheev. 2002. Periods, capitalized words, etc.
Comput. Linguist., 28(3):289?318.
Donald R. Morrison. 1968. Patricia ? Practical Algorithm
To Retrieve Information Coded in Alphanumeric. Jour-
nal of the ACM, 15(4):514?534.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami, and
Toshihisa Takagi. 2001. Automated extraction of infor-
mation on protein-protein interactions from the biological
literature. Bioinformatics, 17(2):155?161.
Conrad Plake, Jorg Hakenberg, and Ulf Leser. 2005. Learn-
ing patterns for information extraction from free text. In
Proceedings of AKKD 2005, Karlsruhe, Germany.
Lance Ramshaw and Mitch Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings of
the Third Workshop on Very Large Corpora, pages 82?94,
Somerset, New Jersey.
Soumya Raychaudhuri. 2006. Computational Text Analy-
sis: For Functional Genomics and Bioinformatics. Ox-
ford University Press.
Andre Skusa, Alexander Ruegg, and Jacob Kohler. 2005.
Extraction of biological interaction networks from scien-
tific literature. Brief Bioinform, 6(3):263?276.
141
Proceedings of BioNLP Shared Task 2011 Workshop, pages 164?172,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
               From Graphs to Events: A Subgraph Matching Approach
                for Information Eextraction from Biomedical Text
Haibin Liu, Ravikumar Komandur, Karin Verspoor
Center for Computational Pharmacology
University of Colorado School of Medicine
PO Box 6511, MS 8303, Aurora, CO, 80045 USA
Abstract
We participated in the BioNLP Shared Task 2011,
addressing the GENIA event extraction (GE) and
the Epigenetics and Post-translational Modifica-
tions (EPI) tasks. A graph-based approach is
employed to automatically learn rules for detect-
ing biological events in the life-science literature.
The event rules are learned by identifying the
key contextual dependencies from full syntactic
parsing of annotated text. Event recognition is
performed by searching for an isomorphism be-
tween event rules and the dependency graphs of
sentences in the input texts. While we explored
methods such as performance-based rule rank-
ing to improve precision, we merged rules across
multiple event types in order to increase recall.
We achieved a 41.13% F-score in detecting events
of nine types in the Task 1 of the GE task, and a
52.67% F-score in identifying events across fif-
teen types in the core task of the EPI task. Our
performance on both tasks is comparable to the
state-of-the-art systems. Our approach does not
require any external domain-specific resources.
The consistent performance on the two tasks sup-
ports the claim that the method generalizes well
to extract events from different domains where
training data is available.
1 Introduction
Recent research in information extraction in the biolog-
ical domain has focused on extracting semantic events
involving genes or proteins, such as binding events or
post-translational modifications. To date, most of the
biological knowledge about these events has only been
available in the form of unstructured text in scientific
articles (Abulaish and Dey, 2007; Ananiadou et al,
2010).
When a biological event is described in text, it can
be analyzed by recognizing its type, the trigger that sig-
nals the event, and one or more event arguments. The
BioNLP-ST 2009 (Kim et al, 2009) focused on the
recognition of semantically typed, complex events in
the biological literature. Although the best-performing
system achieved a 51.95% F-score in identifying events
across nine types, only 4 of the rest 23 participating
teams obtained an F-score in the 40% range. This sug-
gests that the problem of biological event extraction is
difficult and far from solved.
Graphs provide a powerful primitive for modeling
biological data such as pathways and protein interac-
tion networks (Tian et al, 2007; Yan et al, 2006). More
recently, the dependency representations obtained from
full syntactic parsing, with its ability to reveal long-
range dependencies, has shown an advantage in bi-
ological relation extraction over the traditional Penn
Treebank-style phrase structure trees (Miyao et al,
2009). Since the dependency representation maps
straightforwardly onto a directed graph, operations on
graphs can be naturally applied to the problem of bio-
logical event extraction.
We participated in the BioNLP-ST 2011 (Kim et al,
2011a), and applied a graph matching-based approach
(Liu et al, 2010) to tackling the Task 1 of the GE-
NIA event extraction (GE) task (Kim et al, 2011b), and
the core task of the Epigenetics and Post-translational
Modifications (EPI) task (Ohta et al, 2011), two main
tasks of the BioNLP-ST 2011. Event recognition is
performed by searching for an isomorphism between
dependency representations of automatically learned
event rules and complete sentences in the input texts.
This process is treated as a subgraph matching problem,
which corresponds to the search for a subgraph isomor-
phic to a rule graph within a sentence graph. While
we explored methods such as performance-based rule
ranking to improve the precision of the GE and EPI
tasks, we merged rules across multiple event types in
order to increase the recall of the EPI task.
The rest of the paper is organized as follows: In Sec-
tion 2, we introduce the BioNLP Shared Task 2011.
Section 3 describes the subgraph matching-based event
extraction method. Section 4 and Section 5 elabo-
164
rate the implementation details and our performance
respectively. Finally, Section 6 summarizes the paper
and introduces future work.
2 BioNLP Shared Task 2011
The BioNLP-ST 2011 is the extension of the BioNLP-
ST 2009 that focused on the recognition of events in the
biological literature. The BioNLP-ST 2011 extends the
previous task in three directions: the type of the inves-
tigated text, the domain of the subject, and the targeted
event types. As a result, the shared task was organized
into four independent tasks: GENIA Event Extraction
Task (GE), Epigenetics and Post-translational Modifi-
cations Task (EPI), Infectious Diseases Task (ID) and
Bacteria Track.
The definition of the GE task remained the same as
the BioNLP-ST 2009. However, additional annotated
texts that come from full papers were provided together
with the dataset of the 2009 task to generalize the task
from PubMed abstracts to full text articles. The pri-
mary task of the GE task was to detect biological events
of nine types such as protein binding and regulation,
given the annotation of protein names. It was required
to extract type, trigger, and primary arguments of each
event. This task is an example of extraction of seman-
tically typed, complex events for which the arguments
can also be other events. Such embedding results in a
nested structure that captures the underlying biological
statements more accurately.
Different from the subject domain of the GE task on
transcription factors in human blood cells, the EPI task
focused on events related to epigenetic change, includ-
ing DNA methylation and histone modification, as well
as other common post-translational protein modifica-
tions. The core task followed the definition for Phos-
phorylation event extraction in the 2009 task, and ex-
tended that basic event type to a total of fifteen types
including both positive and negative variants, for ex-
ample Acetylation and Deacetylation. The task dataset
was prepared from relevant PubMed abstracts, with
additional evidence sentences from databases such as
PubMeth (Ongenaert et al, 2007). Given the annota-
tion of protein names, the core task required to extract
type, trigger, and primary arguments of each event.
We focused on the primary task of GE and the core
task of EPI, and tackled the event extraction problem in
both cases using a graph matching-based method.
inhibit-2/VBP
Interferons-1/NNS
nsubj
activation-3/NN
dobj
inducing-12/VBG
prepc_by
BIO_Entity-5/NNP(STAT6)
prep_of
BIO_Entity-7/NNP(interleukin 4)
prep_by
monocytes-10/NNS
prep_in
human-9/JJ
amod
expression-15/NN
dobj
BIO_Entity-13/NNP(SOCS-1)
nn
gene-14/NN
nn
Figure 1: Dependency Graph Example
3 Subgraph Matching-based Event Extraction
3.1 Dependency Representation
The dependency representation of a sentence is formed
by tokens in the sentence and binary relations between
them. A single dependency relation is represented
as relation(governor, dependent), where governor and
dependent are tokens, and relation is a type of the
grammatical dependency relation. This representation
is essentially a labeled directed graph, which is named
dependency graph and defined as follows:
Definition 1. A dependency graph is a pair of sets
G = (V,E), where V is a set of nodes that correspond
to the tokens in a sentence, and E is a set of directed
edges, for which the edge labels are types of depen-
dency relations between the tokens, and the edge direc-
tion is from governor to dependent node.
Figure 1 illustrates the dependency graph for the sen-
tence: ?Interferons inhibit activation of STAT6 by in-
terleukin 4 in human monocytes by inducing SOCS-1
gene expression.? (MEDLINE: 10485906). The token
number in the sentence is appended to each token in
order to differentiate identical tokens that co-occur in a
sentence. All the protein names in the sentence have
been replaced with a unified tag ?BIO Entity?. The
POS tag of each token is noted. ?BIO Entity? tokens
are uniformly tagged as proper nouns.
3.2 Event Rule Induction
The premise of our work is that there is a set of fre-
quently occurring event rules that match a majority of
165
stated events about protein biology. We consider that
an event rule encodes the detailed description and char-
acterizes the typical contextual structure of a group of
biological events. The rules are learned from labeled
training sentences using a graph-based rule induction
method (Liu et al, 2010), and we briefly describe the
algorithm as follows.
Starting with the dependency graph of each training
sentence, edge directions are first removed so that the
directed graph is transformed into an undirected graph,
where a path must exist between any two nodes since
the graph is always connected. For each gold event, the
shortest dependency path in the undirected graph con-
necting the event trigger nodes to each event argument
node is selected. The union of all shortest dependency
paths is then computed, and the original directed de-
pendency representation of the path union is retrieved
and used as the graph representation of the event.
For multi-token event triggers, the shortest depen-
dency path connecting the node of every trigger token
to the node of each event argument is selected, and the
union of the paths is then computed for each trigger.
For regulation events, when a sub-event is used as an
argument, only the type and the trigger of the sub-event
are preserved as the argument of the main events. The
shortest dependency path is extracted so as to connect
the trigger nodes of the main event to the trigger nodes
of the sub-event. In case that there exists more than
one shortest path, all of the paths are considered. As a
result, each gold event is transformed into the form of
a biological event rule. The algorithm is elaborated in
more detail in (Liu et al, 2010). The obtained rules are
categorized in terms of the event types of the tasks.
3.3 Sentence Matching
We attempted to match event rules to each testing sen-
tence to extract events from the sentence using a sen-
tence matching approach. Since the event rules and the
sentences all possess a dependency graph, the matching
process is a subgraph matching problem, which cor-
responds to the search for a subgraph isomorphic to
an event rule graph within the graph of a testing sen-
tence. The subgraph matching problem is also called
subgraph isomorphism, defined in this work as follows:
Definition 2. An event rule graph Gr = (Vr, Er)
is isomorphic to a subgraph of a sentence graph Gs =
(Vs, Es), denoted by Gr ?= Ss ? Gs, if there is an
injective mapping f : Vr ? Vs such that, for every
directed pair of nodes vi, vj ? Vr, if (vi, vj) ? Er then
(f(vi), f(vj)) ? Es, and the edge label of (vi, vj) is
the same as the edge label of (f(vi), f(vj)).
The subgraph isomorphism problem is NP-complete
(Cormen et al, 2001). A number of algorithms have
been designed to tackle the problem of subgraph iso-
morphism in different applications (Ullmann, 1976;
Cordella et al, 2004; Pelillo et al, 1999). Considering
that the graphs of rules and sentences involved in the
matching process are small, a simple subgraph match-
ing algorithm using a backtracking approach (Liu et
al., 2010) was used in this work. It is named ?Injec-
tive Graph Embedding Algorithm? and designed based
on the Huet?s graph unification algorithm (Huet, 1975).
The formalized algorithm and the detailed description
are given in (Liu et al, 2010).
When matching between graphs, different combina-
tions of matching features can be applied, resulting in
different matching criteria. The features include edge
features (E) which are edge label and edge direction,
and node features which are POS tags (P), trigger to-
kens (T), and all tokens (A), ranging from the least spe-
cific matching criterion, E, to the much stricter crite-
rion, A. For each sentence, the algorithm returns all the
matched rules together with the corresponding injec-
tive mappings from rule nodes to sentence tokens. Bio-
logical events are then extracted by applying the event
descriptions of tokens in each matched rule consisting
of the type, the trigger and the arguments to the corre-
sponding tokens of the sentence.
4 Implementation
4.1 Preprocessing
The same preprocessing steps as in (Liu et al, 2010)
are completed on the datasets of the GE and the EPI
tasks before performing text mining strategies. These
include sentence segmentation and tokenization, Part-
of-Speech tagging, and sentence parsing.
The Stanford unlexicalized natural language parser
(version 1.6.5), which includes Genia Treebank 1.0
(Ohta et al, 2005) as training material, is used to ana-
lyze the syntactic structure of the sentences. The parser
returns a dependency graph for each sentence.
4.2 Rule Induction and Sentence Matching
For each gold event, the shortest path in the undirected
graph connecting the event trigger to each event argu-
ment is extracted using Dijkstra?s algorithm (Cormen
et al, 2001) with equal weight for edges.
Sentence matching is performed and the raw match-
ing results are then postprocessed based on the specifi-
cations of the shared task, such as event trigger cannot
166
be a protein name or another event.
5 Results and Evaluation
This section presents our results on the GE and the EPI
tasks (Kim et al, 2011b; Ohta et al, 2011) respectively.
Different experimental methods in processing the ob-
tained event rules are described for the purpose of im-
proving the precision of both tasks and increasing the
recall of the EPI task.
5.1 GE task
5.1.1 Preprocessing Results
For training data, only sentences that contain at least
one protein and one event are considered candidates
for further processing. For testing data, candidate sen-
tences contain at least one protein. Our event recog-
nition method focuses on extracting events from sen-
tences. Therefore, only sentence-based events are con-
sidered in this work. Table 1 presents some statistics of
the preprocessed datasets.
Attributes Counted Training Dev. Testing
Abstracts&Full articles 908 259 347
Total sentences 8,759 2,954 3,437
Candidate sentences 3,615 1,989 2,353
Total events 10,287 3,243 4,457
Sentence-based events 9,583 3,058 hidden
Table 1: Statistics of GE dataset
We were able to build event rules for 9,414 gold
events. Gold events in which the event trigger and
an event argument are not connected by a path in the
undirected dependency graph of the sentence could not
be transformed into a biological event rule. After re-
moving duplicate rules, we obtained 8,677 event rules,
which are distributed over nine event types. The rules
that are isomorphic to each other in terms of their graph
representation are not filtered at this stage as the dupli-
cate events they produce will be removed eventually to
prepare the annotations for the shared task.
5.1.2 Probability-based rule refining
We observed that some event rules of an event type
overlap with rules of other event types. For instance, a
Transcription rule is isomorphic to a Gene expression
rule in terms of the graph representation and they also
share a same event trigger token. In fact, tokens like
?gene expression? and ?induction? are used as event
trigger of both Transcription and Gene expression
in training data. Therefore, the detection of some
Gene expression events is always accompanied by cer-
tain Transcription events. This will have detrimen-
tal effects on the precision of both Transcription and
Gene expression event types.
As transcription is the first step leading to gene ex-
pression (Ananiadou and Mcnaught, 2005), there ex-
ist some correlations or associations between the two
event types. In tackling this problem, we processed
the overlapping rules based on a conditional probability
P (t|E), where t stands for an event trigger and E repre-
sents one of the event types. Eq.(1) is used to estimate
the value of P (ti|E).
P (ti|E) = f(ti, E)?
i f(ti, E)
, (1)
where f(ti, E) is the frequency of the event trigger ti
of the event type E in the training data, and?i f(ti, E)
calculates the total frequency of all event triggers of the
event type E in the training data.
P (ti|E) evaluates the degree of the importance of a
trigger to an event type. When the dependency graphs
of two rules of different event types are isomorphic to
each other, and two rules share a same event trigger,
we examine the P (ti|E) of each event type, and only
retain the rule for which the P (ti|E) is higher.
Compared to the ?once a trigger, always a trigger?
method employed in other work (Buyko et al, 2009;
Kilicoglu and Bergler, 2009), triggers are treated in a
more flexible way in our work. A token is not neces-
sarily always a trigger unless it appears in the appropri-
ate context. Also, the same token can serve as trigger
for different event types as long as it appears in the dif-
ferent context. A trigger will only be classified into a
fixed event type when it could serve as trigger for dif-
ferent event types in the same context.
5.1.3 Performance-based rule ranking
In addition to the process of refining rules across
event types, we proposed a performance-based rule
ranking method to evaluate each rule under one event
type. We matched each rule to sentences in the de-
velopment set using the subgraph matching approach.
For rules that produce at least one event prediction, we
ranked them by PRC(ri), the precision of each rule ri,
which is computed via Eq.(2).
PRC(ri) = #correctly predicted events by ri#predicted events by ri (2)
167
We manually examined the rules with low rank. In
our experiments, the PRC(ri) ratio of these rules is
bigger than 4:1. We removed the ones that are either in-
correct or ambiguous in semantics and syntactics based
on our domain knowledge. Our assumption is that these
rules will keep producing false positive events on the
testing data if they are retained in the rule set. For
rules that do not make any predictions on the develop-
ment data, we keep them in the set in the hope that they
may contribute to the event recognition from the testing
data. Without affecting much on the recall, this process
helps to improve the precision of the events extracted
from the development data.
5.1.4 GE Results on Development Set
In our previous work (Liu et al, 2010), the match-
ing criteria, ?E+P+T? and and ?E+P+A?, achieved the
highest F-score and the highest precision respectively
among all the investigated matching criteria. ?E+P+T?
requires that edge directions and labels of all edges (E)
be identical, POS tags (P) of all tokens be identical, and
tokens of only event triggers (T) be identical for the
edges and the nodes of a rule and a sentence to match
with each other. ?E+P+A? requires that edges (E), POS
tags (P) and all tokens (A) be exactly the same. In this
work, we focused on these two criteria and explored
to extend them for graph matching between event rules
and sentences.
We attempted to relax the matching criterion of POS
tags for nouns and verbs. For nouns, the plural form of
nouns is allowed to match with the singular form, and
proper nouns are allowed to match with regular nouns.
For verbs, past tense, present tense and base present
form are allowed to match with each other.
Next, letters of each token are transformed into lower
case, and tokens containing hyphens are normalized
into non-hyphenated forms. Lemmatization is then per-
formed on every pair of tokens to be matched using
WordNet (Fellbaum, 1998) as the lemmatizer to al-
low tokens that share a same lemma to match. Since
WordNet is a lexical database only for the general Eng-
lish language, the lemma of a fair amount of domain-
specific vocabulary cannot be found in WordNet, such
as ?Phosphorylation? and ?Methylation?. In this case,
a backup process is invoked to stem the tokens to
their root forms using the Porter?s stemming algorithm
(Porter, 1997) allowing the tokens derived from a same
root word to match.
To further generalize event rules, we extended
the matching criteria ?E+P*+A*? to ?E+P*+A*S?
to allow tokens to match if their lemmatized forms
have a common synonym in terms of the synsets
of WordNet. Since WordNet will relate verbs such
as ?induce? and ?receive? together as they share
a synonym ?have?, and allow nouns like ?expres-
sion? and ?aspect? to match as they share a syn-
onym ?face?, we limited this extension to only ad-
jective tokens to avoid too many false positive events
and allow tokens like ?crucial? and ?critical? to match.
Table 2 shows the event extraction results on the
development data based on different matching cri-
teria. The performance is evaluated by ?Approxi-
mate Span Matching/Approximate Recursive Match-
ing?, the primary evaluation measure of the shared task.
?E+P*+T*?, ?E+P*+A*? and ?E+P*+A*S? demon-
strate the performance of the extended criteria.
Feature Recall(%) Prec.(%) F-score(%)
E+P+A 28.03 66.74 39.48
E+P+T 31.17 52.38 39.09
E+P*+A* 31.45 63.51 42.07
E+P*+T* 35.71 46.26 40.31
E+P*+A*S 31.51 63.32 42.08
Table 2: GE results on development set using different
matching criteria
As the strictest matching criteria, ?E+P+A? performs
better than ?E+P+T? in both precision and F-score. Al-
though ?E+P+T? achieves a better recall, when relax-
ing the matching criteria from all tokens being the same
to only event trigger tokens having to be identical, the
precision of ?E+P+T? is decreased by a large margin,
nearly 14%. This indicates that a certain number of bi-
ological events are described in very similar ways in
the literature, involving same grammatical structures
and identical contextual contents. While producing
more incorrect events, ?E+P*+A*? and ?E+P*+T*?
significantly improve the recall, leading to a better
F-score over ?E+P+A? and ?E+P+T?. This confirms
the effectiveness of the POS relaxation and the to-
ken lemmatization on the generalization of event rules.
?E+P*+A*S? obtains a comparable performance with
?E+P*+A*? with only a 0.06% increase in recall and a
0.2% drop in precision.
5.1.5 GE Results on Testing Set
Table 3 shows our results of ?E+P*+A*? on the test-
ing data using the official metric. We are listed as
team ?CCP-BTMG?. Ranked by F-score, our perfor-
mance ranked 10th out of 15 participating groups. It
168
is worth noting that our result on the event type ?Pro-
tein catabolism? ranked 1st.
Event type Rec.(%) Prec.(%) F(%)
Gene expression 58.68 75.77 66.14
Transcription 39.08 51.91 44.59
Protein catabolism 66.67 83.33 74.07
Phosphorylation 63.78 85.51 73.07
Localization 29.32 91.80 44.44
Binding 22.61 49.12 30.96
Regulation 12.99 46.73 20.33
Positive regulation 21.90 44.51 29.35
Negative regulation 15.76 40.18 22.64
All total 31.57 58.99 41.13
Table 3: GE results of ?E+P*+A*? on testing set by ?Ap-
proximate Span /Approximate Recursive Matching?
The performance of our system on the testing set
is consistent with that of the development set. We
achieved a comparable precision with the top systems
and ranked 6th by precision. However, our recall was
lower, ranking 11th. This adversely impacted the over-
all F-score. The lower recall is not surprising because
the graph matching criteria ?E+P*+A*? strictly de-
mand that every lemmatized token in the patterns, other
than protein names represented as?BIO Entity?, has to
find its exact match in the input sentences. The detailed
analysis on the recall problem is presented in the ?Error
Classification? section.
While examining the false positives, we found that
for many cases our result matched the gold annotation
but for the trigger word. We believe that event type and
their arguments are more important biologically than
the trigger. We consulted some domain experts who
reinforced our intuition in many cases that different
words could be considered as trigger for the event in
question. Following this we contacted organizers and
they agreed to release a new evaluation scheme to ig-
nore the trigger match requirement in order to support
evaluation of the event extraction itself.
Table 4 shows our results of ?E+P*+A*? evaluated
by other official evaluation metrics of the task. The
strict matching scheme requires exact trigger span as
well as all its nested events to be recursively correct
for an event to be considered correctly extracted. Our
F-score in terms of the strict matching is only 2.65%
lower than the relaxed, primary measure, indicating
that most of the detected triggers are captured with cor-
rect text span. The organizers also provided the eval-
uation results on PubMed abstracts and PMC full text
articles separately. Our system performs consistently
on both abstracts and full papers and the difference be-
tween F-scores is less than 1% (41.39% vs. 40.47%)
mostly due to the small recall loss on full texts.
Measures R(%) P(%) F(%)
Strict Matching 29.55 55.13 38.48
Appr. SpanNoTrigger/Recur. 33.68 62.17 43.69
Appr. Span/Recur./Decomp. 32.56 66.20 43.65
Appr. Sp. No T./Recur./Decomp. 34.96 69.87 46.60
Appr. Span/Recur. (Abstract) 31.87 59.02 41.39
Appr. Span/Recur. (Full paper) 30.82 58.92 40.47
Table 4: GE results on testing set by other evaluation measures
5.2 EPI task
5.2.1 Preprocessing Results
Table 5 presents some statistics of the datasets. We
were able to build event rules for 1598 gold events. Af-
ter removing duplicate rules, we obtained 1,562 event
rules distributed over fifteen event types.
Attributes Counted Training Dev. Testing
Abstracts 600 200 440
Total sentences 6,411 2,218 4,640
Candidate sentences 1,054 1,241 2,839
Total events 1,738 582 1,194
Sentence-based events 1,643 536 hidden
Table 5: Statistics of EPI dataset
We processed the obtained rules following the
same rule refining and ranking processes of the GE
task. We experimented with two graph matching
criteria for extracting EPI events, ?E+P*+T*? and
?E+P*+A*?. From the preliminary results, we ob-
served that ?E+P*+A*? achieves a high precision over
80% but a lower recall around 33%. Compared to
the GE task results, ?E+P*+T*? achieves a better re-
call against a small tradeoff for precision. We consider
that this is because the event triggers themselves for
the EPI task such as ?acetylation?, ?deglycosylation?
and ?demethylation? are powerful enough to differen-
tiate among event types without the need to resort to
more contextual content of the patterns. Therefore, we
focused on using ?E+P*+T*? to extract events.
5.2.2 Recall-oriented rule merging
Since all the event types except Catalysis,
DNA methylation and DNA demethylation in the
169
EPI task involve addition or removal of biochemical
functional groups at a particular amino acid residue of
a protein (Hunter, 2009), common syntactic structures
of expressing the protein PTM events might be shared
across event types. To further improve the recall, we
proposed a rule merging strategy to take advantage of
the syntactic structures of rules across event types.
We first experimented with a ?pairwise flip? ap-
proach which combines rules of the pairwise, positive
and negative event types by flipping the type and the
trigger of event rules. For instance, the event rules
of Phosphorylation and Dephosphorylation are merged
together and then used to detect events of the two types
respectively.
Next, the ?pairwise flip? approach was extended to
an ?all in one? method. For one event type, the rules
of all other PTM event types are processed and merged
into the rules of the current type if the trigger of rules
of other types contains one of these 12 morphemes:
?acetyl?, ?glycosyl?, ?hydroxyl?, ?methyl?, ?phospho-
ryl?, ?ubiqui?, ?deacetyl?, ?deglycosyl?, ?dehydroxyl?,
?demethyl?, ?dephosphoryl?, ?deubiqui?. We consider
that event rules involving these morphemes in trigger
are more likely to discuss representative protein post-
translational modifications.
5.2.3 EPI Results on Development Set
Table 6 shows the event extraction results on the de-
velopment data using different matching criteria and
rule merging methods. The performance is evaluated
by the primary evaluation measure.
Feature Recall(%) Prec.(%) F(%)
E+P*+A* 32.65 79.83 46.34
E+P*+T* 38.14 73.51 50.23
E+P*+A*(pairwise) 35.22 80.39 48.98
E+P*+T*(pairwise) 40.89 77.52 53.54
E+P*+T*(all in one) 46.39 63.08 53.47
Table 6: EPI results on development set
The two rule merging methods using ?E+P*+T*?
outperform others in terms of F-score. The ?pairwise
flip? method achieves higher precision as the syntac-
tic structures of rules to describe the pairwise, positive
and negative events tend to be highly similar. However,
when merging all the rules across PTM event types,
although more events are captured, rules that involve
syntactic structures for expressing very specific events
of certain types may not generalize well on some other
types, resulting in incorrect events. Thus, the ?all in
one? approach significantly improves the recall while
producing many false positive events, leading to a F-
score comparable with the ?pairwise flip? method.
5.2.4 EPI Results on Testing Set
We conducted two runs on the testing data in terms
of ?E+P*+T*(pairwise)? and ?E+P*+T*(all in one)?.
Since the two rule merging methods achieve compara-
ble F-scores, we decided to submit a run with higher
recall. Table 7 shows our results of ?E+P*+T*? using
the ?all in one? approach on the official metrics. Only
7 teams participated in this task. For the core task, our
performance ranked 7th, only 0.16% lower in F-score
than the 6th team. When evaluating our results in terms
of the full task, we ranked 6th.
Feature Recall(%) Prec.(%) F(%)
E+P*+T*(core task) 45.06 63.37 52.67
E+P*+T*(full task) 23.44 37.93 28.97
Table 7: EPI results on testing set
Compared to the top teams, our F-score is mostly af-
fected by the lower recall. Although the run we submit-
ted achieves the highest recall among all our runs, our
recall is about 20% less than the best performing sys-
tem. Considering that most of the event types of the EPI
task tend to use tokens containing only a small fixed
set of domain-specific morphemes as triggers, the re-
call deficit is assumed to be lack of event rules that de-
scribe syntactic structures of expressing a fair amount
of EPI events.
5.3 Error Classification
Since the gold event annotation of the testing data is
hidden, we examined the event extraction results of the
development data to analyze the underlying errors. The
detailed analysis is reported in terms of false negative
and false positive events.
5.3.1 False negatives
It is shown that false negative events have a substan-
tial impact on the performance of all 15 participating
teams of the GE task. The best recall, 49.56%, cap-
tures less than half of the gold events in the testing set.
In our work, three major causes of false negatives are
determined for both tasks.
(1) Low coverage of rule set: For the GE task, the
graph matching criteria ?E+P*+A*? strictly asks every
lemmatized token in the patterns to find its exact match
in the input sentences. Although maintaining the pre-
cision at a high level, this directly limits the contextual
170
structure and content around the proteins and thus pre-
vents the recall from being higher.
Lemmatization helps to detect more events, however,
further generalization needs to be performed on the ex-
isting rules to relax the token matching requirement.
For instance, when ?lysine? appears in an event rule,
knowing that ?lysine? is an amino acid, the rule might
be further generalized to allow all amino acids to match
with each other in order to recognize more events.
For the EPI task, although ?E+P*+T*? requires to-
kens of only event triggers to be identical, we captured
less than half of the gold events. We noticed that many
trigger tokens in the development sentences do not ap-
pear as triggers in the training set. This leads to the
failure of extracting the corresponding events. Since
the training data is the only source of triggers in our
work, the coverage of triggers limits the generalization
power of event rules.
For both tasks, we found that many gold events are
described in grammatical structures that are not cov-
ered by the existing rules induced from the training sen-
tences. These structures tend to be more complex, in-
volving a long dependency path from the trigger to ar-
guments in the graphs of sentences. Events that consist
of these structures are not recognized as no matched
rules will be returned from the subgraph matching.
In order to further improve the recall, some post-
processing steps are necessary to be performed on the
raw dependency graphs of both rules and sentences in-
stead of using them in the graph matching directly. By
eliminating semantically unimportant nodes and group-
ing lexically connected nodes together, the rules can
be generalized to retain only their skeleton structures
while complex sentences can be syntactically simpli-
fied to allow event rules to match them.
(2) Compound error effect: In both tasks, reg-
ulation and catalysis event types can take sub-events
as arguments. Therefore, if the nested sub-events are
not correctly identified, the main events will not be ex-
tracted due to the compound error effect.
(3) Anaphora and coreference: Since our system
focuses on extracting events from sentences, events that
contain protein names spanning multiple sentences will
not be captured. Recognition of these events requires
the ability to do anaphora and coreference resolution in
biological text (Gasperin and Briscoe, 2008).
5.4 False positives
Three major causes of false positives are generalized
from our analysis.
(1) Assignment of overlapping event rules: The
conditional probability-based method to assign over-
lapped rules of different event types effectively reduces
the number of event candidates but leads to errors. For
instance, ?methylation? is used as the trigger for two
overlapping rules of DNA methylation and Methyla-
tion. Based on the P (ti|E), ?methylation? is classified
into DNA methylation. An erroneous DNA methylation
event is then detected from a development sentence in-
stead of the gold Methylation event. Although the trig-
ger and the participant are all identified correctly, the
event type is assigned wrongly.
In fact, the same contextual structure and con-
tent appear in both DNA methylation and Methylation
events in the training data. According to the EPI
task (Ohta et al, 2011), Methylation is to abbreviate
for ?protein methylation? and thus is different from
DNA methylation. In this case, the only way to dis-
tinguish between the two types is to identify that the
biological entity mentioned in the sentence is a gene for
DNA methylation and a protein for Methylation. Since
genes and their products are uniformly annotated as
?Protein? in the task, it is not possible to assign a cor-
rect event type in this case from the perspective of the
event extraction itself.
(2) Lack of postprocessing rules: Some misiden-
tified events require customized postprocessing rules.
For instance, a Gene expression event is detected from
the phrase ?Tax expression vector? of a development
sentence. However, since ?Tax expression? is only
used as an adjective to describe ?vector? in this context,
the identified Gene expression event is not appropriate.
Likewise, ?Sp1 transcription? should not be identified
as an event in the context of ?Sp1 transcription factors?.
(4) Inconsistencies in gold annotation: Some ex-
tracted events are considered biologically meaningful
but evaluated as false positives due to the inconsisten-
cies in the gold annotation. In Table 4, the 3.2% in-
crease in precision of the no-trigger evaluation measure
over the primary evaluation scheme indicates that the
inconsistent gold annotations of event triggers.
6 Conclusion and future work
We used dependency graphs to automatically induce
biological event rules from annotated events. We ex-
plored methods such as performance-based rule rank-
ing to improve the accuracy of the obtained rules, and
we merged rules across multiple event types in order to
increase the coverage of the rules. The event extraction
process is treated as a subgraph matching problem to
171
search for the graph of an event rule within the graph of
a sentence. We tackled two main tasks of the BioNLP
Shared Task 2011. We achieved a 41.13% F-score in
detecting events across nine types in the Task 1 of the
GE task, and a 52.67% F-score in identifying events
across fifteen types in the core task of the EPI task.
In future work, we would like to explore the ap-
proaches of generalizing the raw dependency graphs of
both event rules and sentences in order to improve the
recall of our event extraction system. We also plan to
extend our system to tackle the other sub-tasks in GE
and EPI tasks, such as to extract events with additional
arguments like site and location, and to recognize nega-
tions and speculations regarding the extracted events.
References
Muhammad Abulaish and Lipika Dey. 2007. Biological re-
lation extraction and query answering from medline ab-
stracts using ontology-based text mining. Data & Knowl-
edge Engineering, 61(2):228?262.
Sophia Ananiadou and John Mcnaught. 2005. Text Mining
for Biology And Biomedicine. Artech House Publishers.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems bi-
ology by text mining the literature. Trends in Biotechnol-
ogy, 28(7):381?390.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and Udo
Hahn. 2009. Event extraction from trimmed dependency
graphs. In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 19?27, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Luigi P. Cordella, Pasquale Foggia, Carlo Sansone, and
Mario Vento. 2004. A (sub)graph isomorphism algo-
rithm for matching large graphs. IEEE Trans. Pattern
Anal. Mach. Intell., 26(10):1367?1372.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,
and Clifford Stein. 2001. Introduction to Algorithms.
The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In COLING
?08: Proceedings of the 22nd International Conference on
Computational Linguistics, pages 257?264, Morristown,
NJ, USA. Association for Computational Linguistics.
Ge?rard P. Huet. 1975. A unification algorithm for typed
lambda-calculus. Theor. Comput. Sci., 1(1):27?57.
Lawrence Hunter. 2009. The Processes of Life: An Intro-
duction to Molecular Biology. The MIT Press.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extraction.
In Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing: Shared Task,
BioNLP ?09, pages 119?127.
Jin-Dong Kim, Yoshinobu Kano Tomoko Ohta,
Sampo Pyysalo, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Proceedings
of the NAACL-HLT 2009 Workshop on Natural Language
Processing in Biomedicine (BioNLP?09), pages 1?9.
ACL.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview of BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori
Yonezawa. 2011b. Overview of the Genia Event task in
BioNLP Shared Task 2011. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational Lin-
guistics.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2010. Bio-
logical event extraction using subgraph matching. In Pro-
ceedings of the 4th International Symposium on Semantic
Mining in Biomedicine (SMBM-2010), October.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contribu-
tions of natural language parsers to protein?protein inter-
action extraction. Bioinformatics, 25(3):394?400.
Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2005. Syn-
tax annotation for the genia corpus. In Proceedings of the
IJCNLP 2005, pages 222?227.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational Mod-
ifications (EPI) task of BioNLP Shared Task 2011. In
Proceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. Associ-
ation for Computational Linguistics.
Mate Ongenaert, Leander Van Neste, Tim De Meyer, Ger-
ben Menschaert, Sofie Bekaert, and Wim Van Criekinge.
2007. Pubmeth: a cancer methylation database combin-
ing text-mining and expert annotation. Nucleic Acids Re-
search, pages 1?5.
Marcello Pelillo, Kaleem Siddiqi, and Steven W. Zucker.
1999. Matching hierarchical structures using associa-
tion graphs. IEEE Trans. Pattern Anal. Mach. Intell.,
21(11):1105?1120.
M. F. Porter. 1997. An algorithm for suffix stripping. pages
313?316.
Yuanyuan Tian, Richard C. Mceachin, Carlos Santos,
David J. States, and Jignesh M. Patel. 2007. Saga: a
subgraph matching tool for biological graphs. Bioinfor-
matics, 23(2):232?239.
J. R. Ullmann. 1976. An algorithm for subgraph isomor-
phism. J. ACM, 23(1):31?42.
Xifeng Yan, Feida Zhu, Jiawei Han, and Philip S. Yu. 2006.
Searching substructures with superimposed distance. In
ICDE ?06: Proceedings of the 22nd International Con-
ference on Data Engineering, page 88, Washington, DC,
USA. IEEE Computer Society.
172
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Generalizing an Approximate Subgraph Matching-based System to Extract
Events in Molecular Biology and Cancer Genetics
Haibin Liu
haibin.liu@nih.gov
NCBI, Bethesda, MD, USA
Karin Verspoor
karin.verspoor@nicta.com.au
NICTA, Melbourne, VIC, Australia
Donald C. Comeau
comeau@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Andrew MacKinlay
andrew.mackinlay@nicta.com.au
NICTA, Melbourne, VIC, Australia
W. John Wilbur
wilbur@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Abstract
We participated in the BioNLP 2013 shared
tasks, addressing the GENIA (GE) and the Can-
cer Genetics (CG) event extraction tasks. Our
event extraction is based on the system we re-
cently proposed for mining relations and events
involving genes or proteins in the biomedical
literature using a novel, approximate subgraph
matching-based approach. In addition to han-
dling the GE task involving 13 event types uni-
formly related to molecular biology, we gener-
alized our system to address the CG task tar-
geting a challenging set of 40 event types re-
lated to cancer biology with various arguments
involving 18 kinds of biological entities. More-
over, we attempted to integrate a distributional
similarity model into our system to extend the
graph matching scheme for more events. In ad-
dition, we evaluated the impact of using paths of
all possible lengths among event participants as
key contextual dependencies to extract potential
events as compared to using only the shortest
paths within the framework of our system.
We achieved a 46.38% F-score in the CG task
and a 48.93% F-score in the GE task, ranking
3rd and 4th respectively. The consistent perfor-
mance confirms that our system generalizes well
to various event extraction tasks and scales to
handle a large number of event and entity types.
1 Introduction
Understanding the sophisticated interactions between
various components of biological systems and conse-
quences of these biological processes on the function
and behavior of the systems provides profound im-
pacts on translational biomedical research, leading to
more rapid development of new therapeutics and vac-
cines for combating diseases. For the past five years,
the BioNLP shared task series has served as an in-
strumental platform to promote the development of
text mining methodologies and resources for the au-
tomatic extraction of semantic events involving genes
or proteins such as gene expression, binding, or reg-
ulatory events from the biomedical literature (Kim et
al., 2009; Kim et al, 2011). An event typically cap-
tures the association of multiple participants of vary-
ing numbers and with diverse semantic roles (Anani-
adou et al, 2010). Since events often serve as partic-
ipants in other events, the extraction of such nested
event structures provides an integrated, network view
of these biological processes.
Previous shared tasks focused exclusively on
events at the molecular and sub-cellular level. How-
ever, biological processes at higher levels of organi-
zation are equally important, such as cell prolifer-
ation, organ growth and blood vessel development.
While preserving the classic event extraction tasks
such as the GE task, the BioNLP-ST 2013 broad-
ens the scope of application domains by introducing
many new issues in biology such as cancer genetics
and pathway curation. On behalf of NCBI (National
Center for Biotechnology Information), our team par-
ticipated in the GENIA (GE) task and the Cancer Ge-
netics (CG) task. Compared to the GE task that aims
for 13 types of events concerning the protein NF-?B,
the CG task targets a challenging set of 40 types of
biological processes related to the development and
progression of cancer involving 18 entity types. This
additionally requires that event extraction systems be
able to associate entities and events at the molecular
level with anatomy level effects and organism level
outcomes of cancer biology.
Our event extraction is based on the system we re-
cently proposed for mining relations and events in-
volving genes or proteins in the biomedical litera-
ture using a novel, Approximate Subgraph Matching-
based (ASM) approach (Liu et al, 2013a). When
evaluated on the GE task of the BioNLP-ST 2011, its
performance is comparable to the top systems in ex-
tracting 9 types of biological events. In the BioNLP-
76
ST 2013, we generalized our system to investigate
both CG and GE tasks. Moreover, we attempted to in-
tegrate a distributional similarity model into the sys-
tem to extend the graph matching scheme for more
events. The graph representation that considers paths
of all possible lengths (all-paths) between any two
nodes has been encoded in graph kernels used in
conjunction with Support Vector Machines (SVM),
and led to state-of-the-art performance in extracting
protein-protein (Airola et al, 2008) and drug-drug in-
teractions (Zhang et al, 2012). Borrowing from the
idea of the all-paths representation, in addition, we
evaluated the impact of using all-paths among event
participants as key contextual dependencies to extract
potential events as compared to using only the short-
est paths within the framework of our system.
The rest of the paper is organized as follows: In
Section 2, we briefly introduce our ASM-based event
extraction system. Section 3 describes our experi-
ments aiming to extend our system. Section 4 elab-
orates some implementation details and Section 5
presents our results and discussion. Finally, Section
6 summarizes the paper and introduces future work.
2 ASM-based Event Extraction
The underlying assumption of our event extraction
approach is that the contextual dependencies of each
stated biological event represent a typical context for
such events in the biomedical literature. Our ap-
proach falls into the machine learning category of
instance-based reasoning (Alpaydin, 2004). Specif-
ically, the key contextual structures are learned from
each labeled positive instance in a set of train-
ing data and maintained as event rules in the form
of subgraphs. Extraction of events is performed
by searching for an approximate subgraph isomor-
phism between key dependencies and input sen-
tence graphs using an approximate subgraph match-
ing (ASM) algorithm designed for literature-based
relational knowledge extraction (Liu et al, 2013a).
By introducing error tolerance into the graph match-
ing process, our approach is capable of retrieving
events encoded within complex dependency contexts
while maintaining the extraction precision at a high
level. The ASM algorithm has been released as open
source software1. See (Liu et al, 2013a) for more de-
tails on the ASM algorithm, its complexity and the
comparison with existing graph distance metrics.
Figure 1 illustrates the overall architecture of our
ASM-based system with three core components high-
1http://asmalgorithm.sourceforge.net
lighted: rule induction, sentence matching and rule
set optimization. Our approach focuses on extract-
ing events expressed within the boundaries of a single
sentence. It is also assumed that entities involved in
the target event have been annotated. Next, we briefly
describe the core components of the system.
Rule Induction
Preprocessing
Sentence Matching
Postprocessing
Training data Testing data
Rule Set
Optimization
Figure 1: ASM-based Event Extraction Framework
2.1 Rule Induction
Event rules are learned automatically using the fol-
lowing method. Starting with the dependency graph
of each training sentence, for each annotated event,
the shortest dependency path connecting the event
trigger to each event argument in the undirected ver-
sion of the graph is selected. While additional in-
formation such as individual words in each sentence
(bag-of-words), sequences of words (n-grams) and
semantic concepts is typically used in the state-of-
the-art supervised learning-based systems to cover a
broader context (Airola et al, 2008; Buyko et al,
2009; Bjo?rne et al, 2012), the shortest path be-
tween two tokens in the dependency graph is par-
ticularly likely to carry the most valuable informa-
tion about their mutual relationship (Bunescu and
Mooney, 2005a; Thomas et al, 2011b; Rinaldi et
al., 2010). In case there exists more than one short-
est path, all of them are considered. For multi-token
event triggers, the shortest path connecting every trig-
ger token to each event argument is extracted, and the
union of the paths is then computed for each trigger.
For regulatory events that take a sub-event as an ar-
gument, the shortest path is extracted so as to connect
the trigger of the main event to that of the sub-event.
For complex events that involve multiple argu-
ments, we computed the dependency path union of
all shortest paths from trigger to each event argument,
resulting in a graph in which all event participants are
jointly depicted. Individual dependency paths con-
necting triggers to each argument are also considered
to determine event arguments independently. If the
77
resulting arguments share the same event trigger, they
are grouped together to form a potential event. In our
approach, the individual paths aim to retrieve more
potential events while the path unions retain the pre-
cision advantage of joint inference.
While the dependencies of such paths are used as
the graph representation of the event, a detailed de-
scription records the participants of the event, their
semantic role labels and the associated nodes in the
graph. All participating biological entities are re-
placed with a tag denoting their entity type, e.g. ?Pro-
tein? or ?Organism?, to ensure generalization of the
learned rules. As a result, each annotated event is
generalized and transformed into a generic graph-
based rule. The resulting event rules are categorized
into different target event types.
2.2 Sentence Matching
Event extraction is achieved by matching the induced
rules to each testing sentence and applying the de-
scriptions of rule tokens (e.g. role labels) to the cor-
responding sentence tokens. Since rules and sentence
parses all possess a graph representation, event recog-
nition becomes a subgraph matching problem. We
introduced a novel approximate subgraph matching
(ASM) algorithm (Liu et al, 2013a) to identify a sub-
graph isomorphic to a rule graph within the graph of
a testing sentence. The ASM problem is defined as
follows.
Definition 1. An event rule graph Gr =
(Vr, Er) is approximately isomorphic to a subgraph
Ss of a sentence graph Gs = (Vs, Es), denoted
by Gr ?=t Ss ? Gs, if there is an injective
mapping f : Vr ? Vs such that, for a given
threshold t, t ? 0, the subgraph distance be-
tween Gr and Gs satisfies 0 ? subgraphDistf (Gr,
Gs) ? t, where subgraphDistf (Gr, Gs) = ws ?
structDistf (Gr, Gs) + wl ? labelDistf (Gr, Gs) +
wd ? directionalityDistf (Gr, Gs).
The subgraph distance is proposed to be the
weighted summation of three penalty-based measures
for a candidate match between the two graphs. The
measure structDist compares the distance between
each pair of matched nodes in one graph to the
distance between corresponding nodes in the other
graph, and accumulates the structural differences.
The distance in rule graphs is defined as the length
of the shortest path between two nodes. The distance
in sentence graphs is defined as the length of the path
between corresponding nodes that leads to minimum
structural difference with the distance in rule graphs.
Because dependency graphs are edge-labeled, ori-
ented graphs, the measures labelDist and direction-
alityDist evaluate respectively the overall differences
in edge labels and directionalities on the compared
path between each pair of matched nodes in the two
graphs. The real numbers ws, wl and wd are non-
negative weights associated with the measures.
The weights ws, wl and wd are defaulted to be
equal but can be tuned to change the emphasis of the
overall distance function. The distance threshold t
controls the isomorphism quality of the retrieved sub-
graphs from sentences. A smaller t allows only lim-
ited variations and always looks for a sentence sub-
graph as closely isomorphic to the rule graph as pos-
sible. A larger t enables the extraction of events de-
scribed in complicated dependency contexts, thus in-
creasing the chance of retrieving more events. How-
ever, it can incur a bigger search cost due to the eval-
uation of more potential solutions.
An iterative, bottom-up matching process is used
to ensure the extraction of complex and nested events.
Starting with the extraction of simple events, simple
event rules are first matched with a testing sentence.
Next, as potential arguments of higher level events,
obtained simple events continue to participate in the
subsequent matching process between complex event
rules and the sentence to initiate the iterative process
for detecting complex events with nested structures.
The process terminates when no new candidate event
is generated for the testing sentence.
During the matching phase we relax the event
rules that contain sub-event arguments such that any
matched event can substitute for the sub-event. We
believe that the contextual structures linking anno-
tated sub-events of a certain type are generalizable
to other event types. This relaxation increases the
chance of extracting complex events with nested
structures but still takes advantage of the contextual
constraints encoded in the rule graphs.
2.3 Rule Set Optimization
Typical of instance-based reasoners, the accuracy of
rules with which to compare an unseen sentence is
crucial to the success of our approach. For instance, a
Transcription rule encoding a noun compound mod-
ification dependency between ?TNF? and ?mRNA?
derived from an event context ?expression of TNF
mRNA? should not produce a Transcription event
for the general phrase ?level of TNF mRNA? even
though they share a matchable dependency. Such
matches result in false positive events.
78
Therefore, we measured the accuracy of each rule
ri in terms of its prediction result via Eq.(1). For rules
that produce at least one prediction, we ranked them
byAcc(ri) and excluded the ones with aAcc(ri) ratio
lower than an empirical threshold, e.g. 1:4.
Acc(ri) =
#correct predictions by ri
#total predictions by ri
(1)
Because of nested event structures, the removal
of some rules might incur a propagating effect on
rules relying on them to produce arguments for the
extraction of higher order events. Therefore, an it-
erative rule set optimization process, in which each
iteration performs sentence matching, rule ranking
and rule removal sequentially, is conducted, lead-
ing to a converged, optimized rule set. While the
ASM algorithm aims to extract more potential events,
this performance-based evaluation component en-
sures the precision of our event extraction framework.
3 Extensions to Event Extraction System
In the BioNLP-ST 2013, we attempted two different
ways to extend the current event extraction system:
(1) integrate a distributional similarity model into the
system to extend the graph matching scheme for more
events; (2) use paths of all possible lengths (all-paths)
among event participants as key contextual depen-
dencies to extract events. We next elaborate these
system extensions in detail.
3.1 Integrating Distributional Similarity Model
The proposed subgraph distance measure of the ASM
algorithm focuses on capturing differences in the
overall graph structure, edge labels and directional-
ities. However, when determining the injective node
mapping between graphs, the matching remains at the
surface word level.
In the current setting, various node features can be
considered when comparing two graph nodes, result-
ing in different matching criteria. The features in-
clude POS tags (P), event trigger (T), token lemmas
(L) and tokens themselves (A). For instance, a match-
ing criterion, ?P*+L?, requires that the relaxed POS
tags (P*) and the lemmatized form (L) of tokens be
identical for each rule node to match with a sentence
node. The relaxed POS allows the plural form of
nouns to match with the singular form, and the con-
jugations of verbs to match with each other. How-
ever, the inability to go beyond surface level match-
ing prevents node tokens that share similar meaning
but possess distinct orthography from matching with
each other. For instance, a mismatch between rule
token ?crucial? and a sentence token ?critical? could
lead to an undiscovered Positive regulation event.
We attempted to use only POS information in the
node matching scheme and observed a nearly 14%
increase in recall (Liu et al, 2013b). However, the
precision drops sharply, resulting in an undesirable
F-score. This indicates that the lexical information
is a critical supplement to the contextual dependency
constraints in accurately capturing events within the
framework of our system. Moreover, we attempted to
extend the node matching using the synsets of Word-
Net (Fellbaum, 1998) to allow tokens to match with
their synonyms (Liu et al, 2011). However, since
WordNet is developed for the general English lan-
guage, it relates biomedical terms e.g., ?expression?
with general words such as ?aspect? and ?face?, thus
leading to incorrect events.
In this work, we integrated a distributional simi-
larity model (DSM) into our node matching scheme
to further improve the generalization of event rules.
A distributional similarity model is constructed
based on the distributional hypothesis (Harris, 1954):
words that occur in the same contexts tend to share
similar meanings. We expect that the incorporation
of DSM will enable our system to capture matching
tokens in testing sentences that do not appear in the
training data while maintaining the extraction pre-
cision at a high level. There have been many ap-
proaches to compute the similarity between words
based on their distribution in a corpus (Landauer and
Dumais, 1997; Pantel and Lin, 2002). The output is a
ranked list of similar words to each word. We reim-
plemented the model proposed by (Pantel and Lin,
2002) in which each word is represented by a fea-
ture vector and each feature corresponds to a context
where the word appears. The value of the feature
is the pointwise mutual information (Manning and
Schu?tze, 1999) between the feature and the word. Let
c be a context and Fc(w) be the frequency count of a
word w occurring in context c. The pointwise mutual
information, miw,c between c and w is defined as:
miw,c =
Fc(w)
N?
i
Fi(w)
N ?
?
j
Fc(j)
N
(2)
where N =
?
i
?
j
Fi(j) is the total frequency count
of all words and their contexts.
Since mutual information is known to be biased
towards infrequent words/features, the above mutual
79
information value is multiplied by a discounting fac-
tor as described in (Pantel and Lin, 2002). The simi-
larity between two words is then computed using the
cosine coefficient (Salton and McGill, 1986) of their
mutual information vectors.
We experimented with two different approaches to
integrate the DSM into our event extraction system.
First, the model is directly embedded into the node
matching scheme. Once a match cannot be deter-
mined by surface tokens, the DSM is invoked to allow
a match if the sentence token appears in the list of the
top M most similar words to the rule token. Sec-
ond, additional event rules are generated by replac-
ing corresponding rule tokens with their top M most
similar words, rather than allow DSM to participate
in the node matching. While the first method mea-
sures the consolidated extraction ability of an event
rule by combining its DSM-generalized performance,
the second approach provides a chance to evaluate the
impact of each DSM-introduced similar word indi-
vidually on event extraction.
3.2 Adopting All-paths for Event Rules
Airola et al proposed an all-paths graph (APG) ker-
nel for extracting protein-protein interactions (PPI),
in which the kernel function counts weighted shared
dependency paths of all possible lengths (Airola et
al., 2008). Thomas et al adopted this kernel as
one of the three models used in the ensemble learn-
ing for extracting drug-drug interactions (Thomas et
al., 2011a) and won the recent DDIExtraction 2011
challenge (Segura-Bedmar et al, 2011). The JULIE
lab adapted the APG kernel to event extraction us-
ing syntactically pruned and semantically enriched
dependency graphs (Buyko et al, 2009).
The graph representation of the kernel consists of
two sub-representations: the full dependency parse
and the surface word sequence of the sentence where
a pair of interacting entities occurs. At the expense
of computational complexity, this representation en-
ables the kernel to explore broader contexts of an
interaction, thus taking advantage of the entire de-
pendency graph of the sentence. When comparing
two interaction instances, instead of using only the
shortest path that might not always provide suffi-
cient syntactic information about relations, the ker-
nel considers paths of all possible lengths between
any two nodes. More recently, a hash subgraph pair-
wise (HSP) kernel-based approach was also proposed
for drug-drug interactions and adopts the same graph
representation as the APG kernel (Zhang et al, 2012).
In contrast, the graph representation that our ASM
algorithm searches in a sentence is inherently re-
stricted to the shortest path among target entities in
event rules, as described in Section 2.2. Borrowing
from the idea of the all-path graph representation, in
this work we attempted to explore contexts beyond
the shortest paths to enrich our rule set. We evalu-
ated within the framework of our system the impact
of using acyclic paths of all possible lengths among
event participants as key contextual dependencies to
populate the event rule set as compared to using only
the shortest paths in the current system setting.
4 Implementation
4.1 Preprocessing
We employed the preprocessed data in the
BioC (Comeau et al, 2013) compliant XML format
provided by the shared task organizers as supporting
resources. The BioC project attempts to address
the interoperability among existing natural language
processing tools by providing a unified BioC XML
format. The supporting analyses include tokeniza-
tion, sentence segmentation, POS tagging and
lemmatization. Different syntactic parsers analyze
text based on different underlying methodologies, for
instances, the Stanford parser (Klein and Manning,
2003) performs joint inference over the product of an
unlexicalized Probabilistic Context-Free Grammar
(PCFG) parser and a lexicalized dependency parser
while the McClosky-Charniak-Johnson (Charniak)
parser (McClosky and Charniak, 2008) is based on
N -best parse reranking over a lexicalized PCFG
model. In order to take advantage of multiple aspects
of structural analysis of sentences, both Stanford
parser and Charniak parser, which are among the best
performing parsers trained on the GENIA Treebank
corpus, are used to parse the training sentences and
produce dependency graphs for learning event rules.
Only the Charniak parser is used on the testing
sentences in the event extraction phase.
4.2 ASM Parameter Setting
The GE task includes 13 different event types. Since
each type possesses its own event contexts, an indi-
vidual threshold te is assigned to each type. Together
with the 3 distance function weights ws, wl and wd,
the ASM requires 16 parameters for the GE event ex-
traction task. Similarly, the ASM requires 43 param-
eters to cater to the 40 diverse event types of the CG
task. As reported in (Liu et al, 2013a), we used a
genetic algorithm (GA) (Cormen et al, 2001) to au-
80
tomatically determine values of the 12 ASM param-
eters for the 2011 GE task using the training data.
We inherited these previously determined parameters
and adapted them into the 2013 tasks according to
the event type and its argument configuration. For in-
stance, ?Pathway? events in the CG task is assigned
the same te as the ?Binding? events in the GE task as
they possess similar argument configurations.
Table 1 shows the parameter setting for the 2013
GE task with the equal weights ws = wl = wd con-
straint. The graph node matching criterion ?P*+L?
that requires the relaxed POS tags and the token lem-
mas to be identical is used in the ASM.
Parameter Value Parameter Value
tGene expression 8 tUbiquitination 3
tTranscription 7 tBinding 7
tProtein catabolism 10 tRegulation 3
tPhosphorylation 8 tPositive regulation 3
tLocalization 8 tNegative regulation 3
tAcetylation 3 ws 10
tDeacetylation 3 wl 10
tProteinmodification 3 wd 10
Table 1: ASM parameter setting for the 2013 GE task
4.3 Distributional Similarity Model
In our implementation, we made following improve-
ments to the original Pantel model (Pantel and Lin,
2002): (1) lemmas of words generated by the Bi-
oLemmatizer (Liu et al, 2012) are used to achieve
generalization. The POS information is combined
with each lemmatized word to disambiguate its cat-
egory. (2) instead of the linear context where a
word occurs, we take advantage of dependency con-
texts inferred from dependency graphs. For instance,
?toxicity?amod? is extracted as a feature of the to-
ken ?nonhematopoietic JJ?. It captures the dependent
token, the type and the directionality of the depen-
dency. (3) the resulting miw,c is scaled into the [0, 1]
range by
? ?miw,c
1 + ? ?miw,c
to avoid greater miw,c values
dominating the similarity calculation between words.
An empirical ? = 0.01 is used. (4) while only the
immediate dependency contexts of a word are used
in our model, our implementation is flexible so that
contexts of various dependency depths could be taken
into consideration.
In order to cover a wide range of words and capture
the diverse usages of them in biomedical texts, in-
stead of resorting to an existing corpus, our distribu-
tional similarity model is built based on a random se-
lection of 5 million abstracts from the entire PubMed.
When computing miw,c, we filtered out contexts of
each word where the word occurs less than 5 times.
Eventually, the model contains 2.8 million distinct to-
kens and 0.4 million features. When it is queried with
an amino acid, e.g, ?lysine?, the top 15 tokens in the
resulting ranked list are all correct amino acid names.
5 Results and Discussion
This section reports our results on the GE and the CG
tasks respectively, including the attempted extensions
to our ASM-based event extraction system.
5.1 GE task
5.1.1 Datasets
The 2013 GE task dataset is composed of full-text
articles from PubMed Central, which are divided into
smaller segments by the task organizers according to
various sections of the articles. Table 2 presents some
statistics of the GE dataset.
Attributes Counted Training Development Testing
Full article segments 222 249 305
Proteins 3,571 4,138 4,359
Annotated events 2,817 3,199 3,301
Table 2: Statistics of BioNLP-ST 2013 GE dataset
As distributed, the development set is bigger than
the training set. For better system generalization, we
randomly reshuffled the data and created a 353/118
training/development division, a roughly 3:1 ratio
consistent with the settings in previous GE tasks.
The results reported on the training/development data
thereafter are based on our new data partition.
5.1.2 GE Results on Development Set
Table 3 shows the event extraction results on the 118
development documents based on event rules derived
from different parsers. Only the numbers of unique,
optimized rules are reported and those that possess
isomorphic graph representations determined by an
Exact Subgraph Matching (ESM) algorithm (Liu et
al., 2013b) are removed. The ensemble rule set com-
bines rules derived from both parsers and achieves
a better performance than that of using individual
parsers. It makes sense that the Charniak parser is
favored and leads to a performance close to the en-
semble performance because sentences from which
events are extracted are parsed by the Charniak parser
as well. However, we retained the additional rules
from the Stanford parser in the hope that they may
contribute to the testing data.
When embedding the distributional similarity
model (DSM) directly into the graph node matching
81
Parser Type Event Rule Recall Precision F-score
Charniak 2,923 47.01% 66.01% 54.91%
Stanford 3,305 43.66% 67.67% 53.08%
Ensemble 4,617 47.45% 65.65% 55.09%
Table 3: Performance of using different parsers
scheme, we performed the DSM on all rule tokens ex-
cept biological entities, meaning that for each rule to-
ken, if a match will be granted if a rule token appears
in the top M most similar word list of a sentence to-
ken, e.g., ?DSM 3? denotes the top 3 similar words
determined by the DSM. We further performed DSM
only on trigger tokens for comparison, as presented
in Table 4.
All Tokens Recall Precision F-score
DSM 1 47.98% 52.56% 50.17%
DSM 3 48.68% 35.07% 40.77%
DSM 10 53.43% 19.38% 28.44%
Trigger Tokens Recall Precision F-score
DSM 1 48.06% 54.22% 50.95%
DSM 3 48.59% 37.00% 42.01%
DSM 10 53.35% 24.65% 33.72%
Table 4: Performance of integrated DSM
Even though the DSM helps to substantially in-
crease the recall to 53.43%, we observed a significant
precision drop which leads to an inferior F-score to
the ensemble baseline in Table 3. A close evaluation
of the generated graph matches reveals that antonyms
produced by the DSM contributes to most of the false
positive events. For instance, the most similar words
for the verb ?increase? and the adjective ?high? re-
turned by the model are ?decrease? and ?low? be-
cause they tend to occur in the same contexts. Fur-
ther investigation is needed to automatically filter out
the antonyms. When generating additional rules us-
ing the top M most similar words from the DSM,
since all the rules undergo the optimization process,
the event extraction precision is ensured. However,
the recall increase from simple events is diluted by
the counter effect of the introduced false positives in
detecting regulation-related complex events, result-
ing in a comparable performance to the baseline.
Table 5 gives the performance comparison of us-
ing all-paths and the shortest paths in our event ex-
traction system. Using all-paths does not bring in a
significant improvement in F-score but takes 27 it-
erations to optimize as compared to the 5-iteration
optimization on shortest paths. Most of the rules in-
duced from all-paths are eventually discarded by the
optimization process. The all-paths graph represen-
tation was motivated by the observation that short-
est paths between candidate entities often exclude
relation-signaling words when detecting binary re-
lationships (Airola et al, 2008). Exploring broader
contexts ensures such words to be considered. In the
event extraction task, however, since triggers have
been annotated, they are naturally incorporated into
the shortest paths connecting trigger to each event ar-
gument. This in part explains why contexts beyond
shortest paths did not bring in an appreciable benefit.
All Tokens Recall Precision F-score
All-paths 48.77% 64.64% 55.59%
Shortest paths 47.45% 65.65% 55.09%
Table 5: Performance of using all-paths
5.1.3 GE Results on Testing Set
Since integrating the DSM and all-paths do not pro-
vide significant performance improvements to our
system, we decided to retain the original settings in
the ASM when extracting events from the testing
data. While most of the 2011 shared task datasets are
composed of PubMed abstracts compared to full-text
articles in the 2013 GE task, our system focuses on
extracting events expressed within the boundaries of
a single sentence. Therefore, in order to take advan-
tage of existing annotated resources, we incorporated
the annotated data of 2011 GE task and EPI (Epi-
genetics and Post-translational Modifications) task to
enrich the training instances of corresponding event
types of the 2013 GE task. Eventually, we obtained a
total of 14,448 rules of different event types from our
training data. In practice, it takes the ASM less than a
second to match the entire rule set with one document
and return results.
Our submitted system achieves a 48.93% F-score
on the 305 testing documents of the GE task, ranking
4th among 12 participating teams. Table 6 presents
the performance of the top eight systems.
System Recall Precision F-score
EVEX 45.44% 58.03% 50.97%
TEES 2.1 46.17% 56.32% 50.74%
BioSEM 42.47% 62.83% 50.68%
NCBI 40.53% 61.72% 48.93%
DlutNLP 40.81% 57.00% 47.56%
HDS4NLP 37.11% 51.19% 43.03%
NICTANLM 36.99% 50.68% 42.77%
USheff 31.69% 63.28% 42.23%
Table 6: Performance of top 8 systems in GE task
Our performance is within a reasonable mar-
gin from the best-performing system ?EVEX?, and
shows an overall superior precision over most partic-
ipating teams; only two of the top 5 systems obtained
82
a precision in the 60% range. Particularly for the
regulation-related complex events, we are the only
team that achieved a precision over 55% among all
12 participating systems. This indicates that event
rules automatically learned and optimized over train-
ing data generalize well to the unseen text, and have
the ability to identify precisely corresponding events.
We further evaluated the impact of the additonal
training instances from 2011 tasks and the ensemble
rule set derived from different parsers as presented
in Table 7. With the help from the 2011 data, our
F-score is increased by 3% and we became the only
team that detected ?Ubiquitination? events from test-
ing data. In addition, rules derived from the Stanford
parser do not provide additional benefits on the test-
ing data compared to using the Charniak parser alone.
System Attribute Recall Precision F-score
Ensemble 2013 + 2011 data 40.53% 61.72% 48.93%
Ensemble 2013 data 35.63% 63.91% 45.75%
Charniak 2013 data 35.29% 65.71% 45.92%
Table 7: Impact of 2011 data and ensemble rule set
5.2 CG task
5.2.1 Datasets
The CG task dataset is prepared based on a previ-
ously released corpus of angiogenesis domain ab-
stracts (Wang et al, 2011). It targets a challenging
set of 40 types of biological processes related to the
development and progression of cancer involving 18
entity types (Pyysalo et al, 2012). Table 8 presents
some statistics of the CG dataset.
Attributes Counted Training Development Testing
Abstracts 300 100 200
Entities 10,935 3,634 6,955
Annotated events 8,803 2,915 5,972
Table 8: Statistics of BioNLP-ST 2013 CG dataset
5.2.2 CG Results on Testing Set
We generalized our event extraction system to the CG
task and the corresponding annotated data of the 2011
tasks is also incorporated in the training phase to ob-
tain the optimized event rule set. Due to time con-
straints, the impact of integrating the DSM and all-
paths is not evaluated on the CG task. We achieved
a 46.38% F-score on the 200 testing documents of
the CG task, ranking 3rd among the 6 participating
teams. Table 9 gives the primary evaluation results of
the 6 participating teams; only ?TEES-2.1? and we
participated in both GE and CG tasks. The detailed
results of each of the targeted 40 event types is avail-
able from the official CG task website.
Team Recall Precision F-score
TEES-2.1 48.76% 64.17% 55.41%
NaCTeM 48.83% 55.82% 52.09%
NCBI 38.28% 58.84% 46.38%
RelAgent 41.73% 49.58% 45.32%
UET-NII 19.66% 62.73% 29.94%
ISI 16.44% 47.83% 24.47%
Table 9: Performance of all systems in 2013 CG task
Inconsistent with other biological entities, the en-
tity annotation for the optional ?Site? argument in-
volved in events such as ?Binding?, ?Mutation? and
?Phosphorylation? are not provided by the task orga-
nizers. We consider that detecting ?Site? entities is
related to entity detection and we would like to focus
our system on the event extraction itself. Thus, we
decided to ignore the ?Site? argument in our system.
However, a problem will arise that even though the
other arguments are correctly identified for an event,
it might still be evaluated as false positive if a ?Site?
argument is not detected. This results in both false
positive and false negative events. In addition, since
we did not perform the secondary task which requires
us to detect modifications of the predicted events, in-
cluding negation and speculation, about 7.5% anno-
tated instances in the testing data are thus missed,
causing damage to our recall in the overall evalua-
tion. The organizers have agreed to issue an additonal
evaluation that will focus on core event extraction tar-
gets excluding optional arguments such as ?Site? and
the secondary task. We will conduct more detailed
analysis on the results once they are made available.
6 Conclusion and Future Work
In the BioNLP-ST 2013, we generalized our ASM-
based system to address both GE and CG tasks.
We attempted to integrate a distributional similarity
model into our system to extend the graph match-
ing scheme. We also evaluated the impact of using
paths of all possible lengths among event participants
as key contextual dependencies to extract potential
events as compared to using only the shortest paths
within the framework of our system.
We achieved a 46.38% F-score in the CG task and
a 48.93% F-score in the GE task, ranking 3rd and
4th respectively. While the distributional similarity
model did not improve the overall performance of our
system in the tasks, we would like to further investi-
gate the antonym problem introduced by the model in
our future work.
83
Acknowledgments
This research was supported by the Intramural Re-
search Program of the NIH, NLM.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski1. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9 Suppl 11:s2.
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems
biology by text mining the literature. Trends in Biotech-
nology, 28(7):381?390.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012. Uni-
versity of turku in the BioNLP?11 shared task. BMC
Bioinformatics, 13 Suppl 11:S4.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Information
Processing Systems (NIPS). Vancouver, BC, December.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 19?27, Morristown, NJ,
USA. Association for Computational Linguistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,
Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.
2013. BioC: A minimalist approach to interoperability
for biomedical text processing. submitted.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. Bradford Books.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP Shared Task 2009 Workshop, pages
1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6. As-
sociation for Computational Linguistics, June.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 423?430. Association for Computa-
tional Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching ap-
proach for information extraction from biomedical text.
In Proceedings of BioNLP Shared Task 2011 Work-
shop, pages 164?172. Association for Computational
Linguistics, June.
Haibin Liu, Tom Christiansen, William A Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a lemmati-
zation tool for morphological processing of biomedical
text. Journal of Biomedical Semantics, 3:3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLOS ONE, 8:4 e60954.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2013b.
Exploring a subgraph matching approach for extracting
biological events from literature. Computational Intel-
ligence.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language processing.
MIT Press, Cambridge, MA, USA.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
Association for Computational Linguistics, pages 101?
104, Columbus, Ohio. The Association for Computer
Linguistics.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
New York, NY, USA. ACM.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol
Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012.
Event extraction across multiple levels of biological or-
ganization. Bioinformatics, 28:i575?i581.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon
Clematide, Thrse Vachon, and Martin Romacker. 2010.
Ontogene in BioCreative II.5. IEEE/ACM Trans. Com-
put. Biology Bioinform., 7(3):472?480.
84
Gerard Salton and Michael J. McGill. 1986. Introduction
to Modern Information Retrieval. McGraw-Hill, Inc.,
New York, NY, USA.
Isabel Segura-Bedmar, Paloma Martinez, and Daniel
Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011
Challenge Task: Extraction of Drug-Drug Interactions
from Biomedical Texts. In Proceedings of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction 2011,
pages 1?9.
Philippe Thomas, Mariana Neves, Illes Solt, Domonkos
Tikk, and Ulf Leser. 2011a. Relation extraction for
drug-drug interactions using ensemble learning. In Pro-
ceedings of DDIExtraction-2011 challenge task, pages
11?18.
Philippe Thomas, Stefan Pietschmann, Ille?s Solt,
Domonkos Tikk, and Ulf Leser. 2011b. Not all
links are equal: Exploiting dependency types for the
extraction of protein-protein interactions from text. In
Proceedings of BioNLP 2011 Workshop, pages 1?9.
Association for Computational Linguistics, June.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein?protein
interactions from literature. PLoS Computational Biol-
ogy, 6:e1000837, July.
Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,
Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.
2011. Automatic extraction of angiogenesis bioprocess
from text. Bioinformatics, 27(19):2730?2737.
Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, and
Yanpeng Li. 2012. A single kernel-based approach to
extract drug-drug interactions from biomedical litera-
ture. PLOS ONE, 7(11): e48901.
85
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
