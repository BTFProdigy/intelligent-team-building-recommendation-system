Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475?484,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Adapting a Lexicalized-Grammar Parser to Contrasting Domains
Laura Rimell and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
{laura.rimell,stephen.clark}@comlab.ox.ac.uk
Abstract
Most state-of-the-art wide-coverage parsers
are trained on newspaper text and suffer a
loss of accuracy in other domains, making
parser adaptation a pressing issue. In this
paper we demonstrate that a CCG parser can
be adapted to two new domains, biomedical
text and questions for a QA system, by us-
ing manually-annotated training data at the
POS and lexical category levels only. This ap-
proach achieves parser accuracy comparable
to that on newspaper data without the need
for annotated parse trees in the new domain.
We find that retraining at the lexical category
level yields a larger performance increase for
questions than for biomedical text and analyze
the two datasets to investigate why different
domains might behave differently for parser
adaptation.
1 Introduction
Most state-of-the-art wide-coverage parsers are
based on the Penn Treebank (Marcus et al, 1993),
making such parsers highly tuned to newspaper text.
A pressing question facing the parsing community
is how to adapt these parsers to other domains, such
as biomedical research papers and web pages. A re-
lated question is how to improve the performance
of these parsers on constructions that are rare in the
Penn Treebank, such as questions. Questions are
particularly important since a question parser is a
component in most Question Answering (QA) sys-
tems (Harabagiu et al, 2001).
In this paper we investigate parser adaptation in
the context of lexicalized grammars, by using a
parser based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). A key property of CCG is
that it is lexicalized, meaning that each word in a
sentence is associated with an elementary syntactic
structure. In the case of CCG this is a lexical cate-
gory expressing subcategorization information. We
exploit this property of CCG by performing manual
annotation in the new domain, but only up to this
level of representation, where the annotation can be
carried out relatively quickly. Since CCG lexical cat-
egories are so expressive, many of the syntactic char-
acteristics of a domain are captured at this level.
The two domains we consider are the biomedical
domain and questions for a QA system. We use the
term ?domain? somewhat loosely here, since ques-
tions are best described as a particular set of syn-
tactic constructions, rather than a set of documents
about a particular topic. However, we consider ques-
tion data to be interesting in the context of domain
adaptation for the following reasons: 1) there are
few examples in the Penn Treebank (PTB) and so
PTB parsers typically perform poorly on them; 2)
questions form a fairly homogeneous set with re-
spect to the syntactic constructions employed, and
it is an interesting question how easy it is to adapt a
parser to such data; and 3) QA is becoming an impor-
tant example of NLP technology, and question pars-
ing is an important task for QA systems.
The CCG parser we use (Clark and Curran, 2007b)
makes use of three levels of representation: one, a
POS tag level based on the fairly coarse-grained POS
tags in the Penn Treebank; two, a lexical category
level based on the more fine-grained CCG lexical cat-
egories, which are assigned to words by a CCG su-
475
pertagger; and three, a hierarchical level consisting
of CCG derivations. A key idea in this paper, follow-
ing a pilot study in Clark et al (2004), is to perform
manual annotation only at the first two levels. Since
the lexical category level consists of sequences of
tags, rather than hierarchical derivations, the anno-
tation can be performed relatively quickly.
For the biomedical and question domains we
manually annotated approximately 1,000 and 2,000
sentences, respectively, with CCG lexical categories.
We also created a gold standard set of grammati-
cal relations (GR) in the Stanford format (de Marn-
effe et al, 2006), using 500 of the questions. For
the biomedical domain we used the BioInfer corpus
(Pyysalo et al, 2007a), an existing gold-standard GR
resource also in the Stanford format. We evaluated
the parser on both lexical category assignment and
recovery of GRs.
The results show that the domain adaptation ap-
proach used here is successful in two very different
domains, achieving parsing accuracy comparable to
state-of-the-art accuracy for newspaper text. The re-
sults also show, however, that the two domains have
different profiles with regard to the levels of repre-
sentation used by the parser. We find that simply re-
training the POS tagger used by the parser leads to a
large improvement in performance for the biomed-
ical domain, and that retraining the CCG supertag-
ger on the annotated biomedical data improves the
performance further. For the question data, retrain-
ing just the POS tagger also improves parser perfor-
mance, but retraining the supertagger has a much
greater effect. We perform some analysis of the two
datasets in order to explain the different behaviours
with regard to porting the CCG parser.
2 The CCG Parser
The CCG parser is described in detail in Clark and
Curran (2007b) and so we provide only a brief de-
scription. The stages in the CCG parsing pipeline are
as follows. First, a maximum entropy POS tagger
assigns a single POS tag to each word in a sentence.
POS tags are fairly coarse-grained grammatical la-
bels indicating part-of-speech; the Penn Treebank
set, used here, contains approximately 50 labels.
Second, a maximum entropy supertagger assigns
CCG lexical categories to the words in the sentence.
Lexical categories can be thought of as fine-grained
POS tags expressing subcategorization information,
i.e. information about the argument frame of the
word. There are 425 categories in the set used by the
CCG parser. Supertagging was originally developed
for Lexicalized Tree Adjoining Grammar (Banga-
lore and Joshi, 1999), but has been particularly suc-
cessful for wide-coverage CCG parsing (Clark and
Curran, 2007b). Rather than assign a single category
to each word, the supertagger operates as a multi-
tagger, sometimes assigning more than one category
if the context is not sufficiently discriminating to
suggest a single tag (Curran et al, 2006). Since
the taggers have linear time complexity, the first two
stages can be performed extremely quickly.
Finally, the parsing stage combines the lexical cat-
egories, using a small set of combinatory rules that
are part of the grammar of CCG, and builds a packed
chart representation containing all the derivations
which can be built from the lexical categories. The
Viterbi algorithm efficiently finds the highest scor-
ing derivation from the packed chart, using a log-
linear model to score the derivations. The grammar
and training data for the newspaper version of the
CCG parser are obtained from CCGbank (Hocken-
maier and Steedman, 2007), a CCG version of the
Penn Treebank.
The aspect of the pipeline which is most relevant
to this paper is the supertagging phase. Figure 1
gives an example sentence from each target domain,
with the CCG lexical category assigned to each word
shown below the word, and the POS tag to the right.
Note that the categories contain a significant amount
of grammatical information, in particular subcatego-
rization information. The verb acts in the biomedi-
cal sentence, for example, looks for a prepositional
phrase (PP, as a linkage protein) to its right and a
noun phrase (NP, Talin) to its left, with the resulting
category a declarative sentence (S[dcl]).
Bangalore and Joshi (1999) refer to supertagging
as almost parsing, because once the correct lexical
categories have been assigned, the parser is left with
much less work to do. The CCG supertagger is not
able to assign a single category to each word with
extremely high accuracy ? hence the need for it to
operate as a multi-tagger ? but even in multi-tagger
mode it dramatically reduces the ambiguity passed
through to the parser (Clark and Curran, 2007b).
476
Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|.
NP (S\NP)/(S\NP) (S [dcl ]\NP)/PP PP/NP NP [nb]/N N /N N .
What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|.
(S [wq ]/(S [dcl ]\NP))/N N (S [dcl ]\NP)/NP NP [nb]/N N /N N .
Figure 1: Example sentences with lexical category assignment.
The parser has been evaluated on DepBank (King
et al, 2003), using the GR scheme of Briscoe et
al. (2006), and it scores 82.4% labelled precision
and 81.2% labelled recall overall (Clark and Curran,
2007a). Section 4.4 describes how the CCG depen-
dencies can be mapped into the Stanford GR scheme
(de Marneffe et al, 2006) and gives the results of
evaluating the parser on biomedical and question GR
resources.
The CCG parser is particularly well suited to the
biomedical and question domains. First, use of CCG
allows recovery of long-distance dependencies. In
the sentence What does target heart rate mean?, the
word What is an underlying object of the verb mean.
The parser recovers this information despite the dis-
tance between the two words. This capability is
crucial for question parsing, and also useful in the
biomedical field for extraction of relationships be-
tween biological entities. Additionally, the speed of
the parser (tens of sentences per second) is useful
for the large volumes of biomedical data that require
processing for biomedical text mining.
3 Approach
Our approach to domain adaptation is to target the
coarser-grained, less syntactically complex, levels of
representation used by the parser, and to train new
models with manually annotated data at these levels.
The motivation for this approach is twofold. First,
accuracy at each stage of the pipeline depends on ac-
curacy at the earlier stages. If the POS tagger assigns
incorrect tags, it is unlikely that the supertagger will
be able to recover and produce the correct lexical
categories, since it relies heavily on POS tags as fea-
tures. Without the correct categories, the parser in
turn will be unable to find a correct parse.
In the sentence What year did the Vietnam War
end?, the newspaper-trained POS tagger incorrectly
assigns the POS tag NN (common noun) to the verb
end, since verb-final sentences are atypical for the
PTB. As a result, the supertagger is virtually cer-
tain (greater than 99% probability) that the correct
CCG lexical category for end is N (noun). The parser
then assigns the Vietnam War end the structure of a
noun phrase, and chooses an unusual subcategoriza-
tion frame for did in which it takes three arguments:
What, year, and the Vietnam War end.
In the sentence How many siblings does
she have?, on the other hand, the supertag-
ger assigns an incorrect category to the word
How despite it having the correct POS tag
(WRB for wh-adverb). The correct category is
((S [wq ]/(S [q ]/NP))/N )/(NP/N ), which takes
many (category NP/N ) and siblings (category N )
as arguments. Instead it is tagged as S [wq ]/S [q ],
the category for a sentential adverb (i.e. the man-
ner reading of how), which prevents a correct parse.
Our intention was that creating new training data at
the lower levels of representation would improve the
accuracy of the POS tagger and supertagger in the
target domains, thereby improving the accuracy of
later stages in the pipeline as well.
The second motivation for our approach is to re-
duce annotation overhead. Full syntactic deriva-
tions are costly to produce by hand. POS tags, how-
ever, are relatively easy to annotate; even an out-
of-domain tagger will provide a good starting point,
and manual correction is quick, especially in a do-
main without much unfamiliar vocabulary. CCG lex-
ical categories require more expertise, but our ex-
perience shows that an out-of-domain supertagger
can again provide a starting point for correction, and
since the annotation is flat rather than hierarchical,
we hypothesize that it is not as difficult or time-
consuming as annotation of full derivations.
Our adaptation approach has been partially ex-
plored in previous work which targets one or another
of the different levels of representation.
477
Lease and Charniak (2005) obtained an improve-
ment in the accuracy of the Charniak (2000) parser,
as well as POS tagging accuracy, when applied to
the biomedical domain, by training a new POS tag-
ger model with a combination of newspaper and
biomedical data. The parser improvement was due
solely to the new POS tagger, without retraining the
parser model. Since the Charniak parser does not
use a lexicalized grammar with an intermediate level
of representation, any further improvements would
have to come from the parser model itself.
Clark et al (2004) obtained an improvement in
CCG supertagging accuracy for What-questions by
training a new supertagger model with a combina-
tion of newspaper and question data annotated with
CCG lexical categories. Because a question resource
annotated with GRs was not available, they did not
perform a parser evaluation, and the effects of the
POS tagging level were not compared to the lexi-
cal category level. In this paper, we extend the pi-
lot experiments performed by Clark et al (2004) in
four ways. First, we use a larger corpus of TREC
questions covering additional question types, thus
extending the experiments to the question domain
more broadly, as well as to the biomedical domain.
Second, we create a gold standard GR resource en-
abling a full parser evaluation on question data.
Third, we show that the POS level is important for
adaptation, reinforcing the work of Lease and Char-
niak (2005). A key finding of the present paper is
that the combination of retraining at the POS tag and
lexical category levels provides additional improve-
ments beyond those gained by retraining at a single
level. Finally, we provide analysis comparing the
adaptation methodology for question and biomedi-
cal data.
Hara et al (2007) followed a similar approach to
Clark et al (2004), using the parser of Ninomiya
et al (2006), a version of the Enju parser (Miyao
and Tsujii, 2005). Enju is based on HPSG, a lex-
icalized grammar formalism. They obtained an im-
provement in parsing accuracy in the biomedical do-
main by training a new probabilistic model of lexi-
cal entry assignments on a combination of newspa-
per and biomedical data without changing the orig-
inal newspaper-trained parsing model. Hara et al
(2007) did not consider the role of POS tagging. The
lexical category data in Hara et al (2007) was de-
rived from a gold standard treebank, while the an-
notation of lexical categories in this paper was per-
formed without reference to gold standard syntactic
derivations.
Judge et al (2006) produced a corpus of 4,000
questions annotated with syntactic trees, and ob-
tained an improvement in parsing accuracy for
Bikel?s reimplementation of the Collins parser
(Collins, 1997) by training a new parser model with
a combination of newspaper and question data. Our
approach differs in retraining only at the levels of
representation below parse trees.
4 Experiments and Results
4.1 Resources
We have used a combination of existing resources
and new, manually annotated data. The baseline POS
tagger, supertagger, and parser are trained on WSJ
Sections 02-21 of CCGbank. The baseline perfor-
mance at each level of representation is on WSJ Sec-
tion 00 of CCGbank, which contains 1913 sentences
and approximately 45,000 words.
For the biomedical domain, we trained the POS
tagger on gold-standard POS tags from GENIA (Kim
et al, 2003), a corpus of 2,000 MEDLINE abstracts
containing a total of approximately 18,500 sentences
and 440,000 words. We also annotated the first
1,000 sentences of GENIA with CCG lexical cate-
gories. This set of 1,000 sentences, containing ap-
proximately 27,000 words, was used for POS tagger
evaluation and for development and evaluation of a
new supertagger model. For parser evaluation, we
used BioInfer (Pyysalo et al, 2007a), a corpus of
MEDLINE abstracts (on a different topic from those
in GENIA) containing 1,100 sentences, and with syn-
tactic dependencies encoded as grammatical rela-
tions in the Stanford GR format. We used the same
evaluation set of 500 sentences as in Pyysalo et al
(2007b), and the remaining 600 for development of
the mapping to Stanford format. Two parsers have
already been evaluated on BioInfer, which makes it
a useful resource for comparative evaluation.
For the question domain, we extended the dataset
described in Clark et al (2004). That dataset con-
tained 1,171 questions beginning with the word
What, from the TREC 9-12 competitions (2000-
2003), manually POS tagged and annotated with
478
CCG lexical categories. We annotated all the addi-
tional TREC question types and improved the exist-
ing annotation, for a total of 1,828 sentences. We ad-
ditionally annotated a random subset of 500 of these
with GRs in the Stanford format. This subset served
as our evaluation set at all levels of representation. It
contains approximately 4,000 words, fewer than the
other domains because of the significantly shorter
sentence lengths of typical questions. The remain-
ing 1,328 sentences were used as training data. A
set of about a dozen sentences from the evaluation
and training sets were used to develop the mapping
to Stanford format for lexical categories not occur-
ring in the biomedical data.
4.2 POS tagger
We began by training new models at the POS tag
level of representation. All datasets use the PTB
tagset. As a baseline, we used the original WSJ 02-
21 model on the biomedical and question datasets.
For comparison we also evaluated on Section 00 us-
ing the WSJ-trained model.
For the question data, the new POS tagger was
trained on CCGbank Sections 02-21 plus ten copies
of the 1,328 training sentences. The WSJ data pro-
vides additional robustness and wide grammatical
coverage, and the weighting factor of ten was chosen
in preliminary experiments to prevent the newspaper
data from ?overwhelming? the question data. For
the biomedical data, the new POS tagger was trained
on the full GENIA corpus, minus the first 1,000 sen-
tences. GENIA is large enough that combination with
the newspaper data was not needed.
Table 1 gives the results. For both of the new do-
mains the performance of the WSJ model decreased
compared to Section 00, but the retrained model per-
formed at least as well as the WSJ model did on 00.1
Improving the POS tagger performance has a posi-
tive effect on the performance of the supertagger and
parser, which will be discussed in Sections 4.3-4.4.
1Since GENIA does not use the proper noun tag, NNP, for
names of genes and other biomedical entities, all figures in
this paper collapse the NNP-NN distinction where relevant for
biomedical data. The question data uses NNP and the distinc-
tion is not collapsed.
WSJ 02-21 Retrained
Sec. 00 96.7 ?
Qus 92.2 97.1
Bio 93.4 98.7
Table 1: POS tagger accuracy (%) for original and re-
trained models.
Orig
pipeline
Retrained
POS
Retrained
POS and
super
Sec. 00 91.5 ? ?
Qus 71.6 74.0 92.1
Bio 89.0 91.2 93.0
Table 2: Supertagging accuracy (%) and the effect of re-
training the POS model and the supertagger model.
4.3 Supertagger
We next trained new models at the CCG lexical cat-
egory level. The training data consisted of manu-
ally annotated biomedical and question sentences;
specifically, lexical categories were automatically
assigned by the original parsing pipeline and then
manually corrected. Whenever possible we used
categories from the parser?s original set of 425, al-
though occasionally it was necessary to use a new
category for a syntactic construction not occurring
in CCGbank Sections 02-21. (The parser can be con-
figured to recognize additional categories.) Question
data in particular requires the use of categories that
are rare or unseen in CCGbank.
For the questions, the new supertagger model,
like the POS tagger, was trained on WSJ 02-21 plus
ten copies of the 1,328 training sentences. For the
biomedical data, a ten-fold cross-validation was per-
formed, training each supertagger model on WSJ 02-
21 plus ten copies of 90% of the 1,000 annotated
sentences. Table 2 gives the supertagger accuracy
with and without the retrained POS and supertagger
models. The figure for the retrained biomedical su-
pertagger is the average of the ten-fold split.
The results show an improvement in accuracy of
lexical category assignment solely from retraining
the POS tagger, and an additional improvement from
retraining the supertagger. Supertagger accuracy for
the two domains with a retrained supertagger was
comparable, and in both cases was at least as high
479
What car company invented the Edsel?
(nsubj invented company)
(det Edsel the)
(dobj invented Edsel)
(det company What)
(nn company car)
Figure 2: Example of grammatical relations in the Stan-
ford grammatical relation format.
as for the original pipeline on Section 00. The ques-
tion data started from a much lower baseline figure,
however.
4.4 Parser
We evaluated the parser on the 500 questions anno-
tated with Stanford GRs and on the 500 evaluation
sentences from the BioInfer corpus. We used the
original newspaper pipeline, a pipeline with a re-
trained POS tagger, and a pipeline with both a re-
trained POS tagger and supertagger.
In order to perform these evaluations we devel-
ooped a mapping from the parser?s native CCG syn-
tactic dependencies to GRs in the Stanford format.
The mapping was based on the same principles as
the mapping that produces GR output in the style
of Briscoe et al (2006). These principles are dis-
cussed in detail in Clark and Curran (2007a); in
summary, the argument slots in the CCG dependen-
cies are mapped to argument slots in Stanford GRs,
a fairly complex, many-to-many mapping. An ad-
ditional post-processing script applies some manu-
ally developed rules to bring the output closer to the
Stanford format. Figure 2 gives an example of Stan-
ford GRs, where the label of the relation is followed
by two arguments, head and dependent.
Table 3 gives the results of the parser evaluation
on GRs. Since the parser model was not retrained,
the improvements in accuracy are due solely to the
new POS and supertaggers. The results are given as
an F-score over labelled GRs.2
The F-scores given in Table 3 are only for sen-
tences for which a parse was found. However, there
were also improvements in coverage with the re-
trained models. For the question data, parser cov-
2Only GRs at the lowest level of the Stanford hierarchy were
considered in the evaluation; more generic relations such as de-
pendent were not considered.
Orig POS
and super
New POS New POS
and super
Qus 64.4 69.4 86.6
BioInfer 76.0 80.4 81.5
Table 3: Parser F-score on grammatical relations and the
effect of retraining the POS and supertagger models.
erage was 94% for the original pipeline and the
pipeline with just the retrained POS tagger, and
99.6% with the retrained POS and supertaggers. For
the biomedical data, coverage was 97.2% for the
original pipeline, 99.0% for the pipeline with the re-
trained POS tagger, and 99.8% for the pipeline with
the retrained POS and supertaggers.
The final accuracy for both domains is in the same
range as that of the original parser on newspaper
data (81.8%) (Clark and Curran, 2007b), although
the results are not directly comparable, since the
newspaper resource uses a different GR scheme. For
the BioInfer corpus, the final accuracy is also in
line with results reported in the literature for other
parsers (Pyysalo et al, 2007b). (No comparable GR
results are available for questions.) A score in this
range is thought to be near the upper bound when
evaluating a CCG parser on GRs, since some loss is
inherent in the mapping to GRs (Clark and Curran,
2007a).
5 Analysis
Although domain adaptation was successful for both
of our target domains, the impact of the different
levels of representation on parsing accuracy was not
uniform. Table 3 shows that retraining the POS tag-
ger accounted for a greater proportion of the im-
provement on biomedical data, while retraining the
supertagger accounted for a much greater proportion
on questions. In this section we discuss some of the
differences between the domains which may have
contributed to their behaviour in this regard, with
the intention of highlighting attributes that may be
relevant for domain adaptation in general.
Informally, we believe that the main difference
between newspaper and biomedical text is vocabu-
lary, and that their syntactic structures are essentially
similar (with some isolated exceptions, such as more
frequent use of parentheses and comma-separated
480
Tag Errors Freq confused
Qus
WDT 129 WP
VB 46 NN, VBP
NNP 33 JJ, NN
NN 32 JJ, NNP
Bio
NN 801 JJ, CD
JJ 268 NN, VBN
VBN 113 JJ, VBD
FW 95 NN, IN
Table 4: Tags with the most frequent errors by the
newspaper-trained POS tagger and the tags they were
most frequently confused with.
lists in biomedical text). Once the POS tagger had
been retrained for biomedical text, accounting for
unfamiliar vocabulary, the original supertagger al-
ready performed well. The main difference between
newspaper and question data, on the other hand, is
syntactic. Retraining the POS tagger for questions
therefore had less effect; even with the correct POS
tags the supertagger was unable to assign the correct
lexical categories. Since lexical categories encode
syntactic information, the domain with the more di-
vergent syntax is likely to benefit most from new
training data at the lexical category level.
5.1 POS tagger
Table 1 showed that the accuracy of the newspaper-
trained POS tagger was in the same range for both
biomedical and question data. However, the distri-
bution of errors was different. Table 4 shows the tags
with the most frequent errors, accounting for about
75% of all POS tag errors in each domain, and the
tags that they were most frequently confused with.
For the question data, the most frequent error was
tagging a wh-determiner (WDT) as a wh-pronoun
(WP). A determiner combines with a noun to form
a noun phrase, as in the sentence What Liverpool
club spawned the Beatles?. A pronoun, on the other
hand, is a noun phrase in its own right, as in What
are the colors of the German flag?. This tagger er-
ror arises from the fact that the word What occurs
only once in WSJ 02-21 with a WDT tag. The sec-
ond most common error was on bare verbs (VB), be-
cause the newspaper model gives a low probability
of bare verbs occurring in sentence-final position, or
not directly following an auxiliary.
Unknown word
rate
Unknown
word-POS rate
Sec. 00 3.8 4.4
Qus 7.5 8.3
Bio 23.6 25.3
Table 5: Unknown word rate and word-POS tag pair rate
(%) compared to WSJ 02-21 (by token).
For the biomedical data, the most frequent errors
by far were confusions of noun (NN) and adjective
(JJ). This is most likely due to the prevalence of long
noun phrases in the biomedical data, such as major
histocompatibility complex class II molecules. Al-
though the words preceding the head noun are rec-
ognized as nominal modifiers, the classification into
noun and adjective is difficult, especially when the
word is previously unseen. There were also prob-
lems distinguishing verbal past participles (VBN)
from adjectives (JJ) and identifying foreign words
(FW), for example the phrase in vitro.
The fact that the newspaper-trained POS tagger
performed comparably in the two target domains
(Table 1) is surprising, since their lexical profiles
are quite different. Lease and Charniak (2005) dis-
cussed unknown word rate as a predictor of POS
tagger accuracy. However, the unknown word rate
compared with WSJ 02-21 is much higher for the
biomedical data than for the question data, as seen
in Table 5. (The unknown word rate for the question
data is still higher than that for WSJ 00, which may
be due to the high proportion of proper nouns in the
question data.)
Some POS tagging errors can be attributed, not
to an unknown word, but to the use of a known
word with an unfamiliar tag (as in the WDT exam-
ple above). However, it is not the case that the ques-
tion data contains many known words with unknown
tags, since the rate of unknown word-tag pairs is also
much higher for biomedical than for question data,
as seen in the rightmost column of Table 5.
We do know that the newspaper-trained POS tag-
ger performs better on unknown words for biomedi-
cal (84.7%) than for question data (80.4%). We hy-
pothesize that the syntactic context of the biomed-
ical data, being more similar to newspaper data,
provides more information for the POS tagger in
481
WSJ 02-21 New train-
ing sets
3-grams
Sec. 00 0.4 ?
Qus 3.6 0.7
Bio 0.7 0.5
5-grams
Sec. 00 12.1 ?
Qus 22.0 7.4
Bio 10.9 9.2
Table 6: Unknown POS n-gram rate (%) compared to WSJ
02-21, and when in-domain data is added (by token).
biomedical than in question data. Syntactic differ-
ences are discussed in the next section.
5.2 Supertagger
To quantify the syntactic distance between domains,
we propose using the unknown POS n-gram rate
compared to WSJ Sections 02-21. In the absence of
parse trees, POS n-grams can serve as a rough proxy
for the syntactic characteristics of a domain, reflect-
ing local word order configurations. POS n-grams
have been used in document modeling for text cate-
gorization (Baayen et al, 1996; Argamon-Engelson
et al, 1998), but we believe our proposed use of the
unknown POS n-gram rate is novel.
The leftmost column of Table 6 gives the un-
known POS trigram and 5-gram rates compared to
WSJ Sections 02-21. The rates for the biomedical
data are quite similar to those for Section 00. The
question data, however, shows higher rates of un-
known POS n-grams.
For both biomedical and question data, adding in-
domain data to the training set makes its syntactic
profile more like that of the evaluation set. The right-
most column of Table 6 shows the unknown POS n-
gram rates compared to the datasets used for training
the new supertagger models, consisting of WSJ 02-
21 plus annotated question or biomedical data. (For
the biomedical data, the figures are averages of the
same ten-fold split used for evaluation). It can be
seen that adding in-domain data reduces the rate of
unknown POS n-grams to about the same level ob-
served for newspaper text.
The unknown POS n-gram rate requires POS
tagged data for a new domain and thus cannot be
3-grams 5-grams
Sec. 00 18 19
Qus 8 5
Bio 16 13
Table 7: Number of the 20 most frequent POS n-grams
that are also in the 20 most frequent POS n-grams of WSJ
Sections 02-21.
WSJ 02-21 Bio Qus
. ? ? JJ NN NN ? ? WP
IN DT NN IN JJ NN ? WP VBZ
NN . ? NN IN JJ ? ? WDT
DT JJ NN NNS IN NN WP VBZ DT
Table 8: Four most frequent POS trigrams for WSJ 02-
21; four most frequent POS trigrams for biomedical and
question data that are not in the 20 most frequent for WSJ
02-21. The dash represents the sentence boundary.
used with unlabelled data. However, since POS tag-
ging is relatively inexpensive, it might be possible to
use this rate as one measure of syntactic distance be-
tween a training corpus and a target domain, prior to
undertaking parser domain adaptation. The measure
does not capture all aspects of syntactic distance,
however. As pointed out by an anonymous reviewer,
if the syntactic tree structures are similar across do-
mains but lexical distributions are different ? e.g. a
large number of words with unfamiliar categories in
the new domain ? this measure will not be sensitive
to the difference.
Another useful measure for comparing domain
adaptation in the biomedical and question domains
is frequent POS n-grams. Table 7 shows how many
of the 20 most frequent POS n-grams in each dataset
overlap with the 20 most frequent POS n-grams in
WSJ 02-21. It can be seen that the overlap is the
highest for Section 00, but much lower for the ques-
tion data than for the biomedical data, again demon-
strating that the question data makes frequent use of
syntactic constructions which are rare in the PTB.
Table 8 shows the four most frequent POS tri-
grams in WSJ Sections 02-21,3 and the four most
frequent POS trigrams in the biomedical and ques-
tion data that are not among the 20 most frequent
3Collapsing the NNP-NN distinction yields a slightly differ-
ent set.
482
for WSJ 02-21. The frequent question trigrams in-
clude two sentence-initial question words as well as
the pattern ? WP VBZ, occurring in sentences be-
ginning with e.g. What is or Who is. Though not
among the top four, the pattern VB . ?, represent-
ing a sentence-final bare verb, is also frequent. The
most frequent biomedical POS trigrams are not dra-
matically different from the newspaper trigrams, but
do appear to reflect the prevalence of NPs and PPs
in the data.
One final measure of syntactic distance is the
frequency with which CCG lexical categories that
are rare or unseen in CCGbank are used in a do-
main. It is typical to use a few such categories,
even for in-domain data, for unusual syntactic con-
structions, but each one is usually used only a hand-
ful of times. The question data is unique in the
frequency with which previously rare or unseen
categories are required. For example, the unseen
category (S [wq ]/S [q ])/N , representing the word
What in a question such as What day did Nintendo
64 come out? is used 11 times in the evaluation
set; the rare category (S [wq ]/(S [dcl ]\NP))/N ,
used in subject questions like Which river runs
through Dublin?, is used 61 times; and the rare cat-
egory (S [q ]/(S [pss]\NP))/NP , representing pas-
sive verbs in sentences like What is Jane Goodall
known for?, is used 59 times.
6 Conclusion
We have targeted lower levels of representation in
order to adapt a lexicalized-grammar parser to two
new domains, biomedical text and questions. Al-
though each of the lower levels has been targeted in-
dependently in previous work, this is the first study
that examines both levels together to determine how
they affect parsing accuracy. We achieved an accu-
racy on grammatical relations in the same range as
that of the original parser for newspaper text, with-
out requiring costly annotation of full parse trees.
Both biomedical and question data are domains in
which there is an immediate need for accurate pars-
ing. The question dataset is in some ways an ex-
treme example for domain adaptation, since the sen-
tences are syntactically uniform; on the other hand,
it is of interest as a set of constructions where the
parser initially performed poorly, and is a realistic
parsing challenge in the context of QA systems.
Interestingly, although an increase in accuracy at
each stage of the pipeline did yield an increase at
the following stage, these increases were not uni-
form across the two domains. The new POS tagger
model was responsible for most of the improvement
in parsing for the biomedical domain, while the new
supertagger model was necessary to see a large im-
provement in the question domain. We attribute this
to the fact that question syntax is significantly differ-
ent from newspaper syntax. We expect these consid-
erations to apply to any lexicalized-grammar parser.
Of course, it would be useful to have a way of
predicting which level of annotation would be most
effective for adapting to a new domain before the an-
notation begins. The utility of measures such as un-
known word rate (which can be performed with un-
labelled data) and unknown POS n-gram rate (which
can be performed with only POS tags) is not yet suffi-
ciently clear to rely on them as predictive measures,
but it seems a fruitful avenue for future work to in-
vestigate the importance of such measures for parser
domain adaptation.
Acknowledgments
We would like to thank Marie-Catherine de Marn-
effe for advice on the use of the Stanford GR for-
mat, Sampo Pyysalo for sharing information about
the BioInfer corpus, and Mark Steedman for advice
on encoding question data in CCG. We would also
like to thank three anonymous reviewers for their
suggestions. This work was supported by EPSRC
grant EP/E035698/1: Accurate and Efficient Parsing
of Biomedical Text.
References
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization: What
newspaper am I reading? In Proceedings of AAAI
Workshop on Learning for Text Categorization, pages
1?4.
Harald Baayen, Hans Van Halteren, and Fiona Tweedie.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121?131.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
483
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of the Joint Con-
ference of the International Committee on Computa-
tional Linguistics and the Association for Computa-
tional Linguistics (COLING/ACL-06), Sydney, Aus-
trailia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Meeting of the ACL,
pages 248?255, Prague, Czech Republic.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111?118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Meeting of the ACL, pages 16?23, Madrid, Spain.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697?704, Sydney, Aus-
trailia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th LREC Conference, pages 449?454,
Genoa, Italy.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of IWPT, pages 11?22, Prague,
Czech Republic.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana
Girju, Vasile Rus, and Paul Morarescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Meeting of the ACL, pages 274?281, Toulose, France.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 497?504, Sydney, Australia.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichi
Tsujii. 2003. GENIA corpus ? a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19:i180?i182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 Dependency Bank. In Proceedings of the 4th
International Workshop on Linguistically Interpreted
Corpora, Budapest, Hungary.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of the Second
International Joint Conference on Natural Language
Processing (IJCNLP-05), Jeju Island, Korea.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 83?90, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proceedings of the EMNLP Conference.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8:50.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In ACL?07 workshop on Biolog-
ical, translational, and clinical language processing,
pages 25?32, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
484
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813?821,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Unbounded Dependency Recovery for Parser Evaluation
Laura Rimell and Stephen Clark
University of Cambridge
Computer Laboratory
laura.rimell@cl.cam.ac.uk
stephen.clark@cl.cam.ac.uk
Mark Steedman
University of Edinburgh
School of Informatics
steedman@inf.ed.ac.uk
Abstract
This paper introduces a new parser eval-
uation corpus containing around 700 sen-
tences annotated with unbounded depen-
dencies, from seven different grammatical
constructions. We run a series of off-the-
shelf parsers on the corpus to evaluate how
well state-of-the-art parsing technology is
able to recover such dependencies. The
overall results range from 25% accuracy
to 59%. These low scores call into ques-
tion the validity of using Parseval scores
as a general measure of parsing capability.
We discuss the importance of parsers be-
ing able to recover unbounded dependen-
cies, given their relatively low frequency
in corpora. We also analyse the various er-
rors made on these constructions by one of
the more successful parsers.
1 Introduction
Statistical parsers are now obtaining Parseval
scores of over 90% on the WSJ section of the Penn
Treebank (Bod, 2003; Petrov and Klein, 2007;
Huang, 2008; Carreras et al, 2008). McClosky et
al. (2006) report an F-score of 92.1% using self-
training applied to the reranker of Charniak and
Johnson (2005). Such scores, in isolation, may
suggest that statistical parsing is close to becom-
ing a solved problem, and that further incremental
improvements will lead to parsers becoming as ac-
curate as POS taggers.
A single score in isolation can be misleading,
however, for a number of reasons. First, the single
score is an aggregate over a highly skewed distri-
bution of all constituent types; evaluations which
look at individual constituent or dependency types
show that the accuracies on some, semantically
important, constructions, such as coordination and
PP-attachment, are much lower (Collins, 1999).
Second, it is well known that the accuracy of
parsers trained on the Penn Treebank degrades
when they are applied to different genres and do-
mains (Gildea, 2001). Finally, some researchers
have argued that the Parseval metrics (Black et al,
1991) are too forgiving with respect to certain er-
rors and that an evaluation based on syntactic de-
pendencies, for which scores are typically lower,
is a better test of parser performance (Lin, 1995;
Carroll et al, 1998).
In this paper we focus on the first issue, that the
performance of parsers on some constructions is
much lower than the overall score. The construc-
tions that we focus on are various unbounded de-
pendency constructions. These are interesting for
parser evaluation for the following reasons: one,
they provide a strong test of the parser?s knowl-
edge of the grammar of the language, since many
instances of unbounded dependencies are diffi-
cult to recover using shallow techniques in which
the grammar is only superficially represented; and
two, recovering these dependencies is necessary
to completely represent the underlying predicate-
argument structure of a sentence, useful for appli-
cations such as Question Answering and Informa-
tion Extraction.
To give an example of the sorts of constructions
we are considering, and the (in)ability of parsers
to recover the corresponding unbounded depen-
dencies, none of the parsers that we have tested
were able to recover the dependencies shown in
bold from the following sentences:
We have also developed techniques for recognizing and
locating underground nuclear tests through the waves in the
ground which they generate.
By Monday , they hope to have a sheaf of documents both
sides can trust.
By means of charts showing wave-travel times and depths
in the ocean at various locations , it is possible to estimate
the rate of approach and probable time of arrival at Hawaii
of a tsunami getting under way at any spot in the Pacific .
813
The contributions of this paper are as follows.
First, we present the first set of results for the
recovery of a variety of unbounded dependen-
cies, for a range of existing parsers. Second, we
describe the creation of a publicly available un-
bounded dependency test suite, and give statistics
summarising properties of these dependencies in
naturally occurring text. Third, we demonstrate
that performing the evaluation is surprisingly dif-
ficult, because of different conventions across the
parsers as to how the underlying grammar is rep-
resented. Fourth, we show that current parsing
technology is very poor at representing some im-
portant elements of the argument structure of sen-
tences, and argue for a more focused construction-
based parser evaluation as a complement to exist-
ing grammatical relation-based evaluations. We
also perform an error-analysis for one of the more
successful parsers.
There has been some prior work on evaluating
parsers on long-range dependencies, but no work
we are aware of that has the scope and focus of
this paper. Clark et al (2004) evaluated a CCG
parser on a small corpus of object extraction cases.
Johnson (2002) began the body of work on insert-
ing traces into the output of Penn Treebank (PTB)
parsers, followed by Levy and Manning (2004),
among others. This PTB work focused heavily
on the representation in the Treebank, evaluat-
ing against patterns in the trace annotation. In
this paper we have tried to be more ?formalism-
independent? and construction focused.
2 Unbounded Dependency Corpus
2.1 The constructions
An unbounded dependency construction contains
a word or phrase which appears to have been
moved, while being interpreted in the position
of the resulting ?gap?. An unlimited number
of clause boundaries may intervene between the
moved element and the gap (hence ?unbounded?).
The seven constructions in our corpus were cho-
sen for being relatively frequent in text, compared
to other unbounded dependency types, and rela-
tively easy to identify. An example of each con-
struction, along with its associated dependencies,
is shown in Table 1. Here we give a brief descrip-
tion of each construction.
Object extraction from a relative clause is
characterised by a relative pronoun (a wh-word or
that) introducing a clause from which an argument
in object position has apparently been extracted:
the paper which I wrote. Our corpus includes
cases where the extracted word is (semantically)
the object of a preposition in the verb phrase: the
agency that I applied to.
Object extraction from a reduced relative
clause is essentially the same, except that there is
no overt relative pronoun: the paper I wrote; the
agency I applied to. We did not include participial
reduced relatives such as the paper written by the
professor.
Subject extraction from a relative clause is
characterised by the apparent extraction of an ar-
gument from subject position: the instrument that
measures depth. A relative pronoun is obligatory
in this construction. Our corpus includes passive
subjects: the instrument which was used by the
professor.
Free relatives contain relative pronouns with-
out antecedents: I heard what she said, where
what does not refer to any other noun in the sen-
tence. Free relatives can usually be paraphrased by
noun phrases such as the thing she said (a standard
diagnostic for distinguishing them from embedded
interrogatives like I wonder what she said). The
majority of sentences in our corpus are object free
relatives, but we also included some adverbial free
relatives: She told us how to do it.
Objectwh-questions are questions in which the
wh-word is the semantic object of the verb: What
did you eat?. Objects of prepositions are included:
What city does she live in?. Also included are a
few cases where the wh-word is arguably adver-
bial, but is selected for by the verb: Where is the
park located?.
Right node raising (RNR) is characterised by
coordinated phrases from which a shared element
apparently moves to the right: Mary saw and Su-
san bought the book. This construction is unique
within our corpus in that the ?raised? element can
have a wide variety of grammatical functions. Ex-
amples include: noun phrase object of verb, noun
phrase object of preposition (material about or
messages from the communicator), a combination
of the two (applied for and won approval), prepo-
sitional phrase modifier (president and chief exec-
utive of the company), infinitival modifier (the will
and the capacity to prevent the event), and modi-
fied noun (a good or a bad decision).
Subject extraction from an embedded clause
is characterised by a semantic subject which is ap-
814
Object extraction from a relative clause
Each must match Wisman?s ?pie? with the fragment that he carries with him.
dobj(carries, fragment)
Object extraction from a reduced relative clause
Put another way, the decline in the yield suggests stocks have gotten pretty rich in price relative to the
dividends they pay, some market analysts say.
dobj(pay, dividends)
Subject extraction from a relative clause
It consists of a series of pipes and a pressure-measuring chamber which record the rise and fall of the
water surface.
nsubj(record, series)
nsubj(record, chamber)
Free relative
He tried to ignore what his own common sense told him, but it wasn?t possible; her motives were too
blatant.
dobj(told, what)
Object wh-question
What city does the Tour de France end in?
pobj(in, city)
Right node raising
For the third year in a row, consumers voted Bill Cosby first and James Garner second in persuasiveness
as spokesmen in TV commercials, according to Video Storyboard Tests, New York.
prep(first, in)
prep(second, in)
Subject extraction from an embedded clause
In assigning to God the responsibility which he learned could not rest with his doctors, Eisenhower
gave evidence of that weakening of the moral intuition which was to characterize his administration
in the years to follow.
nsubj(rest, responsibility)
Table 1: Examples of the seven constructions in the unbounded dependency corpus.
parently extracted across two clause boundaries,
as shown in the following bracketing (where ?
marks the origin of the extracted element): the
responsibility which [the government said [? lay
with the voters]]. Our corpus includes sentences
where the embedded clause is a so-called small
clause, i.e. one with a null copula verb: the plan
that she considered foolish, where plan is the se-
mantic subject of foolish.
2.2 The data
The corpus consists of approximately 100 sen-
tences for each of the seven constructions; 80 of
these were reserved for each construction for test-
ing, giving a test set of 560 sentences in total, and
the remainder were used for initial experimenta-
tion (for example to ensure that default settings for
the various parsers were appropriate for this data).
We did not annotate the full sentences, since we
are only interested in the unbounded dependencies
and full annotation of such a corpus would be ex-
tremely time-consuming.
With the exception of the question construc-
tion, all sentences were taken from the PTB, with
roughly half from the WSJ sections (excluding
2-21 which provided the training data for many
815
of the parsers in our set) and half from Brown
(roughly balanced across the different sections).
The questions were taken from the question data
in Rimell and Clark (2008), which was obtained
from various years of the TREC QA track. We
chose to use the PTB as the main source because
the use of traces in the PTB annotation provides a
starting point for the identification of unbounded
dependencies.
Sentences were selected for the corpus by a
combination of automatic and manual processes.
A regular expression applied to PTB trees, search-
ing for appropriate traces for a particular con-
struction, was first used to extract a set of can-
didate sentences. All candidates were manually
reviewed and, if selected, annotated with one or
more grammatical relations representing the rel-
evant unbounded dependencies in the sentence.
Some of the annotation in the treebank makes
identification of some constructions straightfor-
ward; for example right node raising is explicitly
represented as RNR. Indeed it may have been pos-
sible to fully automate this process with use of
the tgrep search tool. However, in order to ob-
tain reliable statistics regarding frequency of oc-
currence, and to ensure a high-quality resource,
we used fairly broad regular expressions to iden-
tify the original set followed by manual review.
We chose to represent the dependencies as
grammatical relations (GRs) since this format
seemed best suited to represent the kind of seman-
tic relationship we are interested in. GRs are head-
based dependencies that have been suggested as a
more appropriate representation for general parser
evaluation than phrase-structure trees (Carroll et
al., 1998). Table 1 gives examples of how GRs
are used to represent the relevant dependencies.
The particular GR scheme we used was based on
the Stanford scheme (de Marneffe et al, 2006);
however, the specific GR scheme is not too crucial
since the whole sentence is not being represented
in the corpus, only the unbounded dependencies.
3 Experiments
The five parsers described in Section 3.2 were used
to parse the test sentences in the corpus, and the
percentage of dependencies in the test set recov-
ered by each parser for each construction was cal-
culated. The details of how the parsers were run
and how the parser output was matched against
the gold standard are given in Section 3.3. This
Construction WSJ Brown Overall
Obj rel clause 2.3 1.1 1.4
Obj reduced rel 2.7 2.8 2.8
Sbj rel clause 10.1 5.7 7.4
Free rel 2.6 0.9 1.3
RNR 2.2 0.9 1.2
Sbj embedded 2.0 0.3 0.4
Table 2: Frequency of constructions in the PTB
(percentage of sentences).
is essentially a recall evaluation, and so is open
to abuse; for example, a program which returns all
the possible word pairs in a sentence, together with
all possible labels, would score 100%. However,
this is easily guarded against: we simply assume
that each parser is being run in a ?standard? mode,
and that each parser has already been evaluated on
a full corpus of GRs in order to measure precision
and recall across all dependency types. (Calculat-
ing precision for the unbounded dependency eval-
uation would be difficult since that would require
us to know howmany incorrect unbounded depen-
dencies were returned by each parser.)
3.1 Statistics relating to the constructions
Table 2 shows the percentage of sentences in the
PTB, from those sections that were examined,
which contain an example of each type of un-
bounded dependency. Perhaps not surprisingly,
root subject extractions from relative clauses are
by far the most common, with the remaining con-
structions occurring in roughly between 1 and 2%
of sentences. Note that, although examples of
each individual construction are relatively rare, the
combined total is over 10% (assuming that each
construction occurs independently). Section 6
contains a discussion regarding the frequency of
occurrence of these events and the consequences
of this for parser performance.
Table 3 shows the average and maximum dis-
tance between head and dependent for each con-
struction, as measured by the difference between
word indices. This is a fairly crude measure of
distance but gives some indication of how ?long-
range? the dependencies are for each construc-
tion. The cases of object extraction from a relative
clause and subject extraction from an embedded
clause provide the longest dependencies, on aver-
age. The following sentence gives an example of
how far apart the head and dependent can be in a
816
Construction Avg Dist Max Dist
Obj rel clause 6.8 21
Obj reduced rel 3.4 8
Sbj rel clause 4.4 18
Free rel 3.4 16
Obj wh-question 4.8 9
RNR 4.8 23
Sbj embedded 7.0 21
Table 3: Distance between head and dependent.
subject embedded construction:
the same stump which had impaled the car of
many a guest in the past thirty years and which he
refused to have removed.
3.2 The parsers
The parsers that we chose to evaluate are the C&C
CCG parser (Clark and Curran, 2007), the Enju
HPSG parser (Miyao and Tsujii, 2005), the RASP
parser (Briscoe et al, 2006), the Stanford parser
(Klein and Manning, 2003), and the DCU post-
processor of PTB parsers (Cahill et al, 2004),
based on LFG and applied to the output of the
Charniak and Johnson reranking parser. Of course
we were unable to evaluate every publicly avail-
able parser, but we believe these are representative
of current wide-coverage robust parsing technol-
ogy.
1
The C&C parser is based on CCGbank (Hock-
enmaier and Steedman, 2007), a CCG version of
the Penn Treebank. It is ideally suited for this eval-
uation because CCG was designed to capture the
unbounded dependencies being considered. The
Enju parser was designed with a similar motiva-
tion to C&C, and is also based on an automat-
ically extracted grammar derived from the PTB,
but the grammar formalism is HPSG rather than
CCG. Both parsers produce head-word dependen-
cies reflecting the underlying predicate-argument
structure of a sentence, and so in theory should be
straightforward to evaluate.
The RASP parser is based on a manually con-
structed POS tag-sequence grammar, with a sta-
tistical parse selection component and a robust
1
One obvious omission is any form of dependency parser
(McDonald et al, 2005; Nivre and Scholz, 2004). However,
the dependencies returned by these parsers are local, and it
would be non-trivial to infer from a series of links whether a
long-range dependency had been correctly represented. Also,
dependency parsers are not significantly better at recovering
head-based dependencies than constituent parsers based on
the PTB (McDonald et al, 2005).
partial-parsing technique which allows it to re-
turn output for sentences which do not obtain a
full spanning analysis according to the grammar.
RASP has not been designed to capture many of the
dependencies in our corpus; for example, the tag-
sequence grammar has no explicit representation
of verb subcategorisation, and so may not know
that there is a missing object in the case of extrac-
tion from a relative clause (though it does recover
some of these dependencies). However, RASP is
a popular parser used in a number of applications,
and it returns dependencies in a suitable format for
evaluation, and so we considered it to be an appro-
priate and useful member of our parser set.
The Stanford parser is representative of a large
number of PTB parsers, exemplified by Collins
(1997) and Charniak (2000). The Parseval scores
reported for the Stanford parser are not the highest
in the literature, but are competitive enough for our
purposes. The advantage of the Stanford parser is
that it returns dependencies in a suitable format for
our evaluation. The dependencies are obtained by
a set of manually defined rules operating over the
phrase-structure trees returned by the parser (de
Marneffe et al, 2006). Like RASP, the Stanford
parser has not been designed to capture unbounded
dependencies; in particular it does not make use of
any of the trace information in the PTB. However,
we wanted to include a ?standard? PTB parser in
our set to see which of the unbounded dependency
constructions it is able to deal with.
Finally, there is a body of work on inserting
trace information into the output of PTB parsers
(Johnson, 2002; Levy and Manning, 2004), which
is the annotation used in the PTB for representing
unbounded dependencies. The work which deals
with the PTB representation directly, such as John-
son (2002), is difficult for us to evaluate because it
does not produce explicit dependencies. However,
the DCU post-processor is ideal because it does
produce dependencies in a GR format. It has also
obtained competitive scores on general GR evalu-
ation corpora (Cahill et al, 2004).
3.3 Parser evaluation
The parsers were run essentially out-of-the-box
when parsing the test sentences. The one excep-
tion was C&C, which required some minor adjust-
ing of parameters, as described in the parser doc-
umentation, to obtain close to full coverage on the
data. In addition, the C&C parser comes with a
817
Obj RC Obj Red Sbj RC Free Obj Q RNR Sbj Embed Total
C&C 59.3 62.6 80.0 72.6 (81.2) 27.5 49.4 22.4 (59.7) 53.6
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.4
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 35.7
Rasp 16.5 1.1 53.7 17.9 27.5 34.5 15.3 25.3
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 38.1
Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each construction
is in bold; the figures in brackets for C&C derive from the use of a separate question model.
specially designed question model, and so we ap-
plied both this and the standard model to the object
wh-question cases.
The parser output was evaluated against each
dependency in the corpus. Due to the various GR
schemes used by the parsers, an exact match on the
dependency label could not always be expected.
We considered a correctly recovered dependency
to be one where the gold-standard head and depen-
dent were correctly identified, and the label was
an ?acceptable match? to the gold-standard label.
To be an acceptable match, the label had to indi-
cate the grammatical function of the extracted el-
ement at least to the level of distinguishing active
subjects, passive subjects, objects, and adjuncts.
For example, we allowed an obj (object) relation
as a close enough match for dobj (direct object)
in the corpus, even though obj does not distin-
guish different kinds of objects, but we did not al-
low generic ?relative pronoun? relations that are
underspecified for the grammatical role of the ex-
tracted element.
The differences in GR schemes were such that
we ended up performing a time-consuming largely
manual evaluation. We list here some of the key
differences that made the evaluation difficult.
In some cases, the parser?s set of labels was less
fine-grained than the gold standard. For example,
RASP represents the direct objects of both verbs
and prepositions as dobj (direct object), whereas
the gold-standard uses pobj for the preposition
case. We counted the RASP output as correctly
matching the gold standard.
In other cases, the label on the dependency
containing the gold-standard head and depen-
dent was too underspecified to be acceptable by
itself. For example, where the gold-standard
relation was dobj(placed,buckets), DCU
produced relmod(buckets,placed) with
a generic ?relative modifier? label. However,
the correct label could be recovered from else-
where in the parser output, specifically a com-
bination of relpro(buckets,which) and
obj(placed,which). In this case we counted
the DCU output as correctly matching the gold
standard.
In some constructions the Stanford scheme,
upon which the gold-standard was based, makes
different choices about heads than other schemes.
For example, in the the phrase Honolulu, which is
the center of the warning system, the corpus con-
tains a subject dependency with center as the head:
nsubj(center,Honolulu). Other schemes,
however, treat the auxiliary verb is as the head of
the dependency, rather than the predicate nominal
center. As long as the difference in head selec-
tion was due solely to the idiosyncracies of the GR
schemes involved, we counted the relation as cor-
rect.
Finally, the different GR schemes treat coordi-
nation differently. In the corpus, coordinated ele-
ments are always represented with two dependen-
cies. Thus the phrase they may half see and half
imagine the old splendor has two gold-standard
dependencies: dobj(see,splendor) and
dobj(imagine,splendor). If a parser pro-
duced only the former dependency, but appeared
to have the coordination correct, then we awarded
two marks, even though the second dependency
was not explicitly represented.
4 Results
Accuracies for the various parsers are shown in Ta-
ble 4, with the highest score for each construction
in bold. Enju and C&C are the top performers,
operating at roughly the same level of accuracy
across most of the constructions. Use of the C&C
question model made a huge difference for the wh-
object construction (81.2% vs. 27.5%), showing
that adaptation techniques specific to a particular
818
construction can be successful (Rimell and Clark,
2008).
In order to learn more from these results, in Sec-
tion 5 we analyse the various errors made by the
C&C parser on each construction. The conclusions
that we arrive at for the C&C parser we would also
expect to apply to Enju, on the whole, since the de-
sign of the two parsers is so similar. In fact, some
of the recommendations for improvement on this
corpus, such as the need for a better parsing model
to make better attachment decisions, are parser in-
dependent.
The poor performance of RASP on this corpus
is clearly related to a lack of subcategorisation in-
formation, since this is crucial for recovering ex-
tracted arguments. For Stanford, incorporating the
trace information from the PTB into the statistical
model in some way is likely to help. The C&C and
Enju parsers do this through their respective gram-
mar formalisms. Our informal impression of the
DCU post-processor is that it has much of the ma-
chinery available to recover the dependencies that
the Enju and C&C parsers do, but for some reason
which is unclear to us it performs much worse.
5 Analysis of the C&C Parser
We categorised the errors made by the C&C parser
on the development data for each construction. We
chose the C&C parser for the analysis because it
was one of the top performers and we have more
knowledge of its workings than those of Enju.
The C&C parser first uses a supertagger to as-
sign a small number of CCG lexical categories (es-
sentially subcategorisation frames) to each word in
the sentence. These categories are then combined
using a set of combinatory rules to build a CCG
derivation. The parser uses a log-linear probabil-
ity model to select the highest-scoring derivation
(Clark and Curran, 2007). In general, errors in de-
pendency recovery may occur if the correct lexical
category is not assigned by the supertagger for one
or more of the words in a sentence, or if an incor-
rect derivation is chosen by the parsing model.
For unbounded dependency recovery, one
source of errors (labeled type 1 in Table 5) is the
wrong lexical category being assigned to the word
(normally a verb or preposition) governing the ex-
traction site. In these testaments that I would sub-
mit here, if submit is assigned a category for an
intransitive rather than transitive verb, the verb-
object relation will not be recovered.
1a 1b 1c 1d 2 3 Errs Tot
ObjRC 6 5 2 13 20
ObjRed 2 1 1 1 3 8 23
SbjRC 8 1 9 43
Free 1 1 2 22
ObjQ 2 2 4 25
RNR 2 1 7 3 13 28
SbjEmb 3 2 1 4 10 13
Subtotal 6 2 12 4
Total 24 21 14 59 174
Table 5: Error analysis for C&C. Errs is the to-
tal number of errors for a construction, Tot is the
number of dependencies of that type in the devel-
opment data.
There are a number of reasons why the wrong
category may be assigned. First, the lexicon may
not contain enough information about possible
categories for the word (1a), or the necessary cat-
egory may not exist in the parser?s grammar at all
(1b). Even if the grammar contains the correct cat-
egory and the lexicon makes it available, the pars-
ing model may not choose it (1c). Finally, a POS-
tagging error on the word may mislead the parser
into assigning the wrong category (1d).
2
A second source of errors (type 2) is attach-
ment decisions that the parser makes indepen-
dently of the unbounded dependency. In Morgan
. . . carried in several buckets of water from the
spring which he poured into the copper boiler, the
parser assigns the correct categories for the rela-
tive pronoun and verb, but chooses spring rather
than buckets as the head of the relativized NP (i.e.
the object of pour). Most attachment errors in-
volve prepositional phrases (PPs) and coordina-
tion, which have long been known to be areas
where parsers need improvement.
Finally, errors in unbounded dependency recov-
ery may be due to complex errors in the surround-
ing parse context (type 3). We will not comment
more on these cases since they do not tell us much
about unbounded dependencies in particular.
Table 5 shows the distribution of error types
across constructions for the C&C parser. Subject
relative clauses, for example, did not have any er-
rors of type 1, because a verb with an extracted
2
We considered an error to be type 1 only when the cate-
gory error occurred on the word governing the extraction site,
except in the subject embedded sentences, where we also in-
cluded the embedding verb, since the category of this verb is
key to dependency recovery.
819
subject does not require a special lexical category.
Most of the errors here are of type 2. For exam-
ple, in a series of pipes and a pressure-measuring
chamber which record the rise and fall of the wa-
ter surface, the parser attaches the relative clause
to chamber but not to series.
Subject embedded sentences show a different
pattern. Many of the errors can be attributed to
problems with the lexicon and grammar (1a and
1b). For example, in shadows that they imagined
were Apaches, the word imagined never appears in
the training data with the correct category, and so
the required entry is missing from the lexicon.
Object extraction from a relative clause had
a higher number of errors involving the parsing
model (1c). In the first carefree, dreamless sleep
that she had known, the transitive category is
available for known, but not selected by the model.
The majority of the errors made by the parser
are due to insufficient grammar coverage or weak-
ness in the parsing model due to sparsity of head
dependency data, the same fundamental problems
that have dogged automatic parsing since its in-
ception. Hence one view of statistical parsing is
that it has allowed us to solve the easy problems,
but we are still no closer to a general solution for
the recovery of the ?difficult? dependencies. One
possibility is to create more training data target-
ing these constructions ? effectively ?active learn-
ing by construction? ? in the way that Rimell and
Clark (2008) were able to build a question parser.
We leave this idea for future work.
6 Discussion
Unbounded dependencies are rare events, out in
the Zipfian ?long tail?. They will always consti-
tute a fraction of a percent of the overall total of
head-dependencies in any corpus, a proportion too
small to make a significant impact on global mea-
sures of parser accuracy, when expressive parsers
are compared to those that merely approximate
human grammar using finite-state or context-free
covers. This will remain the case even when such
measures are based on dependencies, rather than
on parse trees.
Nevertheless, unbounded dependencies remain
highly significant in a much more important sense.
They support the constructions that are central to
those applications of parsing technology for which
precision is as important as recall, such as open-
domain question-answering. As low-power ap-
proximate parsing methods improve (as they must
if they are ever to be usable at all for such tasks),
we predict that the impact of the constructions we
examine here will become evident. No matter how
infrequent object questions like ?What do frogs
eat?? are, if they are answered as if they were sub-
ject questions (?Herons?), users will rightly reject
any excuse in terms of the overall statistical distri-
bution of related bags of words.
Whether such improvements in parsers come
from the availability of more human-labeled data,
or from a breakthrough in unsupervised machine
learning, we predict an imminent ?Uncanny Val-
ley? in parsing applications, due to the inability of
parsers to recover certain semantically important
dependencies, of the kind familiar from humanoid
robotics and photorealistic animation. In such ap-
plications, the closer the superficial resemblance
to human behavior gets, the more disturbing sub-
tle departures become, and the more they induce
mistrust and revulsion in the user.
7 Conclusion
In this paper we have demonstrated that current
parsing technology is poor at recovering some
of the unbounded dependencies which are crucial
for fully representing the underlying predicate-
argument structure of a sentence. We have also
argued that correct recovery of such dependen-
cies will become more important as parsing tech-
nology improves, despite the relatively low fre-
quency of occurrence of the corresponding gram-
matical constructions. We also see this more fo-
cused parser evaluation methodology ? in this
case construction-focused ? as a way of improv-
ing parsing technology, as an alternative to the
exclusive focus on incremental improvements in
overall accuracy measures such as Parseval.
Acknowledgments
Laura Rimell and Stephen Clark were supported
by EPSRC grant EP/E035698/1. Mark Steed-
man was supported by EU IST Cognitive Systems
grant IP FP6-2004-IST-4-27657 (PACO-PLUS).
We would like to thank Aoife Cahill for produc-
ing the DCU data.
820
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of English grammars. In HLT ?91: Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, pages 306?311.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Meeting of
the EACL, pages 19?26, Budapest, Hungary.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In
Proceedings of the Interactive Demo Session of
COLING/ACL-06, Sydney, Australia.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of the 42nd Meeting of the ACL, pages 320?327,
Barcelona, Spain.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Dynamic programming and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Natural Language Learning
(CoNLL-08), pages 9?16, Manchester, UK.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173?180, Michigan, Ann Arbor.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111?118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th LREC Conference, Genoa,
Italy.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 EMNLP Con-
ference, Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceed-
ings of the 46th Meeting of the ACL, pages 586?594,
Columbus, Ohio.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, pages 136?143, Philadelphia, PA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of Association for Computa-
tional Linguistics, pages 423?430, Sapporo, Japan.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: cor-
recting the surface dependency approximation. In
Proceedings of the 42nd Meeting of the ACL, pages
328?335, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425, Montreal, Canada.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics Confer-
ence, pages 152?159, Brooklyn, NY.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd Meet-
ing of the ACL, pages 91?98, Michigan, Ann Arbor.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Meeting of the
ACL, pages 83?90, Michigan, Ann Arbor.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64?70, Geneva, Switzerland.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL Conference, Rochester, NY.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 EMNLP Conference,
pages 475?484, Honolulu, Hawai?i.
821
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833?841,
Beijing, August 2010
Evaluation of Dependency Parsers on Unbounded Dependencies
Joakim Nivre Laura Rimell Ryan McDonald Carlos Go?mez-Rodr??guez
Uppsala University Univ. of Cambridge Google Inc. Universidade da Corun?a
joakim.nivre@lingfil.uu.se laura.rimell@cl.cam.ac.uk ryanmcd@google.com cgomezr@udc.es
Abstract
We evaluate two dependency parsers,
MSTParser and MaltParser, with respect
to their capacity to recover unbounded de-
pendencies in English, a type of evalu-
ation that has been applied to grammar-
based parsers and statistical phrase struc-
ture parsers but not to dependency parsers.
The evaluation shows that when combined
with simple post-processing heuristics,
the parsers correctly recall unbounded
dependencies roughly 50% of the time,
which is only slightly worse than two
grammar-based parsers specifically de-
signed to cope with such dependencies.
1 Introduction
Though syntactic parsers for English are re-
ported to have accuracies over 90% on the Wall
Street Journal (WSJ) section of the Penn Tree-
bank (PTB) (McDonald et al, 2005; Sagae and
Lavie, 2006; Huang, 2008; Carreras et al, 2008),
broad-coverage parsing is still far from being a
solved problem. In particular, metrics like attach-
ment score for dependency parsers (Buchholz and
Marsi, 2006) and Parseval for constituency parsers
(Black et al, 1991) suffer from being an aver-
age over a highly skewed distribution of differ-
ent grammatical constructions. As a result, in-
frequent yet semantically important construction
types could be parsed with accuracies far below
what one might expect.
This shortcoming of aggregate parsing met-
rics was highlighted in a recent study by Rimell
et al (2009), introducing a new parser evalua-
tion corpus containing around 700 sentences an-
notated with unbounded dependencies in seven
different grammatical constructions. This corpus
was used to evaluate five state-of-the-art parsers
for English, focusing on grammar-based and sta-
tistical phrase structure parsers. For example, in
the sentence By Monday, they hope to have a
sheaf of documents both sides can trust., parsers
should recognize that there is a dependency be-
tween trust and documents, an instance of object
extraction out of a (reduced) relative clause. In the
evaluation, the recall of state-of-the-art parsers on
this kind of dependency varies from a high of 65%
to a low of 1%. When averaging over the seven
constructions in the corpus, none of the parsers
had an accuracy higher than 61%.
In this paper, we extend the evaluation of
Rimell et al (2009) to two dependency parsers,
MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006a), trained on data from the
PTB, converted to Stanford typed dependencies
(de Marneffe et al, 2006), and combined with a
simple post-processor to extract unbounded de-
pendencies from the basic dependency tree. Ex-
tending the evaluation to dependency parsers is of
interest because it sheds light on whether highly
tuned grammars or computationally expensive
parsing formalisms are necessary for extracting
complex linguistic phenomena in practice. Unlike
the best performing grammar-based parsers stud-
ied in Rimell et al (2009), neither MSTParser nor
MaltParser was developed specifically as a parser
for English, and neither has any special mecha-
nism for dealing with unbounded dependencies.
Dependency parsers are also often asymptotically
faster than grammar-based or constituent parsers,
e.g., MaltParser parses sentences in linear time.
Our evaluation ultimately shows that the re-
call of MSTParser and MaltParser on unbounded
dependencies is much lower than the average
(un)labeled attachment score for each system.
Nevertheless, the two dependency parsers are
found to perform only slightly worse than the best
grammar-based parsers evaluated in Rimell et al
833
Each must match Wisman 's "pie" with the fragment that he carries with him
nsubj dobj
prepaux pobjposs've
prep rcmoddobj
nsubjdet
pobj
poss
dobj
a: ObRC
Five things you can do for $ 15,000  or less
pobjnsubjaux
rcmod
num num
prep cc conj
dobj
b: ObRed
They will remain on a lower-priority list that includes 17 other countries
pobjnsubj
aux
rcmod
nsubj
num
prep amod
det
nsubj
c: SbRC
amod
dobj
What you see are self-help projects
nsubj
csubj cop
amod
dobj
dobj d: Free
What effect does a prism have on light
pobjnsubj
aux
det
det prep
dobj
dobj
e: ObQ
Now he felt ready for the many actions he saw spreading out before him
pobj rcmod
prtmod
detprepacompnsubj
nsubj
amod xcompnsubj
prep
pobj
g: SbEmadv
The men were at first puzz led, then angered by the aimless tacking
pobj
cop conj
advmod
detadvmod
prep
det
prep
amod
pobjnsubjpass
f : RNR
Figure 1: Examples of seven unbounded dependency constructions (a?g). Arcs drawn below each sentence represent the
dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,
with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets.
(2009) and considerably better than the other sta-
tistical parsers in that evaluation. Interestingly,
though the two systems have similar accuracies
overall, there is a clear distinction between the
kinds of errors each system makes, which we ar-
gue is consistent with observations by McDonald
and Nivre (2007).
2 Unbounded Dependency Evaluation
An unbounded dependency involves a word or
phrase interpreted at a distance from its surface
position, where an unlimited number of clause
boundaries may in principle intervene. The
unbounded dependency corpus of Rimell et al
(2009) includes seven grammatical constructions:
object extraction from a relative clause (ObRC),
object extraction from a reduced relative clause
(ObRed), subject extraction from a relative clause
(SbRC), free relatives (Free), object questions
(ObQ), right node raising (RNR), and subject ex-
traction from an embedded clause (SbEm), all
chosen for being relatively frequent and easy to
identify in PTB trees. Examples of the con-
structions can be seen in Figure 1. The evalu-
ation set contains 80 sentences per construction
(which may translate into more than 80 depen-
dencies, since sentences containing coordinations
may have more than one gold-standard depen-
dency), while the development set contains be-
tween 13 and 37 sentences per construction. The
data for ObQ sentences was obtained from various
years of TREC, and for the rest of the construc-
tions from the WSJ (0-1 and 22-24) and Brown
sections of the PTB.
Each sentence is annotated with one or more
gold-standard dependency relations representing
the relevant unbounded dependency. The gold-
standard dependencies are shown as arcs below
the sentences in Figure 1. The format of the de-
pendencies in the corpus is loosely based on the
Stanford typed dependency scheme, although the
evaluation procedure permits alternative represen-
tations and does not require that the parser out-
put match the gold-standard exactly, as long as the
?spirit? of the construction is correct.
The ability to recover unbounded dependencies
is important because they frequently form part of
the basic predicate-argument structure of a sen-
tence. Subject and object dependencies in par-
ticular are crucial for a number of tasks, includ-
ing information extraction and question answer-
ing. Moreover, Rimell et al (2009) show that,
although individual types of unbounded depen-
dencies may be rare, the unbounded dependency
types in the corpus, considered as a class, occur in
as many as 10% of sentences in the PTB.
In Rimell et al (2009), five state-of-the-art
parsers were evaluated for their recall on the gold-
standard dependencies. Three of the parsers were
based on grammars automatically extracted from
the PTB: the C&C CCG parser (Clark and Curran,
2007), the Enju HPSG parser (Miyao and Tsujii,
2005), and the Stanford parser (Klein and Man-
ning, 2003). The two remaining systems were the
834
RASP parser (Briscoe et al, 2006), using a man-
ually constructed grammar and a statistical parse
selection component, and the DCU post-processor
of PTB parsers (Cahill et al, 2004) using the out-
put of the Charniak and Johnson reranking parser
(Charniak and Johnson, 2005). Because of the
wide variation in parser output representations, a
mostly manual evaluation was performed to en-
sure that each parser got credit for the construc-
tions it recovered correctly. The parsers were run
essentially ?out of the box?, meaning that the de-
velopment set was used to confirm input and out-
put formats, but no real tuning was performed. In
addition, since a separate question model is avail-
able for C&C, this was also evaluated on ObQ
sentences. The best overall performers were C&C
and Enju, which is unsurprising since they are
deep parsers based on grammar formalisms de-
signed to recover just such dependencies. The
DCU post-processor performed somewhat worse
than expected, often identifying the existence of
an unbounded dependency but failing to iden-
tify the grammatical class (subject, object, etc.).
RASP and Stanford, although not designed to re-
cover such dependencies, nevertheless recovered
a subset of them. Performance of the parsers also
varied widely across the different constructions.
3 Dependency Parsers
In this paper we repeat the study of Rimell et al
(2009) for two dependency parsers, with the goal
of evaluating how parsers based on dependency
grammars perform on unbounded dependencies.
MSTParser1 is a freely available implementa-
tion of the parsing models described in McDon-
ald (2006). According to the categorization of
parsers in Ku?bler et al (2008) it is a graph-based
parsing system in that core parsing algorithms can
be equated to finding directed maximum span-
ning trees (either projective or non-projective)
from a dense graph representation of the sentence.
Graph-based parsers typically rely on global train-
ing and inference algorithms, where the goal is to
learn models in which the weight/probability of
correct trees is higher than that of incorrect trees.
At inference time a global search is run to find the
1http://mstparser.sourceforge.net
highest weighted dependency tree. Unfortunately,
global inference and learning for graph-based de-
pendency parsing is typically NP-hard (McDonald
and Satta, 2007). As a result, graph-based parsers
(including MSTParser) often limit the scope of
their features to a small number of adjacent arcs
(usually two) and/or resort to approximate infer-
ence (McDonald and Pereira, 2006).
MaltParser2 is a freely available implementa-
tion of the parsing models described in Nivre et
al. (2006a) and Nivre et al (2006b). MaltParser is
categorized as a transition-based parsing system,
characterized by parsing algorithms that produce
dependency trees by transitioning through abstract
state machines (Ku?bler et al, 2008). Transition-
based parsers learn models that predict the next
state given the current state of the system as well
as features over the history of parsing decisions
and the input sentence. At inference time, the
parser starts in an initial state, then greedily moves
to subsequent states ? based on the predictions of
the model ? until a termination state is reached.
Transition-based parsing is highly efficient, with
run-times often linear in sentence length. Further-
more, transition-based parsers can easily incorpo-
rate arbitrary non-local features, since the current
parse structure is fixed by the state. However, the
greedy nature of these systems can lead to error
propagation if early predictions place the parser
in incorrect states.
McDonald and Nivre (2007) compared the ac-
curacy of MSTParser and MaltParser along a
number of structural and linguistic dimensions.
They observed that, though the two parsers ex-
hibit indistinguishable accuracies overall, MST-
Parser tends to outperform MaltParser on longer
dependencies as well as those dependencies closer
to the root of the tree (e.g., verb, conjunction and
preposition dependencies), whereas MaltParser
performs better on short dependencies and those
further from the root (e.g., pronouns and noun de-
pendencies). Since long dependencies and those
near to the root are typically the last constructed
in transition-based parsing systems, it was con-
cluded that MaltParser does suffer from some
form of error propagation. On the other hand, the
2http://www.maltparser.org
835
richer feature representations of MaltParser led to
improved performance in cases where error prop-
agation has not occurred. However, that study did
not investigate unbounded dependencies.
4 Methodology
In this section, we describe the methodological
setup for the evaluation, including parser training,
post-processing, and evaluation.3
4.1 Parser Training
One important difference between MSTParser and
MaltParser, on the one hand, and the best perform-
ing parsers evaluated in Rimell et al (2009), on
the other, is that the former were never developed
specifically as parsers for English. Instead, they
are best understood as data-driven parser gener-
ators, that is, tools for generating a parser given
a training set of sentences annotated with de-
pendency structures. Over the years, both sys-
tems have been applied to a wide range of lan-
guages (see, e.g., McDonald et al (2006), Mc-
Donald (2006), Nivre et al (2006b), Hall et al
(2007), Nivre et al (2007)), but they come with
no language-specific enhancements and are not
equipped specifically to deal with unbounded de-
pendencies.
Since the dependency representation used in
the evaluation corpus is based on the Stanford
typed dependency scheme (de Marneffe et al,
2006), we opted for using the WSJ section of
the PTB, converted to Stanford dependencies, as
our primary source of training data. Thus, both
parsers were trained on section 2?21 of the WSJ
data, which we converted to Stanford dependen-
cies using the Stanford parser (Klein and Man-
ning, 2003). The Stanford scheme comes in sev-
eral varieties, but because both parsers require the
dependency structure for each sentence to be a
tree, we had to use the so-called basic variety (de
Marneffe et al, 2006).
It is well known that questions are very rare
in the WSJ data, and Rimell et al (2009) found
that parsers trained only on WSJ data generally
performed badly on the questions included in the
3To ensure replicability, we provide all experimental
settings, post-processing scripts and additional information
about the evaluation at http://stp.ling.uu.se/?nivre/exp/.
evaluation corpus, while the C&C parser equipped
with a model trained on a combination of WSJ
and question data had much better performance.
To investigate whether the performance of MST-
Parser and MaltParser on questions could also be
improved by adding more questions to the train-
ing data, we trained one variant of each parser
using data that was extended with 3924 ques-
tions taken from QuestionBank (QB) (Judge et al,
2006).4 Since the QB sentences are annotated in
PTB style, it was possible to use the same conver-
sion procedure as for the WSJ data. However, it is
clear that the conversion did not always produce
adequate dependency structures for the questions,
an observation that we will return to in the error
analysis below.
In comparison to the five parsers evaluated in
Rimell et al (2009), it is worth noting that MST-
Parser and MaltParser were trained on the same
basic data as four of the five, but with a differ-
ent kind of syntactic representation ? dependency
trees instead of phrase structure trees or theory-
specific representations from CCG and HPSG. It
is especially interesting to compare MSTParser
and MaltParser to the Stanford parser, which es-
sentially produces the same kind of dependency
structures as output but uses the original phrase
structure trees from the PTB as input to training.
For our experiments we used MSTParser with
the same parsing algorithms and features as re-
ported in McDonald et al (2006). However, un-
like that work we used an atomic maximum en-
tropy model as the second stage arc predictor as
opposed to the more time consuming sequence la-
beler. McDonald et al (2006) showed that there is
negligible accuracy loss when using atomic rather
than structured labeling. For MaltParser we used
the projective Stack algorithm (Nivre, 2009) with
default settings and a slightly enriched feature
model. All parsing was projective because the
Stanford dependency trees are strictly projective.
4QB contains 4000 questions, but we removed all ques-
tions that also occurred in the test or development set of
Rimell et al (2009), who sampled their questions from the
same TREC QA test sets.
836
4.2 Post-Processing
All the development and test sets in the corpus
of Rimell et al (2009) were parsed using MST-
Parser and MaltParser after part-of-speech tagging
the input using SVMTool (Gime?nez and Ma`rquez,
2004) trained on section 2?21 of the WSJ data in
Stanford basic dependency format. The Stanford
parser has an internal module that converts the
basic dependency representation to the collapsed
representation, which explicitly represents addi-
tional dependencies, including unbounded depen-
dencies, that can be inferred from the basic rep-
resentation (de Marneffe et al, 2006). We per-
formed a similar conversion using our own tool.
Broadly speaking, there are three ways in which
unbounded dependencies can be inferred from the
Stanford basic dependency trees, which we will
refer to as simple, complex, and indirect. In the
simple case, the dependency coincides with a sin-
gle, direct dependency relation in the tree. This
is the case, for example, in Figure 1d?e, where
all that is required is that the parser identifies
the dependency relation from a governor to an
argument (dobj(see, What), dobj(have,
effect)), which we call the Arg relation; no
post-processing is needed.
In the complex case, the dependency is repre-
sented by a path of direct dependencies in the tree,
as exemplified in Figure 1a. In this case, it is
not enough that the parser correctly identifies the
Arg relation dobj(carries, that); it must
also find the dependency rcmod(fragment,
carries). We call this the Link relation, be-
cause it links the argument role inside the relative
clause to an element outside the clause. Other ex-
amples of the complex case are found in Figure 1c
and in Figure 1f.
In the indirect case, finally, the dependency
cannot be defined by a path of labeled depen-
dencies, whether simple or complex, but must
be inferred from a larger context of the tree us-
ing heuristics. Consider Figure 1b, where there
is a Link relation (rcmod(things, do)), but
no corresponding Arg relation inside the relative
clause (because there is no overt relative pro-
noun). However, given the other dependencies,
we can infer with high probability that the im-
plicit relation is dobj. Another example of the
indirect case is in Figure 1g. Our post-processing
tool performs more heuristic inference for the in-
direct case than the Stanford parser does (cf. Sec-
tion 4.3).
In order to handle the complex and indirect
cases, our post-processor is triggered by the oc-
currence of a Link relation (rcmod or conj) and
first tries to add dependencies that are directly im-
plied by a single Arg relation (relations involving
relative pronouns for rcmod, shared heads and
dependents for conj). If there is no overt rela-
tive pronoun, or the function of the relative pro-
noun is underspecified, the post-processor relies
on the obliqueness hierarchy subj < dobj <
pobj and simply picks the first ?missing func-
tion?, unless it finds a clausal complement (indi-
cated by the labels ccomp and xcomp), in which
case it descends to the lower clause and restarts
the search there.
4.3 Parser Evaluation
The evaluation was performed using the same cri-
teria as in Rimell et al (2009). A dependency
was considered correctly recovered if the gold-
standard head and dependent were correct and
the label was an ?acceptable match? to the gold-
standard label, indicating the grammatical func-
tion of the extracted element at least to the level
of subject, passive subject, object, or adjunct.
The evaluation in Rimell et al (2009) took
into account a wide variety of parser output for-
mats, some of which differed significantly from
the gold-standard. Since MSTParser and Malt-
Parser produced Stanford dependencies for this
experiment, evaluation required less manual ex-
amination than for some of the other parsers, as
was also the case for the output of the Stanford
parser in the original evaluation. However, a man-
ual evaluation was still performed in order to re-
solve questionable cases.
5 Results
The results are shown in Table 1, where the ac-
curacy for each construction is the percentage of
gold-standard dependencies recovered correctly.
The Avg column represents a macroaverage, i.e.
the average of the individual scores on the seven
constructions, while the WAvg column represents
837
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
MST 34.1 47.3 78.9 65.5 13.8 45.4 37.6 46.1 63.4
Malt 40.7 50.5 84.2 70.2 16.2 39.7 23.5 46.4 66.9
MST-Q 41.2 50.0
Malt-Q 31.2 48.5
Table 1: Parser accuracy on the unbounded dependency corpus.
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
C&C 59.3 62.6 80.0 72.6 81.2 49.4 22.4 61.1 69.9
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.9 70.9
MST 34.1 47.3 78.9 65.5 41.2 45.4 37.6 50.0 63.4
Malt 40.7 50.5 84.2 70.2 31.2 39.7 23.5 48.5 66.9
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 34.6 47.0
RASP 16.5 1.1 53.7 17.9 27.5 34.5 15.3 23.8 34.1
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 37.0 50.3
Table 2: Parser accuracy on the unbounded dependency corpus. The ObQ score for C&C, MSTParser, and MaltParser is for
a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).
a weighted macroaverage, where the construc-
tions are weighted proportionally to their relative
frequency in the PTB. WAvg excludes ObQ sen-
tences, since frequency statistics were not avail-
able for this construction in Rimell et al (2009).
Our first observation is that the accuracies for
both systems are considerably below the ?90%
unlabeled and ?88% labeled attachment scores
for English that have been reported previously
(McDonald and Pereira, 2006; Hall et al, 2006).
Comparing the two parsers, we see that Malt-
Parser is more accurate on dependencies in rela-
tive clause constructions (ObRC, ObRed, SbRC,
and Free), where argument relations tend to be
relatively local, while MSTParser is more accu-
rate on dependencies in RNR and SbEm, which
involve more distant relations. Without the ad-
ditional QB training data, the average scores for
the two parsers are indistinguishable, but MST-
Parser appears to have been better able to take
advantage of the question training, since MST-Q
performs better than Malt-Q on ObQ sentences.
On the weighted average MaltParser scores 3.5
points higher, because the constructions on which
it outperforms MSTParser are more frequent in
the PTB, and because WAvg excludes ObQ, where
MSTParser is more accurate.
Table 2 shows the results for MSTParser and
MaltParser in the context of the other parsers eval-
uated in Rimell et al (2009).5 For the parsers
5The average scores reported differ slightly from those in
which have a model trained on questions, namely
C&C, MSTParser, and MaltParser, the figure
shown for ObQ sentences is that of the question
model. It can be seen that MSTParser and Malt-
Parser perform below C&C and Enju, but above
the other parsers, and that MSTParser achieves the
highest score on SbEm sentences and MaltParser
on SbRC sentences. It should be noted, however,
that Table 2 does not represent a direct compar-
ison across all parsers, since most of the other
parsers would have benefited from heuristic post-
processing of the kind implemented here for MST-
Parser and MaltParser. This is especially true for
RASP, where the grammar explicitly leaves some
types of attachment decisions for post-processing.
For DCU, improved labeling heuristics would sig-
nificantly improve performance. It is instructive to
compare the dependency parsers to the Stanford
parser, which uses the same output representation
and has been used to prepare the training data for
our experiments. Stanford has very low recall on
ObRed and SbEm, the categories where heuristic
inference plays the largest role, but mirrors MST-
Parser for most other categories.
6 Error Analysis
We now proceed to a more detailed error analy-
sis, based on the development sets, and classify
Rimell et al (2009), where a microaverage (i.e., average over
all dependencies in the corpus, regardless of construction)
was reported.
838
the errors made by the parsers into three cate-
gories: A global error is one where the parser
completely fails to build the relevant clausal struc-
ture ? the relative clause in ObRC, ObRed, SbRC,
Free, SbEmb; the interrogative clause in ObQ; and
the clause headed by the higher conjunct in RNR
? often as a result of surrounding parsing errors.
When a global error occurs, it is usually mean-
ingless to further classify the error, which means
that this category excludes the other two. An Arg
error is one where the parser has constructed the
relevant clausal structure but fails to find the Arg
relation ? in the simple and complex cases ? or the
set of surrounding Arg relations needed to infer
an implicit Arg relation ? in the indirect case (cf.
Section 4.2). A Link error is one where the parser
fails to find the crucial Link relation ? rcmod
in ObRC, ObRed, SbRC, SbEmb; conj in RNR
(cf. Section 4.2). Link errors are not relevant for
Free and ObQ, where all the crucial relations are
clause-internal.
Table 3 shows the frequency of different error
types for MSTParser (first) and MaltParser (sec-
ond) in the seven development sets. First of all,
we can see that the overall error distribution is
very similar for the two parsers, which is proba-
bly due to the fact that they have been trained on
exactly the same data with exactly the same an-
notation (unlike the five parsers previously eval-
uated). However, there is a tendency for MST-
Parser to make fewer Link errors, especially in
the relative clause categories ObRC, ObRed and
SbRC, which is compatible with the observation
from the test results that MSTParser does better
on more global dependencies, while MaltParser
has an advantage on more local dependencies, al-
though this is not evident from the statistics from
the relatively small development set.
Comparing the different grammatical construc-
tions, we see that Link errors dominate for the rel-
ative clause categories ObRC, ObRed and SbRC,
where the parsers make very few errors with
respect to the internal structure of the relative
clauses (in fact, no errors at all for MaltParser
on SbRC). This is different for SbEm, where the
analysis of the argument structure is more com-
plex, both because there are (at least) two clauses
involved and because the unbounded dependency
Type Glo
bal
Arg Lin
k
A+
L
Err
ors
# D
eps
ObRC 0/1 1/1 7/11 5/3 13/16 20
ObRed 0/1 0/1 6/7 3/4 9/13 23
SbRC 2/1 1/0 7/13 0/0 10/14 43
Free 2/1 3/5 ? ? 5/6 22
ObQ 4/7 13/13 ? ? 17/20 25
RNR 6/4 4/6 0/0 4/5 14/15 28
SbEm 3/4 3/2 0/0 3/3 9/9 13
Table 3: Distribution of error types in the development
sets; frequencies for MSTParser listed first and MaltParser
second. The columns Arg and Link give frequencies for
Arg/Link errors occurring without the other error type, while
A+L give frequencies for joint Arg and Link errors.
can only be inferred indirectly from the basic de-
pendency representation (cf. Section 4.2). An-
other category where Arg errors are frequent is
RNR, where all such errors consist in attaching
the relevant dependent to the second conjunct in-
stead of to the first.6 Thus, in the example in Fig-
ure 1f, both parsers found the conj relation be-
tween puzzled and angered but attached by to the
second verb.
Global errors are most frequent for RNR, prob-
ably indicating that coordinate structures are diffi-
cult to parse in general, and for ObQ (especially
for MaltParser), probably indicating that ques-
tions are not well represented in the training set
even after the addition of QB data.7 As noted
in Section 4.1, this may be partly due to the fact
that conversion to Stanford dependencies did not
seem to work as well for QB as for the WSJ data.
Another problem is that the part-of-speech tagger
used was trained on WSJ data only and did not
perform as well on the ObQ data. Uses of What as
a determiner were consistently mistagged as pro-
nouns, which led to errors in parsing. Thus, for
the example in Figure 1e, both parsers produced
the correct analysis except that, because of the tag-
ging error, they treated What rather than effect as
the head of the wh-phrase, which counts as an er-
ror in the evaluation.
In order to get a closer look specifically at the
Arg errors, Table 4 gives the confusion matrix
6In the Stanford scheme, an argument or adjunct must be
attached to the first conjunct in a coordination to indicate that
it belongs to both conjuncts.
7Parsers trained without QB had twice as many global
errors.
839
Sb Ob POb EmSb EmOb Other Total
Sb ? 0/0 0/0 0/0 0/0 2/1 2/1
Ob 2/3 ? 0/0 0/1 0/0 4/2 6/6
POb 2/0 7/5 ? 0/0 0/0 5/8 14/13
EmSb 1/1 4/2 0/0 ? 0/0 1/2 6/5
EmOb 0/0 3/1 0/0 0/0 ? 1/6 4/7
Total 5/4 14/8 0/0 0/1 0/0 13/19 32/32
Table 4: Confusion matrix for Arg errors (excluding RNR
and using parsers trained on QB for ObQ); frequencies for
MSTParser listed first and MaltParser second. The column
Other covers errors where the function is left unspecified or
the argument is attached to the wrong head.
for such errors, showing which grammatical func-
tions are mistaken for each other, with an extra
category Other for cases where the function is left
unspecified by the parser or the error is an attach-
ment error rather than a labeling error (and ex-
cluding the RNR category because of the special
nature of the Arg errors in this category). The
results again confirm that the two parsers make
very few errors on subjects and objects clause-
internally. The few cases where an object is
mistaken as a subject occur in ObQ, where both
parsers perform rather poorly in general. By con-
trast, there are many more errors on prepositional
objects and on embedded subjects and objects. We
believe an important part of the explanation for
this pattern is to be found in the Stanford depen-
dency representation, where subjects and objects
are marked as such but all other functions real-
ized by wh elements are left unspecified (using the
generic rel dependency), which means that the re-
covery of these functions currently has to rely on
heuristic rules as described in Section 4.2. Finally,
we think it is possible to observe the tendency for
MaltParser to be more accurate at local labeling
decisions ? reflected in fewer cross-label confu-
sions ? and for MSTParser to perform better on
more distant attachment decisions ? reflected in
fewer errors in the Other category (and in fewer
Link errors).
7 Conclusion
In conclusion, the capacity of MSTParser and
MaltParser to recover unbounded dependencies is
very similar on the macro and weighted macro
level, but there is a clear distinction in their
strengths ? constructions involving more distant
dependencies such as ObQ, RNR and SbEm for
MSTParser and constructions with more locally
defined configurations such as ObRC, ObRed,
SbRC and Free for MaltParser. This is a pattern
that has been observed in previous evaluations of
the parsers and can be explained by the global
learning and inference strategy of MSTParser and
the richer feature space of MaltParser (McDonald
and Nivre, 2007).
Perhaps more interestingly, the accuracies of
MSTParser and MaltParser are only slightly be-
low the best performing systems in Rimell et al
(2009) ? C&C and Enju. This is true even though
MSTParser and MaltParser have not been engi-
neered specifically for English and lack special
mechanisms for handling unbounded dependen-
cies, beyond the simple post-processing heuristic
used to extract them from the output trees. Thus,
it is reasonable to speculate that the addition of
such mechanisms could lead to computationally
lightweight parsers with the ability to extract un-
bounded dependencies with high accuracy.
Acknowledgments
We thank Marie-Catherine de Marneffe for great
help with the Stanford parser and dependency
scheme, Llu??s Ma`rquez and Jesu?s Gime?nez for
great support with SVMTool, Josef van Gen-
abith for sharing the QuestionBank data, and
Stephen Clark and Mark Steedman for helpful
comments on the evaluation process and the pa-
per. Laura Rimell was supported by EPSRC grant
EP/E035698/1 and Carlos Go?mez-Rodr??guez
by MEC/FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus,
Bolsas Estadas INCITE/FSE cofinanced).
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of 4th DARPAWorkshop,
306?311.
Briscoe, T., J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings
840
of the COLING/ACL 2006 Interactive Presentation
Sessions, 77?80.
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Cahill, A., M. Burke, R. O?Donovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL, 320?327.
Carreras, X., M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of
CoNLL, 9?16.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL, 173?180.
Clark, S. and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
Gime?nez, J. and L. Ma`rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of LREC.
Hall, J., J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, 316?323.
Hall, J., J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, 586?594.
Judge, J., A. Cahill, and J. van Genabith. 2006. Ques-
tionBank: Creating a corpus of parse-annotated
questions. In Proceedings of COLING-ACL, 497?
504.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Ku?bler, S., R. McDonald, and J. Nivre. 2008. Depen-
dency Parsing. Morgan and Claypool.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of EACL, 81?88.
McDonald, R. and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing.
In Proceedings of IWPT, 122?131.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, 91?98.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL, 216?
220.
McDonald, R.. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proceedings of ACL, 83?90.
Nivre, J., J. Hall, and J. Nilsson. 2006a. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC, 2216?2219.
Nivre, J., J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proceed-
ings of CoNLL, 221?225.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of ACL-
IJCNLP, 351?359.
Rimell, L., S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings EMNLP, 813?821.
Sagae, K. and A. Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL HLT: Short
Papers, 129?132.
841
Coling 2010: Poster Volume, pages 1471?1479,
Beijing, August 2010
Chart Pruning for Fast Lexicalised-Grammar Parsing
Yue Zhanga? Byung-Gyu Ahn b? Stephen Clarka? Curt Van Wyk c
James R. Currand Laura Rimella
Computer Laboratorya Computer Scienceb Computer Sciencec School of ITd
Cambridge Johns Hopkins Northwestern College Sydney
{yue.zhang,stephen.clark}@cl.cam.ac.uka? bahn@jhu.edu b?
Abstract
Given the increasing need to process mas-
sive amounts of textual data, efficiency of
NLP tools is becoming a pressing concern.
Parsers based on lexicalised grammar for-
malisms, such as TAG and CCG, can be
made more efficient using supertagging,
which for CCG is so effective that every
derivation consistent with the supertagger
output can be stored in a packed chart.
However, wide-coverage CCG parsers still
produce a very large number of deriva-
tions for typical newspaper or Wikipedia
sentences. In this paper we investigate
two forms of chart pruning, and develop a
novel method for pruning complete cells
in a parse chart. The result is a wide-
coverage CCG parser that can process al-
most 100 sentences per second, with lit-
tle or no loss in accuracy over the baseline
with no pruning.
1 Introduction
Many NLP tasks and applications require the pro-
cessing of massive amounts of textual data. For
example, knowledge acquisition efforts can in-
volve processing billions of words of text (Cur-
ran, 2004). Also, the increasing need to process
large amounts of web data places an efficiency
demand on existing NLP tools. TextRunner, for
example, is a system that performs open infor-
mation extraction on the web (Lin et al, 2009).
However, the text processing that is performed by
TextRunner, in particular the parsing, is rudimen-
tary: finite-state shallow parsing technology that
is now decades old. TextRunner uses this technol-
ogy largely for efficiency reasons.
Many of the popular wide-coverage parsers
available today operate at around one newspa-
per sentence per second (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007). There are de-
pendency parsers that operate orders of magni-
tude faster, by exploiting the fact that accurate
dependency parsing can be achieved by using a
shift-reduce linear-time process which makes a
single decision at each point in the parsing pro-
cess (Nivre and Scholz, 2004).
In this paper we focus on the Combinatory Cat-
egorial Grammar (CCG) parser of Clark and Cur-
ran (2007). One advantage of the CCG parser is
that it is able to assign rich structural descriptions
to sentences, from a variety of representations,
e.g. CCG derivations, CCG dependency structures,
grammatical relations (Carroll et al, 1998), and
first-order logical forms (Bos et al, 2004). One
of the properties of the grammar formalism is
that it is lexicalised, associating CCG lexical cate-
gories, or CCG supertags, with the words in a sen-
tence (Steedman, 2000). Clark and Curran (2004)
adapt the technique of supertagging (Bangalore
and Joshi, 1999) to CCG, using a standard max-
imum entropy tagger to assign small sets of su-
pertags to each word. The reduction in ambiguity
resulting from the supertagging stage results in a
surprisingly efficient parser, given the rich struc-
tural output, operating at tens of newspaper sen-
tences per second.
In this paper we demonstrate that the CCG
parser can be made more than twice as fast, with
little or no loss in accuracy. A noteworthy feature
of the CCG parser is that, after the supertagging
1471
stage, the parser builds a complete packed chart,
storing all sentences consistent with the assigned
supertags and the parser?s CCG combinatory rules,
with no chart pruning whatsoever. The use of
chart pruning techniques, typically some form of
beam search, is essential for practical parsing us-
ing Penn Treebank parsers (Collins, 1999; Petrov
and Klein, 2007; Charniak and Johnson, 2005), as
well as practical parsers based on linguistic for-
malisms, such as HPSG (Ninomiya et al, 2005)
and LFG (Kaplan et al, 2004). However, in the
CCG case, the use of the supertagger means that
enough ambiguity has already been resolved to al-
low the complete chart to be represented.
Despite the effectiveness of the supertagging
stage, the number of derivations stored in a packed
chart can still be enormous for typical newspa-
per sentences. Hence it is an obvious question
whether chart pruning techniques can be prof-
itably applied to the CCG parser. Some previous
work (Djordjevic et al, 2007) has investigated this
question but with little success.
In this paper we investigate two types of chart
pruning: a standard beam search, similar to that
used in the Collins parser (Collins, 1999), and a
more aggressive strategy in which complete cells
are pruned, following Roark and Hollingshead
(2009). Roark and Hollingshead use a finite-state
tagger to decide which words in a sentence can
end or begin constituents, from which whole cells
in the chart can be removed. We develop a novel
extension to this approach, in which a tagger is
trained to infer the maximum length constituent
that can begin or end at a particular word. These
lengths can then be used in a more agressive prun-
ing strategy which we show to be significantly
more effective than the basic approach.
Both beam search and cell pruning are highly
effective, with the resulting CCG parser able to
process almost 100 sentences per second using
a single CPU, for both newspaper and Wikipedia
data, with little or no loss in accuracy.
2 The CCG Parser
The parser is described in detail in Clark and Cur-
ran (2007). It is based on CCGbank, a CCG ver-
sion of the Penn Treebank developed by Hocken-
maier and Steedman (2007).
The stages in the parsing pipeline are as fol-
lows. First, a POS tagger assigns a single POS tag
to each word in a sentence. Second, a CCG su-
pertagger assigns lexical categories to the words
in the sentence. Third, the parsing stage combines
the categories, using CCG?s combinatory rules,
and builds a packed chart representation contain-
ing all the derivations which can be built from
the lexical categories. Finally, the Viterbi algo-
rithm finds the highest scoring derivation from
the packed chart, using the normal-form log-linear
model described in Clark and Curran (2007).
Sometimes the parser is unable to build an anal-
ysis which spans the whole sentence. When this
happens the parser and supertagger interact us-
ing the adaptive supertagging strategy described
in Clark and Curran (2004): the parser effectively
asks the supertagger to provide more lexical cate-
gories for each word. This potentially continues
for a number of iterations until the parser does
create a spanning analysis, or else it gives up and
moves to the next sentence.
The parser uses the CKY algorithm (Kasami,
1965; Younger, 1967) described in Steedman
(2000) to create a packed chart. The CKY al-
gorithm applies naturally to CCG since the gram-
mar is binary. It builds the chart bottom-up, start-
ing with two-word constituents (assuming the su-
pertagging phase has been completed), incremen-
tally increasing the span until the whole sentence
is covered. The chart is packed in the standard
sense that any two equivalent constituents created
during the parsing process are placed in the same
equivalence class, with pointers to the children
used in the creation. Equivalence is defined in
terms of the category and head of the constituent,
to enable the Viterbi algorithm to efficiently find
the highest scoring derivation.1 A textbook treat-
ment of CKY applied to statistical parsing is given
in Jurafsky and Martin (2000).
3 Data and Evaluation Metrics
We performed efficiency and accuracy tests on
newspaper and Wikipedia data. For the newspa-
per data, we used the standard test sections from
1Use of the Viterbi algorithm in this way requires the fea-
tures in the parser model to be local to a single rule applica-
tion; Clark and Curran (2007) has more discussion.
1472
(ncmod num hundred 1 Seven 0)
(conj and 2 sixty-one 3)
(conj and 2 hundred 1)
(dobj in 6 total 7)
(ncmod made 5 in 6)
(aux made 5 were 4)
(ncsubj made 5 and 2 obj)
(passive made 5)
Seven hundred and sixty-one were made in
total.
Figure 1: Example Wikipedia test sentence anno-
tated with grammatical relations.
CCGbank. Following Clark and Curran (2007) we
used the CCG dependencies for accuracy evalua-
tion, comparing those output by the parser with
the gold-standard dependencies in CCGbank. Un-
like Clark and Curran, we calculated recall scores
over all sentences, including those for which the
parser did not find an analysis. For the WSJ data
the parser fails on a small number of sentences
(less than 1%), but the chart pruning has the effect
of reducing this failure rate further, and we felt
that this should be factored into the calculation of
recall and hence F-score.
In order to test the parser on Wikipedia text,
we created two test sets. The first, Wiki 300, for
testing accuracy, consists of 300 sentences man-
ually annotated with grammatical relations (GRs)
in the style of Briscoe and Carroll (2006). An
example sentence is given in Figure 1. The data
was created by manually correcting the output of
the parser on these sentences, with the annotation
being performed by Clark and Rimell, including
checks on a subset of these cases to ensure con-
sistency across the two annotators. For the ac-
curacy evaluation, we calculated precision, recall
and balanced F-measure over the GRs in the stan-
dard way.
For testing speed on Wikipedia, we used a cor-
pus of 2500 randomly chosen sentences, Wiki
2500. For all speed tests we measured the num-
ber of sentences per second, using a single CPU
and standard hardware.
4 Beam Search
The beam search approach used in our exper-
iments prunes all constituents in a cell having
scores below a multiple (?) of the score of the
? Speed Gain F-score Gain
Baseline 43.0 85.55
0.001 48.6 13% 85.82 0.27
0.002 54.2 26% 85.88 0.33
0.005 59.0 37% 85.73 0.18
0.01 66.7 55% 85.53 -0.02
Table 1: Accuracy and speed results using differ-
ent beam values ?.
? Speed Gain F-score Gain
Baseline 43.0 85.55
10 60.1 39% 85.55 0.00
20 70.6 64% 85.66 0.11
30 72.3 68% 85.65 0.10
40 76.4 77% 85.63 0.08
50 76.7 78% 85.62 0.07
60 74.5 73% 85.71 0.16
80 68.4 59% 85.71 0.16
100 62.0 44% 85.73 0.18
None 59.0 37% 85.73 0.18
Table 2: Accuracy and speed results for different
values of ? where ? = 0.005.
highest scoring constituent for that cell.2 The
scores for a constituent are calculated using the
same model used to find the highest scoring
derivation. We consider two scores: the Viterbi
score, which is the score of the highest scoring
sub-derivation for that constituent; and the inside
score, which is the sum over all sub-derviations
for that constituent. We investigated the follow-
ing: the trade-off between the aggressiveness of
the beam search and accuracy; the comparison be-
tween the Viterbi and inside scores; and whether
applying the beam to only certain cells in the chart
can improve performance.
Table 1 shows results on Section 00 of CCG-
bank, using the Viterbi score to prune. As ex-
pected, the parsing speed increases as the value
of ? increases, since more constituents are pruned
with a higher ? value. The pruning is effective,
with a ? value of 0.01 giving a 55% speed increase
with neglible loss in accuracy.3
2One restriction we apply in practice is that only con-
stituents resulting from the application of a CCG binary rule,
rather than a unary rule, are pruned.
3The small accuracy increase for some ? values could be
attributable to two factors: one, the parser may select a lower
1473
Speed F-score
Dataset Baseline Beam Gain Baseline Beam Gain
WSJ 00 43.0 76.4 77% 85.55 85.63 0.08
WSJ 02-21 53.4 99.4 86% 93.61 93.27 -0.34
WSJ 23 55.0 107.0 94% 87.12 86.90 -0.22
Wiki 300 35.5 80.3 126% 84.23 85.06 0.83
Wiki 2500 47.6 90.3 89%
Table 4: Beam search results on WSJ 00, 02-21, 23 and Wikipedia texts with ? = 0.005 and ? = 40.
? ? Speed F-score
Baseline 24.7 85.55
inside scores
0.01 37.7 85.52
0.001 25.3 85.79
0.005 10 33.4 85.54
0.005 20 39.5 85.64
0.005 50 42.9 85.58
Viterbi scores
0.01 38.1 85.53
0.001 28.2 85.82
0.005 10 33.6 85.55
0.005 20 39.4 85.66
0.005 50 43.1 85.62
Table 3: Comparison between using Viterbi scores
and inside scores as beam scores.
We also studied the effect of the beam search
at different levels of the chart. We applied a selec-
tive beam in which pruning is only applied to con-
stituents of length less than or equal to a threshold
?. For example, if ? = 20, pruning is applied only
to constituents spanning 20 words or less. The re-
sults are shown in Table 2. The selective beam
is also highly effective, showing speed gains over
the baseline (which does not use a beam) with no
loss in F-score. For a ? value of 50 the speed in-
crease is 78% with no loss in accuracy.
Note that for ? greater than 50, the speed re-
duces. We believe that this is due to the cost
of calculating the beam scores and the reduced
effectiveness of pruning for cells with longer
spans (since pruning shorter constituents early in
the chart-parsing process prevents the creation of
many larger, low-scoring constituents later).
Table 3 shows the comparison between the in-
scoring but more accurate derivation; and two, a possible in-
crease in recall, discussed in Section 3, can lead to a higher
F-score.
side and Viterbi scores. The results are similar,
with Viterbi marginally outperforming the inside
score in most cases. The interesting result from
these experiments is that the summing used in cal-
culating the inside score does not improve perfor-
mance over the max operator used by Viterbi.
Table 4 gives results on Wikipedia text, com-
pared with a number of sections from CCGbank.
(Sections 02-21 provide the training data for the
parser which explains the high accuracy results
on these sections.) Despite the fact that the prun-
ing model is derived from CCGbank and based on
WSJ text, the speed improvements for Wikipedia
were even greater than for WSJ text, with param-
eters ? = 0.005 and ? = 40 leading to almost a
doubling of speed on the Wiki 2500 set, with the
parser operating at 90 sentences per second.
5 Cell Pruning
Whole cells can be pruned from the chart by tag-
ging words in a sentence. Roark and Hollingshead
(2009) used a binary tagging approach to prune a
CFG CKY chart, where tags are assigned to input
words to indicate whether they can be the start or
end of multiple-word constituents. We adapt their
method to CCG chart pruning. We also show the
limitation of binary tagging, and propose a novel
tagging method which leads to increased speeds
and accuracies over the binary taggers.
5.1 Binary tagging
Following Roark and Hollingshead (2009), we as-
sign the binary begin and end tags separately us-
ing two independent taggers. Given the input
?We like playing cards together?, the pruning ef-
fects of each type of tag on the CKY chart are
shown in Figure 2. In this chart, rows repre-
1474
XWe like playing cards together
1 2 3 4 5
1
2
4
5
3
1 1 1 0 0
X X
X
We like playing cards together
1 2 3 4 5
1
2
4
5
3
0 0 0 1 1
Figure 2: The pruning effect of begin (top) and
end (bottom) tags; X indicates a removed cell.
sent consituent sizes and columns represent initial
words of constituents. No cell in the first row of
the chart is pruned, since these cells correspond
to single words, and are necessary for finding a
parse. The begin tag for the input word ?cards? is
0, which means that it cannot begin a multi-word
constituent. Therefore, no cell in column 4 can
contain any constituent. The pruning effect of a
binary begin tag is to cross out a column of chart
cells (ignoring the first row) when the tag value
is zero. Similarly, the end tag of the word ?play-
ing? is 0, which means that it cannot be the end
of a multi-word constituent. Consequently cell (2,
2), which contains constituents for ?like playing?,
and cell (1, 3), which contains constituents for
?We like playing?, must be empty. The pruning
effect of a binary end tag is to cross out a diagonal
of cells (ignoring the first row) when the tag value
is zero.
We use a maximum entropy trigram tagger
(Ratnaparkhi, 1996; Curran and Clark, 2003) to
Model Speed F-score
baseline 25.10 84.89
begin only 27.49 84.71
end only 30.33 84.56
both 33.90 84.60
oracle 33.60 85.67
Table 5: Accuracy and speed results for the binary
taggers on Section 00 of CCGbank.
assign the begin and end tags. Features based on
the words and POS in a 5-word window, plus the
two previously assigned tags, are extracted from
the trigram ending with the current tag and the
five-word window with the current word in the
middle. In our development experiments, both the
begin and the end taggers gave a per-word accu-
racy of around 96%, similar to the accuracy re-
ported in Roark and Hollingshead (2009).
Table 5 shows accuracy and speed results for
the binary taggers.4 Using begin or end tags alone,
the parser achieved speed increases with a small
loss in accuracy. When both begin and end tags
are applied, the parser achieved further speed in-
creases, with no loss in accuracy compared to the
end tag alone. Row ?oracle? shows what happens
using the perfect begin and end taggers, by using
gold-standard constituent information from CCG-
bank. The F-score is higher, since the parser is
being guided away from incorrect derivations, al-
though the speed is no higher than when using au-
tomatically assigned tags.
5.2 Level tagging
A binary tag cannot take effect when there is any
chart cell in the corresponding column or diagonal
that contains constituents. For example, the begin
tag for the word ?card? in Figure 3 cannot be 0 be-
cause ?card? begins a two-word constituent ?card
games?. Hence none of the cells in the column can
be pruned using the binary begin tag, even though
all the cells from the third row above are empty.
We propose what we call a level tagging approach
to address this problem.
Instead of taking a binary value that indicates
4The baseline differs slightly to the previous section be-
cause gold-standard POS tags were used for the beam-search
experiments.
1475
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
Figure 3: The limitation of binary begin tags.
whether a whole column or diagonal of cells can
be pruned, a level tag (begin or end) takes an in-
teger value which indicates the row from which
a column or diagonal can be pruned in the up-
ward direction. For example, a level begin tag
with value 2 allows the column of chart cells for
the word ?card? in Figure 3 to be pruned from the
third row upwards. A level tag (begin or end) with
value 1 prunes the corresponding row or diago-
nal from the second row upwards; it has the same
pruning effect as a binary tag with value 0. For
convenience, value 0 for a level tag means that the
corresponding word can be the beginning or end
of any constituent, which is the same as a binary
tag value 1.
A comparison of the pruning effect of binary
and level tags for the sentence ?Playing card
games is fun? is shown in Figure 4. With a level
begin tag, more cells can be pruned from the col-
umn for ?card?. Therefore, level tags are poten-
tially more powerful for pruning.
We now need a method for assigning level tags
to words in a sentence. However, we cannot
achieve this with a straighforward classifier since
level tags are related; for example, a level tag (be-
gin or end) with value 2 implies level tags with
values 3 and above. We develop a novel method
for calculating the probability of a level tag for
a particular word. Our mechanism for calculat-
ing these probabilities uses what we call maxspan
tags, which can be assigned using a maximum en-
tropy tagger.
Maxspan tags take the same values as level tags.
However, the meanings of maxspan tags and level
X
XX
X
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
X
XX
X
Playing card games is fun
1 2 3 4 5
1
2
4
5
3
Figure 4: The pruning effect of binary (top) and
level (bottom) tags.
tags are different. While a level tag indicates the
row from which a column or diagonal of cells is
pruned, a maxspan tag represents the size of the
largest constituent a word begins or ends. For ex-
ample, in Figure 3, the level end tag for the word
?games? has value 3, since the largest constituent
this words ends spans ?playing card games?.
We use the standard maximum entropy trigram
tagger for maxspan tagging, where features are
extracted from tag trigrams and surrounding five-
word windows, as for the binary taggers. Parse
trees can be turned directly into training data for
a maxspan tagger. Since the level tag set is fi-
nite, we a require a maximum value N that a level
tag can take. We experimented with N = 2 and
N = 4, which reflects the limited range of the
features used by the taggers.5
During decoding, the maxspan tagger uses the
forward-backward algorithm to compute the prob-
ability of maxspan tag values for each word in the
5Higher values of N did not lead to improvements during
development experiments.
1476
Model Speed F-score
baseline 25.10 84.89
binary 33.90 84.60
binary oracle 33.60 85.67
level N = 2 32.79 84.92
level N = 4 34.91 84.95
level N = 4 oracle 47.45 86.49
Table 6: Accuracy and speed results for the level
taggers on Section 00 of CCGbank.
input. Then for each word, the probability of its
level tag tl having value x is the sum of the prob-
abilities of its maxspan tm tag having values 1..x:
P (tl = x) =
x?
i=1
P (tm = i)
Maxspan tag values i from 1 to x represent dis-
joint events in which the largest constituent that
the corresponding word begins or ends has size i.
Summing the probabilities of these disjoint events
gives the probability that the largest constituent
the word begins or ends has a size between 1 and
x, inclusive. That is also the probability that all
the constituents the word begins or ends are in the
range of cells from rows 1 to row x in the corre-
sponding column or diagonal. And therefore that
is also the probability that the chart cells above
row x in the corresponding column or diagonal
do not contain any constituents, which means that
the column and diagonal can be pruned from row
x upward. Therefore, it is also the probability of a
level tag with value x.
The probability of a level tag having value x
increases as x increases from 1 to N . We set a
probability threshold Q and choose the smallest
level tag value x with probability P (tl = x) ? Q
as the level tag for a word. If P (tl = N) < Q, we
set the level tag to 0 and do not prune the column
or diagonal. The threshold value determines a bal-
ance between pruning power and accuracy, with a
higher value pruning more cells but increasing the
risk of incorrectly pruning a cell. During devel-
opment we arrived at a threshold value of 0.8 as
providing a suitable compromise between pruning
power and accuracy.
Table 6 shows accuracy and speed results for
the level tagger, using a threshold value of 0.8.
Model Speed F-score
baseline 36.64 84.23
binary gold 49.59 84.36
binary self 40K 48.79 83.64
binary self 200K 51.51 83.71
binary self 1M 47.78 83.75
level gold 58.23 84.12
level self 40K 54.76 83.83
level self 200K 48.57 83.39
level self 1M 52.54 83.71
Table 7: Accuracy tests on Wiki 300 comparing
gold training (gold) with self training (self) for
different sizes of parser output for self-training.
We compare the effect of the binary tagger and
level taggers with N = 2 and N = 4. The accu-
racies with the level taggers are higher than those
with the binary tagger; they are also higher than
the baseline parsing accuracy. The parser achieves
the highest speed and accuracy when pruned with
the N = 4 level tagger. Comparing the oracle
scores, the level taggers lead to higher speeds than
the binary tagger, reflecting the increased pruning
power of the level taggers compared with the bi-
nary taggers.
5.2.1 Final experiments using gold training
and self training
In this section we report our final tests using
Wikipedia data. We used two methods to derive
training data for the taggers. The first is the stan-
dard method, which is to transform gold-standard
parse trees into begin and end tag sequences. This
method is the method that we used for all previ-
ous experiments, and we call it ?gold training?.
In addition to gold training, we also investigate
an alternative method, which is to obtain training
data for the taggers from the output of the parser
itself, in a form of self-training (McClosky et al,
2006). The intuition is that the tagger will learn
what constituents a trained parser will eventually
choose, and as long as the constituents favoured
by the parsing model are not pruned, no reduction
in accuracy can occur. There is the potential for
an increase in speed, however, due to the pruning
effect.
For gold training, we used sections 02-21 of
1477
Model Speed
baseline 47.6
binary gold 80.8
binary 40K 75.5
binary 200K 77.4
binary 1M 78.6
level gold 93.7
level 40K 92.8
level 200K 92.5
level 1M 96.6
Table 8: Speed tests with gold and self-training on
Wiki 2500.
CCGBank (which consists of about 40K training
sentences) to derive training data. For self train-
ing, we trained the parser on sections 02-21 of
CCGBank, and used the parser to parse 40 thou-
sand, 200 thousand and 1 million sentences from
Wikipedia, respectively. Then we derive three sets
of self training data from the three sets of parser
outputs. We then used our Wiki 300 set to test the
accuracy, and the Wiki 2500 set to test the speed
of the parser.
The results are shown in Tables 7 and 8, where
each row represents a training data set. Rows ?bi-
nary gold? and ?level gold? represent binary and
level taggers trained using gold training. Rows
?binary self X? and ?level self X? represent bi-
nary and level taggers trained using self training,
with the size of the training data being X sen-
tences.
It can be seen from the Tables that the accuracy
loss with self-trained binary or level taggers was
not large (in the worst case, the accuracy dropped
from 84.23% to 83.39%), while the speed was
significantly improved. Using binary taggers, the
largest speed improvement was from 47.6 sen-
tences per second to 80.8 sentences per second
(a 69.7% relative increase). Using level taggers,
the largest speed improvement was from 47.6 sen-
tences per second to 96.6 sentences per second (a
103% relative increase).
A potential advantage of self-training is the
availability of large amounts of training data.
However, our results are somewhat negative in
this regard, in that we find training the tagger on
more than 40,000 parsed sentences (the size of
CCGbank) did not improve the self-training re-
sults. We did see the usual speed improvements
from using the self-trained taggers, however, over
the baseline parser with no pruning.
6 Conclusion
Using our novel method of level tagging for prun-
ing complete cells in a CKY chart, the CCG parser
was able to process almost 100 Wikipedia sen-
tences per second, using both CCGbank and the
output of the parser to train the taggers, with little
or no loss in accuracy. This was a 103% increase
over the baseline with no pruning.
We also demonstrated that standard beam
search is highly effective in increasing the speed
of the CCG parser, despite the fact that the su-
pertagger has already had a significant pruning
effect. In future work we plan to investigate the
gains that can be achieved from combining the
two pruning methods, as well as other pruning
methods such as the self-training technique de-
scribed in Kummerfeld et al (2010) which re-
duces the number of lexical categories assigned
by the supertagger (leading to a speed increase).
Since these methods are largely orthogonal, we
expect to achieve further gains, leading to a re-
markably fast wide-coverage parser outputting
complex linguistic representations.
Acknowledgements
This work was largely carried out at the Johns
Hopkins University Summer Workshop and (par-
tially) supported by National Science Founda-
tion Grant Number IIS-0833652. Yue Zhang and
Stephen Clark are supported by the European
Union Seventh Framework Programme (FP7-ICT-
2009-4) under grant agreement no. 247762.
References
Bangalore, Srinivas and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240?
1246, Geneva, Switzerland.
1478
Briscoe, Ted and John Carroll. 2006. Evaluating
the accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the Poster
Session of COLING/ACL-06, pages 41?48, Sydney,
Australia.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine N-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173?180, Michigan, Ann Arbor.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132?139, Seattle, WA.
Clark, Stephen and James R. Curran. 2004. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proceedings of COLING-04, pages 282?
288, Geneva, Switzerland.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Curran, James R. and Stephen Clark. 2003. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 10th Meeting of the
EACL, pages 91?98, Budapest, Hungary.
Curran, James R. 2004. From Distributional to Se-
mantic Similarity. Ph.D. thesis, University of Edin-
burgh.
Djordjevic, Bojan, James R. Curran, and Stephen
Clark. 2007. Improving the efficiency of a wide-
coverage CCG parser. In Proceedings of IWPT-07,
pages 39?47, Prague, Czech Republic.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Jurafsky, Daniel and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, New
Jersey.
Kaplan, Ron, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of HLT-
NAACL?04, Boston, MA.
Kummerfeld, Jonathan K., Jessika Roesner, Tim
Dawborn, James Haggerty, James R. Curran, and
Stephen Clark. 2010. Faster parsing by supertag-
ger adaptation. In Proceedings of ACL-10, Uppsala,
Sweden.
Lin, Thomas, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
Proceedings of the 18th Conference on Information
and Knowledge Management (CIKM 2009), Hong
Kong.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of NAACL-06, pages 152?159, Brook-
lyn, NY.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic HPSG parsing. In Proceedings
of IWPT-05, pages 103?114, Vancouver, Canada.
Nivre, J. and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64?70, Geneva, Switzerland.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL conference, Rochester, NY.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP-96, pages 133?142, Somerset, New Jer-
sey.
Roark, Brian and Kristy Hollingshead. 2009. Lin-
ear complexity context-free parsing pipelines via
chart constraints. In Proceedings of HLT/NAACL-
09, pages 647?655, Boulder, Colorado.
Steedman, Mark. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
1479
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511?519,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Distributional Lexical Entailment by Topic Coherence
Laura Rimell
University of Cambridge
Computer Laboratory
laura.rimell@cl.cam.ac.uk
Abstract
Automatic detection of lexical entailment,
or hypernym detection, is an important
NLP task. Recent hypernym detection
measures have been based on the Distri-
butional Inclusion Hypothesis (DIH). This
paper assumes that the DIH sometimes
fails, and investigates other ways of quan-
tifying the relationship between the co-
occurrence contexts of two terms. We con-
sider the top features in a context vector
as a topic, and introduce a new entailment
detection measure based on Topic Coher-
ence (TC). Our measure successfully de-
tects hypernyms, and a TC-based family
of measures contributes to multi-way rela-
tion classification.
1 Introduction
Automatically detecting lexical entailment ? for
example, that lion entails animal or guitar entails
instrument, also known as hypernym detection ?
is an important linguistic task in its own right, and
is also a prerequisite for recognizing entailments
between longer text segments such as phrases or
sentences (Bos and Markert, 2005; Garrette et al.,
2011; Baroni et al., 2012; Beltagy et al., 2013).
Several recent techniques for hypernym de-
tection have made use of distributional seman-
tics (Weeds and Weir, 2003; Weeds et al., 2004;
Clarke, 2009; Kotlerman et al., 2010; Lenci and
Benotto, 2012). These techniques are based on the
Distributional Inclusion Hypothesis (Geffet and
Dagan, 2005), hereafter DIH, which proposes that
if term A entails term B (B is a hypernym of A),
then the contexts in which A occurs are a subset of
those in which B occurs. For example, all the con-
texts (co-occurrences) of lion ? which might in-
clude zoo, hunt, wild, food, etc. ? are also contexts
of animal. Existing measures look at the amount
of overlap between the co-occurrences of A and B,
in order to judge whether B is a hypernym of A.
The motivation for the present paper is the well-
known fact that the DIH is not fully correct. There
are many reasons why a hyponym might occur
in contexts where its hypernym does not. Some
contexts are collocational, e.g. lion king. Other
contexts are highly specific, e.g. mane applies
uniquely to lions, horses, and zebras; it would be
unusual to see text about animals with manes. The
need to be informative is also relevant: lion cub
will occur much more frequently than animal cub,
since animal is of the wrong level of generality to
pair with cub.
Moreover, the more general a hypernym be-
comes ? up to the level of WordNet root elements,
such as entity ? its predominant sense ceases to
correspond to the sense intended in hyponym-
hypernym chains. Thus we never hear about going
to visit an entity at the zoo.
This paper starts from the assumption that the
DIH sometimes fails, and investigates not the
amount of containment of A?s features in B?s fea-
tures, but rather the nature of the non-contained
features. We consider the top features of a dis-
tributional vector as a topic, and use recent mea-
sures for automatically measuring Topic Coher-
ence (Newman et al., 2010; Mimno et al., 2011)
to evaluate how the topics change under various
conditions. Using a notion of vector negation, we
investigate whether the distributional topic of e.g.
lion becomes more or less coherent when we sub-
tract the contexts of animal.
We introduce a new measure, Ratio of Change
in Topic Coherence (RCTC), for detecting lexical
entailment. The measure detects hypernyms with
reasonable accuracy, and a family of Topic Coher-
ence measures is used to perform amulti-way clas-
sification of tuples by relation class. Finally, we
investigate how the level of generality of a hyper-
nym affects entailment measures.
511
2 Related Work
Historically, manually developed resources such
as WordNet (Miller, 1995) have been used to sup-
ply lexical entailment information to NLP appli-
cations (Bos and Markert, 2005). More recently,
a number of techniques for detecting lexical en-
tailment have been developed using distributional
semantics (Weeds and Weir, 2003; Weeds et al.,
2004; Geffet and Dagan, 2005; Clarke, 2009;
Kotlerman et al., 2010; Lenci and Benotto, 2012).
These measures quantify to what extent the co-
occurrence features of a term A are included in
those of another term B, by a direct comparison
of the distributional vectors
~
A and
~
B. Kotlerman
et al. (2010) use the notion of Average Precision
from Information Retrieval to weight the relative
importance of the overlapping features. Lenci and
Benotto (2012) also check the extent to which B?s
features are not a subset of A?s, as a proxy for the
more general character of B. The success of these
feature inclusion measures has provided general
support for the DIH. Following Szpektor and Da-
gan (2008), inclusion measures are also sometimes
balanced with similarity measures such as LIN
similarity (Lin, 1998), to ensure that A and B are
semantically related, since unrelated pairs that dif-
fer in frequency can mimic feature inclusion.
Previous distributional approaches to hypernym
detection have generally involved a single mea-
sure, designed to rank hypernyms above other re-
lation classes. Evaluation has largely involved
either ranking or binary classification tasks, and
there has been little work on using a variety of
measures to distinguish multiple relation classes.
Lenci and Benotto (2012) perform a ranking task
using the multi-class BLESS dataset (Baroni and
Lenci, 2011), but not a classification. We perform
a multi-way classification using a variety of Topic
Coherence measures. Recent Semantic Relation
Classification shared tasks (SemEval-2010 Task 8,
SemEval-2012 Task 2) are also relevant, though
the relation classes and approaches have differed.
3 Topic Coherence for Distributional
Lexical Entailment
The intuition behind our approach is to investigate
whether term A, the candidate hyponym, has a co-
herent topic reflected in its distributional features,
which apply only to A and not to its hypernym B.
Consider A=beer, B=beverage. They may share
features such as drink, cold, and party. But if we
minimize or exclude B?s features and examine the
remaining features of A (we discuss how to do this
in Section 3.3), we might be left with more specific
features such as pint, lager, and brew.
If A and B share almost all contexts, we would
be left with a set of uninformative features, merely
corpus noise. If A and B share few contexts, there
would be little change to A?s topic when excluding
B?s features. Between the extremes, a range of
change in A?s topic is possible; we seek to quantify
this change and relate it to entailment.
To do this we need a way of treating a distri-
butional context vector as a topic. We treat the
N highest-weighted context features in
~
A as the
topic of A (topicA). If we represent the vector
~
A ? {f
c
i
,A
}
i
, where f
c
i
,A
is the weighted co-
occurrence value of context feature c
i
, then topicA
is a set {c
j
}, j ? 1...N , of the N highest-weighted
context features c
j
in
~
A.
3.1 Hypotheses
We consider two opposing hypotheses.
Hypothesis 1: Removing hypernym B?s fea-
tures from topicA will decrease the coherence of
topicA. If being a B is very important to being an
A, then the collection of remaining features may
become more random. Hypothesis 1 is consistent
with the DIH, since it implies that the important
features of A are also features of B.
As a corollary, removing A?s features from B
may not change the coherence of topicB very
much. Since A is just an instance of B, topicB
retains coherence (i.e. there?s a lot to being an an-
imal besides what?s involved in being a lion).
Hypothesis 2: Removing hypernym B?s fea-
tures from topicA will increase the coherence of
topicA. Perhaps A, by virtue of being more spe-
cific, occurs in a highly coherent set of contexts
where B does not. Hypothesis 2 is inconsistent
with the DIH, since it imples that a hyponym al-
ways has specific features which the hypernym
does not share.
As a corollary, removing hyponym A?s features
from hypernym B might decrease the coherence of
topicB, if removing specific features leaves only
more general, less informative features behind.
3.2 Topic Coherence Measure
We use a Topic Coherence (TC) measure from re-
cent work on automatic evalution of topics gener-
ated from corpora by latent variable models (New-
man et al., 2010; Mimno et al., 2011; Stevens et
512
al., 2012). TC measures are applied to the top N
words from a generated topic. They assign pair-
wise relatedness scores to the words, and return
the mean or median from the word-pair scores.
We adopt the best method from Newman et al.
(2010), equal to the median pairwise Pointwise
Mutual Information (PMI) of the top N words, us-
ing Wikipedia as a background corpus for PMI.
1
The measure is given in Equation (1):
TC({c
j
}) = median(PMI(c
i
, c
k
), i, k ? 1...N, i < k)
(1)
where {c
j
} is the topic, and PMI is defined as:
PMI(c
i
, c
k
) = log
p(c
i
, c
k
)
p(c
i
)p(c
k
)
(2)
We use intra-sentence co-occurrence in Wikipedia
for calculating PMI.
Note that our definition of a topic, namely the
top N features from a distributional vector, does
not correspond to a topic generated by a latent
variable model, because it does not have a prob-
ability distribution over words. However, the TC
measures we adopt do not make use of such a
probability distribution except for choosing the top
N words from a topic, which are then treated as an
unordered set for the pairwise operations. New-
man et al. (2010) uses N=10, and Mimno et al.
(2011) uses N=5...20; we investigate a range of N.
3.3 Vector Negation
For removing one topic from another, we draw on
the concept of vector negation (Widdows and Pe-
ters, 2003; Widdows, 2003). Vector negation has
proved useful for modeling word senses in Infor-
mation Retrieval. For example, one might want to
formulate a query for suitNOT lawsuit, which will
retrieve terms such as shirt and jacket and exclude
plaintiff and damages.
We test two versions of vector negation. The
first, Widdows (Widdows, 2003), represents A
NOT B as the projection of
~
A onto
~
B
?
, the sub-
space orthogonal to
~
B in the vector space V .
Specifically,
~
B
?
? {v ? V : v ?
~
B = 0}. The
formula for Widdows A NOT B is:
A NOT B ?
~
A?
~
A ?
~
B
|
~
B|
2
~
B (3)
The second, Strict negation, simply zeros out
any context features of A that are non-zero in B:
f
c
i
,AnotB
?
(
0 if f
c
i
,B
6= 0
f
i,A
if f
c
i
,B
= 0
(4)
1
In our case, Wikipedia is also the source corpus for our
context vectors.
This measure is harsher than Widdows negation,
which decreases the value of common features but
does not remove them completely.
3.4 Generality Measure
Herbelot and Ganesalingam (2013) experiment
with hypernym detection using a generality mea-
sure. They measure the Kullback-Leibler (KL) di-
vergence (Eq. 5) between the probability distribu-
tion over context words for a term A, and the back-
ground probability distribution. The idea is that
the greater the KL divergence, the more informa-
tive and therefore specific the term is, while hyper-
nyms are likely to be more general.
D
KL
(p(f
i
|A)||p(f
i
)) = ?
i
ln(
p(f
i
|A)
p(f
i
)
)p(f
i
) (5)
Herbelot and Ganesalingam (2013) found that
KL divergence on its own was not sufficient for
successful hypernym detection. We experiment
with it in combination with TC measures.
4 Methods
4.1 Context Vectors
We produced context vectors from a 2010
Wikipedia download, lemmatized using morpha
(Minnen et al., 2001). The 10Kmost frequent lem-
mas in the corpus, minus common stop words and
the 25 most frequent lemmas, served as the context
features. Feature co-occurrences were counted in
a 7-word window around the target lemma (three
words each side of the target lemma), and limited
to intra-sentence co-occurrences.
Co-occurrence counts were weighted using T-
test. We chose T-test because it does not over-
emphasize infrequent features; however, early ex-
periments with Positive PMI weighting showed
the overall performance of our measures to be sim-
ilar with both weighting schemes.
We benchmarked our context vectors on the
WS353 word similarity task (Finkelstein et al.,
2002) and found them to be of comparable accu-
racy with previous literature.
Rel Class Target Related Word Total
HYPER alligator animal 638
COORD alligator lizard 1,760
MERO alligator mouth 1,402
RAND-N alligator message 3,253
Table 1: Examples from the BLESS subset; num-
ber of tuples per relation in the development set.
513
Macroaverage Microaverage
Relation Class Relation Class
Coherence of HYPER MERO COORD RAND-N HYPER MERO COORD RAND-N
TopicA 5.14 ?1.63 5.16 ?1.66 5.13 ?1.63 5.16 ?1.66 5.14 ?1.59 5.37 ?1.56 5.22 ?1.63 5.28 ?1.62
TopicAnotB 3.82 ?1.27 3.86 ?1.02 3.49 ?0.94 5.07 ?1.50 3.88 ?1.73 4.07 ?1.42 3.58 ?1.51 5.17 ?1.64
TopicA-TopicAnotB 1.32 ?1.54 1.30 ?1.28 1.64 ?1.58 0.09 ?0.43 1.26 ?1.86 1.30 ?1.49 1.64 ?1.92 0.11 ?0.83
TopicB 4.97 ?0.58 4.51 ?0.52 5.02 ?0.73 4.49 ?0.24 5.01 ?1.15 4.53 ?1.44 5.07 ?1.63 4.50 ?1.30
TopicBnotA 4.36 ?0.55 3.92 ?0.53 3.33 ?0.67 4.45 ?0.27 4.37 ?1.15 3.89 ?1.32 3.35 ?1.61 4.46 ?1.41
TopicB-TopicBnotA 0.61 ?0.69 0.59 ?0.48 1.68 ?0.88 0.04 ?0.14 0.64 ?1.34 0.64 ?1.33 1.72 ?2.07 0.04 ?0.77
Table 2: Average Topic Coherence measures on the development set, using N=10, Strict negation.
4.2 Evaluation Dataset
We used a subset of the BLESS dataset (Baroni
and Lenci, 2011) as defined by Lenci and Benotto
(2012). The entire dataset consists of 200 con-
crete nouns in 17 broad noun classes (e.g. cloth-
ing, amphibian/reptile, vegetable, container), par-
ticipating in a variety of relations. The subset con-
tains the relation classes hypernym (HYPER), co-
ordinate (COORD, i.e. co-hyponym), meronym
(MERO, i.e. part-of), and random-noun (RAND-
N, an unrelated noun). It consists of 14,547 tuples
in total. Table 1 gives an example of each rela-
tion class, along with the total number of tuples
per class in the development data.
Since there was no pre-defined development-
test split for the BLESS subset, we randomly se-
lected half of the data for development. For each
of the 17 broad noun classes, we randomly chose
half of the target nouns, and included all their HY-
PER, COORD, MERO, and RAND-N tuples. This
resulted in a development set consisting of 96 tar-
get nouns and 7,053 tuples; and a test set consist-
ing of 104 nouns and 7,494 tuples.
5 Topic Coherence Behavior
We first investigate how topic coherence behaves
across the four relation classes. Table 2 shows
the average values and standard deviation of TC-
related measures on the development data. The
left-hand side gives macro-averages, where values
are first averaged per-class for each target word,
then averaged across the 96 target words in the de-
velopment set. The right-hand side gives micro-
averages across all tuples in the development set.
The micro- and macro-averages are similar, and
we report macro-averages from now on.
2
Row 1 of Table 2 shows the original coherence
of topicA, and row 2 the coherence of topicAnotB.
2
Lenci and Benotto (2012) also report macro-averages,
but our figures are not comparable to theirs, which are based
on a nearest-neighbor analysis.
Row 3 is simply the difference between the two,
showing the absolute change in coherence. Rows
4-6 are analogous. In general, coherence values
for A and B ranged from the 3?s to the 6?s, with
very high coherence of 7 or 8 and very low coher-
ence of 1 or 2. We did not normalize TC values.
Comparing rows 1 and 4, we see that the B top-
ics are slightly less coherent than the A topics,
probably due to the makeup of the dataset (B terms
include hypernyms and random words, while A
terms are concrete nouns).
Column 1 shows that removing hypernym B
from A results in a decrease in coherence, from
5.14 to 3.82. The difference in coherence, 1.32
in this case, is shown in row 3. Removing A
from B also results in a coherence decrease, but
a much smaller one: only a 0.61 average absolute
decrease. Because the starting coherence values of
A and B may be different, we focus on the amount
of change in coherence when we perform the nega-
tion (rows 3 and 6), rather than the absolute coher-
ence of the negated vectors (rows 2 and 5).
Interestingly, column 2 shows that the be-
haviour of meronyms is almost identical to hyper-
nyms. This is surprising for two reasons: first,
meronyms are intuitively more specific than their
holonyms; and second, previous studies tended to
conflate hypernyms with coordinates rather than
meronyms (Lenci and Benotto, 2012).
Column 3, rows 3 and 6, show that coor-
dinates behave differently from hypernyms and
meronyms. Vector negation in both directions
results in a similar loss of coherence (1.64 and
1.68), reflecting the fact that coordinates have a
symmetrical relationship. The average change is
also greater, although there is a wide variance. In
column 4, the coherence differences for random
nouns are again symmetrical, but in this case very
small, since a randomly selected noun will not
share many contexts with the target word.
We can also define a TC-based similarity mea-
514
Relation Class
Measure HYPER MERO COORD RAND-N
TC Meet 5.36 5.12 5.98 3.62
LIN 0.41 0.41 0.48 0.22
GenKLA 4.89 4.89 4.89 4.89
GenKLB 4.60 4.49 5.01 4.95
DiffGenKL 0.29 0.40 -0.12 -0.05
Table 3: Average similarity and generality mea-
sures on the dev. set, using N=10, Strict negation.
sure. We define
~
A MEET
~
B as the intersec-
tion of two vectors, where each feature value
f
c
i
,A MEET B
? min(f
c
i
,A
, f
c
i
,B
). Table 3 shows
TC(A MEET B), with LIN similarity (Lin, 1998)
between A and B for comparison. We expect that
if A and B are similar, their common features
will form a coherent topic. Indeed hypernyms
and meronyms have high values, with coordinates
slightly higher and random nouns much lower.
Table 3 also shows the KL divergence-based
generality measure from Section 3.4. Term B is
slightly more general (lower score) than termA for
hypernyms and meronyms. This may suggest that
meronyms are more general distributionally than
their holonyms, e.g. leg is a holonym of alligator,
but also associated with many other animals.
Table 4 shows the topics for owl and its hyper-
nym creature. Using Strict negation to create owl
NOT creature causes a number of contexts to be
removed from owl: sized, owl, burrow, hawk, typ-
ical, medium, eagle, large, nest. Instead, more
idiosyncratic contexts rise to the top, including
northern, mexican, grouping, and bar (as in an
owl?s markings). These idiosyncratic contexts are
not mutually informative and cause a sizeable de-
crease in TC.
On the other hand, removing owl from crea-
ture does not decrease the coherence nearly as
much. The contexts that are promoted ? fantas-
tic, bizarre, fairy ? are mutually consistent with
the other creature contexts.
So far our results support Hypothesis 1: remov-
ing B from A decreases its coherence. However,
we hypothesize that this may not be the case for
hypernyms at all levels of generality. Consider-
ing the pair owl-chordate, there is no change from
topicA to topicAnotB. But chordate loses a size-
able amount of coherence when owl is removed;
the topic changes from primitive, ancestral, ances-
tor, evolution, lineage, basal, earliest, fossil, non-,
neural (TC 6.62), to earliest, non-, neural, affinity,
probable, genome, suspected, universally, group,
approximation (TC 3.60).
6 Hypernym Detection Measures
Since we use the same dataset as Lenci and
Benotto (2012), we report the invCL measure in-
troduced in that paper, which outperformed the
other measures reported there, including those of
Weeds and Weir (2003), Weeds et al. (2004), and
Clarke (2009). Let f
A
be the weight of feature f in
~
A, and let F
A
be the set of features with non-zero
weights in
~
A. Then we have:
CL(A,B) =
?
f?F
A
?F
b
min(f
A
, f
B
)
?
f?F
A
f
A
(6)
invCL(A,B) =
p
CL(A,B) ? (1? CL(B,A)) (7)
We also report the balAPinc measure of Kotler-
man et al. (2010), which is not included in the
Lenci and Benotto (2012) evaluation. This mea-
sure begins with APinc, in which the features of A
are ranked by weight, highest to lowest:
APinc(A,B) =
?
r?1...|F
A
|
P (r) ? rel(f
r
)
|F
A
|
(8)
where P (r) is the ?precision? at rank r, that is,
how many of B?s features are included at rank r
in the features of A; and rel(f
r
) is a relevance
feature reflecting how important f
r
is in B (see
Kotlerman et al. (2010) for details). The balanced
version balAPinc is:
balAPinc(A,B) =
p
LIN(A,B) ?APinc(A,B) (9)
owl (5.19) owl not creature (3.25) creature (5.91) creature not owl (5.09) owl meet creature (4.14)
barn barn mythical mythical small
sized grey -like supernatural large
owl northern strange alien burrow
burrow mexican supernatural legendary night
hawk falcon magical fantastic elf
typical creek alien bizarre little
medium mountains evil aquatic giant
eagle grouping legendary dangerous prey
large bar giant vicious hunt
nest california resemble fairy purple
Table 4: Topics from the development data with Topic Coherence values.
515
Widdows
N = 5 10 15 20
HYPER 1.00 1.00 1.00 1.00
MERO 0.99 1.00 1.00 1.00
COORD 1.02 1.00 1.00 1.01
RAND-N 1.00 1.00 1.00 1.00
Strict
N = 5 10 15 20
HYPER 1.64 1.42 1.23 1.19
MERO 1.91 1.23 1.24 1.20
COORD 1.36 1.15 1.10 1.16
RAND-N 1.08 1.03 1.03 1.02
Table 5: RCTC with varying N and neg type.
We introduce a new measure, Ratio of Change
in Topic Coherence (RCTC). Based on Section 5,
we expect that for hypernyms the change in coher-
ence from A to AnotB is greater than the change
from B to BnotA. However, we cannot simply use
the ratio (A-AnotB)/(B-BnotA), because the very
small changes in the RAND-N class result in very
small denominators and unstable values. Instead,
we consider two ratios: the magnitude of TC(A)
compared to TC(AnotB), and the magnitude of
TC(B) compared to TC(BnotA). We take the ra-
tio of these figures:
RCTC(A,B) =
TC(topicA)
TC(topicAnotB)
TC(topicB)
TC(topicBnotA)
(10)
If topicA is much more coherent than AnotB,
the numerator will be relatively large. If topicB
is not much more coherent than topicBnotA, the
denominator will be relatively small. Both of these
factors encourage RCTC to be larger.
3
We also balanced RCTC with three different
factors: LIN similarity, a generality ratio, and
TC(MeetAB). In each case we calculated the bal-
anced value as
?
RCTC ? factor.
7 Experiments and Discussion
We first look at the effect of N (topic size) and
negation type on RCTC on the development data
(Table 5). It is clear that RCTC distinguishes rela-
tion types using Strict but not Widdows negation.
We believe this is because, as the ?harsher? ver-
sion of negation, it allows less-related features to
rise to the top of the topic and reveal greater dif-
ferences in topic coherence. N=10 was the only
3
Although TC values are PMI values, which can be neg-
ative, in practice the median pairwise PMI is almost never
negative, because there tend to be more positive than nega-
tive values among the pairwise comparisons. Therefore, we
have not accounted for sign in the ratio. We have handled
as special cases the few instances where TC(topicAnotB) or
TC(topicBnotA) takes the value of ?infinity due to zero co-
occurrences between many of the features.
invCL bal RCTC RCTC RCTC RCTC
APinc bal bal bal
LIN GEN MEET
HYPER 0.41 0.23 1.37 0.72 1.09 2.62
MERO 0.39 0.22 1.28 0.70 1.06 2.51
COORD 0.38 0.22 1.44 0.71 1.05 2.50
RAND-N 0.25 0.10 1.03 0.46 1.01 1.92
Table 6: Hypernym identification on full dataset:
average value by relation.
value that ranked hypernyms the highest; we use
N=10 for the remaining experiments.
We then proceed to hypernym identification on
the full dataset (Table 6). All measures we tested
assigned the highest average value to hypernyms
(in bold) compared to the other relations.
7.1 Ranking Task
Lenci and Benotto (2012) introduced a ranking
task for hypernym detection on the BLESS data,
which we replicate here. In this task a measure is
used to rank all tuples from the data. The accuracy
of the ranking is assessed from the point of view
of each relation class. The goal is for hypernyms
to have the highest accuracy of all the classes.
We report the Information Retrieval (IR) mea-
sure Mean Average Precision (MAP) for each
class, following Lenci and Benotto (2012). We
also report Mean R-Precision (RPrec), equal to the
precision at rank R where R is the number of ele-
ments in the class. None of the measures we eval-
uated achieves the highest result for hypernyms
4
,
though invCL consistently performs better for hy-
pernyms than do the other measures (Table 7).
Both MAP and RPrec give more weight to cor-
rect rankings near the top of the list, as is suit-
able for IR applications. In the context of hyper-
nym detection, they could test a system?s ability to
find one or two good-quality hypernyms quickly
from a set of candidates. However, these measures
are less appropriate for testing whether a system
can, in general, rank hypernyms over other rela-
tions. Therefore, we also report Mean Area Un-
der the ROC Curve, or Wilcoxon-Mann-Whitney
statistic (AUC), which gives equal weight to cor-
rect rankings at the top and bottom of the list, and
also compensates for unbalanced data. Table 7
shows that RCTCbalMEET performs identically
to invCL on the AUC measure. This comparison
suggests that invCL is better at placing hypernyms
4
Lenci and Benotto (2012) report a different result, possi-
bly due to the use of different context vectors.
516
invCL balAPinc RCTC RCTC RCTC RCTC
balLIN balGEN balMEET
RPrec
Hyper 0.30 0.25 0.17 0.20 0.12 0.19
Mero 0.32 0.29 0.30 0.31 0.21 0.32
Coord 0.39 0.43 0.27 0.42 0.27 0.40
Rand-N 0.18 0.19 0.38 0.16 0.42 0.18
AUC
Hyper 0.18 0.17 0.16 0.17 0.14 0.18
Mero 0.31 0.31 0.27 0.31 0.24 0.31
Coord 0.38 0.39 0.25 0.39 0.28 0.37
Rand-N 0.13 0.13 0.32 0.12 0.34 0.15
MAP
Hyper 0.35 0.30 0.22 0.24 0.17 0.24
Mero 0.37 0.35 0.35 0.36 0.27 0.37
Coord 0.41 0.46 0.30 0.45 0.32 0.43
Rand-N 0.32 0.32 0.43 0.31 0.46 0.33
Table 7: Ranking results. Bold indicates best result for hypernyms by evaluation measure.
at the top of the ranking, but over the whole dataset
the two measures rank hypernyms above other tu-
ples equally.
7.2 Classification Task
We performed a four-way classification of tuples
by relation class. We used LIBSVM (Chang and
Lin, 2011). As described in Section 4.2, the
BLESS data is unbalanced, with hypernyms ? our
target class ? making up only about 9% of the
data. To address this imbalance, we used LIB-
SVM?s option to increase the cost associated with
the smaller classes during parameter tuning and
training. We based the weights on the develop-
ment data only (HYPER: 9% of the data, weight
factor 10; MERO: 20% of the data, weight factor
5; COORD: 25% of the data, weight factor 4).
We used LIBSVM?s default Radial Basis Func-
tion kernel. On the development data we per-
formed 10-fold cross-validation. We used LIB-
SVM?s grid.py utility for tuning the parameters C
and ? separately for each fold. We also tuned and
trained models on the development data and tested
them on the test data.
We used four sets of features (Table 8): (1)
invCL on its own; (2) TC features; (3) all features
(invCL, TC, plus additional similarity and gener-
ality measures); and (4) all except TC features.
The results of classification on the development
data are shown in Table 9, and on the test data in
Table 10. Although we report overall accuracy,
this is a poor measure of classificaton quality for
unbalanced data. The tables therefore provide the
Precision, Recall, and F-score by relation class.
The overall accuracy is respectable, although
it can be seen that the hypernym class was the
most difficult to predict, despite weighting the cost
function. Hypernyms may be particularly difficult
Feature Description
invCL Lenci?s invCL(A,B) (Eq. 7)
topicA TC(A)
topicAnotB TC(B)
diffTopicA TC(A)? TC(A NOT B)
ratioTopicsA TC(A NOT B)/TC(A)
topicB TC(B)
topicBnotA TC(B NOT A)
diffTopicB TC(B)? TC(B NOT A)
ratioTopicsB TC(B NOT A)/TC(B)
topicMeetAB TC(A MEET B)
ratioTopics1 TC(A NOT B)/TC(B NOT A)
ratioTopics2 diffTopicA / diffTopicB
DiffTopics1 diffTopicA - diffTopicB
DiffTopics2 diffTopicA + diffTopicB
RCTC RCTC(A,B) (Eq. 10)
RCTCbalMEET RCTCbalMEET(A,B)
APinc Kotlerman?s APinc(A,B) (Eq. 8)
balAPinc Kotlerman?s balAPinc(A,B) (Eq. 9)
LIN LIN similarity
genKLA D
KL
(p(f
i
|A)||p(f
i
)) (Eq. 5)
genKLB D
KL
(p(f
i
|B)||p(f
i
)) (Eq. 5)
diffGenKL genKLA - genKLB
ratioGenKL genKLA / genKLB
RCTCbalLIN RCTCbalLIN(A,B)
RCTCbalGEN RCTCbalGEN(A,B)
RCTCbalInvCL RCTC(A,B) bal. with invCL(A,B)
Table 8: Features used in classification experi-
ment. InvCL; TC features; additional features.
to isolate given their similarity to meronyms and
intermediate status between coordinates and ran-
dom nouns on some of the features.
Importantly, while previous work has focused
on single measures such as invCL, the classifica-
tion task highlights a key aspect of the TC ap-
proach. Because we can measure the TC of sev-
eral different vectors for any given tuple (original
terms, negated topics, intersection, etc.) we can
perform multi-way classification much more accu-
rately than with the invCL measure alone. More-
over, the TC features make an important contribu-
tion to the multi-way classification over and above
invCL and other previous similarity and generality
517
Feature Set Acc Class P R F
invCL 39.2
Hyper 29.2 19.6 22.5
Mero 25.5 51.7 34.0
Coord 19.3 26.4 21.3
Rand-N 73.5 44.9 55.6
TC Feats 56.7
Hyper 20.3 41.4 27.1
Mero 36.5 48.4 41.4
Coord 66.5 54.5 59.5
Rand-N 87.1 64.7 74.2
All except TC 59.2
Hyper 28.7 19.7 22.9
Mero 35.1 56.2 43.2
Coord 58.2 54.5 56.2
Rand-N 85.5 71.0 77.5
All 64.0
Hyper 30.5 24.4 26.7
Mero 44.9 44.6 44.6
Coord 60.3 65.6 62.8
Rand-N 80.0 79.6 79.7
Table 9: Classification results on development
data using 10-fold cross-validation.
Feature Set Acc Class P R F
invCL 42.2
Hyper 31.1 19.3 23.8
Mero 32.6 54.3 40.7
Coord 23.1 29.3 25.8
Rand-N 75.8 48.2 59.0
TC Feats 56.2
Hyper 20.0 45.1 27.7
Mero 36.7 42.9 40.0
Coord 64.2 56.5 60.1
Rand-N 88.6 64.5 74.6
All except TC 60.6
Hyper 23.9 17.9 20.5
Mero 38.1 56.4 45.5
Coord 58.2 56.1 57.1
Rand-N 86.5 73.8 79.6
All 63.1
Hyper 33.9 28.6 31.0
Mero 44.1 36.9 40.2
Coord 57.2 64.3 60.6
Rand-N 78.2 81.5 79.8
Table 10: Classification results on test data using
development data as training.
measures, with the set of all features yielding the
highest overall accuracy.
Another interesting result is that classification
with the TC features alone results in much higher
recall (though lower precision) for hypernyms
than any of the other feature sets, and on the de-
velopment data (Table 9) results in the highest F-
score for hypernyms.
8 Hypernym Depth
We performed a simple preliminary experiment to
test the speculation that the interaction between
topics depends on the level of generality of the hy-
pernym. Using the WordNet::Similarity package
(Pedersen et al., 2004), we divided the develop-
ment data into bins according to the depth of the
hypernym from the WordNet root node. Table 11
shows average values by hypernym depth.
D Qty diffA diffB RCTC invCL balAPinc
1 1 0.66 0.27 1.08 0.15 0.01
3 35 0.33 0.16 1.12 0.44 0.23
5 108 0.32 -0.65 1.32 0.33 0.16
6 41 1.21 0.24 1.50 0.44 0.21
7 160 1.45 0.64 1.34 0.44 0.27
8 136 1.30 0.90 1.25 0.35 0.19
9 71 1.37 1.09 1.26 0.41 0.23
10 51 1.90 2.10 2.08 0.41 0.24
11 15 1.85 1.50 1.23 0.48 0.31
12 13 2.08 1.45 1.24 0.28 0.17
13 3 2.49 0.97 1.67 0.27 0.12
14 4 2.02 1.97 1.05 0.27 0.09
Table 11: Average value by depth D of hypernym.
There is a striking result for diffA, i.e.
TC(topicA) - TC(topicAnotB): the deeper the hy-
pernym in the WordNet hierarchy, the greater the
value. This suggests that more abstract hypernyms
have less interaction with their hyponyms? topics.
A similar, though less pronounced, effect is seen
for diffB. However, the three measures RCTC,
invCL, and balAPinc remain relatively stable as
the hypernym depth changes. While this is some-
what reassuring, these averages clearly have not
yet captured the difficulty which the DIH encoun-
ters in individual cases such as owl?chordate.
9 Conclusions
We have introduced a set of Topic Coherence mea-
sures, particularly the Ratio of Change in Topic
Coherence, to identify hypernyms. These mea-
sures perform comparably to previous hypernym
detection measures on many tasks, while provid-
ing a different view of the relationship between the
distributional vectors of two terms, and contribut-
ing to a more accurate multi-way relation classifi-
cation, especially higher recall for hypernyms.
The approach presented here provides a start-
ing point for entailment measures that do not rely
solely on the Distributional Inclusion Hypothesis.
One issue with the current proposal is that it tests
for a single coherent distributional topic, whereas
multiple senses may be represented in a word?s top
context features. Future work will integrate Word
Sense Disambiguation methods into the Topic Co-
herence based lexical entailment approach.
Acknowledgments
This work is supported by EPSRC grant
EP/I037512/1. We gratefully acknowledge
helpful discussion from Stephen Clark, Tamara
Polajnar, Julie Weeds, Jeremy Reffin, David Weir,
and the anonymous reviewers.
518
References
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP workshop on GEMS:
GEometrical Models of natural language Semantics,
pages 1?10, Edinburgh.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of EACL, pages 23?32.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. In Proceedings of *SEM,
pages 11?21, Atlanta, Georgia.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of HLT-EMNLP, pages 628?635, Vancouver.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the EACL workshop on GEMS: GEometrical Mod-
els of natural language Semantics, pages 112?119,
Athens.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116?131.
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov Logic. In Pro-
ceedings of IWCS, Oxford, UK.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL, Michigan.
Aur?elie Herbelot and Mohan Ganesalingam. 2013.
Measuring semantic content in distributional vec-
tors. In Proceedings of ACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16:359?389.
Alessandro Lenci and Giuli Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of *SEM, pages 75?79, Montreal.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of ICML, Madi-
son, Wisconson.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP, pages 262?272, Edinburgh.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Proceedings of NAACL, pages 100?
108, Los Angeles, California.
Ted Pedersen, Siddarth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the
relatedness of concepts. In Proceedings of NAACL
(Demonstration System), pages 38?41, Boston, MA.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Butler. 2012. Exploring topic coher-
ence over many models and many topics. In Pro-
ceedings of EMNLP, pages 952?961, Jeju Island,
Korea.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING, Manchester, UK.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP, pages 81?88, Sapporo, Japan.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021, Geneva.
Dominic Widdows and Stanley Peters. 2003. Word
vectors and quantum logic. In Proceedings of
the Eight Mathematics of Language Conference,
Bloomington, Indiana.
Dominic Widdows. 2003. Orthogonal negation in vec-
tor spaces for modelling word-meanings and docu-
ment retrieval. In Proceedings of ACL, pages 136?
143, Sapporo, Japan.
519
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 268?271,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
Cambridge: Parser Evaluation using Textual Entailment by
Grammatical Relation Comparison
Laura Rimell and Stephen Clark
University of Cambridge
Computer Laboratory
{laura.rimell,stephen.clark}@cl.cam.ac.uk
Abstract
This paper describes the Cambridge sub-
mission to the SemEval-2010 Parser Eval-
uation using Textual Entailment (PETE)
task. We used a simple definition of en-
tailment, parsing both T and H with the
C&C parser and checking whether the core
grammatical relations (subject and object)
produced for H were a subset of those for
T. This simple system achieved the top
score for the task out of those systems sub-
mitted. We analyze the errors made by the
system and the potential role of the task in
parser evaluation.
1 Introduction
SemEval-2010 Task 12, Parser Evaluation using
Textual Entailment (PETE) (Yuret et al, 2010),
was designed as a new, formalism-independent
type of parser evaluation scheme. The task is
broadly Recognizing Textual Entailment (RTE),
but unlike typical RTE tasks, its intention is to fo-
cus on purely syntactic entailments, assuming no
background knowledge or reasoning ability. For
example, given a text (T) The man with the hat
was tired., the hypothesis (H) The man was tired.
is entailed, but The hat was tired. is not. A cor-
rect decision on whether H is entailed can be used
as a diagnostic for the parser?s analysis of (some
aspect of) T. By requiring only a binary decision
on the entailment, instead of a full syntactic anal-
ysis, a parser can be evaluated while its underlying
formalism remains a ?black box?.
Our system had two components: a parser, and
an entailment system which decided whether T en-
tails H based on the parser?s output. We distin-
guish two types of evaluation. Task evaluation,
i.e. the official task scoring, indicates whether the
entailment decisions ? made by the parser and en-
tailment system together ? tally with the gold stan-
dard dataset. Entailment system evaluation, on the
other hand, indicates whether the entailment sys-
tem is an appropriate parser evaluation tool. In
the PETE task the parser is not evaluated directly
on the dataset, since the entailment system acts
as intermediary. Therefore, for PETE to be a vi-
able parser evaluation scheme, each parser must
be coupled with an entailment system which accu-
rately reflects the parser?s analysis of the data.
2 System
We used the C&C parser (Clark and Curran,
2007), which can produce output in the form of
grammatical relations (GRs), i.e. labelled head-
dependencies. For example, (nsubj tired
man) for the example in Section 1 represents the
fact that the NP headed by man is the subject of
the predicate headed by tired. We chose to use the
Stanford Dependency GR scheme (de Marneffe et
al., 2006), but the same approach should work for
other schemes (and other parsers producing GRs).
Our entailment system was very simple, and
based on the assumption that H is a simplified ver-
sion of T (true for this task though not for RTE in
general). We parsed both T and H with the C&C
parser. Let grs(S) be the GRs the parser produces
for a sentence S. In principle, if grs(H) ? grs(T),
then we would consider H an entailment. In prac-
tice, a few refinements to this rule are necessary.
We identified three exceptional cases. First,
syntactic transformations between T and H may
change GR labels. The most common transforma-
tion in this dataset was passivization, meaning that
a direct object in T could be a passive subject in H.
Second, H could contain tokens not present in T.
Auxiliary verbs were introduced by passivization.
Pronouns such as somebody and something were
introduced into some H sentences to indicate an
NP or other phrase not targeted for evaluation. De-
terminers were sometimes introduced or changed,
e.g. prices to the prices. Expletive subjects were
also sometimes introduced.
268
Third, the parses of T and H might be incon-
sistent in an incidental way. Consider the pair I
reached into that funny little pocket that is high up
on my dress. ? The pocket is high up on some-
thing. The intended focus of the evaluation (as in-
dicated by the content word pair supplied as a sup-
plement to the gold standard development data)
is (pocket, high). As long as the parser analyzes
pocket as the subject of high, we want to avoid
penalizing it for, say, treating the PP up on X dif-
ferently in T and H.
To address these issues we used a small set of
heuristics. First, we ignored any GR in grs(H) con-
taining a token not in T. This addressed the pas-
sive auxiliaries, pronouns, determiners, and exple-
tive subjects. Second, we equated passive subjects
with direct objects. Similar rules could be defined
for other transformations, but we implemented
only this one based on the prevalence of passiviza-
tion in the development data. Third, when check-
ing whether grs(H) ? grs(T), we considered only
the core relations subject and object. The intention
was that incidental differences between the parses
of T and H would not be counted as errors. We
chose these GR types based on the nature of the en-
tailments in the development data, but the system
could easily be reconfigured to focus on other rela-
tion types. Finally, we required grs(H) ? grs(T) to
be non-empty (no vacuous positives), but did not
restrict this criterion to subjects and objects.
We used a PTB tokenizer
1
for consistency with
the parser?s training data. We used the morpha
lemmatizer (Minnen et al, 2000), which is built
into the C&C tools, to match tokens across T and
H; and we converted all tokens to lowercase. If the
parser failed to find a spanning analysis for either
T or H, the entailment decision was NO. The full
pipeline is shown in Figure 1.
3 Results
A total of 19 systems were submitted. The base-
line score for ?always YES? was 51.8% accuracy.
Our system achieved 72.4% accuracy, which was
the highest score among the submitted systems.
Table 1 shows the results for our system, as well
as SCHWA (University of Sydney), also based on
the C&C parser and the next-highest scorer (see
Section 6 for a comparison), and the median and
lowest scores. The parser found an analysis for
1
http://www.cis.upenn.edu/
?
treebank/
tokenizer.sed.
Tokenize T and H with PTB tokenizer
?
Parse T and H with C&C parser
?
Lowercase and lemmatize all tokens
?
Discard any GR in grs(H) containing a token not in T
?
YES if core(H) ? core(T) and grs(H) ? grs(T) 6= ?,
NO otherwise
Figure 1: Full pipeline for parser and entailment
system. core(S): the set of core (subject and ob-
ject) GRs in grs(S).
99.0% of T sentences and 99.7% of H sentences
in the test data.
4 Error Analysis
Table 2 shows the results for our system on the de-
velopment data (66 sentences). The parser found
an analysis for 100% of sentences and the overall
accuracy was 66.7%. In the majority of cases the
parser and entailment system worked together to
find the correct answer as expected. For example,
for Trading in AMR shares was suspended shortly
after 3 p.m. EDT Friday and didn?t resume. ?
Trading didn?t resume., the parser produced three
GRs for H (tokens are shown lemmatized and low-
ercase): (nsubj resume trading), (neg
do n?t), and (aux resume do). All of
these were also in grs(T), and the correct YES
decision was made. For Moreland sat brood-
ing for a full minute, during which I made each
of us a new drink. ? Minute is made., the
parser produced two GRs for H. One, (auxpass
make be), was ignored because the passive
auxiliary be is not in T. The second, pas-
sive subject GR(nsubjpass make minute)
was equated with a direct object (dobj make
minute). This GR was not in grs(T), so the cor-
rect NO decision was made.
In some cases a correct YES answer was
reached via arguably insufficient positive evi-
dence. For He would wake up in the middle of
the night and fret about it. ? He would wake up.,
the parser produces incorrect analyses for the VP
would wake up for both T and H. However, these
GRs are ignored since they are non-core (not sub-
ject or object), and a YES decision is based on
the single GR match (nsubj would he). This
269
Score on YES entailments Score on NO entailments Overall
System correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)
Cambridge 98 58 62.8 120 25 82.8 72.4
SCHWA 125 31 80.1 87 58 60.0 70.4
Median 71 85 45.5 88 57 60.7 52.8
Low 68 88 43.6 76 69 52.4 47.8
Table 1: Results on the test data.
Score on YES entailments Score on NO entailments Overall
System correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)
Cambridge 22 16 57.9 22 6 78.6 66.7
Table 2: Results on the development data.
Type FN FP Total
Unbounded dependency 8 1 9
Other parser error 6 2 8
Entailment system 1 3 4
Difficult entailment 1 0 1
Total 16 6 22
Table 3: Error breakdown on the development
data. FN: false negative, FP: false positive.
is not entirely a lucky guess, since the entailment
system has correctly ignored the odd analyses of
would wake up and focused on the role of he as the
subject of the sentence. However, especially since
the target content word pair was (he, wake), more
positive evidence would be desirable. Of the 22
correct YES decisions, only two were truly lucky
guesses in that the single match was a determiner;
others had at least one core match.
Table 3 shows the breakdown of errors. The
largest category was false negatives due to un-
bounded dependencies not recovered by the
parser, for example It required an energy he no
longer possessed to be satirical about his father.
? Somebody no longer possessed the energy..
Here the parser fails to recover the direct object re-
lation between possess and energy in T. It is known
that parsers have difficulty with unbounded depen-
dencies (Rimell et al, 2009, from which the un-
bounded examples in this dataset were obtained),
so this result is not surprising.
The next category was other parser errors. This
is a miscellaneous category including e.g. errors
on coordination, parenthetical elements, identify-
ing the head of a clausal subject, and one due to
the POS tagger. For example, for Then at least he
would have a place to hang his tools and some-
thing to work on. ? He would have something to
work on., the parser incorrectly coordinated tools
and something for T. As a result (dobj have
something) was in grs(H) but not grs(T), yield-
ing an incorrect NO.
Four errors were due to the entailment system
rather than the parser; these will be dicsussed in
Section 5. We also identified one sentence where
the gold standard entailment appears to rely on
extra-syntactic information, or at least informa-
tion that is difficult for a parser to recover. This
is Index-arbitrage trading is ?something we want
to watch closely,? an official at London?s Stock Ex-
change said. ?We want to watch index-arbitrage
trading. Recovering the entailment would require
resolving the reference of something, arguably the
role of a semantic rather than syntactic module.
5 Entailment System Evaluation
We now consider whether our entailment system
was an appropriate tool for evaluating the C&C
parser on the PETE dataset. It is easy to imag-
ine a poor entailment system that makes incorrect
guesses in spite of good parser output, or con-
versely one that uses additional reasoning to sup-
plement the parser?s analysis. To be an appropri-
ate parser evaluation tool, the entailment system
must decide whether the information in H is also
contained in the parse of T, without ?introducing?
or ?correcting? any errors.
Assuming our GR-based approach is valid, then
given gold-standard GRs for T and H, we expect an
appropriate entailment system to result in 100%
accuracy on the task evaluation. To perform this
oracle experiment we annotated the development
270
data with gold-standard GRs. Using our entailment
system with the gold GRs we achieved 90.9% task
accuracy. Six incorrect entailment decisions were
made, of which one was on the arguably extra-
syntactic entailment discussed in Section 4.
Three errors were due to transformations be-
tween T and H which changed the GR label or
head. For example, consider Occasionally, the
children find steamed, whole-wheat grains for ce-
real which they call ?buckshot?. ? Grains are
steamed.. In T, steamed is a prenominal adjective,
with grains as its head; while in H, it is a passive,
with grains as its subject. The entailment system
did not account for this transformation, although
in principle it could have. The other two errors
occurred because GRs involving a non-core rela-
tion or a pronoun introduced in H, both of which
our system ignored, were crucial for the correct
entailment decision.
Table 3 shows that with automatically-
generated GRs, four errors on the task evaluation
were attributable to the entailment system. Three
of these were also found in the oracle experiment.
The fourth resulted from a POS change between T
and H for There was the revolution in Tibet which
we pretended did not exist. ? The pretended did
not exist.. The crucial GR was (nsubj exist
pretended) in grs(H), but the entailment
system ignored it because the lemmatizer did
not give pretend as the lemma for pretended as a
noun. This type of error might be prevented by
answering NO if the POS of any word changes
between T and H, but the implementation is
non-trivial since word indices may also change.
There were eight POS changes in the development
data, most of which did not result in errors. We
also observed two cases where the entailment
system ?corrected? parser errors, yielding a
correct entailment decision despite the parser?s
incorrect analysis of T. When compared with a
manual analysis of whether T entailed H based
on automatically-generated GRs, the entailment
system achieved 89.4% overall accuracy.
6 Conclusion
We achieved a successful result on the PETE task
using a state-of-the-art parser and a simple entail-
ment system, which tested syntactic entailments
by comparing the GRs produced by the parser for
T and H. We also showed that our entailment sys-
tem had accuracy of approximately 90% as a tool
for evaluating the C&C parser (or potentially any
parser producing GR-style output) on the PETE
development data. This latter result is perhaps
even more important than the task score since it
suggests that PETE is worth pursuing as a viable
approach to parser evaluation.
The second-highest scoring system, SCHWA
(University of Sydney), was also based on the
C&C parser and used a similar approach (though
using CCG dependency output rather than GRs).
It achieved almost identical task accuracy to the
Cambridge system, but interestingly with higher
accuracy on YES entailments, while our system
was more accurate on NO entailments (Table 1).
We attribute this difference to the decision crite-
ria: both systems required at least one matching
relation between T and H for a YES answer; but
we additionally answered NO if any core GR in
grs(H) was not in grs(T). This difference shows
that a GR-based entailment system can be tuned to
favour precision or recall.
Finally, we note that although this was a sim-
ple entailment system with some dataset-specific
characteristics ? such as a focus on subject and
object relations rather than, say, PP-attachment ?
these aspects should be amenable to customization
or generalization for other related tasks.
Acknowledgments
The authors were supported by EPSRC grant
EP/E035698/1. We thank Matthew Honnibal for
his help in producing the gold-standard GRs.
References
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, Genoa, Italy.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of INLG, Mitzpe Ramon, Israel.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proceedings of EMNLP, Singapore.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of SemEval-2010,
Uppsala, Sweden.
271
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 85?89, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UCAM-CORE: Incorporating structured distributional similarity into STS
Tamara Polajnar Laura Rimell Douwe Kiela
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
{tamara.polajnar,laura.rimell,douwe.kiela}@cl.cam.ac.uk
Abstract
This paper describes methods that were sub-
mitted as part of the *SEM shared task on
Semantic Textual Similarity. Multiple kernels
provide different views of syntactic structure,
from both tree and dependency parses. The
kernels are then combined with simple lex-
ical features using Gaussian process regres-
sion, which is trained on different subsets of
training data for each run. We found that the
simplest combination has the highest consis-
tency across the different data sets, while in-
troduction of more training data and models
requires training and test data with matching
qualities.
1 Introduction
The Semantic Textual Similarity (STS) shared task
consists of several data sets of paired passages of
text. The aim is to predict the similarity that hu-
man annotators have assigned to these aligned pairs.
Text length and grammatical quality vary between
the data sets, so our submissions to the task aimed to
investigate whether models that incorporate syntac-
tic structure in similarity calculation can be consis-
tently applied to diverse and noisy data.
We model the problem as a combination of ker-
nels (Shawe-Taylor and Cristianini, 2004), each of
which calculates similarity based on a different view
of the text. State-of-the-art results on text classifi-
cation have been achieved with kernel-based classi-
fication algorithms, such as the support vector ma-
chine (SVM) (Joachims, 1998), and the methods
here can be adapted for use in multiple kernel classi-
fication, as in Polajnar et al (2011). The kernels are
combined using Gaussian process regression (GPR)
(Rasmussen and Williams, 2006). It is important
to note that the combination strategy described here
is only a different way of viewing the regression-
combined mixture of similarity measures approach
that is already popular in STS systems, including
several that participated in previous SemEval tasks
(Croce et al, 2012; Ba?r et al, 2012). Likewise, oth-
ers, such as Croce et al (2012), have used tree and
dependency parse information as part of their sys-
tems; however, we use a tree kernel approach based
on a novel encoding method introduced by Zanzotto
et al (2011) and from there derive two dependency-
based methods.
In the rest of this paper we will describe our sys-
tem, which consists of distributional similarity (Sec-
tion 2.1), several kernel measures (Section 2.2), and
a combination method (Section 2.3). This will be
followed by the description of our three submissions
(Section 3), and a discussion of the results (Sec-
tion 4).
2 Methods
At the core of all the kernel methods is either sur-
face, distributional, or syntactic similarity between
sentence constituents. The methods themselves en-
code sentences into vectors or sets of vectors, while
the similarity between any two vectors is calculated
using cosine.
2.1 Distributional Similarity
Target words are the non-stopwords that occur
within our training and test data. The two distri-
butional methods we use here both represent target
85
words as vectors that encode word occurrence within
a set of contexts. The first method is a variation on
BEAGLE (Jones and Mewhort, 2007), which con-
siders contexts to be words that surround targets.
The second method is based on ESA (Gabrilovich
and Markovitch, 2007), which considers contexts to
be Wikipedia documents that contain target words.
To gather the distributional data with both of
these approaches we used 316,305 documents from
the September 2012 snapshot of Wikipedia. The
training corpus for BEAGLE is generated by pool-
ing the top 20 documents retrieved by querying the
Wikipedia snapshot index for each target word in the
training and test data sets.
2.1.1 BEAGLE
Random indexing (Kaski, 1998) is a technique for
dimensionality reduction where pseudo-orthogonal
bases are generated by randomly sampling a distri-
bution. BEAGLE is a model where random indexing
is used to represent word co-occurrence vectors in a
distributional model.
Each context word is represented as a D-
dimensional vector of normally distributed random
values drawn from the Gaussian distribution
N (0, ?2), where ? =
1
?
D
and D = 4096 (1)
A target word is represented as the sum of the
vectors of all the context words that occur within a
certain context window around the target word. In
BEAGLE this window is considered to be the sen-
tence in which the target word occurs; however, to
avoid segmenting the entire corpus, we assume the
window to include 5 words to either side of the tar-
get. This method has the advantage of keeping the
dimensionality of the context space constant even
if more context words are added, but we limit the
context words to the top 10,000 most frequent non-
stopwords in the corpus.
2.1.2 ESA
ESA represents a target word as a weighted
ranked list of the top N documents that contain the
word, retrieved from a high quality collection. We
used the BM25F (Robertson et al, 2004) weighting
function and the topN = 700 documents. These pa-
rameters were chosen by testing on the WordSim353
dataset.1 The list of retrieved documents can be rep-
resented as a very sparse vector whose dimensions
match the number of documents in the collection,
or in a more computationally efficient manner as
a hash map linking document identifiers to the re-
trieval weights. Similarity between lists was calcu-
lated using the cosine measure augmented to work
on the hash map data type.
2.2 Kernel Measures
In our experiments we use six basic kernel types,
which are described below. Effectively we have
eight kernels, because we also use the tree and de-
pendency kernels with and without distributional in-
formation. Each kernel is a function which is passed
a pair of short texts, which it then encodes into a spe-
cific format and compares using a defined similarity
function. LK uses the regular cosine similarity func-
tion, but LEK, TK, DK, MDK, DGK use the follow-
ing cosine similarity redefined for sets of vectors. If
the texts are represented as sets of vectors X and Y ,
the set similarity kernel function is:
?set(X,Y ) =
?
i
?
j
cos(~xi, ~yj) (2)
and normalisation is accomplished in the standard
way for kernels by:
?set?n(X,Y ) =
?set(X,Y )
?
(?set(X,X)?set(Y, Y ))
(3)
LK - The lexical kernel calculates the overlap be-
tween the tokens that occur in each of the paired
texts, where the tokens consist of Porter stemmed
(Porter, 1980) non-stopwords. Each text is repre-
sented as a frequency vector of tokens that occur
within it and the similarity between the pair is cal-
culated using cosine.
LEK - The lexical ESA kernel represents each
example in the pair as the set of words that do not
occur in the intersection of the two texts. The simi-
larity is calculated as in Equation (3) with X and Y
being the ESA vectors of each word from the first
and second text representations, respectively.
TK - The tree kernel representation is based on
the definition by Zanzotto et al (2011). Briefly,
1http://www.cs.technion.ac.il/?gabr/resources/
data/wordsim353/
86
each piece of text is parsed2; the non-terminal
nodes of the parse tree, stopwords, and out-of-
dictionary terms are all assigned a new random vec-
tor (Equation 1); while the leaves that occurred
in the BEAGLE training corpus are assigned their
learned distributional vectors (Section 2.1.1).
Each subtree of a tree is encoded recursively as
a vector, where the distributional vectors represent-
ing each node are combined using the circular con-
volution operator (Plate, 1994; Jones and Mewhort,
2007). The whole tree is represented as a set of vec-
tors, one for each subtree.
DK - The dependency kernel representation en-
codes each dependency pair as a separate vector, dis-
counting the labels. The non-stopword terminals are
represented as their distributional vectors, while the
stopwords and out-of-dictionary terms are given a
unique random vector. The vector for the depen-
dency pair is obtained via a circular convolution of
the individual word vectors.
MDK - The multiple dependency kernel is con-
structed like the dependency kernel, but similarity is
calculated separately between all the the pairs that
share the same dependency label. The combined
similarity for all dependency labels in the parse is
then calculated using least squares linear regression.
While at the later stage we use GPR to combine all
of the different kernels, for MDK we found that lin-
ear regression provided better performance.
DGK - The depgram kernel represents each de-
pendency pair as an ESA vector obtained by search-
ing the ESA collection for the two words in the
dependency pair joined by the AND operator. The
DGK representation only contains the dependencies
that occur in one similarity text or the other, but not
in both.
2.3 Regression
Each of the kernel measures above is used to calcu-
late a similarity score between a pair of texts. The
different similarity scores are then combined using
2Because many of the datasets contained incomplete or un-
grammatical sentences, we had to approximate some parses.
The parsing was done using the Stanford parser (Klein and
Manning, 2003), which failed on some overly long sentences,
which we therefore segmented at conjunctions or commas.
Since our methods only compared subtrees of parses, we simply
took the union of all the partial parses for a given sentence.
Gaussian process regression (GPR) (Rasmussen and
Williams, 2006). GPR is a probabilistic regression
method where the weights are modelled as Gaussian
random variables. GPR is defined by a covariance
function, which is akin to the kernel function in the
support vector machine. We used the squared expo-
nential isotropic covariance function (also known as
the radial basis function):
cov(xi, xj) = p
2
1e
(xi?xj)
T ?(p2?I)
?1?(xi?xj)
2 + p23?ij
with parameters p1 = 1, p2 = 1, and p3 = 0.01. We
found that training for parameters increased overfit-
ting and produced worse results in validation exper-
iments.
3 Submitted Runs
We submitted three runs. This is not sufficient for
a full evaluation of the new methods we proposed
here, but it gives us an inkling of general trends. To
choose the composition of the submissions, we used
STS 2012 training data for training, and STS 2012
test data for validation (Agirre et al, 2012). The
final submitted runs also used some of the STS 2012
test data for training.
Basic - With this run we were examining if a sim-
ple introduction of syntactic structure can improve
over the baseline performance. We trained a GPR
combination of the linear and tree kernels (LK-TK)
on the MSRpar training data. In validation experi-
ments we found that this data set in general gave the
most consistent performance for regression training.
Custom - Here we tried to approximate the best
training setup for each type of data. We only had
training data for OnWN and for this dataset we were
able to improve over the LK-TK setup; however, the
settings for the rest of the data sets were guesses
based on observations from the validation experi-
ments and overall performed poorly. OnWN was
trained on MSRpar train with LK and DK. The head-
lines model was trained on MSRpar train and Eu-
roparl test, with LK-LEK-TK-DK-TKND-DKND-
MDK (trained on Europarl).3 FNWN was trained on
MSRpar train and OnWN test with LK-LEK-DGK-
TK-DK-TKND-DKND. Finally, the SMT model
3TKND and DKND are the versions of the tree and depen-
dency kernels where no distributional vectors were used.
87
010
20
30
40
50
60
0 1 2 3 4 5
Number 
of STS p
airs
Score
Gold Standard
0
5
10
15
20
25
0 1 2 3 4 5
Number 
of STS p
airs
Score
Basic
0
5
10
15
20
25
30
35
0 1 2 3 4 5
Number 
of STS p
airs
Score
Custom
0
5
10
15
20
2 2.5 3 3.5 4 4.5 5
Number 
of STS p
airs
Score
All
Figure 1: Score distributions of different runs on the OnWN dataset
was trained on MSRpar train and Europarl test with
LK-LEK-TK-DK-TKND-DKND-MDK (trained on
MSRpar).
All - As in the LK-TK experiment, we used
the same model on all of the data sets. It was
trained on all of the training data except MSRvid,
using all eight kernel types defined above. In sum-
mary we used the LK-LEK-TK-TKND-DK-DKND-
MDK-DGK kernel combination. MDK was trained
on the 2012 training portion of MSRpar.
4 Discussion
From the shared task results in Table 1, we can see
that Basic is our highest ranked run. It has also
achieved the best performance on all data sets. The
LK on its own improves slightly on the task baseline
by removing stop words and using stemming, while
the introduction of TK contributes syntactic and dis-
tributional information. With the Custom run, we
were trying to manually estimate which training data
would best reflect properties of particular test data,
and to customise the kernel combination through
validation experiments. The only data set for which
this led to an improvement is OnWN, indicating
that customised settings can be beneficial, but that
a more scientific method for matching of training
and test data properties is required. In the All run,
we were examining the effects that maximising the
amount of training data and the number of kernel
hdlns OnWN FNWN SMT mean rank
BL 0.5399 0.2828 0.2146 0.2861 0.3639 71
Basic 0.6399 0.4440 0.3995 0.3400 0.4709 51
Cstm 0.4962 0.5639 0.1724 0.3006 0.4207 60
All 0.5510 0.3099 0.2385 0.1171 0.3200 78
Table 1: Shared task results: Pearson correlation with the
gold standard
measures has on the output predictions. The results
show that swamping the regression with models and
training data leads to overly normalised output and
a decrease in performance.
While the evaluation measure, Pearson correla-
tion, does not take into account the shape of the out-
put distribution, Figure 1 shows that this informa-
tion may be a useful indicator of model quality and
behaviour. In particular, the role of the regression
component in our approach is to learn a transforma-
tion from the output distributions of the models to
the distribution of the training data gold standard.
This makes it sensitive to the choice of training data,
which ideally would have similar characteristics to
the individual kernels, as well as a similar gold stan-
dard distribution to the test data. We can see in Fig-
ure 1 that the training data and choice of kernels in-
fluence the output distribution.
Analysis of the minimum, first quartile, median,
third quartile, and maximum statistics of the distri-
butions in Figure 1 demonstrates that, while it is dif-
ficult to visually evaluate the similarities of the dif-
ferent distributions, the smallest squared error is be-
tween the gold standard and the Custom run. This
suggests that properties other than the rank order
may also be good indicators in training and testing
of STS methods.
Acknowledgments
Tamara Polajnar is supported by the ERC Starting
Grant, DisCoTex, awarded to Stephen Clark, and
Laura Rimell and Douwe Kiela by EPSRC grant
EP/I037512/1: A Unified Model of Compositional
and Distributional Semantics: Theory and Applica-
tions.
88
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montre?al, Canada,
June. Association for Computational Linguistics.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012. UNITOR: Combining semantic text
similarity functions through sv regression. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation, held in conjunction with the 1st Joint
Conference on Lexical and Computational Semantics,
pages 597?602, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, ECML ?98, pages 137?
142, London, UK, UK. Springer-Verlag.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1?37.
S. Kaski. 1998. Dimensionality reduction by random
mapping: fast similarity computation for clustering.
In Proceedings of the 1998 IEEE International Joint
Conference on Neural Networks, volume 1, pages
413?418 vol.1, May.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. A. Plate. 1994. Distributed Representations and
Nested Compositional Structure. Ph.D. thesis, Univer-
sity of Toronto.
T Polajnar, T Damoulas, and M Girolami. 2011. Protein
interaction sentence detection using multiple semantic
kernels. J Biomed Semantics, 2(1):1?1.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137, July.
C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian
Processes for Machine Learning. MIT Press.
Stephen Robertson, Hugo Zaragoza, and Michael Taylor.
2004. Simple BM25 extension to multiple weighted
fields. In Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?04, pages 42?49, New York, NY,
USA. ACM.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011. Distributed structures and distributional mean-
ing. In Proceedings of the Workshop on Distributional
Semantics and Compositionality, DiSCo ?11, pages
10?15, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
89
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 44?50
Manchester, August 2008
Constructing a Parser Evaluation Scheme
Laura Rimell and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, United Kingdom
{laura.rimell,stephen.clark}@comlab.ox.ac.uk
Abstract
In this paper we examine the process of
developing a relational parser evaluation
scheme, identifying a number of decisions
which must be made by the designer of
such a scheme. Making the process more
modular may help the parsing community
converge on a single scheme. Examples
from the shared task at the COLING parser
evaluation workshop are used to highlight
decisions made by various developers, and
the impact these decisions have on any re-
sulting scoring mechanism. We show that
quite subtle distinctions, such as howmany
grammatical relations are used to encode a
linguistic construction, can have a signifi-
cant effect on the resulting scores.
1 Introduction
In this paper we examine the various decisions
made by designers of parser evaluation schemes
based on grammatical relations (Lin, 1995; Car-
roll et al, 1998). Following Carroll et al (1998),
we use the term grammatical relations to refer
to syntactic dependencies between heads and de-
pendents. We assume that grammatical relation
schemes are currently the best method available
for parser evaluation due to their relative inde-
pendence of any particular parser or linguistic
theory. There are several grammatical relation
schemes currently available, for example Carroll et
al. (1998), King et al (2003), and de Marneffe et
al. (2006). However, there has been little analysis
of the decisions made by the designers in creating
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
what turns out to be a complex set of dependen-
cies for naturally occurring sentences. In particu-
lar, in this paper we consider how the process can
be made more modular to help the parsing commu-
nity converge on a single scheme.
The first decision to be made by the scheme
designer is what types of linguistic constructions
should be covered by the scheme. By construction
we mean syntactic phenomena such as subject of
verb, direct object of verb, passive voice, coordina-
tion, relative clause, apposition, and so on. In this
paper we assume that the constructions of interest
have already been identified (and there does appear
to be broad agreement on this point across the ex-
isting schemes). A construction can be thought of
as a unitary linguistic object, although it is often
represented by several grammatical relations.
The second decision to be made is which words
are involved in a particular construction. This is
important because a subset of these words will
be arguments of the grammatical relations repre-
senting the construction. Again, we assume that
there is already broad agreement among the exist-
ing schemes regarding this question. One possible
point of disagreement is whether to include empty
elements in the representation, for example when
a passive verb has no overt subject, but we will not
address that issue here.
The next question, somewhat orthogonal to the
previous one, and a source of disagreement be-
tween schemes, is how informative the represen-
tation should be. By informative we mean the
amount of linguistic information represented in the
scheme. As well as relations between heads, some
schemes include one or more features, each of
which expresses information about an individual
head. These features can be the locus of richer lin-
guistic information than is represented in the de-
44
pendencies. A useful example here is tense and
mood information for verbs. This is included in the
PARC scheme, for example, but not in the Briscoe
and Carroll or Stanford schemes; PARC is in gen-
eral more informative and detailed than competing
schemes. Although features are technically dif-
ferent from relations, they form part of an overall
evaluation scheme and must be considered by the
scheme designer. We will not consider here the
question of how informative schemes should be;
we only note the importance of this question for
the resulting scoring mechanism.
The penultimate question, also a source of
disagreement among existing schemes, is which
words among all those involved in the construc-
tion should be used to represent it in the scheme.
This decision may arise when identifying syntac-
tic heads; for example, in the sentence Brown
said house prices will continue to fall, we assume
there is no disagreement about which words are
involved in the clausal complement construction
({said, house, prices, will, continue, to, fall}), but
there may be disagreement about which subset to
use to represent the construction in the grammat-
ical relations. Here, either will or continue could
be used to represent the complement of said. This
decision may also be theory-dependent to some de-
gree, for example whether to use the determiner or
the noun as the head of a noun phrase.
The final decision to make is the choice of rela-
tions and their arguments. This can also be thought
of as the choice of how the set of representative
words should be grouped into relations. For exam-
ple, in a relative clause construction, the scheme
designer must decide whether the relation between
the relative pronoun and the head noun is impor-
tant, or the relation between the relative pronoun
and the verb, between the head noun and the verb,
or some subset of these. The choice of label for
each relation will be a natural part of this decision.
An important property of the representation,
closely related to the choices made about represen-
tative words and how they are grouped into rela-
tions, is the number of relations used for a partic-
ular construction. We refer to this as the compact-
ness property. Compactness essentially boils down
to the valency of each relation and the information
encoded in the label(s) used for the relation. We
show that this property is closely related to the as-
signing of partial credit ? awarding points even
when a construction is not recovered completely
correctly ? and that it can have a significant effect
on the resulting scoring mechanism.
The dividing lines between the various ques-
tions we have described are subtle, and in partic-
ular the last two questions (which words should
represent the construction and which relations to
use, and consequently how compactly the rela-
tions are represented) have significant overlap with
one another. For example, if the auxiliary are
in the passive construction prices are affected is
chosen as one of the representative words, then
a relation type which relates are to either prices
or affected must also be chosen. For the relative
clause construction woman who likes apples and
pears, if the words and relations chosen include
a representation along the lines of relative-clause-
subject(likes, woman) and subject(likes, who), then
it is unlikely that the more compact relation
relative-clause(likes, woman, who) would also be
chosen. Despite the overlap, each question can
provide a useful perspective for the designer of an
evaluation scheme.
Decisions must be made not only about the rep-
resentations of the individual constructions, but
also about the interfaces between constructions.
For example, in the sentenceMary likes apples and
pears, the coordination structure apples and pears
serves as direct object of likes, and it must be de-
termined which word(s) are used to represent the
coordination in the direct object relation.
We will illustrate some of the consequences of
the decisions described here with detailed exam-
ples of three construction types. We focus on pas-
sive, coordination, and relative clause construc-
tions, as analysed in the PARC (King et al, 2003),
GR (Briscoe and Carroll, 2006), and Stanford (de
Marneffe et al, 2006) evaluation schemes, using
sentences from the shared task of the COLING 2008
parser evaluation workshop.
1
These three con-
structions were chosen because we believe they
provide particularly good illustrations of the var-
ious decisions and their consequences for scoring.
Furthermore, they are constructions whose repre-
sentation differs across at least two of the three
grammatical relation schemes under dicsussion,
which makes them more interesting as examples.
We believe that the principles involved, however,
1
The shared task includes a number of additional formats
besides the three grammatical relation schemes that we con-
sider here, but the representations are sufficiently different
that we don?t consider a comparison fruitful for the present
discussion.
45
apply to any linguistic construction.
We also wish to point out that at this stage we are
not recommending any particular scheme or any
answers to the questions we raise, but only sug-
gesting ways to clarify the decision points. Nor do
we intend to imply that the ideal representation of
any linguistic construction, for any particular pur-
pose, is one of the representations in an existing
scheme; we merely use the existing schemes as
concrete and familiar illustrations of the issues in-
volved.
2 The Passive Construction
The following is an extract from Sentence 9 of the
shared task:
how many things are made out of eggs
We expect general agreement that this is a pas-
sive construction, and that it should be included in
the evaluation scheme.
2
We also expect agreement
that all the words in this extract are involved in the
construction.
Potential disagreements arise when we consider
which words should represent the construction.
Things, as the head of the noun phrase which is the
underlying object of the passive, and made, as the
main verb, seem uncontroversial. We discard how
and many as modifiers of things, and the prepo-
sitional phrase out of eggs as a modifier of made;
again we consider these decisions to be straightfor-
ward. More controversial is whether to include the
auxiliary verb are. PARC, for example, does not
include it in the scheme at all, considering it an in-
herent part of the passive construction. Even if the
auxiliary verb is included in the overall scheme, it
is debatable whether this word should be consid-
ered part of the passive construction or part of a
separate verb-auxiliary construction. Stanford, for
example, uses the label auxpass for the relation be-
tween made and are, indicating that it is part of the
passive construction.
The next decision to be made is what relations
to use. We consider it uncontroversial to include
a relation between things and made, which will be
some kind of subject relation. We also want to rep-
resent the fact that made is in the passive voice,
since this is an essential part of the construction
and makes it possible to derive the underlying ob-
ject position of things. If the auxiliary are is in-
2
PARC recognises it as an interrogative as well as a passive
construction.
cluded, then there should be a verb-auxiliary rela-
tion between made and are, and perhaps a subject
relation between are and things (although none of
the schemes under consideration use the latter rela-
tion). PARC includes a variety of additional infor-
mation about the selected words in the construc-
tion, including person and number information for
the nouns, as well as tense and mood for the verbs.
Since this is not included in the other two schemes,
we ignore it here.
The relevant relations from the three schemes
under consideration are shown below.
3
PARC
passive(make, +)
subj(make, thing)
GR
(ncsubj made things obj)
(passive made)
(aux made are)
Stanford
nsubjpass(made, things)
auxpass(made, are)
PARC encodes the grammatical relations less
compactly, with one subject relation joining make
and thing, and a separate relation expressing the
fact that make is in the passive voice. Stanford
is more compact, with a single relation nsubj-
pass that expresses both verb-subject (via the argu-
ments) and passive voice (via the label). GR has an
equally compact relation since the obj marker sig-
nifies passive when found in the ncsubj relation.
GR, however, also includes an additional feature
passive, which redundantly encodes the fact that
made is in passive voice.
4
Table 1 shows how different kinds of parsing er-
rors are scored in the three schemes. First note the
differences in the ?everything correct? row, which
shows how many points are available for the con-
struction. A parser that is good at identifying pas-
sives will earn more points in GR than in PARC
and Stanford. Of course, it is always possible to
look at accuracy figures by dependency type in or-
der to understand what a parser is good at, as rec-
ommended by Briscoe and Carroll (2006), but it is
3
Schemes typically include indices on the words for iden-
tification, but we omit these from the examples unless re-
quired for disambiguation. Note also that PARC uses the
lemma rather than the inflected form for the head words.
4
Although passive is technically a feature and not a rela-
tion, as long as it is included in the evaluation the effect will
be of double scoring.
46
PARC GR Stanf
Everything correct 2 3 2
Misidentify subject 1 2 1
Misidentify verb 0 0 0
Miss passive constr 1 1 0
Miss auxiliary 2 2 1
Table 1: Scores for passive construction.
also desirable to have a single score reflecting the
overall accuracy of a parser, which means that the
construction?s overall contribution to the score is
relevant.
5
Observe also that partial credit is assigned dif-
ferently in the three schemes. If the parser recog-
nises the subject of made but misses the fact that
the construction is a passive, for example, it will
earn one out of two possible points in PARC, one
out of three in GR (if it recognizes the auxiliary),
but zero out of two in Stanford. This type of error
may seem unlikely, yet examples are readily avail-
able. In related work we have evaluated the C&C
parser of Clark and Curran (2007) on the BioIn-
fer corpus of biomedical abstracts (Pyysalo et al,
2007), which includes the following sentence:
Acanthamoeba profilin was cross-linked
to actin via a zero-length isopeptide
bond using carbodiimide.
The parser correctly identifies profilin as the sub-
ject of cross-linked, yet because it misidentifies
cross-linked as an adjectival rather than verbal
predicate, it misses the passive construction.
Finally, note an asymmetry in the partial credit
scoring: a parser that misidentifies the subject (e.g.
by selecting the wrong head), but basically gets the
construction correct, will receive partial credit in
all three schemes; misidentifying the verb, how-
ever (again, this would likely occur by selecting
the wrong head within the verb phrase) will cause
the parser to lose all points for the construction.
3 The Coordination Construction
The coordination construction is particularly inter-
esting with regard to the questions at hand, both
because there are many options for representing
the construction itself and because the interface
with other constructions is non-trivial. Here we
5
We assume that the overall score will be an F-score over
all dependencies/features in the relevant test set.
consider an extract from Sentence 1 of the shared
task:
electronic, computer and building prod-
ucts
The coordination here is of nominal modifiers,
which means that there is a decision to make about
how the coordination interfaces with the modified
noun. All the conjuncts could interact with the
noun, or there could be a single relationship, usu-
ally represented as a relationship between the con-
junction and and the noun.
Again we consider the decisions about whether
to represent coordination constructions in an eval-
uation scheme, and about which words are in-
volved in the construction, to be generally agreed
upon. The choice of words to represent the
construction in the grammatical relations is quite
straightforward: we need all three conjuncts, elec-
tronic, computer, and building, and also the con-
junction itself since this is contentful. It also seems
reasonably uncontroversial to discard the comma
(although we know of at least one parser that
outputs relations involving the comma, the C&C
parser).
The most difficult decision here is whether the
conjuncts should be related to one another or to
the conjunction (or both). Shown below is how the
three schemes represent the coordination, consid-
ering also the interface of the coordination and the
nominal modification construction.
PARC
adjunct(product, coord)
adjunct type(coord, nominal)
conj(coord, building)
conj(coord, computer)
conj(coord, electronic)
coord form(coord, and)
coord level(coord, AP)
GR
(conj and electronic)
(conj and computer)
(conj and building)
(ncmod products and)
Stanford
conj and(electronic, computer)
conj and(electronic, building)
amod(products, electronic)
amod(products, computer)
amod(products, building)
47
Table 2 shows the range of scores assigned for
correct and partially correct parses across the three
schemes. A parser that analyses the entire con-
struction correctly will earn anywhere from four
points in GR, to seven points in PARC. Therefore,
a parser that does very well (or poorly) at coordi-
nation will earn (or lose) points disproportionately
in the different schemes.
Parc GR Stanf
Everything correct 7 4 5
Misidentify
conjunction 6 0 3
Misidentify one
conjunct 6
a
3 3
b
Misidentify two
conjuncts 5
a
2 1
a
The parser might also be incorrect about the co-
ord level relation if the conjuncts are misidentified.
b
The score would be 2 if it is the first conjunct that
is misidentified.
Table 2: Scores for coordination, including
interface with nominal modification.
A parser that recognises the conjuncts correctly
but misidentifies the conjunction would lose only
one point in PARC, where the conjunction is sep-
arated out into a single coord form relation, but
would lose all four available points in GR, because
the word and itself takes part in all four GR de-
pendencies. Only two points are lost in Stanford
(and it is worth noting that there is also an ?uncol-
lapsed? variant of the Stanford scheme in which
the coordination type is not rolled into the depen-
dency label, in which case only one point would be
lost).
Note also an oddity in Stanford which means
that if the first conjunct is missed, all the dependen-
cies are compromised, because the first conjunct
enters into relations with all the others. The more
conjuncts there are in the construction, the more
points are lost for a single parsing error, which can
easily result from an error in head selection.
Another issue is how the conjuncts are repre-
sented relative to the nominal modifier construc-
tion. In PARC and GR, the conjunct and stands in
for all the conjuncts in the modifier relation. This
means that if a conjunct is missed, no extra points
are lost on the modifier relation; whereas in Stan-
ford, points are lost doubly ? on the relations in-
volving both conjunction and modification.
4 The Relative Clause Construction
For the relative clause construction, as for coordi-
nation, the choice of words used to represent the
construction is straightforward, but the choice of
relations is less so. Consider the following relative
clause construction from Sentence 2 of the shared
task:
not all those who wrote
All three schemes under consideration use the set
{those, who, wrote} to describe this construction.
6
PARC
pron form(pro
3
, those)
adjunct(pro
3
, write)
adjunct type(write, relative)
pron form(pro
4
, who)
pron type(pro
4
, relative)
pron rel(write, pro
4
)
topic rel(write, pro
4
)
GR
(cmod who those wrote)
(ncsubj wrote those )
Stanford
nsubj(wrote, those)
rel(wrote, who)
rcmod(those, wrote)
Note that PARC represents the pronouns who
and those, as it does all pronouns, at a more ab-
stract level than GR or Stanford, creating a rep-
resentation that is less compact than the others.
GR and Stanford differ in terms of compactness as
well: GR?s cmod relation contains all three words;
in fact, the ncsubj relationship might be considered
redundant from the point of view of an evaluation
scheme, since an error in ncsubj entails an error in
cmod. Stanford?s representation is less compact,
containing only binary relations, although there is
also a redundancy between nsubj and rcmod since
the two relations are mirror images of each other.
For the sake of comparison, we include here two
additional hypothetical schemes which have dif-
ferent characteristics from those of the three tar-
get schemes. In Hypothetical Scheme 1 (HS1),
there are three relations: one between the head
noun and the relative clause verb, one between the
6
PARC also encodes the fact that pro
3
is a demonstrative
pronoun, but we don?t consider this part of the relative clause
construction.
48
PARC GR Stanf HS1 HS2
Everything correct 7 2 3 3 1
Misidentify head noun 6 0 1 1 0
Misidentify verb 3 0 0 2 0
Miss relative clause construction 3 0 0 1 0
Table 3: Scores for relative clauses.
relative pronoun and the relative clause verb, and
a third which relates the relative pronoun to the
head noun. This third relation is not included in
any of the other schemes. Hypothetical Scheme 2
(HS2) involves only one relation, which includes
the same words as GR?s cmod relation; the repre-
sentation as a whole is quite compact since only
one dependency is involved and it includes all
three words.
Hypothetical Scheme 1
relative-subject(wrote, those)
subject(wrote, who)
relative-pronoun(those, who)
Hypothetical Scheme 2
relative-clause(wrote, those, who)
Table 3 shows the range of scores that can be at-
tained in the different schemes. The total possible
score varies from one for HS2, to three for Stan-
ford and HS1, and up to seven for PARC.
Observe that any of the three types of error in
Table 3 will immediately lose all points in both GR
and HS2. Since all the schemes use the same set
of words, this is due solely to the choice of rela-
tions and the compactness of the representations.
Neither GR nor HS2 allow for partial credit, even
when the parser assigns an essentially correct rel-
ative clause structure. This is a scenario which
could easily occur due to a head selection error.
For example, consider the following phrase from
the shared task GENIA (Kim et al, 2003) data set ,
Sentence 8:
. . . the RelA ( p65 ) subunit of NF-kappa
B , which activates transcription of the c-
rel gene . . .
The C&C parser correctly identifies the relative
clause structure, including the pronoun which and
the verb activates, but incorrectly identifies the
head noun as B instead of subunit.
Even between GR and HS2, which share the
characteristic of not allowing for partial credit,
there is a difference in scoring. Because GR starts
with two dependencies, there is a loss of two
points, rather than just one, for any error, which
means errors in relative clauses are weighted more
heavily in GR than in HS2.
Stanford also has a problematic redundancy,
since the nsubj and rcmod relations are mirror im-
ages of each other. It therefore duplicates the GR
characteristic of penalising the parser by at least
two points if either the head noun or the relative
clause verb is misidentified (in fact three points for
the verb).
Observe also the asymmetry between misidenti-
fying the head noun (one out of seven points lost in
PARC, two out of three lost in Stanford and HS1)
compared to misidentifying the verb (three points
lost in PARC, all three lost in Stanford, but only one
point lost in HS1). This reflects a difference be-
tween the schemes in whether the relative pronoun
enters into a relation with the subject, the verb, or
both.
5 Conclusion
In this paper we have shown how the design pro-
cess for a relational parser evaluation scheme can
be broken up into a number of decisions, and how
these decisions can significantly affect the scoring
mechanism for the scheme. Although we have fo-
cused in detail on three construction types, we be-
lieve the decisions involved are relevant to any lin-
guistic construction, although some decisions will
be more difficult than others for certain construc-
tions. A direct object construction, for example,
will normally be represented by a single relation
between a verbal head and a nominal head, and in-
deed this is so in all three schemes considered here.
This does not mean that the representation is triv-
ial, however. The choice of which heads will rep-
resent the construction is important. In addition,
Stanford distinguishes objects of prepositions from
objects of verbs, while PARC and GR collapse the
two into a single relation. Although part of speech
information can be used to distinguish the two, a
49
parser which produces PARC- or GR-style output
in this regard will lose points in Stanford without
some additional processing.
We have made no judgements about which deci-
sions are best in the evaluation scheme design pro-
cess. There are no easy answers to the questions
raised here, and it may be that different solutions
will suit different evaluation situations. We leave
these questions for the parsing community to de-
cide. This process may be aided by an empirical
study of how the decisions affect the scores given
to various parsers. For example, it might be use-
ful to know whether one parser could be made to
score significantly higher than another simply by
changing the way coordination is represented. We
leave this for future work.
References
Briscoe, Ted and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the ACL-Coling
?06 Main Conf. Poster Session, pages 41?48, Syd-
ney, Austrailia.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th LREC Conference, pages
449?454, Genoa, Italy.
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182.
King, Tracy H., Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of the
4th International Workshop on Linguistically Inter-
preted Corpora, Budapest, Hungary.
Lin, Dekang. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425, Montreal, Canada.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bj?orne, Jorma Boberg, Jouni J?arvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8:50.
50
