DETECTION OF AGREEMENT vs. DISAGREEMENT IN MEETINGS:
TRAINING WITH UNLABELED DATA
Dustin Hillard and Mari Ostendorf
University of Washington, EE
{hillard,mo}@ee.washington.edu
Elizabeth Shriberg
SRI International and ICSI
ees@speech.sri.com
Abstract
To support summarization of automatically
transcribed meetings, we introduce a classifier
to recognize agreement or disagreement utter-
ances, utilizing both word-based and prosodic
cues. We show that hand-labeling efforts can
be minimized by using unsupervised training
on a large unlabeled data set combined with
supervised training on a small amount of data.
For ASR transcripts with over 45% WER, the
system recovers nearly 80% of agree/disagree
utterances with a confusion rate of only 3%.
1 Introduction
Meetings are an integral component of life in most or-
ganizations, and records of meetings are important for
helping people recall (or learn for the first time) what
took place in a meeting. Audio (or audio-visual) record-
ings of meetings offer a complete record of the inter-
actions, but listening to the complete recording is im-
practical. To facilitate browsing and summarization of
meeting recordings, it is useful to automatically annotate
topic and participant interaction characteristics. Here, we
focus on interactions, specifically identifying agreement
and disagreement. These categories are particularly im-
portant for identifying decisions in meetings and infer-
ring whether the decisions are controversial, which can be
useful for automatic summarization. In addition, detect-
ing agreement is important for associating action items
with meeting participants and for understanding social
dynamics. In this study, we focus on detection using both
prosodic and language cues, contrasting results for hand-
transcribed and automatically transcribed data.
The agreement/disagreement labels can be thought of
as a sort of speech act categorization. Automatic classifi-
cation of speech acts has been the subject of several stud-
ies. Our work builds on (Shriberg et al, 1998), which
showed that prosodic features are useful for classifying
speech acts and lead to increased accuracy when com-
bined with word based cues. Other studies look at predic-
tion of speech acts primarily from word-based cues, using
language models or syntactic structure and discourse his-
tory (Chu-Carroll, 1998; Reithinger and Klesen, 1997).
Our work is informed by these studies, but departs signif-
icantly by exploring unsupervised training techniques.
2 Approach
Our experiments are based on a subset of meeting record-
ings collected and transcribed by ICSI (Morgan et al,
2001). Seven meetings were segmented (automatically,
but with human adjustment) into 9854 total spurts. We
define a ?spurt? as a period of speech by one speaker that
has no pauses of greater than one half second (Shriberg et
al., 2001). Spurts are used here, rather than sentences,
because our goal is to use ASR outputs and unsuper-
vised training paradigms, where hand-labeled sentence
segmentations are not available.
We define four categories: positive, backchannel, neg-
ative, and other. Frequent single-word spurts (specifi-
cally, yeah, right, yep, uh-huh, and ok) are separated out
from the ?positive? category as backchannels because of
the trivial nature of their detection and because they may
reflect encouragement for the speaker to continue more
than actual agreement. Examples include:
Neg: (6%) ?This doesn?t answer the question.?
Pos: (9%) ?Yeah, that sounds great.?
Back: (23%) ?Uh-huh.?
Other: (62%) ?Let?s move on to the next topic.?
The first 450 spurts in each of four meetings were
hand-labeled with these four categories based on listening
to speech while viewing transcripts (so a sarcastic ?yeah,
right? is labeled as a disagreement despite the positive
wording). Comparing tags on 250 spurts from two label-
ers produced a kappa coefficient (Siegel and Castellan,
1988) of .6, which is generally considered acceptable.
Additionally, unlabeled spurts from six hand-transcribed
training meetings are used in unsupervised training ex-
periments, as described later. The total number of au-
tomatically labeled spurts (8094) is about five times the
amount of hand-labeled data.
For system development and as a control, we use hand-
transcripts in learning word-based cues and in training.
We then evaluate the model with both hand-transcribed
words and ASR output. The category labels from the
hand transcriptions are mapped to the ASR transcripts,
assigning an ASR spurt to a hand-labeled reference if
more than half (time wise) of the ASR spurt overlaps the
reference spurt.
Feature Extraction. The features used in classification
include heuristic word types and counts, word-based fea-
tures derived from n-gram scores, and prosodic features.
Simple word-based features include: the total num-
ber of words in a spurt, the number of ?positive? and
?negative? keywords, and the class (positive, negative,
backchannel, discourse marker, other) of the first word
based on the keywords. The keywords were chosen based
on an ?effectiveness ratio,? defined as the frequency of a
word (or word pair) in the desired class divided by the fre-
quency over all dissimilar classes combined. A minimum
of five occurrences was required and then all instances
with a ratio greater than .6 were selected as keywords.
Other word-based features are found by computing the
perplexity (average log probability) of the sequence of
words in a spurt using a bigram language model (LM)
for each of the four classes. The perplexity indicates the
goodness of fit of a spurt to each class. We used both
word and class LMs (with part-of-speech classes for all
words except keywords). In addition, the word-based LM
is used to score the first two words of the spurt, which
often contain the most information about agreement and
disagreement. The label of the most likely class for each
type of LM is a categorical feature, and we also compute
the posterior probability for each class.
Prosodic features include pause, fundamental fre-
quency (F0), and duration (Baron et al, 2002). Features
are derived for the first word alone and for the entire
spurt. Average, maximum and initial pause duration fea-
tures are used. The F0 average and maximum features
are computed using different methods for normalizing F0
relative to a speaker-dependent baseline, mean and max.
For duration, the average and maximum vowel duration
from a forced alignment are used, both unnormalized and
normalized for vowel identity and phone context. Spurt
length in terms of number of words is also used.
Classifier design and feature selection. The overall
approach to classifying spurts uses a decision tree clas-
sifier (Breiman et al, 1984) to combine the word based
and prosodic cues. In order to facilitate learning of cues
for the less frequent classes, the data was upsampled (du-
plicated) so that there were the same number of training
points per class. The decision tree size was determined
using error-based cost-complexity pruning with 4-fold
cross validation. To reduce our initial candidate feature
set, we used an iterative feature selection algorithm that
involved running multiple decision trees (Shriberg et al,
2000). The algorithm combines elements of brute-force
search (in a leave-one-out paradigm) with previously de-
termined heuristics for narrowing the search space. We
used entropy reduction of the tree after cross-validation
as a criterion for selecting the best subtree.
Unsupervised training. In order to train the models
with as much data as possible, we used an unsupervised
clustering strategy for incorporating unlabeled data. Four
bigram models, one for each class, were initialized by
dividing the hand transcribed training data into the four
classes based upon keywords. First, all spurts which con-
tain the negative keywords are assigned to the negative
class. Backchannels are then pulled out when a spurt con-
tains only one word and it falls in the backchannel word
list. Next, spurts are selected as agreements if they con-
tain positive keywords. Finally, the remaining spurts are
associated with the ?other? class.
The keyword separation gives an initial grouping; fur-
ther regrouping involves unsupervised clustering using a
maximum likelihood criterion. A preliminary language
model is trained for each of the initial groups. Then, by
evaluating each spurt in the corpus against each of the
four language models, new groups are formed by asso-
ciating spurts with the language model that produces the
lowest perplexity. New language models are then trained
for the reorganized groups and the process is iterated un-
til there is no movement between groups. The final class
assignments are used as ?truth? for unsupervised training
of language and prosodic models, as well as contributing
features to decision trees.
3 Results and Discussion
Hand-labeled data from one meeting is held out for test
data, and the hand-labeled subset of three other meet-
ings are used for training decision trees. Unlabeled spurts
taken from six meetings, different from the test meeting,
are used for unsupervised training. Performance is mea-
sured in terms of overall 3-way classification accuracy,
merging the backchannel and agreement classes. The
overall accuracy results can be compared to the ?chance?
rate of 50%, since testing is on 4-way upsampled data.
In addition, we report the confusion rate between agree-
ments and disagreements and their recovery (recall) rate,
since these two classes are most important for our appli-
cation.
Results are presented in Table 1 for models using only
word-based cues. The simple keyword indicators used
in a decision tree give the best performance on hand-
transcribed speech, but performance degrades dramati-
cally on ASR output (with WER > 45%). For all other
training conditions, the degradation in performance for
the system based on ASR transcripts is not as large,
though still significant. The system using unsupervised
training clearly outperforms the system trained only on a
small amount of hand-labeled data. Interestingly, when
Hand Transcriptions ASR Transcriptions
Overall A/D A/D Overall A/D A/D
Features Accuracy Confusion Recovery Accuracy Confusion Recovery
Keywords 82% 2% 87% 61% 7% 53%
Hand Trained LM 71% 13% 74% 64% 10% 67%
Unsupervised LM 78% 10% 81% 67% 14% 70%
All word based 79% 8% 83% 71% 3% 78%
Table 1: Results for detection with different classifiers using word based features.
the keywords are used in combination with the language
model, they do provide some benefit in the case where
the system uses ASR transcripts.
The results in Table 2 correspond to models using only
prosodic cues. When these models are trained on only a
small amount of hand-labeled data, the overall accuracy
is similar to the system using keywords when operating
on the ASR transcript. Performance is somewhat better
than chance, and use of hand vs. ASR transcripts (and as-
sociated word alignments) has little impact. There is a
small gain in accuracy but a large gain in agree/disagree
recovery from using the data that was labeled via the un-
supervised language model clustering technique. Unfor-
tunately, when the prosody features are combined with
the word-based features, there is no performance gain,
even for the case of errorful ASR transcripts.
Transcripts Overall A/D A/D
Train/Test Accuracy Confusion Recovery
Hand/Hand 62% 17% 62%
Unsup./Hand 66% 13% 72%
Hand/ASR 62% 16% 61%
Unsup./ASR 64% 14% 75%
Table 2: Results for classifiers using prosodic features.
4 Conclusion
In summary, we have described an approach for au-
tomatic recognition of agreement and disagreement in
meeting data, using both prosodic and word-based fea-
tures. The methods can be implemented with a small
amount of hand-labeled data by using unsupervised LM
clustering to label additional data, which leads to signifi-
cant gains in both word-based and prosody-based classi-
fiers. The approach is extensible to other types of speech
acts, and is especially important for domains in which
very little annotated data exists. Even operating on ASR
transcripts with high WERs (45%), we obtain a 78% rate
of recovery of agreements and disagreements, with a very
low rate of confusion between these classes. Prosodic
features alone provide results almost as good as the word-
based models on ASR transcripts, but no additional ben-
efit when used with word-based features. However, the
good performance from prosody alone offers hope for
performance gains given a richer set of speech acts with
more lexically ambiguous cases (Bhagat et al, 2003).
Acknowledgments
This work is supported in part by the NSF under grants 0121396
and 0619921, DARPA grant N660019928924, and NASA grant
NCC 2-1256. Any opinions, conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of these agencies.
References
D. Baron et al 2002. Automatic punctuation and disfluency
detection in multi-party meetings using prosodic and lexical
cues. In Proc. ICSLP, pages 949?952.
S. Bhagat, H. Carvey, and E. Shriberg. 2003. Automatically
generated prosodic cues to lexically ambiguous dialog acts
in multi-party meetings. In ICPhS.
L. Breiman et al 1984. Classification And Regression Trees.
Wadsworth International Group, Belmont, CA.
J. Chu-Carroll. 1998. A statistical model for discourse act
recognition in dialogue interactions. In Applying Machine
Learning to Discourse Processing. Papers from the 1998
AAAI Spring Symposium, pages 12?17.
N. Morgan et al 2001. The meeting project at ICSI. In
Proc. Conf. on Human Language Technology, pages 246?
252, March.
N. Reithinger and M. Klesen. 1997. Dialogue act classification
using language models. In Proc. Eurospeech, pages 2235?
2238, September.
E. Shriberg et al 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Language
and Speech, 41(3?4), pages 439?487.
E. Shriberg et al 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32(1-2):127?154, September.
E. Shriberg et al 2001. Observations on overlap: Findings and
implications for automatic processing of multi-party conver-
sation. In Proc. Eurospeech, pages 1359?1362.
S. Siegel and J. Castellan. 1988. Nonparametric Statistics For
the Behavioral Sciences. McGraw-Hill Inc., New York, NY,
second edition edition.
Improving Automatic Sentence Boundary Detection
with Confusion Networks
D. Hillard
 
M. Ostendorf
 
University of Washington, EE
 

hillard,mo  @ee.washington.edu
A. Stolcke  Y. Liu  E. Shriberg 
ICSI  and SRI International 

stolcke,ees  @speech.sri.com
yangl@icsi.berkeley.edu
Abstract
We extend existing methods for automatic sen-
tence boundary detection by leveraging multi-
ple recognizer hypotheses in order to provide
robustness to speech recognition errors. For
each hypothesized word sequence, an HMM
is used to estimate the posterior probability of
a sentence boundary at each word boundary.
The hypotheses are combined using confusion
networks to determine the overall most likely
events. Experiments show improved detec-
tion of sentences for conversational telephone
speech, though results are mixed for broadcast
news.
1 Introduction
The output of most current automatic speech recognition
systems is an unstructured sequence of words. Additional
information such as sentence boundaries and speaker la-
bels are useful to improve readability and can provide
structure relevant to subsequent language processing, in-
cluding parsing, topic segmentation and summarization.
In this study, we focus on identifying sentence boundaries
using word-based and prosodic cues, and in particular we
develop a method that leverages additional information
available from multiple recognizer hypotheses.
Multiple hypotheses are helpful because the single best
recognizer output still has many errors even for state-
of-the-art systems. For conversational telephone speech
(CTS) word error rates can be from 20-30%, and for
broadcast news (BN) word error rates are 10-15%. These
errors limit the effectiveness of sentence boundary pre-
diction, because they introduce incorrect words to the
word stream. Sentence boundary detection error rates
on a baseline system increased by 50% relative for CTS
when moving from the reference to the automatic speech
condition, while for BN error rates increased by about
20% relative (Liu et al, 2003). Including additional rec-
ognizer hypotheses allows for alternative word choices to
inform sentence boundary prediction.
To integrate the information from different alterna-
tives, we first predict sentence boundaries in each hypoth-
esized word sequence, using an HMM structure that in-
tegrates prosodic features in a decision tree with hidden
event language modeling. To facilitate merging predic-
tions from multiple hypotheses, we represent each hy-
pothesis as a confusion network, with confidences for
sentence predictions from a baseline system. The final
prediction is based on a combination of predictions from
individual hypotheses, each weighted by the recognizer
posterior for that hypothesis.
Our methods build on related work in sentence bound-
ary detection and confusion networks, as described in
Section 2, and a baseline system and task domain re-
viewed in Section 3. Our approach integrates prediction
on multiple recognizer hypotheses using confusion net-
works, as outlined in Section 4. Experimental results are
detailed in Section 5, and the main conclusions of this
work are summarized in Section 6.
2 Related Work
2.1 Sentence Boundary Detection
Previous work on sentence boundary detection for auto-
matically recognized words has focused on the prosodic
features and words of the single best recognizer output
(Shriberg et al, 2000). That system had an HMM struc-
ture that integrates hidden event language modeling with
prosodic decision tree outputs (Breiman et al, 1984). The
HMM states predicted at each word boundary consisted
of either a sentence or non-sentence boundary classifica-
tion, each of which received a confidence score. Improve-
ments to the hidden event framework have included inter-
polation of multiple language models (Liu et al, 2003).
A related model has been used to investigate punc-
tuation prediction for multiple hypotheses in a speech
recognition system (Kim and Woodland, 2001). That sys-
tem found improvement in punctuation prediction when
rescoring using the classification tree prosodic feature
model, but it also introduced a small increase in word
error rate. More recent work has also implemented a sim-
ilar model, but used prosodic features in a neural net in-
stead of a decision tree (Srivastava and Kubala, 2003).
A maximum entropy model that included pause informa-
tion was used in (Huang and Zweig, 2002). Both finite-
state models and neural nets have been investigated for
prosodic and lexical feature combination in (Christensen
et al, 2001).
2.2 Confusion Networks
Confusion networks are a compacted representation of
word lattices that have strictly ordered word hypothesis
slots (Mangu et al, 2000). The complexity of lattice rep-
resentations is reduced to a simpler form that maintains
all possible paths from the lattice (and more), but trans-
forms the space to a series of slots which each have word
hypotheses (and null arcs) derived from the lattice and as-
sociated posterior probabilities. Confusion networks may
also be constructed from an N-best list, which is the case
for these experiments. Confusion networks are used to
optimize word error rate (WER) by selecting the word
with the highest probability in each particular slot.
3 Tasks & Baseline
This work specifically detects boundaries of sentence-
like units called SUs. An SU roughly corresponds to a
sentence, except that SUs are for the most part defined as
units that include only one independent main clause, and
they may sometimes be incomplete as when a speaker
is interrupted and does not complete their sentence. A
more specific annotation guideline for SUs is available
(Strassel, 2003), which we refer to as the ?V5? standard.
In this work, we focus only on detecting SUs and do not
differentiate among the different types (e.g. statement,
question, etc.) that were used for annotation. We work
with a relatively new corpus and set of evaluation tools,
which are described below.
3.1 Corpora
The system is evaluated for both conversational telephone
speech (CTS) and broadcast news (BN), in both cases us-
ing training, development and test data annotated accord-
ing to the V5 standard. The test data is that used in the
DARPA Rich Transcription (RT) Fall 2003 evaluations;
the development and evaluation test sets together com-
prise the Spring 2003 RT evaluation test sets.
For CTS, there are 40 hours of conversations available
for training from the Switchboard corpus, and 3 hours
(72 conversation sides) each of development and evalua-
tion test data drawn from both the Switchboard and Fisher
corpora. The development and evaluation set each have
roughly 6000 SUs.
The BN data consists of a set of 20 hours of news
shows for training, and 3 hours (6 shows) for testing. The
development and evaluation test data contains 1.5 hours
(3 shows) each for development and evaluation, each with
roughly 1000 SUs. Test data comes from the month of
February in 2001; training data is taken from a previous
time period.
3.2 Baseline System
The automatic speech recognition systems used were up-
dated versions of those used by SRI in the Spring 2003
RT evaluations (NIST, 2003), with a WER of 12.1%
on BN data and 22.9% on CTS data. Both systems
perform multiple recognition and adaptation passes, and
eventually produce up to 2000-best hypotheses per wave-
form segment, which are then rescored with a number of
knowledge sources, such as higher-order language mod-
els, pronunciation scores, and duration models (for CTS).
For best results, the systems combine decoding output
from multiple front ends, each producing a separate N-
best list. All N-best lists for the same waveform segment
are then combined into a single word confusion network
(Mangu et al, 2000) from which the hypothesis with low-
est expected word error is extracted. In our baseline SU
system, the single best word stream thus obtained is then
used as the basis for SU recognition.
Our baseline SU system builds on previous work on
sentence boundary detection using lexical and prosodic
features (Shriberg et al, 2000). The system takes as in-
put alignments from either reference or recognized (1-
best) words, and combines lexical and prosodic infor-
mation using an HMM. Prosodic features include about
100 features reflecting pause, duration, F0, energy, and
speaker change information. The prosody model is a de-
cision tree classifier that generates the posterior probabil-
ity of an SU boundary at each interword boundary given
the prosodic features. Trees are trained from sampled
training data in order to make the model sensitive to fea-
tures of the minority SU class. Recent prosody model im-
provements include the use of bagging techniques in deci-
sion tree training to reduce the variability due to a single
tree (Liu et al, 2003). Language model improvements
include adding information from a POS-based model, a
model using automatically-induced word classes, and a
model trained on separate data.
3.3 Evaluation
Errors are measured by a slot error rate similar to the
WER metric utilized by the speech recognition commu-
nity, i.e. dividing the total number of inserted and deleted
SUs by the total number of reference SUs. (There are
no substitution errors because there is only one sentence
class.) When recognition output is used, the words will
generally not align perfectly with the reference transcrip-
tion and hence the SU boundary predictions will require
some alignment procedure to match to the reference lo-
cation. Here, the alignment is based on the minimum
word error alignment of the reference and hypothesized
word strings, and the minimum SU error alignment if the
WER is equal for multiple alignments. We report num-
bers computed with the su-eval scoring tool from NIST.
SU error rates for the reference words condition of our
baseline system are 49.04% for BN, and 30.13% for CTS,
as reported at the NIST RT03F evaluation (Liu et al,
2003). Results for the automatic speech recognition con-
dition are described in Section 5.
4 Using N-Best Sentence Hypotheses
The large increase in SU detection error rate in mov-
ing from reference to recognizer transcripts motivates an
approach that reduces the mistakes introduced by word
recognition errors. Although the best recognizer output is
optimized to reduce word error rate, alternative hypothe-
ses may together reinforce alternative (more accurate) SU
predictions. The oracle WER for the confusion networks
is much lower than for the single best hypothesis, in the
range of 13-16% WER for the CTS test sets.
4.1 Feature Extraction and SU Detection
Prediction of SUs using multiple hypotheses requires
prosodic feature extraction for each hypothesis, which
in turn requires a forced alignment of each hypothesis.
Thousands of hypotheses are output by the recognizer,
but we prune to a smaller set to reduce the cost of run-
ning forced alignments and prosodic feature extraction.
The recognizer outputs an N-best list of hypotheses and
assigns a posterior probability to each hypothesis, which
is normalized to sum to 1 over all hypotheses. We collect
hypotheses from the N-best list for each acoustic segment
up to 90% of the posterior mass (or to a maximum count
of 1000).
Next, forced alignment and prosodic feature extraction
are run for all segments in this pruned set of hypothe-
ses. Statistics for prosodic feature normalization (such as
speaker and turn F0 mean) are collected from the single
best hypothesis. After obtaining the prosodic features,
the HMM predicts sentence boundaries for each word se-
quence hypothesis independently. For each hypothesis,
an SU prediction is made at all word boundaries, result-
ing in a posterior probability for SU and no SU at each
boundary. The same models are used as in the 1-best pre-
dictions ? no parameters were re-optimized for the N-best
framework. Given independent predictions for the indi-
vidual hypotheses, we then build a system to incorporate
the multiple predictions into a single hypothesis, as de-
scribed next.
4.2 Combining Hypotheses
The prediction results for an individual hypothesis are
represented in a confusion network that consists of a
series of word slots, each followed by a slot with SU and
no SU, as shown in Figure 1 with hypothetical confi-
dences for the between-word events. (This representation
is a somewhat unusual form because the word slots have
only a single hypothesis.) The words in the individual
hypotheses have probability one, and each arc with an
SU or no SU token has a confidence (posterior prob-
ability) assigned from the HMM. The overall network
has a score associated with its N-best hypothesis-level
posterior probability, scaled by a weight corresponding to
the goodness of the system that generated that hypothesis.
president
SU
no_SU at
SU
no_SU war
SU
no_SU
1 1 1
.2
.8
.1
.9
.7
.3
Figure 1: Confusion network for a single hypothesis.
The confusion networks for each hypothesis are
then merged with the SRI Language Modeling Toolkit
(Stolcke, 2002) to create a single confusion network
for an overall hypothesis. This confusion network is
derived from an alignment of the confusion networks
of each individual hypothesis. The resulting network
contains slots with the word hypotheses from the N-best
list and slots with the combined SU/no SU probability,
as shown in Figure 2. The confidences assigned to each
token in the new confusion network are a weighted
linear combination of the probabilities from individual
hypotheses that align to each other, compiled from
the entire hypothesis list, where the weights are the
hypothesis-level scores from the recognizer.
president
SU
no_SU at
SU
no_SU war SU
no_SU
1 .2 .4
.2
.8
.15
.85 .3
or
of
.6
.2
 - .6  - .6
.1
Figure 2: Confusion network for a merged hypothesis.
Finally, the best decision at each point is selected by
choosing the words and boundaries with the highest prob-
ability. Here, the words and SUs are selected indepen-
dently, so that we obtain the same words as would be
selected without inserting the SU tokens and guarantee
no degradation in WER. The key improvement is that the
SU detection is now a result of detection across all recog-
nizer hypotheses, which reduces the effect of word errors
in the top hypothesis.
5 Experiments
Table 1 shows the results in terms of slot error rate on
the four test sets. The middle column indicates the per-
formance on a single hypothesis, with the words derived
from the pruned set of N-best hypotheses. The right col-
umn indicates the performance of the system using mul-
tiple hypotheses merged with confusion networks.
Multiple hypotheses provide a reduction of error for
both test sets of CTS (significant at p   .02 using the Mc-
Nemar test), but give insignificant (and mixed) results for
BN. The small increase in error for the BN evaluation set
WER SU error rate
Single Best Confusion Nets
BN Dev 12.2 55.79% 54.45%
BN Eval 12.0 57.78% 58.42%
CTS Dev 23.6 44.14% 42.72%
CTS Eval 22.2 44.95% 44.01%
Table 1: Word and SU error rates for single best vs. con-
fusion nets.
may be due to the fact that the 1-best parameters were
tuned on different news shows than were represented in
the evaluation data.
We expected a greater gain from the use of confusion
networks in CTS than BN, given the previously shown
impact of WER on 1-best SU detection. Additionally,
incorporating a larger number of N-best hypotheses has
improved results in all experiments so far, so we would
expect this trend to continue for additional increases, but
time constraints limited our ability to run these larger ex-
periments. One possible explanation for the relatively
small performance gains is that we constrained the con-
fusion network topology so that there was no change in
the word recognition results. We imposed this constraint
in our initial investigations to allow us to compare per-
formance using the same words. It it possible that better
performance could be obtained by using confusion net-
work topologies that link words and metadata.
A more specific breakout of error improvement for the
CTS development set is given in Table 2, showing that
both recall and precision benefit from using the N-best
framework. Including multiple hypotheses reduces the
number of SU deletions (improves recall), but the pri-
mary gain is in reducing insertion errors (higher preci-
sion). The same effect holds for the CTS evaluation set.
Single Best Confusion Nets Change
Deletions 1623 1597 -1.6%
Insertions 872 818 -6.2%
Total 2495 2415 -3.2%
Table 2: Errors for CTS development set
6 Conclusion
Detecting sentence structure in automatic speech recog-
nition provides important information for language pro-
cessing or human understanding. Incorporating multiple
hypotheses from word recognition output can improve
overall detection of SUs in comparison to prediction on a
single hypothesis. This is especially true for CTS, which
suffers more from word errors and can therefore benefit
from considering alternative hypotheses.
Future work will involve a tighter integration of SU de-
tection and word recognition by including SU events di-
rectly in the recognition lattice. This will provide oppor-
tunities to investigate the interaction of automatic word
recognition and structural metadata, hopefully resulting
in reduced WER. We also plan to extend these methods
to additional tasks such as disfluency detection.
Acknowledgments
This work is supported in part by DARPA contract no.
MDA972-02-C-0038, and made use of prosodic feature extrac-
tion and modeling tools developed under NSF-STIMULATE
grant IRI-9619921. Any opinions, conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of these agencies.
References
L. Breiman et al 1984. Classification And Regression Trees.
Wadsworth International Group, Belmont, CA.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctua-
tion annotation using statistical prosody models. In Proc.
ISCA Workshop on Prosody in Speech Recognition and Un-
derstanding.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proc. Eurospeech.
J.-H. Kim and P. Woodland. 2001. The use of prosody in
a combined system for punctuation generation and speech
recognition. In Proc. Eurospeech, pages 2757?2760.
Y. Liu, E. Shriberg, and A. Stolcke. 2003. Automatic disflu-
ency identification in conversational speech using multiple
knowledge sources. In Proc. Eurospeech, volume 1, pages
957?960.
Y. Liu et al 2003. MDE Research at
ICSI+SRI+UW, NIST RT-03F Workshop.
http://www.nist.gov/speech/tests/rt/rt2003/fall/presentations/.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, pages 373?400.
NIST. 2003. RT-03S Workshop Agenda and Presentations.
http://www.nist.gov/speech/tests/rt/rt2003/spring/presentations/.
E. Shriberg et al 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32(1-2):127?154, September.
A. Srivastava and F. Kubala. 2003. Sentence boundary detec-
tion in arabic speech. In Proc. Eurospeech, pages 949?952.
A. Stolcke. 2002. SRILM?an extensible language modeling
toolkit. In Proc. ICSLP, volume 2, pages 901?904.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
Proceedings of NAACL HLT 2007, Companion Volume, pages 65?68,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
??ROVER: Improving System Combination with Classification
D. Hillard?, B. Hoffmeister?, M. Ostendorf?, R. Schlu?ter?, H. Ney?
?SSLI, Electrical Engineering Dept., University of Washington, Seattle, WA
{hillard,mo}@ee.washington.edu
?Informatik 6, Computer Science Dept., RWTH Aachen University, Aachen, Germany
{hoffmeister,schlueter,ney}@cs.rwth-aachen.de
Abstract
We present an improved system combination
technique, ??ROVER. Our approach obtains sig-
nificant improvements over ROVER, and is
consistently better across varying numbers of
component systems. A classifier is trained on
features from the system lattices, and selects
the final word hypothesis by learning cues to
choose the system that is most likely to be
correct at each word location. This approach
achieves the best result published to date on
the TC-STAR 2006 English speech recognition
evaluation set.
1 Introduction
State-of-the-art automatic speech recognition (ASR) sys-
tems today usually include multiple contrasting systems,
which are ultimately combined to produce the final hy-
pothesis. There is consensus that improvements from
combination are usually best when systems are suffi-
ciently different, but there is uncertainty about which sys-
tem combination method performs the best. In addition,
the success of commonly used combination techniques
varies depending on the number of systems that are com-
bined (Hoffmeister et al, 2007). In this work, we develop
a system combination method that outperforms all previ-
ously known techniques and is also robust to the number
of component systems. The relative improvements over
ROVER are particularly large for combination when only
using two systems.
The aim of system combination for ASR is to mini-
mize the expected word error rate (WER) given multiple
system outputs, which are ideally annotated with word
confidence information. The most widely used system
combination approach to date is ROVER (Fiscus, 1997).
It is a simple voting mechanism over just the top hy-
pothesis from each component system. Two alternatives
that incorporate information about multiple hypotheses
and leverage word posterior probabilities are confusion
network (CN) combination (Mangu et al, 2000; Ever-
mann and Woodland, 2000) and minimum Time Frame
Word Error (min-fWER) decoding (Hoffmeister et al,
2006), discussed further in the next section. Previous
work found that among ROVER, CN combination, and
min-fWER combination, no one method was consistently
superior across varying numbers and types of systems
(Hoffmeister et al, 2007).
The main contribution of this work is to develop an
approach that always outperforms other possible system
combination methods. We train a classifier to learn which
system should be selected for each output word, using
features that describe the characteristics of the compo-
nent systems. ROVER alignments on the 1-best hypothe-
ses are used for decoding, but many of the features are
derived from the system lattices. The classifier learns a
selection strategy (i.e. a decision function) from a devel-
opment set and then is able to make better selections on
the evaluation data then the current 1-best or lattice-based
system combination approaches.
Next, Section 2 describes previous work in system
combination techniques. Section 3 describes our ap-
proach, and Section 4 provides experiments and results.
Finally, we summarize the approach and findings in Sec-
tion 5.
2 Previous Work
Previous work in speech recognition system combination
has produced significant improvements over the results
possible with just a single system. The most popular, and
often best performing method is ROVER (Fiscus, 1997),
which selects the word that the most systems agree on
at a particular location (majority voting). An extended
version of ROVER also weights system votes by the word
confidence produced by the system (confidence voting).
Further improvements have been achieved by includ-
ing multiple system alternatives, with methods such as
Confusion Network Combination (CNC) (Evermann and
Woodland, 2000), or N-Best ROVER (Stolcke et al,
2000), which is a special case of CNC. Alternatively, the
combination can be performed at the frame level (min-
fWER) (Hoffmeister et al, 2006). Recent work found
that the best system combination method depended on the
number of systems being combined (Hoffmeister et al,
2007). When only two systems are available, approaches
considering multiple alternatives per system were bet-
65
ter, but as the number of systems increased the standard
ROVER with confidence scores was more robust and
sometimes even better than CNC or min-fWER combi-
nation.
Another approach (Zhang and Rudnicky, 2006) used
two stages of neural networks to select a system at each
word, with features that capture word frequency, posteri-
ors at the frame, word, and utterance level, LM back-off
mode, and system accuracy. They obtained consistent but
small improvements over ROVER: between 0.7 and 1.7%
relative gains for systems with about 30% WER.
3 Approach
We develop a system that uses the ROVER alignment but
learns to consistently make better decisions than those
of standard ROVER. We call the new system ??ROVER,
where the ?? stands for improved results, and/or intelligent
decisions. The following sections discuss the compo-
nents of our approach. First, we emulate the approach
of ROVER in our lattice preprocessing and system align-
ment. We then introduce new methods to extract hypoth-
esis features and train a classifier that selects the best
system at each slot in the alignment.
3.1 Lattice Preparation
Our experiments use lattice sets from four different sites.
Naturally, these lattice sets differ in their vocabulary,
segmentation, and density. A compatible vocabulary is
essential for good combination performance. The main
problems are related to contractions, e.g. ?you?ve? and
?you have?, and the alternatives in writing foreign names,
e.g. ?Schro?der? and ?Schroder?. In ASR this problem is
well-known and is addressed in scoring by using map-
pings that allow alternative forms of the same word.
Such a mapping is provided within the TC-STAR Eval-
uation Campaign and we used it to normalize the lat-
tices. In case of multiple alternative forms we used only
the most frequent one. Allowing multiple parallel alter-
natives would have distorted the posterior probabilities
derived from the lattice. Furthermore, we allowed only
one-to-one or one-to-many mappings. In the latter case
we distributed the time of the lattice arc according to the
character lengths of the target words.
In order to create comparable posterior probabilities
over the lattice sets we pruned them to equal average
density. The least dense lattice set defined the target
density: around 25 for the development and around 30
for the evaluation set.
Finally, we unified the segmentation by concatenat-
ing the lattices recording-wise. The concatenation was
complicated by segmentations with overlapping regions,
but our final concatenated lattices scored equally to the
original lattice sets. The unified segmentation is needed
for lattice-based system combination methods like frame-
based combination.
3.2 System Alignments
In this work we decided to use the ROVER alignment as
the basis for our system combination approach. At first
glance the search space used by ROVER is very limited
because only the first-best hypothesis from each compo-
nent system is used. But the oracle error rate is often very
low, normally less than half of the best system?s error rate.
The ROVER alignment can be interpreted as a confu-
sion network with an equal number of arcs in each slot.
The number of arcs per slot equals the number of compo-
nent systems and thus makes the training and application
of a classifier straightforward.
For the production of the alignments we use a stan-
dard, dynamic programming-based matching algorithm
that minimizes the global cost between two hypothesis.
The local cost function is based on the time overlap of
two words and is identical to the one used by the ROVER
tool. We also did experiments with alternative local cost
functions based on word equalities, but could not outper-
form the simple, time overlap-based distance function.
3.3 Hypothesis Features
We generate a cohort of features for each slot in the
alignment, which is then used as input to train the classi-
fier. The features incorporate knowledge about the scores
from the original systems, as well as comparisons among
each of the systems. The following paragraphs enumerate
the six classes of feature types used in our experiments
(with their names rendered in italics).
The primary, and most important feature class covers
the basic set of features which indicate string matches
among the top hypotheses from each system. In addition,
we include the systems? frame-based word confidence.
These features are all the information available to the
standard ROVER with confidences voting.
An additional class of features provides extended con-
fidence information about each system?s hypothesis. This
feature class includes the confusion network (CN) word
confidence, CN slot entropy, and the number of alter-
natives in the CN slot. The raw language model and
acoustic scores are also available. In addition, it in-
cludes a frame-based confidence that is computed from
only the acoustic model, and a frame-based confidence
that is computed from only the language model score.
Frame-based confidences are calculated from the lattices
according to (Wessel et al, 1998); the CN-algorithm is
an extension of (Xue and Zhao, 2005).
The next class of features describes durational aspects
of the top hypothesis for each system, including: charac-
ter length, frame duration, frames per character, and if the
word is the empty or null word. A feature that normalizes
the frames per character by the average over a window
of ten words is also generated. Here we use characters
as a proxy for phones, because phone information is not
available from all component systems.
We also identify the system dependent top error words
for the development set, as well as the words that occur
to the left and right of the system errors. We encode this
information by indicating if a system word is on the list
of top ten errors or the top one hundred list, and likewise
if the left or right system context word is found in their
corresponding lists.
In order to provide comparisons across systems, we
compute the character distance (the cost of aligning the
words at the character level) between the system words
66
and provide that as a feature. In addition, we include the
confidence of a system word as computed by the frame-
wise posteriors of each of the other systems. This allows
each of the other systems to ?score? the hypothesis of
a system in question. These cross-system confidences
could also act as an indicator for when one system?s hy-
pothesis is an OOV-word for another system. We also
compute the standard, confidence-based ROVER hypoth-
esis at each slot, and indicate whether or not a system
agrees with ROVER?s decision.
The last set of features is computed relative to the
combined min-fWER decoding. A confidence for each
system word is calculated from the combined frame-wise
posteriors of all component systems. The final feature
indicates whether each system word agrees with the com-
bined systems? min-fWER hypothesis.
3.4 Classifier
After producing a set of features to characterize the sys-
tems, we train a classifier with these features that will
decide which system will propose the final hypothesis at
each slot in the multiple alignment. The target classes
include one for each system and a null class (which is
selected when none of the system outputs are chosen, i.e.
a system insertion).
The training data begins with the multiple alignment
of the hypothesis systems, which is then aligned to the
reference words. The learning target for each slot is the
set of systems which match the reference word, or the
null class if no systems match the reference word. Only
slots where there is disagreement between the systems?
1-best hypotheses are included in training and testing.
The classifier for our work is Boostexter (Schapire and
Singer, 2000) using real Adaboost.MH with logistic loss
(which outperformed exponential loss in preliminary ex-
periments). Boostexter trains a series of weak classifiers
(tree stumps), while also updating the weights of each
training sample such that examples that are harder to
classify receive more weight. The weak classifiers are
then combined with the weights learned in training to
predict the most likely class in testing. The main dimen-
sions for model tuning are feature selection and number
of iterations, which are selected on the development set
as described in the next section.
4 Experiments
We first perform experiments using cross-validation on
the development set to determine the impact of different
feature classes, and to select the optimal number of iter-
ations for Boostexter training. We then apply the models
to the evaluation set.
4.1 Experimental setup
In our experiments we combine lattice sets for the English
task of the TC-STAR 2006 Evaluation Campaign from
four sites. The TC-STAR project partners kindly pro-
vided RWTH their development and evaluation lattices.
Systems and lattice sets are described in (Hoffmeister et
al., 2007).
Table 1 summarizes the best results achieved on the
single lattice sets. The latter columns show the results of
Viterbi min-fWER CN
dev eval dev eval dev eval
1 10.5 9.0 10.3 8.6 10.4 8.6
2 11.4 9.0 11.4 9.5 11.6 9.1
3 12.8 10.4 12.5 10.4 12.6 10.2
4 13.9 11.9 13.9 11.8 13.9 11.8
Table 1: WER[%] results for single systems.
CN and min-fWER based posterior decoding (Mangu et
al., 2000; Wessel et al, 2001).
4.2 Feature analysis on development data
We evaluate the various feature classes from Section 3.3
on the development set with a cross validation testing
strategy. The results in Tables 2 and 3 are generated
with ten-fold cross validation, which maintains a clean
separation of training and testing data. The total number
of training samples (alignment slots where there is system
disagreement) is about 3,700 for the 2 system case, 5,500
for the 3 system case, and 6,800 for the 4 system case.
The WER results for different feature conditions on the
development set are presented in Table 2. The typical
ROVER with word confidences is provided in the first
row for comparison, and the remainder of the rows con-
tain the results for various configurations of features that
are made available to the classifier.
The basic features are just those that encode the same
information as ROVER, but the classifier is still able to
learn better decisions than ROVER with only these fea-
tures. Each of the following rows provides the results for
adding a single feature class to the basic features, so that
the impact of each type can be evaluated.
The last two rows contain combinations of feature
classes. First, the best three classes are added, and then
all features. Using just the best three classes achieves
almost the best results, but a small improvement is gained
when all features are added. The number of iterations in
training is also optimized on the development set by se-
lecting the number with the lowest average classification
error across the ten splits of the training data.
Features 2 System 3 System 4 System
ROVER 10.2% 8.8% 9.0%
basic 9.4% 8.6% 8.5%
+confidences 9.3% 8.7% 8.4%
+durational 9.2% 8.6% 8.4%
+top error 9.0% 8.5% 8.4%
+comparisons 8.9% 8.6% 8.4%
+min-fWER 8.5% 8.5% 8.4%
+top+cmp+fWER 8.3% 8.3% 8.2%
all features 8.3% 8.2% 8.2%
Table 2: WER results for development data with different
feature classes.
67
 8
 8.5
 9
 9.5
 10
 10.5
 11
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER (maj.) 10.8% 9.1% 9.1%
ROVER (conf.) 10.1% 8.8% 9.0%
min-fWER 9.6% 9.2 % 8.9 %
??ROVER 8.3% 8.2% 8.2%
oracle 6.5% 5.4% 4.7%
Table 3: WER[%] results for development data with
manual segmentation, and using cross-validation for
??ROVER.
4.3 Results on evaluation data
After analyzing the features and selecting the optimal
number of training iterations on the development data,
we train a final model on the full development set and
then apply it to the evaluation set. In all cases our clas-
sifier achieves a lower WER than ROVER (statistically
significant by NIST matched pairs test). Table 3 and Ta-
ble 4 present a comparison of the ROVER with majority
voting, confidence voting, frame-based combination, and
our improved ROVER (??ROVER).
5 Conclusions
In summary, we develop ??ROVER, a method for sys-
tem combination that outperforms ROVER consistently
across varying numbers of component systems. The rela-
tive improvement compared to ROVER is especially large
for the case of combining two systems (14.5% on the
evaluation set). The relative improvements are larger than
any we know of to date, and the four system case achieves
the best published result on the TC-STAR English evalu-
ation set. The classifier requires relatively little training
data and utilizes features easily available from system
lattices.
Future work will investigate additional classifiers, clas-
sifier combination, and expanded training data. We are
also interested in applying a language model to decode
an alignment network that has been scored with our clas-
sifier.
References
G. Evermann and P. Woodland. 2000. Posterior probability
decoding, confidence estimation and system combination. In
NIST Speech Transcription Workshop.
 6.5
 7
 7.5
 8
 8.5
 9
 9.5
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER(maj.) 9.0% 7.2% 7.3%
ROVER(conf.) 8.2% 7.1% 7.0%
min-fWER 7.6 % 7.4 % 7.2 %
??ROVER 7.1% 6.9% 6.7%
oracle 5.2% 4.1% 3.6%
Table 4: WER[%] results for evaluation data.
J.G. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer Output Voting Error Reduction
(ROVER). In Proc. ASRU.
B. Hoffmeister, T. Klein, R. Schlu?ter, and H. Ney. 2006. Frame
based system combination and a comparison with weighted
ROVER and CNC. In Proc. ICSLP.
B. Hoffmeister, D. Hillard, S. Hahn, R. Schu?lter, M. Ostendorf,
and H. Ney. 2007. Cross-site and intra-site ASR system
combination: Comparisons on lattice and 1-best methods. In
Proc. ICASSP.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, 14:373?400.
R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, J. Zheng,
and F. Weng. 2000. The SRI March 2000 Hub-5 conver-
sational speech transcription system. In NIST Speech Tran-
scription Workshop.
F. Wessel, K. Macherey, and R. Schlu?ter. 1998. Using word
probabilities as confidence measures. In Proc. ICASSP.
F. Wessel, R. Schlu?ter, and H. Ney. 2001. Explicit word error
minimization using word hypothesis posterior probabilities.
In Proc. ICASSP, volume 1.
Jian Xue and Yunxin Zhao. 2005. Improved confusion network
algorithm and shortest path search from word lattice. In
Proc. ICASSP.
R. Zhang and A. Rudnicky. 2006. Investigations of issues
for using multiple acoustic models to improve continuous
speech recognition. In Proc. ICSLP.
68
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73?81,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Minimally-Supervised Extraction of Entities from Text Advertisements
Sameer Singh
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
sameer@cs.umass.edu
Dustin Hillard
Advertising Sciences
Yahoo! Labs Silicon Valley
Santa Clara, CA 95054
dhillard@yahoo-inc.com
Chris Leggetter
Advertising Sciences
Yahoo! Labs Silicon Valley
Santa Clara, CA 95054
cjl@yahoo-inc.com
Abstract
Extraction of entities from ad creatives is an
important problem that can benefit many com-
putational advertising tasks. Supervised and
semi-supervised solutions rely on labeled data
which is expensive, time consuming, and dif-
ficult to procure for ad creatives. A small
set of manually derived constraints on fea-
ture expectations over unlabeled data can be
used to partially and probabilistically label
large amounts of data. Utilizing recent work
in constraint-based semi-supervised learning,
this paper injects light weight supervision
specified as these ?constraints? into a semi-
Markov conditional random field model of en-
tity extraction in ad creatives. Relying solely
on the constraints, the model is trained on a set
of unlabeled ads using an online learning al-
gorithm. We demonstrate significant accuracy
improvements on a manually labeled test set
as compared to a baseline dictionary approach.
We also achieve accuracy that approaches a
fully supervised classifier.
1 Introduction
Growth and competition in web search in recent
years has created an increasing need for improve-
ments in organic and sponsored search. While foun-
dational approaches still focus on matching the exact
words of a search to potential results, there is emerg-
ing need to better understand the underlying intent in
queries and documents. The implicit intent is partic-
ularly important when little text is available, such as
for user queries and advertiser creatives.
This work specifically explores the extraction of
named-entities, i.e. discovering and labeling phrases
in ad creatives. For example, for an ad ?Move to
San Francisco!?, we would like to extract the entity
san francisco and label it a CITY. Similarly, for an
ad ?Find DVD players at Amazon?, we would ex-
tract dvd players as a PRODUCT and amazon as a
ORGNAME. The named-entities provide important
features to downstream tasks about what words and
phrases are important, as well as information on the
intent. Much recent research has focused on extract-
ing useful information from text advertisement cre-
atives that can be used for better retrieval and rank-
ing of ads. Semantic annotation of queries and ad
creatives allows for more powerful retrieval models.
Structured representations of semantics, like the one
studied in our task, can be directly framed as infor-
mation extraction tasks, such as segmentation and
named-entity recognition.
Information extraction methods commonly rely
on labeled data for training the models. The hu-
man labeling of ad creatives would have to pro-
vide the complete segmentation and entity labels for
the ads, which the information extraction algorithm
would then rely on as the truth. For entity extraction
from advertisements this involves familiarity with
a large number of different domains, such as elec-
tronics, transportation, apparel, lodging, sports, din-
ing, services, etc. This leads to an arduous and time
consuming labeling process that can result in noisy
and error-prone data. The problem is further com-
pounded by the inherent ambiguity of the task, lead-
ing to the human editors often presenting conflicting
and incorrect labeling.
Similar problems, to a certain degree, are also
faced by a number of other machine learning tasks
where completely relying on the labeled data leads
to unsatisfactory results. To counter the noisy
and sparse labels, semi-supervised learning meth-
73
ods utilize unlabeled data to improve the model
(see (Chapelle et al, 2006) for an overview). Fur-
thermore, recent work on constraint-based semi-
supervised learning allows domain experts to eas-
ily provide additional light supervision, enabling the
learning algorithm to learn using the prior domain
knowledge, labeled and unlabeled data (Chang et
al., 2007; Mann and McCallum, 2008; Bellare et al,
2009; Singh et al, 2010).
Prior domain knowledge, if it can be easily ex-
pressed and incorporated into the learning algo-
rithm, can often be a high-quality and cheap sub-
stitute for labeled data. For example, previous
work has often used dictionaries or lexicons (lists
of phrases of a particular label) to bootstrap the
model (Agichtein and Ganti, 2004; Canisius and
Sporleder, 2007), leading to a partial labeling of the
data. Domain knowledge can also be more proba-
bilistic in nature, representing the probability of cer-
tain token taking on a certain label. For most tasks,
labeled data is a convenient representation of the do-
main knowledge, but for complex domains such as
structured information extraction from ads, these al-
ternative easily expressible representations may be
as effective as labeled data.
Our approach to solving the the named entity ex-
traction problem for ads relies completely on do-
main knowledge not expressed as labeled data, an
approach that is termed minimally supervised. Each
ad creative is represented as a semi-Markov condi-
tional random field that probabilistically represents
the segmentation and labeling of the creative. Exter-
nal domain knowledge is expressed as a set of targets
for the expectations of a small subset of the features
of the model. We use alternating projections (Bel-
lare et al, 2009) to train our model using this knowl-
edge, relying on the rest of the features of the model
to ?dissipate? the knowledge. Topic model and co-
occurrence based features help this propagation by
generalizing the supervision to a large number of
similar ads.
This method is applied to a large dataset of text
advertisements sampled from a variety of different
domains. The minimally supervised model performs
significantly better than a model that incorporates
the domain knowledge as hard constraints. Our
model also performs competitively when compared
to a supervised model trained on labeled data from a
similar domain (web search queries).
Background material on semi-CRFs and con-
straint based semi-supervised learning is summa-
rized in Section 2. In Section 3, we describe the
problem of named entity recognition in ad creatives
as a semi-CRF, and describe the features in Sec-
tion 4. The constraints that we use to inject super-
vision into our model are listed in Section 5. We
demonstrate the success of our approach in Sec-
tion 6. This work is compared with related literature
in Section 7.
2 Background
This section covers introductory material on
the probabilistic representation of our model
(semi-Markov conditional random fields) and the
constraint-driven semi-supervised method that we
use to inject supervision into the model.
2.1 Semi-Markov Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) use a Markov random field to model the
conditional probability P (y|x). CRFs are com-
monly used to learn sequential models, where the
Markov field is a linear-chain, and y is a linear se-
quence of labels and each label yi ? Y . Let f be a
vector of local feature functions f = ?f1, . . . , fK?,
each of which maps a pair (x,y) and an index i to
a measurement fk(i,x,y) ? <. Let f(i,x,y) be
the vector of these measurements, and let F(x,y) =
?|x|
i f(i,x,y). CRFs use these feature functions in
conjunction with the parameters ? to represent the
conditional probability as follows:
P (y|x, ?) =
1
Z(x)
e??F(x,y)
where Z(x) =
?
y? e
??F(x,y?).
For sequential models where the same labels ap-
pear within a sequence as contiguous blocks (e.g.,
named entity recognition) it is more convenient to
represent these blocks directly as segments. This
representation was formulated as semi-Markov con-
ditional random fields (Semi-CRFs) in (Sarawagi
and Cohen, 2004). The segmentation of a sequence
is represented by s = ?s1, . . . , sp? where each seg-
ment sj = ?tj , uj , yj? consists of a start position
tj , an end position uj , and a label yj ? Y . Similar
to the CRF, let g be the vector of segment feature
74
functions g = ?g1, . . . , gK?, each of which maps
the pair (x, s) and an index j to a measurement
gk(j,x, s) ? <, and G(x, s) =
?|s|
j g(j,x, s). The
conditional probability is represented as:
P (s|x, ?) =
1
Z(x)
e??G(x,s)
where Z(x) =
?
s? e
??G(x,s?). To assert the Marko-
vian assumption, each gk(j,x, s) only computes
features based on x, sj , and yj?11.
An exact inference algorithm was described in
(Sarawagi and Cohen, 2004), and was later im-
proved to be more efficient (Sarawagi, 2006).
2.2 Constraint Driven Learning Using
Alternating Projections
Recent work in semi-supervised learning uses
constraints as external supervision (Chang et al,
2007; Mann and McCallum, 2008; Bellare et al,
2009; Singh et al, 2010). These external constraints
are specified as constraints on the expectations of a
set of auxiliary features g? = {g?1, . . . , g
?
k} over the
unlabeled data. In particular, given the targets u =
{u1, . . . , uk} corresponding to the auxiliary features
g?, the constraints can take different forms, for ex-
ampleL2 penalty ( 12??ui?
?
j Ep[g
?
i(xj , s)]?
2
2 = 0),
L1 box constraints (|ui ?
?
j Ep[g
?
i(xj , s)]| ? ?)
and Affine constraints2 (Ep[g?i(x, s)] ? ui). In this
work, we only use the affine form of the constraints.
For an example, using domain knowledge, we
may know that token ?arizona? should get the label
STATE in at least half of the occurrences in our data.
To capture this, we introduce an auxiliary feature g? :
[[Label=STATE given Token=?arizona?]]. The
affine constraint is written as Ep[g?(x, y)] ? 0.5.
These constraints have been incorporated into
learning using Alternating Projections (Bellare et
al., 2009). Instead of directly optimizing an ob-
jective function that includes the constraints, this
method considers two distributions, p? and q?,?,
where p?(s|x) = 1Z(x)e
??G(x,s) is the usual semi-
Markov model, and q?,? = 1Z(x)e
(??G(x,s)+??G?(x,s))
is an auxiliary distribution that satisfies the con-
straints and has low divergence with the model p?.
1i.e. gk(j,x, s) can be written as gk(yj?1,x, sj)
2where Ep[g] represents the expectation of g over the unla-
beled data using the model p.
In the batch setting, parameters ? and ? are
learned using an EM-like algorithm, where ? is fixed
while optimizing ? and vice versa. Each of the up-
dates in these steps decomposes according to the in-
stances, leading to a stochastic gradient based online
algorithm, as follows:
1. For t = 1, . . . , T , let ? = 1t+t0 where t0 =
1/?0, ?0 the initial learning rate. Let labeled
and unlabeled data set sizes be m and n ? m
respectively. Let the initial parameters be ?0
and ?0, and ? be the weight of L2 regulariza-
tion on ?.
2. For a new labeled instance xt with segmen-
tation st, set ?t = ?t?1 and ?t = ?t?1 +
?
[
g(xt, st)? Ep?t?1 [g(xt, s)]?
??t?1
n
]
.
3. For a new unlabeled instance xt, ?t =
?t?1 + ?
[
u
(n?m) ? Eq?t?1,?t?1 [g
?(xt, s)]
]
and ?t = ?t?1 +
?
[
Eq?t?1,?t?1 [g(xt, s)]? Ep?t?1 [g(xt, s)]?
??t?1
n
]
.
Online training enables scaling the approach to
large data sets, as is the case with ads. In our ap-
proach we rely only on unlabeled data (m = 0, and
step 2 of the above algorithm does not apply).
3 Model
Most text ads consist of a brief title and an ac-
companying abstract that provides additional infor-
mation. The objective of our paper is to extract
the named-entity phrases within these titles and ab-
stracts, then label them with a type from a pre-
determined taxonomy. An example of such an ex-
traction is shown in Fig 1.
We represent the ad creatives as a sequence of
individual tokens, with a special token inserted be-
tween the title and the abstract of the ad. The dis-
tribution over possible phrases and labels of the ad
is expressed as a semi-Markov conditional random
field, as described earlier in Section 2.1.
3.1 Label Taxonomy
In most applications of CRFs and semi-CRFs, the
domain of labels is a fixed set Y , where each label
indexes into one value. Instead, in our approach, we
represent our set of labels as a taxonomy (tree). The
labels higher in the taxonomy are more generic (for
75
Ad Title: Bradley International Airport Hotel
Ad Abstract: Marriott Hartford, CT Airport hotel - free shuttle service & parking.
Output: Bradley International Airport Hotel
Marriott Hartford, CT Airport hotel free shuttle service & parking.
Label Segment
PLACE: AIRPORT Bradley International
BUSINESS: TRAVEL Hotel
ORGNAME: LODGING Marriott
PLACE: CITY Hartford
PLACE: STATE CT
BUSINESS: TRAVEL hotel
PRODUCT: TRAVEL shuttle service & parking.
Figure 1: Example Prediction: An example of an ad creative (title and abstract), along with a set of probable ex-
tracted entities. Note that even in this relatively simple example, there is some ambiguity about what is the correct
segmentation and labeling.
instance, PLACE) and the labels lower in the taxon-
omy are more specific (for instance, STATE may be
a child of PLACE). The taxonomy of labels that we
use for tagging phrases is shown in Figure 2.
When the model predicts a label for a segment,
it can be from any of the levels in the tree. The
benefits of this is multi-fold. First, this allows the
model to be flexible in predicting labels at a lower
(or higher) level based on its confidence. For ex-
ample, the model may have enough evidence to la-
bel ?san francisco? a CITY, however, for ?georgia?
it may not have enough context to discriminate be-
tween STATE or COUNTRY, but could confidently
label it a PLACE. Secondly, this also allows us to
design the features over multiple levels of label gran-
ularity, which leads to a more expressive model. Ex-
pectation constraints can be specified over this ex-
panded set of features, at any level of the taxonomy.
In order to incorporate the nested labels into our
model, we observe that every feature that fires for
a non-leaf label should also fire for all descendants
of that label, e.g. every feature that is active for la-
bel PLACE should also be active for a label CITY,
COUNTRY, etc 3. Following the observation, for ev-
ery feature gk(x, ?tj , uj , yj?) that is active, we also
3Note that this argument works similarly for the taxonomy
represented as a DAG, where the descendants are of a node are
all nodes reachable from it. We do not explore this structure of
the taxonomy in this paper.
fire ?y? ? desc(yj), gk(x, ?tj , uj , y??)4. The same
procedure is applied to the constraints.
4 Features
Our learning algorithm relies on constraints g? as
supervision to extract entities, but even though con-
straints are designed to be generic they do not cover
the whole dataset. The learning algorithm needs
to propagate the supervision to instances where the
constraints are not applicable, guided by the set
of feature functions g. More expressive and rele-
vant features will provide better propagation. Even
though these feature functions represent the ?unsu-
pervised? part of the model (in that they are only
dependent on the unlabeled sequences), they play
an important role in propagating the supervision
throughout the dataset.
4.1 Sequence and Segment Features
Our first set of features are the commonly used
features employed in linear-chain sequence models
such as CRFs and HMMs. These consist of factors
between each token and its corresponding label, and
neighboring labels. They also include transition fac-
tors between the labels. These are local feature func-
tions that are defined only over pairs of token-wise
4This example describes when gk(yj?1,x, sj) ignores
yj?1. For the usual case gk(yj?1,x, sj), features between all
pairs of descendants of yj?1 and yj are enabled.
76
Proper Nouns Common Nouns
PLACE
CITY STATE
COUNTRY CONTINENT
AIRPORT ZIPCODE
PERSON
MANUFACTURER
PRODUCTNAME
MEDIATITLE
EVENT
PRODUCT and BUSINESS
FINANCE MEDIA
EDUCATION APPAREL
TRAVEL AUTO
TECHNOLOGY RESTAURANT
ORGNAME
AIRLINE SPORTSLEAGUE APPAREL AUTO
MEDIA TECHNOLOGY FINANCE LODGING
EDUCATION SPORTSTEAM RESTAURANT
OCCASION
Figure 2: Label Taxonomy: The set of labels that are used are shown grouped by the parent label. PRODUCT and
BUSINESS labels have been merged for brevity, i.e. there are two labels of each child label shown (e.g. PRODUCT:
AUTO and BUSINESS: AUTO). An additional label OTHER is used for the tokens that do not belong to any entities.
labels yj and yj?1. To utilize the semi-Markov rep-
resentation that allows features over the predicted
segmentation, we add the segment length and pre-
fix/suffix tokens of the segment as features.
4.2 Segment Clusters
Although the sequence and segment features cap-
ture a lot of useful information, they are not suffi-
cient for propagation. For example, if we have a
constraint about the token ?london? being a CITY,
but not about ?boston?, the model can only rely on
similar contexts between ?london? and ?boston? to
propagate the information. To allow more compli-
cated propagation to occur, we use features based
on a clustering of segments.
The segment cluster features are based on simi-
larity between segments from English sentences. A
large corpus of English documents were taken from
web, from which 5.1 billion unique sentences were
extracted. Using the co-occurrence of segments in
the sentences as a distance measure, K-Means is
used to identify clusters of segments as described in
(Pantel et al, 2009). The cluster identity of each seg-
ment is added as a feature to the model, capturing
the intuition that segments that appear in the same
cluster should get the same label.
4.3 Topic Model
Most of the ads lie in separate domains with
very little overlap, for example travel and electron-
ics. Additional information about the domain can
be very useful for identifying entities in the ad. For
example, consider the token ?amazon?. It may be
difficult to discern whether the token refers to the
geographical region or the website from just the fea-
tures in the model, however given that the domain
of the ad is travel (or conversely, electronics), the
choice becomes easier.
The problem of domain identification is often
posed as a document classification task, which re-
quires labeled data to train and thus is not applica-
ble for our task. Additionally, we are not concerned
with accurately specifying the exact domain of each
ad, instead any information about similarity between
ads according to their domains is helpful. This kind
of representation can be obtained in an unsupervised
fashion by using topic cluster models (Steyvers and
Griffiths, 2007; Blei et al, 2003). Given a large
set of unlabeled documents, topic models define a
distribution of topics over each document, such that
documents that are similar to each other have similar
topic distributions.
The LDA (Blei et al, 2003) implementation of
topic models in the Mallet toolkit (McCallum, 2002)
was used to construct a model with 1000 topics for
a dataset containing 3 million ads. For each ad, the
discrete distribution over the topics, in conjunction
with each possible label, was added as a feature.
This captures a potential for each label given an ap-
proximation of the ad?s domain captured as topics.
77
5 Constraints
Constraints are used to inject light supervision
into the learning algorithm and are defined as tar-
gets u for expectations of features G? over the data.
Any feature that can be included in the model can be
used as a constraint. This allows us to capture a va-
riety of different forms of domain knowledge, some
of which we shall explore in this section.
Labeled data xl, sl can be incorporated as a spe-
cial case when constraints have a target expectation
of 1.0 for the features that are defined only for the
sequence xl and with segmentation sl. This allows
us to easily use labeled data in form of constraints,
but in this work we do not include any labeled data.
A more interesting case is that of partial labeling,
where the domain expert may have prior knowledge
about the probability that certain tokens and/or con-
texts result in a specific label. These constraints
can cover more instances than labeled data, however
they only provide partial and stochastic labels. All
of the constraints described in this section are also
included as simple features.
Many different methods have been suggested in
recent work for finding the correct target values for
the feature expectations. First, if ample labeled data
is available, features expectations can be calculated,
and assumptions can be made that the same expec-
tations hold for the unlabeled data. This method
cannot be applied to our work due to lack of la-
beled data. Second, for certain constraints, the prior
knowledge can be used directly to specify these val-
ues. Third, if the constraints are an output of a
previous machine learning model, we can use that
model?s confidence in the prediction as the target
expectation of the constraint. Finally, a search for
the ideal values of the target expectations can be
performed by evaluating on small evaluation data.
Our target values for feature expectations were set
based on domain knowledge, then adjusted manu-
ally based on minimal manual examination of ex-
amples on a small held-out data set.
5.1 Dictionary-Based
Dictionary constraints are the form of constraints
that apply to the feature between an individual token
and its label. For a set of tokens in the dictionary, the
constraints specify which label they are likely to be.
Dictionaries can be easily constructed using various
sources, for example product databases, lexicons,
manual collections, or predictions from other mod-
els. These dictionary constraints are often used to
bootstrap models (Agichtein and Ganti, 2004; Cani-
sius and Sporleder, 2007) and have also been used in
the ads domain (Li et al, 2009). For our application,
we rely on dictionary constraints from two sources.
First, the predictions of a previous model are used
to construct a dictionary. A model for entity extrac-
tion is trained on a large amount of labeled search
query data. The domain and style of web queries
differs from advertisements, but the set of labels is
essentially the same. The supervised query entity
extraction model is used to infer segments and la-
bels for the ads domain, and each of the predicted
segments are added to the dictionary of the corre-
sponding predicted label. Even though the predic-
tions of the model are not perfect (see Section 6.1)
the predictions of some of the labels are of high pre-
cision, and thus can be used for supervision in form
of noisy dictionary constraints.
The second source of prior information for dictio-
nary constraints are external databases. Lists of vari-
ous types of places can be obtained easily, for exam-
ple CITY, COUNTRY, STATE, AIRPORT, etc. Ad-
ditionally, product databases available internally to
our research group are used for MANUFACTURERS,
BRANDS, PRODUCTS, MEDIATITLE, etc. Some of
these databases are noisy, and the constraints based
on them are given lower target expectations.
5.2 Pattern-Based
Prior knowledge can often be easily expressed as
patterns that appear for a specific domain. Pattern
based matching has been used to express supervision
for information extraction tasks (Califf and Mooney,
1999; Muslea, 1999). The usual use case involves
a domain expert specifying a number of ?prototyp-
ical? patterns, while additional patterns are discov-
ered based on these initial patterns.
We incorporate noisy forms of patterns as con-
straints. Simple regular expression based patterns
were used to identify and label segments for a few
domains (e.g. ?flights to {PLACE}? and ?looking
for {PRODUCT}??). We do not employ a pattern-
discovery algorithm for finding other contexts; the
model propagates these labels, as before, using the
78
features of the rest of the model. However if the
output of a pattern-discovery algorithm is available,
it can be directly incorporated into the model as ad-
ditional constraints.
5.3 Domain-Based
A number of label-independent constraints are
also added to avoid unrealistic segmentation predic-
tions. For example, an expectation over segment
lengths was included, which denotes that the seg-
ment length is usually 1 or 2, and almost never more
than 6. A constraint is also added to avoid segments
that overlap the separator token between title and
abstract by ensuring that the segment that includes
the separator token is always of length 1 and of la-
bel OTHER. Finally, an additional constraint ensures
that the label OTHER is the most common label.
6 Results
The feature expectations of the model are cal-
culated with modifications to an open source
semi-CRF package5. We collect two datasets of
ad creatives randomly sampled from Yahoo!?s ads
database: a smaller dataset contains 14k ads and a
larger dataset of 42k ads. The ads were not restricted
to any particular domain (such as travel, electronics,
etc.). The average length of the complete ad text
was ?14 tokens. Preprocessing of the text involved
lower-casing, basic cleaning, and stemming.
The training time for each iteration through the
data was ?90 minutes for the smaller dataset and
?360 minutes for the larger dataset. Inference over
the dataset, using Viterbi decoding for semi-CRFs,
took a total of ?8 and ?32 minutes. The initial
learning rate ? is set to 10.0.
6.1 Discussion
We compare our approach to a baseline ?Dictio-
nary? system that deterministically selects a label
based on the dictionaries described in Section 5.1.
A segment is given a label corresponding to the dic-
tionary it appears in, or OTHER if it does not ap-
pear in any dictionary. In addition, we compare to
an external supervised system that has been trained
on tens-of-thousands of manually-annotated search
queries that use the same taxonomy (the same sys-
tem as used in Section 5.1 to derive dictionaries).
5Available on http://crf.sourceforge.net/
This CRF-based model contains mostly the same
features as our unsupervised system, and approxi-
mates what a fully supervised system might achieve,
although it is trained on search queries. Results for
our approach and these two systems are presented
in Table 1. Our evaluation data consists of 2,157
randomly sampled ads that were manually labeled
by professional editors. This labeled data size was
too small to sufficiently train a supervised semi-CRF
model that out-performed the dictionary baseline for
our task (which consists of 45 potential labels).
We measure the token-wise accuracy and macro
F-score over the manually labeled dataset. Typi-
cally, these metrics measure only exact matches be-
tween the true and the predicted label, but this leads
to cases where the model may predict PLACE for a
true CITY. To allow a ?partial credit? for these cases,
we introduce ?weighted? version of these measures,
where a predicted label is given 0.5 credit if the true
label is its direct child or parent, and 0.25 credit if
the true label is a sibling. Our F-score measures the
recall of all true labels except OTHER and similarly
the precision of all predicted labels except OTHER.
We focus on these labels because the OTHER la-
bel is mostly uninformative for downstream tasks.
The token-wise accuracy over all labels (including
OTHER) is included as ?Overall Accuracy?.
Our method significantly outperforms the base-
line dictionary method while approaching the results
obtained with the sophisticated supervised model.
Overall accuracy is 50% greater than the dictionary
baseline, and comes within 10% of the supervised
model6. Increasing unlabeled data from 14k to 42k
ads provides an increase in overall accuracy and
non-OTHER precision, but somewhat reduces recall
for the remaining labels. We also include the F2-
score which gives more weight to recall, because
we are interested in extracting informative labels for
downstream models (which may be able to com-
pensate for a lower precision in label prediction).
Our model trained on 14k samples out-performs the
query-based supervised model in terms of F2, which
is promising for future work that will incorporate
predicted labels in ad retrieval and ranking systems.
6Comparisons and trends for normal and weighted measures
are consistent throughout the results.
79
Table 1: Evaluation: Token-wise accuracy and F-score for the methods evaluated on labeled data (Normal / Weighted)
Metric Dictionary Our Method (14k) Our Method (42k) Query-based Sup. Model
Overall Accuracy 0.454 / 0.466 0.596 / 0.627 0.629 / 0.649 0.665 / 0.685
non-OTHER Recall 0.170 / 0.205 0.329 / 0.412 0.271 / 0.325 0.286 / 0.342
non-OTHER Precision 0.136 / 0.163 0.265 / 0.333 0.297 / 0.357 0.392 / 0.469
F1-score 0.151 / 0.182 0.293 / 0.368 0.283 / 0.340 0.331 / 0.395
F2-score 0.162 / 0.195 0.313 / 0.393 0.276 / 0.331 0.303 / 0.361
7 Related Work
Extraction of structured information from text is
of interest to a large number of communities. How-
ever, in the ads domain, the task has usually been
simplified to that of classification or ranking. Pre-
vious work has focused on retrieval (Raghavan and
Iyer, 2008), user click prediction (Shaparenko et
al., 2009; Richardson et al, 2007; Ciaramita et al,
2008), ad relevance (Hillard et al, 2010) and bounce
rate prediction (Sculley et al, 2009). As far we
know, our method is the only one that aims to solve a
much more complex task of segmentation and entity
extraction from ad creatives. Supervised methods
are a poor choice to solve this task as they require
large amounts of labeled ads, which is expensive,
time-consuming and noisy. Most semi-supervised
methods also rely on some labeled data, and scale
badly with the size of unlabeled data, which is in-
tractable for most ad databases.
Considerable research has been undertaken to ex-
ploit forms of domain knowledge other than la-
beled data to efficiently train a model while utiliz-
ing the unlabeled data. These include methods that
express domain knowledge as constraints on fea-
tures, which have shown to provide high accuracy
on natural language datasets (Chang et al, 2007;
Chang et al, 2008; Mann and McCallum, 2008;
Bellare et al, 2009; Singh et al, 2010). We use
the method of alternating projections for constraint-
driven learning (Bellare et al, 2009) since it spec-
ifies constraints on feature expectations instead of
less intuitive constraints on feature parameters (as
in (Chang et al, 2008)). Additionally, the alternat-
ing projection method is computationally more effi-
cient than Generalized Expectation (Mann and Mc-
Callum, 2008) and can be applied in an online fash-
ion using stochastic gradient.
Our approach is most similar to (Li et al, 2009),
which uses semi-supervised learning for CRFs to ex-
tract structured information from user queries. They
also use a constraint-driven method that utilizes an
external data source. Their method, however, relies
on labeled data for part of the supervision while our
method uses only unlabeled data. Also, evaluation
was only shown for a small domain of user queries,
while our work does not restrict itself to any specific
domain of ads for evaluation.
8 Conclusions
Although important for a number of tasks in spon-
sored search, extraction of structured information
from text advertisements is not a well-studied prob-
lem. The difficulty of the problem lies in the expen-
sive, time-consuming and error-prone labeling pro-
cess. In this work, the aim was to explore machine
learning methods that do not use labeled data, re-
lying instead on light supervision specified as con-
straints on feature expectations. The results clearly
show this minimally-supervised method performs
significantly better than a dictionary based baseline.
Our method also approaches the performance of a
supervised model trained to extract entities from
web search queries. These findings strongly suggest
that domain knowledge expressed in forms other
than directly labeled data may be preferable in do-
mains for which labeling data is unsuitable.
The most important limitation lies in the fact
that specifying the target expectations of constraints
is an ad-hoc process, and robustness of the semi-
supervised learning method to noise in these target
values needs to be investigated. Further research
will also explore using the extracted entities from
advertisements to improve downstream sponsored
search tasks.
80
References
Eugene Agichtein and Venkatesh Ganti. 2004. Min-
ing reference tables for automatic text segmentation.
In KDD: ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 20?29,
New York, NY, USA.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In UAI: Conference on Uncertainty
in Artificial Intelligence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal on Machine
Learning Research, 3:993?1022.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI / IAAI ?99: National conference
on Artificial intelligence and the Innovative Applica-
tions of Artificial Intelligence conference, pages 328?
334.
Sander Canisius and Caroline Sporleder. 2007. Boot-
strapping information extraction from field books.
In EMNLP-CoNLL: Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 827?836.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL: Annual meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI: National Conference on Artificial
Intelligence, pages 1513?1518.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors.
2006. Semi-Supervised Learning (Adaptive Computa-
tion and Machine Learning). The MIT Press, Septem-
ber.
Massimiliano Ciaramita, Vanessa Murdock, and Vassilis
Plachouras. 2008. Online learning from click data for
sponsored search. In WWW: International World Wide
Web Conference.
Dustin Hillard, Stefan Schroedl, Eren Manavoglu, Hema
Raghavan, and Chris Leggetter. 2010. Improving
ad relevance in sponsored search. In WSDM: Inter-
national conference on Web search and data mining,
pages 361?370.
John Lafferty, Andrew Mccallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML:
International Conference on Machine Learning, pages
282?289.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In SIGIR: In-
ternational Conference on research and development
in information retrieval, pages 572?579. ACM.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In ACL: Annual meet-
ing of the Association for Computational Linguistics,
pages 870?878.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Ion Muslea. 1999. Extraction patterns for information
extraction tasks: A survey. In AAAI: Workshop on Ma-
chine Learning for Information Extraction, pages 1?6.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
EMNLP: Conference on Empirical Methods in Natu-
ral Language Processing, pages 938?947.
Hema Raghavan and Rukmini Iyer. 2008. Evaluating
vector-space and probabilistic models for query to ad
matching. In SIGIR Workshop on Information Re-
trieval in Advertising (IRA).
Matthew Richardson, Ewa Dominowska, and Robert
Ragno. 2007. Predicting clicks: estimating the click-
through rate for new ads. In WWW: International
World Wide Web Conference.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS: Neural Information Processing Sys-
tems.
Sunita Sarawagi. 2006. Efficient inference on sequence
segmentation models. In ICML: International Confer-
ence on Machine Learning, pages 793?800.
D. Sculley, Robert G. Malkin, Sugato Basu, and
Roberto J. Bayardo. 2009. Predicting bounce
rates in sponsored search advertisements. In KDD:
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data mining, pages 1325?1334.
Benyah Shaparenko, Ozgur Cetin, and Rukmini Iyer.
2009. Data driven text features for sponsored search
click prediction. In AdKDD: Workshop on Data min-
ing and audience intelligence for advertising.
Sameer Singh, Limin Yao, Sebastian Riedel, and Andrew
McCallum. 2010. Constraint-driven rank-based learn-
ing for information extraction. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT).
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
81
