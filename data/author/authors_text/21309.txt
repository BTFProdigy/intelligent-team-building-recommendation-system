Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77?85,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
SeedLing: Building and using a seed corpus
for the Human Language Project
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri
Universit?at des Saarlandes
66123 Saarbr?ucken, Germany
{emerson, liling, susfert, apalmer, regneri}
@coli.uni-saarland.de
Abstract
A broad-coverage corpus such as the Hu-
man Language Project envisioned by Ab-
ney and Bird (2010) would be a powerful
resource for the study of endangered lan-
guages. Existing corpora are limited in
the range of languages covered, in stan-
dardisation, or in machine-readability. In
this paper we present SeedLing, a seed
corpus for the Human Language Project.
We first survey existing efforts to compile
cross-linguistic resources, then describe
our own approach. To build the foundation
text for a Universal Corpus, we crawl and
clean texts from several web sources that
contain data from a large number of lan-
guages, and convert them into a standard-
ised form consistent with the guidelines
of Abney and Bird (2011). The result-
ing corpus is more easily-accessible and
machine-readable than any of the underly-
ing data sources, and, with data from 1451
languages covering 105 language fami-
lies, represents a significant base corpus
for researchers to draw on and add to in
the future. To demonstrate the utility of
SeedLing for cross-lingual computational
research, we use our data in the test appli-
cation of detecting similar languages.
1 Introduction
At the time of writing, 7105 living languages
are documented in Ethnologue,
1
but Simons and
Lewis (2011) calculated that 37% of extant lan-
guages were at various stages of losing trans-
misson to new generations. Only a fraction
of the world?s languages are well documented,
fewer have machine-readable resources, and fewer
again have resources with linguistic annotations
1
http://www.ethnologue.com
(Maxwell and Hughes, 2006) - so the time to work
on compiling these resources is now.
Several years ago, Abney and Bird (2010; 2011)
posed the challenge of building a Universal Cor-
pus, naming it the Human Language Project. Such
a corpus would include data from all the world?s
languages, in a consistent structure, facilitating
large-scale cross-linguistic processing. The chal-
lenge was issued to the computational linguistics
community, from the perspective that the language
processing, machine learning, and data manipula-
tion and management tools well-known in com-
putational linguistics must be brought to bear on
the problems of documentary linguistics, if we
are to make any serious progress toward build-
ing such a resource. The Universal Corpus as
envisioned would facilitate broadly cross-lingual
natural language processing (NLP), in particular
driving innovation in research addressing NLP for
low-resource languages, which in turn supports
the language documentation process.
We have accepted this challenge and have be-
gun converting existing resources into a format
consistent with Abney and Bird?s specifications.
We aim for a collection of resources that includes
data: (a) from as many languages as possible, and
(b) in a format both in accordance with best prac-
tice archiving recommendations and also readily
accessible for computational methods. Of course,
there are many relevant efforts toward producing
cross-linguistic resources, which we survey in sec-
tion 2. To the best of our knowledge, though, no
existing effort meets these two desiderata to the
extent of our corpus, which we name SeedLing: a
seed corpus for the Human Language Project.
To produce SeedLing, we have drawn on four
web sources, described in section 3.2. To bring
the four resources into a common format and
data structure (section 3.1), each required differ-
ent degrees and types of cleaning and standardis-
ation. We describe the steps required in section 4,
77
presenting each resource as a separate mini-case
study. We hope that the lessons we learned in
assembling our seed corpus can guide future re-
source conversion efforts. To that end, many of the
resources described in section 2 are candidates for
inclusion in the next stage of building a Universal
Corpus.
We believe the resulting corpus, which at
present covers 1451 languages from 105 language
families, is the first of its kind: large enough and
consistent enough to allow broadly multilingual
language processing. To test this claim, we use
SeedLing in a sample application (section 5): the
task of language clustering. With no additional
pre-processing, we extract surface-level features
(frequencies of character n-grams and words) to
estimate the similarity of two languages. Unlike
most previous approaches to the task, we make
no use of resources curated for linguistic typol-
ogy (e.g. values of typological features as in
WALS (Dryer and Haspelmath, 2013), Swadesh
word lists). Despite our approach being highly
dependent on orthography, our clustering perfor-
mance matches the results obtained by Georgi
et al. (2010) using typolological features, which
demonstrates SeedLing?s utility in cross-linguistic
research.
2 Related Work
In this section, we review existing efforts to com-
pile multilingual machine-readable resources. Al-
though some commercial resources are available,
we restrict attention to freely accessible data.
2
Traditional archives. Many archives exist to
store the wealth of traditional resources produced
by the documentary linguistics community. Such
documents are increasingly being digitised, or
produced in a digital form, and there are a number
of archives which now offer free online access.
Some archives aim for a universal scope, such
as The Language Archive (maintained by the
Max Planck Institute of Psycholinguistics), Col-
lection Pangloss (maintained by LACITO), and
The Endangered Languages Archive (maintained
by SOAS). Most archives are regional, including
AILLA, ANLA, PARADISEC, and many others.
However, there are two main problems common
to all of the above data sources. Firstly, the data
2
All figures given below were correct at the time of writ-
ing, but it must be borne in mind that most of these resources
are constantly growing.
is not always machine readable. Even where the
data is available digitally, these often take the form
of scanned images or audio files. While both can
provide invaluable information, they are extremely
difficult to process with a computer, requiring an
impractical level of image or video pre-processing
before linguistic analysis can begin. Even textual
data, which avoids these issues, may not be avail-
able in a machine-readable form, being stored as
pdfs or other opaque formats. Secondly, when data
is machine readable, the format can vary wildly.
This makes automated processing difficult, espe-
cially if one is not aware of the details of each
project. Even when metadata standards and en-
codings agree, there can be idiosyncractic markup
or non-linguistic information, such as labels for
speakers in the transcript of a conversation.
We can see that there is still much work to be
done by individual researchers in digitising and
standardising linguistic data, and it is outside of
the scope of this paper to attempt this for the above
archives. Guidelines for producing new materi-
als are available from the E-MELD project (Elec-
tronic Metastructure for Endangered Languages
Data), which specifically aimed to deal with the
expanding number of standards for linguistic data.
It gives best practice recommendations, illustrated
with eleven case studies, and provides input tools
which link to the GOLD ontology language, and
the OLAC metadata set. Further recommenda-
tions are given by Bird and Simons (2003), who
describe seven dimensions along which the porta-
bility of linguistic data can vary. Various tools are
available from The Language Archive at the Max
Planck Institute for Psycholinguistics.
Many archives are part of the Open Language
Archive Community (OLAC), a subcommunity
of the Open Archives Initiative. OLAC main-
tains a metadata standard, based on the 15-element
Dublin Core, which allows a user to search
through all participating archives in a unified fash-
ion. However, centralising access to disparate re-
sources, while of course extremely helpful, does
not solve the problem of inconsistent standards.
Indeed, it can even be hard to answer simple ques-
tions like ?how many languages are represented??
In short, while traditional archives are invalu-
able for many purposes, for large-scale machine
processing, they leave much to be desired.
Generic corpus collections. Some corpus col-
lections exist which do not focus on endangered
78
languages, but which nonetheless cover an in-
creasing number of languages.
MetaShare (Multilingual Europe Technology
Alliance) provides data in a little over 100 lan-
guages. While language codes are used, they have
not been standardised, so that multiple codes are
used for the same language. Linguistic Data Con-
sortium (LDC) and the European Language Re-
sources Association (ELRA) both offer data in
multiple languages. However, while large in size,
they cover only a limited number of languages.
Furthermore, the corpora they contain are stored
separately, making it difficult to access data ac-
cording to language.
Parallel corpora. The Machine Translation
community has assembled a number of parallel
corpora, which are crucial for statistical machine
translation. The OPUS corpus (Tiedemann, 2012)
subsumes a number of other well-known parallel
corpora, such as Europarl, and covers documents
from 350 languages, with various language pairs.
Web corpora. There has been increasing inter-
est in deriving corpora from the web, due to the
promise of large amounts of data. The majority
of web corpora are however aimed at either one or
a small number of languages, which is perhaps to
be expected, given that the majority of online text
is written in a handful of high-resource languages.
Nonetheless, there have been a few efforts to apply
the same methods to a wider range of languages.
HC Corpora currently provides download of
corpora in 68 different language varieties, which
vary in size from 2M to 150M words. The cor-
pora are thus of a respectable size, but only 1% of
the world?s languages are represented. A further
difficulty is that languages are named, without the
corresponding ISO language codes.
The Leipzig Corpora Collection (LCC)
3
(Bie-
mann et al., 2007) provides download of corpora
in 117 languages, and dictionaries in a number of
others, bringing the total number of represented
languages up to 230. The corpora are large, read-
ily available, in plain-text, and labelled with ISO
language codes.
The Cr?ubad?an Project aims to crawl the web for
text in low-resource languages, and data is cur-
rently available for 1872 languages. This rep-
resents a significant portion of the world?s lan-
guages; unfortunately, due to copyright restric-
3
http://corpora.uni-leipzig.de
tions, only lists of n-grams and their frequencies
are publically available, not the texts themselves.
While the breadth of languages covered makes this
a useful resource for cross-linguistic research, the
lack of actual texts means that only a limited range
of applications are possible with this data.
Cross-linguistic projects. Responding to the
call to document and preserve the world?s lan-
guages, highly cross-linguistic projects have
sprung up, striving towards the aim of universality.
Of particular note are the Endangered Languages
Project, and the Rosetta Project. These projects
are to be praised for their commitment to univer-
sality, but in their current forms it is difficult to use
their data to perform large-scale NLP.
3 The Data
3.1 Universal Corpus and Data Structure
Building on their previous paper, Abney and Bird
(2011) describe the data structure they envisage
for a Universal Corpus in more detail, and we aim
to adopt this structure where possible. Two types
of text are distinguished:
Aligned texts consist of parallel documents,
aligned at the document, sentence, or word level.
Note that monolingual documents are viewed as
aligned texts only tied to a single language.
Analysed texts, in addition to the raw text, con-
tain more detailed annotations including parts of
speech, morphological information, and syntactic
relations. This is stored as a table, where rows rep-
resent words, and columns represent: document
ID, language code, sentence ID, word ID, word-
form, lemma, morphological information, part of
speech, gloss, head/governor, and relation/role.
Out of our data sources, three can be straight-
forwardly represented in the aligned text struc-
ture. However, ODIN contains richer annotations,
which are in fact difficult to fit into Abney and
Bird?s proposal, and which we discuss in section
3.2 below.
3.2 Data Sources
Although data size matters in general NLP, uni-
versality is the top priority for a Universal Corpus.
We focus on the following data sources, because
they include a large number of languages, include
several parallel texts, and demonstrate a variety of
data types which a linguist might encounter (struc-
tured, semi-structured, unstructured): the Online
79
Langs. Families Tokens Size
ODIN 1,270 100 351,161 39 MB
Omniglot 129 20 31,318 677 KB
UDHR 352 46 640,588 5.2 MB
Wikipedia 271 21 37 GB
Combined 1,451 105
Table 1: Corpus Coverage
Database of Interlinear Text (ODIN), the Om-
niglot website, the Universal Declaration of Hu-
man Rights (UHDR), and Wikipedia.
Our resulting corpus runs the full gamut of text
types outlined by Abney and Bird, ranging from
single-language text (Wikipedia) to parallel text
(UDHR and Omniglot) to IGTs (ODIN). Table 1
gives some coverage statistics, and we describe
each source in the following subsections. For 332
languages, the corpus contains data from more
than one source.
Universal Declaration of Human Rights. The
Universal Declaration of Human Rights (UDHR)
is a document released by the United Nations in
1948, and represents the first global expression of
human rights. It consists of 30 articles, amounting
to about four pages of text. This is a useful doc-
ument for NLP, since it has been translated into a
wide variety of languages, providing a highly par-
allel text.
Wikipedia. Wikipedia is a collaboratively-
edited encyclopedia, appealing to use for NLP
because of its large size and easy availability.
At the time of writing, it contained 30.8 million
articles in 286 languages, which provides a
sizeable amount of monolingual text in a fairly
wide range of languages. Text dumps are made
regularly available, and can be downloaded from
http://dumps.wikimedia.org.
Omniglot. The Omniglot website
4
is an online
encyclopedia of writing systems and languages.
We extract information from pages on ?Useful for-
eign phrases? and the ?Tower of Babel? story, both
of which give us parallel data in a reasonably large
number of languages.
ODIN. ODIN (The Online Database of Inter-
linear Text) is a repository of interlinear glossed
texts (IGTs) extracted from scholarly documents
(Lewis, 2006; Lewis and Xia, 2010). Compared to
other resources, it is notable for the breadth of lan-
4
http://www.omniglot.com
guages included and the level of linguistic annota-
tion. An IGT canonically consists of three lines:
(i) the source, a sentence in a target language, (ii)
the gloss, an analysis of each source element, and
(iii) the translation, done at the sentence level. The
gloss line can additionally include a number of lin-
guistic terms, which means that the gloss is written
in metalanguage rather than natural language. In
ODIN, translations are into English, and glosses
are written in an English-based metalanguage. An
accepted set of guidelines are given by the Leipzig
Glossing Rules,
5
where morphemes within words
are separated by hyphens (or equal signs, for cli-
tics), and the same number of hyphens should ap-
pear in each word of the source and gloss.
The data from ODIN poses the first obstacle to
straightforwardly adopting Abney and Bird?s pro-
posal. The suggested data structure is aligned at
the word level, and includes a specific list of rel-
evant features which should be used to annotate
words. When we try to adapt IGTs into this for-
mat, we run into certain problems. Firstly, there
is the problem that the most fundamental unit of
analysis according to the Leipzig Glossing Rules
is the morpheme, not the word. Ideally, we should
encode this information explicitly in a Universal
Corpus, assigning a unique identifier to each mor-
pheme (instead of, or in addition to each word).
Indeed, Haspelmath (2011) argues that there is no
cross-linguistically valid definition of word, which
undermines the central position of words in the
proposed data structure.
Secondly, it is unclear how to represent the
gloss. Since the gloss line is not written in a natu-
ral language, we cannot treat it as a simple trans-
lation. However, it is not straightforward to incor-
porate it into the proposed structure for analysed
texts, either. One possible resolution is to move
all elements of the gloss written in capital letters to
the MORPH field (as functional elements are usu-
ally annotated in this way), and all remaining el-
ements to the GLOSS field. However, this loses
information, since we no longer know which mor-
pheme has which meaning. To keep all informa-
tion encoded in the IGT, we need to modify Abney
and Bird (2011)?s proposal.
The simplest solution we can see is to allow
morphemes to be a level of structure in the Uni-
versal Corpus, just as documents, sentences, and
5
http://www.eva.mpg.de/lingua/
resources/glossing-rules.php
80
Figure 1: Heatmap of languages in SeedLing according to endangerment status
words already are. The overall architecture re-
mains unchanged. We must then decide how to
represent the glosses.
Even though glosses in ODIN are based on
English, having been extracted from English-
language documents, this is not true of IGTs in
general. For example, it is common for documen-
tary linguists working on indigenous languages of
the Americas to provide glosses and translations
based on Spanish. For this reason, we believe it
would be wise to specify the language used to pro-
duce the gloss. Since it is not quite the language
itself, but a metalanguage, one solution would be
to use new language codes that make it clear both
that a metalanguage is being used, and also what
natural language it is based on. The five-letter
code gloss cannot be confused with any code
in any version of ISO 639 (with codes of length
two to four). Following the convention that sub-
varieties of a language are indicated with suffixes,
we can append the code of the natural language.
For example, glosses into English and Spanish-
based metalanguages would be given the codes
gloss-eng and gloss-spa, respectively.
One benefit of this approach is that glossed texts
are treated in exactly the same way as parallel
texts. There is a unique identifier for each mor-
pheme, and glosses are stored under this identifier
and the corresponding gloss code. Furthermore,
to motivate the important place of parallel texts in
a Universal Corpus, Abney and Bird view trans-
lations into a high-resource reference language as
a convenient surrogate of meaning. By the same
reasoning, we can use glosses to provide a more
detailed surrogate of meaning, only written in a
metalanguage instead of a natural one.
3.3 Representation and Universality
According to Ethnologue, there are 7105 liv-
ing languages, and 147 living language families.
Across all our data sources, we manage to cover
1451 languages in 105 families, which represents
19.0% of the world?s languages. To get a bet-
ter idea of the kinds of languages represented,
we give a breakdown according to their EGIDS
scores (Expanded Graded Intergenerational Dis-
ruption Scale) (Lewis and Simons, 2010) in Fig-
ure 1. The values in each cell have been colored
according to proportion of languages represented,
with green indicating good coverage and red poor.
It?s interesting to note that vigorous languages (6a)
are poorly represented across all data sources, and
worse than more endangered categories. In terms
of language documentation, vigorous languages
are less urgent goals than those in categories 6b
and up, but this highlights an unexpected gap in
linguistic resources.
4 Data Clean-Up, Consistency, and
Standardisation
Consistency in data structures and formatting is
essential to facilitate use of data in computational
linguistics research (Palmer et al., 2010). In the
following subsections, we describe the process-
ing required to convert the data into a standardised
form. We then discuss standardisation of language
codes and file formats.
81
4.1 Case Studies
UDHR. We used the plain-text UDHR files
available from the Unicode website
6
which uses
UTF-8 encoding for all languages. The first four
lines of each file record metadata, and the rest is
the translation of the UDHR. This dataset is ex-
tremely clean, and simply required segmentation
into sentences.
Wikipedia. One major issue with using the
Wikipedia dump is the problem of separating text
from abundant source-specific markup. To con-
vert compressed Wikipedia dumps to textfiles, we
used the WikiExtractor
7
tool. After conversion
into textfiles, we used several regular expressions
to delete residual Wikipedia markup and so-called
?magic words?.
8
Omniglot. The main issue with extracting the
Omniglot data is that the pages are designed to
be human-readable, not machine-readable. Clean-
ing this data required parsing the HTML source,
and extracting the relevant content, which required
different code for the two types of page we con-
sidered (?Useful foreign phrases? and ?Tower of
Babel?). Even after automatic extraction, some
noise in the data remained, such as explanatory
notes given in parentheses, which are written in
English and not the target language. Even though
the total amount of data here is small compared to
our other sources, the amount of effort required
to process it was not, because of these idiosyn-
cracies. We expect that researchers seeking to
convert data from human-readable to machine-
readable formats will encounter similar problems,
but unfortunately there is unlikely to be a one-size-
fits-all solution to this problem.
ODIN. The ODIN data is easily accessible in
XML format from the online database
9
. Data
for each language is saved in a separate XML
file and the IGTs are encoded in tags of the form
<igt><example>...</example></igt>.
For example, the IGT in Figure 2 is represented
by the XML snippet in Figure 3.
The primary problem in extracting the data is a
lack of consistency in the IGTs. In the above ex-
6
http://unicode.org/udhr/d
7
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
8
http://en.wikipedia.org/wiki/Help:
Magic_words
9
http://odin.linguistlist.org/download
21 a. o lesu mai
2sg return here
?You return here.?
Figure 2: Fijian IGT from ODIN
<igt>
<example>
<line>21 a. o lesu mai</line>
<line>2sg return here</line>
<line>?You return here.?</line>
</example>
</igt>
Figure 3: Fijian IGT in ODIN?s XML format
amples, the sentence is introduced by a letter or
number, which needs to be removed; however, the
form of such indexing elements varies. In addi-
tion, the source line in Figure 4 includes two types
of metadata: the language name, and a citation,
both of which introduce noise. Finally, extrane-
ous punctuation such as the quotation marks in the
translation line need to be removed. We used regu-
lar expressions for cleaning lines within the IGTs.
4.2 Language Codes
As Xia et al. (2010) explain, language names do
not always suffice to identify languages, since
many names are ambiguous. For this reason, sets
of language codes exist to more accurately identify
languages. We use ISO 639-3
10
as our standard set
of codes, since it aims for universal coverage, and
has widespread acceptance in the community. The
data from ODIN and the UDHR already used this
standard.
To facilitate the standardization of language
codes, we have written a python API that can be
used to query information about a language or a
code, fetching up-to-date information from SIL
International (which maintains the ISO 639-3 code
set), as well as from Ethnologue.
Wikipedia uses its own set of language codes,
most of which are in ISO 639-1 or ISO 639-3.
The older ISO 639-1 codes are easy to recognise,
being two letters long instead of three, and can
be straightforwardly converted. However, a small
number of Wikipedia codes are not ISO codes at
all - we converted these to ISO 639-3, following
10
http://www-01.sil.org/iso639-3/
default.asp
82
<igt>
<example>
<line>(69) na-Na-tmi-kwalca-t
Yimas (Foley 1991)</line>
<line>3sgA-1sgO-say-rise-PERF
</line>
<line>?She woke me up?
(by verbal action)</line>
</example>
</igit>
Figure 4: Yimas IGT in ODIN?s XML format
documentation from the Wikimedia Foundation.
11
Omniglot does not give codes at all, but only the
language name. To resolve this issue, we automat-
ically converted language names to codes using in-
formation from the SIL website.
Some languages have more than one orthog-
raphy. For example, Mandarin Chinese is writ-
ten with either traditional or simplified charac-
ters; Serbian is written with either the Cyrillic or
the Roman alphabet. For cross-linguistic NLP, it
could be helpful to have standard codes to identify
orthographies, but at present none exist.
4.3 File Formats
It is important to make sure that the data we have
compiled will be available to future researchers,
regardless of how the surrounding infrastructure
changes. Bird and Simons (2003) describe a set of
best practices for maintaining portability of digi-
tal information, outlining seven dimensions along
which this can vary. Following this advice, we
have ensured that all our data is available as plain-
text files, with UTF-8 encoding, labelled with the
relevant ISO 639-3 code. Metadata is stored sepa-
rately. This allows users to easily process the data
using the programming language or software of
their choice.
To allow access to the data following Abney
and Bird?s guidelines, as discussed in section 3,
we have written an API, which we distribute along
with the data. Abney and Bird remain agnostic
to the specific file format used, but if an alterna-
tive format would be preferred, the data would
be straightfoward to convert since it can be ac-
cessed according to these guidelines. As exam-
ples of functionality, our API allows a user to fetch
all sentences in a given language, or all sentences
from a given source.
11
http://meta.wikimedia.org/wiki/
Special_language_codes
5 Detecting Similar Languages
To exemplify the use of SeedLing for compu-
tational research on low-resource languages, we
experiment with automatic detection of similar
languages. When working on endangered lan-
guages, documentary and computational linguists
alike face a lack of resources. It is often helpful to
exploit lexical, syntactic or morphological knowl-
edge of related languages. For example, similar
high-resource languages can be used in bootstrap-
ping approaches, such as described by Yarowsky
and Ngai (2001) or Xia and Lewis (2007).
Language classification can be carried out in
various ways. Two common approaches are ge-
nealogical classification, mapping languages onto
family trees according to their historical related-
ness (Swadesh, 1952; Starostin, 2010); and ty-
pological classification, grouping languages ac-
cording to linguistic features (Georgi et al., 2010;
Daum?e III, 2009). Both of these approaches re-
quire linguistic analysis. By contrast, we use
surface features (character n-gram and word uni-
gram frequencies) extracted from SeedLing, and
apply an off-the-shelf hierarchical clustering al-
gorithm.
12
Specifically, each language is repre-
sented as a vector of frequencies of character bi-
grams, character trigrams, and word unigrams.
Each of these three components is normalised to
unit length. Data was taken from ODIN, Om-
niglot, and the UDHR.
Experimental Setup. We first perform hierar-
chical clustering, which produces a tree structure:
each leaf represents a language, and each node
a cluster. We use linkage methods, which recur-
sively build the tree starting from the leaves. Ini-
tially, each language is in a separate cluster, then
we iteratively find the closest two clusters and
merge them. Each time we do this, we take the
two corresponding subtrees, and introduce a new
node to join them.
We define the distance between two clusters by
considering all possible pairs of languages, with
one from each cluster, and taking the largest dis-
tance. We experimented with other ways to de-
fine the distance between clusters, but results were
poor and we omit results for brevity.
To ease evaluation, we produce a partitional
clustering, by stopping when we reach a certain
number of clusters, set in advance.
12
http://www.scipy.org
83
Precision Recall F-score
SeedLing 0.255 0.205 0.150
Base. 1: random 0.184 0.092 0.068
Base. 2: together 0.061 1.000 0.112
Base. 3: separate 1.000 0.086 0.122
Table 2: Clustering compared with baselines
Figure 5: Performance against number of clusters
Evaluation. We compare our clustering to the
language families in Ethnologue. However, there
are many ways to evaluate clustering quality.
Amig?o et al. (2009) propose a set of criteria which
a clustering evaluation metric should satisfy, and
demonstrate that most popular metrics fail to sat-
isfy at least one of these criteria. However, they
prove that all criteria are satisfied by the BCubed
metric, which we therefore adopt. To calculate the
BCubed score, we take the induced cluster and
gold standard class for each language, and cal-
culate the F-score of the cluster compared to the
class. These F-scores are then averaged across all
languages.
In Table 2, we set the number of clusters to be
105, the number of language families in our data,
and compare this with three baselines: a random
baseline (averaged over 20 runs); putting all lan-
guages in a single cluster; and putting each lan-
guage in a separate cluster. Our clustering outper-
forms all baselines. It is worth noting that pre-
cision is higher than recall, which is perhaps ex-
pected, given that related languages using wildly
differing orthographies will appear distinct.
To allow a closer comparison with Georgi et al.
(2010), we calculate pairwise scores - i.e. consid-
ering if pairs of languages are in the same cluster
or the same class. For 105 clusters, we achieve
a pairwise f-score of 0.147, while Georgi et al.
report 0.140. The figures are not quite compa-
rable since we are evaluating over a different set
of languages; nonetheless, we only use surface
features, while Georgi et al. use typological fea-
tures from WALS. This suggests the possibility for
cross-linguistic research to be conducted based on
shallow features.
In Figure 5, we vary the number of clusters. The
highest f-score is obtained for 199 clusters. There
is a notable jump in performance between 98 and
99, just before the true number of families, 105.
Interpreting the clusters directly is difficult, be-
cause they are noisy. However, the distribution of
cluster sizes mirrors the true distribution - for 105
clusters, we have 48 clusters of size 1 or 2, with
the largest cluster of size 130; while in our gold
standard, there are 51 families with only 1 or 2
languages in the data, with the largest of size 150.
6 Conclusion and Outlook
In this paper, we have described the creation of
SeedLing, a foundation text for a Universal Cor-
pus, following the guidelines of Abney and Bird
(2010; 2011). To do this, we cleaned and standard-
ised data from several multilingual data sources:
ODIN, Omniglot, the UDHR, Wikipedia. The
resulting corpus is more easily machine-readable
than any of the underlying data sources, and has
been stored according to the best practices sug-
gested by Bird and Simons (2003). At present,
SeedLing has data from 19% of the world?s liv-
ing languages, covering 72% of language families.
We believe that a corpus with such diversity of lan-
guages, uniformity of format, cleanliness of data,
and ease of access provides an excellent seed for a
Universal Corpus. It is our hope that taking steps
toward creating this resource will spur both further
data contributions and interesting computational
research with cross-linguistic or typological per-
spectives; we have here demonstrated SeedLing?s
utility for such research by using the data to per-
form language clustering, with promising results.
SeedLing (data, API and documentation) is cur-
rently available via a GitHub repository.
13
We
have yet to fully address questions of long-term
access, and we welcome ideas or collaborations
along these lines.
13
https://github.com/alvations/SeedLing
84
Acknowledgements
We thank the three anonymous reviewers for their
helpful comments. This research was supported
in part by the Cluster of Excellence ?Multi-modal
Computing and Interaction? in the German Excel-
lence Initiative.
References
Steven Abney and Steven Bird. 2010. The Hu-
man Language Project: Building a Universal Cor-
pus of the world?s languages. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 88?97. Association for
Computational Linguistics.
Steven Abney and Steven Bird. 2011. Towards a data
model for the Universal Corpus. In Proceedings of
the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 120?127. Association for Computational Lin-
guistics.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461?486.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora
Collection-monolingual corpora of standard size.
Proceedings of Corpus Linguistic 2007.
Steven Bird and Gary Simons. 2003. Seven dimen-
sions of portability for language documentation and
description. Language, pages 557?582.
Hal Daum?e III. 2009. Non-parametric bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Martin Haspelmath. 2011. The indeterminacy of word
segmentation and the nature of morphology and syn-
tax. Folia Linguistica, 45(1):31?80.
M Paul Lewis and Gary F Simons. 2010. Assessing
endangerment: expanding fishman?s GIDS. Revue
roumaine de linguistique, 2:103?119.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
William D Lewis. 2006. ODIN: A model for adapt-
ing and enriching legacy infrastructure. In e-Science
and Grid Computing, 2006. e-Science?06. Second
IEEE International Conference on, pages 137?137.
IEEE.
Mike Maxwell and Baden Hughes. 2006. Frontiers in
linguistic annotation for lower-density languages. In
Proceedings of the workshop on frontiers in linguis-
tically annotated corpora 2006, pages 29?37. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Gary F Simons and M Paul Lewis. 2011. The world?s
languages in crisis: A 20-year update. In 26th
Linguistic Symposium: Language Death, Endanger-
ment, Documentation, and Revitalization. Univer-
sity of Wisconsin, Milwaukee, pages 20?22.
George Starostin. 2010. Preliminary lexicostatistics as
a basis for language classification: a new approach.
Journal of Language Relationship, 3:79?117.
Morris Swadesh. 1952. Lexico-statistic dating of pre-
historic ethnic contacts: with special reference to
north american indians and eskimos. Proceedings of
the American philosophical society, pages 452?463.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In LREC, pages 2214?2218.
Fei Xia and William D Lewis. 2007. Multilingual
structural projection across interlinear text. In HLT-
NAACL, pages 452?459.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of NAACL-2001, pages 200?207.
85
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 30?38,
Coling 2014, Dublin, Ireland, August 24 2014.
SentiMerge: Combining Sentiment Lexicons in a Bayesian Framework
Guy Emerson and Thierry Declerck
DFKI GmbH
Universit?at Campus
66123 Saarbr?ucken
{guy.emerson, thierry.declerck}@dfki.de
Abstract
Many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity.
This is particularly true for languages other than English, where labelled training data is not
easily available. Existing efforts to produce such lexicons exist, and to avoid duplicated effort, a
principled way to combine multiple resources is required. In this paper, we introduce a Bayesian
probabilistic model, which can simultaneously combine polarity scores from several data sources
and estimate the quality of each source. We apply this algorithm to a set of four German sentiment
lexicons, to produce the SentiMerge lexicon, which we make publically available. In a simple
classification task, we show that this lexicon outperforms each of the underlying resources, as
well as a majority vote model.
1 Introduction
Wiegand (2011) describes sentiment analysis as the task of identifying and classifying opinionated con-
tent in natural language text. There are a number of subtasks within this field, such as identifying the
holder of the opinion, and the target of the opinion.
In this paper, however, we are concerned with the more specific task of identifying polar language -
that is, expressing either positive or negative opinions. Throughout the rest of this paper, we will use the
terms sentiment and polarity more or less interchangeably.
As Pang and Lee (2008) explain, sentiment analysis has become a major area of research within natural
language processing (NLP), with many established techniques, and a range of potential applications.
Indeed, in recent years there has been increasing interest in sentiment analysis for commercial purposes.
Despite the rapid growth of this area, there is a lack of gold-standard corpora which can be used to
train supervised models, particularly for languages other than English. Consequently, many algorithms
rely on sentiment lexicons, which provide prior knowledge about which lexical items might indicate
opinionated language. Such lexicons can be used directly to define features in a classifier, or can be
combined with a bootstrapping approach.
However, when presented with a number of overlapping and potentially contradictory sentiment lex-
icons, many machine learning techniques break down, and we therefore require a way to merge them
into a single resource - or else a researcher must choose between resources, and we are left with a leaky
pipeline between resource creation and application. We review methods for combining sources of infor-
mation in section 2, and then describe four German sentiment lexicons in section 3.
To merge these resources, we first want to make them match as closely as possible, and then deal with
the differences that remain. We deal with the first step in section 4, describing how to align the polarity
scores in different lexicons so that they can be directly compared. Then in section 5, we describe how to
combine these scores together.
We report results in section 6, including evaluation against a small annotated corpus, where our merged
resource outperforms both the original resources and also a majority vote baseline. Finally, we discuss
distribution of our resource in section 7, future work in section 8, and conclude in section 9.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
30
Lexicon # Entries
C&K 8714
PolarityClues 9228
SentiWS 1896
SentiSpin 95572
SentiMerge 96918
Table 1: Comparison of lexicon sizes
2 Related Work
A general problem is how to deal with missing data - in our case, we cannot expect every word to
appear in every lexicon. Schafer and Graham (2002) review techniques to deal with missing data, and
recommend two approaches: maximum likelihood estimation and Bayesian multiple imputation. The
latter is a Monte Carlo method, helpful when the marginal probability distribution cannot be calculated
analytically. The probabilistic model presented in section 5.1 is straightforward enough for marginal
probabilities to be calculated directly, and we employ maximum likelihood estimation for this reason.
A second problem is how to combine multiple sources of information, which possibly conflict, and
where some sources are more reliable than others. This becomes particularly challenging in the case
when no gold-standard data exists, and so the sources can not be evaluated directly. Raykar et al. (2010)
discusses this problem from the point of view of crowdsourcing, where there are multiple expert views
and no certain ground truth - but we can equally apply this in the context of sentiment analysis, viewing
each source as an expert. However, unlike their approach, our algorithm does not directly produce a
classifier, but rather a newly labelled resource.
Confronted with a multiplicity of data sources, some researchers have opted to link resources together
(Eckle-Kohler and Gurevych, 2013). Indeed, the lexicons we consider in section 3 have already been
compiled into a common format by Declerck and Krieger (2014). However, while linking resources
makes it easier to access a larger amount of data, it does not solve the problem of how best to process it.
To the best of our knowledge, there has not been a previous attempt to use a probabilistic model to
merge a number of sentiment lexicons into a single resource.
3 Data Sources
In the following subsections, we first describe four existing sentiment lexicons for German. These four
lexicons represent the data we have merged into a single resource, with a size comparison given in table 1,
where we count the number of distinct lemmas, not considering parts of speech. Finally, in section 3.5,
we describe the manually annotated MLSA corpus, which we use for evaluation.
3.1 Clematide and Klenner
Clematide and Klenner (2010) manually curated a lexicon
1
of around 8000 words, based on the synsets
in GermaNet, a WordNet-like database (Hamp and Feldweg, 1997). A semi-automatic approach was used
to extend the lexicon, first generating candidate polar words by searching in a corpus for coordination
with known polar words, and then presenting these words to human annotators. We will refer to this
resource as the C&K lexicon.
3.2 SentimentWortschatz
Remus et al. (2010) compiled a sentiment lexicon
2
from three data sources: a German translation of
Stone et al. (1966)?s General Inquirer lexicon, a set of rated product reviews, and a German collocation
dictionary. At this stage, words have binary polarity: positive or negative. To assign polarity weights,
they use a corpus to calculate the mutual information of a target word with a small set of seed words.
1
http://bics.sentimental.li/index.php/downloads
2
http://asv.informatik.uni-leipzig.de/download/sentiws.html
31
3.3 GermanSentiSpin
Takamura et al. (2005) produced SentiSpin, a sentiment lexicon for English. It is so named becaused
it applies the Ising Model of electron spins. The lexicon is modelled as an undirected graph, with each
word type represented by a single node. A dictionary is used to define edges: two nodes are connected if
one word appears in the other?s definition. Each word is modelled as having either positive or negative
sentiment, analogous to electrons being spin up or spin down. An energy function is defined across the
whole graph, which prefers words to have the same sentiment if they are linked together. By using a
small seed set of words which are manually assigned positive or negative sentiment, this energy function
allows us to propagate sentiment across the entire graph, assigning each word a real-valued sentiment
score in the interval [?1, 1].
Waltinger (2010b) translated the SentiSpin resource into German
3
using an online dictionary, taking
at most three translations of each English word.
3.4 GermanPolarityClues
Waltinger (2010a) utilised automatic translations of two English resources: the SentiSpin lexicon, de-
scribed in section 3.3 above; and the Subjectivity Clues lexicon, a manually annotated lexicon produced
by Wilson et al. (2005). The sentiment orientations of the German translations were then manually
assessed and corrected where necessary, to produce a new resource.
4
3.5 MLSA
To evaluate a sentiment lexicon, separately from the general task of judging the sentiment of an entire
sentence, we relied on the MLSA (Multi-Layered reference corpus for German Sentiment Analysis).
This corpus was produced by Clematide et al. (2012), independently of the above four lexicons, and
consists of 270 sentences annotated at three levels of granularity. In the first layer, annotators judged
the sentiment of whole sentences; in the second layer, the sentiment of words and phrases; and finally in
the third layer, they produced a FrameNet-like analysis of each sentence. The third layer also includes
lemmas, parts of speech, and a syntactic parse.
We extracted the sentiment judgements of individual words from the second layer, using the majority
judgement of the three annotators. Each token was mapped to its lemmatised form and part of speech,
using the information in the third layer. In some cases, the lemma was listed as ambiguous or unknown,
and in these cases, we manually added the correct lemma. Additionally, we changed the annotation of
nominalised verbs from nouns to verbs, to match the lexical entries. Finally, we kept all content words
(nouns, verbs, and adjectives) to form a set of test data. In total, there were 1001 distinct lemma types,
and 1424 tokens. Of these, 378 tokens were annotated as having positive polarity, and 399 as negative.
4 Normalising Scores
By considering positive polarity as a positive real number, and negative polarity as a negative real number,
all of the four data sources give polarity scores between ?1 and 1. However, we cannot assume that the
values directly correspond to one another. For example, does a 0.5 in one source mean the same thing
in another? An example of the kind of data we are trying to combine is given in table 2, and we can see
that the polarity strengths vary wildly between the sources.
The simplest model is to rescale scores linearly, i.e. for each source, we multiply all of its scores by
a constant factor. Intuitively, the factors should be chosen to harmonise the values - a source with large
scores should have them made smaller, and a source with small scores should have them made larger.
4.1 Linear Rescaling for Two Sources
To exemplify our method, we first restrict ourselves to the simpler case of only dealing with two lexicons.
Note that when trying to determine the normalisation factors, we only consider words in the overlap
between the two; otherwise, we would introduce a bias according to what words are considered in each
3
http://www.ulliwaltinger.de/sentiment
4
http://www.ulliwaltinger.de/sentiment
32
Lemma, POS verg?ottern, V
C&K 1.000
PolarityClues 0.333
SentiWS 0.004
SentiSpin 0.245
Table 2: An example lemma, labelled with polarity strengths from each data source
source - it is only in the overlap that we can compare them. However, once these factors have been
determined, we can use them to rescale the scores across the entire lexicon, including items that only
appear in one source.
We consider lemmas with their parts of speech, so that the same orthographic word with two possible
parts of speech is treated as two independent lexical entries, in all of the following calculations. However,
we do not distinguish homophonous or polysemous lemmas within the same part of speech, since none
of our data sources provided different sentiment scores for distinct senses.
For each word i, let u
i
and v
i
be the polarity scores for the two sources. We would like to find
positive real values ? and ? to rescale these to ?u
i
and ?v
i
respectively, minimising the loss function
?
i
(?u
i
? ?v
i
)
2
. Intuitively, we are trying to rescale the sources so that the scores are as similar as
possible. The loss function is trivially minimised when ? = ? = 0, since reducing the sizes of the scores
also reduces their difference. Hence, we can introduce the constraint that ?? = 1, so that we cannot
simultaneously make the values smaller in both sources. We would then like to minimise:
?
i
(
?u
i
?
1
?
v
i
)
2
= |u|
2
?
2
? 2u.v + |v|
2
?
?2
Note that we use vector notation, so that |u|
2
= ?
i
u
2
i
. Differentiating this with respect to ?, we get:
2? |u|
2
? 2 |v|
2
?
?3
= 0 ? ? =
?
|v|
?
|u|
However, observe that we are free to multiply both ? and ? by a constant factor, since this doesn?t
affect the relationship between the two sources, only the overall size of the polarity values. By dividing
by
?
|u| |v|, we derive the simpler expressions ? = |u|
?1
and ? = |v|
?1
, i.e. we should divide by the
root mean square. In other words, after normalising, the average squared polarity value is 1 for both
sources.
5
4.2 Rescaling for Multiple Sources
For multiple sources, the above method needs tweaking. Although we could use the overlap between all
sources, this could potentially be much smaller than the overlap between any two sources, introducing
data sparsity and making the method susceptible to noise. In the given data, 10749 lexical items appear
in at least two sources, but only 1205 appear in all four. We would like to exploit this extra information,
but the missing data means that methods such as linear regression cannot be applied.
A simple solution is to calculate the root mean square values for each pair of sources, and then average
these values for each source. These averaged values define normalisation factors, as a compromise
between the various sources.
4.3 Unspecified scores
Some lexical items in the PolarityClues dataset were not assigned a numerical score, only a polarity
direction. In these cases, the task is not to normalise the score, but to assign one. To do this, we can first
normalise the scores of all other words, as described above. Then, we can consider the words without
scores, and calculate the root mean square polarity of these words in the other sources, and assign them
this value, either positive or negative.
5
In most sentiment lexicons, polarity strengths are at most 1. This will no longer be true after this normalisation.
33
5 Combining Scores
Now that we have normalised scores, we need to calculate a combined value. Here, we take a Bayesian
approach, where we assume that there is a latent ?true? polarity value, and each source is an observation
of this value, plus some noise.
5.1 Gaussian Model
A simple model is to assume that we have a prior distribution of polarity values across the vocabulary,
distributed normally. If we further assume that a language is on average neither positive nor negative,
then this distribution has mean 0. We denote the variance as ?
2
. Each source independently introduces
a linear error term, which we also model with a normal distribution: errors from source a are distributed
with mean 0 and standard deviation ?
2
a
, which varies according to the source.
6
5.2 Hyperparameter Selection
If we observe a subset S = {a
1
, . . . , a
n
} of the sources, the marginal distribution of the observations
will be normally distributed, with mean 0 and covariance matrix as shown below. If the error variances
?
2
a
are small compared to the background variance ?
2
, then this implies a strong correlation between the
observations.
?
?
?
?
?
?
?
2
+ ?
2
a
1
?
2
. . . ?
2
?
2
?
2
+ ?
2
a
2
? ? ? ?
2
.
.
.
.
.
.
.
.
.
.
.
.
?
2
?
2
? ? ? ?
2
+ ?
2
a
n
?
?
?
?
?
?
To choose the values for ?
2
and ?
2
a
, we can aim to maximise the likelihood of the observations, i.e.
maximise the value of the above marginal distributions at the observed points. This is in line with Schafer
and Graham (2002)?s recommendations. Such an optimisation problem can be dealt with using existing
software, such as included in the SciPy
7
package for Python.
5.3 Inference
Given a model as above (whether or not the hyperparameters have been optimised), we can calculate the
posterior distribution of polarity values, given the observations x
a
i
. This again turns out to be normally
distributed, with mean ?? and variance ??
2
given by:
?? =
?
?
?2
a
i
x
a
i
?
?2
+
?
?
?2
a
i
??
?2
= ?
?2
+
?
?
?2
a
i
The mean is almost a weighted average of the observed polarity values, where each source has weight
?
?2
a
. However, there is an additional term ?
?2
in the denominator - this means we can interpret this
as a weighted average if we add an additional polarity value 0, with weight ?
?2
. This additional term
corresponds to the prior.
The weights for each source intuitively mean that we trust sources more if they have less noise. The
extra 0 term from the prior means that we interpret the observations conservatively, skewing values
towards 0 when there are fewer observations. For example, if all sources give a large positive polarity
value, we can be reasonably certain that the true value is also large and positive, but if we only have data
from one source, then we are less certain if this is true - our estimate ?? is correspondingly smaller, and
the posterior variance ??
2
correspondingly larger.
6
Because of the independence assumptions, this model can alternatively be viewed as a Markov Network, where we have
one node to represent the latent true polarity strengths, four nodes to represent observations from each source, and five nodes
to represent the hyperparameters (variances)
7
http://www.scipy.org
34
Figure 1: Gaussian kernel density estimate
6 Experiments and Results
6.1 Parameter Values
The root mean square sentiment values for the sources were: C&K 0.845; PolarityClues 0.608; SentiWS
0.267; and SentiSpin 0.560. We can see that there is a large discrepancy between the sizes of the scores
used, with SentiWS having the smallest of all. It is precisely for this reason that we need to normalise
the scores.
The optimal variances calculated during hyperparameter selection (section 5.2) were: prior 0.528;
C&K 0.328; PolarityClues 0.317; SentiWS 0.446; and SentiSpin 0.609. These values correlate with our
intuition: C&K and PolarityClues have been hand-crafted, and have smaller error variances; SentiWS
was manually finalised, and has a larger error; while finally SentiSpin was automatically generated, and
has the largest error of all, larger in fact than the variance in the prior. We would expect the polarity
values from a hand-crafted source to be more accurate, and this appears to be justified by our analysis.
6.2 Experimental Setup
The MLSA data (see section 3.5) consists of discrete polarity judgements - a word is positive, negative, or
neutral, but nothing in between.
8
To allow direct evaluation against such a resource, we need to discretise
the continuous range of polarity values; i.e. if the polarity value is above some positive threshold, we
judge it to be positive; if it is below a negative threshold, negative; and if it is between the two thresholds,
neutral. To choose this threshold before evaluation, we calculated a Gaussian kernel density estimate of
the polarity values in the entire lexicon, as shown in figure 1. There is a large density near 0, reflecting
that the bulk of the vocabulary is not strongly polar; indeed, so that the density of polar items is clearly
visible, we have chosen a scale that forces this bulk to go off the top of the chart. The high density stops
at around ?0.23, and we have accordingly set this as our threshold.
We compared the merged resource to each of the original lexicons, as well as a ?majority vote? baseline
which represents an alternative method to combine lexicons. This baseline involves considering the
polarity judgements of each lexicon (positive, negative, or neutral), and taking the most common answer.
To break ties, we took the first answer when consulting the lexicons in the following order, reflecting their
reliability: C&K, PolarityClues, SentiWS, SentiSpin.
For the automatically derived resources, we can introduce a threshold as we did for SentiMerge. How-
ever, to make these baselines as competitive as possible, we optimised them on the test data, rather than
choosing them in advance. They were chosen to maximise the macro-averaged f-score. For SentiWS,
the threshold was 0, and for SentiSpin, 0.02.
Note that a perfect score would be impossible to achieve, since 31 lemmas were annotated with more
than polarity type. These cases generally involve polysemous words which could be interpreted with
different polarities depending on the context. Indeed, two words appeared with all three labels: Span-
nung (tension) and Widerstand (resistance). In a political context, interpreting Widerstand as positive or
8
The annotation scheme also allows a further three labels: intensifier, diminisher, and shifter. While this information is
useful, we treat these values as neutral in our evaluation, since we are only concerned with words that have an inherent positive
or negative polarity.
35
Lexicon Precision Recall F-score
C&K 0.754 0.733 0.743
PolarityClues 0.705 0.564 0.626
SentiWS 0.803 0.513 0.621
SentiSpin 0.557 0.668 0.607
majority vote 0.548 0.898 0.679
SentiMerge 0.708 0.815 0.757
Table 3: Performance on MLSA, macro-averaged
negative depends very much on whose side you support. In such cases, a greater context is necessary to
decide on polarity, and a lexicon simply cannot suffice.
6.3 Evaluation on MLSA
We calculated precision, recall, and f-score (the harmonic mean of precision and recall) for both positive
and negative polarity. We report the average of these two scores in 3. We can see that in terms of f-
score, SentiMerge outperforms all four data sources, as well as the majority vote. In applications where
either precision or recall is deemed to be more important, it would be possible to adjust the threshold
accordingly. Indeed, by dropping the threshold to zero, we achieve recall of 0.894, competitive with the
majority vote method; and by increasing the threshold to 0.4, we achieve precision of 0.755, competitive
with the C&K lexicon. Furthermore, in this latter case, the f-score also increases to 0.760. We do not
report this figure in the table above because it would not be possible to predict such a judicious choice
of threshold without peeking at the test data. Nonetheless, this demonstrates that our method is robust to
changes in parameter settings.
The majority vote method performs considerably worse than SentiMerge, at least in terms of f-score.
Indeed, it actually performs worse than the C&K lexicon, with noticeably lower precision. This finding
is consistent with the results of Raykar et al. (2010), who argue against using majority voting, and who
also find that it performs poorly.
The C&K lexicon achieves almost the same level of performance as SentiMerge, so it is reasonable
to ask if there is any point in building a merged lexicon at all. We believe there are two good reasons
for doing this. Firstly, although the C&K lexicon may be the most accurate, it is also small, especially
compared to SentiSpin. SentiMerge thus manages to exploit the complementary nature of the different
lexicons, achieving the broad coverage of SentiSpin, but maintaining the precision of the C&K lexicon
for the most important lexical items.
Secondly, SentiMerge can provide much more accurate values for polarity strength than any human-
annotated resource can. As Clematide and Klenner (2010) show, inter-annotator agreement for polarity
strength is low, even when agreement for polarity direction is high. Nonetheless, some notion of po-
larity strength can still be helpful in computational applications. To demonstrate this, we calculated the
precision, recall, and f-scores again, but weighting each answer as a function of the distance from the
estimated polarity strength to the threshold. With this weighted approach, we get a macro-averaged f-
score of 0.852. This is considerably higher than the results given in table 3, which demonstrates that the
polarity scores in SentiMerge are useful as a measure of classification certainty.
6.4 Manual Inspection
In cases where all sources agree on whether a word is positive or negative, our algorithm simply serves
to assign a more accurate polarity strength. So, it is more interesting to consider those cases where the
sources disagree on polarity direction. Out of the 1205 lexemes for which we have data from all four
sources, only 22 differ between SentiMerge and the C&K lexicon, and only 16 differ between SentiMerge
and PolarityClues. One example is Beschwichtigung (appeasement). Here we can see the problem with
trying to assign a single numeric value to polarity - in a political context, Beschwichtugung could be
interpreted either as positive, since it implies an attempt to ease tension; or as negative, since it could be
36
viewed as a sign of weakness. Another example is unantastbar, which again can be interpreted positively
or negatively.
The controversial words generally denote abstract notions, or have established metaphorical senses.
In the authors? view, their polarity is heavily context-dependent, and a one-dimensional score is not
sufficient to model their contibution to sentiment.
In fact, most of these words have been assigned very small polarity values in the combined lexicon,
which reflects the conflicting evidence present in the various sources. Of the 22 items which differ in
C&K, the one with the largest value in the combined lexicon is dominieren, which has been assigned a
fairly negative combined score, but was rated positive (0.5) in C&K.
7 Distribution
We are making SentiMerge freely available for download. However, with the expanding number of
language resources, it is becoming increasingly important to link resources together, as mentioned in
section 2. For this reason, we are publishing our resource as part of the Linguistic Linked Open Data
9
initiative. In particular, we have decided to follow the specifications set forth by Buitelaar et al. (2013),
who propose a representation for sentiment resources based on Lemon (McCrae et al., 2011) and Marl
(Westerski et al., 2011). Lemon
10
is a model for resource description which builds on LMF (Lexical
Markup Framework),
11
and facilitates combination of lexicons with ontologies. Marl is an an ontology
language designed for sentiment analysis, which has been fully implemented.
12
8 Future Work
To align the disparate sources, a simple linear rescaling was used. However, in principle any monotonic
function could be considered. A more general function that would still be tractable could be u
i
7? ?u
?
i
.
Furthermore, the probabilistic model described in section 5.1 makes several simplifying assumptions,
which could be weaked or modified. For instance, we have assumed a normal distribution, with zero
mean, both for the prior distribution and for the error terms. The data is not perfectly modelled by a
normal distribution, since there are very clear bounds on the polarity scores, and some of the data takes
discrete values. Indeed, we can see in figure 1 that the data is not normally distributed. An alternative
choice of distribution might yield better results.
More generally, our method can be applied to any context where there are multiple resources to be
merged, as long as there is some real-valued property to be aligned.
9 Conclusion
We have described the merging of four sentiment lexicons into a single resource, which we have named
SentiMerge. To demonstrate the utility of the combined lexicon, we set up a word-level sentiment clas-
sification task using the MLSA corpus, in which SentiMerge outperformed all four of the underlying
resources, as well as a majority vote baseline. As a natural by-product of the merging process, we are
also able to indirectly evaluate the quality of each resource, and the results match both intuition and the
performance in the aformentioned classification task. The approach we have taken requires no parameter
setting on the part of the researcher, so we believe that the same method can be applied to other settings
where different language resources present conflicting information. This work helps to bridge the gap be-
tween resource creation efforts, which may overlap in scope, and NLP research, where researchers often
want to use all available data. Furthermore, by grounding our work in a well-defined Bayesian frame-
work, we leave scope for future improvements using more sophisticated probabilistic models. To allow
the community at large to use and build on this work, we are making SentiMerge publically available for
download, and are incorporating it into the Linguistic Linked Open Data initiative.
9
http://linguistics.okfn.org/resources/llod
10
http://lemon-model.net
11
http://www.lexicalmarkupframework.org
12
http://www.gsi.dit.upm.es/ontologies/marl
37
Acknowledgements
This work was co-financed by the European Commission, within the FP7 ICT project TrendMiner,
13
under contract number 287863. We would like to thank the authors of the all the resources mentioned
in this paper for permission to use their data. We also thank the anonymous reviewers for their helpful
comments.
References
Paul Buitelaar, Mihael Arcan, Carlos A Iglesias, J Fernando S?anchez-Rada, and Carlo Strapparava. 2013. Lin-
guistic linked data for sentiment analysis. In Proceedings of the 2nd Workshop on Linked Data in Linguistics.
Simon Clematide and Manfred Klenner. 2010. Evaluation and extension of a polarity lexicon for German. In
Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 7.
Simon Clematide, Stefan Gindl, Manfred Klenner, Stefanos Petrakis, Robert Remus, Josef Ruppenhofer, Ulli
Waltinger, and Michael Wiegand. 2012. MLSA ? a multi-layered reference corpus for German sentiment
analysis. pages 3551?3556. European Language Resources Association (ELRA).
Thierry Declerck and Hans-Ulrich Krieger. 2014. TMO ? the federated ontology of the TrendMiner project. In
Proceedings of the 9th International Language Resources and Evaluation Conference (LREC 2014).
Judith Eckle-Kohler and Iryna Gurevych. 2013. The practitioner?s cookbook for linked lexical resources.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet ? a lexical-semantic net for German. In Proceedings of
the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15. Association for Computational Linguistics.
John McCrae, Dennis Spohr, and Phillip Cimiano. 2011. Linking lexical resources and ontologies on the semantic
web with lemon. In Proceedings of the 8th Extended Semantic Web Conference.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda
Moy. 2010. Learning from crowds. The Journal of Machine Learning Research, 11:1297?1322.
Robert Remus, Uwe Quasthoff, and Gerhard Heyer. 2010. SentiWS ? a publicly available German-language
resource for sentiment analysis. In Proceedings of the 7th International Language Resources and Evaluation
Conference (LREC 2010).
Joseph L Schafer and John W Graham. 2002. Missing data: our view of the state of the art. Psychological
methods, 7(2):147.
Philip J Stone, Dexter C Dunphy, and Marshall S Smith. 1966. The general inquirer: A computer approach to
content analysis.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics.
Ulli Waltinger. 2010a. GermanPolarityClues: A lexical resource for German sentiment analysis. In Proceedings
of the 7th International Language Resources and Evaluation Conference (LREC 2010).
Ulli Waltinger. 2010b. Sentiment analysis reloaded - a comparative study on sentiment polarity identification
combining machine learning and subjectivity features. In Proceedings of the 6th International Conferenceon
Web Information Systems and Technologies (WEBIST 2010). INSTICC Press.
Adam Westerski, Carlos A. Iglesias, and Fernando Tapia. 2011. Linked opinions: Describing sentiments on the
structured web of data. In Proceedings of the 4th International Workshop Social Data on the Web.
Michael Wiegand. 2011. Hybrid approaches for sentiment analysis. PhD dissertation, Universit?at des Saarlandes.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the conference on human language technology and empirical methods in
natural language processing, pages 347?354. Association for Computational Linguistics.
13
http://www.trendminer-project.eu
38
