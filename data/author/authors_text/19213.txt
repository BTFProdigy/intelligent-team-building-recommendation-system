Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370?376,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Feature Learning for Visual Sign Language Identification
Binyam Gebrekidan Gebre
1
, Onno Crasborn
2
, Peter Wittenburg
1
,
Sebastian Drude
1
, Tom Heskes
2
1
Max Planck Institute for Psycholinguistics,
2
Radboud University Nijmegen
bingeb@mpi.nl,o.crasborn@let.ru.nl,peter.wittenburg@mpi.nl,
sebastian.drude@mpi.nl,t.heskes@science.ru.nl
Abstract
Prior research on language identification fo-
cused primarily on text and speech. In this
paper, we focus on the visual modality and
present a method for identifying sign lan-
guages solely from short video samples. The
method is trained on unlabelled video data (un-
supervised feature learning) and using these
features, it is trained to discriminate between
six sign languages (supervised learning). We
ran experiments on short video samples in-
volving 30 signers (about 6 hours in total). Us-
ing leave-one-signer-out cross-validation, our
evaluation shows an average best accuracy of
84%. Given that sign languages are under-
resourced, unsupervised feature learning tech-
niques are the right tools and our results indi-
cate that this is realistic for sign language iden-
tification.
1 Introduction
The task of automatic language identification is
to quickly identify the identity of the language
given utterances. Performing this task is key in
applications involving multiple languages such as
machine translation and information retrieval (e.g.
metadata creation for large audiovisual archives).
Prior research on language identification is
heavily biased towards written and spoken lan-
guages (Dunning, 1994; Zissman, 1996; Li et al,
2007; Singer et al, 2012). While language iden-
tification in signed languages is yet to be studied,
significant progress has been recorded for written
and spoken languages.
Written languages can be identified to about
99% accuracy using Markov models (Dunning,
1994). This accuracy is so high that current
research has shifted to related more challeng-
ing problems: language variety identification
(Zampieri and Gebre, 2012), native language iden-
tification (Tetreault et al, 2013) and identification
at the extremes of scales; many more languages,
smaller training data, shorter document lengths
(Baldwin and Lui, 2010).
Spoken languages can be identified to accura-
cies that range from 79-98% using different mod-
els (Zissman, 1996; Singer et al, 2003). The
methods used in spoken language identification
have also been extended to a related class of prob-
lems: native accent identification (Chen et al,
2001; Choueiter et al, 2008; Wu et al, 2010) and
foreign accent identification (Teixeira et al, 1996).
While some work exists on sign language
recognition
1
(Starner and Pentland, 1997; Starner
et al, 1998; Gavrila, 1999; Cooper et al, 2012),
very little research exists on sign language iden-
tification except for the work by (Gebre et al,
2013), where it is shown that sign language identi-
fication can be done using linguistically motivated
features. Accuracies of 78% and 95% are reported
on signer independent and signer dependent iden-
tification of two sign languages.
This paper has two goals. First, to present a
method to identify sign languages using features
learned by unsupervised techniques (Hinton and
Salakhutdinov, 2006; Coates et al, 2011). Sec-
ond, to evaluate the method on six sign languages
under different conditions.
Our contributions: a) show that unsupervised
feature learning techniques, currently popular in
many pattern recognition problems, also work for
visual sign languages. More specifically, we show
how K-means and sparse autoencoder can be used
to learn features for sign language identification.
b) demonstrate the impact on performance of vary-
ing the number of features (aka, feature maps or
filter sizes), the patch dimensions (from 2D to 3D)
and the number of frames (video length).
1
There is a difference between sign language recognition
and identification. Sign language recognition is the recogni-
tion of the meaning of the signs in a given known sign lan-
guage, whereas sign language identification is the recognition
of the sign language itself from given signs.
370
2 The challenges in sign language
identification
The challenges in sign language identification
arise from three sources as described below.
2.1 Iconicity in sign languages
The relationship between forms and meanings are
not totally arbitrary (Perniss et al, 2010). Both
signed and spoken languages manifest iconicity,
that is forms of words or signs are somehow mo-
tivated by the meaning of the word or sign. While
sign languages show a lot of iconicity in the lex-
icon (Taub, 2001), this has not led to a universal
sign language. The same concept can be iconi-
cally realised by the manual articulators in a way
that conforms to the phonological regularities of
the languages, but still lead to different sign forms.
Iconicity is also used in the morphosyntax and
discourse structure of all sign languages, however,
and there we see many similarities between sign
languages. Both real-world and imaginary objects
and locations are visualised in the space in front
of the signer, and can have an impact on the artic-
ulation of signs in various ways. Also, the use of
constructed action appears to be used in many sign
languages in similar ways. The same holds for the
rich use of non-manual articulators in sentences
and the limited role of facial expressions in the
lexicon: these too make sign languages across the
world very similar in appearance, even though the
meaning of specific articulations may differ (Cras-
born, 2006).
2.2 Differences between signers
Just as speakers have different voices unique to
each individual, signers have also different sign-
ing styles that are likely unique to each individual.
Signers? uniqueness results from how they articu-
late the shapes and movements that are specified
by the linguistic structure of the language. The
variability between signers either in terms of phys-
ical properties (hand sizes, colors, etc) or in terms
of articulation (movements) is such that it does not
affect the understanding of the sign language by
humans, but that it may be difficult for machines
to generalize over multiple individuals. At present
we do not know whether the differences between
signers using the same language are of a similar or
different nature than the differences between dif-
ferent languages. At the level of phonology, there
are few differences between sign languages, but
the differences in the phonetic realization of words
(their articulation) may be much larger.
2.3 Diverse environments
The visual ?activity? of signing comes in a context
of a specific environment. This environment can
include the visual background and camera noises.
The background objects of the video may also in-
clude dynamic objects ? increasing the ambiguity
of signing activity. The properties and configu-
rations of the camera induce variations of scale,
translation, rotation, view, occlusion, etc. These
variations coupled with lighting conditions may
introduce noise. These challenges are by no means
specific to sign interaction, and are found in many
other computer vision tasks.
3 Method
Our method performs two important tasks. First,
it learns a feature representation from patches of
unlabelled raw video data (Hinton and Salakhut-
dinov, 2006; Coates et al, 2011). Second, it looks
for activations of the learned representation (by
convolution) and uses these activations to learn a
classifier to discriminate between sign languages.
3.1 Unsupervised feature learning
Given samples of sign language videos (unknown
sign language with one signer per video), our sys-
tem performs the following steps to learn a feature
representation (note that these video samples are
separate from the video samples that are later used
for classifier learning or testing):
1. Extract patches. Extract small videos (here-
after called patches) randomly from any-
where in the video samples. We fix the
size of the patches such that they all have r
rows, c columns and f frames and we ex-
tract patches m times. This gives us X =
{x
(1)
, x
(1)
, . . . , x
(m)
}, where x
(i)
? R
N
and
N = r?c?f (the size of a patch). For our ex-
periments, we extract 100,000 patches of size
15 ? 15 ? 1 (2D) and 15 ? 15 ? 2 (3D).
2. Normalize the patches. There is evidence
that normalization and whitening (Hyv?arinen
and Oja, 2000) improve performance in un-
supervised feature learning (Coates et al,
2011). We therefore normalize every patch
x
(i)
by subtracting the mean and dividing by
371
Figure 1: Illustration of feature extraction: convolution and pooling.
the standard deviation of its elements. For vi-
sual data, normalization corresponds to local
brightness and contrast normalization.
3. Learn a feature-mapping. Our unsuper-
vised algorithm takes in the normalized and
whitened datasetX = {x
(1)
, x
(1)
, . . . , x
(m)
}
and maps each input vector x
(i)
to a new fea-
ture vector of K features (f : R
N
? R
K
).
We use two unsupervised learning algorithms
a) K-means b) sparse autoencoders.
(a) K-means clustering: we train K-means
to learns K c
(k)
centroids that mini-
mize the distance between data points
and their nearest centroids (Coates and
Ng, 2012). Given the learned centroids
c
(k)
, we measure the distance of each
data point (patch) to the centroids. Natu-
rally, the data points are at different dis-
tances to each centroid, we keep the dis-
tances that are below the average of the
distances and we set the other to zero:
f
k
(x) = max{0, ?(z)? z
k
} (1)
where z
k
= ||x? c
(k)
||
2
and ?(z) is the
mean of the elements of z.
(b) Sparse autoencoder: we train a sin-
gle layer autoencoder with K hid-
den nodes using backpropagation to
minimize squared reconstruction error.
At the hidden layer, the features are
mapped using a rectified linear (ReL)
function (Maas et al, 2013) as follows:
f(x) = g(Wx+ b) (2)
where g(z) = max(z, 0). Note that ReL
nodes have advantages over sigmoid or
tanh functions; they create sparse repre-
sentations and are suitable for naturally
sparse data (Glorot et al, 2011).
From K-means, we get K R
N
centroids and from
the sparse autoencoder, we get W ? R
KxN
and
b ? R
K
filters. We call both the centroids and
filters as the learned features.
3.2 Classifier learning
Given the learned features, the feature mapping
functions and a set of labeled training videos, we
extract features as follows:
1. Convolutional extraction: Extract features
from equally spaced sub-patches covering the
video sample.
2. Pooling: Pool features together over four
non-overlapping regions of the input video to
reduce the number of features. We perform
max pooling for K-means and mean pooling
for the sparse autoencoder over 2D regions
(per frame) and over 3D regions (per all se-
quence of frames).
3. Learning: Learn a linear classifier to predict
the labels given the feature vectors. We use
logistic regression classifier and support vec-
tor machines (Pedregosa et al, 2011).
The extraction of classifier features through
convolution and pooling is illustrated in figure 1.
372
4 Experiments
4.1 Datasets
Our experimental data consist of videos of 30
signers equally divided between six sign lan-
guages: British sign language (BSL), Danish
(DSL), French Belgian (FBSL), Flemish (FSL),
Greek (GSL), and Dutch (NGT). The data for the
unsupervised feature learning comes from half of
the BSL and GSL videos in the Dicta-Sign cor-
pus
2
. Part of the other half, involving 5 signers, is
used along with the other sign language videos for
learning and testing classifiers.
For the unsupervised feature learning, two types
of patches are created: 2D dimensions (15 ? 15)
and 3D (15 ? 15 ? 2). Each type consists of ran-
domly selected 100,000 patches and involves 16
different signers. For the supervised learning, 200
videos (consisting of 1 through 4 frames taken at a
step of 2) are randomly sampled per sign language
per signer (for a total of 6,000 samples).
4.2 Data preprocessing
The data preprocessing stage has two goals.
First, to remove any non-signing signals that re-
main constant within videos of a single sign lan-
guage but that are different across sign languages.
For example, if the background of the videos is
different across sign languages, then classifying
the sign languages could be done with perfection
by using signals from the background. To avoid
this problem, we removed the background by us-
ing background subtraction techniques and manu-
ally selected thresholds.
The second reason for data preprocessing is to
make the input size smaller and uniform. The
videos are colored and their resolutions vary from
320 ? 180 to 720 ? 576. We converted the videos
to grayscale and resized their heights to 144 and
cropped out the central 144 ? 144 patches.
4.3 Evaluation
We evaluate our system in terms of average accu-
racies. We train and test our system in leave-one-
signer-out cross-validation, where videos from
four signers are used for training and videos of the
remaining signer are used for testing. Classifica-
tion algorithms are used with their default settings
and the classification strategy is one-vs.-rest.
2
http://www.dictasign.eu/
5 Results and Discussion
Our best average accuracy (84.03%) is obtained
using 500 K-means features which are extracted
over four frames (taken at a step of 2). This ac-
curacy obtained for six languages is much higher
than the 78% accuracy obtained for two sign lan-
guages (Gebre et al, 2013). The latter uses lin-
guistically motivated features that are extracted
over video lengths of at least 10 seconds. Our sys-
tem uses learned features that are extracted over
much smaller video lengths (about half a second).
All classification accuracies are presented in ta-
ble 5 for 2D and table 5 for 3D. Classification con-
fusions are shown in table 5. Figure 2 shows fea-
tures learned by K-means and sparse autoencoder.
(a) K-means features (b) SAE features
Figure 2: All 100 features learned from 100,000
patches of size 15?15. K-means learned relatively
more curving edges than the sparse auto encoder.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 1
100 69.23 70.60 67.42 73.85 74.53 71.8
300 76.08 77.37 74.80 72.27 70.67 68.90
500 83.03 79.88 77.92 67.50 69.38 66.20
# of frames = 2
100 71.15 72.07 67.42 72.78 74.62 72.08
300 77.33 78.27 76.60 71.85 71.07 68.27
500 83.58 79.50 79.90 67.73 70.15 66.45
# of frames = 3
100 71.42 73.10 67.82 65.70 67.52 63.68
300 78.40 78.57 76.50 72.53 71.68 68.18
500 83.48 80.05 80.57 67.85 70.85 66.77
# of frames = 4
100 71.88 73.05 68.70 64.93 67.48 63.80
300 79.32 78.65 76.42 72.27 72.18 68.35
500 84.03 80.38 80.50 68.25 71.57 67.27
K = # of features, SVM = SVM with linear kernel
LR-L? = Logistic Regression with L1 and L2 penalty
Table 1: 2D filters (15?15): Leave-one-signer-out
cross-validation average accuracies.
373
1 2 3 4 5 6 7 8 9 1012345678910
BSL
1 2 3 4 5 6 7 8 9 1012345678910
DSL
1 2 3 4 5 6 7 8 9 1012345678910
FBSL
1 2 3 4 5 6 7 8 9 1012345678910
FSL
1 2 3 4 5 6 7 8 9 1012345678910
GSL
1 2 3 4 5 6 7 8 9 1012345678910
NGT
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign lan-
guage with respect to each of the 100 filters of the sparse autoencoder. The 100 filters are shown in figure
2(b). Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 2
100 70.63 69.62 68.87 67.40 66.53 65.73
300 73.73 74.05 73.03 72.83 73.48 70.52
500 75.30 76.53 75.40 72.28 74.65 68.72
# of frames = 3
100 72.48 73.30 70.33 68.68 67.40 68.33
300 74.78 74.95 74.77 74.20 74.72 70.85
500 77.27 77.50 76.17 72.40 75.45 69.42
# of frames = 4
100 74.85 73.97 69.23 68.68 67.80 68.80
300 76.23 76.58 74.08 74.43 75.20 70.65
500 79.08 78.63 76.63 73.50 76.23 70.53
Table 2: 3D filters (15?15?2): Leave-one-signer-
out cross-validation average accuracies.
BSL DSL FBSL FSL GSL NGT
BSL 56.11 2.98 1.79 3.38 24.11 11.63
DSL 2.87 92.37 0.95 0.46 3.16 0.18
FBSL 1.48 1.96 79.04 4.69 6.62 6.21
FSL 6.96 2.96 2.06 60.81 18.15 9.07
GSL 5.50 2.55 1.67 2.57 86.05 1.65
NGT 9.08 1.33 3.98 18.76 4.41 62.44
Table 3: Confusion matrix ? confusions averaged
over all settings for K-means and sparse autoen-
coder with 2D and 3D filters (i.e. for all # of
frames, all filter sizes and all classifiers).
Tables 5 and 5 indicate that K-means performs
better with 2D filters and that sparse autoencoder
performs better with 3D filters. Note that features
from 2D filters are pooled over each frame and
concatenated whereas, features from 3D filters are
pooled over all frames.
Which filters are active for which language?
Figure 3 shows visualization of the strength of fil-
ter activation for each sign language. The figure
shows what Lasso looks for when it identifies any
of the six sign languages.
6 Conclusions and Future Work
Given that sign languages are under-resourced,
unsupervised feature learning techniques are the
right tools and our results show that this is realis-
tic for sign language identification.
Future work can extend this work in two direc-
tions: 1) by increasing the number of sign lan-
guages and signers to check the stability of the
learned feature activations and to relate these to
iconicity and signer differences 2) by comparing
our method with deep learning techniques. In our
experiments, we used a single hidden layer of fea-
tures, but it is worth researching into deeper layers
to improve performance and gain more insight into
the hierarchical composition of features.
Other questions for future work. How good are
human beings at identifying sign languages? Can
a machine be used to evaluate the quality of sign
language interpreters by comparing them to a na-
tive language model? The latter question is partic-
ularly important given what happened at the Nel-
son Mandela?s memorial service
3
.
3
http://www.youtube.com/watch?v=X-DxGoIVUWo
374
References
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Tao Chen, Chao Huang, E. Chang, and Jingchun Wang.
2001. Automatic accent identification using gaus-
sian mixture models. In Automatic Speech Recog-
nition and Understanding, 2001. ASRU ?01. IEEE
Workshop on, pages 343?346.
Ghinwa Choueiter, Geoffrey Zweig, and Patrick
Nguyen. 2008. An empirical study of automatic ac-
cent classification. In Acoustics, Speech and Signal
Processing, 2008. ICASSP 2008. IEEE International
Conference on, pages 4265?4268. IEEE.
Adam Coates and Andrew Y Ng. 2012. Learn-
ing feature representations with k-means. In Neu-
ral Networks: Tricks of the Trade, pages 561?580.
Springer.
Adam Coates, Andrew Y Ng, and Honglak Lee. 2011.
An analysis of single-layer networks in unsuper-
vised feature learning. In International Conference
on Artificial Intelligence and Statistics, pages 215?
223.
H. Cooper, E.J. Ong, N. Pugeault, and R. Bowden.
2012. Sign language recognition using sub-units.
Journal of Machine Learning Research, 13:2205?
2231.
Onno Crasborn, 2006. Nonmanual structures in sign
languages, volume 8, pages 668?672. Elsevier, Ox-
ford.
T. Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Dariu M Gavrila. 1999. The visual analysis of human
movement: A survey. Computer vision and image
understanding, 73(1):82?98.
Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom
Heskes. 2013. Automatic sign language identifica-
tion. In Proceedings of ICIP 2013.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507.
Aapo Hyv?arinen and Erkki Oja. 2000. Independent
component analysis: algorithms and applications.
Neural networks, 13(4):411?430.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A
vector space modeling approach to spoken language
identification. Audio, Speech, and Language Pro-
cessing, IEEE Transactions on, 15(1):271?284.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of the ICML.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825?2830.
Pamela Perniss, Robin L Thompson, and Gabriella
Vigliocco. 2010. Iconicity as a general property
of language: evidence from spoken and signed lan-
guages. Frontiers in psychology, 1.
E. Singer, PA Torres-Carrasquillo, TP Gleason,
WM Campbell, and D.A. Reynolds. 2003. Acous-
tic, phonetic, and discriminative approaches to auto-
matic language identification. In Proc. Eurospeech,
volume 9.
E. Singer, P. Torres-Carrasquillo, D. Reynolds, A. Mc-
Cree, F. Richardson, N. Dehak, and D. Sturim.
2012. The mitll nist lre 2011 language recogni-
tion system. In Odyssey 2012-The Speaker and Lan-
guage Recognition Workshop.
Thad Starner and Alex Pentland. 1997. Real-time
american sign language recognition from video us-
ing hidden markov models. In Motion-Based Recog-
nition, pages 227?243. Springer.
Thad Starner, Joshua Weaver, and Alex Pentland.
1998. Real-time american sign language recogni-
tion using desk and wearable computer based video.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1371?1375.
Sarah Taub. 2001. Language from the body: iconicity
and metaphor in American Sign Language. Cam-
bridge University Press, Cambridge.
C. Teixeira, I. Trancoso, and A. Serralheiro. 1996. Ac-
cent identification. In Spoken Language, 1996. IC-
SLP 96. Proceedings., Fourth International Confer-
ence on, volume 3, pages 1784?1787 vol.3.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens,
and Dirk Van Compernolle. 2010. Feature subset
selection for improved native accent identification.
Speech Communication, 52(2):83?98.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2012. Automatic identification of language vari-
eties: The case of portuguese. In Proceedings of
KONVENS, pages 233?237.
375
M.A. Zissman. 1996. Comparison of four approaches
to automatic language identification of telephone
speech. IEEE Transactions on Speech and Audio
Processing, 4(1):31?44.
376
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 7?12,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
A high speed transcription interface for annotating primary linguistic
data
Mark Dingemanse, Jeremy Hammond, Herman Stehouwer,
Aarthy Somasundaram, Sebastian Drude
Max Planck Institute for Psycholinguistics
Nijmegen
{mark.dingemanse, jeremy.hammond, herman.stehouwer,
aarthy.somasundaram, sebastian.drude}@mpi.nl
Abstract
We present a new transcription mode for
the annotation tool ELAN. This mode is
designed to speed up the process of creat-
ing transcriptions of primary linguistic data
(video and/or audio recordings of linguistic
behaviour). We survey the basic transcrip-
tion workflow of some commonly used
tools (Transcriber, BlitzScribe, and ELAN)
and describe how the new transcription in-
terface improves on these existing imple-
mentations. We describe the design of
the transcription interface and explore some
further possibilities for improvement in the
areas of segmentation and computational
enrichment of annotations.
1 Introduction
Recent years have seen an increasing interest in
language documentation: the creation and preser-
vation of multipurpose records of linguistic pri-
mary data (Gippert et al, 2006; Himmelmann,
2008). The increasing availability of portable
recording devices enables the collection of pri-
mary data even in the remotest field sites, and the
exponential growth in storage makes it possible
to store more of this data than ever before. How-
ever, without content annotation for searching and
analysis, such corpora are of limited use. Ad-
vances in machine learning can bring some mea-
sure of automation to the process (Tscho?pel et
al., 2011), but the need for human annotation re-
mains, especially in the case of primary data from
undocumented languages. This paper describes
the development and use of a new rapid transcrip-
tion interface, its integration in an open source
software framework for multimodality research,
and the possibilities it opens up for computational
uses of the annotated data.
Transcription, the production of a written rep-
resentation of audio and video recordings of
communicative behaviour, is one of the most
time-intensive tasks faced by researchers work-
ing with language data. The resulting data is use-
ful in many different scientific fields. Estimates
for the ratio of transcription time to data time
length range from 10:1 or 20:1 for English data
(Tomasello and Stahl, 2004, p. 104), but may
go up to 35:1 for data from lesser known and en-
dangered languages (Auer et al, 2010). As in all
fields of research, time is a most important limit-
ing factor, so any significant improvement in this
area will make available more data and resources
for analysis and model building. The new tran-
scription interface described here is designed for
carrying out high-speed transcription of linguis-
tic audiovisual material, with built-in support for
multiple annotation tiers and for both audio and
video streams.
Basic transcription is only the first step; fur-
ther analysis often necessitates more fine-grained
annotations, for instance part of speech tagging
or morpheme glossing. Such operations are even
more time intensive. Time spent on further an-
notations generally goes well over a 100:1 anno-
tation time to media duration ratio1 (Auer et al,
2010).The post-transcription work is also an area
with numerous possibilities for further reduction
of annotation time by applying semi-automated
annotation suggestions, and some ongoing work
1Cf. a blog post by P.K.Austin http://blogs.usyd.edu.au
/elac/2010/04/how long is a piece of string.html.
7
to integrate such techniques in our annotation sys-
tem is discussed below.
2 Semi-automatic transcription:
terminology and existing tools
Transcription of linguistic primary data has long
been a concern of researchers in linguistics and
neighbouring fields, and accordingly several tools
are available today for time-aligned annotation
and transcription. To describe the different user
interfaces these tools provide, we adopt a model
of the transcription process by (Roy and Roy,
2009), adjusting its terminology to also cover the
use case of transcribing sign language. According
to this model, the transcription of primary linguis-
tic data can be divided into four basic subtasks:
1) find linguistic utterances in the audio or video
stream, 2) segment the stream into short chunks
of utterances, 3) play the segment, and 4) type the
transcription for the segment.
Existing transcription tools implement these
four steps in different ways. To exemplify this we
discuss three such tools below. All three can be
used to create time-aligned annotations of audio
and/or video recordings, but since they have dif-
ferent origins and were created for different goals,
they present the user with interfaces that differ
quite radically.
Transcriber (Barras et al, 2001) was ?designed
for the manual segmentation and transcription of
long duration broadcast news recordings, includ-
ing annotation of speech turns, topics and acoustic
condition? (Barras et al, 2001, p. 5). It provides
a graphical interface with a text editor at the top
and a waveform viewer at the bottom. All four
subtasks from the model above, FSPT, are done
in this same interface. The text editor, where Seg-
menting and Typing are done, is a vertically ori-
ented list of annotations. Strengths of the Tran-
scriber implementation are the top-to-bottom ori-
entation of the text editor, which is in line with
the default layout of transcripts in the discipline,
and the fact that it is possible to rely on only one
input device (the keyboard) for all four subtasks.
Weaknesses are the fact that it does not mark an-
notation ends, only beginnings,and that it treats
the data as a single stream and insists on a strict
partitioning, making it difficult to handle overlap-
ping speech, common in conversational data (Bar-
ras et al, 2001, p. 18).
BlitzScribe (Roy and Roy, 2009) was devel-
oped in the context of the Human Speechome
project at the MIT Media Lab as a custom solu-
tion for the transcription of massive amounts of
unstructured English speech data collected over a
period of three years (Roy et al, 2006). It is not
available to the academic community, but we de-
scribe it here because its user interface presents
significant improvements over previous models.
BlitzScribe uses automatic speech detection for
segmentation, and thus eliminates the first two
steps of the FSPT model, Find and Segment, from
the user interface. The result is a minimalist de-
sign which focuses only on Playing and Typing.
The main strength of BlitzScribe is this stream-
lined interface, which measurably improves tran-
scription speed ? it is about four times as fast as
Transcriber (Roy and Roy, 2009, p. 1649). Weak-
nesses include its monolingual, speech-centric fo-
cus, its lack of a mechanism for speaker identi-
fication, and its single-purpose design which ties
it to the Human Speechome project and makes it
unavailable to the wider academic community.
ELAN (Wittenburg et al, 2006) was developed
as a multimedia linguistic annotation framework.
Unlike most other tools it was built with multi-
modal linguistic data in mind, supporting the si-
multaneous display and annotation of multiple au-
dio and video streams. Its data model is tier-
based, with multiple tiers available for annota-
tions of different speakers or different modalities
(e.g. speech and gesture). Its strengths are its
support for multimodal data, its handling of over-
lapping speech, its flexible tier structure, and its
open source nature. Its noted weaknesses include
a steep learning curve and a user interface that
was, as of 2007, ?not the best place to work on a
?first pass? of a transcript? (Berez, 2007, p. 288).
The new user interface we describe in this pa-
per is integrated in ELAN as a separate ?Tran-
scription Mode?, and was developed to combine
the strengths of existing implementations while at
the same time addressing their weaknesses. Fig-
ure 1 shows a screenshot of the new transcription
mode.
3 Description of the interface
From the default Annotation Mode in ELAN, the
user can switch to several other modes, one of
which is Transcription Mode. Transcription Mode
displays annotations in one or more columns. A
column collects annotations of a single type. For
8
Figure 1: The interface of the transcription mode, showing two columns: transcriptions and the corresponding
translations.
instance, the first column in Figure 1 displays all
annotations of the type ?practical orthography?
in chronological order, colour-coding for differ-
ent speakers. The second column displays cor-
responding, i.e., time aligned, annotations of the
type ?literal translation?. Beside the annotation
columns there is a pane showing the data (video
and/or audio stream) for the selected utterance.
Below it are basic playback parameters like vol-
ume and rate, some essential interface settings,
and a button ?Configure? which brings up the col-
umn selection dialog window. We provide an ex-
ample of this preference pane in Figure 2.
The basic organisation of the Transcription
Mode interface reflects its task-oriented design:
the annotation columns occupy pride of place and
only the most frequently accessed settings are
directly visible. Throughout, the user interface
is keyboard-driven and designed to minimise the
number of actions the user needs to carry out. For
instance, selecting a segment (by mouse or key-
board) will automatically trigger playback of that
segment (the user can play and pause using the
Tab key). Selecting a grey (non-existent) field in
a dependent column will automatically create an
annotation. Selection always opens up the field
for immediate editing. Arrow keys as well as user-
configurable shortcuts move to adjacent fields.
ELAN Transcription Mode improves the tran-
scription workflow by taking apart the FSPT
model and focusing only on the last two steps:
Play and Type. In this respect it is like
BlitzScribe; but it is more advanced than that and
other tools in at least two important ways. First,
it is agnostic to the type of data transcribed. Sec-
ond, it does not presuppose monolingualism and
is ready for multilingual work. It allows the dis-
play of multiple annotation layers and makes for
easy navigation between them.Further, when tran-
scription is done with the help of a native speaker
it allows for them to provide other relevant infor-
mation at the same time (such as cultural back-
ground explanations) keeping primary data and
meta-data time aligned and linked.
Some less prominently visible features of the
user interface design include: the ability to re-
order annotation columns by drag and drop; a tog-
gle for the position of the data streams (to the left
or to the right of the annotation columns); the abil-
ity to detach the video stream (for instance for dis-
play on a secondary monitor); the option to show
names (i.e. participant ID?s) in the flow of anno-
9
Figure 2: The interface of the transcription mode; the configuration dialog.
tations or to indicate them by colour-coding only;
the option to keep the active annotation centered;
and settings for font size and number of columns
(in the ?Configure? pane). These features enable
the user to customise the transcription experience
to their own needs.
The overall design of Transcription Mode
makes the process of transcription as smooth as
possible by removing unnecessary clutter, fore-
grounding the interface elements that matter, and
enabling a limited degree of customisation. Over-
all, the new interface has realised significant
speedups for many people2. User feedback in re-
sponse to the new transcription mode has been
overwhelmingly positive, e.g., the members of
mailing lists such as the Resource Network for
Linguistic Diversity3.
4 A prerequisite: semi-automatic
segmentation
As we noted in the introduction, the most im-
portant step before transcription is that of seg-
mentation (steps Find and Segment in the FSPT
model). Segmentation is a large task that involves
subdividing the audio or video stream in, possi-
bly overlapping, segments. The segments each
denote a distinct period of speech or any other
communicative act and each segment is com-
2Including ourselves, Jeremy Hammond claims that:
?Based on my last two field work trips, I am getting my tran-
scription time down below that of transcriber (but perhaps
not by much) but still keeping the higher level of data that
ELANs tiers provide - probably around 18-20 hours for an
hour of somewhat detailed trilingual annotation.?
3www.rnld.org
monly assigned to a specific speaker. This step
can potentially be sped up significantly by doing
it semi-automatically using pattern recognition
techniques, as pursued in the AVATecH project
(Auer et al, 2010).
In the AVATecH project, audio and video
streams can be sent to detection components
called ?recognisers?. Some detection compo-
nents accept the output of other recognisers as
additional input, next to the audio and/or video
streams, thus facilitating cascaded processing of
these streams. Amongst the tasks that can be per-
formed by these recognisers is the segmentation
of audio and video, including speaker assignment.
A special challenge for the recognisers in this
project is the requirement of language indepen-
dence (in contrast to the English-only situation
in the Human Speechome project that produced
Blitzscribe(Roy et al, 2006)). The recognisers
should ideally accommodate the work of field
linguists and other alike researchers and there-
fore cannot simply apply existing language and
acoustic models. Furthermore, the conditions that
are encountered in the field are often not ideal,
e.g., loud and irregular background noises such as
those from animals are common. Nevertheless,
automatic segmentation has the potential to speed
up the segmentation step greatly.
5 Future possibilities: computational
approaches to data enrichment
While a basic transcription and translation is es-
sential as a first way into the data, it is not suf-
ficient for many research questions, linguistic or
10
otherwise. Typically a morphological segmenta-
tion of the words and a labelling of each individ-
ual morph is required. This level of annotation is
also known as basic glossing (Bow et al, 2003b;
Bow et al, 2003a).
Automatically segmenting the words into their
morphological parts, without resorting to the use
of pre-existing knowledge has seen a wide vari-
ety of research (Hammarstro?m and Borin, 2011).
Based on the knowledge-free induction of mor-
phological boundaries the linguist will usually
perform corrections. Above all, a system must
learn from the input of the linguist, and must in-
corporate it in the results, improving the segmen-
tation of words going forward. However, it is well
known from typological research that languages
differ tremendously in their morphosyntactic or-
ganisation and the specific morphological means
that are employed to construct complex meanings
(Evans and Levinson, 2009; Hocket, 1954).
As far as we know, there is no current morpho-
logical segmentation or glossing system that deals
well with all language types, in particular inflec-
tional and polysynthetic languages or languages
that heavily employ tonal patterns to mark differ-
ent forms of the same word. Therefore, there is
a need for an interactive, modular glossing sys-
tem. For each step of the glossing task, one would
use one, or a set of complementary modules. We
call such modules ?annotyzers?. They generate
content on the basis of the source tiers and addi-
tional data, e.g. lexical data (or learnt states from
earlier passes). Using such modules will result
in a speedup for the researcher. We remark that
there are existing modular NLP systems, such as
GATE(Cunningham et al, 2011), however these
are tied to different workflows, i.e., they are not as
suitable for the multimodal multi-participant an-
notation process.
Currently a limited set of such functionality is
available in Toolbox and FLEX. In the case of
both Toolbox and FLEX the functionality is lim-
ited to a set of rules written by the linguist (i.e.
in a database-lookup approach). Even though
the ELAN modules will offer support for such
rules, our focus is on the automation of machine-
learning systems in order to scale the annotation
process.
Our main aim for the future is to incorporate
learning systems that support the linguists by im-
proving the suggested new annotations on the
bases of choices the linguist made earlier. The
goal there is, again, to reduce annotation time, so
that the linguist can work more on linguistic anal-
ysis and less on annotating. At the same time,
a working set of annotyzers will promote more
standardised glossing, which can then be used for
further automated research, cf. automatic tree-
bank production or similar (Bender et al, 2011).
6 Conclusions
The diversity of the world?s languages is in dan-
ger. Perhaps user interface design is not the first
thing that comes to mind in response to this sober-
ing fact. Yet in a field that increasingly works with
digital annotations of primary linguistic data, it is
imperative that the basic tools for annotation and
transcription are optimally designed to get the job
done.
We have described Transcription Mode, a new
user interface in ELAN that accelerates the tran-
scription process. This interface offers several ad-
vantages compared to similar tools in the software
landscape. It automates actions wherever pos-
sible, displays multiple parallel information and
annotation streams, is controllable with just the
keyboard, and can handle sign language as well
as spoken language data. Transcription Mode re-
duces the required transcription time by providing
an optimised workflow.
The next step is to optimise the preceding and
following stages in the annotation process. Pre-
ceding the transcription stage is segmentation and
speaker labelling, which we address using auto-
matic audio/video recogniser techniques that are
independent of the language that is transcribed.
Following transcription, we aim to support basic
glossing (and similar additional annotations based
on transcriptions) with a modular software archi-
tecture. These semi-automated steps lead to fur-
ther time savings, allowing researchers to focus
on the analysis of language data rather than on the
production of annotations.
The overall goal of the developments described
here is to help researchers working with primary
language data to use their time more optimally.
Ultimately, these improvements will lead to an in-
crease in both quality and quantity of primary data
available for analysis. Better data and better anal-
yses for a stronger digital humanities.
11
References
Eric Auer, Peter Wittenburg, Han Sloetjes, Oliver
Schreer, Stefano Masneri, Daniel Schneider, and
Sebastian Tscho?pel. 2010. Automatic annotation
of media field recordings. In Proceedings of the
ECAI 2010 Workshop on Language Technology for
Cultural Heritage, Social Sciences, and Humanities
(LaTeCH 2010), pages 31?34.
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and
Mark Liberman. 2001. Transcriber: Develop-
ment and use of a tool for assisting speech corpora
production. Speech Communication, 33(1-2):5?22,
January.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and
non-local deep dependencies in a large corpus. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
397?408, Edinburgh, Scotland, UK., July. Associ-
ation for Computational Linguistics.
Andrea L. Berez. 2007. Review of EUDICO linguis-
tic annotator (ELAN). Language Documentation &
Conservation, 1(2):283?289, December.
Catherine Bow, Baden Hughes, and Steven Bird.
2003a. A four-level model for interlinear text.
Cathy Bow, Baden Hughes, and Steven Bird. 2003b.
Towards a general model of interlinear text. In
Proceedings of EMELD Workshop 2003: Digitizing
and Annotating Texts and Field Recordings. Lans-
ing MI, USA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behav-
ioral and Brain Sciences, 32(05):429?448.
Jost Gippert, Nikolaus P. Himmelmann, and Ulrike
Mosel, editors. 2006. Essentials of language docu-
mentation. Mouton de Gruyter, Berlin / New York.
Harald Hammarstro?m and Lars Borin. 2011. Un-
supervised learning of morphology. To Appear in
Computational Linguistics.
Nikolaus P. Himmelmann. 2008. Reproduction and
preservation of linguistic knowledge: Linguistics?
response to language endangerment. In Annual Re-
view of Anthropology, volume 37 (1), pages 337?
350.
Charles F. Hocket. 1954. Two models of grammatical
description. Word 10, pages 210?234.
Chris Rogers. 2010. Review of fieldworks language
explorer (flex) 3.0. In Language Documentation
& Conservation 4, pages 1934?5275. University of
Hawai?i Press.
Brandon C. Roy and Deb Roy. 2009. Fast transcrip-
tion of unstructured audio recordings. In Proceed-
ings of Interspeech 2009, Brighton, England.
Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat,
Michael Fleischman, Brandon C. Roy, Nikolaos
Mavridis, Stefanie Tellex, Alexia Salata, Jethran
Guinness, Micheal Levit, and Peter Gorniak. 2006.
The human speechome project. In Paul Vogt, Yu-
uga Sugita, Elio Tuci, and Chrystopher Nehaniv, ed-
itors, Symbol Grounding and Beyond, volume 4211
of Lecture Notes in Computer Science, pages 192?
196. Springer, Berlin / Heidelberg.
Michael Tomasello and Daniel Stahl. 2004. Sam-
pling children?s spontaneous speech: How much is
enough? Journal of Child Language, 31(01):101?
121.
Sebastian Tscho?pel, Daniel Schneider, Rolf Bardeli,
Peter Wittenburg, Han Sloetjes, Oliver Schreer, Ste-
fano Masneri, Przemek Lenkiewicz, and Eric Auer.
2011. AVATecH: Audio/Video technology for hu-
manities research. Language Technologies for Dig-
ital Humanities and Cultural Heritage, page 86.
Peter Wittenburg, Hennie Brugman, Albert Russel,
and Han Sloetjes. 2006. ELAN: a professional
framework for multimodality research. In Proceed-
ings of LREC 2006.
12
