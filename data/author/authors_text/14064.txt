Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 703?711,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Computational Approach to the Automation of Creative Naming
Go?zde O?zbal
FBK-Irst / Trento, Italy
gozbalde@gmail.com
Carlo Strapparava
FBK-Irst / Trento, Italy
strappa@fbk.eu
Abstract
In this paper, we propose a computational ap-
proach to generate neologisms consisting of
homophonic puns and metaphors based on the
category of the service to be named and the
properties to be underlined. We describe all
the linguistic resources and natural language
processing techniques that we have exploited
for this task. Then, we analyze the perfor-
mance of the system that we have developed.
The empirical results show that our approach
is generally effective and it constitutes a solid
starting point for the automation of the naming
process.
1 Introduction
A catchy, memorable and creative name is an im-
portant key to a successful business since the name
provides the first image and defines the identity of
the service to be promoted. A good name is able to
state the area of competition and communicate the
promise given to customers by evoking semantic as-
sociations. However, finding such a name is a chal-
lenging and time consuming activity, as only few
words (in most cases only one or two) can be used to
fulfill all these objectives at once. Besides, this task
requires a good understanding of the service to be
promoted, creativity and high linguistic skills to be
able to play with words. Furthermore, since many
new products and companies emerge every year, the
naming style is continuously changing and creativ-
ity standards need to be adapted to rapidly changing
requirements.
The creation of a name is both an art and a science
(Keller, 2003). Naming has a precise methodology
and effective names do not come out of the blue. Al-
though it might not be easy to perceive all the effort
behind the naming process just based on the final
output, both a training phase and a long process con-
sisting of many iterations are certainly required for
coming up with a good name.
From a practical point of view, naming agencies
and branding firms, together with automatic name
generators, can be considered as two alternative ser-
vices that facilitate the naming process. However,
while the first type is generally expensive and pro-
cessing can take rather long, the current automatic
generators are rather na??ve in the sense that they are
based on straightforward combinations of random
words. Furthermore, they do not take semantic rea-
soning into account.
To overcome the shortcomings of these two alter-
native ways (i.e. naming agencies and na??ve gener-
ators) that can be used for obtaining name sugges-
tions, we propose a system which combines several
linguistic resources and natural language processing
(NLP) techniques to generate creative names, more
specifically neologisms based on homophonic puns
and metaphors. In this system, similarly to the pre-
viously mentioned generators, users are able to de-
termine the category of the service to be promoted
together with the features to be emphasized. Our
improvement lies in the fact that instead of random
generation, we take semantic, phonetic, lexical and
morphological knowledge into consideration to au-
tomatize the naming process.
Although various resources provide distinct tips
for inventing creative names, no attempt has been
made to combine all means of creativity that can be
used during the naming process. Furthermore, in
addition to the devices stated by copywriters, there
703
might be other latent methods that these experts un-
consciously use. Therefore, we consider the task
of discovering and accumulating all crucial features
of creativity to be essential before attempting to au-
tomatize the naming process. Accordingly, we cre-
ate a gold standard of creative names and the corre-
sponding creative devices that we collect from var-
ious sources. This resource is the starting point of
our research in linguistic creativity for naming.
The rest of the paper is structured as follows.
First, we review the state-of-the-art relevant to the
naming task. Then, we give brief information about
the annotation task that we have conducted. Later
on, we describe the model that we have designed
for the automatization of the naming process. Af-
terwards, we summarize the annotation task that we
have carried out and analyze the performance of
the system with concrete examples by discussing its
virtues and limitations. Finally, we draw conclu-
sions and outline ideas for possible future work.
2 Related Work
In this section, we will analyze the state of the art
concerning the naming task from three different as-
pects: i) linguistic ii) computational iii) commercial.
2.1 Linguistic
Little research has been carried out to investigate
the linguistic aspects of the naming mechanism.
B. V. Bergh (1987) built a four-fold linguistic topol-
ogy consisting of phonetic, orthographic, morpho-
logical and semantic categories to evaluate the fre-
quency of linguistic devices in brand names. Bao
et al (2008) investigated the effects of relevance,
connotation, and pronunciation of brand names on
preferences of consumers. Klink (2000) based
his research on the area of sound symbolism (i.e.
?the direct linkage between sound and meaning?
(Leanne Hinton, 2006)) by investigating whether the
sound of a brand name conveys an inherent mean-
ing and the findings showed that both vowels and
consonants of brand names communicate informa-
tion related to products when no marketing com-
munications are available. Kohli et al (2005) ana-
lyzed consumer evaluations of meaningful and non-
meaningful brand names and the results suggested
that non-meaningful brand names are evaluated less
favorably than meaningful ones even after repeated
exposure. Lastly, cog (2011) focused on the seman-
tics of branding and based on the analysis of several
international brand names, it was shown that cogni-
tive operations such as domain reduction/expansion,
mitigation, and strengthening might be used uncon-
sciously while creating a new brand name.
2.2 Computational
To the best of our knowledge, there is only one com-
putational study in the literature that can be applied
to the automatization of name generation. Stock and
Strapparava (2006) introduce an acronym ironic re-
analyzer and generator called HAHAcronym. This
system both makes fun of existing acronyms, and
produces funny acronyms that are constrained to be
words of the given language by starting from con-
cepts provided by users. HAHAcronym is mainly
based on lexical substitution via semantic field op-
position, rhyme, rhythm and semantic relations such
as antonyms retrieved from WordNet (Stark and
Riesenfeld, 1998) for adjectives.
As more na??ve solutions, automatic name gener-
ators can be used as a source of inspiration in the
brainstorming phase to get ideas for good names.
As an example, www.business-name-generators.
com randomly combines abbreviations, syllables and
generic short words from different domains to ob-
tain creative combinations. The domain genera-
tor on www.namestation.com randomly generates
name ideas and available domains based on allit-
erations, compound words and custom word lists.
Users can determine the prefix and suffix of the
names to be generated. The brand name generator
on www.netsubstance.com takes keywords as in-
puts and here users can configure the percentage of
the shifting of keyword letters. Lastly, the mecha-
nism of www.naming.net is based on name combi-
nations among common words, Greek and Latin pre-
fixes, suffixes and roots, beginning and ending word
parts and rhymes. A shortcoming of these kinds of
automatic generators is that random generation can
output so many bad suggestions and users have to be
patient to find the name that they are looking for. In
addition, these generations are based on straightfor-
ward combinations of words and they do not include
a mechanism to also take semantics into account.
2.3 Commercial
Many naming agencies and branding firms1 provide
professional service to aid with the naming of new
1e.g. www.eatmywords.com, www.designbridge.
com, www.ahundredmonkeys.com
704
products, domains, companies and brands. Such ser-
vices generally require customers to provide brief
information about the business to be named, fill in
questionnaires to learn about their markets, competi-
tors, and expectations. In the end, they present a list
of name candidates to be chosen from. Although the
resulting names can be successful and satisfactory,
these services are very expensive and the processing
time is rather long.
3 Dataset and Annotation
In order to create a gold standard for linguistic cre-
ativity in naming, collect the common creativity de-
vices used in the naming process and determine the
suitable ones for automation, we conducted an an-
notation task on a dataset of 1000 brand and com-
pany names from various domains (O?zbal et al,
2012). These names were compiled from a book
dedicated to brand naming strategies (Botton and
Cegarra, 1990) and various web resources related
to creative naming such as adslogans.co.uk and
brandsandtags.com.
Our list contains names which were invented via
various creativity methods. While the creativity in
some of these names is independent of the context
and the names themselves are sufficient to realize the
methods used (e.g. alliteration in Peak Performance,
modification of one letter in Vimeo), for some of
them the context information such as the description
of the product or the area of the company is also
necessary to fully understand the methods used. For
instance, Thanks a Latte is a coffee bar name where
the phonetic similarity between ?lot? and ?latte? (a
coffee type meaning ?milk? in Italian) is exploited.
The name Caterpillar, which is an earth-moving
equipment company, is used as a metaphor. There-
fore, we need extra information regarding the do-
main description in addition to the names. Accord-
ingly, while building our dataset, we conducted two
separate branches of annotation. The first branch re-
quired the annotators to fill in the domain descrip-
tion of the names in question together with their et-
ymologies if required, while the second asked them
to determine the devices of creativity used in each
name.
In order to obtain the list of creativity devices, we
collected a total of 31 attributes used in the naming
process from various resources including academic
papers, naming agents, branding and advertisement
experts. To facilitate the task for the annotators,
we subsumed the most similar attributes when re-
quired. Adopting the four-fold linguistic topology
suggested by Bergh et al (B. V. Bergh, 1987), we
mapped these attributes into phonetic, orthographic,
morphological and semantic categories. The pho-
netic category includes attributes such as rhyme (i.e.
repetition of similar sounds in two or more words
- e.g. Etch-a-sketch) and reduplication (i.e. repeat-
ing the root or stem of a word or part of it exactly
or with a slight change - e.g. Teenie Weenie), while
the orthographic category consists of devices such as
acronyms (e.g. BMW) and palindromes (i.e. words,
phrases, numbers that can be read the same way in
either direction e.g. Honda ?Civic?). The third cat-
egory is the morphology which contains affixation
(i.e. forming different words by adding morphemes
at the beginning, middle or end of words - e.g.
Nutella) and blending (i.e. forming a word by blend-
ing sounds from two or more distinct words and
combining their meanings - e.g. Wikipedia by blend-
ing ?Wiki? and ?encyclopedia?). Finally, the seman-
tic category includes attributes such as metaphors
(i.e. Expressing an idea through the image of another
object - e.g. Virgin) and punning (i.e. using a word
in different senses or words with sound similarity to
achieve specific effect such as humor - e.g. Thai Me
Up for a Thai restaurant).
4 System Description
The resource that we have obtained after the anno-
tation task provides us with a starting point to study
and try to replicate the linguistic and cognitive pro-
cesses behind the creation of a successful name. Ac-
cordingly, we have made a systematic attempt to
replicate these processes, and implemented a system
which combines methods and resources used in var-
ious areas of Natural Language Processing (NLP) to
create neologisms based on homophonic puns and
metaphors. While the variety of creativity devices
is actually much bigger, our work can be consid-
ered as a starting point to investigate which kinds of
technologies can successfully be exploited in which
way to support the naming process. The task that we
deal with requires: 1) reasoning of relations between
entities and concepts; 2) understanding the desired
properties of entities determined by users; 3) identi-
fying semantically related terms which are also con-
sistent with the objectives of the advertisement; 4)
finding terms which are suitable metaphors for the
properties that need to be emphasized; 5) reasoning
705
about phonetic properties of words; 6) combining
all this information to create natural sounding neol-
ogisms.
In this section, we will describe in detail the work
flow of the system that we have designed and imple-
mented to fulfill these requirements.
4.1 Specifying the category and properties
Our design allows users to determine the category
of the product/brand/company to be advertised (e.g.
shampoo, car, chocolate) optionally together with
the properties (e.g. softening, comfortable, addic-
tive) that they want to emphasize. In the current
implementation, categories are required to be nouns
while properties are required to be adjectives. These
inputs that are specified by users constitute the main
ingredients of the naming process. After the de-
termination of these ingredients, several techniques
and resources are utilized to enlarge the ingredient
list, and thereby to increase the variety of new and
creative names.
4.2 Adding common sense knowledge
After the word defining the category is determined
by the user, we need to automatically retrieve more
information about this word. For instance, if the cat-
egory has been determined as ?shampoo?, we need
to learn that ?it is used for washing hair? or ?it
can be found in the bathroom?, so that all this ex-
tra information can be included in the naming pro-
cess. To achieve that, we use ConceptNet (Liu and
Singh, 2004), which is a semantic network contain-
ing common sense, cultural and scientific knowl-
edge. This resource consists of nodes representing
concepts which are in the form of words or short
phrases of natural language, and labeled relations
between them.
ConceptNet has a closed class of relations ex-
pressing connections between concepts. After the
analysis of these relations according to the require-
ments of the task, we have decided to use the ones
listed in Table 1 together with their description in
the second column. The third column states whether
the category word should be the first or second ar-
gument of the relation in order for us to consider
the new word that we discover with that relation.
Since, for instance, the relations MadeOf(milk, *)
and MadeOf(*, milk) can be used for different goals
(the former to obtain the ingredients of milk, and
the latter to obtain products containing milk), we
Relation Description # POS
HasA What does it possess? 1 n
PartOf What is it part of? 2 n
UsedFor What do you use it for? 1 n,v
AtLocation Where would you find it? 2 n
MadeOf What is it made of 1 n
CreatedBy How do you bring it into existence? 1 n
HasSubevent What do you do to accomplish it? 2 v
Causes What does it make happen? 1 n,v
Desires What does it want? 1 n,v
CausesDesire What does it make you want to do? 1 n,v
HasProperty What properties does it have? 1 a
ReceivesAction What can you do to it? 1 v
Table 1: ConceptNet relations.
need to make this differentiation. Via ConceptNet 5,
the latest version of ConceptNet, we obtain a list of
relations such as AtLocation(shampoo, bathroom),
UsedFor(shampoo, clean) and MadeOf(shampoo,
perfume) with the query word ?shampoo?. We add
all the words appearing in relations with the category
word to our ingredient list. Among these new words,
multiwords are filtered out since most of them are
noisy and for our task a high precision is more im-
portant than a high recall.
Since sense information is not provided, one of
the major problems in utilizing ConceptNet is the
difficulty in disambiguating the concepts. In our
current design, we only consider the most common
senses of words. As another problem, the part-of-
speech (POS) information is not available in Con-
ceptNet. To handle this problem, we have deter-
mined the required POS tags of the new words that
can be obtained from the relations with an additional
goal of filtering out the noise. These tags are stated
in the fourth column of Table 1.
4.3 Adding semantically related words
To further increase the size of the ingredient list,
we utilize another resource called WordNet (Miller,
1995), which is a large lexical database for English.
In WordNet, nouns, verbs, adjectives and adverbs
are grouped into sets of cognitive synonyms called
synsets. Each synset in WordNet expresses a dif-
ferent concept and they are connected to each other
with lexical, semantic and conceptual relations.
We use the direct hypernym relation of WordNet
to retrieve the superordinates of the category word
(e.g. cleansing agent, cleanser and cleaner for the
category word shampoo). We prefer to use this re-
lation of WordNet instead of the relation ?IsA? in
706
ConceptNet to avoid getting too general words. Al-
though we can obtain only the direct hypernyms in
WordNet, no such mechanism exists in ConceptNet.
In addition, while WordNet has been built by lin-
guists, ConceptNet is built from the contributions of
many thousands of people across the Web and natu-
rally it also contains a lot of noise.
In addition to the direct hypernyms of the cate-
gory word, we increase the size of the ingredient list
by adding synonyms of the category word, the new
words coming from the relations and the properties
determined by the user.
It should be noted that we do not consider any
other statistical or knowledge based techniques for
semantic relatedness. Although they would allow us
to discover more concepts, it is difficult to under-
stand if and how these concepts pertain to the con-
text. In WordNet we can decide what relations to
explore, with the result of a more precise process
with possibly less recall.
4.4 Retrieving metaphors
A metaphor is a figure of speech in which an implied
comparison is made to indicate how two things that
are not alike in most ways are similar in one impor-
tant way. Metaphors are common devices for evo-
cation, which has been found to be a very important
technique used in naming according to the analysis
of our dataset.
In order to generate metaphors, we start with the
set of properties determined by the user and adopt
a similar technique to the one proposed by (Veale,
2011). In this work, to metaphorically ascribe a
property to a term, stereotypes for which the prop-
erty is culturally salient are intersected with stereo-
types to which the term is pragmatically compara-
ble. The stereotypes for a property are found by
querying on the web with the simile pattern ?as
?property? as *?. Unlike the proposed approach,
we do not apply any intersection with comparable
stereotypes since the naming task should favor fur-
ther terms to the category word in order to exagger-
ate, to evoke and thereby to be more effective.
The first constituent of our approach uses the
pattern ?as ?property? as *? with the addition of
??property? like *?, which is another important
block for building similes. Given a property, these
patterns are harnessed to make queries through the
web api of Google Suggest. This service performs
auto-completion of search queries based on popu-
lar searches. Although top 10 (or fewer) sugges-
tions are provided for any query term by Google
Suggest, we expand these sets by adding each let-
ter of the alphabet at the end of the provided phrase.
Thereby, we obtain 10 more suggestions for each of
these queries. Among the metaphor candidates that
we obtain, we filter out multiwords to avoid noise as
much as possible. Afterwards, we conduct a lemma-
tization process on the rest of the candidates. From
the list of lemmas, we only consider the ones which
appear in WordNet as a noun. Although the list
that we obtain in the end has many potentially valu-
able metaphors (e.g. sun, diamond, star, neon for
the property bright), it also contains a lot of uncom-
mon and unrelated words (e.g. downlaod, myspace,
house). Therefore, we need a filtering mechanism to
remove the noise and keep only the best metaphors.
To achieve that, the second constituent of the
metaphor retrieval mechanism makes a query in
ConceptNet with the given property. Then, all the
nouns coming from the relations in the form of
HasProperty(*, property) are collected to find words
having that property. The POS check to obtain only
nouns is conducted with a look-up in WordNet as
before. It should be noted that this technique would
not be enough to retrieve metaphors alone since it
can also return noise (e.g. blouse, idea, color, home-
schooler for the property bright).
After we obtain two different lists of metaphor
candidates with the two mechanisms mentioned
above, we take the intersection of these lists and
consider only the words appearing in both lists as
metaphors. In this manner, we aim to remove the
noise coming from each list and obtain more reli-
able metaphors. To illustrate, for the same example
property bright, the metaphors obtained at the end
of the process are sun, light and day.
4.5 Generating neologisms
After the ingredient list is complete, the phonetic
module analyzes all ingredient pairs to generate ne-
ologisms with possibly homophonic puns based on
phonetic similarity.
To retrieve the pronunciation of the ingredients,
we utilize the CMU Pronouncing Dictionary (Lenzo,
2007). This resource is a machine-readable pro-
nunciation dictionary of English which is suitable
for uses in speech technology, and it contains over
125,000 words together with their transcriptions. It
has mappings from words to their pronunciations
707
Input Successful output Unsuccessful output
Category Properties Word Ingredients Word Ingredients
bar
irish lively wooden traditional
warm hospitable friendly
beertender bartender, beer barkplace workplace, bar
barty party, bar barl girl, bar
giness guinness, gin bark work, bar
perfume
attractive strong intoxicating
unforgettable feminine mystic
sexy audacious provocative
mysticious mysterious, mystic provocadeepe provocative, deep
bussling buss, puzzling
mysteelious mysterious, steel
sunglasses
cool elite though authentic
cheap sporty
spectacools spectacles, cool spocleang sporting, clean
electacles spectacles, elect
polarice polarize, ice
restaurant
warm elegant friendly original
italian tasty cozy modern
eatalian italian, eat dusta pasta, dust
pastarant restaurant, pasta hometess hostess, home
peatza pizza, eat
shampoo
smooth bright soft volumizing
hydrating quality
fragrinse fragrance, rinse furl girl, fur
cleansun cleanser, sun sasun satin, sun
Table 2: A selection of succesful and unsuccessful neologisms generated by the model.
and the current phoneme set contains 39 phonemes
based on the ARPAbet symbol set, which has been
developed for speech recognition uses. We con-
ducted a mapping from the ARPAbet phonemes to
the international phonetic alphabet (IPA) phonemes
and we grouped the IPA phonemes based on the
phoneme classification documented in IPA. More
specifically, we grouped the ones which appear in
the same category such as p-b, t-d and s-z for the
consonants; i-y and e-? for the vowels.
After having the pronunciation of each word in
the ingredient list, shorter pronunciation strings are
compared against the substrings of longer ones.
Among the different possible distance metrics that
can be applied for calculating the phonetic similarity
between two pronunciation strings, we have chosen
the Levenshtein distance (Levenshtein, 1966). This
distance is a metric for measuring the amount of dif-
ference between two sequences, defined as the min-
imum number of edits required for the transforma-
tion of one sequence into the other. The allowable
edit operations for this transformation are insertion,
deletion, or substitution of a single character. For ex-
ample, the Levenshtein distance between the strings
?kitten? and ?sitting? is 3, since the following three
edits change one into the other, and there is no way
to do it with fewer than three edits: kitten? sitten
(substitution of ?k? with ?s?), sitten? sittin (substi-
tution of ?e? with ?i?), sittin ? sitting (insertion of
?g? at the end). For the distance calculation, we em-
ploy relaxation by giving a smaller penalty for the
phonemes appearing in the same phoneme groups
mentioned previously. We normalize each distance
by the length of the pronunciation string considered
for the distance calculation and we only allow the
combination of word pairs that have a normalized
distance score less than 0.5, which was set empiri-
cally.
Since there is no one-to-one relationship between
letters and phonemes and no information about
which phoneme is related to which letter(s) is avail-
able, it is not straightforward to combine two words
after determining the pairs via Levenshtein distance
calculation. To solve this issue, we use the Berke-
ley word aligner2 for the alignment of letters and
phonemes. The Berkeley Word Aligner is a sta-
tistical machine translation tool that automatically
aligns words in a sentence-aligned parallel corpus.
To adapt this tool according to our needs, we split
all the words in our dictionary into letters and their
mapped pronunciation to their phonemes, so that the
aligner could learn a mapping from phonemes to
characters. The resulting alignment provides the in-
formation about from which index to which index
the replacement of the substring of a word should
occur. Accordingly, the substring of the word which
has a high phonetic similarity with a specific word
is replaced with that word. As an example, if the
first ingredient is bright and the second ingredient is
light, the name blight can be obtained at the end of
2http://code.google.com/p/berkeleyaligner/
708
this process.
4.6 Checking phonetic likelihood
To check the likelihood and well-formedness of the
new string after the replacement, we learn a 3-gram
language model with absolute smoothing. For learn-
ing the language model, we only consider the words
in the CMU pronunciation dictionary which also ex-
ist in WordNet. This filtering is required in order
to eliminate a large number of non-English trigrams
which would otherwise cause too high probabilities
to be assigned to very unlikely sequences of charac-
ters. We remove the words containing at least one
trigram which is very unlikely according to the lan-
guage model. The threshold to determine the un-
likely words is set to the probability of the least fre-
quent trigram observed in the training data.
5 Evaluation
We evaluated the performance of our system with
a manual annotation in which 5 annotators judged
a set of neologisms along 4 dimensions: 1) appro-
priateness, i.e. the number of ingredients (0, 1 or
2) used to generate the neologism which are appro-
priate for the input; 2) pleasantness, i.e. a binary de-
cision concerning the conformance of the neologism
to the sound patterns of English; 3) humor/wittiness,
i.e. a binary decision concerning the wittiness of the
neologism; 4) success, i.e. an assessment of the fit-
ness of the neologism as a name for the target cate-
gory/properties (unsuccessful, neutral, successful).
To create the dataset, we first compiled a list
of 50 categories by selecting 50 hyponyms of the
synset consumer goods in WordNet. To determine
the properties to be underlined, we asked two anno-
tators to state the properties that they would expect
to have in a product or company belonging to each
category in our category list. Then, we merged the
answers coming from the two annotators to create
the final set of properties for each category.
Although our system is actually able to produce
a limitless number of results for a given input, we
limited the number of outputs for each input to
reduce the effort required for the annotation task.
Therefore, we implemented a ranking mechanism
which used a hybrid scoring method by giving equal
weights to the language model and the normalized
phonetic similarity. Among the ranked neologisms
for each input, we only selected the top 20 to build
the dataset. It should be noted that for some input
Dimension
APP PLE HUM SUX
2 9.54 0 0 27.04
3 33.3 25.34 32.77 49.52
4 41.68 38.6 34.57 18.77
5 15.48 36.06 32.66 4.67
3+ 90.46 100 100 72.96
Table 3: Inter-annotator agreement (in terms of majority
class, MC) on the four annotation dimensions.
combinations the system produced less than 20 neol-
ogisms. Accordingly, our dataset consists of a total
number of 50 inputs and 943 neologisms.
To have a concrete idea about the agreement be-
tween annotators, we calculated the majority class
for each dimension. With 5 annotators, a majority
class greater than or equal to 3 means that the abso-
lute majority of the annotators agreed on the same
decision. Table 3 shows the distribution of majority
classes along the four dimensions of the annotation.
For pleasantness (PLE) and humor (HUM), the ab-
solute majority of the annotators (i.e. 3/5) agreed on
the same decision in 100% of the cases, while for ap-
propriateness (APP) the figure is only slightly lower.
Concerning success, arguably the most subjective of
the four dimensions, in 27% of the cases it is not
possible to take a majority decision. Nevertheless,
in almost 73% of the cases the absolute majority of
the annotators agreed on the annotation of this di-
mension.
Table 4 shows the micro and macro-average of
the percentage of cases in which at least 3 anno-
tators have labeled the ingredients as appropriate
(APP), and the neologisms as pleasant (PLE), hu-
morous (HUM) or successful (SUX). The system se-
lects appropriate ingredients in approximately 60%
of the cases, and outputs pleasant, English-sounding
names in ?87% of the cases. Almost one name out
of four is labeled as successful by the majority of the
annotators, which we regard as a very positive result
considering the difficulty of the task. Even though
we do not explicitly try to inject humor in the neol-
ogisms, more than 15% of the generated names turn
out to be witty or amusing. The system managed to
generate at least one successful name for all 50 input
categories and at least one witty name for 42. As ex-
pected, we found out that there is a very high corre-
lation (91.56%) between the appropriateness of the
709
Dimension
Accuracy APP PLE HUM SUX
micro 59.60 87.49 16.33 23.86
macro 60.76 87.01 15.86 24.18
Table 4: Accuracy of the generation process along the
four dimensions.
ingredients and the success of the name. A success-
ful name is also humorous in 42.67% of the cases,
while 62.34% of the humorous names are labeled as
successful. This finding confirms our intuition that
amusing names have the potential to be very appeal-
ing to the customers. In more than 76% of the cases,
a humorous name is the product of the combination
of appropriate ingredients.
In Table 2, we show a selection of successful
and unsuccessful outputs generated for the category
and the set of properties listed under the block of
columns labeled as Input according to the majority
of annotators (i.e. 3 or more). As an example of pos-
itive outcomes, we can focus on the columns under
Successful output for the input target word restau-
rant. The model correctly selects the ingredients
eat (a restaurant is UsedFor eating), pizza and pasta
(which are found AtLocation restaurant) to generate
an appropriate name. The three ?palatable? neolo-
gisms generated are eatalian (from the combination
of eat and Italian), pastarant (pasta + restaurant)
and peatza (pizza + eat). These three suggestions are
amusing and have a nice ring to them. As a matter
of fact, it turns out that the name Eatalian is actually
used by at least one real Italian restaurant located in
Los Angeles, CA3.
For the same set of stimuli, the model also se-
lects some ingredients which are not really related
to the use-case, e.g., dust and hostess (both of which
can be found AtLocation restaurant) and home (a
synonym for plate, which can be found AtLocation
restaurant, in the baseball jargon). With these in-
gredients, the model produces the suggestion dusta
which sounds nice but has a negative connotation,
and hometess which can hardly be associated to the
input category.
A rather common class of unsuccessful outputs
include words that, by pure chance, happen to be
already existing in English. In these cases, no actual
neologism is generated. Sometimes, the generated
3http://www.eataliancafe.com/
words have rather unpleasant or irrelevant meanings,
as in the case of bark for bar. Luckily enough, these
kinds of outputs can easily be eliminated by filtering
out all the output words which can already be found
in an English dictionary or which are found to have
a negative valence with state-of-the-art techniques
(e.g. SentiWordNet (Esuli and Sebastiani, 2006)).
Another class of negative results includes neolo-
gisms generated from ingredients that the model
cannot combine in a good English-sounding neol-
ogism (e.g. spocleang from sporting and clean for
sunglasses or sasun from satin and sun for sham-
poo).
6 Conclusion
In this paper, we have focused on the task of automa-
tizing the naming process and described a computa-
tional approach to generate neologisms with homo-
phonic puns based on phonetic similarity. This study
is our first step towards the systematic emulation of
the various creative devices involved in the naming
process by means of computational methods.
Due to the complexity of the problem, a unified
model to handle all the creative devices at the same
time seems outside the reach of the current state-of-
the-art NLP techniques. Nevertheless, the resource
that we collected, together with the initial imple-
mentation of this model should provide a good start-
ing point for other researchers in the area. We be-
lieve that our contribution will motivate other re-
search teams to invest more effort in trying to tackle
the related research problems.
As future work, we plan to improve the quality of
the output by considering word sense disambigua-
tion techniques to reduce the effect of inappropriate
ingredients. We also want to extend the model to in-
clude multiword ingredients and to generate not only
words but also short phrases. Then, we would like
to focus on other classes of creative devices, such
as affixation or rhyming. Lastly, we plan to make
the system that we have developed publicly avail-
able and collect user feedback for further develop-
ment and improvement.
Acknowledgments
The authors were partially supported by a Google
Research Award.
710
References
L. Oliver B. V. Bergh, K. Adler. 1987. Linguistic distinc-
tion among top brand names. Journal of Advertising
Research, pages 39?44.
Yeqing Bao, Alan T Shao, and Drew Rivers. 2008. Cre-
ating new brand names: Effects of relevance, conno-
tation, and pronunciation. Journal of Advertising Re-
search, 48(1):148.
Marcel Botton and Jean-Jack Cegarra, editors. 1990. Le
nom de marque. Paris McGraw Hill.
2011. Cognitive tools for successful branding. Applied
Linguistics, 32:369?388.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. pages 417?422.
Kevin Lane Keller. 2003. Strategic brand management:
building, measuring and managing brand equity. New
Jersey: Prentice Hall.
Richard R. Klink. 2000. Creating brand names with
meaning: The use of sound symbolism. Marketing
Letters, 11(1):5?20.
C Kohli, K Harich, and Lance Leuthesser. 2005. Creat-
ing brand identity: a study of evaluation of new brand
names. Journal of Business Research, 58(11):1506?
1515.
John J. Ohala Leanne Hinton, Johanna Nichols. 2006.
Sound Symbolism. Cambridge University Press.
Kevin Lenzo. 2007. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10:707?710.
H. Liu and P. Singh. 2004. Conceptnet ? a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Go?zde O?zbal, Carlo Strapparava, and Marco Guerini.
2012. Brand Pitt: A corpus to explore the art of nam-
ing. In Proceedings of the eighth international confer-
ence on Language Resources and Evaluation (LREC-
2012), Istanbul, Turkey, May.
Michael M. Stark and Richard F. Riesenfeld. 1998.
Wordnet: An electronic lexical database. In Proceed-
ings of 11th Eurographics Workshop on Rendering.
MIT Press.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with HAHAcronym, a computational humor system.
In proceedings of the 21st national conference on Arti-
ficial intelligence - Volume 2, pages 1675?1678. AAAI
Press.
Tony Veale. 2011. Creative language retrieval: A robust
hybrid of information retrieval and linguistic creativ-
ity. In Proceedings of ACL 2011, Portland, Oregon,
USA, June.
711
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1446?1455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
BRAINSUP: Brainstorming Support for Creative Sentence Generation
Go?zde O?zbal
FBK-irst
Trento, Italy
gozbalde@gmail.com
Daniele Pighin
Google Inc.
Zu?rich, Switzerland
daniele.pighin@gmail.com
Carlo Strapparava
FBK-irst
Trento, Italy
strappa@fbk.eu
Abstract
We present BRAINSUP, an extensible
framework for the generation of creative
sentences in which users are able to
force several words to appear in the sen-
tences and to control the generation pro-
cess across several semantic dimensions,
namely emotions, colors, domain related-
ness and phonetic properties. We evalu-
ate its performance on a creative sentence
generation task, showing its capability of
generating well-formed, catchy and effec-
tive sentences that have all the good qual-
ities of slogans produced by human copy-
writers.
1 Introduction
A variety of real-world scenarios involve talented
and knowledgable people in a time-consuming
process to write creative, original sentences gen-
erated according to well-defined requisites. For
instance, to advertise a new product it could be
desirable to have its name appearing in a punchy
sentence together with some keywords relevant for
marketing, e.g. ?fresh?, or ?thirst? for the adver-
tisement of a drink. Besides, it could be interesting
to characterize the sentence with respect to a spe-
cific color, like ?blue? to convey the idea of fresh-
ness, or to a color more related to the brand of the
company, e.g. ?red? for a new Ferrari. Moreover,
making the slogan evoke ?joy? or ?satisfaction?
could make the advertisement even more catchy
for customers. On the other hand, there are many
examples of provocative slogans in which copy-
writers try to impress their readers by suscitating
strong negative feelings, as in the case of anti-
smoke campaigns (e.g., ?there are cooler ways to
die than smoking? or ?cancer cures smoking?), or
the famous beer motto ?Guinness is not good for
you?. As another scenario, creative sentence gen-
eration is also a useful teaching device. For ex-
ample, the keyword or linkword method used for
second language learning links the translation of
a foreign (target) word to one or more keywords
in the native language which are phonologically
or lexically similar to the target word (Sagarra and
Alba, 2006). To illustrate, for teaching the Ital-
ian word ?tenda?, which means ?curtain? in En-
glish, the learners are asked to imagine ?rubbing
a tender part of their leg with a curtain?. These
words should co-occur in the same sentence, but
constructing such sentences by hand can be a dif-
ficult and very time-consuming process. O?zbal
and Strapparava (2011), who attempted to auto-
mate the process, conclude that the inability to re-
trieve from the web a good sentence for all cases
is a major bottleneck.
Although state of the art computational mod-
els of creativity often produce remarkable results,
e.g., Manurung et al (2008), Greene et al (2010),
Guerini et al (2011), Colton et al (2012) just to
name a few, to our best knowledge there is no at-
tempt to develop an unified framework for the gen-
eration of creative sentences in which users can
control all the variables involved in the creative
process to achieve the desired effect.
In this paper, we advocate the use of syntactic
information to generate creative utterances by de-
scribing a methodology that accounts for lexical
and phonetic constraints and multiple semantic di-
mensions at the same time. We present BRAIN-
SUP, an extensible framework for creative sen-
tence generation in which users can control all the
parameters of the creative process, thus generat-
ing sentences that can be used for practical ap-
plications. First, users can define a set of key-
words which must appear in the final sentence.
Second, they can slant the output towards a spe-
1446
Domain Keywords BRAINSUP output examples
coffee waking,
cup
Between waking and doing there
is a wondrous cup.
coke drink, ex-
haustion
The physical exhaustion wants
the dark drink.
health day, juice,
sunshine
With juice and cereal the normal
day becomes a summer sunshine.
beauty kiss,lips
Passionate kiss, perfect lips. ?
Lips and eyes want the kiss.
mascara drama,lash
Lash your drama to the stage. ?
A mighty drama, a biting lash.
pickle crunch, bite Crunch your bite to the top. ?
Crunch of a savage byte. ? A
large byte may crunch a little at-
tention.
soap
skin,
love,
touch
A touch of love is worth a fortune
of skin. ? The touch of froth is
the skin of love. ? A skin of water
is worth a touch of love.
Table 1: A selection of sentences automatically
generated by BRAINSUP for specific domains.
cific emotion, color or domain. At the same time,
they can require a sentence to include desired pho-
netic properties, such as rhymes, alliteration or
plosives. The combination of these features al-
lows for the generation of potentially catchy and
memorable sentences by establishing connections
between linguistic, emotional (LaBar and Cabeza,
2006), echoic and visual (Borman et al, 2005)
memory, as exemplified by the system outputs
showcased in Table 1. Other creative dimensions
can easily be plugged in, due to the inherently
modular structure of the system.
BRAINSUP supports the creative process by
greedily exploring a huge solution space to pro-
duce completely novel utterances responding to
user requisites. It exploits syntactic constraints to
dramatically cut the size of the search space, thus
making it possible to focus on the creative aspects
of sentence generation.
2 Related work
Research in creative language generation has
bloomed in recent years. In this section, we pro-
vide a necessarily succint overview of a selection
of the studies that most heavily inspired and influ-
enced the development of BRAINSUP.
Humor generators are a notable class of sys-
tems exploring new venues in computational cre-
ativity (Binsted and Ritchie, 1997; McKay, 2002;
Manurung et al, 2008). Valitutti et al (2009)
present an interactive system which generates hu-
morous puns obtained through variation of famil-
iar expressions with word substitution. The varia-
tion takes place considering the phonetic distance
and semantic constraints such as semantic similar-
ity, semantic domain opposition and affective po-
larity difference. Possibly closer to slogan genera-
tion, Guerini et al (2011) slant existing textual ex-
pressions to obtain more positively or negatively
valenced versions using WordNet (Miller, 1995)
semantic relations and SentiWordNet (Esuli and
Sebastiani, 2006) annotations. Stock and Strap-
parava (2006) generate acronyms based on lexical
substitution via semantic field opposition, rhyme,
rythm and semantic relations. The model is lim-
ited to the generation of noun phrases.
Poetry generation systems face similar chal-
lenges to BRAINSUP as they struggle to combine
semantic, lexical and phonetic features in a unified
framework. Greene et al (2010) describe a model
for poetry generation in which users can control
meter and rhyme scheme. Generation is modeled
as a cascade of weighted Finite State Transduc-
ers that only accept strings conforming to the de-
sired rhyming scheme. Toivanen et al (2012) at-
tempt to generate novel poems by replacing words
in existing poetry with morphologically compat-
ible words that are semantically related to a tar-
get domain. Content control and the inclusion of
phonetic features are left as future work and syn-
tactic information is not taken into account. The
Electronic Text Composition project1 is a corpus
based approach to poetry generation which recur-
sively combines automatically generated linguistic
constituents into grammatical sentences. Colton et
al. (2012) propose another data-driven approach to
poetry generation based on simile transformation.
The mood and theme of the poems are influenced
by daily news. Constraints about phonetic proper-
ties of the selected words or their frequencies can
be enforced during retrieval. Unlike these exam-
ples, BRAINSUP makes heavy use of syntactic in-
formation to enforce well-formed sentences and to
constraint the search for a solution, and provides
an extensible framework in which various forms
of linguistic creativity can easily be incorporated.
Several slogan generators are available on the
web2, but their capabilities are very limited as they
can only replace single words or word sequences
within existing slogan. This often results in syn-
tactically incorrect outputs. Furthermore, they do
not allow for other forms of user control.
1http://slought.org/content/11199
2E.g.: http://www.procato.com/slogan+
generator, http://www.sloganizer.net/en/,
http://www.sloganmania.com/index.htm.
1447
3 Architecture of BRAINSUP
To effectively support the creative process with
useful suggestions, we must be able to generate
sentences conforming to the user needs. First of
all, users can select the target words that need to
appear in the sentence. In the context of second
language learning, these might be the words that a
learner must associate in order to expand her vo-
cabulary. For slogan generation, the target words
could be the key features of a product, or target-
defining keywords that copywriters want to explic-
itly mention. On top of that, a user can character-
ize the generated sentences according to several
dimensions, namely: 1) a specific semantic do-
main, e.g.: ?sports? or ?blankets?; 2) a specific
emotion, e.g., ?joy?, ?anger? or just ?negative?; 3)
a specific color, e.g., ?red? or ?blue?; 4) a com-
bination of phonetic properties of the words that
will appear in the sentence, i.e., rhymes, allitera-
tions and plosives. More formally, the user input
is a tuple: U = ?t,d, c, e, p,w? , where t is the
set of target words, d is a set of words defining the
target domain, c and p are, respectively, the color
and the emotion towards which the user wants to
slant the sentence, p represents the desired pho-
netic features, and w is a set of weights that control
the influence of each dimension on the generative
process, as detailed in Section 3.3. For target and
domain words, users can explicitly select one or
more POSes to be considered, e.g., ?drink/verb?
or ?drink/verb,noun?.
The sentence generation process is based on
morpho-syntactic patterns which we automati-
cally discover from a corpus of dependency parsed
sentences P . These patterns represent very gen-
eral skeletons of well-formed sentences that we
employ to generate creative sentences by only
focusing on the lexical aspects of the process.
Candidate fillers for each empty position (slot)
in the patterns are chosen according to the lexi-
cal and syntactic constraints enforced by the de-
pendency relations in the patterns. These con-
straints are learned from relation-head-modifier
co-occurrence counts estimated from a depen-
dency treebank L. A beam search in the space of
all possible lexicalizations of a syntactic pattern
promotes the words with the highest likelihood of
satisfying the user specification.
Algorithm 1 provides a high-level description of
the creative sentence generation process. Here, ?
is a set of meta-parameters that affect search com-
plexity and running time of the algorithm, such
as the minimum/maximum number of solutions to
Algorithm 1 SentenceGeneration(U,?,P,L): U is the
user specification, ? is a set of meta-parameters; P and L are
two dependency treebanks.
O ? ?
for all p ? CompatiblePatterns?(U,P) dowhile NotEnoughSolutions?(O) do
O ? O ? FillInPattern?(U, p,L)
return SelectBestSolutions?(O)
DT NNS VBD DT JJ NN IN DT NN
The * * a * * in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 1: Example of a syntactic pattern. A ?*?
represents an empty slot to be filled with a filler.
be generated, the maximum number of patterns to
consider, or the maximum size of the generated
sentences. CompatiblePatterns(?) finds the most
frequent syntactic patterns in P that are compat-
ible with the user specification, as explained in
Section 3.1; FillInPattern(?) carries out the beam
search, and returns the best solutions generated for
each pattern p given U . The algorithm terminates
when at least a minimum number of solutions have
been generated, or when all the compatible pat-
terns have been exhausted. Finally, only the best
among the generated solutions are shown to the
user. More details about the search in the solution
space are provided in Section 3.2.
3.1 Pattern selection
We generate creative sentences starting from
morpho-syntactic patterns which have been au-
tomatically learned from a large corpus P . The
choice of the corpus from which the patterns
are extracted constitutes the first element of the
creative sentence generation process, as differ-
ent choices will generate sentences with different
styles. For example, a corpus of slogans or punch-
lines can result in short, catchy and memorable
sentences, whereas a corpus of simplified English
would be a better choice to learn a second lan-
guage or to address low reading level audiences.
A pattern is the syntactic skeleton of a class
of sentences observed in P . Within a pattern, a
second element of creativity involves the selec-
tion of original combinations of words (fillers) that
do not violate the grammaticality of the sentence.
The patterns that we employ are automatic de-
pendency trees from which all content-words have
been removed, as exemplified in Figure 1. After
selecting the target corpus, we parse all the sen-
tences with the Stanford Parser (Klein and Man-
1448
ning, 2003) and produce the patterns by stripping
away all content words from the parses. Then,
for each pattern we count how many times it has
been observed in the corpus. Additionally, we
keep track of what kind of empty slots, i.e., empty
positions, are available in each pattern. For ex-
ample, the pattern in Figure 1 can accommodate
up to two singular nouns (NN), one plural noun
(NNS), one adjective (JJ) and one verb in the past
tense (VBD). This information is needed to se-
lect the patterns which are compatible with the
target words t in the user specification U . For
example, this pattern is not compatible with t =
[heading/VBG, edge/NN] as the pattern does not
have an empty slot for a gerundive verb, while it
satisfies t = [heading/NN, edge/NN] as it can
accommodate the two singular nouns. While re-
trieving patterns, we also need to enforce that a
pattern be not completely filled just by adding the
target words t, as under these conditions there
would be no room to achieve any kind of creative
effect. Therefore, we also require that the pat-
terns retrieved by CompatiblePatterns(?) have
more empty slots than the size of t. The mini-
mum and maximum number of excess slots in the
pattern are two other meta-parameters controlled
by ?. CompatiblePatterns(?) returns compati-
ble patterns ordered by their frequency, i.e. when
generating solutions the first patterns that are ex-
plored are the most frequently observed ones. In
this way, we achieve the following two objectives:
1) we compensate for the unavoidable errors intro-
duced by the automatic parser, as frequently ob-
served parses are less likely to be the result of
an erroneous interpretation of a sentence; and 2)
we generate sentences that are most likely to be
catchy and memorable, being based on syntactic
constructs that are used more frequently. To avoid
always selecting the same patterns for the same
kinds of inputs, we add a small random compo-
nent (also controlled by ?) to the pattern sorting
algorithm, thus allowing for sentences to be gen-
erated also from non-top ranked patterns.
3.2 Searching the solution space
With the compatible patterns selected, we can ini-
tiate a beam search in the space of all possible
lexicalizations of the patterns, i.e., the space of
all sentences that can be generated by respect-
ing the syntactic constraints encoded by each pat-
tern. The process starts with a syntactic pattern
p containing only stop words, syntactic relations
and morphologic constraints (i.e., part-of-speech
DT NNS VBD DT JJ NN IN DT NN
The fires X a * smoke in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 2: A partially lexicalized sentence with a
highlighted empty slot marked with X. The rele-
vant dependencies to fill in the slot are shown in
boldface.
tags) for the empty slots. The search advances to-
wards a complete solution by selecting an empty
slot to fill and trying to place candidate fillers in
the selected position. Each partially lexicalized
solution is scored by a battery of scoring func-
tions that compete to generate creative sentences
respecting the user specificationU , as explained in
Section 3.3. The most promising solutions are ex-
tended by filling another slot, until completely lex-
icalized sentences, i.e., sentences without empty
slots, are generated.
To limit the number of words that can occupy
a given position in a sentence, we define a set of
operators that return a list of candidate fillers for
a slot solely based on syntactic clues. To achieve
that, we analyze a large corpus of parsed sentences
L3 and store counts of observed head-relation-
modifier (?h, r,m?) dependency relations. Let
?r(h) be an operator that, when applied to a head
word h in a relation r, returns the set of words in
L which have been observed as modifiers for h in
r with a specific POS. To simplify the notation,
we assume that the relation r also carries along
the POS of the head and modifier slots. As an
example, with respect to the tree depicted in Fig-
ure 2, ?amod(smoke) would return all the words
with POS equal to ?JJ? that have been observed as
adjective modifiers for the singular noun ?smoke?.
We will refer to ?r(?) as the dependency operator
for r. For every ?r(?), we also define an inverse
dependency operator ??1r (?), which returns the list
of the possible heads in r when applied to a mod-
ifier word m. For instance, with respect to Fig-
ure 2, ??1nsubj(fires) would return the set of verbs inthe past tense of which ?fires? as a plural noun can
be a subject.
While filling in a given slot X , the dependency
operators can be combined to obtain a list of words
which are likely to occupy that position given the
syntactic constraints induced by the structure of
the pattern. Let W = ?i{wi} be the set of words
which are directly connected to the empty slot by
3Distinct from the corpus used for pattern selection, P .
1449
a dependency relation. Each word wi implies a
constraint that candidate fillers for X must satisfy.
If wi is the head of X , then a direct operator is
used to retrieve a list of fillers that satisfy the ith
constraint. Conversely, if wi is a modifier of X ,
an inverse operator is employed.
As an example, let us consider the partially
completed sentence shown in Figure 2 having
an empty slot marked with X . Here, the word
?smoke? is a modifier for X , to which it is con-
nected by a dobj relation. Therefore, we can ex-
ploit ??1dobj(smoke) to obtain a ranked list of wordsthat can occupy X according to this constraint.
Similarly, the ??1nsubj(fires) operator can be used toretrieve a list of verbs in the past tense that ac-
cept ?fires? as nsubj modifier. Finally ??1prep(in)
can further restrict our options to verbs that ac-
cepts complements introduced by the preposition
?in?. For example, the words ?generated?, ?pro-
duced?, ?caused? or ?formed? would be good can-
didates to fill in the slot considering all the pre-
vious constraints. More formally, we can de-
fine the set of candidate fillers for a slot X , CX ,
as: CX = ??1rhX,X(hX) ? (
?
wi|wi?MX ?rwi,X(wi)),where rwi,X is the type of relation between wi and
X , MX is the set of modifiers of X and hX is the
syntactic head of X .4
Concerning the order in which slots are filled,
we start from those that have the highest num-
ber of dependencies (both head or modifiers) that
have been already instantiated in the sentence, i.e.,
we start from the slots that are connected to the
highest number of non-empty slots. In doing so
we maximize the constraints that we can rely on
when inserting a new word, and eventually gener-
ate more reliable outputs.
3.3 Filler selection and solution scoring
We have devised a set of feature functions that ac-
count for different aspects of the creative sentence
generation process. By changing the weight w of
the feature functions in U , users can control the
extent to which each creativity component will af-
fect the sentence generation process, and tune the
output of the system to better match their needs.
As explained in the remainder of this section, fea-
ture functions are responsible for ranking the can-
didate slot fillers to be used during sentence gen-
eration and for selecting the best solutions to be
4An empty slot does not generate constraints for X . In
addition, there might be cases in which it is not possible to
find a filler that satisfies all the constraints at the same time.
In such cases, all the fillers that satisfy the maximum number
of constraints are considered.
Algorithm 2 RankCandidates(U, f , c1, c2, s,X): c1
and c2 are two candidate fillers for the slot X in the sentence
s = [s0, . . . sn]; f is the set of feature functions; U is the user
specification.
sc1 ? s, sc2 ? s, sc1 [X]? c1, sc2 [X]? c2
for all f ? SortFeatureFunctions?(U, f) do
if f(sc1 , U) > f(sc2 , U) then return c1  c2
else if f(sc1 , U) < f(sc2 , U) then
return c1 ? c2
return c1 ? c2
shown to the users.
Algorithm 2 details the process of ranking can-
didate fillers. To compare two candidates c1 and c2
for the slot X in the sentence s, we first generate
two sentences sc1 and sc2 in which the empty slot
X is occupied by c1 and c2, respectively. Then, we
sort the feature functions based on their weights
in descending order, and in turn we apply them
to score the two sentences. As soon as we find
a scorer for which one sentence is better than the
other, we can take a decision about the ranking of
the fillers. This approach makes it possible to es-
tablish a strict order of precedence among feature
functions and to select fillers that have a highest
chance of maximizing the user satisfaction.
Concerning the scoring of partial solutions and
complete sentences, we adopt a simple linear com-
bination of scoring functions. Let s be a (partial)
sentence, f = [f0, . . . , fk] be the vector of scor-
ing functions and w = [w0, . . . , wk] the associ-
ated vector of weights in U . The overall score of s
is calculated as score(s, U) = ?ki=0wifi(s, U) .Solutions that do not contain all the required target
words are discarded and not shown to the user.
Currently, the model employs the following 12
feature functions:
Chromatic and emotional connota-
tion. The chromatic connotation of a
sentence s = [s0, . . . , sn] is computed as
f(s, U) =
?
si(sim(si, c) ?
?
cj 6=c sim(si, cj)),where c is the user selected target color and
sim(si, cj) is the degree of association between
the word si and the color cj as calculated by
Mohammad (2011). All the words in the sentence
which have an association with the target color
c give a positive contribution, while those that
are associated with a color ci 6= c contribute
negatively. Emotional connotation works exactly
in the same way, but in this case word-emotion
associations are taken from (Mohammad and
Turney, 2010).
Domain relatedness. This feature function uses
an LSA (Deerwester et al, 1990) vector space
1450
model to measure the similarity between the words
in the sentence and the target domain d speci-
fied by the user. It is calculated as: f(s, U) =?
di v(di)?
?
si v(si)
??di v(di)???
?
si v(si)?
where v(?) returns the rep-
resentation of a word in the vector space.
Semantic cohesion. This feature behaves ex-
actly like domain relatedness, with the only dif-
ference that it measures the similarity between the
words in the sentence and the target words t.
Target-words scorer. This feature function
simply counts what fraction of the target
words t is present in a partial solution:
f(s, U) = (
?
si|si?t 1)/|t|. The target word scorertakes care of enforcing the presence of the target
words in the sentences. Letting beam search find
the best placement for the target words comes at
no extra cost and results in a simple and elegant
model.
Phonetic features (plosives, alliteration and
rhyme). All the phonetic features are based on
the phonetic representation of English words of
the Carnegie Mellon University pronouncing dic-
tionary (Lenzo, 1998). The plosives feature is cal-
culated as the ratio between the number of plo-
sive sounds in a sentence and the overall num-
ber of phonemes. For the alliteration scorer, we
store the phonetic representation of each word in
s in a trie (i.e., prefix tree), and count how many
times each node ni of the trie (corresponding to a
phoneme) is traversed. Let ci be the value of the
counts for ni. The alliteration score is then cal-
culated as f(s, U) = (?i|ci>1 ci)/
?
i ci. Moresimply put, we count how many of the phonetic
prefixes of the words in the sentence are repeated,
and then we normalize this value by the total num-
ber of phonemes in s. The rhyme feature works
exactly in the same way, with the only difference
that we invert the phonetic representation of each
word before adding it to the TRIE. Thus, we give
higher scores to sentences in which several words
share the same phonetic ending.
Variety scorer. This feature function promotes
sentences that contain as many different words as
possible. It is calculated as the number of distinct
words in the sentence over the size of the sentence.
Unusual-words scorer. To increase the ability
of the model to generate sentences containing non-
trivial word associations, we may want to prefer
solutions in which relatively uncommon words are
employed. Inversely, we may want to lower lex-
ical complexity to generate sentences more ap-
propriate for certain education or reading levels.
We define ci as the number of times each word
si ? s is observed in a corpus V . Accord-
ingly, the value of this feature is calculated as:
f(s, U) = (1/|s|)(?si 1/ci).
N-gram likelihood. This is simply the likeli-
hood of a sentence estimated by an n-gram lan-
guage model, to enforce the generation of well-
formed word sequences. When a solution is not
complete, in the computation we include only the
sequences of contiguous words (i.e., not inter-
rupted by empty slots) having length greater than
or equal to the order of the n-gram model.
Dependency likelihood. This feature is re-
lated to the dependency operators introduced
in Section 3.2 and it enforces sentences in
which dependency chains are well formed. We
estimate the probability of a modifier word
m and its head h to be in the relation r
as pr(h,m) = cr(h,m)/(?hi
?
mi cr(hi,mi)),where cr(?) is the number of times that m
depends on h in the dependency treebank
L and hi,mi are all the head/modifier pairs
observed in L. The dependency-likelihood
of a sentence s can then be calculated as
f(s, U) = exp(
?
?h,m,r??r(s) log pr(h,m)), r(s)being the set of dependency relations in s.
4 Evaluation
We evaluated our model on a creative sentence
generation task. The objective of the evaluation
is twofold: we wanted to demonstrate 1) the effec-
tiveness of our approach for creative sentence gen-
eration, in general, and 2) the potential of BRAIN-
SUP to support the brainstorming process behind
slogan generation. To this end, the annotation tem-
plate included one question asking the annotators
to rate the quality of the generated sentences as
slogans.
Five experienced annotators were asked to rate
432 creative sentences according to the follow-
ing criteria, namely: 1) Catchiness: is the sen-
tence attractive, catchy or memorable? [Yes/No]
2) Humor: is the sentence witty or humorous?
[Yes/No]; 3) Relatedness: is the sentence seman-
tically related to the target domain? [Yes/No]; 4)
Correctness: is the sentence grammatically cor-
rect? [Ungrammatical/Slightly disfluent/Fluent];
5) Success: could the sentence be a good slogan
for the target domain? [As it is/With minor edit-
ing/No]. In these last two cases, the annotators
1451
were instructed to select the middle option only
in cases where the gap with a correct/successful
sentence could be filled just by performing minor
editing. The annotation form had no default val-
ues, and the annotators did not know how the eval-
uated sentences were generated, or whether they
were the outcome of one or more systems.
We started by collecting slogans from an on-
line repository of slogans5. Then, we randomly
selected a subset of these slogans and for each of
them we generated an input specification U for the
system. We used the commercial domain of the
advertised product as the target domain d. Two
or three content words appearing in each slogan
were randomly selected as the target words t. We
did so to simulate the brainstorming phase behind
the slogan generation process, where copywriters
start with a set of relevant keywords to come up
with a catchy slogan. In all cases, we set the tar-
get emotion to ?positive? as we could not estab-
lish a generally valid criteria to associate a spe-
cific emotion to a product. Concerning chromatic
slanting, for target domains having a strong chro-
matic correlation we allowed the system to slant
the generated sentences accordingly. In the other
cases, a random color association was selected. In
this manner, we produced 10 tuples ?t,d, c, e, p?.
Then, from each tuple we produced 5 complete
user specifications by enabling or disabling differ-
ent feature function combinations6. The four com-
binations of features are: base: Target-word scorer
+ N-gram likelihood + Dependency likelihood +
Variety scorer + Unusual-words scorer + Seman-
tic cohesion; base+D: all the scorers in base +
Domain relatedness; base+D+C: all the scorers in
base+D + Chromatic connotation; base+D+E: all
the scorers in base+D + Emotional connotation;
base+D+P: all the scorers in base+D + Phonetic
features. For each of the resulting 50 input config-
urations, we generated up to 10 creative sentences.
As the system could not generate exactly 10 solu-
tions in all the cases, we ended up with a set of
432 items to annotate. The weights of the feature
functions were set heuristically, due to the lack
of an annotated dataset suitable to learn an opti-
5http://www.tvacres.com/advertising_
slogans.htm
6An alternative strategy to keep the annotation effort un-
der control would have been to generate fewer sentences from
a larger number of inputs. We adopted the former setting
since we regarded it as more similar to a brainstorming ses-
sion, where the system proposes different alternatives to in-
spire human operators. Forcing BRAINSUP to only output
one or two sentences would have limited its ability to explore
and suggest potentially valuable outputs.
MC Cat. Hum. Corr. Rel. Succ. RND2 RND3
2 - - 16.67 - 22.22 - 37.043 47.45 39.58 43.52 13.66 44.21 62.50 49.384 33.10 37.73 32.18 21.99 22.22 31.25 12.355 19.44 22.69 07.64 64.35 11.34 06.25 01.23
Table 2: Majority classes (%) for the five dimen-
sions of the annotation.
mal weight configuration. We started by assign-
ing the highest weight to the Target Word scorer
(i.e., 1.0), followed by the Variety and Unusual
Word scorers (0.99), the Phonetic Features, Chro-
matic/Emotional Connotation and Semantic Co-
hesion scorers (0.98) and finally the Domain, N -
gram and Dependency Likelihood scorers (0.97).
These settings allow us to enforce an order of
precedence among the scorers during slot-filling,
while giving them virtually equal relevance for so-
lution ranking.
As discussed in Section 3 we use two differ-
ent treebanks to learn the syntactic patterns (P)
and the dependency operators (L). For these ex-
periments, patterns were learned from a corpus
of 16,000 proverbs (Mihalcea and Strapparava,
2006), which offers a good selection of short sen-
tences with a good potential to be used for slo-
gan generation. This choice seemed to be a good
compromise as, to our best knowledge, there is
no published slogan dataset with an adequate size.
Besides, using existing slogans might have legal
implications that we might not be aware of. De-
pendency operators were learned by dependency
parsing the British National Corpus7. To reduce
the amount of noise introduced by the automatic
parses, we only considered sentences having less
than 20 words. Furthermore, we only considered
sentences in which all the content words are listed
in WordNet (Miller, 1995) with the observed part
of speech.8 The LSA space used for the semantic
feature functions was also learned on BNC data,
but in this case no filtering was applied.
4.1 Results
To measure the agreement among the annota-
tors, similarly to Mohammad (2011) and Ozbal
and Strapparava (2012) we calculated the majority
class for each dimension of the annotation task. A
7http://www.natcorp.ox.ac.uk/
8Since the CMU pronouncing dictionary used by the pho-
netic scorers is based on the American pronunciation of
words, we actually pre-processed the whole BNC by replac-
ing all British-English words with their American-English
counterparts. To this end, we used the mapping available at
http://wordlist.sourceforge.net/.
1452
Cat. Rel. Hum. Succ. Corr.
Yes 67.59 93.98 12.73 32.41 64.35
Partly - - - 23.15 31.71
No 32.41 06.02 87.27 44.44 03.94
Table 3: Majority decisions (%) for each annota-
tion dimension.
majority class greater than or equal to 3 means that
the absolute majority of the 5 annotators agreed
on the same decision9. Table 2 shows the ob-
served agreement for each dimension. The column
labeled RND2 (RND3) shows the random agree-
ment for a given number of annotators and a binary
(ternary) decision. For example, all five annotators
(MC=5) agreed on the annotation of the catchiness
of the slogans in 19.44% of the cases. The random
chance of agreement for 5 annotators on the binary
decision problem is 6.25%. The figures for MC ?
4 are generally high, confirming a good agreement
among the annotators. The agreement on the relat-
edness of the slogans is especially high, with all 5
annotators taking the same decision in almost two
cases out of three, i.e., 64.35%.
Table 3 lists the distribution of answers for each
dimension in the cases where a decision can be
taken by majority vote. The generated slogans
are found to be catchy in more than 2/3 of the
cases, (i.e., 67.59%), completely successful in 1/3
of the cases (32.41%) and completely correct in
2/3 of the cases (64.35%). These figures demon-
strate that BRAINSUP is very effective in gener-
ating grammatical utterances that have all the ap-
pealing properties of a successful slogan. As for
humor, the sentences are found to have this prop-
erty in only 12.73% of cases. Even though the
figure is not very high, we should also consider
that BRAINSUP is not explicitly trying to gener-
ate amusing utterances. Concerning success, we
should point out that in 23.15% of the cases the
annotators have found that the generated slogans
have the potential to be turned into successful ones
only with minor editing. This is a very important
piece of result, as it corroborates our claim that
BRAINSUP can indeed be a valuable tool for copy-
writing, even when it does not manage to output a
perfectly good sentence. Similar conclusions can
be drawn concerning the correctness of the output,
as in almost one third of the cases the slogans are
9For the binary decisions (i.e., catchiness, relatedness and
humor), at least 3 annotators out of 5 must necessarily agree
on the same option.
only affected by minor disfluencies.
The relatedness figure is especially high, as in
almost 94% of the cases the majority of annota-
tors found the slogans to be pertinent to the tar-
get domain. This result is not surprising, as all
the slogans are generated by considering keywords
that already exist in real slogans for the same do-
main. Anyhow, this is exactly the kind of setting in
which we expect BRAINSUP to be employed, i.e.,
to support creative sentence generation starting
from a good set of relevant keywords. Nonethe-
less, it is very encouraging to observe that the gen-
eration process does not deteriorate the positive
impact of the input keywords.
We would also like to mention that in 63 cases
(14.58%) the majority of the annotators have la-
beled the slogans favorably across all 5 dimen-
sions. The examples listed in Table 1 are selected
from this set. It is interesting to observe how
the word associations established by BRAINSUP
can result in pertinent yet unintentional rhetori-
cal devices such as metaphors (?a summer sun-
shine?), puns (?lash your drama?) and personifica-
tions (?lips and eyes want?). Some examples show
the effect of the phonetic features, e.g. plosives in
?passionate kiss, perfect lips?, alliteration in ?the
dark drink? and rhyming in ?lips and eyes want
the kiss?. In some cases, the output of BRAINSUP
seems to be governed by mysterious philosophical
reasoning, as in the delicate examples generated
for ?soap?.
For comparison, Table 4 lists a selection of
the examples that have been labeled as unsuc-
cessful by the majority of raters. In some cases,
BRAINSUP is improperly selecting attributes that
highlight undesirable properties in the target do-
main, e.g., ?A pleasant tasting, a heady wine?. To
avoid similar errors, it would be necessary to rea-
son about the valence of an attribute for a spe-
cific domain. In other cases, the N -gram and the
Dependency Likelihood features may introduce
phrases which are very cohesive but unrelated to
the rest of the sentence, e.g., ?Unscrupulous doc-
tors smoke armored units?. Many of these errors
could be solved by increasing the weight of the
Semantic Cohesion and Domain Relatedness scor-
ers. In other cases, such as ?A sixth calorie may
taste an own good? or ?A same sunshine is fewer
than a juice of day?, more sophisticated reason-
ing about syntactic and semantic relations in the
output might be necessary in order to enforce the
generation of sound and grammatical sentences.
We could not find a significant correlation be-
1453
Domain Keywords BRAINSUP output examples
pleasure wine, tast-
ing
A pleasant tasting, a heady wine.
? A fruity tasting may drink a
sparkling wine.
healthy day, juice,
sunshine
Drink juice of your sunshine, and
your weight will choose day of
you. ? A same sunshine is fewer
than a juice of day.
cigarette doctors,
smoke
Unscrupulous doctors smoke ar-
mored units. ? Doctors smoke no
arrow.
mascara drama,
lash
The such drama is the lash.
soap skin, love,
touch
The touch of skin is the love of
cacophony. ? You love an own
skin for a first touch.
coke calorie,
taste, good
A sixth calorie may taste an own
good.
coffee waking,
cup
You cannot cup hands without
waking some fats.
Table 4: Unsuccessful BRAINSUP outputs.
tween the input variables (e.g., presence or ab-
sence of phonetic features or chromatic slanting)
and the outcome of the annotation, i.e. the sys-
tem by and large produces correct, catchy, related
and (at least potentially) successful outputs regard-
less of the specific input configurations. In this re-
spect, it should be noted that we did not carry out
any kind of optimization of the feature weights,
which might be needed to obtain more heavily
characterized sentences. Furthermore, to better
appreciate the contribution of the individual fea-
tures, comparative experiments in which the users
evaluate the system before and after triggering a
feature function might be necessary. Concern-
ing the correlation among output dimensions, we
only observed relatively high Spearman correla-
tion between correctness and relatedness (0.65),
and catchiness and success (0.68).
5 Conclusion
We have presented BRAINSUP, a novel system
for creative sentence generation that allows users
to control many aspects of the creativity process,
from the presence of specific target words in the
output, to the selection of a target domain, and
to the injection of phonetic and semantic proper-
ties in the generated sentences. BRAINSUP makes
heavy use of dependency parsed data and statistics
collected from dependency treebanks to ensure the
grammaticality of the generated sentences, and to
trim the search space while seeking the sentences
that maximize the user satisfaction.
The system has been designed as a support-
ing tool for a variety of real-world applications,
from advertisement to entertainment and educa-
tion, where at the very least it can be a valu-
able support for time-consuming and knowledge-
intensive sentence generation needs. To demon-
strate this point, we carried out an evaluation on a
creative sentence generation benchmark showing
that BRAINSUP can effectively produce catchy,
memorable and successful sentences that have the
potential to inspire the work of copywriters.
To our best knowledge, this is the first system-
atic attempt to build an extensible framework that
allows for multi-dimensional creativity while at
the same time relying on syntactic constraints to
enforce grammaticality. In this regard, our ap-
proach is dual with respect to previous work based
on lexical substitution, which suffers from limited
expressivity and creativity latitude. In addition, by
acquiring the lexicon and the sentence structure
from two distinct corpora, we can guarantee that
the sentences that we generate have never been
observed. We believe that our contribution con-
stitutes a valid starting point for other researchers
to deal with unexplored dimensions of creativity.
As future work, we plan to use machine learn-
ing techniques to estimate optimal weights for the
feature functions in different use cases. We would
also like to consider syntactic clues while reason-
ing about semantic properties of the sentence, e.g.,
color and emotion associations, instead on relying
solely on lexical semantics. Concerning the exten-
sion of the capabilities of BRAINSUP, we want to
include common-sense knowledge and reasoning
to profit from more sophisticated semantic rela-
tions and to inject humor on demand. Further tun-
ing of BRAINSUP to build a dedicated system for
slogan generation is also part of our future plans.
After these improvements, we would like to con-
duct a more focused evaluation on slogan genera-
tion involving human copywriters and domain ex-
perts in an interactive setting.
We would like to conclude this paper with a pearl
of BRAINSUP?s wisdom:
It is wiser to believe in science
than in everlasting love.
Acknowledgments
Go?zde O?zbal and Carlo Strapparava were partially
supported by the PerTe project (Trento RISE).
1454
References
Kim Binsted and Graeme Ritchie. 1997. Computa-
tional rules for generating punning riddles. Humor -
International Journal of Humor Research, 10(1):25?
76, January.
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Pic-net: Pictorial representations for illustrated se-
mantic networks. In Proceedings of the AAAI Spring
Symposium on Knowledge Collection from Volun-
teer Contributors.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of
the 3rd International Conference on Computational
Creativity, pages 95?102.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
Of The American Society for Information Science,
41(6):391?407.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC?06), pages 417?422.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry
with applications to generation and translation. In
EMNLP, pages 524?533.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2011. Slanting existing text with valentino. In Pro-
ceedings of the 16th international conference on In-
telligent user interfaces, IUI ?11, pages 439?440,
New York, NY, USA. ACM.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin S. LaBar and Roberto Cabeza. 2006. Cognitive
neuroscience of emotional memory. Nature reviews.
Neuroscience, 7(1):54?64, January.
Kevin Lenzo. 1998. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O?Mara, and Rolf Black. 2008.
The Construction of a Pun Generator for Language
Skills Development. Applied Artificial Intelligence,
22(9):841?869, October.
J McKay. 2002. Generation of idiom-based witticisms
to aid second language learning. In Twente Work-
shop on Language Technology 20, pages 70?74.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126?142, May.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011. Even the abstract have color:
Consensus in word-colour associations. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 368?373, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Go?zde O?zbal and Carlo Strapparava. 2011. Autom-
atized Memory Techniques for Vocabulary Acquisi-
tion in a Second Language. In Alexander Verbraeck,
Markus Helfert, Jose? Cordeiro, and Boris Shishkov,
editors, CSEDU, pages 79?87. SciTePress.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 703?711, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228?243.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with hahacronym, a computational humor system.
In proceedings of the 21st national conference on
Artificial intelligence - Volume 2, pages 1675?1678.
AAAI Press.
J. M. Toivanen, H. Toivonen, A. Valitutti, and O. Gross.
2012. Corpus-based Generation of Content and
Form in Poetry. In International Conference on
Computational Creativity, pages 175?179.
A. Valitutti, C. Strapparava, , and O. Stock. 2009.
Graphlaugh: a tool for the interactive generation of
humorous puns. In Proceedings of ACII-2009, Third
Conference on Affective Computing and Intelligent
Interaction, Demo track.
1455
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 28?32,
Beijing, August 2010
The Color of Emotions in Texts
Carlo Strapparava and Gozde Ozbal
FBK-irst
strappa@fbk.eu, gozbalde@gmail.com
Abstract
Color affects every aspect of our lives.
There has been a considerable interest
in the psycholinguistic research area ad-
dressing the impact of color on emotions.
In the experiments conducted by these
studies, subjects have usually been asked
to indicate their emotional responses to
different colors. On the other side, sensing
emotions in text by using NLP techniques
has recently become a popular topic in
computational linguistics. In this paper,
we introduce a semantic similarity mecha-
nism acquired from a large corpus of texts
in order to check the similarity of col-
ors and emotions. Then we investigate
the correlation of our results with the out-
comes of some psycholinguistic experi-
ments. The conclusions are quite inter-
esting. The correlation varies among dif-
ferent colors and globally we achieve very
good results.
1 Introduction
In our daily speech, we frequently make use of
colors in order to increase our expressiveness by
invoking different emotions. For instance, we usu-
ally stress the redness of someone?s face for the
implication of his/her anger or excitement, or we
use phrases including the color black to refer to a
depressed mood. On the other hand, the color pink
is mostly used with positive connotations such as
?to see everything in pink light?, where the mean-
ing is related to optimism and happiness.
Actually, the parts of the nervous system which
are responsible for emotional arousal are affected
as soon as a color is perceived. Thus, the term
color emotion has lately been used to represent
the emotions arousing in human beings when they
percept a color (Xin et al, 2004).
The correlation of color and emotion has been
the focus of a lot of psycholinguistic studies so
far. In the experiments conducted by these studies,
subjects have usually been asked to indicate their
emotional responses to different colors so that
some general results stating which color arouses
what kind of emotion could be obtained.
Sensing emotion, or in other words, affective
sensing in text by using Natural Language Pro-
cessing (NLP) techniques is recently a very pop-
ular topic in computational linguistics. There ex-
ist several studies focusing on the automatic iden-
tification of emotions in text with the help of
both knowledge-based and corpus-based methods.
Thus it is conceivable to explore whether state-of-
the-art corpus analysis techniques can give sup-
port to psycholinguistic experiments.
Considering that psycholinguistic experiments
are very costly since a lot of resources are required
for both the setup and evaluation phases, employ-
ing a corpus-based approach for affective sensing
could be much more efficient for all analysis to be
held in the future, if this technique was proven to
give reasonable results.
In this paper, we employ a semantic similarity
mechanism automatically obtained in an unsuper-
vised way from a large corpus of texts in order
to check the similarity of color and emotion via
computational analysis methods. We adopt the
psycholinguistic experiments as references, with
which we compare our results to find out if there
is a correlation between the two approaches.
28
The paper is organized as follows. In Section
2, we introduce some related work focusing on
the association of color and emotion only from
a psycholinguistic point of view, since this topic
has not been addressed by computational analysis
techniques so far. In Section 3, we describe the
methodology for implementing a similarity be-
tween colors and emotions, in particular how to
represent an emotion in a latent semantic space.
We present the evaluation of our approach and
make a comparison with the results of psycholin-
guistic experiments in Section 4. Lastly, we report
the conclusions and possible future work in Sec-
tion 5.
2 Background
As we mentioned previously, there has been a con-
siderable interest in the psycholinguistic research
area addressing the impact of color on emotions.
(Zentner, 2001) mainly addressed the question
of whether young children could show reliable
color preferences. This study also tried to make a
comparison with the results obtained with adults
and older children. Subjects? color preferences
were obtained by asking them to choose the one
that they prefer among an array of colored card-
board rectangles. As an alternative way to repre-
sent musical information for providing feedback
on emotion expression in music, (Bresin, 2005)
suggested to use a graphical non-verbal represen-
tation of expressivity in music performance by ex-
ploiting color as an index of emotion. And for the
purpose of determining which colors were most
suitable for an emotional expression, some ex-
periments were conducted, where subjects rated
how well several colors and their nuances corre-
sponded to music performances expressing differ-
ent emotions. (Kaya, 2004) tried to investigate
and discuss the associations between color and
emotion by conducting experiments where college
students were asked to indicate their emotional re-
sponses to principal, intermediate and achromatic
colors, and the reasons for their choices.
There exist also some research investigating
whether the color perception is related to the re-
gion of the subject. For instance, (Gao et al,
2007) analyzed and compared the color emotions
of people from seven regions in a psychophysical
experiment, with an attempt to clarify the influ-
ences of culture and color perception attributes on
color emotions. This study suggested that it might
be possible to compose a color emotion space by
using a restricted number of factors. As for (So-
riano and Valenzuela, 2009), this study tried to
find out why there was often a relationship be-
tween color and emotion words in different lan-
guages. In order to achieve this, a new experi-
mental methodology called the Implicit Associa-
tion Test (IAT) was used to explore the implicit
connotative structure of the Peninsular Spanish
color terms in terms of Osgood?s universal se-
mantic dimensions explained in (Adams and Os-
good, 1973). The research conducted by (Xin et
al., 2004) tried to compare the color emotional re-
sponses that were obtained by conducting visual
experiments in different regions by using a set of
color samples. A quantitative approach was used
in this study in an attempt to compare the color
emotions among these regions. (Madden et al,
2000) focused on the possible power of color for
creating and sustaining brand and corporate im-
ages in international marketing. This study tried
to explore the favorite colors of consumers from
different countries, the meanings they associated
with colors, and their color preferences for a logo.
The study that we will use for evaluating our re-
sults is a work which focused on the topic ?emo-
tional responses to color used in advertisement?
(Alt, 2008). During the experiments, this study
conducted a survey where the subjects were re-
quired to view an advertisement with a dominant
color hue, and then select a specific emotional re-
sponse and a positive/negative orientation related
with this color. More than 150 subjects partici-
pated in this study, roughly equally partitioned in
gender. There are two main reasons why we pre-
ferred to use this study for our evaluation proce-
dure. Firstly, the presentation and organization of
the results provide a good reference for our own
experiments. In addition, it focusses on adver-
tisement, which is one of the applicative fields we
want to address in future work.
3 Methodology
Sensing emotions from text is an appealing task
of natural language processing (Pang and Lee,
29
2008; Strapparava and Mihalcea, 2007): the au-
tomatic recognition of affective states is becom-
ing a fundamental issue in several domains such
as human-computer interaction or sentiment anal-
ysis for opinion mining. Indeed, a large amount
of textual material has become available form the
Web (e.g. blogs, forums, social networks), rais-
ing the attractiveness of empirical methods analy-
sis on this field.
For representing the emotions, we exploit the
methodology described in (Strapparava and Mi-
halcea, 2008). The idea underlying the method is
the distinction between direct and indirect affec-
tive words.
For direct affective words (i.e. words that di-
rectly denote emotions), authors refer to the
WORDNET AFFECT (Strapparava and Valitutti,
2004) lexicon, a freely available extension of the
WORDNET database which employs some basic
emotion labels (e.g. anger, disgust, fear, joy, sad-
ness) to annotate WORDNET synsets.
For indirect affective words, a crucial aspect
is building a mechanism to represent an emotion
starting from affective lexical concepts and to in-
troduce a semantic similarity among generic terms
(and hence also words denoting colors) and these
emotion representations.
Latent Semantic Analysis is used to acquire,
in an unsupervised setting, a vector space from
the British National Corpus1. In LSA, term co-
occurrences in a corpus are captured by means of
a dimensionality reduction operated by a singu-
lar value decomposition on the term-by-document
matrix representing the corpus. LSA has the
advantage of allowing homogeneous representa-
tion and comparison of words, word sets (e.g.
synsets), text fragments or entire documents. For
representing word sets and texts by means of a
LSA vector, it is possible to use a variation of
the pseudo-document methodology described in
(Berry, 1992). This variation takes into account
also a tf-idf weighting schema. In practice, each
document can be represented in the LSA space by
summing up the normalized LSA vectors of all the
1BNC is a very large (over 100 million words) cor-
pus of modern English, both spoken and written (see
http://www.hcu.ox.ac.uk/bnc/). Other more spe-
cific corpora could also be considered, to obtain a more do-
main oriented similarity.
terms contained in it. Therefore a set of words
(and even all the words labeled with a particular
emotion) can be represented in the LSA space,
performing the pseudo-document technique on
them.
As stated in (Strapparava and Mihalcea, 2008),
each emotion can be represented in various ways
in the LSA space. The particular one that we are
employing is the ?LSA Emotion Synset? setting,
which has proved to give the best results in terms
of fine-grained emotion sensing. In this setting,
the synsets of direct emotion words, taken form
WORDNET AFFECT, are considered.
For our purposes, we compare the similarities
among the representations of colors and emotions
in the latent similarity space.
4 Experiments
For the experiments in this paper, we built an
LSA vector space on the full BNC corpus us-
ing 400 dimensions. To compare our approach
with the psycholinguistic experiments reported in
(Alt, 2008), we represent the following emotions:
anger, aversion/disgust, fear, happiness/joy, and
sadness. And we consider the colors Blue, Red,
Green, Orange, Purple, Yellow. Table 1 reports
the rankings of emotions according to colors from
(Alt, 2008).
Color Ranking of Emotions
Anger Aversion/
Disgust
Fear Joy Sadness
Blue 5 2 4 1 3
Red 1 4 2 3 5
Green 5 2 3 1 4
Orange 4 2 3 1 5
Purple 5 2 4 1 3
Yellow 5 2 4 1 3
Table 1: Emotions ranked by colors from psy-
cholinguistic experiments
In Table 2 we report our results of ranking emo-
tions with respect to colors using the similarity
mechanism described in the previous section. To
evaluate our results with respect to the psycholin-
guistic reference, we use Spearman correlation
coefficient. The resulting correlation between two
approaches for each color is reported in Table 3.
We can observe that the global correlation is
rather good (0.75). In particular, it is very high
30
Color Ranking Emotions using Similarity with Color
Anger Aversion/
Disgust
Fear Joy Sadness
Blue 4 2 3 1 5
Red 4 3 2 1 5
Green 4 2 3 1 5
Orange 4 2 3 1 5
Purple 5 2 3 1 4
Yellow 4 2 3 1 5
Table 2: Emotions ranked by similarity with col-
ors
Color Correlation
Blue 0.7
Red 0.3
Green 0.9
Orange 1.0
Purple 0.9
Yellow 0.7
Total 0.75
Table 3: Correlation
for the colors Orange, Green and Purple, which
implies that the use of language for these colors is
quite in accordance with psycholinguistic results.
The results are good for Blue and Yellow as well,
while the correlation is not so high for Red. This
could suggest that Red is a quite ambiguous color
with respect to emotions.
5 Conclusions
There are emotional and symbolic associations
with different colors. This is also revealed in our
daily use of language, as we frequently make ref-
erences to colors for increasing our expressiveness
by invoking different emotions. While most of
the research conducted so far with the aim of an-
alyzing the relationship between color and emo-
tion was based on psycholinguistic experiments,
the goal of this study was exploring this associ-
ation by employing a corpus-based approach for
affective sensing.
In order to show that our approach was provid-
ing reasonable results, we adopted one of the ex-
isting psycholinguistic experiments as a reference.
Following that adoption, we made a comparison
between the results of this research and our own
technique. Since we have observed that these two
results were highly correlated as we expected, we
would like to explore further this direction. Cer-
tainly different cultures can play a role for variant
emotional responses (Adams and Osgood, 1973).
Thus, as a next step, we are planning to investi-
gate how the perception of color by human be-
ings varies in different languages by again con-
ducting a computational analysis with NLP tech-
niques. Employing this approach could be very
useful and efficient for the design of applications
related to the fields of multimedia, automatic ad-
vertisement production, marketing and education
(e.g. e-learning environments)
In addition, based on our exploration about the
color perception of emotions from a corpus-based
point of view, we suggest that ?visual? informa-
tion regarding objects and events could be ex-
tracted from large amounts of text, using the same
kind of techniques proposed in the present paper.
This information can be easily exploited for cre-
ation of dictionaries or used in dynamic visualiza-
tion of text such as kinetic typography (Strappar-
ava et al, 2007). As a concrete example, our ap-
proach can be extended to discover the association
of colors not only with emotions, but also with in-
direct affective words in various languages. We
believe that the discovery of this kind of relation-
ship would allow us to automatically build col-
orful dictionaries, which could substantially help
users with both interpretation and memorization
processes.
References
Adams, F. M. and C. E. Osgood. 1973. A cross-
cultural study of the affective meanings of colour.
Journal of cross-cultural psychology, 4:135?156.
Alt, M. 2008. Emotional responses to color associated
with an advertisement. Master?s thesis, Graduate
College of Bowling Green State University, Bowl-
ing Green, Ohio.
Berry, M. 1992. Large-scale sparse singular value
computations. International Journal of Supercom-
puter Applications, 6(1):13?49.
Bresin, R. 2005. What is the color of that music
performance? In Proceedings of the International
Computer Music Conference (ICMA 2005), pages
367?370.
Gao, X.P., J.H. Xin, T. Sato, A. Hansuebsai, M. Scalzo,
K. Kajiwara, S. Guan, J. Valldeperas, M. Lis Jose,
31
and M. Billger. 2007. Analysis of cross-cultural
color emotion. Color research and application,
32(223?229).
Kaya, N. 2004. Relationship between color and emo-
tion: a study of college students. College Student
Journal, pages 396?405.
Madden, T. J., K. Hewett, and S. Roth Martin. 2000.
Managing images in different cultures: A cross-
national study of color meanings and preferences.
Journal of International Marketing, 8(4):90?107.
Ortony, A., G. L. Clore, and M. A. Foss. 1987.
The psychological foundations of the affective lexi-
con. Journal of Personality and Social Psychology,
53:751?766.
Pang, B. and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Soriano, C. and J. Valenzuela. 2009. Emotion and
colour across languages: implicit associations in
spanish colour terms. Social Science Information,
48:421?445, September.
Strapparava, C. and R. Mihalcea. 2007. SemEval-
2007 task 14: Affective Text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval 2007), pages 70?74, Prague, June.
Strapparava, C. and R. Mihalcea. 2008. Learning to
identify emotions in text. In SAC ?08: Proceedings
of the 2008 ACM symposium on Applied computing,
pages 1556?1560, New York, NY, USA. ACM.
Strapparava, C. and A. Valitutti. 2004. WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of LREC, volume 4, pages 1083?1086.
Strapparava, C., A. Valitutti, and O. Stock. 2007.
Dances with words. In Proc. of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07), Hyderabad, India, January.
Xin, J.H., K.M. Cheng, G. Taylor, T. Sato, and A. Han-
suebsai. 2004. A cross-regional comparison of
colour emotions. part I. quantitative analysis. Color
Research and Application, 29:451?457.
Zentner, M. R. 2001. Preferences for colors and color-
emotion combinations in early childhood. Develop-
mental Science, 4(4):389?398.
32
