Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223?1233,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Inducing Probabilistic CCG Grammars from Logical Form
with Higher-Order Unification
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
This paper addresses the problem of learn-
ing to map sentences to logical form, given
training data consisting of natural language
sentences paired with logical representations
of their meaning. Previous approaches have
been designed for particular natural languages
or specific meaning representations; here we
present a more general method. The approach
induces a probabilistic CCG grammar that
represents the meaning of individual words
and defines how these meanings can be com-
bined to analyze complete sentences. We
use higher-order unification to define a hy-
pothesis space containing all grammars con-
sistent with the training data, and develop
an online learning algorithm that efficiently
searches this space while simultaneously es-
timating the parameters of a log-linear parsing
model. Experiments demonstrate high accu-
racy on benchmark data sets in four languages
with two different meaning representations.
1 Introduction
A key aim in natural language processing is to learn
a mapping from natural language sentences to for-
mal representations of their meaning. Recent work
has addressed this problem by learning semantic
parsers given sentences paired with logical meaning
representations (Thompson & Mooney, 2002; Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006, 2007; Zettlemoyer & Collins, 2005,
2007; Lu et al, 2008). For example, the training
data might consist of English sentences paired with
lambda-calculus meaning representations:
Sentence: which states border texas
Meaning: ?x.state(x) ? next to(x, tex)
Given pairs like this, the goal is to learn to map new,
unseen, sentences to their corresponding meaning.
Previous approaches to this problem have been
tailored to specific natural languages, specific mean-
ing representations, or both. Here, we develop an
approach that can learn to map any natural language
to a wide variety of logical representations of lin-
guistic meaning. In addition to data like the above,
this approach can also learn from examples such as:
Sentence: hangi eyaletin texas ye siniri vardir
Meaning: answer(state(borders(tex)))
where the sentence is in Turkish and the meaning
representation is a variable-free logical expression
of the type that has been used in recent work (Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006; Lu et al, 2008).
The reason for generalizing to multiple languages
is obvious. The need to learn over multiple repre-
sentations arises from the fact that there is no stan-
dard representation for logical form for natural lan-
guage. Instead, existing representations are ad hoc,
tailored to the application of interest. For example,
the variable-free representation above was designed
for building natural language interfaces to databases.
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 1996, 2000).
A CCG grammar consists of a language-specific
lexicon, whose entries pair individual words and
phrases with both syntactic and semantic informa-
tion, and a universal set of combinatory rules that
1223
project that lexicon onto the sentences and meanings
of the language via syntactic derivations. The learn-
ing process starts by postulating, for each sentence
in the training data, a single multi-word lexical item
pairing that sentence with its complete logical form.
These entries are iteratively refined with a restricted
higher-order unification procedure (Huet, 1975) that
defines all possible ways to subdivide them, consis-
tent with the requirement that each training sentence
can still be parsed to yield its labeled meaning.
For the data sets we consider, the space of pos-
sible grammars is too large to explicitly enumerate.
The induced grammar is also typically highly am-
biguous, producing a large number of possible anal-
yses for each sentence. Our approach discriminates
between analyses using a log-linear CCG parsing
model, similar to those used in previous work (Clark
& Curran, 2003, 2007), but differing in that the syn-
tactic parses are treated as a hidden variable during
training, following the approach of Zettlemoyer &
Collins (2005, 2007). We present an algorithm that
incrementally learns the parameters of this model
while simultaneously exploring the space of possi-
ble grammars. The model is used to guide the pro-
cess of grammar refinement during training as well
as providing a metric for selecting the best analysis
for each new sentence.
We evaluate the approach on benchmark datasets
from a natural language interface to a database of
US Geography (Zelle & Mooney, 1996). We show
that accurate models can be learned for multiple
languages with both the variable-free and lambda-
calculus meaning representations introduced above.
We also compare performance to previous methods
(Kate & Mooney, 2006; Wong & Mooney, 2006,
2007; Zettlemoyer & Collins, 2005, 2007; Lu et al,
2008), which are designed with either language- or
representation- specific constraints that limit gener-
alization, as discussed in more detail in Section 6.
Despite being the only approach that is general
enough to run on all of the data sets, our algorithm
achieves similar performance to the others, even out-
performing them in several cases.
2 Overview of the Approach
The goal of our algorithm is to find a function
f : x ? z that maps sentences x to logical ex-
pressions z. We learn this function by inducing a
probabilistic CCG (PCCG) grammar from a train-
ing set {(xi, zi)|i = 1 . . . n} containing example
(sentence, logical-form) pairs such as (?New York
borders Vermont?, next to(ny, vt)). The induced
grammar consists of two components which the al-
gorithm must learn:
? A CCG lexicon, ?, containing lexical items
that define the space of possible parses y for
an input sentence x. Each parse contains both
syntactic and semantic information, and defines
the output logical form z.
? A parameter vector, ?, that defines a distribu-
tion over the possible parses y, conditioned on
the sentence x.
We will present the approach in two parts. The
lexical induction process (Section 4) uses a re-
stricted form of higher order unification along with
the CCG combinatory rules to propose new entries
for ?. The complete learning algorithm (Section 5)
integrates this lexical induction with a parameter es-
timation scheme that learns ?. Before presenting the
details, we first review necessary background.
3 Background
This section provides an introduction to the ways in
which we will use lambda calculus and higher-order
unification to construct meaning representations. It
also reviews the CCG grammar formalism and prob-
abilistic extensions to it, including existing parsing
and parameter estimation techniques.
3.1 Lambda Calculus and Higher-Order
Unification
We assume that sentence meanings are represented
as logical expressions, which we will construct from
the meaning of individual words by using the op-
erations defined in the lambda calculus. We use a
version of the typed lambda calculus (cf. Carpenter
(1997)), in which the basic types include e, for en-
tities; t, for truth values; and i for numbers. There
are also function types of the form ?e, t? that are as-
signed to lambda expressions, such as ?x.state(x),
which take entities and return truth values. We
represent the meaning of words and phrases using
1224
lambda-calculus expressions that can contain con-
stants, quantifiers, logical connectors, and lambda
abstractions.
The advantage of using the lambda calculus
lies in its generality. The meanings of individ-
ual words and phrases can be arbitrary lambda ex-
pressions, while the final meaning for a sentence
can take different forms. It can be a full lambda-
calculus expression, a variable-free expression such
as answer(state(borders(tex))), or any other log-
ical expression that can be built from the primitive
meanings via function application and composition.
The higher-order unification problem (Huet,
1975) involves finding a substitution for the free
variables in a pair of lambda-calculus expressions
that, when applied, makes the expressions equal
each other. This problem is notoriously complex;
in the unrestricted form (Huet, 1973), it is undecid-
able. In this paper, we will guide the grammar in-
duction process using a restricted version of higher-
order unification that is tractable. For a given ex-
pression h, we will need to find expressions for f
and g such that either h = f(g) or h = ?x.f(g(x)).
This limited form of the unification problem will al-
low us to define the ways to split h into subparts
that can be recombined with CCG parsing opera-
tions, which we will define in the next section, to
reconstruct h.
3.2 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a linguistic formalism
that tightly couples syntax and semantics, and
can be used to model a wide range of language
phenomena. For present purposes a CCG grammar
includes a lexicon ? with entries like the following:
New York ` NP : ny
borders ` S\NP/NP : ?x?y.next to(y, x)
Vermont ` NP : vt
where each lexical item w`X : h has words w, a
syntactic categoryX , and a logical form h expressed
as a lambda-calculus expression. For the first exam-
ple, these are ?New York,? NP , and ny. CCG syn-
tactic categories may be atomic (such as S, NP ) or
complex (such as S\NP/NP ).
CCG combines categories using a set of com-
binatory rules. For example, the forward (>) and
backward (<) application rules are:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
sentence New York borders Vermont can be parsed
to produce:
New York borders Vermont
NP (S\NP )/NP NP
ny ?x?y.next to(y, x) vt
>
(S\NP )
?y.next to(y, vt)
<
S
next to(ny, vt)
where each step in the parse is labeled with the com-
binatory rule (? > or ? <) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
These rules provide for a relaxed notion of con-
stituency which will be useful during learning as we
reason about possible refinements of the grammar.
We also allow vertical slashes in CCG categories,
which act as wild cards. For example, with this
extension the forward application combinator (>)
could be used to combine the category S/(S|NP )
with any of S\NP , S/NP , or S|NP . Figure 1
shows two parses where the composition combina-
tors and vertical slashes are used. These parses
closely resemble the types of analyses that will be
possible under the grammars we learn in the experi-
ments described in Section 8.
3.3 Probabilistic CCGs
Given a CCG lexicon ?, there will, in general, be
many possible parses for each sentence. We select
the most likely alternative using a log-linear model,
which consists of a feature vector ? and a parame-
ter vector ?. The joint probability of a logical form
z constructed with a parse y, given a sentence x is
1225
hangi eyaletin texas ye siniri vardir
S/NP NP/NP NP NP\NP
?x.answer(x) ?x.state(x) tex ?x.border(x)
<
NP
border(tex)
>
NP
state(border(tex))
>
S
answer(state(border(tex)))
what states border texas
S/(S|NP ) S|NP/(S|NP ) S\NP/NP NP
?f?x.f(x) ?f?x.state(x)?f(x) ?y?x.next to(x, y) tex
>B
S|NP/NP
?y?x.state(x) ? next to(x, y)
>
S|NP
?x.state(x) ? next to(x, tex)
>
S
?x.state(x) ? next to(x, tex)
Figure 1: Two examples of CCG parses with different logical form representations.
defined as:
P (y, z|x; ?,?) =
e???(x,y,z)
?
(y?,z?) e
???(x,y?,z?)
(1)
Section 7 defines the features used in the experi-
ments, which include, for example, lexical features
that indicate when specific lexical items in ? are
used in the parse y. For parsing and parameter es-
timation, we use standard algorithms (Clark & Cur-
ran, 2007), as described below.
The parsing, or inference, problem is to find the
most likely logical form z given a sentence x, as-
suming the parameters ? and lexicon ? are known:
f(x) = arg max
z
p(z|x; ?,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x; ?,?) =
?
y
p(y, z|x; ?,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi, zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
Oi = logP (zi|xi; ?,?). The local gradient of the
individual parameter ?j associated with feature ?j
and training instance (xi, zi) is given by:
?Oi
??j
= Ep(y|xi,zi;?,?)[?j(xi, y, zi)]
?Ep(y,z|xi;?,?)[?j(xi, y, z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. In the experiments,
each chart cell was pruned to the top 200 entries.
4 Splitting Lexical Items
Before presenting a complete learning algorithm, we
first describe how to use higher-order unification to
define a procedure for splitting CCG lexical entries.
This splitting process is used to expand the lexicon
during learning. We seed the lexical induction with
a multi-word lexical item xi`S :zi for each training
example (xi, zi), consisting of the entire sentence xi
and its associated meaning representation zi. For ex-
ample, one initial lexical item might be:
New York borders Vermont `S:next to(ny, vt) (5)
Although these initial, sentential lexical items
can parse the training data, they will not generalize
well to unseen data. To learn effectively, we will
need to split overly specific entries of this type into
pairs of new, smaller, entries that generalize better.
For example, one possible split of the lexical entry
given in (5) would be the pair:
New York borders ` S/NP : ?x.next to(ny, x),
Vermont `NP : vt
where we broke the original logical expression into
two new ones ?x.next to(ny, x) and vt, and paired
them with syntactic categories that allow the new
lexical entries to be recombined to produce the orig-
inal analysis. The next three subsections define the
set of possible splits for any given lexical item. The
process is driven by solving a higher-order unifica-
tion problem that defines all of the ways of splitting
the logical expression into two parts, as described in
Section 4.1. Section 4.2 describes how to construct
1226
syntactic categories that are consistent with the two
new fragments of logical form and which will allow
the new lexical items to recombine. Finally, Sec-
tion 4.3 defines the full set of lexical entry pairs that
can be created by splitting a lexical entry.
As we will see, this splitting process is overly pro-
lific for any single language and will yield many
lexical items that do not generalize well. For
example, there is nothing in our original lexical
entry above that provides evidence that the split
should pair ?Vermont? with the constant vt and not
?x.next to(ny, x). Section 5 describes how we
estimate the parameters of a probabilistic parsing
model and how this parsing model can be used to
guide the selection of items to add to the lexicon.
4.1 Restricted Higher-Order Unification
The set of possible splits for a logical expression
h is defined as the solution to a pair of higher-
order unification problems. We find pairs of logi-
cal expressions (f, g) such that either f(g) = h or
?x.f(g(x)) = h. Solving these problems creates
new expressions f and g that can be recombined ac-
cording to the CCG combinators, as defined in Sec-
tion 3.2, to produce h.
In the unrestricted case, there can be infinitely
many solution pairs (f, g) for a given expression h.
For example, when h = tex and f = ?x.tex, the
expression g can be anything. Although it would be
simple enough to forbid vacuous variables in f and
g, the number of solutions would still be exponen-
tial in the size of h. For example, when h contains a
conjunction, such as h = ?x.city(x)?major(x)?
in(x, tex), any subset of the expressions in the con-
junction can be assigned to f (or g).
To limit the number of possible splits, we enforce
the following restrictions on the possible higher-
order solutions that will be used during learning:
? No Vacuous Variables: Neither g or f can be a
function of the form ?x.e where the expression
e does not contain the variable x. This rules out
functions such as ?x.tex.
? Limited Coordination Extraction: The ex-
pression g cannot contain more than N of the
conjuncts that appear in any coordination in
h. For example, with N = 1 the expression
g = ?x.city(x)?major(x) could not be used
as a solution given the h conjuction above. We
use N = 4 in our experimental evaluation.
? Limited Application: The function f can-
not contain new variables applied to any non-
variable subexpressions from h. For example,
if h = ?x.in(x, tex), the pair f = ?q.q(tex)
and g = ?y?x.in(x, y) is forbidden.
Together, these three restrictions guarantee that
the number of splits is, in the worst case, an N -
degree polynomial of the number of constants in h.
The constraints were designed to increase the effi-
ciency of the splitting algorithm without impacting
performance on the development data.
4.2 Splitting Categories
We define the set of possible splits for a category
X :h with syntax X and logical form h by enumer-
ating the solution pairs (f, g) to the higher-order
unification problems defined above and creating
syntactic categories for the resulting expressions.
For example, given X :h = S\NP :?x.in(x, tex),
f = ?y?x.in(x, y), and g = tex, we would
produce the following two pairs of new categories:
( S\NP/NP :?y?x.in(x, y) , NP :tex )
( NP :tex , S\NP\NP :?y?x.in(x, y) )
which were constructed by first choosing the syntac-
tic category for g, in this caseNP , and then enumer-
ating the possible directions for the new slash in the
category containing f . We consider each of these
two steps in more detail below.
The new syntactic category for g is determined
based on its type, T (g). For example, T (tex) = e
and T (?x.state(x)) = ?e, t?. Then, the function
C(T ) takes an input type T and returns the syntactic
category of T as follows:
C(T ) =
?
?
?
NP if T = e
S if T = t
C(T2)|C(T1) when T = ?T1, T2?
The basic types e and t are assigned syntactic
categories NP and S, and all functional types
are assigned categories recursively. For exam-
ple C(?e, t?) = S|NP and C(?e, ?e, t??) =
S|NP |NP . This definition of CCG categories is
unconventional in that it never assigns atomic cate-
gories to functional types. For example, there is no
1227
distinct syntactic category N for nouns (which have
semantic type ?e, t?). Instead, the more complex cat-
egory S|NP is used.
Now, we are ready to define the set of all category
splits. For a category A = X:h we can define
SC(A) = {FA(A) ? BA(A) ? FC(A) ? BC(A)}
which is a union of sets, each of which includes
splits for a single CCG operator. For example,
FA(X:h) is the set of category pairs
FA(X:h) = {(X/Y :f, Y :g) | h=f(g) ? Y=C(T (g))
}
where each pair can be combined with the forward
application combinator, described in Section 3.2, to
reconstruct X:h.
The remaining three sets are defined similarly,
and are associated with the backward application
and forward and backward composition operators,
respectively:
BA(X:h) = {(Y :g,X\Y :f) | h=f(g) ? Y=C(T (g))
}
FC(X/Y :h) = {(X/W :f,W/Y :g) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
BC(X\Y :h) = {(W\Y :g,X\W :f) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
where the composition sets FC and BC only accept
input categories with the appropriate outermost slash
direction, for example FC(X/Y :h).
4.3 Splitting Lexical Items
We can now define the lexical splits that will be used
during learning. For lexical entry w0:n ` A, with
word sequence w0:n = ?w0, . . . , wn? and CCG cat-
egory A, define the set SL of splits to be:
SL(w0:n`A) = {(w0:i`B,wi+1:n`C) |
0 ? i < n ? (B,C) ? SC(A)}
where we enumerate all ways of splitting the words
sequence w0:n and aligning the subsequences with
categories in SC(A), as defined in the last section.
5 Learning Algorithm
The previous section described how a splitting pro-
cedure can be used to break apart overly specific
lexical items into smaller ones that may generalize
better to unseen data. The space of possible lexi-
cal items supported by this splitting procedure is too
large to explicitly enumerate. Instead, we learn the
parameters of a PCCG, which is used both to guide
the splitting process, and also to select the best parse,
given a learned lexicon.
Figure 2 presents the unification-based learning
algorithm, UBL. This algorithm steps through the
data incrementally and performs two steps for each
training example. First, new lexical items are in-
duced for the training instance by splitting and merg-
ing nodes in the best correct parse, given the current
parameters. Next, the parameters of the PCCG are
updated by making a stochastic gradient update on
the marginal likelihood, given the updated lexicon.
Inputs and Initialization The algorithm takes as
input the training set of n (sentence, logical form)
pairs {(xi, zi) : i = 1...n} along with an NP list,
?NP , of proper noun lexical items such as Texas`
NP :tex. The lexicon, ?, is initialized with a single
lexical item xi`S :zi for each of the training pairs
along with the contents of the NP list. It is possible
to run the algorithm without the initial NP list; we
include it to allow direct comparisons with previous
approaches, which also included NP lists. Features
and initial feature weights are described in Section 7.
Step 1: Updating the Lexicon In the lexical up-
date step the algorithm first computes the best cor-
rect parse tree y? for the current training exam-
ple and then uses y? as input to the procedure
NEW-LEX, which determines which (if any) new
lexical items to add to ?. NEW-LEX begins by enu-
merating all pairs (C,wi:j), for i < j, where C is a
category occurring at a node in y? and wi:j are the
(two or more) words it spans. For example, in the
left parse in Figure 1, there would be four pairs: one
with the category C = NP\NP :?x.border(x) and
the phrase wi:j =?ye siniri vardir?, and one for each
non-leaf node in the tree.
For each pair (C,wi:j), NEW-LEX considers in-
troducing a new lexical item wi:j`C, which allows
for the possibility of a parse where the subtree rooted
at C is replaced with this new entry. (If C is a leaf
node, this item will already exist.) NEW-LEX also
considers adding each pair of new lexical items that
is obtained by splitting wi:j`C as described in Sec-
tion 4, thereby considering many different ways of
reanalyzing the node. This process creates a set of
possible new lexicons, where each lexicon expands
1228
? in a different way by adding the items from either
a single split or a single merge of a node in y?.
For each potential new lexicon ??, NEW-LEX
computes the probability p(y?|xi, zi; ??,??) of the
original parse y? under ?? and parameters ?? that are
the same as ? but have weights for the new lexical
items, as described in Section 7. It also finds the
best new parse y? = arg maxy p(y|xi, zi; ??,??).1
Finally, NEW-LEX selects the ?? with the largest
difference in log probability between y? and y?, and
returns the new entries in ??. If y? is the best parse
for every ??, NEW-LEX returns the empty set; the
lexicon will not change.
Step 2: Parameter Updates For each training ex-
ample we update the parameters ? using the stochas-
tic gradient updates given by Eq. 4.
Discussion The alternation between refining the
lexicon and updating the parameters drives the learn-
ing process. The initial model assigns a conditional
likelihood of one to each training example (there
is a single lexical item for each sentence xi, and
it contains the labeled logical form zi). Although
the splitting step often decreases the probability of
the data, the new entries it produces are less spe-
cific and should generalize better. Since we initially
assign positive weights to the parameters for new
lexical items, the overall approach prefers splitting;
trees with many lexical items will initially be much
more likely. However, if the learned lexical items
are used in too many incorrect parses, the stochastic
gradient updates will down weight them to the point
where the lexical induction step can merge or re-split
nodes in the trees that contain them. This allows the
approach to correct the lexicon and, hopefully, im-
prove future performance.
6 Related Work
Previous work has focused on a variety of different
meaning representations. Several approaches have
been designed for the variable-free logical repre-
sentations shown in examples throughout this pa-
per. For example, Kate & Mooney (2006) present a
method (KRISP) that extends an existing SVM learn-
ing algorithm to recover logical representations. The
1This computation can be performed efficiently by incre-
mentally updating the parse chart used to find y?.
Inputs: Training set {(xi, zi) : i = 1 . . . n} where each
example is a sentence xi paired with a logical form
zi. Set of NP lexical items ?NP . Number of iter-
ations T . Learning rate parameter ?0 and cooling
rate parameter c.
Definitions: The function NEW-LEX(y) takes a parse
y and returns a set of new lexical items found by
splitting and merging categories in y, as described
in Section 5. The distributions p(y|x, z; ?,?) and
p(y, z|x; ?,?) are defined by the log-linear model,
as described in Section 3.3.
Initialization:
? Set ? = {xi ` S : zi} for all i = 1 . . . n.
? Set ? = ? ? ?NP
? Initialize ? using coocurrence statistics, as de-
scribed in Section 7.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Update Lexicon)
? Let y? = arg maxy p(y|xi, zi; ?,?)
? Set ? = ? ?NEW-LEX(y?) and expand the
parameter vector ? to contain entries for the
new lexical items, as described in Section 7.
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t? n.
? Let ? = Ep(y|xi,zi;?,?)[?(xi, y, zi)]
?Ep(y,z|xi;?,?)[?(xi, y, z)]
? Set ? = ? + ??
Output: Lexicon ? and parameters ?.
Figure 2: The UBL learning algorithm.
WASP system (Wong & Mooney, 2006) uses statis-
tical machine translation techniques to learn syn-
chronous context free grammars containing both
words and logic. Lu et al (2008) (Lu08) developed
a generative model that builds a single hybrid tree
of words, syntax and meaning representation. These
algorithms are all language independent but repre-
sentation specific.
Other algorithms have been designed to re-
cover lambda-calculus representations. For exam-
ple, Wong & Mooney (2007) developed a variant
of WASP (?-WASP) specifically designed for this
alternate representation. Zettlemoyer & Collins
(2005, 2007) developed CCG grammar induction
techniques where lexical items are proposed accord-
ing to a set of hand-engineered lexical templates.
1229
Our approach eliminates this need for manual effort.
Another line of work has focused on recover-
ing meaning representations that are not based on
logic. Examples include an early statistical method
for learning to fill slot-value representations (Miller
et al, 1996) and a more recent approach for recover-
ing semantic parse trees (Ge & Mooney, 2006). Ex-
ploring the extent to which these representations are
compatible with the logic-based learning approach
we developed is an important area for future work.
Finally, there is work on using categorial gram-
mars to solve other, related learning problems.
For example, Buszkowski & Penn (1990) describe
a unification-based approach for grammar discov-
ery from bracketed natural language sentences and
Villavicencio (2002) developed an approach for
modeling child language acquisition. Additionally,
Bos et al (2004) consider the challenging problem
of constructing broad-coverage semantic representa-
tions with CCG, but do not learn the lexicon.
7 Experimental Setup
Features We use two types of features in our
model. First, we include a set of lexical features:
For each lexical item L ? ?, we include a feature
?L that fires when L is used. Second, we include se-
mantic features that are functions of the output logi-
cal expression z. Each time a predicate p in z takes
an argument a with type T (a) in position i it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,T (a),i) for the
predicate argument-type relation.
Initialization The weights for the semantic fea-
tures are initialized to zero. The weights for the lex-
ical features are initialized according to coocurrance
statistics estimated with the Giza++ (Och & Ney,
2003) implementation of IBM Model 1. We com-
pute translation scores for (word, constant) pairs that
cooccur in examples in the training data. The initial
weight for each ?L is set to ten times the average
score over the (word, constant) pairs in L, except for
the weights of seed lexical entries in ?NP which are
set to 10 (equivalent to the highest possible coocur-
rence score). We used the learning rate ?0 = 1.0
and cooling rate c = 10?5 in all training scenar-
ios, and ran the algorithm for T = 20 iterations.
These values were selected with cross validation on
the Geo880 development set, described below.
Data and Evaluation We evaluate our system
on the GeoQuery datasets, which contain natural-
language queries of a geographical database paired
with logical representations of each query?s mean-
ing. The full Geo880 dataset contains 880 (English-
sentence, logical-form) pairs, which we split into a
development set of 600 pairs and a test set of 280
pairs, following Zettlemoyer & Collins (2005). The
Geo250 dataset is a subset of Geo880 containing
250 sentences that have been translated into Turk-
ish, Spanish and Japanese as well as the original En-
glish. Due to the small size of this dataset we use
10-fold cross validation for evaluation. We use the
same folds as Wong & Mooney (2006, 2007) and Lu
et al (2008), allowing a direct comparison.
The GeoQuery data is annotated with both
lambda-calculus and variable-free meaning rep-
resentations, which we have seen examples of
throughout the paper. We report results for both rep-
resentations, using the standard measures of Recall
(percentage of test sentences assigned correct log-
ical forms), Precision (percentage of logical forms
returned that are correct) and F1 (the harmonic mean
of Precision and Recall).
Two-Pass Parsing To investigate the trade-off be-
tween precision and recall, we report results with a
two-pass parsing strategy. When the parser fails to
return an analysis for a test sentence due to novel
words or usage, we reparse the sentence and allow
the parser to skip words, with a fixed cost. Skip-
ping words can potentially increase recall, if the ig-
nored word is an unknown function word that does
not contribute semantic content.
8 Results and Discussion
Tables 1, 2, and 3 present the results for all of the ex-
periments. In aggregate, they demonstrate that our
algorithm, UBL, learns accurate models across lan-
guages and for both meaning representations. This
is a new result; no previous system is as general.
We also see the expected tradeoff between preci-
sion and recall that comes from the two-pass parsing
approach, which is labeled UBL-s. With the abil-
ity to skip words, UBL-s achieves the highest recall
of all reported systems for all evaluation conditions.
1230
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
WASP 70.0 95.4 80.8 72.4 91.2 81.0
Lu08 72.8 91.5 81.1 79.2 95.2 86.5
UBL 78.1 88.2 82.7 76.8 86.8 81.4
UBL-s 80.4 80.8 80.6 79.7 80.6 80.1
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
WASP 74.4 92.0 82.9 62.4 97.0 75.9
Lu08 76.0 87.6 81.4 66.8 93.8 78.0
UBL 78.5 85.5 81.8 70.4 89.4 78.6
UBL-s 80.5 80.6 80.6 74.2 75.6 74.9
Table 1: Performance across languages on Geo250 with
variable-free meaning representations.
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 78.0 93.2 84.7 75.9 93.4 83.6
UBL-s 81.8 83.5 82.6 81.4 83.4 82.4
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 78.9 90.9 84.4 67.4 93.4 78.1
UBL-s 83.0 83.2 83.1 71.8 77.8 74.6
Table 2: Performance across languages on Geo250 with
lambda-calculus meaning representations.
However, UBL achieves much higher precision and
better overall F1 scores, which are generally compa-
rable to the best performing systems.
The comparison to the CCG induction techniques
of ZC05 and ZC07 (Table 3) is particularly striking.
These approaches used language-specific templates
to propose new lexical items and also required as in-
put a set of hand-engineered lexical entries to model
phenomena such as quantification and determiners.
However, the use of higher-order unification allows
UBL to achieve comparable performance while au-
tomatically inducing these types of entries.
For a more qualitative evaluation, Table 4 shows a
selection of lexical items learned with high weights
for the lambda-calculus meaning representations.
Nouns such as ?state? or ?estado? are consistently
learned across languages with the category S|NP ,
which stands in for the more conventional N . The
algorithm also learns language-specific construc-
tions such as the Japanese case markers ?no? and
?wa?, which are treated as modifiers that do not add
semantic content. Language-specific word order is
also encoded, using the slash directions of the CCG
System
Variable Free Lambda Calculus
Rec. Pre. F1 Rec. Pre. F1
Cross Validation Results
KRISP 71.7 93.3 81.1 ? ? ?
WASP 74.8 87.2 80.5 ? ? ?
Lu08 81.5 89.3 85.2 ? ? ?
?-WASP ? ? ? 86.6 92.0 89.2
Independent Test Set
ZC05 ? ? ? 79.3 96.3 87.0
ZC07 ? ? ? 86.1 91.6 88.8
UBL 81.4 89.4 85.2 85.0 94.1 89.3
UBL-s 84.3 85.2 84.7 87.9 88.5 88.2
Table 3: Performance on the Geo880 data set, with varied
meaning representations.
categories. For example, ?what? and ?que? take
their arguments to the right in the wh-initial English
and Spanish. However, the Turkish wh-word ?nel-
erdir? and the Japanese question marker ?nan desu
ka? are sentence final, and therefore take their argu-
ments to the left. Learning regularities of this type
allows UBL to generalize well to unseen data.
There is less variation and complexity in the
learned lexical items for the variable-free represen-
tation. The fact that the meaning representation is
deeply nested influences the form of the induced
grammar. For example, recall that the sentence
?what states border texas? would be paired with the
meaning answer(state(borders(tex))). For this
representation, lexical items such as:
what ` S/NP : ?x.answer(x)
states `NP/NP : ?x.state(x)
border `NP/NP : ?x.borders(x)
texas `NP : tex
can be used to construct the desired output. In
practice, UBL often learns entries with only a sin-
gle slash, like those above, varying only in the di-
rection, as required for the language. Even the
more complex items, such as those for quantifiers,
are consistently simpler than those induced from
the lambda-calculus meaning representations. For
example, one of the most complex entries learned
in the experiments for English is the smallest `
NP\NP/(NP |NP ):?f?x.smallest one(f(x)).
There are also differences in the aggregate statis-
tics of the learned lexicons. For example, the aver-
age length of a learned lexical item for the (lambda-
calculus, variable-free) meaning representations is:
1231
(1.21,1.08) for Turkish, (1.34,1.19) for English,
(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.
For both meaning representations the model learns
significantly more multiword lexical items for the
somewhat analytic Japanese than the agglutinative
Turkish. There are also variations in the average
number of learned lexical items in the best parses
during the final pass of training: 192 for Japanese,
206 for Spanish, 188 for English and 295 for Turk-
ish. As compared to the other languages, the mor-
pologically rich Turkish requires significantly more
lexical variation to explain the data.
Finally, there are a number of cases where the
UBL algorithm could be improved in future work.
In cases where there are multiple allowable word or-
ders, the UBL algorithm must learn individual en-
tries for each possibility. For example, the following
two categories are often learned with high weight for
the Japanese word ?chiisai?:
NP/(S|NP )\(NP |NP ):?f?g.argmin(x, g(x), f(x))
NP |(S|NP )/(NP |NP ):?f?g.argmin(x, g(x), f(x))
and are treated as distinct entries in the lexicon. Sim-
ilarly, the approach presented here does not model
morphology, and must repeatedly learn the correct
categories for the Turkish words ?nehri,? ?nehir,?
?nehirler,? and ?nehirlerin?, all of which correspond
to the logical form ?x.river(x).
9 Conclusions and Future Work
This paper has presented a method for inducing
probabilistic CCGs from sentences paired with log-
ical forms. The approach uses higher-order unifi-
cation to define the space of possible grammars in
a language- and representation-independent manner,
paired with an algorithm that learns a probabilistic
parsing model. We evaluated the approach on four
languages with two meaning representations each,
achieving high accuracy across all scenarios.
For future work, we are interested in exploring
the generality of the approach while extending it to
new understanding problems. One potential limi-
tation is in the constraints we introduced to ensure
the tractability of the higher-order unification proce-
dure. These restrictions will not allow the approach
to induce lexical items that would be used with,
among other things, many of the type-raised combi-
nators commonly employed in CCG grammars. We
English
population of ` NP/NP : ?x.population(x)
smallest ` NP/(S|NP ) : ?f.arg min(y, f(y), size(y))
what ` S|NP/(S|NP ) : ?f?x.f(x)
border ` S|NP/NP : ?x?y.next to(y, x)
state ` S|NP : ?x.state(x)
most ` NP/(S|NP )\(S|NP )\(S|NP |NP ) :
?f?g?h?x.argmax(y, g(y), count(z, f(z, y) ? h(z)))
Japanese
no ` NP |NP/(NP |NP ) : ?f?x.f(x)
shuu ` S|NP : ?x.state(x)
nan desu ka ` S\NP\(NP |NP ) : ?f?x.f(x)
wa ` NP |NP\(NP |NP ) : ?f?x.f(x)
ikutsu ` NP |(S|NP )\(S|NP |(S|NP )) :
?f?g.count(x, f(g(x)))
chiiki ` NP\NP :?x.area(x)
Turkish
nedir ` S\NP\(NP |NP ) : ?f?x.f(x)
sehir ` S|NP : ?x.city(x)
nufus yogunlugu ` NP |NP : ?x.density(x)
siniri` S|NP/NP : ?x?y.next to(y, x)
kac tane ` S\NP/(S|NP |NP )\(S|NP ) :
?f?g?x.count(y, f(y) ? g(y, x))
ya siniri ` S|NP\NP : ?x?y.next to(y, x)
Spanish
en ` S|NP/NP : ?x?y.loc(y, x)
que es la ` S/NP/(NP |NP ): ?f?x.f(x)
pequena ` NP\(S|NP )\(NP |NP ) :
?g?f.arg min(y, f(y), g(y))
estado ` S|NP : ?x.state(x)
mas ` S\(S|NP )/(S|NP )\(NP |NP |(S|NP )) :
?f?g?h.argmax(x, h(x), f(g, x))
mayores `S|NP\(S|NP ) :?f?x.f(x) ?major(x)
Table 4: Example learned lexical items for each language
on the Geo250 lambda-calculus data sets.
are also interested in developing similar grammar
induction techniques for context-dependent under-
standing problems, such as the one considered by
Zettlemoyer & Collins (2009). Such an approach
would complement ideas for using high-order unifi-
cation to model a wider range of language phenom-
ena, such as VP ellipsis (Dalrymple et al, 1991).
Acknowledgements
We thank the reviewers for useful feedback. This
work was supported by the EU under IST Cog-
nitive Systems grant IP FP6-2004-IST-4-27657
?Paco-Plus? and ERC Advanced Fellowship 249520
?GRAMPLUS? to Steedman. Kwiatkowski was
supported by an EPRSC studentship. Zettlemoyer
was supported by a US NSF International Research
Fellowship.
1232
References
Bos, J., Clark, S., Steedman, M., Curran, J. R., & Hock-
enmaier, J. (2004). Wide-coverage semantic represen-
tations from a CCG parser. In Proceedings of the In-
ternational Conference on Computational Linguistics.
Buszkowski, W. & Penn, G. (1990). Categorial grammars
determined from linguistic data by unification. Studia
Logica, 49, 431?454.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Clark, S. & Curran, J. R. (2003). Log-linear models
for wide-coverage CCG parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Clark, S. & Curran, J. R. (2007). Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Dalrymple, M., Shieber, S., & Pereira, F. (1991). Ellipsis
and higher-order unification. Linguistics and Philoso-
phy, 14, 399?452.
Ge, R. & Mooney, R. J. (2006). Discriminative rerank-
ing for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Huet, G. (1975). A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1, 27?57.
Huet, G. P. (1973). The undecidability of unification in
third order logic. Information and Control, 22(3), 257?
267.
Kate, R. J. & Mooney, R. J. (2006). Using string-kernels
for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Kate, R. J., Wong, Y. W., & Mooney, R. J. (2005). Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278?2324.
Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S.
(2008). A generative model for parsing natural lan-
guage to meaning representations. In Proceedings of
The Conference on Empirical Methods in Natural Lan-
guage Processing.
Miller, S., Stallard, D., Bobrow, R. J., & Schwartz, R. L.
(1996). A fully statistical approach to natural language
interfaces. In Proc. of the Association for Computa-
tional Linguistics.
Och, F. J. & Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), 19?51.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Thompson, C. A. & Mooney, R. J. (2002). Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research, 18.
Villavicencio, A. (2002). The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis,
University of Cambridge.
Wong, Y. W. & Mooney, R. (2006). Learning for seman-
tic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL.
Wong, Y. W. & Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Zelle, J. M. & Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L. S. & Collins, M. (2005). Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L. S. & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical form.
In Proc. of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Zettlemoyer, L. S. & Collins, M. (2009). Learning
context-dependent mappings from sentences to logical
form. In Proceedings of The Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
1233
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512?1523,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lexical Generalization in CCG Grammar Induction for Semantic Parsing
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
We consider the problem of learning fac-
tored probabilistic CCG grammars for seman-
tic parsing from data containing sentences
paired with logical-form meaning representa-
tions. Traditional CCG lexicons list lexical
items that pair words and phrases with syntac-
tic and semantic content. Such lexicons can
be inefficient when words appear repeatedly
with closely related lexical content. In this
paper, we introduce factored lexicons, which
include both lexemes to model word meaning
and templates to model systematic variation in
word usage. We also present an algorithm for
learning factored CCG lexicons, along with a
probabilistic parse-selection model. Evalua-
tions on benchmark datasets demonstrate that
the approach learns highly accurate parsers,
whose generalization performance benefits
greatly from the lexical factoring.
1 Introduction
Semantic parsers automatically recover representa-
tions of meaning from natural language sentences.
Recent work has focused on learning such parsers
directly from corpora made up of sentences paired
with logical meaning representations (Kate et al,
2005; Kate and Mooney, 2006; Wong and Mooney,
2006, 2007; Zettlemoyer and Collins, 2005, 2007;
Lu et al, 2008; Kwiatkowski et al, 2010).
For example, in a flight booking domain we
might have access to training examples such as:
Sentence: I want flights from Boston
Meaning: ?x. f light(x)? f rom(x,bos)
and the goal is to learn a grammar that can map new,
unseen, sentences onto their corresponding mean-
ings, or logical forms.
One approach to this problem has developed al-
gorithms for leaning probabilistic CCG grammars
(Zettlemoyer and Collins, 2005, 2007; Kwiatkowski
et al, 2010). These grammars are well-suited to the
task of semantic parsing, as they closely link syn-
tax and semantics. They can be used to model a
wide range of complex linguistic phenomena and are
strongly lexicalized, storing all language-specific
grammatical information directly with the words in
the lexicon. For example, a typical learned lexicon
might include entries such as:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
(3) f light `N\N :? f?x. f light(x)? f (x)
(4) f are`N :?x.cost(x)
(5) f are`N/(S|NP) :? f?x.cost(x)? f (x)
(6) f are`N\N :? f?x.cost(x)? f (x)
(7) Boston`NP :bos
(8) Boston`N\N :? f?x. f rom(x,bos)? f (x)
(9) New York `NP :nyc
(10) New York `N\N :? f?x. f rom(x,nyc)? f (x)
Although lexicalization of this kind is useful
for learning, as we will see, these grammars can
also suffer from sparsity in the training data, since
closely related entries must be repeatedly learned for
all members of a certain class of words. For exam-
ple, the list above shows a selection of lexical items
that would have to be learned separately.
In this list, the word ?flight? is paired with the
predicate flight in three separate lexical items which
are required for different syntactic contexts. Item
1512
(1) has the standard N category for entries of this
type, item (2) allows the use of the word ?flight?
with that-less relative clauses such as ?flight depart-
ing Boston?, and item (3) is useful for phrases with
unconventional word order such as ?from Boston
flight to New York?. Representing these three lexi-
cal items separately is inefficient, since each word of
this class (such as ?fare?) will require three similarly
structured lexical entries differing only in predicate
name. There may also be systemtatic semantic vari-
ation between entries for a certain class of words.
For example, in (6) ?Boston? is paired with the con-
stant bos that represents its meaning. However, item
(7) also adds the predicate from to the logical form.
This might be used to analyse somewhat elliptical,
unedited sentences such as ?Show me flights Boston
to New York,? which can be challenging for seman-
tic parsers (Zettlemoyer and Collins, 2007).
This paper builds upon the insight that a large pro-
portion of the variation between lexical items for
a given class of words is systematic. Therefore it
should be represented once and applied to a small set
of basic lexical units. 1 We develop a factored lex-
icon that captures this insight by distinguishing lex-
emes, which pair words with logical constants, from
lexical templates, which map lexemes to full lexical
items. As we will see, this can lead to a significantly
more compact lexicon that can be learned from less
data. Each word or phrase will be associated with a
few lexemes that can be combined with a shared set
of general templates.
We develop an approach to learning factored,
probabilistic CCG grammars for semantic pars-
ing. Following previous work (Kwiatkowski et al,
2010), we make use of a higher-order unification
learning scheme that defines a space of CCG gram-
mars consistent with the (sentence, logical form)
training pairs. However, instead of constructing
fully specified lexical items for the learned grammar,
we automatically generate sets of lexemes and lexi-
cal templates to model each example. This is a dif-
ficult learning problem, since the CCG analyses that
1A related tactic is commonly used in wide-coverage CCG
parsers derived from treebanks, such as work by Hockenmaier
and Steedman (2002) and Clark and Curran (2007). These
parsers make extensive use of category-changing unary rules,
to avoid data sparsity for systematically related categories (such
as those related by type-raising). We will automatically learn to
represent these types of generalizations in the factored lexicon.
are required to construct the final meaning represen-
tations are not explicitly labeled in the training data.
Instead, we model them with hidden variables and
develop an online learning approach that simultane-
ously estimates the parameters of a log-linear pars-
ing model, while inducing the factored lexicon.
We evaluate the approach on the benchmark Atis
and GeoQuery domains. This is a challenging setup,
since the GeoQuery data has complex meaning rep-
resentations and sentences in multiple languages,
while the Atis data contains spontaneous, unedited
text that can be difficult to analyze with a formal
grammar representation. Our approach achieves at
or near state-of-the-art recall across all conditions,
despite having no English or domain-specific infor-
mation built in. We believe that ours is the only sys-
tem of sufficient generality to run with this degree of
success on all of these datasets.
2 Related work
There has been significant previous work on learn-
ing semantic parsers from training sentences la-
belled with logical form meaning representations.
We extend a line of research that has addressed
this problem by developing CCG grammar induc-
tion techniques. Zettlemoyer and Collins (2005,
2007) presented approaches that use hand gener-
ated, English-language specific rules to generate lex-
ical items from logical forms as well as English
specific type-shifting rules and relaxations of the
CCG combinators to model spontaneous, unedited
sentences. Zettlemoyer and Collins (2009) extends
this work to the case of learning in context depen-
dent environments. Kwiatkowski et al (2010) de-
scribed an approach for language-independent learn-
ing that replaces the hand-specified templates with
a higher-order-unification-based lexical induction
method, but their approach does not scale well to
challenging, unedited sentences. The learning ap-
proach we develop for inducing factored lexicons is
also language independent, but scales well to these
challenging sentences.
There have been a number of other approaches
for learning semantic parsers, including ones based
on machine translation techniques (Papineni et al,
1997; Ramaswamy and Kleindienst, 2000; Wong
and Mooney, 2006), parsing models (Miller et al,
1996; Ge and Mooney, 2006; Lu et al, 2008), in-
1513
ductive logic programming algorithms (Zelle and
Mooney, 1996; Thompson and Mooney, 2002; Tang
and Mooney, 2000), probabilistic automata (He and
Young, 2005, 2006), and ideas from string kernels
and support vector machines (Kate and Mooney,
2006; Nguyen et al, 2006).
More recent work has focused on training se-
mantic parsers without supervision in the form of
logical-form annotations. Clarke et al (2010) and
Liang et al (2011) replace semantic annotations in
the training set with target answers which are more
easily available. Goldwasser et al (2011) present
work on unsupervised learning of logical form struc-
ture. However, all of these systems require signifi-
cantly more domain and language specific initializa-
tion than the approach presented here.
Other work has learnt semantic analyses from text
in the context of interactions in computational envi-
ronments (Branavan et al (2010), Vogel and Juraf-
sky (2010)); text grounded in partial observations of
a world state (Liang et al, 2009); and from raw text
alone (Poon and Domingos, 2009, 2010).
There is also related work that uses the CCG
grammar formalism. Clark and Curran (2003)
present a method for learning the parameters of a
log-linear CCG parsing model from fully annotated
normal?form parse trees. Watkinson and Manand-
har (1999) describe an unsupervised approach for
learning syntactic CCG lexicons. Bos et al (2004)
present an algorithm for building semantic represen-
tations from CCG parses but requires fully?specified
CCG derivations in the training data.
3 Overview of the Approach
Here we give a formal definition of the problem and
an overview of the learning approach.
Problem We will learn a semantic parser that
takes a sentences x and returns a logical form z repre-
senting its underlying meaning. We assume we have
input data {(xi,zi)|i = 1 . . .n} containing sentences
xi and logical forms zi, for example xi =?Show me
flights to Boston? and zi = ?x. f light(x)? to(x,bos).
Model We will represent the parser as a factored,
probabilistic CCG (PCCG) grammar. A traditional
CCG lexical item would fully specify the syntax and
semantics for a word (reviewed in Section 4). For
example, Boston`NP : bos represents the entry for
the word ?Boston? with syntactic category NP and
meaning represented by the constant bos. Where a
lexicon would usually list lexical items such as this,
we instead use a factored lexicon (L,T ) containing:
? A list of lexemes L. Each lexeme pairs a word
or phrase with a list of logical constants that can
be used to construct its meaning. For example,
one lexeme might be (Boston, [bos]).
? A list of lexical templates T . Each template
takes a lexeme and maps it on to a full lexical
item. For example, there is a single template
that can map the lexeme above to the final lex-
ical entry Boston `NP : bos.
We will make central use of this factored repre-
sentation to provide a more compact representation
of the lexicon that can be learned efficiently.
The factored PCCG will also contain a parameter
vector, ? , that defines a log-linear distribution over
the possible parses y, conditioned on the sentence x.
Learning Our approach for learning factored PC-
CGs extends the work of Kwiatkowski et al (2010),
as reviewed in Section 7. Specifically, we modify
the lexical learning, to produce lexemes and tem-
plates, as well as the feature space of the model, but
reuse the existing parameter estimation techniques
and overall learning cycle, as described in Section 7.
We present the complete approach in three parts
by describing the factored representation of the lex-
icon (Section 5), techniques for proposing potential
new lexemes and templates (Section 6), and finally
a complete learning algorithm (Section 7). How-
ever, the next section first reviews the required back-
ground on semantic parsing with CCG.
4 Background
4.1 Lambda Calculus
We represent the meanings of sentences, words
and phrases with logical expressions that can con-
tain constants, quantifiers, logical connectors and
lambda abstractions. We construct the meanings of
sentences from the meanings of words and phrases
using lambda-calculus operations. We use a version
of the typed lambda calculus (Carpenter, 1997), in
which the basic types include e, for entities; t, for
truth values; and i for numbers. We also have func-
tion types that are assigned to lambda expressions.
1514
The expression ?x. f light(x) takes an entity and re-
turns a truth value, and has the function type ?e, t?.
4.2 Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a linguistic formal-
ism that tightly couples syntax and semantics, and
can be used to model a wide range of language phe-
nomena. A traditional CCG grammar includes a lex-
icon ? with entries like the following:
f lights`N :?x. f light(x)
to` (N\N)/NP :?y.? f .?x. f (x)? to(x,y)
Boston`NP :bos
where each lexical item w`X : h has words w, a syn-
tactic category X , and a logical form h. For the first
example, these are ?flights,? N, and ?x. f light(x).
In this paper, we introduce a new way of represent-
ing lexical items as (lexeme, template) pairs, as de-
scribed in section 5.
CCG syntactic categories may be atomic (such
as S or NP) or complex (such as (N\N)/NP)
where the slash combinators encode word order
information. CCG uses a small set of combinatory
rules to build syntactic parses and semantic repre-
sentations concurrently. Two example combinatory
rules are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f (g) (>)
Y : g X\Y : f ? X : f (g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
phrase ?flights to Boston? can be parsed to produce:
flights to Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? to(x,y) bos
>
(N\N)? f?x. f (x)? to(x,bos)
<N?x. f light(x)? to(x,bos)
where each step in the parse is labeled with the com-
binatory rule (?> or ?<) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g? X/Z : ?x. f (g(x)) (> B)
Y\Z : g X\Y : f ? X\Z : ?x. f (g(x)) (< B)
These rules allow a relaxed notion of constituency
which helps limit the number of distinct CCG lexical
items required.
To the standard forward and backward slashes of
CCG we also add a vertical slash for which the di-
rection of application is underspecified. We shall see
examples of this in Section 10.
4.3 Probabilistic CCGs
Due to ambiguity in both the CCG lexicon and the
order in which combinators are applied, there will
be many parses for each sentence. We discriminate
between competing parses using a log-linear model
which has a feature vector ? and a parameter vector
? . The probability of a parse y that returns logical
form z, given a sentence x is defined as:
P(y,z|x;? ,?) = e
? ??(x,y,z)
?(y?,z?) e? ??(x,y?,z?) (1)
Section 8 fully defines the set of features used in the
system presented. The most important of these con-
trol the generation of lexical items from (lexeme,
template) pairs. Each (lexeme, template) pair used
in a parse fires three features as we will see in more
detail later.
The parsing, or inference, problem done at test
time requires us to find the most likely logical form
z given a sentence x, assuming the parameters ? and
lexicon ? are known:
f (x) = argmaxz p(z|x;? ,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x;? ,?) =?
y
p(y,z|x;? ,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi,zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
1515
Oi = logP(zi|xi;? ,?). The local gradient of the in-
dividual parameter ? j associated with feature ? j and
training instance (xi,zi) is given by:
?Oi
?? j = Ep(y|xi,zi;? ,?)[? j(xi,y,zi)]
?Ep(y,z|xi;? ,?)[? j(xi,y,z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. For a sentence
of length m, each parse chart span is pruned using
a beam width proportional to m 23 , to allow larger
beams for shorter sentences.
5 Factored Lexicons
A factored lexicon includes a set L of lexemes and
a set T of lexical templates. In this section, we for-
mally define these sets, and describe how they are
used to build CCG parses. We will use a set of lex-
ical items from our running example to discuss the
details of how the following lexical items:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
. . .
(6) Boston`NP :bos
(7) Boston`N\N :? f?x. f rom(x,bos)? f (x)
are constructed from specific lexemes and templates.
5.1 Lexemes
A lexeme (w,~c) pairs a word sequence w with an
ordered list of logical constants ~c = [c1 . . .cm]. For
example, item (1) and (2) above would come from
a single lexeme (flight, [ f light]). Similar lexemes
would be represented for other predicates, for exam-
ple (fare, [cost]). Lexemes also can contain multiple
constants, for example (cheapest, [argmin,cost]),
which we will see more examples of later.
5.2 Lexical Templates
A lexical template takes a lexeme and produces a
lexical item. Templates have the general form
? (?,~v).[? `X : h~v]
where h~v is a logical expression that contains vari-
ables from the list ~v. Applying this template to the
input lexeme (w,~c) gives the full lexical item w `
X :h where the variable ? has been replaced with the
wordspan w and the logical form h has been created
by replacing each of the variables in~v with the coun-
terpart constant from ~c. For example, the lexical
item (6) above would be constructed from the lex-
eme (Boston, [bos]) using the template ? (?,~v).[? `
NP :v1]. Items (1) and (2) would both be constructed
from the single lexeme (flight, [ f light]) with the two
different templates ? (?,~v).[? ` N : ?x.v1(x)] and
? (?,~v).[? `N/(S|NP) :? f?x.v1(x)? f (x)]
5.3 Parsing with a Factored Lexicon
In general, there can by many different (lexeme,
template) pairs that produce the same lexical item.
For example, lexical item (7) in our running ex-
ample above can be constructed from the lexemes
(Boston, [bos]) and (Boston, [ f rom,bos]), given ap-
propriate templates.
To model this ambiguity, we include the selection
of a (lexeme, template) pair as a decision to be made
while constructing a CCG parse tree. Given the lex-
ical item produced by the chosen lexeme and tem-
plate, parsing continues with the traditional combi-
nators, as reviewed in Section 4.2. This direct inte-
gration allows for features that signal which lexemes
and templates have been used while also allowing
for well defined marginal probabilities, by summing
over all ways of deriving a specific lexical item.
6 Learning Factored Lexicons
To induce factored lexicons, we will make use of two
procedures, presented in this section, that factor lexi-
cal items into lexemes and templates. Section 7 will
describe how this factoring operation is integrated
into the complete learning algorithm.
6.1 Maximal Factorings
Given a lexical item l of the form w `X : h with
words w, a syntactic category X , and a logical form
h, we define the maximal factoring to be the unique
(lexeme, template) pair that can be used to recon-
struct l and includes all of the constants of h in
the lexeme (listed in a fixed order based on an
ordered traversal of h). For example, the maxi-
mal factoring for the lexical item Boston ` NP :
bos is the pair we saw before: (Boston, [bos]) and
? (?,~v).[? ` NP : v1]. Similarly, the lexical item
Boston ` N\N : ? f .?x. f (x)? f rom(x,bos) would
be factored to produce (Boston, [ f rom,bos]) and
? (?,~v).[? ` N\N :? f .?x. f (x)? v1(x,v2)].
As we will see in Section 7, this notion of factor-
1516
ing can be directly incorporated into existing algo-
rithms that learn CCG lexicons. When the original
algorithm would have added an entry l to the lexi-
con, we can instead compute the factoring of l and
add the corresponding lexeme and template to the
factored lexicon.
6.2 Introducing Templates with Content
Maximal factorings, as just described, provide for
significant lexical generalization but do not handle
all of the cases needed to learn effectively. For
instance, the maximal split for the item Boston `
N\N : ? f .?x. f (x) ? f rom(x,bos) would introduce
the lexeme (Boston, [ f rom,bos]), which is subopti-
mal since each possible city would need a lexeme
of this type, with the additional from constant in-
cluded. Instead, we would ideally like to learn the
lexeme (Boston, [bos]) and have a template that in-
troduces the from constant. This would model the
desired generalization with a single lexeme per city.
In order to permit the introduction of extra con-
stants into lexical items, we allow the creation of
templates that contain logical constants through par-
tial factorings. For instance, the template below can
introduce the predicate from
? (?,~v).[? `N\N :? f .?x. f (x)? f rom(x,v1)]
The use of templates to introduce extra semantic
constants into a lexical item is similar to, but more
general than, the English-specific type-shifting rules
used in Zettlemoyer and Collins (2007), which were
introduced to model spontaneous, unedited text.
They are useful, as we will see, in learning to re-
cover semantic content that is implied, but not ex-
plicitly stated, such as our original motivating phrase
?flights Boston to New York.?
To propose templates which introduce semantic
content, during learning, we build on the intuition
that we need to recover from missing words, such
as in the example above. In this scenario, there
should also be other sentences that actually include
the word, in our example this would be something
like ?flights from Boston.? We will also assume
that we have learned a good factored lexicon for the
complete example that could produce the parse:
flights from Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? f rom(x,y) bos
>
(N\N)? f?x. f (x)? f rom(x,bos)
<N?x. f light(x)? f rom(x,bos)
Given analyses of this form, we introduce new
templates that will allow us to recover from miss-
ing words, for example if ?from? was dropped. We
identify commonly occurring nodes in the best parse
trees found during training, in this case the non-
terminal spanning ?from Boston,? and introduce
templates that can produce the nonterminal, even if
one of the words is missing. Here, this approach
would introduce the desired template ? (?,~v).[? `
N\N : ? f .?x. f (x) ? f rom(x,v1)] for mapping the
lexeme (Boston, [bos]) directly to the intermediate
structure.
Not all templates introduced this way will model
valid generalizations. However, we will incorporate
them into a learning algorithm with indicator fea-
tures that can be weighted to control their use. The
next section presents the complete approach.
7 Learning Factored PCCGs
Our Factored Unification Based Learning (FUBL)
method extends the UBL algorithm (Kwiatkowski
et al, 2010) to induce factored lexicons, while also
simultanously estimating the parameters of a log-
linear CCG parsing model. In this section, we first
review the NEW-LEX lexical induction procedure
from UBL, and then present the FUBL algorithm.
7.1 Background: NEW-LEX
NEW-LEX generates lexical items by splitting and
merging nodes in the best parse tree of each training
example. Each parse node has a CCG category X : h
and a sequence of words w that it spans. We will
present an overview of the approach using the run-
ning example with the phrase w =?in Boston? and
the category X : h = S\NP :?x.loc(x,bos), which is
of the type commonly seen during learning. The
splitting procedure is a two step process that first
splits the logical form h, then splits the CCG syn-
tactic category X and finally splits the string w.
The first step enumerates all possible splits of
the logical form h into a pair of new expressions
1517
( f ,g) that can be used to reconstruct h by ei-
ther function application (h = f (g)) or composition
(h = ?x. f (g(x))). For example, one possible split is:
( f = ?y.?x.loc(x,y) , g = bos)
which corresponds to the function application case.
The next two steps enumerate all ways of splitting
the syntactic category X and words w to introduce
two new lexical items which can be recombined with
CCG combinators (application or composition) to
recreate the original parse node X : h spanning w. In
our example, one possibility would be:
(in` (S\NP)/NP :?y.?x.loc(x,y) , Boston`NP :bos)
which could be recombined with the forward appli-
cation combinator from Section 4.2.
To assign categories while splitting, the grammar
used by NEW-LEX only uses two atomic syntac-
tic categories S and NP. This allows NEW-LEX to
make use of a direct mapping from semantic type
to syntactic category when proposing syntactic cate-
gories. In this schema, the standard syntactic cat-
egory N is replaced by the category S|NP which
matches the type ?e, t? and uses the vertical slash in-
troduced in Section 4.2. We will see categories such
as this in the evaluation.
7.2 The FUBL Algorithm
Figure 1 shows the FUBL learning algorithm. We
assume training data {(xi,zi) : i= 1 . . .n}where each
example is a sentence xi paired with a logical form
zi. The algorithm induces a factored PCCG, includ-
ing the lexemes L, templates T , and parameters ? .
The algorithm is online, repeatedly performing
both lexical expansion (Step 1) and a parameter up-
date (Step 2) for each training example. The over-
all approach is closely related to the UBL algo-
rithm (Kwiatkowski et al, 2010), but includes exten-
sions for updating the factored lexicon, as motivated
in Section 6.
Initialization The model is initialized with a fac-
tored lexicon as follows. MAX-FAC is a function
that takes a lexical item l and returns the maximal
factoring of it, that is the unique, maximal (lexeme,
template) pair that can be combined to construct l,
as described in Section 6.1. We apply MAX-FAC to
each of the training examples (xi,zi), creating a sin-
gle way of producing the desired meaning zi from a
Inputs: Training set {(xi,zi) : i = 1 . . .n} where each
example is a sentence xi paired with a logical form
zi. Set of entity name lexemes Le. Number of itera-
tions J. Learning rate parameter ?0 and cooling rate
parameter c. Empty lexeme set L. Empty template
set T .
Definitions: NEW-LEX(y) returns a set of new lex-
ical items from a parse y as described in Sec-
tion 7.1. MAX-FAC(l) generates a (lexeme, tem-
plate) pair from a lexical item l. PART-FAC(y)
generates a set of templates from parse y. Both of
these are described in Section 7.2. The distributions
p(y|x,z;? ,(L,T )) and p(y,z|x;? ,(L,T )) are defined
by the log-linear model described in Section 4.3.
Initialization:
? For i = 1 . . .n
? (?,pi) = MAX-FAC(xi ` S : zi)
? L = L?? , T = T ?pi
? Set L = L?Le.
? Initialize ? using coocurrence statistics, as de-
scribed in Section 8.
Algorithm:
For t = 1 . . .J, i = 1 . . .n :
Step 1: (Add Lexemes and Templates)
? Let y? = argmaxy p(y|xi,zi;? ,(L,T ))
? For l ? NEW-LEX(y?)
? (?,pi) = MAX-FAC(l)
? L = L?? , T = T ?pi
? ?= PART-FAC(y?) , T = T ??
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t?n.
? Let ?= Ep(y|xi,zi;? ,(L,T ))[?(xi,y,zi)]
?Ep(y,z|xi;? ,(L,T ))[?(xi,y,z)]
? Set ? = ? + ??
Output: Lexemes L, templates T , and parameters ? .
Figure 1: The FUBL learning algorithm.
lexeme containing all of the words in xi. The lex-
emes and templates created in this way provide the
initial factored lexicon.
Step 1 The first step of the learning algorithm in
Figure 1 adds lexemes and templates to the fac-
tored model given by performing manipulations on
the highest scoring correct parse y? of the current
training example (xi,zi). First the NEW-LEX pro-
cedure is run on y? as described in Section 6.1 to
1518
generate new lexical items. We then use the func-
tion MAX-FAC to create the maximal factorings of
each of these new lexical items as described in Sec-
tion 6 and these are added to the factored represen-
tation of the lexicon. New templates can also be in-
troduced through partial factorings of internal parse
nodes as described in Section 6.2. These templates
are generated by using the function PART-FAC to
abstract over the wordspan and a subset of the con-
stants contained in the internal parse nodes of y?.
This step allows for templates that introduce new
semantic content to model elliptical language, as de-
scribed in Section 6.2.
Step 2 The second step does a stochastic gradient
descent update on the parameters ? used in the pars-
ing model. This update is described in Section 4.3
Discussion The FUBL algorithm makes use of a
direct online approach, where lexemes and tem-
plates are introduced in place while analyzing spe-
cific sentences. In general, this will overgeneralize;
not all ways of combining lexemes and templates
will produce high quality lexical items. However,
the overall approach includes features, presented in
Section 8, that can be used to learn which ones are
best in practice. The complete algorithm iterates be-
tween adding new lexical content and updating the
parameters of the parsing model with each proce-
dure guiding the other.
8 Experimental setup
Data Sets We evaluate on two benchmark seman-
tic parsing datasets: GeoQuery, which is made up of
natural language queries to a database of geograph-
ical information; and Atis, which contains natural
language queries to a flight booking system. The
Geo880 dataset has 880 (English-sentence, logical-
form) pairs split into a training set of 600 pairs and
a test set of 280. The Geo250 data is a subset of
the Geo880 sentences that have been translated into
Japanese, Spanish and Turkish as well as the original
English. We follow the standard evaluation proce-
dure for Geo250, using 10-fold cross validation ex-
periments with the same splits of the data as Wong
and Mooney (2007). The Atis dataset contains 5410
(sentence, logical-form) pairs split into a 4480 ex-
ample training set, a 480 example development set
and a 450 example test set.
Evaluation Metrics We report exact match Re-
call (percentage of sentences for which the correct
logical-form was returned), Precision (percentage of
returned logical-forms that are correct) and F1 (har-
monic mean of Precision and Recall). For Atis we
also report partial match Recall (percentage of cor-
rect literals returned), Precision (percentage of re-
turned literals that are correct) and F1, computed as
described by Zettlemoyer and Collins (2007).
Features We introduce two types of features to
discriminate between parses: lexical features and
logical-form features.
Lexical features fire on the lexemes and templates
used to build the lexical items used in a parse. For
each (lexeme,template) pair used to create a lexi-
cal item we have indicator features ?l for the lex-
eme used, ?t for the template used, and ?(l,t) for the
pair that was used. We assign the features on lexi-
cal templates a weight of 0.1 to prevent them from
swamping the far less frequent but equally informa-
tive lexeme features.
Logical-form features are computed on the
lambda-calculus expression z returned at the root of
the parse. Each time a predicate p in z takes an
argument a with type Ty(a) in position i, it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,Ty(a),i) for the
predicate argument-type relation. Boolean opera-
tor features look at predicates that occurr together
in conjunctions and disjunctions. For each variable
vi that fills argument slot i in two conjoined pred-
icates p1 and p2 we introduce a binary indicator
feature ?con j(i,p1,p2). We introduce similar features?dis j(i,p1,p2) for variables vi that are shared by predi-cates in a disjunction.
Initialization The weights for lexeme features are
initialized according to coocurrance statistics be-
tween words and logical constants. These are esti-
mated with the Giza++ (Och and Ney, 2003) imple-
mentation of IBM Model 1. The initial weights for
templates are set by adding ?0.1 for each slash in
the syntactic category and ?2 if the template con-
tains logical constants. Features on lexeme-template
pairs and all parse features are initialized to zero.
Systems We compare performance to all recently-
published, directly-comparable results. For Geo-
Query, this includes the ZC05, ZC07 (Zettlemoyer
1519
System Exact MatchRec. Pre. F1
ZC07 74.4 87.3 80.4
UBL 65.6 67.1 66.3
FUBL 81.9 82.1 82.0
Table 1: Performance on the Atis development set.
System Exact Match Partial MatchRec. Pre. F1. Rec. Pre. F1
ZC07 84.6 85.8 85.2 96.7 95.1 95.9
HY06 - - - - - 90.3
UBL 71.4 72.1 71.7 78.2 98.2 87.1
FUBL 82.8 82.8 82.8 95.2 93.6 94.6
Table 2: Performance on the Atis test set.
and Collins, 2005, 2007), ? -WASP (Wong and
Mooney, 2007), UBL (Kwiatkowski et al, 2010)
systems and DCS (Liang et al, 2011). For Atis,
we report results from HY06 (He and Young, 2006),
ZC07, and UBL.
9 Results
Tables 1-4 present the results on the Atis and Geo-
query domains. In all cases, FUBL achieves at or
near state-of-the-art recall (overall number of correct
parses) when compared to directly comparable sys-
tems and it significantly outperforms UBL on Atis.
On Geo880 the only higher recall is achieved
by DCS with prototypes - which uses signifi-
cant English-specific resources, including manually
specified lexical content, but does not require train-
ing sentences annotated with logical-forms. On
Geo250, FUBL achieves the highest recall across
languages. Each individual result should be inter-
preted with care, as a single percentage point cor-
responds to 2-3 sentences, but the overall trend is
encouraging.
On the Atis development set, FUBL outperforms
ZC07 by 7.5% of recall but on the Atis test set
FUBL lags ZC07 by 2%. The reasons for this dis-
crepancy are not clear, however, it is possible that
the syntactic constructions found in the Atis test set
do not exhibit the same degree of variation as those
seen in the development set. This would negate the
need for the very general lexicon learnt by FUBL.
Across the evaluations, despite achieving high re-
call, FUBL achieves significantly lower precision
than ZC07 and ? -WASP. This illustrates the trade-
off from having a very general model of proposing
lexical structure. With the ability to skip unseen
System Rec. Pre. F1
Labelled Logical Forms
ZC05 79.3 96.3 87.0
ZC07 86.1 91.6 88.8
UBL 87.9 88.5 88.2
FUBL 88.6 88.6 88.6
Labelled Question Answers
DCS 91.1 - -
Table 3: Exact match accuracy on the Geo880 test set.
System English SpanishRec. Pre. F1 Rec. Pre. F1
? -WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 81.8 83.5 82.6 81.4 83.4 82.4
FUBL 83.7 83.7 83.7 85.6 85.8 85.7
System Japanese TurkishRec. Pre. F1 Rec. Pre. F1
? -WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 83.0 83.2 83.1 71.8 77.8 74.6
FUBL 83.2 83.8 83.5 72.5 73.7 73.1
Table 4: Exact-match accuracy on the Geo250 data set.
words, FUBL returns a parse for all of the Atis test
sentences, since the factored lexicons we are learn-
ing can produce a very large number of lexical items.
These parses are, however, not always correct.
10 Analysis
The Atis results in Tables 1 and 2 highlight the ad-
vantages of factored lexicons. FUBL outperforms
the UBL baseline by 16 and 11 points respectively
in exact-match recall. Without making any modi-
fication to the CCG grammars or parsing combina-
tors, we are able to induce a lexicon that is general
enough model the natural occurring variations in the
data, for example due to sloppy, unedited sentences.
Figure 2 shows a parse returned by FUBL for
a sentence on which UBL failed. While
the word ?cheapest? is seen 208 times in the
training data, in only a handful of these in-
stances is it seen in the middle of an utter-
ance. For this reason, UBL never proposes
the lexical item, cheapest ` NP\(S|NP)/(S|NP) :
? f?g.argmin(?x. f (x)? g(x),?y.cost(y)), which is
used to parse the sentence in Figure 2. In contrast,
FUBL uses a lexeme learned from the same word in
different contexts, along with a template learnt from
similar words in a similar context, to learn to per-
1520
pittsburgh to atlanta the cheapest on july twentieth
NP (S|NP)\NP/NP NP NP\(S|NP)/(S|NP) (S|NP)/NP/NP NP NP
pit ?x?y? z.to(z,x) atl ? f?g.argmin(?x. f (x)?g(x),?y.cost(y)) ?x?y? z.month(z,x) jul 20
? f rom(z,y) ?day(z,y)
> >
(S|NP)\NP (S|NP)/NP?x?y.to(y,atl)? f rom(y,x) ?x?y.month(y, jul)?day(y,x)
< >
(S|NP) (S|NP)?x.to(x,atl)? f rom(x, pit) ?x.month(x, jul)?day(x,20)
>NP\(S|NP)? f .argmin(?x. f (x)?month(x, jul)?day(x,20),?y.cost(y))
<NP
argmin(?x. f rom(x, pit)? to(x,atl)?month(x, jul)?day(x,20),?y.cost(y))
Figure 2: An example learned parse. FUBL can learn this type of analysis with novel combinations of lexemes and
templates at test time, even if the individual words, like ?cheapest,? were never seen in similar syntactic constructions
during training, as described in Section 10.
form the desired analysis.
As well as providing a new way to search the lex-
icon during training, the factored lexicon provides a
way of proposing new, unseen, lexical items at test
time. We find that new, non-NP, lexical items are
used in 6% of the development set parses.
Interestingly, the addition of templates that intro-
duce semantic content (as described in Section 6.2)
account for only 1.2% of recall on the Atis develop-
ment set. This is suprising as elliptical constructions
are found in a much larger proportion of the sen-
tences than this. In practice, FUBL learns to model
many elliptical constructions with lexemes and tem-
plates introduced through maximal factorings. For
example, the lexeme (to, [ f rom, to]) can be used
with the correct lexical template to deal with our
motivating example ?flights Boston to New York?.
Templates that introduce content are therefore only
used in truly novel elliptical constructions for which
an alternative analysis could not be learned.
Table 5 shows a selection of lexemes and tem-
plates learned for Atis. Examples 2 and 3 show that
morphological variants of the same word must still
be stored in separate lexemes. However, as these
lexemes now share templates, the total number of
lexical variants that must be learned is reduced.
11 Discussion
We argued that factored CCG lexicons, which in-
clude both lexemes and lexical templates, provide
a compact representation of lexical knowledge that
can have advantages for learning. We also described
a complete approach for inducing factored, prob-
abilistic CCGs for semantic parsing, and demon-
Most common lexemes by type of constants in~c.
1 e (Boston, [bos]) (Denver, [den])
2 ?e, t? (flight, [ f light]) (flights, [ f light])
3 ?e, i? (fare, [cost]) (fares, [cost])
4 ?e,?e, t?? (from, [ f rom]) (to, [to])
5 ?e, i?, (cheapest, [argmin,cost])?e, t? (earliest, [argmin,dep time])
6 ?i,?i, t??, (after, [>,dep time])
?e, i? (before, [<,dep time])
Most common templates matching lexemes above.
1 ? (?,~v).? `NP :v1
2 ? (?,~v).? `S|NP :?x.v1(x)
3 ? (?,~v).? `NP|NP :?x.v1(x)
4 ? (?,~v).? `S|NP/NP\(S|NP) :?x?y.v1(x,y)
5 ? (?,~v).? `NP/(S|NP) :? f .v1(?x. f (x),?y,v2(y))
6 ? (?,~v).? `S|NP\(S|NP)/NP :
?x?y? z.v1(v2(z),x)? y(x)
Table 5: Example lexemes and templates learned from
the Atis development set.
strated strong performance across a wider range of
benchmark datasets that any previous approach.
In the future, it will also be important to ex-
plore morphological models, to better model vari-
ation within the existing lexemes. The factored lex-
ical representation also has significant potential for
lexical transfer learning, where we would need to
learn new lexemes for each target application, but
much of the information in the templates could, po-
tentially, be ported across domains.
Acknowledgements
The work was supported in part by EU ERC Ad-
vanced Fellowship 249520 GRAMPLUS, and an
ESPRC PhD studentship. We would like to thank
Yoav Artzi for helpful discussions.
1521
References
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of the International Conference on Computa-
tional Linguistics.
Branavan, S.R.K., Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to map
high-level instructions to commands. In Association
for Computational Linguistics (ACL).
Carpenter, Bob. 1997. Type-Logical Semantics. The MIT
Press.
Clark, Stephen and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics
33(4):493?552.
Clarke, James, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2010). Uppsala, Sweden,
pages 18?27.
Ge, Ruifang and Raymond J. Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
the COLING/ACL 2006 Main Conference Poster Ses-
sions.
Goldwasser, Dan, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguistics
(ACL).
He, Yulan and Steve Young. 2005. Semantic processing
using the hidden vector state model. Computer Speech
and Language .
He, Yulan and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication 48(3-4).
Hockenmaier, Julia and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th Meet-
ing of the ACL. Philadelphia, PA, pages 335?342.
Kate, Rohit J. and Raymond J. Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics.
Kate, Rohit J., Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings of the National Conference
on Artificial Intelligence.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the Conference on Em-
perical Methods in Natural Language Processing.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE 86(11):2278?2324.
Liang, P., M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
Liang, P., M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Asso-
ciation for Computational Linguistics (ACL).
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of The Conference on Empirical Methods in Natural
Language Processing.
Miller, Scott, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proc. of the Associ-
ation for Computational Linguistics.
Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan.
2006. Semantic parsing with structured SVM ensem-
ble classification models. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics 29(1):19?51.
Papineni, K. A., S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communica-
tion and Technology.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Poon, Hoifung and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Association for
Computational Linguistics (ACL).
Ramaswamy, Ganesh N. and Jan Kleindienst. 2000. Hier-
archical feature-based translation for scalable natural
language understanding. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
Steedman, Mark. 1996. Surface Structure and Interpre-
tation. The MIT Press.
1522
Steedman, Mark. 2000. The Syntactic Process. The MIT
Press.
Tang, Lappoon R. and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Integrat-
ing statistical and relational learning for semantic pars-
ing. In Proceedings of the Joint Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora.
Thompson, Cynthia A. and Raymond J. Mooney. 2002.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence Re-
search 18.
Vogel, Adam and Dan Jurafsky. 2010. Learning to follow
navigational directions. In Association for Computa-
tional Linguistics (ACL).
Watkinson, Stephen and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In Proceedings of the 1st Work-
shop on Learning Language in Logic.
Wong, Yuk Wah and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL.
Wong, Yuk Wah and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Association for
Computational Linguistics.
Zelle, John M. and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In Proc. of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, Luke S. and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of The Joint Conference
of the Association for Computational Linguistics and
International Joint Conference on Natural Language
Processing.
1523
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545?1556,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling Semantic Parsers with On-the-fly Ontology Matching
Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{tomk,eunsol,yoav,lsz}@cs.washington.edu
Abstract
We consider the challenge of learning seman-
tic parsers that scale to large, open-domain
problems, such as question answering with
Freebase. In such settings, the sentences cover
a wide variety of topics and include many
phrases whose meaning is difficult to rep-
resent in a fixed target ontology. For ex-
ample, even simple phrases such as ?daugh-
ter? and ?number of people living in? can-
not be directly represented in Freebase, whose
ontology instead encodes facts about gen-
der, parenthood, and population. In this pa-
per, we introduce a new semantic parsing ap-
proach that learns to resolve such ontologi-
cal mismatches. The parser is learned from
question-answer pairs, uses a probabilistic
CCG to build linguistically motivated logical-
form meaning representations, and includes
an ontology matching model that adapts the
output logical forms for each target ontology.
Experiments demonstrate state-of-the-art per-
formance on two benchmark semantic parsing
datasets, including a nine point accuracy im-
provement on a recent Freebase QA corpus.
1 Introduction
Semantic parsers map sentences to formal represen-
tations of their underlying meaning. Recently, al-
gorithms have been developed to learn such parsers
for many applications, including question answering
(QA) (Kwiatkowski et al, 2011; Liang et al, 2011),
relation extraction (Krishnamurthy and Mitchell,
2012), robot control (Matuszek et al, 2012; Kr-
ishnamurthy and Kollar, 2013), interpreting instruc-
tions (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), and generating programs (Kushman
and Barzilay, 2013).
In each case, the parser uses a predefined set
of logical constants, or an ontology, to construct
meaning representations. In practice, the choice
of ontology significantly impacts learning. For
example, consider the following questions (Q) and
candidate meaning representations (MR):
Q1: What is the population of Seattle?
Q2: How many people live in Seattle?
MR1: ?x.population(Seattle, x)
MR2: count(?x.person(x) ? live(x, Seattle))
A semantic parser might aim to construct MR1 for
Q1 and MR2 for Q2; these pairings align constants
(count, person, etc.) directly to phrases (?How
many,? ?people,? etc.). Unfortunately, few ontologies
have sufficient coverage to support both meaning
representations, for example many QA databases
would only include the population relation required
for MR1. Most existing approaches would, given
this deficiency, simply aim to produce MR1 for Q2,
thereby introducing significant lexical ambiguity
that complicates learning. Such ontological mis-
matches become increasingly common as domain
and language complexity increases.
In this paper, we introduce a semantic parsing ap-
proach that supports scalable, open-domain ontolog-
ical reasoning. The parser first constructs a linguis-
tically motivated domain-independent meaning rep-
resentation. For example, possibly producing MR1
for Q1 and MR2 for Q2 above. It then uses a learned
ontology matching model to transform this represen-
1545
x : How many people visit the public library of New York annually
l0 : ?x.eq(x, count(?y.people(y) ? ?e.visit(y, ?z.public(z) ? library(z) ? of(z, new york), e) ? annually(e)))
y : ?x.library.public library system.annual visits(x, new york public library)
a : 13,554,002
x : What works did Mozart dedicate to Joseph Haydn
l0 : ?x.works(x) ? ?e.dedicate(mozart, x, e) ? to(haydn, e)))
y : ?x.dedicated work(x) ? ?e.dedicated by(mozart, e) ? dedication(x, e) ? dedicated to(haydn, e)))
a : { String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified
logical forms y, and answers a drawn from the Freebase domain.
tation for the target domain. In our example, pro-
ducing either MR1, MR2 or another more appropri-
ate option, depending on the QA database schema.
This two stage approach enables parsing without
any domain-dependent lexicon that pairs words with
logical constants. Instead, word meaning is filled
in on-the-fly through ontology matching, enabling
the parser to infer the meaning of previously un-
seen words and more easily transfer across domains.
Figure 1 shows the desired outputs for two example
Freebase sentences.
The first parsing stage uses a probabilistic combi-
natory categorial grammar (CCG) (Steedman, 2000;
Clark and Curran, 2007) to map sentences to
new, underspecified logical-form meaning represen-
tations containing generic logical constants that are
not tied to any specific ontology. This approach en-
ables us to share grammar structure across domains,
instead of repeatedly re-learning different grammars
for each target ontology. The ontology-matching
step considers a large number of type-equivalent
domain-specific meanings. It enables us to incorpo-
rate a number of cues, including the target ontology
structure and lexical similarity between the names of
the domain-independent and dependent constants, to
construct the final logical forms.
During learning, we estimate a linear model over
derivations that include all of the CCG parsing de-
cisions and the choices for ontology matching. Fol-
lowing a number of recent approaches (Clarke et al,
2010; Liang et al, 2011), we treat all intermediate
decisions as latent and learn from data containing
only easily gathered question answer pairs. This ap-
proach aligns naturally with our two-stage parsing
setup, where the final logical expression can be di-
rectly used to provide answers.
We report performance on two benchmark
datasets: GeoQuery (Zelle and Mooney, 1996) and
Freebase QA (FQ) (Cai and Yates, 2013a). Geo-
Query includes a geography database with a small
ontology and questions with relatively complex,
compositional structure. FQ includes questions to
Freebase, a large community-authored database that
spans many sub-domains. Experiments demonstrate
state-of-the-art performance in both cases, including
a nine point improvement in recall for the FQ test.
2 Formal Overview
Task Let an ontology O be a set of logical con-
stants and a knowledge base K be a collection of
logical statements constructed with constants from
O. For example, K could be facts in Freebase (Bol-
lacker et al, 2008) and O would define the set
of entities and relation types used to encode those
facts. Also, let y be a logical expression that can
be executed against K to return an answer a =
EXEC(y,K). Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x,O) for mapping a natural language
sentence x to a domain-dependent logical form y.
Parsing We use a two-stage approach to define
the space of possible parses GEN(x,O) (Section 5).
First, we use a CCG and word-class information
from Wiktionary1 to build domain-independent un-
derspecified logical forms, which closely mirror the
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme-
diate representation to a logical form that uses con-
stants from O, such as the y seen in Figure 1.
1www.wiktionary.com
1546
Learning We assume access to data containing
question-answer pairs {(xi, ai) : i = 1 . . . n} and
a corresponding knowledge base K. The learn-
ing algorithm (Section 7.1) estimates the parame-
ters of a linear model for ranking the possible en-
tires in GEN(x,O). Unlike much previous work
(e.g., Zettlemoyer and Collins (2005)), we do not
induce a CCG lexicon. The lexicon is open domain,
using no symbols from the ontology O for K. This
allows us to write a single set of lexical templates
that are reused in every domain (Section 5.1). The
burden of learning word meaning is shifted to the
second, ontology matching, stage of parsing (Sec-
tion 5.2), and modeled with a number of new fea-
tures (Section 7.2) as part of the joint model.
Evaluation We evaluate on held out question-
answer pairs in two benchmark domains, Freebase
and GeoQuery. Following Cai and Yates (2013a),
we also report a cross-domain evaluation where the
Freebase data is divided by topics such as sports,
film, and business. This condition ensures that the
test data has a large percentage of previously unseen
words, allowing us to measure the effectiveness of
the real time ontology matching.
3 Related Work
Supervised approaches for learning semantic parsers
have received significant attention, e.g. (Kate and
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al, 2010, 2011, 2012; Jones
et al, 2012). However, these techniques require
training data with hand-labeled domain-specific log-
ical expressions. Recently, alternative forms of su-
pervision were introduced, including learning from
question-answer pairs (Clarke et al, 2010; Liang
et al, 2011), from conversational logs (Artzi and
Zettlemoyer, 2011), with distant supervision (Kr-
ishnamurthy and Mitchell, 2012; Cai and Yates,
2013b), and from sentences paired with system
behavior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013). Our
work adds to these efforts by demonstrating a new
approach for learning with latent meaning represen-
tations that scales to large databases like Freebase.
Cai and Yates (2013a) present the most closely
related work. They applied schema matching tech-
niques to expand a CCG lexicon learned with the
UBL algorithm (Kwiatkowski et al, 2010). This ap-
proach was one of the first to scale to Freebase, but
required labeled logical forms and did not jointly
model semantic parsing and ontological reasoning.
This method serves as the state of the art for our
comparison in Section 9.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning rep-
resentations (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al, 2010, 2011), building deriva-
tions to transform the output of the CCG parser
based on context (Zettlemoyer and Collins, 2009),
and using weakly supervised margin-sensitive pa-
rameter updates (Artzi and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning
an open-domain CCG semantic parser; all previous
methods suffered, to various degrees, from the onto-
logical mismatch problem that motivates our work.
The challenge of ontological mismatch has been
previously recognized in many settings. Hobbs
(1985) describes the need for ontological promiscu-
ity in general language understanding. Many pre-
vious hand-engineered natural language understand-
ing systems (Grosz et al, 1987; Alshawi, 1992; Bos,
2008) are designed to build general meaning rep-
resentations that are adapted for different domains.
Recent efforts to build natural language interfaces to
large databases, for example DBpedia (Yahya et al,
2012; Unger et al, 2012), have also used hand-
engineered ontology matching techniques. Fader
et al (2013) recently presented a scalable approach
to learning an open domain QA system, where onto-
logical mismatches are resolved with learned para-
phrases. Finally, the databases research commu-
nity has a long history of developing schema match-
ing techniques (Doan et al, 2004; Euzenat et al,
2007), which has inspired more recent work on dis-
tant supervision for relation extraction with Free-
base (Zhang et al, 2012).
4 Background
Semantic Modeling We use the typed lambda cal-
culus to build logical forms that represent the mean-
ings of words, phrases and sentences. Logical forms
contain constants, variables, lambda abstractions,
and literals. In this paper, we use the term literal to
refer to the application of a constant to a sequence of
1547
library of new york
N N\N/NP NP
?x.library(x) ?y?f?x.f(x) ? loc(x, y) NY C
>
N\N
?f.?x.f(x) ? loc(x,NY C)
<
N
?x.library(x) ? loc(x,NY C)
Figure 2: A sample CCG parse.
arguments. We include types for entities e, truth val-
ues t, numbers i, events ev, and higher-order func-
tions, such as ?e, t? and ??e, t?, e?. We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and
conjunctive modifiers to capture thematic roles.
Combinatory Categorial Grammars (CCG)
CCGs are a linguistically-motivated formalism
for modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by
a lexicon and a set of combinators. The lexicon
contains entries that pair words or phrases with
CCG categories. For example, the lexical entry
library ` N : ?x.library(x) in Figure 2 pairs
the word ?library? with the CCG category that has
syntactic category N and meaning ?x.library(x).
A CCG parse starts from assigning lexical entries to
words and phrases. These are then combined using
the set of CCG combinators to build a logical form
that captures the meaning of the entire sentence. We
use the application, composition, and coordination
combinators. Figure 2 shows an example parse.
5 Parsing Sentences to Meanings
The function GEN(x,O) defines the set of possible
derivations for an input sentence x. Each derivation
d = ??,M? builds a logical form y using constants
from the ontology O. ? is a CCG parse tree that
maps x to an underspecified logical form l0. M is an
ontological match that maps l0 onto the fully spec-
ified logical form y. This section describes, with
reference to the example in Figure 3, the operations
used by ? and M .
5.1 Domain Independent Parsing
Domain-independent CCG parse trees ? are built
using a predefined set of 56 underspecified lexi-
cal categories, 49 domain-independent lexical items,
and the combinatory rules introduced in Section 4.
An underspecified CCG lexical category has a
syntactic category and a logical form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced
during ontology matching. For example, a noun
might be assigned the lexical category N : ?x.p(x),
where p is an underspecified ?e, t?-type constant.
During parsing, lexical categories are created dy-
namically. We manually define a set of POS tags for
each underspecified lexical category, and use Wik-
tionary as a tag dictionary to define the possible POS
tags for words and phrases. Each phrase is assigned
every matching lexical category. For example, the
word ?visit? can be either a verb or a noun in Wik-
tionary. We accordingly assign it all underspecified
categories for the classes, including:
N :?x.p(x) , S\NP/NP :?x?y?ev.p(y, x, ev)
for nouns and transitive verbs respectively.
We also define domain-independent lexical items
for function words such as ?what,? ?when,? and
?how many,? ?and,? and ?is.? These lexi-
cal items pair a word with a lexical cate-
gory containing only domain-independent con-
stants. For example, how many ` S/(S\NP)/N :
?f.?g.?x.eq(x, count(?y.f(y) ? g(y))) contains
the function count and the predicate eq.
Figure 3a shows the lexical categories and combi-
nator applications used to construct the underspeci-
fied logical form l0. Underspecified constants in this
figure have been labeled with the words that they are
associated with for readability.
5.2 Ontological Matching
The second, domain specific, step M maps the un-
derspecified logical form l0 onto the fully specified
logical form y. The mapping from constants in l0
to constants in y is not one-to-one. For example, in
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching
operations M = ?o1 . . . , on? that can transform the
structure of the logical form or replace underspeci-
fied constants with constants from O.
1548
(a) Underspecified CCG parse ?: Map words onto underspecified lexical categories as described in Section 5.1. Use
the CCG combinators to combine lexical categories to give the full underpecified logical form l0.
how many people visit the public library of new york annually
S/(S\NP )/N N S\NP/NP NP/N N/N N N\N/NP NP AP
?f.?g.?x.eq(x, count( ?x.People(x) ?x.?y.?ev. ?f.?x.f(x) ?f.?x.f(x)? ?x.Library(x) ?y.?f.?x.Of NewY ork ?ev.Annually(ev)
?y.f(y) ? g(y))) V isit(y, x, ev) Public(x) (x, y) ? f(x)
> >
<
>
>
> <
>
S
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ? Of(z,NewY ork)) ? Annually(e)))
(b) Structure Matching Steps in M : Use the operators described in Section 5.2.1 and Figure 4 to transform l0. In
each step one of the operators is applied to a subexpression of the existing logical form to generate a modified logical
form with a new underspecified constant marked in bold.
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ?Of(z,NewY ork), e) ?Annually(e)))
l1 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y,PublicLibraryOfNewYork, e) ?Annually(e)))
l2 : ?x.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewY ork)))
(c) Constant Matching Steps in M : Replace all underspecified constants in the transformed logical form with a
similarly typed constant from O, as described in Section 5.2.2. The underspecified constant to be replaced is marked
in bold and constants from O are written in typeset.
?x.HowManyPeopleV isitAnnually(x,PublicLibraryOfNewYork)
l3 : 7? ?x.HowManyPeopleV isitAnnually(x, new york public library)
?x.HowManyPeopleVisitAnnually(x, new york public library)
y : 7? ?x.public library system.annual visits(x, new york public library)
Figure 3: Example derivation for the query ?how many people visit the public library of new york annu-
ally.? Underspecified constants are labelled with the words from the query that they are associated with for
readability. Constants from O, written in typeset, are introduced in step (c).
Operator Definition and Conditions Example
a.
Collapse
Literal
to
Constant
P (a1, . . . , an) 7? c
?z.Public(z) ? Library(z) ?Of(z,NewY ork))
7? PublicLibraryOfNewY ork
s.t. type(P (a1, . . . , an)) = type(c) Input and output have type e.
type(c) ? {e, i} e is allowed in O.
freev(P (a1, . . . , an)) = ? Input contains no free variables.
b.
Collapse
Literal
to
Literal
P (a1, . . . , an) 7? Q(b1, . . . , bm)
eq(x, count(?y.People(y) ? ?e.V isit(y,
PublicLibraryOfNewY ork) ?Annually(e)))
7? CountPeopleV isitAnnually(x,
PublicLibraryOfNewY ork)
s.t. type(P (a1, . . . , an)) = type(Q(b1, . . . , bm)) Input and output have type t.
type(Q) ? {type(c) : c ? O} New constant has type ?i, ?e, t??, allowed in O.
freev(P (a1, . . . , an)) = freev(Q(b1, . . . , bm)) Input and output contain single free variable x.
{b1, . . . , bm} ? subexps(P (a1, . . . , an)) Arguments of output literal are subexpressions of input.
c. Split
Literal
P (a1, . . . , ak, x, ak+1, . . . , an)
7? Q(b1, . . . , x, . . . bn) ?Q??(c1, . . . , x, . . . cm)
Dedicate(Mozart,Haydn, ev)
7? Dedicate(Mozart, ev) ?Dedicate??(Haydn, ev)
s.t. type(P (. . . )) = t Input has type t. This matches output type by definition.
{type(Q), type(Q??)} ? {type(c) : c ? O} New constants have allowed type ?e, ?ev, t??.
{b1, . . . , bn, c1, . . . , cm} = {a1, . . . , an} All arguments of input literal are preserved in output.
Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to
match the ontology O. The function type(c) calculates a constant c?s type. The function freev(lf) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf)
generates the set of all subexpressions of the lambda calculus expression lf .
1549
5.2.1 Structure Matching
Three structure matching operators, illustrated in
Figure 4, are used to collapse or expand literals in
l0. Collapses merge a subexpression from l0 to cre-
ate a new underspecified constant, generating a log-
ical form with fewer constants. Expansions split a
subexpression from l0 to generate a new logical form
containing one extra constant.
Collapsing Operators The collapsing operator
defined in Figure 4a merges all constants in a
literal to generate a single constant of the same
type. This operator is used to map ?z.Public(z)?
Library(z)?Of(z,NewY ork) to PublicLibraryOfNewY ork
in Figure 3b. Its operation is limited to entity typed
expressions that do not contain free variables.
The operator in Figure 4b, in contrast, can be used
to collapse the expression eq(x,count(?y.People(y)?
?e.V isit(y,PublicLibraryOfNewY ork,e))?Annually(e))),
which contains free variable x onto a new expression
CountPeopleV isitAnnually(x,PublicLibraryOfNewY ork).
This is only possible when the type of the newly
created constant is allowed in O and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by
creating ad-hoc conjunctions that encapsulate them.
Disjunctions are treated similarly.
Performing collapses on the underspecified logi-
cal form allows non-contiguous phrases to be rep-
resented in the collapsed form. In this exam-
ple, the logical form representing the phrase ?how
many people visit? has been merged with the logi-
cal form representing the non-adjacent adverb ?an-
nually.? This generates a new underspecified con-
stant that can be mapped onto the Freebase relation
public library system annual visits that re-
lates to both phrases.
The collapsing operations preserve semantic type,
ensuring that all logical forms generated by the
derivation sequence are well typed. The full set of
allowed collapses of l0 is given by the transitive clo-
sure of the collapsing operations. The size of this
set is limited by the number of constants in l0, since
each collapse removes at least one constant. At each
step, the number of possible collapses is polynomial
in the number of constants in l0 and exponential in
the arity of the most complex type in O. For do-
mains of interest this arity is unlikely to be high and
for triple stores such as Freebase it is 2.
Expansion Operators The fully specified logical
form y can contain constants relating to multiple
words in x. It can also use multiple constants to rep-
resent the meaning of a single word. For example,
Freebase does not contain a relation representing the
concept ?daughter?, instead using two relations rep-
resenting ?female? and ?child?. The expansion oper-
ator in Figure 4c allows a single predicate to be split
into a pair of conjoined predicates sharing an argu-
ment variable. For example, in Figure 1, the constant
for ?dedicate? is split in two to match its represen-
tation in Freebase. Underspecified constants from
l0 can be split once. For the experiments in Sec-
tion 8, we constrain the expansion operator to work
on event modifiers but the procedure generalizes to
all predicates.
5.2.2 Constant Matching
To build an executable logical form y, all under-
specified constants must be replaced with constants
from O. This is done through a sequence of con-
stant replacement operations, each of which replaces
a single underspecified constant with a constant of
the same type from O. Two example replacements
are shown in Figure 3c. The output from the last re-
placement operation is a fully specified logical form.
6 Building and Scoring Derivations
This section introduces a dynamic program used to
construct derivations and a linear scoring model.
6.1 Building Derivations
The space of derivations is too large to explicitly
enumerate. However, each logical form (both final
and interim) can be constructed with many differ-
ent derivations, and we only need to find the highest
scoring one. This allows us to develop a simple dy-
namic program for our two-stage semantic parser.
We use a CKY style chart parser to calculate the
k-best logical forms output by parses of x. We then
store each interim logical form generated by an op-
erator in M once in a hyper-graph chart structure.
The branching factor of this hypergraph is polyno-
mial in the number of constants in l0 and linear in
the size of O. Subsequently, there are too many
possible logical forms to enumerate explicitly; we
1550
prune as follows. We allow the top N scoring on-
tological matches for each original subexpression in
l0 and remove matches that differ from score from
the maximum scoring match by more than a thresh-
old ? . When building derivations, we apply constant
matching operators as soon as they are applicable to
new underspecified constants created by collapses
and expansions. This allows the scoring function
used by the pruning strategy to take advantage of all
features defined in Section 7.2.
6.2 Ranking Derivations
Given feature vector ? and weight vector ?, the score
of a derivation d = ??,M? is a linear function that
decomposes over the parse tree ? and the individual
ontology-matching steps o.
SCORE(d) = ?(d)? (1)
= ?(?)? +
?
o?M
?(o)?
The function PARSE(x,O) introduced as our goal in
Section 2 returns the logical form associated with
the highest scoring derivation of x:
PARSE(x,O) = arg max
d?GEN(x,O)
(SCORE(d))
The features and learning algorithm used to estimate
? are defined in the next section.
7 Learning
This section describes an online learning algorithm
for question-answering data, along with the domain-
independent feature set.
7.1 Learning Model Parameters
Our learning algorithm estimates the parameters ?
from a set {(xi, ai) : i = 1 . . . n} of questions xi
paired with answers ai from the knowledge base
K. Each derivation d generated by the parser is
associated with a fully specified logical form y =
YIELD(d) that can be executed in K. A derivation d
of xi is correct if EXEC(YIELD(d),K) = ai. We use
a perceptron to estimate a weight vector ? that sup-
port a separation of ? between correct and incorrect
answers. Figure 5 presents the learning algorithm.
Input: Q/A pairs {(xi, ai) : i = 1 . . . n}; Knowledge base
K; Ontology O; Function GEN(x,O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield
of derivation d; Function EXEC(y,K) that calculates execu-
tion of y in K; Margin ?; Number of iterations T .
Output: Linear model parameters ?.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
C = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) = ai}
W = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) 6= ai}
C? = argmaxd?C(?(d)?)
W ? = {d : d ?W ; ?c ? C? s.t. ?(c)? ? ?(d)? < ?)}
If |C?| > 0 ? |W ?| > 0 :
? = ? + 1|C?|
?
c?C? ?(c)?
1
|W?|
?
e?W? ?(e)
Figure 5: Parameter estimation from Q/A pairs.
7.2 Features
The feature vector ?(d) introduced in Section 6.2
decomposes over each of the derivation steps in d.
CCG Parse Features Each lexical item in ? has
three indicator features. The first indicates the num-
ber of times each underspecified category is used.
For example, the parse in Figure 3a uses the under-
specified category N : ?x.p(x) twice. The second
feature indicates (word, category) pairings ? e.g.
that N : ?x.p(x) is paired with ?library? and ?pub-
lic? once each in Figure 3a. The final lexical feature
indicates (part-of-speech, category) pairings for all
parts of speech associated with the word.
Structural Features The structure matching op-
erators (Section 5.2.1) in M generate new under-
specified constants that define the types of constants
in the output logical form y. These operators are
scored using features that indicate the type of each
complex-typed constant present in y and the iden-
tity of domain-independent functional constants in
y. The logical form y generated in Figure 3 contains
one complex typed constant with type ?i, ?e, t?? and
no domain-independent functional constants. Struc-
tural features allow the model to adapt to different
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity such as ?pop-
ulation? is likely to be explicitly listed in K or if it
should be computed with the count function.
Lexical Features Each constant replacement op-
erator (Section 5.2.2) in M replaces an underspec-
1551
ified constant cu with a constant cO from O. The
underspecified constant cu is associated with the se-
quence of words ~wu used in the CCG lexical en-
tries that introduced it in ?. We assume that each
of the constants cO in O is associated with a string
label ~wO. This allows us to introduce five domain-
independent features that measure the similarity of
~wu and ~wO.
The feature ?np(cu, cO) signals the replacement
of an entity-typed constant cu with entity cO that has
label ~wu. For the second example in Figure 1 this
feature indicates the replacement of the underspeci-
fied constant associated with the word ?mozart? with
the Freebase entity mozart. Stem and synonymy
features ?stem(cu, cO) and ?syn(cu, cO) signal the
existence of words wu ? ~wu and wu ? ~wO that
share a stem or synonym respectively. Stems are
computed with the Porter stemmer and synonyms
are extracted from Wiktionary. A single Freebase
specific feature ?fp:stem(cu, cO) indicates a word
stem match between wu ? ~wu and the word filling
the most specific position in ~wu under Freebase?s hi-
erarchical naming schema.
A final feature ?gl(cu, cO) calculates the overlap
between Wiktionary definitions for ~wu and ~wO. Let
gl(w) be the Wiktionary definition for w. Then:
?gl(cu, cO) =
?
wu? ~wu;wO? ~wO
2?|gl(wO)?gl(wc)|
| ~wO |?| ~wu|?|gl(wO)|+|gl(wc)|
Domain-indepedent lexical features allow the
model to reason about the meaning of unseen words.
In small domains, however, the majority of word us-
ages may be covered by training data. We make use
of this fact in the GeoQuery domain with features
?m(cu, cO) that indicate the pairing of ~wu with cO.
Knowledge Base Features Guided by the obser-
vation that we generally want to create queries y
which have answers in knowledge base K, we de-
fine features to signal whether each operation could
build a logical form y with an answer in K.
If a predicate-argument relation in y does not
exist in K, then the execution of y against K
will not return an answer. Two features indicate
whether predicate-argument relations in y exist inK.
?direct(y,K) indicates predicate-argument applica-
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex-
ists in Freebase, ?direct(y,K) will fire. ?join(y,K)
indicates entities separated from a predicate by one
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship in K.
If two predicates that share a variable in y
do not share an argument in that position in K
then the execution of y against K will fail. The
predicate-predicate ?pp(y,K) feature indicates pairs
of predicates that share a variable in y but can-
not occur in this relationship in K. For ex-
ample, since the subject of the Freebase prop-
erty date of birth does not take arguments of
type location, ?pp(y,K) will fire if y con-
tains the logical form ?x?y.date of birth(x, y)?
location(x).
Both the predicate-argument and predicate-
predicate features operate on subexpressions of y.
We also define the execution features: ?emp(y,K) to
signal an empty answer for y in K; ?0(y,K) to sig-
nal a zero-valued answer created by counting over
an empty set; and ?1(y,K) to signal a one-valued
answer created by counting over a singleton set.
As with the lexical cues, we use knowledge base
features as soft constraints since it is possible for
natural language queries to refer to concepts that do
not exist in K.
8 Experimental Setup
Data We evaluate performance on the benchmark
GeoQuery dataset (Zelle and Mooney, 1996), and a
newly introduced Freebase Query (FQ) dataset (Cai
and Yates, 2013a). FQ contains 917 questions la-
beled with logical form meaning representations for
querying Freebase. We gathered question answer la-
bels by executing the logical forms against Freebase,
and manually correcting any inconsistencies.
Freebase (Bollacker et al, 2008) is a large, col-
laboratively authored database containing almost 40
million entities and two billion facts, covering more
than 100 domains. We filter Freebase to cover the
domains contained in the FQ dataset resulting in a
database containing 18 million entities, 2072 rela-
tions, 635 types, 135 million facts and 81 domains,
including for example film, sports, and business. We
use this schema to define our target domain, allow-
ing for a wider variety of queries than could be en-
coded with the 635 collapsed relations previously
used to label the FQ data.
1552
We report two different experiments on the FQ
data: test results on the existing 642/275 train/test
split and domain adaptation results where the data is
split three ways, partitioning the topics so that the
logical meaning expressions do not share any sym-
bols across folds. We report on the standard 600/280
training/test split for GeoQuery.
Parameter Initialization and Training We ini-
tialize weights for ?np and ?direct to 10, and weights
for ?stem and ?join to 5. This promotes the use of
entities and relations named in sentences. We ini-
tialize weights for ?pp and ?emp to -1 to favour log-
ical forms that have an interpretation in the knowl-
edge base K. All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which point perfor-
mance on the development set had converged. This
fast convergence is due to the very small number of
matching parameters used (5 lexical features and 8
K features). For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations. We set the pruning pa-
rameters from Section 6.1 as follows: k = 5 for
Freebase, k = 30 for GeoQuery, N = 50, ? = 10.
Comparison Systems We compare performance
to state-of-the-art systems in both domains. On
GeoQuery, we report results from DCS (Liang
et al, 2011) without special initialization (DCS) and
with an small hand-engineered lexicon (DCS with
L+). We also include results for the FUBL algo-
rithm (Kwiatkowski et al, 2011), the CCG learning
approach that is most closely related to our work. On
FQ, we compare to Cai and Yates (2013a) (CY13).
Evaluation We evaluate by comparing the pro-
duced question answers to the labeled ones, with no
partial credit. Because the parser can fail to pro-
duce a complete query, we report recall, the percent
of total questions answered correctly, and precision,
the percentage of produced queries with correct an-
swers. CY13 and FUBL report fully correct logical
forms, which is a close proxy to our numbers.
9 Results
Quantitative Analysis For FQ, we report results
on the test set and in the cross-domain setting, as de-
fined in Section 8. Figure 6 shows both results. Our
Setting System R P F1
Test Our Approach 68.0 76.7 72.1
CY13 59 67 63
Cross Our Approach 67.9 73.5 71.5
Domain CY13 60 69 65
Figure 6: Results on the FQ dataset.
R P F1
All Features 68.6 72.0 70.3
Without Wiktionary 67.2 70.7 68.9
Without K Features 61.8 62.5 62.1
Figure 7: Ablation Results
approach outperforms the previous state of the art,
achieving a nine point improvement in test recall,
while not requiring labeled logical forms in train-
ing. We also see consistent improvements on both
scenarios, indicating that our approach is generaliz-
ing well across topic domains. The learned ontology
matching model is able to reason about previously
unseen ontological subdomains as well as if it was
provided explicit, in-domain training data.
We also performed feature ablations with 5-fold
cross validation on the training set, as seen in Fig-
ure 7. Both the Wiktionary features and knowledge
base features were helpful. Without the Wiktionary
features, the model must rely on word stem matches
which, in combination with graph constraints, can
still recover many of the correct queries. However,
without the knowledge base constraints, the model
produces many queries that return empty answers,
and significantly impacts overall performance.
For GeoQuery, we report test results in Figure 8.
Our approach outperforms the most closely related
CCG model (FUBL) and DCS without initialization,
but falls short of DCS with a small hand-built initial
lexicon. Given the small size of the test set, it is fair
to say that all algorithms are performing at state-of-
the-art levels. This result demonstrates that our ap-
Recall
FUBL 88.6
DCS 87.9
DCS with L+ 91.1
Our Approach 89.0
Figure 8: GeoQuery Results
1553
Parse Failures (20%)
1. Query in what year did motorola have the most revenue
2 Query on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query how many children does jerry seinfeld have
3. Labeled ?x.eq(x, count(?y.people.person.children(jerry seinfeld, y)))
Predicted ?x.eq(x, count(?y.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)
Query how many countries participated in the 2006 winter olympics
4. Labeled ?y.olympics.olympic games.number of countries(2006 winter olympics, y)
Predicted ?y.eq(y, count(?y.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query what programming languages were used for aol instant messenger
5. Labeled ?y.computer.software.languages used(aol instant messenger, y)
Predicted ?y.computer.software.languages used(aol instant messenger, y) ? computer.programming language(y)
Lexical Ambiguity (35%)
Query when was the frida kahlo exhibit at the philadelphia art museum
Labeled ?y.?x.exhibition run.exhibition(x, frida kahlo)?
6. exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.opened on(x, y)
Predicted ?y.?x.exhibition run.exhibition(x, frida kahlo)?
exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.closed on(x, y)
Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard
references. 5% of the cases were miscellaneous or otherwise difficult to categorize.
proach can handle the high degree of lexical ambi-
guity in the FQ data, without sacrificing the ability
to understanding the rich, compositional phenomena
that are common in the GeoQuery data.
Qualitative Analysis We also did a qualitative
analysis of errors in the FQ domain. The model
learns to correctly produce complex forms that join
multiple relations. However, there are a number of
systematic error cases, grouped into four categories
as seen in Figure 9.
The first and second examples show parse fail-
ures, where the underspecified CCG grammar did
not have sufficient coverage. The third shows a
failed structural match, where all of the correct logi-
cal constants are selected, but the argument order is
reversed for one of the literals. The fourth and fifth
examples demonstrate a failures due to database in-
completeness. In both cases, the predicted queries
would have returned the same answers as the gold-
truth ones if Freebase contained all of the required
facts. Developing models that are robust to database
incompleteness is a challenging problem for future
work. Finally, the last example demonstrates a lex-
ical ambiguity, where the system was unable to de-
termine if the query should include the opening date
or the closing date for the exhibit.
10 Conclusion
We considered the problem of learning domain-
independent semantic parsers, with application to
QA against large knowledge bases. We introduced
a new approach for learning a two-stage semantic
parser that enables scalable, on-the-fly ontological
matching. Experiments demonstrated state-of-the-
art performance on benchmark datasets, including
effective generalization to previously unseen words.
We would like to investigate more nuanced no-
tions of semantic correctness, for example to support
many of the essentially equivalent meaning repre-
sentations we found in the error analysis. Although
we focused exclusively on QA applications, the gen-
eral two-stage analysis approach should allow for
the reuse of learned grammars across a number of
different domains, including robotics or dialog ap-
plications, where data is more challenging to gather.
11 Acknowledgements
This research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), the NSF (IIS-1115966), and
by a gift from Google. The authors thank Anthony
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel
Weld, and the anonymous reviewers for their helpful
comments and feedback.
1554
References
Alshawi, H. (1992). The core language engine. The
MIT Press.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013). Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associ-
ation for Computational Linguistics, 1(1):49?62.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. (2008). Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data.
Bos, J. (2008). Wide-coverage semantic analysis
with boxer. In Proceedings of the Conference on
Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale semantic
parsing via schema matching and lexicon exten-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic parsing
freebase: Towards open-domain semantic pars-
ing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S. and Curran, J. (2007). Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Clarke, J., Goldwasser, D., Chang, M., and Roth,
D. (2010). Driving semantic parsing from the
world?s response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, pages 105?
148.
Doan, A., Madhavan, J., Domingos, P., and Halevy,
A. (2004). Ontology matching: A machine
learning approach. In Handbook on ontologies.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al (2007).
Ontology matching. Springer.
Fader, A., Zettlemoyer, L., and Etzioni, O. (2013).
Paraphrase-driven learning for open question an-
swering. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Goldwasser, D. and Roth, D. (2011). Learning from
natural instructions. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence.
Grosz, B. J., Appelt, D. E., Martin, P. A., and
Pereira, F. (1987). TEAM: An experiment in
the design of transportable natural language inter-
faces. Artificial Intelligence, 32(2):173?243.
Hobbs, J. R. (1985). Ontological promiscuity. In
Proceedings of the Annual Meeting on Associa-
tion for Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers.
In Proceedings of the 50th Annual Meeting of the
Association of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2).
Krishnamurthy, J. and Mitchell, T. (2012). Weakly
supervised training of semantic parsers. In Pro-
ceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics.
1555
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics.
In Proceedings of the Conference of the Associ-
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012). A joint model of language
and perception for grounded attribute learning. In
Proceedings of the International Conference on
Machine Learning.
Muresan, S. (2011). Learning for deep language un-
derstanding. In Proceedings of the International
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The
MIT Press.
Unger, C., Bu?hmann, L., Lehmann, J.,
Ngonga Ngomo, A., Gerber, D., and Cimiano, P.
(2012). Template-based question answering over
RDF data. In Proceedings of the International
Conference on World Wide Web.
Wong, Y. and Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Confer-
ence of the Association for Computational Lin-
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath,
M., Tresp, V., and Weikum, G. (2012). Natural
language questions for the web of data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sentences
to logical form. In Proceedings of the Joint Con-
ference of the Association for Computational Lin-
guistics and International Joint Conference on
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for relation extraction
with minimal supervision. In Proceeds of the
Conference on Artificial Intelligence.
1556
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Morpho-syntactic Lexical Generalization
for CCG Semantic Parsing
Adrienne Wang
Computer Science & Engineering
University of Washington
Seattle, WA
axwang@cs.washington.edu
Tom Kwiatkowski
Allen Institute for AI
Seattle, WA
tomk@allenai.org
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
In this paper, we demonstrate that
significant performance gains can be
achieved in CCG semantic parsing
by introducing a linguistically moti-
vated grammar induction scheme. We
present a new morpho-syntactic fac-
tored lexicon that models systematic
variations in morphology, syntax, and
semantics across word classes. The
grammar uses domain-independent
facts about the English language to
restrict the number of incorrect parses
that must be considered, thereby
enabling effective learning from less
data. Experiments in benchmark
domains match previous models with
one quarter of the data and provide
new state-of-the-art results with all
available data, including up to 45%
relative test-error reduction.
1 Introduction
Semantic parsers map sentences to formal
representations of their meaning (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011). One common approach is
to induce a probabilistic CCG grammar, which
defines the meanings of individual words and
phrases and how to best combine them to an-
alyze complete sentences. There has been re-
cent work developing learning algorithms for
CCG semantic parsers (Kwiatkowski et al.,
2010; Artzi and Zettlemoyer, 2011) and using
them for applications ranging from question
answering (Cai and Yates, 2013b; Kwiatkowski
et al., 2013) to robot control (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013).
One key learning challenge for this style
of learning is to induce the CCG lexicon,
which lists possible meanings for each phrase
and defines a set of possible parses for
each sentence. Previous approaches have
either hand-engineered a small set of lexi-
cal templates (Zettlemoyer and Collins, 2005,
2007) or automatically learned such tem-
plates (Kwiatkowski et al., 2010, 2011). These
methods are designed to learn grammars that
overgenerate; they produce spurious parses
that can complicate parameter estimation.
In this paper, we demonstrate that signif-
icant gains can instead be achieved by using
a more constrained, linguistically motivated
grammar induction scheme. The grammar
is restricted by introducing detailed syntac-
tic modeling of a wider range of constructions
than considered in previous work, for example
introducing explicit treatments of relational
nouns, Davidsonian events, and verb tense.
We also introduce a new lexical generalization
model that abstracts over systematic morpho-
logical, syntactic, and semantic alternations
within word classes. This includes, for exam-
ple, the facts that verbs in relative clauses and
nominal constructions (e.g., ?flights departing
NYC? and ?departing flights?) should be in-
finitival while verbs in phrases (e.g., ?What
flights depart at noon??) should have tense.
These grammar modeling techniques use uni-
versal, domain-independent facts about the
English language to restrict word usage to ap-
propriate syntactic contexts, and as such are
potentially applicable to any semantic parsing
application.
More specifically, we introduce a new
morpho-syntactic, factored CCG lexicon that
imposes our grammar restrictions during
learning. Each lexical entry has (1) a word
stem, automatically constructed from Wik-
tionary, with part-of-speech and morpholog-
ical attributes, (2) a lexeme that is learned
1284
and pairs the stem with semantic content that
is invariant to syntactic usage, and (3) a lexi-
cal template that specifies the remaining syn-
tactic and semantic content. The full set of
templates is defined in terms of a small set of
base templates and template transformations
that model morphological variants such as pas-
sivization and nominalization of verbs. This
approach allows us to efficiently encode a gen-
eral grammar for semantic parsing while also
eliminating large classes of incorrect analyses
considered by previous work.
We perform experiments in two benchmark
semantic parsing datasets: GeoQuery (Zelle
and Mooney, 1996) and ATIS (Dahl et al.,
1994). In both cases, our approach
achieves state-of-the-art performance, includ-
ing a nearly 45% relative error reduction on
the ATIS test set. We also show that the gains
increase with less data, including matching
previous model?s performance with less than
25% of the training data. Such gains are par-
ticularly practical for semantic parsers; they
can greatly reduce the amount of data that is
needed for each new application domain.
2 Related Work
Grammar induction methods for CCG seman-
tic parsers have either used hand-engineered
lexical templates, e.g. (Zettlemoyer and
Collins, 2005, 2007; Artzi and Zettlemoyer,
2011), or algorithms to learn such templates
directly from data, e.g. (Kwiatkowski et al.,
2010, 2011). Here, we extend the first ap-
proach, and show that better lexical general-
ization provides significant performance gains.
Although CCG is a common choice
for semantic parsers, many other for-
malisms have been studied, including DCS
trees (Liang et al., 2011), integer linear pro-
grams (Clarke et al., 2010), and synchronous
grammars (Wong and Mooney, 2007; Jones
et al., 2012; Andreas et al., 2013). All of these
approaches build complete meaning represen-
tations for individual sentences, but the data
we use has also been studied in related work on
cross-sentence reasoning (Miller et al., 1996;
Zettlemoyer and Collins, 2009) and model-
ing semantic interpretation as a tagging prob-
lem (Tur et al., 2013; Heck et al., 2013). Al-
though we focus on full analysis with CCG,
the general idea of using linguistic constraints
to improve learning is broadly applicable.
Semantic parsers are also commonly learned
from a variety of different types of supervision,
including logical forms (Kate and Mooney,
2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al., 2012), question-
answer pairs (Clarke et al., 2010; Liang et al.,
2011), conversational logs (Artzi and Zettle-
moyer, 2011), distant supervision (Krishna-
murthy and Mitchell, 2012; Cai and Yates,
2013b), sentences paired with system behav-
ior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013b),
and even from database constraints with no
explicit semantic supervision (Poon, 2013).
We learn from logical forms, but CCG learn-
ing algorithms have been developed for each
case above, making our techniques applicable.
There has been significant related work that
influenced the design of our morpho-syntactic
grammars. This includes linguistics stud-
ies of relational nouns (Partee and Borschev,
1998; de Bruin and Scha, 1988), Davidsonian
events (Davidson, 1967), parsing as abduc-
tion (Hobbs et al., 1988), and other more gen-
eral theories for lexicons (Pustejovsky, 1991)
and CCG (Steedman, 2011). It also includes
work on using morphology in CCG syntac-
tic parsing (Honnibal et al., 2010) and more
broad-coverage semantics in CCG (Bos, 2008;
Lewis and Steedman, 2013). However, our
work is unique in studying the use of related
ideas for semantic parsing.
Finally, there has also been recent progress
on semantic parsing against large, open do-
main databases such as Freebase (Cai and
Yates, 2013a; Kwiatkowski et al., 2013; Berant
et al., 2013). Unfortuantely, existing Freebase
datasets are not a good fit to test our approach
because the sentences they include have rela-
tively simple structure and can be interepreted
accurately using only factoid lookups with no
database joins (Yao and Van Durme, 2014).
Our work focuses on learning more syntacti-
cally rich models that support compositional
reasoning.
3 Background
Lambda Calculus We represent the mean-
ings of sentences, words and phrases with
1285
list one way flights from various cities
S/N N/N N PP/NP NP/N N
?f.f ?f?x.oneway(x) ? f(x) ?x.flight(x) ?x?y.from(y, x) ?fAx.f(x) ?x.city(x)
>
NP
Ax.city(x)
>
PP
?x.from(x,Ay.city(y))
>T
N\N
?x.from(x,Ay.city(y))
<
N
?x.flight(x) ? from(x,Ay.city(y))
>
N
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
>
S
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
Figure 1: An example CCG parse.
lambda calculus logical expressions. We use a
version of the typed lambda calculus (Carpen-
ter, 1997), in which the basic types include en-
tities, events, truth values and numbers. Func-
tion types are assigned to lambda expressions.
The expression ?x.flight(x) with type ?e, t?
takes an entity and returns a truth value, and
represents a set of flights.
Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a formalism
that tightly couples syntax and semantics,
and can be used to model a wide range of
linguistic phenomena. A traditional CCG
grammar includes a lexicon ? with lexical
entries like the following:
flights ` N :?x.flight(x)
from ` PP/NP :?y.?x.from(x, y)
cities ` N :?x.city(x)
where a lexical item w `X : h has words w,
syntactic category X, and logical expression h.
CCG uses a small set of combinatory rules
to jointly build syntactic parses and semantic
representations. Two common combinatory
rules are forward (>) and backward (<)
application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
CCG also includes combinatory rules of for-
ward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ? X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ? X\Z : ?x.f(g(x)) (< B)
These rules apply to build syntactic and se-
mantic derivations concurrently.
In this paper, we also implement type
raising rules for compact representation of
PP (prepositional phrase) and AP (adverbial
phrase).
PP : g ? N\N : ?f?x.f(x) ? g(x) (T)
AP : g ? S\S : ?f?e.f(e) ? g(e) (T)
AP : g ? S/S : ?f?e.f(e) ? g(e) (T)
Figure 1 shows an example CCG
parse (Steedman, 1996, 2000) where the
lexical entries are listed across the top and
the output lambda-calculus meaning repre-
sentation is at the bottom. This meaning is a
function (denoted by ?x...) that defines a set
of flights with certain properties and includes
a generalized Skolem constant (Steedman,
2011) (Ay...) that performs existential quan-
tification. Following recent work (Artzi and
Zettlemoyer, 2013b), we use meaning repre-
sentations that model a variety of linguistic
constructions, for example including Skolem
constants for plurals and Davidson quantifiers
for events, which we will introduce briefly
throughout this paper as they appear.
Weighted CCGs A weighted CCG gram-
mar is defined as G = (?,?), where ? is a
CCG lexicon and ? ? R
d
is a d-dimensional
parameter vector, which will be used to rank
the parses allowed under ?.
For a sentence x, G produces a set of candi-
1286
date parse trees Y = Y(x;G). Given a feature
vector ? ? R
d
, each parse tree y for sentence
x is scored by S(y; ?) = ? ??(x, y). The output
logical form z? is then defined to be at the root
of the highest-scoring parse y?:
y? = arg max
y?Y(x;G)
S(y; ?) (1)
We use existing CKY-style parsing algo-
rithms for this computation, implemented
with UW SPF (Artzi and Zettlemoyer, 2013a).
Section 7 describes the set of features we use
in the learned models.
Learning with GENLEX We will also
make use of an existing learning algo-
rithm (Zettlemoyer and Collins, 2007) (ZC07).
We first briefly review the ZC07 algorithm,
and describe our modifications in Section 7.
Given a set of training examples D =
{(x
i
, z
i
) : i = 1...n}, x
i
being the ith sentence
and z
i
being its annotated logical form, the al-
gorithm learns a set of parameters ? for the
grammar, while also inducing the lexicon ?.
The ZC07 learning algorithm uses a function
GENLEX(x, z) to define a set of lexical entries
that could be used to parse the sentence x to
construct the logical form z. For each training
example (x, z), GENLEX(x, z) maps all sub-
strings x to a set of potential lexical entries,
generated by exhaustively pairing the logical
constants in z using a set of hand-engineered
templates. The example is then parsed with
this much bigger lexicon and lexical entries
from the highest scoring parses are added to ?.
The parameters ? used to score parses are up-
dated using a perceptron learning algorithm.
4 Morpho-Syntactic Lexicon
This section defines our morpho-syntactic lex-
ical formalism. Table 1 shows examples of how
lexemes, templates, and morphological trans-
formations are used to build lexical entries for
example verbs. In this section, we formally de-
fine each of these components and show how
they are used to specify the space of possible
lexical entries that can be built for each input
word. In the following two sections, we will
provide more discussion of the complete sets
of templates (Section 5) and transformations
(Section 6).
Verb, Noun, Preposition, Pronoun, Adjective,
Adverb, Conjunction, Numeral, Symbol,
Proper Noun, Interjection, Expression
Table 2: Part-of-Speech types
POS Attribute Values
Noun Number singular, plural
Verb Person first, second, third
Verb Voice active, passive
Verb Tense present, past
Verb Aspect simple, progressive, perfect
Verb Participle present participle,
past participle
Adj, Degree of comparative, superlative
Adv, comparison
Det
Table 3: Morphological attributes and values.
We build on the factored CCG lexicon in-
troduced by Kwiatkowski et al. (2011) but (a)
further generalize lexemes to represent word
stems, (b) constrain the use of templates with
widely available syntactic information, and (c)
efficiently model common morphological vari-
ations between related words.
The first step, given an input word w, is
to do morphological and part-of-speech analy-
sis with the morpho-syntactic function F .
F maps a word to a set of possible morpho-
syntactic representations, each containing a
triple (s, p,m) of word stem s, part-of-speech
p and morphological category m. For exam-
ple, F maps the word flies to two possible
representations:
F (flies) = {(fly,Noun, (plural)),
(fly,Verb, (third, singular, simple, present))}
for the plural noun and present-tense verb
senses of the word. F is defined based on the
stems, part-of-speech types, and morpholog-
ical attributes marked for each definition in
Wiktionary.
1
The full sets of possible part-of-
speech and morphological types required for
our domains are shown in Table 2 and Table 3.
Each morpho-syntactic analysis a ? F (w)
is then paired with lexemes based on stem
match. A lexeme (s,~c) pairs a word stem
s with a list of logical constants ~c = [c
1
. . . c
k
].
Table 1 shows the words ?depart?, ?departing?,
?departure?, which are all assigned the lex-
eme (depart, [depart]). In general, there can
1
www.wiktionary.com
1287
Word Lexeme : Base Template Trans Lexical entry
depart
(depart, [depart]) :
I depart `S\NP :?x?e.depart(e, x)
departing I departing `S\NP :?x?e.depart(e, x)
departing
? `S\NP :?x?e.v
1
(e, x)
f
pres
departing `PP :?x?e.depart(e, x)
departure f
nom
departure `N :?x?e.depart(e, x)
use
(use, [airline]) :
I use `S\NP/NP :?x?y?e.airline(e, y, x)
using I using `S\NP/NP :?x?y?e.airline(e, y, x)
using
? `S\NP/NP :?x?y?e.v
1
(e, y, x)
f
pres
using `PP/NP :?x?e.airline(e, y, x)
use f
nom
use `N/NP :?x?y?e.airline(e, y, x)
Trans Template Transformation
f
pres
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
f
nom
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
Table 1: Lexical entries constructed by combining a lexeme, base template, and transformation
for the intransitive verb ?depart? and the transitive verb ?use?.
be many different lexemes for each stem, that
vary in the selection of which logical constants
are included.
Given analysis (s, p,m) and lexeme (s,~c), we
can use a lexical template to construct a
lexical entry. Each template has the form:
?(?,~v).[? `X : h
~v
]
where ? and ~v are variables that abstract over
the words and logical constants that will be
used to define a lexical entry with syntax X
and templated logical form h
~v
.
To instantiate a template, ? is filled with the
original word w and the constants in ~c replace
the variables ~v. For example, the template
?(?,~v).[? ` S\NP : ?x?e.v
1
(e, x)] could be
used with the word ?departing? and the lexeme
(depart, [depart]) to produce the lexical entry
departing ` S\NP : ?x?e.depart(e, x). When
clear from context, we will omit the function
signature ?
p
(?,~v). for all templates, as seen in
Table 1.
In general, there can be many applicable
templates, which we organize as follows. Each
final template is defined by applying a mor-
phological transformation to one of a small
set of possible base templates. The pairing
is found based on the morphological analysis
(s, p,m), where each base template is associ-
ated with part-of-speech p and each transfor-
mation is indexed by the morphology m. A
transformation f
m
is a function:
f
m
(?
p
(?,~v).[? `X : h
~v
]) = ?
p
(?,~v).[? `X
?
: h
?
~v
]
that takes the base template as input and pro-
duces a new template to model the inflected
form specified by m.
For example, both base templates in Ta-
ble 1 are for verbs. The template ? `
S\NP : ?x?e.v
1
(e, x) can be translated into
three other templates based on the transfor-
mations I, f
pres
, and f
nom
, depending on the
analysis of the original words. These transfor-
mations generalize across word type; they can
be used for the transitive verb ?use? as well as
the intransitive ?depart.? Each resulting tem-
plate, potentially including the original input
if the identity transformation I is available,
can then be used to make an output lexical
entry, as we described above.
5 Lexical Templates
The templates in our lexicon, as introduced
in Section 4, model the syntactic and seman-
tic aspects of lexical entries that are shared
within each word class. Previous approaches
have also used hand-engineered lexical tem-
plates, as described in Section 2, but we dif-
fer by (1) using more templates allowing for
more fine grained analysis and (2) using word
class information to restrict template use, for
example ensuring that words which cannot be
verbs are never paired with templates designed
for verbs. This section describes the templates
used during learning, first presenting those de-
signed to model grammatical sentences and
then a small second set designed for more el-
liptical spoken utterances.
Base Forms Table 4 lists the primary tem-
plate set, where each row shows an example
with a sentence illustrating its use. Templates
are also grouped by the word classes, including
adjectives, adverbs, prepositions, and several
types of nouns and verbs. While there is not
enough space to discuss each row, it is worth
1288
word class example usage base template
Noun phrase Boston ? `NP : v
Noun (regular) What flight is provided by delta? ? `N : ?x.v(x)
Noun (relation) I need fares of flights ? `N/PP : ?x?y.v(x, y)
delta schedule ? `N\(N/N) : ?f?x.v(Ay.f(?z.true, y), x)
Noun (function) size of California ? `NP/NP : ?x.v(x)
V
intrans
What flights depart from New York? ? `S\NP : ?x?e.v(e, x)
V
trans
Which airlines serve Seattle (active verb) ? `S\NP/NP :?x?y?e.v(e, y, x)
What airlines have flights (passive verb) ? `S\NP/NP :?x?y?e.v(e, x, y)
V
ditrans
They give him a book ? `S\NP/NP/NP : ?x?y?z?e.v(e, z, y, x)
V
imperson
It costs $500 to fly to Boston ? `S\NP/NP/NP :?x?y?z?e.v(e, y, x)
V
aux
The flights have arrived at Boston ? `S\NP/(S\NP ) :?f.f
? `S/NP/(S/NP ) :?f.f
Does delta provide flights from Seattle? ? `S/S :?f.f
V
copula
The flights are from Boston ? `S\NP/PP :?f?x.f(x)
What flight is cheap? ? `S\NP/(N/N) :?f?x.f(?y.true, x)
Alaska is the state with the most rivers ? `S\NP/NP :?x?y.equals(y, x)
Adjective I need a one way flight ? `N/N :?f?x.f(x) ? v(x)
Boston flights round trip ? `PP :?x.v(x)
How long is mississippi? ? `DEG :?x.v(x)
Preposition List flights from Boston ? `PP/NP :?x?y.v(y, x)
List flights that go to Dallas ? `AP/NP :?x?e.v(e, x)
List flights between Dallas and Boston ? `PP/NP/NP :?x?y?z.v
1
(z, x) ? v
2
(z, y)
What flights leave between 8am and 9am? ? `AP/NP/NP :?x?y?e.v
1
(e, x) ? v
2
(e, y)
Adverb Which flight departs daily? ? `AP :?e.v(e)
How early does the flight arrive? ? `DEG :?x.v(x)
Determiner Which airline has a flight from Boston? ? `NP/N :?fAx.f(x)
Table 4: Base templates that define different syntactic roles.
type example usage base template
t
elliptical
flights Newark to Cleveland ? `PP :?x.P (x, v)
flights arriving 2pm ? `AP :?e.P (e, v)
american airline from Denver ? `N :?x.P (x, v)
t
metonymy
List airlines from Seattle ? `N/PP :?f?x.v(x) ? P (Ay.f(y), x))
Shat airlines depart from Seattle? ? `N/(S\NP ) :?f?x.v(x) ? P (Ay.f(y), x)
fares from miami to New York ? `N/PP :?f?x.v(Ay.f(y), x)
Table 5: Base templates for ungrammatical linguistic phenomena
considering nouns as an illustrative example.
We model nouns as denoting a set of entities
that satisfy a given property. Regular nouns
are represented using unary predicates. Rela-
tional nouns syntactically function as regular
nouns but semantically describe sets of enti-
ties that have some relationship with a comple-
ment (Partee and Borschev, 1998). For exam-
ple, the relational noun fare describes a binary
relationship between flights and their price in-
formation, as we see in this parse:
fares of flights
N/PP PP/NP N
?x?y.fare(x, y) ?x.x ?x.flight(x)
>T
NP
Ax.flight(x)
>
PP
Ax.flight(x)
>
N
?x.fare(Ay.flight(y), x)
This analysis differs from previous ap-
proaches (Zettlemoyer and Collins, 2007),
where relational nouns were treated as regu-
lar nouns and prepositions introduced the bi-
nary relationship. The relational noun model
reduces lexical ambiguity for the prepositions,
which are otherwise highly polysemous.
Adjectives are nominal modifiers that take
a noun or a noun phrase as an argument and
add properties through conjunction. Preposi-
tions take nominal objects and function as ad-
jectival modifiers for nouns or adverbial modi-
fiers for verbs. Verbs can be subcategorized
by their grammatical structures into transi-
tive (V
trans
), intransitive (V
intrans
), imper-
sonal (V
imperson
), auxiliary (V
aux
) and copula
(V
copula
). Adverbs are verb modifiers defin-
ing aspects like time, rate and duration. The
adoption of event semantics allows adverbial
modifiers to be represented by predicates and
1289
linked by the shared events. Determiners pre-
cede nouns or noun phrases and distinguish
a reference of the noun. Following the gen-
eralized Skolem terms, we model determiners,
including indefinite and definite articles, as a
??e, t?, e? function that selects a unique indi-
vidual from a ?e, t?-typed function defining a
singleton set.
Missing Words The templates presented so
far model grammatically correct input. How-
ever, in dialogue domains such as ATIS, speak-
ers often omit words. For example, speak-
ers can drop the preposition ?from? in ?flights
from Newark to Cleveland? to create the ellip-
tical utterance ?flights Newark to Cleveland?.
We address this issue with the templates
t
elliptical
illustrated in Table 5. Each of these
adds a binary relation P to a lexeme with a
single entity typed constant. For our example,
the word ?Newark? could be assigned the lexi-
cal item Newark `PP : ?x.from(x, newark)
by selecting the first template and P = from.
Another common problem is the use of
metonymy. In the utterance ?What airlines
depart from New York??, the word ?airlines?
is used to reference flight services operated by
a specific airline. This is problematic because
the word ?depart? needs to modify an event of
type flight. We solve this with the t
metonymy
templates in Table 5. These introduce a binary
predicate P that would, in the case of our ex-
ample, map airlines on to the flights that they
operate.
The templates in Table 5 handle the ma-
jor cases of missing words seen in our data
and are more efficient than the approach taken
by (Zettlemoyer and Collins, 2007) who intro-
duced complex type shifting rules and relaxed
the grammar to allow every word order.
6 Morphological Transformations
Finally, the morpho-syntactic lexicon intro-
duces morphological transformations, which
are functions from base lexical templates to
lexical templates that model the syntactic and
semantic variation as the word is inflected.
These transformations allow us to compactly
model, for example, the facts that argument
order is reversed when moving from active to
passive forms of the same verb, and that the
subject can be omitted. To the best of our
knowledge, we are the first to study such trans-
formations for semantic parsing.
Table 6 shows the transformations. Each
row groups a set of transformations by linguis-
tic category, including singular vs. plural num-
ber, active vs. passive voice, and so on, and
also includes example sentences where the out-
put templates could be used. Again, for space,
we do not detail the motivation for every class,
but it is worth looking at some of the alterna-
tions for verbs and nouns as our prototypical
example.
Some verbs can act as noun modifiers. For
example, the present participle ?using? mod-
ifies ?flights? in ?flights using twa?. To
model this variation, we use the transforma-
tion f
present part
, a mapping that changes the
root of the verb signature S\NP to PP :
f
present part
: ? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
)
? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
where T = [,NP,NP/NP ] instantiates this
rule for verbs that take different sets of argu-
ments, effectively allowing any verb that is in
its finite or -ing form to behave syntactically
like a prepositional phrase.
Intransitive present participles can also act
as prenominal adjectival modifiers as in ?the
departing flight?. We add a second mapping
that maps the intransitive category S\NP to
the noun modifier N/N .
f
present part
: ? `S\NP :?x?e.v(e, x)
? ? `N/N : ?f?x?e.f(x) ? v(e, x)
Finally, verbal nouns have meanings derived
from actions typically described by verbs but
syntactically function as nouns. For example,
landing in the phrase ?landing from jfk? is the
gerundive use of the verb land. We add the
following mapping to f
present part
and f
nominal
:
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ?
? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
with T from above. This allows for reuse of the
same meaning across quite different syntac-
tic constructs, including for example ?flights
that depart from Boston? and ?departure from
Boston.?
1290
Template transformations f
m
Example usage
Plural Number (f
plural
)
I flight ? early flights
? `N : ?x.v(x) ? ? `NP :Ax.v(x) city ? flights to cities
Singular Number (f
singular
)
I flight ? flight
Possessive (f
possess
)
? `NP : v ? ? `N/N :?f?x.f(x) ? P (x, v) delta ? delta?s flights
? `N : ?x.v(x) ? ? `N/N :?f?x.f(x) ? P (x,Ay.v(y)) airline ? airline?s flights
Passive Voice (f
passive
)
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
..x
n
) ? ? `Y/PP : ?x
1
..x
n
?e.v(e, x
n
..x
1
) serves ?is served by
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
, .., x
n
) ? ? `Y : ?x
1
..x
n?1
?e.v(e, x
n?1
..x
1
) name ?city named Austin
Present Participle (f
present
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) use ?flights using twa
? `S\NP :?x?e.v(e, x) ? ? `N/N : ?f?x?e.f(x) ? v(e, x) arrive ?arriving flights
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) land ? landings at jfk
Past Participle (f
past
)
? `S\NP/NP :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/PP : ?x
1
..x
n
?e.v(e, x
1
..x
n
) use ? plane used by
Nominalization (f
nominal
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) depart ? departure
Comparative (f
comp
)
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) < v(x) short ? shorter
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) > v(x) long ? longer
Superlative (f
super
)
? `DEG :?x.v(x) ? ? `NP/N :?f.argmin(?x.f(x), ?x.v(x)) short ? shortest
? `DEG :?x.v(x) ? ? `NP/N :?f.argmax(?x.f(x), ?x.v(x)) long ? longest
Table 6: Morphological transformations with examples. T = [,NP,NP/NP ] and Y =
[S\NP,S\NP/NP ] allow a single transformation to generalize across word type.
Nouns can be inflected by number to de-
note singular and plural forms or by adding
an apostrophe to mark a possessive case. The
transformation function f
singular
is an identity
transformation. Plurals may have different in-
terpretations: one is the generic ?e, t? set rep-
resentation, which requires no transformation
on the base, or plurals can occur without overt
determiners (bare plurals), but semantically
imply quantification. We create a plural to
singular type shifting rule which implements
the ??e, t?, e? skolem function to select a unique
individual from the set. The possessive trans-
formation f
possess
transfers the base template
to a noun modifier, and adds a binary predi-
cate P that encodes the relation.
There are also a number of instances of the
identity transformation function I, which does
not change the base template. Because the se-
mantics we are constructing was designed to
answer questions against a static database, it
does not need to represent certain phenomena
to return the correct answer. This includes
more advanced variants of person, tense, as-
pect, and potentially many others. Ideally,
these morphological attributes should add se-
mantic modifiers to the base meaning, for ex-
ample, tense can constrain the time at which
an event occurs. However, none of our do-
mains support such reasoning, so we assign the
identity transformation, and leave the explo-
ration of these issues to future work.
7 Learning
One advantage of our morpho-syntactic, fac-
tored lexicon is that it can be easily learned
with small modifications to existing algo-
rithms (Zettlemoyer and Collins, 2007). We
only need to modify the GENLEX proce-
dure that defines the space of possible lexi-
cal entries. For each training example (x, z),
GENLEX(x, z, F ) first maps each substring in
the sentence x into the morphological repre-
sentation (s, p, c) using F introduced in Sec-
tion 4. A candidate lexeme set L
?
is then gen-
erated by exhaustively pairing the word stems
with all subsets of the logical constants from
z. Lexical templates are applied to the lexemes
in L
?
to generate candidate lexical entries for
x. Finally, the lexemes that participate in the
top scoring correct parse of x are added to the
permanent lexicon.
Initialization Following standard practice,
we compile an initial lexicon ?
0
, which con-
sists of a list of domain independent lexical
1291
items for function words, such as interrogative
words and conjunctions. These lexical items
are mostly semantically vacuous and serve par-
ticular syntactic functions that are not gener-
alizable to other word classes. We also initial-
ize the lexemes with a list of NP entities com-
plied from the database, e.g., (Boston, [bos]).
Features We use two types of features in
the model for discriminating parses. Four lex-
ical features are fired on each lexical item:
?
(s,~c)
for the lexeme, ?
t
p
for the base tem-
plate, ?
t
m
for the morphologically modified
template, and ?
l
for the complete lexical
item. We also compute the standard logical
expression features (Zettlemoyer and Collins,
2007) on the root semantics to track the pair-
wise predicate-argument relations and the co-
occuring predicate-predicate relations in con-
junctions and disjunctions.
8 Experimental Setup
Data and Metrics We evaluate perfor-
mance on two benchmark semantic pars-
ing datasets, Geo880 and ATIS. We use
the standard data splits, including 600/280
train/test for Geo880 and 4460/480/450
train/develop/test for ATIS. To support the
new representations in Section 5, we sys-
tematically convert annotations with existen-
tial quantifiers, temporal events and relational
nouns to new logical forms with equivalent
meanings. All systems are evaluated with ex-
act match accuracy, the percentage of fully
correct logical forms.
Initialization We assign positive initial
weights to the indicator features for entries in
the initial lexicon, as defined in Section 7, to
encourage their use. The elliptical template
and metonymy template features are initial-
ized with negative weights to initially discour-
age word skipping.
Comparison Systems We compare perfor-
mance with all recent CCG grammar induc-
tion algorithms that work with our datasets.
This includes methods that used a limited
set of hand-engineered templates for inducing
the lexicon, ZC05 (Zettlemoyer and Collins,
2005) and ZC07 (Zettlemoyer and Collins,
2007), and those that learned grammar struc-
ture by automatically splitting the labeled log-
System Test
ZC05 79.3
ZC07 86.1
UBL 87.9
FUBL 88.6
DCS 87.9
FULL 90.4
DCS
+
91.1
Table 7: Exact-match Geo880 test accuracy.
System Dev Test
ZC07 74.4 84.6
UBL 65.6 71.4
FUBL 81.9 82.8
GUSP - 83.5
TEMP-ONLY 85.5 87.2
FULL 87.5 91.3
Table 8: Exact-match accuracy on the ATIS
development and test sets.
ical forms, UBL (Kwiatkowski et al., 2010)
and FUBL (Kwiatkowski et al., 2011). We
also compare the state-of-the-art for Geo880
(DCS (Liang et al., 2011) and DCS+ which in-
cludes an engineered seed lexicon) and ATIS
(which is ZC07). Finally, we include results
for GUSP (Poon, 2013), a recent unsupervised
approach for ATIS.
System Variants We report results for a
complete approach (Full), and variants which
use different aspects of the morpho-syntactic
lexicon. The TEMP-ONLY variant learned
with the templates from Section 5 but, like
ZC07, does not use any word class information
to restrict their use. The TEMP-POS removes
morphology from the lexemes, but includes the
word class information from Wiktionary. Fi-
nally, we also include DCS
+
, which initialize a
set of words with POS tag JJ, NN, and NNS.
9 Results
Full Models Tables 7 and 8 report the
main learning results. Our approach achieves
state-of-the-art accuracies on both datasets,
demonstrating that our new grammar induc-
tion scheme provides a type of linguistically
motivated regularization; restricting the algo-
rithm to consider a much smaller hypothesis
space allows to learn better models.
1292
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 100  500  1000  2000  4460
Re
cal
l
Training samples
TEMP_ONLY
TEMP_POS
FULL
FUBL
Figure 2: ATIS Learning Curve
On Geo880 the full method edges out the
best systems by 2% absolute on the test set,
as compared to other systems with no domain-
specific lexical initialization. Although DCS
requires less supervision, it also uses external
signals including a POS tagger.
We see similarly strong results for ATIS,
outperforming FUBL on the ATIS develop-
ment set by 6.8%, and improving the accu-
racy on the test set by 7.9% over the previous
best system ZC07. Unlike FUBL, which excels
at the development set but trails ZC07?s tem-
plated grammar by almost 2 points on the test
set, our approach demonstrates consistent im-
provements on both. Additionally, although
the unsupervised model (GUSP) rivals previ-
ous approaches, we are able to show that more
careful use of supervision open a much wider
performance gap.
Learning Curve with Ablations Figure 2
presents a learning curve for the ATIS domain,
demonstrating that the learning improvements
become even more dramatic for smaller train-
ing set sizes. Our model outperforms FUBL by
wide margins, matching its final accuracy with
only 22% of the total training examples. Our
full model also consistently beats the variants
with fewer word class restrictions, although
by smaller margins. Again, these results fur-
ther highlight the benefit of importing external
syntactic resources and enforcing linguistically
motivated constraints during learning.
Learned Lexicon The learned lexicon is
also more compact. Table 9 summarizes
statistics on unique lexical entries required
to parse the ATIS development set. The
System Lexical Entries Lexemes
FUBL 1019 721
Our Approach 818 495
Table 9: Lexicon size comparison on the ATIS
dev set (460 unique tokens).
morpho-syntactic model uses 80.3% of the lex-
ical entries and 63.7% of the lexemes that
FUBL needs, while increase performance by
nearly 7 points. Upon inspection, our model
achieves better lexical decomposition by learn-
ing shorter lexical units, for example, the
adoption of Davidsonian events allows us to
learn unambiguous adverbial modifiers, and
the formal modeling of nominalized nouns and
relational nouns treats prepositions as syntac-
tic modifiers, instead of being encoded in the
semantics. Such restrictions generalize to a
much wider variety of syntactic contexts.
10 Summary and Future Work
We demonstrated that significant performance
gains can be achieved in CCG semantic pars-
ing by introducing a more constrained, linguis-
tically motivated grammar induction scheme.
We introduced a morpho-syntactic factored
lexicon that uses domain-independent facts
about the English language to restrict the
number of incorrect parses that must be con-
sidered and demonstrated empirically that it
enables effective learning of complete parsers,
achieving state-of-the-art performance.
Because our methods are domain indepen-
dent they should also benefit other semantic
parsing applications and other learning algo-
rithms that use different types of supervision,
as we hope to verify in future work. We would
also like to study how to generalize these gains
to languages other than English, by inducing
more of the syntactic structure.
Acknowledgements
The research was supported by the NSF (IIS-
1115966, IIS-1252835) and the Intel Center
for Pervasive Computing at the Univeristy
of Washington. The authors thank Robert
Gens, Xiao Ling, Xu Miao, Mark Yatskar and
the UW NLP group for helpful discussions,
and the anonymous reviewers for helpful com-
ments.
1293
References
Andreas, J., Vlachos, A., and Clark, S. (2013).
Semantic parsing as machine translation.
Artzi, Y. and Zettlemoyer, L. (2011). Boot-
strapping semantic parsers from conversa-
tions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing.
Artzi, Y. and Zettlemoyer, L. (2013a). UW
SPF: The University of Washington Seman-
tic Parsing Framework.
Artzi, Y. and Zettlemoyer, L. (2013b). Weakly
supervised learning of semantic parsers for
mapping instructions to actions. Transac-
tions of the Association for Computational
Linguistics, 1(1):49?62.
Berant, J., Chou, A., Frostig, R., and Liang, P.
(2013). Semantic parsing on freebase from
question-answer pairs. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Bos, J. (2008). Wide-coverage semantic anal-
ysis with boxer. In Proceedings of the Con-
ference on Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale
semantic parsing via schema matching and
lexicon extension. In Proceedings of the An-
nual Meeting of the Association for Compu-
tational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic pars-
ing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Con-
ference on Lexical and Computational Se-
mantics.
Carpenter, B. (1997). Type-Logical Semantics.
The MITPress.
Chen, D. and Mooney, R. (2011). Learning
to interpret natural language navigation in-
structions from observations. In Proceedings
of the National Conference on Artificial In-
telligence.
Clarke, J., Goldwasser, D., Chang, M., and
Roth, D. (2010). Driving semantic parsing
from the world?s response. In Proceedings
of the Conference on Computational Natural
Language Learning.
Dahl, D. A., Bates, M., Brown, M., Fisher,
W., Hunicke-Smith, K., Pallett, D., Pao, C.,
Rudnicky, A., and Shriberg, E. (1994). Ex-
panding the scope of the atis task: The atis-
3 corpus. In Proceedings of the workshop on
Human Language Technology.
Davidson, D. (1967). The logical form of
action sentences. Essays on actions and
events, pages 105?148.
de Bruin, J. and Scha, R. (1988). The interpre-
tation of relational nouns. In Proceedings of
the Conference of the Association of Com-
putational Linguistics, pages 25?32. ACL.
Goldwasser, D. and Roth, D. (2011). Learning
from natural instructions. In Proceedings of
the International Joint Conference on Arti-
ficial Intelligence.
Heck, L., Hakkani-Tu?r, D., and Tur, G.
(2013). Leveraging knowledge graphs for
web-scale unsupervised semantic parsing. In
Proc. of the INTERSPEECH.
Hobbs, J. R., Stickel, M., Martin, P., and Ed-
wards, D. (1988). Interpretation as abduc-
tion. In Proceedings of the Association for
Computational Linguistics.
Honnibal, M., Kummerfeld, J. K., and Cur-
ran, J. R. (2010). Morphological analysis
can improve a ccg parser for english. In Pro-
ceedings of the International Conference on
Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S.
(2012). Semantic parsing with bayesian tree
transducers. In Proceedings of Association
of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In
Proceedings of the Conference of the Asso-
ciation for Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013).
Jointly learning to parse and perceive: Con-
necting natural language to the physical
world. Transactions of the Association for
Computational Linguistics, 1(2).
Krishnamurthy, J. and Mitchell, T. (2012).
Weakly supervised training of semantic
parsers. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Computational Natu-
ral Language Learning.
Kwiatkowski, T., Choi, E., Artzi, Y., and
1294
Zettlemoyer, L. (2013). Scaling semantic
parsers with on-the-fly ontology matching.
Kwiatkowski, T., Goldwater, S., Zettlemoyer,
L., and Steedman, M. (2012). A probabilis-
tic model of syntactic and semantic acquisi-
tion from child-directed utterances and their
meanings. Proceedings of the Conference of
the European Chapter of the Association of
Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2010). Induc-
ing probabilistic CCG grammars from log-
ical form with higher-order unification. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2011). Lexical
generalization in CCG grammar induction
for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Lewis, M. and Steedman, M. (2013). Com-
bined distributional and logical semantics.
Transactions of the Association for Compu-
tational Linguistics, 1:179?192.
Liang, P., Jordan, M., and Klein, D. (2011).
Learning dependency-based compositional
semantics. In Proceedings of the Conference
of the Association for Computational Lin-
guistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L.,
Bo, L., and Fox, D. (2012). A joint model
of language and perception for grounded at-
tribute learning. In Proceedings of the Inter-
national Conference on Machine Learning.
Miller, S., Stallard, D., Bobrow, R., and
Schwartz, R. (1996). A fully statistical ap-
proach to natural language interfaces. In
Proceedings Association for Computational
Linguistics.
Muresan, S. (2011). Learning for deep lan-
guage understanding. In Proceedings of the
International Joint Conference on Artificial
Intelligence.
Partee, B. H. and Borschev, V. (1998). Inte-
grating lexical and formal sematics: Gen-
itives, relational nouns, and type-shifting.
In Proceedings of the Second Tbilisi Sympo-
sium on Language, Logic, and Computation.
Poon, H. (2013). Grounded unsupervised se-
mantic parsing. In Association for Compu-
tational Linguistics (ACL).
Pustejovsky, J. (1991). The generative lexicon.
volume 17.
Steedman, M. (1996). Surface Structure and
Interpretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process.
The MIT Press.
Steedman, M. (2011). Taking Scope. The MIT
Press.
Tur, G., Deoras, A., and Hakkani-Tur, D.
(2013). Semantic parsing using word con-
fusion networks with conditional random
fields. In Proc. of the INTERSPEECH.
Wong, Y. and Mooney, R. (2007). Learning
synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the
Conference of the Association for Computa-
tional Linguistics.
Yao, X. and Van Durme, B. (2014). Informa-
tion extraction over structured data: Ques-
tion answering with freebase. In Association
for Computational Linguistics (ACL).
Zelle, J. and Mooney, R. (1996). Learning to
parse database queries using inductive logic
programming. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic cate-
gorial grammars. In Proceedings of the Con-
ference on Uncertainty in Artificial Intelli-
gence.
Zettlemoyer, L. and Collins, M. (2007). On-
line learning of relaxed CCG grammars for
parsing to logical form. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and Com-
putational Natural Language Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of
the Joint Conference of the Association
for Computational Linguistics and Interna-
tional Joint Conference on Natural Lan-
guage Processing.
1295
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234?244,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model of Syntactic and Semantic Acquisition from
Child-Directed Utterances and their Meanings
Tom Kwiatkowski* ?
tomk@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Mark Steedman?
steedman@inf.ed.ac.uk
? ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Abstract
This paper presents an incremental prob-
abilistic learner that models the acquis-
tion of syntax and semantics from a cor-
pus of child-directed utterances paired with
possible representations of their meanings.
These meaning representations approxi-
mate the contextual input available to the
child; they do not specify the meanings of
individual words or syntactic derivations.
The learner then has to infer the meanings
and syntactic properties of the words in the
input along with a parsing model. We use
the CCG grammatical framework and train
a non-parametric Bayesian model of parse
structure with online variational Bayesian
expectation maximization. When tested on
utterances from the CHILDES corpus, our
learner outperforms a state-of-the-art se-
mantic parser. In addition, it models such
aspects of child acquisition as ?fast map-
ping,? while also countering previous crit-
icisms of statistical syntactic learners.
1 Introduction
Children learn language by mapping the utter-
ances they hear onto what they believe those ut-
terances mean. The precise nature of the child?s
prelinguistic representation of meaning is not
known. We assume for present purposes that
it can be approximated by compositional logical
representations such as (1), where the meaning is
a logical expression that describes a relationship
have between the person you refers to and the
object another(x, cookie(x)):
Utterance : you have another cookie (1)
Meaning : have(you, another(x, cookie(x)))
Most situations will support a number of plausi-
ble meanings, so the child has to learn in the face
of propositional uncertainty1, from a set of con-
textually afforded meaning candidates, as here:
Utterance : you have another cookie
Candidate
Meanings
?
?
?
have(you, another(x, cookie(x)))
eat(you, your(x, cake(x)))
want(i, another(x, cookie(x)))
The task is then to learn, from a sequence of such
(utterance, meaning-candidates) pairs, the correct
lexicon and parsing model. Here we present a
probabilistic account of this task with an empha-
sis on cognitive plausibility.
Our criteria for plausibility are that the learner
must not require any language-specific informa-
tion prior to learning and that the learning algo-
rithm must be strictly incremental: it sees each
training instance sequentially and exactly once.
We define a Bayesian model of parse structure
with Dirichlet process priors and train this on a
set of (utterance, meaning-candidates) pairs de-
rived from the CHILDES corpus (MacWhinney,
2000) using online variational Bayesian EM.
We evaluate the learnt grammar in three ways.
First, we test the accuracy of the trained model
in parsing unseen utterances onto gold standard
annotations of their meaning. We show that
it outperforms a state-of-the-art semantic parser
(Kwiatkowski et al 2010) when run with similar
training conditions (i.e., neither system is given
the corpus based initialization originally used by
Kwiatkowski et al. We then examine the learn-
ing curves of some individual words, showing that
the model can learn word meanings on the ba-
sis of a single exposure, similar to the fast map-
ping phenomenon observed in children (Carey
and Bartlett, 1978). Finally, we show that our
1Similar to referential uncertainty but relating to propo-
sitions rather than referents.
234
learner captures the step-like learning curves for
word order regularities that Thornton and Tesan
(2007) claim children show. This result coun-
ters Thornton and Tesan?s criticism of statistical
grammar learners?that they tend to exhibit grad-
ual learning curves rather than the abrupt changes
in linguistic competence observed in children.
1.1 Related Work
Models of syntactic acquisition, whether they
have addressed the task of learning both syn-
tax and semantics (Siskind, 1992; Villavicencio,
2002; Buttery, 2006) or syntax alone (Gibson
and Wexler, 1994; Sakas and Fodor, 2001; Yang,
2002) have aimed to learn a single, correct, deter-
ministic grammar. With the exception of Buttery
(2006) they also adopt the Principles and Param-
eters grammatical framework, which assumes de-
tailed knowledge of linguistic regularities2. Our
approach contrasts with all previous models in as-
suming a very general kind of linguistic knowl-
edge and a probabilistic grammar. Specifically,
we use the probabilistic Combinatory Categorial
Grammar (CCG) framework, and assume only
that the learner has access to a small set of general
combinatory schemata and a functional mapping
from semantic type to syntactic category. Further-
more, this paper is the first to evaluate a model
of child syntactic-semantic acquisition by parsing
unseen data.
Models of child word learning have focused
on semantics only, learning word meanings from
utterances paired with either sets of concept sym-
bols (Yu and Ballard, 2007; Frank et al 2008; Fa-
zly et al 2010) or a compositional meaning rep-
resentation of the type used here (Siskind, 1996).
The models of Alishahi and Stevenson (2008)
and Maurits et al(2009) learn, as well as word-
meanings, orderings for verb-argument structures
but not the full parsing model that we learn here.
Semantic parser induction as addressed by
Zettlemoyer and Collins (2005, 2007, 2009), Kate
and Mooney (2007), Wong and Mooney (2006,
2007), Lu et al(2008), Chen et al(2010),
Kwiatkowski et al(2010, 2011) and Bo?rschinger
et al(2011) has the same task definition as the
one addressed by this paper. However, the learn-
ing approaches presented in those previous pa-
2This linguistic use of the term ?parameter? is distinct
from the statistical use found elsewhere in this paper.
pers are not designed to be cognitively plausible,
using batch training algorithms, multiple passes
over the data, and language specific initialisations
(lists of noun phrases and additional corpus statis-
tics), all of which we dispense with here. In
particular, our approach is closely related that of
Kwiatkowski et al(2010) but, whereas that work
required careful initialisation and multiple passes
over the training data to learn a discriminative
parsing model, here we learn a generative parsing
model without either.
1.2 Overview of the approach
Our approach takes, as input, a corpus of (ut-
terance, meaning-candidates) pairs {(si, {m}i) :
i = 1, . . . , N}, and learns a CCG lexicon ? and
the probability of each production a ? b that
could be used in a parse. Together, these define
a probabilistic parser that can be used to find the
most probable meaning for any new sentence.
We learn both the lexicon and production prob-
abilities from allowable parses of the training
pairs. The set of allowable parses {t} for a sin-
gle (utterance, meaning-candidates) pair consists
of those parses that map the utterance onto one of
the meanings. This set is generated with the func-
tional mapping T :
{t} = T (s,m), (2)
which is defined, following Kwiatkowski et al
(2010), using only the CCG combinators and a
mapping from semantic type to syntactic category
(presented in in Section 4).
The CCG lexicon ? is learnt by reading off
the lexical items used in all parses of all training
pairs. Production probabilities are learnt in con-
junction with ? through the use of an incremen-
tal parameter estimation algorithm, online Varia-
tional Bayesian EM, as described in Section 5.
Before presenting the probabilistic model, the
mapping T , and the parameter training algorithm,
we first provide some background on the meaning
representations we use and on CCG.
2 Background
2.1 Meaning Representations
We represent the meanings of utterances in first-
order predicate logic using the lambda-calculus.
An example logical expression (henceforth also
referred to as a lambda expression) is:
like(eve,mummy) (3)
235
which expresses a logical relationship like be-
tween the entity eve and the entity mummy. In
Section 6.1 we will see how logical expressions
like this are created for a set of child-directed ut-
terances (to use in training our model).
The lambda-calculus uses ? operators to define
functions. These may be used to represent func-
tional meanings of utterances but they may also be
used as a ?glue language?, to compose elements of
first order logical expressions. For example, the
function ?x?y.like(y, x) can be combined with
the object mummy to give the phrasal mean-
ing ?y.like(y,mummy) through the lambda-
calculus operation of function application.
2.2 CCG
Combinatory Categorial Grammar (CCG; Steed-
man 2000) is a strongly lexicalised linguistic for-
malism that tightly couples syntax and seman-
tics. Each CCG lexical item in the lexicon ? is
a triple, written as word ` syntactic category :
logical expression . Examples are:
You ` NP : you
read ` S\NP/NP : ?x?y.read(y, x)
the ` NP/N : ?f.the(x, f(x))
book ` N : ?x.book(x)
A full CCG category X : h has syntactic cate-
gory X and logical expression h. Syntactic cat-
egories may be atomic (e.g., S or NP) or com-
plex (e.g., (S\NP)/NP). Slash operators in com-
plex categories define functions from the range on
the right of the slash to the result on the left in
much the same way as lambda operators do in the
lambda-calculus. The direction of the slash de-
fines the linear order of function and argument.
CCG uses a small set of combinatory rules to
concurrently build syntactic parses and semantic
representations. Two example combinatory rules
are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
Given the lexicon above, the phrase ?You read the
book? can be parsed using these rules, as illus-
trated in Figure 1 (with additional notation dis-
cussed in the following section)..
CCG also includes combinatory rules of
forward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
3 Modelling Derivations
The objective of our learning algorithm is to
learn the correct parameterisation of a probabilis-
tic model P (s,m, t) over (utterance, meaning,
derivation) triples. This model assigns a proba-
bility to each of the grammar productions a ? b
used to build the derivation tree t. The probabil-
ity of any given CCG derivation t with sentence
s and semantics m is calculated as the product of
all of its production probabilities.
P (s,m, t) =
?
a?b?t
P (b|a) (4)
For example, the derivation in Figure 1 contains
13 productions, and its probability is the product
of the 13 production probabilities. Grammar pro-
ductions may be either syntactic?used to build a
syntactic derivation tree, or lexical?used to gen-
erate logical expressions and words at the leaves
of this tree.
A syntactic production Ch ? R expands a
head node Ch into a result R that is either an
ordered pair of syntactic parse nodes ?Cl,Cr?
(for a binary production) or a single parse node
(for a unary production). Only two unary syn-
tactic productions are allowed in the grammar:
START? A to generate A as the top syntactic
node of a parse tree and A? [A]lex to indicate
that A is a leaf node in the syntactic derivation
and should be used to generate a logical expres-
sion and word. Syntactic derivations are built by
recursively applying syntactic productions to non-
leaf nodes in the derivation tree. Each syntactic
production Ch ? R has conditional probability
P (R|Ch). There are 3 binary and 5 unary syntac-
tic productions in Figure 1.
Lexical productions have two forms. Logical
expressions are produced from leaf nodes in the
syntactic derivation tree Alex ? m with condi-
tional probability P (m|Alex). Words are then pro-
duced from these logical expressions with condi-
tional probability P (w|m). An example logical
production from Figure 1 is [NP]lex ? you. An
example word production is you? You.
Every production a ? b used in a parse tree t
is chosen from the set of productions that could
be used to expand a head node a. If there are a
finite K productions that could expand a then a
K-dimensional Multinomial distribution parame-
terised by ?a can be used to model the categorical
236
START
Sdcl
NP
[NP]lex
you
You
Sdcl\NP
(Sdcl\NP)/NP
[(Sdcl\NP)/NP]lex
?x?y.read(y, x)
read
NP
NP/N
[NP/N]lex
?f?x.the(x, f(x))
the
N
[N]lex
?x.book(x)
book
Figure 1: Derivation of sentence You read the
book with meaning read(you, the(x, book(x))).
choice of production:
b ? Multinomial(?a) (5)
However, before training a model of language ac-
quisition the dimensionality and contents of both
the syntactic grammar and lexicon are unknown.
In order to maintain a probability model with
cover over the countably infinite number of pos-
sible productions, we define a Dirichlet Process
(DP) prior for each possible production head a.
For the production head a, DP (?a, Ha) assigns
some probability mass to all possible production
targets {b} covered by the base distribution Ha.
It is possible to use the DP as an infinite prior
from which the parameter set of a finite dimen-
sional Multinomial may be drawn provided that
we can choose a suitable partition of {b}. When
calculating the probability of an (s,m, t) triple,
the choice of this partition is easy. For any given
production head a there is a finite set of usable
production targets {b1, . . . , bk?1} in t. We create
a partition that includes one entry for each of these
along with a final entry {bk, . . . } that includes all
other ways in which a could be expanded in dif-
ferent contexts. Then, by applying the distribution
Ga drawn from the DP to this partition, we get a
parameter vector ?a that is equivalent to a draw
from a k dimensional Dirichlet distribution:
Ga ? DP (?a, Ha) (6)
?a = (Ga(b1), . . . , Ga(bk?1), Ga({bk, . . . })
? Dir(?aH(b1), . . . , ?aHa(bk?1), (7)
?aHa({bk, . . . }))
Together, Equations 4-7 describe the joint distri-
bution P (X,S, ?) over the observed training data
X = {(si, {m}i) : i = 1, . . . , N}, the latent vari-
ables S (containing the productions used in each
parse t) and the parsing parameters ?.
4 Generating Parses
The previous section defined a parameterisation
over parses assuming that the CCG lexicon ? was
known. In practice ? is empty prior to training
and must be populated with the lexical items from
parses t consistent with training pairs (s, {m}).
The set of allowed parses {t} is defined by the
function T from Equation 2. Here we review the
splitting procedure of Kwiatkowski et al(2010)
that is used to generate CCG lexical items and de-
scribe how it is used by T to create a packed chart
representation of all parses {t} that are consistent
with s and at least one of the meaning represen-
tations in {m}. In this section we assume that s
is paired at each point with only a single meaning
m. Later we will show how T is used multiple
times to create the set of parses consistent with s
and a set of candidate meanings {m}.
The splitting procedure takes as input a CCG
category X :h, such as NP : a(x, cookie(x)), and
returns a set of category splits. Each category split
is a pair of CCG categories (Cl :ml,Cr :mr) that
can be recombined to give X : h using one of the
CCG combinators in Section 2.2. The CCG cat-
egory splitting procedure has two parts: logical
splitting of the category semantics h; and syntac-
tic splitting of the syntactic category X. Each logi-
cal split of h is a pair of lambda expressions (f, g)
in the following set:
{(f, g) | h = f(g) ? h = ?x.f(g(x))}, (8)
which means that f and g can be recombined us-
ing either function application or function com-
position to give the original lambda expression
h. An example split of the lambda expression
h = a(x, cookie(x)) is the pair
(?y.a(x, y(x)), ?x.cookie(x)), (9)
where ?y.a(x, y(x)) applied to ?x.cookie(x) re-
turns the original expression a(x, cookie(x)).
Syntactic splitting assigns linear order and syn-
tactic categories to the two lambda expressions f
and g. The initial syntactic category X is split by
a reversal of the CCG application combinators in
Section 2.2 if f and g can be recombined to give
237
Syntactic Category Semantic Type Example Phrase
Sdcl ?ev, t? I took it ` Sdcl :?e.took(i, it, e)
St t I?m angry ` St :angry(i)
Swh ?e, ?ev, t?? Who took it? ` Swh :?x?e.took(x, it, e)
Sq ?ev, t? Did you take it? ` Sq :?e.Q(take(you, it, e))
N ?e, t? cookie `N:?x.cookie(x)
NP e John `NP:john
PP ?ev, t? on John ` PP:?e.on(john, e)
Figure 2: Atomic Syntactic Categories.
h with function application:
{(X/Y : f Y : g), (10)
(Y : g : X\Y : f)|h = f(g)}
or by a reversal of the CCG composition combi-
nators if f and g can be recombined to give hwith
function composition:
{(X/Z : f Z/Y : g, (11)
(Z\Y : g : X\Z : f)|h = ?x.f(g(x))}
Unknown category names in the result of a
split (Y in (10) and Z in (11)) are labelled via a
functional mapping cat from semantic type T to
syntactic category:
cat(T ) =
?
?
?
Atomic(T ) if T ? Figure 2
cat(T1)/cat(T2) if T = ?T1, T2?
cat(T1)\cat(T2) if T = ?T1, T2?
?
?
?
which uses the Atomic function illustrated
in Figure 2 to map semantic-type to basic CCG
syntactic category. As an example, the logical
split in (9) supports two CCG category splits, one
for each of the CCG application rules.
(NP/N :?y.a(x, y(x)), N :?x.cookie(x)) (12)
(N :?x.cookie(x), NP\N :?y.a(x, y(x))) (13)
The parse generation algorithm T uses the func-
tion split to generate all CCG category pairs that
are an allowed split of an input category X :h:
{(Cl :ml,Cr :mr)} = split(X :h),
and then packs a chart representation of {t} in a
top-down fashion starting with a single cell entry
Cm :m for the top node shared by all parses {t}.
For the utterance and meaning in (1) the top parse
node, spanning the entire word-string, is
S :have(you, another(x, cookie(x))).
T cycles over all cell entries in increasingly small
spans and populates the chart with their splits. For
any cell entry X :h spanning more than one word
T generates a set of pairs representing the splits of
X :h. For each split (Cl :ml,Cr :mr) and every bi-
nary partition (wi:k, wk:j) of the word-span T cre-
ates two new cell entries in the chart: (Cl :ml)i:k
and (Cr :mr)k:j .
Input : Sentence [w1, . . . , wn], top node Cm :m
Output: Packed parse chart Ch containing {t}
Ch = [ [{}1, . . . , {}n]1, . . . , [{}1, . . . , {}n]n ]
Ch[1][n? 1] = Cm :m
for i = n, . . . , 2; j = 1 . . . (n? i) + 1 do
for X:h ? Ch[j][i] do
for (Cl :ml,Cr :mr) ? split(X:h) do
for k = 1, . . . , i? 1 do
Ch[j][k]? Cl :ml
Ch[j + k][i? k]? Cr :mr
Algorithm 1: Generating {t} with T .
Algorithm 1 shows how the learner uses T to
generate a packed chart representation of {t} in
the chart Ch. The function T massively overgen-
erates parses for any given natural language. The
probabilistic parsing model introduced in Sec-
tion 3 is used to choose the best parse from the
overgenerated set.
5 Training
5.1 Parameter Estimation
The probabilistic model of the grammar describes
a distribution over the observed training data X,
latent variables S, and parameters ?. The goal of
training is to estimate the posterior distribution:
p(S, ?|X) = p(S,X|?)p(?)
p(X)
(14)
which we do with online Variational Bayesian Ex-
pectation Maximisation (oVBEM; Sato (2001),
Hoffman et al(2010)). oVBEM is an online
238
Bayesian extension of the EM algorithm that
accumulates observation pseudocounts na?b for
each of the productions a ? b in the grammar.
These pseudocounts define the posterior over pro-
duction probabilities as follows:
(?a?b1 , . . . , ?a?b{k,... })) | X,S ? (15)
Dir(?H(b1) + na?b1 , . . . ,
??
j=k
?H(bj) + na?bj )
These pseudocounts are computed in two steps:
oVBE-step For the training pair (si, {m}i)
which supports the set of parses {t}, the expec-
tation E{t}[a ? b] of each production a ? b is
calculated by creating a packed chart representa-
tion of {t} and running the inside-outside algo-
rithm. This is similar to the E-step in standard
EM apart from the fact that each production is
scored with the current expectation of its parame-
ter weight ??i?1a?b, where:
??i?1a?b =
e?(?aHa(a?b)+n
i?1
a?b)
e
?
(?K
{b?} ?aHa(a?b
?)+ni?1
a?b?
) (16)
and ? is the digamma function (Beal, 2003).
oVBM-step The expectations from the oVBE
step are used to update the pseudocounts in Equa-
tion 15 as follows,
nia?b = n
i?1
a?b + ?i(N ? E{t}[a? b]? ni?1a?b)
(17)
where ?i is the learning rate and N is the size of
the dataset.
5.2 The Training Algorithm
Now the training algorithm used to learn the lex-
icon ? and pseudocounts {na?b} can be defined.
The algorithm, shown in Algorithm 2, passes over
the training data only once and one training in-
stance at a time. For each (si, {m}i) it uses the
function T |{m}i| times to generate a set of con-
sistent parses {t}?. The lexicon is populated by
using the lex function to read all of the lexical
items off from the derivations in each {t}?. In
the parameter update step, the training algorithm
updates the pseudocounts associated with each of
the productions a ? b that have ever been seen
during training according to Equation (17).
Only non-zero pseudocounts are stored in our
model. The count vector is expanded with a new
entry every time a new production is used. While
Input : Corpus D = {(si, {m}i)|i = 1, . . . , N},
Function T , Semantics to syntactic cate-
gory mapping cat, function lex to read
lexical items off derivations.
Output: Lexicon ?, Pseudocounts {na?b}.
? = {}, {t} = {}
for i = 1, . . . , N do
{t}i = {}
form? ? {m}i do
Cm? = cat(m?)
{t}? = T (si,Cm? :m?)
{t}i = {t}i ? {t}?, {t} = {t} ? {t}?
? = ? ? lex ({t}?)
for a? b ? {t} do
nia?b = n
i?1
a?b + ?i(N ? E{t}i [a? b]?
ni?1a?b)
Algorithm 2: Learning ? and {na?b}
the parameter update step cycles over all produc-
tions in {t} it is not neccessary to store {t}, just
the set of productions that it uses.
6 Experimental Setup
6.1 Data
The Eve corpus, collected by Brown (1973), con-
tains 14, 124 English utterances spoken to a sin-
gle child between the ages of 18 and 27 months.
These have been hand annotated by Sagae et al
(2004) with labelled syntactic dependency graphs.
An example annotation is shown in Figure 3.
While these annotations are designed to rep-
resent syntactic information, the parent-child re-
lationships in the parse can also be viewed as a
proxy for the predicate-argument structure of the
semantics. We developed a template based de-
terministic procedure for mapping this predicate-
argument structure onto logical expressions of the
type discussed in Section 2.1. For example, the
dependency graph in Figure 3 is automatically
transformed into the logical expression
?e.have(you,another(y, cookie(y)), e) (18)
? on(the(z, table(z)), e),
where e is a Davidsonian event variable used to
deal with adverbial and prepositional attachments.
The deterministic mapping to logical expressions
uses 19 templates, three of which are used in this
example: one for the verb and its arguments, one
for the prepositional attachment and one (used
twice) for the quantifier-noun constructions.
239
SUBJ ROOT DET OBJ JCT DET POBJ
pro|you v|have qn|another n|cookie prep|on det|the n|table
You have another cookie on the table
Figure 3: Syntactic dependency graph from Eve corpus.
This mapping from graph to logical expression
makes use of a predefined dictionary of allowed,
typed, logical constants. The mapping is success-
ful for 31% of the child-directed utterances in the
Eve corpus3. The remaining data is mostly ac-
counted for by one-word utterances that have no
straightforward interpretation in our typed logi-
cal language (e.g. what; okay; alright; no; yeah;
hmm; yes; uhhuh; mhm; thankyou), missing ver-
bal arguments that cannot be properly guessed
from the context (largely in imperative sentences
such as drink the water), and complex noun con-
structions that are hard to match with a small set
of templates (e.g. as top to a jar). We also re-
move the small number of utterances containing
more than 10 words for reasons of computational
efficiency (see discussion in Section 8).
Following Alishahi and Stevenson (2010), we
generate a context set {m}i for each utterance si
by pairing that utterance with its correct logical
expression along with the logical expressions of
the preceding and following (|{m}i|?1)/2 utter-
ances.
6.2 Base Distributions and Learning Rate
Each of the production heads a in the grammar
requires a base distribution Ha and concentration
parameter ?a. For word-productions the base dis-
tribution is a geometric distribution over character
strings and spaces. For syntactic-productions the
base distribution is defined in terms of the new
category to be named by cat and the probability
of splitting the rule by reversing either the appli-
cation or composition combinators.
Semantic-productions? base distributions are
defined by a probabilistic branching process con-
ditioned on the type of the syntactic category.
This distribution prefers less complex logical ex-
pressions. All concentration parameters are set to
1.0. The learning rate for parameter updates is
?i = (0.8 + i)?0.5.
3Data available at www.tomkwiat.com/resources.html
0.0 0.2 0.4 0.6 0.8 1.0Proportion of Data Seen0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Accu
racy
Our ApproachOur Approach + Guess UBL
1
UBL10
Figure 4: Meaning Prediction: Train on files 1, . . . , n
test on file n+ 1.
7 Experiments
7.1 Parsing Unseen Sentences
We test the parsing model that is learnt by training
on the first i files of the longitudinally ordered Eve
corpus and testing on file i + 1, for i = 1 . . . 19.
For each utterance s? in the test file we use the
parsing model to predict a meaning m? and com-
pare this to the target meaning m?. We report the
proportion of utterances for which the prediction
m? is returned correctly both with and without
word-meaning guessing. When a word has never
been seen at training time our parser has the abil-
ity to ?guess? a typed logical meaning with place-
holders for constant and predicate names.
For comparison we use the UBL semantic
parser of Kwiatkowski et al(2010) trained in
a similar setting?i.e., with no language specific
initialisation4. Figure 4 shows accuracy for our
approach with and without guessing, for UBL
4Kwiatkowski et al(2010) initialise lexical weights in
their learning algorithm using corpus-wide alignment statis-
tics across words and meaning elements. Instead we run
UBL with small positive weight for all lexical items. When
run with Giza++ parameter initialisations, UBL10 achieves
48.1% across folds compared to 49.2% for our approach.
240
when run over the training data once (UBL1) and
for UBL when run over the training data 10 times
(UBL10) as in Kwiatkowski et al(2010). Each
of the points represents accuracy on one of the
19 test files. All of these results are from parsers
trained on utterances paired with a single candi-
date meaning. The lines of best fit show the up-
ward trend in parser performance over time.
Despite only seeing each training instance
once, our approach, due to its broader lexi-
cal search strategy, outperforms both versions of
UBL which performs a greedy search in the space
of lexicons and requires initialisation with co-
occurence statistics between words and logical
constants to guide this search. These statistics are
not justified in a model of language acquisition
and so they are not used here. The low perfor-
mance of all systems is due largely to the sparsity
of the data with 32.9% of all sentences containing
a previously unseen word.
7.2 Word Learning
Due to the sparsity of the data, the training algo-
rithm needs to be able to learn word-meanings on
the basis of very few exposures. This is also a de-
sirable feature from the perspective of modelling
language acquisition as Carey and Bartlett (1978)
have shown that children have the ability to learn
word meanings on the basis of one, or very few,
exposures through the process of fast mapping.
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(m|w
) 1 Meaning
0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 2000Number of Utterances0.0
0.20.4
0.60.8
1.0
P(m|w
) 5 Meanings
0 500 1000 1500 2000Number of Utterances
7 Meanings
f = 168 a? ?f.a(x, f (x))f = 10 another? ?f.another(x, f (x))f = 2 any? ?f.any(x, f (x))
Figure 5: Learning quantifiers with frequency f.
Figure 5 shows the posterior probability of the
correct meanings for the quantifiers ?a?, ?another?
and ?any? over the course of training with 1, 3,
5 and 7 candidate meanings for each utterance5.
These three words are all of the same class but
have very different frequencies in the training
subset shown (168, 10 and 2 respectively). In all
training settings, the word ?a? is learnt gradually
from many observations but the rarer words ?an-
other? and ?any? are learnt (when they are learnt)
through large updates to the posterior on the ba-
sis of few observations. These large updates re-
sult from a syntactic bootstrapping effect (Gleit-
man, 1990). When the model has great confidence
about the derivation in which an unseen lexical
item occurs, the pseudocounts for that lexical item
get a large update under Equation 17. This large
update has a greater effect on rare words which
are associated with small amounts of probability
mass than it does on common ones that have al-
ready accumulated large pseudocounts. The fast
learning of rare words later in learning correlates
with observations of word learning in children.
7.3 Word Order Learning
Figure 6 shows the posterior probability of the
correct SVO word order learnt from increasing
amounts of training data. This is calculated by
summing over all lexical items containing transi-
tive verb semantics and sampling in the space of
parse trees that could have generated them. With
no propositional uncertainty in the training data
the correct word order is learnt very quickly and
stabilises. As the amount of propositional uncer-
tainty increases, the rate at which this rule is learnt
decreases. However, even in the face of ambigu-
ous training data, the model can learn the cor-
rect word-order rule. The distribution over word
orders also exhibits initial uncertainty, followed
by a sharp convergence to the correct analysis.
This ability to learn syntactic regularities abruptly
means that our system is not subject to the crit-
icisms that Thornton and Tesan (2007) levelled
at statistical models of language acquisition?that
their learning rates are too gradual.
5The term ?fast mapping? is generally used to refer to
noun learning. We chose to examine quantifier learning here
as there is a greater variation in quantifier frequencies. Fast
mapping of nouns is also achieved.
241
0 500 1000 1500 2000Number of Utterances
7 Meanings
0 500 1000 1500 2000Number of Utterances0.00.2
0.40.6
0.81.0
P(word
order)
5 Meanings 0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(word
order)
1 Meaning
vsosvo ovssov vososv
Figure 6: Learning SVO word order.
8 Discussion
We have presented an incremental model of lan-
guage acquisition that learns a probabilistic CCG
grammar from utterances paired with one or
more potential meanings. The model assumes
no language-specific knowledge, but does assume
that the learner has access to language-universal
correspondences between syntactic and semantic
types, as well as a Bayesian prior encouraging
grammars with heavy reuse of existing rules and
lexical items. We have shown that this model
not only outperforms a state-of-the-art semantic
parser, but also exhibits learning curves similar
to children?s: lexical items can be acquired on a
single exposure and word order is learnt suddenly
rather than gradually.
Although we use a Bayesian model, our ap-
proach is different from many of the Bayesian
models proposed in cognitive science and lan-
guage acquisition (Xu and Tenenbaum, 2007;
Goldwater et al 2009; Frank et al 2009; Grif-
fiths and Tenenbaum, 2006; Griffiths, 2005; Per-
fors et al 2011). These models are intended
as ideal observer analyses, demonstrating what
would be learned by a probabilistically optimal
learner. Our learner uses a more cognitively plau-
sible but approximate online learning algorithm.
In this way, it is similar to other cognitively plau-
sible approximate Bayesian learners (Pearl et al
2010; Sanborn et al 2010; Shi et al 2010).
Of course, despite the incremental nature of our
learning algorithm, there are still many aspects
that could be criticized as cognitively implausi-
ble. In particular, it generates all parses consistent
with each training instance, which can be both
memory- and processor-intensive. It is unlikely
that children do this once they have learnt at least
some of the target language. In future, we plan
to investigate more efficient parameter estimation
methods. One possibility would be an approxi-
mate oVBEM algorithm in which the expectations
in Equation 17 are calculated according to a high
probability subset of the parses {t}. Another op-
tion would be particle filtering, which has been
investigated as a cognitively plausible method for
approximate Bayesian inference (Shi et al 2010;
Levy et al 2009; Sanborn et al 2010).
As a crude approximation to the context in
which an utterance is heard, the logical represen-
tations of meaning that we present to the learner
are also open to criticism. However, Steedman
(2002) argues that children do have access to
structured meaning representations from a much
older apparatus used for planning actions and we
wish to eventually ground these in sensory input.
Despite the limitations listed above, our ap-
proach makes several important contributions to
the computational study of language acquisition.
It is the first model to learn syntax and seman-
tics concurrently; previous systems (Villavicen-
cio, 2002; Buttery, 2006) learnt categorial gram-
mars from sentences where all word meanings
were known. Our model is also the first to be
evaluated by parsing sentences onto their mean-
ings, in contrast to the work mentioned above and
that of Gibson and Wexler (1994), Siskind (1992)
Sakas and Fodor (2001), and Yang (2002). These
all evaluate their learners on the basis of a small
number of predefined syntactic parameters.
Finally, our work addresses a misunderstand-
ing about statistical learners?that their learn-
ing curves must be gradual (Thornton and Tesan,
2007). By demonstrating sudden learning of word
order and fast mapping, our model shows that sta-
tistical learners can account for sudden changes in
children?s grammars. In future, we hope to extend
these results by examining other learning behav-
iors and testing the model on other languages.
9 Acknowledgements
We thank Mark Johnson for suggesting an analy-
sis of learning rates. This work was funded by the
ERC Advanced Fellowship 24952 GramPlus and
EU IP grant EC-FP7-270273 Xperience.
242
References
Alishahi and Stevenson, S. (2008). A computa-
tional model for early argument structure ac-
quisition. Cognitive Science, 32:5:789?834.
Alishahi, A. and Stevenson, S. (2010). Learning
general properties of semantic roles from usage
data: a computational model. Language and
Cognitive Processes, 25:1.
Beal, M. J. (2003). Variational algorithms for ap-
proximate Bayesian inference. Technical re-
port, Gatsby Institute, UCL.
Bo?rschinger, B., Jones, B. K., and Johnson, M.
(2011). Reducing grounded learning tasks
to grammatical inference. In Proceedings of
the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1416?
1425, Edinburgh, Scotland, UK. Association
for Computational Linguistics.
Brown, R. (1973). A First Language: the Early
Stages. Harvard University Press, Cambridge
MA.
Buttery, P. J. (2006). Computational models for
first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Carey, S. and Bartlett, E. (1978). Acquring a sin-
gle new word. Papers and Reports on Child
Language Development, 15.
Chen, D. L., Kim, J., and Mooney, R. J. (2010).
Training a multilingual sportscaster: Using per-
ceptual context to learn language. J. Artif. In-
tell. Res. (JAIR), 37:397?435.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A probabilistic computational model of cross-
situational word learning. Cognitive Science,
34(6):1017?1063.
Frank, M., Goodman, S., and Tenenbaum, J.
(2009). Using speakers referential intentions
to model early cross-situational word learning.
Psychological Science, 20(5):578?585.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2008). A bayesian framework for cross-
situational word-learning. Advances in Neural
Information Processing Systems 20.
Gibson, E. and Wexler, K. (1994). Triggers. Lin-
guistic Inquiry, 25:355?407.
Gleitman, L. (1990). The structural sources of
verb meanings. Language Acquisition, 1:1?55.
Goldwater, S., Griffiths, T. L., and Johnson, M.
(2009). A Bayesian framework for word seg-
mentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Griffiths, T. L., . T. J. B. (2005). Structure and
strength in causal induction. Cognitive Psy-
chology, 51:354?384.
Griffiths, T. L. and Tenenbaum, J. B. (2006). Op-
timal predictions in everyday cognition. Psy-
chological Science.
Hoffman, M., Blei, D. M., and Bach, F. (2010).
Online learning for latent dirichlet alcation.
In NIPS.
Kate, R. J. and Mooney, R. J. (2007). Learning
language semantics from ambiguous supervi-
sion. In Proceedings of the 22nd Conference
on Artificial Intelligence (AAAI-07).
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing proba-
bilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the
Conference on Emperical Methods in Natural
Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical general-
ization in ccg grammar induction for semantic
parsing. In Proceedings of the Conference on
Emperical Methods in Natural Language Pro-
cessing.
Levy, R., Reali, F., and Griffiths, T. (2009). Mod-
eling the effects of memory on human online
sentence processing with particle filters. In Ad-
vances in Neural Information Processing Sys-
tems 21.
Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer,
L. S. (2008). A generative model for parsing
natural language to meaning representations. In
Proceedings of The Conference on Empirical
Methods in Natural Language Processing.
MacWhinney, B. (2000). The CHILDES project:
tools for analyzing talk. Lawrence Erlbaum,
Mahwah, NJ u.a. EN.
Maurits, L., Perfors, A., and Navarro, D. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31th Annual Con-
ference of the Cognitive Science Society.
Pearl, L., Goldwater, S., and Steyvers, M. (2010).
How ideal are we? Incorporating human limi-
243
tations into Bayesian models of word segmen-
tation. pages 315?326, Somerville, MA. Cas-
cadilla Press.
Perfors, A., Tenenbaum, J. B., and Regier, T.
(2011). The learnability of abstract syntactic
principles. Cognition, 118(3):306 ? 338.
Sagae, K., MacWhinney, B., and Lavie, A.
(2004). Adding syntactic annotations to tran-
scripts of parent-child dialogs. In Proceed-
ings of the 4th International Conference on
Language Resources and Evaluation. Lisbon,
LREC.
Sakas, W. and Fodor, J. D. (2001). The struc-
tural triggers learner. In Bertolo, S., editor,
Language Acquisition and Learnability, pages
172?233. Cambridge University Press, Cam-
bridge.
Sanborn, A. N., Griffiths, T. L., and Navarro,
D. J. (2010). Rational approximations to ratio-
nal models: Alternative algorithms for category
learning. Psychological Review.
Sato, M. (2001). Online model selection based
on the variational bayes. Neural Computation,
13(7):1649?1681.
Shi, L., Griffiths, T. L., Feldman, N. H., and San-
born, A. N. (2010). Exemplar models as a
mechanism for performing bayesian inference.
Psychonomic Bulletin & Review, 17(4):443?
464.
Siskind, J. M. (1992). Naive Physics, Event Per-
ception, Lexical Semantics, and Language Ac-
quisition. PhD thesis, Massachusetts Institute
of Technology.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):1?
38.
Steedman, M. (2000). The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, M. (2002). Plans, affordances, and
combinatory grammar. Linguistics and Philos-
ophy, 25.
Thornton, R. and Tesan, G. (2007). Categori-
cal acquisition: Parameter setting in universal
grammar. Biolinguistics, 1.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial gram-
mar. Technical Report UCAM-CL-TR-533,
University of Cambridge, Computer Labora-
tory.
Wong, Y. W. and Mooney, R. (2006). Learning for
semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language
Technology Conference of the NAACL.
Wong, Y. W. and Mooney, R. (2007). Learn-
ing synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of
the Association for Computational Linguistics.
Xu, F. and Tenenbaum, J. B. (2007). Word learn-
ing as Bayesian inference. Psychological Re-
view, 114:245?272.
Yang, C. (2002). Knowledge and Learning in Nat-
ural Language. Oxford University Press, Ox-
ford.
Yu, C. and Ballard, D. H. (2007). A unified model
of early word learning: Integrating statisti-
cal and social cues. Neurocomputing, 70(13-
15):2149 ? 2165.
Zettlemoyer, L. S. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence.
Zettlemoyer, L. S. and Collins, M. (2007). Online
learning of relaxed CCG grammars for pars-
ing to logical form. In Proc. of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning.
Zettlemoyer, L. S. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of The
Joint Conference of the Association for Com-
putational Linguistics and International Joint
Conference on Natural Language Processing.
244
