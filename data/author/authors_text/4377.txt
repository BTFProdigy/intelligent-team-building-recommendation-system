Proceedings of NAACL-HLT 2013, pages 550?555,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Derivation Trees for Informative Treebank Inter-Annotator
Agreement Evaluation
Seth Kulick and Ann Bies and Justin Mott
and Mohamed Maamouri
Linguistic Data Consortium
University of Pennsylvania
{skulick,bies,jmott,maamouri}
@ldc.upenn.edu
Beatrice Santorini and
Anthony Kroch
Department of Linguistics
University of Pennsylvania
{beatrice,kroch}
@ling.upenn.edu
Abstract
This paper discusses the extension of a sys-
tem developed for automatic discovery of tree-
bank annotation inconsistencies over an entire
corpus to the particular case of evaluation of
inter-annotator agreement. This system makes
for a more informative IAA evaluation than
other systems because it pinpoints the incon-
sistencies and groups them by their structural
types. We evaluate the system on two corpora
- (1) a corpus of English web text, and (2) a
corpus of Modern British English.
1 Introduction
This paper discusses the extension of a system de-
veloped for automatic discovery of treebank annota-
tion inconsistencies over an entire corpus to the par-
ticular case of evaluation of inter-annotator agree-
ment (IAA). In IAA, two or more annotators anno-
tate the same sentences, and a comparison identi-
fies areas in which the annotators might need more
training, or the annotation guidelines some refine-
ment. Unlike other IAA evaluation systems, this
system application results in a precise pinpointing of
inconsistencies and the grouping of inconsistencies
by their structural types, making for a more infor-
mative IAA evaluation.
Treebank annotation, consisting of syntactic
structure with words as the terminals, is by its na-
ture more complex and therefore more prone to error
than many other annotation tasks. However, high an-
notation consistency is crucial to providing reliable
training and testing data for parsers and linguistic
research. Error detection is therefore an important
area of research, and the importance of work such as
Dickinson and Meurers (2003) is that errors and an-
notation inconsistencies might be automatically dis-
covered, and once discovered, be targeted for subse-
quent quality control.
A recent approach to this problem (Kulick et al,
2011; Kulick et al, 2012) (which we will call the
KBM system) improves upon Dickinson and Meur-
ers (2003) by decomposing the full syntactic tree
into smaller units, using ideas from Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997). This al-
lows the comparison to be based on small syntactic
units instead of string n-grams, improving the detec-
tion of inconsistent annotation.
The KBM system, like that of Dickinson and
Meurers (2003) before it, is based on the notion of
comparing identical strings. In the general case, this
is a problematic assumption, since annotation in-
consistencies are missed because of superficial word
differences between strings which one would want
to compare.1 However, this limitation is not present
for IAA evaluation, since the strings to compare are,
by definition, identical.2 The same is also true of
parser evaluation, since the parser output and the
gold standard are based on the same sentences.
We therefore take the logical step of applying the
KBM system developed for automatic discovery of
annotation inconsistency to the special case of IAA.3
1Boyd et al (2007) and other current work tackles this prob-
lem. However, that is not the focus of this paper.
2Aside from possible tokenization differences by annotators.
3In this paper, we do not yet apply the system to parser eval-
uation, although it is conceptually the same problem as IAA
evaluation. We wanted to first refine the system using annota-
tor input for the IAA application before applying it to parser
550
(1) a. NP-SBJ
NP
NP
The word
NP
renaissance
NP
-LRB- NP
Rinascimento
PP
in NP
Italian
-RRB-
b. NP-SBJ
The word renaissance PRN
-LRB- FRAG
NP
Rinascimento
PP
in NP
Italian
-RRB-
Figure 1: Two example trees showing a difference in IAA
To our knowledge, this work is the first to utilize
such a general system for this special case.
The advantages of the KBM system play out
somewhat differently in the context of IAA evalu-
ation than in the more general case. In this con-
text, the comparison of word sequences based on
syntactic units allows for a precise pinpointing of
differences. The system also retains the ability to
group inconsistencies together by their structural
type, which we have found to be useful for the more
general case. Together, these two properties make
for a useful and informative system for IAA evalua-
tion.
In Section 2 we describe the basic working of our
system. In Section 3 we discuss in more detail the
advantages of this approach. In Section 4 we evalu-
ate the system on two treebanks, a corpus of English
web text and a corpus of Modern British English.
Section 5 discusses future work.
2 System Overview
The basic idea of the KBM system is to detect word
sequences that are annotated in inconsistent ways by
evaluation.
wordNP
The RinascimentoNP -RRB--LRB-renaissance
NP
renaissanceNPb1The -RRB--LRB- Rinascimento
FRAGword PRN
a1
a2
a3
a4
a5
a8
b2
b3 b5
b4 b8
inPPa6 ItalianNP a7
inPP b6 ItalianNPb7
NP
A
A
A A A A
A
A
MM MM M
A
(2) a.
b.
Figure 2: E-trees and derivation trees corresponding to
(1ab)
comparing local syntactic units. Following Dickin-
son and Meurers (2003), we refer to sequences ex-
amined for inconsistent annotation as nuclei. The
sentence excerpts (1ab) in Figure 1, from the test
corpora used in this work, illustrate an inconsistency
in the annotation of corresponding strings. We fo-
cus here on the difference in the annotation of the
nucleus The word renaissance, which in (1a) is an-
notated as an appositive structure, while in (1b) it is
flat.
Following the TAG approach, KBM decomposes
the full phrase structure into smaller chunks called
elementary trees (henceforth, e-trees). The relation-
ship of the e-trees underlying a full phrase struc-
ture to each other is recorded in a derivation tree,
in which each node is an e-tree, related to its par-
ent node by a composition operation, as shown in
(2ab).4
KBM uses two composition operations, each with
left and right variants, shown in Figure 3: (1) ad-
4The decomposition is based on head-finding heuristics,
with the result here that word is the head of (1a), while renais-
sance is the head of (1b), as reflected in their respective deriva-
tion trees (2a) and (2b). We omit the POS tags in (1ab) and
(2ab) to avoid clutter.
551
wo ro wo wo ro
dNPTheRinaschNmswot wo
-BLhebN1hBFeRinaschNms
ro t ro
-BLheRinaschNmswot wo
dNPThebN1hBFeRinaschNmsro
tro
woro ro wo wo
Figure 3: Composition operations (left and right)
junction, which attaches one tree to a target node in
another tree by creating a copy of the target node,
and (2) sister adjunction, which attaches one tree as
a sister to a target node in another tree. Each arc in
Figure 2 is labeled by an ?M? for adjunction and ?A?
for sister-adjunction. 5
The system uses the tree decomposition and re-
sulting derivation tree for the comparison of differ-
ent instances of the same nucleus. The full deriva-
tion tree for a sentence is not used, but rather only
that slice of it having e-trees with words that are in
the nucleus being examined, which we call a deriva-
tion tree fragment. That is, for a given nucleus with
a set of instances, we compare the derivation frag-
ments for each instance.
For example, for the nucleus The word renais-
sance, the derivation tree fragment for the instance
in (1a) consists of the e-trees a1, a2, a3 (and their
arcs) in (2a), and likewise the derivation tree from
the instance in (1b) consists of the e-trees b1, b2, b3
in (2b). These derivation fragments have a differ-
ent structure, and so the two instances of The word
renaissance are recognized as inconsistent.
Two important aspects of the overall system re-
quire mention here: (1) Nuclei are identified by us-
ing sequences that occur as a constituent anywhere
5KBM is based on a variant of Spinal TAG (Shen et al,
2008), and uses sister adjunction without substitution. Space
prohibits full discussion, but multiple adjunction to a single
node (e.g., a4, a6, a8 to a5 in (2a)) does not create multiple
levels of recursion, while a special specification handles the ex-
tra NP recursion for the apposition with a2, a3, and a5. For
reasons of space, we also leave aside a precise comparison to
Tree Insertion Grammar (Chiang, 2003) and Spinal TAG (Shen
et al, 2008).
in the corpus, even if other instances of the same
sequence are not constituents. Both instances of
The word renaissance are compared, because the
sequence occurs at least once as a constituent. (2)
We partition each comparison of the instances of a
nucleus by the lowest nonterminal in the derivation
tree fragment that covers the sequence. The two in-
stances of The word renaissance are compared be-
cause the lowest nonterminal is an NP in both in-
stances.
3 Advantages of this approach
As Kulick et al (2012) stressed, using derivation
tree fragments allows the comparison to abstract
away from interference by irrelevant modifiers, an
issue with Dickinson and Meurers (2003). However,
in the context of IAA, this advantage of KBM plays
out in a different way, in that it allows for a pre-
cise pinpointing of the inconsistencies. For IAA,
the concern is not whether an inconsistent annota-
tion will be reported, since at some level higher in
the tree every difference will be found, even if the
context is the entire tree. KBM, however, will find
the inconsistencies in a more informative way, for
example reporting just The word renaissance, not
some larger unit. Likewise, it reports Rinascimento
in Italian as an inconsistently annotated sequence.6
A critical desirable property of KBM that carries
over from the more general case is that it allows for
different nuclei to be grouped together in the sys-
tem?s output if they have the same annotation in-
consistency type. As in Kulick et al (2011), each
nucleus found to be inconsistent is categorized by
an inconsistency type, which is simply the collec-
tion of different derivation tree fragments used for
the comparison of its instances, including POS tags
but not the words. For example, the inconsistency
type of the nucleus The word renaissance in (1ab) is
the pair of derivation tree fragments (a1,a2,a3) and
(b1,b2,b3) from (2ab), with the POS tags. This nu-
6Note however that it does not report -LRB- Rinascimento
in Italian -RRB- which is also a constituent, and so might be
expected to be compared. The lowest nonterminal above this
substring in the two derivation trees in Figure 2 is the NP in a5
and the FRAG in b5, thus exempting them from comparison. It
is exactly this sort of case that motivated the ?external check?
discussed in Kulick et al (2012), which we have not yet imple-
mented for IAA.
552
Inconsistency type # Found # Accurate
Function tags only 53 53
POS tags only 18 13
Structural 129 122
Table 1: Inconsistency types found for system evaluation
cleus is then reported together with other nuclei that
use the same derivation fragments. In this case, it
therefore also reports the nucleus The term renais-
sance, which appears elsewhere in the corpus with
the two annotations from the different annotators as
in (3):
(3) a. NP
NP
The term
NP
renaissance
b. NP
The term renaissance
KBM reports The word renaissance and The term
renaissance together because they are inconsistently
annotated in exactly the same way, in spite of the dif-
ference in words. This grouping together of incon-
sistencies based on structural characteristics of the
inconsistency is critically important for understand-
ing the nature of the annotation inconsistencies.
It is the combination of these two characteristics -
(1) pinpointing of errors and (2) grouping by struc-
ture - that makes the system so useful for IAA. This
is an improvement over alternatives such as using
evalb (Sekine and Collins, 2008) for IAA. No other
system to our knowledge groups inconsistencies by
structural type, as KBM does. The use of the deriva-
tion tree fragments greatly lessens the multiple re-
porting of a single annotation difference, which is
a difficulty for using evalb (Manning and Schuetze,
1999, p. 436) or Dickinson and Meurers (2003).
4 Evaluation
4.1 English web text
We applied our approach to pre-release subset of
(Bies et al, 2012), dually annotated and used for
annotator training, from which the examples in Sec-
tions 2 and 3 are taken. It is a small section of the
corpus, with 4,270 words dually annotated.
For this work, we also took the further step of
characterizing the inconsistency types themselves,
allowing for an even higher-level view of the incon-
sistencies found. In addition to grouping together
different strings as having the same inconsistent an-
notation, the types can also be grouped together for
comparison at a higher level. For this IAA sample,
we separated the inconsistency types into the three
groups in Table 1, with the derivation tree fragments
differing (1) only on function tags, (2) only on POS
tags7, and (3) on structural differences. We man-
ually examined each inconsistency group to deter-
mine if it was an actual inconsistency found, or a
spurious false positive. As shown in Table 1, the pre-
cision of the reported inconsistencies is very high.
It is in fact even higher than it appears, because
the seven (out of 129) instances incorrectly listed
as structural problems were actually either POS or
function tag inconsistencies, that were discovered
by the system only by a difference in the derivation
tree fragment, and so were categorized as structural
problems instead of POS or function tag inconsis-
tencies. 8
Because of the small size of the corpus, there
are relatively few nuclei grouped into inconsistency
types. The 129 structural inconsistency types in-
clude 130 nuclei, with the only inconsistency type
with more than one nucleus being the type with The
word renaissance and The term renaissance, as dis-
cussed above. There is more grouping together in
the ?POS tags only? case (37 nuclei included in
the 18 inconsistency types), and the ?function tags
only? case (56 nuclei included in the 53 inconsis-
tency types).
4.2 Modern British English corpus
We also applied our approach to a supplemental sec-
tion (Kroch and Santorini, in preparation) to a cor-
pus of modern British English (Kroch et al, 2010),
part of a series of corpora used for research into lan-
guage change. The annotation style is similar to that
of the Penn Treebank, although with some differ-
ences. In this case, because neither the function tags
nor part-of-speech tags were part of the IAA work,
7As mentioned in footnote 4, although POS tags were left
out of Figure 2 for readability, they are included in the actual e-
trees. This allows POS differences in a similar syntactic context
to be naturally captured within the overall KBM framework.
8A small percentage of inconsistencies are the result of lin-
guistic ambiguities and not an error by one of the annotators.
553
we do not separate out the inconsistency types, as
done in Section 4.1.
The supplement section consisted of 82,701
words dually annotated. The larger size, as com-
pared with the corpus in Section 4.1, results in some
differences in the system output. Because of the
larger size, there are more substantial cases of dif-
ferent nuclei grouped together as the same inconsis-
tency type than in Section 4.1. The first inconsis-
tency type (sorted by number of nuclei) has 88 nu-
clei, and the second has 37 nuclei. In total, there are
1,532 inconsistency types found, consisting of 2,194
nuclei in total. We manually examined the first 20
inconsistency types (sorted by number of nuclei),
consisting in total of 375 nuclei. All were found to
be true instances of inconsistent annotation.
(4) a. NP
the ADJP
only true
thing
b. NP
the only true thing
(5) a. NP
their ADJP
only actual
argument
b. NP
their only actual argument
The trees in (4) and (5) show two of the 88 nu-
clei grouped into the first inconsistency type. As
with The word renaissance and The term renais-
sance in the English web corpus, nuclei with similar
(although not identical) words are often grouped into
the same inconsistency type. To repeat the point,
this is not because of any search for similarity of
the words in the nuclei. It arises from the fact that
the nuclei are annotated inconstantly in the same
way. Of course not all nuclei in an inconsistency
type have the same words. Nuclei found in this in-
consistency type include only true and only actual
as shown above, and also nuclei such as new En-
glish, greatest possible, thin square, only necessary.
Taken together, they clearly indicate an issue with
the annotation of multi-word adjective phrases.9
9Note that the inconsistencies discussed throughout this pa-
per are not taken from the the published corpora. These results
are only from internal annotator training files.
5 Future work
There are several ways in which we plan to improve
the current approach. As mentioned above, there is
a certain class of inconsistencies which KBM will
not pinpoint precisely, which requires adopting the
?external check? from Kulick et al (2012). The ab-
straction on inconsistency types described in Sec-
tion 4 can also be taken further. For example, one
might want to examine in particular inconsistency
types that arise from PP attachment or that have to
do with the PRN function tag.
One main area for future work is the application
of this work to parser evaluation as well as IAA. For
this area, there is some connection to the work of
Goldberg and Elhadad (2010) and Dickinson (2010),
which are both concerned with examining depen-
dency structures of more than one edge. The con-
nection is that those works are focused on depen-
dency representations, and ithe KBM system does
phrase structure analysis using a TAG-like deriva-
tion tree, which strongly resembles a dependency
tree (Rambow and Joshi, 1997). There is much in
this area of common concern that is worth examin-
ing further.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-11-C-0145.
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred. This applies to the
first four authors. The first, fifth, and sixth authors
were supported in part by National Science Foun-
dation Grant # BCS-114749. We would also like
to thank Colin Warner, Aravind Joshi, Mitch Mar-
cus, and the computational linguistics group at the
University of Pennsylvania for helpful conversations
and feedback.
554
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 729?738,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Inspecting
the structural biases of dependency parsing algorithms.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 234?
242, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Anthony Kroch and Beatrice Santorini. in preparation.
Supplement to the Penn Parsed Corpus of Modern
British English.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Mod-
ern British English. http://www.ling.upenn.edu/hist-
corpora/PPCMBE-RELEASE-1/index.html.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 693?698, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Further
developments in treebank error detection using deriva-
tion trees. In LREC 2012: 8th International Confer-
ence on Language Resources and Evaluation, Istanbul.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new re-
source for incremental, dependency and semantic pars-
ing. Language Resources and Evaluation, 42(1):1?19.
555
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662?667,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
The Penn Parsed Corpus of Modern British English:
First Parsing Results and Analysis
Seth Kulick
Linguistic Data Consortium
University of Pennsylvania
skulick@ldc.upenn.edu
Anthony Kroch and Beatrice Santorini
Dept. of Linguistics
University of Pennsylvania
{kroch,beatrice}@ling.upenn.edu
Abstract
This paper presents the first results on
parsing the Penn Parsed Corpus of Mod-
ern British English (PPCMBE), a million-
word historical treebank with an annota-
tion style similar to that of the Penn Tree-
bank (PTB). We describe key features of
the PPCMBE annotation style that differ
from the PTB, and present some exper-
iments with tree transformations to bet-
ter compare the results to the PTB. First
steps in parser analysis focus on problem-
atic structures created by the parser.
1 Introduction
We present the first parsing results for the
Penn Parsed Corpus of Modern British English
(PPCMBE) (Kroch et al, 2010), showing that it
can be parsed at a few points lower in F-score than
the Penn Treebank (PTB) (Marcus et al, 1999).
We discuss some of the differences in annotation
style and source material that make a direct com-
parison problematic. Some first steps at analysis
of the parsing results indicate aspects of the anno-
tation style that are difficult for the parser, and also
show that the parser is creating structures that are
not present in the training material.
The PPCMBE is a million-word treebank cre-
ated for researching changes in English syntax. It
covers the years 1700-1914 and is the most mod-
ern in the series of treebanks created for histori-
cal research.
1
Due to the historical nature of the
PPCMBE, it shares some of the characteristics of
treebanks based on modern unedited text (Bies et
al., 2012), such as spelling variation.
1
The other treebanks in the series cover Early Modern En-
glish (Kroch et al, 2004) (1.8 million words), Middle Eng-
lish (Kroch and Taylor, 2000) (1.2 million words), and Early
English Correspondence (Taylor et al, 2006) (2.2 million
words).
The size of the PPCMBE is roughly the same
as the WSJ section of the PTB, and its annotation
style is similar to that of the PTB, but with dif-
ferences, particularly with regard to coordination
and NP structure. However, except for Lin et al
(2012), we have found no discussion of this corpus
in the literature.
2
There is also much additional
material annotated in this style, increasing the im-
portance of analyzing parser performance on this
annotation style.
3
2 Corpus description
The PPCMBE
4
consists of 101 files, but we leave
aside 7 files that consist of legal material with very
different properties than the rest of the corpus.
The remaining 94 files contain 1,018,736 tokens
(words).
2.1 Part-of-speech tags
The PPCMBE uses a part-of-speech (POS) tag set
containing 248 POS tags, in contrast to the 45 tags
used by the PTB. The more complex tag set is
mainly due to the desire to tag orthographic vari-
ants consistently throughout the series of historical
corpora. For example ?gentlemen? and its ortho-
graphic variant ?gen?l?men? are tagged with the
complex tag ADJ+NS (adjective and plural noun)
on the grounds that in earlier time periods, the lex-
ical item is spelled and tagged as two orthographic
words (?gentle?/ADJ and ?men?/NS).
While only 81 of the 248 tags are ?simple? (i.e.,
not associated with lexical merging or splitting),
2
Lin et al (2012) report some results on POS tagging us-
ing their own mapping to different tags, but no parsing results.
3
Aside from the corpora listed in fn. 1, there are also
historical corpora of Old English (Taylor et al, 2003), Ice-
landic (Wallenberg et al, 2011), French (Martineau and oth-
ers, 2009), and Portuguese (Galves and Faria, 2010), totaling
4.5 million words.
4
We are working with a pre-release copy of the next re-
vision of the official version. Some annotation errors in the
currently available version have been corrected, but the dif-
ferences are relatively minor.
662
Type # Tags # Tokens % coverage
Simple 81 1,005,243 98.7%
Complex 167 13,493 1.3%
Total 248 1,018,736 100.0%
Table 1: Distribution of POS tags. Complex tags
indicate lexical merging or splitting.
(1) (a) NP
NP
a Ham
CONJP
and NP
a Hare
(b) NP
NP
a Ham
and NP
a Hare
Figure 1: Coordination in the PPCMBE (1a) and
the PTB (1b).
they cover the vast majority of the words in the
corpus, as summarized in Table 1. Of these 81
tags, some are more specialized than in the PTB,
accounting for the increased number of tags com-
pared to the PTB. For instance, for historical con-
sistency, words like ?one? and ?else? each have
their own tag.
2.2 Syntactic annotation
As mentioned above, the syntactic annotation
guidelines do not differ radically from those of the
PTB. There are some important differences, how-
ever, which we highlight in the following three
subsections.
2.2.1 Coordination
A coordinating conjunction and conjunct form a
CONJP, as shown in (1a) in Figure 1. (1b) shows
the corresponding annotation in the PTB.
In a conjoined NP, if part of a first conjunct
potentially scopes over two or more conjuncts
(shared pre-modifiers), the first conjunct has no
phrasal node in the PPCMBE, and the label of the
(2) (a) NP
their husbands CONJP
or NX
fathers
(b) NP
their husbands or fathers
Figure 2: (2a) is an example of coordination with
a shared pre-modifier in the PPCMBE, and (2b)
shows the corresponding annotation in the PTB.
(3) (a) NP
The back PP
of this Spider
(b) NP
NP
a teacher
PP
of chemistry
(4) (a) NP
The Spiders CP-REL
which have..
(b) NP
a conviction CP-THT
that..
Figure 3: (3a) shows that a PP is sister to the
noun in the PPCMBE, in contrast to the adjunction
structure in the PTB (3b). (4ab) show that clausal
complements and modifiers of a noun are distin-
guished by function tags, rather than structurally
as in the PTB, which would adjoin the CP in (a),
but not in (b).
subsequent conjuncts becomes NX instead of NP,
as shown in (2a) in Figure 2. The corresponding
PTB annotation is flat, as in (2b).
5
2.2.2 Noun Phrase structure
Neither the PPCMBE nor the PTB distinguish be-
tween PP complements and modifiers of nouns.
However, the PPCMBE annotates both types of
dependents as sisters of the noun, while the PTB
adjoins both types. For instance in (3a) in Fig-
ure 3, the modifier PP is a sister to the noun in
the PPCMBE, while in (3b), the complement PP
is adjoined in the PTB.
Clausal complements and modifiers are also
both treated as sisters to the noun in the PPCMBE.
In this case, though, the complement/modifier dis-
tinction is encoded by a function tag. For exam-
ple, in (4a) and (4b), the status of the CPs as mod-
ifier and complement is indicated by their func-
tion tags: REL for relative clause and THT ?that?
complement. In the PTB, the distinction would be
encoded structurally; the relative clause would be
adjoined, whereas the ?that? complement would
not.
2.2.3 Clausal structure
The major difference in the clausal structure as
compared to the PTB is the absence of a VP level
6
,
yielding flatter trees than in the PTB. An example
clause is shown in (5) in Figure 4.
5
Similar coordination structures exist for categories other
than NP, although NP is by far the most common.
6
This is due to the changing headedness of VP in the over-
all series of English historical corpora.
663
(5) IP
NP-SBJ
The poor fellow
was shot PP
with NP
three Arrows
Figure 4: An example of clausal structure, without
VP.
(6) (a) NP
NP
The back
PP
of this Spider
(b)NP
NP
The Spiders
CP-REL
which have..
Figure 5: (6a) shows how (3a) is transformed in
the ?reduced +NPs? version to include a level of
NP recursion, and (6b) shows the same for (4a).
3 Corpus transformations
We refer to the pre-release version of the corpus
described in Section 2 as the ?Release? version,
and experiment with three other corpus versions.
3.1 Reduced
As mentioned earlier, the PPCMBE?s relatively
large POS tag set aims to maximize annotation
consistency across the entire time period covered
by the historical corpora, beginning with Middle
English. Since we are concerned here with pars-
ing just the PPCMBE, we simplified the tag set.
The complex tags are simplified in a fully deter-
ministic way, based on the trees and the tags. For
example, the POS tag for ?gentleman?, originally
ADJ+N is changed to N. The P tag is split, so that
it is either left as P, if a preposition, or changed
to CONJS, if a subordinating conjunction. The re-
duced tag set contains 76 tags. We call the version
of the corpus with the reduced tag set the ?Re-
duced? version.
3.2 Reduced+NPs
As discussed in Section 2.2.2, noun modifiers are
sisters to the noun, instead of being adjoined, as in
the PTB. As a result, there are fewer NP brackets
in the PPCMBE than there would be if the PTB-
style were followed. To evaluate the effect of the
difference in annotation guidelines on the parsing
score, we added PTB-style NP brackets to the re-
duced corpus described in Section 3.1. For ex-
ample, (3a) in Figure 3 is transformed into (6a)
Section # Files Token count %
Train 81 890,150 87.4%
Val 4 38,670 3.8%
Dev 4 39,527 3.9%
Test 5 50,389 4.9%
Total 94 1,018,736 100.0%
Table 2: Token count and data split for PPCMBE
in Figure 5, and likewise (4a) is transformed into
(6b). However, (4b) remains as it is, because the
following CP in that case is a complement, as in-
dicated by the THT function tag. This is a signif-
icant transformation of the corpus, adding 43,884
NPs to the already-existing 291,422.
3.3 Reduced+NPs+VPs
We carry out a similar transformation to add VP
nodes to the IPs in the Reduced+NPs version,
making them more like the clausal structures in
the PTB. This added 169,877 VP nodes to the cor-
pus (there are 131,671 IP nodes, some of which
contain more than one auxiliary verb).
It is worth emphasizing that the brackets added
in Sections 3.2 and 3.3 add no information, since
they are added automatically. They are added only
to roughly compensate for the difference in anno-
tation styles between the PPCMBE and the PTB.
4 Data split
We split the data into four sections, as shown in
Table 2. The validation section consists of the four
files beginning with ?a? or ?v? (spanning the years
1711-1860), the development section consists of
the four files beginning with ?l? (1753-1866), the
test section consists of the five files beginning with
?f? (1749-1900), and the training section consists
of the remaining 81 files (1712-1913). The data
split sizes used here for the PPCMBE closely ap-
proximate that used for the PTB, as described in
Petrov et al (2006).
7
For this first work, we used
a split that was roughly the same as far as time-
spans across the four sections. In future work, we
will do a more proper cross-validation evaluation.
Table 3 shows the average sentence length and
percentage of sentences of length <= 40 in the
PPCMBE and PTB. The PPCMBE sentences are
a bit longer on average, and fewer are of length
<= 40. However, the match is close enough that
7
Sections 2-21 for Training Section 1 for Val, 22 for Dev
and 23 for Test.
664
Gold Tags Parser Tags
all <=40 all <=40
Corpus Prec Rec F Prec Rec F Prec Rec F Prec Rec F Tags
1 Rl/Dev 83.7 83.7 83.7 86.3 86.4 86.3 83.8 83.1 83.4 86.2 85.8 86.0 96.9
2 Rd/Dev 84.9 84.5 84.7 86.6 86.7 86.7 84.5 83.7 84.1 86.5 86.2 86.3 96.9
3 Rd/Tst 85.8 85.2 85.5 87.9 87.3 87.6 84.8 83.9 84.3 86.7 85.8 86.2 97.1
4 RdNPs/Dev 87.1 86.3 86.7 88.9 88.5 88.7 86.3 85.1 85.7 88.4 87.6 88.0 96.9
5 RdNPsVPs/Dev 87.2 87.0 87.1 89.5 89.4 89.5 86.3 85.7 86.0 88.6 88.2 88.4 97.0
6 PTB/23 90.3 89.8 90.1 90.9 90.4 90.6 90.0 89.5 89.8 90.6 90.1 90.3 96.9
Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),
Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced
to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,
the tagging accuracy is shown in the last column.
Corpus Section Avg. len % <= 40
PPCMBE Dev 24.1 85.5
Test 21.2 89.9
PTB Dev 23.6 92.9
Test 23.5 91.3
Table 3: Average sentence length and percentage
of sentences of length <=40 in the PPCMBE and
PTB.
we will report the parsing results for sentences of
length <= 40 and all sentences, as with the PTB.
5 Parsing Experiments
The PPCMBE is a phrase-structure corpus, and so
we parse with the Berkeley parser (Petrov et al,
2008) and score using the standard evalb program
(Sekine and Collins, 2008). We used the Train and
Val sections for training, with the parser using the
Val section for fine-tuning parameters (Petrov et
al., 2006). Since the Berkeley parser is capable
of doing its own POS tagging, we ran it using the
gold tags or supplying its own tags. Table 4 shows
the results for both modes.
8
Consider first the results for the Dev section
with the parser using the gold tags. The score
for all sentences increases from 83.7 for the Re-
lease corpus (row 1) to 84.7 for the Reduced cor-
pus (row 2), reflecting the POS tag simplifications
in the Reduced corpus. The score goes up by a fur-
ther 2.0 to 86.7 (row 2 to 4) for the Reduced+NPs
corpus and up again by 0.4 to 87.1 (row 5) for
the Reduced+NPs+VPs corpus, showing the ef-
8
We modified the evalb parameter file to exclude punctu-
ation in PPCMBE, just as for PTB. The results are based on a
single run for each corpus/section. We expect some variance
to occur, and in future work will average results over several
runs of the training/Dev cycle, following Petrov et al (2006).
fects of the extra NP and VP brackets. We evalu-
ated the Test section on the Reduced corpus (row
3), with a result 0.8 higher than the Dev (85.5 in
row 3 compared to 84.7 in row 2). The score for
sentences of length <= 40 (a larger percentage
of the PPCMBE than the PTB) is 2.4 higher than
the score for all sentences, with both the gold and
parser tags (row 5).
The results with the parser choosing its own
POS tags naturally go down, with the Test section
suffering more. In general, the PPCMBE is af-
fected by the lack of gold tags more than the PTB.
In sum, the parser results show that the
PPCMBE can be parsed at a level approaching that
of the PTB. We are not proposing that the current
version be replaced by the Reduced+NPs+VPs
version, on the grounds that the latter gets the
highest score. Our goal was to determine whether
the parsing results fell in the same general range
as for the PTB by roughly compensating for the
difference in annotation style. The results in Table
4 show that this is the case.
As a final note, the PPCMBE consists of
unedited data spanning more than 200 years, while
the PTB is edited newswire, and so to some extent
there would almost certainly be some difference in
score.
6 Parser Analysis
We are currently developing techniques to better
understand the types of errors is making, which
have already led to interesting results. The parser
is creating some odd structures that violate basic
well-formedness conditions of clauses. Tree (7a)
in Figure 6 is a tree from from the ?Reduced? cor-
pus, in which the verb ?formed? projects to IP,
665
(7) (a) IP-SUB
NP-SBJ
the earth?s crust
had been formed PP
by NP
causes RRC
ADVP-TMP
now
acting
(b) IP
NP
the earth?s crust
had been formed PP
by NP
causes
ADVP
now
acting
(8) (a) VP
would VP
be VP
teaching NP
the doctrine
(b) VP
would VP
be IP
VP
teaching NP
the doctrine
(9) IP
It VP
is IP-INF
VP
to VP
be VP
observed
Figure 6: Examples of issues with parser output
with two auxiliary verbs (?had? and ?been?). In
the corresponding parser output (7b), the parser
misses the reduced relative RRC, turning ?acting?
into the rightmost verb in the IP. The parser is cre-
ating an IP with two main verbs - an ungrammati-
cal structure that is not attested in the gold.
It might be thought that the parser is having
trouble with the flat-IP annotation style, but the
parser posits incorrect structures that are not at-
tested in the gold even in the Reduced+NPs+VPs
version of the corpus. Tree (8a) shows a fragment
of a gold tree from the corpus, with the VPs ap-
propriately inserted. The parser output (8b) has
an extra IP above ?teaching?. The POS tags for
?be? (BE) and ?teaching? (VAG) do not appear in
this configuration at all in the training material. In
general, the parser seems to be getting confused
as to when such an IP should appear. We hypoth-
esized that this is due to confusion with infiniti-
val clauses, which can have an unary-branching IP
over a VP, as in the gold tree (9). We retrained the
parser, directing it to retain the INF function tag
that appears in infinitival clauses as in (9). Over-
all, the evalb score went down slightly, but it did
fix cases such as (8b). We do not yet know why the
overall score went down, but what?s surprising is
one would have thought that IP-INF is recoverable
from the absence of a tensed verb.
Preliminary analysis shows that the CONJP
structures are also difficult for the parser. Since
these are structures that are different than the
PTB
9
, we were particularly interested in them.
Cases where the CONJP is missing an overt co-
ordinating cord (such as ?and?), are particularly
difficult, not surprisingly. These can appear as in-
termediate conjuncts in a string of conjuncts, with
the structure (CONJP word). The shared pre-
modifier structure described in (2a) is also difficult
for the parser.
7 Conclusion
We have presented the first results on parsing the
PPCMBE and discussed some significant annota-
tion style differences from the PTB. Adjusting for
two major differences that are a matter of anno-
tation convention, we showed that the PPCMBE
can be parsed at approximately the same level of
accuracy as the PTB. The first steps in an inves-
tigation of the parser differences show that the
parser is generating structures that violate basic
well-formedness conditions of the annotation.
For future work, we will carry out a more se-
rious analysis of the parser output, trying to more
properly account for the differences in bracketing
structure between the PPCMBE and PTB. There
is also a great deal of data annotated in the style
of the PPCMBE, as indicated in footnotes 1 and
3, and we are interested in how the parser per-
forms on these, especially comparing the results
on the modern English corpora to the older histor-
ical ones, which will have greater issues of ortho-
graphic and tokenization complications.
Acknowledgments
This work was supported by National Science
Foundation Grant # BCS-114749. We would like
to thank Ann Bies, Justin Mott, and Mark Liber-
man for helpful discussions.
9
The CONJP nonterminal in the PTB serves a different
purpose than in the PPCMBE and is much more limited.
666
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Charlotte Galves and Pabol Faria. 2010. Tycho
Brahe Parsed Corpus of Historical Portuguese.
http://www.tycho.iel.unicamp.br/
?
tycho/corpus/en/index.html.
Anthony Kroch and Ann Taylor. 2000. Penn-
Helsinki Parsed Corpus of Middle English, second
edition. http://www.ling.upenn.edu/
hist-corpora/PPCME2-RELEASE-3/
index.html.
Anthony Kroch, Beatrice Santorini, and Ariel
Diertani. 2004. Penn-Helsinki Parsed Cor-
pus of Early Modern English. http:
//www.ling.upenn.edu/hist-corpora/
PPCEME-RELEASE-2/index.html.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Modern
British English. http://www.ling.upenn.
edu/hist-corpora/PPCMBE-RELEASE-1/
index.html.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169?174, Jeju Island, Korea,
July. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
France Martineau et al 2009. Mod?eliser le change-
ment: les voies du franc?ais, a Parsed Corpus of His-
torical French.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL, pages 433?440, Sydney, Australia,
July. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Ann Taylor, Anthony Warner, Susan Pintzuk,
and Frank Beths. 2003. The York-Toronto-
Helsinki Parsed Corpus of Old English Prose.
Distributed through the Oxford Text Archive.
http://www-users.york.ac.uk/
?
lang22/YCOE/YcoeHome.htm.
Ann Taylor, Arja Nurmi, Anthony Warner, Susan
Pintzuk, and Terttu Nevalainen. 2006. Parsed
Corpus of Early English Correspondence. Com-
piled by the CEEC Project Team. York: Uni-
versity of York and Helsinki: University of
Helsinki. Distributed through the Oxford Text
Archive. http://www-users.york.ac.uk/
?
lang22/PCEEC-manual/index.htm.
Joel Wallenberg, Anton Karl Ingason, Einar Freyr
Sigursson, and Eirkur Rgnvaldsson. 2011.
Icelandic Parsed Historical Corpus (IcePaHC)
version 0.4. http://www.linguist.is/
icelandic_treebank.
667
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668?673,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Parser Evaluation Using Derivation Trees:
A Complement to evalb
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Anthony Kroch and Mark Liberman and Beatrice Santorini
Department of Linguistics, University of Pennsylvania, Philadelphia, PA 19104
{kroch,myl,beatrice}@ling.upenn.edu
Abstract
This paper introduces a new technique for
phrase-structure parser analysis, catego-
rizing possible treebank structures by inte-
grating regular expressions into derivation
trees. We analyze the performance of the
Berkeley parser on OntoNotes WSJ and
the English Web Treebank. This provides
some insight into the evalb scores, and
the problem of domain adaptation with the
web data. We also analyze a ?test-on-
train? dataset, showing a wide variance in
how the parser is generalizing from differ-
ent structures in the training material.
1 Introduction
Phrase-structure parsing is usually evaluated using
evalb (Sekine and Collins, 2008), which provides
a score based on matching brackets. While this
metric serves a valuable purpose in pushing parser
research forward, it has limited utility for under-
standing what sorts of errors a parser is making.
This is the case even if the score is broken down
by brackets (NP, VP, etc.), because the brackets
can represent different types of structures. We
would also like to have answers to such questions
as ?How does the parser do on non-recursive NPs,
separate from NPs resulting from modification?
On PP attachment?? etc.
Answering such questions is the goal of this
work, which combines two strands of research.
First, inspired by the tradition of Tree Adjoin-
ing Grammar-based research (Joshi and Schabes,
1997; Bangalore and Joshi, 2010), we use a de-
composition of the full trees into ?elementary
trees? (henceforth ?etrees?), with a derivation tree
that records how the etrees relate to each other,
as in Kulick et al (2011). In particular, we use
the ?spinal? structure approach of (Shen et al,
2008; Shen and Joshi, 2008), where etrees are con-
strained to be unary-branching.
Second, we use a set of regular expressions
(henceforth ?regexes?) that categorize the possible
structures in the treebank. These are best thought
of as an extension of head-finding rules, which not
only find a head but simultaneously identify each
parent/children relation as one of a limited number
of types of structures (right-modification, etc.).
The crucial step is that we integrate these
regexes into the spinal etrees. The derivation trees
provide elements of a dependency analysis, which
allow us to calculate scores for head identification
and attachment for different projections (e.g., PP).
The regexes allow us to also provide scores based
on spans of different construction types. Together
these two aspects break down the evalb brackets
into more meaningful categories, and the simulta-
neous head and span scoring allows us to separate
these aspects in the analysis.
After describing in more detail the basic frame-
work, we show some aspects of the resulting anal-
ysis of the performance of the Berkeley parser
(Petrov et al, 2008) on three datasets: (a)
OntoNotes WSJ sections 2-21 (Weischedel et al,
2011)
1
, (b) OntoNotes WSJ section 22, and (c)
the ?Answers? section of the English Web Tree-
bank (Bies et al, 2012). We trained the parser on
sections 2-21, and so (a) is ?test-on-train?. These
three results together show how the parser is gen-
eralizing from the training data, and what aspects
of the ?domain adaptation? problem to the web
material are particularly important.
2
2 Framework for analyzing parsing
performance
We first describe the use of the regexes in tree de-
composition, and then give some examples of in-
1
We refer only to the WSJ treebank portion of OntoNotes,
which is roughly a subset of the Penn Treebank (Marcus et
al., 1999) with annotation revisions including the addition of
NML nodes.
2
We parse (c) while training on (a) to follow the procedure
in Petrov and McDonald (2012)
668
corporating these regexes into the derivation trees.
2.1 Use of regular expressions
Decomposing the original phrase-structure tree
into the smaller components requires some
method of determining the ?head? of a nonter-
minal, from among its children nodes, similar to
parsing work such as Collins (1999). As described
above, we are also interested in the type of lin-
guistic construction represented by that one-level
structure, each of which instantiates one of a few
types - recursive coordination, simple head-and-
sister, etc. We address both tasks together with the
regexes. In contrast to the sort of head rules in
(Collins, 1999), these refer as little as possible to
specific POS tags. Instead of explicitly listing the
POS tags of possible heads, the heads are in most
cases determined by their location in the structure.
Sample regexes are shown in Figure 1. There
are 49 regexes used.
3
Regexes ADJP-t and
ADVP-t in (a) identify their terminal head to
be the rightmost terminal, possibly preceded by
some number of terminals or nonterminals, rely-
ing on a mapping that maps all terminals (except
CC, which is mapped to CONJ) to TAG and all
nonterminals (except CONJP and NML) to NT.
Structures with a CONJ/CONJP/NML child do
not match this rule and are handled by different
regexes, which are all mutually exclusive. In some
cases, we need to search for particular nonterminal
heads, such as with the (b) regexes S-vp and SQ-
vp, which identify the rightmost VP among the
children of a S or SQ as the head. (c) NP-modr
is a regex for a recursive NP with a right modifier.
In this case, the NP on the left is identified as the
head. (d) VP-crd is also a regex for a recursive
structure, in this case for VP coordination, pick-
ing out the leftmost conjunct as the head of the
structure. The regex names roughly describe their
purpose - ?mod? for right-modification, ?crd? for
coordination, etc. The suffix ?-t? is for the simple
non-recursive case in which the head is a terminal.
2.2 Regexes in the derivation trees
The names of these regexes are incorporated into
the etrees themselves, as labels of the nontermi-
nals. This allows an etree to contain information
3
Some among the 49 are duplicates, used for different
nonterminals, as with (a) and (b) in Figure 1. We derived
the regexes via an iterative process of inspection of tree de-
composition on dataset (a), together with taking advantage of
the treebanking experience from some of the co-authors.
(a)ADJP-t,ADVP-t:
?(TAG|NT|NML)
*
(head:TAG) (NT)
*
$
(b)S-vp, SQ-vp: ?([? ]+)
*
(head:VP)$
(c)NP-modr:
?(head:NP)(SBAR|S|VP|ADJP|PP|ADVP|NP)+$
(d)VP-crd: ?(head:VP) (VP)
*
CONJ VP$
Figure 1: Some sample regexes
such as ?this node represents right modification?.
For example, Figure 2 shows the derivation tree
resulting from the decomposition of the tree in
Figure 4. Each structure within a circle is one
etree, and the derivation as a whole indicates how
these etrees are combined. Here we indicate with
arrows that point to the relevant regex. For ex-
ample, the PP-t etree #a6 points to the NP-modr
regex, which consists of the NP-t together with
the PP-t. The nonterminals of the spinal etrees are
the names of the regexes, with the simpler non-
terminal labels trivially derivable from the regex
names.
4
The tree in Figure 5 is the parser output corre-
sponding to the gold tree in Figure 4, and in this
case gets the PP-t attachment wrong, while every-
thing else is the same as the gold.
5
This is reflected
in the derivation tree in Figure 3, in which the NP-
modr regex is absent, with the NP-t and PP-t etrees
#b5 and #b6 both pointing to the VP-t regex in
#b3. We show in Section 2.3 how this derivation
tree representation is used to score this attachment
error directly, rather than obscuring it as an NP
bracket error as evalb would do.
2.3 Scoring
We decompose both the gold and parser output
trees into derivation trees with spinal etrees, and
score based on the regexes projected by each word.
There is a match for a regex if the corresponding
words in gold/parser files project to that regex, a
precision error if the parser file does but the gold
does not, and a recall error if the gold does but the
parser file does not.
For example, comparing the trees in Figures 4
and 5 via their derivation trees in Figures 2 and
Figures 3, the word ?trip? has a match for the regex
NP-t, but a recall error for NP-modr. The word
4
We do not have space here to discuss the data structure
in complete detail, but multiple regex names at a node, such a
VP-aux and VP-t at tree a3 in Figure 2, indicate multiple VP
nonterminals.
5
We leave function tags aside for future work. The gold
tree is shown without the SBJ function tag.
669
#a1 TheyNP-t tripNP-modr  NP-t toPP-t#a6
#a3
#a4themake
VP-auxVP-tS-vp#a2will #a7FloridaNP-t
#a5
Figure 2: Derivation Tree for Figure 4
#b1TheyNP-t toPP-t#b6tripNP-t#b4themake
VP-auxVP-t#b3 S-vp#b2will #b7FloridaNP-t
#b5
Figure 3: Derivation Tree for Figure 5)
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 4: Gold tree
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 5: Parser output tree
Corpus tokens brackets coverage % evalb
2-21 g 650877 578597 571243 98.7
p 575744 569480 98.9 93.8
22 g 32092 24819 24532 98.8
p 24801 24528 98.9 90.1
Ans g 53960 48492 47348 97.6
p 48750 47423 97.3 80.8
Table 1: Corpus information for gold(g) and
parsed(p) sections of each corpus
?make? has a match for the regexes VP-t, VP-
aux, and S-vp, and so on. Summing such scores
over the corresponding gold/parser trees gives us
F-scores for each regex.
There are two modifications/extensions to these
F-scores that we also use:
(1) For each regex match, we score whether it
matches based on the span as well. For exam-
ple, ?make? is a match for VP-t in Figures 2
and 3, and is also a match for the span as well,
since in both derivation trees it includes the words
?make. . .Florida?. It is this matching for span as
well as head that allows us to compare our results
to evalb. We call the match just for the head the ?F-
h? score and the match that also includes the span
information the ?F-s? score. The F-s score roughly
corresponds to the evalb score. However, the F-
s score is for separate syntactic constructions (in-
cluding also head identification), although we can
also sum it over all the structures, as done later in
Figure 6. The simultaneous F-h and F-s scores let
us identify constructions where the parser has the
head projection correct, but gets the span wrong.
(2) Since the derivation tree is really a depen-
dency tree with more complex nodes (Rambow
and Joshi, 1997; Kulick et al, 2012), we can also
score each regex for attachment.
6
For example,
while ?to? is a match for PP-t, its attachment is
not, since in Figure 2 it is a child of the ?trip? etree
(#a5) and in Figure 3 it is a child of the ?make?
etree (#b3). Therefore our analysis results in an
attachment score for every regex.
2.4 Comparison with previous work
This work is in the same basic line of research
as the inter-annotator agreement analysis work in
Kulick et al (2013). However, that work did
not utilize regexes, and focused on comparing se-
quences of identical strings. The current work
scores on general categories of structures, without
6
A regex intermediate in a etree, such as VP-t above, is
considered to have a default null attachment. Also, the at-
tachment score is not relevant for regexes that already express
a recursive structure, such as NP-modr. In Figure 2, NP-t in
etree #a5 is considered as having the attachment to #a3.
670
Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank)
regex %gold F-h F-s att spanR %gold F-h F-s att spanR %gold F-h F-s att spanR
NP-t 30.7 98.9 97.6 96.5 99.6 31.1 98.0 95.8 94.4 99.6 28.5 95.4 91.5 90.9 99.3
VP-t 13.5 98.8 94.5 98.4 95.8 13.4 98.1 91.7 97.3 93.7 16.0 96.7 81.7 96.1 85.4
PP-t 12.2 99.2 91.0 90.5 92.0 12.1 98.7 86.4 86.1 88.2 8.4 96.4 80.5 80.7 84.7
S-vp 12.2 97.9 92.8 96.8 96.3 11.9 96.5 89.1 95.4 95.0 14.2 94.1 72.9 88.0 84.1
NP-modr 8.6 88.4 80.3 - 91.5 8.5 82.9 71.8 - 87.9 4.4 69.0 54.2 - 80.5
VP-aux 5.5 97.9 94.0 - 96.1 5.0 96.5 91.0 - 94.6 6.2 94.4 81.7 - 86.7
SBAR-s 3.7 96.1 91.1 91.8 95.3 3.5 94.3 87.2 86.4 93.5 4.0 84.8 68.2 81.9 81.9
ADVP-t 2.7 95.2 93.3 93.9 98.6 3.0 89.6 84.5 88.0 95.9 4.5 84.0 78.2 80.3 96.8
NML-t 2.3 91.6 90.3 97.6 99.8 2.6 85.6 82.2 93.5 99.8 0.7 42.1 37.7 88.8 100.0
ADJP-t 1.9 94.6 88.4 95.5 94.6 1.8 86.8 77.0 93.6 90.7 2.5 84.7 67.0 88.1 84.2
QP-t 1.0 95.3 93.8 98.3 99.6 1.2 91.0 89.0 97.1 100.0 0.2 57.7 57.7 94.4 100.0
NP-crd 0.8 80.3 73.7 - 92.4 0.6 68.6 58.4 - 86.1 0.5 55.3 47.8 - 88.1
VP-crd 0.4 84.3 82.8 - 98.2 0.4 75.3 73.5 - 97.6 0.8 65.5 58.3 - 89.8
S-crd 0.3 83.7 83.2 - 99.6 0.4 70.9 68.6 - 96.7 0.8 68.5 63.0 - 93.4
SQ-v 0.1 88.3 82.0 93.3 97.8 0.1 66.7 66.7 88.9 100.0 0.9 81.9 72.4 93.4 95.8
FRAG-nt 0.1 49.9 48.6 95.4 97.9 0.1 28.6 28.6 100.0 100.0 0.8 22.7 21.3 96.3 96.3
Table 2: Scores for the most frequent categories of brackets in the three datasets of corpora, as determined
by the regexes. % gold is the frequency of this regex type compared to all the brackets in the gold. F-h
is the score based on matching heads, F-s also incorporates the span information, att is the attachment
accuracy for words that match in F-h, and spanR is the span-right accuracy for words that match in F-h.
the reliance on sequences of individual strings.
7
3 Analysis of parsing results
We worked with the three datasets as described
in the introduction. We trained the parser on sec-
tions 2-21 of OntoNotes WSJ, and parsed the three
datasets with the gold tags, since at present we
wish to analyze the parser performance in isola-
tion from Part-of-Speech tagging errors. Table 1
shows the sizes of the three corpora in terms of
tokens and brackets, for both the gold and parsed
versions, with the evalb scores for the parsed ver-
sions. The score is lower for Answers, as also
found by Petrov and McDonald (2012).
To facilitate comparison of our analysis with
evalb, we used corpora versions with the same
bracket deletion (empty yields and most punctua-
tion) as evalb. We ran the gold and parsed versions
through our regex decomposition and derivation
tree creation. Table 1 shows the number and per-
centage of brackets handled by our regexes. The
high coverage (%) reinforces the point that there is
a limited number of core structures in the treebank.
In the results below in Table 2 and Figure 6 we
combine the nonterminals that are not covered by
one of the regexes with the simple non-recursive
regex case for that nonterminal.
8
7
In future work we will compare our approach to that
of Kummerfeld et al (2012), who also move beyond evalb
scores in an effort to provide more meaningful error analysis.
8
We also combine a few other non-recursive regexes to-
gether with NP-t, such as the special one for possessives.
We present the results in two ways. Table 2 lists
the most frequent categories in the three datasets,
with their percentage of the overall number of
brackets (%gold), their score based just on the
head identification (F-h), their score based on head
identification and (left and right) span (F-s), and
the attachment (att) and span-right (spanR) scores
for those that match based on the head.
9
The two graphs in Figure 6 show the cumu-
lative results based on F-h and F-s, respectively.
These show the cumulative score in order of the
frequency of categories. For example, for sections
2-21, the score for NP-t is shown first, with 30.7%
of the brackets, and then together with the VP-t
category, they cover 45.2% of the brackets, etc.
10
The benefit of the approach described here is that
now we can see the contribution to the evalb score
of the particular types of constructions, and within
those constructions, how well the parser is doing
at getting the same head projection, but failing or
9
The score for the left edge is almost always very high for
every category, and we just list here the right edge score. The
attachment score does not apply to the recursive categories,
as mentioned above.
10
The final F-s value is lower than the evalb score - e.g.
92.5 for sections 2-21 (the rightmost point in the graph for
sections 2-21 in the F-s graph in Figure 6) compared to the
93.8 evalb score. Space prevents full explanation, but there
are two reasons for this. One is that there are cases in which
bracket spans match, but the head, as found by our regexes, is
different in the gold and parser trees. The other cases is when
brackets match, and may even have the same head, but their
regex is different. In future work we will provide a full ac-
counting of such cases, but they do not affect the main aspects
of the analysis.
671
F-scores by head identification
cumulative % of all brackets
0 5 10 20 30 40 50 60 70 80 90 100
89
.2
91
.2
93
.2
95
.2
97
.2
99
.0
2-21
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10
12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
F-scores by head identification and span
cumulative % of all backets
0 5 10 20 30 40 50 60 70 80 90 100
78
.0
82
.0
86
.0
90
.0
94
.0
97
.6
2-21
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10 12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
Figure 6: Cumulative scores based on F-h (left) and F-s (right). These graphs are both cumulative in
exactly the same way, in that each point represents the total percentage of brackets accounted for so far.
So for the 2-21 line, point 1, meaning the NP non-recursive regex, accounts for 30.7% of the brackets,
point 2, meaning the VP non-recursive regex, accounts for another 13.5%, so 44.2% cumulatively, etc.
not on the spans.
3.1 Analysis and future work
As this is work-in-progress, the analysis is not yet
complete. We highlight a few points here.
(1) The high performance on the OntoNotes WSJ
material is in large part due to the score on the
non-recursive regexes of NP-t, VP-t, S-vp, and the
auxiliaries (points 1, 2, 4, 6 in the graphs). Critical
to this is the fact that the parser does well on deter-
mining the right edge of verbal structures, which
affects the F-s score for VP-t (non-recursive), VP-
aux, and S-vp. The spanR score for VP-t is 95.8
for Sections 2-21 and 93.7 for Section 22.
(2) We wouldn?t expect the test-on-training evalb
score to be 100%, since it has to back off from
the training data, but the results for the different
categories vary widely, with e.g., the NP-modr F-
h score much lower than other frequent regexes.
This variance from the test-on-training dataset car-
ries over almost exactly to Section 22.
(3) The different distribution of structures in
Answers hurts performance. For example, the
mediocre performance of the parser on SQ-vp
barely affects the score with OntoNotes, but has
a larger negative effect with Answers, due to its
increased frequency in the latter.
(4) While the different distribution of construc-
tions is a problem for Answers, more critical is
the poor performance of the parser on determin-
ing the right edge of verbal constructions. This is
only 85.4 for VP-t in Answers, compared to the
OntoNotes results mentioned in (1). Since this af-
fects the F-s scores for VP-t, VP-aux, and S-vp,
the negative effect is large. Preliminary investi-
gation shows that this is due in part to incorrect
PP and SBAR placement (the PP-t and SBAR-s
attachment scores (80.7 and 81.9) are worse for
Answers compared to Section 22 (86.1 and 86.4)),
and coordinated S-clauses with no conjunction.
In sum, there is a wealth of information from
this new type of analysis that we will use in our on-
going work to better understand what the parser is
learning and how it works on different genres.
Acknowledgments
This material is based upon work supported by Na-
tional Science Foundation Grant # BCS-114749
(first, fourth, and sixth authors) and by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-11-C-0145 (first,
second, and third authors). The content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
672
References
Srinivas Bangalore and Aravind K. Joshi, editors.
2010. Supertagging: Using Complex Lexical De-
scriptions in Natural Language Processing. MIT
Press.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Department of Computer and Information Sciences,
University of Pennsylvania.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, ed-
itors, Handbook of Formal Languages, Volume 3:
Beyond Words, pages 69?124. Springer, New York.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. Asso-
ciation for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Using
supertags and encoded annotation principles for im-
proved dependency to phrase structure conversion.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 305?314, Montr?eal, Canada, June. Associa-
tion for Computational Linguistics.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550?555, Atlanta, Georgia, June. Association for
Computational Linguistics.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of error
types in parser output. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Pro-
ceedings of the First Workshop on Syntactic Analysis
of Non-Canonical Language.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495?504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new
resource for incremental, dependency and seman-
tic parsing. Language Resources and Evaluation,
42(1):1?19.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes 4.0. Linguistic
Data Consortium LDC2011T03.
673
