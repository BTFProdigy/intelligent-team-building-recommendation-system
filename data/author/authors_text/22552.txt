Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115?2126,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Prune-and-Score: Learning for Greedy Coreference Resolution
Chao Ma, Janardhan Rao Doppa
?
, J. Walker Orr, Prashanth Mannem
Xiaoli Fern, Tom Dietterich and Prasad Tadepalli
School of Electrical Engineering and Computer Science, Oregon State University
{machao,orr,mannemp,xfern,tgd,tadepall}@eecs.oregonstate.edu
? School of Electrical Engineering and Computer Science, Washington State University
jana@eecs.wsu.edu
Abstract
We propose a novel search-based approach
for greedy coreference resolution, where
the mentions are processed in order and
added to previous coreference clusters.
Our method is distinguished by the use
of two functions to make each corefer-
ence decision: a pruning function that
prunes bad coreference decisions from fur-
ther consideration, and a scoring function
that then selects the best among the re-
maining decisions. Our framework re-
duces learning of these functions to rank
learning, which helps leverage powerful
off-the-shelf rank-learners. We show that
our Prune-and-Score approach is superior
to using a single scoring function to make
both decisions and outperforms sever-
al state-of-the-art approaches on multiple
benchmark corpora including OntoNotes.
1 Introduction
Coreference resolution is the task of clustering a
set of mentions in the text such that all mentions in
the same cluster refer to the same entity. It is one
of the first stages in deep language understanding
and has a big potential impact on the rest of the
stages. Several of the state-of-the-art approaches
learn a scoring function defined over mention pair,
cluster-mention or cluster-cluster pair to guide the
coreference decision-making process (Daum?e II-
I, 2006; Bengtson and Roth, 2008; Rahman and
Ng, 2011b; Stoyanov and Eisner, 2012; Chang et
al., 2013; Durrett et al., 2013; Durrett and Klein,
2013). One common and persistent problem with
these approaches is that the scoring function has to
make all the coreference decisions, which leads to
a highly non-realizable learning problem.
Inspired by the recent success of theHC-Search
Framework (Doppa et al., 2014a) for studying a
variety of structured prediction problems (Lam et
al., 2013; Doppa et al., 2014c), we study a novel
approach for search-based coreference resolution
called Prune-and-Score. HC-Search is a divide-
and-conquer solution that learns multiple compo-
nents with pre-defined roles, and each of them
contribute towards the overall goal by making the
role of the other components easier. The HC-
Search framework operates in the space of com-
plete outputs, and relies on the loss function which
is only defined on the complete outputs to drive it-
s learning. Unfortunately, this method does not
work for incremental coreference resolution since
the search space for coreference resolution con-
sists of partial outputs, i.e., a set of mentions only
some of which have been clustered so far.
We develop an alternative framework to HC-
Search that allows us to effectively learn from par-
tial output spaces and apply it to greedy corefer-
ence resolution. The key idea of our work is to
address the problem of non-realizability of the s-
coring function by learning two different function-
s: 1) a pruning function to prune most of the bad
decisions, and 2) a scoring function to pick the
best decision among those that are remaining. Our
Prune-and-Score approach is a particular instanti-
ation of the general idea of learning nearly-sound
constraints for pruning, and leveraging the learned
constraints to learn improved heuristic function-
s for guiding the search. The pruning constraints
can take different forms (e.g., classifiers, decision-
list, or ranking functions) depending on the search
architecture. Therefore, other coreference resolu-
tion systems (Chang et al., 2013; Durrett and K-
lein, 2013; Bj?orkelund and Kuhn, 2014) can also
benefit from this idea. While our basic idea of two-
level selection might appear similar to the coarse-
to-fine inference architectures (Felzenszwalb and
McAllester, 2007; Weiss and Taskar, 2010), the
details differ significantly. Importantly, our prun-
ing and scoring functions operate sequentially at
2115
each greedy search step, whereas in the cascades
approach, the second level function makes its pre-
diction only when the first level decision-making
is done.
Summary of Contributions. The main contribu-
tions of our work are as follows. First, we moti-
vate and introduce the Prune-and-Score approach
to search-based coreference resolution. Second,
we identify a decomposition of the overall loss
of the Prune-and-Score approach into the pruning
loss and the scoring loss, and reduce the problem
of learning these two functions to rank learning,
which allows us to leverage powerful and efficien-
t off-the-shelf rank learners. Third, we evaluate
our approach on OntoNotes, ACE, and MUC da-
ta, and show that it compares favorably to sever-
al state-of-the-art approaches as well as a greedy
search-based approach that uses a single scoring
function.
The remainder of the paper proceeds as follows.
In Section 2, we dicuss the related work. We intro-
duce our problem setup in Section 3 and then de-
scribe our Prune-and-Score approach in Section 4.
We explain our approaches for learning the prun-
ing and scoring functions in Section 5. Section 6
presents our experimental results followed by the
conclusions in Section 7.
2 Related Work
The work on learning-based coreference resolu-
tion can be broadly classified into three types.
First, the pair-wise classifier approaches learn a
classifier on mention pairs (edges) (Soon et al.,
2001; Ng and Cardie, 2002; Bengtson and Roth,
2008), and perform some form of approximate de-
coding or post-processing using the pair-wise s-
cores to make predictions. However, the pair-wise
classifier approach suffers from several drawback-
s including class imbalance (fewer positive edges
compared to negative edges) and not being able to
leverage the global structure (instead making in-
dependent local decisions).
Second, the global approaches such as Struc-
tured SVMs and Conditional Random Fields
(CRFs) learn a cost function to score a potential
clustering output for a given input set of men-
tions (Mccallum and Wellner, 2003; Finley and
Joachims, 2005; Culotta et al., 2007; Yu and
Joachims, 2009; Haghighi and Klein, 2010; Wick
et al., 2011; Wick et al., 2012; Fernandes et al.,
2012). These methods address some of the prob-
lems with pair-wise classifiers, however, they suf-
fer from the intractability of ?Argmin? inference
(finding the least cost clustering output among ex-
ponential possibilities) that is encountered during
both training and testing. As a result, they resort to
approximate inference algorithms (e.g., MCMC,
loopy belief propagation), which can suffer from
local optima.
Third, the incremental approaches construct the
clustering output incrementally by processing the
mentions in some order (Daum?e III, 2006; De-
nis and Baldridge, 2008; Rahman and Ng, 2011b;
Stoyanov and Eisner, 2012; Chang et al., 2013;
Durrett et al., 2013; Durrett and Klein, 2013).
These methods learn a scoring function to guide
the decision-making process and differ in the form
of the scoring function (e.g., mention pair, cluster-
mention or cluster-cluster pair) and how it is being
learned. They have shown great success and are
very efficient. Indeed, several of the approach-
es that have achieved state-of-the-art results on
OntoNotes fall under this category (Chang et al.,
2013; Durrett et al., 2013; Durrett and Klein,
2013; Bj?orkelund and Kuhn, 2014). However,
their efficiency requirement leads to a highly non-
realizable learning problem. Our Prune-and-Score
approach is complementary to these methods, as
we show that having a pruning function (or a set
of learned pruning rules) makes the learning prob-
lem easier and can improve over the performance
of scoring-only approaches. Also, the models in
(Chang et al., 2013; Durrett et al., 2013) try to
leverage cluster-level information implicitly (vi-
a latent antecedents) from mention-pair features,
whereas our model explicitly leverages the cluster
level information.
Coreference resolution systems can benefit
by incorporating the world knowledge including
rules, constraints, and additional information from
external knowledge bases (Lee et al., 2013; Rah-
man and Ng, 2011a; Ratinov and Roth, 2012;
Chang et al., 2013; Zheng et al., 2013; Hajishirzi
et al., 2013). Our work is orthogonal to this line
of work, but domain constraints and rules can be
incorporated into our model as done in (Chang et
al., 2013).
3 Problem Setup
Coreference resolution is a structured pre-
diction problem where the set of mentions
m
1
,m
2
, ? ? ? ,m
D
extracted from a document cor-
2116
reponds to a structured input x and the structured
output y corresponds to a partition of the men-
tions into a set of clusters C
1
, C
2
, ? ? ? , C
k
. Each
mention m
i
belongs to exactly one of the clusters
C
j
. We are provided with a training set of input-
output pairs drawn from an unknown distribution
D, and the goal is to return a function/predictor
from inputs to outputs. The learned predictor
is evaluated against a non-negative loss function
L : X ?Y?Y 7? <
+
, L(x, y
?
, y) is the loss asso-
ciated with predicting incorrect output y
?
for input
x when the true output is y (e.g., B-Cubed Score).
In this work, we formulate the coreference
resolution problem in a search-based framework.
There are three key elements in this framework:
1) the Search space S
p
whose states correspond
to partial clustering outputs; 2) the Action prun-
ing function F
prune
that is used to prune irrelevant
actions at each state; and 3) the Action scoring
function F
score
that is used to construct a com-
plete clustering output by selecting actions from
those that are left after pruning. S
p
is a 3-tuple
?I, A, T ?, where I is the initial state function, A
gives the set of possible actions in a given state,
and T is a predicate which is true for terminal s-
tates. In our case, s
0
= I(x) corresponds to a s-
tate where every mention is unresolved, and A(s
i
)
consists of actions to place the next mention m
i+1
in each cluster in s
i
or a NEW action which creates
a new cluster for it. Terminal nodes correspond to
states with all mentions resolved.
We focus on greedy search. The decision pro-
cess for constructing an output corresponds to s-
electing a sequence of actions leading from the
initial state to a terminal state using both F
prune
and F
score
, which are parameterized functions
over state-action pairs (F
prune
(?
1
(s, a)) ? < and
F
score
(?
2
(s, a)) ? <), where ?
1
and ?
2
stand for
feature functions. We want to learn the parameters
of both F
prune
and F
score
such that the predicted
outputs on unseen inputs have low expected loss.
4 Greedy Prune-and-Score Approach
Our greedy Prune-and-Score approach for coref-
erence resolution is parameterized by a pruning
function F
prune
: S ? A 7? <, a scoring func-
tion F
score
: S ? A 7? <, and a pruning param-
eter b ? [1, A
max
], where A
max
is the maximum
number of actions at any state s ? S . Given a
set of input mentions m
1
,m
2
, ? ? ? ,m
D
extracted
from a document (input x), and a pruning param-
Algorithm 1 Greedy Prune-and-Score Resolver
Input: x = set of mentions m
1
,m
2
, ? ? ? ,m
D
from
a document D, ?I, A, T ? = Search space defini-
tion, F
prune
= learned pruning function, b = prun-
ing parameter, F
score
= learned scoring function
1: s? I(x) // initial state
2: while not T (s) do
3: A
?
? Top b actions from A(s) according to
F
prune
// prune
4: a
p
? argmax
a?A
?
F
score
(s, a) // score
5: s? Apply a
p
on s
6: end while
7: return coreference output corresponding to s
eter b, our Prune-and-Score approach makes pre-
dictions as follows. The search starts at the ini-
tial state s
0
= I(x) (see Algorithm 1). At each
non-terminal state s, the pruning function F
prune
retains only the top b actions (A
?
) from A(s) (Step
3), and the scoring function F
score
picks the best
scoring action a
p
? A
?
(Step 4) to reach the next
state. When a terminal state is reached its con-
tents are returned as the prediction. Figure 1 illus-
trates the decision-making process of our Prune-
and-Score approach for an example state.
We now formalize the learning objective of our
Prune-and-Score approach. Let y? be the predicted
coreference output for a coreference input-output
pair (x, y
?
). The expected loss of the greedy
Prune-and-Score approach E(F
prune
,F
score
) for a
given pruning function F
prune
and scoring func-
tion F
score
can be defined as follows.
E(F
prune
,F
score
) = E
(x,y
?
)?D
L (x, y?, y
?
)
Our goal is to learn an optimal pair of pruning
and scoring functions
(
F
o
prune
,F
o
score
)
that min-
imizes the expected loss of the Prune-and-Score
approach. The behavior of our Prune-and-Score
approach depends on the pruning parameter b,
which dictates the workload of pruning and scor-
ing functions. For small values of b (aggressive
pruning), pruning function learning may be harder,
but scoring function learning will be easier. Simi-
larly, for large values of b (conservative pruning),
scoring function learning becomes hard, but prun-
ing function learning is easy. Therefore, we would
expect beneficial behavior if pruning function can
aggressively prune (small values of b) with little
loss in accuracy. It is interesting to note that our
Prune-and-Score approach degenerates to existing
incremental approaches that use only the scoring
function for search (Daum?e III, 2006; Rahman and
2117
(a) Text with input set of mentions
Ramallah ( West Bank
2
)
1
10-15 ( AFP
3
) - Eyewitnesses
4
reported that Palestinians
5
demonstrated today Sunday in the West Bank
6
against the Sharm el-Sheikh
7
summit to be
held in Egypt
8
tomorrow Monday. In Ramallah
9
, around 500 people
10
took to the town
11
?s
streets chanting slogans denouncing the summit ...
(b) Illustration of Prune-and-Score approach
1m
9m 3m 4m6m2m
1C
1a 2a 3a 4a 5a 6a 7a
5m
10m 7m 11m
2C 3C 4C 5C 6C
State: s = {C
1
, C
2
, C
3
, C
4
, C
5
, C
6
} Actions: A(s) = {a
1
, a
2
, a
3
, a
4
, a
5
, a
6
, a
7
}Pruning step:
Scoring step:
2.5             2.2               1.9                1.5              1.4              0.7              0.4
4.5             3.1              2.6
2a 1a 7a 5a 6a 3a 4a
1a 2a 7aA?(s) = {a2, a1, a7}
b = 3
Decision: a
1
is the best action for state s
F
prune
values
F
score
values
Figure 1: Illustration of Prune-and-Score approach. (a) Text with input set of mentions. Mentions are highlighted
and numbered. (b) Illustration of decision-making process for mention m
11
. The partial clustering output corre-
sponding to the current state s consists of six clusters denoted by C
1
, C
2
, ? ? ? , C
6
. Highlighted circles correspond
to the clusters. Edges from mention m
11
to each of the six clusters and to itself stand for the set of possible actions
A(s) in state s, and are denoted by a
1
, a
2
, ? ? ? , a
7
. The pruning function F
prune
scores all the actions in A(s) and
only keeps the top 3 actions A
?
= {a
2
, a
1
, a
7
} as specified by the pruning parameter b. The scoring function picks
the best scoring action a
1
? A
?
as the final decision, and mention m
11
is merged with cluster C
1
.
Ng, 2011b) when b =?. Additionally, for b = 1,
our pruning function coincides with the scoring
function.
Analysis of Representational Power. The fol-
lowing proposition formalizes the intuition that t-
wo functions are strictly better than one in expres-
sive power. See Appendix for the proof.
Proposition 1. Let F
prune
and F
score
be func-
tions from the same function space. Then for all
learning problems, min
F
score
E(F
score
,F
score
) ?
min
(F
prune
,F
score
)
E(F
prune
,F
score
). More-
over there exist learning problems for which
min
F
score
E(F
score
,F
score
) can be arbitrarily
worse than min
(F
prune
,F
score
)
E(F
prune
,F
score
).
5 Learning Algorithms
In general, learning the optimal
(
F
o
prune
,F
o
score
)
pair can be intractable due to their potential inter-
dependence. Specifically, when learning F
prune
in the worst case there can be ambiguity about
which of the non-optimal actions to retain, and
for only some of those an effective F
score
can be
found. However, we observe a loss decomposi-
tion in terms of the individual losses due to F
prune
and F
score
, and develop a stage-wise learning ap-
proach that first learns F
prune
and then learns a
corresponding F
score
.
5.1 Loss Decomposition
The overall loss of the Prune-and-Score approach
E (F
prune
,F
score
) can be decomposed into prun-
ing loss 
prune
, the loss due to F
prune
not be-
ing able to retain the optimal terminal state in
the search space; and scoring loss 
score|F
prune
,
the additional loss due to F
score
not guiding the
greedy search to the best terminal state after prun-
ing using F
prune
. Below, we will define these
losses more formally.
Pruning Loss is defined as the expected loss of
the Prune-and-Score approach when we perform
greedy search with F
prune
and F
?
score
, the opti-
mal scoring function. A scoring function is said to
be optimal if at every state s in the search space
2118
Sp
, and for any set of remaining actions A(s), it
can score each action a ? A(s) such that greedy
search can reach the best terminal state (as eval-
uated by task loss function L) that is reachable
from s through A(s). Unfortunately, computing
the optimal scoring function is highly intractable
for the non-decomposable loss functions that are
employed in coreference resolution (e.g., B-Cubed
F1). The main difficulty is that the decision at any
one state has interdependencies with future deci-
sions (see Section 5.5 in (Daum?e III, 2006) for
more details). So we need to resort to some form
of approximate optimal scoring function that ex-
hibits the intended behavior. This is very similar
to the dynamic oracle concept developed for de-
pendency parsing (Goldberg and Nivre, 2013).
Let y
?
prune
be the coreference output corre-
sponding to the terminal state reached from input
x by Prune-and-Score approach when performing
search using F
prune
and F
?
score
. Then the pruning
loss can be expressed as follows.

prune
= E
(x,y
?
)?D
L
(
x, y
?
prune
, y
?
)
Scoring Loss is defined as the additional loss due
to F
score
not guiding the greedy search to the best
terminal state reachable via the pruning function
F
score
(i.e., y
?
prune
). Let y? be the coreference out-
put corresponding to the terminal state reached by
Prune-and-Score approach by performing search
with F
prune
and F
score
for an input x. Then the
scoring loss can be expressed as follows:

score|F
prune
= E
(x,y
?
)?D
L (x, y?, y
?
)? L
(
x, y
?
prune
, y
?
)
The overall loss decomposition of our Prune-and-
Score approach can be expressed as follows.
E (F
prune
,F
score
)
= E
(x,y
?
)?D
L
(
x, y
?
prune
, y
?
)
? ?? ?

prune
+
E
(x,y
?
)?D
L (x, y?, y
?
)? L
(
x, y
?
prune
, y
?
)
? ?? ?

score|F
prune
5.2 Stage-wise Learning
The loss decomposition motivates a learning ap-
proach that targets minimizing the errors of prun-
ing and scoring functions independently. In par-
ticular, we optimize the overall loss of the Prune-
and-Score approach in a stage-wise manner. We
first train a pruning function
?
F
prune
to optimize
the pruning loss component 
prune
and then train
a scoring function
?
F
score
to optimize the scoring
loss 
score|
?
F
prune
conditioned on
?
F
prune
.
?
F
prune
? argmin
F
prune
?F
p

prune
?
F
score
? argmin
F
score
?F
s

score|
?
F
prune
Note that this approach is myopic in the sense that
?
F
prune
is learned without considering the impli-
cations for learning
?
F
score
. Below, we first de-
scribe our approach for pruning function learning,
and then explain our scoring function learning al-
gorithm.
5.3 Pruning Function Learning
In our greedy Prune-and-Score approach, the role
of the pruning function F
prune
is to prune away
irrelevant actions (as specified by the pruning pa-
rameter b) at each search step. More specifically,
we want F
prune
to score actions A(s) at each s-
tate s such that the optimal action a
?
? A(s) is
ranked within the top b actions to minimize 
prune
.
For this, we assume that for any training input-
output pair (x, y
?
) there exists a unique action se-
quence, or solution path (initial state to terminal
state), for producing y
?
from x. More formally, let
(s
?
0
, a
?
0
), (s
?
1
, a
?
1
), ? ? ? , (s
?
D
,?) correspond to the
sequence of state-action pairs along this solution
path, where s
?
0
is the initial state and s
?
D
is the ter-
minal state. The goal is to learn the parameters of
F
prune
such that at each state s
?
i
, a
?
i
? A(s
?
i
) is
ranked among the top b actions.
While we can employ an online-LaSO style ap-
proach (III and Marcu, 2005; Xu et al., 2009) to
learn the parameters of the pruning function, it is
quite inefficient, as it must regenerate the same
search trajectory again and again until it learn-
s to make the right decision. Additionally, this
approach limits applicability of the off-the-shelf
learners to learn the parameters of F
prune
. To
overcome these drawbacks, we apply offline train-
ing.
Reduction to Rank Learning. We reduce the
pruning function learning to a rank learning prob-
lem. This allows us to leverage powerful and effi-
cient off-the-shelf rank-learners (Liu, 2009). The
reduction is as follows. At each state s
?
i
on the so-
lution path of a training example (x, y
?
), we create
an example by labeling optimal action a
?
i
? A(s
?
i
)
as the only relevant action, and then try to learn
2119
a ranking function that can rank actions such that
the relevant action a
?
i
is in the top b actions, where
b is the input pruning paramter. In other word-
s, we have a rank learning problem, where the
learner?s goal is to optimize the Precision at Top-
b. The training approach creates such an exam-
ple for each state s in the solution path. The set
of aggregate imitation examples collected over al-
l the training data is then given to a rank learner
(e.g., LambdaMART (Burges, 2010)) to learn the
parameters of F
prune
by optimizing the Precision
at Top-b loss. See appendix for the pseudocode.
If we can learn a function F
prune
that is con-
sistent with these imitation examples, then the
learned pruning function is guaranteed to keep
the solution path within the pruned space for al-
l the training examples. We can also employ
more advanced imitation learning algorithms in-
cluding DAgger (Ross et al., 2011) and SEARN
(Hal Daum?e III et al., 2009) if we are provid-
ed with an (approximate) optimal scoring function
F
?
score
that can pick optimal actions at states that
are not in the solution path (i.e., off-trajectory s-
tates).
5.4 Scoring Function Learning
Given a learned pruning function F
prune
, we want
to learn a scoring function that can pick the best
action from the b actions that remain after prun-
ing at each state. We formulate this problem in the
framework of imitation learning (Khardon, 1999).
More formally, let (s?
0
, a
?
0
), (s?
1
, a
?
1
), ? ? ? , (s?
?
D
,?)
correspond to the sequence of state-action pairs
along the greedy trajectory obtained by running
the Prune-and-Score approach with F
prune
and
F
?
score
, the optimal scoring function, on a train-
ing example (x, y
?
), where s?
?
D
is the best terminal
state in the pruned space. The goal of our imita-
tion training approach is to learn the parameters
of F
score
such that at each state s?
i
, a
?
i
? A
?
is
ranked higher than all other actions in A
?
, where
A
?
? A(s?
i
) is the set of b actions that remain after
pruning.
It is important to note that the distribution of
states in the pruned space due to F
prune
on the
testing data may be somewhat different from those
on training data. Therefore, we train our scoring
function via cross-validation by training the scor-
ing function on heldout data that was not used to
train the pruning function. This methodology is
commonly employed in Re-Ranking and Stacking
approaches (Collins, 2000; Cohen and de Carval-
ho, 2005).
Our scoring function learning procedure uses
cross validation and consists of the following four
steps. First, we divide the training data D in-
to k folds. Second, we learn k different pruners,
where each pruning function F
i
prune
is learned us-
ing the data from all the folds excluding the i
th
fold. Third, we generate ranking examples for
scoring function learning as described above us-
ing each pruning function F
i
prune
on the data it
was not trained on. Finally, we give the aggregate
set of ranking examples R to a rank learner (e.g.,
SVM-Rank or LambdaMART) to learn the scoring
function F
score
. See appendix for the pseudocode.
Approximate Optimal Scoring Function. If the
learned pruning function is not consistent with the
training data, we will encounter states s?
i
that are
not on the target path, and we will need some su-
pervision for learning in those cases. As discussed
before in Section 5.1, computing an optimal scor-
ing functionF
?
score
is intractable for combinatorial
loss functions that are used for coreference resolu-
tion. So we employ an approximate function from
existing work that is amenable to evaluate partial
outputs (Daum?e III, 2006). It is a variant of the
ACE scoring function that removes the bipartite
matching step from the ACE metric. Moreover
this score is computed only on the partial coref-
erence output corresponding to the ?after state?
s
?
resulting from taking action a in state s, i.e.,
F
?
score
(s, a) = F
?
score
(s
?
). To further simplify the
computation, we give uniform weight to the three
types of costs: 1) Credit for correct linking, 2)
Penalty for incorrect linking, and 3) Penalty for
missing links. Intuitively, this is similar to the
correct-link count computed only on a subgraph.
We direct the reader to (Daum?e III, 2006) for more
details (see Section 5.5).
6 Experiments and Results
In this section, we evaluate our greedy Prune-
and-Score approach on three benchmark corpora
? OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004
(NIST, 2004), and MUC6 (MUC6, 1995) ? and
compare it against the state-of-the-art approaches
for coreference resolution. For OntoNotes data,
we report the results on both gold mentions and
predicted mentions. We also report the results on
gold mentions for ACE 2004 and MUC6 data.
2120
6.1 Experimental Setup
Datasets. For OntoNotes corpus, we employ the
official split for training, validation, and testing.
There are 2802 documents in the training set; 343
documents in the validation set; and 345 docu-
ments in the testing set. The ACE 2004 corpus
contains 443 documents. We follow the (Culot-
ta et al., 2007; Bengtson and Roth, 2008) split
in our experiments by employing 268 documents
for training, 68 documents for validation, and 107
documents (ACE2004-CULOTTA-TEST) for test-
ing. We also evaluate our system on the 128
newswire documents in ACE 2004 corpus for a
fair comparison with the state-of-the-art. The
MUC6 corpus containts 255 documents. We em-
ploy the official test set of 30 documents (MUC6-
TEST) for testing purposes. From the remaining
225 documents, which includes 195 official train-
ing documents and 30 dry-run test documents, we
randomly pick 30 documents for validation, and
use the remaining ones for training.
Evaluation Metrics. We compute three most pop-
ular performance metrics for coreference resolu-
tion: MUC (Vilain et al., 1995), B-Cubed (Bag-
ga and Baldwin, 1998), and Entity-based CEAF
(CEAF
?4
) (Luo, 2005). As it is commonly done
in CoNLL shared tasks (Pradhan et al., 2012), we
employ the average F1 score (CoNLL F1) of these
three metrics for comparison purposes. We evalu-
ate all the results using the updated version
1
(7.0)
of the coreference scorer.
Features. We built
2
our coreference resolver
based on the Easy-first coreference system (Stoy-
anov and Eisner, 2012), which is derived from the
Reconcile system (Stoyanov et al., 2010). We es-
sentially employ the same features as in the Easy-
first system. However, we provide some high-
level details that are necessary for subsequent dis-
cussion. Recall that our features ?(s, a) for both
pruning and scoring functions are defined over
state-action pairs, where each state s consists of
a set of clusters and an action a corresponds to
merging an unprocessed mention m with a clus-
ter C in state s or create one for itself. Therefore,
?(s, a) defines features over cluster-mention pairs
(C,m). Our feature vector consists of three part-
s: a) mention pair features; b) entity pair features;
and c) a single indicator feature to represent NEW
1
http://code.google.com/p/reference-coreference-scorers/
2
See http://research.engr.oregonstate.edu/dral/ for our
software.
action (i.e., mention m starts its own cluster). For
mention pair features, we average the pair-wise
features over all links between m and every men-
tion m
c
in cluster C (often referred to as average-
link). Note that, we cannot employ the best-link
feature representation because we perform offline
training and do not have weights for scoring the
links. For entity pair features, we treat mention
m as a singleton entity and compute features by
pairing it with the entity represented by cluster C
(exactly as in the Easy-first system). The indica-
tor feature will be 1 for the NEW action and 0 for
all other actions.We have a total of 140 features:
90 mention pair features; 49 entity pair features;
and one NEW indicator feature. We believe that
our approach can benefit from employing features
of the mention for the NEW action (Rahman and
Ng, 2011b; Durrett and Klein, 2013). However,
we were constrained by the Reconcile system and
could not leverage these features for the NEW ac-
tion.
Base Rank-Learner. Our pruning and scoring
function learning algorithms need a base rank-
learner. We employ LambdaMART (Burges,
2010), a state-of-the art rank learner from the
RankLib
3
library. LambdaMART is a variant of
boosted regression trees. We use a learning rate
of 0.1, specify the maximum number of boost-
ing iterations (or trees) as 1000 noting that its ac-
tual value is automatically decided based on the
validation set, and tune the number of leaves per
tree based on the validation data. Once we fix
the hyper-parameters of LambdaMART, we train
the final model on all of the training data. Lamb-
daMART uses an internal train/validation split of
the input ranking examples to decide when to stop
the boosting iterations. We fixed this ratio to 0.8
noting that the performance is not sensitive to this
parameter. For scoring function learning, we used
5 folds for the cross-validation training.
Pruning Parameter b. The hyper-parameter b
controls the amount of pruning in our Prune-and-
Score approach. We perform experiments with d-
ifferent values of b and pick the best value based
on the performance on the validation set.
Singleton Mention Filter for OntoNotes Cor-
pus. We employ the Illinois-Coref system (Chang
et al., 2012) to extract system mentions for our
OntoNotes experiments, and observe that the num-
3
http://sourceforge.net/p/lemur/wiki/RankLib/
2121
ber of predicted mentions is thrice the number of
gold mentions. Since the training data provides the
clustering supervision for only gold mentions, it is
not clear how to train with the system mention-
s that are not part of gold mentions. A common
way of dealing with this problem is to treat all the
extra system mentions as singleton clusters (Dur-
rett and Klein, 2013; Chang et al., 2013). Howev-
er, this solution most likely will not work with our
current feature representation (i.e., NEW action is
represented as a single indicator feature). Recall
that to predict these extra system mentions as s-
ingleton clusters with our incremental clustering
approach, the learned model should first predic-
t a NEW action while processing these mention-
s to form a temporary singleton cluster, and then
refrain from merging any of the subsequent men-
tions with that cluster so that it becomes a single-
ton cluster in the final clustering output. Howev-
er, in OntoNotes corpus, the training data does not
include singleton clusters for the gold mentions.
Therefore, only the large number (57%) of system
mentions that are not part of gold mentions will
constitute the set of singleton clusters. This leads
to a highly imbalanced learning problem because
our model needs to learn (the weight of the sin-
gle indicator feature) to predict NEW as the best
action for a large set of mentions, which will bias
our model to predict large number of NEW actions
during testing. As a result, we will generate many
singleton clusters, which will hurt the recall of the
mention detection after post-processing. There-
fore, we aim to learn a singleton mention filter
that will be used as a pre-processor before training
and testing to overcome this problem. We would
like to point out that our filter is complementary to
other solutions (e.g., employing features that can
discriminate a given mention to be anaphoric or
not in place of our single indicator feature, or us-
ing a customized loss to weight our ranking exam-
ples for cost-sensitive training)(Durrett and Klein,
2013).
Filter Learning. The singleton mention filter is
a classifier that will label a given mention as ?s-
ingleton? or not. We represent each mention m
in a document by averaging the mention-pair fea-
tures ?(m,m
?
) of the k-most similar mentions
(obtained by ranking all other mentions m
?
in the
document with a learned ranking functionR given
m) and then learn a decision-tree classifier by opti-
mizing the F1 loss. We learn the mention-ranking
function R by optimizing the recall of positive
pairs for a given k, and employ LambdaMART as
our base ranker. The hyper-parameters are tuned
based on the performance on the validation set.
6.2 Results
We first describe the results of the learned single-
ton mention filter, and then the performance of
our Prune-and-Score approach with and without
the filter. Next, we compare the results of our ap-
proach with several state-of-the-art approaches for
coreference resolution.
Singleton Mention Filter Results. Table 1 shows
the performance of the learned singleton mention
filter with k = 2 noting that the results are ro-
bust for all values of k ? 2. As we can see, the
learned filter improves the precision of the men-
tion detection with only small loss in the recall of
gold mentions.
Mention Detection Accuracy
P R F1
Before- 43.18% 86.99% 57.71%
filtering (16664/38596) (16664/19156)
After- 79.02% 80.98% 79.97%
filtering (15516/19640) (15516/19156)
Table 1: Performance of the singleton mention filter on
the OntoNotes 5.0 development set. The numerators of
the fractions in the brackets show the exact numbers of
mentions that are matched with the gold mentions.
Prune-and-Score Results. Table 2 shows the per-
formance of Prune-and-Score approach with and
without the singleton mention filter. We can see
that the results with filter are much better than the
corresponding results without the filter. These re-
sults show that our approach can benefit from hav-
ing a good singleton mention filter.
Filter settings MUC B
3
CEAF
?4
CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
O.S. (w.o. Filter) 66.73 53.40 44.23 54.79
P&S (w.o. Filter) 65.93 52.96 50.24 56.38
P&S (w. Filter) 71.18 58.87 57.88 62.64
Table 2: Performance of Prune-and-Score approach
with and without the singleton mention filter, and Only-
Score approach without the filter.
Table 3 shows the performance of different con-
figurations of our Prune-and-Score approach. As
we can see, Prune-and-Score gives better results
than the configuration where we employ only the
scoring function (b = ?) for small values of b.
2122
MUC B
3
CEAF
?4
CoNLL
P R F1 P R F1 P R F1 Avg-F1
a. Results on OntoNotes 5.0 Test Set with Predicted Mentions
Prune-and-Score 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56
Only-Scoring 75.95 61.53 67.98 63.94 47.37 54.42 58.54 49.76 53.79 58.73
HOTCoref 67.46 74.3 70.72 54.96 62.71 58.58 52.27 59.4 55.61 61.63
CPL
3
M - - 69.48 - - 57.44 - - 53.07 60.00
Berkeley 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41
Fernandes et al., 2012 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65
Stanford 65.31 64.11 64.71 56.54 48.58 52.26 46.67 52.29 49.32 55.43
b. Results on OntoNotes 5.0 Test Set with Gold Mentions
Prune-and-Score 88.10 85.85 86.96 76.82 76.16 76.49 80.90 74.06 77.33 80.26
Only-Scoring 86.96 84.52 85.73 74.51 74.25 74.38 79.04 70.67 74.62 78.24
CPL
3
M - - 84.80 - - 78.74 - - 68.75 77.43
Berkeley 85.73 89.26 87.46 78.23 75.11 76.63 82.89 70.86 76.40 80.16
Stanford 89.94 78.17 83.64 81.75 68.95 74.81 73.97 61.20 66.98 75.14
c. Results on ACE2004 Culotta Test Set with Gold Mentions
Prune-and-Score 85.57 72.68 78.60 90.09 77.02 83.04 74.64 86.02 79.42 80.35
Only-Scoring 82.75 69.25 75.40 88.54 74.22 80.75 73.69 85.22 78.58 78.24
CPL
3
M - - 78.29 - - 82.20 - - 79.26 79.91
Stanford 82.91 69.90 75.85 89.14 74.05 80.90 75.67 77.45 76.55 77.77
d. Results on ACE2004 Newswire with Gold Mentions
Prune-and-Score 89.72 75.72 82.13 90.89 76.15 82.87 72.43 86.83 78.69 81.23
Only-Scoring 86.92 76.49 81.37 88.10 75.83 81.51 73.15 84.31 78.05 80.31
Easy-first - - 80.1 - - 81.8 - - - -
Stanford 84.75 75.34 79.77 87.50 74.59 80.53 73.32 81.49 77.19 79.16
e. Results on MUC6 Test Set with Gold Mentions
Prune-and-Score 89.53 82.75 86.01 86.48 76.18 81.00 60.74 80.33 68.68 78.56
Only-Scoring 86.77 80.96 83.76 81.72 72.99 77.11 57.56 75.38 64.91 75.26
Easy-first - - 88.2 - - 77.5 - - - -
Stanford 91.19 69.54 78.91 91.07 63.39 74.75 62.43 69.62 65.83 73.16
Table 4: Comparison of Prune-and-Score with state-of-the-art approaches. Metric values reflect version 7 of
CoNLL scorer.
The performance is clearly better than the degen-
erate case (b = ?) over a wide range of b values,
suggesting that it is not necessary to carefully tune
the parameter b.
Pruning param. b MUC B
3
CEAF
?4
CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
2 69.12 56.80 56.30 60.74
3 70.50 57.89 57.24 61.88
4 71.00 58.65 57.41 62.35
5 71.18 58.87 57.88 62.64
6 70.93 58.66 57.85 62.48
8 70.12 58.13 57.37 61.87
10 70.24 58.34 56.27 61.61
20 67.97 57.73 56.63 60.78
? 67.03 56.31 55.56 59.63
Table 3: Performance of Prune-and-Score approach
with different values of the pruning parameter b. For
b =?, Prune-and-Score becomes an Only-Scoring al-
gorithm.
Comparison to State-of-the-Art. Table 4
shows the results of our Prune-and-Score ap-
proach compared with the following state-of-the-
art coreference resolution approaches: HOTCoref
system (Bj?orkelund and Kuhn, 2014); Berkeley
system with the FINAL feature set (Durrett and K-
lein, 2013); CPL
3
M system (Chang et al., 2013);
Stanford system (Lee et al., 2013); Easy-first sys-
tem (Stoyanov and Eisner, 2012); and Fernan-
des et al., 2012 (Fernandes et al., 2012). On-
ly Scoring is the special case of our Prune-and-
Score approach where we employ only the scoring
function. This corresponds to existing incremen-
tal approaches (Daum?e III, 2006; Rahman and Ng,
2011b). We report the best published results for
CPL
3
M system, Easy-first, and Fernandes et al.,
2012. We ran the publicly available software to
generate the results for Berkeley and Stanford sys-
tems with the updated CoNLL scorer. We include
the results of Prune-and-Score for best b on the de-
velopment set with singleton mention filter for the
comparison. In Table 4, ?-? indicates that we could
not find published results for those cases. We see
2123
that results of the Prune-and-Score approach are
comparable to or better than the state-of-the-art in-
cluding Only-Scoring.
7 Conclusions and Future Work
We introduced the Prune-and-Score approach for
greedy coreference resolution whose main idea
is to learn a pruning function along with a scor-
ing function to effectively guide the search. We
showed that our approach improves over the meth-
ods that only learn a scoring function, and gives
comparable or better results than several state-of-
the-art coreference resolution systems.
Our Prune-and-Score approach is a particular
instantiation of the general idea of learning nearly-
sound constraints for pruning, and leveraging the
learned constraints to learn improved heuristic
functions for guiding the search (See (Chen et
al., 2014) for another instantiation of this idea for
multi-object tracking in videos). Therefore, oth-
er coreference resolution systems (Chang et al.,
2013; Durrett and Klein, 2013; Bj?orkelund and
Kuhn, 2014) can also benefit from this idea. One
way to further improve the peformance of our
approach is to perform a search in the Limited
Discrepancy Search (LDS) space (Doppa et al.,
2014b) using the learned functions.
Future work should apply this general idea to
other natural language processing tasks including
dependency parsing (Nivre et al., 2007) and in-
formation extraction (Li et al., 2013). We would
expect more beneficial behavior with the prun-
ing constraints for problems with large action sets
(e.g., labeled dependency parsing). It would be in-
teresting and useful to generalize this approach to
search spaces where there are multiple target paths
from the initial state to the terminal state, e.g., as
in the Easy-first framework.
Acknowledgments
Authors would like to thank Veselin Stoyanov
(JHU) for answering several questions related to
the Easy-first and Reconcile systems; Van Dang
(UMass, Amherst) for technical discussions relat-
ed to the RankLib library; Kai-Wei Chang (UIUC)
for the help related to the Illinois-Coref mention
extractor; and Greg Durrett (UC Berkeley) for his
help with the Berkeley system. This work was
supported in part by NSF grants IIS 1219258, I-
IS 1018490 and in part by the Defense Advanced
Research Projects Agency (DARPA) and the Air
Force Research Laboratory (AFRL) under Con-
tract No. FA8750-13-2-0033. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the views of the NS-
F, the DARPA, the Air Force Research Laboratory
(AFRL), or the US government.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP), pages 294?303.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolution
with latent antecedents and non-local features. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 47?57, Baltimore, Maryland,
June. Association for Computational Linguistics.
Christopher Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Microsoft
Technical Report, (MSR-TR-2010).
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
Coref: The UI system in the CoNLL-2012 shared
task. In Joint Conference on EMNLP and CoNLL
- Shared Task, pages 113?117, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for
coreference resolution. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 601?612.
Sheng Chen, Alan Fern, and Sinisa Todorovic. 2014.
Multi-object tracking via constrained sequential la-
beling. In To appear in Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR).
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proceedings of In-
ternational Joint Conference on Artificial Intelli-
gence (IJCAI), pages 671?676.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of Inter-
national Conference on Machine Learning (ICML),
pages 175?182.
2124
Aron Culotta, Michael L. Wick, and Andrew Mc-
Callum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL), pages 81?88.
Hal Daum?e III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 660?669.
Janardhan Rao Doppa, Alan Fern, and Prasad Tadepal-
li. 2014a. HC-Search: A learning framework for
search-based structured prediction. Journal of Arti-
ficial Intelligence Research (JAIR), 50:369?407.
Janardhan Rao Doppa, Alan Fern, and Prasad Tade-
palli. 2014b. Structured prediction via output s-
pace search. Journal of Machine Learning Research
(JMLR), 15:1317?1350.
Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern,
and Prasad Tadepalli. 2014c. HC-Search for multi-
label prediction: An empirical study. In Proceed-
ings of AAAI Conference on Artificial Intelligence
(AAAI).
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1971?
1982.
Greg Durrett, David Leo Wright Hall, and Dan Klein.
2013. Decentralized entity-level modeling for coref-
erence resolution. In Proceedings of Association of
Computational Linguistics (ACL) Conference, pages
114?124.
Pedro F. Felzenszwalb and David A. McAllester. 2007.
The generalized A* architecture. Journal of Artifi-
cial Intelligence Research (JAIR), 29:153?190.
Eraldo Rezende Fernandes, C??cero Nogueira dos San-
tos, and Ruy Luiz Milidi?u. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. International Conference on
Computational Natural Language Learning (CoNL-
L), pages 41?48.
Thomas Finley and Thorsten Joachims. 2005. Su-
pervised clustering with support vector machines.
In Proceedings of International Conference on Ma-
chine Learning (ICML), pages 217?224.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403?414.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics (HLT-NAACL).
Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld,
and Luke S. Zettlemoyer. 2013. Joint corefer-
ence resolution and named-entity linking with multi-
pass sieves. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 289?299.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning Journal (MLJ), 75(3):297?325.
Hal Daum?e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In ICML.
Roni Khardon. 1999. Learning to take actions. Ma-
chine Learning Journal (MLJ), 35(1):57?90.
Michael Lam, Janardhan Rao Doppa, Xu Hu, Sinisa
Todorovic, Thomas Dietterich, Abigail Reft, and
Marymegan Daly. 2013. Learning to detect basal
tubules of nematocysts in sem images. In ICCV
Workshop on Computer Vision for Accelerated Bio-
sciences (CVAB). IEEE.
Heeyoung Lee, Angel X. Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885?916.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (A-
CL), pages 73?82.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empir-
ical Methods in Natural Language Processing, HLT
?05, pages 25?32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Andrew Mccallum and Ben Wellner. 2003. To-
ward conditional models of identity uncertainty with
application to proper noun coreference. In Pro-
ceedings of Neural Information Processing Systems
(NIPS), pages 905?912. MIT Press.
MUC6. 1995. Coreference task definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
2125
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of Association of Computation-
al Linguistics (ACL) Conference, pages 104?111.
NIST. 2004. The ACE evaluation plan.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in ontonotes. In Proceedings of the
Joint Conference on EMNLP and CoNLL: Shared
Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2011a. Coreference
resolution with world knowledge. In Proceedings
of Association of Computational Linguistics (ACL)
Conference, pages 814?824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research (JAIR), 40:469?521.
Lev-Arie Ratinov and Dan Roth. 2012. Learning-
based multi-sieve co-reference resolution with
knowledge. In Proceedings of Empirical Methods
in Natural Language Processing (EMNLP) Confer-
ence, pages 1234?1244.
St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. Jour-
nal of Machine Learning Research - Proceedings
Track, 15:627?635.
Wee Meng Soon, Daniel Chung, Daniel Chung Yong
Lim, Yong Lim, and Hwee Tou Ng. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of Inter-
national Conference on Computational Linguistics
(COLING), pages 2519?2534.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Proceed-
ings of Association of Computational Linguistics (A-
CL) Conference, pages 156?161.
Marc B. Vilain, John D. Burger, John S. Aberdeen,
Dennis Connolly, and Lynette Hirschman. 1995.
A model-theoretic coreference scoring scheme. In
MUC, pages 45?52.
David Weiss and Benjamin Taskar. 2010. Structured
prediction cascades. Journal of Machine Learning
Research - Proceedings Track, 9:916?923.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. SampleRank: Training factor graphs with
atomic gradients. In Proceedings of International
Conference on Machine Learning (ICML).
Michael L. Wick, Sameer Singh, and Andrew McCal-
lum. 2012. A discriminative hierarchical model for
fast coreference at large scale. In Proceedings of As-
sociation of Computational Linguistics (ACL) Con-
ference, pages 379?388.
Yuehua Xu, Alan Fern, and Sung Wook Yoon. 2009.
Learning linear ranking functions for beam search
with application to planning. Journal of Machine
Learning Research (JMLR), 10:1571?1610.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of International Conference on Ma-
chine Learning (ICML).
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolu-
tion. In Conference on Computational Natural Lan-
guage Learning (CoNLL).
2126
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 70?77,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Learning Rules from Natural Texts
Janardhan Rao Doppa, Mohammad NasrEsfahani, Mohammad S. Sorower
Thomas G. Dietterich, Xiaoli Fern, and Prasad Tadepalli
School of EECS, Oregon State University
Corvallis, OR 97330, USA
fdoppa,nasresfm,sorower,tgd,xfern,tadepallg@cs.orst.edu
Abstract
In this paper, we consider the problem of in-
ductively learning rules from specific facts ex-
tracted from texts. This problem is challeng-
ing due to two reasons. First, natural texts
are radically incomplete since there are always
too many facts to mention. Second, natural
texts are systematically biased towards nov-
elty and surprise, which presents an unrep-
resentative sample to the learner. Our solu-
tions to these two problems are based on build-
ing a generative observation model of what is
mentioned and what is extracted given what
is true. We first present a Multiple-predicate
Bootstrapping approach that consists of it-
eratively learning if-then rules based on an
implicit observation model and then imput-
ing new facts implied by the learned rules.
Second, we present an iterative ensemble co-
learning approach, where multiple decision-
trees are learned from bootstrap samples of
the incomplete training data, and facts are im-
puted based on weighted majority.
1 Introduction
One of the principal goals of learning by reading
is to make the vast amount of natural language text
 This material is based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. FA8750-09-C-0179. Any opinions, findings and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of
the DARPA, or the Air Force Research Laboratory (AFRL). We
thank the reviewers for their insightful comments and helpful
suggestions.
which is on the web accessible to automatic process-
ing. There are at least three different ways in which
this can be done. First, factual knowledge on the
web can be extracted as formal relations or tuples
of a data base. A number of information extraction
systems, starting from the WebKb project (Craven et
al., 2000), to Whirl (Cohen, 2000) to the TextRunner
(Etzioni et al, 2008) project are of this kind. They
typically learn patterns or rules that can be applied
to text to extract instances of relations. A second
possibility is to learn general knowledge, rules, or
general processes and procedures by reading natural
language descriptions of them, for example, extract-
ing formal descriptions of the rules of the United
States Senate or a recipe to make a dessert. A third
instance of machine reading is to generalize the facts
extracted from the text to learn more general knowl-
edge. For example, one might learn by generalizing
from reading the obituaries that most people live less
than 90 years, or people tend to live and die in the
countries they were born in. In this paper, we con-
sider the problem of learning such general rules by
reading about specific facts.
At first blush, learning rules by reading specific
facts appears to be a composition of information ex-
traction followed by rule induction. In the above ex-
ample of learning from obituaries, there is reason to
believe that this reductionist approach would work
well. However, there are two principal reasons why
this approach of learning directly from natural texts
is problematic. One is that, unlike databases, the nat-
ural texts are radically incomplete. By this we mean
that many of the facts that are relevant to predicting
the target relation might be missing in the text. This
70
is so because in most cases the set of relevant facts
is open ended.
The second problem, in some ways more worri-
some, is that the natural language texts are systemat-
ically biased towards newsworthiness, which corre-
lates with infrequency or novelty. This is sometimes
called ?the man bites a dog phenomenon.?1 Un-
fortunately the novelty bias violates the most com-
mon assumption of machine learning that the train-
ing data is representative of the underlying truth, or
equivalently, that any missing information is miss-
ing at random. In particular, since natural langauge
texts are written for people who already possess a
vast amount of prior knowledge, communication ef-
ficiency demands that facts that can be easily in-
ferred by most people are left out of the text.
To empirically validate our two hypotheses of rad-
ical incompleteness and systematic bias of natural
texts, we have examined a collection of 248 doc-
uments related to the topics of people, organiza-
tions, and relationships collected by the Linguistic
Data Consortium (LDC). We chose the target rela-
tionship of the birth place of a person. It turned
out that the birth place of some person is only men-
tioned 23 times in the 248 documents, illustrating
the radical incompleteness of texts mentioned ear-
lier. Moreover, in 14 out of the 23 mentions of the
birth place, the information violates some default in-
ferences. For example, one of the sentences reads:
?Ahmed Said Khadr, an Egyptian-born Cana-
dian, was killed last October in Pakistan.?
Presumably the phrase ?Egyptian-born? was con-
sidered important by the reporter because it vio-
lates our expectation that most Canadians are born
in Canada. If Khadr was instead born in Canada,
the reporter would mostly likely have left out
?Canadian-born? because it is too obvious to men-
tion given he is a Canadian. In all the 9 cases where
the birth place does not violate the default assump-
tions, the story is biographical, e.g., an obituary.
In general, only a small part of the whole truth is
ever mentioned in a given document. Thus, the re-
porter has to make some choices as to what to men-
tion and what to leave out. The key insight of this
paper is that considering how these choices are made
1
?When a dog bites a man, that is not news, because it hap-
pens so often. But if a man bites a dog, that is news,? attributed
to John Bogart of New York Sun among others.
is important in making correct statistical inferences.
In the above example, wrong probabilities would be
derived if one assumes that the birth place informa-
tion is missing at random.
In this paper we introduce the notion of a ?men-
tion model,? which models the generative process
of what is mentioned in a document. We also ex-
tend this using an ?extraction model,? which rep-
resents the errors in the process of extracting facts
from the text documents. The mention model and
the extraction model together represent the probabil-
ity that some facts are extracted given the true facts.
For learning, we could use an explicit mention
model to score hypothesized rules by calculating the
probability that a rule is satisfied by the observed
evidence and then pick the rules that are most likely
given the evidence. In this paper, we take the sim-
pler approach of directly adapting the learning al-
gorithms to an implicit mention model, by changing
the way a rule is scored by the available evidence.
Since each text document involves multiple pred-
icates with relationships between them, we learn
rules to predict each predicate from the other pred-
icates. Thus, the goal of the system is to learn a
sufficiently large set of rules to infer all the miss-
ing information as accurately as possible. To ef-
fectively bootstrap the learning process, the learned
rules are used on the incomplete training data to im-
pute new facts, which are then used to induce more
rules in subsequent iterations. This approach is most
similar to the coupled semi-supervised learning of
(Carlson et al, 2010) and general bootstrapping ap-
proaches in natural language processing (Yarowsky,
1995). Since this is in the context of multiple-
predicate learning in inductive logic programming
(ILP) (DeRaedt and Lavra?c, 1996), we call this ap-
proach ?Multiple-predicate Bootstrapping.?
One problem with Multiple-predicate Bootstrap-
ping is potentially large variance. To mitigae this,
we consider the bagging approach, where multi-
ple rule sets are learned from bootstrap samples of
the training data with an implicit mention model to
score the rules. We then use these sets of rules as an
ensemble to impute new facts, and repeat the pro-
cess.
We evaluate both of these approaches on real
world data processed through synthetic observation
models. Our results indicate that when the assump-
71
tions of the learner suit the observation model, the
learner?s performance is quite good. Further, we
show that the ensemble approach significantly im-
proves the performance of Multiple-predicate Boot-
strapping.
2 Probabilistic Observation Model
In this section, we will introduce a notional prob-
abilistic observation model that captures what facts
are extracted by the programs from the text given
the true facts about the world and the common sense
rules of the domain of discourse.
The observation model is composed of the men-
tion model and the extraction model. The men-
tion model P (MentDBjTrueDB;Rules) mod-
els the probability distribution of mentioned facts,
MentDB, given the set of true facts TrueDB and
the rules of the domain, Rules. For example, if
a fact is always true, then the novelty bias dictates
that it is not mentioned with a high probability. The
same is true of any fact entailed by a generally valid
rule that is common knowledge. For example, this
model predicts that since it is common knowledge
that Canadians are born in Canada, the birth place
is not mentioned if a person is a Canadian and was
born in Canada.
The extraction model P (ExtrDBjMentDB)
models the probability distribution of extracted
facts, given the set of mentioned facts MentDB.
For example, it might model that explicit facts are
extracted with high probability and that the extracted
facts are corrupted by coreference errors. Note that
the extraction process operates only on the men-
tioned part of the database MentDB; it has no in-
dependent access to the TrueDB or the Rules. In
other words, the mentioned database MentDB d-
separates the extracted database ExtrDB from the
true database TrueDB and the Rules, and the con-
ditional probability decomposes.
We could also model multiple documents gener-
ated about the same set of facts TrueDB, and multi-
ple databases independently extracted from the same
document by different extraction systems. Given an
explicit observation model, the learner can use it to
consider different rule sets and evaluate their like-
lihood given some data. The posterior probability
of a rule set given an extracted database can be ob-
tained by marginalizing over possible true and men-
tioned databases. Thus, in principle, the maximum
likelihood approach to rule learning could work by
considering each set of rules and evaluating its pos-
terior given the extracted database, and picking the
best set. While conceptually straightforward, this
approach is highly intractable due to the need to
marginalize over all possible mentioned and true
databases. Moreover, it seems unnecessary to force
a choice between sets of rules, since different rule
sets do not always conflict. In the next section, we
describe a simpler approach of adapting the learning
algorithms directly to score and learn rules using an
implicit mention model.
3 Multiple-predicate Bootstrapping with
an Implicit Mention Model
Our first approach, called ?Multiple-predicate Boot-
strapping,? is inspired by several pieces of work
including co-training (Blum and Mitchell, 1998),
multitask learning (Caruana, 1997), coupled semi-
supervised learning (Carlson et al, 2010) and self-
training (Yarowsky, 1995). It is based on learning a
set of rules for all the predicates in the domain given
the others and using them to infer (impute) the miss-
ing facts in the training data. This is repeated for
several iterations until no more facts can be inferred.
The support of a rule is measured by the number of
records which satisfy the body of the rule, where
each record roughly corresponds to a collection of
related facts that can be independently generated,
e.g., information about a single football game or a
single news item. The higher the support, the more
statistical evidence we have for judging its predictive
accuracy. To use a rule to impute facts, it needs to
be ?promoted,? which means it should pass a certain
threshold support level. We measure the precision of
a rule as the ratio of the number of records that non-
trivially satisfy the rule to the number that satisfy its
body, which is a proxy for the conditional probabil-
ity of the head given the body. A rule is non-trivially
satisfied by a record if the rule evaluates to true on
that record for all possible instantiations of its vari-
ables, and there is at least one instantiation that sat-
isfies its body. Given multiple promoted rules which
apply to a given instance, we pick the rule with the
highest precision to impute its value.
72
3.1 Implicit Mention Models
We adapt the multiple-predicate bootstrapping ap-
proach to the case of incomplete data by adjusting
the scoring function of the learning algorithm to re-
spect the assumed mention model. Unlike in the
maximum likelihood approach discussed in the pre-
vious section, there is no explicit mention model
used by the learner. Instead the scoring function is
optimized for a presumed implicit mention model.
We now discuss three specific mention models and
the corresponding scoring functions.
Positive Mention Model: In the ?positive men-
tion model,? it is assumed that any missing fact
is false. This justifies counting evidence using
the negation by failure assumption of Prolog. We
call this scoring method ?conservative.? For exam-
ple, the text ?Khadr, a Canadian citizen, was killed
in Pakistan? is counted as not supporting the rule
citizen(X,Y) ) bornIn(X,Y), as we are
not told that bornIn(Khadr,Canada). Positive
mention model is inapplicable for most instances of
learning from natural texts, except for special cases
such as directory web pages.
Novelty Mention Model: In the ?novelty mention
model,? it is assumed that facts are missing only
when they are entailed by other mentioned facts and
rules that are common knowledge. This suggests
an ?aggressive? or optimistic scoring of candidate
rules, which interprets a missing fact so that it sup-
ports the candidate rule. More precisely, a rule is
counted as non-trivially satisfied by a record if there
is some way of imputing the missing facts in the
record without causing contradiction. For exam-
ple, the text ?Khadr, a Canadian citizen was killed
in Pakistan? is counted as non-trivially supporting
the rule citizen(X,Y) ) bornIn(X,Y) be-
cause, adding bornIn(Khadr, Canada) sup-
ports the rule without contradicting the available ev-
idence. On the other hand, the above text does
not support the rule killedIn(X,Y) ) cit-
izen(X,Y) because the rule contradicts the evi-
dence, assuming that citizen is a functional rela-
tionship.
Random Mention Model: In the ?random men-
tion model,? it is assumed that facts are missing at
random. Since the random facts can be true or false,
this mention model suggests counting the evidence
fractionally in proportion to its predicted prevalence.
Following the previous work on learning from miss-
ing data, we call this scoring method ?distributional?
(Saar-Tsechansky and Provost, 2007). In distribu-
tional scoring, we typically learn a distribution over
the values of a literal given its argument and use it to
assign a fractional count to the evidence. This is the
approach taken to account for missing data in Quin-
lan?s decision tree algorithm (Quinlan, 1986). We
will use this as part of our Ensemble Co-Learning
approach of the next section.
3.2 Experimental Results
We evaluated Multiple-predicate Bootstrap-
ping with implicit mention models on the
schema-based NFL database retrieved from
www.databasefootball.com. We developed
two different synthetic observation models. The
observation models are based on the Novelty
mention model and the Random mention model
and assume perfect extraction in each case. The
following predicates are manually provided:
 gameWinner (Game, Team),
 gameLoser(Game, Team),
 homeTeam(Game, Team),
 awayTeam(Game, Team), and
 teamInGame(Team, Game),
with the natural interpretations. To simplify arith-
metic reasoning we replaced the numeric team
scores in the real database with two defined predi-
cates teamSmallerScore(Team, Game) and
teamGreaterScore(Team, Game) to indi-
cate the teams with the smaller and the greater
scores.
We generate two sets of synthetic data as follows.
In the Random mention model, each predicate ex-
cept the teamInGame predicate is omitted inde-
pendently with probability p. The Novelty men-
tion model, on the other hand, relies on the fact
that gameWinner, gameLoser, and teamFi-
nalScore are mutually correlated, as are home-
Team and awayTeam. Thus, it picks one predicate
73
05 0
1 0 0
1 5 0
2 0 0 0
0 . 2
0 . 4
0 . 6
0 . 8
0
0 . 5
1
F r a c t i o n
o f m
i s
s
i
n g p
r e d
i
c
a
t
e
s
S u p
p
o r t
t h
r e s
h
o l d
A c
c
u
r
a
c
y
M u l t i p l e ? p r e d i c a t e B o o t s t r a p p i n g R e s u l t s f o r F A R M E R
( A g g r e s s i v e ? N o v e l t y M o d e l )
0
5 0
1 0 0
1 5 0
2 0 0 0 0 . 2
0 . 4 0 . 6
0 . 8
0 . 8
0 . 8 5
0 . 9
0 . 9 5
1
M u l t i p l e ? p r e d i c a t e B o o t s t r a p p i n g R e s u l t s f o r F O I L
( A g g r e s s i v e ? N o v e l t y M o d e l )
F r a c t i o n
o f m
i s
s
i
n g p r e
d
i
c a
t
e
s
S u p
p
o r t
t h
r e s
h
o l d
A c
c
u
r
a
c
y
(a) (b)
 0
 20
 40
 60
 80
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6
Ac
cu
ra
cy
Fraction of missing predicates
Multiple-predicate Bootstrapping Results for FARMER 
 (Support threshold = 120)
Novelty-Aggressive
Random-Aggressive
Random-Conservative
Novelty-Conservative
 0
 20
 40
 60
 80
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6
Ac
cu
ra
cy
Fraction of missing predicates
Multiple-predicate Bootstrapping Results for FOIL 
 (Support threshold = 120)
Novelty-Aggressive
Random-Aggressive
Random-Conservative
Novelty-Conservative
(c) (d)
Figure 1: Multiple-predicate bootstrapping Results for (a) FARMER using aggressive-novelty model (b) FOIL using
aggressive-novelty model (c) FARMER with support threshold 120 (d) FOIL with support threshold 120
from the first group to mention its values, and omit-
seach of the other predicates independently with
some probability q. Similarly it gives a value to one
of the two predicates in the second group and omits
the other predicate with probability q. One conse-
quence of this model is that it always has one of the
predicates in the first group and one of the predicates
in the second group, which is sufficient to infer ev-
erything if one knew the correct domain rules. We
evaluate two scoring methods: the aggressive scor-
ing and the conservative scoring.
We employed two learning systems: Quinlan?s
FOIL, which learns relational rules using a greedy
covering algorithm (Quinlan, 1990; Cameron-
Jones and Quinlan, 1994), and Nijssen and Kok?s
FARMER, which is a relational data mining algo-
rithm that searches for conjunctions of literals of
large support using a bottom-up depth first search
(Nijssen and Kok, 2003). Both systems were applied
to learn rules for all target predicates. One important
difference to note here is that while FARMER seeks
all rules that exceed the necessary support threshold,
FOIL only learns rules that are sufficient to classify
all training instances into those that satisfy the tar-
get predicate and those that do not. Secondly, FOIL
tries to learn maximally deterministic rules, while
FARMER is parameterized by the minimum preci-
sion of a rule. We have not modified the way they
interpret missing features during learning. However,
after the learning is complete, the rules learned by
both approaches are scored by interpreting the miss-
ing data either aggressively or conservatively as de-
scribed in the previous section.
We ran both systems on synthetic data generated
using different parameters that control the fraction
of missing data and the minimum support threshold
74
needed for promotion. In Figures 1(a) and 1(b), the
X and Y-axes show the fraction of missing predi-
cates and the support threshold for the novelty men-
tion model and aggressive scoring of rules for FOIL
and FARMER. On the Z-axis is the accuracy of pre-
dictions on the missing data, which is the fraction
of the total number of initially missing entries that
are correctly imputed. We can see that aggressive
scoring of rules with the novelty mention model per-
forms very well even for large numbers of missing
values for both FARMER and FOIL. FARMER?s
performance is more robust than FOIL?s because
FARMER learns all correct rules and uses whichever
rule fires. For example, in the NFL domain, it
could infer gameWinner from gameLoser or
teamSmallerScore or teamGreaterScore.
In contrast, FOIL?s covering strategy prevents it
from learning more than one rule if it finds one per-
fect rule. The results show that FOIL?s performance
degrades at larger fractions of missing data and large
support thresholds.
Figures 1(c) and 1(d) show the accuracy of pre-
diction vs. percentage of missing predicates for
each of the mention models and the scoring meth-
ods for FARMER and FOIL for a support threshold
of 120. They show that agressive scoring clearly out-
performs conservative scoring for data generated us-
ing the novelty mention model. In FOIL, aggressive
scoring also seems to outperform conservative scor-
ing on the dataset generated by the random mention
model at high levels of missing data. In FARMER,
the two methods perform similarly. However, these
results should be interpreted cautiously as they are
derived from a single dataset which enjoys determin-
istic rules. We are working towards a more robust
evaluation in multiple domains as well as data ex-
tracted from natural texts.
4 Ensemble Co-learning with an Implicit
Mention Model
One weakness of Multiple-predicate Bootstrapping
is its high variance especially when significant
amounts of training data are missing. Aggressive
evaluation of rules in this case would amplify the
contradictory conclusions of different rules. Thus,
picking only one rule among the many possible rules
could lead to dangerously large variance.
One way to guard against the variance problem
is to use an ensemble approach. In this section we
test the hypothesis that an ensemble approach would
be more robust and exhibit less variance in the con-
text of learning from incomplete examples with an
implicit mention model. For the experiments in this
section, we employ a decision tree learner that uses a
distributional scoring scheme to handle missing data
as described in (Quinlan, 1986).
While classifying an instance, when a missing
value is encountered, the instance is split into multi-
ple pseudo-instances each with a different value for
the missing feature and a weight corresponding to
the estimated probability for the particular missing
value (based on the frequency of values at this split
in the training data). Each pseudo-instance is passed
down the tree according to its assigned value. Af-
ter reaching a leaf node, the frequency of the class
in the training instances associated with this leaf is
returned as the class-membership probability of the
pseudo-instance. The overall estimated probability
of class membership is calculated as the weighted
average of class membership probabilities over all
pseudo-instances. If there is more than one missing
value, the process recurses with the weights com-
bining multiplicatively. The process is similar at
the training time, except that the information gain
at the internal nodes and the class probabilities at
the leaves are calculated based on the weights of the
relevant pseudo-instances.
We use the confidence level for pruning a decision
tree as a proxy for support of a rule in this case. By
setting this parameter to different values, we can ob-
tain different degrees of pruning.
Experimental Results: We use the Congres-
sional Voting Records2 database for our experi-
ments. The (non-text) database includes the party
affiliation and votes on 16 measures for each mem-
ber of the U.S House Representatives. Although this
database (just like the NFL database) is complete,
we generate two different synthetic versions of it to
simulate the extracted facts from typical news sto-
ries on this topic. We use all the instances including
those with unknown values for training, but do not
count the errors on these unknown values. We ex-
2http://archive.ics.uci.edu/ml/datasets/
Congressional+Voting+Records
75
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6
Ac
cu
ra
cy
Percentage of missing values
Ensemble Co-learning with Random model
Ensemble size 1
Ensemble size 5
Ensemble size 10
Ensemble size 15
Ensemble size 20
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6
Ac
cu
ra
cy
Percentage of missing values
Ensemble Co-learning with Novelty model
Ensemble size 1
Ensemble size 5
Ensemble size 10
Ensemble size 15
Ensemble size 20
(a) (b)
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6
Ac
cu
ra
cy
Percentage of missing values
Ensemble Co-learning vs Multiple-predicate Bootstrapping 
 (Random model)
Multiple-predicate Bootstrapping
Ensemble Colearning with size 20
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.1  0.2  0.3  0.4  0.5
Ac
cu
ra
cy
Percentage of missing values
Ensemble Co-learning vs Multiple-predicate Bootstrapping 
 (Novelty model)
Multiple-predicate Bootstrapping
Ensemble Colearning with size 20
(c) (d)
Figure 2: Results for (a) Ensemble co-learning with Random mention model (b) Ensemble co-learning with Nov-
elty mention model (c) Ensemble co-learning vs Multiple-predicate Bootstrapping with Random mention model (d)
Ensemble co-learning vs Multiple-predicate Bootstrapping with Novelty mention model
periment with two different implicit mention mod-
els: Random and Novelty. These are similar to those
we defined in the previous section. In the Random
mention model, each feature in the dataset is omitted
independently with a probability p. Since we don?t
know the truely predictive rules here unlike in the
football domain, we learn the novelty model from
the complete dataset. Using the complete dataset
which has n features, we learn a decision tree to pre-
dict each feature from all the remaining features. We
use these n decision trees to define our novelty men-
tion model in the following way. For each instance
in the complete dataset, we randomly pick a feature
and see if it can be predicted from all the remain-
ing features using the predictive model. If it can
be predicted, then we will omit it with probability
p and mention it otherwise. We use different boot-
strap samples to learn the ensemble of trees and im-
pute the values using a majority vote. Note that, the
decision tree cannot always classify an instance suc-
cessfully. Therefore, we will impute the values only
if the count of majority vote is greater than some
minimum threshold (margin). In our experiments,
we use a margin value equal to half of the ensem-
ble size and a fixed support of 0.3 (i.e., the confi-
dence level for pruning) while learning the decision
trees. We employ J48, the WEKA version of Quin-
lan?s C4.5 algorithm to learn our decision trees. We
compute the accuracy of predictions on the missing
data, which is the fraction of the total number of ini-
tially missing entries that are imputed correctly. We
report the average results of 20 independent runs.
We test the hypothesis that the Ensemble Co-
learning is more robust and exhibit less variance
in the context of learning from incomplete exam-
ples when compared to Multiple-predicate Boot-
76
strapping. In Figures 2(a)-(d), the X and Y-axes
show the percentage of missing values and the pre-
diction accuracy. Figures 2(a) and (b) shows the be-
havior with different ensemble sizes (1, 5, 10, 15 and
20) for both Random and Novelty mention model.
We can see that the performance improves as the
ensemble size grows for both random and novelty
models. Figures 2(c) and (d) compares Multiple-
predicate Bootstrapping with the best results over
the different ensemble sizes. We can see that En-
semble Co-learning outperforms Multiple-predicate
Bootstrapping.
5 Discussion
Learning general rules by reading natural language
texts faces the challenges of radical incompleteness
and systematic bias. Statistically, our notion of in-
completeness corresponds to the Missing Not At
Random (MNAR) case, where the probability of an
entry being missed may depend on its value or the
values of other observed variables (Rubin, 1976).
One of the key insights of statisticians is to
build an explicit probabilistic model of missingness,
which is captured by our mention model and ex-
traction model. This missingness model might then
be used in an Expectation Maximization (EM) ap-
proach (Schafer and Graham, 2002), where alter-
nately, the missing values are imputed by their ex-
pected values according to the missingness model
and the model parameters are estimated using the
maximum likelihood approach. Our ?Multiple-
predicate Bootstrapping? is loosely analogous to this
approach, except that the imputation of missing val-
ues is done implicitly while scoring the rules, and
the maximum likelihood parameter learning is re-
placed with the learning of relational if-then rules.
In the Multiple Imputation (MI) framework of
(Rubin, 1987; Schafer and Graham, 2002), the goal
is to reduce the variance due to single imputation
by combining the results of multiple imputations.
This is analogous to Ensemble Co-learning, where
we learn multiple hypotheses from different boot-
strap samples of the training data and impute values
using the weighted majority algorithm over the en-
semble. We have shown that the ensemble approach
improves performance.
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92?100. Morgan Kaufmann Pub-
lishers.
R. M. Cameron-Jones and J. R. Quinlan. 1994. Effi-
cient top-down induction of logic programs. In ACM
SIGART Bulletin.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM 2010).
R. Caruana. 1997. Multitask learning: A knowledge-
based source of inductive bias. Machine Learning,
28:41?75.
William W. Cohen. 2000. WHIRL: a word-based infor-
mation representation language. Artif. Intell., 118(1-
2):163?196.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew
McCallum, Tom M. Mitchell, Kamal Nigam, and Sea?n
Slattery. 2000. Learning to construct knowledge bases
from the world wide web. Artif. Intell., 118(1-2):69?
113.
Luc DeRaedt and Nada Lavra?c. 1996. Multiple predi-
cate learning in two inductive logic programming set-
tings. Logic Journal of the IGPL, 4(2):227?254.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74.
Siegfried Nijssen and Joost N. Kok. 2003. Efficient fre-
quent query discovery in FARMER. In PKDD, pages
350?362.
Ross Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1(1):81?106.
Ross Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5:239?266.
D. B. Rubin. 1976. Inference and missing data.
Biometrika, 63(3):581.
D. B. Rubin. 1987. Multiple Imputation for non-
response in surveys. Wiley New York.
Maytal Saar-Tsechansky and Foster Provost. 2007.
Handling missing values when applying classifica-
tion models. Journal of Machine Learning Research,
8:1625?1657.
J. L. Schafer and J. W. Graham. 2002. Missing data: Our
view of the state of the art. Psychological methods,
7(2):147?177.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of ACL.
77
